Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 124?133,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsNatural Language Generation with Vocabulary ConstraintsBen SwansonBrown UniversityProvidence, RIchonger@cs.brown.eduElif YamangilGoogle Inc.Mountain View, CAleafer@google.comEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduAbstractWe investigate data driven natural lan-guage generation under the constraintsthat all words must come from a fixed vo-cabulary and a specified word must ap-pear in the generated sentence, motivatedby the possibility for automatic genera-tion of language education exercises.
Wepresent fast and accurate approximationsto the ideal rejection samplers for theseconstraints and compare various sentencelevel generative language models.
Ourbest systems produce output that is withhigh frequency both novel and error free,which we validate with human and auto-matic evaluations.1 IntroductionFreeform data driven Natural Language Genera-tion (NLG) is a topic explored by academics andartists alike, but motivating its empirical study is adifficult task.
While many language models usedin statistical NLP are generative and can easilyproduce sample sentences by running their ?gen-erative mode?, if all that is required is a plausiblesentence one might as well pick a sentence at ran-dom from any existing corpus.NLG becomes useful when constraints existsuch that only certain sentences are valid.
Themajority of NLG applies a semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their specificmeaning.We study two constraints concerning the wordsthat are allowed in a sentence.
The first sets afixed vocabulary such that only sentences whereall words are in-vocab are allowed.
The seconddemands not only that all words are in-vocab,but also requires the inclusion of a specific wordsomewhere in the sentence.These constraints are natural in the constructionof language education exercises, where studentshave small known vocabularies and exercises thatreinforce the knowledge of arbitrary words are re-quired.
To provide an example, consider a Chi-nese teacher composing a quiz that asks studentsto translate sentences from English to Chinese.The teacher cannot ask students to translate wordsthat have not been taught in class, and would likeensure that each vocabulary word from the currentbook chapter is included in at least one sentence.Using a system such as ours, she could easily gen-erate a number of usable sentences that contain agiven vocab word and select her favorite, repeat-ing this process for each vocab word until the quizis complete.The construction of such a system presents twoprimary technical challenges.
First, while highlyparameterized models trained on large corpora area good fit for data driven NLG, sparsity is stillan issue when constraints are introduced.
Tradi-tional smoothing techniques used for predictionbased tasks are inappropriate, however, as they lib-erally assign probability to implausible text.
Weinvestigate smoothing techniques better suited forNLG that smooth more precisely, sharing proba-bility only between words that have strong seman-tic connections.The second challenge arises from the fact thatboth vocabulary and word inclusion constraintsare easily handled with a rejection sampler that re-peatedly generates sentences until one that obeysthe constraints is produced.
Unfortunately, for124models with a sufficiently wide range of outputsthe computation wasted by rejection quickly be-comes prohibitive, especially when the word in-clusion constraint is applied.
We define modelsthat sample directly from the possible outputs foreach constraint without rejection or backtracking,and closely approximate the distribution of thetrue rejection samplers.We contrast several generative systems throughboth human and automatic evaluation.
Our bestsystem effectively captures the compositional na-ture of our training data, producing error-free textwith nearly 80 percent accuracy without wastingcomputation on backtracking or rejection.
Whenthe word inclusion constraint is introduced, weshow clear empirical advantages over the simplesolution of searching a large corpus for an appro-priate sentence.2 Related WorkThe majority of NLG focuses on the satisfactionof a communicative goal, with examples such asBelz (2008) which produces weather reports fromstructured data or Mitchell et al.
(2013) which gen-erates descriptions of objects from images.
Ourwork is more similar to NLG work that concen-trates on structural constraints such as generativepoetry (Greene et al., 2010) (Colton et al., 2012)(Jiang and Zhou, 2008) or song lyrics (Wu et al.,2013) (Ramakrishnan A et al., 2009), where spec-ified meter or rhyme schemes are enforced.
Inthese papers soft semantic goals are sometimesalso introduced that seek responses to previouslines of poetry or lyric.Computational creativity is another subfield ofNLG that often does not fix an a priori meaning inits output.
Examples such as?Ozbal et al.
(2013)and Valitutti et al.
(2013) use template filling tech-niques guided by quantified notions of humor orhow catchy a phrase is.Our motivation for generation of material forlanguage education exists in work such as Sumitaet al.
(2005) and Mostow and Jang (2012), whichdeal with automatic generation of classic fill in theblank questions.
Our work is naturally comple-mentary to these efforts, as their methods require acorpus of in-vocab text to serve as seed sentences.3 Freeform GenerationFor clarity in our discussion, we phrase the sen-tence generation process in the following generalterms based around two classes of atomic units :contexts and outcomes.
In order to specify a gen-eration system, we must define1.
the set C of contexts c2.
the set O of outcomes o3.
the ?Imply?
function I(c, o)?
List[c ?
C]4.
M : derivation tree sentencewhere I(c, o) defines the further contexts impliedby the choice of outcome o for the context c. Be-ginning with a unique root context, a derivationtree is created by repeatedly choosing an outcomeo for a leaf context c and expanding c to the newleaf contexts specified by I(c, o).
M converts be-tween derivation tree and sentence text form.This is simply a convenient rephrasing of theContext Free Grammar formalism, and as suchthe systems we describe all have some equivalentCFG interpretation.
Indeed, to describe a tradi-tional CFG, let C be the set of symbols, O be therules of the CFG, and I(c, o) return a list of thesymbols on the right hand side of the rule o.
To de-fine an n-gram model, a context is a list of words,an outcome a single word, and I(c, o) can be pro-cedurally defined to drop the first element of c andappend o.To perform the sampling required for derivationtree construction we must define P (o|c).
UsingM, we begin by converting a large corpus of sen-tence segmented text into a training set of deriva-tion trees.
Maximum likelihood estimation ofP (o|c) is then as simple as normalizing the countsof the observed outcomes for each observed con-text.
However, in order to obtain contexts forwhich the conditional independence assumptionof P (o|c) is appropriate, it is necessary to con-dition on a large amount of information.
Thisleads to sparse estimates even on large amounts oftraining data, a problem that can be addressed bysmoothing.
We identify two complementary typesof smoothing, and illustrate them with the follow-ing sentences.The furry dog bit me.The cute cat licked me.An unsmoothed bigram model trained on thisdata can only generate the two sentences verba-tim.
If, however, we know that the tokens ?dog?and ?cat?
are semantically similar, we can smoothby assuming the words that follow ?cat?
are alsolikely to follow ?dog?.
This is easily handled with125traditional smoothing techniques that interpolatebetween distributions estimated for both coarse,P (w|w?1=[animal]), and fine, P (w|w?1=?dog?),contexts.
We refer to this as context smoothing.However, we would also like to capture the in-tuition that words which can be followed by ?dog?can also be followed by ?cat?, which we will calloutcome smoothing.
We extend our terminologyto describe a system that performs both types ofsmoothing with the following?
the set?C of smooth contexts c??
the set?O of smooth outcomes o??
a smoothing function SC: C ??C?
a smoothing function SO: O ?
?Odog [animal]bit[action]42135762 Related WorkThe application of structural constraints appearsin previous work in the form of generative po-etry (Greene et al., 2010) or lyrics (Wu et al.,2013), where specified meter or rhyme schemesare enforced.
?Ozbal et al.
(2013) producesfreeform text by filling templates with respect toabstract notions such as humor.3 Freeform GenerationWe first address the problem of freeform datadriven language generation directly.
We do notset a semantic goal but instead ask only that theoutput be considered a valid sentence, seeking amodel that captures the variability of language.For clarity in our discussion, we phrase thegeneration process in the following general termsbased around two classes of atomic units : Con-texts and Outcomes.
In order to specify a genera-tion system, we must define1.
the set C of contexts c2.
the set O of outcomes o3.
the ?Imply?
function I(c, o) ?
List[c ?
C]where I(c, o) defines the further contexts impliedby the choice of outcome o for the context c. Thismodel can be made probabilistic by the definitionof P (o|c), where each outcome is sampled inde-pendently given its context.
We also require theexistence of a single unique root context, and referto the result of repeated sampling of outcomes forcontexts as a derivation tree.
Finally, a mappingfrom derivation tree to surface form is required toproduce actual text.This is simply a convenient rephrasing of theContext Free Grammar formalism, and as suchthe systems we describe all have some equivalentCFG interpretation.
Indeed, to describe a tradi-tional CFG, let C be the set of nonterminals, O bethe rules of the CFG, and I(c, o) returns a list ofthe nonterminals on the right hand side of the ruleo and does not depend on c. P (o|c) would enforcethe choice of rules with appropriate lefthand sides.The Context-Outcome terms can be more natu-ral when describing other models where we do notwant to explicitly define the space of nonterminals.A simple example is an n-gram model, for whicha context is an ordered list of words, an outcomea single word, and I(c, o) can be procedurally de-fined to produce a list containg a single contextmade by dropping the first word of the previouscontext and appending the outcome to the end.This formulation is well suited to data drivenestimation from a corpus of derivation trees.While our methods are easily extended to mul-tiple derivations for each single sentence, in thiswork we assume access to a single derivation foreach sentence in our data set.
Maximum likeli-hood estimation of P (o|c) is then as simple as nor-malizing the counts of the observed outcomes foreach observed context.
However, in order to ob-tain contexts for which the conditional indepen-dence assumption of P (o|c) is appropriate, it isnecessary to condition on a large amount of in-formation.
This leads to sparse estimates even onlarge amounts of training data, a problem that canbe addressed by smoothing.We identify two complementary types ofsmoothing, and illustrate them with the followingsentences.The furry dog bit me.The cute cat licked me.Assuming a simple bigram model where con-text is the previous word and the outcome a sin-gle word, an unsmoothed model tr ined on thisdata can only generate the two sentences verba-tim.
Imagine we have some way of knowing thatthe tokens ?dog?
and ?cat?
are similar and wouldlike to leverage this fact .
In our bigram model,this amounts to the claim that the words that follow?cat?
are perhaps also likely to follow ?dog?.
Thisis easily handled with traditional smoothing tech-niques, which interpolate between distributionsestimated for both coarse, P (w|w?1=[is-animal]),and fine, P (w|w?1=?dog?
), contexts.
We refer tothis as context smoothing.However, we would also like to capture the in-tuition that words which can be followed by ?dog?can also be followed by ?cat?, which we will calloutcome smoothing.
We extend our terminologyto describe a system that performs both types ofsmoothing with the following?
the set?C of smooth contexts c??
the set?O of smooth outcomes o??
a smoothing function SC: C ??C?
a smoothing function SO: O ?
?OWe describe the generative process with the fol-lowing flowchart2 Related WorkThe application of structural constraints appearsin previous work in the form of generative po-etry (Greene et al., 2010) or lyrics (Wu et al.,2013), wher specified meter or rhyme schemesare enforce .
?Ozbal et al.
(2013) producesfreeform text by filling templates with respect toabstract notions such as humor.3 Freeform GenerationWe first address the problem of freeform datadriven language generation directly.
We do notset a semantic goal but in tead ask only that theoutput b co sidered a valid sentence, seekingm del that captures the variability of language.For clarity in our discussion, we phrase thegeneration proce s in the following general ter sbased around two classes of atomic unit : Con-texts and Outcomes.
In order to specify a genera-tion system, we must define1.
the set C of contexts c2.
the set O of outcomes o3.
the ?Imply?
function I(c, o) ?
List[c ?
C]wher I(c, o) d fines the further contexts impliedby the choice of outcome o for the context c. Thismodel can be made probabilistic by the definitionof P (o|c), where each outcome is sampled inde-pendently given its context.
W lso require theexist nce of a single un que r ot context, and referto the result of repeated sampling of outcomes f rcontexts as a derivation tree.
Fi a ly, a mappingf om derivation tree to s rface form is r quired toproduce actual text.Thi is simply a co veni nt rephrasing of theContext Free Grammar formalism, and as suchthe systems w describe all have some equiv lentCFG nterpretat on.
Indeed, to describe a tradi-tional CFG let C be the set of nonterminals, O bethe r le of the CFG, and I(c, o) returns a list ofthe nonterminals on the right hand side of the rulo and does ot d pend on c. P (o|c) would enforcethe choice of rules with appropriate lefthand sides.The C ntext-Outcome term can be re natu-ral wh n describing other models where we do nowant to expl cit y define the space of onterm nals.A simple example is an n-gram model, for whicha context is an ordered list of words, an outcomea single word, and I(c, o) can be procedurally d -fined to pr duce a list containg a single contextmade by dropping the first word of the previouscon ext and appending the outcome to the end.This formulation is well suited to data drivenestim f om a corpus of derivation trees.While our methods are easily extended to mul-tiple derivations for each single sentence, in thiswork we assume access to a single derivation foreach sentence in our data set.
Maximum likeli-hood estim tion of P (o|c) is then as simple as nor-malizing th counts of the observed outcomes foreach bs rved context.
However, in order to ob-tain contexts for which the conditional indepen-dence assump ion of P (o|c) is appropriate, it isn cessary t condition on a large amount of in-for ion.
This leads to sparse estimates even onla ge a ounts of training data, a problem that canbe addressed by smoothing.We identify two complementary types ofsmoothing, and illustrate them with the followingsentences.The furry dog bit me.The cute cat licked me.Assuming a simple bigram model where con-text is the previous word and the outcome a sin-gle wor , an unsmooth d model trained n thisdata can only generate the two sentences verba-tim.
Imagine we have some way of knowing thatthe toke s ?d g?
and ?cat?
are similar and wouldlike to leverag this fact .
In our bigram model,this amounts to the claim that the words that follow?cat?
are perhaps also likely t follow ?dog?.
Thisis easily handled with traditional smoothing tech-niques, which interpolate between distributionsestimated for both coarse, P (w|w?1=[is-animal]),and fine, P (w|w?1=?dog?
), contexts.
We refer tothis as context smoothing.However, we would also like to capture the in-tuition that words which can be followed by ?dog?can also be followed by ?cat?, which we will calloutcome sm thing.
We extend our terminologyto describe a system that performs both types ofsmoothing with the following?
the set?C of smooth contexts c??
the set?O of smooth outcomes o??
a smoothing function SC: C ??C?
a smoothing function SO: O ?
?OWe describe the generative process with the fol-lowing flowchart2 Related WorkThe application of structural constraints appearsin previous work in the form of generative po-etry (Greene et al., 2010) or lyrics (Wu et al.,2013), where specified meter or rhyme schemesare enforced.
?Ozbal et al.
(2013) producesfreeform text by filling templates with respect toabstract notions such as humor.3 Freeform GenerationWe first address the problem of freeform datadriven language generation directly.
We do notset a s mantic goal but instead ask only that theoutput be onsidered a valid sentence, seeking amodel that captures the variability of language.For clarity in our discussion, we phrase thegeneration process in the following general termsbased around two classes of atomic units : Con-texts and Outcomes.
In order to specify a genera-tion system, we must define1.
the set C of cont xts c2.
the set O of outco es o3.
the ?Imply?
function I(c, o) ?
List[c ?
C]where I(c, o) define the fur r contexts impli dby the choice of outcome o for the context .
Thismodel can be made probabi istic by the definitionof P (o|c), wher each outcome is sampled inde-pend ntly give its context.
We also r quire theexiste ce of a single unique ro t context, and referto esult of r peated samp ing of utcomes forcontexts as a derivati n tree.
Finally, a mappingfrom derivation tree to sur ac form is required toproduce actual text.T is is simply a convenient rephrasing of theCo t xt Free Grammar formalism, and as sucthe systems we describe all hav some equivalentCFG interpret tion.
Indeed, to describe a tradi-tional CFG, let C be the set of onterminals, O bethe rules of the CFG, and I( , o) returns a list ofthe nonterminals on the right hand side of the ruleo and does not depend on c. P (o|c) would enforcethe choice of rules with appropriate lefthand sides.The Context-Outcome terms can be more natu-ral when describing other models where we do notwant to explicitly define the space of nonterminals.A simple example is an n-gram model, for whicha context is an ordered list of words, an outcomea single word, and I(c, o) can be procedurally de-fined to produce a list containg a single contextmade by dropping the first word of the pr iousc ntext nd appe ding the outcome to the end.This f rmul tion is w ll suited to data drivenestimation from a corpus f derivation trees.While our methods are eas ly extended to mul-tiple derivations for eac single se tence, in thisw rk we assum cces to a single derivation foreach sentence i our data set.
Maximum lik li-hood s imation of P (o|c) is then as simple as nor-malizing the cou ts of the observed outcomes foreach observed context.
However, in order to ob-tain contexts for which the conditional indepen-dence assumption of P (o|c) is appropriate, it isnecessary to condition on a large amount of in-formation.
This leads to sparse estimates even onlarge amounts of training data, a problem that canbe addressed by smoothing.We identify two complementary types ofsmoothing, and illustrate them with the followingsentences.The furry d g bit me.The cut cat licked me.Assuming a simple bigram model wher con-text is the previous word and the outcome a sin-gle word, an u smoothed model trained on thisat can only generate the two sentences verba-tim.
Imagine we have som way of knowing thatthe tokens ?dog?
and ?cat?
a e sim lar and wou dlike to leverage this fact .
In our bigram model,this amounts to the clai that e words that follow?cat?
e perhap al o likely to ollow ?dog?.
Thiis easily handled wit traditional smo thi tech-niques, which interpolate between distributionsestimated for both coar , P (w|w?1=[is-a imal]),and fine, P (w|w?1=?dog?
), contexts.
We refer tothis as context smoothing.However, we would also like to apture the in-tuition that words which can be followed by ?dog?can also be f llowed by ?cat?, which we will allout m smoothing.
We extend our terminologyto describe a system that performs both types ofsmoothing with the following?
the set?C of smooth contexts c??
the set?O of smooth outcomes o??
a smoothing function SC: C ??C?
a smoothing function SO: O ?
?OWe describe the generative process with the fol-lowing flowchart2 Related WorkThe application of structural constraints appearsin previous work in the form of generative po-etry (Greene et al., 2010) or lyrics (Wu et al.,2013), where specified meter or rhyme schemesare enforced.
?Ozbal et al.
(2013) producesfreeform text by filling templates with respect toabstract notions such as humor.3 Freeform Gene ationWe first addr ss th problem of freeform datadriven language g neration directly.
We do notset a semantic goal but instead ask only that theoutput b considered a valid s ntenc , seeking amodel that captures the vari bility f language.For clarity in our discussion, we phrase thegeneration process in the following general termsbased around two classes of atomic units : Con-texts and Outcomes.
In order to specify a genera-tion system, we must define1.
the set C of contexts c2.
the set O of outcomes o3.
the ?Imply?
function I(c, o) ?
List[c ?
C]where I(c, o) defines the further contexts impliedby the choice of outcome o for the context c. Thismodel can be made probabilistic by the definitionof P (o|c), where each outcome is sampled inde-pendently given its context.
We also require theexistence of a single unique root context, and referto the result of repeated sampling of outcomes forcontexts as a derivation tree.
Finally, a mappingfrom derivation tree to surface form is required toproduce actual text.This is simply a convenient rephrasing of theContext Free Grammar formalism, and as suchthe systems we describe all have some equivalentCFG interpretation.
Indeed, to describe a tradi-tional CFG, let C be the set of nonterminals, O bethe rules of the CFG, and I(c, o) returns a list ofthe nonterminals on the right hand side of the ruleo and does not depend on c. P (o|c) would enforcethe choice of rules with appropriate lefthand sides.The Context-Outcome terms can be more natu-ral when describing other models where we do notwant to explicitly define the space of nonterminals.A simple example is an n-gram model, for whicha context is an ordered list of words, an outcomea single word, and I(c, o) can be procedurally de-fined to produce a list containg a single contextmade by dro ping the first word of the previouscontext and appending the outcome to the end.This formulation is well suited to data drivenestimation from a corpus of derivation trees.While our me hods are easily extended to mul-tiple derivations for each single sentence, in thiswork we assume access to a single derivation foreach sentence in our data set.
Maximum likeli-hood estimation of P (o|c) is then as simple as nor-malizing the counts of the observed outcomes foreach observed context.
However, in order to ob-tain contexts for which the conditional indepen-dence assumption of P (o|c) is appropriate, it isnecessary to condition on a large amount of in-f rmation.
This leads to sparse estimates even onlarge amounts of training data, a problem that canbe addressed by smoothing.We identify two complementary types ofsmoothing, and illustrate them with the followingsentences.The furry d g bit me.The cute cat licked me.Assuming a simple bigram model where con-text is the previous word and the outcome a sin-gle word, an unsmoothed model trained on thisdata can only gener e the two senten es verba-tim.
Imagine we have some way of knowing thatthe tokens ?dog?
and ?cat?
are similar and wouldlike to leverage this fact .
In our bigram model,this amounts to the claim that the words that follow?cat?
are perhaps also likely to follow ?dog?.
Thisis easily handled with traditional smoothing tech-niques, which interpolate between distributionsestimated for bot coarse, P (w|w?1=[is-animal]),and fine, P (w|w?1=?dog?
), contexts.
We refer tothis as context smoothing.However, we would also like to capture the in-tuition that words which can be followed by ?dog?can also be followed by ?cat?, which we will calloutcome smoothing.
We extend our terminologyto describe a system that performs both types ofsmoothing with the following?
the set?C of smooth contexts c??
the set?O of smooth outcomes o??
a smoothing function SC: C ??C?
a smoothing function SO: O ?
?OWe describe the generative process with the fol-lowing flowchartFigure 1: A flow chart depicting the decisionsmade when choosing an outcome for a context.The large circles show the set of items associatedwith each decision, and contain examples itemsfor a bigram model where SCand SOmap words(e.g.
dog) to semantic classes (e.g.
[animal]).We describe the smoothed generative processwith the flowchart shown in Figure 1.
In order tochoose an outcome for a given context, two deci-sions must be made.
First, we must decide whichcontext we will employ, the true context or thesmooth context, marked by edges 1 or 2 respec-tively.
Next, we choose to generate a true outcomeor a smooth outcome, and if we select the latterwe use edge 6 to choose a true outcome given thesmooth outcome.
The decision between edges 1and 2 can be sampled from a Bernoulli randomvariable with parameter ?c, with one variable es-timated for each context c. The decision betweenedges 5 and 3 and the one between 4 and 7 can alsobe made with Bernoulli random variables, with pa-rameter sets ?cand ?c?respectively.This yields the full form of the unconstrainedprobabilistic generative model as followsP (o|c) = ?cP1(o|c) + (1?
?c)P2(o|SC(c))P1(o|c) = ?cP5(o|c)+(1?
?c)P7(o|o?
)P3(o?|c) (1)P2(o|c?)
= ?c?P6(o|c)+(1?
?c?)P7(o|o?)P4(o?|c?
)requiring estimation of the ?
and ?
variables aswell as the five multinomial distributions P3?7.This can be done with a straightforward applica-tion of EM.4 Limiting VocabularyA primary concern in the generation of languageeducation exercises is the working vocabulary ofthe students.
If efficiency were not a concern, thenatural solution to the vocabulary constraint wouldbe rejection sampling: simply generate sentencesuntil one happens to obey the constraint.
In thissection we show how to generate a sentence di-rectly from this constrained set with a distributionclosely approximating that of the rejection sam-pler.4.1 PruningThe first step is to prune the space of possible sen-tences to those that obey the vocabulary constraint.For the models we investigate there is a naturalpredicate V (o) that is true if and only if an out-come introduces a word that is out of vocab, andso the vocabulary constraint is equivalent to therequirement that V (o) is false for all possible out-comes o.
Considering transitions along edges inFigure 1, the removal of all transitions along edges5,6, and 7 that lead to outcomes where V (o) is truesatisfies this property.Our remaining concern is that the generationprocess does not reach a failure case.
Againconsidering transitions in Figure 1, failure occurswhen we require P (o|c) for some c and there is notransition to c on edge 1 or SC(c) along edge 2.We refer to such a context as invalid.
Our goal,which we refer to as consistency, is that for all126valid contexts c, all outcomes o that can be reachedin Figure 1 satisfy the property that all members ofI(c, o) are valid contexts.To see how we might end up in failure, considera trigram model on POS/word pairs for which SCis the identity function and SObacks off to thePOS tag.
Given a context c = ((t?2w?2),(t?1w?1)) ifwe generate along a path using edge 6 we willchoose a smooth outcome t0that we have seenfollowing c in the data and then independenentlychoose a w0that has been observed with tag t0.This implies a following context ((t?1w?1),(t0w0)).
Ifwe have estimated our model with observationsfrom data, there is no guarantee that this contextever appeared, and if so there will be no availabletransition along edges 1 or 2.Let the list?I(c, o) be the result of the mappedapplication of SCto each element of I(c, o).
Inorder to define an efficient algorithm, we requirethe following property D referring to the amountof information needed to determine?I(c, o).
Sim-ply put, D states if the smoothed context and out-come are fixed, then the implied smooth contextsare determined.D {SC(c), SO(o)} ?
?I(c, o)To highlight the statement D makes, consider thetrigram POS/word model described above, but letSCalso map the POS/word pairs in the contextto their POS tags alone.
D holds here becausegiven SC(c) = (t?2, t?1) and SO(o) = t0fromthe outcome, we are able to determine the impliedsmooth context (t?1, t0).
If context smoothing in-stead produced SC(c) = (t?2),D would not hold.If D holds then we can show consistency basedon the transitions in Figure 1 alone as any com-plete path through Figure 1 defines both c?
and o?.By D we can determine?I(c, o) for any path andverify that all its members have possible transi-tions along edge 2.
If the verification passes forall paths then the model is consistent.Algorithm 1 produces a consistent model byverifying each complete path in the manner justdescribed.
One important feature is that it pre-serves the invariant that if a context c can bereached on edge 1, then SC(c) can be reached onedge 2.
This means that if the verification failsthen the complete path produces an invalid con-text, even though we have only checked the mem-bers of?I(c, o) against path 2.If a complete path produces an invalid con-text, some transition along that path must be re-Algorithm 1 Pruning AlgorithmInitialize with all observed transitionsfor all out of vocab o doremove ??
o from edges 5,6, and 7end forrepeatfor all paths in flow chart doif ?c?
?
?I(c, o) s.t.
c?
is invalid thenremove transition from edge 5,7,3 or 4end ifend forRun FIXUPuntil edge 2 transitions did not changemoved.
It is never optimal to remove transitionsfrom edges 1 or 2 as this unnecessarily removesall downstream complete paths as well, and so forinvalid complete paths along 1-5 and 2-7 Algo-rithm 1 removes the transitions along edges 5 and7.
The choice is not so simple for the completepaths 1-3-6 and 2-4-6, as there are two remainingchoices.
Fortunately, D implies that breaking theconnection on edge 3 or 4 is optimal as regardlessof which outcome is chosen on edge 6,?I(c, o) willstill produce the same invalid c?.After removing transitions in this manner, sometransitions on edges 1-4 may no longer have anyoutgoing transitions.
The subroutine FIXUP re-moves such transitions, checking edges 3 and 4before 1 and 2.
If FIXUP does not modify edge 2then the model is consistent and Algorithm 1 ter-minates.4.2 EstimationIn order to replicate the behavior of the rejectionsampler, which uses the original probability modelP (o|c) from Equation 1, we must set the probabil-ities PV(o|c) of the pruned model appropriately.We note that for moderately sized vocabularies itis feasible to recursively enumerate CV, the set ofall reachable contexts in the pruned model.
Infurther discussion we simplify the representationof the model to a standard PCFG with CVas itssymbol set and its PCFG rules indexed by out-comes.
This also allows us to construct the reach-ability graph for CV, with an edge from cito cjfor each cj?
I(ci, o).
Such an edge is givenweight P (o|c), the probability under the uncon-strained model, and zero weight edges are not in-cluded.Our goal is to retain the form of the stan-127dard incremental recursive sampling algorithm forPCFGs.
The correctness of this algorithm comesfrom the fact that the probability of a rule R ex-panding a symbolX is precisely the probability ofall trees rooted atX whose first rule isR.
This im-plies that the correct sampling distribution is sim-ply the distribution over rules itself.
When con-straints that disallow certain trees are introduced,the probability of all trees whose first rule is Ronly includes the mass from valid trees, and thecorrect sampling distribution is the renormaliza-tion of these values.Let the goodness of a contextG(c) be the proba-bility that a full subtree generated from c using theunconstrained model obeys the vocabulary con-straint.
Knowledge of G(c) for all c ?
CVal-lows the calculation of probabilities for the prunedmodel withPV(o|c) ?
P (o|c)?c??I(c,o)G(c?)
(2)While G(c) can be defined recursively asG(c) =?o?OP (o|c)?c??I(c,o)G(c?)
(3)its calculation requires that the reachability graphbe acyclic.
We approximate an acyclic graph bylisting all edges in order of decreasing weight andintroducing edges as long as they do not create cy-cles.
This can be done efficiently with a binarysearch over the edges by weight.
Note that this ap-proximate graph is used only in recursive estima-tion of G(c), and the true graph can still be usedin Equation 2.5 Generating UpIn this section we show how to efficiently gener-ate sentences that contain an arbitrary word w?inaddition to the vocabulary constraint.
We assumethe ability to easily find Cw?, a subset of CVwhoseuse guarantees that the resulting sentence containsw?.
Our goal is once again to efficiently emulatethe rejection sampler, which generates a derivationtree T and accepts if and only if it contains at leastone member of Cw?.Let Tw?be the set of derivation trees that wouldbe accepted by the rejection sampler.
We presenta three stage generative model and its associatedprobability distribution Pw?(?)
over items ?
forwhich there is a functional mapping into Tw?.In addition to the probabilities PV(o|c) from theprevious section, we require an estimate of E(c),the expected number of times each context c ap-pears in a single tree.
This can be computed effi-ciently using the mean matrix, described in Millerand Osullivan (1992).
This |CV| ?
|CV| matri x Mhas its entries defined asM(i, j) =?o?OP (o|ci)#(cj, ci, o) (4)where the operator # returns the number of timescontext cjappears I(ci, o).
Defining a 1 ?
|CV|start state vector z0that is zero everywhere and 1in the entry corresponding to the root context givesE(z) =?
?i=0z0Miwhich can be iteratively computed with sparse ma-trix multiplication.
Note that the ith term in thesum corresponds to expected counts at depth i inthe derivation tree.
With definitions of context andoutcome for which very deep derivations are im-probable, it is reasonable to approximate this sumby truncation.Our generation model operates in three phases.1.
Chose a start context c0?
Cw?2.
Generate a spine S of contexts and outcomesconnecting c0to the root context3.
Fill in the full derivation tree T below all re-maining unexpanded contextsIn the first phase, c0is sampled from the multi-nomialP1(c0) =E(c0)?c?Cw?E(c)(5)The second step produces a spine S, which isformally an ordered list of triples.
Each elementof S records a context ci, an outcome oi, and theindex k in I(ci, oi) of the child along which thespine progresses.
The members of S are sampledindependantly given the previously sampled con-text, starting from c0and terminating when theroot context is reached.
Intuitively this is equiv-alent to generating the path from the root to c0ina bottom up fashion.We define the probability P?of a triple(ci, oi, k) given a previously sampled context cj128asP?
({ci, oi, k}|cj) ?
{E(ci)PV(oi|ci) I(ci, oi)[k] = cj0 otherwise(6)Let S = (c1, o1, k1) .
.
.
(cn, on, kn) be the re-sults of this recursive sampling algorithm, wherecnis the root context, and c1is the parent contextof c0.
The total probability of a spine S is thenP2(S|c0) =|S|?i=1E(ci)PV(oi|ci)Zi?1(7)Zi?1=?
(c,o)?Ici?1E(c)PV(o|c)#(ci?1, c, o)(8)where Ic?1is the set of all (c, o) for whichP?
(c, o, k|ci?1) is non-zero for some k. A keyobservation is that Zi?1= E(ci?1), which can-cels nearly all of the expected counts from the fullproduct.
Along with the fact that the expectedcount of the root context is one, the formula sim-plifies toP2(S|c0) =|S|?i=1PV(oi|ci)E(c0)(9)The third step generates a final tree T by fill-ing in subtrees below unexpanded contexts on thespine S using the original generation algorithm,yielding results with probabilityP3(T |S) =?
(c,o)?T/SPV(o|c) (10)where the set T/S includes all contexts that arenot ancestors of c0, as their outcomes are alreadyspecified in S.We validate this algorithm by considering itsdistrubution over complete derivation trees T ?Tw?.
The algorithm generates ?
= (T, S, c0) andhas a simple functional mapping into Tw?by ex-tracting the first member of ?
.Combining the probabilities of our three stepsgivesPw?(?)
=E(c0)?c?Cw?E(c)|S|?i=1PV(oi|ci)E(c0)?(c,o)?T/SPV(o|c)Pw?(?)
=PV(T )?c?Cw?E(c)=1?PV(T ) (11)where ?
is a constant andPV(T ) =?
(c,o)?TPV(o|c)is the probability of T under the original model.Note that several ?
may map to the same T byusing different spines, and soPw?
(T ) =?
(T )?PV(T ) (12)where ?
(T ) is the number of possible spines, orequivalently the number of contexts c ?
Cw?in T .Recall that our goal is to efficiently emulate theoutput of a rejection sampler.
An ideal system Pw?would produce the complete set of derivation treesaccepted by the rejection sampler using PV, withprobabilities of each derivation tree T satisfyingPw?
(T ) ?
PV(T ) (13)Consider the implications of the following as-sumptionA each T ?
Tw?contains exactly one c ?
Cw?A ensures that ?
(T ) = 1 for all T , unifying Equa-tions 12 and 13.
A does not generally hold in prac-tice, but its clear exposition allows us to designmodels for which it holds most of the time, lead-ing to a tight approximation.The most important consideration of this type isto limit redundancy in Cw?.
For illustration con-sider a dependency grammar model with parentannotation where a context is the current word andits parent word.
When specifying Cw?for a partic-ular w?, we might choose all contexts in which w?appears as either the current or parent word, buta better choice that more closely satisfies A is tochoose contexts where w?appears as the currentword only.129ENDENDFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivat ng its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP ar generative andcan easily produce sample sentences from distri-bu ions estim ed rom d ta, if all that is requiredis a plausible sentence one might as well pick oneat random from any exi ting corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreefor Generation fro a Fixed Vocabularybstracte investigate data driven natural lan-guage generation under the constraint thatall ords ust co e fro a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified ordust also appear in the sentence.
epresent fast approxi ations to the ideal re-jection sa plers and increase variability ingenerated text through controlled s ooth-ing.I tr ctiJJs li s i slilJJbigJJ Sbig dogsata driven atural anguage eneration( ) is a fascinating topic explored by aca-de ics and artists alike, but otivating its e piri-cal st is a iffic lt tas .
ile a la a eels se i statistical are e erati e aca easil r ce sa le se te ces fr istri-ti s sti t r t , if ll t t is r iris l si l s t i t s ll it r fr i ti r .i f l tr i t r lit l rt i l i l t r li .j it li t ti t i tt t , i t iti ti l .
t i tiFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations t the ideal re-jection samplers and increase variability ingenerated text thro gh controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe d gsROOT VBZ NNSlik s dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-v cab, but specifies the inclusion of a single ar-bitrary wo d somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNShe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andc n easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is useful when co straints are applied suchthat only certain plausible sentences are valid.
Theajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximatio s to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat r ndom from any existing corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst s ts a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, w re students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreeform Generation from a Fixed VocabularyAbstractWe investigate data driven natural lan-guage generation under the constraint thatall words must come from a fixed arbi-trary vocabul ry.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific c nstraints concern-i g the word that are all wed in a s ntence.
Thefirst sets a fixed vocabulary such that only sen-tences where ll words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge f arbitrary words are required.
This useFreeform Generatio fro a Fixed VocabularyAbstractWe inves igate data driven natural lan-guage g ne ation under the constraint thatall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specifi d wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 In roductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natural Language Generation(NLG) is a fascinating topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is useful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The second demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFreeform Generation from a Fixed VocabularyAbstractWe inves gate data driven natural l -guage generation u der the constraint thaall words must come from a fixed arbi-trary vocabulary.
This constraint is thenextended such that a user specified wordmust also appear in the sentence.
Wepresent fast approximations to the ideal re-jection samplers and increase variability ingenerated text through controlled smooth-ing.1 IntroductionROOT PRP VBZ JJ NNSshe likes big dogsROOT VBZlikesPRP NNSshe dogsROOT VBZ NNSlikes dogsROOT PRP VBZshe likesJJbigVBZ JJ NNSbig dogsData driven Natur l Language Generation(NLG) is a fascinati g topic explored by aca-demics and artists alike, but motivating its empiri-cal study is a difficult task.
While many languagemodels used in statistical NLP are generative andcan easily produce sample sentences from distri-butions estimated from data, if all that is requiredis a plausible sentence one might as well pick oneat random from any existing corpus.NLG is us ful when constraints are applied suchthat only certain plausible sentences are valid.
Themajority of NLG applies the semantic constraint of?what to say?, producing sentences with commu-nicative goals.
Other work such as ours investi-gates constraints in structure; producing sentencesof a certain form without concern for their mean-ing.We motivate two specific constraints concern-ing the words that are allowed in a sentence.
Thefirst sets a fixed vocabulary such that only sen-tences where all words are in-vocab are allowed.The seco d demands not only that all words arein-vocab, but specifies the inclusion of a single ar-bitrary word somewhere in the sentence.
Thesecontraints are most natural in the case of languageeducation, where students have small known vo-cabularies and exercises that reinforce the knowl-edge of arbitrary words are required.
This useFigure 2: The generation system SPINEDEP draws on ep n ency tree sy t x where we use the termnode to refer to a POS/word pair.
Contexts consist of a nod , its parent ode, and grandparent POS tag,as shown in squares.
Outcomes, shown in squares with rounded right s des, are full lists of dependentsor the END symbol.
The shaded rectangles contain the results o I(c, o) from the indicated (c, o) pair.6 ExperimentsWe train our models on sentences drawn from theSimple English Wikipedia1.
We obtained thesesentences from a data dump which we liberally fil-tered to remove items such as lists and sentenceslonger than 15 words or shorter then 3 words.
Weparsed this data with the recently updated StanfordParser (Socher et al., 2013) to Penn Treebank con-stituent form, and removed any sentence that didnot parse to a top level S containing at least oneNP and one VP child.
Even with such strong fil-ters, we retained over 140K sentences for use astraining data, and provide this exact set of parsetrees for use in future work.2Inspired by the application in language educa-tion, for our vocabulary list we use the English Vo-cabulary Profile (Capel, 2012), which predicts stu-dent vocabulary at different stages of learning En-glish as a second language.
We take the most ba-sic American English vocabulary (the A1 list), andretrieve all inflections for each word using Sim-pleNLG (Gatt and Reiter, 2009), yielding a vocab-ulary of 1226 simple words and punctuation.To mitigate noise in the data, we discard anypair of context and outcome that appears only oncein the training data, and estimate the parameters ofthe unconstrained model using EM.6.1 Model ComparisonWe experimented with many generation modelsbefore converging on SPINEDEP, described inFigure 2, which we use in these experiments.1http://simple.wikipedia.org2data url anon for reviewCorr(%) % uniqSPINEDEP unsmoothed 87.6 5.0SPINEDEP WordNe 78.3 32.5SPINEDEP word2vec 5000 72.6 52.9SPINEDEP word2vec 500 65.3 60.2KneserNey-5 64.0 25.8DMV 33.7 71.2Figure 3: System comparison based on humanjudged correctness and the percentage of uniquesentences in a sample of 100K.SPINEDEP uses dependency grammar elements,with parent and grandparent information in thecontexts to capture such distinctions as that be-tween main and clausal verbs.
Its outcomes arefull configurations of dependents, capturing co-ordinations such as subject-object pairings.
Thisspecificity greatly increases the size of the modeland in turn reduces the speed of the true rejectionsampler, which fails over 90% of the time to pro-duce an in-vocab sentence.We found that large amounts of smoothingquickly diminishes the amount of error free out-put, and so we smooth very cautiously, map-ping words in the contexts and outcomes tofine semantic classes.
We compare the useof human annotated hypernyms from Word-net (Miller, 1995) with automatic word clustersfrom word2vec (Mikolov et al., 2013), based onvector space word embeddings, evaluating both500 and 5000 clusters for the latter.We compare these models against several base-line alternatives, shown in Figure 3.
To determine130correctness, used Amazon Mechanical Turk, ask-ing the question: ?Is this sentence plausible??.
Wefurther clarified this question in the instructionswith alternative definitions of plausibility as wellas both positive and negative examples.
Every sen-tence was rated by five reviewers and its correct-ness was determined by majority vote, with a .496Fleiss kappa agreement.
To avoid spammers, welimited our hits to Turkers with an over 95% ap-proval rating.Traditional language modeling techniques suchas such as the Dependency Model with Va-lence (Klein and Manning, 2004) and 5-gramKneser Ney (Chen and Goodman, 1996) performpoorly, which is unsurprising as they are designedfor tasks in recognition rather than generation.
Forn-gram models, accuracy can be greatly increasedby decreasing the amount of smoothing, but it be-comes difficult to find long n-grams that are com-pletely in-vocab and results become redundant,parroting the few completely in-vocab sentencesfrom the training data.
The DMV is more flex-ible, but makes assumptions of conditional inde-pendence that are far too strong.
As a result itis unable to avoid red flags such as sentences notending in punctuation or strange subject-object co-ordinations.
Without smoothing, SPINEDEP suf-fers from a similar problem as unsmoothed n-grammodels; high accuracy but quickly vanishing pro-ductivity.All of the smoothed SPINEDEP systems showclear advantages over their competitors.
Thetradeoff between correctness and generative ca-pacity is also clear, and our results suggest that thenumber of clusters created from the word2vec em-beddings can be used to trace this curve.
As for theideal position in this tradeoff, we leave such deci-sions which are particular to specific application tofuture work, arbitrarily using SPINEDEP WordNetfor our following experiments.6.2 Fixed VocabularyTo show the tightness of the approximation pre-sented in Section 4.2, we evaluate three settingsfor the probabilities of the pruned model.
The firstis a weak baseline that sets all distributions to uni-form.
For the second, we simply renormalize thetrue model?s probabilities, which is equivalent tosetting G(c) = 1 for all c in Equation 2.
Finally,we use our proposed method to estimate G(c).We show in Figure 4 that our estimation methodCorr(%) -LLRTrue RS 79.3 ?Uniform 47.3 96.2G(c) = 1 77.0 25.0G(c) estimated 78.3 1.0Figure 4: A comparison of our system against botha weak and a strong baseline based on correctnessand the negative log of the likelihood ratio mea-suring closeness to the true rejection sampler.more closely approximates the distribution of therejection sampler by drawing 500K samples fromeach model and comparing them with 500K sam-ples from the rejection sampler itself.
We quantifythis comparison with the likelihood ratio statistic,evaluating the null hypothesis that the two sam-ples were drawn from the same distribution.
Notonly does our method more closely emulate that ofthe rejection sampler, be we see welcome evidencethat closeness to the true distribution is correlatedwith correctness.6.3 Word InclusionTo explore the word inclusion constraint, for eachword in our vocabulary list we sample 1000 sen-tences that are constrained to include that wordusing both unsmoothed and WordNet smoothedSPINEDEP.
We compare these results to the ?Cor-pus?
model that simply searches the training dataand uniformly samples from the existing sentencesthat satisfy the constraints.
This corpus search ap-proach is quite a strong baseline, as it is trivial toimplement and we assume perfect correctness forits results.This experiment is especially relevant to ourmotivation of language education.
The naturalquestion when proposing any NLG approach iswhether or not the ability to automatically producesentences outweighs the requirement of a post-process to ensure goal-appropriate output.
Thisis a challenging task in the context of languageeducation, as most applications such as exam orhomework creation require only a handful of sen-tences.
In order for an NLG solution to be appro-priate, the constraints must be so strong that a cor-pus search based method will frequently producetoo few options to be useful.
The word inclusionconstraint highlights the strengths of our methodas it is not only highly plausible in a language ed-131# < 10 # > 100 Corr(%)Corpus 987 26 100Unsmooth 957 56 89.0Smooth 544 586 79.0Figure 5: Using systems that implement the wordinclusion constraint, this table shows the numberof words for which the amount of unique sentencesout of 1000 samples was less than 10 or greaterthan 100, along with the correctness of each sys-tem.ucation setting but difficult to satisfy by chance inlarge corpora.Figure 5 shows that the corpus search approachfails to find more than ten sentences that obey theword inclusion constraints for most target words.Moreover, it is arguably the case that unsmoothedSPINEDEP is even worse due to its inferior cor-rectness.
With the addition of smoothing, how-ever, we see a drastic shift in the number of wordsfor which a large number of sentences can be pro-duced.
For the majority of the vocabulary wordsthis model generates over 100 sentences that obeyboth constraints, of which approximately 80% arevalid English sentences.7 ConclusionIn this work we address two novel NLG con-straints, fixed vocabulary and fixed vocabularywith word inclusion, that are motivated by lan-guage education scenarios.
We showed that un-der these constraints a highly parameterized modelbased on dependency tree syntax can produce awide range of accurate sentences, outperformingthe strong baselines of popular generative lan-guage models.
We developed a pruning and es-timation algorithm for the fixed vocabulary con-straint and showed that it not only closely approx-imates the true rejection sampler but also that thetightness of approximation is correlated with hu-man judgments of correctness.
We showed thatunder the word inclusion constraint, precise se-mantic smoothing produces a system whose abili-ties exceed the simple but powerful alternative oflooking up sentences in large corpora.SPINEDEP works surprisingly well given thewidely held stigma that freeform NLG produceseither memorized sentences or gibberish.
Still, weexpect that better models exist, especially in termsof definition of smoothing operators.
We have pre-sented our algorithms in the flexible terms of con-text and outcome, and clearly stated the propertiesthat are required for the full use of our methodol-ogy.
We have also implemented our code in thesegeneral terms3, which performs EM based param-eter estimation as well as efficient generation un-der the constraints discussed above.
All systemsused in this work with the exception of 5-gram in-terpolated Kneser-Ney were implemented in thisway, are included with the code, and can be usedas templates.We recognize several avenues for continuedwork on this topic.
The use of form-based con-straints such as word inclusion has clear applica-tion in language education, but many other con-straints are also desirable.
The clearest is perhapsthe ability to constrain results based on a ?vocab-ulary?
of syntactic patterns such as ?Not only ...but also ...?.
Another extension would be to incor-porate the rough communicative goal of responseto a previous sentence as in Wu et al.
(2013) andattempt to produce in-vocab dialogs such as areubiquitous in language education textbooks.Another possible direction is in the improve-ment of the context-outcome framework itself.While we have assumed a data set of one deriva-tion tree per sentence, our current methods eas-ily extend to sets of weighted derivations for eachsentence.
This suggests the use of techinques thathave proved effective in grammar estimation thatreason over large numbers of possible derivationssuch as Bayesian tree substitution grammars or un-supervised symbol refinement.ReferencesAnja Belz.
2008.
Automatic generation of weatherforecast texts using comprehensive probabilisticgeneration-space models.
Natural Language Engi-neering, 14(4):431?455.A.
Capel.
2012.
The english vocabulary profile.http://vocabulary.englishprofile.org/.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Simon Colton, Jacob Goodwin, and Tony Veale.
2012.Full face poetry generation.
In Proceedings of the3url anon for review132Third International Conference on ComputationalCreativity, pages 95?102.Albert Gatt and Ehud Reiter.
2009.
Simplenlg: A re-alisation engine for practical applications.
In Pro-ceedings of the 12th European Workshop on Natu-ral Language Generation, ENLG ?09, pages 90?93,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Erica Greene, Tugba Bodrumlu, and Kevin Knight.2010.
Automatic analysis of rhythmic poetry withapplications to generation and translation.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 524?533, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Long Jiang and Ming Zhou.
2008.
Generating chi-nese couplets using a statistical mt approach.
InProceedings of the 22nd International Conferenceon Computational Linguistics-Volume 1, pages 377?384.
Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure: Mod-els of dependency and constituency.
In Proceed-ings of the 42Nd Annual Meeting on Association forComputational Linguistics, ACL ?04, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Michael I. Miller and Joseph A. Osullivan.
1992.
En-tropies and combinatorics of random branching pro-cesses and context-free languages.
IEEE Transac-tions on Information Theory, 38.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
COMMUNICATIONS OF THE ACM,38:39?41.Margaret Mitchell, Kees van Deemter, and Ehud Reiter.2013.
Generating expressions that refer to visibleobjects.
In HLT-NAACL, pages 1174?1184.Jack Mostow and Hyeju Jang.
2012.
Generating di-agnostic multiple choice comprehension cloze ques-tions.
In Proceedings of the Seventh Workshopon Building Educational Applications Using NLP,pages 136?146.
Association for Computational Lin-guistics.G?ozde?Ozbal, Daniele Pighin, and Carlo Strapparava.2013.
Brainsup: Brainstorming support for creativesentence generation.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1446?1455, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Ananth Ramakrishnan A, Sankar Kuppan, andSobha Lalitha Devi.
2009.
Automatic generationof tamil lyrics for melodies.
In Proceedings of theWorkshop on Computational Approaches to Linguis-tic Creativity, pages 40?46.
Association for Compu-tational Linguistics.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing With Composi-tional Vector Grammars.
In ACL.Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-mamoto.
2005.
Measuring non-native speak-ers?
proficiency of english by using a test withautomatically-generated fill-in-the-blank questions.In Proceedings of the second workshop on BuildingEducational Applications Using NLP, pages 61?68.Association for Computational Linguistics.Alessandro Valitutti, Hannu Toivonen, AntoineDoucet, and Jukka M. Toivanen.
2013.
?let every-thing turn well in your wife?
: Generation of adulthumor using lexical constraints.
In ACL (2), pages243?248.Dekai Wu, Karteek Addanki, Markus Saers, andMeriem Beloucif.
2013.
Learning to freestyle: Hiphop challenge-response induction via transductionrule segmentation.
In EMNLP, pages 102?112.133
