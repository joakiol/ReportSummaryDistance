On-Line Cursive Handwriting Recognition UsingHidden Markov Models and Statistical GrammarsJohn Makhoul, Thad Starner~, Richard Schwartz, and George ChouBBN Systems and  Techno log ies70  Fawcet t  S t reetCambr idge ,  MA 02138Emai l :  Makhou l  @bbn.comABSTRACTThe BYBLOS continuous peech recognition system is applied toon-line cursive handwriting recognition.
By exploiting similaritiesbetween on-line cursive handwriting and continuous speech recogni-tion, we can use the same base system adapted to handwriting featurevectors instead of speech.
The use of hidden Markov models obvi-ates the need for segmentation f the handwritten script sentencesbefore recognition.
To test our system, we collected handwrittensentences u ing text from the ARPA Airline Travel Information Ser-vice (ATIS) and the ARPA Wall Street Journal (WSJ) corpora.
In aninitial experiment on the ATIS data, a word error rate of 1.1% wasachieved with a 3050-word lexicon, 52-character set, collected fromone writer.
In a subsequent writer-dependent test on the WSJ data,error rates ranging between 2%-5% were obtained with a 25,595-word lexicon, 86-character set, collected from six different writers.Details of the recognition system, the data collection process, andanalysis of the experiments are presented.I .
INTRODUCTIONThe segmentation f written words into component characters i  of-ten the first step of handwriting recognition systems \[1\].
In somecases, segmentation is forced on the user by providing boxes for thewriting of discrete letters.
However, in modem continuous speechrecognition efforts, segmentation f phonemes i not performed be-fore either of the training or the recognition steps.
Instead, segmenta-tion occurs simultaneously with recognition.
If such a system couldbe adapted for handwriting, the very difficult and time consumingissue of segmentation could be avoided.
This paper addresses such asystem, where automatic recognition of on-line cursive handwritingis achieved by the use of continuous speech recognition methods.
Inthis context, on-line refers to the situation where the time sequenceof samples comprising the script is known (as with pen computers).The recognition of the on-line handwriting is performed through theuse of hidden Markov models and statistical grammars in a mannervery similar to several modem speech recognizers.
In fact, we showthat, with essentially no modification, a speech recognition systemcan perform accurate on-line handwriting recognition with the inputfeatures being those of writing instead of speech.Hidden Markov models have intrinsic properties which make themvery attractive for handwriting recognition.
For training, all that isnecessary is a data stream and its transcription (the text matching thehandwriting).
The training process automatically aligns the com-ponents of the transcription to the data.
Thus, no special effort isneeded to label training data.
Segmentation, i  the traditional sense,is avoided altogether.
Recognition is performed on another datastream.
Again, no explicit segmentation is necessary.
The segmen-teui~rently with the MIT Media Lab.tation of words into characters or even sentences into words occursnaturally by incorporating the use of a lexicon and a language modelinto the recognition process.
The result is a text stream that can becompared to a reference text for error calculation.Section 2discusses the similarities of speech and handwriting recog-nition tasks and provides ome background on technique.
Section3 describes an initial 3050 word, 52 symbol, writer dependent ex-periment.
Section 4 discusses a more ambitious 25,595 word, 86symbol, writer dependent system involving multiple writers.
Sec-tion 5 examines experimental results and discusses future work.2.
COMPARISON OF CONTINUOUSSPEECH RECOGNIT ION TO ON-L INEHANDWRIT ING RECOGNIT IONOn-line handwriting and continuous peech share many commoncharacteristics.
On-line handwriting can be viewed as a signal (x,ycoordinates) over time, just like in speech.
The items to be rec-ognized are well-defined (usually the alphanumeric characters) andfinite in number, as are the phonemes in speech.
The shape of ahandwritten character depends on its neighbors.
Correspondingly,spoken phonemes change due to coarticulation i speech.
In bothcases, these basic units form words and the words form phrases.Thus, language modeling can be applied to improve recognition per-formance for both problems.In spite of the above similarities, handwriting recognition has somebasic differences to speech recognition.
Unlike continuous speech,word boundaries are usually distinct in handwriting.
Thus, wordsshould be easier to distinguish.
However, in cursive writing thedots and crosses involved in the characters "i", "j", "x", and "t"are not added until after the whole word is written.
Thus, all theevidence for a character may not be contiguous.
Additionally, inwords with multiple crossings ("t" and "x") and/or dottings ('T' and"j") the order of pen strokes is ambiguous.
Even so, with the manyparallels between on-line writing and speech, speech recognitionmethods hould be applicable to on-line handwriting recognition.Since hidden Markov models currently constitute the state of the artin speech recognition, this method also seems a likel3~ candidate forhandwriting recognition.There has been some interest in the use of HMMs for on-line hand-writing recognition (see, for example, \[2, 3\]).
However, the fewstudies that have used HMMs have dealt with small vocabularies,isolated characters, or isolated words.
In this study, our objectiveis to deal with continuous cursive handwriting and large vocabu-laries (thousands of words) using a speech recognition system andlanguage models.432Training Speech+ TextT ramrSpeech _\[ Future  LF_~__~R~ognition__~ Mat  Likely Input r I Extraction \] Vectea's I Search Senten~Figure 1: BYBLOS speech system.3.
A IRL INE  TRAVEL  INFORMATIONSERVICE: AN INITIAL 3050 WORD, 52SYMBOL TASKIn the initial system, the BBN BYBLOS Continuous Speech Recog-nition system \[4, 5, 6\] (see Figure I) was used without modificationon an on-line cursive handwriting corpus created from prompts fromthe ARPA Airline Travel Information Service (ATIS) corpus \[7\].These full sentence prompts (approximately 10 words per sentence)were written by a single subject.
These sentences were then reviewed(verified) to make sure that the prompts were transcribed correctly.After verification, these sentences were separated into a set of 381training sentences and a mutually exclusive set of 94 test sentences.The lexicon for this task consisted of 3050 words, where lowercaseand capitalized versions of a word are considered distinct.
@ ??
o ?
?
??
2__oa_?
o ?
v lan.-A0n = O n- 0 ~1Figure 3: Angle and delta angle feature vector.For each sample point, an analysis program computed a two-elementfeature vector: the writing angle at that sample and the change in thewriting angle \[2\] (see Figure 3).
These time series of feature vectorswere then fed into the BYBLOS system.
For this task, BYBLOSquantizes the feature vectors for a sentence into 64 different clusters.These new time series are then used with their respective sentencetranscriptions totrain HMMs representing the script characters (notethat the alignment of the clusters with the sentence transcriptionsoccurs automatically in this process).
A 7-state HMM model waschosen to represent each symbol (see Figure 4).
Since the penning ofa script letter often differs depending on the letters written before andafter it, additional HMMs are used to model these contextual effects\[8\].
Adjacent effects between two letters (bilets) are modeled as wellas three letter (trilet) contexts.
In a given set of sentences there maybe many tfilets, up to the number of symbols cubed.
However, inEnglish only a subset of these are allowed.
In the ATIS task thereare 3639 different rilets in the training sentences.For this initial system there were 54 characters: 52 lower and uppercase alphabetic, a space character, and a "backspace" character.
Thebackspace character is appended onto words that contain "i", "j","x", or "t".
This character models the space the pen moves afterfinishing the body of the word to add the dot or the cross whendrawing one of these characters.Figure 4: 7-state HMM used to model each character.A statistical grammar can also be used to improve recognition per-formance.
For this experiment, a bigram grammar (to relate pairs ofwords) was created using a larger set of 17209 sentences from theATIS corpus (the 94 test sentences were not included).
The resultantgrammar has a perplexity of 20.
Table 1 shows the word error ratesfor this task when doing recognition using context without he gram-mar (perplexity = 3050), using the grammar without context, andusing both context and the grammar.
Word error rate is measured asthe sum of the percentage of words deleted, the percentage of wordsinserted, and the percentage of words that are substituted for otherwords in the set of test sentences.Figure 2: Connecting strokes.The data was acquired using a Momenta pentop which stored thescript in a simple time series of x and y coordinates at a samplingrate of 66 Hz.
The handwriting data is sampled continuously intime, except when the pen is lifted (Momenta pentops provide noinformation about pen movement between strokes).
Because wewanted to use our speech recognition system with no modification,we decided to simulate acontinuous-time feature vector by arbitrarilyconnecting the samples from pen-up to pen-down with a straight lineand then sampling that line ten times.
Thus, the data effectivelybecame one long cfiss-crossing stroke for the entire sentence, wherewords run together and "i" and "j" dots and "t" and "x" crosses causebacktracing over previously drawn script (see Figure 2).context  + no  context  + context  +no gram.
gram.
gram.word  er ror  rate 4.2% 2.2% 1.1%Table 1: ATIS 3050 word, writer-dependent test results.As can be seen from the table, both context and a grammar are verypowerful tools in aiding recognition.
With no grammar but withcontext an error rate of 4.2% was observed.
When the grammar wasadded and context not used, the error rate dropped to 2.2%.
However,the best result used both context and a grammar for an word errorrate of 1.1%.
Of interest is the factors of 2 relating the error rates433shown.
Similar factors of 2 have also been observed in the researchon the speech version of this corpus.
With the best (1.1%) word errorrate, only 10 errors occu~ed for the entire test set.
Experimentationwas suspended atthis point since so few errors did not allow anyfurther analysis of the problems in our methods.The above xperiments demonstrated the potential utility of speechrecognition methods, especially the use of HMMs and grammars, tothe problem of on-line cursive handwriting recognition.
Based onthese good preliminary results, we embarked on a more ambitioustask with a larger vocabulary and more writers.4.
WALL STREET JOURNAL: A 25,000WORD, 86 SYMBOL TASKDuring the past year, we have collected cursive written data usingtext from the ARPA Wall Street Journal task (WSJ) \[10\], includingnumerals, punctuation, and other symbols, for a total of 88 symbols(62 alphanumeric, 24punctuation a d special symbols, space, andbackspace).
The prompts from the Wall Street Journal consist mainlyof full sentences with scattered article headings and stock listings(all are referred to as sentences for convenience).
We have thusfar collected over 7000 sentences (175,000 words total or about 25words/sentence) from 21 writers on two GRiD Convertible pentops.See Figure 5 for an example of the data collected.
The writers weregathered from the Cambridge, Massachusetts area and were mainlystudents and young professionals.
Several non-native writers wereincluded (writers whose first working language was not English).While the handwriting input was constrained, the rules given thesubjects were simple: write the given sentence incursive; keep thebody of a word connected ( o not lift the pen in the middle of aword); and do crossings and dottings after completing the body of aword.
However, since many writers could not remember how to writecapital letters in cursive, great leniency was allowed.
Furthermore,apostrophes were allowed to be written both in the body of the word,or at the end of the word like a cross or dot.
For example, the word"don't" could be written as "dont" followed by the placement oftheapostrophe or "don", apostrophe, and "t".
Overall, this task mightbe best described as "pure cursive" in the handwriting recognitionliterature.For the purposes of this experiment, punctuation, umerals, andsymbols are counted as words.
Thus, ".
", ",", "0", "1", "$", "{",etc., are each counted as a word.
However, apostrophes within wordsare counted as part of that word.
Again, a capitalized version of aword is counted as distinct from the lowercase version of the word.While these standards may artifically inflate the word error rates,they are a simple way to disambiguate the definition of a word.In addition to the angle and delta angle features described in thelast section, the following features were added: delta x, delta y, penup/pen down, and sgn(x - max(x)).
Pen up/pen down is 1 only duringthe ten samples connecting one pen stroke to another; everywhereelse it is 0.
Sgn(x - max(x)) is 1 only when, at that time, the currentsample is the right-most sample of the data to date.
Also, twopreprocessing steps were used on the subjects' data.
The first wasa simple noise filter which required that the pen traverse over onehundredth ofan inch before allowing anew sample.
The second steppadded each pen stroke to a minimum size of ten samples.At the time of this writing, samples from six subjects were used forwriter dependent experiments.
Three fourths of a subject's sentenceswere used for training with the remaining fourth used for testing (see434Table 2.
A lexicon of 25,595 words was used since it spanned allof the data.
A bigram grammar was created from approximatelytwo million Wall Street Journal sentences from 1987 to 1989 (notincluding the sentences u ed in data collection).
The results of thewalter dependent tests are shown in Table 3.
Substitution, deletion,insertion, and the total word error rates are included.
Table 4 showsestimated character recognition error rates for each class of charac-ter: alphabetic, numeral, and punctuation a d other symbols.
Thesum of the substituion and deletion error rates for each class is rep-resented in this table since insertions are not directly attdbuteableto a particular class of character.
However, the total character rrorshown incorporates insertion errors since these rrors are distributedover the entire set of classes.
On average, the test sets consist of 1.9%numerals, 4.1% punctuation a d other symbols, and 94% alphabet-ics.
Both aim and shs are non-native writers.
A test experimentwas performed without agrammar (but with context) on subject shsresulting in an error ate approximately four times the previous errorrate.
This result was the same ratio seen in the ATIS task.subjectaimdsfrgbshsslbwcd#tram # ~stsentences sen~nces423 141404 135437 146423 141411 137314 105Table 2: Division of subjects' entences into training and test.5.
ANALYSIS AND FURTHEREXPERIMENTATIONThese results are quite startling when put in context.
The BYBLOSspeech system was not significantly modified for handwriting recog-nition, yet if handled several difficult handwriting tasks.
Futhermore,none of the BYBLOS automatic optimization features were used toimprove the results of any writer (or group of writers).
No particularstroke order was enforced on the writers for dottings and crossings(besides being after the body of the word), and there are known in-accuracies in the transcription files.
Note that a significantly argererror rate was observed for numerals and symbols than for alpha-betics.
Even with all insertion errors added to the estimate of thealphabetic error, the error rates for numerals and symbols are stillsignificantly higher.
One way to improve the digit recognition maysubject Subst.
Delet.
Insert.
Totalaim 2.7% 0.4% 1.4% 4.5%dsf 3.6% 0.4% 1.2% 5.2%rgb 3.3% 0.5% 1.7% 5.5%shs 1.5% 0.1% 0.5% 2.1%slb 2.9% 0.1% 1.3% 4.3%wcd 2.1% 0.4% 0.5% 3.0%ave.
2.8% 0.3% 1.1% 4.1%Table 3: WSJ 25,595 word, writer dependent word errors.Figure 5: Writing from subjects aim, dsf, rgb, shs, slb, and wcd respectively.
( Est.
I Est.
I Est.
Isubject ( num.
( sym.
( alpha.
I totalaim 1 7.1% 1 4.7% 1 .47% 1 1.4%wcd 1 5.4% ( 5.7% 1 .47% 1 1.0%ave.
1 6.2% 1 7.5% 1 .57% 1 1.4%ds frgbshsslbTable 4: Estimated character error rates for alphabetics, numerals,and symbols.be to specifically train on common digit strings such as "1989","80286", and "747" (presently, "1989" is recognized as four sepa-rate words instead of the more salient whole).
Symbol recognitionmay be further improved by tuning the minimum stroke length inpreprocessing.
If the minimum stroke length is too small, a period or8.3%3.2%6.6%7.2%comma may be completely ignored due to too few samples compris-ing the symbol.
However, if the minimum stroke length is too large,insertion errors may occur.
A better solution would allow a varyingnumber of states for different letter models.
Thus, complicated let-ters like "G" would be given 7 to 11 states while a period (or letterdotting) would be given 3.
This method may improve all classes ofrecognition.
Another known improvement deals with apostrophes.8.6%1 l.%5.0%7.1%Presently, apostrophes are handled incorrectly by expecting only theintra-word stroke version.
By expecting both standard stroke ordersin words with apostrophes, the system can increase the recognitionaccuracy of these words significantly.
By fixing these problems andusing BYBLOS's optimizing features, a 10-50% reduction in worderror rate may occur..78%.77%.19%.64%In this experiment we used a large number of training sentences per1.9%1.8%0.65%1.7%writer.
Supplying such a large amount of training text may be tir-ing for just one writer.
However, there is some evidence that not asmany training sentences per writer are needed for good performance.Furthermore, if good word error rates for the cursive dictation taskcan be assured, a writer may be willing to spend some time writingsample sentences.
A possible compro&se is to create a writer inde-pendent sytem which can then be adapted to a particular writer witha few sample sentences.
With this level of training it may be possibleto relax the few restrictions made on the writers in this experiment.However, a more robust feature set may be necessary for creatingthe writer independent system.A practical issue in handwriting recognition is the speed of the rec-ognizer.
Approximately 20 seconds per word are required for recog-nition in the present experimental system.
However, we suspect thatreal-time performance is attainable by increasing the efficiency of thecode and porting the decoder to a more powerful hardware platform.Future experiments will be directed at further reduction of the errorrates for the writer dependent task.
More writers may also be in-corporated into the test.
In addition, writer independent and writeradaptive systems may be attempted.
Scalability of the number oftraining sentences will be addressed along with possible changes tothe BYBLOS system to better accomodate handwriting.
Adaptingthe system to off-line handwriting recognition may also be exploredat a later date.6.
CONCLUSIONWe have shown that a HMM based speech recognition system canperform well on on-line cursive handwriting tasks without needingsegmentation of training or test data.
On a 25,595 word, 86 symbol,writer dependent task over six writers, an average of 4.1 % word errorrate and an average of 1.4% character error rate was achieved.
Withsome simple tuning, significant reduction in these error rates is ex-pected.
These findings suggest that HMM-based methods combinedwith statistical grammars will prove to be a very powerful tool inhandwriting recognition.7.
AcknowledgmentsThe authors wish to thank Long Nguyen and George Zavaliagkosfor their help with the BYBLOS system, Tavenner Hall and BrendaPendleton for their assistance in verifying data, and the Vision &Modeling Group, MIT Media Lab for use of their facilities.References1.
C. Tappert, C. Suen, and T. Wakahara.
"The State of the Artin On-Line Handwriting Recognition," IEEE T. Pat.
Anal &Mach.
Int., pp.
787-808, August 1990.2.
R. Nag, K. H. Wong, E Fallside.
"Script Recognition usingHidden Markov Models," In Proc.
ICASSP, pp.
2071-2074,TOkyo, Japan, 1986.3.
K. Nathan, J. Bellegarda, D. Nahamoo, E. Bellegarda.
"On-Line Handwriting Recognition Using Continuous ParameterHidden Markov Models," In Proc.
ICASSP, pp.
V-121-124,Minneapolis, MN, 1993.4.
Y.L.
Chow, M.O.
Dunham, O.A.
Kimball, M.A.
Krasner, G.EKubala, J. Makhoul, P.J.
Price, S. Roucos, and R.M.
Schwartz.
"BYBLOS: The BBN Continuous Speech Recognition Sys-tem," IEEE Int.
Conf.
Acoust., Speech, Signal Processing, Dal-las, TX, Paper No.
3.7, pp.
89-92, April 1987.5.
M. Bates, R. Bobrow, P. Fung, R. Ingda, F. Kubala, J. Makhoul,L.
Nguyen, R. Schwartz, D. Stallard.
"The BBN/HARC SpokenLanguage Understanding System," IEEE Int.
Conf.
Acoust.,Speech, Signal Processing, Minneapolis, MN, April 1993.6.
E Kubala, A. Anastasakos, J. Makhoul, L. Nguyen,R.
Schwartz, G. Zavaliagkos.
"Comparative Experiments onLarge Vocabulary Speech Recognition," To be presented atICASSP, Adelaide, Australia, 1994.7.
MADCOW.
"Multi-Site Data Collection for a Spoken Lan-guage Corpus," Proc.
DARPA Speech and Natural LanguageWorkshop, pp.
7-14, Harriman, NY, Morgan Kaufmann Pub-lishers, 1992.8.
R. M. Schwartz, Y. L. Chow, O.
A. Kimball, S. Roucos,M.
Krasner, and J. Makhoul.
"Context-Dependent Model-ing for Acoustic-Phonetic Recognition of Continuous Speech,"Proc.
ICASSP, pp.1205-1208,Tampa, FL, March 1985.9.
Y.L.
Chow, R.M.
Schwartz, S. Roucos, O.A.
Kimball, P.J.Price, G.F. Kubala, M.O.
Dunham, M.A.
Krasner, and J.Makhoul.
"The Role of Word-Dependent Coarticulatory Ef-fects in a Phoneme-Based Speech Recognition System," IEEEInt.
Conf.
Acoust., Speech, Signal Processing, Tokyo, Japan,pp.
1593-1596, April 1986.10.
D. Paul.
"The Design for the Wall Street Journal-based CSRCorpus," Proc.
DARPA Speech and Natural Language Work-shop, pp.
357-360, Morgan Kaufmann Publishers, 1992.436
