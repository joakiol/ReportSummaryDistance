Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 486?496, Dublin, Ireland, August 23-29 2014.Fast Tweet Retrieval with Compact Binary CodesWeiwei Guo?Wei Liu?Mona Diab?
?Computer Science Department, Columbia University, New York, NY, USA?IBM T. J. Watson Research Center, Yorktown Heights, NY, USA?Department of Computer Science, George Washington University, Washington, D.C., USAweiwei@cs.columbia.edu weiliu@us.ibm.com mtdiab@gwu.eduAbstractThe most widely used similarity measure in the field of natural language processing may be co-sine similarity.
However, in the context of Twitter, the large scale of massive tweet data inevitablymakes it expensive to perform cosine similarity computations among tremendous data samples.In this paper, we exploit binary coding to tackle the scalability issue, which compresses each datasample into a compact binary code and hence enables highly efficient similarity computations viaHamming distances between the generated codes.
In order to yield semantics sensitive binarycodes for tweet data, we design a binarized matrix factorization model and further improve it intwo aspects.
First, we force the projection directions employed by the model nearly orthogonal toreduce the redundant information in their resulting binary bits.
Second, we leverage the tweets?neighborhood information to encourage similar tweets to have adjacent binary codes.
Evaluatedon a tweet dataset using hashtags to create gold labels in an information retrieval scenario, ourproposed model shows significant performance gains over competing methods.1 IntroductionTwitter is rapidly gaining worldwide popularity, with 500 million active users generating more than340 million tweets daily1.
Massive-scale tweet data which is freely available on the Web contains richlinguistic phenomena and valuable information, therefore making it one of most favorite data sourcesused by a variety of Natural Language Processing (NLP) applications.
Successful examples includefirst story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter eventdiscovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc.In these NLP applications, one of core technical components is tweet similarity computing to searchfor the desired tweets with respect to some sample tweets.
For example, in first story detection (Petrovicet al., 2010), the purpose is to find an incoming tweet that is expected to report a novel event not revealedby the previous tweets.
This is done by measuring cosine similarity between the incoming tweet andeach previous tweet.One obvious issue is that cosine similarity computations among tweet data will become very slow oncethe scale of tweet data grows drastically.
In this paper, we investigate the problem of searching for mostsimilar tweets given a query tweet.
Specifically, we propose a binary coding approach to render com-putationally efficient tweet comparisons that should benefit practical NLP applications, especially in theface of massive data scenarios.
Using the proposed approach, each tweet is compressed into short-lengthbinary bits (i.e., a compact binary code), so that tweet comparisons can be performed substantially fasterthrough measuring Hamming distances between the generated compact codes.
Crucially, Hammingdistance computation only involves very cheap NOR and popcount operations instead of floating-pointoperations needed by cosine similarity computation.Compared to other genres of data, similarity search in tweet data is very challenging due to the shortnature of Twitter messages, that is, a tweet contains too little information for traditional models to extractThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://en.wikipedia.org/wiki/Twitter486Symbol Definitionn Number of tweets in the corpus.d Dimension of a tweet vector, i.e., the vocabulary size.xiThe sparse tf-idf vector corresponding to the i-th tweet in the corpus.
?xiThe vector subtracted by the mean ?
of the tweet corpus:?xi= xi?
?.X,?X The tweet corpus in a matrix format, and the zero-centered tweet data.r The number of binary coding functions, i.e., the number of latent topics.fkThe k-th binary coding function.Table 1: Symbols used in binary coding.latent topical semantics.
For instance, in our collected dataset, there exist only 11 words per tweet onaverage.
We address the sparsity issue pertaining to tweet data by converting our previously proposedtopic modelWeighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to a binarized version.WTMF maps a tweet to a low-dimensional semantic vector which can easily be transformed to a binarycode by virtue of a sign function.
We consider WTMF a good baseline for the task of tweet retrieval, asit has achieved state-of-the-art performance among unsupervised systems on two benchmark short-textdatasets released by Li et al.
(2006) and Agirre et al.
(2012).In this paper, we improve WTMF in two aspects.
The first drawback of the WTMF model is that itfocuses on exhaustively encoding the local context, and hence introduces some overlapping informationthat is reflected in its associated projections.
In order to remove the redundant information and meanwhilediscover more distinct topics, we employ a gradient descent method to make the projection directionsnearly orthogonal.The second aspect is to enrich each tweet by its neighbors.
Because of the short context, most tweetsdo not contain sufficient information of an event, as noticed by previous work (Agarwal et al., 2012; Guoet al., 2013).
Ideally, we would like to learn a model such that the tweets related to the same event aremapped to adjacent binary codes.
We fulfill this purpose by augmenting each tweet in a given trainingdataset with its neighboring tweets within a temporal window, and assuming that these neighboring(or similar) tweets are triggered by the same event.
We name the improved model Orthogonal MatrixFactorization with Neighbors (OrMFN).In our experiments, we use Twitter hashtags to create the gold (i.e., groundtruth) labels, where tweetswith the same hashtag are considered semantically related, hence relevant.
We collect a tweet datasetwhich consists of 1.35 million tweets over 3 months where each tweet has exactly one hashtag.
Theexperimental results show that our proposed model OrMFN significantly outperforms competing binarycoding methods.2 Background and Related Work2.1 PreliminariesWe first introduce some notations used in this paper to formulate our problem.
Suppose that we aregiven a dataset of n tweets and the size of the vocabulary is d. A tweet is represented by all the words itcontains.
We use notationx ?
Rdto denote a sparse d-dimensional tf-idf vector corresponding to a tweet,where each word stands for a dimension.
For ease of notation, we represent all n tweets in a matrix X =[x1,x2, ?
?
?
,xn] ?
Rd?n.
For binary coding, we seek r binarization functions{fk: Rd?
{1,?1}}rk=1so that a tweet xiis encoded into an r-bit binary code (i.e., a string of r binary bits).
Table 1 illustratesthe symbols used in this paper for notation.Hamming Ranking: In the paper we evaluate the quality of binary codes in terms of Hamming ranking.Given a query tweet, all data items are ranked in an ascending order according to the Hamming distancesbetween their binary codes and the query?s binary code, where a Hamming distance is the number ofbit positions in which bits of two codes differ.
Compared with cosine similarity, computing Hammingdistance can be substantially efficient.
This is because fixed-length binary bits enable very cheap logicoperations for Hamming distance computation, whereas real-valued vectors require floating-point op-487erations for cosine similarity computation.
Since logic operations are much faster than floating-pointoperations, Hamming distance computation is typically much faster than cosine similarity computation22.2 Binary CodingEarly explorations of binary coding focused on using random permutations or random projections to ob-tain binary coding functions (aka, hash functions), such as Min-wise Hashing (MinHash) (Broder et al.,1998) and Locality-Sensitive Hashing (LSH) (Indyk and Motwani, 1998).
MinHash and LSH are gen-erally considered data-independent approaches, as their coding functions are generated in a randomizedfashion.
In the context of Twitter, the simple LSH scheme proposed in (Charikar, 2002) is of particularinterest.
Charikar proved that the probability of two data points colliding is proportional to the anglebetween them, and then employed a random projection w ?
Rdto construct a binary coding function:f(x) = sgn(w>x)={1, if w>x > 0,?1, otherwise.
(1)The current held view is that data-dependent binary coding can lead to better performance.
A data-dependent coding scheme typically includes two steps: 1) learning a series of binary coding functionswith a small amount of training data; 2) applying the learned functions to larger scale data to producebinary codes.In the context of tweet data, Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) can di-rectly be used for data-dependent binary coding.
LSA reduces the dimensionality of the data in X byperforming singular value decomposition (SVD) over X: X = U?V>.
Let?X be the zero-centered datamatrix, where each tweet vector xiis subtracted by the mean vector ?, resulting in?xi= xi?
?.
The rcoding functions are then constructed by using the r eigenvectors u1,u2, ?
?
?
,urassociated with the rlargest eigenvalues, that is, fk(x) = sgn(u>k?x)= sgn(u>k(x ?
?
))(k = 1, ?
?
?
, r).
The goal of usingzero-centered data?X is to balance 1 bits and ?1 bits.Iterative Quantization (ITQ) (Gong and Lazebnik, 2011) is another popular unsupervised binary cod-ing approach.
ITQ attempts to find an orthogonal rotation matrix R ?
Rr?rto minimize the squaredquantization error: ?B?RV ?2F, whereB ?
{1,?1}r?ncontains the binary codes of all data, V ?
Rr?ncontains the LSA-projected and zero-centered vectors, and ?
?
?Fdenotes Frobenius norm.
After R isoptimized, the binary codes are simply obtained by B = sgn(RV ).Much recent work learns nonlinear binary coding functions, including Spectral Hashing (Weiss etal., 2008), Anchor Graph Hashing (Liu et al., 2011), Bilinear Hashing (Liu et al., 2012b), KernelizedLSH (Kulis and Grauman, 2012), etc.
Concurrently, supervised information defined among training datasamples was incorporated into coding function learning such as Minimal Loss Hashing (Norouzi andFleet, 2011) and Kernel-Based Supervised Hashing (Liu et al., 2012a).
Our proposed method falls intothe category of unsupervised, linear, data-dependent binary coding.2.3 Applications in NLPThe NLP community has successfully applied LSH in several tasks such as first story detection (Petrovicet al., 2010), and paraphrase retrieval for relation extraction (Bhagat and Ravichandran, 2008), etc.
Thispaper shows that our proposed data-dependent binary coding approach is superior to data-independentLSH in terms of the quality of generated binary codes.Subercaze et al.
(2013) proposed a binary coding approach to encode user profiles for recommenda-tions.
Compared to (Subercaze et al., 2013) in which a data unit is a whole user profile consisting of allhis/her Twitter posts, we tackle a more challenging problem, since our data units are extremely short ?namely, a single tweet.2We recognize that different hardware exploiting techniques such as GPU or parallelization accelerate cosine similarity.However, they don?t change the inherent nature of the data representation.
They can be equally applied to Hamming distanceand we anticipate significant speed gains.
We relegate this exploration of different implementations of Hamming distance tofuture work.488X?
P?
QT???
?
?Figure 1: Weighted Textual Matrix Factorization.
The d ?
n matrix X is approximated by the productof a d?
r matrix P and an n?
r matrix Q.
Note in the figure we used the transpose of the Q matrix.3 Weighted Textual Matrix FactorizationThe WTMF model proposed by Guo and Diab (2012) is designed to extract latent semantic vectors forshort textual data.
The low-dimensional semantic vectors can be used to represent the tweets in theoriginal high-dimensional space.
WTMF achieved state-of-the-art unsupervised performance on twoshort text similarity datasets, which can be attributed to the fact that WTMF carefully handles missingwords (the missing words of a text are the words with 0 values in a data vector x).Assume that there are r latent dimensions/topics in the data, the matrix X is approximated by theproduct of a d?r matrix P and an n?r matrixQ, as in Figure 1.
Accordingly, a tweet xjis representedby an r-dimensional vector Qj,?
; similarly, a word wiis generalized by the r-dimensional vector Pi,?
(theith row in matrix P ).
The matrix factorization scheme has an intuitive explanation: the inner-productof a word profile vector Pi,?and a tweet profile vector Qj,?is to approximate the TF-IDF value Xij:Pi,?>Qj,??
Xij(as illustrated by the shaded parts in Figure 1).Intuitively, Xij= 0 suggests that the latent topics of the text xjare not relevant to the word wi.Note that 99% of the cells in X are 0 because of the short contexts, which significantly diminishes thecontribution of the observed words to the searching of optimal P andQ.
To reduce the impact of missingwords, a small weight wmis assigned to each 0 cell of X in the objective function:?i?jWij(Pi,?>Qj,?
?Xij)2+ ?||P ||22+ ?||Q||22,Wi,j={1, if Xij6= 0,wm, if Xij= 0.
(2)where ?
is the regularization parameter.
Alternating Least Squares (Srebro and Jaakkola, 2003) is usedto iteratively compute the latent semantic vectors in P and Q:Pi,?=(Q>?W(i)Q+ ?I)?1Q>?W(i)X>i,?,Qj,?=(P>?W(j)P + ?I)?1P>?W(j)X?,j(3)where?W(i)= diag(Wi,?)
is a n ?
n diagonal matrix containing the i-th row of the weight matrix W .Similarly,?W(j)= diag(W?,j) is a d?
d diagonal matrix containing the j-th column of W .As in Algorithm 1 line 6-9, P andQ are computed iteratively, i.e., in a iteration each Pi,?
(i = 1, ?
?
?
, d)is calculated based on Q, then each Qj,?
(j = 1, ?
?
?
, n) is calculated based on P .
This can be computedefficiently since: (1) all Pi,?share the same Q>Q; similarly all Qj,?share the same P>P ; (2) X is verysparse.
More details can be found in (Steck, 2010).Adapting WTMF to binary coding is straightforward.
Following LSA, we use the matrix P to linearlyproject tweets into low-dimensional vectors, and then apply the sign function.
The k-th binarizationfunction uses the k-th column of the P matrix (P?,k) as followsfk(x) = sgn (P?,k?x) ={1, if P?,k?x > 0,?1, otherwise.
(4)4 Removing Redundant InformationIt is worth noting that there are two explanations of the d?
r matrix P .
The rows of P , denoted by Pi,?,may be viewed as the collection of r-dimensional latent profiles of words, which we observe frequently489Algorithm 1: OrMF1 Procedure P = OrMF(X,W, ?, n itr, ?
)2 n words, n docs?
size(X);3 randomly initialize P,Q;4 itr ?
1;5 while itr < n itr do6 for j ?
1 to n docs do7 Qj,?=(P>?W(j)P + ?I)?1P>?W(j)X?,j8 for i?
1 to n words do9 Pi,?=(Q>?W(i)Q+ ?I)?1Q>?W(i)X>i,?10 c = mean(diag(P>P ));11 P ?
P ?
?P (P>P ?
cI);12 itr ?
itr + 1;in the WTMF model.
Meanwhile, columns of P are projection vectors, denoted by P?,k, which aresimilar to eigenvectors U obtained by LSA.
The projection vector P?,kis employed to multiply to a zerocentered data vector?x to generate a binary string: sgn(P?,k>?x).
In this section, we focus on the propertyof the P matrix columns.As in equation 3, each row in matrices P and Q is iteratively optimized to approximate the data:Pi,?>Qj,??
Xij.
While it does a good job at preserving the existence/relevance of each word in a shorttext, it might encode repetitive information by means of the dimensionality reduction or the projectionvectors P?,k(the columns of P ).
For example, the first dimension P?,1may be 90% about the politicstopic and 10% about the economics topic, and the second dimension P?,2is 95% on economics and 5%on technology topics, respectively.Ideally we would like the dimensions to be uncorrelated, so that more distinct topics of data couldbe captured.
One way to ensure the uncorrelatedness is to force P to be orthogonal, i.e., P>P = I .
Itimplies P?,j>P?,k= 0 if k 6= j.4.1 Implementation of Orthogonal ProjectionsTo produce nearly orthogonal projections in the current framework, we could add a regularizer ?
(P>P?I)2with the weight ?
in the objective function of the WTMF model (equation 6).
However, in practicethis method does not lead to the convergence of P .
This is mainly caused by the phenomenon that anyword profile Pi,?becomes dependent of all other word profiles after an iteration.Therefore, we adopt a simpler method, gradient descent, in which P is updated by taking a small stepin the direction of the negative gradient of (P>P ?
I)2.
It is also worth noting that (P>P ?
I)2requireseach projection P?,kto be a unit vector because of P?,k>P?,k= 1, which is infeasible when the nonzerovalues in X are large.
Therefore, we multiply the matrix I by a coefficient c, which is calculated fromthe mean of the diagonal of P>P in the current iteration.
The following two lines are added at the endof an iteration:c?
mean(diag(P>P )),P ?P ?
?P (P>P ?
cI).
(5)This procedure is presented in Algorithm 1.
Accordingly, the magnitude of P is not affected.
The stepsize ?
is fixed to 0.0001.
We refer to this model as Orthogonal Matrix Factorization (OrMF).5 Exploiting Nearest Neighbors for TweetsWe observe that tweets triggered by the same event do not have very high cosine similarity scores amongthem.
This is caused by the inherent short length of tweets such that usually a tweet only describes one490aspect of an event (Agarwal et al., 2012; Guo et al., 2013).
Our objective is to find the relevant tweetsgiven a tweet, and then learn a model that assigns similar binary bits to these relevant tweets.5.1 Modeling Neighboring TweetsGiven a tweet, we treat its nearest neighbors in a temporal window as its most relevant tweets.
Weassume that the other aspects of an event can be found in its nearest neighbors.
Accordingly, we extractt neighbors for a tweet from 10,000 most chronologically close tweets.
In this current implementation,we set t = 5.Under the weighted matrix factorization framework, we extend each tweet by its t nearest neighbors.Specifically, for each tweet, we incorporate additional words from its neighboring tweets.
The valuesof the new words are averaged.
Moreover, these new words are treated differently by assigning a newweight wnto them, since we believe that the new words are not as informative as the original words inthe tweet.We present an illustrative example of how to use neighbors to extend the tweets.
Let x1be a tweetwith the following words (the numbers after the colon are TF-IDF values):x1= {obama:5.5, medicare:8.3, website:3.8}which has two nearest neighbors:x27= {obama:5.5, medicare:8.3, website:3.8, down:5.4}x356= {obama:5.5, medicare:8.3, website:3.8, problem:7.0}Then there are two additional words added in x1whose values are averaged.
The new data vector x?1is:x?1= {obama:5.5, medicare:8.3, website:3.8, down:2.7, problem:3.5}Therefore, the algorithm is run on the new neighbor-augmented data matrix, denoted by X?, and theweight matrix W becomesWi,j=??
?1, if X?ij6= 0 &j is an original word,wn, if X?ij6= 0, &j is from neighbor tweets,wm, if X?ij= 0.
(6)This model is referred to as Orthogonal Matrix Factorization with Neighbors (OrMFN).5.2 Binary coding without NeighborsIt is important to point out that the data used by OrMFN, X?, could be a very small subset of the wholedataset.
Therefore we only need to find neighbors for a small portion of the data.
After the P matrixis learned, the neighborhood information is implicitly encoded in the matrix P , and we still apply thesame binarization function sgn(P?,k>?x) on the whole dataset (in large scale) without neighborhoodinformation.
We randomly sample 200,000 tweets for OrMFN to learn P ; neighbors are extracted onlyfor these 200,000 tweets (note that the neighbors are from the 200,000 tweets as well), and then we usethe learned P to generate binary codes for the whole dataset 1.35 million tweets without searching fortheir nearest neighbors.3Our scheme has a clear advantage: the binary coding remains very efficient.
During binarization forany data, there is no need to compare 10,000 most recent tweets to find nearest neighbors, which couldbe time-consuming.
An opposite example is the method presented in (Guo et al., 2013), where t mostnearest neighbor tweets were extracted, and a tweet profile Qj,?was explicitly forced to be similar to itsneighbors?
profiles.
However, for each new data, the approach proposed in (Guo et al., 2013) requirescomputing its nearest neighbors.6 Experiments6.1 Tweet DataWe crawled English tweets spanning three months from October 5th 2013 to January 5th 2014 using theTwitter API.4We cleaned the data such that each hashtag appears at least 100 times in the corpus, and3When generating the binary codes for the 200,000 tweets, these tweets are not augmented with neighbor words.4https://dev.twitter.com491each word appears at least 10 times.
This data collection consists of 1,350,159 tweets, 15 million wordtokens, 30,608 unique words, and 3,214 unique hashtags.One of main reasons to use hashtags is to enhance accessing topically similar tweets (Efron, 2010).In a large-scale data setting, it is impossible to manually identify relevant tweets.
Therefore, we useTwitter hashtags to create groundtruth labels, which means that tweets marked by the same hashtagas the query tweet are considered relevant.
Accordingly, in our experiments all hashtags are removedfrom the original data corpus.
We chose a subset of hashtags from the most frequent hashtags to creategroundtruth labels: we manually removed some tags from the subset that are not topic-related (e.g.,#truth, #lol) or are ambiguous; we also removed all the tags that are referring to TV series (the relevanttweets can be trivially obtained by named entity matching).
The resulting subset contains 18 hashtags.5100 tweets are randomly selected as queries (test data) for each of the 18 hashtags.
The mediannumber of relevant tweets per query is 5,621.
The small size of gold standard makes the task relativelychallenging.
We need to identify 5,621 (0.42% of the whole dataset) tweets out of 1.35 million tweets.200,000 tweets are randomly selected (not including the 1,800 queries) as training data for the datadependent models to learn binarization functions.6The functions are subsequently applied on all the1.35 million tweets, including the 1,800 query tweets.6.2 EvaluationWe evaluate a model by the search quality: given a tweet as query, we would like to rank the relevanttweets as high as possible.
Following previous work (Weiss et al., 2008; Liu et al., 2011), we use meanprecision among top 1000 returned list (MP@1000) to measure the ranking quality.
Let pre@k be theprecision among top k return data, then MP@1000 is the average value of pre@1, pre@2...pre@1000.Obviously MP gives more reward on the systems that can rank relevant data in the top places, e.g., ifthe highest ranked tweet is a relevant tweet, then all the precision values (pre@2, pre@3, pre@4...) areincreased.
We also calculate the precision and recall curve at varying values of top k returned list.6.3 MethodsWe evaluate the proposed unsupervised binary coding models OrMF and OrMFN, whose performance iscompared against 5 other unsupervised methods, LSH, SH, LSA, ITQ, and WTMF.
All the binary codingfunctions except LSH are learned on the 200,000 tweet set.
All the methods have the same form of binarycoding functions: sgn(P?,k>?x), where they differ only in the projection vector P?,k.
The retrieved tweetsare ranked according to their Hamming distance to the query, where Hamming distance is the number ofdifferent bit positions between the binary codes of a tweet and the query.For ITQ and SH, we use the code provided by the authors.
Note that the dense matrix?X?X>isimpossible to compute due the large vocabulary, therefore we replace it by sparse matrix XX>.
For thethree matrix factorization based methods (WTMF, OrMF, OrMFN) we run 10 iterations.
The regularizer?
in equation 6 is fixed at 20 as in (Guo and Diab, 2012).
A small set of 500 tweets is selected fromthe training set as tuning set to choose the missing word weight wmin the baseline WTMF, and then itsvalue is fixed for OrMF and OrMFN.
The same 500 tweets tuning set is used to choose the neighbor wordweight wn.
In fact these models are very stable, consistently outperforming the baselines regardless ofdifferent values of wmand wn, as later shown in Figure 4 and 5.We also present the results of cosine similarity on the original word space (COSINE) as an upperbound of the binary coding methods.
We implemented an efficient algorithm for COSINE, which is thealgorithm 1 in (Petrovic et al., 2010).
It firstly normalizes each data to a unit vector, then cosine similarityis calculated by traversing only once the tweets via inverted word index.6.4 ResultsTable 2 summarizes the ranking performance measured by MP@1000 (the mean precision at top 1000returned list).
Figures 2 and 3 illustrate the corresponding precision and recall curve for the Hamming5The tweet dataset and their associated list of hashtags will be available upon request.6Although we use the word ?training?, the hashtags are never seen by the models.
The training data is used for the modelsto learn the word co-occurrence, and construct binary coding functions.492Models Parameters r=64 r=96 r=128LSH ?
19.21% 21.84% 23.75%SH ?
18.29% 19.32% 19.95%LSA ?
21.04% 22.07% 22.67%ITQ ?
20.8% 22.06% 22.86%WTMF wm= 0.1 26.64% 29.39% 30.38%OrMF wm= 0.1 27.7% 30.48% 31.26%OrMFN wm= 0.1, wn= 0.5 29.73% 31.73% 32.55%COSINE ?
33.68%Table 2: Mean precision among top 1000 returned list0 200 400 600 800 10000.150.20.250.30.350.40.450.5Precision# of samplesLSHSHLSAITQWTMFOrMFOrMFN(a) r = 640 200 400 600 800 10000.150.20.250.30.350.40.450.5Precision# of samplesLSHSHLSAITQWTMFOrMFOrMFN(b) r = 960 200 400 600 800 10000.150.20.250.30.350.40.450.5Precision# of samplesLSHSHLSAITQWTMFOrMFOrMFN(c) r = 128Figure 2: Hamming ranking: precision curve under top 1000 returned listdistance ranking.
The number of r binary coding functions corresponds to the number of dimensions inthe 6 data-dependent models LSA, SH, ITQ, WTMF, OrMF and OrMFN.
The missing words weight wmis fixed as 0.1 based on the tuning set in the three weighted matrix factorization based models WTMF,OrMF and OrMFN.
The neighbor word weight wnis chosen as 0.5 for OrMFN.
Later in Section 6.4.1we show that the performance is robust using varying values of wmand wn.As the number of bits increases, all binary coding models yield better results.
This is understandablesince the binary bits really record very tiny bits of information from each tweet, and more bits, the morethey are able to capture more semantic information.SH has the worst MP@1000 performance.
The reason might be it is designed for vision data wherethe data vector is relatively dense.
ITQ yields comparable results to LSA in terms of MP@1000, yet therecall curve in Figure 3b,c clearly shows the superiority of ITQ over LSA.WTMF outperforms LSA by a large margin (around 5% to 7%) through properly modeling missingwords, which is also observed in (Guo and Diab, 2012).
Although WTMF already reaches a very highMP@1000 performance level, OrMF can still achieve around 1% improvement over WTMF, which canbe attributed to orthogonal projections that captures more distinct topics.
At last, leveraging neighbor-hood information, OrMFN is the best performing model (around 1% improvement over OrMF).
Thetrend holds consistently across all conditions.
The precision and recall curves in Figures 2 and 3 confirmthe trend observed in Table 2 as well.All the binary coding models yield worse performance than COSINE baseline.
This is expected, as thebinary bits are employed to gain efficiency at the cost of accuracy: the 128 bits significantly compressthe data losing a lot of nuanced information, whereas in the high dimensional word space 128 bits canbe only used to record two words (32 bits for two word indices and 32 bits for two TF-IDF values).
Wemanually examined the ranking list.
We found in the binary coding models, there exist a lot of ties (128bits only result in 128 possible Hamming distance values), whereas the COSINE baseline can correctlyrank them by detecting the subtle difference signaled by the real-valued TF-IDF values.4930 2 4 6 8 10 12x 10400.050.10.150.20.250.3Recall# of samplesLSHSHLSAITQWTMFOrMFOrMFN(a) r = 640 2 4 6 8 10 12x 10400.050.10.150.20.250.3Recall# of samplesLSHSHLSAITQWTMFOrMFOrMFN(b) r = 960 2 4 6 8 10 12x 10400.050.10.150.20.250.3Recall# of samplesLSHSHLSAITQWTMFOrMFOrMFN(c) r = 128Figure 3: Hamming ranking: recall curve under top 100,000 returned list0.05 0.08 0.1 0.15 0.20.250.260.270.280.290.30.310.320.330.34MP@1000wmWTMFOrMFOrMFN(a) r = 640.05 0.08 0.1 0.15 0.20.250.260.270.280.290.30.310.320.330.34MP@1000wmWTMFOrMFOrMFN(b) r = 960.05 0.08 0.1 0.15 0.20.250.260.270.280.290.30.310.320.330.34MP@1000wmWTMFOrMFOrMFN(c) r = 128Figure 4: Weighted matrix factorization based models: MP@1000 vs. missing word weight wm6.4.1 AnalysisWe are interested in whether other values of wmand wncan generate good results ?
in other words,whether the performance is robust to the two parameter values.
Accordingly, we present their im-pact on MP@1000 in Figure 4 and 5.
In Figure 4, the missing word weight wmis chosen from{0.05, 0.08, 0.1, 0.15, 0.2}, where in OrMFN the neighbor weight wnis fixed as 0.5.
The figure in-dicates we can achieve even better MP@1000 around 33.2% when selecting the optimal wm= 0.05.In general, the curves for all the code length are very smooth; the chosen value of wmdoes not have anegative impact, e.g., the gain from OrMF over WTMF is always positive.Figure 5 demonstrates the impact of varying the values of neighbor word weight wnfrom{0, 0.25, 0.5, 0.75, 1} on OrMFN tested in different r conditions.
Note that when wn= 0 indicatingthat no neighbor information is exploited, the OrMFN model is simply reduced to the OrMF model.Based on the Figure illustration we can conclude that integrating neighboring word information alwaysyields a positive effect, since any value of wn> 0 yields a performance gain over wn= 0 which isOrMF.6.5 Computation CostThe data-dependent models involve 2 steps: 1) learning coding functions from a small dataset, and 2)binary coding for the large scale whole dataset.7In real-time scenarios, the time is only spent on the2nd step that involves no matrix factorization.
The computation cost of binary coding for all models(LSH, ITQ, LSA, WTMF, OrMF and OrMFN) are roughly the same: sgn(P?,k>?x).
Note that P?,k>?x =P?,k>x?
P?,k>?
where x is a very sparse vector (with 11 non-zeros values on average) and P?,k>?
canbe precomputed.
On the other hand, calculating Hamming distance on binary codes is also very fastusing the logic operations.7Learning the binarization functions can be always done on a small dataset, for example in this paper all the data dependentmodels are run on the 200,000 tweets, hence it performs very fast.
In addition, in the OrMFN model, there is no need to findnearest neighbors for the whole dataset in the 2nd step (the binary coding step).4940 0.2 0.4 0.6 0.8 10.270.280.290.30.310.320.330.34MP@1000wnr=64r=96r=128Figure 5: OrMFN model: MP@1000 vs. neighbor word weight wn7 ConclusionIn this paper, we proposed a novel unsupervised binary coding model which provides efficient similaritysearch in massive tweet data.
The proposed model, OrMFN, improves an existing matrix factorizationmodel through learning nearly orthogonal projection directions and leveraging the neighborhood infor-mation hidden in tweet data.
We collected a dataset whose groundtruth labels are created from Twitterhashtags.
Our experiments conducted on this dataset showed significant performance gains of OrMFNover the competing methods.AcknowledgementsWe thank Boyi Xie and three anonymous reviewers for their valuable comments.
This project is sup-ported by the DARPA DEFT Program.ReferencesPuneet Agarwal, Rajgopal Vaithiyanathan, Saurabh Sharma, and Gautam Shroff.
2012.
Catching the long-tail: Ex-tracting local news events from twitter.
In Proceedings of the Sixth International AAAI Conference on Weblogsand Social Media.Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre.
2012.
Semeval-2012 task 6: A pilot onsemantic textual similarity.
In First Joint Conference on Lexical and Computational Semantics (*SEM).Edward Benson, Aria Haghighi, and Regina Barzilay.
2011.
Event discovery in social media feeds.
In Proceedingsof the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.Rahul Bhagat and Deepak Ravichandran.
2008.
Large scale acquisition of paraphrases for learning surface pat-terns.
In Proceedings of ACL-08: HLT.Andrei Z Broder, Moses Charikar, Alan M Frieze, and Michael Mitzenmacher.
1998.
Min-wise independentpermutations.
In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing.Deepayan Chakrabarti and Kunal Punera.
2011.
Event summarization using tweets.
In Proceedings of the FifthInternational AAAI Conference on Weblogs and Social Media.Moses S. Charikar.
2002.
Similarity estimation techniques from rounding algorithms.
In Proceedings of theThiry-fourth Annual ACM Symposium on Theory of Computing.Miles Efron.
2010.
Information search and retrieval in microblogs.
In Journal of the American Society forInformation Science and Technology.Yunchao Gong and Svetlana Lazebnik.
2011.
Iterative quantization: A procrustean approach to learning binarycodes.
In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.495Weiwei Guo and Mona Diab.
2012.
Modeling sentences in the latent space.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguistics.Weiwei Guo, Hao Li, Heng Ji, and Mona Diab.
2013.
Linking tweets to news: A framework to enrich onlineshort text data in social media.
In Proceedings of the 51th Annual Meeting of the Association for ComputationalLinguistics.Piotr Indyk and Rajeev Motwani.
1998.
Approximate nearest neighbors: towards removing the curse of dimen-sionality.
In Proceedings of the thirtieth annual ACM symposium on Theory of computing.Brian Kulis and Kristen Grauman.
2012.
Kernelized locality-sensitive hashing.
IEEE Transactions On PatternAnalysis and Machine Intelligence, 34(6):1092?1104.Thomas K. Landauer and Susan T. Dumais.
1997.
A solution to plato?s problem: The latent semantic analysistheory of acquisition, induction and representation of knowledge.
In Psychological review.Yuhua Li, David McLean, Zuhair A. Bandar, James D. O?Shea, and Keeley Crockett.
2006.
Sentence similaritybased on semantic nets and corpus statistics.
IEEE Transaction on Knowledge and Data Engineering, 18.Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang.
2011.
Hashing with graphs.
In Proceedings of the 28thInternational Conference on Machine Learning.Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and Shih-Fu Chang.
2012a.
Supervised hashing with kernels.In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, and Shih-Fu Chang.
2012b.
Compact hyperplane hashing withbilinear functions.
In Proceedings of the 29th International Conference on Machine Learning.Mohammad Norouzi and David J.
Fleet.
2011.
Minimal loss hashing for compact binary codes.
In Proceedingsof the 28th International Conference on Machine Learning.Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010.
Streaming first story detection with application totwitter.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of theAssociation for Computational Linguistics.Nathan Srebro and Tommi Jaakkola.
2003.
Weighted low-rank approximations.
In Proceedings of the TwentiethInternational Conference on Machine Learning.Harald Steck.
2010.
Training and testing of recommender systems on data missing not at random.
In Proceedingsof the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.Julien Subercaze, Christophe Gravier, and Frederique Laforest.
2013.
Towards an expressive and scalable twitter?susers profiles.
In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent AgentTechnologies.Yair Weiss, Antonio Torralba, and Rob Fergus.
2008.
Spectral hashing.
In Advances in Neural InformationProcessing Systems.496
