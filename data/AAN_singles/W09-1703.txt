Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18?26,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsCorpus-based Semantic Lexicon Induction with Web-based CorroborationSean P. IgoCenter for High Performance ComputingUniversity of UtahSalt Lake City, UT 84112 USASean.Igo@utah.eduEllen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112 USAriloff@cs.utah.eduAbstractVarious techniques have been developed to au-tomatically induce semantic dictionaries fromtext corpora and from the Web.
Our researchcombines corpus-based semantic lexicon in-duction with statistics acquired from the Webto improve the accuracy of automatically ac-quired domain-specific dictionaries.
We usea weakly supervised bootstrapping algorithmto induce a semantic lexicon from a text cor-pus, and then issue Web queries to generateco-occurrence statistics between each lexiconentry and semantically related terms.
The Webstatistics provide a source of independent ev-idence to confirm, or disconfirm, that a wordbelongs to the intended semantic category.
Weevaluate this approach on 7 semantic cate-gories representing two domains.
Our resultsshow that the Web statistics dramatically im-prove the ranking of lexicon entries, and canalso be used to filter incorrect entries.1 IntroductionSemantic resources are extremely valuable for manynatural language processing (NLP) tasks, as evi-denced by the wide popularity of WordNet (Miller,1990) and a multitude of efforts to create similar?WordNets?
for additional languages (e.g.
(Atseriaset al, 1997; Vossen, 1998; Stamou et al, 2002)).Semantic resources can take many forms, but one ofthe most basic types is a dictionary that associatesa word (or word sense) with one or more semanticcategories (hypernyms).
For example, truck mightbe identified as a VEHICLE, and dog might be identi-fied as an ANIMAL.
Automated methods for generat-ing such dictionaries have been developed under therubrics of lexical acquisition, hyponym learning, se-mantic class induction, and Web-based informationextraction.
These techniques can be used to rapidlycreate semantic lexicons for new domains and lan-guages, and to automatically increase the coverageof existing resources.Techniques for semantic lexicon induction can besubdivided into two groups: corpus-based methodsand Web-based methods.
Although the Web can beviewed as a (gigantic) corpus, these two approachestend to have different goals.
Corpus-based methodsare typically designed to induce domain-specific se-mantic lexicons from a collection of domain-specifictexts.
In contrast, Web-based methods are typicallydesigned to induce broad-coverage resources, simi-lar to WordNet.
Ideally, one would hope that broad-coverage resources would be sufficient for any do-main, but this is often not the case.
Many domainsuse specialized vocabularies and jargon that are notadequately represented in broad-coverage resources(e.g., medicine, genomics, etc.).
Furthermore, evenrelatively general text genres, such as news, con-tain subdomains that require extensive knowledgeof specific semantic categories.
For example, ourwork uses a corpus of news articles about terror-ism that includes many arcane weapon terms (e.g.,M-79, AR-15, an-fo, and gelignite).
Similarly, ourdisease-related documents mention obscure diseases(e.g., psittacosis) and contain many informal terms,abbreviations, and spelling variants that do not evenoccur in most medical dictionaries.
For example, yfrefers to yellow fever, tularaemia is an alternativespelling for tularemia, and nv-cjd is frequently used18to refer to new variant Creutzfeldt Jacob Disease.The Web is such a vast repository of knowledgethat specialized terminology for nearly any domainprobably exists in some niche or cranny, but find-ing the appropriate corner of the Web to tap into is achallenge.
You have to know where to look to findspecialized knowledge.
In contrast, corpus-basedmethods can learn specialized terminology directlyfrom a domain-specific corpus, but accuracy can bea problem because most corpora are relatively small.In this paper, we seek to exploit the best of bothworlds by combining a weakly supervised corpus-based method for semantic lexicon induction withstatistics obtained from the Web.
First, we usea bootstrapping algorithm, Basilisk (Thelen andRiloff, 2002), to automatically induce a semanticlexicon from a domain-specific corpus.
This pro-duces a set of words that are hypothesized to be-long to the targeted semantic category.
Second, weuse the Web as a source of corroborating evidenceto confirm, or disconfirm, whether each term trulybelongs to the semantic category.
For each candi-date word, we search the Web for pages that con-tain both the word and a semantically related term.We expect that true semantic category members willco-occur with semantically similar words more of-ten than non-members.This paper is organized as follows.
Section 2 dis-cusses prior work on weakly supervised methods forsemantic lexicon induction.
Section 3 overviewsour approach: we briefly describe the weakly su-pervised bootstrapping algorithm that we use forcorpus-based semantic lexicon induction, and thenpresent our procedure for gathering corroboratingevidence from the Web.
Section 4 presents exper-imental results on seven semantic categories repre-senting two domains: Latin American terrorism anddisease-related documents.
Section 5 summarizesour results and discusses future work.2 Related WorkOur research focuses on semantic lexicon induc-tion, where the goal is to create a list of wordsthat belong to a desired semantic class.
A sub-stantial amount of previous work has been done onweakly supervised and unsupervised creation of se-mantic lexicons.
Weakly supervised corpus-basedmethods have utilized noun co-occurrence statis-tics (Riloff and Shepherd, 1997; Roark and Char-niak, 1998), syntactic information (Widdows andDorow, 2002; Phillips and Riloff, 2002; Pantel andRavichandran, 2004; Tanev and Magnini, 2006),and lexico-syntactic contextual patterns (e.g., ?re-sides in <location>?
or ?moved to <location>?
)(Riloff and Jones, 1999; Thelen and Riloff, 2002).Due to the need for POS tagging and/or parsing,these types of methods have been evaluated onlyon fixed corpora1, although (Pantel et al, 2004)demonstrated how to scale up their algorithms forthe Web.
The goal of our work is to improve uponcorpus-based bootstrapping algorithms by using co-occurrence statistics obtained from the Web to re-rank and filter the hypothesized category members.Techniques for semantic class learning have alsobeen developed specifically for the Web.
Sev-eral Web-based semantic class learners build uponHearst?s early work (Hearst, 1992) with hyponympatterns.
Hearst exploited patterns that explicitlyidentify a hyponym relation between a semanticclass and a word (e.g., ?such authors as <X>?)
toautomatically acquire new hyponyms.
(Pas?ca, 2004)applied hyponym patterns to the Web and learned se-mantic class instances and groups by acquiring con-texts around the patterns.
Later, (Pasca, 2007) cre-ated context vectors for a group of seed instances bysearching Web query logs, and used them to learnsimilar instances.
The KnowItAll system (Etzioniet al, 2005) also uses hyponym patterns to extractclass instances from the Web and evaluates them fur-ther by computing mutual information scores basedon Web queries.
(Kozareva et al, 2008) proposedthe use of a doubly-anchored hyponym pattern anda graph to represent the links between hyponym oc-currences in these patterns.Our work builds upon Turney?s work on seman-tic orientation (Turney, 2002) and synonym learning(Turney, 2001), in which he used a PMI-IR algo-rithm to measure the similarity of words and phrasesbased on Web queries.
We use a similar PMI (point-wise mutual information) metric for the purposes ofsemantic class verification.There has also been work on fully unsupervised1Meta-bootstrapping (Riloff and Jones, 1999) was evaluatedon Web pages, but used a precompiled corpus of downloadedWeb pages.19semantic clustering (e.g., (Lin, 1998; Lin and Pan-tel, 2002; Davidov and Rappoport, 2006; Davidov etal., 2007)), however clustering methods may or maynot produce the types and granularities of seman-tic classes desired by a user.
Another related lineof work is automated ontology construction, whichaims to create lexical hierarchies based on semanticclasses (e.g., (Caraballo, 1999; Cimiano and Volker,2005; Mann, 2002)).3 Semantic Lexicon Induction withWeb-based CorroborationOur approach combines a weakly supervised learn-ing algorithm for corpus-based semantic lexicon in-duction with a follow-on procedure that gathers cor-roborating statistical evidence from the Web.
Inthis section, we describe both of these components.First, we give a brief overview of the Basilisk boot-strapping algorithm that we use for corpus-based se-mantic lexicon induction.
Second, we present ournew strategies for acquiring and utilizing corrobo-rating statistical evidence from the Web.3.1 Corpus-based Semantic Lexicon Inductionvia BootstrappingFor corpus-based semantic lexicon induction, weuse a weakly supervised bootstrapping algorithmcalled Basilisk (Thelen and Riloff, 2002).
As in-put, Basilisk requires a small set of seed words foreach semantic category, and a collection of (unanno-tated) texts.
Basilisk iteratively generates new wordsthat are hypothesized to belong to the same seman-tic class as the seeds.
Here we give an overview ofBasilisk?s algorithm and refer the reader to (Thelenand Riloff, 2002) for more details.The key idea behind Basilisk is to use pattern con-texts around a word to identify its semantic class.Basilisk?s bootstrapping process has two main steps:Pattern Pool Creation and Candidate Word Selec-tion.
First, Basilisk applies the AutoSlog patterngenerator (Riloff, 1996) to create a set of lexico-syntactic patterns that, collectively, can extract everynoun phrase in the corpus.
Basilisk then ranks thepatterns according to how often they extract the seedwords, under the assumption that patterns which ex-tract known category members are likely to extractother category members as well.
The highest-rankedpatterns are placed in a pattern pool.Second, Basilisk gathers every noun phrase that isextracted by at least one pattern in the pattern pool,and designates each head noun as a candidate for thesemantic category.
The candidates are then scoredand ranked.
For each candidate, Basilisk collects allof the patterns that extracted that word, computes thelogarithm of the number of seeds extracted by eachof those patterns, and finally computes the averageof these log values as the score for the candidate.Intuitively, a candidate word receives a high scoreif it was extracted by patterns that, on average, alsoextract many known category members.The N highest ranked candidates are automati-cally added to the list of seed words, taking a leapof faith that they are true members of the semanticcategory.
The bootstrapping process then repeats,using the larger set of seed words as known categorymembers in the next iteration.Basilisk learns many good category members,but its accuracy varies a lot across semantic cate-gories (Thelen and Riloff, 2002).
One problem withBasilisk, and bootstrapping algorithms in general, isthat accuracy tends to deteriorate as bootstrappingprogresses.
Basilisk generates candidates by iden-tifying the contexts in which they occur and wordsunrelated to the desired category can sometimes alsooccur in those contexts.
Some patterns consistentlyextract members of several semantic classes; for ex-ample, ?attack on <NP>?
will extract both people(?attack on the president?)
and buildings (?attackon the U.S.
embassy?).
Idiomatic expressions andparsing errors can also lead to undesirable words be-ing learned.
Incorrect words tend to accumulate asbootstrapping progresses, which can lead to gradu-ally deteriorating performance.
(Thelen and Riloff, 2002) tried to address thisproblem by learning multiple semantic categories si-multaneously.
This helps to keep the bootstrappingfocused by flagging words that are potentially prob-lematic because they are strongly associated with acompeting category.
This improved Basilisk?s accu-racy, but by a relatively small amount, and this ap-proach depends on the often unrealistic assumptionthat a word cannot belong to more than one seman-tic category.
In our work, we use the single-categoryversion of Basilisk that learns each semantic cate-gory independently so that we do not need to make20this assumption.3.2 Web-based Semantic Class CorroborationThe novel aspect of our work is that we introduce anew mechanism to independently verify each candi-date word?s category membership using the Web asan external knowledge source.
We gather statisticsfrom the Web to provide evidence for (or against)the semantic class of a word in a manner completelyindependent of Basilisk?s criteria.
Our approachis based on the distributional hypothesis (Harris,1954), which says that words that occur in the samecontexts tend to have similar meanings.
We seek tocorroborate a word?s semantic class through statis-tics that measure how often the word co-occurs withsemantically related words.For each candidate word produced by Basilisk, weconstruct a Web query that pairs the word with a se-mantically related word.
Our goal is not just to findWeb pages that contain both terms, but to find Webpages that contain both terms in close proximity toone another.
We consider two terms to be collo-cated if they occur within ten words of each otheron the same Web page, which corresponds to thefunctionality of the NEAR operator used by the Al-taVista search engine2.
Turney (Turney, 2001; Tur-ney, 2002) reported that the NEAR operator outper-formed simple page co-occurrence for his purposes;our early experiments informally showed the samefor this work.We want our technique to remain weakly super-vised, so we do not want to require additional hu-man input or effort beyond what is already requiredfor Basilisk.
With this in mind, we investigated twotypes of collocation relations as possible indicatorsof semantic class membership:Hypernym Collocation: We compute co-occurrence statistics between the candidate wordand the name of the targeted semantic class (i.e.,the word?s hypothesized hypernym).
For example,given the candidate word jeep and the semanticcategory VEHICLE, we would issue the Web query?jeep NEAR vehicle?.
Our intuition is that suchqueries would identify definition-type Web hits.For example, the query ?cow NEAR animal?
mightretrieve snippets such as ?A cow is an animal found2http://www.altavista.comon dairy farms?
or ?An animal such as a cowhas...?.Seed Collocation: We compute co-occurrencestatistics between the candidate word and each seedword that was given to Basilisk as input.
For ex-ample, given the candidate word jeep and the seedword truck, we would issue the Web query ?jeepNEAR truck?.
Here the intuition is that members ofthe same semantic category tend to occur near oneanother - in lists, for example.As a statistical measure of co-occurrence, wecompute a variation of Pointwise Mutual Informa-tion (PMI), which is defined as:PMI(x, y) = log( p(x,y)p(x)?p(y) )where p(x, y) is the probability that x and y are col-located (near each other) on a Web page, p(x) is theprobability that x occurs on a Web page, and p(y) isthe probability that y occurs on a Web page.p(x) is calculated as count(x)N , where count(x) isthe number of hits returned by AltaVista, searchingfor x by itself, and N is the total number of docu-ments on the World Wide Web at the time the queryis made.
Similarly, p(x, y) is count(x NEAR y)N .Given this, the PMI equation can be rewritten as:log(N) + log( count(x NEAR y)count(x)?count(y) )N is not known, but it is the same for everyquery (assuming the queries were made at roughlythe same time).
We will use these scores solely tocompare the relative goodness of candidates, so wecan omit N from the equation because it will notchange the relative ordering of the scores.
Thus, ourPMI score3 for a candidate word and related term(hypernym or seed) is:log( count(x NEAR y)count(x)?count(y) )Finally, we created three different scoring func-tions that use PMI values in different ways to cap-ture different types of co-occurrence information:Hypernym Score: PMI based on collocation be-tween the hypernym term and candidate word.3In the rare cases when a term had a zero hit count, we as-signed -99999 as the PMI score, which effectively ranks it at thebottom.21Average of Seeds Score: The mean of the PMIscores computed for the candidate and eachseed word:1|seeds||seeds|?i=1PMI(candidate, seedi)Max of Seeds Score: The maximum (highest) ofthe PMI scores computed for the candidate andeach seed word.The rationale for the Average of Seeds Score isthat the seeds are all members of the semantic cat-egory, so we might expect other members to occurnear many of them.
Averaging over all of the seedscan diffuse unusually high or low collocation countsthat might result from an anomalous seed.
The ra-tionale for the Max of Seeds Score is that a wordmay naturally co-occur with some category mem-bers more often than others.
For example, one wouldexpect dog to co-occur with cat much more fre-quently than with frog.
A high Max of Seeds Scoreindicates that there is at least one seed word that fre-quently co-occurs with the candidate.Since Web queries are relatively expensive, it isworth taking stock of how many queries are nec-essary.
Let N be the number of candidate wordsproduced by Basilisk, and S be the number ofseed words given to Basilisk as input.
To com-pute the Hypernym Score for a candidate, we need3 queries: count(hypernym), count(candidate),and count(hypernym NEAR candidate).
Thefirst query is the same for all candidates, so for Ncandidate words we need 2N + 1 queries in total.To compute the Average or Max of Seeds Score fora candidate, we need S queries for count(seedi), Squeries for count(seedi NEAR candidate), and 1query for count(candidate).
So for N candidatewords we need N ?
(2S + 1) queries.
S is typicallysmall for weakly supervised algorithms (S=10 in ourexperiments), which means that this Web-based cor-roboration process requires O(N) queries to processa semantic lexicon of size N .4 Evaluation4.1 Data SetsWe ran experiments on two corpora: 1700 MUC-4terrorism articles (MUC-4 Proceedings, 1992) anda combination of 6000 disease-related documents,consisting of 2000 ProMed disease outbreak re-ports (ProMed-mail, 2006) and 4000 disease-relatedPubMed abstracts (PubMed, 2009).
For the terror-ism domain, we created lexicons for four semanticcategories: BUILDING, HUMAN, LOCATION, andWEAPON.
For the disease domain, we created lexi-cons for three semantic categories: ANIMAL4, DIS-EASE, and SYMPTOM.
For each category, we gaveBasilisk 10 seed words as input.
The seeds werechosen by applying a shallow parser to each corpus,extracting the head nouns of all the NPs, and sort-ing the nouns by frequency.
A human then walkeddown the sorted list and identified the 10 most fre-quent nouns that belong to each semantic category5.This strategy ensures that the bootstrapping processis given seed words that occur in the corpus withhigh frequency.
The seed words are shown in Ta-ble 1.BUILDING: embassy office headquarters churchoffices house home residence hospital airportHUMAN: people guerrillas members troopsCristiani rebels president terrorists soldiers leadersLOCATION: country El Salvador SalvadorUnited States area Colombia city countriesdepartment NicaraguaWEAPON: weapons bomb bombs explosives armsmissiles dynamite rifles materiel bulletsANIMAL: bird mosquito cow horse pig chickensheep dog deer fishDISEASE: SARS BSE anthrax influenza WNVFMD encephalitis malaria pneumonia fluSYMPTOM: fever diarrhea vomiting rash paralysisweakness necrosis chills headaches hemorrhageTable 1: Seed WordsTo evaluate our results, we used the gold standardanswer key that Thelen & Riloff created to evaluateBasilisk on the MUC4 corpus (Thelen and Riloff,2002); they manually labeled every head noun in thecorpus with its semantic class.
For the ProMed /PubMed disease corpus, we created our own answerkey.
For all of the lexicon entries hypothesized byBasilisk, a human annotator (not any of the authors)4ANIMAL was chosen because many of the ProMed diseaseoutbreak stories concerned outbreaks among animal popula-tions.5The disease domain seed words were chosen from a largerset of ProMed documents, which included the 2000 used forlexicon induction.22BUILDING HUMAN LOCATION WEAPONN Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx25 .40 .56 .52 .56 .40 .72 .80 .84 .68 .88 .88 1.0 .56 .84 1.0 1.050 .44 .56 .46 .40 .56 .80 .88 .86 .80 .86 .84 .98 .52 .74 .76 .9075 .44 .45 .41 .39 .65 .84 .85 .85 .80 .88 .80 .99 .52 .63 .65 .79100 .42 .41 .38 .36 .69 .81 .80 .87 .81 .85 .78 .95 .55 .55 .56 .63300 .22 .82 .75 .26ANIMAL DISEASE SYMPTOMN Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx25 .48 .88 .92 .92 .64 .84 .80 .84 .64 .84 .92 .8050 .58 .82 .84 .80 .72 .84 .60 .82 .62 .76 .90 .7475 .55 .68 .67 .69 .69 .83 .59 .81 .61 .68 .79 .71100 .45 .55 .54 .57 .69 .78 .58 .80 .59 .71 .77 .64300 .20 .62 .38Table 2: Ranking results for 7 semantic categories, showing accuracies for the top-ranked N words.
(Ba=Basilisk, Hy=Hypernym Re-ranking, Av=Average of Seeds Re-ranking, Mx=Max of Seeds Re-rankinglabeled each word as either correct or incorrect forthe hypothesized semantic class.
A word is consid-ered to be correct if any sense of the word is seman-tically correct.4.2 Ranking ResultsWe ran Basilisk for 60 iterations, learning 5 newwords in each bootstrapping cycle, which produceda lexicon of 300 words for each semantic category.The columns labeled Ba in Table 2 show the accu-racy results for Basilisk.6 As we explained in Sec-tion 3.1, accuracy tends to decrease as bootstrappingprogresses, so we computed accuracy scores for thetop-ranked 100 words, in increments of 25, and alsofor the entire lexicon of 300 words.Overall, we see that Basilisk learns many cor-rect words for each semantic category, and the top-ranked terms are generally more accurate than thelower-ranked terms.
For the top 100 words, accu-racies are generally in the 50-70% range, except forLOCATION which achieves about 80% accuracy.
Forthe HUMAN category, Basilisk obtained 82% accu-racy over all 300 words, but the top-ranked wordsactually produced lower accuracy.Basilisk?s ranking is clearly not as good as it couldbe because there are correct terms co-mingled withincorrect terms throughout the ranked lists.
This has6These results are not comparable to the Basilisk results re-ported by (Thelen and Riloff, 2002) because our implementa-tion only does single-category learning while the results in thatpaper are based on simultaneously learning multiple categories.two ramifications.
First, if we want a human to man-ually review each lexicon before adding the wordsto an external resource, then the rankings may notbe very helpful (i.e., the human will need to reviewall of the words), and (2) incorrect terms generatedduring the early stages of bootstrapping may be hin-dering the learning process because they introducenoise during bootstrapping.
The HUMAN categoryseems to have recovered from early mistakes, butthe lower accuracies for some other categories maybe the result of this problem.
The purpose of ourWeb-based corroboration process is to automaticallyre-evaluate the lexicons produced by Basilisk, usingWeb-based statistics to create more separation be-tween the good entries and the bad ones.Our first set of experiments uses the Web-basedco-occurrence statistics to re-rank the lexicon en-tries.
The Hy, Av, and Mx columns in Ta-ble 2 show the re-ranking results using each of theHypernym, Average of Seeds, and Maximum ofSeeds scoring functions.
In all cases, Web-basedre-ranking outperforms Basilisk?s original rank-ings.
Every semantic category except for BUILDINGyielded accuracies of 80-100% among the top can-didates.
For each row, the highest accuracy for eachsemantic category is shown in boldface (as are anytied for highest).Overall, the Max of Seeds Scores were best, per-forming better than or as well as the other scoringfunctions on 5 of the 7 categories.
It was only out-23BUILDING HUMAN LOCATION WEAPON ANIMAL DISEASE SYMPTOMconsulate guerrilla San Salvador shotguns bird-to-bird meningo-encephalitis nauseapharmacies extremists Las Hojas carbines cervids bse).austria diarrhoeaaiport sympathizers Tejutepeque armaments goats inhalational myalgiaszacamil assassins Ayutuxtepeque revolvers ewes anthrax disease chlorosisairports patrols Copinol detonators ruminants otitis media myalgiaparishes militiamen Cuscatancingo pistols swine airport malaria salivationMasariegos battalion Jiboa car bombs calf taeniorhynchus dysenterychancery Ellacuria Chinameca calibers lambs hyopneumonia crampingresidences rebel Zacamil M-16 wolsington monkeypox dizzinesspolice station policemen Chalantenango grenades piglets kala-azar inappetanceTable 3: Top 10 words ranked by Max of Seeds Scores.performed once by the Hypernym Scores (BUILD-ING) and once by the Average of Seeds Scores(SYMPTOM).The strong performance of the Max of Seedsscores suggests that one seed is often an especiallygood collocation indicator for category membership?
though it may not be the same seed word for all ofthe lexicon words.
The relatively poor performanceof the Average of Seeds scores may be attributableto the same principle; perhaps even if one seed isespecially strong, averaging over the less-effectiveseeds?
scores dilutes the results.
Averaging is alsosusceptible to damage from words that receive thespecial-case score of -99999 when a hit count is zero(see Section 3.2).Table 3 shows the 10 top-ranked candidates foreach semantic category based on the Max of Seedsscores.
The table illustrates that this scoring func-tion does a good job of identifying semantically cor-rect words, although of course there are some mis-takes.
Mistakes can happen due to parsing errors(e.g., bird-to-bird is an adjective and not a noun, asin bird-to-bird transmission), and some are due toissues associated with Web querying.
For exam-ple, the nonsense term ?bse).austria?
was rankedhighly because Altavista split this term into 2 sep-arate words because of the punctuation, and bse byitself is indeed a disease term (bovine spongiformencephalitis).4.3 Filtering ResultsTable 2 revealed that the 300-word lexicons pro-duced by Basilisk vary widely in the number of truecategory words that they contain.
The least densecategory is ANIMAL, with only 61 correct words,and the most dense is HUMAN with 247 correctwords.
Interestingly, the densest categories are notalways the easiest to rank.
For example, the HU-MAN category is the densest category but Basilisk?sranking of the human terms was poor.?
Category Acc Cor/Tot-22WEAPON .88 46/52LOCATION .98 59/60HUMAN .80 8/10BUILDING .83 5/6ANIMAL .91 30/33DISEASE .82 64/78SYMPTOM .65 64/99-23WEAPON .79 59/75LOCATION .96 82/85HUMAN .85 23/27BUILDING .71 12/17ANIMAL .87 40/46DISEASE .78 82/105SYMPTOM .62 86/139-24WEAPON .63 63/100LOCATION .93 111/120HUMAN .87 54/62BUILDING .45 17/38ANIMAL .75 47/63DISEASE .74 94/127SYMPTOM .60 100/166Table 4: Filtering results using the Max of Seeds Scores.The ultimate goal behind a better ranking mech-anism is to completely automate the process of se-mantic lexicon induction.
If we can produce high-quality rankings, then we can discard the lowerranked words and keep only the highest rankedwords for our semantic dictionary.
However, this24presupposes that we know where to draw the line be-tween the good and bad entries, and Table 2 showsthat this boundary varies across categories.
For HU-MANS, the top 100 words are 87% accurate, and infact we get 82% accuracy over all 300 words.
Butfor ANIMALS we achieve 80% accuracy only for thetop 50 words.
It is paramount for semantic dictio-naries to have high integrity, so accuracy must behigh if we want to use the resulting lexicons withoutmanual review.As an alternative to ranking, another way that wecould use the Web-based corroboration statistics isto automatically filter words that do not receive ahigh score.
The key question is whether the valuesof the scores are consistent enough across categoriesto set a single threshold that will work well acrossthe different categories.Table 4 shows the results of using the Max ofSeeds Scores as a filtering mechanism: given athreshold ?, all words that have a score < ?
are dis-carded.
For each threshold value ?
and semantic cat-egory, we computed the accuracy (Acc) of the lex-icon after all words with a score < ?
have been re-moved.
The Cor/Tot column shows the number ofcorrect category members and the number of totalwords that passed the threshold.We experimented with a variety of threshold val-ues and found that ?=-22 performed best.
Table 4shows that this threshold produces a relatively high-precision filtering mechanism, with 6 of the 7 cat-egories achieving lexicon accuracies ?
80%.
Asexpected, the Cor/Tot column shows that the num-ber of words varies widely across categories.
Au-tomatic filtering represents a trade-off: a relativelyhigh-precision lexicon can be created, but some cor-rect words will be lost.
The threshold can be ad-justed to increase the number of learned words, butwith a corresponding drop in precision.
Dependingupon a user?s needs, a high threshold may be desir-able to identify only the most confident lexicon en-tries, or a lower threshold may be desirable to retainmost of the correct entries while reliably removingsome of the incorrect ones.5 ConclusionsWe have demonstrated that co-occurrence statis-tics gathered from the Web can dramatically im-prove the ranking of lexicon entries produced bya weakly-supervised corpus-based bootstrapping al-gorithm, without requiring any additional supervi-sion.
We found that computing Web-based co-occurrence statistics across a set of seed words andthen using the highest score was the most success-ful approach.
Co-occurrence with a hypernym termalso performed well for some categories, and couldbe easily combined with the Max of Seeds approachby choosing the highest value among the seeds aswell as the hypernym.In future work, we would like to incorporate thisWeb-based re-ranking procedure into the bootstrap-ping algorithm itself to dynamically ?clean up?
thelearned words before they are cycled back into thebootstrapping process.
Basilisk could consult theWeb-based statistics to select the best 5 words togenerate before the next bootstrapping cycle begins.This integrated approach has the potential to sub-stantially improve Basilisk?s performance becauseit would improve the precision of the induced lex-icon entries during the earliest stages of bootstrap-ping when the learning process is most fragile.AcknowledgmentsMany thanks to Julia James for annotating the goldstandards for the disease domain.
This research wassupported in part by Department of Homeland Secu-rity Grant N0014-07-1-0152.ReferencesJ.
Atserias, S. Climent, X. Farreres, G. Rigau, and H. Ro-driguez.
1997.
Combining Multiple Methods for theAutomatic Construction of Multilingual WordNets.
InProceedings of the International Conference on RecentAdvances in Natural Language Processing.S.
Caraballo.
1999.
Automatic Acquisition of aHypernym-Labeled Noun Hierarchy from Text.
InProc.
of the 37th Annual Meeting of the Associationfor Computational Linguistics, pages 120?126.P.
Cimiano and J. Volker.
2005.
Towards large-scale,open-domain and ontology-based named entity classi-fication.
In Proc.
of Recent Advances in Natural Lan-guage Processing, pages 166?172.D.
Davidov and A. Rappoport.
2006.
Efficient unsu-pervised discovery of word categories using symmet-ric patterns and high frequency words.
In Proc.
of the21st International Conference on Computational Lin-guistics and the 44th annual meeting of the ACL.25D.
Davidov, A. Rappoport, and M. Koppel.
2007.
Fullyunsupervised discovery of concept-specific relation-ships by web mining.
In Proc.
of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 232?239, June.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: an experimental study.
Artificial Intelligence,165(1):91?134, June.Z.
Harris.
1954.
Distributional Structure.
In J.
A. Fodorand J. J. Katz, editor, The Structure of Language, pages33?49.
Prentice-Hall.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of the 14th In-ternational Conference on Computational Linguistics(COLING-92).Z.
Kozareva, E. Riloff, and E. Hovy.
2008.
SemanticClass Learning from the Web with Hyponym PatternLinkage Graphs.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-08).D.
Lin and P. Pantel.
2002.
Concept discovery from text.In Proc.
of the 19th International Conference on Com-putational linguistics, pages 1?7.D.
Lin.
1998.
Dependency-based Evaluation of MINI-PAR.
In Workshop on the Evaluation of Parsing Sys-tems, Granada, Spain.G.
Mann.
2002.
Fine-grained proper noun ontologies forquestion answering.
In Proc.
of the 19th InternationalConference on Computational Linguistics, pages 1?7.G.
Miller.
1990.
Wordnet: An On-line Lexical Database.International Journal of Lexicography, 3(4).MUC-4 Proceedings.
1992.
Proceedings of the FourthMessage Understanding Conference (MUC-4).
Mor-gan Kaufmann.M.
Pas?ca.
2004.
Acquisition of categorized named en-tities for web search.
In Proc.
of the Thirteenth ACMInternational Conference on Information and Knowl-edge Management, pages 137?145.P.
Pantel and D. Ravichandran.
2004.
Automaticallylabeling semantic classes.
In Proc.
of Conference ofHLT / North American Chapter of the Association forComputational Linguistics, pages 321?328.P.
Pantel, D. Ravichandran, and E. Hovy.
2004.
To-wards terascale knowledge acquisition.
In Proc.
of the20th international conference on Computational Lin-guistics, page 771.M.
Pasca.
2007. weakly-supervised Discovery of NamedEntities using Web Search Queries.
In CIKM, pages683?690.W.
Phillips and E. Riloff.
2002.
Exploiting Strong Syn-tactic Heuristics and Co-Training to Learn SemanticLexicons.
In Proceedings of the 2002 Conference onEmpirical Methods in Natural Language Processing,pages 125?132.ProMed-mail.
2006. http://www.promedmail.org/.PubMed.
2009. http://www.ncbi.nlm.nih.gov/sites/entrez.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117?124.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044?1049.
The AAAI Press/MIT Press.B.
Roark and E. Charniak.
1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic SemanticLexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics, pages 1110?1116.Sofia Stamou, Kemal Oflazer, Karel Pala, Dimitris Chris-toudoulakis, Dan Cristea, Dan Tufis, Svetla Koeva,George Totkov, Dominique Dutoit, and Maria Grigo-riadou.
2002.
Balkanet: A multilingual semantic net-work for the balkan languages.
In Proceedings of the1st Global WordNet Association conference.H.
Tanev and B. Magnini.
2006.
Weakly supervised ap-proaches for ontology population.
In Proc.
of 11stConference of the European Chapter of the Associa-tion for Computational Linguistics.M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pattern Contexts.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Process-ing, pages 214?221.Peter D. Turney.
2001.
Mining the Web for Syn-onyms: PMI-IR versus LSA on TOEFL.
In EMCL?01: Proceedings of the 12th European Conferenceon Machine Learning, pages 491?502, London, UK.Springer-Verlag.P.
D. Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), pages 417?424.Piek Vossen, editor.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks.
KluwerAcademic Publishers, Norwell, MA, USA.D.
Widdows and B. Dorow.
2002.
A graph model forunsupervised lexical acquisition.
In Proc.
of the 19thInternational Conference on Computational Linguis-tics, pages 1?7.26
