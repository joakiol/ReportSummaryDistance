HYPERBUG: A Scalable Natural Language Generation ApproachMartin KlarnerUniversity Erlangen-Nurembergklarner@cs.fau.deAbstractA scalable natural language generation (NLG)system called HYPERBUG 1  embedded in anagent-based, multimodal dialog system is pre-sented.
To motivate this presentation, severalscenarios (including a domain shift) are iden-tified where scalability in dialog systems isreally needed, and NLG is argued to be oneway of easing this desired scalability.
There-fore the novel approach to hybrid NLG in theHYPERBUG system is described and the scal-ability of its parts and resources is investi-gated.
Concluding with a few remarks todiscourse generation, we argue that NLG canboth contribute to and benefit from scalabilityin dialog systems.1 IntroductionScalability in dialog systems is, of course, not only amatter of the natural language understanding (NLU)component, but also of the NLG part of the system.2 Wenevertheless see a lot of the effort spent in designingand implementing spoken dialog systems go into analy-sis of speech and language; generation is often left asideor squeezed in afterwards.
Therefore an NLG compo-nent must fit into the existing dialog system frameworkand answer to the preset priorities in dialog system de-velopment.We present a scalable NLG system embedded in anagent-based, multimodal dialog system.
To this end, we1 The acronym stands for hybrid, pragmatically embeddedrealization with bottom-up generation.2 In fact, for a working spoken dialog system even morecomponents have to be scalable, including the speech recog-nizer, the speech synthesizer and, most important of all, thedialog manager.
But for now we concentrate on the opposi-tion of NLU and NLG.first address common scenarios for scalability in dialogsystems and describe the role of NLG in such systemsbefore focusing on a classification of NLG systems ingeneral and our hybrid linguistic realization approach inparticular.
After giving an overview of our NLG systemwe will be able to show how our notion of reversibilitywith respect to internal and external resources and theinteraction and combination of shallow and deep NLGin HYPERBUG work towards and profit from scalabilityin our dialog system.2 Scalability in Dialog SystemsScalability for spoken dialog systems is needed in sev-eral situations, including the following scenarios:1.
Enlarging the domain content modifies and extendsthe thematic orientation of the domain.2.
Refining the domain language extends the linguis-tic coverage and expressibility of the domain.3.
Changing the application domain refers to usuallyboth of the first two and can lead to completely newrequirements for a dialog system and its parts.4.
Changing the discourse domain alters the dis-course type within the same domain.The common consequence of these four scenarios istheir impact on both NLU and NLG: If scalability is notbiased between these two parts, one cannot expect thesystem to be scalable as a whole.
Especially in the situa-tion of a domain shift, the degree of automated knowl-edge acquisition is an important issue to minimizecostly (both in terms of time and money) manual efforts.If these are unavoidable for the NLU component, as itwill often be the case in a real-world scenario, at leastthe NLG module must automatically benefit from them.3 NLG in Dialog SystemsNLG itself and for its own can be seen as a way to im-prove the scalability of a dialog system.
(Reiter, 1995)analyze the costs and benefits of NLG in comparison toother technologies and approaches, such as graphics,mail merging and human authoring, and argue for a hy-brid approach of combining NLG and templates in theIDAS system.
Generally speaking, the application ofNLG techniques in a real-world system must be justifiedwith respect to linguistic and economic requirements.If templates are used, a smart approach is needed in allscenarios mentioned in section 2 to avoid being forcedto completely redesign at least the data part of the sys-tem output component.
Nevertheless, fielded dialogsystems often settle for a shallow generation modulewhich relies on canned text and templates because oflimited financial means and linguistic knowledge.3.1 NLG and Dialog ManagementIn our spoken dialog system (B?cher et al, 2001), thedialog manager (DM) is responsible for the integrationof user utterances into the discourse context.
Moreover,the DM initiates system answers to user?s questions andsystem error messages if a user?s goal is unsatisfiable ora system task cannot be fulfilled.
But these system ut-terances must be verbalized as well, i.e.
translated fromabstract semantic representations to natural languagesentences.
This task is not placed within the DM, but?outsourced?
to a component with adequate linguisticcompetence, the NLG module.
This way, the DM can bedesigned completely amodal, i.e.
it does not need tohave any linguistic knowledge.
Moreover, we can seethat scenario 4 in section 2 can be separated into a lin-guistic and a dialog part.
The first part corresponds toscenario 2, the second is covered by the DM in our sys-tem; hence we may skip scenario 4 for the remainder ofthis paper.3.2 Reversibility in dialog systemsGiven a spoken dialog system in general and an NLGcomponent in such a system in particular, we considerreversibility a central means to allow for scalability.Considering reversibility, we want to introduce two dis-tinctions: We discriminate between reversibility of al-gorithms and reversibility of data on the one hand andbetween static (at developing time or at compile time)and dynamic reversibility (at runtime) on the otherhand.
In this terminology, reversibility of data means re-using existing system resources by the NLG component.We can classify NLG resources into two groups: Thelanguage analysis part contains the (syntactic and se-mantic) lexicon, the morphology component, and thegrammar, while the DM part comprises the discoursememory, the domain model, and the user model.33 For a different classification of knowledge resources fortext planning, see (Maier, 1999).4 Scalability in tactical generationWe focus on a special part of generation, the tacticalgeneration or linguistic realization, according to theclassification in (Reiter and Dale, 2000)4.
We are able todo so mainly because, as mentioned in 3.1, the DM isresponsible for content determination in our system.This leaves the realization task for the NLG component(besides some microplanning, which has to be per-formed as well).4.1 A taxonomy of existing systemsIn this section we will classify existing tactical genera-tion systems into three groups and address problemswith scalability in each one of them.Shallow generation systems form the first group; e.g.COMRIS (Geldof, 2000).
The approach taken thererelies on canned text and templates, and the domaindependency of these constructs is inherent.
Therefore, inscenario 3 of section Fehler!
Verweisquelle konntenicht gefunden werden., once a domain shift is pro-jected, the whole data part of the NLG component mustbe redesigned from scratch.
In scenarios 1 and 2 theexisting resources must be extended only, but even thiscan become a hard task if the existing template databaseis large enough.Deep generation systems make up the second group, e.g.
KPML (Bateman 1997); they often suffer from largeovergenerating grammars and slow processing time.Also often well-founded linguistic knowledge is re-quired to create and maintain the grammars needed.Their problems with scalability arise primarily in sce-narios 1 and 2, when thematic or linguistic coveragemust be increased.The third group, ?modern?
generation5  systems ide-ally avoid the shortcomings of both of the above men-tioned classical approaches.
We distinguish betweenthree types here: NLG with XSLT (Wilcock, 2003),which is basically template-based generation from XMLinput; stochastic approaches like (Oh and Rudnicky,2000), where the deep generation grammar is replacedby a stochastic language model, and hybrid generationapproaches like D2S (Theune et al, 2000), whichbridges the gap between NLG and speech synthesis by aprosody module.4 Dale and Reiter distinguish between linguistic and struc-ture realization, the former corresponding to the content andthe latter to the structural part of tactical generation.
We findthis distinction somewhat artificial, because the content mustbe already determined for realization, but want to use itanyway to further clarify the task carried out by our system.5 For these systems, the term ?hybrid?
is normally used inthe literature, but we want to spare it for hybridization be-tween shallow and deep generation; see 4.2.4.2 Hybrid tactical generationIn our terminology, hybrid generation means the combi-nation of shallow and deep generation in a single sys-tem; therefore, hybrid systems are a special case of the?modern?
approaches, which were mentioned in thepreceding section and do not necessarily contain any ofthe two ?classical?
approaches.
For practical needs, wefocus on how to combine deep (grammar-based) andshallow (template-based) generation techniques underthe term hybrid NLG.
We distinguish three types ofsuch hybrid NLG systems: Type I: Shallow NLG with deep elements Type II: Deep NLG with shallow elements Type III: Concurring deep and shallow NLGHere are two examples of existing systems to illustratethe classification just given: D2S fills slots in syntactictemplates containing derivation trees and therefore canbe classified as a type I system.
(Wahlster, 2000) hasseparate shallow and deep generation modules resultingin a system of type III.65 The HYPERBUG Approach5.1 System core functionalityOur approach to realization is to combine all three typesof hybrid tactical generation mentioned in section 4.2 ina single system called HYPERBUG.
The goals for its de-sign and implementation were:6 Type II was, though theoretically sound and possible, dif-ficult to find in existing systems.1.
To re-use existing system resources originally de-signed for parsing2.
To generate templates at runtime rather than meresentences3.
To dynamically reduce the workload on deep gen-eration and gradually let shallow generation takeover4.
To learn the domain-dependent part of the tacticalgeneration task while the dialog is running and, ul-timately, enable automatic adaptation to domain-shiftsFigure 1 shows the system core of HYPERBUG.
As ashallow generation component, we implemented a pow-erful template engine with recursion and embeddeddeep generation parts, including a lexicon, a morphol-ogy component, inflection, and constituent aggregation,resulting in a system of type I in our classification.For the deep generation branch, we decided to settle fora combination of a microplanning and a realizationcomponent: The first module, a ?sentence planner?, inessence converts the input discourse representationstructure (DRS, Kamp and Reyle, 1993) which is pro-vided by the DM into a different semantic representa-tion, the extended logical form (ELF) 7 .
This ELFstructure serves as input for the second module, a modi-fied and improved version of bottom-up generation(BUG, van Noord, 1990) in Java, using a unification-based grammar with feature structures.
We also incor-porated elements of shallow generation in our version ofBUG: Surface text parts, e.g.
proper nouns, may occurin the semantic input structure, the ELF.
The precom-7  The extensions allowed (as compared to a conventionalLF) include syntactic functions like tense and mode, topical-ization information and subordination clause type.Figure 1: System core of HYPERBUGHYPERBUG       offflfiffi!
"#	%$ &' (*)+,(-.
/01	-243/ 5-67	98:;&<6%2shallow generation>=>= !ff?fiffi@= !Legendserves as resource for transmits data toA8	-)67	%)fl$B?/ 5-))	-A<C D-E%Fo GHJI!Bfl5*6K6%J)+,5-67$Lfl )01	-243/ 5%6"#5%675-M5-&<8:;)675-$67 $N;	%5-/  O<5-67 (-)P ffRQCSKfiT K!UWVKXXflY>Z<[\[,Z'Ydeep generationW]_^DRSsurfacestructurepiled exception lexicon is searched first, whenever aproper noun occurs in the LF8.
Thus, we get a type IIsystem.Links between shallow and deep generation are pro-vided in two ways: At first, a decision module analyzesthe input and invokes the appropriate realization branch,making HYPERBUG a system of type III.
Stage 1 of thisdecision module is to use shallow generation as defaultand deep generation only as a fallback strategy.
Stage 2uses keyword spotting techniques in the input XMLstructure with XPATH and a lookup in an index tablecontaining references to available canned text and tem-plates.
For stage 3, a planning procedure makes use ofthe speech act, the discourse situation and the usermodel to ensure that the most appropriate processingbranch is selected; in extension of the approach for text8 Proper nouns are indicated simply by capitalizing them.planning presented in (Stent, 2001), we have appliedConversation Acts Theory (Poesio, 1994) to linguisticrealization here.Not only do we combine all three types of hybrid reali-zation in our system, but we also interleave shallow anddeep generation in another way:  At last, after the com-plete utterance has been generated, a ?bridge?
betweenshallow and deep generation implements a feedbackloop from BUG to the template system.
This type of?bootstrapping?
9  is mainly responsible for the novelapproach taken in HYPERBUG.Figure 1 depicts, besides the system core, also the firsttwo parts of the ?bootstrapping?
procedure developed inHYPERBUG.9 We use the term ?bootstrapping?
mainly as an analogy toclassical bootstrapping procedures, hence the quotationmarks.HYPERBUG 	     fffiflffiffi ffi ! " #!ff%$fifl#!&'(  " #!ff)*,+ -.
  .
 /102 -.34567389::7 0	+ 0*6+ 0	+525;-*,+ -=<> 8?A@CBED9FGEHI@CJ1K9LM<N4- 	O3:80:<PK9Q9FR@CG- +;+ 0+64 454- + S6+4 TVUWAW,X:YX:Z[X\ U	]=^_ U:`a bAbNcedVf9gihjdekmlAnNoIkCpqgid lqril>ktsucVvxwCyzvshallow generation{%d lAyz|Ade}Ch	def~ril>ktsucVvxwCyzvoIde}Cfmk|C9}1v?rxl?kisucVvxwCyzv??????
)%?C?1? "
( ?v??
?h	v?l>gic?v?l>v?9gih	dek9l?
?ktw|AcVvLegendserves as resource for transmits data to corresponds to(1) KQML + DRS(3a)sentence+ wave files(2b)DRS(2a)DRS(2c)ELF(3b)sentence???j???
?=??
?\ ?j?
??
U:???
:U	?
?mT?TU	?_ ^?j]M<z4- 	*,?
0?=+;+ 0+6?=- + ?
+?#!  #!&??
V'ffi ff " #!ff)??TV???
?X?WAW1X	?deep generationFigure 2: System resources of HYPERBUG(I) BUG passes the generated derivation tree to thebridge, where it is converted into a template.
(II) The bridge sends the template to the template sys-tem where it is stored for further use and to the decisionmodule where a reference is saved in the index table.This way, the next dialog turn with similar semanticinput can be realized faster using a template withouthaving to invoke the deep generation branch again.5.2 Resources and reversibilityExtending our description from the core functionalityexplained in the previous section, we now turn our at-tention to the resources used in HYPERBUG.
Figure 2gives an overview of the internal and external resourcesof HYPERBUG and their usage.The linguistic knowledge is contained in the systemlexicon and the morphology component.
The former canbe distinguished further into a valency lexicon contain-ing case frames and a lemmata lexicon for domain ex-ceptions10.
These exceptions are compiled into the NLGlexicon used by both the shallow and deep generationbranch, resulting in a static reversibility of data.
Themorphology component is separated from the core lexi-con and implemented as a server which can both ana-lyze and generate and is consequently used for parsingand generation.
As a server with a uniform query inter-face, from the outside it looks like it were algorithmi-cally reversible, but internally the algorithms for parsingand generation are implemented differently, resulting ina dynamic reversibility of data, because identical dataare used and the processing direction is decided at run-time in the query sent to the server.A separated module called context manager (CM) isresponsible for sentence-spanning processing: It per-forms macro-aggregation11 and completion of underspe-cified utterances from the DM.
For this task, access tothe dialog memory is required, which is performed indi-rectly via a DM request12.The templates are stored in a database which corre-sponds to the grammar in the deep generation branch.Unfortunately, the chunk grammar used for analysis isinsufficient for generation13, resulting in a lack of evenstatic reversibility: In this case, the requirements forrobust parsing differ too much from the ones for vari-able NLG.10 The domain independent lexicon entries are derived fromWORDNET synsets.11 By this term we mean aggregation on the sentence level,as opposed to micro-aggregation which occurs between con-stituents.12 In the current implementation of our system, the dialogmemory is only accessible this way.13 The chunk grammar does, of course, not contain phraserules up to the sentence level.The domain model is divided into a linguistic and anapplication-specific part.
Only the linguistic part is usedin HYPERBUG, mainly for substitution of synonyms,hyperonyms and hyponyms to enlarge variability inlexical choice, a part of microplanning also handled byour approach to NLG.The processing steps in the system are as follows:(1) The dialog manager sends to HYPERBUG a KQML14message with the semantic representation of the systemutterance to be realized (a DRS encoded in an XMLstructure), enriched with pragmatic information such assender, KQML speech act, and pragmatic dialog act.
(2) The decision module determines, based on linguisticand pragmatic information (such as the speech act andthe user model), whether shallow or deep generation is(more) appropriate to process the message and feeds it(a) to the template system or (b) to the deep generationbranch (or to both of them, as a fall-back strategy).
(3 a) The template system passes the generated surfacestructure with additional information 15  to the speechsynthesis agent16, including wave files of utterance partswhich were already synthesized before.
(3 b) BUG passes the generated surface structure withadditional information to the speech synthesizer.There is also a third bootstrapping aspect depicted inFigure 2:(III) The synthesizer agent returns the synthesized wavefiles to the template system to enhance the stored tem-plates.
This way, the synthesizer does not have to pro-cess identical utterance parts more than once, thusincreasing the efficiency in synthesis and the real-time-capacity of the system.
175.3 Scalability in HYPERBUGAfter giving an overview of our NLG system, we willnow address scalability issues in its parts and resources.Template system.
Templates in HYPERBUG are at leastalgorithmically unproblematic: The pattern matchingalgorithm is linear complex with the number of entries.But normally, template systems are poorly maintainableand need to be rewritten from scratch after a domainshift.
We try to overcome this difficulty by isolating aconsiderably large domain-independent part, such asmetadialog (ambiguity, coherence state, and plan/action14 The language KQML is currently used for agent commu-nication in our system, but we are in the process of transitionto FIPA-ACL.15  sentence accent to influence prosody generation in thesynthesizer and deixis to synchronize textual output with theavatar in our multimodal system16 in essence a wrapper agent around the open-source syn-thesizer MBROLA17  System profiling has shown a considerable amount ofprocessing time of the generation component going into syn-thesis with MBROLA.state), greeting, and default messages18, avoiding muchof the effort needed in scenario 3.
Scenarios 1 and 2, onthe other hand, are treated in our template system byusing modularity via inclusion: The templates are recur-sive so that we can easily extend and refine the existingdatabase.
All in all, we can state that relatively few newentries are required for our template system in all threescenarios 1-3.Lexicon and morphology component.
The lexicon andthe morphology component are also algorithmicallyunproblematic, as they are linear complex with the num-ber of entries.
Furthermore, we were able to re-use alarge part of the existing system resources initially de-signed for parsing (i.e.
the exception lexicon and themorphology server).
We can summarize that, for thesetwo components, NLG automatically grows with NLU,and that no NLG-specific effort is required in the firstthree scenarios mentioned above.Grammar.
For the deep generation branch, the gram-mar can lead to algorithmic problems: The algorithmhas exponential complexity, but only in the number ofcategories within the rules.
However, this number isfinite with a low upper bound.
The algorithm has linearcomplexity in the number of words, just like the under-lying lexicon does.
Disjunctive unification can causeproblems, but not if it is restricted to simple features, asit is in our system.
Anyway, a large part of the rules canbe re-used in all three scenarios19, but a proper grammarorganization is required for the inevitable manual main-tenance.Hybrid approach.
The central argument for the scal-ability of HYPERBUG, however, lies in its special hybriddesign: The decision module before and the bridge afterthe two generation branches constitute the bootstrappingapproach which continually improves the system per-formance in terms of efficiency and linguistic coverageat runtime (useful for scenarios 1 and 2) and enablesautomatic adaptation to domain shifts (scenario 3).Speech synthesis.
HYPERBUG has a built-in feature ena-bling intrasentential multilingual speech synthesis 20 ,rendering the system scalable in terms of languagechanges.
The second aspect of scalability within speechsynthesis is the other ?bridge?
between this externalmodule and the template system which gradually im-proves the system response time in all three scenarios.Pragmatic resources.
The pragmatic resources com-prise the dialog memory, the domain model and the usermodel.
For the domain model, it is possible that new18 i.e.
ok and error messages, the latter tending to be ratherdomain-specific, though19 This is, of course, the inherent advantage of deep overshallow NLG.20  Basically, lexical information about the language of aproper noun (e.g.
a person?s name) is included in the outputto the synthesizer which uses this information to switch be-tween target languages, even within a single sentence.NLG-specific entries are required in the scenariosabove.
But these entries are not a critical factor in termsof scalability.
The complexity of the discourse memorymainly depends on the dialog length.
This is a largelydomain-independent factor and not affected by our sce-narios.
All we can say about the user model is that as aprimarily non-linguistic resource it is not in the focus ofNLG.
If it needs to be enriched or refined in scenario 1and 2 or even redefined in scenario 3, its usage in ourNLG system retains its complexity.We conclude that the external pragmatic resources canbe extended and re-used without any impact onHYPERBUG and do therefore not influence the scalabilityof our NLG component.Discourse generation.
Finally, we want to briefly ad-dress some aspects of discourse generation as a way ofscalability in terms of linguistic expressibility.
Deicticexpressions are currently hard-coded in special tem-plates, because they are highly domain-dependent.
Inour multimodal system, they must be synchronized withthe other output modalities, such as the avatar perform-ing deictic gestures.
The expected place for anaphorageneration in our system is the CM.
As a pragmatic re-source, the dialog memory is used for this task via theDM.
Pronominal references are enabled by the CMwhich checks for appropriate discourse referents to bepronominalized; they are executed by the sentenceplanner which substitutes nouns by matching pronounsin the LF.
The conditions for appropriate (i.e.
unambi-guous) anaphora and replacements of nouns by pro-nouns are not easy to meet and check.
Our current ideainvolves a generate-and-test approach, i.e.
we want totentatively generate an anaphor or a pronoun and use theanalysis part of our dialog system to determine whetherthey are ambiguous or not.6 Related WorkGenerally, the method known as explanation-based gen-eralization from machine learning is comparable to thebootstrapping approach described here; but normallylearning is achieved by offline training.In (Neumann, 1997) a training phase with an appropri-ate corpus is needed, while we perform generation atruntime without such a corpus.
Furthermore, Neumannextracts complex subgrammars; we generate annotatedsurface sentences instead, which are less expressible,but faster to instantiate.
And finally, Neumann performsa static template choice as opposed to our runtime deci-sion module which can opt for deep generation based onpragmatic constraints, even if a semantically appropriatetemplate is already available.(Corston-Oliver.
2002) has a machine learning approachfor realization similar to (Neumann, 1997): Transforma-tion rules are learned offline from examples in a corpus.Again, a separate training phase is needed beforehand.
(Scott, 1998) can be seen as offline interface generationusing a GUI and therefore as a manual version of thebootstrapping approach described here, but her systemis used for content determination, not for realization.7 Conclusion and Further WorkWe have presented a hybrid NLG system that can bothcontribute to and benefit from the scalability in its em-bedding multimodal dialog system.
Various scenariosrequiring a scalable NLG system where identified andapplied to our system components and resources in or-der to analyze their scalability.A prototype of the system is implemented and used inseveral different domains, namely home A/V and caraudio management (B?cher, 2001), B2B e-procurement(Kie?ling, 2001), and model train controlling (Huberand Ludwig, 2002), but we need further evaluation ofthe requirements for a domain shift and of the user ac-ceptance to improve the quality of our output languageand speech.What remains to do on the implementation side?
Tech-nically, we still lack a fully implemented ?sentenceplanner?
with in-depth analysis of the semantic inputstructure (which is only processed in a shallow mannerby now), and a separation of pure canned text fromtemplates for efficiency.
Also, the interface to the lin-guistic part of the domain model which is represented indescription logics must be implemented using an appro-priate inference machine.
Conceptually, we want tobroaden the bridge between shallow and deep genera-tion, refine the specification of stage 3 in the decisionmodule, and work out a way to access the user modeldirectly (currently, it is accessed indirectly via the DM,just like the discourse memory).AcknowledgementsThe author wants to thank Bernd Ludwig and PeterReiss for fruitful discussions and interesting ideas con-tributing to the research reported in this paper.ReferencesJohn Bateman.
1997.
Enabling technology formultilingual natural language generation: the KPMLdevelopment environment.
Journ.
Natural LanguageEngineering 3 (1):15-55.Kerstin B?cher et al 2001.
Discourse and ApplicationModeling for Dialogue Systems.
Proc.
KI-2001Workshop on Applications of Description Logics.Simon Corston-Oliver.
2002.
An overview of Amalgam:A machine-learned generation module.
Proc.
Int.Natural Language Generation Conference, NewYork:33-40.Sabine Geldof.
2000.
Context-sensitivity in advisory textgeneration.
PhD Thesis, University of Antwerp.Alexander Huber and Bernd Ludwig.
2002.
A NaturalLanguage Multi-Agent System for Controlling ModelTrains.
Proc.
AI, Simulation, and Planning in HighAutonomy Systems (AIS-2002):145-149.Hans Kamp and Ulrich Reyle.
1993.
From Discourse ToLogic.
Kluwer, Boston/Dordrecht/London.Werner Kie?ling et al 2001.
Design and Implementa-tion of COSIMA - A Smart and Speaking E-Sales As-sistant.
Proc.
3rd International Workshop onAdvanced Issues of E-Commerce and Web-Based In-formation Systems (WECWIS '01):21-30.G?nther Neumann.
1997.
Applying Explanation-basedLearning to Control and Speeding-up Natural Lan-guage Generation.
Proc.
ACL/EACL-97.Gertjan van Noord.
1990.
An Overview of Head-DrivenBottom-Up Generation.
In: Robert Dale et al Cur-rent Research in Natural Language Generation.Springer, Berlin/Heidelberg/New York.Alice Oh and Alexander Rudnicky: Stochastic languagegeneration for spoken dialogue systems.
Proc.ANLP/NAACL 2000 Workshop on ConversationalSystems: 27-32.Ehud Reiter.
1995.
NLG vs. Templates.
Proc.
5th Euro-pean Workshop on Natural Language Generation(EWNLG-1995).Ehud Reiter and Robert Dale.
2000.
Bulding NaturalLanguage Generation Systems.
Cambridge Univer-sity Press, Cambridge, UK.Donia Scott et al 1998.
Generation as a Solution to itsOwn Problem.
Proc.
9th Int.
Workshop on NaturalLanguage Generation (INLG-98).Amanda J. Stent.
2001.
Dialogue Systems as Conver-sational Partners: Applying Conversation ActsTheory to Natural Language Generation for Task-Oriented Mixed-Initiative Spoken Dialogue.
PhDThesis, Rochester, NJ.Marie Theune et al 2000.
From Data to Speech: AGeneral Approach.
Natural Language Engineering7(1):47-86.Wolfgang Wahlster.
2000.
Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer, Berlin/Hei-delberg/New York.Graham Wilcock.
2003.
Generating Responses andExplanations from RDF/XML and DAML+OIL.
Proc.IJCAI-2003:58-63.
