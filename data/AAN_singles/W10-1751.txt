Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 339?342,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsMETEOR-NEXT and the METEOR Paraphrase Tables: ImprovedEvaluation Support for Five Target LanguagesMichael Denkowski and Alon LavieLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15232, USA{mdenkows,alavie}@cs.cmu.eduAbstractThis paper describes our submission tothe WMT10 Shared Evaluation Task andMetricsMATR10.
We present a versionof the METEOR-NEXT metric with para-phrase tables for five target languages.
Wedescribe the creation of these paraphrasetables and conduct a tuning experimentthat demonstrates consistent improvementacross all languages over baseline ver-sions of the metric without paraphrase re-sources.1 IntroductionWorkshops such as WMT (Callison-Burch et al,2009) and MetricsMATR (Przybocki et al, 2008)focus on the need for accurate automatic met-rics for evaluating the quality of machine transla-tion (MT) output.
While these workshops evalu-ate metric performance on many target languages,most metrics are limited to English due to the rel-ative lack of lexical resources for other languages.This paper describes a language-independentmethod for adding paraphrase support to theMETEOR-NEXT metric for all WMT10 target lan-guages.
Taking advantage of the large parallel cor-pora released for the translation tasks often accom-panying evaluation tasks, we automatically con-struct paraphrase tables using the pivot method(Bannard and Callison-Burch, 2005).
We use theWMT09 human evaluation data to tune versionsof METEOR-NEXT with and without paraphrasesand report significantly better performance for ver-sions with paraphrase support.2 The METEOR-NEXT MetricThe METEOR-NEXT metric (Denkowski andLavie, 2010) evaluates a machine translation hy-pothesis against a reference translation by calcu-lating a similarity score based on an alignment be-tween the two strings.
When multiple referencesare provided, the hypothesis is scored against eachand the reference producing the highest score isused.
Alignments are formed in two stages: searchspace construction and alignment selection.For a single hypothesis-reference pair, the spaceof possible alignments is constructed by identify-ing all possible word and phrase matches betweenthe strings according to the following matchers:Exact: Words are matched if and only if their sur-face forms are identical.Stem: Words are stemmed using a language-appropriate Snowball Stemmer (Porter, 2001) andmatched if the stems are identical.Synonym: Words are matched if they are bothmembers of a synonym set according to the Word-Net (Miller and Fellbaum, 2007) database.Paraphrase: Phrases are matched if they arelisted as paraphrases in a paraphrase table.
Thetables used are described in Section 3.Previously, full support has been limited to En-glish, with French, German, and Spanish havingexact and stem match support only, and Czechhaving exact match support only.Although the exact, stem, and synonym match-ers identify word matches while the paraphrasematcher identifies phrase matches, all matches canbe generalized to phrase matches with a start po-sition and phrase length in each string.
A wordoccurring less than length positions after a matchstart is considered covered by the match.
Ex-act, stem, and synonym matches always cover oneword in each string.Once the search space is constructed, the finalalignment is identified as the largest possible sub-set of all matches meeting the following criteria inorder of importance:1.
Each word in each sentence is covered byzero or one matches2.
Largest number of covered words across both339sentences3.
Smallest number of chunks, where a chunkis defined as a series of matched phrases thatis contiguous and identically ordered in bothsentences4.
Smallest sum of absolute distances betweenmatch start positions in the two sentences(prefer to align words and phrases that occurat similar positions in both sentences)Once an alignment is selected, the METEOR-NEXT score is calculated as follows.
The num-ber of words in the translation hypothesis (t) andreference (r) are counted.
For each of the match-ers (mi), count the number of words covered bymatches of this type in the hypothesis (mi(t)) andreference (mi(r)) and apply matcher weight (wi).The weighted Precision and Recall are then calcu-lated:P =?iwi ?mi(t)|t|R =?iwi ?mi(r)|r|The parameterized harmonic mean of P and R(van Rijsbergen, 1979) is then calculated:Fmean =P ?R?
?
P + (1?
?)
?RTo account for gaps and differences in word or-der, a fragmentation penalty (Lavie and Agar-wal, 2007) is calculated using the total number ofmatched words (m) and number of chunks (ch):Pen = ?
?
(chm)?The final METEOR-NEXT score is then calculated:Score = (1?
Pen) ?
FmeanThe parameters ?, ?, ?, and wi...wn can betuned to maximize correlation with various typesof human judgments.3 The METEOR Paraphrase TablesTo extend support for WMT10 target languages,we use released parallel corpora to construct para-phrase tables for English, Czech, German, Span-ish, and French.
These tables are used by theMETEOR-NEXT paraphrase matcher to identifyadditional phrase matches in each language.3.1 Paraphrasing with Parallel CorporaFollowing Bannard and Callison-Burch (2005),we extract paraphrases automatically from bilin-gual corpora using a pivot phrase method.
For agiven language pair, word alignment, phrase ex-traction, and phrase scoring are conducted on par-allel corpora to build a single bilingual phrase ta-ble for the language pair.
For each native phrase(n1) in the table, we identify each foreign phrase(f ) that translates n1.
Each alternate native phrase(n2 6= n1) that translates f is considered a para-phrase of n1 with probability P (f |n1) ?
P (n2|f).The total probability of n2 paraphrasing n1 isgiven as the sum over all f :P (n2|n1) =?fP (f |n1) ?
P (n2|f)The same method can be used to identify foreignparaphrases (f1, f2) given native pivot phrasesn.
To merge same-language paraphrases ex-tracted from different parallel corpora, we take themean of the corpus-specific paraphrase probabili-ties (PC) weighted by the size of the corpora (C)used for paraphrase extraction:P (n2|n1) =?C |C| ?
PC(n2|n1)?C |C|To improve paraphrase accuracy, we apply mul-tiple filtering techniques during paraphrase extrac-tion.
The following are applied to each paraphraseinstance (n1, f, n2):1.
Discard paraphrases with very low probabil-ity (P (f |n1) ?
P (n2|f) < 0.001)2.
Discard paraphrases for which n1, f , or n2contain any punctuation characters.3.
Discard paraphrases for which n1, f , orn2 contain only common words.
Commonwords are defined as having relative fre-quency of 0.001 or greater in the parallel cor-pus.Remaining phrase instances are summed to con-struct corpus-specific paraphrase tables.
Same-language paraphrase tables are selectively mergedas part of the tuning process described in Sec-tion 4.2.
Final paraphrase tables are further fil-tered to include only paraphrases with probabili-ties above a final threshold (0.01).340Language Pair Corpus Phrase TableTarget Source Sentences Phrase PairsEnglish Czech 7,321,950 128,326,269English German 1,630,132 84,035,599English Spanish 7,965,250 363,714,779English French 8,993,161 404,883,736German Spanish 1,305,650 70,992,157Table 1: Sizes of training corpora and phrase ta-bles used for paraphrase extractionLanguage Pivot Languages Phrase PairsEnglish German, Spanish, 6,236,236FrenchCzech English 756,113German English, Spanish 3,521,052Spanish English, German 6,352,690French English 3,382,847Table 2: Sizes of final paraphrase tables3.2 Available DataWe conduct paraphrase extraction using parallelcorpora released for the WMT10 Shared Trans-lation Task.
This includes Europarl corpora(French-English, Spanish-English, and German-English), news commentary (French-English,Spanish-English, German-English, and Czech-English), United Nations corpora (French-Englishand Spanish-English), and the CzEng (Bojar andZ?abokrtsky?, 2009) corpus sections 0-8 (Czech-English).
In addition, we use the German-SpanishEuroparl corpus released for WMT08 (Callison-Burch et al, 2008).3.3 Paraphrase Table ConstructionUsing all available data for each language pair,we create bilingual phrase tables for the follow-ing: French-English, Spanish-English, German-English, Czech-English, and German-Spanish.The full training corpora and resulting phrase ta-bles are described in Table 1.
For each phrase ta-ble, both foreign and native paraphrases are ex-tracted.
Same-language paraphrases are selec-tively merged as described in Section 4.2 to pro-duce the final paraphrase tables described in Ta-ble 2.
To keep table size reasonable, we only ex-tract paraphrases for phrases occurring in targetcorpora consisting of the pooled development datafrom the WMT08, WMT09, and WMT10 trans-lation tasks (10,158 sentences for Czech, 20,258sentences for all other languages).Target Systems Usable JudgmentsEnglish 45 20,357Czech 5 11,242German 11 6,563Spanish 9 3,249French 12 2,967Table 3: Human ranking judgment data fromWMT094 Tuning METEOR-NEXT4.1 Development DataAs part of the WMT10 Shared Evaluation Task,data from WMT09 (Callison-Burch et al, 2009),including system output, reference translations,and human judgments, is available for metric de-velopment.
As metrics are evaluated primarilyon their ability to rank system output on the seg-ment level, we select the human ranking judg-ments from WMT09 as our development set (de-scribed in Table 3).4.2 Tuning ProcedureTuning a version of METEOR-NEXT consists ofselecting parameters (?, ?, ?, wi...wn) that opti-mize an objective function for a given language.If multiple paraphrase tables exist for a language,tuning also requires selecting the optimal set of ta-bles to merge.For WMT10, we tune to rank consistency on theWMT09 data.
Following Callison-Burch et.
al(2009), we discard judgments where system out-puts are deemed equivalent and calculate the pro-portion of remaining judgments preserved whensystem outputs are ranked by automatic metricscores.
For each target language, tuning is con-ducted as an exhaustive grid search over metric pa-rameters and possible paraphrase tables, resultingin global optima for both.5 ExperimentsTo evaluate the impact of our paraphrase ta-bles on metric performance, we tune versions ofMETEOR-NEXT with and without the paraphrasematchers for each language.
For further compar-ison, we tune a version of METEOR-NEXT usingthe TERp English paraphrase table (Snover et al,2009) used by previous versions of the metric.As shown in Table 4, the addition of paraphrasesleads to a better tuning point for every target lan-guage.
The best scoring subset of paraphrase ta-341Language Paraphrases Rank Consistency ?
?
?
wexact wstem wsyn wparEnglish none 0.619 0.85 2.35 0.45 1.00 0.80 0.60 ?TERp 0.625 0.70 1.40 0.25 1.00 0.80 0.80 0.60de+es+fr 0.629 0.75 0.60 0.35 1.00 0.80 0.80 0.60Czech none 0.564 0.95 0.20 0.70 1.00 ?
?
?en 0.574 0.95 2.15 0.35 1.00 ?
?
0.40German none 0.550 0.20 0.75 0.25 1.00 0.80 ?
?en+es 0.576 0.75 0.80 0.90 1.00 0.20 ?
0.80Spanish none 0.586 0.95 0.55 0.90 1.00 0.80 ?
?en+de 0.608 0.15 0.25 0.75 1.00 0.80 ?
0.40French none 0.696 0.95 0.80 0.35 1.00 0.60 ?
?en 0.707 0.90 0.85 0.45 1.00 0.00 ?
0.60Table 4: Optimal METEOR-NEXT parameters with and without paraphrases for WMT10 target languagesbles for English also outperforms the TERp para-phrase table.Analysis of the phrase matches contributed bythe paraphrase matchers reveals an interestingpoint about the task of paraphrasing for MT eval-uation.
Despite filtering techniques, the final para-phrase tables include some unusual, inaccurate,or highly context-dependent paraphrases.
How-ever, the vast majority of matches identified be-tween actual system output and reference trans-lations correspond to valid paraphrases.
In manycases, the evaluation task itself acts as a final filter;to produce a phrase that can match a spurious para-phrase, not only must a MT system produce incor-rect output, but it must produce output that over-laps exactly with an obscure paraphrase of somephrase in the reference translation.
As systemsare far more likely to produce phrases with similarwords to those in reference translations, far morevalid paraphrases exist in typical system output.6 ConclusionsWe have presented versions of METEOR-NEXTand paraphrase tables for five target languages.Tuning experiments indicate consistent improve-ments across all languages over baseline versionsof the metric.
Created for MT evaluation, the ME-TEOR paraphrase tables can also be used for othertasks in MT and natural language processing.
Fur-ther, the techniques used to build the paraphrasetables are language-independent and can be usedto improve evaluation support for other target lan-guages.
METEOR-NEXT, the METEOR paraphrasetables, and the software used to generate para-phrases are released under an open source licenseand made available via the METEOR website.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proc.
ofACL05.Ondr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng0.9: Large Parallel Treebank with Rich Annotation.Prague Bulletin of Mathematical Linguistics.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProc.
of WMT08.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of WMT09.
InProc.
of WMT09.Michael Denkowski and Alon Lavie.
2010.
Extend-ing the METEOR Machine Translation Metric tothe Phrase Level for Improved Correlation with Hu-man Post-Editing Judgments.
In Proc.
NAACL/HLT2010.Alon Lavie and Abhaya Agarwal.
2007.
METEOR:An Automatic Metric for MT Evaluation with HighLevels of Correlation with Human Judgments.
InProc.
of WMT07.George Miller and Christiane Fellbaum.
2007.
Word-Net.
http://wordnet.princeton.edu/.Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/texts/.M.
Przybocki, K. Peterson, and S Bronsart.
2008.
Offi-cial results of the NIST 2008 ?Metrics for MAchineTRanslation?
Challenge (MetricsMATR08).Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, Adequacy, orHTER?
Exploring Different Human Judgments witha Tunable MT Metric.
In Proc.
of WMT09.C.
van Rijsbergen, 1979.
Information Retrieval, chap-ter 7.
2nd edition.342
