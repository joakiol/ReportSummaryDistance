Combining Semantic and Temporal Constraints for Multimodal Integra-tion in Conversation SystemsJoyce Y. ChaiDepartment of ComputerScience and EngineeringMichigan State UniversityEast Lansing, MI 48864jchai@cse.msu.eduPengyu HongDepartment of StatisticsHarvard UniversityCambridge, MA 02138hong@stat.harvard.eduMichelle X. ZhouIBM T. J. Watson ResearchCenter19 Skyline DriveHawthorne, NY 10532mzhou@us.ibm.comAbstractIn a multimodal conversation, user refer-ring patterns could be complex, involvingmultiple referring expressions fromspeech utterances and multiple gestures.To resolve those references, multimodalintegration based on semantic constraintsis insufficient.
In this paper, we describe agraph-based probabilistic approach thatsimultaneously combines both semanticand temporal constraints to achieve a highperformance.1IntroductionMultimodal conversation systems allow users toconverse with systems through multiple modalitiessuch as speech, gesture and gaze (Cohen et al,1996; Wahlster, 1998).
In such an environment,not only are more interaction modalities available,but also richer contexts are established during theinteraction.
Understanding user inputs, forexample, what users refer to is important.
Previouswork on multimodal reference resolution includesthe use of a focus space model (Neal et al, 1998),the centering framework (Zancanaro et al, 1997),context factors (Huls et al, 1995), and rules(Kehler 2000).
These previous approaches focuson semantics constraints without fully addressingtemporal constraints.
In a user study1, we foundthat the majority of user referring behaviorinvolved one referring expression and one gesture(as in [S2, G2] in Table 1).
The earlier approachesworked well for these types of references.However, we found that 14.1% of the inputs werecomplex, which involved multiple referringexpressions from speech utterances and multiplegestures (S3 in Table 1).
To resolve those complexreferences, we have to not only apply semanticconstraints, but also apply temporal constraints atthe same time.For example, Figure 1 shows three inputs wherethe number of referring expressions is the sameand the number of gestures is the same.
The speechutterances and gestures are aligned along the timeaxis.
The first case (Figure 1a) and the second case(Figure 1b) have the same speech utterance butdifferent temporal alignment between the gesturesand the speech input.
The second case and the thirdcase (Figure 1c) have a similar alignment, but thethird case provides an additional constraint on thenumber of referents (from the word ?two?
).Although all three cases are similar, but theobjects they refer to are quite different in eachcase.
In the first case, most likely ?this?
refers tothe house selected by the first point gesture and?these houses?
refers to two houses selected by theother two gestures.
In the second case, ?this?
mostlikely refers to the highlighted house on the displayand ?these houses?
refer to three houses selectedby the gestures.
In the third case, ?this?
most likelyrefers to the house selected by the first pointgesture and ?these two houses?
refers to twohouses selected by the other two point gestures.1561712415Total Num221561S3: mul.
expressions131211712S2: one expression3012S1: no expressionTotalNumG3mul.
gest.G2one gestG1no gest.Table 1: Referring patterns from the user study 1 We are developing a system that helps users find real estateproperties.
So here we use real estate as the testing domain.Gesture input: ??...?????.??.?.???????
(a)TimeSpeech input: Compare  this     with     these    houses.Gesture input: ??...??????????...????
(b)TimeSpeech input: Compare  this  with       these         houses.Gesture input: ??...????..
..??.?.??...????
(c)TimeSpeech input: Compare  this   with     these   two     houses.Figure 1.
Three multimodal inputs under the sameinteraction context.
The timings of the point gesturesare denoted by ??
?.Resolving these complex cases requiressimultaneously satisfying semantic constraintsfrom inputs and the interaction contexts, and thetemporal constraints between speech and gesture.2 Graph-based ApproachWe use a probabilistic approach based on attrib-uted relational graphs (ARGs) to combine semanticand temporal constraints for reference resolution.First, ARGs can adequately capture the semanticand temporal information (for both referring ex-pressions and potential referents).
Second, thegraph match mechanism allows a simultaneousapplication of temporal constraints and semanticconstraints.
Specifically, we use two attributed re-lational graphs (ARGs).
One graph corresponds toall referring expressions in the speech utterances,called the referring graph.
The other graph corre-sponds to all potential referents (either comingfrom gestures or contexts), called the referentgraph.
By finding the best match between the re-ferring graph and the referent graph, we can findthe most possible referent(s) to each referring ex-pression.An ARG consists of a set of nodes and a set ofedges.
For example, Figure 2(a) is the referringgraph for the speech utterance in Figure 1(c).There are two nodes corresponding to two refer-ring expressions ?this?
and ?these two houses?
re-spectively.
Each node encodes the semantic andtemporal information of the corresponding refer-ring expression such as the semantic type of thepotential referent, the number, the start and endtime the expression was uttered, etc.
The edge be-tween two nodes indicates the semantic and tempo-ral relations between these two expressions.Similarly, Figure 2(b) is the referent graph for theinput in Figure 1(c).
This referent graph consists offour sub-graphs.
Three sub-graphs correspond tothree gestures respectively.
Each node in these sub-graphs corresponds to one object selected by thegesture.
Each node encodes the semantic and tem-poral information of the selected object, as well asthe probability this object is actually selected.There is also a sub-graph corresponding to the in-teraction context.
Each node in this sub-graphrepresents an object in the focus in the last interac-tion turn.
The sub-graphs are connected via seman-tic type and temporal relations.With the ARG representations described above,the reference resolution problem becomes match-ing the referent graph with the referring graph.Suppose we have two graphs to be matched:?
The referent graph Gc = ?
{ax}, {rxy}?, where {ax}is the node list and {rxy} is the edge list.
Theedge rxy connects nodes ax and ay.Node 1Surface: ?this?Base: UnknownNumber: 1Begin Time: 32264270End Time: 32264273Surface: ?these two houses?Base: HouseNumber: 2Begin Time: 32264381End Time: 32264398Node 2Relation: 1Direction: Node1 ?
Node2Temporal: PrecedingSemantic type: Same(a)Node 1Node 2Node 4Node3 Node 5Sub-graph of the1st point gestureNode 6Node 7Sub-graph of the2nd point gestureSub-graph of the3rd point gestureNode 8Sub-graph of theinteraction contextBase: HouseIdentifier: 4Attr: {Price, Size, ?
}Begin Time: 32264365End Time: 32264366Prob: 0.4356Node 2Base: TownIdentifier: 1Attr: {Area, Population ?
}Begin Time: 32264365End Time: 32264366Prob: 0.3321Node 3Relation 5Direction: Node 1 ?
Node 4Temporal: PrecedingSemantic Type: Same(b)Figure 2.
The ARG representation for references in Figure1(c).
(a) The referring graph (b) The referent graph, wheredashed rectangles represent sub-graphs.?
The referring graph Gs = ?
{?m}, {?mn}?, where{?m} is the node list and {?mn} is the edge list.The edge ?mn connects nodes ?m and ?n.The match process is to maximize the followingfunction:( , ) ( , ) ( , )( , ) ( , ) ( , )c s x m x mx mx m y n xy mnx y m nQ G G P a aP a P a r?
?
??
?
?
?=?
??
?
?
?+(1)with respect to P(ax,?m), the matching probabili-ties between the referent node ax and the referringnode ?m.The function Q(Gc,Gs) measures the degree ofthe overall match between the referent graph andthe referring graph.
This function not only consid-ers the similarities between nodes as indicated bythe function ?
(ax,?m), but also considers the simi-larities between edges as indicated by the function?(rxy,?mn).
Both node similarity and edge similarityfunctions are further defined by a combination ofsemantic and temporal constraints.
For example,?
(ax,?m)=Sem(ax,?m)Tem(ax,?m), where Sem(ax,?m)measures the semantic compatibility by determin-ing whether the semantic categories of ax and ?mare the same, whether their attributes are compati-ble, and so on.
Tem(ax,?m) measures the temporalalignment and is empirically defined as follows:?????
??????
??=contextfromisagesturefromisatimeatimeaTemxxmxmx,1.0,2000|)()(|exp),(?
?To maximize (1), we modified the graduated as-signment algorithm (Gold and Rangarajan, 1996).When the algorithm converges, P(ax,?m) gives usthe matching probabilities.
Details are described ina separate paper.3 DiscussionDuring the study, we collected 156 inputs.
Thesystem assigned time stamps to each recognizedword in the utterance, and each gesture.
Figure 3shows an example of an input that consisted of twogesture inputs and a speech utterance ?comparethis house with this house?.
The first two lines rep-resent two gestures.
Each line gives informationabout when the gesture started and ended, as wellas the selected objects with their probabilities.These data provided us information on how thespeech and gesture were aligned (to the accuracyof milliseconds).
These data will help us furthervalidate the temporal compatibility function usedin the matching process.We described an approach that uses graphmatching algorithm to combine semantic and tem-poral constraints for reference resolution.
Thestudy showed that this approach worked quite well(93% accuracy) when the referring expressionswere correctly recognized by the ASR.
In the fu-ture, we plan to incorporate spatial constraints.ReferencesP.
Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman,I.
Smith, L. Chen, and J.Clow.
1996.
Quickset:Multimodal Interaction for Distributed Applications.Proceedings of ACM Multimedia, 31-40.S.
Gold and A. Rangarajan.
1996.
A graduatedassignment algorithm for graph matching, IEEETrans.
Pattern Analysis and Machine Intelligence,vol.
18, no.
4.
377?388.C.
Huls, E. Bos, and W. Classen.
1995.
AutomaticReferent Resolution of Deictic and AnaphoricExpressions.
Computational Linguistics, 21(1):59-79.A.
Kehler.
2000.
Cognitive Status and Form ofReference in Multimodal Human-ComputerInteraction, Proceedings of AAAI?00.
685-689.J.
G. Neal, C. Y. Thielman, Z. Dobes, S. M. Haller, andS.
C. Shapiro.
1998.
Natural Language withIntegrated Deictic and Graphic Gestures.
IntelligentUser Interfaces, M. Maybury and W. Wahlster (eds.),38-51.W.
H. Tsai and K. S. Fu.
1979.
Error-correctingisomorphism of attributed relational graphs for patternanalysis.
IEEE Trans.
Sys., Man and Cyb., vol.
9,757?768.Input received on port 3334: 67275921 67277343 2    6923 3 1 2 67275921 67277343 39218 10000 0 0 2550.28571     70 23 2 2 2 67275921 67277343 39218 100000 0 255 1.Input received on port 3334: 67278140 67279078 2    7124 4 1 2 67278140 67279078 797 10000 255 0 0 0.7454572 24 3 2 2 67278140 67279078 797 10000 0 0 255 1.speech input: compare_67273821 this_67274160House_67274490 with_67275547 this_67275847House_67276096Figure 3.
Gesture and speech dataW.
Wahlster.
1998.
User and Discourse Models forMultimodal Communication, Intelligent UserInterfaces, M. Maybury and W. Wahlster (eds.
), 359-370.M.
Zancanaro, O.
Stock, and C. Strapparava.
1997.Multimodal Interaction for Information Access:Exploiting Cohesion.
Computational Intelligence13(7):439-464.
