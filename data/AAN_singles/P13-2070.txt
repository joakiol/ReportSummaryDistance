Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393?398,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Tightly-coupled Unsupervised Clustering andBilingual Alignment Model for TransliterationTingting Li1, Tiejun Zhao1, Andrew Finch2, Chunyue Zhang11Harbin Institute of Technology, Harbin, China2NICT, Japan1{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn2andrew.finch@nict.go.jpAbstractMachine Transliteration is an essentialtask for many NLP applications.
Howev-er, names and loan words typically orig-inate from various languages, obey dif-ferent transliteration rules, and thereforemay benefit from being modeled inde-pendently.
Recently, transliteration mod-els based on Bayesian learning have over-come issues with over-fitting allowing formany-to-many alignment in the training oftransliteration models.
We propose a nov-el coupled Dirichlet process mixture mod-el (cDPMM) that simultaneously clustersand bilingually aligns transliteration datawithin a single unified model.
The un-ified model decomposes into two class-es of non-parametric Bayesian componentmodels: a Dirichlet process mixture mod-el for clustering, and a set of multino-mial Dirichlet process models that perf-orm bilingual alignment independently foreach cluster.
The experimental resultsshow that our method considerably outper-forms conventional alignment models.1 IntroductionMachine transliteration methods can be catego-rized into phonetic-based models (Knight et al,1998), spelling-based models (Brill et al, 2000),and hybrid models which utilize both phoneticand spelling information (Oh et al, 2005; Oh etal., 2006).
Among them, statistical spelling-basedmodels which directly align characters in the train-ing corpus have become popular because theyare language-independent, do not require phonet-ic knowledge, and are capable of achieving state-of-the-art performance (Zhang et al, 2012b).
Amajor problem with real-word transliteration cor-pora is that they are usually not clean, may con-tain name pairs with various linguistic origins andthis can hinder the performance of spelling-basedmodels because names from different origins obeydifferent pronunciation rules, for example:?Kim Jong-il/????
(Korea),?Kana Gaski/???
(Japan),?Haw King/???
(England),?Jin yong/???
(China).The same Chinese character ???
should bealigned to different romanized character se-quences: ?Kim?, ?Kana?, ?King?, ?Jin?.
To ad-dress this issue, many name classification metho-ds have been proposed, such as the supervised lan-guage model-based approach of (Li et al, 2007),and the unsupervised approach of (Huang et al,2005) that used a bottom-up clustering algorithm.
(Li et al, 2007) proposed a supervised translitera-tion model which classifies names based on theirorigins and genders using a language model; itswitches between transliteration models based onthe input.
(Hagiwara et al, 2011) tackled the is-sue by using an unsupervised method based on theEM algorithm to perform a soft classification.Recently, non-parametric Bayesianmodels (Finch et al, 2010; Huang et al,2011; Hagiwara et al, 2012) have attractedmuch attention in the transliteration field.
Incomparison to many of the previous alignmentmodels (Li et al, 2004; Jiampojamarn et al,2007; Berg-Kirkpatrick et al, 2011), the non-parametric Bayesian models allow unconstrainedmonotonic many-to-many alignment and are ableto overcome the inherent over-fitting problem.Until now most of the previous work (Li et al,2007; Hagiwara et al, 2011) is either affected bythe multi-origins factor, or has issues with over-fitting.
(Hagiwara et al, 2012) took these two fac-tors into consideration, but their approach still op-erates within an EM framework and model orderselection by hand is necessary prior to training.393We propose a simple, elegant, fully-unsupervised solution based on a single generativemodel able to both cluster and align simultaneous-ly.
The coupled Dirichlet Process Mixture Model(cDPMM) integrates a Dirichlet process mixturemodel (DPMM) (Antoniak, 1974) and a BayesianBilingual Alignment Model (BBAM) (Finch etal., 2010).
The two component models worksynergistically to support one another: the clus-tering model sorts the data into classes so thatself-consistent alignment models can be builtusing data of the same type, and at the same timethe alignment probabilities from the alignmentmodels drive the clustering process.In summary, the key advantages of our modelare as follows:?
it is based on a single, unified generativemodel;?
it is fully unsupervised;?
it is an infinite mixture model, and does notrequire model order selection ?
it is effec-tively capable of discovering an appropriatenumber of clusters from the data;?
it is able to handle data from multiple origins;?
it can perform many-to-many alignmentwithout over-fitting.2 Model DescriptionIn this section we describe the methodology andrealization of the proposed cDPMM in detail.2.1 TerminologyIn this paper, we concentrate on the alignmentprocess for transliteration.
The proposed cDP-MM segments a bilingual corpus of transliterationpairs into bilingual character sequence-pairs.
Wewill call these sequence-pairs Transliteration U-nits (TUs).
We denote the source and target ofa TU as sm1 = ?s1, ..., sm?
and tn1 = ?t1, ..., tn?respectively, where si (ti) is a single character insource (target) language.
We use the same no-tation (s, t) = (?s1, ..., sm?, ?t1, ..., tn?)
to de-note a transliteration pair, which we can write asx = (sm1 , tn1 ) for simplicity.
Finally, we expressthe training set itself as a set of sequence pairs:D = {xi}Ii=1.
Our aim is to obtain a bilingualalignment ?
(s1, t1), ..., (sl, tl)?
for each transliter-ation pair xi, where each (sj , tj) is a segment ofthe whole pair (a TU) and l is the number of seg-ments used to segment xi.2.2 MethodologyOur cDPMM integrates two Dirichlet processmodels: the DPMM clustering model, and theBBAM alignment model which is a multinomialDirichlet process.A Dirichlet process mixture model, models thedata as a mixture of distributions ?
one for eachcluster.
It is an infinite mixture model, and thenumber of components is not fixed prior to train-ing.
Equation 1 expresses the DPMM hierarchi-cally.Gc|?c, G0c ?
DP (?c, G0c)?k|Gc ?
Gcxi|?k ?
f(xi|?k) (1)where G0c is the base measure and ?c > 0 is theconcentration parameter for the distribution Gc.xi is a name pair in training data, and ?k repre-sents the parameters of a candidate cluster k forxi.
Specifically ?k contains the probabilities of allthe TUs in cluster k. f(xi|?k) (defined in Equa-tion 7) is the probability that mixture componentk parameterized by ?k will generate xi.The alignment component of our cDPMM isa multinomial Dirichlet process and is defined asfollows:Ga|?a, G0a ?
DP (?a, G0a)(sj , tj)|Ga ?
Ga (2)The subscripts ?c?
and ?a?
in Equations 1 and 2indicate whether the terms belong to the clusteringor alignment model respectively.The generative story for the cDPMM is sim-ple: first generate an infinite number of clusters,choose one, then generate a transliteration pair us-ing the parameters that describe the cluster.
Thebasic sampling unit of the cDPMM for the cluster-ing process is a transliteration pair, but the basicsampling unit for BBAM is a TU.
In order to inte-grate the two processes in a single model we treata transliteration pair as a sequence of TUs gener-ated by a BBAM model.
The BBAM generates asequence (a transliteration pair) based on the jointsource-channel model (Li et al, 2004).
We use ablocked version of a Gibbs sampler to train eachBBAM (see (Mochihashi et al, 2009) for detailsof this process).2.3 The Alignment ModelThis model is a multinomial DP model.
Under theChinese restaurant process (CRP) (Aldous, 1985)394interpretation, each unique TU corresponds to adish served at a table, and the number of customersin each table represents the count of a particularTU in the model.The probability of generating the jth TU (sj , tj)is,P((sj , tj)|(s?j , t?j))=N((sj , tj))+ ?aG0a((sj , tj))N + ?a (3)where N is the total number of TUs generatedso far, and N((sj , tj))is the count of (sj , tj).
(s?j , t?j) are all the TUs generated so far except(sj , tj).
The base measure G0a is a joint spellingmodel:G0a((s, t))= P (|s|)P (s||s|)P (|t|)P (t||t|)= ?|s|s|s|!
e?
?sv?|s|s ??|t|t|t|!
e?
?tv?|t|t(4)where |s| (|t|) is the length of the source (target)sequence, vs (vt) is the vocabulary (alphabet) sizeof the source (target) language, and ?s (?t) is theexpected length of source (target) side.2.4 The Clustering ModelThis model is a DPMM.
Under the CRP interpre-tation, a transliteration pair corresponds to a cus-tomer, the dish served on each table correspondsto an origin of names.We use z = (z1, ..., zI), zi ?
{1, ...,K} to in-dicate the cluster of each transliteration pair xi inthe training set and ?
= (?1, ..., ?K) to representthe parameters of the component associated witheach cluster.In our model, each mixture component is amultinomial DP model, and since ?k contains theprobabilities of all the TUs in cluster k, the num-ber of parameters in each ?k is uncertain andchanges with the transliteration pairs that belongto the cluster.
For a new cluster (the K + 1th clus-ter), we use Equation 4 to calculate the probabilityof each TU.
The cluster membership probabilityof a transliteration pair xi is calculated as follows,P (zi = k|D, ?, z?i) ?
nkn?
1 + ?cP (xi|z, ?k) (5)P (zi = K + 1|D, ?, z?i) ?
?cn?
1 + ?cP (xi|z, ?K+1)(6)where nk is the number of transliteration pairs inthe existing cluster k ?
{1, ...,K} (cluster K + 1is a newly created cluster), zi is the cluster indi-cator for xi, and z?i is the sequence of observedclusters up to xi.
As mentioned earlier, basic sam-pling units are inconsistent for the clustering andalignment model, therefore to couple the modelsthe BBAM generates transliteration pairs as a se-quence of TUs, these pairs are then used directlyin the DPMM.Let ?
= ?
(s1, t1), ..., (sl, tl)?
be a derivation ofa transliteration pair xi.
To make the model inte-gration process explicit, we use function f to cal-culate the probability P (xi|z, ?k), where f is de-fined as follows,f(xi|?k) ={ ???R?(s,t)??
P (s, t|?k) k ?
{1, ...,K}???R?(s,t)??
G0c(s, t) k = K + 1(7)where R denotes the set of all derivations of xi,G0c is the same as Equation 4.The cluster membership zi is sampled togetherwith the derivation ?
in a single step according toP (zi = k|D, ?, z?i) and f(xi|?k).
Following themethod of (Mochihashi et al, 2009), first f(xi|?k)is calculated by forward filtering, and then a sam-ple ?
is taken by backward sampling.3 Experiments3.1 CorporaTo empirically validate our approach, we investi-gate the effectiveness of our model by conduct-ing English-Chinese name transliteration genera-tion on three corpora containing name pairs ofvarying degrees of mixed origin.
The first two cor-pora were drawn from the ?Names of The World?sPeoples?
dictionary published by Xin Hua Pub-lishing House.
The first corpus was construct-ed with names only originating from English lan-guage (EO), and the second with names originat-ing from English, Chinese, Japanese evenly (ECJ-O).
The third corpus was created by extractingname pairs from LDC (Linguistic Data Consor-tium) Named Entity List, which contains namesfrom all over the world (Multi-O).
We divided thedatasets into training, development and test setsfor each corpus with a ratio of 10:1:1.
The detailsof the division are displayed in Table 2.395cDPMM Alignment BBAM Alignmentmun|?
din|?
ger|?
(0, English) mun|?
din|?
ger|?ding|?
guo|?
(2, Chinese) din|?
g| guo|?tei|?
be|?
(3, Japanese) t| |?
e| ibe|?fan|?
chun|?
yi|?
(2, Chinese) fan|?
chun|?
y| i|?hong|?
il|?
sik|?
(5, Korea) hong|?
i|?
l| si|?
k|sei|?
ichi|?
ro|?
(4, Japanese) seii|?
ch| i|?
ro|?dom|?
b|?
ro|?
w|?
s|?
ki|?
(0, Russian) do|?
mb|?
ro|?
w|?
s|?
ki|?he|?
dong|?
chang|?
(2, Chinese) he|?
don|?
gchang|?b|?
ran|?
don|?
(0, English) b|?
ran|?
don|?Table 1: Typical alignments from the BBAM and cDPMM.3.2 BaselinesWe compare our alignment model withGIZA++ (Och et al, 2003) and the Bayesianbilingual alignment model (BBAM).
We employtwo decoding models: a phrase-based machinetranslation decoder (specifically Moses (Koehnet al, 2007)), and the DirecTL decoder (Jiampo-jamarn et al, 2009).
They are based on differentdecoding strategies and optimization targets, andtherefore make the comparison more compre-hensive.
For the Moses decoder, we applied thegrow-diag-final-and heuristic algorithm to extractthe phrase table, and tuned the parameters usingthe BLEU metric.Corpora Corpus ScaleTraining Development TestingEO 32,681 3,267 3,267ECJ-O 32,500 3,250 3,250Multi-O 33,291 3,328 3,328Table 2: Statistics of the experimental corpora.To evaluate the experimental results, we uti-lized 3 metrics from the Named Entities Workshop(NEWS) (Zhang et al, 2012a): word accuracy intop-1 (ACC), fuzziness in top-1 (Mean F-score)and mean reciprocal rank (MRR).3.3 Parameter SettingIn our model, there are several important parame-ters: 1) max s, the maximum length of the sourcesequences of the alignment tokens; 2) max t, themaximum length of the target sequences of thealignment tokens; and 3) nc, the initial number ofclasses for the training data.
We set max s = 6,max t = 1 and nc = 5 empirically based on asmall pilot experiment.
The Moses decoder wasused with default settings except for the distortion-limit which was set to 0 to ensure monotonic de-coding.
For the DirecTL decoder the followingsettings were used: cs = 4, ng = 9 and nBest =5.
cs denotes the size of context window for fea-tures, ng indicates the size of n-gram features andnBest is the size of transliteration candidate listfor updating the model in each iteration.
The con-centration parameter ?c, ?a of the clustering mod-el and the BBAM was learned by sampling its val-ue.
Following (Blunsom et al, 2009) we useda vague gamma prior ?
(10?4, 104), and samplednew values from a log-normal distribution whosemean was the value of the parameter, and variancewas 0.3.
We used the Metropolis-Hastings algo-rithm to determine whether this new sample wouldbe accepted.
The parameters ?s and ?t in Equa-tion 4 were set to ?s = 4 and ?t = 1.Model EO ECJ-O Multi-O#(Clusters) cDPMM 5.8 9.5 14.3#(Targets)GIZA++ 14.43 5.35 6.62BBAM 6.06 2.45 2.91cDPMM 9.32 3.45 4.28Table 3: Alignment statistics.3.4 Experimental ResultsTable 3 shows some details of the alignment re-sults.
The #(Clusters) represents the average num-ber of clusters from the cDPMM.
It is averagedover the final 50 iterations, and the classes whichcontain less than 10 name pairs are excluded.
The#(Targets) represents the average number of En-glish character sequences that are aligned to eachChinese sequence.
From the results we can seethat in terms of the number of alignment targe-ts: GIZA++ > cDPMM > BBAM.
GIZA++ hasconsiderably more targets than the other approach-es, and this is likely to be a symptom of it over-fitting the data.
cDPMM can alleviate the over-fitting through its BBAM component, and at thesame time effectively model the diversity in Chi-nese character sequences caused by multi-origin.Table 1 shows some typical TUs from the align-ments produced by BBAM and cDPMM on cor-pus Multi-O.
The information in brackets in Ta-ble 1, represents the ID of the class and origin of396Corpora Model EvaluationACC M-Fscore MRREOGIZA 0.7241 0.8881 0.8061BBAM 0.7286 0.8920 0.8043cDPMM 0.7398 0.8983 0.8126ECJ-OGIZA 0.5471 0.7278 0.6268BBAM 0.5522 0.7370 0.6344cDPMM 0.5643 0.7420 0.6446Multi-OGIZA 0.4993 0.7587 0.5986BBAM 0.5163 0.7769 0.6123cDPMM 0.5237 0.7796 0.6188Table 4: Comparison of different methods usingthe Moses phrase-based decoder.the name pair; the symbol ?
?
indicates a ?NUL-L?
alignment.
We can see the Chinese characters??
(ding) ?
(yi) ?(dong)?
have different align-ments in different origins, and that the cDPMMhas provided the correct alignments for them.We used the sampled alignment from runningthe BBAM and cDPMMmodels for 100 iterations,and combined the alignment tables of each classtogether.
The experiments are therefore investigat-ing whether the alignment has been meaningfullyimproved by the clustering process.
We would ex-pect further gains from exploiting the class infor-mation in the decoding process (as in (Li et al,2007)), but this remains future research.
The top-10 transliteration candidates were used for testing.The detailed experimental results are shown in Ta-bles 4 and 5.Our proposed model obtained the highest per-formance on all three datasets for all evaluationmetrics by a considerable margin.
Surprisingly,for dataset EO although there is no multi-originfactor, we still observed a respectable improve-ment in every metric.
This shows that althoughnames may have monolingual origin, there are hid-den factors which can allow our model to succeed,possibly related to gender or convention.
Othermodels based on supervised classification or clus-tering with fixed classes may fail to capture thesecharacteristics.To guarantee the reliability of the compara-tive results, we performed significance testingbased on paired bootstrap resampling (Efron et al,1993).
We found all differences to be significant(p < 0.05).4 ConclusionIn this paper we propose an elegant unsupervisedtechnique for monotonic sequence alignmentbased on a single generative model.
The key ben-Corpora Model EvaluationACC M-Fscore MRREOGIZA 0.6950 0.8812 0.7632BBAM 0.7152 0.8899 0.7839cDPMM 0.7231 0.8933 0.7941ECJ-OGIZA 0.3325 0.6208 0.4064BBAM 0.3427 0.6259 0.4192cDPMM 0.3521 0.6302 0.4316Multi-OGIZA 0.3815 0.7053 0.4592BBAM 0.3934 0.7146 0.4799cDPMM 0.3970 0.7179 0.4833Table 5: Comparison of different methods usingthe DirecTL decoder.efits of our model are that it can handle data frommultiple origins, and model using many-to-manyalignment without over-fitting.
The model oper-ates by clustering the data into classes while si-multaneously aligning it, and is able to discoveran appropriate number of classes from the data.Our results show that our alignment model can im-prove the performance of a transliteration gener-ation system relative to two other state-of-the-artaligners.
Furthermore, the system produced gainseven on data of monolingual origin, where no ob-vious clusters in the data were expected.AcknowledgmentsWe thank the anonymous reviewers for their valu-able comments and helpful suggestions.We alsothank Chonghui Zhu, Mo Yu, and Wenwen Zhangfor insightful discussions.
This work was support-ed by National Natural Science Foundation of Chi-na (61173073), and the Key Project of the Nation-al High Technology Research and DevelopmentProgram of China (2011AA01A207).ReferencesD.J.
Aldous.
1985.
Exchangeability and Related Top-ics.
E?cole d?E?te?
St Flour 1983.
Springer, 1985,1117:1?198.C.E.
Antoniak.
1974.
Mixtures of Dirichlet processeswith applications to Bayesian nonparametric prob-lems.
Annals of Statistics.
2:1152, 174.Taylor Berg-Kirkpatrick and Dan Klein.
2011.
Simpleeffective decipherment via combinatorial optimiza-tion.
In Proc.
of EMNLP, pages 313?321.P.
Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009.A Gibbs sampler for phrasal synchronous grammarinduction.
In Proc.
of ACL, pages 782?790.Eric Brill and Robert C. Moore.
2000.
An ImprovedError Model for Noisy Channel Spelling Correction.In Proc.
of ACL, pages 286?293.397B.
Efron and R. J. Tibshirani 1993.
An Introduction tothe Bootstrap.
Chapman & Hall, New York, NY.Andrew Finch and Eiichiro Sumita.
2010.
A BayesianModel of Bilingual Segmentation for Translitera-tion.
In Proc.
of the 7th International Workshop onSpoken Language Translation, pages 259?266.Masato Hagiwara and Satoshi Sekine.
2011.
LatentClass Transliteration based on Source Language O-rigin.
In Proc.
of ACL (Short Papers), pages 53-57.Masato Hagiwara and Satoshi Sekine.
2012.
Latentsemantic transliteration using dirichlet mixture.
InProc.
of the 4th Named Entity Workshop, pages 30?37.Fei Huang, Stephan Vogel, and Alex Waibel.
2005.Clustering and Classifying Person Names by Origin.In Proc.
of AAAI, pages 1056?1061.Yun Huang, Min Zhang and Chew Lim Tan.
2011.Nonparametric Bayesian Machine Transliterationwith Synchronous Adaptor Grammars.
In Proc.
ofACL, pages 534?539.Sittichai Jiampojamarn, Grzegorz Kondrak and TarekSherif.
2007.
Applying Many-to-Many Alignmentsand Hidden Markov Models to Letter-to-PhonemeConversion.
In Proc.
of NAACL, pages 372?379.Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,Kenneth Dwyer and Grzegorz Kondrak.
2009.DirecTL: a Language Independent Approach toTransliteration.
In Proc.
of the 2009 Named EntitiesWorkshop: Shared Task on Transliteration (NEWS2009), pages 1056?1061.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Journal of Computational Linguis-tics, pages 28?31.Philipp Koehn and Hieu Hoang and Alexandra Birchand Chris Callison-Burch and Marcello Federicoand Nicola Bertoldi and Brooke Cowan and WadeShen and Christine Moran and Richard Zens andChris Dyer and Ondrej Bojar and Alexandra Con-stantin and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proc.
of ACL.Haizou Li, Min Zhang, and Jian Su 2004.
A join-t source-channel model for machine transliteration.In ACL ?04: Proceedings of the 42nd Annual Meet-ing on Association for Computational Linguistics.Association for Computational Linguistics, Morris-town, NJ, USA, 159.Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and MinghuiDong.
2007.
Semantic Transliteration of PersonalNames.
In Proc.
of ACL, pages 120?127.Daichi Mochihashi, Takeshi Yamada, and Naonori Ue-da.
2009.
Bayesian Unsupervised Word Segmen-tation with Nested Pitman-Yor Language Modeling.In Proc.
of ACL/IJCNLP, pages 100?108.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Journal of Comput.
Linguist., 29(1):19-51.Jong-Hoon Oh, and Key-Sun Choi.
2005.
MachineLearning Based English-to-Korean TransliterationUsing Grapheme and Phoneme Information.
Jour-nal of IEICE Transactions, 88-D(7):1737-1748.Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.2006.
A machine transliteration model based oncorrespondence between graphemes and phonemes.Journal of ACM Trans.
Asian Lang.
Inf.
Process.,5(3):185-208.Min Zhang, Haizhou Li, Ming Liu and A Kumaran.2012a.
Whitepaper of NEWS 2012 shared task onmachine transliteration.
In Proc.
of the 4th NamedEntity Workshop (NEWS 2012), pages 1?9.Min Zhang, Haizhou Li, A Kumaran and Ming Liu.2012b.
Report of NEWS 2012 Machine Translitera-tion Shared Task.
In Proc.
of the 4th Named EntityWorkshop (NEWS 2012), pages 10?20.398
