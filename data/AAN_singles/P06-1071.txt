Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 561?568,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Progressive Feature Selection Algorithm for UltraLarge Feature SpacesQi ZhangComputer Science DepartmentFudan UniversityShanghai 200433, P.R.
Chinaqi_zhang@fudan.edu.cnFuliang WengResearch and Technology CenterRobert Bosch Corp.Palo Alto, CA 94304, USAfuliang.weng@rtc.bosch.comZhe FengResearch and Technology CenterRobert Bosch Corp.Palo Alto, CA 94304, USAzhe.feng@rtc.bosch.comAbstractRecent developments in statistical modelingof various linguistic phenomena have shownthat additional features give consistent per-formance improvements.
Quite often, im-provements are limited by the number of fea-tures a system is able to explore.
This paperdescribes a novel progressive training algo-rithm that selects features from virtuallyunlimited feature spaces for conditionalmaximum entropy (CME) modeling.
Experi-mental results in edit region identificationdemonstrate the benefits of the progressivefeature selection (PFS) algorithm: the PFSalgorithm maintains the same accuracy per-formance as previous CME feature selectionalgorithms (e.g., Zhou et al, 2003) when thesame feature spaces are used.
When addi-tional features and their combinations areused, the PFS gives 17.66% relative im-provement over the previously reported bestresult in edit region identification onSwitchboard corpus (Kahn et al, 2005),which leads to a 20% relative error reductionin parsing the Switchboard corpus when goldedits are used as the upper bound.1 IntroductionConditional Maximum Entropy (CME) modelinghas received a great amount of attention withinnatural language processing community for thepast decade (e.g., Berger et al, 1996; Reynar andRatnaparkhi, 1997; Koeling, 2000; Malouf, 2002;Zhou et al, 2003; Riezler and Vasserman, 2004).One of the main advantages of CME modeling isthe ability to incorporate a variety of features in auniform framework with a sound mathematicalfoundation.
Recent improvements on the originalincremental feature selection (IFS) algorithm,such as Malouf (2002) and Zhou et al (2003),greatly speed up the feature selection process.However, like many other statistical modelingalgorithms, such as boosting (Schapire andSinger, 1999) and support vector machine (Vap-nik 1995), the algorithm is limited by the size ofthe defined feature space.
Past results show thatlarger feature spaces tend to give better results.However, finding a way to include an unlimitedamount of features is still an open research prob-lem.In this paper, we propose a novel progressivefeature selection (PFS) algorithm that addressesthe feature space size limitation.
The algorithm isimplemented on top of the Selective Gain Com-putation (SGC) algorithm (Zhou et al, 2003),which offers fast training and high quality mod-els.
Theoretically, the new algorithm is able toexplore an unlimited amount of features.
Be-cause of the improved capability of the CMEalgorithm, we are able to consider many newfeatures and feature combinations during modelconstruction.To demonstrate the effectiveness of our newalgorithm, we conducted a number of experi-ments on the task of identifying edit regions, apractical task in spoken language processing.Based on the convention from Shriberg (1994)and Charniak and Johnson (2001), a disfluentspoken utterance is divided into three parts: thereparandum, the part that is repaired; the inter-561regnum, which can be filler words or empty; andthe repair/repeat, the part that replaces or repeatsthe reparandum.
The first two parts combined arecalled an edit or edit region.
An example isshown below:interregnumIt is, you know, this is a tough problem.reparandum repairIn section 2, we briefly review the CME mod-eling and SGC algorithm.
Then, section 3 gives adetailed description of the PFS algorithm.
In sec-tion 4, we describe the Switchboard corpus, fea-tures used in the experiments, and the effective-ness of the PFS with different feature spaces.Section 5 concludes the paper.2 BackgroundBefore presenting the PFS algorithm, we firstgive a brief review of the conditional maximumentropy modeling, its training process, and theSGC algorithm.
This is to provide the back-ground and motivation for our PFS algorithm.2.1 Conditional Maximum Entropy ModelThe goal of CME is to find the most uniformconditional distribution of y given observationx, ( )xyp , subject to constraints specified by a setof features ( )yxf i , , where features typically takethe value of either 0 or 1 (Berger et al, 1996).More precisely, we want to maximize( ) ( ) ( ) ( )( )xypxypxppHyxlog~,?
?=           (1)given the constraints:( ) ( )ii fEfE ~=                         (2)where( ) ( ) ( )?=yxii yxfyxpfE,,,~~is the empirical expected feature count from thetraining data and( ) ( ) ( ) ( )?=yxii yxfxypxpfE,,~is the feature expectation from  the conditionalmodel ( )xyp .This results in the following exponentialmodel:( ) ( ) ( )???????
?= ?jjj yxfxZxyp ,exp1 ?
(3)where ?j  is the weight corresponding to the fea-ture fj, and Z(x) is a normalization factor.A variety of different phenomena, includinglexical, structural, and semantic aspects, in natu-ral language processing tasks can be expressed interms of features.
For example, a feature can bewhether the word in the current position is a verb,or the word is a particular lexical item.
A featurecan also be about a particular syntactic subtree,or a dependency relation (e.g., Charniak andJohnson, 2005).2.2 Selective Gain Computation AlgorithmIn real world applications, the number of possi-ble features can be in the millions or beyond.Including all the features in a model may lead todata over-fitting, as well as poor efficiency andmemory overflow.
Good feature selection algo-rithms are required to produce efficient and highquality models.
This leads to a good amount ofwork in this area (Ratnaparkhi et al, 1994; Ber-ger et al, 1996; Pietra et al 1997; Zhou et al,2003; Riezler and Vasserman, 2004)In the most basic approach, such as Ratna-parkhi et al (1994) and Berger et al (1996),training starts with a uniform distribution over allvalues of y and an empty feature set.
For eachcandidate feature in a predefined feature space, itcomputes the likelihood gain achieved by includ-ing the feature in the model.
The feature thatmaximizes the gain is selected and added to thecurrent model.
This process is repeated until thegain from the best candidate feature only givesmarginal improvement.
The process is very slow,because it has to re-compute the gain for everyfeature at each selection stage, and the computa-tion of a parameter using Newton?s method be-comes expensive, considering that it has to berepeated many times.The idea behind the SGC algorithm (Zhou etal., 2003) is to use the gains computed in theprevious step as approximate upper bounds forthe subsequent steps.
The gain for a featureneeds to be re-computed only when the featurereaches the top of a priority queue ordered bygain.
In other words, this happens when the fea-ture is the top candidate for inclusion in themodel.
If the re-computed gain is smaller thanthat of the next candidate in the list, the feature isre-ranked according to its newly computed gain,and the feature now at the top of the list goesthrough the same gain re-computing process.This heuristics comes from evidences that thegains become smaller and smaller as more andmore good features are added to the model.
Thiscan be explained as follows: assume that theMaximum Likelihood (ML) estimation lead tothe best model that reaches a ML value.
The MLvalue is the upper bound.
Since the gains need tobe positive to proceed the process, the difference562between the Likelihood of the current and theML value becomes smaller and smaller.
In otherwords, the possible gain each feature may add tothe model gets smaller.
Experiments in Zhou etal.
(2003) also confirm the prediction that thegains become smaller when more and more fea-tures are added to the model, and the gains donot get unexpectively bigger or smaller as themodel grows.
Furthermore, the experiments inZhou et al (2003) show no significant advantagefor looking ahead beyond the first element in thefeature list.
The SGC algorithm runs hundreds tothousands of times faster than the original IFSalgorithm without degrading classification per-formance.
We used this algorithm for it enablesus to find high quality CME models quickly.The original SGC algorithm uses a techniqueproposed by Darroch and Ratcliff (1972) andelaborated by Goodman (2002): when consider-ing a feature fi, the algorithm only modifies thoseun-normalized conditional probabilities: ( )( )?
j jj yxf ,exp ?for (x, y) that satisfy fi (x, y)=1, and subsequentlyadjusts the corresponding normalizing factorsZ(x) in (3).
An implementation often uses a map-ping table, which maps features to the traininginstance pairs (x, y).3 Progressive Feature Selection Algo-rithmIn general, the more contextual information isused, the better a system performs.
However,richer context can lead to combinatorial explo-sion of the feature space.
When the feature spaceis huge (e.g., in the order of tens of millions offeatures or even more), the SGC algorithm ex-ceeds the memory limitation on commonly avail-able computing platforms with gigabytes ofmemory.To address the limitation of the SGC algo-rithm, we propose a progressive feature selectionalgorithm that selects features in multiple rounds.The main idea of the PFS algorithm is to split thefeature space into tractable disjoint sub-spacessuch that the SGC algorithm can be performedon each one of them.
In the merge step, the fea-tures that SGC selects from different sub-spacesare merged into groups.
Instead of re-generatingthe feature-to-instance mapping table for eachsub-space during the time of splitting and merg-ing, we create the new mapping table from theprevious round?s tables by collecting those en-tries that correspond to the selected features.Then, the SGC algorithm is performed on eachof the feature groups and new features are se-lected from each of them.
In other words, thefeature space splitting and subspace merging areperformed mainly on the feature-to-instancemapping tables.
This is a key step that leads tothis very efficient PFS algorithm.At the beginning of each round for feature se-lection, a uniform prior distribution is alwaysassumed for the new CME model.
A more pre-cise description of the PFS algorithm is given inTable 1, and it is also graphically illustrated inFigure 1.Given:Feature space F(0) = {f1(0), f2(0), ?, fN(0)},step_num = m,  select_factor = s1.
Split the feature space into N1 parts{F1(1), F2(1), ?, FN1(1)} = split(F(0))2. for k=1 to m-1 do//2.1 Feature selectionfor each feature space Fi(k) doFSi(k) = SGC(Fi(k), s)//2.2 Combine selected features{F1(k+1), ?, FNk+1(k+1)}  =merge(FS1(k), ?, FSNk(k))3.
Final feature selection & optimizationF(m) = merge(FS1(m-1), ?, FSNm-1(m-1))FS(m) = SGC(F(m), s)Mfinal = Opt(FS(m))Table 1.
The PFS algorithm.M)2(1F)1(1FS)1(1iFSMM)1(2iFSM)1(1NFSLselectStep 1 Step m)1(1F)1(1iFMM)1(2iFM)1(1NF)2(1FS)2(2NFS)(mFMmergeStep 2)0(FSplitselect mergeselect)2(2NFMfinal)(mFSoptimizeFigure 1.
Graphic illustration of PFS algorithm.In Table 1, SGC() invokes the SGC algorithm,and Opt() optimizes feature weights.
The func-tions split() and merge() are used to split andmerge the feature space respectively.Two variations of the split() function are in-vestigated in the paper and they are describedbelow:1. random-split: randomly split a featurespace into n- disjoint subspaces, and selectan equal amount of features for each fea-ture subspace.2.
dimension-based-split: split a featurespace into disjoint subspaces based on fea-563ture dimensions/variables, and select thenumber of features for each feature sub-space with a certain distribution.We use a simple method for merge() in theexperiments reported here, i.e., adding togetherthe features from a set of selected feature sub-spaces.One may image other variations of the split()function, such as allowing overlapping sub-spaces.
Other alternatives for merge() are alsopossible, such as randomly grouping the selectedfeature subspaces in the dimension-based split.Due to the limitation of the space, they are notdiscussed here.This approach can in principle be applied toother machine learning algorithms as well.4 Experiments with PFS for Edit Re-gion IdentificationIn this section, we will demonstrate the benefitsof the PFS algorithm for identifying edit regions.The main reason that we use this task is that theedit region detection task uses features from sev-eral levels, including prosodic, lexical, and syn-tactic ones.
It presents a big challenge to find aset of good features from a huge feature space.First we will present the additional featuresthat the PFS algorithm allows us to include.Then, we will briefly introduce the variant of theSwitchboard corpus used in the experiments.
Fi-nally, we will compare results from two variantsof the PFS algorithm.4.1 Edit Region Identification TaskIn spoken utterances, disfluencies, such as self-editing, pauses and repairs, are common phe-nomena.
Charniak and Johnson (2001) and Kahnet al (2005) have shown that improved edit re-gion identification leads to better parsing accu-racy ?
they observe a relative reduction in pars-ing f-score error of 14% (2% absolute) betweenautomatic and oracle edit removal.The focus of our work is to show that our newPFS algorithm enables the exploration of muchlarger feature spaces for edit identification ?
in-cluding prosodic features, their confidencescores, and various feature combinations ?
andconsequently, it further improves edit regionidentification.
Memory limitation prevents usfrom including all of these features in experi-ments using the boosting method described inJohnson and Charniak (2004) and Zhang andWeng (2005).
We couldn?t use the new featureswith the SGC algorithm either for the same rea-son.The features used here are grouped accordingto variables, which define feature sub-spaces asin Charniak and Johnson (2001) and Zhang andWeng (2005).
In this work, we use a total of 62variables, which include 16 1  variables fromCharniak and Johnson (2001) and Johnson andCharniak (2004), an additional 29 variables fromZhang and Weng (2005), 11 hierarchical POS tagvariables, and 8 prosody variables (labels andtheir confidence scores).
Furthermore, we ex-plore 377 combinations of these 62 variables,which include 40 combinations from Zhang andWeng (2005).
The complete list of the variablesis given in Table 2, and the combinations used inthe experiments are given in Table 3.
One addi-tional note is that some features are obtained af-ter the rough copy procedure is performed, wherewe used the same procedure as the one by Zhangand Weng (2005).
For a fair comparison with thework by Kahn et al (2005), word fragment in-formation is retained.4.2 The Re-segmented Switchboard DataIn order to include prosodic features and be ableto compare with the state-oft-art, we use theUniversity of Washington re-segmentedSwitchboard corpus, described in Kahn et al(2005).
In this corpus, the Switchboard sentenceswere segmented into V5-style sentence-like units(SUs) (LDC, 2004).
The resulting sentences fitmore closely with the boundaries that can be de-tected through automatic procedures (e.g., Liu etal., 2005).
Because the edit region identificationresults on the original Switchboard are not di-rectly comparable with the results on the newlysegmented data, the state-of-art results reportedby Charniak and Johnson (2001) and Johnsonand Charniak (2004) are repeated on this newcorpus by Kahn et al (2005).The re-segmented UW Switchboard corpus islabeled with a simplified subset of the ToBI pro-sodic system (Ostendorf et al, 2001).
The threesimplified labels in the subset are p, 1 and 4,where p refers to a general class of disfluentboundaries (e.g., word fragments, abruptly short-ened words, and hesitation); 4 refers to breaklevel 4, which describes a boundary that has aboundary tone and phrase-final lengthening;1 Among the original 18 variables, two variables, Pf and Tfare not used in our experiments, because they are mostlycovered by the other variables.
Partial word flags only con-tribute to 3 features in the final selected feature list.564Categories Variable Name Short DescriptionOrthographicWords W-5, ?
, W+5Words at the current position and the left and right 5positions.Partial Word Flags P-3, ?, P+3Partial word flags at the current position and the leftand right 3 positionsWordsDistance DINTJ, DW, DBigram, DTrigram Distance featuresPOS Tags T-5, ?, T+5POS tags at the current position and the left andright 5 positions.
TagsHierarchicalPOS Tags (HTag) HT-5, ?, HT+5Hierarchical POS tags at the current position and theleft and right 5 positions.HTag Rough Copy Nm, Nn, Ni, Nl, Nr, Ti Hierarchical POS rough copy features.Rough CopyWord Rough Copy WNm, WNi, WNl, WNr Word rough copy features.Prosody Labels PL0, ?, PL3Prosody label with largest post possibility at thecurrent position and the right 3 positions.
ProsodyProsody Scores PC0, ?, PC3Prosody confidence at the current position and theright 3 positions.Table 2.
A complete list of variables used in the experiments.Categories Short Description Number of  CombinationsTags HTagComb Combinations among Hierarchical POS Tags  55Words OrthWordComb Combinations among Orthographic Words 55TagsWTCombWTTComb Combinations of Orthographic Words and POS Tags; Combination among POS Tags 176Rough Copy RCComb Combinations of HTag Rough Copy and Word Rough Copy 55Prosody PComb Combinations among Prosody, and with Words 36Table 3.
All the variable combinations used in the experiments.and 1 is used to include the break index levelsBL 0, 1, 2, and 3.
Since the majority of the cor-pus is labeled via automatic methods, the f-scores for the prosodic labels are not high.
Inparticular, 4 and p have f-scores of about 70%and 60% respectively (Wong et al, 2005).
There-fore, in our experiments, we also take prosodyconfidence scores into consideration.Besides the symbolic prosody labels, the cor-pus preserves the majority of the previously an-notated syntactic information as well as edit re-gion labels.In following experiments, to make the resultscomparable, the same data subsets described inKahn et al (2005) are used for training, develop-ing and testing.4.3 ExperimentsThe best result on the UW Switchboard for editregion identification uses a TAG-based approach(Kahn et al, 2005).
On the original Switchboardcorpus, Zhang and Weng (2005) reported nearly20% better results using the boosting methodwith a much larger feature space 2 .
To allowcomparison with the best past results, we create anew CME baseline with the same set of featuresas that used in Zhang and Weng (2005).We design a number of experiments to test thefollowing hypotheses:1.
PFS can include a huge number of newfeatures, which leads to an overall per-formance improvement.2.
Richer context, represented by the combi-nations of different variables, has a posi-tive impact on performance.3.
When the same feature space is used, PFSperforms equally well as the original SGCalgorithm.The new models from the PFS algorithm aretrained on the training data and tuned on the de-velopment data.
The results of our experimentson the test data are summarized in Table 4.
Thefirst three lines show that the TAG-based ap-proach is outperformed by the new CME base-line (line 3) using all the features in Zhang andWeng (2005).
However, the improvement from2 PFS is not applied to the boosting algorithm at this timebecause it would require significant changes to the availablealgorithm.565Results on test data Feature Space Codes number of features Precision Recall F-ValueTAG-based result on UW-SWBD reported in Kahn et al (2005)    78.20CME with all the variables from Zhang and Weng (2005) 2412382 89.42 71.22 79.29CME with all the variables from Zhang and Weng (2005) + post 2412382 87.15 73.78 79.91+HTag +HTagComb +WTComb +RCComb 17116957 90.44 72.53 80.50+HTag +HTagComb +WTComb +RCComb +PL0 ?
PL3 17116981 88.69 74.01 80.69+HTag +HTagComb +WTComb +RCComb +PComb: without cut 20445375 89.43 73.78 80.86+HTag +HTagComb +WTComb +RCComb +PComb: cut2 19294583 88.95 74.66 81.18+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 19294583 90.37 74.40 81.61+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +post 19294583 86.88 77.29 81.80+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau+post  19294583 87.79 77.02 82.05Table 4.
Summary of experimental results with PFS.CME is significantly smaller than the reportedresults using the boosting method.
In otherwords, using CME instead of boosting incurs aperformance hit.The next four lines in Table 4 show that addi-tional combinations of the feature variables usedin Zhang and Weng (2005) give an absolute im-provement of more than 1%.
This improvementis realized through increasing the search space tomore than 20 million features, 8 times the maxi-mum size that the original boosting and CMEalgorithms are able to handle.Table 4 shows that prosody labels alone makeno difference in performance.
Instead, for eachposition in the sentence, we compute the entropyof the distribution of the labels?
confidencescores.
We normalize the entropy to the range [0,1], according to the formula below:( ) ( )UniformHpHscore ?= 1        (4)Including this feature does result in a goodimprovement.
In the table, cut2 means that weequally divide the feature scores into 10 bucketsand any number below 0.2 is ignored.
The totalcontribution from the combined feature variablesleads to a 1.9% absolute improvement.
This con-firms the first two hypotheses.When Gaussian smoothing (Chen andRosenfeld, 1999), labeled as +Gau, and post-processing (Zhang and Weng, 2005), labeled as+post, are added, we observe 17.66% relativeimprovement (or 3.85% absolute) over the previ-ous best f-score of 78.2 from Kahn et al (2005).To test hypothesis 3, we are constrained to thefeature spaces that both PFS and SGC algorithmscan process.
Therefore, we take all the variablesfrom Zhang and Weng (2005) as the featurespace for the experiments.
The results are listedin Table 5.
We observed no f-score degradationwith PFS.
Surprisingly, the total amount of timePFS spends on selecting its best features issmaller than the time SGC uses in selecting itsbest features.
This confirms our hypothesis 3.Results on test data Split / Non-split Precision Recall F-Valuenon-split 89.42 71.22 79.29split by 4 parts 89.67 71.68 79.67split by 10 parts 89.65 71.29 79.42Table 5.
Comparison between PFS and SGC withall the variables from Zhang and Weng (2005).The last set of experiments for edit identifica-tion is designed to find out what split strategiesPFS algorithm should adopt in order to obtaingood results.
Two different split strategies aretested here.
In all the experiments reported so far,we use 10 random splits, i.e., all the features arerandomly assigned to 10 subsets of equal size.We may also envision a split strategy that dividesthe features based on feature variables (or dimen-sions), such as word-based, tag-based, etc.
Thefour dimensions used in the experiments arelisted as the top categories in Tables 2 and 3, andthe results are given in Table 6.Results on test data SplitCriteriaAllocationCriteria Precision Recall F-ValueRandom Uniform 88.95 74.66 81.18Dimension Uniform 89.78 73.42 80.78Dimension Prior 89.78 74.01 81.14Table 6.
Comparison of split strategies using feature space+HTag+HTagComb+WTComb+RCComb+PComb: cut2In Table 6, the first two columns show criteriafor splitting feature spaces and the number offeatures to be allocated for each group.
Randomand Dimension mean random-split and dimen-sion-based-split, respectively.
When the criterion566is Random, the features are allocated to differentgroups randomly, and each group gets the samenumber of features.
In the case of dimension-based split, we determine the number of featuresallocated for each dimension in two ways.
Whenthe split is Uniform, the same number of featuresis allocated for each dimension.
When the split isPrior, the number of features to be allocated ineach dimension is determined in proportion tothe importance of each dimension.
To determinethe importance, we use the distribution of theselected features from each dimension in themodel ?+ HTag + HTagComb + WTComb +RCComb + PComb: cut2?, namely: Word-based15%, Tag-based 70%, RoughCopy-based 7.5%and Prosody-based 7.5%3.
From the results, wecan see no significant difference between therandom-split and the dimension-based-split.To see whether the improvements are trans-lated into parsing results, we have conducted onemore set of experiments on the UW Switchboardcorpus.
We apply the latest version of Charniak?sparser (2005-08-16) and the same procedure asCharniak and Johnson (2001) and Kahn et al(2005) to the output from our best edit detectorin this paper.
To make it more comparable withthe results in Kahn et al (2005), we repeat thesame experiment with the gold edits, using thelatest parser.
Both results are listed in Table 7.The difference between our best detector and thegold edits in parsing (1.51%) is smaller than thedifference between the TAG-based detector andthe gold edits (1.9%).
In other words, if we usethe gold edits as the upper bound, we see a rela-tive error reduction of 20.5%.Parsing F-scoreMethods Edit  F-scoreReportedin Kahn etal.
(2005)LatestCharniakParserDiff.withOracleOracle 100 86.9 87.92 --Kahn etal.
(2005) 78.2 85.0 -- 1.90PFS bestresults 82.05 -- 86.41 1.51Table 7.
Parsing F-score various different editregion identification results.3 It is a bit of cheating to use the distribution from the se-lected model.
However, even with this distribution, we donot see any improvement over the version with random-split.5 ConclusionThis paper presents our progressive feature selec-tion algorithm that greatly extends the featurespace for conditional maximum entropy model-ing.
The new algorithm is able to select featuresfrom feature space in the order of tens of mil-lions in practice, i.e., 8 times the maximal sizeprevious algorithms are able to process, andunlimited space size in theory.
Experiments onedit region identification task have shown thatthe increased feature space leads to 17.66% rela-tive improvement (or 3.85% absolute) over thebest result reported by Kahn et al (2005), and10.65% relative improvement (or 2.14% abso-lute) over the new baseline SGC algorithm withall the variables from Zhang and Weng (2005).We also show that symbolic prosody labels to-gether with confidence scores are useful in editregion identification task.In addition, the improvements in the edit iden-tification lead to a relative 20% error reduction inparsing disfluent sentences when gold edits areused as the upper bound.AcknowledgementThis work is partly sponsored by a NIST ATPfunding.
The authors would like to express theirmany thanks to Mari Ostendorf and Jeremy Kahnfor providing us with the re-segmented UWSwitchboard Treebank and the correspondingprosodic labels.
Our thanks also go to Jeff Rus-sell for his careful proof reading, and the anony-mous reviewers for their useful comments.
Allthe remaining errors are ours.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Maximum EntropyApproach to Natural Language Processing.
Com-putational Linguistics, 22 (1): 39-71.Eugene Charniak and Mark Johnson.
2001.
Edit De-tection and Parsing for Transcribed Speech.
InProceedings of the 2nd Meeting of the North Ameri-can Chapter of the Association for ComputationalLinguistics, 118-126, Pittsburgh, PA, USA.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd AnnualMeeting of Association for Computational Linguis-tics, 173-180, Ann Arbor, MI, USA.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaus-sian Prior for Smoothing Maximum Entropy Mod-567els.
Technical Report CMUCS-99-108, CarnegieMellon University.John N. Darroch and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-Linear Models.
In Annalsof Mathematical Statistics, 43(5): 1470-1480.Stephen A. Della Pietra, Vincent J. Della Pietra, andJohn Lafferty.
1997.
Inducing Features of RandomFields.
In IEEE Transactions on Pattern Analysisand Machine Intelligence, 19(4): 380-393.Joshua Goodman.
2002.
Sequential Conditional Gen-eralized Iterative Scaling.
In Proceedings of the40th Annual Meeting of Association for Computa-tional Linguistics, 9-16, Philadelphia, PA, USA.Mark Johnson, and Eugene Charniak.
2004.
A TAG-based noisy-channel model of speech repairs.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, 33-39,Barcelona, Spain.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
Effec-tive Use of Prosody in Parsing ConversationalSpeech.
In Proceedings of the 2005 Conference onEmpirical Methods in Natural Language Process-ing, 233-240, Vancouver, Canada.Rob Koeling.
2000.
Chunking with Maximum En-tropy Models.
In Proceedings of the CoNLL-2000and LLL-2000, 139-141, Lisbon, Portugal.LDC.
2004.
Simple MetaData Annotation Specifica-tion.
Technical Report of Linguistic Data Consor-tium.
(http://www.ldc.upenn.edu/Projects/MDE).Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-bara Peskin, Jeremy Ang, Dustin Hillard, Mari Os-tendorf, Marcus Tomalin, Phil Woodland and MaryHarper.
2005.
Structural Metadata Research in theEARS Program.
In Proceedings of the 30thICASSP, volume V, 957-960, Philadelphia, PA,USA.Robert Malouf.
2002.
A Comparison of Algorithmsfor Maximum Entropy Parameter Estimation.
InProceedings of the 6th  Conference on Natural Lan-guage Learning (CoNLL-2002), 49-55, Taibei,Taiwan.Mari Ostendorf, Izhak Shafran, Stefanie Shattuck-Hufnagel, Leslie Charmichael, and William Byrne.2001.
A Prosodically Labeled Database of Sponta-neous Speech.
In Proceedings of the ISCA Work-shop of Prosody in Speech Recognition and Under-standing, 119-121, Red Bank, NJ, USA.Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos.1994.
A Maximum Entropy Model for Preposi-tional Phrase Attachment.
In Proceedings of theARPA Workshop on Human Language Technology,250-255, Plainsboro, NJ, USA.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
AMaximum Entropy Approach to Identifying Sen-tence Boundaries.
In Proceedings of the 5th Con-ference on Applied Natural Language Processing,16-19, Washington D.C., USA.Stefan Riezler and Alexander Vasserman.
2004.
In-cremental Feature Selection and L1 Regularizationfor Relaxed Maximum-entropy Modeling.
In Pro-ceedings of the 2004 Conference on EmpiricalMethods in Natural Language Processing, 174-181, Barcelona, Spain.Robert E. Schapire and Yoram Singer, 1999.
Im-proved Boosting Algorithms Using Confidence-rated Predictions.
Machine Learning, 37(3): 297-336.Elizabeth Shriberg.
1994.
Preliminaries to a Theoryof Speech Disfluencies.
Ph.D. Thesis, University ofCalifornia, Berkeley.Vladimir Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York, NY, USA.Darby Wong, Mari Ostendorf, Jeremy G. Kahn.
2005.Using Weakly Supervised Learning to ImproveProsody Labeling.
Technical Report UWEETR-2005-0003, University of Washington.Qi Zhang and Fuliang Weng.
2005.
Exploring Fea-tures for Identifying Edited Regions in DisfluentSentences.
In Proc.
of the 9th International Work-shop on Parsing Technologies, 179-185, Vancou-ver, Canada.Yaqian Zhou, Fuliang Weng, Lide Wu, and HaukeSchmidt.
2003.
A Fast Algorithm for Feature Se-lection in Conditional Maximum Entropy Model-ing.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Process-ing, 153-159, Sapporo, Japan.568
