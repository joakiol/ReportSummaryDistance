Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145?148,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPHidden Markov Tree Model in Dependency-based Machine Translation?Zden?ek?Zabokrtsk?yCharles University in PragueInstitute of Formal and Applied Linguisticszabokrtsky@ufal.mff.cuni.czMartin PopelCharles University in PragueInstitute of Formal and Applied Linguisticspopel@matfyz.czAbstractWe would like to draw attention to Hid-den Markov Tree Models (HMTM), whichare to our knowledge still unexploited inthe field of Computational Linguistics, inspite of highly successful Hidden Markov(Chain) Models.
In dependency trees,the independence assumptions made byHMTM correspond to the intuition of lin-guistic dependency.
Therefore we suggestto use HMTM and tree-modified Viterbialgorithm for tasks interpretable as label-ing nodes of dependency trees.
In par-ticular, we show that the transfer phasein a Machine Translation system basedon tectogrammatical dependency trees canbe seen as a task suitable for HMTM.When using the HMTM approach forthe English-Czech translation, we reach amoderate improvement over the baseline.1 IntroductionHidden Markov Tree Models (HMTM) were intro-duced in (Crouse et al, 1998) and used in appli-cations such as image segmentation, signal classi-fication, denoising, and image document catego-rization, see (Durand et al, 2004) for references.Although Hidden Markov Models belong to themost successful techniques in Computational Lin-guistics (CL), the HMTM modification remains tothe best of our knowledge unknown in the field.The first novel claim made in this paper is thatthe independence assumptions made by MarkovTree Models can be useful for modeling syntactictrees.
Especially, they fit dependency trees well,because these models assume conditional depen-dence (in the probabilistic sense) only along tree?The work on this project was supported by the grantsMSM 0021620838, GAAV?CR 1ET101120503, and M?SMT?CR LC536.
We thank Jan Haji?c and three anonymous review-ers for many useful comments.edges, which corresponds to intuition behind de-pendency relations (in the linguistic sense) in de-pendency trees.
Moreover, analogously to applica-tions of HMM on sequence labeling, HMTM canbe used for labeling nodes of a dependency tree,interpreted as revealing the hidden states1in thetree nodes, given another (observable) labeling ofthe nodes of the same tree.The second novel claim is that HMTMs aresuitable for modeling the transfer phase in Ma-chine Translation systems based on deep-syntacticdependency trees.
Emission probabilities rep-resent the translation model, whereas transition(edge) probabilities represent the target-languagetree model.
This decomposition can be seen asa tree-shaped analogy to the popular n-gram ap-proaches to Statistical Machine Translation (e.g.
(Koehn et al, 2003)), in which translation and lan-guage models are trainable separately too.
More-over, given the input dependency tree and HMTMparameters, there is a computationally efficientHMTM-modified Viterbi algorithm for finding theglobally optimal target dependency tree.It should be noted that when using HMTM, thesource-language and target-language trees are re-quired to be isomorphic.
Obviously, this is an un-realistic assumption in real translation.
However,we argue that tectogrammatical deep-syntactic de-pendency trees (as introduced in the FunctionalGenerative Description framework, (Sgall, 1967))are relatively close to this requirement, whichmakes the HMTM approach practically testable.As for the related work, one can found a num-ber of experiments with dependency-based MTin the literature, e.g., (Boguslavsky et al, 2004),(Menezes and Richardson, 2001), (Bojar, 2008).However, to our knowledge none of the publishedsystems searches for the optimal target representa-1HMTM looses the HMM?s time and finite automaton in-terpretability, as the observations are not organized linearly.However, the terms ?state?
and ?transition?
are still used.145		  !"!
!# $%&'Figure 1: Tectogrammatical transfer as a task for HMTM.tion in a way similar to HMTM.2 Hidden Markov Tree ModelsHMTM are described very briefly in this section.More detailed information can be found in (Du-rand et al, 2004) and in (Diligenti et al, 2003).Suppose that V = {v1, .
.
.
, v|V |} is the set oftree nodes, r is the root node and ?
is a functionfrom V \r to V storing the parent node of eachnon-root node.
Suppose two sequences of ran-dom variables, X = (X(v1), .
.
.
, X(v|V |)) andY = (Y (v1), .
.
.
, Y (v|V |)), which label all nodesfrom V .
Let X(v) be understood as a hidden stateof the node v, taking a value from a finite statespace S = {s1, .
.
.
, sK}.
Let Y (v) be understoodas a symbol observable on the node v, takinga value from an alphabet K = {k1, .
.
.
, k2}.Analogously to (first-order) HMMs, (first-order)HMTMs make two independence assumptions:(1) given X(?
(v)), X(v) is conditionally inde-pendent of any other nodes, and (2) given X(v),Y (v) is conditionally independent of any othernodes.
Given these independence assumptions,the following factorization formula holds:2P (Y ,X) = P (Y (r)|X(r))P (X(r)) ?
?v?V \rP (Y (v)|X(v))P (X(v)|X(?
(v))) (1)We see that HMTM (analogously to HMM,again) is defined by the following parameters:2In this work we limit ourselves to fully stationaryHMTMs.
This means that the transition and emission prob-abilities are independent of v. This ?node invariance?
is ananalogy to HMM?s time invariance.?
P (X(v)|X(?
(v))) ?
transition probabilitiesbetween the hidden states of two tree-adjacent nodes,3?
P (Y (v)|X(v)) ?
emission probabilities.Naturally the question appears how to restorethe most probable hidden tree labeling given theobserved tree labeling (and given the tree topol-ogy, of course).
As shown in (Durand et al, 2004),a modification of the HMM Viterbi algorithm canbe found for HMTM.
Briefly, the algorithm startsat leaf nodes and continues upwards, storing ineach node for each state and each its child the op-timal downward pointer to the child?s hidden state.When the root is reached, the optimal state tree isretrieved by downward recursion along the point-ers from the optimal root state.3 Tree Transfer as a Task for HMTMHMTM Assumptions from the MT Viewpoint.We suggest to use HMTM in the conventionaltree-based analysis-transfer-synthesis translationscheme: (1) First we analyze an input sentence toa certain level of abstraction on which the sentencerepresentation is tree-shaped.
(2) Then we useHMTM-modified Viterbi algorithm for creatingthe target-language tree from the source-languagetree.
Labels on the source-language nodes aretreated as emitted (observable) symbols, while la-bels on the target-language nodes are understoodas hidden states which are being searched for3The need for parametrizing also P (X(r)) (prior proba-bilites of hidden states in the root node) can be avoided byadding an artificial root whose state is fixed.146(Figure 1).
(3) Finally, we synthesize the target-language sentence from the target-language tree.In the HMTM transfer step, the HMTM emis-sion probabilities can be interpreted as probabil-ities from the ?backward?
(source given target)node-to-node translation model.
HMTM transi-tion probabilities can be interpreted as probabil-ities from the target-language tree model.
This isan important feature from the MT viewpoint, sincethe decomposition into translation model and lan-guage model proved to be extremely useful in sta-tistical MT since (Brown et al, 1993).
It allowsto compensate the lack of parallel resources by therelative abundance of monolingual resources.Another advantage of the HMTM approach isthat it allows us to disregard the ordering of de-cisions made with the individual nodes (whichwould be otherwise nontrivial, as for a given nodethere might be constraints and preferences comingboth from its parent and from its children).
Like inHMM, it is the notion of hidden states that facil-itates ?summarizing?
distributed information andfinding the global optimum.On the other hand, there are several limitationsimplied by HMTMs which we have to consider be-fore applying it to MT: (1) There can be only onelabeling function on the source-language nodes,and one labeling function on the target-languagenodes.
(2) The set of hidden states and the al-phabet of emitted symbols must be finite.
(3) Thesource-language tree and the target-language treeare required to be isomorphic.
In other words, onlynode labeling can be changed in the transfer step.The first two assumption are easy to fulfill, butthe third assumption concerning the tree isomor-phism is problematic.
There is no known linguistictheory guaranteeing identically shaped tree repre-sentations of a sentence and its translation.
How-ever, we would like to show in the following thatthe tectogrammatical layer of language descriptionis close enough to this ideal to make the HMTMapproach practically applicable.Why Tectogrammatical Trees?
Tectogram-matical layer of language description wasintroduced within the Functional GenerativeDescription framework, (Sgall, 1967) and hasbeen further elaborated in the Prague DependencyTreebank project, (Haji?c and others, 2006).On the tectogrammatical layer, each sentence isrepresented as a tectogrammatical tree (t-tree forshort; abbreviations t-node and t-layer are used inthe further text too).
The main features of t-trees(from the viewpoint of our experiments) are fol-lowing.
Each sentence is represented as a depen-dency tree, whose nodes correspond to autoseman-tic (meaningful) words and whose edges corre-spond to syntactic-semantic relations (dependen-cies).
The nodes are labeled with the lemmas ofthe autosemantic words.
Functional words (suchas prepositions, auxiliary verbs, and subordinat-ing conjunctions) do not have nodes of their own.Information conveyed by word inflection or func-tional words in the surface sentence shape is repre-sented by specialized semantic attributes attachedto t-nodes (such as number or tense).T-trees are still language specific (e.g.
be-cause of lemmas), but they largely abstract fromlanguage-specific means of expressing non-lexicalmeanings (such as inflection, agglutination, func-tional words).
Next reason for using t-trees as thetransfer medium is that they allow for a naturaltransfer factorization.
One can separate the trans-fer into three relatively independent channels:4(1)transfer of lexicalization (stored in t-node?s lemmaattribute), (2) transfer of syntactizations (storedin t-node?s formeme attribute),5and (3) transferof semantically indispensable grammatical cate-gories6such as number with nouns and tense withverbs (stored in specialized t-node?s attributes).Another motivation for using t-trees is thatwe believe that local tree contexts in t-treescarry more information relevant for correct lexicalchoice, compared to linear contexts in the surfacesentence shapes, mainly because of long-distancedependencies and coordination structures.Observed Symbols, Hidden States, and HMTMParameters.
The most difficult part of thetectogrammatical transfer step lies in transfer-4Full independence assumption about the three channelswould be inadequate, but it can be at least used for smoothingthe translation probabilities.5Under the term syntactization (the second channel) weunderstand morphosyntactic form ?
how the given lemma is?shaped?
on the surface.
We use the t-node attribute formeme(which is not a genuine element of the semantically ori-ented t-layer, but rather only a technical means that facili-tates modeling the transition between t-trees and surface sen-tence shapes) to capture syntactization of the given t-node,with values such as n:subj ?
semantic noun (s.n.)
in sub-ject position, n:for+X ?
s.n.
with preposition for, n:poss ?possessive form of s.n., v:because+fin ?
semantic verb as asubordinating finite clause introduced by because), adj:attr ?semantic adjective in attributive position.6Categories only imposed by grammatical constraints(e.g.
grammatical number with verbs imposed by subject-verb agreement in Czech) are disregarded on the t-layer.147ring lexicalization and syntactization (attributeslemma and formeme), while the other attributes(node ordering, grammatical number, gender,tense, person, negation, degree of comparisonetc.)
can be transferred by much less complexmethods.
As there can be only one input labelingfunction, we treat the following ordered pair asthe observed symbol: Y (v) = (Lsrc(v), Fsrc(v))where Lsrc(v) is the source-language lemma ofthe node v and Fsrc(v) is its source-languageformeme.
Analogously, hidden state of node v isthe ordered couple X(v) = (Ltrg(v), Ftrg(v)),where Ltrg(v) is the target-language lemma ofthe node v and Ftrg(v) is its target-languageformeme.
Parameters of such HMTM are thenfollowing:P (X(v)|X(?
(v))) = P (Ltrg(v), Ftrg(v)|Ltrg(?
(v)), Ftrg(?(v)))?
probability of a node labeling given its parentlabeling; it can be estimated from a parsedtarget-language monolingual corpus, andP (Y (v)|X(v)) = P (Lsrc(v), Fsrc(v)|Ltrg(v), Ftrg(v))?
backward translation probability; it can be esti-mated from a parsed and aligned parallel corpus.To summarize: the task of tectogrammaticaltransfer can be formulated as revealing the valuesof node labeling functions Ltrgand Ftrggiven thetree topology and given the values of node label-ing functions Lsrcand Fsrc.
Given the HMTMparameters specified above, the task can be solvedusing HMTM-modified Viterbi algorithm by inter-preting the first pair as the hidden state and thesecond pair as the observation.4 ExperimentTo check the real applicability of HMTM transfer,we performed the following preliminary MT ex-periment.
First, we used the tectogrammar-basedMT system described in (?Zabokrtsk?y et al, 2008)as a baseline.7Then we substituted its transferphase by the HMTM variant, with parameters esti-mated from 800 million word Czech corpus and 60million word parallel corpus.
As shown in Table 1,the HMTM approach outperforms the baseline so-lution both in terms of BLEU and NIST metrics.5 ConclusionHMTM is a new approach in the field of CL.
In ouropinion, it has a big potential for modeling syntac-7For evaluation purposes we used 2700 sentences fromthe evaluation section of WMT 2009 Shared TranslationTask.
http://www.statmt.org/wmt09/System BLEU NISTbaseline system 0.0898 4.5672HMTM modification 0.1043 4.8445Table 1: Evaluation of English-Czech translation.tic trees.
To show how it can be used, we appliedHMTM in an experiment on English-Czech tree-based Machine Translation and reached an im-provement over the solution without HMTM.ReferencesIgor Boguslavsky, Leonid Iomdin, and Victor Sizov.2004.
Multilinguality in ETAP-3: Reuse of Lexi-cal Resources.
In Proceedings of Workshop Multi-lingual Linguistic Resources, COLING, pages 7?14.Ond?rej Bojar.
2008.
Exploiting Linguistic Data in Ma-chine Translation.
Ph.D. thesis,?UFAL, MFF UK,Prague, Czech Republic.Peter E. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics.Matthew Crouse, Robert Nowak, and Richard Bara-niuk.
1998.
Wavelet-based statistical signal pro-cessing using hidden markov models.
IEEE Trans-actions on Signal Processing, 46(4):886?902.Michelangelo Diligenti, Paolo Frasconi, and MarcoGori.
2003.
Hidden tree Markov models for doc-ument image classification.
IEEE Transactions onPattern Analysis and Machine Intelligence, 25:2003.Jean-Baptiste Durand, Paulo Goncalv`es, and YannGu?edon.
2004.
Computational methods for hid-den Markov tree models - An application to wavelettrees.
IEEE Transactions on Signal Processing,52(9):2551?2560.Jan Haji?c et al 2006.
Prague Dependency Treebank2.0.
Linguistic Data Consortium, LDC Catalog No.
:LDC2006T01, Philadelphia.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase based translation.
In Pro-ceedings of the HLT/NAACL.Arul Menezes and Stephen D. Richardson.
2001.
Abest-first alignment algorithm for automatic extrac-tion of transfer mappings from bilingual corpora.
InProceedings of the workshop on Data-driven meth-ods in machine translation, volume 14, pages 1?8.Petr Sgall.
1967.
Generativn??
popis jazyka a ?cesk?adeklinace.
Academia, Prague, Czech Republic.Zden?ek?Zabokrtsk?y, Jan Pt?a?cek, and Petr Pajas.
2008.TectoMT: Highly Modular MT System with Tec-togrammatics Used as Transfer Layer.
In Proceed-ings of the 3rd Workshop on SMT, ACL.148
