A Statistical Model for Parsing and Word-Sense DisambiguationDanie l  M.  B ike lDept.
of Computer  & Information ScienceUniversity of Pennsylvania200 South 33rd Street, Philadelphia, PA 19104-6389, U.S.A.db ike l@c is ,  upenn,  eduAbst rac tThis paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigna-tion.
On a new data set we have constructedfor the task, while we were disappointed notto find parsing improvement over a traditionalparsing model, our model achieves a recall of84.0% and a precision of 67.3% of exact synsetmatches on our test corpus, where the goldstandard has a reported inter-annotator agree-ment  of 78.6%.1 I n t roduct ionIn this paper we describe a generative, statis-tical model for simultaneously producing syn-tactic parses and word senses in sentences.We begin by motivating this new approach tothese two, previously-separate problems, then,after reviewing previous work in these areas,we describe our model in detail.
Finally, wewill present he promising results of this, ourfirst attempt, and the direction of future work.2 Mot ivat ion  for  the  Approach2.1 Mot ivat ion f rom examplesConsider the following examples:1.
IBM bought Lotus for $200 million.2.
Sony widened its product line with per-sonal computers.3.
The bank issued a check for $100,000.4.
Apple is expecting \[NP strong results\].5.
IBM expected \[SBAa each employee towear a shirt and tie\].With Example 1, the reading \[IBM bought\[Lotus for $200 million\]\] is nearly impossi-ble, for the simple reason that a monetaryamount is a likely instrument for buying andnot for describing a company.
Similarly, thereis a reasonably strong preference in Example2 for \[pp with personal computers\] to attachto widened, because personal computers areproducts with which a product line could bewidened.
As pointed out by (Stetina and Na-gao, 1997), word sense information can be aproxy for the semantic- and world-knowledgewe as humans bring to bear on attachmentdecisions uch as these.
This proxy effect isdue to the "lightweight semantics" that wordsenses--in particular WordNet word senses--convey.Conversely, both the syntactic and semanticcontext in Example 3 let us know that bankis not a river bank and that check is not arestaurant bill.
In Examples 4 and 5, knowingthat the complement of expect is an NP or anSBAR provides information as to whether thesense is "await" or "require".
Thus, Examples3-5 illustrate how the syntactic ontext of aword can help determine its meaning.2.2 Mot ivat ion  f rom prev ious  work2.2.1 Pars ingIn recent years, the success of statistical pars-ing techniques can be attributed to several fac-tors, such as the increasing size of comput-ing machinery to accommodate larger models,the availability of resources uch as the PennTreebank (Marcus et al, 1993) and the suc-cess of machine learning techniques for lower-level NLP problems, such as part-of-speechtagging (Church, 1988; Brill, 1995), and PP-attachment (Brill and Resnik, 1994; Collinsand Brooks, 1995).
However, perhaps evenmore significant has been the lexicalizationof the grammar formalisms being probabilis-tically modeled: crucially, all the recent, suc-cessful statistical parsers have in some waymade use of bilexical dependencies.
This in-cludes both the parsers that attach probabili-ties to parser moves (Magerman, 1995; Ratna-parkhi, 1997), but also those of the lexicalizedPCFG variety (Collins, 1997; Charniak, 1997).155Even more crucially, the bilexical dependen-cies involve head-modifier relations (hereafterreferred to simply as "head relations").
The in-tuition behind the lexicalization of a grammarformalism is to capture lexical items' idiosyn-cratic parsing preferences.
The intuition be-hind using heads as the members of the bilex-ical relations is twofold.
First, many linguis-tic theories tell us that the head of a phraseprojects the skeleton of that phrase, to be filledin by specifiers, complements and adjuncts;such a notion is captured quite directly bya formalism such as LTAG (Joshi and Sch-abes, 1997).
Second, the head of a phrase usu-ally conveys ome large component of the se-mantics of that phraseJ In this way, usinghead-relation statistics encodes a bit of thepredicate-argument structure in the syntac-tic model.
While there are cases such as Johnwas believed to have been shot by Bill wherestructural preference virtually eliminates oneof the two semantically plausible analyses, itis quite clear that semantics--and, in particu-lar, lexical head semantics--play a very im-portant role in reducing parsing ambiguity.
(See (Collins, 1999), pp.
207ff., for an excel-lent discussion of structural vs. semantic pars-ing preferences, including the above John wasbelieved.., example.
)Another motivation for incorporating wordsenses into a statistical parsing model hasbeen to ameliorate the sparse data prob-lem.
Inspired by the PP-attachment work of(Stetina and Nagao, 1997), we use Word-Net vl.6 (Miller et al, 1990) as our seman-tic dictionary, where the hypernym structureprovides the basis for semantically-motivatedsoft clusters.
2 We discuss this benefit of wordsenses and the details of our implementationfurther in Section 4.2.2.2 Word-sense  d isambiguat ionWhile there has been much work in this area,let us examine the features used in recent1Heads originated this way, but it has become nec-essary to distinguish "semantic" heads, such as nounsand verbs, that correspond roughly to predicates andarguments, from "functional" heads, such as deter-miners, INFL's and complemeutizers, that correspondroughly to logical operators or are purely syntactic el-ements.
In this paper, we almost always intend "head"to mean "semantic head".2Soft clusters are sets where the elements haveweights indicating the strength of their membershipin the set, which in this case allows for a probabilitydistribution to be defined over a word's membershipin all the clusters.statistical approaches.
(Yarowsky, 1992) useswide "bag-of-words" contexts with a naiveBayes classifier.
(Yarowsky, 1995) also useswide context, but incorporates the one-sense-per-discourse and one-sense-per-collocationconstraints, using an unsupervised learn-ing technique.
The supervised technique in(Yarowsky, 1994) has a more specific notionof context, employing not just words that canappear within a window of I k ,  but cruciallywords that abut and fall in the ~2 windowof the target word.
More recently, (Lin, 1997)has shown how syntactic ontext, and depen-dency structures in particular, can be suc-cessfully employed for word sense disambigua-tion.
(Stetina and Nagao, 1997) have shownthat by employing a fairly simple and some-what ad-hoc unsupervised method of WSD us-ing a WordNet-based similarity heuristic, theycould enhance PP-attachment performance toa significantly higher level than systems thatmade no use of lexical semantics (88.1% accu-racy).
Most recently, in (Stetina et al, 1998),the authors made use of head-driven bilexi-cal dependencies with syntactic relations toattack the problem of generalized word-sensedisambiguation, precisely one of the two prob-lems we are dealing with here.3 The  Mode l3.1 Overv iewThe parsing model we started with was ex-tracted from BBN's SIFT system (Miller etal., 1998), which we briefly present again here,using examples from Figure 1 to illustrate themodel's parameters.
3The model generates the head of a con-stituent first, then each of the left- and right-modifiers, generating from the head outward,using a bigram model of node labels.
Here arethe first few elements generated by the modelfor the tree of Figure 1:1.
S and its head word and part of speech,caught- VB D.2.
The head constituent of S, VP.3.
The head word of the VP, caught-VBD.4.
The premodifier constituent ADVP.3We began with the BBN parser because its authorswere kind enough to allow us to extent it, and becauseits design allowed easy integration with our existingWordNet code.156S(caught-VBD)NP(boy-NN) ADVP(also-RB) VP(caught-VBD)DET NN RB VBD NP(ball-NN) I I IThe boy also \[ caught DET NNi Ithe ballFigure 1: A sample sentence with parse tree.5.
The head word of the premodifier ADVP,also-RB.6.
The premodifier constituent NP.7.
The head word of the premodifier NP,boy-NN.8.
The +END+ (null) postmodifier con-stituent of the VP.This process recurses on each of the modifierconstituents (in this case, the subject NP andthe VP) until all words have been generated.
(Note that many words effectively get gener-ated high up in the tree; in this example sen-tence, the last words to get generated are thetwo the's )More formally, the lexicalized PCFG thatsits behind the parsing model has rules of theformFigure 1.
For brevity, we omit the smooth-ing details of BBN's model (see (Miller et al,1998) for a complete description); we note thatall smoothing weights are computed via thetechnique described in (Bikel et al, 1997).The probability of generating p as theroot label is predicted conditioning on only+TOP+,  which is the hidden root of all parsetrees:P(P l  +TOP+) ,  e.g., P(S I + TOP+).
(2)The probability of generating a head node hwith a parent p isP(hlp), e.g., P(VP  I S).
(3)The probability of generating a left-modifier l~isP ~ LnLn- I " "L1HRI" "Rn- iRn  (1)where P,  H, L, and .P~ are all lexicalized non-terminals, i.e., of the form X(w, t, f l ,  where Xis a traditional CFG nonterminal and (w, t, f /is the word-part-of.-speech-word-feature triplthat is the head of the phrase denoted by X.
4The lexicalized nonterminal H is so named be-cause it is the head constztuent, where P inher-its its head triple from this head constituent.The constituents labeled L~ and .R~ are left-and right-modifier constituents, respectively.3.2 P robab i l i ty  s t ruc ture  o f  theoriginal  mode lWe use p to denote the unlexicalized nontermi-nal corresponding to P, and similarly for li, riand h. We now present he top-level genera-tion probabilities, along with examples from4The inclusion of the word feature in the BBNmodel was due to the work described in (Weischedelet al, 1993), where word features helped reduce partof speech ambiguity for unknown words.PL(Iz Ilz-l,p,h, wh),, e.g., (4)PL (NP I ADVP, S, VP, caught)when generating the NP for NP(boy-NN), andthe probability of generating a right modifierr; isPR(r~ I r i- l ,p, h, Wh), e.g., (5)PR(NP I + BEGIN+, VP, VBD, caught)when generating the NP for NP(ball-NN).
5The probabilities for generating lexical el-ements (part-of-speech tags, words and wordfeatures) are as follows.
The part of speechtag of the head of the entire sentence, th, is5The bidden onterminal +BEGIN+ is used to pro-vide a convenient mechanism for determining the ini-tial probability of the underlying Markov process gen-erating the modifying nonterminals; the hidden non-terminal +END+ is used to provide consistency to theunderlying Markov process, i.e., so that the probabil-ities of all possible nonterminal sequences sum to 1.157computed conditioning only on the top-mostsymbol p:6P(th I P).
(6)Part of speech tags of modifier constituents,tt, and tri, are predicted conditioning on themodifier constituent lz or ri, the tag of thehead constituent, h, and the word of the headconstituent, Wh:P(tt, Ill, th, Wh) and P(tr,  \[ri, th, Wh).
(7)The head word of the entire sentence, Wh, ispredicted conditioning only on the top-mostsymbol p and th.P(Wh\[th,p).
(8)Head words of modifier constituents, wl, andwr,, are predicted conditioning on all the con-text used for predicting parts of speech in (7),as well as the parts of speech themslevesP(wt, \]tt,, li, th, Wh)and P(wr, \] try, r~, th, Wh).
(9)The word feature of the head of the entire sen-tence, fh, is predicted conditioning on the top-most symbol p, its head word, wh, and its headtag, th:P(fh \[Wh, th,p).
(10)Finally, the word features for the head wordsof modifier constituents, fz, and fr,, are pre-dicted conditioning on all the context used topredict modifier head words in (9), as well asthe modifier head words themselves:P(ft, I known(wt, ) tt~, li, th, Wh) (11)and P(fr, I known(w~,), tr,, ri, th, Wh)where known(x) is a predicate returning trueif the word x was observed more than 4 timesin the training data.The probability of an entire parse tree is theproduct of the probabifities of generating all ofthe elements of that parse tree, where an el-ement is either a constituent label, a part ofspeech tag, a word or a word feature.
We ob-tain maximum-likelihood estimates of the pa-rameters of this model using frequencies gath-ered from the training data.6This is the one place where we have altered theoriginal model, as the lexical components ofthe headof the entire sentence were all being estimated incor-rectly, causing an inconsistency in the model.
We havecorrected the estimation of th, Wh and fh in our im-plementation.4 Word-sense  Extens ions  to  theLex ica l  Mode lThe desired output structure of our com-bined parser/word-sense disambiguator is astandard, Treebank-style parse tree, where thewords not only have parts ef speech, but alsoWordNet synsets.
Incorporating synsets intothe lexical part of the model is fairly straight-forward: a synset is yet another element o begenerated.
The question is when to generate it.The lexical model has decomposed the genera-tion of the (w, t, f )  triple into three steps, eachconditioning on all the history of the previ-ous step.
While it is probabilistically identicalto predict synsets at any of the four possiblepoints if we continue to condition on all thehistory at each step, we would like to pick thepoint that is most well-founded both in termsof the underlying linguistic structure and interms of what can be well-estimated.
In Sec-tion 2.2.1 we mentioned the soft-clustering as-pect of synsets; in fact, they have a duality.On the one hand, they serve to add specificityto what might otherwise be an ambiguous lexi-cal item; on the other, they are sets, clusteringlexical items that have similar meanings.
Evenfurther, noun and verb synsets form a con-cept taxonomy, the hypernym relation forminga partial ordering on the lemmas containedin WordNet.
The former aspect correspondsroughly to what we as human listeners or read-ers do: we hear or see a sequence of words incontext, and determine incrementally the par-ticular meaning of each of those words.
Thelatter aspect corresponds more closely to amental model of generation: we have a desireor intention to convey, we choose the appropri-ate concepts with which to convey it, and werealize that desire or intention with the mostfelicitous syntactic structure and lexical real-izations of those concepts.
As this is a genera-tive model, we generate a word's synset aftergenerating the part of speech tag but beforegenerating the word itself /The synset of the head of the entire sen-tence, Sh is predicted conditioning only on thetop-most symbol p and the head tag, th:P(Sh\[th,p).
(12)We accordingly changed the probability of7We believe that synsets and parts of speech arelargely orthogonal with respect to tlieir lexical infor-mation, and thus their relative order of prediction wasnot a concern.158generating the head word of the entire sen-tence to beP(Wh I Sh, th,p).
(13)The probability estimates for (12) and (13) arenot smoothed.The probability model for generatingsynsets of modifier constituents mi, completewith smoothing components, i  as follows:P(Sm, I tin,, m~, Wh, Sh) = (14)~0P(Sm, I tm,, m,, w~, sh)+ AlP(SIn, I tm,,m,,sh)+ A2P(sm, It~,,rn,,@l(Sh)).
.
.+ )~n+llS(Sm, \[tm,,mi,@n(Sh))+ ~+2P(Sm, I tin,, m,)+ ~n+3P(Sm, ltm,)where @'(Sh) is the i th hypernym of Sh.
TheWordNet hypernym relations, however, do notform a tree, but a DAG, so whenever there aremultiple hypernyms, the uniformly-weightedmean is taken of the probabilities condition-ing on each of the hypernyms.
That is,P(Sm, I t~,,mi, @3(Sh) = (15)n1 Z P(Sm, Itm,,m~, @~(sh))n k=lwhen@~(~h) = (~(~h) ,  .
.
.
,  @~(sh)}.Note that in the first level of back-off, we nolonger condition on the head word, but strictlyon its synset, and thereafter on hypernyms ofthat synset; these models, then, get at theheart of our approach, which is to abstractaway from lexical head relations, and moveto the more general lexico-semantic relations,here represented by synset relations.Now that we generate synsets for words us-ing (14), we can also change the word genera-tion model to have synsets in its history:P(w~, I sm,, t~,, m,, Wh, Sh) = (16)),0P(wm, Isin,, t~,  mi, wh)+ ~lP(wm, I sin,, t~,  mi, Sh)+ A2P(wm, ISm,,tm,,mi,@l(sh)).
.o+ A~,+lP(wm, I s~,,tm,,m,,@~(Sh))+ ~,~+2.P(w~, I sin,, tin,, m,)+ ,Xn+3P(Wm, ISm,,tm,)?
~n+4P(Wm, \]Sin,)where once again, @i(Sh) is the zth hypernymof Sh.
For both the word and synset predictionmodels, by backing off up the hypernym chain,there is an appropriate confiation of similarhead relations.
For example, if in training theverb phrase \[strike the target\] had been seen, ifthe unseen verb phrase \[attack the target\] ap-peared during testing, then the training fromthe semantically-similar training phrase couldbe used, since this sense of attack is the hy-pernym of this sense of stroke.Finally, we note that both of these synset-and word-prediction probability estimatescontain an enormous number of back-off lev-els for nouns and verbs, corresponding to thehead word's depth in the synset hierarchy.
Avalid concern would be that the model mightbe backing off using histories that are fax toogeneral, so we experimented with limiting thehypernym back-off to only two, three and fourlevels.
This change produced a negligible dif-ference in parsing performance, s5 A New Approach ,  A New DataSetIdeally, the well-established gold standard forsyntax, the Penn Treebank, would have aparallel word-sense-annotated corpus; unfor-tunately, no such word-sense corpus exists.However, we do have SemCor (Miller et al,1994), where every noun, verb, adjective andadverb from a 455k word portion of the BrownCorpus has been assigned a WordNet synset.While all of the Brown Corpus was anno-tated in the style of Treebank I, a great dealwas also more recently annotated in Tree-bank II format, and this corpus has recentlybeen released by the Linguistic Data Con-sortium.
9 As it happens, the intersection be-tween the Treebank-II-annotated Brown andSemCor comprises ome 220k words, most ofwhich is fiction, with some nonfiction and hu-mor writing as well.We went through all 220k words of the cor-pora, synchronizing them.
That is, we madesure that the corpora were identical up tothe spelling of individual tokens, correcting all8We aim to investigate the precise effects of ourback-off strategy in the next version of our combinedparsing/WSD model.9We were given permission to use a pre-release ver-sion of this Treebank II-style corpus.159tokenization and sentence-breaking discrepan-cies.
This correcton task ranged from the sim-ple, such as connecting two sentences in onecorpus that were erroneously broken, to themiddling, such as joining two tokens in Sem-Cor that comprised a hyphenate in Brown, tothe difficult, such as correcting egregious parseannotation errors, or annotating entire sen-tences that were omitted from SemCor.
In par-ticular, the case of hyphenates was quite fre-quent, as it was the default in SemCor to splitup all such words and assign them their indi-vidual word senses (synsets).
In general, we at-tempted to make SemCor look as much as pos-sible like the Treebank II-annotated Brown,and we used the following guidelines for as-signing word senses to hyphenates:1.
Assign the word sense of the head ofthe hyphenate.
E g., both twelve-foot andten-foot get the word sense of foot_ l  (theunit of measure qual to 12 inches).2.
If there is no clear head, then attemptto annotate with the word sense of thehypernym of the senses of the hyphenatecomponents.
E.g., U.S.-Soviet gets theword sense of country_2 (a state or na-tion).3.
If options 1 and 2 are not possible, thehyphenate is split in the Treebank II file.4.
If the hyphenate has the prefix non- oranti-, annotate with the word sense ofthat which follows, with the understand-ing that a post-processing step could re-cover the antonymous word sense, if nec-essary.After three passes through the corpora, theywere perfectly synchronized.
We are seekingpermission to make this data set available toany who already have access to both SemCorand the Treebank II version of Brown.After this synchronization process, wemerged the word-sense annotations ofour cor-rected SemCor with the tokens of our cor-rected version of the Treebank II Brown data.Here we were forced to make two decisions.First, SemCor allows multiple synsets to be as-signed to a particular word; in these cases, wesimply discard all but the first assigned synset.Second, WordNet has collocations, whereasTreebank does not.
To deal with this dis-parity, we re-analyze annotated collocationsas a sequence of separate words that haveall been assigned the same synset as was as-signed the collocation as a whole.
This is notas unreasonable as it may sound; for exam-ple, v ice_pres ident  is a lemma in WordNetand appears in SemCor, so the merged corpushas instances where the word president hasthe synset vice pres ident  l, but only whenpreceded by the word vice.
The cost of thisdecision is an increase in average polysemy.6 T ra in ing  and  Decod ingUsing this merged corpus, actual training ofour model proceeds in an identical fashionto training the non-WordNet-extended model,except that for each lexical relation, the hy-pernym chain of the parent head is followedto derive counts for the various back-off levelsdescribed in Section 4.
We also developed a"plug-'n'-play" lexical model system to facili-tate experimentation with various word- andsynset-prediction models and back-off strate-gies.Even though the model is a top-down, gen-erative one, parsing proceeds bottom-up.
Themodel is searched via a modified version ofCKY, where candidate parse trees that coverthe same span of words are ranked againsteach other.
In the unextended parsing model,the cells corresponding to spans of length oneare seeded with (w,t , f )  triples, with everypossible tag t for a given word zv (the word-feature f is computed eterministically forw);this step introduces the first degree of ambi-guity in the decoding process.
Our WordNet-extended model adds to this initial ambiguity,for each cell is seeded with (w, t, f, s) quadru-ples, with every possible synset s for a givenword-tag pair.During decoding, two forms of pruning areemployed: a beam is applied to each cell in thechart, pruning away all parses whose rankingscore is not within a factor of e -k  of the top-ranked parse, and only the top-ranked n sub-trees are maintained, and the rest are prunedaway.
The "out-of-the-box" BBN program usesvalues of-5 and 25 for k and n, respectively.We changed these to default o -9 and 50, be-cause generating additional unseen items (inour case, synsets) will necessarily ower inter-mediate ranking scores.7 Exper iments  and  Resu l t s7.1 Pars ingInitially, we created a small test set, blindlychoosing the last 117 sentences, or 1%, of160our 220k word corpus, sentences which were,as it happens, from section "r" of the BrownCorpus.
After some disappointing parsingresults using both the regular parser andour WordNet-extended version, we peeked in(Francis and Ku~era, 1979) and discoveredthis was the humor writing section; our ini-tial test corpus was literally a joke.
To cre-ate a more representative t st set, we sam-pled every 100th sentence to create a new liT-sentence test set that spanned the entire rangeof styles in the 220k words; we put all othersentences in the training set.
1?
For the sake ofcomparison, we present results for both testsets (from section "r" and the balanced testset) and both the standard model (Norm) andour WN-extended model (WN-ext) in Table1.11 We note that after we switched to the bal-anced test set, we did not use the "out-of-the-box" version of the BBN parser, as its defaultsettings for pruning away low-count items andthe threshold at which to count a word as "un-known" were too high to yield decent results.Instead, we used precisely the same settings asfor our WordNet-extended version, completewith the larger beam width discussed in theprevious ection32, The reader will note that our extendedmodel performs at roughly the same levelas the unextended version with respect toparsing--a shave better with the "r" test set,and slightly worse on the balanced test set.Recall, however, that this is in spite of addingmore intermediate ambiguity during the de-coding process, and yet using the same beamwidth.
Furthermore, our extensions have oc-curred strictly within the framework of theoriginal model, but we believe that for thetrue advantages of synsets to become appar-ent, we must use trilexical or even tetralex-~?We realize these are very small test sets, but wepresume they are large enough to at least give a goodindicator of performance onthe tasks evaluated.
Theywere kept small to allow for a rapid train-test-analyzecycle, z.e., they were actually used as development testsets.
With the completion of these initial experiments,we are going to designate a proper three-way divsionof training, devtest and test set of this new mergedcorpus.UThe scores in the rows labeled Norm, "r", indicat-ing the performance of the standard BBN model onthe "r" test set, are actually scores based on 116 of the117 sentences, asone sentence did not get parsed ueto a timeout in the program.~2This is partly an unfair comparison, then, sinceours is a larger model, but we wanted to give the stan-dard model every conceivable advantage.Model,test setNorm, '~r"*WN-ext, "r"Norm, balWN-ext, balNorm, "r"*WN-ext, "r"Norm, balWN-ext, bal<40 words tLR LP I ~ 0CB <2CB69.7 72.6 2.93 31.9 55.069.7 72.7 2.86 30.8 56.083.1 85.0 0.82 75.9 85.782.9 84.0 1.02 70.5 81.3All sentencesLR LP CB 0CB <2CB68.6 71.2 3.83 25.9 44.869.7 71.5 3.77 i25.0 45.782.0 84.4 1.00 73 .5  83.880.5 82.2 1.43 !68.4 78.6Table 1: Results for both parsing models onboth test sets.
All results are percentages, ex-cept for those in the CB column.
*See footnote11.S(will)NP(Jane) VP(will)Jane will VP(kill)kill NP(Bob)IBobFigure 2: Head rules are tuned for syntax, notsemantics.ical dependencies.
Whereas such long-rangedependencies might cripple a standard gen-erative model, the soft-clustering aspects ofsynsets hould offset the sparse data problem.As an example of the lack of such dependen-cies, in the current model when predicting theattachment of \[bought company \[for million\]\],there is no current dependence between theverb bought and the object of the prepositionmill ion--a dependence shown to be useful invirtually all the PP attachment work, and par-ticularly in (Stetina and Nagao, 1997).
Re-lated to this issue, we note that the head rules,which were nearly identical to those used in(Collins, 1997), have not been tuned at all tothis task.
For example, in the sentence in Fig-ure 2, the subject Jane is predicted condition-ing on the head of the VP, which is the modalwdl, as opposed to the more semantically-content-rich kill.
So, while the head relationsprovide a very useful structure for many syn-tactic decisions the parser needs to make, it isquite possible that the synset relations of thismodel would require additional or different de-161NounVerbAdjAdvI Recall Precision86.5% 70.9%84.0% 59.5%80.2% 70.4%78.5% ~ 75.8%I T?tal  I 84"0% I 67"3% ITable 2: Word sense disambiguation results forbalanced test set.pendencies that would help in the predictionof correct synsets, and in turn help further e-duce certain syntactic ambiguities, uch as PPattachment.
This is because the "lightweightsemantics" offered by synset relations can pro-vide selectional and world-knowledge r stric-tions that simple lexicalized nonterminal rela-tions cannot.7.2 Word-sense d isambiguat ionThe WSD results on the balanced test set areshown in Table 2.
A few important points mustbe made when evaluating these results.
First,almost all other WSD approaches are aimed atdistinguishing homonyms, as opposed to thetype of fine-grained istinctions that can bemade by WordNet.
Second, almost all otherWSD approaches attempt o disambiguate asmall set of such homonymous terms, whereashere we are attacking the generahzed word-sense disambiguation problem.
Third, we callattention to the fact that SemCor has a re-ported inter-annotator agreement of 78.6%overall, and as low as 70% for words with pol-ysemy of 8 or above (Fellbaum et al, 1998), soit is with this upper bound in mind that onemust consider the precision of any generalizedWSD system.
Finally, we note that the scoresin Table 2 are for exact synset matches; thatis, if our program delivers a synset hat is, say,the hypernym or sibling of the correct answer,no credit is given.While it is tempting to compare these re-sults to those of (Stetina et al, 1998), who re-ported 79.4% overall accuracy on a different,larger test set using their non-discourse model,we note that that was more of an upper-bound study, examining how well a WSD al-gorithm could perform if it had access to gold-standard-perfect parse trees33 By way of fur-ther comparison, that algorithm has a featurespace similar to the synset-prediction compo-1nit is not clear how or why the results of (Stetina etal., 1998) exceeded the reported inter-annotator agree-ment of the entire corpus.nents of our model, but the steps used to rankpossible answers are based largely on heuris-tics; in contrast, our model is based entirelyon maximum-likelihood probability estimates.A final note on the scores of Table 2: giventhe fact that there is not a deterministicmapping between the 50-odd Treebank and4 WordNet parts of speech, when our pro-gram delivers a synset for a WordNet part ofspeech that is different from our gold file, wehave called this a recall error, as this is con-sistent with all other WSD work, where partof speech ambiguity is not a component of analgorithm's precision.8 Future  WorkThis paper represents a first attempt at acombined parsing/word sense disambiguationmodel.
Although it has been very useful towork with the BBN model, we are currentlyimplementing and hope to augment a morestate-of-the-art model, vzz., Models 2 and 3 of(Collins, 1997).
We would also like to explorethe use of a more radical model, where nonter-minals only have synsets as their heads, andwords are generated strictly at the leaves.
Wewould also like to incorporate long-distancecontext in the model as an aid to WSD, ademonstrably effective feature in virtually allthe recent, statistical WSD work.
Also, asmentioned earlier, we believe there are severalfeatures that would allow significant parsingimprovement.
Finally, we would like to inves-tigate the incorporation ofunsupervised meth-ods for WSD, such as the heuristically-basedmethods of (Stetina and Nagao, 1997) and(Stetina et al, 1998), and the theoreticallypurer bootstrapping method of (Yarowsky,1995).
Bolstered by the success of (Stetinaand Nagao, 1997), (Lin, 1997) and especially(Stetina et al, 1998), we believe there is greatpromise the incorporation of word-sense intoa probabilistic parsing model.9 AcknowledgementsI would like to greatly acknowledge the re-searchers at BBN who allowed me to use andabuse their parser and who fostered the begin-ning of this research effort: Scott Miller, LanceRamshaw, Heidi Fox, Sean Boisen and RalphWeischedeh Thanks to Michelle Engel, whohelped enormously with the task of prepar-ing the new data set.
Finally, I would like tothank my advisor Mitch Marcus for his invalu-able technical advice and support.162ReferencesDaniel M. Bikel, Richard Schwartz, RalphWeischedel, and Scott Miller.
1997.
Nymble:A high-performance learning name-finder.
InFzfth Conference on Applied Natural LanguageProcessing, pages 194-201,, Washington, D.C.E.
Brill and P. Resnik.
1994.
A rule-based approach to prepositional phrase attach-ment disambiguation.
In Fifteenth Interna-twnal Conference on Computatwnal Linguzstics(COLING-1994).Eric Brill.
1995.
Transformation-based error-driven learning and natural anguage process-ing: A case study in part-of-speech tagging.Computational Linguistics, 21(4):543-565.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the Fourteenth National Con-ference on Artificial Intelligence, Menlo Park.AAAI Press/MIT Press.Kenneth Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Second Conference on Applzed Natu-ral Language Processing, pages 136-143, Austin,Texas.M.
Collins and J. Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.In Thwd Workshop on Very Large Corpora,pages 27-38.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of ACL-EACL '97, pages 16-23.Michael John Collins.
1999.
Head-Driven Stat~stz-cal Models for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Christiane Fellbaum, Jaochim Grabowski, andShah Landes.
1998.
Performance and confi-dence in a semantic annotation task.
In Chris?tiane Fellbaum, editor, WordNet: An ElectronicLexzeal Database, chapter 9.
MIT Press, Cam-bridge, Massachusetts.W.
N. Francis and H. Ku~era.
1979.
Manual ofInformation to accompany A Standard Corpusof Present-Day Edited American English, foruse with Digital Computers.
Department ofLin-guistics, Brown University, Providence, RhodeIsland.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In A. Salomma andG.
Rosenberg, editors, Handbook of Formal Lan-guages and Automata, volume 3, pages 69-124.Springer-Verlag, Heidelberg.Dekang Lin.
1997.
Using syntactic dependency aslocal context o resolve word sense ambiguity.In Proceedings of the 35th Annual Meeting o/the Assoczation for Computational Linguistics,Madrid, Spain.D.
Magerman.
1995.
Statistical decision treemodels for parsing.
In 33rd Annual Meetingof the Association for Computational Linguis-tics, pages 276-283, Cambridge, Massachusetts.Morgan Kaufmann Publishers.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alaxge annotated corpus of English: The PennTreebank.
Computatzonal Linguistics, 19:313-330.George A. Miller, Richard T. Beckwith, Chris-tiane D. Fellbaum, Derek Gross, and Kather-ine J. Miller.
1990.
WordNet: An on-line lexi-cal database.
Internatwnal Journal of Lexicog-raphy, 3(4):235-244.George A. Miller, Martin Chodorow, Shari Lan-des, Claudia Leacock, and Robert G. Thomas.1994.
Using a semantic oncordance for senseidentification.
In Proceedings o/the ARPA Hu-man Language Technology Workshop.Scott Miller, Heidi Fox, Lance Ramshaw, andRalph Weischedel.
1998.
SIFT - Statistically-derived Information From Text.
In SeventhMessage Understanding Conference (MUC-7),Washington, D.C.Adwait Ratnaparkhi.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
In Proceedzngs of the SecondConference on Empzmcal Methods zn NaturalLanguage Processing, Brown University, Prov-idence, Rhode Island.Jiri Stetina and Makoto Nagao.
1997.
Corpusbased PP attachment ambiguity resolution witha semantic dictionary.
In Fifth Workshop onVery Large Corpora, pages 66-80, Beijing.Jiri Stetina, Sadao Kurohashi, and Makoto Na-gao.
1998.
General word sense disambiguationmethod based on a full sentential context.
InCOLING-ACL '98 Workshop: Usage of Word-Net m Natural Language Processing Systems,Montreal, Canada, August.R.
Weischedel, M. Meteer, R. Schwartz,L.
Ramshaw, and J. Palmucci.
1993.
Copingwith ambiguity and unknown words throughprobabilistic methods.
Computational Linguis-tics, 19(2):359-382.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical models of roget's categoriestrained on large corpora.
In Fourteenth Interna-tional Conference on Computational Linguistics(COLING), pages 454-460.David Yarowsky.
1994.
Decision lists for lexi-cal ambiguity resolution: Application to accentrestoration i Spanish and French.
In Proceed-ings of the 32nd Annual Meeting o/the Assoca-tion for Computational Linguistics, pages 88-95.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting ofthe Association for Computational Linguistics,pages 189-196.
- -163
