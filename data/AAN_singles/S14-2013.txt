Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 104?108,Dublin, Ireland, August 23-24, 2014.ASAP: Automatic Semantic Alignment for PhrasesAna O. AlvesCISUC - University of Coimbraand Polytechnic Institute of CoimbraPortugalana@dei.uc.ptAdriana FerrugentoMariana Lourenc?oFilipe RodriguesCISUC - University of CoimbraPortugal{aferr,mrlouren}@student.dei.uc.ptfmpr@dei.uc.ptAbstractIn this paper we describe the ASAP sys-tem (Automatic Semantic Alignment forPhrases)1which participated on the Task1 at the SemEval-2014 contest (Marelli etal., 2014a).
Our assumption is that STS(Semantic Text Similarity) follows a func-tion considering lexical, syntactic, seman-tic and distributional features.
We demon-strate the learning process of this functionwithout any deep preprocessing achievingan acceptable correlation.1 IntroductionEvaluation of compositional semantic models onfull sentences through semantic relatedness andtextual entailment, title of this task on SemEval,aims to collect systems and approaches ableto predict the difference of meaning betweenphrases and sentences based on their includedwords (Baroni and Zamparelli, 2010; Grefenstetteand Sadrzadeh, 2011; Mitchell and Lapata, 2010;Socher et al., 2012).Our contribution is in the use of complemen-tary features in order to learn the function STS,a part of this challenge.
Rather than specifyingrules, constraints and lexicons manually, we advo-cate a system for automatically acquiring linguis-tic knowledge using machine learning (ML) meth-ods.
For this we apply some preprocessing tech-niques over the training set in order to find differ-ent types of features.
Related to the semantic as-pect, we make use of known semantic relatednessand similarity measures on WordNet, in this case,applied to see the relatedness/similarity betweenphrases from sentences.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1This work was supported by the Crowds project-PTDC/EIA-EIA/115014/2009Considering the problem of modeling a text cor-pus to find short descriptions of documents, weaim an efficient processing of large collectionswhile preserving the essential statistical relation-ships that are useful for, in this case, similarityjudgment.
Therefore we also apply topic model-ing in order to get topic distribution over each sen-tence set.
These features are then used to feed anensemble algorithm to learn the STS function.2 Background2.1 WordNetWordNet (Miller, 1995) is a computational lexiconof English created and maintained at PrincetonUniversity.
It encodes concepts in terms of sets ofsynonyms (called synsets).
A synset can be seenas a set of word senses all expressing the samemeaning.
Each word sense uniquely identifiesa single synset.
For instance, car#n#1 usesthe notation followed by WordNet and subscriptword#p#n where p denotes the part-of-speechtag and n the word?s sense identifier, respec-tively.
In this case, the corresponding synsetcar#n#1, auto#n#1, automobile#n#1,machine#n#6, motorcar#n#1 is uniquelydetermined.
As words are not always so ambigu-ous, a word w#p is said to be monosemous whenit can convey only one meaning.
Alternatively,w#p is polysemous if it can convey more mean-ings each one represented by a sense number s inw#p#s.
For each synset, WordNet provides thefollowing information: A gloss, that is, a textualdefinition of the synset; Semantic relations, whichconnect pairs of synsets.
In this context we focusour attention on the Hypernym/Hyponym relationwhich refers to inheritance between nouns, alsoknown as an is-a, or kind-of relation and theirrespective inverses.
Y is a hypernym of X ifevery X is a (kind of) Y (motor vehicle#n#1 is ahypernym of car#n#1 and, conversely, car#n#1 is104a hyponym of vehicle#n#1).2.2 Semantic similarityThere are mainly two approaches to semantic sim-ilarity.
First approach is making use of a large cor-pus and gathering statistical data from this corpusto estimate a score of semantic similarity.
Secondapproach makes use of the relations and the en-tries of a thesaurus (Lesk, 1986), which is gener-ally a hand-crafted lexical database such as Word-Net (Banerjee and Pedersen, 2003).
Hybrid ap-proaches combines both methods (Jiang and Con-rath, 1997).
Semantic similarity can be seenas a different measure from semantic related-ness since the former compute the proximity be-tween concepts in a given concept hierarchy (e.g.car#n#1 is similar tomotorcycle#n); while thelater the common use of both concepts together(e.g.
car#n#1 is related to tire#n).The Lesk algorithm (Lesk, 1986) uses dictio-nary definitions (glosses) to disambiguate a poly-semous word in a sentence context.
The major ob-jective of his idea is to count the number of wordsthat are shared between two glosses, but, some-times, dictionary glosses are often quite brief, andmay not include sufficient vocabulary to identifyrelated sense.
In this sense, Banerjee and Peder-sen (Banerjee and Pedersen, 2003) adapted this al-gorithm to use WordNet as the dictionary for theword definitions and extended this metric to usethe rich network of relationships between conceptspresent in WordNet.The Jiang and Conrath similarity measure(Jiang and Conrath, 1997) computes the informa-tion shared between two concepts.
The sharedinformation is determined by Information contentof the most specific subsume of the two conceptsin the hierarchy.
Furthermore this measure com-bines the distance between this subsuming conceptand the other two concepts, counting the edge-based distance from them in the WordNet Hyper-nym/Hyponym hierarchy.2.3 Topic ModelingTopic models are based upon the idea that docu-ments are mixtures of topics, where a topic is aprobability distribution over words.
A topic modelis a generative model for documents: it specifiesa simple probabilistic procedure by which docu-ments can be generated.
To make a new document,one chooses a distribution over topics.
Then, foreach word in that document, one chooses a topic atrandom according to this distribution, and draws aword from that topic.Latent Dirichilet allocation (LDA) is a genera-tive probabilistic topic model of a corpus (Blei etal., 2003).
The basic idea is that documents arerepresented as random mixtures over latent top-ics, where each topic is characterized by a distri-bution over words.
This process does not makeany assumptions about the order of words as theyappear in documents.
The only information rel-evant to the model is the number of times wordsare produced.
This is known as the bag-of-wordsassumption.
The main variables of interest in themodel are the topic-word distributions ?
and thetopic distributions ?
for each document.3 Proposed ApproachOur approach to STS is mainly founded on theidea of learning a regression function that com-putes that similarity using other variable/featuresas components.
Before obtaining those features,sentences are preprocessed trough known state-of-the-art Natural Language techniques.
The result-ing preprocessed sentences are then lexically, syn-tactically and semantically decomposed in order toobtain different partial similarities.
These partialsimilarities are the features used in the supervisedlearning.
These specific stages in our system areexplained in detail in the following sections.3.1 Natural Language PreprocessingBefore computing partial similarities consideringdifferent properties of sentences, we need to applysome known Natural Language techniques.
Forthis purpose, we chose OpenNLP2as an open-source tool suite which contains a variety of Java-based NLP components.
Our focus is here on threecore NLP components: tokenization, POS taggingand chunking.
Besides the fact OpenNLP also of-fers a stemmer for English we adopted other im-plementation self-contained in the specific frame-work for Topic Modeling (detailed in section 3.3).OpenNLP is a homogeneous package based ona single machine learning approach, maximum en-tropy (ME) (Berger et al., 1996).
Each OpenNLPtool requires an ME model that contains statis-tics about the components default features com-bining diverse contextual information.
OpenNLPoffers the possibility of both create component oruse pre-built models create for different languages.2http://opennlp.sourceforge.net105On one side, components can be trained and cus-tomizable models are built for the language and/ordomain in study.
On the other, the availabilityof pre-trained models allows the immediate appli-cation of such tools on a new problem.
We fol-lowed the second approach since the sentences areof common-sense and not about a specific domainand are in English3.3.2 Feature EngineeringFeatures, sometimes called attributes, encode in-formation from raw data that allows machinelearning algorithms estimate an unknown value.We focus on, what we call, light features sincethey are completely automatic and unsupervisedcomputed, non-requiring a specific labeled datasetfor this phase.
Each feature is computed as a par-tial similarity metric, which will later feed the pos-terior regression analysis.
This process is fullyautomatized, being all features extracted using apipeline from OpenNLP and other tools that willbe introduced in the specific stage where they areused.
For convenience and an easier identificationin the later machine learning process, we set foreach feature an id in the form f#n, n ?
{1..65}.3.2.1 Lexical FeaturesSome basic similarity metrics are used as featuresrelated exclusively with word forms.
In this setwe include: number of negative words4for eachsentence (f1 and f2 respectively), and the abso-lute value of the difference of these counts (f3 =|f1?
f2|); the absolute value of the difference ofoverlapping words for each sentence pair (f4..7)5.3.2.2 Syntactic FeaturesOpenNLP tokenization, POS (Part-of-Speech)tagging6and text chunking applied on a pipelinefashion allows the identification of (NPs) NounPhrases, VPs (Verbal Phrases) and (PrepositionalPhrases) in sentences.
Heuristically, these NPs are3OpenNLP offers, for the vast majority of components, atleast one pre-trained model for this language.4The Snowball stop word list(Porter, 2001) was used andthose words expressing negation were identified (such as:never, not, neither, no, nobody, aren?t, isn?t, don?t, doesn?t,hasn?t, hadn?t, haven?t)5Thanks to the SemEval organizers in making avail-able the python script which computes baselines com-pute overlap baseline.py which was applied using differentsetting for stop word removal, from 0 to 3.6As alternative models are available, the Maxentmodel with tag dictionary was used on this compo-nent.
Available at http://opennlp.sourceforge.net/models-1.5/en-pos-maxent.binfurther identified as subjects if they are in the be-ginning of sentences.
This kind of shallow parserwill be useful to identify the syntactic structure ofsentences.
Considering only this property, differ-ent features were computed as the absolute valueof the difference of the number of NPs (f8), VPs(f9) and PPs(f10) for each sentence pair.3.2.3 Semantic FeaturesWordNet::Similarity (Pedersen et al., 2004) is afreely available software package for measuringthe semantic similarity or relatedness between apair of concepts (or word senses).
At this stage wehave for each sentence the subject identified as thefirst NP beginning a sentence.This NP can be composed of a simple or com-pound noun, in a root form (lemma) or in ainflected form (plural) (e.g.
electrics or eco-nomic electric cars).
WorNet::Similarity pack-age also contains a lemmatizer, in the mod-ule WordNet::QueryData, which compare a in-flected word form and return all WordNet entrieswhich can be the root form of this word.
Thissearch is made in all four morphological cate-gories in WordNet (Adjectives, Adverbs, Nounsand Verbs), except when indicated the POS inthe end of the queried word, the lemmatizer onlysee in that specific category (e.g.
flies#n re-turns flies#n, fly#n, while flies returns moreentries: flies#n, fly#n, fly#v).
Therefore, alemmatized is successively applied over the Sub-jects found for each pair of sentences.
The com-pound subjects are reduced from left to right untila head noun been found as a valid WordNet en-try (e.g.
the subject economicelectriccars is re-duced until the valid entry electriccar which ispresent on WordNet).After all the subjects been found and a validWordNet entry has been matched semantic simi-larity (f11) (Jiang and Conrath, 1997) and seman-tic relatedness (f12) (Lesk, 1986) is computedfor each sentence pair.
In the case where pairword#n has multiple senses, the one that maxi-mizes partial similarity is selected.3.3 Distributional FeaturesThe distribution of topics over documents (in ourcase, sentences) may contribute to model Distri-butional Semantic in texts since in the way thatthe model is defined, there is no notion of mu-tual exclusivity that restricts words to be part ofone topic only.
This allows topic models to cap-106ture polysemy, where the same word has multiplemeanings.
In this sense we can see topics as nat-ural word sense contexts where words appear indifferent topics with distinct senses.Gensim (?Reh?u?rek and Sojka, 2010) is a machinelearning framework for Topic Modeling whichincludes several preprocessing techniques suchas stop-word removal and TF-IDF.
TF-IDF is astandard statistical method that combines the fre-quency of a term in a particular document with itsinverse document frequency in general use (Saltonand Buckley, 1988).
This score is high for rareterms that appear frequently in a document and aretherefore more likely to be significant.
In a prag-matic view, tf -idft,dassigns to term t a weight indocument d that is: highest when t occurs manytimes within a small number of documents; lowerwhen the term occurs fewer times in a document,or occurs in many documents; lowest when theterm occurs in virtually all documents.Gensim computes a distribution of 25 topicsover sentences not and using TF-IDF (f13...37and f38...63).
Each feature is the absolute valueof the difference of topici(i.e.
topic[i] =|topic[i]s1?
topic[i]s2|).
Euclidean distance overthe difference of topic distribution between sen-tence pairs in each case (without and with TF-IDF)was also considered as a feature (f64 and f65).3.4 Supervised LearningWEKA(Hall et al., 2009) is a large collection ofstate-of-the-art machine learning algorithms writ-ten in Java.
WEKA contains tools for classifica-tion, regression, classifier ensemble, and others.Considering the developer version 3.7.117we usedthe following experiment setup considering the 65features previously computed for both sentencedataset (train and test) (Marelli et al., 2014b).One of four approaches is commonly adoptedfor building classifier ensembles each one focus-ing a different level of action.
Approach A con-cerns the different ways of combining the resultsfrom the classifiers, but there is no evidence thatthis strategy is better than using different mod-els (Approach B).
At feature level (Approach C)different feature subsets can be used for the clas-sifiers, either if they use the same classificationmodel or not.
Finally, the data sets can be modifiedso that each classifier in the ensemble is trained onits own data set (Approach D).7http://www.cs.waikato.ac.nz/ml/weka/downloading.htmlDifferent methods for generating and combin-ing models exist, like Stacking (Seewald, 2002)(Approach B).
These combined models sharesometimes however the disadvantage of being dif-ficult to analyse, once they can comprise dozens ofindividual classifiers.
Stacking is used to combinedifferent types of classifiers and it demands the useof another learner algorithm to predict which ofthe models would be the most reliable for eachcase.
This combination is done using a meta-learner, another learner scheme that combines theoutput of the base learners.
The base learnersare generally called level-0 models, and the meta-learner is a level-1 model.
The predictions of thebase learners are input to the meta-learner.In WEKA, there is a meta classifier called?Stacking?.We use this stacking ensemble com-bining two level-0 models: a K-Nearest Neigh-bour classifier (K = 1) (Aha et al., 1991); anda Linear Regression model without any attributeselection method (?S1) and the ridge parameterby default (1.0 exp?8).
The meta-classifier wasM5P which implements base routines for gener-ating M5 Model trees and rules (Quinlan, 1992;Wang and Witten, 1997).4 Conclusions and Future WorkOur contribution is in the use of complementaryfeatures in order to learn the function of STS, apart of the challenge of building CompositionalDistributional Semantic Models.
For this we ap-plied some preprocessing tasks over the sentenceset in order to find lexical, syntactic, semantic anddistributional features.
On the semantic aspect, wemade use of known semantic relatedness and sim-ilarity measures on WordNet, in this case, appliedto see the relatedness/similarity between phrasesfrom sentences.
We also applied topic modelingin order to get topic distributions over set of sen-tences.
These features were then used to feed anensemble learning algorithm in order to learn theSTS function.
This was achieved with a Pearson?sr of 0.62780.
One direction to follow is to findwhere the ensemble is failing and try to comple-ment the feature set with more semantic features.Indeed, we plan to explore different topic distribu-tion varying number of topics in order to maximizethe log likelihood.
Also we would like to select themost relevant feature from this set.
We are moti-vated after this first participation in continuing toimprove the system here proposed.107ReferencesDavid W. Aha, Dennis Kibler, and Marc K. Albert.1991.
Instance-based learning algorithms.
Mach.Learn., 6(1):37?66.Satanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the 18th International Joint Con-ference on Artificial Intelligence (IJCAI?03), pages805?810, CA, USA.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Represent-ing adjective-noun constructions in semantic space.In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing(EMNLP?10), pages 1183?1193, PA, USA.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Comput.Linguist., 22(1):39?71.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods inNatural Language Processing (EMNLP ?11), pages1394?1404, PA, USA.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explor.
Newsl., 11(1):10?18.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
In Proc.
of the Int?l.
Conf.
on Research inComputational Linguistics, pages 19?33.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceedingsof the 5th Annual International Conference on Sys-tems Documentation (SIGDOC ?86), pages 24?26,NY, USA.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faella Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014a.
Semeval-2014 task 1: Evaluationof compositional distributional semantic models onfull sentences through semantic relatedness and tex-tual entailment.
SemEval-2014.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and RobertomodeZamparelli.
2014b.
A sick cure for the evaluationof compositional distributional semantic models.
InProceedings of LREC 2014.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
COMMUNICATIONS OF THE ACM,38:39?41.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1439.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: Measuring the re-latedness of concepts.
In Demonstration Papers atHLT-NAACL 2004, HLT-NAACL?Demonstrations?04, pages 38?41, PA, USA.Martin F. Porter.
2001.
Snowball: A language forstemming algorithms.
Published online.Ross J. Quinlan.
1992.
Learning with continuousclasses.
In 5th Australian Joint Conference on Ar-tificial Intelligence, pages 343?348, Singapore.Radim?Reh?u?rek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the Workshop on New Chal-lenges for NLP Frameworks (LREC 2010), pages45?50, Valletta, Malta.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.Inf.
Process.
Manage., 24(5):513?523.Alexander K. Seewald.
2002.
How to make stackingbetter and faster while also taking care of an un-known weakness.
In C. Sammut and A. Hoffmann,editors, Nineteenth International Conference on Ma-chine Learning, pages 554?561.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic com-positionality through recursive matrix-vector spaces.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL ?12), pages 1201?1211, PA, USA.Yong Wang and Ian H. Witten.
1997.
Induction ofmodel trees for predicting continuous classes.
InPoster papers of the 9th European Conference onMachine Learning.108
