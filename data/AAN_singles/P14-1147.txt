Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTowards a General Rule for Identifying Deceptive Opinion SpamJiwei Li1, Myle Ott2, Claire Cardie2, Eduard Hovy11Language Technology Institute, Carnegie Mellon University, Pittsburgh, P.A.
15213, USA2Department of Computer Science, Cornell University, Ithaca, N.Y., 14853, USAbdlijiwei@gmail.com, myleott@cs.cornell.educardie@cs.cornell.edu, ehovy@andrew.cmu.eduAbstractConsumers?
purchase decisions are in-creasingly influenced by user-generatedonline reviews.
Accordingly, there hasbeen growing concern about the poten-tial for posting deceptive opinion spam?fictitious reviews that have been deliber-ately written to sound authentic, to de-ceive the reader.
In this paper, we ex-plore generalized approaches for identify-ing online deceptive opinion spam basedon a new gold standard dataset, which iscomprised of data from three different do-mains (i.e.
Hotel, Restaurant, Doctor),each of which contains three types of re-views, i.e.
customer generated truthful re-views, Turker generated deceptive reviewsand employee (domain-expert) generateddeceptive reviews.
Our approach tries tocapture the general difference of languageusage between deceptive and truthful re-views, which we hope will help customerswhen making purchase decisions and re-view portal operators, such as TripAdvisoror Yelp, investigate possible fraudulent ac-tivity on their sites.11 IntroductionConsumers increasingly rely on user-generatedonline reviews when making purchase deci-sion (Cone, 2011; Ipsos, 2012).
Unfortunately,the ease of posting content to the Web, poten-tially anonymously, creates opportunities and in-centives for unscrupulous businesses to post de-ceptive opinion spam?fictitious reviews that aredeliberately written to sound authentic, in order todeceive the reader.2Accordingly, there appears1Dataset available by request from the first author.2Manipulating online reviews may also have legal conse-quences.
For example, the Federal Trade Commission (FTC)to be widespread and growing concern amongboth businesses and the public about this poten-tial abuse (Meyer, 2009; Miller, 2009; Streitfeld,2012; Topping, 2010; Ott, 2013).Existing approaches for spam detection are usu-ally focused on developing supervised learning-based algorithms to help users identify decep-tive opinion spam, which are highly dependentupon high-quality gold-standard labeled data (Jin-dal and Liu, 2008; Jindal et al, 2010; Lim et al,2010; Wang et al, 2011; Wu et al, 2010).
Stud-ies in the literature rely on a couple of approachesfor obtaining labeled data, which usually fall intotwo categories.
The first relies on the judge-ments of human annotators (Jindal et al, 2010;Mukherjee et al, 2012).
However, recent stud-ies show that deceptive opinion spam is not eas-ily identified by human readers (Ott et al, 2011).An alternative approach, as introduced by Ott etal.
(2011), crowdsourced deceptive reviews usingAmazon Mechanical Turk.3A couple of follow-upworks have been introduced based on Ott et al?sdataset, including estimating prevalence of decep-tion in online reviews (Ott et al, 2012), identifica-tion of negative deceptive opinion spam (Ott et al,2013), and identifying manipulated offerings (Liet al, 2013b).Despite the advantages of soliciting deceptivegold-standard material from Turkers (it is easy,large-scale, and affordable), it is unclear whetherTurkers are representative of the general popula-tion that generate fake reviews, or in other words,Ott et al?s data set may correspond to only onetype of online deceptive opinion spam ?
fake re-views generated by people who have never beento offerings or experienced the entities.
Specifi-cally, according to their findings (Ott et al, 2011;has updated their guidelines on the use of endorsements andtestimonials in advertising to suggest that posting deceptivereviews may be unlawful in the United States (FTC, 2009).3http://www.mturk.com1566Li et al, 2013a), truthful hotel reviews encodemore spatial details, characterized by terms suchas ?bathroom?
and ?location?, while deceptive re-views talk about general concepts such as why orwith whom they went to the hotel.
However, ahotel can instead solicit fake reviews from theiremployees or customers who possess substantialdomain knowledge to write fake reviews and en-code more spatial details in their lies.
Indeed,cases have been reported where hotel owners bribeguests in return for good reviews on TripAdvi-sor4, or companies ordered employees to pretendthey were satisfied customers and write glowingreviews of its face-lift procedure on Web sites.5The domain knowledge possessed by domain ex-perts enables them to craft reviews that are muchmore difficult for classifiers to detect, compared tothe crowdsourced fake reviews.Additionally, existing supervised algorithms inthe literature are usually narrowed to one spe-cific domain and heavily rely on domain-specificvocabulary.
For example, classifiers assign highweights to domain-specific terms such as ?hotel?,?rooms?, or even the name of the hotels such as?Hilton?
when trained on reviews on hotels.
Itis unclear whether these classifiers will performwell at detecting deception in other domains, e.g.,Restaurant or Doctor reviews.
Even in a single do-main, e.g., Hotel, classifiers trained from reviewsof one city (e.g., Chicago) may not be effective ifdirectly applied to reviews from other cities (e.g.,New York City) (Li et al, 2013b).
In the exam-ples in Table 1, we trained a linear SVM clas-sifier on Ott?s Chicago-hotel dataset on unigramfeatures and tested it on a couple of different do-mains (the details of data acquisition are illustratedin Section 3).
Good performance is obtained onChicago-hotel reviews (Ott et al, 2011), but not asgood on New York City ones.
The performance isreasonable in Restaurant reviews due to the manyshared properties among restaurants and hotels,but suffers in Doctor settings.In this paper, we try to obtain a deeper under-standing of the general nature of deceptive opin-ion spam.
One contribution of the work presentedhere is the creation of the cross-domain (i.e., Ho-tel, Restaurant and Doctor) gold-standard dataset.4http://www.dailymail.co.uk/travel/article-2013391/Tripadvisor-Hotel-owners-bribe-guests-return-good-reviews.html5http://www.nytimes.com/2009/07/15/technology/internet/15lift.html?_r=0Accuracy Precision Recall F1NYC-Hotel 0.799 0.794 0.758 0.766Chicago-Restaurant 0.785 0.813 0.742 0.778Doctor 0.550 0.537 0.725 0.617Table 1: SVM performance on datasets for a clas-sifier trained on Chicago hotel review based onUnigram feature.In contrast to existing work (Ott et al, 2011; Li etal., 2013b), our new gold standard includes threetypes of reviews: domain expert deceptive opinionspam (Employee), crowdsourced deceptive opin-ion spam (Turker), and truthful Customer reviews(Customer).
In addition, some of domains containboth positive (P) and negative (N) reviews.6To explore the general rule of deceptive opinionspam, we extended SAGE Model (Eisenstein etal., 2011), a bayesian generative approach that cancapture the multiple generative facets (i.e., decep-tive vs truthful, positive vs negative, experiencedvs non-experienced, hotel vs restaurant vs doctor)in the text collection.
We find that more generalfeatures, such as LIWC and POS, are more robustwhen modeled using SAGE, compared with justbag-of-words.We additionally make theoretical contributionsthat may shed light on a longstanding debate in theliterature about deception.
For example, in con-trast to existing findings that highlight the lack ofspatial detail in deceptive reviews (Ott et al, 2011;Li et al, 2013b), we find that a lack of spatial de-tail may not be a universal cue to deception, sinceit does not apply to fake reviews written by domainexperts.
Instead, our finding suggest that other lin-guistic features may offer more robust cues to de-ceptive opinion spam, such as overly highlightedsentiment in the review or the overuse of first-person singular pronouns.The rest of this paper is organized as follows.In Section 2, we briefly go over related work.
Wedescribe the creation of our data set in Section 3and present our model in Section 4.
Experimentalresults are shown in Section 5.
We present anal-ysis of general cues to deception in Section 6 andconclude this paper in Section 7.6For example, a hotel manager could hire people to writepositive reviews to increase the reputation of his own hotelor post negative ones to degrade his competitors.
Identify-ing positive/negative opinion spam is explored in (Ott et al,2011; Ott et al, 2013)15672 Related WorkSpam has been historically studied in the contextsof Web text (Gy?ongyi et al, 2004; Ntoulas et al,2006) or email (Drucker et al, 1999).
Recentlythere has been increasing concern about deceptiveopinion spam (Jindal and Liu, 2008; Ott et al,2011; Wu et al, 2010; Mukherjee et al, 2013b;Wang et al, 2012).Jindal and Liu (2008) first studied the deceptiveopinion problem and trained models using featuresbased on the review text, reviewer, and productto identify duplicate opinions, i.e., opinions thatappear more than once in the corpus with simi-lar contexts.
Wu et al (2010) propose an alter-native strategy to detect deceptive opinion spamin the absence of a gold standard.
Yoo and Gret-zel (2009) gathered 40 truthful and 42 deceptivehotel reviews and manually compare the linguis-tic differences between them.
Ott et al createda gold-standard collection by employing Turkersto write fake reviews, and follow-up research wasbased on their data (Ott et al, 2012; Ott et al,2013; Li et al, 2013b; Feng and Hirst, 2013).
Forexample, Song et al (2012) looked into syntacticfeatures from Context Free Grammar parse treesto improve the classifier performance.
A step fur-ther, Feng and Hirst (2013) make use of degreeof compatibility between the personal experimentand a collection of reference reviews about thesame product rather than simple textual features.In addition to exploring text or linguistic fea-tures in deception, some existing work looksinto customers?
behavior to identify deception(Mukherjee et al, 2013a).
For example, Mukher-jee et al (2011; 2012) delved into group behaviorto identify group of reviewers who work collabo-ratively to write fake reviews.
Qian and Liu (2013)identified multiple user IDs that are generated bythe same author, as these authors are more likelyto generate deceptive reviews.In the psychological literature, researchers havelooked into possible linguistic cues to deception(Newman et al, 2003), such as decreased spatialdetail, which is consistent with theories of realitymonitoring (Johnson and Raye, 1981), increasednegative emotion terms (Newman et al, 2003), orthe writing style difference between informative(truthful) and imaginative (deceptive) writings in(Rayson et al, 2001).
The former typically con-sists of more nouns, adjectives, prepositions, de-terminers, and coordinating conjunctions, whilethe latter consists of more verbs, adverbs, pro-nouns, and pre-determiners.SAGE (Sparse Additive Generative Model):SAGE is an generative bayesian approach in-troduced by Eisenstein et al (2011), whichcan be viewed as an combination of topic mod-els (Blei et al, 2003) and generalized additivemodels (Hastie and Tibshirani, 1990).
Unlikeother derivatives of topic models, SAGE dropsthe Dirichlet-multinomial assumption and adoptsa Laplacian prior, triggering sparsity in topic-worddistribution.
The reason why SAGE is tailored forour task is that SAGE constructs multi-faceted la-tent variable models by simply adding together thecomponent vectors rather than incorporating mul-tiple switching latent variables in multiple facets.3 Dataset ConstructionIn this section, we report our efforts to gather gold-standard opinion spam datasets.
Our datasets con-tain the following domains, namely Hotel, Restau-rant, and Doctor.3.1 Turker set, using Mechanical TurkCrowdsourcing services such as AMT greatly fa-cilitate large-scale data annotation and collectionefforts.
Anyone with basic programming skills cancreate Human Intelligence Tasks (HITs) and ac-cess a marketplace of anonymous online workers(Turkers) willing to complete the tasks.
We bor-rowed some rules used by Ott et al to create theirdataset, such as restricting task to Turkers locatedin the United States, and who maintain an approvalrating of at least 90%.Hotel-Turker : We directly borrowed datasetsfrom Ott7and Li.8Restaurant-Turker : We gathered 20 positive(P) deceptive reviews for each of 10 of the mostpopular restaurants in Chicago, for a total of 200positive deceptive restaurant reviews.Doctor-Turker : We gathered a total number of200 positive reviews from Turkers.3.2 Employee set, by domain expertsWe seek deceptive opinion spam written by peoplewith expert-level domain knowledge.
It is not ap-propriate to use crowdsourcing to obtain this data,7http://myleott.com/op_spam/8http://www.cs.cmu.edu/?jiweil/html/four_city.html1568Turker Expert CustomerHotel (P/N) 400/400 140/140 400/400Restaurant (P/N) 200/0 120/0 200/200Doctor (P/N) 200/0 32/0 200/0Table 2: Statistics for our dataset.so instead we solicit reviews written by employeesin each domain.Hotel-Employee: We asked two hotel employ-ees from each of seven hotels (14 employees to-tal) each to write 10 deceptive positive-sentimentreviews of their own hotel, and 10 deceptivenegative-sentiment reviews of their biggest localcompetitor?s hotel.
In total, we obtained 280 de-ceptive reviews of 14 hotels, including a balancedmix of positive- and negative-sentiment reviews.Restaurant-Employee: We asked employeesfrom selected restaurants (a waiter/waitress orcook) to each write positive-sentiment reviews oftheir restaurant.Doctor-Employee: We asked real doctors towrite positive fake reviews about themselves.
Intotal we obtained 32 reviews from 15 doctors.3.3 Customer set from Actual CustomersHotel-Customer: We borrowed from Ott et al?sdataset.Restaurant/Doctor-Customer: We soliciteddata by matching a set of truthful reviews as Ottet al did in collecting truthful hotel reviews.3.4 Summary for Data CreationStatistics for our data set is presented in Table 2.Due to the difficulty in obtaining gold-standarddata in the literature, there is no doubt that our dataset is not perfect.
Some parts are missing, someare unbalanced, participants in the survey may notbe representative of the general population.
How-ever, as far as we know, this is the most compre-hensive dataset for deceptive opinion spam so far,and may to some extent shed insights on the natureof online deception.4 Feature-based Additive ModelIn this section, we briefly describe our model.Since mathematics are not the main theme of thispaper, we omit the exact details for inference,which can be found in (Eisenstein et al, 2011).Before describing the model in detail, we notethe following advantages of the SAGE model, andour reasons for using it in this paper:1. the ?additive?
nature of SAGE allows a betterunderstanding of which features contributemost to each type of deceptive review andhow much each such feature contributes tothe final decision jointly.
If we instead useSVM, for example, we would have to trainclassifiers one by one (due to the distinct fea-tures from different sources) to draw con-clusions regarding the differences betweenTurker vs Expert vs truthful reviews, positiveexpert vs negative expert reviews, or reviewsfrom different domains.
This would not onlybecome intractable, but would make the con-clusions less clear.2.
For cross-domain classification task, standardmachine learning approaches may suffer dueto domain-specific properties (See Section5.2).4.1 ModelIn SAGE, each termw is drawn from a distributionproportional to exp(m(w)+ ?
(T )(w)yd+ ?(A)(w)zn+?
(I)(w)yd,zn), where m(w)is the observed backgroundterm frequency, ?yd, ?znand ?yd,zndenote the logfrequency deviation representing topic zn, facetyd, and the second-order interaction part respec-tively.
Superscripts T ,A and I respectively denotethe index of the topic, facet, and second-order in-teraction.
In our task, we adapt the SAGE modelas follows:Y = {ySentiment?
{positive, negative},yDomain?
{hotel, restaurant, doctor},ySource?
{employee, turker, customer}}We model three ?
?s, one for each type of y. Leti, j, k denote the index of the different types of y,so that each term w is drawn as follows:P (w|i, j, k) ?
exp(m(w)+ ?(i)(w)ySentiment+?
(j)(w)yDomain+ ?
(k)(w)yScource+ higher order)where the higher order parts denote the interac-tions between different facets.In our approach each document-level feature fis drawn from the following distribution:P (f |i, j, k) ?
exp(m(f)+ ?
(i)(f)ySentiment+ ?
(j)(f)yDomain+ ?
(k)(f)yScource+ higher order)(1)1569where m(f)can be interpreted as the backgroundvalue of feature f .
For each review d, the proba-bility that it is drawn from facets with index i, j, kis as follows:P (d|i, j, k) =?f?dP (f |i, j, k)?w?dP (w|i, j, k) (2)In the training process, parameters ?
(w)yand ?
(f)yare to be learned by maximizing the posteriordistribution following the original SAGE trainingprocedure.
For prediction, we estimate ySourceforeach document given all or part of ?
(w)yand ?
(f)yas follows:ySource=argmaxy?SourceP (d|y?Source, ySentiment, yDomain),where we assume ySentimentand yDomainaregiven for each document d. Note that we as-sume conditional independence between featuresand words given y, similar to other topic mod-els (Blei et al, 2003).
Notably, our revised SAGEmodel degenerates into a model similar to Gen-eralized Additive Model (Hastie and Tibshirani,1990) when word features are not considered.5 ExperimentsIn this section, we report our experimental results.We first restrict experiments to the within-domaintask and see what features most characterize thedeceptive reviews, and how.
We later extend it tocross domains to explore a more general classifierof deceptive opinion spam.5.1 Intra-Domain ClassificationWe explore the effect of both domain expertsand crowdsourcing workers on intra-domain de-ception.
Specifically, we reframe it as a intra-domain multi-class classification task, wheregiven the labeled training data from one domain,we learn a classifier to classify reviews accord-ing to their source, i.e., Employee, Turker andCustomer.
Since the machine learning classi-fier is trained and tested within the same domain,?
(j)(w)yDomainand ?
(i)(f)yDomainare not considered here.We use a One-Versus-Rest (OvR) scheme, inwhich we train m classifiers using SAGE, suchthat each classifier fi, for i ?
[1,m], is trained todistinguish between class i on the one hand, andall classes except i on the other.
To make an m-way decision, we then choose the class c with themost confident prediction.
OvR approaches havebeen shown to produce state-of-art performancecompared to other multi-class approaches such asMultinomial Naive Bayes or One-Versus-One clas-sification scheme.
We train the OvR classifier onthree sets of features, LIWC, Unigram, and POS.9Multi-class classification results are given at Ta-ble 3.
We report both OvR performance and theperformance of three One-versus-One binary clas-sifiers, trained to distinguish between each pairof classes.
In particular, the three-class classifieris around 65% accurate at distinguishing betweenEmployee, Customer, and Turker for each of thedomains using Unigram, significantly higher thanrandom guess.
We also observe that each of thethree One-versus-One binary classifications per-forms significantly better than chance, suggestingthat Employee, Customer, and Turker are in factthree different classes.
In particular, the two-classclassifier is around 0.76 accurate in distinguish-ing between Turker and Employee reviews, de-spite both kinds of reviews being deceptive opin-ion spam.Best performance is achieved on Unigram fea-tures, constantly outperforming LIWC and POSfeatures in both three-class and two-class settingsin the hotel domain.
Similar results are observedfor restaurant and doctor domains and details areexcluded for brevity.
This suggests that a universalset of keyword-based deception cues (e.g., LIWC)is not the best approach for Intra-Domain Classifi-cation.
Similar results were also reported in previ-ous work (Ott et al, 2012; Ott, 2013).5.2 Cross-domain ClassificationIn this subsection, we frame our problem as adomain adaptation task (Pan and Yang, 2010).Again, we explore 3 feature sets: LIWC, Uni-gram and POS.
We train a classifier on hotel re-views, and evaluate the performance on other do-mains.
For simplicity, we focus on truthful (Cus-tomer) versus deceptive (Turker) binary classifi-cation rather than a multi-class classification.We report results from SAGE and SVM10in Ta-ble 4.
We first observe that classifiers trained onhotel reviews apply well in the restaurant domain,which is reasonable due to the many shared prop-9Part-of-speech tags were assigned based on Stan-ford Parser http://nlp.stanford.edu/software/lex-parser.shtml10We use SVMlight (Joachims, 1999) to train our linearSVM classifiers1570Domain Setting FeaturesCustomer Employee TurkerA P R P R P RHotelThree-ClassUnigram 0.664 0.678 0.669 0.589 0.610 0.641 0.582LIWC 0.602 0.617 0.613 0.541 0.598 0.590 0.511POS 0.517 0.532 0.669 0.481 0.479 0.482 0.416Customer vs TurkerUnigram 0.818 0.812 0.840 - - 0.820 0.809LIWC 0.764 0.774 0.771 - - 0.723 0.749POS 0.729 0.748 0.692 - - 0.707 0.759Customer vs EmployeeUnigram 0.799 0.832 0.784 0.804 0.820 - -LIWC 0.732 0.746 0.751 0.714 0.722 - -POS 0.728 0.713 0.742 0.707 0.754 - -Employee vs TurkerUnigram 0.762 - - 0.786 0.806 0.826 0.794LIWC 0.720 - - 0.728 0.726 0.698 0.739POS 0.701 - - 0.688 0.710 0.701 0.697RestaurantThree-ClassUnigram0.647 0.692 0.725 0.625 0.648 0.686 0.702Customer vs Turker 0.817 0.842 0.816 - - 0.804 0.812Customer vs Employee 0.785 0.790 0.814 0.769 0.826 - -Employee vs Turker 0.774 - - 0.784 0.804 0.802 0.763Doctor Customer vs Turker 0.745 0.772 0.701 - - 0.752 0.718Table 3: Within-domain multi-class classifier performance.Model Features Domain A P R F1 Domain A P R F1SVMUnigram Restaurant 0.785 0.813 0.742 0.778 Doctor 0.550 0.537 0.725 0.617LIWC Restaurant 0.745 0.692 0.840 0.759 Doctor 0.521 0.512 0.965 0.669POS Restaurant 0.735 0.697 0.815 0.751 Doctor 0.540 0.521 0.975 0.679SAGEUnigram Restaurant 0.770 0.793 0.750 0.784 Doctor 0.520 0.547 0.705 0.616LIWC Restaurant 0.742 0.728 0.749 0.738 Doctor 0.647 0.650 0.608 0.628POS Restaurant 0.746 0.732 0.687 0.701 Doctor 0.634 0.623 0.682 0.651Table 4: Classifier performance in cross-domain adaptation.erties among restaurants and hotels.
Among threetypes of features, Unigram still performs best.POS and LIWC features are also robust across do-mains.In the doctor domain, we observe that modelstrained on Unigram features from the hotels do-main do not generalize well to doctor reviews, andthe performance is a little bit better than randomguess with only 0.55 accuracy.
For SVM, modelstrained on POS and LIWC features achieve evenlower accuracy than Unigram.
POS and LIWCfeatures obtain around 0.5 precision and 1.0 re-call, indicating that all doctor reviews are classi-fied as deceptive by the classifier.
One plausibleexplanation could be doctor reviews generally en-code some type of positive-weighted (deceptive)features more than hotel reviews and these typesof features dominate the decision making proce-dures, leading all reviews to be classified as de-ceptive.Tables 5 and 6 give the top weighted LIWC andPOS features.
We observe that many features areindeed shared among doctor and hotel domains.Notably, POS features are more robust than LIWCas more shared features are observed.
As domainspecific properties will be considered in the in-teraction part (?LIWCdomainand ?POSdomain) of the addi-LIWC (hotel) LIWC (doctor)deceptive truthful deceptive truthfuli AllPct Sixletters presentfamily number past AllPctpronoun hear work socialSixletters we health shehesee space i numberposemo dash friend timecertain human posemo weleisure exclusive feel youfuture past perceptual negemoperceptual home leisure Periodfeel otherpunct insight relativcomma negemo comma ingestcause dash future moneyTable 5: Top weighted LIWC features for Turkervs Customer in Doctor and Hotel reviews.
Bluedenotes shared positive (deceptive) features andred denotes negative (truthful) features.tive model, SAGE achieve much better results thanSVM, and is around 0.65 accurate in the cross-domain task.6 General Linguistic Cues of DeceptiveOpinion SpamIn this section, we examine a number of generalPOS and LIWC features that may shed light ona general rule for identifying deceptive opinion1571Figure 1: Visualization of the ?
for POS features: Horizontal axes correspond to the values ?
and areNORMALIZED from the log-frequency function.POS (hotel) POS (doctor)deceptive truthful deceptive truthfulPRP$ CD VBD CDPRP RRB NNP VBZVB LRB VB VBPTO CC TO FWNNP NNS VBG RRBVBG RP PRP$ LRBMD VBN JJS RBVBP IN JJ LSRB EX WRB PDTJJS VBZ PRP VBNTable 6: Top weighted POS features for Turker vsCustomer in Doctor and Hotel reviews.
Blue de-notes shared positive (deceptive) features and reddenotes negative (truthful) features.spam.
Our modified SAGE model provides uswith a tailored tool for this analysis.
Specifically,each feature f is associated with a backgroundvaluemf.
For each facetA, ?fA, presents the facet-specific preference value for feature f .
Note thatsentiments are separated into positive and negativedimensions, which is necessary because hotel em-ployee authors wrote positive-sentiment reviewswhen reviewing their own hotels, and negative-sentiment reviews when reviewing their competi-tors?
hotels.6.1 POS featuresEarly findings in the literature (Rayson et al,2001; Buller and Burgoon, 1996; Biber et al,1999) found that informative (truthful) writingstypically consist of more nouns, adjectives, prepo-sitions, determiners, and coordinating conjunc-tions, while imaginative (deceptive) writing con-sist of more verbs, adverbs, pronouns, and pre-determiners (with a few exceptions).
Our find-ings with POS features are largely in agreementwith these findings when distinguishing betweenTurker and Customer reviews, but are violated inthe Employee set.We present the eight types of POS features inFigure 1, namely, N (Noun), JJ (Adjective), IN(Preposition or subordinating conjunction) and DT(Determiner), V (Verb), RB (Adverb), PRP (Pro-nouns, both personal and possessive) and PDT(Pre-Determiner).From Figures 1(a)(b)(e)(f), we observe that withthe exception of PDT, the word frequency ofwhich is too small to draw a conclusion, Turkerand Customer reviews exhibit linguistic patterns inagreement with previous findings in the literature,where truthful reviews (Customer) tend to includemore N, JJ, IN and DT, while deceptive writingstend to encode more V, RB and PRP.However, in the case of the Employee-Positivedataset, which is equally deceptive, most of theserules are violated.
Notably, reviews from theEmployee-Positive set did not encode fewer N, JJand DT terms, as expected (see Figures 1(a)(c)).Instead, they encode even more N, JJ and DTvocabularies than truthful reviews from the Cus-tomer reviews.
Also, fewer V and RB are foundin Employee-Positive reviews compared with Cus-tomer reviews (see Figures 1(e)(g)).One explanation for these observations is thatinformative (truthful) writing tends to be more in-troductory and descriptive, encoding more con-crete details, when compared with imaginary writ-ings.
As domain experts possess considerableknowledge of their own offerings, they highlight1572Figure 2: Visualization of the ?
for LIWC features: Horizontal axes correspond to the values ?
and arenormalized from the log-frequency function.the details and their lies may be even more in-formative and descriptive than those generated byreal customers!
This explains why Employee-Positive contains more N, IN and DT.
Meanwhile,as domain experts are engaged more in talkingabout the details, they inevitably overlook otherinformation, possibly leading to fewer V and RB.For Employee-Positive reviews, shown in Fig-ures 1(d)(h), it turns out that domain experts donot compensate for their lack of prior experiencewhen writing negative reviews for competitors?
of-ferings, as we will see again with LIWC featuresin the next subsection.6.2 LIWC featuresWe explore 3 LIWC categories (from left to rightin subfigures of Figure 2): sentiment (neg emo andpos emo), spatial detail (space), and first-personsingular pronouns (first-person).Space: Note that spatial details are more spe-cific in the Hotel and Restaurant domains,which is reflected in the high positive value of?Hotel,spacedomain(see Figure 2(g)) and negative valueof ?Doctor,spacedomain(see Figure 2(h)).
It illustrates howdomain-specific details can be predictive of decep-tive text.
Similarly predictive LIWC features arehome for the Hotel domain, ingest for the Restau-rant domain, and health and body for the Doctordomain.In Figure 2(i)(j)(k)(l), we can easily see thatboth actual customers and domain experts encodemore spatial details in their reviews (positive valueof ?
), which is in agreement with our expectation.This further demonstrates that a lack of spatial de-tails would not be a general cue for deception.Moreover, it appears that general domain expertisedoes not compensate for the lack of prior experi-ence when writing deceptive negative reviews forcompetitors?
hotels, as demonstrated by the lackof spatial details in the negative-sentiment reviewsby employees shown in Figure 2(k).Sentiment: According to our findings, the pres-ence of sentiment is a general cue to deceptiveopinion spam, as observed when comparing Fig-ure 2(b) to Figure 2(c) and (d).
Participants, bothEmployees and Turkers, tend to exaggerate senti-ment, and include more sentiment-related vocabu-laries in their lies.
In other words, positive decep-tive reviews were generally more positive and neg-ative deceptive reviews were more negative in sen-timent when compared with the truthful reviewsgenerated by actual customers.
A similar patterncan also be observed when comparing Figure 2(i)to Figure 2(j).1573First-Person Singular Pronouns: The litera-ture also associates deception with decreased us-age of first-person singular pronouns, an effect at-tributed to psychological distancing, whereby de-ceivers talk less about themselves due either to alack of personal experience, or to detach them-selves from the lie (Newman et al, 2003; Zhouet al, 2004; Buller et al, 1996; Knapp and Co-maden, 1979).
However, according to our find-ings, we find the opposite to hold.
Increased firstperson singular is an apparent indicator of decep-tion, when comparing Figure 2(b) to 2(c) and 2(e).We suspect that this relates to an effect observedin previous studies of deception, where liars inad-vertently undermine their lies by overemphasizingaspects of their deception that they believe reflectcredibility (Bond and DePaulo, 2006; DePaulo etal., 2003).
One interpretation for this phenomenonwould be that deceivers try to overemphasize theirphysical presence because they believe that this in-creases their credibility.7 Conclusion and DiscussionIn this work, we have developed a multi-domainlarge-scale dataset containing gold-standard de-ceptive opinion spam.
It includes reviews of Ho-tels, Restaurants and Doctors, generated throughcrowdsourcing and domain experts.
We study thisdata using SAGE, which enables us to make ob-servations about the respects in which truthful anddeceptive text differs.
Our model includes sev-eral domain-independent features that shed lighton these differences, which further allows us toformulate some general rules for recognizing de-ceptive opinion spam.We also acknowledge several important caveatsto this work.
By soliciting fake reviews from par-ticipants, including crowd workers and domainexperts, we have found that is possible to de-tect fake reviews with above-chance accuracy, andhave used our models to explore several psycho-logical theories of deception.
However, it is stillvery difficult to estimate the practical impact ofsuch methods, as it is very challenging to obtaingold-standard data in the real world.
Moreover,by soliciting deceptive opinion spam in an arti-ficial environment, we are endorsing the decep-tion, which may influence the cues that we ob-serve (Feeley and others, 1998; Frank and Ekman,1997; Newman et al, 2003; Ott, 2013).
Finally, itmay be possible to train people to tell more con-vincing lies.
Many of the characteristics regard-ing fake review generation might be overcome bywell-trained fake review writers, which would re-sults in opinion spam that is harder for detect.
Fu-ture work may wish to consider some of these ad-ditional challenges.8 AcknowledgementWe thank Wenjie Li and Xun Wang for useful dis-cussions and suggestions.
This work was sup-ported in part by National Science FoundationGrant BCS-0904822, a DARPA Deft grant, as wellas a gift from Google.
We also thank the ACL re-viewers for their helpful comments and advice.ReferencesDouglas Biber, Stig Johansson, Geoffrey Leech, Su-san Conrad, Edward Finegan, and Randolph Quirk.1999.
Longman grammar of spoken and written En-glish, volume 2.
MIT Press.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
the Journal of machineLearning research, 3:993?1022.Charles Bond and Bella DePaulo.
2006.
Accuracy ofdeception judgments.
Personality and Social Psy-chology Review, 10(3):214?234.David B Buller and Judee K Burgoon.
1996.
Inter-personal deception theory.
Communication theory,6(3):203?242.David B Buller, Judee K Burgoon, Aileen Buslig, andJames Roiger.
1996.
Testing interpersonal decep-tion theory: The language of interpersonal decep-tion.
Communication theory, 6(3):268?289.Paul-Alexandru Chirita, J?org Diederich, and WolfgangNejdl.
2005.
Mailrank: using ranking for spamdetection.
In Proceedings of the 14th ACM inter-national conference on Information and knowledgemanagement, pages 373?380.
ACM.Cone.
2011.
2011 Online Influence Trend Tracker.http://www.coneinc.com/negative-reviews-online-reverse-purchase-decisions, August.Bella DePaulo, James Lindsay, Brian Malone, LauraMuhlenbruck, Kelly Charlton, and Harris Cooper.2003.
Cues to deception.
Psychological bulletin,129(1):74.Harris Drucker, Donghui Wu, and Vladimir Vapnik.1999.
Support vector machines for spam catego-rization.
Neural Networks, IEEE Transactions on,10(5):1048?1054.1574Jacob Eisenstein, Amr Ahmed, and Eric P Xing.
2011.Sparse additive generative models of text.
In Pro-ceedings of the 28th International Conference onMachine Learning (ICML-11), pages 1041?1048.Thomas Feeley.
1998.
The behavioral correlates ofsanctioned and unsanctioned deceptive communica-tion.
Journal of Nonverbal Behavior, 22(3):189?204.Vanessa Feng and Graeme Hirst.
2013.
Detecting de-ceptive opinions with profile compatibility.
In Pro-ceedings of the 6th International Joint Conferenceon Natural Language Processing, Nagoya, Japan,pages 14?18.Song Feng, Ritwik Banerjee, and Yejin Choi.
2012.Syntactic stylometry for deception detection.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: ShortPapers-Volume 2, pages 171?175.
Association forComputational Linguistics.Mark Frank and Paul Ekman.
1997.
The ability to de-tect deceit generalizes across different types of high-stake lies.
Journal of personality and social psychol-ogy, 72(6):1429.Zolt?an Gy?ongyi, Hector Garcia-Molina, and Jan Ped-ersen.
2004.
Combating web spam with trustrank.In Proceedings of the Thirtieth international con-ference on Very large data bases-Volume 30, pages576?587.
VLDB Endowment.Trevor J Hastie and Robert J Tibshirani.
1990.
Gener-alized additive models, volume 43.
CRC Press.Ipsos.
2012.
Socialogue: Five Stars?
Thumbs Up?
A+or Just Average?
http://www.ipsos-na.com/news-polls/pressrelease.aspx?id=5929.Nitin Jindal and Bing Liu.
2008.
Opinion spam andanalysis.
In Proceedings of the international con-ference on Web search and web data mining, pages219?230.
ACM.Nitin Jindal, Bing Liu, and Ee-Peng Lim.
2010.
Find-ing unusual review patterns using unexpected rules.In Proceedings of the 19th ACM international con-ference on Information and knowledge management,pages 1549?1552.
ACM.Thorsten Joachims.
1999.
Making large scale svmlearning practical.Marcia K Johnson and Carol L Raye.
1981.
Realitymonitoring.
Psychological review, 88(1):67.Mark Knapp and Mark Comaden.
1979.
Telling it likeit isn?t: A review of theory and research on decep-tive communications.
Human Communication Re-search, 5(3):270?285.Jiwei Li, Claire Cardie, and Sujian Li.
2013a.
Top-icspam: a topic-model-based approach for spam de-tection.
In Proceedings of the 51th Annual Meetingof the Association for Computational Linguis-tics.Jiwei Li, Myle Ott, and Claire Cardie.
2013b.
Iden-tifying manipulated offerings on review portals.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, Seattle,Wash, pages 18?21.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,and Hady Wirawan Lauw.
2010.
Detecting prod-uct review spammers using rating behaviors.
In Pro-ceedings of the 19th ACM international conferenceon Information and knowledge management, pages939?948.
ACM.Juan Martinez-Romo and Lourdes Araujo.
2009.
Webspam identification through language model analy-sis.
In Proceedings of the 5th International Work-shop on Adversarial Information Retrieval on theWeb, pages 21?28.
ACM.David Meyer.
2009.
Fake reviews prompt belkin apol-ogy.Claire Miller.
2009.
Company settles case of reviewsit faked.
New York Times.Arjun Mukherjee, Bing Liu, Junhui Wang, NatalieGlance, and Nitin Jindal.
2011.
Detecting groupreview spam.
In Proceedings of the 20th interna-tional conference companion on World wide web,pages 93?94.
ACM.Arjun Mukherjee, Bing Liu, and Natalie Glance.
2012.Spotting fake reviewer groups in consumer reviews.In Proceedings of the 21st international conferenceon World Wide Web, pages 191?200.
ACM.Arjun Mukherjee, Abhinav Kumar, Bing Liu, JunhuiWang, Meichun Hsu, Malu Castellanos, and Riddhi-man Ghosh.
2013a.
Spotting opinion spammers us-ing behavioral footprints.
In Proceedings of the 19thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 632?640.ACM.Arjun Mukherjee, Vivek Venkataraman, Bing Liu, andNatalie Glance.
2013b.
What yelp fake review fil-ter might be doing.
In Seventh International AAAIConference on Weblogs and Social Media.Matthew L Newman, James W Pennebaker, Diane SBerry, and Jane M Richards.
2003.
Lying words:Predicting deception from linguistic styles.
Person-ality and social psychology bulletin, 29(5):665?675.Alexandros Ntoulas, Marc Najork, Mark Manasse, andDennis Fetterly.
2006.
Detecting spam web pagesthrough content analysis.
In Proceedings of the 15thinternational conference on World Wide Web, pages83?92.
ACM.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.Hancock.
2011.
Finding deceptive opinion spamby any stretch of the imagination.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 309?319.1575Myle Ott, Claire Cardie, and Jeff Hancock.
2012.
Esti-mating the prevalence of deception in online reviewcommunities.
In Proceedings of the 21st interna-tional conference on World Wide Web, pages 201?210.
ACM.Myle Ott, Claire Cardie, and Jeffrey T. Hancock.
2013.Negative deceptive opinion spam.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, Short Papers, At-lanta, Georgia, USA, June.
Association for Compu-tational Linguistics.Myle Ott.
2013.
Computational lingustic models ofdeceptive opinion spam.
PHD, thesis.Sinno Pan and Qiang Yang.
2010.
A survey on transferlearning.
Knowledge and Data Engineering, IEEETransactions on, 22(10):1345?1359.Tieyun Qian and Bing Liu.
2013.
Identifying multipleuserids of the same author.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, Seattle, Wash, pages 18?21.Paul Rayson, Andrew Wilson, and Geoffrey Leech.2001.
Grammatical word class variation withinthe british national corpus sampler.
Language andComputers, 36(1):295?306.David Streitfeld.
2012.
For 2 a star, an online retailergets 5-star product reviews.
New York Times., 26.Alexandra Topping.
2010.
Historian orlando figesagrees to pay damages for fake reviews.
TheGuardian., 16.Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.2011.
Review graph based online store reviewspammer detection.
In Data Mining (ICDM),2011 IEEE 11th International Conference on, pages1242?1247.
IEEE.Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.2012.
Identify online store review spammers via so-cial review graph.
ACM Transactions on IntelligentSystems and Technology (TIST), 3(4):61.Guangyu Wu, Derek Greene, Barry Smyth, and P?adraigCunningham.
2010.
Distortion as a validation cri-terion in the identification of suspicious reviews.
InProceedings of the First Workshop on Social MediaAnalytics, pages 10?13.
ACM.Kyung-Hyan Yoo and Ulrike Gretzel.
2009.
Com-parison of deceptive and truthful travel reviews.In Information and communication technologies intourism 2009, pages 37?47.
Springer.Lina Zhou, Judee K Burgoon, Douglas P Twitchell,Tiantian Qin, and Jay F Nunamaker Jr. 2004.
Acomparison of classification methods for predict-ing deception in computer-mediated communica-tion.
Journal of Management Information Systems,20(4):139?166.1576
