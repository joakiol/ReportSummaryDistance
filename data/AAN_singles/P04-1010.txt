Data-Driven Strategies for an Automated Dialogue SystemHilda HARDY, TomekSTRZALKOWSKI, Min WUILS InstituteUniversity at Albany, SUNY1400 Washington Ave., SS262Albany, NY  12222   USAhhardy|tomek|minwu@cs.albany.eduCristian URSU, Nick WEBBDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello St.Sheffield  S1 4DP   UKc.ursu@sheffield.ac.uk,n.webb@dcs.shef.ac.ukAlan BIERMANN, R. BryceINOUYE, Ashley MCKENZIEDepartment of Computer ScienceDuke UniversityP.O.
Box 90129, Levine ScienceResearch Center, D101Durham, NC  27708   USAawb|rbi|armckenz@cs.duke.eduAbstractWe present a prototype natural-languageproblem-solving application for a financialservices call center, developed as part of theAmiti?s multilingual human-computerdialogue project.
Our automated dialoguesystem, based on empirical evidence from realcall-center conversations, features a data-driven approach that allows for mixedsystem/customer initiative and spontaneousconversation.
Preliminary evaluation resultsindicate efficient dialogues and high usersatisfaction, with performance comparable toor better than that of current conversationaltravel information systems.1 IntroductionRecently there has been a great deal of interest inimproving natural-language human-computerconversation.
Automatic speech recognitioncontinues to improve, and dialogue managementtechniques have progressed beyond menu-drivenprompts and restricted customer responses.
Yetfew researchers have made use of a large body ofhuman-human telephone calls, on which to formthe basis of a data-driven automated system.The Amiti?s project seeks to develop noveltechnologies for building empirically induceddialogue processors to support multilingualhuman-computer interaction, and to integrate thesetechnologies into systems for accessinginformation and services (http://www.dcs.shef.ac.uk/nlp/amities).
Sponsored jointly by the EuropeanCommission and the US Defense AdvancedResearch Projects Agency, the Amiti?s Consortiumincludes partners in both the EU and the US, aswell as financial call centers in the UK and France.A large corpus of recorded, transcribedtelephone conversations between real agents andcustomers gives us a unique opportunity to analyzeand incorporate features of human-humandialogues into our automated system.
(Genericnames and numbers were substituted for allpersonal details in the transcriptions.)
This corpusspans two different application areas: softwaresupport and (a much smaller size) customerbanking.
The banking corpus of several hundredcalls has been collected first and it forms the basisof our initial multilingual triaging application,implemented for English, French and German(Hardy et al, 2003a); as well as our prototypeautomatic financial services system, presented inthis paper, which completes a variety of tasks inEnglish.
The much larger software support corpus(10,000 calls in English and French) is still beingcollected and processed and will be used todevelop the next Amiti?s prototype.We observe that for interactions with structureddata ?
whether these data consist of flightinformation, spare parts, or customer accountinformation ?
domain knowledge need not be builtahead of time.
Rather, methods for handling thedata can arise from the way the data are organized.Once we know the basic data structures, thetransactions, and the protocol to be followed (e.g.,establish caller?s identity before exchangingsensitive information); we need only builddialogue models for handling variousconversational situations, in order to implement adialogue system.
For our corpus, we have used amodified DAMSL tag set (Allen and Core, 1997)to capture the functional layer of the dialogues, anda frame-based semantic scheme to record thesemantic layer (Hardy et al, 2003b).
The ?frames?or transactions in our domain are commoncustomer-service tasks: VerifyId, ChangeAddress,InquireBalance, Lost/StolenCard and MakePayment.
(In this context ?task?
and ?transaction?are synonymous.)
Each frame is associated withattributes or slots that must be filled with values inno particular order during the course of thedialogue; for example, account number, name,payment amount, etc.2 Related WorkRelevant human-computer dialogue researchefforts include the TRAINS project and theDARPA Communicator program.The classic TRAINS natural-language dialogueproject (Allen et al, 1995) is a plan-based systemwhich requires a detailed model of the domain andtherefore cannot be used for a wide-rangingapplication such as financial services.The US DARPA Communicator program hasbeen instrumental in bringing about practicalimplementations of spoken dialogue systems.Systems developed under this program includeCMU?s script-based dialogue manager, in whichthe travel itinerary is a hierarchical composition offrames (Xu and Rudnicky, 2000).
The AT&Tmixed-initiative system uses a sequential decisionprocess model, based on concepts of dialog stateand dialog actions (Levin et al, 2000).
MIT?sMercury flight reservation system uses a dialoguecontrol strategy based on a set of ordered rules as amechanism to manage complex interactions(Seneff and Polifroni, 2000).
CU?s dialoguemanager is event-driven, using a set of hierarchicalforms with prompts associated with fields in theforms.
Decisions are based not on scripts but oncurrent context (Ward and Pellom, 1999).Our data-driven strategy is similar in spirit tothat of CU.
We take a statistical approach, inwhich a large body of transcribed, annotatedconversations forms the basis for taskidentification, dialogue act recognition, and formfilling for task completion.3 System Architecture and ComponentsThe Amiti?s system uses the GalaxyCommunicator Software Infrastructure (Seneff etal., 1998).
Galaxy is a distributed, message-based,hub-and-spoke infrastructure, optimized for spokendialogue systems.Figure 1.
Amiti?s System ArchitectureComponents in the Amiti?s system (Figure 1)include a telephony server, automatic speechrecognizer, natural language understanding unit,dialogue manager, database interface server,response generator, and text-to-speech conversion.3.1 Audio ComponentsAudio components for the Amiti?s system areprovided by LIMSI.
Because acoustic models havenot yet been trained, the current demonstratorsystem uses a Nuance ASR engine and TTSVocalizer.To enhance ASR performance, we integratedstatic GSL (Grammar Specification Language)grammar classes provided by Nuance forrecognizing several high-frequency items:numbers, dates, money amounts, names and yes-nostatements.Training data for the recognizer were collectedboth from our corpus of human-human dialoguesand from dialogues gathered using a text-basedversion of the human-computer system.
Using thisversion we collected around 100 dialogues andannotated important domain-specific information,as in this example: ?Hi my name is [fname ;David] [lname ; Oconnor] and my account numberis [account ; 278 one nine five].
?Next we replaced these annotated entities withgrammar classes.
We also utilized utterances fromthe Amiti?s banking corpus (Hardy et al, 2002) inwhich the customer specifies his/her desired task,as well as utterances which constitute common,domain-independent speech acts such asacceptances, rejections, and indications of non-understanding.
These were also used for trainingthe task identifier and the dialogue act classifier(Section 3.3.2).
The training corpus for therecognizer consists of 1744 utterances totalingaround 10,000 words.Using tools supplied by Nuance for buildingrecognition packages, we created two speechrecognition components: a British model in the UKand an American model at two US sites.For the text to speech synthesizer we usedNuance?s Vocalizer 3.0, which supports multiplelanguages and accents.
We integrated theVocalizer and the ASR using Nuance?s speech andtelephony API into a Galaxy-compliant serveraccessible over a telephone line.3.2 Natural Language UnderstandingThe goal of the language understandingcomponent is to take the word string output of theASR module, and identify key semantic conceptsrelating to the target domain.
This is a specializedkind of information extraction application, and assuch, we have adapted existing IE technology tothis task.HubSpeechRecognitionDialogueManager DatabaseServerNat?l LanguageUnderstandingTelephonyServerResponseGenerationCustomerDatabaseText-to-speechConversionWe have used a modified version of the ANNIEengine (A Nearly-New IE system; Cunningham etal., 2002; Maynard, 2003).
ANNIE is distributed asthe default built-in IE component of the GATEframework (Cunningham et al, 2002).
GATE is apure Java-based architecture developed over thepast eight years in the University of SheffieldNatural Language Processing group.
ANNIE hasbeen used for many language processingapplications, in a number of languages bothEuropean and non-European.
This versatilitymakes it an attractive proposition for use in amultilingual speech processing project.ANNIE includes customizable componentsnecessary to complete the IE task ?
tokenizer,gazetteer, sentence splitter, part of speech taggerand a named entity recognizer based on a powerfulengine named JAPE (Java Annotation PatternEngine; Cunningham et al, 2000).Given an utterance from the user, the NLU unitproduces both a list of tokens for detectingdialogue acts, an important research goal insidethis project, and a frame with the possible namedentities specified by our application.
We areinterested particularly in account numbers, creditcard numbers, person names, dates, amounts ofmoney, locations, addresses and telephonenumbers.In order to recognize these, we have updated thegazetteer, which works by explicit look-up tablesof potential candidates, and modified the rules ofthe transducer engine, which attempts to matchnew instances of named entities based on localgrammatical context.
There are some significantdifferences between the kind of prose text moretypically associated with information extraction,and the kind of text we are expecting to encounter.Current models of IE rely heavily on punctuationas well as certain orthographic information, such ascapitalized words indicating the presence of aname, company or location.
We have access toneither of these in the output of the ASR engine,and so had to retune our processors to data whichreflected that.In addition, we created new processingresources, such as those required to spot numberunits and translate them into textual representationsof numerical values; for example, to take ?twentythousand one hundred and fourteen pounds?, andproduce ??20,114?.
The ability to do this is ofcourse vital for the performance of the system.If none of the main entities can be identifiedfrom the token string, we create a list of possiblefallback entities, in the hope that partial matchingwould help narrow the search space.For instance, if a six-digit account number is notidentified, then the incomplete number recognizedin the utterance is used as a fallback entity and sentto the database server for partial matching.Our robust IE techniques have provedinvaluable to the efficiency and spontaneity of ourdata-driven dialogue system.
In a single utterancethe user is free to supply several values forattributes, prompted or unprompted, allowing tasksto be completed with fewer dialogue turns.3.3 Dialogue ManagerThe dialogue manager identifies the goals of theconversation and performs interactions to achievethose goals.
Several ?Frame Agents?, implementedwithin the dialogue manager, handle tasks such asverifying the customer?s identity, identifying thecustomer?s desired transaction, and executing thosetransactions.
These range from a simple balanceinquiry to the more complex change of address anddebit-card payment.
The structure of the dialoguemanager is illustrated in Figure 2.Rather than depending on a script for theprogression of the dialogue, the dialogue managertakes a data-driven approach, allowing the caller totake the initiative.
Completing a task depends onidentifying that task and filling values in frames,but this may be done in a variety of ways: one at atime, or several at once, and in any order.For example, if the customer identifies himselfor herself before stating the transaction, or even ifhe or she provides several pieces of information inone utterance?transaction, name, account number,payment amount?the dialogue manager is flexibleenough to move ahead after these variations.Prompts for attributes, if needed, are not restrictedto one at a time, but they are usually combined inthe way human agents request them; for example,city and county, expiration date and issue number,birthdate and telephone number.Figure 2.
Amiti?s Dialogue ManagerIf the system fails to obtain the necessary valuesfrom the user, reprompts are used, but no morethan once for any single attribute.
For the customerverification task, different attributes may beResponse DecisionInput:from NLU viaHub (token string,language id,named entities)Task infoExternal files,domain-specificDialogue ActClassifierFrame AgentTask IDFrame AgentVerify-CallerFrame AgentDB ServerCustomerDatabaseTask ExecutionFrame Agents via HubDialogue Historyrequested.
If the system fails even after reprompts,it will gracefully give up with an explanation suchas, ?I?m sorry, we have not been able to obtain theinformation necessary to update your address inour records.
Please hold while I transfer you to acustomer service representative.
?3.3.1 Task ID Frame AgentFor task identification, the Amiti?s team hasmade use of the data collected in over 500conversations from a British call center, recorded,transcribed, and annotated.
Adapting a vector-based approach reported by Chu-Carroll andCarpenter (1999), the Task ID Frame Agent isdomain-independent and automatically trained.Tasks are represented as vectors of terms, builtfrom the utterances requesting them.
Someexamples of labeled utterances are: ?Erm I'd like tocancel the account cover premium that's on my,appeared on my statement?
[CancelInsurance] and?Erm just to report a lost card please?
[Lost/StolenCard].The training process proceeds as follows:1.
Begin with corpus of transcribed, annotatedcalls.2.
Document creation: For each transaction, collectraw text of callers?
queries.
Yield: one?document?
for each transaction (about 14 ofthese in our corpus).3.
Text processing: Remove stopwords, stemcontent words, weight terms by frequency.Yield: one ?document vector?
for each task.4.
Compare queries and documents: Create ?queryvectors.?
Obtain a cosine similarity score foreach query/document pair.
Yield: cosinescores/routing values for each query/documentpair.5.
Obtain coefficients for scoring: Use binarylogistic regression.
Yield: a set of coefficientsfor each task.Next, the Task ID Frame Agent is tested onunseen utterances or queries:1.
Begin with one or more user queries.2.
Text processing: Remove stopwords, stemcontent words, weight terms (constant weights).Yield: ?query vectors?.3.
Compare each query with each document.Yield: cosine similarity scores.4.
Compute confidence scores (use trainingcoefficients).
Yield: confidence scores,representing the system?s confidence that thequeries indicate the user?s choice of a particulartransaction.Tests performed over the entire corpus, 80% ofwhich was used for training and 20% for testing,resulted in a classification accuracy rate of 85%(correct task is one of the system?s top 2 choices).The accuracy rate rises to 93% when we eliminateconfusing or lengthy utterances, such as requestsfor information about payments, statements, andgeneral questions about a customer?s account.These can be difficult even for human annotatorsto classify.3.3.2 Dialogue Act ClassifierThe purpose of the DA Classifier Frame Agentis to identify a caller?s utterance as one or moredomain-independent dialogue acts.
These includeAccept, Reject, Non-understanding, Opening,Closing, Backchannel, and Expression.
Clearly, itis useful for a dialogue system to be able toidentify accurately the various ways a person maysay ?yes?, ?no?, or ?what did you say??
As withthe task identifier, we have trained the DAclassifier on our corpus of transcribed, labeledhuman-human calls, and we have used vector-based classification techniques.
Two differencesfrom the task identifier are 1) an utterance mayhave multiple correct classifications, and 2) adifferent stoplist is necessary.
Here we can filterout the usual stops, including speech dysfluencies,proper names, number words, and words withdigits; but we need to include words such as yeah,uh-huh, hi, ok, thanks, pardon and sorry.Some examples of DA classification results areshown in Figure 3.
For sure, ok, the classifierreturns the categories Backchannel, Expression andAccept.
If the dialogue manager is looking foreither Accept or Reject, it can ignore Backchanneland Expression in order to detect the correctclassification.
In the case of certainly not, the firstword has a strong tendency toward Accept, thoughboth together constitute a Reject act.Text: ?sure, okay?
Text: ?certainly not?Categories returned: Backchannel,Expression, AcceptCategories returned:Reject, AcceptExpressionClosingAcceptBack.00.20.40.60.81Top four cosine scoresExpressionAccept ClosingBack.00.10.20.30.40.50.60.7Confidence scoresRejectReject-partAccept Expression00.10.20.30.40.50.6Top four cosine scoresRejectAccept ExpressionReject-part00.10.20.30.40.50.60.7Confidence scoresFigure 3.
DA Classification examplesOur classifier performs well if the utterance isshort and falls into one of the selected categories(86% accuracy on the British data); and it has theadvantages of automatic training, domainindependence, and the ability to capture a greatvariety of expressions.
However, it can beinaccurate when applied to longer utterances, and itis not yet equipped to handle domain-specificassertions, questions, or queries about atransaction.3.4 Database ManagerOur system identifies users by matchinginformation provided by the caller against adatabase of user information.
It assumes that thespeech recognizer will make errors when the callerattempts to identify himself.
Therefore perfectmatches with the database entries will be rare.Consequently, for each record in the database, weattach a measure of the probability that the recordis the target record.
Initially, these measures areestimates of the probability that this individual willcall.
When additional identifying informationarrives, the system updates these probabilitiesusing Bayes?
rule.Thus, the system might begin with a uniformprobability estimate across all database records.
Ifthe user identifies herself with a name recognizedby the machine as ?Smith?, the system willappropriately increment the probabilities of allentries with the name ?Smith?
and all entries thatare known to be confused with ?Smith?
inproportion to their observed rate of substitution.
Ofcourse, all records not observed to be soconfusable would similarly have their probabilitiesdecreased by Bayes?
rule.
When enoughinformation has come in to raise the probability forsome record above a threshold (in our system 0.99probability), the system assumes that the caller hasbeen correctly identified.
The designer may chooseto include a verification dialog, but our decisionwas to minimize such interactions to shorten thecalls.Our error-correcting database system receivestokens with an identification of what field eachtoken should represent.
The system processes thetokens serially.
Each represents an observationmade by the speech recognizer.
To process a token,the system examines each record in the databaseand updates the probability that the record is thetarget record using Bayes?
rule:where rec is the event where the record underconsideration is the target record.As is common in Bayes?
rule calculations, thedenominator P(obs) is treated as a scaling factor,and is not calculated explicitly.
All probabilitiesare renormalized at the end of the update of all ofthe records.
P(rec) is the previous estimate of theprobability that the record is the target record.P(obs|rec) is the probability that the recognizerreturned the observation that it did given that thetarget record is the current record underexamination.
For some of the fields, such as theaccount number and telephone number, the userresponses consist of digits.
We collected data onthe probability that the speech recognition systemwe are using mistook one digit for another andcalculated the values for P(obs|rec) from the data.For fields involving place names and personalnames, the probabilities were estimated.Once a record has been selected (by virtue of itsprobability being greater than the threshold) thesystem compares the individual fields of the recordwith values obtained by the speech recognizer.
Ifthe values differ greatly, as measured by theirLevenshtein distance, the system returns the fieldname to the dialogue manager as a candidate foradditional verification.
If no record meets thethreshold probability criterion, the system returnsthe most probable record to the dialogue manager,along with the fields which have the greatestLevenshtein distance between the recognized andactual values, as candidates for reprompting.Our database contains 100 entries for the systemtests described in this paper.
We describe thesystem in a more demanding environment with onemillion records in Inouye et al (2004).
In thatproject, we required all information to be enteredby spelling the items out so that the vocabularywas limited to the alphabet plus the ten digits.
Inthe current project, with fewer names to deal with,we allowed the complete vocabulary of thedomain: names, streets, counties, and so forth.3.5 Response GeneratorOur current English-only system preserves thelanguage-independent features of our original tri-lingual generator, storing all language- anddomain-specific information in separate text files.It is a template-based system, easily modified andextended.
The generator constructs utterancesaccording to the dialogue manager?s specificationof one or more speech acts (prompt, request,confirm, respond, inform, backchannel, accept,reject), repetition numbers, and optional lists ofattributes, values, and/or the person?s name.
As faras possible, we modeled utterances after thehuman-human dialogues.For a more natural-sounding system, wecollected variations of the utterances, which thegenerator selects at random.
Requests, forexample, may take one of twelve possible forms:Request, part 1 of 2:Can you just confirm | Can I have | Can I take |What is | What?s | May I have)()()|()|(obsPrecPrecobsPobsrecP ?=Request, part 2 of 2:[list of attributes], [person name]?
| [list ofattributes], please?Offers to close or continue the dialogue aresimilarly varied:Closing offer, part 1 of 2:Is there anything else | Anything else | Is thereanything else at allClosing offer, part 2 of 2:I can do for you today?
| I can help you withtoday?
| I can do for you?
| I can help you with?
|you need today?
| you need?4 Preliminary EvaluationTen native speakers of English, 6 female and 4male, were asked to participate in a preliminary in-lab system evaluation (half in the UK and half inthe US).
The Amiti?s system developers were notamong these volunteers.
Each made 9 phone callsto the system from behind a closed door, accordingto scenarios designed to test various customeridentities as well as single or multiple tasks.
Aftereach call, participants filled out a questionnaire toregister their degree of satisfaction with aspects ofthe interaction.Overall call success was 70%, with 98%successful completions for the VerifyId and 96%for the CheckBalance subtasks (Figure 4).?Failures?
were not system crashes but simulatedtransfers to a human agent.
There were 5 userterminations.Average word error rates were 17% for calls thatwere successfully completed, and 22% for failedcalls.
Word error rate by user ranged from 11% to26%.0.700.98 0.960.88 0.900.570.850.000.200.400.600.801.001.20Call SuccessVerifyIdCheckBalanceLostCardMakePaymentChangeAddressFinishDialogueFigure 4.
Task Completion RatesCall duration was found to reflect thecomplexity of each scenario, where complexity isdefined as the number of ?concepts?
needed tocomplete each task.
The following items arejudged to be concepts: task identification; valuessuch as first name, last name, house number, streetand phone number; and positive or negativeresponses such as whether a new card is desired.Figures 5 and 6 illustrate the relationship betweenlength of call and task complexity.
It should benoted that customer verification, a task performedin every dialogue, requires a minimum of 3personal details to be verified against a databaserecord, but may require more in the case ofrecognition errors.The overall average number of turns perdialogue was 18.28.
The user spoke an average of6.89 words per turn and the system 11.42.User satisfaction for each call was assessed byway of a questionnaire containing five statements.These covered the clarity of the instructions, easeof doing the task, how well the system understandsthe caller, how well the system works, and thecaller?s enjoyment of the system.
Participants ratedeach on a five-point Likert scale.
Summed resultsshowed an average score of 20.45 over all users(range 5?25; higher = stronger agreement).0246810121 2 3 4 5 6 7 8 9ScenarioNumber of concepts85.0135.8119.388.3106.7 114.048.4109.3127.10.020.040.060.080.0100.0120.0140.0160.01 2 3 4 5 6 7 8 9Figures 5 and 6.
Scenario Complexity (top) andAverage Call Duration by Scenario (in seconds)Although user satisfaction was high, we weremore interested in identifying the major problemsfor the callers.
Users were often frustrated byrecognition failures and/or unsuccessful attemptsto capture values such as a new street address,county, or phone number.
Sometimes the systemfailed to determine that the user was finished.Because the system is designed to be non-restrictive in accepting users?
input,misidentifications were common.
One user spokefor many by saying, ?There were times when thesystem would ask me to repeat information I hadnot yet given to it.?
We plan to revise our strategyso that we are a little more cautious in ourreprompts.
Occasionally, the system misidentifiedthe user?s desired task and had difficulty backingoff gracefully and starting the correct task.
We areworking on improving our recovery strategies forthese cases.The following transcription of an interactionwith our system illustrates the efficiency madepossible when the data supplied by the caller drivesthe conversation:AMITI?S:  Hello, this is Am-ih-tee-eh!
What canI do for you?USER:  yes this is Stella Lang.
My card numberis three one six seven one four.
Can you tell me thebalance on my account please?AMITI?S:  Can you just confirm the city andcounty?USER:  uh yes it?s London, MiddlesexAMITI?S:  I?m sorry, I didn?t understand whatyou said.
Can you repeat, please?USER:  London, MiddlesexAMITI?S:  All right, your balance is 1753pounds and 20 pence.
Anything else I can do foryou?USER:  yes I'd like to make a payment of fiftythree pounds with my debit card pleaseAMITI?S:  Can I have the debit card number,Miss Lang?USER:  sure, it's two eight seven four six six fournine five eight nine five three eight nine one, and itexpires October two thousand sixAMITI?S:  What's the issue number, please?USER:  oneAMITI?S:  Ok, Miss Lang, I'll send in thepayment amount of 53 pounds.
Did I get that right?USER:  yes, what's my new balance please?AMITI?S:  Ok, that's gone through.
Yourpayment will be credited overnight and will reflecton your next statement.
Your balance is 1700pounds and 20 pence.
Anything else I can do foryou today?USER:  no, that's it thanksAMITI?S:  Thanks very much for your call,goodbye.USER:  bye5 Discussion, Future WorkThe preliminary evaluation reported hereindicates promise for an automated dialoguesystem such as ours, which incorporates robusttechniques for information extraction, recordmatching, task identification, dialogue actclassification, and an overall data-driven strategy.Task duration and number of turns per dialogueboth appear to indicate greater efficiency andcorresponding user satisfaction than many othersimilar systems.
In the DARPA Communicatorevaluation, for example, between 60 and 79 callswere made to each of 8 participating sites (Walker,et al, 2001, 2002).
A sample scenario for adomestic round-trip flight contained 8 concepts(airline, departure city, state, date, etc.).
Theaverage duration for such a call was over 300seconds; whereas our overall average was 104seconds.
ASR accuracy rates in 2001 were about60% and 75%, for airline itineraries not completedand completed; and task completion rates were56%.
Our average number of user words per turn,6.89, is also higher than that reported forCommunicator systems.
This number seems toreflect lengthier responses to open prompts,responses to system requests for multipleattributes, and greater user initiative.We plan to port the system to a new domain:from telephone banking to information-technologysupport.
As part of this effort we are againcollecting data from real human-human calls.
Foradvanced speech recognition, we hope to train ourASR on new acoustic data.
We also plan to expandour dialogue act classification so that the systemcan recognize more types of acts, and to improveour classification reliability.6 AcknowledgementsThis paper is based on work supported in part bythe European Commission under the 5thFramework IST/HLT Programme, and by the USDefense Advanced Research Projects Agency.ReferencesJ.
Allen and M. Core.
1997.
Draft of DAMSL:Dialog Act Markup in Several Layers.http://www.cs.rochester.edu/research/cisd/resources/damsl/.J.
Allen, L. K. Schubert, G. Ferguson, P. Heeman,Ch.
L. Hwang, T. Kato, M. Light, N. G. Martin,B.
W. Miller, M. Poesio, and D. R. Traum.1995.
The TRAINS Project: A Case Study inBuilding a Conversational Planning Agent.Journal of Experimental and Theoretical AI, 7(1995), 7?48.Amiti?s, http://www.dcs.shef.ac.uk/nlp/amities.J.
Chu-Carroll and B. Carpenter.
1999.
Vector-Based Natural Language Call Routing.Computational Linguistics, 25 (3): 361?388.H.
Cunningham, D. Maynard, K. Bontcheva, V.Tablan.
2002.
GATE: A Framework andGraphical Development Environment for RobustNLP Tools and Applications.
Proceedings of the40th Anniversary Meeting of the Association forComputational Linguistics (ACL'02),Philadelphia, Pennsylvania.H.
Cunningham and D. Maynard and V. Tablan.2000.
JAPE: a Java Annotation Patterns Engine(Second Edition).
Technical report CS--00--10,University of Sheffield, Department ofComputer Science.DARPA,http://www.darpa.mil/iao/Communicator.htm.H.
Hardy, K. Baker, L. Devillers, L. Lamel, S.Rosset, T. Strzalkowski, C. Ursu and N. Webb.2002.
Multi-Layer Dialogue Annotation forAutomated Multilingual Customer Service.Proceedings of the ISLE Workshop on DialogueTagging for Multi-Modal Human ComputerInteraction, Edinburgh, Scotland.H.
Hardy, T. Strzalkowski and M. Wu.
2003a.Dialogue Management for an AutomatedMultilingual Call Center.
Research Directions inDialogue Processing, Proceedings of the HLT-NAACL 2003 Workshop, Edmonton, Alberta,Canada.H.
Hardy, K. Baker, H. Bonneau-Maynard, L.Devillers, S. Rosset and T. Strzalkowski.
2003b.Semantic and Dialogic Annotation forAutomated Multilingual Customer Service.Eurospeech 2003, Geneva, Switzerland.R.
B. Inouye, A. Biermann and A. Mckenzie.2004.
Caller Identification from Spelled-OutPersonal Data Using a Database for ErrorCorrection.
Duke University Internal Report.E.
Levin, S. Narayanan, R. Pieraccini, K. Biatov,E.
Bocchieri, G. Di Fabbrizio, W. Eckert, S.Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, andM.
Walker.
2000.
The AT&T-DARPACommunicator Mixed-Initiative Spoken DialogSystem.
ICSLP 2000.D.
Maynard.
2003.
Multi-Source and MultilingualInformation Extraction.
Expert Update.S.
Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid,and V. Zue.
1998.
Galaxy-II: A ReferenceArchitecture for Conversational SystemDevelopment.
ICSLP 98, Sydney, Australia.S.
Seneff and J. Polifroni.
2000.
DialogueManagement in the Mercury Flight ReservationSystem.
Satellite Dialogue Workshop, ANLP-NAACL, Seattle, Washington.M.
Walker, J. Aberdeen, J. Boland, E. Bratt, J.Garofolo, L. Hirschman, A.
Le, S. Lee, S.Narayanan, K. Papineni, B. Pellom, J. Polifroni,A.
Potamianos, P. Prabhu, A. Rudnicky, G.Sanders, S. Seneff, D. Stallard and S. Whittaker.2001.
DARPA Communicator Dialog TravelPlanning Systems: The June 2000 DataCollection.
Eurospeech 2001.M.
Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J.Garofolo, H. Hastie, A.
Le, B. Pellom, A.Potamianos, R. Passonneau, R. Prasad, S.Roukos, G. Sanders, S. Seneff and D. Stallard.2002.
DARPA Communicator Evaluation:Progress from 2000 to 2001.
ICSLP 2002.W.
Ward and B. Pellom.
1999.
The CUCommunicator System.
IEEE ASRU, pp.
341?344.W.
Xu and A. Rudnicky.
2000.
Task-based DialogManagement Using an Agenda.
ANLP/NAACLWorkshop on Conversational Systems, pp.
42?47.
