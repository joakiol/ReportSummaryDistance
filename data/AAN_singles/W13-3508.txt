Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65?74,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsSentence Compression with Joint Structural InferenceKapil Thadani and Kathleen McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10025, USA{kapil,kathy}@cs.columbia.eduAbstractSentence compression techniques oftenassemble output sentences using frag-ments of lexical sequences such as n-grams or units of syntactic structure suchas edges from a dependency tree repre-sentation.
We present a novel approachfor discriminative sentence compressionthat unifies these notions and jointly pro-duces sequential and syntactic represen-tations for output text, leveraging a com-pact integer linear programming formula-tion to maintain structural integrity.
Oursupervised models permit rich featuresover heterogeneous linguistic structuresand generalize over previous state-of-the-art approaches.
Experiments on corporafeaturing human-generated compressionsdemonstrate a 13-15% relative gain in 4-gram accuracy over a well-studied lan-guage model-based compression system.1 IntroductionRecent years have seen increasing interest in text-to-text generation tasks such as paraphrasing andtext simplification, due in large part to their directutility in high-level natural language tasks such asabstractive summarization.
The task of sentencecompression in particular has benefited from theavailability of a number of useful resources suchas the the Ziff-Davis compression corpus (Knightand Marcu, 2000) and the Edinburgh compressioncorpus (Clarke and Lapata, 2006b) which makecompression problems highly relevant for data-driven approaches involving language generation.The sentence compression task addresses theproblem of minimizing the lexical footprint of asentence, i.e., the number of words or charactersin it, while preserving its most salient informa-tion.
This is illustrated in the following examplefrom the compression corpus of Clarke and Lap-ata (2006b):Original: In 1967 Chapman, who had cultivated aconventional image with his ubiquitous tweed jacketand pipe, by his own later admission stunned a partyattended by his friends and future Python colleaguesby coming out as a homosexual.Compressed: In 1967 Chapman, who had cultivateda conventional image, stunned a party by coming outas a homosexual.Compression can therefore be viewed as analo-gous to text summarization1 defined at the sen-tence level.
Unsurprisingly, independent selec-tion of tokens for an output sentence does notlead to fluent or meaningful compressions; thus,compression systems often assemble output textfrom units that are larger than single tokens suchas n-grams (McDonald, 2006; Clarke and Lap-ata, 2008) or edges in a dependency structure (Fil-ippova and Strube, 2008; Galanis and Androut-sopoulos, 2010).
These systems implicitly rely ona structural representation of text?as a sequenceof tokens or as a dependency tree respectively?toto underpin the generation of an output sentence.In this work, we present structured transduc-tion: a novel supervised framework for sen-tence compression which employs a joint infer-ence strategy to simultaneously recover sentencecompressions under both these structural repre-sentations of text?a token sequence as well as atree of syntactic dependencies.
Sentence genera-tion is treated as a discriminative structured pre-diction task in which rich linguistically-motivated1To further the analogy, compression is most often formu-lated as a word deletion task which parallels the popular viewof summarization as a sentence extraction task.65features can be used to predict the informative-ness of specific tokens within the input text as wellas the fluency of n-grams and dependency rela-tionships in the output text.
We present a novelconstrained integer linear program that optimallysolves the joint inference problem, using the no-tion of commodity flow (Magnanti and Wolsey,1994) to ensure the production of valid acyclic se-quences and trees for an output sentence.The primary contributions of this work are:?
A supervised sequence-based compressionmodel which outperforms Clarke & Lapata?s(2008) state-of-the-art sequence-based com-pression system without relying on any hardsyntactic constraints.?
A formulation to jointly infer tree structuresalongside sequentially-ordered n-grams,thereby permitting features that factor overboth phrases and dependency relations.The structured transduction models offer addi-tional flexibility when compared to existing mod-els that compress via n-gram or dependency fac-torizations.
For instance, the use of commodityflow constraints to ensure well-formed structurepermits arbitrary reorderings of words in the inputand is not restricted to producing text in the sameorder as the input like much previous work (Mc-Donald, 2006; Clarke and Lapata, 2008; Filippovaand Strube, 2008) inter alia.2We ran compression experiments with the pro-posed approaches on well-studied corpora fromthe domains of written news (Clarke and Lapata,2006b) and broadcast news (Clarke and Lapata,2008).
Our supervised approaches show signif-icant gains over the language model-based com-pression system of Clarke and Lapata (2008) un-der a variety of performance measures, yielding13-15% relative F1 improvements for 4-gram re-trieval over Clarke and Lapata (2008) under iden-tical compression rate conditions.2 Joint Structure TransductionThe structured transduction framework is drivenby the fundamental assumption that generatingfluent text involves considerations of diverse struc-tural relationships between tokens in both inputand output sentences.
Models for sentence com-pression often compose text from units that are2We do not evaluate token reordering in the current workas the corpus used for experiments in ?3 features human-generated compressions that preserve token ordering.larger than individual tokens, such as n-gramswhich describe a token sequence or syntactic re-lations which comprise a dependency tree.
How-ever, our approach is specifically motivated by theperspective that both these representations of asentence?a sequence of tokens and a tree of de-pendency relations?are equally meaningful whenconsidering its underlying fluency and integrity.
Inother words, models for compressing a token se-quence must also account for the compression ofits dependency representation and vice versa.In this section, we discuss the problem of re-covering an optimal compression from a sen-tence as a linear optimization problem over het-erogenous substructures (cf.
?2.1) that can beassembled into valid and consistent representa-tions of a sentence (cf.
?2.2).
We then considerrich linguistically-motivated features over thesesubstructures (cf.
?2.3) for which correspondingweights can be learned via supervised structuredprediction (cf.
?2.4).2.1 Linear ObjectiveConsider a single compression instance involvinga source sentence S containing m tokens.
The no-tation S?
is used to denote a well-formed compres-sion of S. In this paper, we follow the standardassumption from compression research in assum-ing that candidate compressions S?
are assembledfrom the tokens in S, thereby treating compressionas a word-deletion task.
The inference step aimsto retrieve the output sentence S??
that is the mostlikely compression of the given input S, i.e., the S?that maximizes p(S?|S) ?
p(S?, S) or, in an equiv-alent discriminative setting, the S?
that maximizesa feature-based score for compressionS??
, argmaxS?w>?
(S, S?)
(1)where ?
(S, S?)
denotes some feature map param-eterized by a weight vector w.Let T , {ti : 1 ?
i ?
m} represent the setof tokens in S and let xi ?
{0, 1} represent atoken indicator variable whose value correspondsto whether token ti is present in the output sen-tence S?.
The incidence vector x , ?x1, .
.
.
, xm?>therefore represents an entire token configurationthat is equivalent to some subset of T .If we were to consider a simplistic bag-of-tokens scenario in which the features factor en-tirely over the tokens from T , the highest-scoring66compression under (1) would simply be the to-ken configuration that maximizes a linear combi-nation of per-token scores, i.e., ?ti?T xi ?
?tok(i)where ?tok : N?
R denotes a linear scoring func-tion which measures the relative value of retain-ing ti in a compression of S based on its features,i.e., ?tok(i) , w>tok?tok(ti).
Although this canbe solved efficiently under compression-rate con-straints, the strong independence assumption usedis clearly unrealistic: a model that cannot considerany relationship between tokens in the output doesnot provide a token ordering or ensure that the re-sulting sentence is grammatical.The natural solution is to include higher-orderfactorizations of linguistic structures such as n-grams in the objective function.
For clarity of ex-position, we assume the use of trigrams withoutloss of generality.
Let U represent the set of allpossible trigrams that can be constructed from thetokens of S; in other words U , {?ti, tj , tk?
: ti ?T ?
{START}, tj ?
T, tk ?
T ?
{END}, i 6= j 6=k}.
Following the notation for token indicators, letyijk ?
{0, 1} represent a trigram indicator variablefor whether the contiguous sequence of tokens?ti, tj , tk?
is in the output sentence.
The incidencevector y , ?yijk?
?ti,tj ,tk?
?U hence representssome subset of the trigrams in U .
Similarly, let Vrepresent the set of all possible dependency edgesthat can be established among the tokens of S andthe pseudo-token ROOT, i.e., V , {?i, j?
: i ?T ?
{ROOT}, j ?
T, tj is a dependent of ti in S}.As before, zij ?
{0, 1} represents a dependencyarc indicator variable indicating whether tj is a di-rect dependent of ti in the dependency structure ofthe output sentence, and z , ?zij??ti,tj?
?V repre-sents a subset of the arcs from V .Using this notation, any output sentence S?
cannow be expressed as a combination of some to-ken, trigram and dependency arc configurations?x,y, z?.
Defining ?ngr and ?dep analogously to?tok for trigrams and dependency arcs respectively,we rewrite (1) asS??
= argmaxx,y,z?ti?Txi ?
?tok(i)+?
?ti,tj ,tk?
?Uyijk ?
?ngr(i, j, k)+??ti,tj?
?Vzij ?
?dep(i, j)= argmaxx,y,zx>?tok + y>?ngr + z>?dep (2)where ?tok , ?
?tok(i)?ti?T denotes the vector oftoken scores for all tokens ti ?
T and ?ngr and?dep represent vectors of scores for all trigramsand dependency arcs in U and V respectively.
Thejoint objective in (2) is an appealingly straightfor-ward and yet general formulation for the compres-sion task.
For instance, the use of standard sub-structures like n-grams permits scoring of the out-put sequence configuration y under probabilisticn-gram language models as in Clarke and Lapata(2008).
Similarly, consideration of dependencyarcs allows the compressed dependency tree z tobe scored using a rich set of indicator features overdependency labels, part-of-speech tags and evenlexical features as in Filippova and Strube (2008).However, unlike the bag-of-tokens scenario,these output structures cannot be constructed effi-ciently due to their interdependence.
Specifically,we need to maintain the following conditions inorder to obtain an interpretable token sequence y:?
Trigram variables yijk must be non-zero ifand only if their corresponding word vari-ables xi, xj and xk are non-zero.?
The non-zero yijk must form a sentence-likelinear ordering, avoiding disjoint structures,cycles and branching.Similarly, a well-formed dependency tree z willneed to satisfy the following conditions:?
Dependency variables zij must be non-zero ifand only if the corresponding word variablesxi and xj are.?
The non-zero zij must form a directed treewith one parent per node, a single root nodeand no cycles.2.2 Constrained ILP FormulationWe now discuss an approach to recover exact so-lutions to (2) under the appropriate structural con-straints, thereby yielding globally optimal com-pressions S?
?
?x,y, z?
given some input sentenceS and model parameters for the scoring functions.For this purpose, we formulate the inference taskfor joint structural transduction as an integer linearprogram (ILP)?a type of linear program (LP) inwhich some or all of the decision variables are re-stricted to integer values.
A number of highly op-timized general-purpose solvers exist for solvingILPs thereby making them tractable for sentence-level natural language problems in which the num-ber of variables and constraints is described by alow-order polynomial over the size of the input.67Recent years have seen ILP applied to manystructured NLP applications including depen-dency parsing (Riedel and Clarke, 2006; Martinset al 2009), text alignment (DeNero and Klein,2008; Chang et al 2010; Thadani et al 2012)and many previous approaches to sentence anddocument compression (Clarke and Lapata, 2008;Filippova and Strube, 2008; Martins and Smith,2009; Clarke and Lapata, 2010; Berg-Kirkpatricket al 2011; Woodsend and Lapata, 2012).2.2.1 Basic structural constraintsWe start with constraints that define the behaviorof terminal tokens.
Let y?jk, yij?
and z?j denoteindicator variables for the sentence-starting tri-gram ?START, tj , tk?, the sentence-ending trigram?ti, tj , END?
and the root dependency ?ROOT, tj?respectively.
A valid output sentence will startedand terminate with exactly one trigram (perhapsthe same); similarly, exactly one word should actas the root of the output dependency tree.
?j,ky?jk = 1 (3)?i,jyij?
= 1 (4)?jz?j = 1 (5)Indicator variables for any substructure, i.e., n-gram or dependency arc, must be kept consistentwith the token variables that the substructure is de-fined over.
For instance, we require constraintswhich specify that tokens can only be active (non-zero) in the solution when, for 1 ?
p ?
n, thereis exactly one active n-gram in the solution whichcontains this word in position p.3 Tokens and de-pendency arcs can similarly be kept consistent byensuring that a word can only be active when oneincoming arc is active.xl ??i,j,k:l?
{i,j,k}yijk = 0, ?tl ?
T (6)xj ?
?izij = 0, ?tj ?
T (7)3Note that this does not always hold for n-grams of or-der n > 2 due to the way terminal n-grams featuring STARTand END are defined.
Specifically, in a valid linear orderingof tokens and ?r ?
1 .
.
.
n?
2, there can be no n-grams thatfeature the last n?r?1 tokens in the r?th position or the firstn?r?1 tokens in the (n?r+1)?th position.
However, thisis easily tackled computationally by assuming that the termi-nal n-gram replaces these missing n-grams for near-terminaltokens in constraint (6).2.2.2 Flow-based structural constraintsA key challenge for structured transduction mod-els lies in ensuring that output token sequences anddependency trees are well formed.
This requiresthat output structures are fully connected and thatcycles are avoided.
In order to accomplish this, weintroduce additional variables to establish single-commodity flow (Magnanti and Wolsey, 1994) be-tween all pairs of tokens, inspired by recent workin dependency parsing (Martins et al 2009).
Lin-ear token ordering is maintained by defining real-valued adjacency commodity flow variables ?adjijwhich must be non-zero whenever tj directly fol-lows ti in an output sentence.
Similarly, tree-structured dependencies are enforced using addi-tional dependency commodity flow variables ?depijwhich must be non-zero whenever tj is the de-pendent of ti in the output sentence.
As with thestructural indicators, flow variables ?adj?j , ?adji?
, ?dep?jare also defined for the terminal pseudo-tokensSTART, END and ROOT respectively.Each active token in the solution consumes oneunit of each commodity from the flow variablesconnected to it.
In conjunction with the consis-tency constraints from equations (6) and (7), thisensures that cycles cannot be present in the flowstructure for either commodity.
?i?cij ?
?k?cjk = xj , ?tj ?
T, (8)?c ?
{adj, dep}By itself, (8) would simply set altoken indica-tors xi simultaneously to 0.
However, since STARTand ROOT have no incoming flow variables, theamount of commodity in the respective outgo-ing flow variables ?adj?j and ?dep?j remains uncon-strained.
These flow variables therefore providea point of origin for their respective commodities.In order for commodity flow to be meaningful,it should be confined to mirroring active structuralindicators; for this, we first restrict the amount ofcommodity in any ?cij to be non-negative.
?cij ?
0, ?ti, tj ?
T (9)?c ?
{adj, dep}The adjacency commodity is then linked to the n-grams that would actually establish an adjacencyrelationship between two tokens, while the depen-dency commodity is linked to its correspondingdependency arcs.
In conjunction with (8?9), these68START ENDROOTProduction was closed down at Ford last night for the Christmas period .8 ?adj1,3 = 7 6 5 4 3 2 17?dep3,1 = 1 2 1211Figure 1: An illustration of commodity values for a valid solution of the program.
The adjacency com-modity ?adj and dependency commodity ?dep are denoted by solid and dashed lines respectively.constraints also serve to establish connectivity fortheir respective structures.
?adjij ?
Cmax?kyijk ?
0, ?ti, tj ?
T (10)?adjjk ?
Cmax?iyijk ?
0, ?tj , tk ?
T (11)?depij ?
Cmaxzij ?
0, ?ti, tj ?
T (12)where Cmax is the maximum amount of commod-ity that the ?ij variables may carry and serves as anupper bound on the number of tokens in the outputsentence.
Since we use commodity flow to avoidcyclical structure and not to specify spanning ar-borescences (Martins et al 2009), Cmax can sim-ply be set to an arbitrary large value.2.2.3 Compression rate constraintsThe constraints specified above are adequate to en-force structural soundness in an output compres-sion.
In addition, compression tasks often involvea restriction on the size of the output sentence.When measured in tokens, this can simply be ex-pressed via constraints over token indicators.
?ixi ?
Rmin (13)?ixi ?
Rmax (14)where the compression rate is enforced by restrict-ing the number of output tokens to [Rmin, Rmax].2.3 FeaturesThe scoring functions ?
that guide inference for aparticular compression instance are defined aboveas linear functions over structure-specific features.We employ the following general classes of fea-tures for tokens, trigrams and dependency arcs.1.
Informativeness: Good compressions mightrequire specific words or relationships be-tween words to be preserved, highlighted, orperhaps explicitly rejected.
This can be ex-pressed through features on token variablesthat indicate a priori salience.4 For this pur-pose, we rely on indicator features for part-of-speech (POS) sequences of length up to 3that surround the token and the POS tag of thetoken?s syntactic governor conjoined with thelabel.
Inspired by McDonald (2006), we alsomaintain indicator features for stems of verbs(at or adjacent to the token) as these can beuseful indications of salience in compression.Finally, we maintain features for whether to-kens are negation words, whether they appearwithin parentheses and if they are part of acapitalized sequence of tokens (an approxi-mation of named entities).2.
Fluency: These features are intended to cap-ture how the presence of a given substructurecontributes to the overall fluency of a sen-tence.
The n-gram variables are scored with afeature expressing their log-likelihood underan LM.
For n-gram variables, we include fea-tures that indicate the POS tags and depen-dency labels corresponding to the tokens itcovers.
Dependency variable features involveindicators for the governor POS tag con-joined with the dependency direction.
In ad-dition, we also use lexical features for prepo-sitions in the governor position of depen-dency variables in order to indicate whethercertain prepositional phrases are likely to bepreserved in compressions.3.
Fidelity: One might reasonably expect thatmany substructures in the input sentence willappear unchanged in the output sentence.Therefore, we propose boolean features thatindicate that a substructure was seen in theinput.
Fidelity scores are included for alln-gram variables alongside label-specific fi-4Many compression systems (Clarke and Lapata, 2008;Filippova and Strube, 2008) use a measure based on tf*idfwhich derives from informativeness score of Hori and Furui(2004), but we found this to be less relevant here.69delity scores for dependency arc variables,which can indicate whether particular labelsare more or less likely to be dropped.4.
Pseudo-normalization: A drawback of us-ing linear models for generation problemsis an inability to employ output sentencelength normalization in structure scoring.
Forthis purpose, we use the common machinetranslation (MT) strategy of employing wordpenalty features.
These are essentially wordcounts whose parameters are intended to bal-ance out the biases in output length which areinduced by other features.Each scale-dependent feature is recorded both ab-solutely as well as normalized by the length of theinput sentence.
This is done in order to permit themodel to acquire some robustness to variation ininput sentence length when learning parameters.2.4 LearningIn order to leverage a training corpus to recoverweight parameters w?
for the above features thatencourage good compressions for unseen data,we rely on the structured perceptron of Collins(2002).
A fixed learning rate is used and param-eters are averaged to limit overfitting.5 In our ex-periments, we observed fairly stable convergencefor compression quality over held-out develop-ment corpora, with peak performance usually en-countered by 10 training epochs.3 ExperimentsIn order to evaluate the performance of thestructured transduction framework, we ran com-pression experiments over the newswire (NW)and broadcast news transcription (BN) corporacollected by Clarke and Lapata (2008).
Sen-tences in these datasets are accompanied by goldcompressions?one per sentence for NW andthree for BN?produced by trained human anno-tators who were restricted to using word deletion,so paraphrasing and word reordering do not playa role.
For this reason, we chose to evaluate thesystems using n-gram precision and recall (amongother metrics), following Unno et al(2006) andstandard MT evaluations.We filtered the corpora to eliminate instanceswith less than 2 and more than 110 tokens and used5Given an appropriate loss function, large-margin struc-tured learners such as k-best MIRA (McDonald et al 2005)can also be used as shown in Clarke and Lapata (2008).the same training/development/test splits fromClarke and Lapata (2008), yielding 953/63/603sentences respectively for the NW corpus and880/78/404 for the BN corpus.
Dependency parseswere retrieved using the Stanford parser6 and ILPswere solved using Gurobi.7 As a state-of-the-artbaseline for these experiments, we used a reim-plementation of the LM-based system of Clarkeand Lapata (2008), which we henceforth refer toas CL08.
This is equivalent to a variant of our pro-posed model that excludes variables for syntacticstructure, uses LM log-likelihood as a feature fortrigram variables and a tf*idf -based significancescore for token variables, and incorporates severaltargeted syntactic constraints based on grammat-ical relations derived from RASP (Briscoe et al2006) designed to encourage fluent output.Due to the absence of word reordering in thegold compressions, trigram variables y that wereconsidered in the structured transduction approachwere restricted to only those for which tokensappear in the same order as the input as is thecase with CL08.
Furthermore, in order to reducecomputational overhead for potentially-expensiveILPs, we also excluded dependency arc variableswhich inverted an existing governor-dependent re-lationship from the input sentence parse.A recent analysis of approaches to evaluatingcompression (Napoles et al 2011b) has shown astrong correlation between the compression rateand human judgments of compression quality,thereby concluding that comparisons of systemswhich compress at different rates are unreliable.Consequently, all comparisons that we carry outhere involve a restriction to a particular compres-sion rate to ensure that observed differences canbe interpreted meaningfully.3.1 ResultsTable 1 summarizes the results from compressionexperiments in which the target compression rateis set to the average gold compression rate foreach instance.
We observe a significant gain forthe joint structured transduction system over theClarke and Lapata (2008) approach for n-gram F1.Since n-gram metrics do not distinguish betweencontent words and function words, we also in-clude an evaluation metric that observes the pre-cision, recall and F-measure of nouns and verbs6http://nlp.stanford.edu/software/7http://www.gurobi.com70Corpus System n-grams F1% Content words Syntactic relations F1%n = 1 2 3 4 P% R% F1% Stanford RASPNW CL08 66.65 53.08 40.35 31.02 73.84 66.41 69.38 51.51 50.21Joint ST 71.91 58.67 45.84 35.62 76.82 76.74 76.33 55.02 50.81BN CL08 75.08 61.31 46.76 37.58 80.21 75.32 76.91 60.70 57.27Joint ST 77.82 66.39 52.81 42.52 80.77 81.93 80.75 61.38 56.47Table 1: Experimental results under various quality metrics (see text for descriptions).
Systems wererestricted to produce compressions that matched their average gold compression rate.
Boldfaced entriesindicate significant differences (p < 0.0005) under the paired t-test and Wilcoxon?s signed rank test.as a proxy for the content in compressed output.From these, we see that the primary contributionof the supervised joint approach is in enhancingthe recall of meaning-bearing words.In addition to the direct measures discussedabove, Napoles et al(2011b) indicate that variousother metrics over syntactic relations such as thoseproduced by RASP also correlate significantlywith human judgments of compression quality.Compressed sentences were therefore parsed withRASP as well as the Stanford dependency parserand their resulting dependency graphs were com-pared to those of the gold compressions.
Thesemetrics show statistically insignificant differencesexcept in the case of F1 over Stanford dependen-cies for the NW corpus.8Comparisons with CL08 do not adequately ad-dress the question of whether the performancegain observed is driven by the novel joint infer-ence approach or the general power of discrimina-tive learning.
To investigate this, we also studieda variant of the proposed model which eliminatesthe dependency variables z and associated com-modity flow machinery, thereby bridging the gapbetween the two systems discussed above.
Thissystem, which we refer to as Seq ST, is other-wise trained under similar conditions as Joint ST.Table 2 contains an example of incorrect systemoutput for the three systems under study and il-lustrates some specific quirks of each, such as thetendency of CL08 to preserve deeply nested nounphrases, the limited ability of Seq ST to identifyheads of constituents and the potential for plausi-ble but unusual output parses from Joint ST.Figure 2 examines the variation of content wordF1% when the target compression rate is variedfor the BN corpus, which contains three refer-8Our RASP F1 results for Clarke and Lapata (2008) inTable 1 outperform their reported results by about 10% (ab-solute) which may stem from our Gigaword-trained LM orimprovements in recent versions of RASP.Input When Los Angeles hosted the Olympics in1932 , Kurtz competed in high platform diving.Gold When Los Angeles hosted the Olympics , Kurtzcompeted in high diving .CL08 When Los Angeles hosted Olympics in 1932 ,in high platform diving .Seq ST When Los Angeles hosted the Olympics , Kurtzcompeted in high platformJoint ST When Los Angeles hosted the Olympics in1932 , Kurtz competed diving .Table 2: Examples of erroneous system compres-sions for a test instance from the NW corpus.ence compressions per instance.
Although thegold compressions are often unreachable underlow rates, this provides a view into a model?s abil-ity to select meaningful words under compressionconstraints.
We observe that the Joint ST modelconsistently identifies content words more accu-rately than the sequence-only models despite shar-ing all token and trigram features with Seq ST.Figure 3 studies the variation of RASP gram-matical relation F1% with compression rate as anapproximate measure of grammatical robustness.As all three systems track each other fairly closely,the plot conveys the absolute difference of the STsystems from the CL08 baseline, which revealsthat Joint ST largely outperforms Seq ST underdifferent compression conditions.
We also notethat a high compression rate, i.e., minimal com-pression, is generally favorable to CL08 under theRASP F1 measure and conjecture that this may bedue to the hard syntactic constraints employed byCL08, some of which are defined over RASP re-lations.
At higher compression rates, these con-straints largely serve to prevent the loss of mean-ingful syntactic relationships, e.g., that between apreposition and its prepositional phrase; however,a restrictive compression rate would likely resultin all such mutually-constrained components be-ing dropped rather than simultaneously preserved.71Figure 2: Informativeness of compressions in theBN test corpus indicated by noun and verb F1%with respect to gold at different compression rates.4 Related WorkAn early notion of compression was proposedby Dras (1997) as reluctant sentence paraphras-ing under length constraints.
Jing and McKe-own (2000) analyzed human-generated summariesand identified a heavy reliance on sentence re-duction (Jing, 2000).
The extraction by Knightand Marcu (2000) of a dataset of natural com-pression instances from the Ziff-Davis corpusspurred interest in supervised approaches to thetask (Knight and Marcu, 2002; Riezler et al 2003;Turner and Charniak, 2005; McDonald, 2006;Unno et al 2006; Galley and McKeown, 2007;Nomoto, 2007).
In particular, McDonald (2006)expanded on Knight & Marcu?s (2002) transition-based model by using dynamic programming torecover optimal transition sequences, and Clarkeand Lapata (2006a) used ILP to replace pairwisetransitions with trigrams.
Other recent work (Fil-ippova and Strube, 2008; Galanis and Androut-sopoulos, 2010) has used dependency trees di-rectly as sentence representations for compres-sion.
Another line of research has attempted tobroaden the notion of compression beyond mereword deletion (Cohn and Lapata, 2009; Ganitke-vitch et al 2011; Napoles et al 2011a).
Finally,progress on standalone compression tasks has alsoenabled document summarization techniques thatjointly address sentence selection and compres-sion (Daume?
and Marcu, 2002; Clarke and Lapata,2007; Martins and Smith, 2009; Berg-Kirkpatricket al 2011; Woodsend and Lapata, 2012), a num-ber of which also rely on ILP-based inference.Monolingual text-to-text generation researchalso faces many obstacles common to MT.
Re-Figure 3: Relative grammaticality of BN test cor-pus compressions indicated by the absolute differ-ence of RASP relation F1% from that of CL08.cent work in MT decoding has proposed more ef-ficient approaches than ILP to produced text op-timally under syntactic and sequential models oflanguage (Rush and Collins, 2011).
We are cur-rently exploring similar ideas for compression andother text-to-text generation problems.5 ConclusionWe have presented a supervised discriminativeapproach to sentence compression that elegantlyaccounts for two complementary aspects of sen-tence structure?token ordering and dependencysyntax.
Our inference formulation permits rich,linguistically-motivated features that factor overthe tokens, n-grams and dependencies of the out-put.
Structural integrity is maintained by linearconstraints based on commodity flow, resulting ina flexible integer linear program for the inferencetask.
We demonstrate that this approach leads tosignificant performance gains over a state-of-the-art baseline compression system without resortingto hand-picked constraints on output content.AcknowledgmentsThis work was supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viaDepartment of Interior National Business Cen-ter (DoI/NBC) contract number D11PC20153.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policiesor endorsements, either expressed or implied, ofIARPA, DoI/NBC, or the U.S. Government.72ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of ACL-HLT, pages 481?490.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the ACL-COLING Interactive Presenta-tion Sessions.Ming-Wei Chang, Dan Goldwasser, Dan Roth, andVivek Srikumar.
2010.
Discriminative learning overconstrained latent representations.
In Proceedingsof HLT-NAACL, pages 429?437.James Clarke and Mirella Lapata.
2006a.
Constraint-based sentence compression: an integer program-ming approach.
In Proceedings of ACL-COLING,pages 144?151.James Clarke and Mirella Lapata.
2006b.
Modelsfor sentence compression: a comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of ACL-COLING, pages 377?384.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Proceed-ings of EMNLP-CoNLL, pages 1?11.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: an integer linearprogramming approach.
Journal for Artificial Intel-ligence Research, 31:399?429, March.James Clarke and Mirella Lapata.
2010.
Discourseconstraints for document compression.
Computa-tional Linguistics, 36(3):411?441.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research, 34(1):637?674, April.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models.
In Proceedings ofEMNLP, pages 1?8.Hal Daume?, III and Daniel Marcu.
2002.
A noisy-channel model for document compression.
In Pro-ceedings of ACL, pages 449?456.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of ACL-HLT, pages 25?28.Mark Dras.
1997.
Reluctant paraphrase: Textual re-structuring under an optimisation model.
In Pro-ceedings of PacLing, pages 98?104.Katja Filippova and Michael Strube.
2008.
Depen-dency tree based sentence compression.
In Proceed-ings of INLG, pages 25?32.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Proceedings of HLT-NAACL, pages885?893.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
In Proceedings of HLT-NAACL, pages 180?187, April.Juri Ganitkevitch, Chris Callison-Burch, CourtneyNapoles, and Benjamin Van Durme.
2011.
Learn-ing sentential paraphrases from bilingual parallelcorpora for text-to-text generation.
In Proceedingsof EMNLP, pages 1168?1179.Chiori Hori and Sadaoki Furui.
2004.
Speech summa-rization: an approach through word extraction and amethod for evaluation.
IEICE Transactions on In-formation and Systems, E87-D(1):15?25.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand paste based text summarization.
In Proceedingsof NAACL, pages 178?185.Hongyan Jing.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of theConference on Applied Natural Language Process-ing, pages 310?315.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of AAAI, pages 703?710.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107, July.Thomas L. Magnanti and Laurence A. Wolsey.
1994.Optimal trees.
In Technical Report 290-94,Massechusetts Institute of Technology, OperationsResearch Center.Andre?
F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extractionand compression.
In Proceedings of the Workshopon Integer Linear Programming for Natural Lan-gauge Processing, pages 1?9.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofACL-IJCNLP, pages 342?350.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of EACL, pages 297?304.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011a.
Paraphras-tic sentence compression with a character-basedmetric: tightening without deletion.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 84?90.73Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011b.
Evaluating sentence com-pression: pitfalls and suggested remedies.
In Pro-ceedings of the Workshop on Monolingual Text-To-Text Generation, pages 91?97.Tadashi Nomoto.
2007.
Discriminative sentence com-pression with conditional random fields.
Infor-mation Processing and Management, 43(6):1571?1587, November.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective de-pendency parsing.
In Proceedings of EMNLP, pages129?137.Stefan Riezler, Tracy H. King, Richard Crouch, andAnnie Zaenen.
2003.
Statistical sentence condensa-tion using ambiguity packing and stochastic disam-biguation methods for lexical-functional grammar.In Proceedings of HLT-NAACL, pages 118?125.Alexander M. Rush and Michael Collins.
2011.
Ex-act decoding of syntactic translation models throughlagrangian relaxation.
In Proceedings of ACL-HLT,pages 72?82.Kapil Thadani, Scott Martin, and Michael White.2012.
A joint phrasal and dependency model forparaphrase alignment.
In Proceedings of COLING,pages 1229?1238.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of ACL, pages 290?297.Yuya Unno, Takashi Ninomiya, Yusuke Miyao, andJun?ichi Tsujii.
2006.
Trimming CFG parse treesfor sentence compression using machine learningapproaches.
In Proceedings of ACL-COLING, pages850?857.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of EMNLP, pages 233?243.74
