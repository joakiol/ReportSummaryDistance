Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 157?162,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLiMoSINe pipeline: Multilingual UIMA-based NLP platformOlga Uryupina1, Barbara Plank2, Gianni Barlacchi1,3,Francisco Valverde Albacete4, Manos Tsagkias5, Antonio Uva1, and Alessandro Moschitti6,11Department of Information Engineering and Computer Science, University of Trento, Italy2University of Groningen, The Netherlands3SKIL - Telecom Italia, Trento, Italy4Dept.
Teoria de Se?nal y Comunicaciones, Universidad Carlos III de Madrid, Spain5904Labs, Amsterdam, The Netherlands6Qatar Computing Research Instituteuryupina@gmail.com, b.plank@rug.nl, gianni.barlacchi@unitn.it,fva@tsc.uc3m.es, manos@904labs.com,antonio.uva@unitn.it, amoschitti@gmail.comAbstractWe present a robust and efficient paralleliz-able multilingual UIMA-based platform for au-tomatically annotating textual inputs with dif-ferent layers of linguistic description, rangingfrom surface level phenomena all the way downto deep discourse-level information.
In partic-ular, given an input text, the pipeline extracts:sentences and tokens; entity mentions; syntac-tic information; opinionated expressions; re-lations between entity mentions; co-referencechains and wikified entities.
The system isavailable in two versions: a standalone distri-bution enables design and optimization of user-specific sub-modules, whereas a server-clientdistribution allows for straightforward high-performance NLP processing, reducing the en-gineering cost for higher-level tasks.1 IntroductionWith the growing amount of textual information avail-able on an everyday basis, Natural Language Process-ing gets more and more large-scale.
Moreover, a lot ofeffort has been invested in the recent years into the de-velopment of multi- and cross-lingual resources.
To ef-ficiently use large amounts of data for high-level tasks,e.g., for Information Extraction, we need robust par-allelizable multilingual preprocessing pipelines to au-tomatically annotate textual inputs with a variety oflinguistic structures.
To address the issue, we presentthe LiMoSINe Pipeline?a platform developed by theFP7 EU project LiMoSINE: Linguistically MotivatedSemantic aggregation engiNes.Several platforms and toolkits for NLP preprocess-ing have been made available to the research commu-nity in the past decades.
The most commonly usedones are OpenNLP1, FreeLing (Padr?o and Stanilovsky,2012) and GATE (Cunningham et al, 2011).
In addi-tion, many research groups publicly release their pre-1http://opennlp.apache.orgprocessing modules.
These approaches, however, poseseveral problems:?
most of these tools require a considerable effort forinstallation, configuration and getting familiar withthe software,?
parallelization might be an issue,?
for languages other than English, many modules aremissing, while the existing ones often have only amoderate performance level.In the LiMoSINe project, we focus on high-performance NLP processing for four European lan-guages: English, Italian, Spanish and Dutch.
Wecombine state-of-the-art solutions with specifically de-signed in-house modules to ensure reliable perfor-mance.
Using the UIMA framework, we opt for a fullyparallelizable approach, making it feasible to processlarge amounts of data.
Finally, we release the sys-tem in two versions: a client application connects tothe pipeline installed on the LiMoSINe server to pro-vide the users with all the annotation they require.
Thisdoes not require any advanced installation or config-uration of the software, thus reducing the engineer-ing cost for the potential stake holders.
A local in-stallation of the pipeline, on the contrary, requiressome effort to get familiar with the system, but it alsogives users a possibility to integrate their own modules,thus allowing for a greater flexibility.
The pipeline isavailable at http://ikernels-portal.disi.unitn.it/projects/limosine/.2 LiMoSINe pipeline: overall structureOur platform supports various levels of linguistic de-scription, representing a document from different an-gles.
It should therefore combine outputs of numer-ous linguistic preprocessors to provide a uniform anddeep representation of a document?s semantics.
Theoverall structure of our pipeline is shown on Figure 1.This complex structure raises an issue of the compat-ibility between preprocessors: with many NLP mod-ules around?publicly available, implemented by theLiMoSINe partners or designed by potential stake-holders?it becomes virtually impossible to ensure that157plain text filestokenization/sentencesplittinginputPoS taggingtokenized sentencesnamed entitytaggingsyntactic/sem-antic parsingpos-tagged sentencesentity mentiondetectionnamed entitiesrelationextractionparsedsentencesopinion miningcoreferenceresolutionmentionsentity linkingannotatedstructuresOutput contains:- entities- relations- syntax/semantics- opinions etc.Figure 1: LiMoSINe pipeline architectureany two modules have the same input/output formatand thus can be run as a pipeline.We have focused on creating a platform that allowsfor straightforward incorporation of various tools, co-ordinating their inputs and outputs in a uniform way.Our LiMoSINe Pipeline is based on Apache UIMA?a framework for Unstructured Information Manage-ment.2UIMA has been successfully used for a num-ber of NLP projects, e.g., for the IBM Watson system(Ferrucci et al, 2010).One of the main features of UIMA is its modularity:the individual annotators only incrementally update thedocument representation (?CAS?
), but do not interactwith each other.
This allows for a straightforward de-ployment of new components: to add a new module toa UIMA system, one only has to create a wrapper con-verting its input and output objects into CAS structures.Moreover, UIMA allows for full parallelization of theprocessing flow, which is especially crucial when weaim at annotating large amounts of data.UIMA-based systems can be deployed both locallyor remotely.
To run a UIMA application on a local ma-chine, the user should follow the instructions on theUIMA web site to download and install UIMA.
The2http://uima.apache.org/LiMoSINe Pipeline should then be downloaded andrun.
While this requires some engineering effort, suchan approach would allow the user to implement and in-tegrate their own modules into the existing pipeline, aswell as to re-optimize (e.g., retraining a parser to covera specific domain).A client-server version of the pipeline has been in-stalled on the LiMoSINe server.
The client applica-tion can be downloaded from the pipeline website.
Theusers do not need to install any UIMA-related soft-ware to use this service.
While this approach does notprovide the flexibility of a local installation, it allowsthe users to obtain state-of-the-art NLP annotations fortheir textual inputs at no engineering cost at all.
Thismight provide a valuable support for projects focusingon higher-level tasks, for example, on Question An-swering, especially for languages other than English,considerably reducing the effort required for imple-menting and integrating all the preprocessing compo-nents needed.3 Integrated modulesThe LiMoSINe project has focused on four Europeanlanguages: English, Italian, Spanish and Dutch.
For allthese languages, we have created a platform that pro-vides robust parallelizable NLP processing up to the158syntactic parsing level.
This already allows to createcomplex structural representations of sentences, to beused for higher-level tasks, such as Opinion Miningor Question Answering (cf.
Section 4 below).
In ad-dition, where possible, we have integrated deeper se-mantic and discourse-level processing, such as relationextraction, coreference, opinion mining and entity link-ing.
Table 1 provides an overview of all the currentlysupported modules.The feasibility of our approach depends crucially onthe performance of linguistic processors for a specificlanguage and on the availability of the manually an-notated data.
Despite a growing interest in the multi-lingual processing in the NLP community, for a num-ber of tasks no robust processors are available for lan-guages other than English and for some others even ageneric model cannot be retrained due to the lack ofdata.
While we tried to rely as much as possible onthe state-of-the-art technology, we had to implement orre-optimize a number of preprocessors.3.1 EnglishStanford tools.
To provide basic preprocessing, re-quired by our high-level components, we createdUIMA wrappers for several Stanford NLP tools (Man-ning et al, 2014): the tokenizer, the parser and thenamed entity analyzer.Entity Mention Detector.
Both coreference re-solver and relation extractor require information onmentions?textual units that correspond to real-worldobjects.
Even though some studies focus on specificsubtypes of mentions (for example, on pronominalcoreference or on relations between named entities),we believe that a reliable pipeline should provide in-formation on all the possible mentions.An entity mention detector (EMD), covering a widevariety of mentions, has been developed at the Uni-versity of Trento as a part of BART (see below).A more recent version has been proposed for theCoNLL-2011/2012 Shared Tasks (Uryupina et al,2011; Uryupina et al, 2012).
It is a rule-based systemthat combines the outputs of a parser and an NE-taggerto extract mention boundaries (both full and minimalnominal spans) and assign mention types (name, nom-inal or pronoun) and semantic classes (inferred fromWordNet for common nouns, from NER labels forproper nouns).
We are currently planning to integratelearning-based EMD (Uryupina and Moschitti, 2013)to cover additional languages, in particular, Arabic.Opinion Mining.
The opinion expression annotatoris a system developed at the University of Trento by Jo-hansson and Moschitti (2011).
It extracts fine-grainedopinion expressions together with their polarity.
To ex-tract opinion expressions, it uses a standard sequencelabeler for subjective expression markup similar to theapproach by (Breck et al, 2007).
The system has beendeveloped on the MPQA corpus that contains news ar-ticles.
It internally uses the syntactic/semantic LTHdependency parser of (Johansson and Nugues, 2008).The opinion mining tool thus requires CoNLL-2008-formatted data as input, as output by the parser, and assuch needs pre-tokenized and tagged input.Relation Extraction.
The relation extractor (RE) is atree-kernel based system developed at the University ofTrento (Moschitti, 2006; Plank and Moschitti, 2013).Tree kernel-based methods have been shown to outper-form feature-based RE approach (Nguyen et al, 2015).The system takes as input the entity mentions detectedby the EMD module (which provides information onthe entity types, i.e.
PERSON, LOCATION, ORGA-NIZATION or ENTITY).The first version of the relation extractor was trainedon the ACE 2004 data.
It provides the following binaryrelations as output: Physical, Personal/Social, Employ-ment/Membership, PER/ORG Affiliation and GPE Af-filiation.An extended version of the Relation Extractor in-cludes an additional model trained on the CoNLL 2004data (Roth and Yih, 2004) following the setup of Giu-liano et al (2007).
The model uses a composite kernelconsisting of a constituency-based path-enclosed treekernel and a linear feature vector encoding local andglobal contexts (Giuliano et al, 2007).
The CoNLL2004 model contains the following relations: LiveIn,LocatedIn, WorkFor, OrgBasedIn, Kill.Both models exhibit state-of-the art performance.For the ACE 2004 data, experiments are reportedin (Plank and Moschitti, 2013).
For the CoNLL 2004data, our model achieves results comparable to oradvancing the state-of-the-art (Giuliano et al, 2007;Ghosh and Muresan, 2012).Coreference Resolution.
Our coreference resolutionAnalysis Engine is a wrapper around BART?a toolkitfor Coreference Resolution developed at the Universityof Trento (Versley et al, 2008; Uryupina et al, 2012).It is a modular anaphora resolution system that sup-ports state-of-the-art statistical approaches to the taskand enables efficient feature engineering.
BART imple-ments several models of anaphora resolution (mention-pair and entity-mention; best-first vs. ranking), has in-terfaces to different machine learners (MaxEnt, SVM,decision trees) and provides a large set of linguisticallymotivated features, along with the possibility to designnew ones.Entity Linking.
The Entity Linking Analysis Engine(?Semanticizer?)
makes use of the Entity Linking WebService developed by the University of Amsterdam159Annotator English Italian Spanish Dutchtokenizer Stanford TextPro IXA xTas/FrogPOS-tagger Stanford TextPro IXA xTas/FrogNER Stanford TextPro IXA xTas/FrogParsing Stanford, LTH FBK-Berkeley IXA xTas/AlpinoEntity Mention Detection BART BART-Ita - -Opinion Mining Johansson&Moschitti (2001) - - -Relation Extraction RE-UNITN RE-UNITN unlex - -Coreference BART Bart-Ita - -Entity Linking Semanticizer Semanticizer Semanticizer SemanticizerTable 1: Supported modules for different languages(Meij et al, 2012).
The web service supports auto-matic linking of an input text to Wikipedia articles: theoutput of the web service API is a list of IDs of recog-nized articles, together with confidence scores as wellas the part of the input text that was matched.
This en-tity linking module can be considered as cross-lingualand cross-document co-reference resolution, since en-tity mentions in documents in different languages aredisambiguated and linked to Wikipedia articles.
Eachannotation unit corresponds to a span in the documentand is labeled with two attributes: the correspondingWikipedia ID and the system?s confidence.3.2 ItalianFor Italian, we have been able to integrate language-specific processors for tokenization, sentence splitting,named entity recognition, parsing, mention detectionand coreference.
For relation extraction, we have fol-lowed a domain adaptation approach, transferring anunlexicalized model learned on the English data.
A de-tailed description of our annotators for Italian is pro-vided below.TextPro wrapper.
To provide basic levels of linguis-tic processing, we rely on TextPro?a suite of Natu-ral Language Processing tools for analysis of Italian(and English) texts (Pianta et al, 2008).
The suitehas been designed to integrate various NLP compo-nents developed by researchers at Fondazione BrunoKessler (FBK).
The TextPro suite has shown excep-tional performance for several NLP tasks at multipleEvalIta competitions.
Moreover, the toolkit is beingconstantly updated and developed further by FBK.
Wecan therefore be sure that TextPro provides state-of-the-art processing for Italian.TextPro combines rule-based and statistical meth-ods.
It also allows for a straightforward integrationof task-specific user-defined pre- and post-processingtechniques.
For example, one can customize TextProto provide better segmentation for web data.TextPro is not a part of the LiMoSINe pipeline, itcan be obtained from FBK and installed on any plat-form in a straightforward way.
No TextPro installationis needed for the client version of the semantic model.Parsing.
A model has been trained for Italian on theTorino Treebank data3using the Berkeley parser by theFondazione Bruno Kessler.
The treebank being rela-tively small, a better performance can be achieved byenforcing TextPro part-of-speech tags when trainingand running the parser.
Both the Torino Treebank it-self and the parsing model use specific tagsets that donot correspond to the Penn TreeBank tags of the En-glish parser.
To facilitate cross-lingual processing andenable unlexicalized cross-lingual modeling for deepsemantic tasks, we have mapped these tagsets to eachother.Entity Mention Detection.
We have adjusted ourEntity Mention Detection analysis engine to cover theItalian data.
Similarly to the English module, we useBART to heuristically extract mention boundaries fromparse trees.
However, due to the specifics of the TorinoTreebank annotation guidelines, we had to change theextraction rules substantially.Relation Extraction.
Since no relation extractiondatasets are available for Italian, we have opted for adomain adaptation solution, learning an unlexicalizedmodel on the English RE data.
This model aims atcapturing structural patterns characteristic for specificrelations through tree kernel-based SVMs.
This solu-tion requires some experiments on making English andItalian parse trees more uniform, for example, on trans-lating the tagsets.
We extract tree-based patterns forCoNLL-2004 relations (see above) from the unlexical-ized variant of the English corpus and then run it onmodified Italian parse trees.
Clearly, this model cannotprovide robust and accurate annotation.
It can, how-ever, be used as a benchmark for supervised RE inItalian.
To improve the model?s precision, we have re-stricted its coverage to named entities in contrast to allthe nominal mentions used by the English RE models.Coreference Resolution.
A coreference model forBART has been trained on the Italian portion of theSemEval-2010 Task 1 dataset (Uryupina and Moschitti,2014).
Apart from retraining the model, we have in-corporated some language-specific features to account,3http://www.di.unito.it/?tutreeb/160for example, for abbreviation and aliasing patterns inItalian.
The Italian version of BART, therefore, isa high-performance language-specific system.
It hasshown reliable performance at the recent shared tasksfor Italian, in particular, at the SemEval-2010 Task 1(Broscheit et al, 2010) and at the EvalIta 2009 (Biggioet al, 2009).Both our English and Italian coreference modulesare based on BART.
Their configurations (parametersettings and features) have been optimized separatelyto enhance the performance level on a specific lan-guage.
Since BART is a highly modular toolkit it-self and its language-specific functionality can be con-trolled via a Language Plugin, no extra BART installa-tion is required to run the Italian coreference resolver.3.3 SpanishWe have tested two publicly available toolkits support-ing language processing in Spanish: OpenNLP andIXA (Agerri et al, 2014).
The latter has shown a betterperformance level and has therefore been integrated forthe final release of the LiMoSINe pipeline.For tokenization, we rely on the ixa-pipe-toklibrary (version 1.5.0) from the IXA pipes project.Since it uses FSA technology for the tokenization anda rule-based segmenter, it is fast (tokenizing around250K words/s) and expected to be valid accross severaldialects of Spanish (Agerri et al, 2014).The POS tags are assigned by using the IXA modelfor Maximum Entropy POS tagging, and reportedto provide 98.88% accuracy (Agerri et al, 2014).Lemmatization uses the morfologik-stemming toolkit,based on FSA for a lower memory footprint (up to 10%the size of a full-fledged dictionary).Named entities (PERSON, LOCATION, ORGANI-ZATION and MISC) are annotated using the MaximumEntropy model of IXA trained on the CONLL 2002dataset and tags.Finally, the IXA pipeline provides a module for con-stituency parsing trained on the (Iberian) Spanish sec-tion of the AnCora corpus.3.4 DutchFor Dutch, we have been able to integrate language-specific processors for tokenization, sentence splitting,lemmatization, named entity recognition, dependencytree, and part-of-speech tagging.To provide basic levels of linguistic processing, werely on xTas?a text analysis suite for English andDutch (de Rooij et al, 2012).
The suite has been de-signed to integrate various NLP components developedby researchers at University of Amsterdam and is ex-tendable to work with components from other parties.xTas is designed to leverage distributed environmentsfor speeding up computationally demanding NLP tasksand is available as a REST web service.
xTas and in-structions on how to install it and set it up can be foundat http://xtas.net.Most of the Dutch processors at xTas come fromFrog, a third-party module.
Frog, formerly known asTadpole, is an integration of memory-based NLP mod-ules developed for Dutch (van den Bosch et al, 2007).All NLP modules are based on Timbl, the Tilburgmemory-based learning software package.
Most mod-ules were created in the 1990s at the ILK ResearchGroup (Tilburg University, the Netherlands) and theCLiPS Research Centre (University of Antwerp, Bel-gium).
Over the years they have been integrated intoa single text processing tool.
More recently, a depen-dency parser, a base phrase chunker, and a named-entity recognizer module were added.For dependency parsing, xTas uses Alpino, a third-party module.4Annotation typically starts with pars-ing a sentence with the Alpino parser, a wide cover-age parser of Dutch text.
The number of parses that isgenerated is reduced through interactive lexical analy-sis and constituent marking.
The selection of the bestparse is done efficiently with the parse selection tool.4 Conclusion and Future/Ongoing workIn this paper, we have presented the LiMoSINepipeline?a platform supporting state-of-the-art NLPtechnology for English, Italian, Spanish and Dutch.Based on UIMA, it allows for efficient parallel process-ing of large volumes of text.
The pipeline is distributedin two versions: the client application is oriented to po-tential users that need high-performance standard toolsat a zero engineering cost.
The local version, on thecontrary, requires some installation and configurationeffort, but in return it offers a great flexibility in imple-menting and integrating user-specific modules.Since the beginning of the LiMoSINe project, theplatform has been used for providing robust prepro-cessing for a variety of high-level tasks.
Thus, wehave recently shown how structural representations, ex-tracted with our pipeline, improve multilingual opinionmining on YouTube (Severyn et al, 2015) or crosswordpuzzle resolution (Barlacchi et al, 2014).The pipeline has been adopted by other parties, mostimportantly by the joint QCRI and MIT project IYAS(Interactive sYstem for Answer Selection).
IYAS fo-cuses on Question Answering, showing that represen-tations, based on linguistic preprocessing, significantlyoutperform more shallow methods (Tymoshenko andMoschitti, 2015; Tymoshenko et al, 2014).As part of the LiMoSINe project, we have createdthe LiMoSINe Common Corpus: a large collection ofdocuments downloaded from different web resources4http://www.let.rug.nl/vannoord/alp/Alpino/161in any of the four addressed languages.
These datawere annotated automatically.
We illustrate the pro-cessing capabilities of our pipeline on the Spanish partof the corpus (EsLCC).
To this end, we developed aUIMA Collection Processing Engine (CPE).
Once theEsLCC was downloaded it was first tidied up withApache Tika.
The pipeline was then applied to cleantext.
It was capable of processing the approximately103K EsLCC documents in a little bit more than 24hours on an Ubuntu 14.04 with 16GB of RAM, on anIntel i7@3.50GHz?
8 core box.Currently, the QCRI team is working on extendingthe pipeline, integrating various preprocessing modulesfor Arabic.5 AcknowledgementsThis work has been supported by the EU Projects FP7LiMoSINe and H2020 5G-CogNet.ReferencesR.
Agerri, J. Bermudez, and G. Rigau.
2014.
IXA pipeline:Efficient and ready to use multilingual NLP tools.
InLREC.G.
Barlacchi, M. Nicosia, and A. Moschitti.
2014.
Learn-ing to rank answer candidates for automatic resolution ofcrossword puzzles.
In CoNLL-2014.S.
M. Bernaola Biggio, C. Giuliano, M. Poesio, Y. Versley,O.
Uryupina, and R. Zanoli.
2009.
Local entity detectionand recognition task.
In EvalIta-2009.E.
Breck, Y. Choi, and C. Cardie.
2007.
Identifying expres-sions of opinion in context.
In IJCAI.S.
Broscheit, M. Poesio, S.P.
Ponzetto, K.J.
Rodriguez,L.
Romano, O. Uryupina, and Y. Versley.
2010.
BART:A multilingual anaphora resolution system.
In SemEval.H.
Cunningham, D. Maynard, K. Bontcheva, V. Tablan,N.
Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,D.
Damljanovic, T. Heitz, M.A.
Greenwood, H. Saggion,J.
Petrak, Y. Li, and W. Peters.
2011.
Text Processingwith GATE (Version 6).O.
de Rooij, J. van Gorp, and Maarten de Rijke.
2012. xtas:Text analysis in a timely manner.
In DIR 2012: 12thDutch-Belgian Information Retrieval Workshop.D.A.
Ferrucci, E.W.
Brown, J. Chu-Carroll, J. Fan,D.
Gondek, A. Kalyanpur, A. Lally, J.W.
Murdock,E.
Nyberg, J.M.
Prager, N. Schlaefer, and Ch.A.
Welty.2010.
Building Watson: An overview of the DeepQAproject.
AI Magazine, pages 59?79.D.
Ghosh and S. Muresan.
2012.
Relation classificationusing entity sequence kernels.
In COLING 2012, pages391?400.C.
Giuliano, A. Lavelli, and L. Romano.
2007.
Relationextraction and the influence of automatic named-entityrecognition.
ACM Trans.
Speech Lang.
Process., 5(1).R.
Johansson and A. Moschitti.
2011.
Extracting opinionexpressions and their polarities ?
exploration of pipelinesand joint models.
In ACL.R.
Johansson and P. Nugues.
2008.
Dependency-based se-mantic role labeling of PropBank.
In EMNLP.C.D.
Manning, M. Surdeanu, J. Bauer, J. Finkel, S.J.Bethard, and D. McClosky.
2014.
The StanfordCoreNLP natural language processing toolkit.
In ACLSystem Demonstrations.E.
Meij, W. Weerkamp, and M. de Rijke.
2012.
Addingsemantics to microblog posts.
In WSDM.A.
Moschitti.
2006.
Efficient convolution kernels for depen-dency and constituent syntactic trees.
Machine Learning:ECML 2006.T.H.
Nguyen, B. Plank, and R. Grishman.
2015.
Semanticrepresentations for domain adaptation: A case study onthe tree kernel-based method for relation extraction.
InACL.L.
Padr?o and E. Stanilovsky.
2012.
Freeling 3.0: Towardswider multilinguality.
In LREC, Istanbul, Turkey, May.ELRA.E.
Pianta, Ch.
Girardi, and R. Zanoli.
2008.
The TextProtool suite.
In LREC.B.
Plank and A. Moschitti.
2013.
Embedding semantic sim-ilarity in tree kernels for domain adaptation of relationextraction.
In ACL.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InCoNLL.Aliaksei Severyn, Alessandro Moschitti, Olga Uryupina,and Barbara Plank.
2015.
Multilingual opinion miningon YouTube.
Information Processing and Management.K.
Tymoshenko and A. Moschitti.
2015.
Assessing the im-pact of syntactic and semantic structures for answer pas-sages reranking.
In ACM CIKM.K.
Tymoshenko, A. Moschitti, and A. Severyn.
2014.
En-coding semantic resources in syntactic structures for pas-sage reranking.
In EACL.O.
Uryupina and A. Moschitti.
2013.
Multilingual mentiondetection for coreference resolution.
In IJCNLP.O.
Uryupina and A. Moschitti.
2014.
Coreference resolu-tion for Italian: Assessing the impact of linguistic com-ponents.
In CLIC-it.O.
Uryupina, S. Saha, A. Ekbal, and M. Poesio.
2011.Multi-metric optimization for coreference: The UniTN /IITP / Essex submission to the 2011 CONLL shared task.In CoNLL.O.
Uryupina, A. Moschitti, and M. Poesio.
2012.
BARTgoes multilingual: The UniTN / Essex submission to theCoNLL-2012 Shared Task.
In CoNLL.A.
van den Bosch, B. Busser, S. Canisius, and W. Daele-mans.
2007.
An efficient memory-based morphosyntac-tic tagger and parser for Dutch.
In CLIN.
Leuven, Bel-gium.Y.
Versley, S.P.
Ponzetto, M. Poesio, V. Eidelman, A. Jern,J.
Smith, X. Yang, and A. Moschitti.
2008.
BART: amodular toolkit for coreference resolution.
In ACL.162
