Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
287?295, Prague, June 2007. c?2007 Association for Computational LinguisticsUsing RBMT Systems to Produce Bilingual Corpus for SMTXiaoguang Hu, Haifeng Wang, Hua WuToshiba (China) Research and Development Center5/F., Tower W2, Oriental PlazaNo.1, East Chang An Ave., Dong Cheng DistrictBeijing, 100738, China{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cnAbstractThis paper proposes a method using the ex-isting Rule-based Machine Translation(RBMT) system as a black box to producesynthetic bilingual corpus, which will beused as training data for the Statistical Ma-chine Translation (SMT) system.
We usethe existing RBMT system to translate themonolingual corpus into synthetic bilingualcorpus.
With the synthetic bilingual corpus,we can build an SMT system even if thereis no real bilingual corpus.
In our experi-ments using BLEU as a metric, the systemachieves a relative improvement of 11.7%over the best RBMT system that is used toproduce the synthetic bilingual corpora.We also interpolate the model trained on areal bilingual corpus and the modelstrained on the synthetic bilingual corpora.The interpolated model achieves an abso-lute improvement of 0.0245 BLEU score(13.1% relative) as compared with the in-dividual model trained on the real bilingualcorpus.1 IntroductionWithin the Machine Translation (MT) field, by farthe most dominant paradigm is SMT, but manyexisting commercial systems are rule-based.
In thisresearch, we are interested in answering the ques-tion of whether the existing RBMT systems couldbe helpful to the development of an SMT system.To find the answer, let us first consider the follow-ing facts:?
Existing RBMT systems are usually pro-vided as a black box.
To make use of suchsystems, the most convenient way mightbe working on the translation results di-rectly.?
SMT methods rely on bilingual corpus.
Asa data driven method, SMT usually needslarge bilingual corpus as the training data.Based on the above facts, in this paper we pro-pose a method using the existing RBMT system asa black box to produce a synthetic bilingual cor-pus1, which will be used as the training data for theSMT system.For a given language pair, the monolingual cor-pus is usually much larger than the real bilingualcorpus.
We use the existing RBMT system totranslate the monolingual corpus into syntheticbilingual corpus.
Then, even if there is no real bi-lingual corpus, we can train an SMT system withthe monolingual corpus and the synthetic bilingualcorpus.
If there exist n available RBMT systemsfor the desired language pair, we use the n systemsto produce n synthetic bilingual corpora, and ntranslation models are trained with the n corporarespectively.
We name such a model the syntheticmodel.
An interpolated translation model is builtby linear interpolating the n synthetic models.
Inour experiments using BLEU (Papineni et al, 2002)as the metric, the interpolated synthetic modelachieves a relative improvement of 11.7% over thebest RBMT system that is used to produce the syn-thetic bilingual corpora.1 In this paper, to be distinguished from the real bilingual cor-pus, the bilingual corpus generated by the RBMT system iscalled a synthetic bilingual corpus.287Moreover, if a real bilingual corpus is availablefor the desired language pair, we build anothertranslation model, which is named the standardmodel.
Then we can build an interpolated modelby interpolating the standard model and the syn-thetic models.
Experimental results show that theinterpolated model achieves an absolute improve-ment of 0.0245 BLEU score (13.1% relative) ascompared with the standard model.The remainder of this paper is organized as fol-lows.
In section 2 we summarize the related work.We then describe our method Using RBMT sys-tems to produce bilingual corpus for SMT in sec-tion 3.
Section 4 describes the resources used in theexperiments.
Section 5 presents the experimentresult, followed by the discussion in section 6.
Fi-nally, we conclude and present the future work insection 7.2 Related WorkIn the MT field, by far the most dominantparadigm is SMT.
SMT has evolved from theoriginal word-based approach (Brown et al, 1993)into phrase-based approaches (Koehn et al, 2003;Och and Ney, 2004) and syntax-based approaches(Wu, 1997; Alshawi et al, 2000; Yamada andKnignt, 2001; Chiang, 2005).
On the other hand,much important work continues to be carried out inExample-Based Machine Translation (EBMT)(Carl et al, 2005; Way and Gough, 2005), andmany existing commercial systems are rule-based.Although we are not aware of any previous at-tempt to use an existing RBMT system as a blackbox to produce synthetic bilingual training corpusfor general purpose SMT systems, there exists agreat deal of work on MT hybrids and Multi-Engine Machine Translation (MEMT).Research into MT hybrids has increased over thelast few years.
Some research focused on the hy-brid of various corpus-based MT methods, such asSMT and EBMT (Vogel and Ney, 2000; Marcu,2001; Groves and Way, 2006; Menezes and Quirk,2005).
Others tried to exploit the advantages ofboth rule-based and corpus-based methods.
Habashet al (2006) built an Arabic-English generation-heavy MT system and boosted it with SMT com-ponents.
METIS-II is a hybrid machine translationsystem, in which insights from SMT, EBMT, andRBMT are used (Vandeghinste et al, 2006).
Seneffet al (2006) combined an interlingual translationframework with phrase-based SMT for spokenlanguage translation in a limited domain.
Theyautomatically generated a corpus of English-Chinese pairs from the same interlingual represen-tation by parsing the English corpus and then para-phrasing each utterance into both English and Chi-nese.Frederking and Nirenburg (1994) produced thefirst MEMT system by combining outputs fromthree different MT engines based on their knowl-edge of the inner workings of the engines.
Nomoto(2004) used voted language models to select thebest output string at sentence level.
Some recentapproaches to MEMT used word alignment tech-niques for comparison between the MT systems(Jayaraman and Lavie, 2005; Zaanen and Somers,2005; Matusov et al 2006).
All the above MEMTsystems operate on MT outputs for complete inputsentences.
Mellebeek et al (2006) presented a dif-ferent approach, using a recursive decompositionalgorithm that produces simple chunks as input tothe MT engines.
A consensus translation is pro-duced by combining the best chunk translation.This paper uses RBMT outputs to improve theperformance of SMT systems.
Instead of RBMToutputs, other researchers have used SMT outputsto boost translation quality.
Callision-Burch andOsborne (2003) used co-training to extend existingparallel corpora, wherein machine translations areselectively added to training corpora with multiplesource texts.
They also created training data for alanguage pair without a parallel corpus by usingmultiple source texts.
Ueffing (2006) exploredmonolingual source-language data to improve anexisting machine translation system via self-training.
The source data is translated by a SMTsystem, and the reliable translations are automati-cally identified.
Both of the methods improvedtranslation quality.3 MethodIn this paper, we use the synthetic and real bilin-gual corpus to train the phrase-based translationmodels.3.1  Phrase-Based ModelsAccording to the translation model presented in(Koehn et al, 2003), given a source sentence f ,the best target translation  can be obtainedusing the following modelbeste288)()()(maxarg)(maxarg||eeeeeffeelengthLMbest?ppp==(1)Where the translation model can bedecomposed into)( | efp?=??=IiiiiiiiIIaefpbadefefp1111),|()()|()|(??
w(2)Where )|( ii ef?
is the phrase translation prob-ability.
denotes the start position of the sourcephrase that was translated into the ith target phrase,and  denotes the end position of the sourcephrase translated into the (i-1)th target phrase.is the distortion probability.ia1?ib)( 1??
ii bad),|( aefp iiw  is the lexical weight, and ?
is thestrength of the lexical weight.3.2 Interpolated ModelsWe train synthetic models with the synthetic bilin-gual corpus produced by the RBMT systems.
Wecan also train a translation model, namely standardmodel, if a real bilingual corpus is available.
Inorder to make full use of these two kinds of cor-pora, we conduct linear interpolation between them.In this paper, the distortion probability in equa-tion (2) is estimated during decoding, using thesame method as described in Pharaoh (Koehn,2004).
For the phrase translation probability andlexical weight, we interpolate them as shown in (3)and (4).
?==niii efef0)|()|( ???
(3)?==niii aefpaefp0),|(),|( w,w ?
(4)Where )|(0 ef?
and ),|( aefpw,0  denote thephrase translation probability and lexical weighttrained with the real bilingual corpus, respectively.
)|( efi?
and ),|( aefp iw,  ( ) are thephrase translation probability and lexical weightestimated by n  synthetic corpora produced by theRBMT systems.ni ,...,1=i?
and i?
are interpolation coef-ficients, ensuring  and .
10=?=nii?
10=?=nii?4 Resources Used in Experiments4.1 DataIn the experiments, we take English-Chinese trans-lation as a case study.
The real bilingual corpusincludes 494,149 English-Chinese bilingual sen-tence pairs.
The monolingual English corpus isselected from the English Gigaword Second Edi-tion, which is provided by Linguistic Data Consor-tium (LDC) (catalog number LDC2005T12).
Theselected monolingual corpus includes 1,087,651sentences.For language model training, we use part of theChinese Gigaword Second Edition provided byLDC (catalog number LDC2005T14).
We use41,418 documents selected from the ZaoBaoNewspaper and 992,261 documents from the Xin-Hua News Agency to train the Chinese languagemodel, amounting to 5,398,616 sentences.The test set and the development set are fromthe corpora distributed for the 2005 HTRDP 2evaluation of machine translation.
It can be ob-tained from Chinese Linguistic Data Consortium(catalog number 2005-863-001).
We use the same494 sentences in the test set and 278 sentences inthe development set.
Each source sentence in thetest set and the development set has 4 different ref-erences.4.2 ToolsIn this paper, we use two off-the-shelf commercialEnglish to Chinese RBMT systems to produce thesynthetic bilingual corpus.We also need a trainer and a decoder to performphrase-based SMT.
We use Koehn's trainingscripts 3  to train the translation model, and theSRILM toolkit (Stolcke, 2002) to train languagemodel.
For the decoder, we use Pharaoh (Koehn,2004).
We run the decoder with its default settings(maximum phrase length 7) and then use Koehn'simplementation of minimum error rate training(Och, 2003) to tune the feature weights on the de-2 The full name of HTRDP is National High Technology Re-search and Development Program of China, also named as 863Program.3  It is located at http://www.statmt.org/wmt06/shared-task/baseline.html.289velopment set.
The translation quality is evaluatedusing a well-established automatic measure: BLEUscore (Papineni et al, 2002).
We use the samemethod described in (Koehn and Monz, 2006) toperform the significance test.5 Experimental Results5.1 Results on Synthetic Corpus OnlyWith the monolingual English corpus and the Eng-lish side of the real bilingual corpus, we translatethem into Chinese using the two commercialRBMT systems and produce two synthetic bilin-gual corpora.
With the corpora, we train two syn-thetic models as described in section 3.1.
Based onthe synthetic models, we also perform linear inter-polation as shown in section 3.2, without the stan-dard models.
We tune the interpolation weightsusing the development set, and achieve the bestperformance when 58.01 =?
, 42.02 =?
,58.01 =?
, and 42.02 =?
.
The translation resultson the test set are shown in Table 1.
Syntheticmodel 1 and 2 are trained using the synthetic bilin-gual corpora produced by RBMT system 1 andRBMT system 2, respectively.Method BLEURBMT system 1 0.1681RBMT system 2 0.1453Synthetic Model 1 0.1644Synthetic Model 2 0.1668Interpolated Synthetic Model 0.1878Table 1.
Translation Results Using Synthetic Bi-lingual CorpusFrom the results, it can be seen that the interpo-lated synthetic model obtains the best result, withan absolute improvement of the 0.0197 BLEU(11.7% relative) as compared with RBMT system1, and 0.0425 BLEU (29.2% relative) as comparedwith RBMT system 2.
It is very promising that ourmethod can build an SMT system that significantlyoutperforms both of the two RBMT systems, usingthe synthetic bilingual corpus produced by twoRBMT systems.5.2 Results on Real and Synthetic CorpusWith the real bilingual corpus, we build a standardmodel.
We interpolate the standard model with thetwo synthetic models built in section 5.1 to obtaininterpolated models.
The translation results areshown in Table 2.
The interpolation coefficientsare both for phrase table probabilities and lexicalweights.
They are also tuned using the develop-ment set.From the results, it can be seen that all the threeinterpolated models perform not only better thanthe RBMT systems but also better than the SMTsystem trained on the real bilingual corpus.
Theinterpolated model combining the standard modeland the two synthetic models performs the best,achieving a statistically significant improvement ofabout 0.0245 BLEU (13.1% relative) as comparedwith the standard model with no synthetic corpus.It also achieves 26.1% and 45.8% relative im-provement as compared with the two RBMT sys-tems respectively.
The results indicate that usingthe corpus produced by RBMT systems, the per-formance of the SMT system can be greatly im-proved.
The results also indicate that the more theRBMT systems are used, the better the translationquality is.Interpolation CoefficientsStandardmodelSyntheticModel 1SyntheticModel 2BLEU1 ?
?
0.18740.90 0.10 ?
0.20560.86 ?
0.14 0.20400.70 0.12 0.18 0.2119Table 2.
Translation Results Using Standard andSynthetic Bilingual Corpus5.3 Effect of Synthetic Corpus SizeTo explore the relationship between the translationquality and the scale of the synthetic bilingual cor-pus, we interpolate the standard model with thesynthetic models trained with synthetic bilingualcorpus of different sizes.
In order to simplify theprocedure, we only use RBMT system 1 to trans-late the 1,087,651 monolingual English sentencesto produce the synthetic bilingual corpus.We randomly select 20%, 40%, 60%, 80%, and100% of the synthetic bilingual corpus to train dif-ferent synthetic models.
The translation results ofthe interpolated models are shown in Figure 1.
Theresults indicate that the larger the synthetic bilin-gual corpus is, the better translation performancewould be.2900.130.150.170.190.2120 40 60 80 100Synthetic Bilingual Corpus (%)BLEUInterpolatedStandardSynthetic0.120.140.160.180.20.2220 40 60 80 100Real Bilingual Corpus (%)BLEUInterpolatedStandardSyntheticFigure 1.
Comparison of Translation Results UsingSynthetic Bilingual Corpus of Different SizesFigure 2.
Comparison of Translation Results UsingReal Bilingual Corpus of Different Sizes5.4 Effect of Real Corpus Size Interpolation CoefficientsStandardmodelSyntheticModel 1SyntheticModel 2BLEU1 ?
?
0.1874?
1 ?
0.1560?
?
1 0.15220.80 0.10 0.10 0.1972Another issue is the relationship between the SMTperformance and the size of the real bilingual cor-pus.
To train different standard models, we ran-domly build five corpora of different sizes, whichcontain 20%, 40%, 60%, 80%, and 100% sentencepairs of the real bilingual corpus, respectively.
Asto the synthetic model, we use the same syntheticmodel 1 that is described in section 5.1.
Then webuild five interpolated models by performing linearinterpolation between the synthetic model and thefive standard models respectively.
The translationresults are shown in Figure 2.Table 3.
Translation Results without AdditionalMonolingual CorpusStandard ModelSyntheticModel 1SyntheticModel 2StandardModel 6,105,260 ?
?SyntheticModel 1 356,795 12,062,068 ?SyntheticModel 2 357,489 881,921 9,216,760From the results, we can see that the larger thereal bilingual corpus is, the better the performanceof both standard models and interpolated modelswould be.
The relative improvement of BLEUscores is up to 27.5% as compared with the corre-sponding standard models.Table 4.
Numbers of Phrase Pairs  5.5 Results without Additional MonolingualCorpus cant improvement of about 0.01 BLEU (5.2% rela-tive) as compared with the standard model withoutusing the synthetic corpus.
In all the above experiments, we use an additional English monolingual corpus to get more syntheticbilingual corpus.
We are also interested in the re-sults without the additional monolingual corpus.
Insuch case, the only English monolingual corpus isthe English side of the real bilingual corpus.
Weuse this smaller size of monolingual corpus and thereal bilingual corpus to conduct similar experi-ments as in section 5.2.
The translation results areshown in Table 3.In order to further analyze the translation results,we examine the overlap and the difference amongthe phrase tables.
The analytic results are shown inTable 4.
More phrase pairs are extracted by thesynthetic models, about twice by the syntheticmodel 1 in particular, than those extracted by thestandard model.
The overlap between each modelis very low.
For example, about 6% phrase pairsextracted by the standard model make appearancein both the standard model and the synthetic model1.
This also explains why the interpolated modeloutperforms that of the standard model in Table 3.From the results, it can be seen that our methodworks well even if no additional monolingual cor-pus is available.
We achieve a statistically signifi-291Methods English Sentence / Chinese Translations BLEUThis move helps spur the enterprise to strengthen technical innovation, man-agement innovation and the creation of a brand name and to strengthen mar-keting, after-sale service, thereby fundamentally enhance the enterprise'scompetitiveness;Standardmodel?
?
??
???
??
??
??
??
??
?
??
??
?
??
????
?
??
?
??
?
??
?
??
??
?
???
?
?
??
??
0.5022RBMT Sys-tem 1??
??
??
??
??
??
??
?
??
?
??
??
?
??
????
??
??
??
?
?
??
??
?
???
??
??
??
?
??
?0.1535RBMT Sys-tem 2??
??
??
??
??
??
??
??
??
?
??
??
?
?
???
?
??
?
??
??
??
?
??
????
??
??
??
???
??
?0.1485InterpolatedModel?
?
??
???
??
??
??
??
??
?
??
??
?
??
???
??
??
??
?
????
?
??
?
??
?
??
??
?
???
?0.7198Table 5.
Translation ExampleThis move  ?
?
??
This move  ?
?
?
?helps  ???
helps  ??
?spur  ??
spur  ?
?the enterprise  ??
the enterprise  ?
?to strengthen  ??
to strengthen  ?
?technical  ??
technical  ?
?innovation  ??
innovation  ?
?, management  ?
??
, management  ?
?
?innovation  ??
innovation  ?
?and the creation of a  ?
??
and the creation of  ?
??
(he jianli)  (he chuangzao)brand name  ??
a brand name  ??
(pinpai)  (pinpai)and to strengthen  ??
?
and to strengthen  ?
?
?marketing ,  ??
marketing ,  ??
??
?
(fuwu) after-sale service  ???
?after-sale  ?
??
(shouhoufuwu)service  ?
??
?
, thereby  ?
?
?, thereby  ??
fundamentally  ?
??
?fundamentally  ??
enhance the  ?
?enhance  ?
???
enterprise 's  ??
?the enterprise  ?
competitiveness  ???
's competitiveness  ?
??
;  ?
;  ??
(shouhou)(a) Results Produced by the Standard Model (b) Results Produced by the Interpolated ModelFigure 3.
Phrase Pairs Used for Translation2926 Discussion6.1 Model Interpolation vs. Corpus MergeIn section 5, we make use of the real bilingual cor-pus and the synthetic bilingual corpora by perform-ing model interpolation.
Another available way isdirectly combining these two kinds of corpora totrain a translation model, namely corpus merge.
Inorder to compare these two methods, we useRBMT system 1 to translate the 1,087,651 mono-lingual English sentences to produce synthetic bi-lingual corpus.
Then we train an SMT system withthe combination of this synthetic bilingual corpusand the real bilingual corpus.
The BLEU score ofsuch system is 0.1887, while that of the model in-terpolation system is 0.2020.
It indicates that themodel interpolation method is significantly betterthan the corpus merge method.6.2 Result AnalysisAs discussed in Section 5.5, the number of theoverlapped phrase pairs among the standard modeland the synthetic models is very small.
The newlyadded phrase pairs from the synthetic models canassist to improve the translation results of the in-terpolated model.
In this section, we will use anexample to further discuss the reason behind theimprovement of the SMT system by using syn-thetic bilingual corpus.
Table 5 shows an Englishsentence and its Chinese translations produced bydifferent methods.
And Figure 3 shows the phrasepairs used for translation.
The results show thatimperfect translations of RBMT systems can bealso used to boost the performance of an SMT sys-tem.Phrase PairsPhrasePairsUsedNewPairsUsedStandardModel 6,105,260 5,509 ?InterpolatedModel 73,221,525 5,306 1993Table 6.
Statistics of Phrase PairsFurther analysis is shown in Table 6.
After add-ing the synthetic corpus produced by the RBMTsystems, the interpolated model outperforms thestandard models mainly for the following two rea-sons: (1) some new phrase pairs are added into theinterpolated model.
37.6% phrase pairs (1993 outof 5306) are newly learned and used for translation.For example, the phrase pair "after-sale service <->????
(shouhoufuwu)" is added; (2) The prob-ability distribution of the phrase pairs is changed.For example, the probabilities of the two pairs "abrand name <-> ??
(pinpai)" and "and the crea-tion of <-> ?
??
(he chuangzao)" increase.
Theprobabilities of the other two pairs "brand name <-> ??
(pinpai)" and "and the creation of a <-> ???
(he jianli)" decrease.
We found that 930phrase pairs, which are also in the phrase table ofthe standard model, are used by the interpolatedmodel for translation but not used by the standardmodel.6.3 Human EvaluationAccording to (Koehn and Monz, 2006; Callison-Burch et al, 2006), the RBMT systems are usuallynot adequately appreciated by BLEU.
We alsomanually evaluated the RBMT systems and SMTsystems in terms of both adequacy and fluency asdefined in (Koehn and Monz, 2006).
The evalua-tion results show that the SMT system with theinterpolated model, which achieves the highestBLEU scores in Table 2, achieves slightly betteradequacy and fluency scores than the two RBMTsystems.7 Conclusion and Future WorkWe presented a method using the existing RBMTsystem as a black box to produce synthetic bilin-gual corpus, which was used as training data forthe SMT system.
We used the existing RBMT sys-tem to translate the monolingual corpus into a syn-thetic bilingual corpus.
With the synthetic bilingualcorpus, we could build an SMT system even ifthere is no real bilingual corpus.
In our experi-ments using BLEU as the metric, such a systemachieves a relative improvement of 11.7% over thebest RBMT system that is used to produce the syn-thetic bilingual corpora.
It indicates that using theexisting RBMT systems to produce a synthetic bi-lingual corpus, we can build an SMT system thatoutperforms the existing RBMT systems.We also interpolated the model trained on a realbilingual corpus and the models trained on the syn-thetic bilingual corpora, the interpolated modelachieves an absolute improvement of 0.0245BLEU score (13.1% relative) as compared with theindividual model trained on the real bilingual cor-293pus.
It indicates that we can build a better SMTsystem by leveraging the real and the synthetic bi-lingual corpus.Further result analysis shows that after addingthe synthetic corpus produced by the RBMT sys-tems, the interpolated model outperforms the stan-dard models mainly because of two reasons: (1)some new phrase pairs are added to the interpo-lated model; (2) the probability distribution of thephrase pairs is changed.In the future work, we will investigate the possi-bility of training a reverse SMT system with theRBMT systems.
For example, we will investigateto train Chinese-to-English SMT system based onnatural English and RBMT-generated syntheticChinese.ReferencesHiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning Dependency Translation Models asCollections of Finite-State Head Transducers.
Com-putational Linguistics, 26(1): 45-60.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics, 19(2):263-311.Chris Callison-Burch and Miles Osborne.
2003.
Boot-strapping Parallel Corpora.
In Proceedings of theHuman Language Technology conference / NorthAmerican chapter of the Association for Computa-tional Linguistics (HLT/NAACL-2003) Workshop onBuilding and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 44-49.Chris Callison-Burch, Miles Osborne and PhilippKoehn, 2006.
Re-evaluating the Role of Bleu in Ma-chine Translation Research.
In Proceedings of the11th Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL-2006), pages 249-256.Michel Carl, Paul Schmidt, and Jorg Schutz.
2005.
Re-versible Template-based Shake & Bake Generation.In Proceedings of the 10th Machine TranslationSummit Workshop on Example-Based MachineTranslation, pages 17-25.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL-2005),pages 263-270.Robert Frederking and Sergei Nirenburg.
1994.
ThreeHeads Are Better Than One.
In Proceedings of the4th Applied Natural Language Processing Confer-ence (ANLP-1994), pages 95-100.Declan Groves and Andy Way.
2006.
Hybridity in MT:Experiments on the Europarl Corpus.
In Proceedingsof the 11th Annual Conference of the European As-sociation for Machine Translation (EAMT-2006),pages 115-124.Nizar Habash, Bonnie Dorr, and Christof Monz.
2006Challenges in Building an Arabic-English GHMTSystem with SMT Components.
In Proceedings ofthe 11th Annual Conference of the European Asso-ciation for Machine Translation (EAMT-2006), pages56-65.Shyamsundar Jayaraman and Alon Lavie.
2005.
Multi-engine Machine Translation Guided by ExplicitWord Matching.
In Proceedings of the 10th AnnualConference of the European Association for MachineTranslation (EAMT-2005), pages 143-152.Philipp Koehn.
2004.
Pharaoh: A Beam Search DecoderFor Phrase-Based Statistical Machine TranslationModels.
In Proceedings of the 6th Conference of theAssociation for Machine Translation in the Americas(AMTA-2004), pages 115-124.Philipp Koehn and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation be-tween European Languages.
In Proceedings of theHuman Language Technology conference / NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT/NAACL-2006) Workshop onStatistical Machine Translation, pages 102-121.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the Human Language Technology con-ference / North American Chapter of the Associationfor Computational Linguistics (HLT/NAACL-2003),pages 127-133.Daniel Marcu.
2001.
Towards a Unified Approach toMemory- and Statistical-based Machine Translation.In Proceedings of the Association for ComputationalLinguistics / European Chapter of the Association forComputational Linguistics (ACL/EACL-2001), pages378-385.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing Consensus Translation from Multi-ple Machine Translation Systems Using EnhancedHypotheses Alignment.
In Proceedings of the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL-2006),pages 33-40.294Bart Mellebeek, Karolina Owczarzak, Josef VanGenabith, and Andy Way.
2006.
Multi-engine Ma-chine Translation by Recursive Sentence Decomposi-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas(AMTA-2006), pages 110-118.Arul Menezes and Chris Quirk.
2005.
Dependencytreelet translation: the convergence of statistical andexample-based machine-translation?
In Proceedingsof the 10th Machine Translation Summit Workshopon Example-Based Machine Translation, pages 99-108.Tadashi Nomoto.
2004.
Multi-Engine machine transla-tion with voted language model.
In Proceedings ofthe 42nd Annual Meeting of the Association forComputational Linguistics (ACL-2004), pages 494-501.Franz Josef Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-2003), pages 160-167.Franz Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach To Statistical MachineTranslation.
Computational Linguistics, 30(4):417-449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL-2002), pages 311-318.Stephanie Seneff, Chao Wang, and John Lee.
2006.Combining Linguistic and Statistical Methods for Bi-Directional English Chinese Translation in the FlightDomain.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas(AMTA-2006), pages 213-222.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the 5thInternational Conference on Spoken Language Proc-essing (ICSLP-2002), pages 901-904.Nicola Ueffing.
2006.
Using Monolingual Source-Language Data to Improve MT Performance.
InProceedings of the International Workshop on Spo-ken Language Translation (IWSLT-2006), pages 174-181.Vincent Vandeghinste, Ineka Schuurman, Michael Carl,Stella Markantonatou, and Toni Badia.
2006.
Metis-II: Machine Translation for Low-Resource Lan-guages.
In Proceedings of the 5th International Con-ference on Language Resources and Evaluation (L-REC-2006), pages 1284-1289.Stephan Vogel and Hermann Ney.
2000.
Constructionof a Hierarchical Translation Memory.
In Proceed-ings of the 18th International Conference on Compu-tational Linguistics (COLING-2000), pages 1131-1135.Andy Way and Nano Gough.
2005.
Comparing Exam-ple-Based and Statistical Machine Translation.
Natu-ral Language Engineering, 11(3): 295-309.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3): 377-403.Kenji Yamada and Kevin Knight.
2001.
A Syntax BasedStatistical Translation Model.
In Proceedings of theAssociation for Computational Linguistics / Euro-pean Chapter of the Association for ComputationalLinguistics (ACL/EACL-2001), pages 523-530.Menno van Zaanen and Harold Somers.
2005.
DE-MOCRAT: Deciding between Multiple Outputs Cre-ated by Automatic Translation.
In Proceedings of the10th Machine Translation Summit, pages 173-180.295
