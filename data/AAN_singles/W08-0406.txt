Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 46?54,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSyntactic Reordering Integrated with Phrase-based SMTJakob ElmingComputational LinguisticsCopenhagen Business Schooljel.isv@cbs.dkAbstractWe present a novel approach to word re-ordering which successfully integrates syn-tactic structural knowledge with phrase-basedSMT.
This is done by constructing a latticeof alternatives based on automatically learnedprobabilistic syntactic rules.
In decoding, thealternatives are scored based on the outputword order, not the order of the input.
Un-like previous approaches, this makes it possi-ble to successfully integrate syntactic reorder-ing with phrase-based SMT.
On an English-Danish task, we achieve an absolute improve-ment in translation quality of 1.1 % BLEU.Manual evaluation supports the claim that thepresent approach is significantly superior toprevious approaches.1 IntroductionThe emergence of phrase-based statistical machinetranslation (PSMT) (Koehn et al, 2003) has beenone of the major developments in statistical ap-proaches to translation.
Allowing translation ofword sequences (phrases) instead of single wordsprovides SMT with a robustness in word selectionand local word reordering.PSMT has two means of reordering the words.
Ei-ther a phrase pair has been learned where the targetword order differs from the source (phrase internalreordering), or distance penalized orderings of targetphrases are attempted in decoding (phrase externalreordering).
The first solution is strong, the secondis weak.The second solution is necessary for reorderingswithin a previously unseen sequence or over dis-tances greater than the maximal phrase length.
Inthis case, the system in essence relies on the tar-get side language model to get the correct word or-der.
The choice is made without knowing what thesource is.
Basically, it is a bias against phrase exter-nal reordering.It seems clear that reordering often depends onhigher level linguistic information, which is absentfrom PSMT.
In recent work, there has been someprogress towards integrating syntactic informationwith the statistical approach to reordering.
In workssuch as (Xia and McCord, 2004; Collins et al, 2005;Wang et al, 2007; Habash, 2007), reordering de-cisions are done ?deterministically?, thus placingthese decisions outside the actual PSMT system bylearning to translate from a reordered source lan-guage.
(Crego andMarin?o, 2007; Zhang et al, 2007;Li et al, 2007) are more in the spirit of PSMT, inthat multiple reorderings are presented to the PSMTsystem as (possibly weighted) options.Still, there remains a basic conflict between thesyntactic reordering rules and the PSMT system:one that is most likely due to the discrepancy be-tween the translation units (phrases) and units of thelinguistic rules, as (Zhang et al, 2007) point out.In this paper, we proceed in the spirit of the non-deterministic approaches by providing the decoderwith multiple source reorderings.
But instead ofscoring the input word order, we score the order ofthe output.
By doing this, we avoid the integrationproblems of previous approaches.It should be noted that even though the experi-ments are conducted within a source reordering ap-proach, this scoring is also compatible with other ap-46proach.
We will, however, not look further into thispossiblity in the present paper.In addition, we automatically learn reorderingrules based on several levels of linguistic informa-tion from word form to subordination and syntac-tic structure to produce reordering rules that are notrestricted to operations on syntactic tree structurenodes.In the next section, we discuss and contrast re-lated work.
Section 3 describes aspects of Englishand Danish structure that are relevant to reordering.Section 4 describes the automatic induction of re-ordering rules and its integration in PSMT.
In sec-tion 5, we describe the SMT system used in theexperiments.
Section 6 evaluates and discusses thepresent approach.2 Related WorkWhile several recent authors have achieved positiveresults, it has been difficult to integrate syntactic in-formation while retaining the strengths of the statis-tical approach.Several approaches do deterministic reordering.These do not integrate the reordering in the PSMTsystem; instead they place it outside the system byfirst reordering the source language, and then havinga PSMT system translate from reordered source lan-guage to target language.
(Collins et al, 2005; Wanget al, 2007) do this using manually created rules,and (Xia and McCord, 2004) and (Habash, 2007)use automatically extracted rules.
All use rules ex-tracted from syntactic parses.As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these determinis-tic choices are beyond the scope of optimization andcannot be undone by the decoder.
That is, there is noway to make up for bad information in later transla-tion steps.Another approach is non-deterministic.
This pro-vides the decoder with both the original and the re-ordered source sentence.
(Crego and Marin?o, 2007)operate within Ngram-based SMT.
They make useof syntactic structure to reorder the input into a wordlattice.
Since the paths are not weighted, the latticemerely narrows down the size of the search space.The decoder is not given reason to trust one path (re-ordering) over another.
(Zhang et al, 2007) assign weights to the pathsof their input word lattice.
Instead of hierarchicallinguistic structure, they use reordering rules basedon POS and syntactic chunks, and train the systemwith both original and reordered source word orderon a restricted data set (<500K words).
Their sys-tem does not out-perform a standard PSMT system.As they themselves point out, a reason for this mightbe that their reordering approach is not fully inte-grated with PSMT.
This is one of the main problemsaddressed in the present work.
(Li et al, 2007) use weighted n-best lists as inputfor the decoder.
They use rules based on a syntac-tic parse, allowing children of a tree node to swapplace.
This is excessively restrictive.
For example,a common reordering in English-Danish translationhas the subject change place with the finite verb.Since the verb is often embedded in a VP contain-ing additional words that should not be moved, suchrules cannot be captured by local reordering on treenodes.In many cases, the exact same word order thatis obtained through a source sentence reordering, isalso accessible through a phrase internal reordering.A negative consequence of source order (SO) scor-ing as done by (Zhang et al, 2007) and (Li et al,2007) is that they bias against the valuable phraseinternal reorderings by only promoting the sourcesentence reordering.
As described in section 4.3, wesolve this problem by reordering the input string, butscoring the output string, thus allowing the strengthsof PSMT to co-exist with rule-based reordering.3 Language comparisonThe two languages examined in this investigation,English and Danish, are very similar from a struc-tural point of view.
A word alignment will most of-ten display an almost one-to-one correlation.
In thehand-aligned data, only 39% of the sentences con-tain reorderings (following the notion of reorderingas defined in 4.1).
On average, a sentence contains0.66 reorderings.One of the main differences between English andDanish word order is that Danish is a verb-secondlanguage: the finite verb of a declarative main clausemust always be the second constituent.
Since thisis not the case for English, a reordering rule should47move the subject of an English sentence to the rightof the finite verb, if the first position is filled bysomething other than the subject.
This is exempli-fied by (1) (examples are annotated with Englishgloss and translation), where ?they?
should move tothe right of ?come?
to get the Danish word order asseen in the gloss.
(1)[nunowkommercomedethey ]?here they come?Another difference is that Danish sentence adver-bials in a subordinate clause move to the left of thefinite verb.
This is illustrated in example (2).
Thisexample also shows the difficulty for a PSMT sys-tem.
Since the trigram ?han kan ikke?
is frequent inDanish main clauses, and ?han ikke kan?
is frequentin subordinate clauses, we need information on sub-ordination to get the correct word order.
This infor-mation can be obtained from the conjunction ?that?.A trigram PSMT system would not be able to handlethe reordering in (2), since ?that?
is beyond the scopeof ?not?.
(2)[hanhesigersaysatthathanheikkenotkancansesee ]?he says that he can not see?In the main clause, on the other hand, Danish prefersthe sentence adverbial to appear to the right of thefinite verb.
Therefore, if the English adverbial ap-pears to the left of the finite verb in a main clause, itshould move right as exemplified by example (3).
(3)[hunshesa?sawaldrigneverskibetthe ship ]?she never saw the ship?Other differences are of a more conventionalized na-ture.
E.g.
address numbers are written after thestreet in Danish (example (4)).
(4)[hanheborlivesnygadenygade1414 ]?he lives at 14 nygade?t7 ?
?
?
?
?
?
t6 ?
?
 ?
?
?
?t5 ?
 ?
?
?
?
?t4 ?
?
?
?
 ?
?t3 ?
?
?
?
?
 ?t2 ?
?
?
 ?
?
?t1  ?
?
?
?
?
?s1 s2 s3 s4 s5 s6 s7Table 1: Reordering example4 Reordering rules4.1 Definition of reorderingIn this experiment, reordering is defined as twoword sequences exchanging positions.
These twosequences are restricted by the following conditions:?
Parallel consecutive: They have to make upconsecutive sequences of words, and each hasto align to a consecutive sequence of words.?
Maximal: They have to be the longest possibleconsecutive sequences changing place.?
Adjacent: They have to appear next to eachother on both source and target side.The sequences are not restricted in length, mak-ing both short and long distance reordering possible.Furthermore, they need not be phrases in the sensethat they appear as an entry in the phrase table.Table 1 illustrates reordering in a word alignmentmatrix.
The table contains reorderings between thelight grey sequences (s32 and s64)1 and the dark greysequences (s55 and s66).
On the other hand, the se-quences s33 and s54 are e.g.
not considered reordered,since neither are maximal, and s54 is not consecutiveon the target side.4.2 Rule inductionIn section 3, we pointed out that subordination isvery important for word order differences betweenEnglish and Danish.
In addition, the sentence posi-tion of constituents plays a role.
All this informa-tion is present in a syntactic sentence parse.
A sub-ordinate clause is defined as inside an SBAR con-1Notation: syx means the consecutive source sequence cov-ering words x to y.48Level LC LS RS RCWORD <s> today , || today , || , he was driving home || home .
|| home .
< /s>POS <S> NN , || NN , || , PRP AUX VBG NN || NN .
|| NN .
< /S>PS <S> NP , || NP , || , NP AUX VBG ADVP || ADVP .
|| ADVP .
< /S>SUBORD main main main mainTable 2: Example of experience for learning.
Possible contexts separated by ||.stituent; otherwise it is a main clause.
The con-stituent position can be extracted from the sentencestart tag and the following syntactic phrases.
POSand word form are also included to allow for morespecific/lexicalized rules.Besides including this information for the candi-date reordering sequences (left sequence (LS) andright sequence (RS)), we also include it for the set ofpossible left (LC) and right (RC) contexts of these.The span of the contexts varies from a single word toall the way to the sentence border.
Table 2 containsan example of the information available to the learn-ing algorithm.
In the example, LS and RS shouldchange place, since the first position is occupied bysomething other than the subject in a main clause.In order to minimize the training data, wordand POS sequences are limited to 4 words, andphrase structure (PS) sequences are limited to 3 con-stituents.
In addition, an entry is only used if at leastone of these three levels is not too long for both LSand RS, and too long contexts are not included inthe set.
This does not constrain the possible lengthof a reordering, since a PS sequence of length 1 cancover an entire sentence.In order to extract rules from the annotated data,we use a rule-based classifier, Ripper (Cohen, 1996).The motivation for using Ripper is that it allows fea-tures to be sets of strings, which fits well with ourrepresentation of the context, and it produces easilyreadable rules that allow better understanding of thedecisions being made.
In section 6.2, extracted rulesare exemplified and analyzed.The probabilities of the rules are estimated usingMaximum Likelihood Estimation based on the in-formation supplied by Ripper on the performance ofthe individual rules on the training data.
These log-arithmic probabilities are easily integratable in thelog-linear PSMT model as an additional parameterby simple addition.The rules are extracted from the hand-aligned,Copenhagen Danish-English Dependency Treebank(Buch-Kromann et al, 2007).
5478 sentences fromthe news paper domain containing 111,805 Englishwords and 100,185 Danish words.
The English sideis parsed using a state-of-the-art statistical Englishparser (Charniak, 2000).4.3 Integrating rule-based reordering in PSMTThe integration of the rule-based reordering in ourPSMT system is carried out in two separate stages:1.
Reorder the source sentence to assimilate theword order of the target language.2.
Score the target word order according to the rel-evant rules.Stage 1) is done in a non-deterministic fashion bygenerating a word lattice as input in the spirit of e.g.
(Zens et al, 2002; Crego and Marin?o, 2007; Zhanget al, 2007).
This way, the system has both the orig-inal word order, and the reorderings predicted by therule set.
The different paths of the word lattice aremerely given as equal suggestions to the decoder.They are in no way individually weighted.Separating stage 2) from stage 1) is motivated bythe fact that reordering can have two distinct ori-gins.
They can occur because of stage 1), i.e.
thelattice reordering of the original English word or-der (phrase external reordering), and they can oc-cur inside a single phrase (phrase internal reorder-ing).
We are, however, interested in doing phrase-independent, word reordering.
We want to promoterule-predicted reorderings, regardless of whetherthey owe their existence to a syntactic rule or aphrase table entry.This is accomplished by letting the actual scoringof the reordering focus on the target string.
The de-49Source sentence: today1 ,2 he3 was4 late5Rule: 3 4 ?
4 3Hypothesis Target string SPTOH1 idag han var 1 3 4H2 idag var han 1 4 3Table 3: Example of SPTO scoring during decoding atsource word 4.coder is informed of where a rule has predicted a re-ordering, howmuch it costs to do the reordering, andhow much it costs to avoid it.
This is then checkedfor each hypothezised target string by keeping trackof what source position target order (SPTO) it cor-responds to.The SPTO is a representation of which sourceposition the word in each target position originatesfrom.
Putting it differently, the hypotheses con-tain two parallel strings; a target word string and itsSPTO string.
In order to access this information,each phrase table entry is annotated with its internalword alignment, which is available as an interme-diate product from phrase table creation.
If a phrasepair has multiple word alignments, the most frequentis chosen.Table 3 exemplifies the SPTO scoring.
The sourcesentence is ?today he was late?, and a rule has pre-dicted that word 3 and 4 should change place.
Whenthe decoder has covered the first four input words,two of the hypothesis target strings might be H1and H2.
At this point, it becomes apparent that H2contains the desired SPTO (namely ?4 3?
), and itget assigned the reordering cost.
H1 does not con-tain the rule-suggested SPTO (in stead, the wordsare in the order ?3 4?
), and it gets the violationcost.
Both these scorings are performed in a phrase-independent manner.
The decoder assigns the re-ordering cost to H2 without knowing whether thereordering is internal (due to a phrase table entry)or external (due to a syntactic rule).Phrase internal reorderings at other points of thesentence, i.e.
points that are not covered by a rule,are not judged by the reordering model.
Our ruleextraction does not learn every possible reorderingbetween the two languages, but only the most gen-eral ones.
If no rule has an opinion at a certain pointin a sentence, the decoder is free to chose the phraseFigure 1: Example word lattice.translation it prefers without reordering cost.Separating the scoring from the source languagereordering also has the advantage that the SPTOscoring in essence is compatible with other ap-proaches such as a traditional PSMT system.
Wewill, however, not examine this possibility further inthe present paper.5 The PSMT systemThe baseline is the PSMT system used for the 2006NAACL SMT workshop (Koehn and Monz, 2006)with phrase length 3 and a trigram language model(Stolcke, 2002).
The system was trained on the En-glish and Danish part of the Europarl corpus version3 (Koehn, 2005).
Fourth quarter of 2000 was re-moved in order to use the common test set of 11369sentences (330,082 English words and 309,942 Dan-ish words with one reference) for testing.
In addi-tion, fourth quarter of 2001 was removed for devel-opment purposes.
Of these, 10194 were used forvarious analysis purposes, thereby keeping the testdata perfectly unseen.
500 sentences were takenfrom the development set for tuning the decoder pa-rameters.
This was done using the Downhill Sim-plex algorithm.
In total, 1,137,088 sentences con-taining 31,376,034 English words and 29,571,518Danish words were left for training the phrase tableand language model.The decoder used for the baseline system isPharaoh (Koehn, 2004) with its distance-penalizingreordering model.
For the experiments, we useour own decoder which ?
except for the reorder-ing model ?
uses the same knowledge sourcesas Pharaoh, i.e.
bidirectional phrase translationmodel and lexical weighting model, phrase and wordpenalty, and target language model.
Its behavior iscomparable to Pharaoh when doing monotone de-coding.The search algorithm of our decoder is similar tothe RG graph decoder of (Zens et al, 2002).
It ex-50System Dev Test Swap SubsetBaseline 0.262 0.252 0.234no scoring 0.267 0.256 0.241SO scoring 0.268 0.258 0.244SPTO scoring 0.268 0.258 0.245Table 4: BLEU scores for different scoring methods.pects a word lattice as input.
Figure 1 shows theword lattice for the example in table 3.Since the input format defines all possible wordorders, a simple monotone search is sufficient.
Us-ing a language model of order n, for each hy-pothezised target string ending in the same n-1-gram, we only have to extend the highest scoringhypothesis.
None of the others can possibly outper-form this one later on.
This is because the maximalcontext evaluating a phrase extending this hypothe-sis, is the history (n-1-gram) of the first word of thatphrase.
The decoder is not able to look any furtherback at the preceeding string.6 Evaluation6.1 Results and discussionThe SPTO reordering approach is evaluated on the11369 sentences of the common test set.
Results arelisted in table 4 along with results on the develop-ment set.
We also report on the swap subset.
Theseare the 3853 sentences where the approach actuallymotivated reorderings in the test set, internal or ex-ternal.
The remaining 7516 sentences were not in-fluenced by the SPTO reordering approach.We report on 1) the baseline PSMT system, 2) asystem provided with a rule reordered word latticebut no scoring, 3) the same system but with an SOscoring in the spirit of (Zhang et al, 2007; Li et al,2007), and finally 4) the same system but with theSPTO scoring.The SPTO approach gets an increase over thebaseline PSMT system of 0.6 % BLEU.
The swapsubset, however, shows that the extracted rules aresomewhat restricted, only resulting in swap in 13 ofthe sentences.
The relevant set, i.e.
the set where thepresent approach actually differs from the baseline,is therefore the swap subset.
This way, we concen-trate on the actual focus of the paper, namely thesyntactically motivated SPTO reordering.
Here weSystem BLEU Avr.
Human ratingBaseline 0.234 3.00 (2.56)no scoring 0.240 3.00 (2.74)SO scoring 0.239 3.00 (2.62)SPTO scoring 0.244 2.00 (2.08)Table 5: Evaluation on the set where SO and SPTO pro-duce different translations.
Average human ratings aremedians with means in parenthesis, lower scores are bet-ter, 1 is the best score.achieve an increase in performance of 1.1 % BLEU.Comparing to the other scoring approaches doesnot show much improvement.
A possible explana-tion is that the rules do not apply very often, in com-bination with the fact that the SO and SPTO scoringmechanisms most often behave alike.
The differencein SO and SPTO scoring only leads to a difference intranslation in 10% of the sentences where reorderingis done.
This set is interesting, since it provides a fo-cus on the difference between the SO and the SPTOapproaches.
In table 5, we evaluate on this set.The BLEU scores on the entire set indicate thatSPTO is a superior scoring method.
To back this ob-servation, the 100 first sentences are manually eval-uated by two native speakers of Danish.
(Callison-Burch et al, 2007) show that ranking sentencesgives higher inter-annotator agreement than scor-ing adequacy and fluency.
We therefore employthis evaluation method, asking the evaluators to ranksentences from the four systems given the input sen-tence.
Ties are allowed.
The annotators had reason-able inter-annotator agreement (?
= 0.523, P (A) =0.69, P (E) = 0.35).
Table 5 shows the aver-age ratings of the systems.
This clearly shows theSPTO scoring to be significantly superior to theother methods (p < 0.05).Most of the cases (55) where SPTO outperformsSO are cases where SPTO knows that a phrase paircontains the desired reordering, but SO does not.Therefore, SO has to use an external reorderingwhich brings poorer translation than the internal re-ordering, because the words are translated individ-ually rather than by a single phrase (37 cases), or ithas to reject the desired reordering (18 cases), whichalso hurts translation, since it does not get the correctword order.51Decoder choice SO SPTOPhrase internal reordering 401 1538Phrase external reordering 3846 2849Reject reordering 1468 1328Table 6: The choices made based on the SO and SPTOscoring for the 5715 reorderings proposed by the rulesfor the test data.Table 6 shows the effect of SO and SPTO scoringin decoding.
Most noticeable is that the SO scoringis strongly biased against phrase internal reorder-ings; SPTO uses nearly four times as many phraseinternal reorderings as SO.
In addition, SPTO is alittle less likely to reject a rule proposed reordering.6.2 Rule analysisThe rule induction resulted in a rule set containing27 rules.
Of these, 22 concerned different ways ofidentifying contexts where a reordering should oc-cur due to the verb second nature of Danish.
4 ruleshad to do with adverbials in main and in subordinateclauses, and the remaining rule expressed that cur-rency is written after the amount in Danish, while itis the other way around in English.
Since the train-ing data however only includes Danish Crowns, therule was lexicalized to ?DKK?.Table 7 shows a few of the most frequently usedrules.
The first three rules deal with the verb secondphenomenon.
The only difference among these isthe left context.
Either it is a prepositional phrase, asubordinate clause or an adverbial.
These are threeways that the algorithm has learned to identify theverb second phenomenon conditions.
Rule 3 is inter-esting in that it is lexicalized.
In the learning data,the Danish correspondent to ?however?
is most of-ten not topicalized, and the subject is therefore notforced from the initial position.
As a consequence,the rule states that it should only apply, if ?however?is not included in the left context of the reordering.Rule 4 handles the placement of adverbials in asubordinate clause.
Since the right context is subor-dinate and a verb phrase, the current sequences mustalso be subordinate.
In contrast, the fifth rule dealswith adverbials in a main clause, since the left con-text noun phrase is in a main clause.A problem with the hand-aligned data used forrule-induction is that it is out of domain comparedNo LC LS RS RC1 PS: <S> PP , PS: NP POS: FV2 PS: SBAR , PS: NP POS: FV3 PS: ADVP , PS: NP POS: FV!
WORD:however ,4 PS: FV POS: RB PS: VPSUB: sub5 PS: <S> NP PS: ADVP POS: FVSUB: mainTable 7: Example rules and their application statistics.to the Europarl data used to train the SMT system.The hand-aligned data is news paper texts, and Eu-roparl is transcribed spoken language from the Euro-pean Parliament.
Due to its spoken nature, Europarlcontains frequent sentence-initial forms of address.That is, left adjacent elements that are not integratedparts of the sentence as illustrated by example (5).This is not straightforward, because on the surfacethese look a lot like topicalized constructions, as inexample (6).
In topicalized constructions, it is anintegrated part of the sentence that is moved to thefront in order to affect the flow of discourse infor-mation.
This difference is crucial for the reorderingrules, since ?i?
and ?have?
should reorder in (6), butnot in (5), in order to get Danish word order.
(5) mr president , i have three points .
(6) as president , i have three points .When translating the development set, it becameclear that many constructions like (5) were reorderedby a rule.
Since these constructions were not presentin the hand-aligned data, the learning algorithm didnot have the data to learn this difference.We therefore included a manual, lexicalized rulestating that if the left context contained one of a setof titles (mr, mrs, ms, madam, gentlemen), the re-ordering should not take place.
Since the learningincludes word form information, this is a rule thatthe learning algorithm is able to learn.
To a greatextent, the rule eliminates the problem.The above examples also illustrate that local re-ordering (in this case as local as two neighboringwords) can be a problem for PSMT, since eventhough the reordering is local, the information aboutwhether to reorder or not is not necessarily local.521 S based on this viewpoint , every small port and every ferry port which handlesa great deal of tourist traffic should feature on the european list .B baseret pa?
dette synspunkt , ethvert lille havn og alle f?rgehavnen somha?ndterer en stor turist trafik skal sta?
pa?
den europ?iske liste .P baseret pa?
dette synspunkt , skal alle de sma?
havne , og alle f?rgehavnensom behandler mange af turister trafik stod pa?
den europ?iske liste .2 S the rapporteur generally welcomes the proposals in the commission white paper on thissubject but is apprehensive of the possible implications of the reform , which aimsprincipally to decentralise the implementation of competition rules .B ordf?reren generelt bifalder forslagene i kommissionens hvidbog om dette emne , men erbekymret for de mulige konsekvenser af den reform , som sigter hovedsagelig atdecentralisere gennemf?relsen af konkurrencereglerne .P ordf?reren bifalder generelt forslagene i kommissionens hvidbog om dette emne , men erbekymret for de mulige konsekvenser af den reform , som is?r sigter mod atdecentralisere gennemf?relsen af konkurrencereglerne .Table 8: Examples of reorderings.
S is source, B is baseline, and P is the SPTO approach.
The elements that havebeen reordered in the P sentence are marked alike in all sentences.
The text in bold has changed place with the text initalics.6.3 Reordering analysisIn this section, we will show and discuss a few ex-amples of the reorderings made by the SPTO ap-proach.
Table 8 contain two translations taken fromthe test set.In translation 1), the subject (bold) is correctlymoved to the right of the finite verb (italics), whichthe baseline system fails to do.
Moving the finiteverb away from the infinite verb ?feature?, however,leads to incorrect agreement between these.
Whilethe baseline correctly retains the infinite form (?sta??
),the language model forces another finite form (thepast tense ?stod?)
in the SPTO reordering approach.Translation 2) illustrates the handling of adver-bials.
The first reordering is in a main clause, there-fore, the adverbial is moved to the right of the finiteverb.
The second reordering occurs in a subordinateclause, and the adverbial is moved to the left of thefinite verb.
Neither of these are handled successfullyby the baseline system.In this case, the reordering leads to better wordselection.
The English ?aims to?
corresponds to theDanish ?sigter mod?, which the SPTO approach getscorrect.
However, the baseline system translates ?to?to its much more common translation ?at?, because?to?
is separated from ?aims?
by the adverbial ?prin-cipally?.7 Conclusion and Future PlansWe have described a novel approach to word re-ordering in SMT, which successfully integratessyntactically motivated reordering in phrase-basedSMT.
This is achieved by reordering the input string,but scoring on the output string.
As opposed to pre-vious approaches, this neither biases against phraseinternal nor external reorderings.
We achieve an ab-solute improvement in translation quality of 1.1 %BLEU.
A result that is supported by manual evalua-tion, which shows that the SPTO approach is signif-icantly superior to previous approaches.In the future, we plan to apply this approach toEnglish-Arabic translation.
We expect greater gains,due to the higher need for reordering between theseless-related languages.
We also want to examine therelation between word alignment method and the ex-tracted rules and the relationship between reorderingand word selection.
Finally, a limitation of the cur-rent experiments is that they only allow rule-basedexternal reorderings.
Since the SPTO scoring is nottied to a source reordering approach, we want to ex-amine the effect of simply adding it as an additionalparameter to the baseline PSMT system.
This way,all external reorderings are made possible, but onlythe rule-supported ones get promoted.53ReferencesY.
Al-Onaizan and K. Papineni.
2006.
Distortion modelsfor statistical machine translation.
In Proceedings of44th ACL.M.
Buch-Kromann, J. Wedekind, and J. Elming.
2007.The Copenhagen Danish-English Dependency Tree-bank v. 2.0. http://www.isv.cbs.dk/?mbk/cdt2.0.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machinetranslation.
In Proceedings of ACL-2007 Workshop onStatistical Machine Translation.E.
Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st NAACL.W.
Cohen.
1996.
Learning trees and rules with set-valued features.
In Proceedings of the 14th AAAI.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In Pro-ceedings of the 43rd ACL.J.
M. Crego and J.
B. Marin?o.
2007.
Syntax-enhanced n-gram-based smt.
In Proceedings of the 11th MT Sum-mit.N.
Habash.
2007.
Syntactic preprocessing for statisticalmachine translation.
In Proceedings of the 11th MTSummit.P.
Koehn and C. Monz.
2006.
Manual and automaticevaluation of machine translation between europeanlanguages.
In Proceedings on the WSMT.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of NAACL.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Proceedings of AMTA.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In Proceedings of MT Sum-mit.C.
Li, M. Li, D. Zhang, M. Li, M. Zhou, and Y. Guan.2007.
A probabilistic approach to syntax-based re-ordering for statistical machine translation.
In Pro-ceedings of the 45th ACL.A.
Stolcke.
2002.
Srilm ?
an extensible language mod-eling toolkit.
In Proceedings of the International Con-ference on Spoken Language Processing.C.
Wang, M. Collins, and P. Koehn.
2007.
Chinese syn-tactic reordering for statistical machine translation.
InProceedings of EMNLP-CoNLL.F.
Xia and M. McCord.
2004.
Improving a statistical mtsystem with automatically learned rewrite patterns.
InProceedings of Coling.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In M. Jarke, J. Koehler,and G. Lakemeyer, editors, KI - 2002: Advances inArtificial Intelligence.
25.
Annual German Conferenceon AI.
Springer Verlag.Y.
Zhang, R. Zens, and H. Ney.
2007.
Improved chunk-level reordering for statistical machine translation.
InProceedings of the IWSLT.54
