Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 62?65,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsReranking with Multiple Features for Better TransliterationYan Song?
Chunyu Kit?
Hai Zhao??
?Department of Chinese, Translation and LinguisticsCity University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong?Department of Computer Science and EngineeringShanghai Jiao Tong University, #800, Dongchuan Rd, Shanghai, China{yansong,ctckit}@cityu.edu.hk, zhaohai@cs.sjtu.edu.cnAbstractEffective transliteration of proper namesvia grapheme conversion needs to findtransliteration patterns in training data,and then generate optimized candidatesfor testing samples accordingly.
However,the top-1 accuracy for the generated candi-dates cannot be good if the right one is notranked at the top.
To tackle this issue, wepropose to rerank the output candidates fora better order using the averaged percep-tron with multiple features.
This paper de-scribes our recent work in this direction forour participation in NEWS2010 transliter-ation evaluation.
The official results con-firm its effectiveness in English-Chinesebidirectional transliteration.1 IntroductionSince transliteration can be considered a direct or-thographic mapping process, one may adopt gen-eral statistical machine translation (SMT) proce-dures for its implementation.
Aimed at findingphonetic equivalence in another language for agiven named entity, however, different translitera-tion options with different syllabification may gen-erate multiple choices with the symphonic formfor the same source text.
Consequently, even theoverall results by SMT output are acceptable, itis still unreliable to rank the candidates simply bytheir statistical translation scores for the purposeof selecting the best one.
In order to make a properchoice, the direct orthographic mapping requires aprecise alignment and a better transliteration op-tion selection.
Thus, powerful algorithms for ef-fective use of the parallel data is indispensable, es-pecially when the available data is limited in vol-ume.Interestingly, although an SMT based approachcould not achieve a precise top-1 transliteration re-sult, it is found in (Song et al, 2009) that, in con-trast to the ordinary top-1 accuracy (ACC) score,its recall rate, which is defined in terms of whetherthe correct answer is generated in the n-best outputlist, is rather high.
This observation suggests thatif we could rearrange those outputs into a betterorder, especially, push the correct one to the top,the overall performance could be enhanced signif-icantly, without any further refinement of the orig-inal generation process.
This reranking strategy isproved to be efficient in transliteration generationwith a multi-engine approach (Oh et al, 2009).In this paper, we present our recent work onreranking the transliteration candidates via an on-line discriminative learning framework, namely,the averaged perceptron.
Multiple features are in-corporated into it for performance enhancement.The following sections will give the technical de-tails of our method and present its results forNEWS2010 shared task for named entity translit-eration.2 GenerationFor the generation of transliteration candidates,we follow the work (Song et al, 2009), using aphrase-based SMT procedure with the log-linearmodelP (t|s) = exp[?ni=1 ?ihi(s, t)]?t exp[?ni=1 ?ihi(s, t)](1)for decoding.
Originally we use two directionalphrase1 tables, which are learned for both direc-tions of source-to-target and target-to-source, con-taining different entries of transliteration options.In order to facilitate the decoding by exploiting allpossible choices in a better way, we combine theforward and backward directed phrase tables to-gether, and recalculate the probability for each en-1It herein refers to a character sequence as described in(Song et al, 2009).62try in it.
After that, we use a phoneme resource2 torefine the phrase table by filtering out the wronglyextracted phrases and cleaning up the noise in it.In the decoding process, a dynamic pruning is per-formed when generating the hypothesis in eachstep, in which the threshold is variable accordingto the current searching space, for we need to ob-tain a good candidate list as precise as possiblefor the next stage.
The parameter for each fea-ture function in log-linear model is optimized byMERT training (Och, 2003).
Finally, a maximumnumber of 50 candidates are generated for eachsource name.3 Reranking3.1 Learning FrameworkFor reranking training and prediction, we adoptthe averaged perceptron (Collins, 2002) as ourlearning framework, which has a more stable per-formance than the non-averaged version.
It is pre-sented in Algorithm 1.
Where ~?
is the vector ofparameters we want to optimize, x, y are the cor-responding source (with different syllabification)and target graphemes in the candidate list, and ?represents the feature vector in the pair of x andy.
In this algorithm, reference y?i is the most ap-propriate output in the candidate list according tothe true target named entity in the training data.We use the Mean-F score to identify which candi-date can be the reference, by locating the one withthe maximum Mean-F score value.
This processupdates the parameters of the feature vector andalso relocate all of the candidates according to theranking scores, which are calculated in terms ofthe resulted parameters in each round of trainingas well as in the testing process.
The number ofiteration for the final model is determined by thedevelopment data.3.2 Multiple FeaturesThe following features are used in our rerankingprocess:Transliteration correspondence feature, f(si, ti);This feature describes the mapping betweensource and target graphemes, similar to thetransliteration options in the phrase table inour previous generation process, where s and2In this work, we use Pinyin as the phonetic representa-tion for Chinese.Algorithm 1 Averaged perceptron trainingInput: Candidate list with reference{LIST (xj , yj)nj=1, y?i }Ni=1Output: Averaged parameters1: ~?
?
0, ~?a ?
0, c?
12: for t = 1 to T do3: for i = 1 to N do4: y?i ?
argmaxy?LIST (xj ,yj)~?
?
?
(xi, yi)5: if y?i 6= y?i then6: ~?
?
~?
+?
(x?i , y?i )?
?
(x?i, y?i)7: ~?a ?
~?a+ c ?
{?
(x?i , y?i )??
(x?i, y?i)}8: end if9: c?
c+ 110: end for11: end for12: return ~?
?
~?a/ct refer to the source and target language re-spectively, and i to the current position.Source grapheme chain feature, f(sii?1);It measures the syllabification for a givensource text.
There are two types of unitsin different levels.
One is on syllable level,e.g., ?aa/bye?, ?aa/gaar/d?, reflecting thesegmentation of the source text, and the otheron character level, such as ?a/b?, ?a/g?,?r/d?, showing the combination power ofseveral characters.
These features on differ-ent source grapheme levels can help the sys-tem to achieve a more reliable syllabificationresult from the candidates.
We only considerbi-grams when using this feature.Target grapheme chain feature, f(tii?2);This feature measures the appropriateness ofthe generated target graphemes on both char-acter and syllables level.
It performs in asimilar way as the language model for SMTdecoding.
We use tri-gram syllables in thislearning framework.Paired source-to-target transition feature, f(<s, t >ii?1);This type of feature is firstly proposed in(Li et al, 2004), aiming at generating sourceand target graphemes simultaneously undera suitable constraint.
We use this featureto restrict the synchronous transition of bothsource and target graphemes, measuring howwell are those transitions, such as for ?st?,63whether ?s?
transliterated by ???
is followedby ?t?
transliterated by ???.
In order to dealwith the data sparseness, only bi-gram transi-tion relations are considered in this feature.Hidden Markov model (HMM) style features;There are a group of features with HMMstyle constraint for evaluating the candi-dates generated in previous SMT process,including, previous syllable HMM features,f(sii?n+1, ti), posterior syllable HMM fea-tures, f(si+n?1i , ti), and posterior characterHMM features, f(si, l, ti), where l denotesthe character following the previous syllablein the source language.
For the last feature,it is effective to use both the current sylla-ble and the first letter of the next syllableto bound the current target grapheme.
Thereason for applying this feature in our learn-ing framework is that, empirically, the lettersfollowing many syllables strongly affect thetransliteration for them, e.g., Aves ?
??
?, ?a?
followed by ?v?
is always translatedinto ???
rather than ??
?.Target grapheme position feature, f(ti, p);This feature is an improved version of thatproposed in (Song et al, 2009), where prefers to the position of ti.
We have a mea-sure for the target graphemes according totheir source graphemes and the current posi-tion of their correspondent target characters.There are three categories of such position,namely, start (S), mediate (M) and end (E).
Srefers to the first character in a target name, Eto the final, and the others belong to M. Thisfeature is used to exploit the observation thatsome characters are more likely to appear atcertain positions in the target name.
Some arealways found at the beginning of a named en-tity while others only at the middle or the end.For example, ?re?
associated to first charac-ter in a target name is always transliterated as??
?, such as Redd ???.
When ?re?
ap-pears at the end of a source name, however,its transliteration will be ???
in most cases,just like Gore ??
?.Target tone feature;This feature is only applied to the translit-eration task with Chinese as the target lan-guage.
It can be seen as a combinationof a target grapheme chain with some posi-tion features, using tone instead of the targetgrapheme itself for evaluation.
There are 5tones (0,1,2,3,4) for Chinese characters.
It iseasy to conduct a comprehensive analysis forthe use of a higher ordered transition chain asa better constraint.
Many fixed tone patternscan be identified in the Chinese translitera-tion training data.
The tone information canalso be extracted from the Pinyin resource weused in the previous stage.Besides the above string features, we also havesome numeric features, as listed below.Transliteration score;This score is the joint probabilities of alltransliteration options, included in the outputcandidates generated by our decoder.Target language model score;This score is calculated from the probabilistictri-gram language model.Source/target Pinyin feature;This feature uses Pinyin representation for asource or target name, depending on whatside the Chinese language is used.
It mea-sures how good the output candidates can bein terms of the comparison between Englishtext and Pinyin representation.
The resultedscore is updated according to the Levenshteindistance for the two input letter strings of En-glish and Pinyin.For a task with English as the target language,we add the following two additional features intothe learning framework.Vowel feature;It is noticed that when English is the targetlanguage, vowels can sometimes be missingin the generated candidates.
This feature isthus used to punish those outputs unqualifiedto be a valid English word for carrying novowel.Syllable consistent feature;This feature measures whether an English tar-get name generated in the previous step hasthe same number of syllables as the sourcename.
In Chinese-to-English transliteration,Chinese characters are single-syllabled, thus64Table 1: Evaluation results for our NEWS2010 task.Task Source Target ACC Mean F MRR Map ref Recall ACCSMTEnCh English Chinese 0.477 0.740 0.506 0.455 0.561 0.381ChEn Chinese English 0.227 0.749 0.269 0.226 0.371 0.152we can easily identify their number.
For syl-labification, we have an independent segmen-tation process for calculating the syllables.4 ResultsFor NEWS2010, we participated in all twoChinese related transliteration tasks, namely,EnCh (English-to-Chinese) and ChEn (Chinese-to-English back transliteration).
The official eval-uation scores for our submissions are presentedin Table 1 with recall rate, and the ACC score(ACCSMT ) for original SMT outputs.
It is easyto see the performance gain for the reranking, andalso from the recall rate that there is still someroom for improvement, in spite of the high ratio ofACC/Recall3 calculated from Table 1.
However, itis also worth noting that, some of the source textscannot be correctly transliterated, due to manymultiple-word name entities with semantic com-ponents in the test data, e.g., ?MANCHESTERBRIDGE?, ?BRIGHAM CITY?
etc.
These seman-tic parts are beyond our transliteration system?s ca-pability to tackle, especially when the training datais limited and the only focus of the system is on thephonetic equivalent correspondence.Compared to the EnCh transliteration, we get arather low ACC score for the ChEn back translit-eration, suggesting that ChEn task is somewhatharder than the EnCh (in which Chinese char-acters are always limited).
The ChEn task is aone-to-many translation, involving a lot of pos-sible choices and combinations of English sylla-bles.
This certainly makes it a more challenge-able task than EnCh.
However, looking into thedetails of the outputs, we find that, in the ChEnback transliteration, some characters in the testcorpus are unseen in the training and the devel-opment data, resulting in incorrect transliterationsfor many graphemes.
This is another factor affect-ing our final results for the ChEn task.5 ConclusionIn this paper, we have presented our work onmultiple feature based reranking for transliteration3Compared to the results from (Song et al, 2009)generation.
It NEWS2010 results show that thisapproach is effective and promising, in the sensethat it ranks the best in EnCh and ChEn tasks.
Thereranking used in this work can also be consid-ered a regeneration process based on an existingset, as part of our features are always used directlyto generate the initial transliteration output in otherresearches.
Though, those features are stronglydependent on the nature of English and Chineselanguages, it is thus not an easy task to transplantthis model for other language pairs.
It is an inter-esting job to turn it into a language independentmodel that can be applied to other languages.AcknowledgmentsThe research described in this paper was par-tially supported by City University of Hong Kongthrough the Strategic Research Grants (SRG)7002267 and 7008003.
Dr. Hai Zhao was sup-ported by the Natural Science Foundation of China(NSFC) through the grant 60903119.
We alsothank Mr. Wenbin Jiang for his helpful sugges-tions on averaged perceptron learning.ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP-2002, pages 1?8, July.Haizhou Li, Min Zhang, and Jian Su.
2004.
Ajoint source-channel model for machine transliter-ation.
In Proceedings of ACL-04, pages 159?166,Barcelona, Spain, July.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL-03, pages 160?167, Sapporo, Japan, July.Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-sawa.
2009.
Machine transliteration using target-language grapheme and phoneme: Multi-enginetransliteration approach.
In Proceedings of NEWS2009, pages 36?39, Suntec, Singapore, August.Yan Song, Chunyu Kit, and Xiao Chen.
2009.
Translit-eration of name entity via improved statistical trans-lation on character sequences.
In Proceedings ofNEWS 2009, pages 57?60, Suntec, Singapore, Au-gust.65
