Recent Progress on the SUMMIT  SystemVictor Zue, James Glass, David Goodine, Hong Leung,Michael Phillips, Joseph Polifroni, and Stephanie SeneffRoom NE43-601Spoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, MA 02139IntroductionThe s u M M IT system is a speaker-independent, continuous-speech recognition system that we have developed at MIT\[12\].
To date, the system has been ported to a variety oftasks with vocabulary sizes up to 1000 words and perplexi-ties up to 73.
The architecture of this system is a productof two guiding principles.
First, we desired a framework thatcould be flexible and modular so that we could explore al-ternative strategies for embedding speech knowledge into thesystem.
Second, we required that the system be stochasticand trainable from a large body of speech data to accountfor our current incomplete knowledge of the acoustic realiza-tion of speech.
The current implementation f the systemis a reflection of both of these ideas.
SUMMIT differs fromthe majority of prevailing HMM approaches in many respectsranging from its use of auditory models and selected acousticmeasurements, o its segmental framework and use of pro-nunciation etworks.
In time, the specific implementation fthese ideas will undoubtedly be modified as we discover su-perior techniques and approaches.
Until phonetic and wordrecognition accuracies are competitive with those of humanlisteners however, we believe it will be appropriate to incor-porate both notions of flexibility and trainability into thesystem.In the past year we have focused our attention on a largerspoken-language effort which integrates SUMMIT with a nat-ural language system.
We have also investigated issues whichrelate to phonetic recognition; namely alternative segmentalrepresentations and classification techniques.
In addition, wehave changed our normalization procedure to make it moreamenable for recording spontaneous speech.
In this paper wefirst describe the normalization procedure.
We then reviewour segmental framework, and describe two alternatives weinvestigated.
Finally, we present some phonetic lassificationand recognition experiments which assess the different seg-mental representations and classification techniques.
Theseexperiments indicated an improvement in classification rateof 4%, and in recognition rate of 8%.NormalizationMost speech recognition systems that measure some typeof absolute value, such as the short-term energy, require thatan utterance is first normalized before processed.
In the SUM-MIT system for instance, a weak signal that is not normalizedwill produce auditory outputs which are smaller than thespontaneous firing rate and will therefore be indistinguish-able from silence.
Currently we use a normalization procedurethat scales a speech waveform with respect o its maximummagnitude value.
This simple procedure has proven quiteeffective for the majority of speech processing and recogni-tion applications ince 1) the utterance has typically beencompletely recorded before it is being normalized, 2) any ex-traneous loud noises or clicks have been edited from the ut-terance so that the largest value in the waveform correspondsto speech (usually a vowel) and, 3) the speech intensity hasbeen relatively uniform throughout the utterance.In situations where a human is interacting with a machine,some of these assumptions become less reasonable.
From acomputational perspective, it would be desirable to be ableto process the signal in near or less than real-time.
Anotherdifficulty - .~ spontaneous speech is that there are frequentlyspurious noises and clicks which occasionally have the largestmagnitude in the signal.
In this case, an utterance is notnormalized with respect o speech, so the speech waveformvalues are subsequently weaker than normal.
Finally, if thespeech intensity does change somewhat over the course of theutterance, a static scaling value will result in a weaker speechsignal in some portions of the utterance.Normalization ProcedureThe normalization procedure we are investigating uses astandard feedback system to compute a gain, g\[n\].
In orderto provide the gain some time to adapt to changing signalconditions, there is a delay of N frames between the pointwhere the gain is computed and where the speech signal isscaled.
Currently, a frame is computed every 25 ms, and thedelay is 8 frames, or 200 ms.
The gain control mechanism isillustrated in Figure 1.
The system input, x\[n\], is a functionof the recent short-term energy (computed with 25 ms rect-angular window) of the speech signal, sin\].
In order to reducethe amount of change in the gain output, the input x\[n\] isthe maximum energy value within the previous N frames,z\[n\] : miaxs\[i \] n -- Y < i < nThe error signal, e\[n\], is generated by comparing the scaledvalue y\[n\] with a target level, t\[n\].
The scaled value is gen-erated from the input and the previous gain value, gin - 1\].All values are in dB.
The influence of e\[n\] on the final gainis controlled by a sealing parameter, a, which controls theinfluence of the error signal on the gain.380Figure h Normalization SchematicSince the normalization structure assumes nothing aboutthe signal to noise ratio, the entire signal is normalized equallywhether or not the signal corresponds to speech or noise.
Ifwe could produce a value p,\[n\], corresponding to the proba-bility of speech at time n, then we could modify the abovealgorithm.
One possible modification would make the targetlevel depend on p,\[n\].
When p,\[n\] = 1, then t\[n\] = T, thetarget level for speech.
When po\[n\] = 0, then t\[n\] = y\[n\] (i.e.,e\[n\]= 0).
In effect, when the signal is considered to be noise,the gain does not change.
The net result of this argument isto maket\[n\] = p,\[n\]T, + (1 - po\[nl)Y\[n\]orThus, we make the error proportional to the probability thatwe have speech.
Note that another alternative procedurewould be to reduce the gain.In addition to an estimate of the presence of speech inthe signal, we can improve the performance of the normaliza-tion procedure if we can set a limit on the minimum allowablesignal to noise ratio.
This minimum can be used to set an up-per limit on the gain value, and can be used for initializationpurposes.EvaluationSince the normalization procedure we have described is anon-linear operation, it is difficult to know how to quantify itsperformance.
We have chosen to use word recognition accu-racy as one guideline.
Currently, we are incorporating a ver-sion of this normalization procedure into the SUMMIT systemand are evaluating the resulting performance as a function ofthe various parameter values.Segmenta l  Representat ionsAs has been described previously \[12\], the SUMMIT sys-tem is based on a segmental framework.
During the acoustic-phonetic analysis, a phonetic unit is mapped to a segmentexplicitly delineated by a begin and end time in the speechsignal.
Segmental frameworks have been investigated by oth-ers \[3,9\] and contrast with the prevailing frame-based struc-ture used by most HMM systems itcom, where sequences ofobservation frames are assumed to be statistically indepen-dent from each other.
We believe that a segmental frameworkoffers us more flexibility than is afforded by a frame-based ap-proach, and could ultimately lead to superior modelling of thetemporal variations in the realization of underlying phonolog-ical units.
It is for this reason that we base our system onsuch an approach.In this section we review the current segmental frameworkof the SUMMIT system and present some alternatives that wehave investigated.
These are discussed again in the followingsection where we present some phonetic recognition results.Segmenta l  Formulat ionIn our framework, we try to maximize the probability of asequence of units.
For phonetic recognition, we represent theprobability of a sequence of phones, ai = {cqb c~i2, .., O~N} as,where N~ is the number of phonetic units in 81, Sj is the jthpossible sequence of N~ connecting segments traversing theutterance, p(c~ik Isjk) is the probability of observing a phone ina given segment, and p(sik) is the probability that a segmentexists.
During phonetic recognition, we select the sequence,&, that maximizes Equation 1.
Thus~p(,~) > p(,~,) viIn order to perform recognition, we need to estimate the twoprobability measures in Equation 1.
The first term, p(c~ls), isa set of a-posteriori classification probabilities which we willdiscuss later in this paper.
The second term, p(s), is a set ofprobabilities of valid segments which we will now describe inmore detail.Segment ProbabilitiesIn order to estimate the segment probabilities, p(s), wehave formulated segmentation i to a boundary classificationproblem.
Let {~, b2, .., bh} be the set of possible boundariesthat might exist within segment, si.
We define p(si) to bethe probability that all internal boundaries do not exist.
Toreduce the complexity of the problem, we assume that allboundaries exist independently.
Thus,p(s~) =p(b~,b2,..,hk)=P(b0P(~)-..P(bk) (2)where p(bj) stands for the probability that the jth boundarydoes not exist.
Viewing the problem as one of classification,the boundary classes can be generated by aligning a phonetictranscription with a segment space through recognition orsome other procedure.Search Space IssuesOne of the disadvantages of a segmental framework is thatthe amount of computation associated with a search can besignificant.
If there are N observation boundaries in an ut-terance then there are 2 N-2 possible segmentations of theseN boundaries, and the number of possible segments, N,, isN(N-1) /2 .
Conceptually, we can consider these segments asbeing part of a large segment network which spans from thebeginning of an utterance to its end.
Since phonetic lassifi-cation is needed in each possible segment, he amount of com-putation and the amount of search throughout the segmentnetwork, can be prohibitively large.
For example, if the ut-terance is 3 seconds long and the boundaries occur 200 timesper second (our standard analysis-rate), then No ~ 180,000.381Several procedures can be adopted to improve the effi-ciency of a search involved in a segmental framework.
First,it is clear that certain search strategies, such as those thatinvolve dynamic programming, can reduce the amount ofsearch.
Currently, we use a modified Viterbi search for pho-netic and word recognition \[12\].
In the following paragraphswe discuss some of the other procedures we have explored forreducing the search space.P run ing  Boundar iesIf we can determine a subset of boundaries, B, that con-tain all boundaries of interest, we can substantially reducethe size of the segment network.
As we saw previously, thenumber of segments in a full network varies as the square ofthe number of boundaries.
In our current system for instance,we use a boundary detector that, on average, locates bound-aries less than one in every five frames.
Thus, the number ofsegments i  reduced by more than an order of magnitude.P run ing  SegmentsThere are many alternative tactics that can be used toreduce the size of the segment space.
For example, Kopecand Bushused conservative duration estimates to eliminatemany candidate segments \[3\].
In our current implementa-tion we have used an acoustic basis for determining a setof regions, organizing the N boundaries into a hierarchicalstructure called a dendrogram \[2\].
Such a hierarchy produces2N-3  possible segments, which reduces the size of a segmentnetwork by a factor of approximately N 7"D is tor t ionsWhile eliminating boundaries and segments from the searchspace can substantially reduce the size of the search space, itcan also increase the amount of distortion involved in match-ing a sequence of phonetic units to a segment network.
Incases where there is no segment o match a phone, we saythere has been a deletion of a boundary.
In cases where thereis no single segment to match a phone we say there has beenan insertion of a segment.
Where there is an alignment be-tween a region and a phone there may be a certain amount ofdistortion involved in the alignment, which ultimately mightcause a classification error.Clearly, if the segment network is pruned there should besome mechanism for handling insertions and deletions.
Asthe previous paragraphs have pointed out, there are variousamounts of pruning that can be done, each attaining a cer-tain level of phonetic-and word recognition performance.
Inexploring the various possibilities our goal is to understandthe behavior of the system with different a~nounts of pruning,and to maximize the phonetic and word recognition perfor-mance.
Given these goals, we now describe some of the mod-ifications we have made to our pruning strategies.
We thendescribe some evaluations we made of alternative segmentalapproaches.Boundary  Mod i f i ca t ionsCurrently in the SUMMIT system the set of boundaries, B,that are used are determined by a sensitive dge detector thatessentially locates local maxima in a spectral derivative func-tion \[12\].
This procedure appears to be quite robust since itoperates on local relative changes in the speech signal.
How-ever, we have observed that there is a substantial number ofboundaries located during portions of silence in the speechsignal.
This phenomenon has become more pronounced withour emphasis on spontaneous speech because speakers tendto false start and hesitate more often than when they arereading.
As a result, there can be a significant silence periodand/or non-linguistic sounds at the beginning and the endof the sentence.
One consequence of this is that the systemspends a large amount of time analyzing the many segmentsproduced uring periods of silence.Aiming at reducing computational load in different partsof the system, we have been investigating techniques to prunethe silence regions.
We have incorporated into SUMMIT asimple algorithm which uses scores for speech and silencebased on the distributions of eight principle components ofthe mean rate response outputs of an auditory model \[11\].We trained the system by collecting histograms of parameterdistributions for phonetically transcribed utterances from aspontaneous-speech database\[13\].
The probability of speechis computed on a frame by frame basis, after some tempo-ral smoothing.
A two-state Markov model contains a-prioritransition probabilities that are incorporated into the scorein order to delineate long regions of silence or speech.
We arein the process of evaluating the effect of this procedure onword recognition accuracy.Dendrogram ModificationsIn the hierarchical clustering procedure currently used inthe SUMMIT system we require a metric, D, to compute adistance between two adjacent regions.
Specifically, let g~be the acoustic vector corresponding to the i th region.
ThenD~ = D(~, a~+l) corresponds to the distance between the i thand i + i st regions.
In our clustering procedure we merge twoadjacent regions when,Di < min{Di-l,Di+l}The current procedure uses a weighted Euclidean distancemetric.
The acoustic vector is an average spectral vector ofthe segment.
We have observed two problems that occur inthe resulting dendrograms.
Due to the Euclidean metric, lit-tle weight is given to correlation across adjacent channels inthe spectral representation.
Thus, the acoustic structure can-not always distinguish similar sounds from those whose spec-tral shape is significantly different.
Second, local extrema inthe representation were not adequately reflected in the re-sulting dendrogram structure.
The combined effects of thesephenomena was to produce a large phonetic alignment dis-tortion for certain sequences of sounds.We attempted to address these issues by first translatingthe spectral representation using principle component anal-ysis.
Each dimension was then normalized by its mean andvariance.
An additional rotation was made based on the aver-age within-class variance of each dimension.
These varianceswere generated from aligned phonetic transcriptions and wereintended to equalize the contribution of each component di-mension to the overall distance metric.
We explored several382distance metrics using this representation a d found that aneffective one was based on a normalized ot-product,which is proportional to a Euclidean distance between thenormalized acoustic vectors.The problem with tl~e local extrema was addressed by in-corporating more information into the distance metric.
Specif-ically, we computed ifference vectors, ~ = 5/ - ai+l andcomputed D(~, ~+1).
The final distance metric, D*, used acombination of both metrics,EvaluationThe evaluation of various segmental representations cantake many forms.
The first analysis we performed aligned aphonetic transcription with the segment network.
The align-ment procedure mapped a phonetic token to the closest re-gion that overlapped by at least 50%.
If no region was founda deletion or insertion was made.
Table 1 summarizes thestatistics of the various representations we have described on150 TIMIT utterances.
These utterances contained 5636 pho-netic tokens.
From the Table we see that the modified hi-erarchical procedures reduce the insertion rate of the currentrepresentation bymore than one third while the deletion ratesare slightly reduced.
Finally, a full segment network createdfrom the entire boundary set B has essentially no insertions.The minimum deletion rate is just under 2%.SegmentSUMMITD ?YDeletion (%) Insertion (%)3.5 5.73.5 4.73.0 3.71.9 0Table 1: Phonetic alignment performance.Acoust i c -Phonet ic  Ana lys i sIn the previous section we outlined the segmental rep-resentation that is being used in the SUMMIT system anddescribed some alternatives to the current implementation.In this section we describe some experiments we have per-formed to explore alternative methods for phonetic lassifica-tion based on multi-layer perceptrons (MLP).
These exper-iments involve both phonetic classification and recognition.They compare classification techniques as well as segmentalrepresentations.
In the next sections we describe the task,and speech corpus used for the experiments, as weU as theMLP classifier and data representations that were used.
Thisis followed by a description of the phonetic lassification andfinally the phonetic recognition experiments.Task and CorpusAll experiments were based on the sx sentences from 350speakers of the TIMIT database \[4\].
1500 sentences from 300speakers were used for training, and 250 sentences from theremaining 50 speakers were used for testing.
As summarizedin Table 2, there were a total of 55,000 phonetic tokens inthe training data and 9,000 tokens in the testing data.
Therewere 38 phonetic labels used which represented 14 vowels.
3semivowels, 3 nasals, 8 fricatives, 2 affricates, 6 stops, 1 flap.and silence.
This particular set was chosen because it hasbeen used in other evaluations both within and outside ourresearch group \[6,12\].TrainingSpeakers (M/F)300 (216/s4)Test Training TestSpeakers(M/F) Tokens Tokens50 (33/17) t 55.000 9.000Table 2: Corpus used for the experiments.MLP  C lass i f ie rRecently, we have been investigating the use of multi-layerperceptrons for phonetic lassification.
Our work was moti-vated by the belief that these networks might offer a flexibleframework for us to utilize our improved, albeit incomplete,speech knowledge.
Until recently, our study was performed onthe constrained task of classifying the 16 vowels in AmericanEnglish, spoken by many speakers and excised from continu-ous speech \[8\].
Our encouraging results suggested that MLPis a promising technique worthy of further investigation.We extended our work to one of classifying 38 vowels andconsonants.
In moving to this larger phonetic classificationproblem, we discovered some major problems in training thenetwork.
In this section, we will describe these problems andsuggest some procedures to overcome them.InitializationWithout any a priori knowledge, the connection weightsof MLP are often randomly initialized.
Since the transitionregion of the sigmoid function is relatively narrow while thesaturation regions are relatively wide, randomly initializingthe network can have the adverse ffect of causing most ofthe basic units to operate in the saturation regions of the sig-moid function, where learning is slower than in the transitionregion.Let z~ = E j  wijxj, where zi is the input to unit i.
If we as-sume that the weights, w~j, are randomly initialized such thatthey are uncorrelated with zero mean and constant variance.It can be shown that \[8\]:2 2~E\ [x~\ ] .
(3) Eiz,\] = ~ E\[w,ilE\[xj\] = 0 and a,, = a~J JThus although the expected value of the input to the sigmoid2 depends on the of a basic unit, E\[z~\], is zero, the variance, a,~ ,variance of the random weights, 2 a~, as well as the magnitudes2 is and the number of dimensions of the input vectors.
If a~large, many basic units may operate in the saturation regions.Normal izat ion of  InputsSeveral procedures have been suggested to enable the ba-sic units to operate initially in the transition regions.
Byinitializing the network with small random weights or biasingthe inputs, a~ can be reduced \[1\].
A method called centerinitialization has also been suggested that guarantees all thebasic units initially operate at the center of the transitionregion, where learning is fastest \[8\].383However, as training proceeds, the connection weights are2 depend progressively more changed, resulting in E\[z~\] and cry,on the set of input vectors, ?.
If E\[xj\] and ~jE\[z~\] arelarge, the hidden units may get driven well into the saturationregions, hindering the learning capability of the network.Let {?}
denote the set of training samples, where g =\[sl, s~ .... \]t is the speech-vector for each phoneme token.
Fur-thermore, let~j = (~J - si____._), (4)~o'3where ?j is the standard deviation of sj, gj is the mean ofsj over all the training tokens, and 7 is a positive constant.Thus, E\[xj\] = E\[z~\] = 0.
Assuming "7o'j > 1,= < E (51 j "7o'j jThus by subtracting and normalizing the input patterns ac-cording to Equation (4), the hidden units may operate moreoften in the transition region of the sigmoid function.Adapt ive  GainAlthough Equation (4) provides a mechanism to increasethe learning capability of the network, it requires that ~j anda,~ be compute d before the network is trained.
In this sec-tion, we discuss a different echnique to deal with the learningcapability of the network.During training, the connection weights are usually modi-fied according to Aw~j = q$ixj, where r/is the gain term, and$i is the error signal for unit i.
(For simplicity, the momen-tum term is ignored in the following analysis.)
Thus IAwij\]depends on txjh which means the training procedure paysmore attention to inputs with larger magnitudes than to in-puts with smaller magnitudes.Alternatively, q can be chosen to be adaptive.
Specifically,(6)= I ,1'iwhere ~0 is a small positive constant.
ThusI~w,~l = ~--71~,1 Iz~l, and ~ IAw,~l = ~01~,1.
(7)JAs a result, the total change in the connection weights toa hidden unit is independent of the input, ~, thus allowingthe training procedure to pay similar attention to all inputvectors.Boundary ClassificationIn our segmental framework formulated in Equation i, themain difference between classification and recognition is theincorporation of a probability for each segment, p(s).
As de-scribed previously in Equation 2, we have simplified the prob-lem of estimating p(s) to one of determining the probabilitythat a boundary exists, p(b).
Currently in the SUMMIT sys-tem, boundary classification is based on the height attainedby a boundary in the dendrogram.
A small VQ codebook ofsize 12 is used to quantize the spectral average of each seg-ment, and distributions are collected and parameterized foreach possible context.Since one of the segment networks we considered was notbased on a dendrogram, an alternative classification proce-dure for the boundaries was adopted.
In this procedure, aMLP  with two output units was used, one for the valid bound-aries and the other for the extraneous boundaries.
By ref-erencing the time-aligned phonetic transcription, the desiredoutputs of the network can be determined.
In our current im-plementation, the probability of a detected boundary, p(b~), isdetermined using four abutting segments.
Let ti stand for thetime at which b~ is located, and si stand for the segment be-tween t~ and t~+1, where ti+t > ti.
The boundary probability,p(bi), is then determined by using the acoustic measurementsin s,_2,si-l,s~, and si+l as inputs to the MLP.Data RepresentationThere were two representations used as input for the MLPclassifier.
The first representation was identical to the SUM-MIT system, and consisted of 82 acoustic attributes.
Theattributes were generated automatically by a search proce-dure that uses the training data to determine the settings offree parameters of a set of generic property detectors using anoptimization procedure\[10\].
The second representation con-sisted of a vector of three average spectra which correspondedto the left, middle, and right thirds of a segment.
Th e spectrawere the mean-rate and synchrony outputs of a 40 channel au-ditory model \[Ii\].
Thus, there were 120 points used for eachrepresentation.
Finally, segment duration was also included.Experimental ResultsPhonetic ClassificationThe first experiments which were performed were basedon phonetic classification.
In these tests the system classifieda token taken from a phonetic transcription that had beenaligned with the speech waveform.
Since there was no de-tection involved in these experiments only substitution errorswere possible.
As has been reported previously, the baselinespeaker-independent classification performance of su M MIT onthe testing data was 70% \[12\].
The performance of the MLPclassifier using the same input representation was 74%.
Inthe second set of classification experiments the representationwas based on the spectral outputs described previously.
Fourexperiments were performed using i) the synchrony outputs,2) the mean-rate outputs, 3) the synchrony and mean-rateoutputs and 4) the synchrony and mean-rate outputs andsegment duration.
The results of all experiments have beensummarized in Table 3.
None of these alternative represen-tations was able to achieve the same level of performance asthe SUMMIT acoustic attributes.Boundary ClassificationWe have evaluated the MLP  boundary classifier using thetraining and testing data described earlier.
The inputs to thenetwork are the averages of the mean rate response in the fourabutting segments, resulting in a total of 160 input units.
Byusing 32 hidden units, the network can classify 87% of theboundaries in the test set correctly.384
