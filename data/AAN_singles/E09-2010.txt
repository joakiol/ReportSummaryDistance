Proceedings of the EACL 2009 Demonstrations Session, pages 37?40,Athens, Greece, 3 April 2009. c?2009 Association for Computational LinguisticsAdaptive Natural Language InteractionStasinos KonstantopoulosAthanasios TegosDimitris BilidasNCSR ?Demokritos?, Athens, GreeceColin MathesonHuman Communication Research CentreEdinburgh University, U.K.Ion AndroutsopoulosGerasimos LampourasProdromos MalakasiotisAthens Univ.
of Economics and BusinessGreeceOlivier DerooAcapela Group, BelgiumAbstractThe subject of this demonstration is natu-ral language interaction, focusing on adap-tivity and profiling of the dialogue man-agement and the generated output (textand speech).
These are demonstrated ina museum guide use-case, operating in asimulated environment.
The main techni-cal innovations presented are the profilingmodel, the dialogue and action manage-ment system, and the text generation andspeech synthesis systems.1 IntroductionIn this demonstration we present a number ofstate-of-the art language technology tools, imple-menting and integrating the latest discourse andknowledge representation theories into a completeapplication suite, including:?
dialogue management, natural language gen-eration, and speech synthesis, all modulatedby a flexible and highly adaptable profilingmechanism;?
robust speech recognition and language inter-pretation; and,?
an authoring environment for developing therepresentation of the domain of discourse aswell as the associated linguistic and adaptiv-ity resources.The system demonstration is based on a usecase of a virtual-tour guide in a museum domain.Demonstration visitors interact with the guide us-ing headsets and are able to experiment with load-ing different interaction profiles and observing thedifferences in the guide?s behaviour.
The demon-stration also includes the screening of videos froman embodied instantiation of the system as a robotguiding visitors in a museum.2 Technical ContentThe demonstration integrates a number of state-of-the-art language components into a highly adap-tive natural language interaction system.
Adap-tivity here refers to using interaction profiles thatmodulate dialogue management as well as textgeneration and speech synthesis.
Interaction pro-files are semantic models that extend the objectiveontological model of the domain of discourse withsubjective information, such as how ?interesting?or ?important?
an entity or statement of the objec-tive domain model is.Advanced multimodal dialogue managementcapabilities involving and combining input andoutput from various interaction modalities andtechnologies, such as speech recognition and syn-thesis, natural language interpretation and gener-ation, and recognition of/response to user actions,gestures, and facial expressions.State-of-the art natural language generationtechnology, capable of producing multi-sentence,coherent natural language descriptions of objectsbased on their abstract semantic representation.The resulting descriptions vary dynamically interms of content as well as surface language ex-pressions used to realize each description, depend-ing on the interaction history (e.g., comparingto previously given information) and the adaptiv-ity parameters (exhibiting system personality andadapting to user background and interests).3 System DescriptionThe system is capable of interacting in a vari-ety of modalities, including non-verbal ones suchas gesture and face-expression recognition, but inthis demonstration we focus on the system?s lan-guage interaction components.
In this modality,abstract, language-independent system actions arefirst planned by the dialogue and action manager(DAM), then realized into language-specific text37by the natural language generation engine, and fi-nally synthesized into speech.
All three layers areparametrized by a profiling and adaptivity module.3.1 Profiling and AdaptationProfiling and adaptation modulates the output ofdialogue management, generation, and speechsynthesis so that the system exhibits a syntheticpersonality, while at the same time adapting touser background and interests.User stereotypes (e.g., ?expert?
or ?child?)
pro-vide generation parameters (such as maximum de-scription length) and also initialize the dynamicuser model with interest rates for all the ontologi-cal entities (individuals and properties) of the do-main of discourse.
This same information is alsoprovided in system profiles reflecting the system?s(as opposed to the users?)
preferences; one can,for example, define a profile that favours usingthe architectural attributes to describe a buildingwhere another profile would choose to concentrateon historical facts regarding the same building.Stereotypes and profiles are combined into asingle set of parameters by means of personal-ity models.
Personality models are many-valuedDescription Logic definitions of the overall pref-erence, grounded in stereotype and profile data.These definitions model recognizable personalitytraits so that, for example, an open personality willattend more to the user?s requests than its owninterests in deriving overall preference (Konstan-topoulos et al, 2008).Furthermore, the system dynamically adaptsoverall preference according to both interactionhistory and the current dialogue state.
So, for one,the initial (static model) interest factor of an ontol-ogy entity is reduced each time this entity is usedin a description in order to avoid repetitions.
Onthe other hand, preference will increase if, for ex-ample, in the current state the user has explicitlyasked about an entity.3.2 Dialogue and Action ManagementThe DAM is built around the information-stateupdate dialogue paradigm of the TRINDIKITdialogue-engine toolkit (Cooper and Larsson,1998) and takes into account the combined user-robot interest factor when determining informa-tion state updates.The DAM combines various interaction modal-ities and technologies in both interpretation/fusionand generation/fission.
In interpreting user ac-tions the system recognizes spoken utterances,simple gestures, and touch-screen input, all ofwhich may be combined into a representation ofa multi-modal user action.
Similarly, when plan-ning robotic actions the DAM coordinates a num-ber of available output modalities, including spo-ken language, text (on the touchscreen), the move-ment and configuration of the robotic platform, fa-cial expressions, and simple head gestures.1To handle multimodal input, the DAM uses a fu-sion module which combines messages from thelanguage interpretation, gesture, and touchscreenmodules into a single XML structure.
Schemati-cally, this can be represented as:<userAction><userUtterance>hello</userUtterance><userButton content="13"/></userAction>This structure represents a user pressing some-thing on the touchscreen and saying hello at thesame time.2The representation is passed essentially un-changed to the DAM, to be processed by its up-date rules, where the ID of button press is inter-preted in context and matched with the speech.In most circumstances, the natural language pro-cessing component (see 3.3) produces a seman-tic representation of the input which appears inthe userUtterance element; the use of ?hello?above is for illustration.
An example update rulewhich will fire in the context of a greeting fromthe user is (in schematic form):ifin(/latest_utterance/moves, hello)thenoutput(start)Update rules contain a list of conditions and alist of effects.
Here there is one condition (that thelatest moves from the user includes ?hello?
), andone effect (the ?start?
procedure).
The latter initi-ates the dialogue by, among other things, havingthe system utter a standardised greeting.As noted above, the DAM is also multimodalon the output side.
An XML representation iscreated which can contain robot utterances androbot movements (both head movements and mo-bile platform moves).
Information can also be pre-sented on the touchscreen.1Expressions and gestures will not be demonstrated, asthey can not be materialized in the simulated robot.2The precise meaning of ?at the same time?
is determinedby the fusion module.383.3 Natural Language ProcessingThe NATURALOWL natural language generation(NLG) engine (Galanis et al 2009) producesmulti-sentence, coherent natural language descrip-tions of objects in multiple languages from a sin-gle semantic representation; the resulting descrip-tions are annotated with prosodic markup for driv-ing the speech synthesisers.The generated descriptions vary dynamically, inboth content and language expressions, dependingon the interaction profile as well as the dynamicinteraction history.
The dynamic preference factorof the item itself is used to decide the level of de-tail of the description being generated.
The prefer-ence factors of the properties are used to order thecontents of the descriptions to ensure that, in caseswhere not all possible facts are to be presented ina single turn, the most relevant ones are chosen.The interaction history is used to check previouslygiven information to avoid repeating the same in-formation in different contexts and to create com-parisons with earlier objects.NaturalOWL demonstrates the benefits ofadopting NLG on the Semantic Web.
Organiza-tions that need to publish information about ob-jects, such as exhibits or products, can publishOWL ontologies instead of texts.
NLG engines,embedded in browsers or Web servers, can thenrender the ontologies in natural language, whereascomputer programs may access the ontologies, ineffect logical statements, directly.
The descrip-tions can be very simple and brief, relying onquestion answering to provide more informationif such is requested.
This way, machine-readableinformation can be more naturally inspected andconsulted by users.In order to generate a list of possible followup questions that the system can handle, we ini-tially construct a list of the particular individualsor classes that are mentioned in the generated de-scription; the follow up questions will most likelyrefer to them.
Only individuals and classes forwhich there is further information in the ontologyare extracted.After identifying the referred individuals andclasses, we proceed to predict definition (e.g.,?Who was Ares??)
and property questions (e.g.,?Where is Mount Penteli??)
about them thatcould be answered by the information in the on-tology.
We avoid generating questions that cannotbe answered.
The expected definition questionsare constructed by inserting the names of the re-ferred individuals and classes into templates suchas ?who is/was person X??
or ?what do you knowabout class or entity Y?
?.In the case of referred individuals, we also gen-erate expected property questions using the pat-terns NaturalOWL generates the descriptions with.These patterns, called microplans, show how toexpress the properties of the ontology as sentencesof the target languages.
For example, if the indi-vidual templeOfAres has the property excavate-dIn, and that property has a microplan of the form?resource was excavated in period?, we anticipatequestions such as ?when was the Temple of Aresexcavated??
and ?which period was the Temple ofAres excavated in?
?.Whenever a description (e.g., of a monument)is generated, the expected follow up questions forthat description (e.g., about the monument?s ar-chitect) are dynamically included in the rules ofthe speech recognizer?s grammar, to increase wordrecognition accuracy.
The rules include compo-nents that extract entities, classes, and propertiesfrom the recognized questions, thus allowing thedialogue and action manager to figure out what theuser wishes to know.3.4 Speech Synthesis and RecognitionThe natural language interface demonstrates ro-bust speech recognition technology, capable ofrecognizing spoken phrases in noisy environ-ments, and advanced speech synthesis, capable ofproducing spoken output of very high quality.
Themain challenge that the automatic speech recogni-tion (ASR) module needs to address is backgroundnoise, especially in the robot-embodied use case.A common technique used in order to handle thisis training acoustic models with the anticipatedbackground noise, but that is not always possi-ble.
The demonstrated ASR module can be trainedon noise-contaminated data where available, butalso incorporates multi-band acoustic modelling(Dupont, 2003) for robust recognition under noisyconditions.
Speech recognition rates are also sub-stantially improved by using the predictions madeby NATURALOWL and the DAM to dynamicallyrestrict the lexical and phrasal expectations at eachdialogue turn.The speech synthesis module of the demon-strated system is based on unit selection technol-ogy, generally recognized as producing more nat-39ural output that previous technologies such as di-phone concatenation or formant synthesis.
Themain innovation that is demonstrated is support foremotion, a key aspect of increasing the naturalnessof synthetic speech.
This is achieved by combin-ing emotional unit recordings with run-time trans-formations.
With respect to the former, a complete?voice?
now comprises three sub-voices (neutral,happy, and sad), based on recordings of the samespeaker.
The recording time needed is substan-tially decreased by prior linguistic analysis that se-lects appropriate text covering all phonetic unitsneeded by the unit selection system.
In addition tothe statically defined sub-voices, the speech syn-thesis module implements dynamic transforma-tions (e.g., emphasis), pauses, and variable speechspeed.
The system combines all these capabilitiesin order to dynamically modulate the synthesisedspeech to convey the impression of emotionallymodulated speech.3.5 AuthoringThe interaction system is complemented byELEON (Bilidas et al, 2007), an authoring tool forannotating domain ontologies with the generationand adaptivity resources described above.
The do-main ontology can be authored in ELEON, but anyexisting OWL ontology can also annotated.More specifically, ELEON supports author-ing linguistic resources, including a domain-dependent lexicon, which associates classes andindividuals of the ontology with nouns and propernames of the target natural languages; microplans,which provide the NLG with patterns for realizingproperty instances as sentences; and a partial or-dering of properties, which allows the system toorder the resulting sentences as a coherent text.The adaptivity and profiling resources includeinterest rates, indicating how interesting the enti-ties of the ontology are in any given profile; andstereotype parameters that control generation as-pects such as the number of facts to include in adescription or the maximum sentence length.Furthermore, ELEON supports the author withimmediate previews, so that the effect of anychange in either the ontology or the associated re-sources can be directly reviewed.
The actual gen-eration of the preview is relegated to external gen-eration engines.4 ConclusionsThe demonstrated system combines semantic rep-resentation and reasoning technologies with lan-guage technology into a human-computer interac-tion system that exhibits a large degree of adapt-ability to audiences and circumstances and is ableto take advantage of existing domain model cre-ated independently of the need to build a naturallanguage interface.
Furthermore by clearly sepa-rating the abstract, semantic layer from that of thelinguistic realization, it allows the re-use of lin-guistic resources across domains and the domainmodel and adaptivity resources across languages.AcknowledgementsThe demonstrated system is being developed bythe European (FP6-IST) project INDIGO.3 IN-DIGO develops and advances human-robot inter-action technology, enabling robots to perceive nat-ural human behaviour, as well as making themact in ways that are more familiar to humans.
Toachieve its goals, INDIGO advances various tech-nologies, which it integrates in a robotic platform.ReferencesDimitris Bilidas, Maria Theologou, and VangelisKarkaletsis.
2007.
Enriching OWL ontologieswith linguistic and user-related annotations: theELEON system.
In Proc.
19th Intl.
Conf.
onTools with Artificial Intelligence (ICTAI-2007).Robin Cooper and Staffan Larsson.
1998.
Dia-logue Moves and Information States.
In: Pro-ceedings of the 3rd Intl.
Workshop on Computa-tional Semantics (IWCS-3).Ste?phane Dupont.
2003.
Robust parametersfor noisy speech recognition.
U.S. Patent2003182114.Dimitrios Galanis, George Karakatsiotis, Gerasi-mos Lampouras and Ion Androutsopoulos.2009.
An open-source natural language gener-ator for OWL ontologies and its use in Prote?ge?and Second Life.
In this volume.Stasinos Konstantopoulos, Vangelis Karkaletsis,and Colin Matheson.
2008.
Robot personality:Representation and externalization.
In Proc.Computational Aspects of Affective and Emo-tional Interaction (CAFFEi 08), Patras, Greece.3http://www.ics.forth.gr/indigo/40
