Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 897?904,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDialogue Segmentation with Large Numbers of Volunteer InternetAnnotatorsT.
Daniel MidgleyDiscipline of Linguistics, School of Computer Science and Software EngineeringUniversity of Western AustraliaPerth, AustraliaDaniel.Midgley@uwa.edu.auAbstractThis paper shows the results of anexperiment in dialogue segmentation.
In thisexperiment, segmentation was done on alevel of analysis similar to adjacency pairs.The method of annotation was somewhatnovel: volunteers were invited to participateover the Web, and their responses wereaggregated using a simple voting method.Though volunteers received a minimum oftraining, the aggregated responses of thegroup showed very high agreement withexpert opinion.
The group, as a unit,performed at the top of the list ofannotators, and in many cases performed aswell as or better than the best annotator.1 IntroductionAggregated human behaviour is a valuablesource of information.
The Internet  shows usmany examples of collaboration as a means ofresource creation.
Wikipedia, Amazon.comreviews, and Yahoo!
Answers are just someexamples of large repositories of informationpowered by individuals who voluntarilycontribute their time and talents.
Some NLPprojects are now using this idea, notably the?ESP  Game?
(von Ahn 2004), a data collectioneffort presented as a game in which players labelimages from the Web.
This paper presents anextension of this collaborative volunteer ethic inthe area of dialogue annotation.For dialogue researchers, the prospect  ofusing volunteer annotators from the Web can bean attractive option.
The task of trainingannotators can be time-consuming, expensive,and (if inter-annotator agreement turns out  to bepoor) risky.Getting Internet volunteers for annotation hasits own pitfalls.
Dialogue annotation is often notvery interesting, so it can be difficult  to attractwilling participants.
Experimenters will havelittle control over the conditions of theannotation and the skill of the annotators.Training will be minimal, limited to whatever anaverage Web surfer is willing to read.
There mayalso be perverse or uncomprehending userswhose answers may skew the data.This project began as an exploratory studyabout the intuitions of language users withregard to dialogue segmentation.
We wantedinformation about how language users perceivedialogue segments, and we wanted to be able touse this information as a kind of gold standardagainst  which we could compare theperformance of an automatic dialoguesegmenter.
For our experiment, the advantagesof Internet  annotation were compelling.
Wecould get  free data from as many language usersas we could attract, instead of just two or threewell-trained experts.
Having more respondentsmeant  that our results could be more readilygeneralised to language users as a whole.We expected that  multiple users wouldconverge upon some kind of uniform result.What  we found (for this task at least) was thatlarge numbers of volunteers show very strongtendencies that  correspond well to expertopinion, and that  these patterns of agreement  aresurprisingly resilient in the face of noisy inputfrom some users.
We also gained some insightsinto the way that people perceived dialoguesegments.2 SegmentationWhile much work in dialogue segmentationcenters around topic (e.g.
Galley et  al.
2003,Hsueh et  al.
2006, Purver et  al.
2006), wedecided to examine dialogue at a more fine-grained level.
The level of analysis that we havechosen corresponds most closely to adjacencypairs (after Sacks, Schegloff and Jefferson1974), where a segment is made of matched setsof utterances from different speakers (e.g.question/answer or suggest/accept).
We chose tosegment  dialogues this way in order to improvedialogue act  tagging, and we think that897examining the back-and-forth detail of themechanics of dialogue will be the most helpfullevel of analysis for this task.The back-and-forth nature of dialogue alsoappears in Clark and Schaefer?s (1989)influential work on contributions in dialogue.
Inthis view, two-party dialogue is seen as a set ofcooperative acts used to add information to thec o m m o n g r o u n d f o r t h e p u r p o s e o faccomplishing some joint  action.
Clark andSchaefer map these speech acts ontocontribution trees.
Each utterance within acontribution tree serves either to present someproposition or to acknowledge a previous one.Accordingly, each contribution tree has apresentation phase and an acceptance phase.Participants in dialogue assume that items theypresent  will be added to the common groundunless there is evidence to the contrary.However, participants do not  always showacceptance of these items explicitly.
Speaker Bmay repeat Speaker?s A?s information verbatimto show understanding (as one does with aphone number), but  for other kinds ofinformation a simple ?uh-huh?
will constituteadequate evidence of understanding.
In general,less and less evidence will be required thefarther on in the segment one goes.In practice, then, segments have a tailing-offquality that we can see in many dialogues.
Table1 shows one example from Verbmobil-2, acorpus of appointment scheduling dialogues.
(Adescription of this corpus appears inAlexandersson 1997.
)A segment begins when WJH brings aquestion to the table (utterances 1 and 2 in ourexample), AHS answers it  (utterance 3), andWJH acknowledges the response (utterance 4).At this point, the question is considered to beresolved, and a new contribution can be issued.WJH starts a new segment in utterance 5, andthis utterance shows features that  will befamiliar to dialogue researchers: the number ofwords increases, as does the incidence of newwords.
By the end of this segment (utterance 8),AHS only needs to offer a simple ?okay?
to showacceptance of the foregoing.Our work is not intended to be a strictimplementation of Clark and Schaefer?scontribution trees.
The segments represented bythese units is what we were asking our volunteerannotators to find.
Other researchers have alsoused a level of analysis similar to our own.J?nsson?s (1991) initiative-response units is oneexample.Taking a cue from Mann (1987), we decidedto describe the behaviour in these segmentsusing an atomic metaphor: dialogue segmentshave nuclei, where someone says something,and someone says something back (roughlycorresponding to adjacency pairs), and satellites,usually shorter utterances that give feedback onwhatever the nucleus is about.For our annotators, the process was simply tofind the nuclei, with both speakers taking part,and then attach any nearby satellites thatpertained to the segment.We did not attempt to distinguish nestedadjacency pairs.
These would be placed withinthe same segment.
Eventually we plan to modifyour system to recognise these nested pairs.3 Experimental Design3.1 CorpusIn the pilot phase of the experiment, volunteerscould choose to segment  up to four randomly-chosen dialogues from the Verbmobil-2 corpus.
(One longer dialogue was separated into two.
)We later ran a replication of the experiment  witheleven dialogues.
For this latter phase, eachvolunteer started on a randomly chosen dialogueto ensure evenness of responses.The dialogues contained between 44 and 109utterances.
The average segment was 3.59utterances in length, by our annotation.Two dialogues have not been examinedbecause they will be used as held-out  data forthe next  phase of our research.
Results from the1 WJH <uhm> basically we have to bein Hanover for a day and a half2 WJH correct3 AHS right4 WJH okay5 WJH <uh> I am looking through myschedule for the next threemonths6 WJH and I just noticed I am workingall of Christmas week7 WJH so I am going to do it inGermany if at all possible8 AHS okayTable 1.
A sample of the corpus.
Two segments arerepresented here.898other thirteen dialogues appear in part  4 of thispaper.3.2 AnnotatorsVolunteers were recruited via postings onvarious email lists and websites.
This included aposting on the university events mailing list,sent  to people associated with the university, butwith no particular linguistic training.
Linguisticsfirst-year students and Computer Sciencestudents and staff were also informed of theproject.
We sent advertisements to a variety ofinternational mailing lists pertaining tolanguage, computation, and cognition, sincethese lists were most likely to have a readershipthat was interested in language.
These includedLinguist  List, Corpora, CogLing-L, andHCSNet.
An invitation also appeared on thepersonal blog of the first author.At the experimental website, volunteers wereasked to read a brief description of how toannotate, including the descriptions of nucleiand satellites.
The instruction page showed someexamples of segments.
Volunteers wererequested not to return to the instruction pageonce they had started the experiment.The annotator guide with examples can beseen at the following URL:http://tinyurl.com/ynwmx9A scheme that relies on volunteer annotationwill need to address the issue of motivation.People have a desire to be entertained, butdialogue annotation can often be tedious anddifficult.
We attempted humor as a way ofkeeping annotators amused and annotating for aslong as possible.
After submitting a dialogue,annotators would see an encouraging page,sometimes with pretend ?badges?
like the onepictured in Figure 1.
This was intended as a wayof keeping annotators interested to see whatcomments would come next.
Figure 2 showsstatistics on how many dialogues were markedby any one IP address.
While over half of thevolunteers marked only one dialogue, manyvolunteers marked all four (or in the replication,all eleven) dialogues.
Sometimes more thaneleven dialogues were submitted from the samelocation, most likely due to multiple userssharing a computer.In all, we received 626 responses from about231 volunteers (though this is difficult todetermine from only the volunteers?
IPnumbers).
We collected between 32 and 73responses for each of the 15 dialogues.3.3 Method of EvaluationWe used the WindowDiff (WD) metric (Pevznerand Hearst 2002) to evaluate the responses ofour volunteers against  expert  opinion (ourresponses).
The WD algorithm calculatesagreement  between a reference copy of thecorpus and a volunteer?s hypothesis by moving awindow over the utterances in the two corpora.The window has a size equal to half the averagesegment length.
Within the window, thealgorithm examines the number of segmentboundaries in the reference and in thehypothesis, and a counter is augmented by one ifthey disagree.
The WD score between thereference and the hypothesis is equal to thenumber of discrepancies divided by the numberof measurements taken.
A score of 0 would begiven to two annotators who agree perfectly, and1 would signify perfect disagreement.Figure 3 shows the WD scores for thevolunteers.
Most volunteers achieved a WDscore between .15 and .2, with an average of?245.Cohen?s Kappa (!)
(Carletta 1996) is anothermethod of comparing inter-annotator agreement03060901201501 2 3 4 5 6 7 8 9 10 11 >111202510323 4 3 1 2 0172Number of annotatorsNumber of dialogues completedFigure 2.
Number of dialogues annotated by singleIP addressesFigure 1.
One of the screens that appears after anannotator submits a marked form.899in segmentation that is widely used incomputational language tasks.
It measures theobserved agreement (AO) against the agreementwe should expect by chance (AE), as follows:!
=AO - AE1 - AEFor segmentation tasks, !
is a more stringentmethod than WindowDiff, as it does notconsider near-misses.
Even so, !
scores arereported in Section 4.About a third of the data came fromvolunteers who chose to complete all eleven ofthe dialogues.
Since they contributed so much ofthe data, we wanted to find out  whether theywere performing better than the othervolunteers.
This group had an average WD scoreof .199, better than the rest  of the group at .268.However, skill does not  appear to increasesmoothly as more dialogues are completed.
Thehighest  performance came from the group thatcompleted 5 dialogues (average WD = .187), the02550751001251500 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Number of responsesWindowDiff rangelowest  from those that  completed 8 dialogues (.299).3.4 AggregationWe wanted to determine, insofar as waspossible, whether there was a group consensusas to where the segment boundaries should go.We decided to try overlaying the results from allrespondents on top of each other, so that eachclick from each respondent  acted as a sort  ofvote.
Figure 4 shows the result  of aggregatingannotator responses from one dialogue in thisway.
There are broad patterns of agreement;high ?peaks?
where many annotators agreed thatan utterance was a segment  boundary, areas ofuncertainty where opinion was split betweentwo adjacent  utterances, and some backgroundnoise from near-random respondents.Group opinion is manifested in these peaks.Figure 5 shows a hypothetical example toillustrate how we defined this notion.
A peak isany local maximum (any utterance u where u - 1< u > u + 1) above background noise, which wedefine as any utterance with a number of votesbelow the arithmetic mean.
Utterance 5, being alocal maximum, is a peak.
Utterance 2, though alocal maximum, is not a peak as it  is below themean.
Utterance 4 has a comparatively largenumber of votes, but it  is not considered a peakbecause its neighbour, utterance 5, is higher.Defining peaks this way allows us to focus onthe points of highest agreement, while ignoringnot only the relatively low-scoring utterances,Figure 4.
The results for one dialogue.
Each utterance in the dialogue is represented in sequence along the x axis.Numbers in dots represent the number of respondents that ?voted?
for that utterance as a segment boundary.
Peaksappear where agreement is strongest.
A circle around a data point indicates our choices for segment boundary.051015202530354045after_e059ach1_000_ANV_00after_e059ach1_000_ANV_01after_e059ach2_001_CNK_02after_e059ach2_001_CNK_03after_e059ach1_002_ANV_04after_e059ach1_002_ANV_05after_e059ach1_002_ANV_06after_e059ach2_003_CNK_07after_e059ach1_004_ANV_08after_e059ach2_005_CNK_09after_e059ach1_006_ANV_10after_e059ach1_006_ANV_11after_e059ach1_006_ANV_12after_e059ach2_007_CNK_13after_e059ach2_007_CNK_14after_e059ach2_007_CNK_15after_e059ach1_008_ANV_16after_e059ach1_008_ANV_17after_e059ach1_008_ANV_18after_e059ach1_008_ANV_19after_e059ach2_009_CNK_20after_e059ach1_010_ANV_21after_e059ach1_010_ANV_22after_e059ach1_010_ANV_23after_e059ach1_010_ANV_24after_e059ach2_011_CNK_25after_e059ach1_012_ANV_26after_e059ach1_012_ANV_27after_e059ach2_013_CNK_28after_e059ach2_014_CNK_29after_e059ach1_015_ANV_30after_e059ach1_016_ANV_31after_e059ach1_016_ANV_32after_e059ach1_016_ANV_33after_e059ach2_017_CNK_34after_e059ach1_018_ANV_35after_e059ach1_018_ANV_36after_e059ach1_018_ANV_37after_e059ach2_019_CNK_38after_e059ach2_019_CNK_39after_e059ach1_020_ANV_40after_e059ach2_021_CNK_41after_e059ach2_021_CNK_42after_e059ach1_022_ANV_43after_e059ach2_023_CNK_44after_e059ach1_024_ANV_45after_e059ach2_025_CNK_46after_e059ach1_026_ANV_47after_e059ach2_027_CNK_48after_e059ach1_028_ANV_49after_e059ach2_029_CNK_50after_e059ach2_030_CNK_51after_e059ach2_030_CNK_52after_e059ach2_030_CNK_53after_e059ach2_030_CNK_54after_e059ach2_030_CNK_55after_e059ach2_030_CNK_56after_e059ach2_030_CNK_57after_e059ach1_031_ANV_58after_e059ach2_032_CNK_59after_e059ach1_033_ANV_60after_e059ach2_034_CNK_61after_e059ach1_035_ANV_62after_e059ach1_035_ANV_63after_e059ach2_036_CNK_64after_e059ach1_037_ANV_65after_e059ach2_038_CNK_66after_e059ach2_039_CNK_67after_e059ach1_040_ANV_68after_e059ach1_040_ANV_69after_e059ach1_040_ANV_70after_e059ach2_041_CNK_71after_e059ach1_042_ANV_72after_e059ach1_042_ANV_73after_e059ach2_043_CNK_74after_e059ach1_044_ANV_752237341112426306364337303242120233844918632381223824171422213732200413215235442510212616147284336e059 n = 42mean = 9.89Figure 3.
WD scores for individual responses.
Ascore of 0 indicates perfect agreement.900but also the potentially misleading utterancesnear a peak.There are three disagreements in the dialoguepresented in Figure 4.
For the first, annotatorssaw a break where we saw a continuation.
Theother two disagreements show the reverse:annotators saw a continuation of topic as acontinuation of segment.4 ResultsTable 2 shows the agreement of the aggregatedgroup votes with regard to expert  opinion.
Theaggregated responses from the volunteerannotators agree extremely well with expertopinion.
Acting as a unit, the group?sWindowDiff scores always perform better thanthe individual annotators on average.
While theindividual annotators attained an average WDscore of .245, the annotators-as-group scoredWD = .108.On five of the thirteen dialogues, the groupperformed as well as or better than the bestindividual annotator.
On the other eightdialogues, the group performance was towardthe top of the group, bested by one annotator(three times), two annotators (once), fourannotators (three times), or six annotators(once), out of a field of 32?73 individuals.
Thissuggests that aggregating the scores in this waycauses a ?majority rule?
effect  that brings out  thebest answers of the group.One drawback of the WD statistic (asopposed to !)
is that  there is no clear consensusfor what constitutes ?good agreement?.
Forcomputational linguistics, !
!
.67 is generallyconsidered strong agreement.
We found that !for the aggregated group ranged from .71 to .94.Over all the dialogues, !
= ?84.
This issurprisingly high agreement  for a dialogue-leveltask, especially considering the stringency of the!
statistic, and that  the data comes fromuntrained volunteers, none of whom weredropped from the sample.5 Comparison to Trivial BaselinesWe used a number of trivial baselines to see ifour results could be bested by simple means.These were random placement  of boundaries,majority class, marking the last  utterance in eachturn as a boundary, and a set of hand-built ruleswe called ?the Trigger?.
The results of thesetrials can be seen in Figure 6.Dialogue nameWD average asmarked byvolunteersWD singleannotator bestWD singleannotatorworstWD for groupopinionHow manyannotators didbetter?Number ofannotatorse041a 0.210 0.094 0.766 0.094 0 39e041b 0.276 0.127 0.794 0.095 0 39e059 0.236 0.080 0.920 0.107 1 42e081a 0.244 0.037 0.611 0.148 4 36e081b 0.267 0.093 0.537 0.148 4 32e096a 0.219 0.083 0.604 - - 32e096b 0.160 0.000 0.689 0.044 1 36e115 0.214 0.079 0.750 0.079 0 34e119 0.241 0.102 0.610 - - 32e123a 0.259 0.043 1.000 0.174 6 34e123b 0.193 0.093 0.581 0.047 0 33e030 0.298 0.110 0.807 0.147 2 55e066 0.288 0.063 0.921 0.063 0 69e076a 0.235 0.026 0.868 0.053 1 73e076b 0.270 0.125 0.700 0.175 4 40ALL 0.245 0.000 1.000 0.108 60 626Table 2.
Summary of WD results for dialogues.
Data has not been aggregated for two dialogues because theyare being held out for future work.mean = 9.5utt1utt2utt3utt4utt5utt627311275Figure 5.
Defining the notion of ?peak?.
Numbers incircles indicate number of ?votes?
for that utteranceas a boundary.9015.1 Majority ClassThis baseline consisted of marking everyutterance with the most common classification,which was ?not a boundary?.
(About one in fourutterances was marked as the end of a segmentin the reference dialogues.)
This was one of theworst case baselines, and gave WD = .551 overall dialogues.5.2 Random Boundary PlacementWe used a random number generator torandomly place as many boundaries in eachdialogue as we had in our reference dialogues.This method gave about the same accuracy asthe ?majority class?
method with WD = .544.5.3 Last Utterance in TurnIn these dialogues, a speaker?s turn could consistof more than one utterance.
For this baseline,every final utterance in a turn was marked as thebeginning of a segment, except when loneutterances would have created a segment  withonly one speaker.This method was suggested by work fromSacks, Schegloff, and Jefferson (1974) whoobserved that the last  utterance in a turn tends tobe the first pair part  for another adjacency pair.Wright, Poesio, and Isard (1999) used a variantof this idea in a dialogue act  tagger, includingnot only the previous utterance as a feature, butalso the previous speaker?s last speech act type.This method gave a WD score of .392.5.4 The TriggerThis method of segmentation was a set of hand-built rules created by the author.
In this method,two conditions have to exist  in order to start anew segment.?
Both speakers have to have spoken.?
One utterance must contain four words orless.The ?four words?
requirement was determinedempirically during the feature selection phase ofan earlier experiment.Once both these conditions have been met,the ?trigger?
is set.
The next utterance to havemore than four words is the start of a newsegment.This method performed comparatively well,with WD = .210, very close to the averageindividual annotator score of .245.As mentioned, the aggregated annotator scorewas WD = .108.00.10.20.30.40.50.6Majority Random Last utterance Trigger Group0.1080.2100.3920.5440.551WDscoresFigure 6.
Comparison of the group?s aggregatedresponses to trivial baselines.5.5 Comparison to Other WorkComparing these results to other work isdifficult because very little research focuses ondialogue segmentation at this level of analysis.J?nsson (1991) uses initiative-response pairs asa part  of a dialogue manager, but  does notattempt to recognise these segments explicitly.Comparable statistics exist  for a differenttask, that  of multiparty topic segmentation.
WDscores for this task fall consistently into the .25range, with Galley et al (2003) at  .254, Hsueh etal.
(2006) at  .283, and Purver et al (2006)at  .?284.
We can only draw tenuous conclusionsbetween this task and our own, however thisdoes show the kind of scores we should beexpecting to see for a dialogue-level task.
Amore similar project would help us to make amore valid comparison.6 DiscussionThe discussion of results will follow the twofoci of the project: first, some comments aboutthe aggregation of the volunteer data, and thensome comments about the segmentation itself.6.1 Discussion of AggregationA combination of factors appear to havecontributed to the success of this method, someinvolving the nature of the task itself, and someinvolving the nature of aggregated groupopinion, which has been called ?the wisdom ofcrowds?
(for an informal introduction, seeSurowiecki 2004).The fact  that  annotator responses wereaggregated means that no one annotator had toperform particularly well.
We noticed a range ofstyles among our annotators.
Some annotatorsagreed very well with the expert opinion.
A few902annotators seemed to mark utterances in near-random ways.
Some ?casual annotators?
seemedto drop in, click only a few of the most obviousboundaries in the dialogue, and then submit theform.
This kind of behaviour would give thatannotator a disastrous individual score, butwhen aggregated, the work of the casualannotator actually contributes to the overallpicture provided by the group.
As long as thewrong responses are randomly wrong, they donot detract  from the overall pattern and novolunteers need to be dropped from the sample.It  may not  be surprising that people withlanguage experience tend to arrive at more orless the same judgments on this kind of task, orthat the aggregation of the group data wouldnormalise out the individual errors.
What issurprising is that the judgments of the group,aggregated in this way, correspond more closelyto expert opinion than (in many cases) the bestindividual annotators.6.2 Discussion of SegmentationThe concept of segmentation as described here,including the description of nuclei and satellites,appears to be one that annotators can grasp evenwith minimal training.The task of segmentation here is somewhatdifferent  from other classification tasks.Annotators were asked to find segmentboundaries, making this essentially a two-classclassification task where each utterance wasmarked as either a boundary or not a boundary.It  may be easier for volunteers to cope withfewer labels than with many, as is more commonin dialogue tasks.
The comparatively lowperplexity would also help to ensure thatvolunteers would see the annotation through.One of the outcomes of seeing annotatoropinion was that we could examine and learnfrom cases where the annotators votedoverwhelmingly contrary to expert  opinion.
Thisgave us a chance to learn from what  the humanannotators thought  about language.
Even thoughthese results do not literally come from oneperson, it  is still interesting to look at  the generalpatterns suggested by these results.
?let?s  see?
: This utterance usually appearsnear boundaries, but  does it  mark the end of asegment, or the beginning of a new one?
Wetended to place it  at the end of the previoussegment, but human annotators showed a verystrong tendency to group it with the nextsegment.
This was despite an example on thetraining page that suggested joining theseutterances with the previous segment.Topic: The segments under study here aredifferent  from topic.
The segments tend to besmaller, and they focus on the mechanics of theexchanges rather than centering around onetopic to its conclusion.
Even though theannotators were asked to mark for adjacencypairs, there was a distinct  tendency to marklonger units more closely pertaining to topic.Table 3 shows one example.
We had marked thespace between utterances 2 and 3 as a boundary;volunteers ignored it.
It  was slightly morecommon for annotators to omit our boundariesthan to suggest  new ones.
The average segmentlength was 3.64 utterances for our volunteers,compared with 3.59 utterances for experts.Areas of uncertainty: At  certain points onthe chart, opinion seemed to be split as one ormore potential boundaries presented themselves.This seemed to happen most  often when two ormore of the same speech act  appearedsequentially, e.g.
two or more questions,information-giving statements, or the like.7 Conclusions and Future WorkWe drew a number of conclusions from thisstudy, both about the viability of our method,and about the outcomes of the study itself.First, it  appears that for this task, aggregatingthe responses from a large number ofanonymous volunteers is a valid method ofannotation.
We would like to see if this patternholds for other kinds of classification tasks.
If itdoes, it  could have tremendous implications fordialogue-level annotation.
Reliable results couldbe obtained quickly and cheaply from largenumbers of volunteers over the Internet, withoutthe time, the expense, and the logisticalcomplexity of training.
At present, however, it  isunclear whether this volunteer annotation1 MGT so what time should we meet2 ADB <uh> well it doesn't matteras long as we both checkedin I mean whenever we meetis kind of irrelevant3 ADB so maybe about try to4 ADB you want to get some lunchat the airport before we go5 MGT that is a good ideaTable 3.
Example from a dialogue.903technique could be extended to otherclassification tasks.
It  is possible that the strongagreement  seen here would also be seen on anytwo-class annotation problem.
A retest isunderway with annotation for a different two-class annotation set and for a multi-class task.Second, it appears that the concept ofsegmentation on the adjacency pair level, withthis description of nuclei and satellites, is onethat annotators can grasp even with minimaltraining.
We found very strong agreementbetween the aggregated group answers and theexpert opinion.We now have a sizable amount ofinformation from language users as to how theyperceive dialogue segmentation.
Our next step isto use these results as the corpus for a machinelearning task that can duplicate humanperformance.
We are consider ing theTransformation-Based Learning algorithm,which has been used successfully in NLP taskssuch as part of speech tagging (Brill 1995) anddialogue act  classification (Samuel 1998).
TBLis attractive because it allows one to start  from amarked up corpus (perhaps the Trigger, as thebest-performing trivial baseline), and improvesperformance from there.We also plan to use the information from thesegmentation to examine the structure ofsegments, especially the sequences of dialogueacts within them, with a view to improving adialogue act tagger.AcknowledgementsThanks to Alan Dench and to T. Mark Ellisonfor reviewing an early draft  of this paper.
Weespecially wish to thank the individualvolunteers who contributed the data for thisresearch.ReferencesLuis von Ahn and Laura Dabbish.
2004.
Labelingimages with a computer game.
In Proceedings ofthe SIGCHI Conference on Human Factors inComputing Systems.
pp.
319?326.Jan Alexandersson,  Bianka Buschbeck-Wolf,Tsutomu Fujinami, Elisabeth Maier, NorbertReithinger, Birte Schmitz, and Melanie Siegel.1997.
Dialogue acts in VERBMOBIL-2 .Verbmobil Report 204, DFKI, University ofSaarbruecken.Eric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21(4): 543?565.Jean C. Carletta.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2): 249?254.Herbert H. Clark and Edward F. Schaefer.
1989.Contributing to discourse.
Cognitive Science,13:259?294.Michael Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discoursesegmentation of multi-party conversation.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics, pp.562?569.Pei-Yun Hsueh, Johanna Moore, and Steve Renals.2006.
Automatic segmentation of multipartydialogue.
In Proceedings of the EACL 2006,  pp.273?280.Arne J?nsson.
1991.
A dialogue manager usinginitiative-response units and distributed control.
InProceedings of the Fifth Conference of theEuropean Association for ComputationalLinguistics, pp.
233?238.William C.  Mann and Sandra A. Thompson.
1987.Rhetorical structure theory: A framework for theanalysis of texts.
In IPRA Papers in Pragmatics 1:1-21.Lev Pevzner and Marti A. Hearst.
2002.
A critiqueand improvement of an evaluation metric for textsegmentation.
Computational Linguistics, 28(1):19?36.Matthew Purver, Konrad P.  K?rding, Thomas L.Griffiths, and Joshua B. Tenenbaum.
2006.Unsupervised topic modelling for multi-partyspoken discourse.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL,pp.
17?24.Harvey Sacks, Emanuel A. Schegloff, and GailJefferson.
1974.
A simplest systematics for theorganization of turn-taking for conversation.Language, 50:696?735.Ken Samuel,  Sandra Carberry, and K. Vijay-Shanker.1998.
Dialogue act tagging with transformation-based learning.
In Proceedings of COLING/ACL'98, pp.
1150?1156.James Surowiecki.
2004.
The wisdom of crowds:Why the many are smarter than the few.
Abacus:London, UK.Helen Wright, Massimo Poesio,  and Stephen Isard.1999.
Using high level dialogue information fordialogue act recognition using prosodic features.In DIAPRO-1999, pp.
139?143.904
