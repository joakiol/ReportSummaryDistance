Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 235?243,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsExtracting decisions from multi-party dialogue using directed graphicalmodels and semantic similarityTrung H. Bui1, Matthew Frampton1, John Dowding2, and Stanley Peters11Center for the Study of Language and Information, Stanford University{thbui|frampton|peters}@stanford.edu2University of California/Santa Cruzjdowding@ucsc.eduAbstractWe use directed graphical models (DGMs)to automatically detect decision discus-sions in multi-party dialogue.
Our ap-proach distinguishes between different di-alogue act (DA) types based on their rolein the formulation of a decision.
DGMsenable us to model dependencies, includ-ing sequential ones.
We summarize deci-sions by extracting suitable phrases fromDAs that concern the issue under discus-sion and its resolution.
Here we use asemantic-similarity metric to improve re-sults on both manual and ASR transcripts.1 IntroductionIn work environments, people share informationand make decisions in multi-party conversationsknown as meetings.
The demand for systems thatcan automatically process, understand and sum-marize information contained in audio and videorecordings of meetings is growing rapidly.
Ourown research, and that of other contemporaryprojects (Janin et al, 2004), aim at meeting thisdemand.At present, we are focusing on the automaticdetection and summarization of decision discus-sions.
Our approach for detecting decision dis-cussions involves distinguishing between differ-ent dialogue act (DA) types based on their rolein the decision-making process.
Two of thesetypes are DAs which describe the Issue under dis-cussion, and DAs which describe its Resolution.To summarize a decision discussion, we identifywords and phrases in the Issue and ResolutionDAs, which can be used to produce a concise, de-scriptive summary.This paper describes new experiments in bothdetecting and summarizing decision discussions.In the detection stage, we investigate the use ofDirected Graphical Models (DGMs).
DGMs areattractive because they can be used to model se-quence and dependencies between predictor vari-ables.
In the summarization stage, we attempt toimprove phrase selection with a new feature thatmeasures the level of semantic similarity betweencandidate Issue phrases and Resolution utterances,and vice-versa.
The feature is generated by asemantic-similarity metric which uses WordNet asa knowledge source.
The motivation is that ordi-narily, the Issue and Resolution components in adecision summary should be semantically similar.The paper proceeds as follows.
Firstly, Sec-tion 2 describes related work, and Section 3, ourdata-set and annotation scheme for decision dis-cussions.
Section 4 then reports our decision de-tection experiments using DGMs, and Section 5,the summarization experiments.
Finally, Section6 draws conclusions and proposes ideas for futurework.2 Related WorkUser studies (Banerjee et al, 2005) have con-firmed that meeting participants consider deci-sions to be one of the most important meetingoutputs, and (Whittaker et al, 2006) found thatthe development of an automatic decision detec-tion component is critical to the re-use of meet-ing archives.
With the new availability of substan-tial meeting corpora such as the AMI corpus (Mc-Cowan et al, 2005), recent years have thereforeseen an increasing amount of research on decision-making dialog.
This research has tackled issuessuch as the automatic detection of agreement anddisagreement (Galley et al, 2004), and of the235level of involvement of conversational participants(Gatica-Perez et al, 2005).
In addition, (Verbreeet al, 2006) created an argumentation scheme in-tended to support automatic production of argu-ment structure diagrams from decision-orientedmeeting transcripts.
As yet, there has been rela-tively little work which specifically addresses theautomatic detection and summarization of deci-sions.Decision discussion detection: (Hsueh andMoore, 2007) used the AMI Meeting Corpus, andattempted to automatically identify DAs in meet-ing transcripts which are ?decision-related?.
Foreach meeting, two manually created summarieswere used to judge which DAs were decision-related: an extractive summary of the whole meet-ing, and an abstractive summary of its decisions.Those DAs in the extractive summary which sup-port any of the decisions in the abstractive sum-mary were manually tagged as decision-related.
(Hsueh and Moore, 2007) then trained a Maxi-mum Entropy classifier to recognize this singleDA class, using a variety of lexical, prosodic, DAand conversational topic features.
They achievedan F-score of 0.35.Unlike (Hsueh and Moore, 2007), (Ferna?ndez etal., 2008b) made an attempt at modelling the struc-ture of decision-making dialogue.
The authors de-signed an annotation scheme that takes account ofthe different roles which utterances can play in thedecision-making process?for example it distin-guishes between DDAs (decision DAs) which ini-tiate a discussion by raising an issue, those whichpropose a resolution, and those which expressagreement for a proposed resolution.
The authorsannotated a portion of the AMI corpus, and thenapplied what they refer to as ?hierarchical classi-fication?.
Here, one sub-classifier per DDA classhypothesizes occurrences of that DDA class, andthen based on these hypotheses, a super-classifierdetermines which regions of dialogue are deci-sion discussions.
All of the classifiers, (sub andsuper), were linear kernel binary Support Vec-tor Machines (SVMs).
Results were better thanthose obtained with (Hsueh and Moore, 2007)?sapproach?the F1-score for detecting decision dis-cussions in manual transcripts was .58 vs. .50.Note that (Purver et al, 2007) had previously pur-sued the same basic approach as (Ferna?ndez et al,2008b) in order to detect action items.In this paper, we build on the promising resultsof (Ferna?ndez et al, 2008b), by using DirectedGraphical Models (DGMs) in place of SVMs.DGMs are attractive because they provide a natu-ral framework for modelling sequence and depen-dencies between variables including the DDAs.We are especially interested in whether DGMsbetter exploit non-lexical features.
(Ferna?ndez etal., 2008b) obtained much more value from lexi-cal than non-lexical features (and indeed no valueat all from prosodic features), but lexical featureshave disadvantages.
In particular, they can be do-main specific, increase the size of the feature spacedramatically, and deteriorate more than other fea-tures in quality when ASR is poor.Decision summarization: Recent years haveseen research on spoken dialogue summarization(e.g.
(Zechner, 2002)).
Most has attempted to gen-erate summaries of full dialogues, but some veryrecent research has focused on specific dialogueevents, namely action items (Purver et al, 2007),and decisions (Ferna?ndez et al, 2008a).
(Ferna?ndez et al, 2008a) used the DDA an-notation scheme mentioned above, and began byextracting the DDAs which raise issues or pro-vide accepted resolutions.
Only manual tran-scripts were used and the DDAs were extractedby hand rather than automatically.
The next stepwas to parse each DDA with a general rule-basedparser (Dowding et al, 1993), producing multi-ple short fragments rather than one full utteranceparse.
Then, for each DDA, an SVM regressionmodel used various features (including parse, se-mantic and lexical features) to select the fragmentwhich was most likely to appear in a gold-standardextractive decision summary.
The entire manualutterance transcriptions were used as the baseline,and although the SVM?s precision was high, it wasnot enough to offset the baseline?s perfect recall,and so its F-score was lower.
The ?Oracle?, whichalways chooses the fragment with the highest F1-score produced very good results.
This motivatesdeeper investigation into how to improve the frag-ment/parse selection phase, and so we assess theusefulness of a semantic-similarity feature for theSVM.
We conduct experiments with ASR as wellas manual transcripts.3 DataFor the experiments reported in this study, we used17 meetings from the AMI Meeting Corpus (Mc-Cowan et al, 2005), a freely available corpus of236multi-party meetings with both audio and videorecordings, and a wide range of annotated in-formation including DAs and topic segmentation.Conversations are in English, but some partici-pants are non-native English speakers.
The meet-ings last around 30 minutes each, and are scenario-driven, wherein four participants play differentroles in a company?s design team: project man-ager, marketing expert, interface designer and in-dustrial designer.3.1 Modelling Decision DiscussionsWe use the same annotation scheme as (Ferna?ndezet al, 2008b) to model decision-making dialogue.As stated in Section 2, this scheme distinguishesbetween a small number of DA types based on therole which they perform in the formulation of a de-cision.
Apart from improving the initial detectionof decision discussions (Ferna?ndez et al, 2008b),such a scheme also aids their subsequent summa-rization, because it indicates which utterances con-tain particular types of information.The annotation scheme is based on the observa-tion that a decision discussion contains the follow-ing main structural components: (a) a topic or is-sue requiring resolution is raised, (b) one or morepossible resolutions are considered, (c) a particularresolution is agreed upon and so becomes the de-cision.
Hence the scheme distinguishes betweenthree main decision dialogue act (DDA) classes:issue (I), resolution (R), and agreement (A).
ClassR is further subdivided into resolution proposal(RP) and resolution restatement (RR).
I utterancesintroduce the topic of the decision discussion, ex-amples being ?Are we going to have a backup?
?and ?But would a backup really be necessary?
?in Dialogue 1.
On the other hand, R utterancesspecify the resolution which is ultimately adoptedas the decision.
RP utterances propose this reso-lution (e.g.
?I think maybe we could just go forthe kinetic energy.
.
.
?
), while RR utterances closethe discussion by confirming/summarizing the de-cision (e.g.
?Okay, fully kinetic energy?)
.
Finally,A utterances agree with the proposed resolution,signalling that it is adopted as the decision, (e.g.
?Yeah?, ?Good?
and ?Okay?).
Note that an utter-ance can be assigned to more than one DDA class,and within a decision discussion, more than oneutterance can be assigned to the same DDA class.We use both manual and ASR one-best tran-scripts1 in the experiments described here.
DDAannotations were first made on the manual tran-scripts, and then transferred onto the ASR tran-scripts.
Inter-annotator agreement was satisfac-tory, with kappa values ranging from .63 to .73 forthe four DDA classes.
Due to different segmen-tation, the manual and ASR transcripts contain atotal of 15,680 and 8,357 utterances respectively,and on average, 40 and 33 DDAs per meeting.Hence DDAs are slightly less sparse in the ASRtranscripts: for all DDAs, 6.7% vs. 4.3% of the to-tal number of utterances, for I, 1.6% vs. 0.9%, forRP, 2% vs. 1%, for RR, 0.5% vs. 0.4%, and for A,2.6% vs.
2%.
(1) A: Are we going to have a backup?
Or we dojust?B: But would a backup really be necessary?A: I think maybe we could just go for thekinetic energy and be bold and innovative.C: Yeah.B: I think?
yeah.A: It could even be one of our selling points.C: Yeah ?laugh?.D: Environmentally conscious or something.A: Yeah.B: Okay, fully kinetic energy.D: Good.24 Decision Discussion Detection usingDirected Graphical ModelsA directed graphical model (DGM) M, (see Mur-phy (2002)), is a directed acyclic graph consistingof nodes which represent random variables, arcswhich represent dependencies among these vari-ables, and a probability distribution P over thevariables.
Let X = {X1, X2, ..., Xn} be a set ofrandom variables that are associated with nodes ina DGM and Pa(Xi) be parents of Xi.
The proba-bility distribution of the model M satisfies:P (X1, X2, ..., Xn) =n?i=1(P (Xi)|Pa(Xi))When a DGM is used as a classifier, the goal is tocorrectly infer the value of the class node Xc ?
Xgiven a vector of values for the observed node(s)1We used SRI?s Decipher for which (Stolcke et al, 2008)reports a word error rate of 26.9% on AMI meetings.2This example was extracted from the AMI dialogueES2015c and has been modified slightly for presentation pur-poses.237Xo ?
X\Xc.
This is done by using M to find thevalue of Xc which gives the highest conditionalprobability P (Xc|Xo).To detect each individual DDA class, we ex-amined the four simple DGMs in Figure 1 (seeAppendix).
The DDA node is binary wherevalue 1 indicates the presence of a DDA and 0its absence.
The evidence node (E) is a multi-dimensional vector of observed values of non-lexical features.
These include utterance features(UTT) such as length in words, duration in mil-liseconds, position within the meeting (as percent-age of elapsed time), manually annotated dialogueact (DA) features3 such as inform, assess, suggest,and prosodic features (PROS) such as energy andpitch.
These features are the same as the non-lexical features used by Ferna?ndez et al (2008b).The hidden component node (C) represents thedistribution of observable evidence E as a singleGaussian in the -sim models, and a mixture in the-mix models.
For the -mix models, the numberof Gaussian components is hand-tuned during thetraining phase.More complex models are constructed from thefour simple models in Figure 1 to allow for depen-dencies between different DDAs.
For example, themodel in Figure 2 (see Appendix) generalizes Fig-ure 1c with arcs connecting the DDA classes basedon analysis of the annotated AMI data.4.1 ExperimentsThe DGM classifiers in Figures 1 and 2 were im-plemented in Matlab using the BNT software4.Since the current BNT version does not sup-port multiple time series training for fully observ-able Dynamic Bayesian Networks (DBNs), we ex-tended the software for training models using thisstructure (e.g., Figure 1c and Figure 2).A DGM classifier is considered to have hy-pothesized a DDA if the marginal probability ofits DDA node is above a hand-tuned threshold.We tested the DGMs on manual and ASR tran-scripts in a 17-fold cross-validation, and evaluatedtheir performance on both a per-utterance basis,and also with the same lenient-match metric asFerna?ndez et al (2008b).
This allows a marginof 20 seconds preceding and following a hypoth-esized DDA, and so we refer to it as the 40 sec-ond metric.
In addition, we hypothesized decision3We use the AMI DA annotations.
These are only avail-able for manual transcripts.4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.htmldiscussion regions using the DGM output and thefollowing two simple rules:?
A decision discussion region begins with anIssue DDA.?
A decision discussion region contains at leastone Issue DDA and one Resolution DDA.To evaluate the accuracy of these hypothesized re-gions, like Ferna?ndez et al (2008b), we dividedthe dialogue into 30-second windows and evalu-ated on a per window basis.4.2 ResultsTables 1 and 2 show the F1-scores for eachDGM when using the best feature sets (I:UTT+DA+PROS, RP: UTT+DA, RR: UTT, A:UTT+DA).
The BN-mix model gives the highestF1-score for A on both evaluation metrics, and theDBN-mix model, the highest for I, RP, and RR,but there are no statistically significant differencesbetween any of the alternative DGMs.Classifier I RP RR ABN-mix .09 .09 .04 .19DBN-mix .16 .14 .05 .17BN-sim .12 .09 .04 .17DBN-sim .15 .11 .04 .16Table 1: F1-score (per utterance) of the DGMs us-ing the best combination of non-lexical features.Classifier I RP RR ABN-mix .19 .24 .07 .38DBN-mix .27 .24 .07 .32BN-sim .23 .22 .06 .36DBN-sim .25 .22 .06 .31Table 2: F1-score (40 seconds) of the DGMs usingthe best combination of non-lexical features.To determine whether modeling dependenciesbetween DDAs improves performance, we exper-imented with the DGMs that are generalized fromthe DBN-sim (Figure 2) and DBN-mix models.The F1-scores did not improve for I, RP, and RR,while for A, the DGM generalized from DBN-simgave a .03 improvement according to the 40 sec-onds metric, but this was not statistically signifi-cant.For each DDA, Table 3 compares the results ofthe best DGM and the hierarchical SVM classi-fication method of Ferna?ndez et al (2008b) (see238Section 2).
The DGM performs better for allDDAs on both evaluation metrics (p < 0.005).Note that while prosodic features proved uselessto SVM classifiers (Ferna?ndez et al (2008b)), withDGMs, they have some predictive power.Per utterance 40 secondsClassifier DDA Pr Re F1 Pr Re F1SVM I .03 .62 .05 .04 .89 .08DGM .11 .28 .16 .20 .44 .27SVM RP .03 .60 .07 .05 .90 .10DGM .09 .35 .14 .16 .57 .24SVM RR .01 .49 .02 .01 .80 .03DGM .02 .42 .05 .04 .58 .07SVM A .05 .70 .10 .07 .90 .13DGM .13 .31 .19 .29 .55 .38Table 3: Performance of the DGM classifier vs.the SVM classifier.
Both use the best combinationof non-lexical features.We also generated results without DA features.Here, the best F1-scores for I, RP, and A degradebetween .07 and .09 (p < 0.05), but they are stillhigher than the equivalent SVM results with DAfeatures.
Since (Ferna?ndez et al, 2008b) reportthat lexical features are the most useful for theSVM classifiers, it will be interesting to see howwell the DGMs perform when they use lexical aswell as non-lexical features.Detecting DDAs in ASR transcripts: Table 4compares the DGM F1-scores when using ASRone-best and manual transcripts.
The DGMs per-form well on ASR output.
For I and RP, the resultson ASR are actually higher, perhaps because theDDAs are less sparse.
In the absence of DA fea-tures, prosodic features improve the performancefor A in both sources.UTT UTT+PROSI RP RR A I RP RR AASR .20 .21 .06 .24 .16 .24 .07 .28Man .18 .17 .07 .27 .16 .15 .05 .30Table 4: F1-scores (40 seconds) computed usingASR one-best vs. manual transcriptions.Detecting decision discussion regions: Table 5shows that according to the 30-second windowmetric, rule-based classification with DGM outputcompares well with hierarchical SVM classifica-tion (Ferna?ndez et al, 2008b).
In fact, even whenthe latter uses lexical as well as non-lexical fea-tures, its F1-score is still about the same as theDGM-based classifier.
Our future work will in-volve dispensing with the rule-based approach anddesigning a DGM which can detect decision dis-cussion regions.Classifier Pr Re F1SVM .35 .88 .50DGM .39 .93 .55Table 5: Results in detecting decision discussionregions for the SVM super-classifier and rule-based DGM classifier, both using the best com-bination of non-lexical features.5 Decision SummarizationWe now turn to the task of extracting usefulphrases for summarization.
Since a summary of adecision discussion should minimally contain theissue under discussion, and its resolution, we leaveAgreement (A) utterances aside, and concentrateon extracting phrases from Issues (I) and Resolu-tions (R).Our basic approach is the same taken in(Ferna?ndez et al, 2008a): The WCN5 of each Iand R utterance is parsed by the Gemini parser(Dowding et al, 1993) to produce multiple shortfragments, and then an SVM regression modeluses certain features in order to select the parsethat is most likely to match a gold-standard extrac-tive summary.
Our work is new in two respects:summarizing from ASR output in addition to man-ual transcriptions, and using a semantic-similarityfeature in the SVM.
This new feature is generatedusing Ted Pedersen?s semantic-similarity package(Pedersen, 2002), and is motivated by the fact thatordinarily the Issue summary should be semanti-cally similar to the Resolution and vice versa.The next section describes the lexical resourcesused by Gemini, and Section 5.2, the metric forcalculating semantic similarity.5.1 Open-Domain Semantic ParserSince human-human spoken dialogue, especiallyafter being processed by an imperfect recognizer,is likely to be highly ungrammatical, we have de-veloped a semantic parser that only attempts tofind basic predicate-argument structures of the ma-jor phrase types (S, VP, NP, and PP) and has accessto a broad-coverage lexicon.
To build a broad-coverage lexicon, we used publicly available lex-ical resources for English, including COMLEX,5When using manual transcripts, we create ?dummyWCNs?
: WCNs with a single path.239VerbNet, WordNet, and NOMLEX.COMLEX provides detailed syntactic informa-tion for the 40k most common words of En-glish, and VerbNet, detailed semantic informationfor verbs, including verb class, verb frames, the-matic roles, mappings of syntactic position to the-matic roles, and selection restrictions on thematicrole fillers.
From WordNet we extracted another15K nouns and the semantic class information forall nouns.
These semantic classes were hand-aligned to the selectional classes used in Verb-Net, based on the upper ontology of EuroWord-Net.
NOMLEX provides syntactic information forevent nominalizations, and information for map-ping the noun arguments to the corresponding verbsyntactic positions.These resources were combined and convertedto the Prolog-based format used in the Geminiframework, which includes a fast bottom-up ro-bust parser in which syntactic and semantic in-formation is applied interleaved.
Gemini cancompute parse probabilities on the context-freeskeleton of the grammar.
In the experiments de-scribed here these parse probabilities are trainedon Switchboard tree-bank data.5.2 Semantic Similarity Metric: NormalizedPath LengthTed Pedersen?s semantic similarity package (Ped-ersen, 2002) can be used to apply a number ofdifferent metrics that use WordNet as a knowl-edge base.
The metric used here, Normalized PathLength (Leacock and Chodorow, 1998), definesthe semantic similarity sim between words w1 andw2 as:simc1,c2 = ?
loglen(c1, c2)2?D (1)where c1 and c2 are concepts corresponding to w1and w2, len(c1, c2) is the length of the shortestpath between them, and D is the maximum depthof the taxonomy.5.3 ExperimentsData: For the manual transcripts in our sub-corpus, the average length in words of I and R ut-terances is 12.2 and 11.9 respectively, and for theASR, 22.4 and 18.1.
To provide a gold-standard,phrases from I and R utterances in the man-ual transcriptions were annotated as summary-worthy.
The aim was to select those phraseswhich should appear in an extractive summary, orcould be the basis of a generated abstractive sum-mary.
As a general guideline, we tried to selectthe phrase(s) which describe the issue/resolutionas succinctly as possible.
This does not includephrases which express the speaker?s attitude to-wards the issue/resolution.
Dialogue 2 is an exam-ple where square brackets indicate which phraseswere selected as summary-worthy.
(2) A:(I) So we we?re looking at [sliders for bothvolume and channel change]B:(R)I was thinking kind of [just for thevolume]Regression models: We use SVMlight(Joachims, 1999) to learn separate SVM re-gression models for Issues and Resolutions.These rank the Gemini parses for each utteranceaccording to their likelihood of matching thegold-standard summary.
The top-ranked parseis then entered into the automatically-generateddecision summary.Features: We train the regression models withvarious types of feature (see Table 6), includingproperties of the WCN paths, parse, semantic andlexical features.
As lexical features are likely to bemore domain-specific, and they dramatically in-crease size of the feature space, we prefer to avoidthem if possible.To generate the semantic-similarity feature foran I/R parse, we compute its semantic similaritywith the full transcripts of each of the R/I utter-ances within the same decision discussion.
Thefeature?s value is then equal to the greatest of theresulting semantic-similarity scores.
Since TedPedersen?s package operates on the noun portionof WordNet, we must first extract all of the nounsin the parse/utterance transcription.
Next, we formall of the possible pairs containing one noun fromthe parse, and one from the utterance transcrip-tion.
Then we compute the semantic similarityfor each pair, and take their sum to be the levelof semantic similarity between the parse and theutterance transcription.
We experimented with av-eraging rather than summing these scores, but theresulting semantic-similarity feature was less pre-dictive.Evaluation: The models are evaluated in 10-fold cross-validations using the same metric as(Ferna?ndez et al, 2008a): Recall is the total pro-portion of the gold-standard extractive summary240WCN phrase length (WCN arcs)start/end point (absolute & percentage)Parse parse probabilityphrase type (S/VP/NP/PP)Semantic main verb VerbNet classhead noun WordNet synsetSem-sim Normalized Path LengthLexical main verb, head nounTable 6: Features for parse fragment rankingIssue ResolutionRe Pr F1 Re Pr F1Baseline 1.0 .50 .67 1.0 .60 .75Oracle .77 .96 .85 .74 .99 .84WCN,parse,sem .63 .69 .66 .61 .66 .64+ sem-sim .65 .71 .68 .64 .69 .67+ lexical .65 .67 .66 .65 .70 .67Table 7: Parse ranking results for I & R Utterancesusing manual transcriptions.covered by the selected parse; precision is the to-tal proportion of the chosen parse which overlapswith the gold-standard summary.
The baseline isthe entire transcription, and we also compare toan ?oracle?
that always chooses a parse with thehighest F1-score.
Note that we use the extractivesummaries from the manual transcriptions as thegold-standard for the evaluation of the results ob-tained with ASR.Results and analysis: Results with manual tran-scriptions are shown in Table 7, and those withASR, in Table 8.
In all cases, when starting witha feature set containing WCN, parse and seman-tic features, the F1-score is improved by addingthe semantic-similarity feature.
For Issues, the F1-score improves from .66 to .68 with manual tran-scripts, and from .30 to .32 with ASR.
The im-provements for Resolutions are highly significant:with manual transcripts, the F1 score increasesfrom .64 to .67 (p < 0.005), and with ASR, from.33 to .37 (p < 0.005).
Note that the further addi-tion of lexical features only produces a significantimprovement in the case of I summarization withASR.Compared to the full transcript baseline, weachieve higher F1-scores for Issues?.68 vs. .67with manual transcriptions, and .35 vs. .31 withASR?but slightly lower for Resolutions.
Thereremains a fairly large gap between our best scoresand their corresponding oracles (especially withASR), and so there may still be potential for sub-stantial improvement.Issue ResolutionRe Pr F1 Re Pr F1Baseline .77 .20 .31 .80 .27 .40Oracle .61 .87 .72 .59 .91 .72WCN,parse,sem .28 .33 .30 .31 .35 .33+ sem-sim .30 .34 .32 .35 .38 .36+ lexical .35 .35 .35 .34 .39 .37Table 8: Parse ranking results for I & R Utterancesusing ASR.6 Conclusions and Future WorkThis paper has presented work on the detec-tion and summarization of decision discussionsin multi-party dialogue.
In the detection experi-ments, we investigated the use of directed graph-ical models (DGMs), and found that when us-ing non-lexical features, the DGMs outperformthe hierarchical SVM classification method ofFerna?ndez et al (2008b).
The F1-score for thefour DDA classes increased between .04 and .19(p < .005), and for identifying decision discus-sion regions, by .05.
This is encouraging becauselexical features have disadvantages?for examplethey can be domain specific and greatly increasethe feature space.
In addition, modelling the de-pendencies between the DDA classes increasedperformance for Agreement utterances, and theDGMs were robust to ASR.In the summarization experiments, we sum-marized decision discussions by extracting keywords/phrases from their Issue (I) and Resolu-tion (R) utterances.
Each utterance?s Word Confu-sion Network was parsed with an open-domain se-mantic parser, thus producing multiple candidatephrases, and then an SVM regression model se-lected one of these phrases to enter into the sum-mary.
The experiments here investigated the use-fulness of a new SVM feature which measures thelevel of semantic similarity between candidate Iparses and R utterances, and vice-versa.
This fea-ture was generated with a semantic-similarity met-ric which uses WordNet as a knowledge source.It was found to improve performance with bothmanual transcripts and ASR, and for R summa-rization, the improvements were highly significant(p < .005).In future work, we plan to integrate lexical fea-tures into our DGMs by using a switching Dy-namic Bayesian Network similar to that reportedin (Ji and Bilmes, 2005).
We also plan to extendthe decision discussion annotation scheme so thatwe can try to automatically extract supporting ar-241guments for decisions.Acknowledgements This material is basedupon work supported by the Defense AdvancedResearch Projects Agency (DARPA) under Con-tract No.
FA8750-07-D-0185/0004, and by theDepartment of the Navy Office of Naval Research(ONR) under Grants No.
N00014-05-1-0187 andN00014-09-1-0106.
Any opinions, findings andconclusions or recommendations expressed inthis material are those of the authors and do notnecessarily reflect the views of DARPA or ONR.ReferencesSatanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.2005.
The necessity of a meeting recording andplayback system, and the benefit of topic-level anno-tations to meeting browsing.
In Proceedings of the10th International Conference on Human-ComputerInteraction.John Dowding, Jean Mark Gawron, Doug Appelt, JohnBear, Lynn Cherny, Robert Moore, and DouglasMoran.
1993.
GEMINI: a natural language systemfor spoken-language understanding.
In Proceedingsof the 31st Annual Meeting of the Association forComputational Linguistics (ACL).Raquel Ferna?ndez, Matthew Frampton, John Dowding,Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-ters.
2008a.
Identifying relevant phrases to summa-rize decisions in spoken meetings.
In Proceedingsof Interspeech.Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,Matthew Purver, and Stanley Peters.
2008b.
Mod-elling and detecting decisions in multi-party dia-logue.
In Proceedings of the 9th SIGdial Workshopon Discourse and Dialogue.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of Bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Daniel Gatica-Perez, Ian McCowan, Dong Zhang, andSamy Bengio.
2005.
Detecting group interest levelin meetings.
In Proceedings of ICASSP.Pey-Yun Hsueh and Johanna Moore.
2007.
Automaticdecision detection in meeting speech.
In Proceed-ings of MLMI 2007, Lecture Notes in Computer Sci-ence.
Springer-Verlag.Adam Janin, Jeremy Ang, Sonali Bhagat, RajdipDhillon, Jane Edwards, Javier Marc?
?as-Guarasa,Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,Andreas Stolcke, Chuck Wooters, and Britta Wrede.2004.
The ICSI meeting project: Resources and re-search.
In Proceedings of the 2004 ICASSP NISTMeeting Recognition Workshop.Gang Ji and Jeff Bilmes.
2005.
Dialog act taggingusing graphical models.
In Proceedings of ICASSP.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods ?Support Vector Learning.
MIT Press.Claudia Leacock and Martin Chodorow, 1998.
Word-Net: An Electronic Lexical Database, chapter Com-bining local context and WordNet similarity forword sense identification.
University of ChicagoPress.Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,S.
Bourban, M. Flynn, M. Guillemot, T. Hain,J.
Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,M.
Lincoln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
InProceedings of Measuring Behavior, the 5th Inter-national Conference on Methods and Techniques inBehavioral Research, Wageningen, Netherlands.Kevin Murphy.
2002.
Dynamic Bayesian Networks:Representation, Inference and Learning.
Ph.D. the-sis, University of California Berkeley.Ted Pedersen.
2002.
Semantic similarity package.http:/www.d.umn.edu/ tpederse/similarity.Matthew Purver, John Dowding, John Niekrasz,Patrick Ehlen, Sharareh Noorbaloochi, and StanleyPeters.
2007.
Detecting and summarizing actionitems in multi-party dialogue.
In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium.Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?rC?etin, Adam Janin, Matthew Magimai-Doss, ChuckWooters, and Jing Zheng.
2008.
The ICSI-SRIspring 2007 meeting and lecture recognition system.In Proceedings of CLEAR 2007 and RT2007.Daan Verbree, Rutger Rienks, and Dirk Heylen.
2006.First steps towards the automatic construction ofargument-diagrams from real discussions.
In Pro-ceedings of the 1st International Conference onComputational Models of Argument, volume 144,pages 183?194.
IOS press.Steve Whittaker, Rachel Laban, and Simon Tucker.2006.
Analysing meeting records: An ethnographicstudy and technological implications.
In S. Renalsand S. Bengio, editors, Machine Learning for Multi-modal Interaction: Second International Workshop,MLMI 2005, Revised Selected Papers, volume 3869of Lecture Notes in Computer Science, pages 101?113.
Springer.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Computational Linguistics, 28(4):447?485.242AppendixDDAEa) BN-simDDAEb) BN-mixCDDAtime t-1 time tEDDAEc) DBN-simDDAtime t-1 time tEDDAEd) DBN-mixCCFigure 1: Simple DGMs for individual decisiondetection.
During training, the shaded nodes arehidden, and the clear nodes are observable.Atime t-1 time tEAEI IRP RPRR RRFigure 2: A DGM that takes the dependencies be-tween decisions into account.243
