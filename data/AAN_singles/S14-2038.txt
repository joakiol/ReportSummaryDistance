Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235?240,Dublin, Ireland, August 23-24, 2014.DLIREC: Aspect Term Extraction and Term Polarity ClassificationSystemZhiqiang TohInstitute for Infocomm Research1 Fusionopolis WaySingapore 138632ztoh@i2r.a-star.edu.sgWenting Wangmagicwwt@gmail.comAbstractThis paper describes our system used inthe Aspect Based Sentiment Analysis Task4 at the SemEval-2014.
Our system con-sists of two components to address two ofthe subtasks respectively: a ConditionalRandom Field (CRF) based classifier forAspect Term Extraction (ATE) and a linearclassifier for Aspect Term Polarity Classi-fication (ATP).
For the ATE subtask, weimplement a variety of lexicon, syntac-tic and semantic features, as well as clus-ter features induced from unlabeled data.Our system achieves state-of-the-art per-formances in ATE, ranking 1st (among 28submissions) and 2rd (among 27 submis-sions) for the restaurant and laptop domainrespectively.1 IntroductionSentiment analysis on document and sentencelevel no longer fulfills user?s needs of getting moreaccurate and precise information.
By perform-ing sentiment analysis at the aspect level, we canhelp users gain more insights on the sentiments ofthe various aspects of the target entity.
Task 4 ofSemEval-2014 provides a good platform for (1)aspect term extraction and (2) aspect term polar-ity classification.For the first subtask, we follow the approach ofJakob and Gurevych (2010) by modeling term ex-traction as a sequential labeling task.
Specifically,we leverage on semantic and syntactic resourcesto extract a variety of features and use CRF as thelearning algorithm.
For the second subtask, weThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence de-tails: http://creativecommons.org/licenses/by/4.0/simply treat it as a multi-class classification prob-lem where a linear classifier is learned to predictthe polarity class.
Our system achieves good per-formances for the first subtask in both domains,ranking 1st for the restaurant domain, and 2nd forthe laptop domain.The remainder of this paper is structured as fol-lows: In Section 2, we describe our ATE systemin detail, including experiments and result analy-sis.
Section 3 describes the general approach ofour ATP system.
Finally, Section 4 summarizesour work.2 Aspect Term ExtractionThis subtask is to identify the aspects of given tar-get entities in the restaurant and laptop domains.Many aspect terms in the laptop domain con-tains digits or special characters such as ?17 inchscreen?
and ?screen/video resolution?
; while inthe restaurant domain, aspects in the sentences arespecific for a type of restaurants such as ?pizza?for Italian restaurants and ?sushi?
for Japaneserestaurants.We model ATE as a sequential labeling taskand extract features to be used for CRF training.Besides the common features used in traditionalNamed Entity Recognition (NER) systems, wealso utilize extensive external resources to buildvarious name lists and word clusters.2.1 PreprocessingFollowing the traditional BIO scheme used in se-quential labeling, we assign a label for each wordin the sentence, where ?B-TERM?
indicates thestart of an aspect term, ?I-TERM?
indicates thecontinuation of an aspect term, and ?O?
indicatesnot an aspect term.All sentences are tokenized and parsed using theStanford Parser1.
The parsing information is used1http://nlp.stanford.edu/software/lex-parser.shtml235to extract various syntactic features (e.g.
POS,head word, dependency relation) described in thenext section.2.2 General (or Closed) FeaturesIn this section, we describe the features commonlyused in traditional NER systems.
Such featurescan easily be extracted from the training set orwith the help of publicly available NLP tools (e.g.Stanford Parser, NLTK, etc).2.2.1 WordThe string of the current token and its lowercaseformat are used as features.
To capture more con-text information, we also extract the previous andnext word strings (in original format) as additionalword features.2.2.2 POSThe part-of-speech (POS) tag of the current tokenis used as a feature.
Since aspect terms are oftennouns, the POS tag provides useful informationabout the lexical category of the word, especiallyfor unseen words in the test sentences.2.2.3 Head WordThis feature represents the head word of the cur-rent token.
If the current token does not have ahead word, the value ?null?
is used.2.2.4 Head Word POSThis feature represents the POS of the head wordof the current token.
If the current token does nothave a head word, the value ?null?
is used.2.2.5 Dependency RelationFrom the dependency parse, we identify the de-pendency relations of the current token.
We ex-tract two different sets of strings: one set containsthe relation strings (e.g.
?amod?, ?nsubj?)
wherethe current token is the governor (i.e.
head) of therelation, the other set contains the relation stringswhere the current token is the dependent of the re-lation.
For each set, we only keep certain rela-tions: ?amod?, ?nsubj?
and ?dep?
for the first setand ?nsubj?, ?dobj?
and ?dep?
for the second set.Each set of strings is used as a feature value for thecurrent token, resulting in two separate features.2.2.6 Name ListName lists (or gazetteers) have proven to be usefulin the task of NER (Ratinov and Roth, 2009).
Wecreate a name list feature that uses the name listsfor membership testing.For each domain, we extract two high precisionname lists from the training set.
For the first list,we collect and keep those aspect terms whose fre-quency counts are greater than c1.
Since an aspectterm can be multi-word, we also extract a secondlist to consider the counts of individual words.
Allwords whose frequency counts greater than c2arecollected.
For each word, the probability of it be-ing annotated as an aspect word in the training setis calculated.
Only those words whose probabil-ity value is greater than t is kept in the second list.The specified values of c1, c2and t for each do-main are determined using 5-fold cross validation.2.3 Open/External Sources GeneratedFeaturesThis section describes additional features we usethat require external resources and/or complexprocessings.2.3.1 WordNet TaxonomyThis feature represents the set of syntactic cate-gories (e.g ?noun.food?)
of the current token asorganized in WordNet lexicographer files (Miller,1995).
We only consider noun synsets of the tokenwhen determining the syntactic categories.2.3.2 Word ClusterTurian et al.
(2010) used unsupervised word rep-resentations as extra word features to improve theaccuracy of both NER and chunking.
We followedtheir approach by inducing Brown clusters and K-means clusters from in-domain unlabeled data.We used the review text from two sourcesof unlabeled dataset: the Multi-Domain Senti-ment Dataset that contains Amazon product re-views (Blitzer et al., 2007)2, and the Yelp PhoenixAcademic Dataset that contains user reviews3.We induce 1000 Brown clusters for eachdataset4.
For each word in the training/testing set,its corresponding binary (prefix) string is used asthe feature value.We experiment with different prefix lengths anduse the best settings using 5-fold cross validation.2We used the unprocessed.tar.gz archive foundat http://www.cs.jhu.edu/?mdredze/datasets/sentiment/3http://www.yelp.com/dataset_challenge/4Brown clustering are induced using the implementa-tion by Percy Liang found at https://github.com/percyliang/brown-cluster/.236For the laptop domain, we create a Brown clusterfeature from Amazon Brown clusters, using prefixlength of 5.
For the restaurant domain, we cre-ated three Brown cluster features: two from YelpBrown clusters, using prefix lengths of 4 and 8,and the last one from Amazon Brown clusters, us-ing prefix length of 10.K-means clusters are induced using theword2vec tool (Mikolov et al., 2013)5.
Similarto Brown cluster feature, the cluster id of eachword is used as the feature value.When running the word2vec tool, we spe-cially tune the values for word vector size (size),cluster size (classes) and sub-sampling threshold(sample) for optimum 5-fold cross validation per-formances.
We create one K-means cluster fea-ture for the laptop domain from Amazon K-meansclusters (size = 100, classes = 400, sample =0.0001), and two K-means cluster features for therestaurant domain, one from Yelp K-means clus-ters (size = 200, classes = 300, sample = 0.001),and the other from Amazon K-means clusters(size = 1000, classes = 300, sample = 0.0001).2.3.3 Name List Generated using DoublePropagationWe implement the Double Propagation (DP) algo-rithm described in Qiu et al.
(2011) to identify pos-sible aspect terms in a semi-supervised way.
Theterms identified are stored in a list which is usedas another name list feature.Our implementation follow the Logic Program-ming approach described in Liu et al.
(2013)6.
Wewrite our rules in Prolog and use SWI-Prolog7asthe solver.We use the seed opinion lexicon provided by Huand Liu (2004) for both domain8.
In addition, forthe restaurant domain, we augment the opinionlexicon with addition seed opinion words by us-ing the 75 restaurant seed words listed in Sauperand Barzilay (2013).
To increase the coverage, weexpand this list of 75 words by including relatedwords (e.g.
antonym, similar to) in WordNet.
Thefinal expanded list contains 551 words.Besides the seed opinion words, we also use thelast word of each aspect term in the training set asa seed aspect word.The propagation rules we use are modifications5https://code.google.com/p/word2vec/6We did not implement incorrect aspect pruning.7http://www.swi-prolog.org/8We ignore the polarity of the opinion word.of the rules presented in Liu et al.
(2013).
A totalof 11 rules and 13 rules are used for the laptopand restaurant domain respectively.
An exampleof a Prolog rule concerning the extraction of aspectwords is stated below:aspect(A) :-relation(nsubj, O, A),relation(cop, O, _),pos(A, P),is_noun(P),opinion(O).For example, given the sentence ?The rice isamazing.
?, and ?amazing?
is a known opinionword, we can extract ?rice?
as a possible aspectword using the rule.All our rules can only identify individual wordsas possible aspect terms.
To consider a phrase asa possible aspect term, we extend the left bound-ary of the identified span to include any consectivenoun words right before the identified word.2.4 Algorithms and EvaluationWe use the CRFsuite tool (Okazaki, 2007) totrain our CRF model.
We use the default set-tings, except for the negative state features (-pfeature.possible states=1).Feature F1Word 0.6641+ Name List 0.7106+ POS 0.7237+ Head Word 0.7280+ DP Name List 0.7298+ Word Cluster 0.7430+ Head Word POS 0.7437+ Dependency Relation 0.7521Table 1: 5-fold cross-validation performances onthe laptop domain.
Each row uses all featuresadded in the previous rows.2.5 Preliminary Results on Training SetTable 1 and Table 2 show the 5-fold cross-validation performances after adding each featuregroup for the laptop and restaurant domain respec-tively.
Most features are included in the optimumfeature set for both domains, except for Word-Net Taxonomy feature (only used in the restaurantdomain) and Dependency Relation feature (onlyused in the laptop domain).237laptop restaurantSystem Precision Recall F1 Precision Recall F1DLIREC constrained 0.7931 0.6330 0.7041 (C) 0.8404 0.7337 0.7834 (C)DLIREC unconstrained 0.8190 0.6713 0.7378 (U) 0.8535 0.8272 0.8401 (U)Baseline 0.4432 0.2982 0.3565 (C) 0.5255 0.4277 0.4716 (C)Ranked 1st 0.8480 0.6651 0.7455 (C) 0.8535 0.8272 0.8401 (U)Ranked 2nd 0.8190 0.6713 0.7378 (U) 0.8625 0.8183 0.8398 (C)Ranked 3rd 0.7931 0.6330 0.7041 (C) 0.8441 0.7637 0.8019 (C)Table 3: Results of the Aspect Term Extraction subtask.
We also indicate whether the system is con-strained (C) or unconstrained (U).Feature F1Word 0.7541+ Name List 0.7808+ POS 0.7951+ Head Word 0.7962+ DP Name List 0.8036+ Word Cluster 0.8224+ WordNet Taxonomy 0.8252+ Head Word POS 0.8274Table 2: 5-fold cross-validation performances onthe restaurant domain.
Each row uses all featuresadded in the previous rows.For each domain, we make submissions in bothconstrained and unconstrained settings.
The con-strained submission only uses the Word and NameList features, while all features listed in Table 1and Table 2 are used in the unconstrained submis-sion for the respective domain.2.6 Results on Test SetUsing the optimum feature set described in Sec-tion 2.5, we train separate models for each domainand evaluate them against the SemEval-2014 Task4 test set9.
Table 3 presents the official results ofour submissions.
We also include the official base-line results and the results of the top three par-ticipating systems for comparison (Pontiki et al.,2014).As shown from the table, our system performedwell for both domains.
For the laptop domain, oursystem is ranked 2nd and 3rd (among 27 submis-sions) for the unconstrained and constrained set-ting respectively.
For the restaurant domain, oursystem is ranked 1st and 9th (among 28 submis-sions) for the unconstrained and constrained set-9We train each model using only single-domain data.ting respectively.Our unconstrained submissions for both do-mains outperformed our constrained submissions,due to a significantly better recall.
This indicatesthe use of additional external resources (e.g.
un-labeled data) can improve the extraction perfor-mance.2.7 Further Analysis of Feature EngineeringTable 4 shows the F1 loss on the test set resultingfrom training with each group of feature removed.We also include the F1 loss when all features areused.Feature laptop restaurantWord 0.0260 0.0241Name List 0.0090 0.0054POS -0.0059 -0.0052Head Word 0.0072 0.0038DP Name List 0.0049 0.0064Word Cluster 0.0061 0.0185WordNet Taxonomy - -0.0018Head Word POS -0.0040 -0.0011Dependency Relation -0.0105 -All features -0.0132 0.0014Table 4: Feature ablation study on the test set.
Thequantity is the F1 loss resulted from the removal ofa single feature group.
The last row indicates theF1 loss when all features are used.Our ablation study showed that a few of our fea-tures are helpful in varying degrees on both do-mains: Word, Name List, Head Word, DP NameList and Word Cluster.
However, the use of therest of the features individually has a negative im-pact.
In particular, we are surprised that the POSand Dependency Relation features are detrimen-tal to the performances, even though our 5-fold238cross validation experiments suggested otherwise.Another observation we make is that the Word-Net Taxonomy feature is actually useful for thelaptop test set: including this feature would haveimproved our laptop unconstrained performancefrom 0.7378 F1 to 0.7510 F1 (+0.0132), which isbetter than the top system performance.
We alsonote that our restaurant performance on the testset can potentially be improved from 0.8401 F1to 0.8454 F1 (+0.0052) if we originally omit thePOS feature.Overall, we see that all the features we pro-posed are potentially beneficial to the task.
How-ever, more thorough feature selection experimentsshould be conducted to prevent overfitting and toidentify the settings (e.g.
domain) in which eachfeature may be useful.3 Aspect Term PolarityIn this section, we describe a baseline classifier forATP, where we treat the problem as a multi-classclassification problem.To correctly identify the polarity of an aspectterm, it is crucial to know which words within thesentence indicate its sentiment.
A general lexiconor WordNet is not sufficient.
Thus, we attempt tobuild the aspect lexicon based on other informa-tion such as POS (Sauper and Barzilay, 2013).
Forexample, sentiment words are more likely to beadjectives.3.1 Features3.1.1 Aspect WordThis is to model the idea that certain aspects tendto have a particular polarity most of the time.
Wecompute the most frequent polarity of each aspectin the training set.
For each aspect instance, thefeature corresponding to its most frequent polarityis set to 1.3.1.2 General Sentiment Word LexiconOne sentence may express opinions on multi-ple aspect terms.
According to our observations,words surrounding the aspect term tend to be asso-ciated with it.
Based on the best settings obtainedfrom 5-fold cross validation, we set a window sizeof 12 words and consider words with the followingPOS: JJ*, RB*, VB*, DT and NN*10.Some sentiment words are consistent across as-pects.
For example, ?great?
for positive and ?ter-10NN* is only used in the restaurant domain.rible?
for negative.
On the other hand, some senti-ment words are quite distinct between aspects.
Incertain cases, they may have opposite sentimentmeanings for different aspects (Kim et al., 2013).For example, ?fast?
is positive when describingboot up speed but negative when describing bat-tery life.
Therefore, a general sentiment word lex-icon is created from the training set.If a general sentiment word occurs in the sur-rounding context of the aspect instance, the fea-ture value for the matched sentiment word is 1.Since the training set does not contain every pos-sible sentiment expression, we use synonyms andantonyms in RiTa WordNet11to expand the gen-eral sentiment word lexicon.
The expanded lex-icon contains 2419 words for the laptop domainand 4262 words for the restaurant domain.3.1.3 Aspect-Sentiment Word PairBesides general sentiment word lexicon, we alsobuild aspect-sentiment word pair lexicon from thetraining set.
This lexicon contains 9073 word pairsfor the laptop domain and 22171 word pairs for therestaurant domain.
If an aspect-sentiment wordoccurs in the surrounding context of the aspect in-stance, the feature value for the matched aspect-sentiment word pair is 1.3.2 Experiments and ResultsWe use LIBLINEAR12to train our logistic regres-sion classifier using default settings.laptop restaurant5-fold cross validation 0.6322 0.6704DLIREC unconstrained 0.3654 0.4233Table 5: Accuracy of the Aspect Term Polaritysubtask.Table 5 shows the classification accuracy of ourbaseline system on the training and test set foreach domain.
The performance drops a lot in thetest set as we use very simple approaches to gener-ate the lexicons.
This may cause overfitting on thetraining set.
We also observe that in the test set ofboth domains, more than half of the instances arepositive.
In the future, we can explore on usingmore sophisticated ways to build more effectivefeatures and to better model data skewness.11http://www.rednoise.org/rita/wordnet/12http://www.csie.ntu.edu.tw/?cjlin/liblinear/2394 ConclusionFor ATE subtask, we leverage on the vast amountof external resources to create additional effectivefeatures, which contribute significantly to the im-provement of our system.
For the unconstrainedsetting, our system is ranked 1st (among 28 sub-missions) and 2rd (among 27 submissions) for therestaurant and laptop domain respectively.
ForATP subtask, we implement a simple baseline sys-tem.Our current work focus on implementing a sep-arate term extraction system for each domain.
Infuture, we hope to investigate on domain adapta-tion methods across different domains.
In addi-tion, we will also address the feature sparsenessproblem in our ATP baseline system.AcknowledgementsThis research work is supported by a researchproject under Baidu-I2R Research Centre.ReferencesJohn Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 440?447, Prague, Czech Republic, June.Minqing Hu and Bing Liu.
2004.
Mining and Sum-marizing Customer Reviews.
In Proceedings of theTenth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, KDD ?04,pages 168?177, New York, NY, USA.Niklas Jakob and Iryna Gurevych.
2010.
ExtractingOpinion Targets in a Single and Cross-Domain Set-ting with Conditional Random Fields.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 1035?1045,Cambridge, MA, October.Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, andShixia Liu.
2013.
A Hierarchical Aspect-SentimentModel for Online Reviews.
In AAAI Conference onArtificial Intelligence.Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.2013.
A Logic Programming Approach to As-pect Extraction in Opinion Mining.
In Web Intelli-gence (WI) and Intelligent Agent Technologies (IAT),2013 IEEE/WIC/ACM International Joint Confer-ences on, volume 1, pages 276?283, Nov.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic Regularities in Continuous SpaceWord Representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 746?751, Atlanta,Georgia, June.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
Commun.
ACM, 38(11):39?41, November.Naoaki Okazaki.
2007.
CRFsuite: a fast implementa-tion of Conditional Random Fields (CRFs).Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect Based Sentiment Analysis.
In Proceedingsof the 8th International Workshop on Semantic Eval-uation (SemEval 2014), Dublin, Ireland.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.2011.
Opinion Word Expansion and Target Extrac-tion through Double Propagation.
ComputationalLinguistics, 37(1):9?27.Lev Ratinov and Dan Roth.
2009.
Design Chal-lenges and Misconceptions in Named Entity Recog-nition.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 147?155, Boulder, Colorado,June.Christina Sauper and Regina Barzilay.
2013.
Auto-matic Aggregation by Joint Modeling of Aspects andValues.
J. Artif.
Int.
Res., 46(1):89?127, January.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-Supervised Learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, Up-psala, Sweden, July.240
