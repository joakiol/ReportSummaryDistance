Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLDA Based Similarity Modeling for Question AnsweringAsli CelikyilmazComputer Science DepartmentUniversity of California, Berkeleyasli@eecs.berkeley.eduDilek Hakkani-TurInternational ComputerScience InstituteBerkeley, CAdilek@icsi.berkeley.eduGokhan TurSpeech Technology andResearch LaboratorySRI InternationalMenlo Park, CA, USAgokhan@speech.sri.comAbstractWe present an exploration of generative mod-eling for the question answering (QA) task torank candidate passages.
We investigate La-tent Dirichlet Allocation (LDA) models to ob-tain ranking scores based on a novel similar-ity measure between a natural language ques-tion posed by the user and a candidate passage.We construct two models each one introducingdeeper evaluations on latent characteristics ofpassages together with given question.
Withthe new representation of topical structures onQA datasets, using a limited amount of worldknowledge, we show improvements on perfor-mance of a QA ranking system.1 IntroductionQuestion Answering (QA) is a task of automaticretrieval of an answer given a question.
Typicallythe question is linguistically processed and searchphrases are extracted, which are then used to retrievethe candidate documents, passages or sentences.A typical QA system has a pipeline structure start-ing from extraction of candidate sentences to rank-ing true answers.
Some approaches to QA usekeyword-based techniques to locate candidate pas-sages/sentences in the retrieved documents and thenfilter based on the presence of the desired answertype in candidate text.
Ranking is then done usingsyntactic features to characterize similarity to query.In cases where simple question formulation is notsatisfactory, many advanced QA systems implementmore sophisticated syntactic, semantic and contex-tual processing such as named-entity recognition(Molla et al, 2006), coreference resolution (Vicedoand Ferrandez, 2000), logical inferences (abductionor entailment) (Harabagiu and Hickl, 2006) trans-lation (Ma and McKeowon, 2009), etc., to improveanswer ranking.
For instance, how questions, or spa-tially constrained questions, etc., require such typesof deeper understanding of the question and the re-trieved documents/passages.Many studies on QA have focused on discrimina-tive models to predict a function of matching fea-tures between each question and candidate passage(set of sentences), namely q/a pairs, e.g., (Ng et al,2001; Echihabi and Marcu, 2003; Harabagiu andHickl, 2006; Shen and Klakow, 2006; Celikyilmazet al, 2009).
Despite their success, they have someroom for improvement which are not usually raised,e.g., they require hand engineered features; or cas-cade features learnt separately from other modulesin a QA pipeline, thus propagating errors.
The struc-tures to be learned can become more complex thanthe amount of training data, e.g., alignment, entail-ment, translation, etc.
In such cases, other sourceof information, e.g., unlabeled examples, or humanprior knowledge, should be used to improve perfor-mance.
Generative modeling is a way of encodingthis additional information, providing a natural wayto use unlabeled data.In this work, we present new similarity measuresto discover deeper relationship between q/a pairsbased on a probabilistic model.
We investigate twomethods using Latent Dirichlet Allocation (LDA)(Blei, 2003) in ?
3, and hierarchical LDA (hLDA)(Blei, 2009) in ?
4 to discover hidden concepts.
Wepresent ways of utilizing this information within adiscriminative classifier in ?
5.
With empirical ex-periments in ?
6, we analyze the effects of gener-ative model outcome on a QA system.
With thenew representation of conceptual structures on QA1datasets, using a limited amount of world knowl-edge, we show performance improvements.2 Background and MotivationPrevious research have focused on improving mod-ules of the QA pipeline such as question processing(Huang et al, 2009), information retrieval (Clarkeet al, 2006), information extraction (Saggion andGaizauskas, 2006).
Recent work on textual en-tailment has shown improvements on QA results(Harabagiu and Hickl, 2006), (Celikyilmaz et al,2009), when used for filtering and ranking answers.They discover similarities between q/a pairs, wherethe answer to a question should be entailed by thetext that supports the correctness of its answer.In this paper, we present a ranking schema fo-cusing on a new similarity modeling approach viagenerative and discriminative methods to utilize bestfeatures of both approaches.
Combinations of dis-criminative and generative methodologies have beenexplored by several authors, e.g.
(Bouchard andTriggs, 2004; McCallum et al, 2006; Bishop andLasserre, 2007; Schmah et al, 2009), in many fieldssuch as natural language processing, speech recog-nition, etc.
In particular, the recent ?deep learning?approaches (Weston et al, 2008) rely heavily on ahybrid generative-discriminative approach: an un-supervised generative learning phase followed by adiscriminative fine-tuning.In an analogical way to the deep learning meth-ods, we discover relations between the q/a pairsbased on the similarities on their latent topics dis-covered via Bayesian probabilistic approach.
We in-vestigate different ways of discovering topic basedsimilarities following the fact that it is more likelythat the candidate passage entails given question andcontains true answer if they share similar topics.Later we combine this information in different waysinto a discriminative classifier-based QA model.The underlying mechanism of our similarity mod-eling approach is Latent Dirichlet Allocation (LDA)(Blei et al, 2003b).
We argue that similarities canbe characterized better if we define a semantic simi-larity measure based on hidden concepts (topics) ontop of lexico-syntactic features.
We later extend oursimilarity model using a hierarchical LDA (hLDA)(Blei et al, 2003a) to discover latent topics that areorganized into hierarchies.
A hierarchical structureis particularly appealing to QA task than a flat LDA,in that one can discover abstract and specific topics.For example, discovering that baseball and footballare both contained in a more abstract class sportscan help to relate to a general topic of a question.3 Similarity Modeling with LDAWe assume that for a question posed by a user, thedocument sets D are retrieved by a search enginebased on the query expanded from the question.
Ouraim is to build a measure to characterize similar-ities between a given question and each candidatepassage/sentence s ?
D in the retrieved documentsbased on similarities of their hidden topics.
Thus,we built bayesian probabilistic models on passagelevel rather than document level to explicitly extracttheir hidden topics.
Moreover, the fact that there islimited amount of retrieved documents D per ques-tion (?100 documents) makes it appealing to buildprobabilistic models on passages in place of docu-ments and define semantically coherent groups inpassages as latent concepts.
Given window size nsentences, we define a passage as s = (|D| ?n) + 1based on a n-sliding-window, where |D| is the to-tal number of sentences in retrieved documents D.There are 25+ sentences in documents, hence we ex-tracted around 2500 passages for each question.3.1 LDA Model for Q/A SystemWe briefly describe LDA (Blei et al, 2003b) modelas used in our QA system.
A passage in retrieveddocuments (document collection) is represented as amixture of fixed topics, with topic z getting weight?
(s)z in passage s and each topic is a distributionover a finite vocabulary of words, with word w hav-ing a probability ?
(z)w in topic z.
Placing symmet-ric Dirichlet priors on ?
(s) and ?
(z), with ?
(s) ?Dirichlet(?)
and ?
(z) ?
Dirichlet(?
), where ?and ?
are hyper-parameters to control the sparsityof distributions, the generative model is given by:wi|zi, ?
(zi)wi ?
Discrete(?
(zi)), i = 1, ...,W?
(z) ?
Dirichlet(?
), z = 1, ...,Kzi|?
(si) ?
Discrete(?
(si)), i = 1, ...,W?
(s) ?
Dirichlet(?
), s = 1, ..., S(1)2where S is the number of passages discovered fromthe document collection, K is the total number oftopics, W is the total number of words in the docu-ment collection, and si and zi are the passage and thetopic of the ith word wi, respectively.
Each word inthe vocabulary wi ?
V = {w1, ...wW } is assignedto each latent topic variable zi=1,...,W of words.After seeing the data, our goal is to calculate theexpected posterior probabilities ??
(zi)wi of a word wiin a candidate passage given a topic zi = k and ex-pected posterior probability ??
(s) of topic mixings ofa given passage s, using the count matrices:??
(zi)wi =nWKwik+?PWj=1 nWKwjk+W???
(s) =nSKsk +?PKj=1 nSKsj +K?
(2)where nWKwik is the count of wi in topic k, and nSKskis the count of topic k in passage s. The LDA modelmakes no attempt to account for the relation of topicmixtures, i.e., topics are distributed flat, and eachpassage is a distribution over all topics.3.2 Degree of Similarity Between Q/A viaTopics from LDA:We build a LDA model on the set of retrieved pas-sages s along with a given question q and calculatethe degree of similarity DESLDA(q,s) between eachq/a pair based on two measures (Algorithm 1):(1) simLDA1 : To capture the lexical similaritieson hidden topics, we represent each s and q astwo probability distributions at each topic z =k.
Thus, we sample sparse unigram distributionsfrom each ??
(z) using the words in q and s. Eachsparse word given topic distribution is denoted asp(z)q = p(wq|z, ??
(z)) with the set of words wq =(w1, ..., w|q|) in q and ps = p(ws|z, ??
(z)) with theset of words ws = (w1, ..., w|s|) in s, and z = 1...Krepresent each topic.The sparse probability distributions per topic arerepresented with only the words in q and s, and theprobabilities of the rest of the words in V are setto zero.
The W dimensional word probabilities isthe expected posteriors obtained from LDA model(Eq.
(2)), p(z)s = (??
(z)w1 , ..., ??
(z)w|s| , 0, 0, ..) ?
(0, 1)W ,p(z)q = (??
(z)w1 , ..., ??
(z)w|q| , 0, 0, ..) ?
(0, 1)W .
Given atopic z, the similarity between p(z)q and p(z)s is mea-sured via transformed information radius (IR).
WePosterior Topic-Word Distributionsq :s :z1.....w5.w4w1w6w2w7w3z2.....w5.w4w1w6w2w7w3zK.....w5.w4w1w6w2w7w3...(b) Magnified view of word given topic and topic given passagedistributions showing  s={w1,w2,w3,w4,w5} and q={w1,w2,w6,w7}(a) Snapshot of Flat Topic Structure of passages sfor a question q on ?global warming?.s: ?Global1warming2may rise3incidence4of malaria5.
?q: ?How does global1warming2effect6humans7??
?Posterior Passage- Topic Distributionsz1z2 .........zKz?
(q)z1z2 .........zKz?
(s)Vw1w2w3..w5w6w7....pq(z1)Vps(z1)w1w2w3w4w5w6w7....Vw1w2w3w4w5w6w7....pq(z2)Vw1w2w3w4w5w6w7....pq(zK)......Vps(zK)w1w2w3w4w5w6w7.......z1warmingpredicthealthdiseaseforecasttemperaturemalariasneezezKz2?coolingTopic ProportionsTopic-Word DistributionsVw1w2w3w4w5w6w7....p(z2)sFigure 1: (a) The topic distributions of a passage s and aquestion q obtained from LDA.
Each topic zk is a distri-bution over words (Most probable terms are illustrated).
(b) magnified view of (a) demonstrating sparse distribu-tions over the vocabulary V, where only words in passages and question q get values.
The passage-topic distribu-tions are topic mixtures, ?
(s) and ?
(q), for s and q.first measure the divergence at each topic using IRbased on Kullback-Liebler (KL) divergence:IR(p(z)q ,p(z)s )=KL(p(z)q ||p(z)q +p(z)s2 )+KL(p(z)s ||p(z)q +p(z)s2 )(3)where, KL(p||q) =?i pi logpiqi.
The divergence istransformed into similarity measure (Manning andSchutze, 1999):W (p(z)q , p(z)s ) = 10?
?IR(p(z)q ,p(z)s )1 (4)To measure the similarity between probability distri-butions we opted for IR instead of commonly usedKL because with IR there is no problem with infinitevalues since pq+ps2 6= 0 if either pq 6= 0 or ps 6= 0,and it is also symmetric, IR(p,q)=IR(q,p).
The simi-larity of q/a pairs on topic-word basis is the average1In experiments ?
= 1 is used.3of transformed divergence over the entire K topics:simLDA1 (q, s) =1K?Kk=1W (p(z=k)q , p(z=k)s ) (5)(2) simLDA2 : We introduce another measure based onpassage-topic mixing proportions in q and s to cap-ture similarities between their topics using the trans-formed IR in Eq.
(4) as follows:simLDA2 (q, s) = 10?IR(??
(q), ??
(s)) (6)The ??
(q) and ??
(s) are K-dimensional discrete topicweights in question q and a passage s from Eq.
(2).In summary, simLDA1 is a measure of lexical simi-larity on topic-word level and simLDA2 is a measureof topical similarity on passage level.
Together theyform the degree of similarity DESLDA(s, q) and arecombined as follows:DESLDA(s,q)=simLDA1 (q,s)*simLDA2 (q, s) (7)Fig.1 shows sparse distributions obtained for sam-ple q and s. Since the topics are not distributed hi-erarchially, each topic distribution is over the entirevocabulary of words in retrieved collection D. Fig.1only shows the most probable words in a given topic.Moreover, each s and q are represented as a discreteprobability distribution over all K topics.Algorithm 1 Flat Topic-Based Similarity Model1: Given a query q and candidate passages s ?
D2: Build an LDA model for the retrieved passages.3: for each passages s ?
D do4: - Calculate sim1(q, s) using Eq.
(5)5: - Calculate sim2(q, s) using Eq.
(6)6: - Calculate degree of similarity between q and s:7: DESLDA(q,s)=sim1(q, s) ?
sim2(q, s)8: end for4 Similarity Modeling with hLDAGiven a question, we discover hidden topic distribu-tions using hLDA (Blei et al, 2003a).
hLDA orga-nizes topics into a tree of a fixed depth L (Fig.2.
(a)),as opposed to flat LDA.
Each candidate passage s isassigned to a path cs in the topic tree and each wordwi in s is assigned to a hidden topic zs at a levell of cs.
Each node is associated with a topic dis-tribution over words.
The Gibbs sampler (Griffithsand Steyvers, 2004) alternates between choosing anew path for each passage through the tree and as-signing each word in each passage to a topic alongthat path.
The structure of tree is learnt along withthe topics using a nested Chinese restaurant process(nCRP) (Blei et al, 2003a), which is used as a prior.The nCRP is a stochastic process, which assignsprobability distributions to infinitely branching anddeep trees.
nCRP specifies a distribution of words inpassages into paths in an L-level tree.
Assignmentsof passages to paths are sampled sequentially: Thefirst passage takes the initial L-level path, startingwith a single branch tree.
Next,mth subsequent pas-sage is assigned to a path drawn from distribution:p(pathold, c|m,mc) =mc?+m?1p(pathnew, c|m,mc) =?
?+m?1(8)pathold and pathnew represent an existing and novel(branch) path consecutively, mc is the number ofprevious passages assigned to path c, m is the to-tal number of passages seen so far, and ?
is a hyper-parameter, which controls the probability of creatingnew paths.
Based on this probability each node canbranch out a different number of child nodes propor-tional to ?.
The generative process for hLDA is:(1) For each topic k ?
T , sample a distribution ?k vDirichlet(?).
(2) For each passage s in retrieved documents,(a) Draw a path cs v nCRP(?
),(b) Sample L-vector ?s mixing weights fromDirichlet distribution ?s ?
Dir(?).
(c) For each word n, choose :(i) a level zs,n|?s, (ii) a word ws,n| {zs,n, cs, ?
}Given passage s, ?s is a vector of topic propor-tions from L dimensional Dirichlet parameterizedby ?
(distribution over levels in the tree.)
Thenth word of s is sampled by first choosing a levelzs,n = l from the discrete distribution ?s with prob-ability ?s,l.
Dirichlet parameter ?
and ?
control thesize of tree effecting the number of topics.
Largevalues of ?
favor more topics (Blei et al, 2003a).Model Learning: Gibbs sampling is a commonmethod to fit the hLDA models.
The aim is to ob-tain the following samples from the posterior of: (i)the latent tree T , (ii) the level assignment z for allwords, (iii) the path assignments c for all passagesconditioned on the observed words w.Given the assignment of words w to levels z andassignments of passages to paths c, the expected4(a) Snapshot of Hierarchical Topic Structure ofpassages s for a question q on ?global warming?.z1z2z3zz1z2z3zPosterior TopicDistributionsvz1z3..........w5z2........w2.z1w5.......w7w1Posterior Topic-Word Distributionscandidate squestion q(b) Magnified view of sample path c [z1,z2,z3] showings={w1,w2,w3,w4,w5} and q={w1,w2,w6,w7}...z1zK-1zKz4z2z3humanwarmingincidenceresearchglobalpredicthealthchangediseaseforecasttemperatureslowmalariasneezestarvingmiddle-eastsiberias: ?Global1warming2may rise3incidence4of malaria5.
?q: ?How does global1warming2effect6humans7?
?vz1vz2vz2vz3vz3w1w5w6....w2w7....w5....w5....w6w1w5w6.....w2w7.....pszp(w|z1, c )s,1sp(w|z2, c  )q,2qp(w|z3, c  )q,3q.pqzp(w|z2, c )s,2sp(w|z3, c )s,3sp(w|z1, c )q,1qlevel:3level:1level:2Figure 2: (a) A sample 3-level tree using hLDA.
Each passage is associated with a path c through the hierarchy, whereeach node zs = l is associated with a distribution over terms (Most probable terms are illustrated).
(b) magnified viewof a path (darker nodes) in (a).
Distribution of words in given passage s and a question (q) using sub-vocabulary ofwords at each level topic vl.
Discrete distributions on the left are topic mixtures for each passage, pzq and pzs .posterior probability of a particular word w at agiven topic z=l of a path c=c is proportional to thenumber of times w was generated by that topic:p(w|z, c,w, ?)
?
n(z=l,c=c,w=w) + ?
(9)Similarly, posterior probability of a particular topicz in a given passage s is proportional to number oftimes z was generated by that passage:p(z|s, z, c, ?)
?
n(c=cc,z=l) + ?
(10)n(.)
is the count of elements of an array satisfyingthe condition.
Posterior probabilities are normalizedwith total counts and their hyperparameters.4.1 Tree-Based Similarity ModelThe hLDA constructs a hierarchical tree structureof candidate passages and given question, each ofwhich are represented by a path in the tree, and eachpath can be shared by many passages/question.
Theassumption is that passages sharing the same pathshould be more similar to each other because theyshare the same topics (Fig.2).
Moreover, if a pathincludes a question, then other passages on that pathare more likely to entail the question than passageson the other paths.
Thus, the similarity of a can-didate passage s to a question q sharing the samepath is a measure of semantic similarity (Algorithm2).
Given a question, we build an hLDA model onretrieved passages.
Let cq be the path for a givenq.
We identify the candidate passages that share thesame path with q, M = {s ?
D|cs = cq}.
Givenpath cq and M , we calculate the degree of similarityDEShLDA(s, q) between q and s by calculating twosimilarity measures:(1) simhLDA1 : We define two sparse (discrete) uni-gram distributions for candidate s and question q ateach node l to define lexical similarities on topiclevel.
The distributions are over a vocabulary ofwords generated by the topic at that node, vl ?V .
Note that, in hLDA the topic distributions ateach level of a path is sampled from the vocabu-lary of passages sharing that path, contrary to LDA,in which the topics are over entire vocabulary ofwords.
This enables defining a similarity measureon specific topics.
Given wq ={w1, ..., w|q|}, letwq,l ?
wq be the set of words in q that are gener-ated from topic zq at level l on path cq.
The discreteunigram distribution pql = p(wq,l|zq = l, cq, vl) rep-resents the probability over all words vl assigned totopic zq at level l, by sampling only for words inwq,l.
The probability of the rest of the words in vl areset 0.
Similarly, ps,l = p(ws,l|zs, cq, vl) is the proba-bility of words ws in s extracted from the same topic(see Fig.2.b).
The word probabilities in pq,l and ps,lare obtained using Eq.
(9) and then normalized.The similarity between pq,l and ps,l at each levelis obtained by transformed information radius:Wcq,l(pq,l, ps,l) = 10?-IRcq,l(pq,l,ps,l) (11)5where the IRcq,l(pq,l, ps,l) is calculated as in Eq.
(3)this time for pq,l and ps,l (?
= 1).
Finally simhLDA1 isobtained by averaging Eq.
(11) over different levels:simhLDA1 (q, s) =1L?Ll=1 Wcq ,l(pq,l, ps,l) ?
l (12)The similarity between pq,l and ps,l is weighted bythe level l because the similarity should be rewardedif there is a specific word overlap at child nodes.Algorithm 2 Tree-Based Similarity Model1: Given candidate passages s and question q.2: Build hLDA on set of s and q to obtain tree T .3: Find path cq on tree T and candidate passages4: on path cq , i.e., M = {s ?
D|cs = cq}.5: for candidate passage s ?M do6: Find DEShDLA(q, s) = simhLDA1 ?
simhLDA27: using Eq.
(12) and Eq.
(13)8: end for9: if s /?M , then DEShDLA(q, s)=0.
(2) simhLDA2 : We introduce a concept-base mea-sure based on passage-topic mixing proportions tocalculate the topical similarities between q and s.We calculate the topic proportions of q and s, rep-resented by pzq = p(zq|cq) and pzs = p(zs|cq) viaEq.(10).
The similarity between the distributions isthen measured with transformed IR as in Eq.
(11) by:simhLDA2 (q, s) = 10?IRcq(pzq ,pzs) (13)In summary, simhLDA1 provides information aboutthe similarity between q and s based on topic-worddistributions, and simhLDA2 is the similarity betweenthe weights of their topics.
The two measures arecombined to calculate the degree of similarity:DEShLDA(q,s)=simhLDA1 (q,s)*simhLDA2 (q, s) (14)Fig.2.b depicts a sample path illustrating sparse uni-gram distributions of a q and s at each level and theirtopic proportions, pzq , and pzs .
The candidate pas-sages that are not on the same path as the questionare assigned DEShLDA(s, q) = 0.5 Discriminitive Model for QAIn (Celikyilmaz et al, 2009), the QA task is posedas a textual entailment problem using lexical and se-mantic features to characterize similarities betweenq/a pairs.
A discriminative classifier is built to pre-dict the existence of an answer in candidate sen-tences.
Although they show that semi-supervisedmethods improve accuracy of their QA model un-der limited amount of labeled data, they suggest thatwith sufficient number of labeled data, supervisedmethods outperform semi-supervised methods.
Weargue that there is a lot to discover from unlabeledtext to help improve QA accuracy.
Thus, we pro-pose using Bayesian probabilistic models.
First webriefly present the baseline method:Baseline: We use the supervised classifiermodel presented in (Celikyilmaz et al, 2009) asour baseline QA model.
Their datasets, provided inhttp://www.eecs.berkeley.edu/?asli/asliPublish.html,are q/a pairs from TREC task.
They define eachq/a pair as a d dimensional feature vector xi ?
<dcharacterizing entailment information betweenthem.
They build a support vector machine (SVM)(Drucker et al, 1997) classifier model to predict theentailment scores for q/a pairs.To characterize the similarity between q/a pairsthey use: (i) features represented by similaritiesbetween semantic components, e.g., subject, ob-ject, verb, or named-entity types discovered in q/apairs, and (ii) lexical features represented by lexico-syntactic alignments such as n-gram word overlapsor cause and entailment relations discovered fromWordNet (Miller, 1995).
For a given question q, theyrank the candidate sentences s based on predictedentailment scores from the classifier, TE(q, s).We extend the baseline by using the degree ofsimilarity between question and candidate passageobtained from LDA, DESLDA(q, s), as well as hLDADEShLDA(q, s), and evaluate different models:Model M-1: Degree of Similarity as RankScores: In this model, the QA is based on a fullygenerative approach in which the similarity mea-sures of Eq.
(7) in ?3 and Eq.
(14) in ?4 are used toobtain ranking scores.
We build two separate mod-els, M-1.1 using DESLDA(q, s), and M-1.2 usingDEShLDA(q, s) as rank scores and measure accu-racy by re-ranking candidate passages accordingly.Given a question, this model requires training indi-vidual LDA and hLDA models.Model M-2: Interpolation BetweenClassifier-Based Entailment Scores and Genera-tive Model Scores: In this model, the underlying6mechanism of QA is the discriminative methodpresented in baseline.
We linearly combine theprobabilistic similarity scores from generativemodels, DES scores in M-1, with the baselinescores.
We build two additional models to calculatethe final rank scores; M-2.1 using:score(s|q) = a?TE(q, s)+b?DESLDA(q, s) (15)and M-2.2 using:score(s|q) = a?TE(q, s)+b?DEShLDA(q, s) (16)where 0 ?
a ?
1 and 0 ?
b ?
1 and a + b = 1.We find the optimum a?
and b?
based on the valida-tion experiments on training dataset.
The candidatesentences are re-ranked based on these scores.Model M-3: Degree of Similarity as Entail-ment Features: Another way to incorporate the la-tent information into the discriminitive QA modelis to utilize the latent similarities as explanatoryvariables in the classifier model.
Particularly webuild M-3.1 by using simLDA1 , simLDA2 as well asDESLDA(q, s) as additional features for the SVM, ontop of the the existing features used in (Celikyilmazet al, 2009).
Similarly, we build M-3.2 by usingsimhLDA1 , simhLDA2 as well as DEShLDA(q, s) as addi-tional features to the SVM classifier model to predictentailment scores.
This model requires building twonew SVM classifier models with the new features.6 Experiments and DiscussionsWe demonstrate the results of our experiments onexploration of the effect of different generative mod-els presented in ?5 on TREC QA datasets.We performed experiments on the datasets used in(Celikyilmaz et al, 2009).
Their train dataset com-poses of a set of 1449 questions from TREC-99-03.
For each question, the 5 top-ranked candidatesentences are extracted from a large newswire cor-pora (Acquaint corpus) through a search engine, i.e.,Lucene 2.
The q/a pairs are labeled as true/false de-pending on the containment of the true answer stringin retrieved passages.
Additionally, to calculate theLDA and hLDA similarity measures for each candi-date passage, we also extract around 100 documentsin the same fashion using Lucene and identify pas-sages to build the probabilistic models.
We calculate2http://lucene.apache.org/java/the probabilistic similarities, i.e., simLDA1 , simLDA2 ,simhLDA1 , simhLDA2 , and the degree of similarity val-ues, i.e., DESLDA(q, s) and DEShLDA(q, s) foreach of the 5 top-ranked candidate sentences intraining dataset at inference time.
Around 7200 q/apairs are compiled accordingly.The provided testing data contains a set of 202questions from TREC2004 along with 20 candidatesentences for each question, which are labeled astrue/false.
To calculate the similarities for the 20candidate sentences, we extract around 100 docu-ments for each question and build LDA and hLDAmodels.
4037 testing q/a pairs are compiled.We report the retrieval performance of our mod-els in terms of Mean Reciprocal Rank (MRR), top1 (Top1) and top 5 prediction accuracies (Top5)(Voorhees, 2004).
We performed parameter opti-mization during training based on prediction ac-curacy to find the best C ={10?2, .., 102}and?
={2?2, .., 23}for RBF kernel SVM.
For theLDA models we present the results with 10 top-ics.
In hLDA models, we use four levels for thetree construction and set the topic Dirichlet hyper-parameters in decreasing order of levels at ?
={1.0, 0.75, 0.5, 0.25} to encourage as many terms inthe mid to low levels as the higher levels in the hi-erarchy, for a better comparison between q/a pairs.The nested CRP parameter ?
is fixed at 1.0.
Weevaluated n-sliding-window size of sentences in se-quence, n = {1, 3, 5}, to compile candidate pas-sages for probabilistic models (Table 1).
The outputscores for SVM models are normalized to [0,1].?
As our baseline (in ?5), we consider supervisedclassifier based QA presented in (Celikyilmaz et al,2009).
The baseline MRR on TREC-2004 dataset isMRR=%67.6, Top1=%58, Top5=%82.2.?
The results of the new models on testing datasetare reported in Table 1.
Incorporating the genera-tive model output to the classifier model as inputfeatures, i.e., M-3.1 and M-3-2, performs con-sistently better than the rest of the models and thebaseline, where MRR result is statistically signifi-cant based on t-test statistics (at p = 0.95 confi-dence level).
When combined with the textual en-tailment scores, i.e., M-2.1 and M-2.2, they pro-vide a slightly better ranking, a minor improvementcompared to the baseline.
However, using the gen-erative model outcome as sole ranking scores in7Window-size 1-window 3-window 5-windowMRR categories MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5ModelsM-1.1 (with LDA) 42.7 30.2 64.4 42.1 30.2 64.4 42.1 30.2 64.4M-1.1 (with hLDA) 55.8 45.5 71.0 55.8 45.5 71.0 54.9 45.5 71.0M-2.1 (with LDA) 66.2 55.1 82.2 65.2 54.5 80.7 65.2 54.5 80.7M-2.2 (with hLDA) 68.2 58.4 82.2 67.6 58.0 82.2 67.4 58.0 81.6M-3.1 (with LDA) 68.0 61.0 82.2 68.0 58.1 82.2 68.2 58.1 82.2M-3.2 (with hLDA) 68.4 63.4 82.2 68.3 61.0 82.2 68.3 61.0 82.2Table 1: The MRR results of the models presented in ?5 on testing dataset (TREC 2004) using different window sizesof candidate passages.
The statistically significant model results in each corresponding MRR category are bolded.Baseline MRR=%67.6, Top1=%58, Top5=%82.2.M-1.1 and M-1.2 do not reveal as good results asthe other models, suggesting room for improvement.?
In Table 1, Top1 MRR yields better improve-ment compared to the other two MRRs, especiallyfor models M-3.1 and M-3.2.
This suggests thatthe probabilistic model outcome rewards the can-didate sentences containing the true answer by es-timating higher scores and moves them up to thehigher levels of the rank.?
The analysis of different passage sizes suggestthat the 1-window size yields best results and no sig-nificant performance improvement is observed whenwindow size is increased.
Thus, the similarity be-tween q/a pairs can be better explained if the candi-date passage contains less redundant sentences.?
The fact that the similarity scores obtained fromthe hLDA models are significantly better than LDAmodels in Table 1 indicates an important propertyof hierarchal topic models.
With the hLDA specificand generic topics can be identified on different lev-els of the hierarchy.
Two candidate passages canbe characterized with different abstract and specifictopics (Fig.
2) enabling representation of better fea-tures to identify similarity measures between them.Whereas in LDA, each candidate passage has a pro-portion in each topic.
Rewarding the similarities onspecific topics with the hLDA models help improvethe QA rank performance.?
In M-3.1 and M-3.2 we use probabilistic sim-ilarities and DES as inputs to the classifier.
In Table2 we show the individual effects of these features onthe MRR testing performance along with other lexi-cal and semantic features of the baseline.
Althoughthe effect of each feature is comparable, the DESLDAFeatures M-3.1 Features M-3.1sim1LDA 67.7 sim1hLDA 67.8sim2LDA 67.5 sim2hLDA 68.0DESLDA 67.9 DEShLDA 68.1Table 2: The MRR results of the similarity measures ontesting dataset (TREC 2004) when used as input features.and DEShLDA features reveal slightly better results.7 Conclusion and Future WorkIn this paper we introduced a set of methods basedon Latent Dirichlet Allocation (LDA) to character-ize the similarity between the question and the can-didate passages, which are used as ranking scores.The results of our experiments suggest that extract-ing information from hidden concepts improves theresults of a classifier-based QA model.Although unlabeled data exploration throughprobabilistic graphical models can help to improveinformation extraction, devising a machinery withsuitable generative models for the given natural lan-guage task is a challenge.
This work helps withsuch understanding via extensive simulations andputs forward and confirms a hypothesis explainingthe mechanisms behind the effect of unsupervisedpre-training for the final discriminant learning task.In the future, we would like to further evaluatethe models presented in this paper for larger datasetsand for different tasks such as question paraphraseretrieval or query expansion.
Moreover, we wouldlike to enhance the similarities with other semanticcomponents extracted from questions such as ques-tion topic and question focus.8ReferencesC.
M. Bishop and J. Lasserre.
Generative or dis-criminative?
getting the best of both worlds.
In InBayesian Statistics 8, Bernardo, J. M. et al (Eds),Oxford University Press, 2007.D.
Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.Hierarchical topic models and the nested chineserestaurant process.
In In Neural Information Pro-cessing Systems [NIPS], 2003a.D.
M. Blei, A. Ng, and M. Jordan.
Latent dirichletallocation.
In Jrnl.
Machine Learning Research,3:993-1022, 2003b.G.
Bouchard and B. Triggs.
The tradeoff betweengenerative and discriminative classifiers.
In Proc.of COMPSTAT?04, 2004.A.
Celikyilmaz, M. Thint, and Z. Huang.
Graph-based semi-supervised learning for question an-swering.
In Proc.
of the ACL-2009, 2009.C.L.A.
Clarke, G. V. Cormack, R. T. Lynam, andE.
L. Terra.
Question answering by passage se-lection.
In In: Advances in open domain questionanswering, Strzalkowski, and Harabagiu (Eds.
),pages 259?283.
Springer, 2006.H.
Drucker, C.J.C.
Burger, L. Kaufman, A. Smola,and V. Vapnik.
Support vector regression ma-chines.
In NIPS 9, 1997.A.
Echihabi and D. Marcu.
A noisy-channel ap-proach to question answering.
In ACL-2003,2003.T.
Griffiths and M. Steyvers.
Finding scientific top-ics.
In PNAS, 101(Supp.
1): 5228-5235, 2004.S.
Harabagiu and A. Hickl.
Methods for using tex-tual entailment in open-domain question answer-ing.
In In Proc.
of ACL-2006, pages 905?912,2006.Z.
Huang, M. Thint, and A. Celikyilmaz.
Investiga-tion of question classifier in question answering.In In EMNLP?09, 2009.W.-Y.
Ma and K. McKeowon.
Where?s the verb?correcting machine translation during questionanswering.
In In ACL-IJCNLP?09, 2009.C.
Manning and H. Schutze.
Foundations of statis-tical natural language processing.
In MIT Press.Cambridge, MA, 1999.A.
McCallum, C. Pal, G. Druck, andX.
Wang.
Multi-conditional learning: Gen-erative/discriminative training for clustering andclassification.
In AAAI 2006, 2006.G.A.
Miller.
Wordnet: A lexical database for en-glish.
In ACM, 1995.D.
Molla, M.V.
Zaanen, and D. Smith.
Named en-tity recognition for question answering.
In InALTW2006, 2006.H.T.
Ng, J.L.P.
Kwan, and Y. Xia.
Question answer-ing using a large text database: A machine learn-ing approach.
In EMNLP-2001, 2001.H.
Saggion and R. Gaizauskas.
Experiments in pas-sage selection and answer extraction for ques-tion answering.
In In: Advances in open domainquestion answering, Strzalkowski, and Harabagiu(Eds.
), pages 291?302.
Springer, 2006.T.
Schmah, G. E Hinton, R. Zemel, S. L. Small,and S. Strother.
Generative versus discriminativetraining of rbms for classification of fmri images.In Proc.
NIPS 2009, 2009.Dan Shen and Dietrich Klakow.
Exploring correla-tion of dependency relation paths for answer ex-traction.
In Proc.
of ACL-2006, 2006.J.L.
Vicedo and A. Ferrandez.
Applying anaphoraresolution to question answering and informationretrieval systems.
In In LNCS, volume 1846,pages 344?355, 2000.Ellen M. Voorhees.
Overview of trec2004 questionanswering track.
2004.J.
Weston, F. Rattle, and R. Collobert.
Deep learningvia semi-supervised embedding.
In ICML, 2008.9
