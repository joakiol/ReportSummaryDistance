CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 263?267Manchester, August 2008Probabilistic Model for Syntactic and Semantic Dependency ParsingEnhong ChenDepartment of ComputerScience, University of Sci-ence and Technology ofChina, Hefei, Chinacheneh@ustc.edu.cnLiu ShiDepartment of ComputerScience, University of Sci-ence and Technology ofChina, Hefei, Chinashiliu@ustc.eduDawei HuDepartment of ComputerScience, University of Sci-ence and Technology ofChina, Hefei, Chinadwhu@mail.ustc.edu.cnAbstractThis paper proposes a novel method toanalyze syntactic dependencies and labelsemantic dependencies around both theverbal predicates and the nouns.
In thismethod, a probabilistic model is designedto obtain a global optimal result.
More-over, a predicate identification model anda disambiguation model are proposed tolabel predicates and their senses.
The ex-perimental results obtained on the wsjand brown test sets show that our systemobtains 77% of labeled macro F1 scorefor the whole task, 84.47% of labeled at-tachment score for syntactic dependencytask, and 69.45% of labeled F1 score forsemantic dependency task.1 IntroductionThere are two difficulties in the CoNLL 2008shared task.
One is how to label semantic role ona dependency-based representation and how tolabel verbal predicates and nouns.
The other oneis how to combine the syntactic task with thesemantic task together.On the basis of statistical analysis of labelingresults, we optimize the traditional approaches ofsyntactic dependency parsing and semantic rolelabeling.
Moreover, we design a predicateidentification model and a disambiguation model,which will be described in section 2.3, forlabeling predicates and their senses.
In thedisambiguation model, an exhaustion method isused to find the best sense which iscorresponding to a frame of predicate.
In order toobtain a global optimization result for every?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.sentence, a probabilistic model is designed tocombine all subtasks.The rest of this paper is organized as follows:our system is described in section 2; and section3 reports our results on development and test sets;at last we conclude the paper in section 4.2 A Probabilistic Model for Syntacticand Semantic Dependency LabelingCompared with previous tasks, this shared task ismore complex.
It aims to merge both syntacticand semantic dependencies under a unifiedrepresentation.
Obviously, it can be divided intotwo subtasks: syntactic dependency parsing andsemantic dependency labeling.
For the secondsubtask, predicates and their senses should belabeled before semantic arguments for predicatesare labeled.
Since many predicates have only onesense, it is inefficient to build a multi-labelclassifier to classify each predicate.
When aclassification approach is used, it is mandatory toconsider multiple senses for those predicateswith only one or two senses.
To preventassigning irrelevant senses to predicates, we donot adopt classification approach.
Instead, twomore subtasks, i.e., predicate identification andpredicate sense labeling, are introduced in thispaper.
The predicate sense labeling and semanticdependency labeling are performed together witha disambiguation model.To ensure that we can get an optimal overallsyntactic and semantic dependency resultsthrough integrating the above steps, a probabilitymodel is proposed.
The probabilistic model isdescribed in Equation (1), where the score Pof a sentence labeling is the combinedconditional probability of its all subtasks,  isthe probability of syntactic dependency parsing,is the probability of predicateidentification,  is the probability ofsentsynPpredP)(iPsem263semantic dependency labeling for the ith-predicate, and n is the number of predicates.
?==n1isempredsynsent )(** iPPPP            (1)For each sentence, its top-N candidates usingsyntactic dependency parsing are obtained.
Thenfor each candidate, predicates and semantic ar-guments are labeled.
At last, the best one withthe highest  is chosen as final labeling result.
sentP2.1 Syntactic Dependency ParsingThere are several approaches for syntactic de-pendency parsing, as demonstrated in the CoNLL2007 shared task.
A commonly used LR algo-rithm is applied to this task.
Unlike the best-firstprobabilistic shift-reduce LR algorithm used by(Kenji and Jun, 2007), here a combined probabil-ity of all parsing steps is used to evaluate parsingresults, and the best one is obtained as the finalresult.
The probability of syntactic dependencyparsing is defined in Equation (2).
?= j actsyn iPP )(=i 1act(2)where  is the probability of every LR ac-tion act at step i, and j is the number of all steps.
)(iPAs the search space of LR parser is exponen-tial growth with the word number, the maximumsize of candidate states is limited to 50.The features that we use are similar to (Kenjiand Jun, 2007).
Hence we do not describe themin this paper.2.2 Predicate IdentificationIn this subtask, a MaxEnt model is adopted forclassification.
The features we used are as follow:?
Base info: FORM, LEMMA, POS (GPOSif available, or is PPOS), SPLIT_FORM,SPLIT_LEMMA, PPOSS.?
Base syntactic dependency info:o Number of modifiers;o Number of modifiers of the previous word;o Number of modifiers of the next word;o PPOSS of left-most modifier;o Deprel of left-most modifier;o PPOSS of right-most modifier;o Deprel of right-most modifier.?
Modifiers infoo POS list of all modifiers: if GPOS is avail-able, POS is GPOS.
Otherwise it is PPOS.o DEPREL list of all modifiers;o SPLIT_LEMMA list of all modifiers;o PPOSS list of all modifiers.?
Head?s base info?
Head?s base syntactic dependency info?
Head?s modifiers info?
Deprel:  the syntactic dependency relationto head.?
Word stem?
Stem of right-most modifier?
PPOSS of right-most modifier?
Suffix: The suffix of the word.
We use thelast 3 characters as this feature.?
Voice: Check if the word is a verb and ispassive voice.?
Previous word info: Check if the previousword is a predicate.?
Pos path to ROOT: PPOSS list fromword to ROOT through the syntactic de-pendency path.?
Deprel path to ROOT: DEPREL list fromword to ROOT through the syntactic de-pendency path.Through statistical analysis, we find thatPPOSS of nearly all predicates are in a particularcategory which contains NN, NNP, NNS, VB,VBD, VBG, VBN, VBP, VBZ, and JJ.
Hence weignore the words without these PPOSS to reducethe number of samples and speed up the processof training and recognition.
Meanwhile, we alsoignore the words having no relational frame inPropBank or NomBank.2.3 Predicate Sense LabelingIn this subtask, we label the sense of each predi-cate.
Different predicates are usually unrelatedeven if they have the same sense number, whichmakes us hardly use a classifier to label them.Hence, we design a disambiguation model tosolve this problem.Firstly, for each word which has been identi-fied to be a predicate, we find out all of its prob-able sense forms (corresponding to the field of?PRED?).
According to statistical analysis, onlyabout 0.05% PREDs are not described inPropBank frames or NomBank frames.
So it isreasonable to assume that all PREDs could befound in PropBank or NomBank.
Moreover, wefind that about 96% PREDs are formed as?SPLIT_LEMMA + .sense?
or ?LEMMA+ .sense?.
As a result, when a word is identifiedto be a predicate, we use its LEMMA andSPLIT_LEMMA to find all possible PREDsfrom PropBank and NomBank.
Furthermore, ifsome special words are unsuitable for these twoforms, we should convert them into their originalforms first and then find their possible PREDs.264For the rest anomalistic words, we build a map-ping dictionary from training data.Secondly, for each possible sense form, we la-bel semantic argument for all words.
If a word isnot a semantic argument, it would be labeled as?_?.
The score of the current possible sense formis calculated as the combination of all probabilityof each labeling.
More details about semanticdependency labeling will be described in section2.4.Thirdly, we choose the sense form and its se-mantic arguments with the highest score.
Theabove steps will be repeated until all predicateshave definite senses.2.4 Semantic Dependency LabelingUnlike CoNLL-2005 shared task, this sharedtask performing Semantic Role Labeling on adependency-based representation (DSRL).
It is anovel way for SRL and the traditional SRLmethods can not directly be used here.Constituent-based SRL model needs to find outall probable constituents, while DSRL onlyconsiders the semantic dependency betweenword and predicate.
Moreover, DSRL usessyntactic dependency parsing tree instead oftraditional full syntactic parsing tree.
As a result,the traditional features need to be amendedaccordingly.
The features we used are as follows:?
Deprel?
Word stem?
POS: if GPOS is available, POS is GPOS.Otherwise it is PPOS.?
Stem of right-most modifier?
PPOSS of right-most modifier?
Predicate: the FORM of predicate.?
PPOSS of predicate?
Suffix of predicate?
Voice: voice of predicate?
Position: The position of the word with re-spect to its predicate.
It has three values,?before?, ?is?
and ?after?, for the predicate.?
Deprel path to predicate: DEPREL listfrom word to its predicate through the syn-tactic dependency path.?
Length of syntactic dependency path topredicate?
Sense: the sense of predicateMoreover, we try to find more features withframes.
Since the PropBank and NomBank areavailable and all predicates with senses are avail-able for this subtask.
Statistical analysis showsthat nearly all core semantic arguments (AA, A0,A1, A2 ?)
of a predicate are described in theframe of predicate.
But it is incorrect contrarily.Based on these observations, we design featuresthe following features for five frequently usedcore arguments:?
A0 is in predicate?s frame: Have twovalues: ?YES?
and ?NO?.?
A1 is in predicate?s frame?
A2 is in predicate?s frame?
A3 is in predicate?s frame?
A4 is in predicate?s frameBecause the other core semantic arguments arerare, we do not need to design features for them.With this method, the labeling efficiency is im-proved while the precision almost keeps un-changed.As the frame information has been used in fea-tures, we do not add any valency check on thelabeling result.3 Experiments and Analysis3.1 Data and EnvironmentThe data provided for this Closed Challenge ofshared task is part of TreeBank and Brown cor-pus.
Training set covers sections 02-21 of Tree-Bank.
Development set covers section 24 ofTreeBank.
Wsj test set covers section 23 ofTreeBank.
Brown test set covers sections ck01,ck02, and ck03 of the Brown corpus.The maximum entropy classier (Berger et al1996) used is Le Zhang's Maximum EntropyModeling Toolkit and the L-BFGS parameterestimation algorithm with gaussian prior smooth-ing (Chen and Rosenfeld, 1999).
The gaussianprior is set to 2 and the iteration count is set to500.
All results we list here are post-evaluatedbecause there are some small modifications.The experiments are performed on a PC withAMD Athlon?
64 x2 4400+ CPU and 2GBmain memory running Microsoft Windows XPwith sp2.
Our system is developed using C++.In our experimental analysis, the abbreviationsused are listed as follows:?
LAS1: Labeled attachment score?
UAS: Unlabeled attachment score?
LAS2: Label accuracy score?
LP: Labeled precision?
LR: Labeled recall?
LF1: Labeled F1?
UP: Unlabeled precision?
UR: Unlabeled recall?
UF1: Unlabeled F12653.2 Syntactic Dependency ParsingWe trained two LR models for syntactic depend-ency parsing.
The first LR model uses MaxEntclassification to determine possible parser actionsand their probabilities.
The second LR modelalso uses MaxEnt classification, but parsing isperformed backwards simply by reversing thesentence before parsing starts.For a sentence, each model can label top-Ncandidates and calculate the probability for everyresult.
We join these two models by finding thecandidate with the highest probability from allcandidates as the final result for the sentence.Table 1 shows the results of each model and jointmodel.
We can see that the two LR models ob-tain similar results.
The joint model can obtainbetter result and increase almost one percentage.The processing time of joint model is twice morethan that of the two other models.LR ModelLR-backModelJointModelLAS1 83.05 83.38 84.43UAS 86.36 86.74 87.74 devLAS2 89.15 89.63 90.08LAS1 84.84 84.06 85.48UAS 87.60 86.74 88.13 wsjLAS2 90.70 90.47 91.21LAS1 77.29 76.95 78.91UAS 82.75 82.61 84.38 brownLAS2 85.00 84.82 85.76LAS1 84.00 83.27 84.75UAS 87.06 86.28 87.71wsj +brown LAS2 90.07 89.84 90.6Speed (sec/sent) 0.49 0.42 0.92Table 1:  Syntactic dependency parsing results3.3 Predicate IdentificationOur predicate identification approach is de-scribed in section 2.2.
We use the gold HEADand DEPREL fields to test our approach.
Theresults are shown in Table 2.
The labeling foreach sentence spends about 14ms.dev wsj brownPrecision 93.56 93.61 87.51Recall 93.24 93.39 89.04F1 93.40 93.50 88.27Table 2:  Predicate identification results3.4 Semantic Dependency LabelingSemantic dependency labeling is the last sub-task.
Our DSRL model uses MaxEnt classifica-tion to determine the semantic dependency be-tween each word and its corresponding predicate.The gold HEAD and DEPREL and PRED fieldsis used to test the model.Statistical analysis shows that, for about 99%semantic argument labels, the length of syntacticdependency path from word to predicate is lessthan 7.
So we ignore the words with the length of7 or more.The final results of semantic dependency la-beling are shown in Table 3.
The labeling foreach sentence spends about 10ms.Brown set is an out-of-domain set and wsj setis an in-domain set.
Usually, the results on wsjare much better than those on brown.
But herewe found that the unlabeled scores are nearly thesame between wsj and brown.
It shows that ourmodel performs well at unlabeled labeling onout-of-domain set, and should be improved atlabeled labeling.dev wsj brownLP 80.50 82.47 77.29LR 70.73 73.58 67.16LF1 75.30 77.77 71.87UP 92.10 92.65 92.87UR 80.92 82.65 80.69UF1 86.15 87.36 86.35Table 3:  Semantic dependency labeling results3.5 Overall ResultAs described in section 2, we use a probabilisticmodel to integrate all subtasks.
In the probabilis-tic model, syntactic dependency parsing shouldparse top-N candidate results.
We do the restparsing for each candidate result and get N inte-grated results.
Then, for each integrated result, itsis calculated and the best one is chose asthe final result.sentPThe DSRL results around verbal predicatesand nouns on wsj set are shown in Table 4.
Itshows that verbal predicates are labeled muchbetter than nouns.Unlabeled PredicateLabeledPredicateLabeled SemanticArgumentsNN* 87.79 79.52 58.09VB* 96.85 80.25 73.77Table 4:  The F1 values of DSRL around verbalpredicates and nouns on wsjTable 5 shows the overall results with differ-ent N.  The results are improved when N changesfrom 1 to 2.
However, there is nearly no im-provement by increasing N from 2 to 3.
So N isset to be 2 in our system.
Meanwhile, the effectof this approach is not obvious.
We find that266there are nearly only one or two different pointsbetween the top-2 candidate dependency parsingresults.
This leads to that the DSRL results withthese top-2 candidate results are almost the same.This is the probable reason that the approach isnot much improved with the increase of N. In thefuture it would be necessary for us to considerthe number of different points when finding thetop-N dependency results.N=1 N=2 N=3LP 78.58 78.93 79.01LR 75.58 75.52 75.33LF1 77.05 77.19 77.13UP 86.56 86.95 87.07UR 83.04 82.94 82.75devUF1 84.76 84.90 84.85LP 79.41 79.76 79.96LR 76.67 76.59 76.49LF1 78.02 78.15 78.19UP 86.59 86.92 87.11UR 83.40 83.25 83.10wsjUF1 84.97 85.04 85.06LP 70.52 70.95 70.79LR 68 67.88 67.54LF1 69.24 69.38 69.13UP 81.87 82.39 82.28UR 78.65 78.47 78.14brownUF1 80.23 80.39 80.16LP 78.45 78.8 78.96LR 75.72 75.64 75.5LF1 77.06 77.18 77.19UP 86.08 86.43 86.59UR 82.89 82.73 82.56wsj +brownUF1 84.45 84.54 84.53Speed (sec/sent) 0.93 0.94 0.95Table 5: Overall macro scores (Wsem = 0.50)4 ConclusionWe divide this shared task into four subtasks:syntactic dependency parsing, predicate identifi-cation, predicate sense labeling and semanticdependency labeling.
Then, we design a prob-abilistic model to combine them.
The purpose ofour system is to find a global optimal result forevery sentence.
If a syntactic dependency parsingresult has the highest probability but it is unrea-sonable, it would be difficult to get a semanticparsing result with high probability again.
Hence,a more reasonable result may be found withlower syntactic dependency parsing probability.In our system, we have not distinguished be-tween nouns and verbal predicates.
The experi-mental results show that the results of verbalpredicates are much better than those of nouns.In the future, it is necessary for us to deal withthem separately.AcknowledgmentsThis work was supported by National NaturalScience Foundation of China (No.60573077,No.60775037), Specialized Research Fund forthe Doctoral Program of Higher Education(No.2007105), and Program for New CenturyExcellent Talents in University (No.NCET-05-0549).ReferencesBerger, S. A. Della Pietra, and V. J. Della Pietra.
1996.A maximum entropy approach to naturallanguageprocessing.
Computational Linguistics, 22(1):39?71.Che Wanxiang, Ting Liu, Sheng Li, Yuxuan Hu, andHuaijun Liu.
2005.
Semantic role labeling systemusing maximum entropy classifier.
In Proceedingsof Computational Natural Language Learning(CoNLL-2005).Gildea Daniel and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Duan Xiangyu, Zhao Jun  and Xu Bo.
2007.
Probabil-istic Parsing Action Models for Multi-Lingual De-pendency Parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007.Hacioglu K. 2004.
Semantic Role Labeling UsingDependency Trees.
In Proceedings of COLING-2004.Johansson R. and Nugues P. 2007.
Extended Con-stituent-to-dependency Conversion for English.
InProceedings of NODALIDA 2007.Sagae, Kenji  and  Tsujii, Jun'ichi.
2007.
DependencyParsing and Domain Adaptation with LR Modelsand Parser Ensembles.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL2007.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical Report CMU-CS-99-108.Surdeanu, Mihai, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing of Syn-tactic and Semantic Dependencies.
In Proceedingsof the 12th Conference on Computational NaturalLanguage Learning (CoNLL-2008).Tsai Tzong-Han, Chia-Wei Wu, Yu-Chun Lin, andWen-Lian Hsu.
2005.
Exploiting full parsing in-formation to label semantic roles using an ensem-ble of me and svm via integer linear programming.In Proceedings of Computational Natural Lan-guage Learning (CoNLL-2005).267
