Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 65?73,Dublin, Ireland, August 23-29 2014.A Dictionary Data Processing Environment and Its Application inAlgorithmic Processing of Pali Dictionary Data for Future NLP TasksDipl.
Inf.
J?rgen KnauthTrier Center for Digital HumanitiesUniversit?tsring 1554296 TrierGermanyknauth@uni-trier.deDavid AlfterTrier Center for Digital HumanitiesBollwerkstrasse 1054290 TrierGermanys2daalft@uni-trier.deAbstractThis paper presents a highly flexible infrastructure for processing digitized dictionaries andthat can be used to build NLP tools in the future.
This infrastructure is especially suitable forlow resource languages where some digitized information is available but not (yet) suitablefor algorithmic use.
It allows researchers to do at least some processing in an algorithmic wayusing the full power of the C# programming language, reducing the effort of manual editingof the data.
To test this in practice, the paper describes the processing steps taken by makinguse of this infrastructure in order to identify word classes and cross references in thedictionary of Pali in the context of the SeNeReKo project.
We also conduct an experiment tomake use of this data and show the importance of the dictionary.
This paper presents theexperiences and results of the selected approach.1 IntroductionPali (also written P?li, Pa?i or P?
?i) is a dead language from the group of Middle Indo-Aryan languages(Burrow, 1955: 2).
Despite its status as dead language, Pali is still widely studied because many of theearly Buddhist scriptures were written in Pali (Bloch, 1970: 8).
It is also said that Buddha himselfspoke Pali or a closely related dialect (Pali Text Society; Thera, 1953: 9).SeNeReKo is a joint research project of the Trier Center for Digital Humanities (TCDH) and theCenter of Religious Studies in Bochum (CERES), Germany.
This project aims to process the PaliCanon ?
which at the same time is the only texts left of Pali ?
in order to research religious contactsbetween the early Buddhists and other religious groups and cultures.To achieve this we aim to develop NLP tools and process this data as we believe that the conceptsof interest will be found in direct verbal expressions within this corpus.
From the information we aimto extract we intend to create networks that allow analysis of these concept.Until now such an attempt has never been made.
Even processing Pali using computer algorithmshas not been in the focus of the scientific community yet.
As we researchers in SeNeReKo try tochange this we now focus on a basic building block for NLP tools: Building a machine readabledictionary that allows building sophisticated NLP tools in the long run.
To attempt this a digitizedcopy of the dictionary of William and Davids (1997) has been provided to our team by the Universityof Chicago.2 Related WorkAs Pali is a low resource language not much work has yet been done in this field, especially not withthe dictionary data.
The only researchers we know of that have tried to use this data is a team of theThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/65University of Copenhagen.
Their goal was to create a new digitized version of this dictionary.Unfortunately they did not succeed and stopped after having edited three letters of the Pali alphabet.To our knowledge we are the first to work with this data again.With good success a language somehow similar to Pali has been addressed in the past: Sanskrit(Hellwig 2009).
Nevertheless attempts to adapt these tools to Pali have not been possible due to thelack of a suitable dictionary.Regarding NLP tools addressing Pali some experiments have already been performed by themembers of the SeNeReKo project team and especially by David Alfter.
Nevertheless no work couldyet reach a state of publication due to the lack of a suitable digital dictionary that would serve as abasis for NLP tasks.3 Technical infrastructureAs it is the nature of digital humanities projects like SeNeReKo a variety of researchers is involvedinto the process of processing and editing data and developing methods for the research intended.
InSeNeReKo this involves Pali experts, Sociologists, Computer Linguists and Scientists (andEgyptologists for performing work with other text corpora not addressed by this paper.)
Aninfrastructure that aims at enabling collaboration is therefore mandatory.
This section describes keyaspects of the infrastructure developed.3.1 Dictionary ServerEach dictionary entry is to be understood as a single document which is self-contained and structured.A dictionary is considered to be a collection of documents.Being self-contained all information relevant to each individual entry is stored in the samedocument.
Each of these entries must be structured to provide information in a clearly defined way forNLP tools in the future.To store the dictionary data a MongoDB data base is used.
This NoSQL data base not only supportssuch kind of data model it also provides the necessary flexibility to define and change the internalstructure of such dictionary document in the future as needed.For ease of use a NodeJS-based dictionary server has been implemented that provides userauthentication and high level data base operations addressing searching, inserting, updating anddeleting specific to the requirements of a dictionary.The pairing of NodeJS and MongoDB is reasonable because of performance reasons: MongoDBreceives and returns data not in XML, but in JSON notation; and as NodeJS provides its functionalitythrough a highly efficient JavaScript engine JSON data can directly be processed without any need ofconversion.For collaboration purposes a REST-API has been implemented with compatibility andinteroperability in mind.
As we aim for algorithmic processing of data and want to enable researchersto easily implement custom NLP tools that make use of the dictionary data independently from eachother.
To support this as best as possible a Java and C# library has been implemented as well as an Rmodule for convenience.As it is the nature of dictionary data to consist of a larger amount of individual entries, classicalrequest-response communication models, as they would be imposed by HTTP, are unsuitable forprocessing (in the sense of algorithm based editing).
Following that approach would result in notableperformance degradation.
Fortunately single processing steps as we intend them for pattern matchingand enriching of dictionary entries have largely no relation between individual entries.
Therefore thedictionary server provides an interface for bulk communication: A large amount of individual protocolfunction calls can be packed into a single package.
As the server processes them in parallel and returnsthe response to all requests again in a single response we are capable of overcoming the problem ofsummation of network latencies and end up with good performance in updating data.3.2 Data Processing ToolIn SeNeReKo we need to process the original - near plaintext - dictionary entries.
This data is insertedinto the dictionary server beforehand and then various analysing and processing steps need to be taken.To perform these, we implemented a processing environment that makes developing of individual66processing units very easy, gives high performance and great transparency about data modificationsintended by these units.Our data processing tool is a programming environment for creating small processing units in C#.Data management issues do not need to be addressed: This is done by the programming environmentautomatically.
The individual units are compiled to native .Net code for speed of processing.
Onexecution data from the dictionary server is retrieved and passed through these units and ?
if necessary?
sent back to the server after modifications have been applied.
Together with the bulk processingsupported by the dictionary server the compilation of the code units speeds up any processing.
Bydirectly making use of C# this approach we achieve great flexibility: It allows making use of all kindsof existing libraries if desired and enables researchers to implement all kinds of data specific patternmatching and processing for research tasks.As it is the nature of dictionary data to consist of a large amount of individual entries, applyingpattern matching and transformation tasks require a great deal of transparency.
Researchersperforming these tasks need to be able to identify which rule is applied to which entry in what formand see what modification an entry will receive.
To achieve this transparency our data processing toolcollects information about all modifications applied to each individual data record and presents themin a large list that can be filtered by some criteria.
Thus our tool aids in debugging by allowing insightinto every details of the tasks a researcher is going to perform.4 Processing of Pali Dictionary DataPrior to any processing we converted the original digitized dictionary entries we received from theUniversity of Chicago into JSON data structures and inserted them into our dictionary server.
In thenext sections we present our processing steps applied to the individual dictionary data records withinthe infrastructure described above.4.1 Transliteration of LemmasAs it turned out the digitized version of the Pali Dictionary we received was not entirely in accordancewith the current transliteration conventions.
Therefore to be able to use the Pali dictionary for researchthe lemmas had to be adjusted.To achieve a valid transformation we first had to verify that no accidental errors had beenintroduced by the original digitization process done by the Pali Text Society.
We thereforeimplemented an alphabet model that follows the old transliteration schema used to represent glyphs ofthe Sinhalese alphabet.
For these single letters one or two Latin ligatures (with diacritics) are usedtoday.
Modelling each word with the original alphabet is mandatory to be able to identify possibleerrors.
We checked all lemmata against our model and were able to identify 14 of 16280 lemmataviolating our model.
The errors could be identified to be printing errors or misinterpretation duringdigitalization and were then corrected manually before continuing processing.The next step was to perform substitutions of the letters ???.
To ensure correct processing this wasnot done on the Unicode based character representation of the data directly but on the original lettersmodelled by our alphabet model.
Substitution is performed on that basis taking the phonetic contextinto account as necessary:?
followed by j, c, h or e => ??
followed by k or kh => ??
followed by d, dh or n => n?
followed by m, p, bh or b => m?
followed by s => ??
followed by ?, ?h => ??
followed by l => l?
followed by v, y or r => ??
followed by a, e, i, o, u, ?, ?, ?
=> ??
not followed by any character => ?675 Pattern Recognition and Enriching Dictionary Entries5.1 Pattern MatcherIn processing Pali we had to take our own pattern matching approach in order to avoid problemsencountered with regular expressions in C#.
We found that some Pali specific diacritics did not getprocessed as the official regular expression syntax specification suggested.
To overcome theselimitations we implemented an own pattern matcher.Nevertheless we were not interested in dealing with space characters as they do not provide anyvaluable information to our pattern recognition tasks.
And for easy communication with Indologists apattern syntax was required that would be easy to understand.
So these requirements specific to ourfield of application were taken into account in building the pattern matcher.The pattern matching system we designed does not process character streams but token streams.The system can distinguish between the following concepts:?
A whitespace ?
which is automatically left out during tokenizing the dictionary articles?
A word ?
which is an alphanumeric character including all diacritics?
A delimiter ?
which is any kind of character not being to a word or whitespaceAs we aimed for an iterative process in order to identify relevant pattern it helped greatly to be ableto express patterns to be matched in the form of expressions that are easy readable by non-computerexperts.
Our syntax supports the following forms:?
Match a specific word token?
Match any word token?
Match a specific delimiter token?
Match any delimiter tokenExamples of this syntax are given in the next sections which address specific pattern recognitiontasks individually.5.2 Cross ReferencesAs P?li grammar is not standardized to the same extent as, e.g., Sanskrit, various alternative wordforms occur.
The Pali dictionary at hand addresses this problem to some extent by containing severalversions of some lemmas.
These entries then contain purely textual information of a reference to thedictionary entry having more information about the selected lemma.
In the Pali dictionary this isexpressed in forms like this:... in general see <b>buddha<b> ...Such a form is matched by a pattern like this:'in'  'general'  'see'  <  'b'  >  W*!
<  /  'b'  >The pattern specified is easy to understand: This is a sequence of individual patterns matchingspecific tokens.
Words in inverted commas express an exact match of a single word.
?W*!?
indicatesthat a word of any kind is expected here (and it should be available for further use after a match hasbeen found).
Other characters match specific delimiter tokens.Two real world examples of dictionary entries:anumattasee <b>a?u?</b> .anois a frequent form of comp<superscript>n.68</superscript><b>an--ava</b> , see <b>ava</b> .As there exist various different forms of patterns like this in the dictionary specifying multiplepossible variants was required.
Within an iterative process we were able to identify 46 different kindsof patterns which we could make use of for automatic identification.To further help manual processing of the dictionary we implemented a verifier that tries to identifythe lemmas each cross reference refers to within the dictionary.
This is done by direct dictionarylookup.
References that do not seem to point to a valid lemma are listed together with candidatesbased on Levenshtein distance for manual processing later by Indologists.5.3 Extracting word class informationAs we aim for lemmatizing and part of speech tagging of the Pali Canon, in the long run havinginformation about the word class of each lemma is mandatory.
Therefore we used pattern matching toaid the generation of data for this purpose.Our algorithmic approach of classification is basically performed in three steps described next.Word class information mainly manifests itself in expressions enclosed in rounded brackets.
E.g.:ap?ra(nt.)
[a + p?ra] 1. the near bank of a river ...s?ha?aCeylon; (adj.)
Singhalese ...susira(adj.--nt.)
[Sk.
?u?ira] perforated, fullof holes, hollow ...p?tika(--?)
(adj.)
[fr.
p?ti] belonging to joy; ...Unfortunately round bracket expressions are used in different semantic contexts within dictionaryentries.
In a first step we therefore extracted all content enclosed in round brackets and identifiedexpressions that represent word class information.
Though an old printed edition of the dictionarycontained a clear definition of these word class expressions used we encountered some variety ofwriting, of combination and of misspelling: Building a list of relevant expressions was the only way toaddress all phenomena in sufficient quality.Secondly we know from Pali grammars that verb lemmata typically end with ?-ti?
in the dictionary.But not all lemmata ending with ?-ti?
are verbs.
Therefore we implemented the following algorithmthat was able to clearly identify lemmata correctly as verbs:for all lemmas doif lemma does not end with ?-ti?
-> reject itif bracket expression in data matches a pattern clearlyclassifiable as non-verb -> reject itif entry does not contain the (English) word ?to?
-> reject itotherwise -> recognize this lemma as being a verbAfter having identified verbs successfully we then were able to address dictionary entries of otherword forms purely according to expressions in round brackets.
The following list gives an overview ofhow many kinds of patterns have been identified and were involved in this process:Word Class Number of Patternsadjective 26 incl.
one misspellingindeclinable 1adverbs 4 incl.
one misspelling69pronouns 1numerals and ordinals 2nouns 86 Word class recognitionIn order to evaluate the importance of the dictionary, we designed the following task: for each word ina manually tagged subset of the Pali Canon, we tried to recognize the word class using a generation-based and a heuristic approach.
We then compared the results of both approaches.For the generation-based approach, we generated all possible word forms, including morphologicalinformation, for every word in the dictionary using the morphological generator.
The generator usesparadigms to generate regularly inflected word forms.
Furthermore, the generator uses the dictionaryto look up morphological information about a word and, if present, uses this information to restrict thegeneration to grammatically adequate forms.
However, since the dictionary entries do not alwayspresent this information, or because it?s not always possible to easily extract this information, we over-generated in cases where no information can be retrieved from the dictionary.
We also generated rareforms according to information presented in available grammars on Pali.
In total, we were able togenerate 11447206 word forms for all words.
This averages to about 702 word forms per dictionaryentry.
In compact notation, this resulted in about 1.5 GByte of data.As we generated possible morphological forms from lemmas, we then reversed the data structure toarrive at a morphological form lookup table.
We saved these results locally for later efficient lookup.As a test corpus for our word class recognition task we used a manually annotated set of 500sentences (about 4600 words).
These sentences have been extracted earlier in the SeNeReKo project,choosing three consecutive sentences at random from the whole Pali corpus.
This preparatory step hasbeen started about a year ago to assist future computational linguistic tasks (a further 500 sentences arework in progress).
Thus, the data is representative of the whole corpus and is not biased.We then stepped through our corpus and checked for each word whether one or more of thegenerated forms corresponded to the word at hand.
If this was the case, we retrieved the relevantentries including all attached morphological information.
From these entries, we then retrieved theword class information for the word.For the heuristic approach, we built a morphological analyzer.
The analyzer can only rely on itsinternal heuristic for guessing the word class of a word.
The heuristic is ending based and usesparadigms to determine to which word class a word could belong.
The analyzer tries to identify andseparate possible endings occurring in different paradigms.
Based on these analyses, the word class isguessed.Before we could start the experiment, we had to map the word classes used by thegenerator/analyzer and the word classes used in the annotated corpus onto a common set of classes.The reference corpus uses a fine-grained tag set that?s standardized for use in more than one corpus inthe SeNeReKo project.
The dictionary uses a simple tag set, which has been created independently ofthe SeNeReKo tag set many decades ago.
The tag sets follow different principles and goals.
It istherefore not always straightforward to map one tag set onto the other.We tried to assign each word of the reference corpus a word class and checked the results againstthe manual annotations.
The results of this algorithmic output are evaluated in the result section below.7 Discussion7.1 Performance of the processing environmentAs a server we use an older 32 bit Linux machine with an Intel Core Duo at 2.4 GHz and 4 GByte ofmemory which runs the dictionary server with its data base.Due to bulk processing of requests we were able to bring down the average time for a single writeoperation to about 0.7ms per dictionary entry from a client?s point of view under ideal circumstances.In a real world application such as our data processing tool this enables us to process all 16280dictionary entries within about 10 seconds if no changes are applied and to about 20 seconds if all70entries must be read and written back to server.
We found this delay very acceptable during our designand implementation of individual processing units for the dictionary data.The following performance measurement chart for data write requests gives an insight into howperformance is affected by network latency:(If the above chart is displayed in black and white: The top line represents the client durationmeasured per operation, the bottom line measures the server duration per individual insert operation.
)This measurement is taken by inserting all Pali dictionary data 10 times with different chunk sizesand averaging the duration as measured by the test software.
For convenience the server performsperformance measurements on his own and sends his results 9together with the response to the client,so that such a kind of analysis can be performed easily.
The difference between both measurementsindicate the overhead introduced (mainly) by network latencies.Please note that the chart starts at a chunk size of 10.
This is for a reason: It turned out that lowervalues will introduce significantly more delay.7.2 Results of pattern matchingOur attempts to process the 16280 dictionary entries resulted in being able to recognize word forms in10016 of all entries.
This is about 61.5% of all dictionary data.Regarding cross references we were able to extract 457 cross references to existing lemmas withinthe dictionary, 52 references to lemmas not in the dictionary and 75 references containing onlyincomplete information and cannot be resolved automatically.At first hand these values do not seem to be very high.
But as we can only rely on clearlyidentifiable patterns within the dictionary entries these values are even better than we hoped at thebeginning of our work.
It has been clear right from the start that a greater amount of dictionary entrieswould need to be the centre of manual work in the future by Pali experts: Many entries simply do notcontain any information that can be recognized by the algorithmic approaches taken.As Pali is a largely dead language we have to consider that our data processing described in thispaper is a one-time task.
The only relevant dictionary at hand is the one we used, containing exactlythose words we have.
We successfully identified word classes for lemmas leaving 6264 for manualprocessing for our Indological colleagues.
If even more time would be spent in finding even morepatterns within the dictionary entries, we might improve our performance by a few percent, but there isno real reason to do this: We have come to a point where finding more patterns will take considerablymore time than identifying word classes and assigning them manually to the dictionary entries.7.3 Results of Word Class recognitionWe tried to recognize word classes based on the generation-based approach and on the heuristicapproach as described above.
We faced the problem that word forms can be analysed in more than oneway, even by using paradigms, which represent regular inflections.
This degree of ambiguity cannotbe resolved currently due to the particularities of Pali, such as a high degree of homonymy.Furthermore, different paradigms yield the same surface form, even though they belong to totallydifferent word classes.Therefore, we evaluated the resulting data in two different ways.
First, we used ?is-any?
matching.If a test corpus word has been assigned more than one word class by our algorithms, we consider the71word classes to match if the two sets share at least one common element.
This way we address theproblem of ambiguities.
Second, we used ?exact?
matching.
In this case, we consider the result to be apositive match if and only if the proposed word class corresponds exactly to the assigned word class.By using this approach, we try to determine the degree of unambiguousness with which we canpropose a word class.
If a word is assigned a word class and the program suggests two word classes, ofwhich one corresponds to the assigned word class, we count this as a failure.Please note that, since it?s not always possible to distinguish clearly between nouns and adjectivesin Pali, we aggregated these word classes into one class.
To this class we also counted words tagged asordinal adjectives, since they are inflected like regular adjectives.The following tables illustrate our results:?is any?
matchingGeneration based HeuristicNoun-adjective-ordinalAdjective63.30% 99.96%Numeral 61.04% 76.62%Pronoun 82.75% 88.57%Verb 51.24% 63.37%As you can gather from the table, the performance of the word form generation based approach didnot match the performance of our heuristic approach in the first experiment.
Further investigationshowed that this is mainly due to the fact that not all necessary word forms encountered in thereference corpus could be generated.
There are several reasons for this: First, the exact ways togenerate word forms are not yet completely covered by literature and in some areas are still underresearch: e.g.
at least regarding verb forms, there is still ongoing research.
Second, our generationprocess was not able to handle irregular forms well because this information is not yet represented inthe dictionary.
This data will probably be entered by Pali experts next year.
Third, most of the formswe could not recognize are sandhi and other compound forms.
This is a task the generation processcannot handle well in general.
A heuristic approach does not encounter these problems.To better judge our algorithms, we therefore evaluated the results only for word forms that could beaddressed by these algorithms.
The following tables give an overview about these results:?is any?
matching (processable words)Generation based HeuristicNoun-adjective-ordinalAdjective97.31% 99.96%Numeral 81.03% 76.62%Pronoun 86.61% 88.57%Verb 76.25% 63.37%As you can see, on word forms that could be processed, both approaches work similarly well.With the current state of the dictionary, these results are as good as can be.
Please note that whilethe heuristic approach must be considered to be final the generation based approach will improve overtime as the dictionary will be improved by the Pali experts in the next years.Our ?exact?
evaluation operator revealed that word forms in the reference corpus that uniquelybelong to a single word class can be recognized much better by the generation based approach than bythe heuristic approach.
Interestingly, though we are still lacking information about irregular verbforms in the dictionary, we achieved up to 60.37% precision on verbs in exact word class recognition,while the heuristic approach surprisingly did not succeed very well.The approaches we took can surely be improved.
However, these approaches rely heavily on adictionary, which is more detailed and even more complete.
Pali experts will provide this data in thefuture but this is an ongoing process which will take a few years.727.4 Conclusion and Future WorkIn this paper we have addressed the task of extracting cross references and word class informationfrom dictionary entries in a Pali dictionary.
For this task as well as for future computer linguistic tasks,we have built an infrastructure suitable for data management and processing.
We have experiencedthat even if the individual articles are not written in a consistent and clear way, some information stillcan be extracted.
We therefore propose that similar approaches might be taken with dictionaries ofother dead languages as well in the future based on the technical infrastructure we created.We tried to complement our approach with taking the English translations, contained in most of thedictionary entries, into consideration.
Unfortunately this did not work well due to the nature of ourdata: Most of the dictionary entries do contain a discussion of a lemma in English, but as theindividual dictionary entries don?t follow a clearly defined structure and even discuss various relatedwords within these entries it turned out this approach is too incomplete and too error prone to beusable in practice.We found the processing environment to be of great help in order to shorten the time consumingmanual processing of data.
Three aspects we like to point out in this context: The concept of having anintegrated development environment that takes data management work off the shoulders of researchersand allows writing small units of code for processing turned out to aid in this process.
Furthermore thetransparency given by the system about processing details for every single word helps greatly to avoidmistakes and therefore saves time of researchers.Our experiment concerning word class recognition showed that the dictionary is essential.
While thedictionary data is still relatively incomplete, we were able to get good results.
Future work needs to bedone in this area, especially the correction of lemmas and part of speech tags in the future.
However,this is a future task that goes beyond the scope of this paper.A custom dictionary editor has been built that connects to the dictionary infrastructure at hand.
Withthis tool our Indological collegues intend to perform the unavoidable manual improvement in the nextyears.
If this process is completed at some point in the future we intend to address lemmatizing andpart of speech tagging again, something that can not yet been done to a fully satisfying extent rightnow.
Nonetheless, as our word class experiment showed, we were able to achieve good results despitethe problems encountered.
It is to be expected that with the improvement of the dictionary, the resultswill also improve in the future.ReferenceAlfter, David.
2014.
Morphological analyzer and generator for Pali.Critical P?li Dictionary.
Web.Collins, Steven.
2006.
A Pali grammar for students.
Chiang Mai: Silkworm Books.
Print.Geiger, Wilhelm.
1943.
A Pali Grammar.
Pali Text Society.
Print.Helwig, Oliver.
2009.
SanskritTagger, a stochastic lexical and POS tagger for Sanskrit.Stede, William and Davids , Rhys.
1997.
Pali-English Dictionary.
2nd ed, Motilal Banarsidass.
Print.Pali Text Society.
Web.Thera, N?rada.
1953.
An elementary P?
?i course.
2nd ed.
Colombo: Associated Newspapers of Ceylon.BuddhaNet eBooks.
Web.
N.d.73
