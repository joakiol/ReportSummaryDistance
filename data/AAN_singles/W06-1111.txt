Proceedings of the Workshop on Linguistic Distances, pages 82?90,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Measure of Aggregate Syntactic DistanceJohn Nerbonne and Wybo WiersmaAlfa-informatica, University of GroningenP.O.Box 716, NL 9700 AS Groningen, The Netherlandsj.nerbonne@rug.nl & wybo@logilogi.orgAbstractWe compare vectors containing counts oftrigrams of part-of-speech (POS) tags inorder to obtain an aggregate measure ofsyntax difference.
Since lexical syntacticcategories reflect more abstract syntax aswell, we argue that this procedure reflectsmore than just the basic syntactic cate-gories.
We tag the material automaticallyand analyze the frequency vectors for POStrigrams using a permutation test.
A testanalysis of a 305,000 word corpus con-taining the English of Finnish emigrantsto Australia is promising in that the proce-dure proposed works well in distinguish-ing two different groups (adult vs. childemigrants) and also in highlighting syntac-tic deviations between the two groups.1 IntroductionLanguage contact is a common phenomenonwhich may even be growing due to the increasedmobility of recent years.
It is also linguisticallysignificant, since contact effects are prominentin linguistic structure and well-recognized con-founders in the task of historical reconstruction.Nonetheless we seem to have no way of assayingthe aggregate affects of contacts, as Weinreich fa-mously noted:?No easy way of measuring or charac-terizing the total impact of one languageon another in the speech of bilingualshas been, or probably can be devised.The only possible procedure is to de-scribe the various forms of interferenceand to tabulate their frequency.?
(Wein-reich, 1953, p. 63)This paper proposes a technique for measuring theaggregate degree of syntactic difference betweentwo varieties.
We shall thus attempt to measurethe ?total impact?
in Weinreich?s sense, albeit withrespect to a single linguistic level, syntax.If such a measure could be developed, it wouldbe important not only in the study of language con-tact, but also in the study of second-language ac-quisition.
A numerical measure of syntactic dif-ference would enable these fields to look afresh atissues such as the time course of second-languageacquisition, the relative importance of factors in-fluencing the degree of difference such as themother tongue of the speakers, other languagesthey know, the length and time of their experiencein the second language, the role of formal instruc-tion, etc.
It would make the data of such studiesamenable to the more powerful statistical analysisreserved for numerical data.Naturally we want more than a measure whichsimply assigns a numerical value to the differencebetween two syntactic varieties: we want to beable to examine the sources of the difference bothin order to win confidence in the measure, but alsoto answer linguistic questions about the relativestability/volatility of syntactic structures.1.1 Related WorkThomason and Kaufmann (1988) and van Coet-sem (1988) noted, nearly simultaneously, that themost radical (structural) effects in language con-tact situations are to be found in the language ofSWITCHERS, i.e., in the language used as a secondor later language.
People MAINTAINING their lan-guage tend to adopt new lexical items from a con-tact language, but this only has structural conse-quences as the lexical items accumulate.
Thus wehear radically different English used in immigrant82communities in the English-speaking world, butthe natives in contact with these groups do not tendto modify their language a great deal.
This sug-gests that we should concentrate on those switch-ing as we begin to develop measures of aggregatedifference.Poplack and Sankoff (1984) introduced tech-niques for studying lexical borrowing and itsphonological effects, and and Poplack, Sankoffand Miller (1988) went on to exploit these ad-vances in order to investigate the social conditionsin which contact effects flourish best.We follow Aarts and Granger (1998) mostclosely, who suggest focusing on tag sequences inlearner corpora, just as we do.
We shall add totheir suggest a means of measuring the aggregatedifference between two varieties, and show howwe can test whether that difference is statisticallysignificant.2 Syntactic FootprintsIn this section we justify using frequency profilesof trigrams of part-of-speech (POS) categories asindicators of syntactic differences.
We shall firstautomatically tag second-language speakers?
cor-pora with syntactic categories:Oh that ?s a just aINT PRON COP ART EXCL ARTfun in a ?
HelsinkiN-COM PREP ART PAUSE N-PROPWe then collect these into overlapping triples (tri-grams).
The tag-trigrams include triples such asINT-PRON-COP and PRON-COP-ART.We consider three possible objections to pro-ceeding this way.
First, one might object that un-igrams, bigrams, also should be compared.
Weare in fact sympathetic to the criticism that n-grams for n 6= 3 should also be compared, atleast with an eye toward refining the technique,and we have performed experiments with bigramsand with combinations of n-grams for larger n, butwe restrict the discussion here to trigrams in or-der to simplify presentation.
Second, our choiceof part-of-speech categories may bias the results,since other research might use other POS cate-gories, and third, that POS trigrams do not reflectsyntax completely.
We first develop these last twoobjections further, and then explain why it is rea-sonable to proceed this way.Ideally we should like to have at our disposalthe syntactic equivalent of an international pho-netic alphabet (IPA, 1949), i.e.
an accepted meansof noting (an interesting level of) syntactic struc-ture for which there was reasonable scientific con-sensus.
But no such system exists.
Moreover,the ideal system would necessarily reflect the hi-erarchical structure of dependency found in allcontemporary theories of syntax, whether directlybased on dependencies or indirectly reflected inconstituent structure.
Since it is unlikely that re-searchers will take the time to hand-annotate largeamounts of data, meaning we shall need automat-ically annotated data, this leads to a second prob-lem, viz., that our parsers, the automatic data an-notators capable of full annotation, are not yet ro-bust enough for this task.
(Even the best scoreonly about 90% per constituent on edited news-paper prose.
)We have no solution to the problem of the miss-ing consensual annotation system, but we wishto press on, since it will be sufficient if we canprovide a measure which correlates strongly withsyntactic differences.
We note that natural lan-guage processing work on tagging has compareddifferent tag sets, noting primarily the obvious,that larger sets result in lower accuracy (Manningand Schu?tze, 1999, 372ff.).
Since we aim hereto contribute to the study of language contact andsecond-language learning, we shall choose a lin-guistically sensitive set, that is, a large set de-signed by linguists.
We have not experimentedwith different tagsets.With regard to the second objection, the factthat syntax concerns more than POS trigrams, wewish to deny that this is a genuine problem forthe development of a measure of difference.
Wenote that our situation in measuring syntactic dif-ferences is similar to other situations in which ef-fective measures have been established.
For ex-ample, even though researchers in first languageacquisition are very aware that syntactic devel-opment is reflected in the number of categories,and rules and/or constructions used, the degreeto which principles of agreement and governmentare respected, the fidelity to adult word order pat-terns, etc., still they are in large agreement thatthe very simple MEAN LENGTH OF UTTERANCE(MLU) is an excellent measure of syntactic matu-rity (Ritchie and Bhatia, 1998).
Similarly, life ex-pectancy and infant mortality rates are consideredreliable indications of health when large popula-tions are compared.
We therefore continue, pos-tulating that the measure we propose will corre-83late with syntactic differences as a whole, even ifit does not measure them directly.In fact we can be rather optimistic about us-ing POS trigrams given the consensus in syntac-tic theory that a great deal of hierarchical struc-ture is predictable given the knowledge of lexicalcategories, in particular given the lexical HEAD.Sells (1982, ??
2.2, 5.3, 4.1) demonstrates thatthis was common to theories in the 1980?s (Gov-ernment and Binding theory, Generalized PhraseStructure Grammar, and Lexical Function Gram-mar), and the situation has changed little in thesuccessor theories (Minimalism and Head-DrivenPhrase Structure Grammar).
There is, on the otherhand, consensus that the very strict lexicalismwhich Sells?s work sketched must be relaxed infavor of ?constructionalism?
(Fillmore and Kay,1999), but even in such theories syntactic headshave a privileged, albeit less dominant status.1Let us further note that the focus on POS tri-grams is poised to identify not only deviant syn-tactic uses, such as the one given as an exam-ple above, but also overuse and under-use of lin-guistic structure, whose importance is empha-sized by researchers on second-language acquisi-tion (Coseriu, 1970), (de Bot et al, 2005, A3,B3).According to these experts it is misleading toconsider only errors, as second language learn-ers likewise tend to overuse certain possibilitiesand tend to avoid (and therefore underuse) oth-ers.
For example, Bot et al (2005) suggest thatnon-transparent constructions are systematicallyavoided even by very good second-language learn-ers).2.1 TaggingWe tagged the material using Thorsten Brants?sTrigrams ?n Tags (TnT) tagger, a hidden Markovmodel tagger which has performed at state-of-the-art levels in organized comparisons, achieving96.7% correct on the material of the Penn Tree-bank (Brants, 2000).Since our material is spoken English (see be-low), we trained the tagger on the spoken part ofthe International Corpus of English (ICE) fromGreat Britain, which consists of 500k words.
Thiswas suboptimal, as the material we wished to ana-lyze was the English of Finnish emigrants to Aus-tralia, but we were unable to acquire sufficient1One referee suggested that one might test the associationbetween POS trigram differences and head differences exper-imentally, and we find this suggestion sensible.Australian material.We used the tagset of the TOSCA-ICE consist-ing of 270 tags (Garside et al, 1997), of which 75were never instantiated in our material.
In a sam-ple of 1, 000 words we found that the tagger wascorrect for 87% of words, 74% of the bigrams, and65% of the trigrams.
As will be obvious in thepresentation of the material (below), it is free con-versation with pervasive foreign influence.
We at-tribute the low tagging accuracy to the roughnessof the material.
It is clear that our procedure wouldimprove in accuracy from a more accurate tagger,which would, in turn, allow application to smallercorpora.We collect the material into a frequency vectorcontaining the counts of 13, 784 different POS tri-grams, one vector for each of the two sub-corporawhich we describe below.
We then ask whetherthe material in the one sub-corpus differs signifi-cantly from that in the other.
We turn now to thattopic.3 Permutation TestsThere is no convenient test we can apply to checkwhether the differences between vectors contain-ing 13, 784 elements are statistically significant,nor how significant the differences are.
Fortu-nately, we may turn to permutation tests in thissituation (Good, 1995), more specifically a per-mutation test using a Monte Carlo technique.Kessler (2001) contains an informal introductionfor an application within linguistics.The fundamental idea in a permutation test isvery simple: we measure the difference betweentwo sets in some convenient fashion, obtaining?(A,B).
We then extract two sets at randomfrom A ?
B, calling these A1, B1, and we calcu-late the difference between these two in the samefashion, ?
(A1, B1), recording the number of times?
(A1, B1) ?
?
(A,B), i.e., how often two ran-domly selected subsets from the entire set of ob-servations are at least as different as (usually moredifferent than) the original sets were.
If we repeatthis process, say, 10, 000 times, then n, the numberof times we obtain more extreme differences, al-lows us to calculate how strongly the original twosets differ from a chance division with respect to?.
In that case we may conclude that if the twosets were not genuinely different, then the origi-nal division into A and B was likely to the degreeof p = n/10, 000.
In more standard hypothesis-84testing terms, this is the p-value with which wemay reject (or retain) the null hypothesis that thereis no relevant difference in the two sets.We would like to guard against three dangers inour calculations.
First, given the ease with whichlarge corpora are obtained, we are uninterestedin obtaining statistical significance through sheercorpus size.
We aim therefore at obtaining a mea-sure that is sensitive only to relative frequency, andnot at all to absolute frequency (Agresti, 1996).Permutation tests effectively guard against thisdanger, if one takes care to judge samples of thesame size within the permutations.Second, we are mindful of a potential confound-ing factor, viz., the syntactical intra-dependencefound within sentences (especially between ad-joining POS trigrams).
If we permuted n-grams,we might in part measure the internal coherence ofthe two initial sub-corpora, i.e., the coherence dueto the fact that both sub-corpora use language con-forming to the rules of English syntax.
If we per-muted n-grams, this coherence would be lost, andthe measurement of difference would be affected.In the terminology of permutation statistics: theelements that are permuted must be reasonably in-dependent.
So we shall permute not n-grams, butrather entire sentences.Third, the decision to permute sentences ratherthan n-grams exposes us to a confound due to sys-tematically different sentence lengths.
While theresult of permuting elements in a Monte Carlofashion always results in two sub-corpora thathave the same number of elements as in the base-case, our problem is that the elements we per-mute are sentences, while what we measure aren-grams.
Now if the original two sub-corpora dif-fer substantially in average sentence length, thenthe result of the Monte Carlo ?shuffling?
will notbe similar to the original split with respect to thenumber of n-grams involved.
The original sub-corpus with longer sentences will therefore havemany more n-grams in the base-case than in therandom re-drawings from the combining corpora,at least on average.
We address this danger sys-tematically in the subsection below on within-permutation normalizations (?
3.2).We note a more subtle dependency we do notattempt to guard against.
Some POS sequences(almost) only occur in relatively long sentences,e.g.
the inversion that occurs in some condition-als Were I in any doubt, I should not .... PerhapsEnglish subjunctives in general occur only in rel-atively long sentences.
If this sort of structureoccurs in one variety more frequently than in an-other, that is a genuine difference, but it might stillbe the reflection of the simpler difference in sen-tence length.
One might then think that the secondvariety would show the same syntax if only it hadlonger sentences.
As far as they are to be con-sidered a problem in the first place, differences insyntax that are related to sentence length cannotbe removed by (our) normalizations.Permutation tests are a very suitable tool forfinding significant syntactical differences, and forfinding the POS trigrams that make a significantcontribution to this difference.3.1 Measuring Vector DifferencesThe choice of vector difference measure, e.g.
co-sine vs. ?2, does not affect the proposed tech-nique greatly, and alternative measures can beused straightforwardly.
Accordingly, we haveworked with both cosine and two measures in-spired by the RECURRENCE (R) metric introducedby Kessler (Kessler, 2001, 157ff).
FollowingKessler, we also call our measures R and Rsq.The advantage of the R and Rsq metrics is thatthey are transparently interpretable as simple ag-gregates, meaning that one may easily see howmuch each trigram contributes to the overall cor-pus difference.
We even used them to calculate aseparate p-value per trigram.Our R is calculated as the sum of the differ-ences of each cell with respect to the average forthat cell.
If we have collected our data into twovectors (c, c?
), and if i is the index of a POS tri-gram,R for each of these two vector cells is equal,as it is defined simply as R =?i |ci ?
ci|, withci = (ci + c?i)/2.
The Rsq measure attributesmore weight to a few large differences than tomany small ones, and it is calculated: Rsq =?i(ci?
ci)2, with ci being the same as above (forR).3.2 Within-Permutation NormalizationEach measurement of difference?whether thedifference is between the original two samplesor between two samples which arise throughpermutations?is taken over the collection of POStrigram frequencies once these have been normal-ized.
We describe first the normalization that is re-quired to cope with differences in sentence length85which we call WITHIN-PERMUTATION NORMAL-IZATION, as it is applied within each permutation.In case sub-corpora differ in sentence length,they will automatically differ in the number of n-grams across permutations as well.
Our MonteCarlo choice of alternatives does not change therelative number of sentences across permutations,but the number of POS trigrams in the groups willvary if no normalization is applied.
Longer sen-tences give rise to larger numbers of POS trigramsper sentence, and therefore per sub-corpora.
Ap-plying the within-permutation normalization oneor more times ensures that this does not infect themeasurement of difference.Protecting the measurement from sensitivity todiffering numbers of POS trigrams per sentenceis for us sufficient reason to normalize, but wealso normalize in order to facilitate interpretation.We return to this below, in the definition of therescaled vectors sy, so.We thus collect from the tagger a sequence ofcounts ci of tag trigrams for each sample.
Wetreat only the case of comparing two samples here,which we shall refer to as young (y) and old (o) forreasons which will become clear in the followingsection.
We shall keep track of the sum-per-tag tri-gram as well, summing over the two sub-corpora.cy = < cy1, cy2, ..., cyn > Ny =?ni=1 cyi+co = < co1, co2, ..., con > No =?ni=1 coic = < c1, c2, ..., cn > N(= Ny +No)=?ni=1 ciAs a first step in normalization, we work withvectors holding the relative frequency fractionsper group:fy = < ..., fyi (= cyi /Ny), ... >fo = < ..., foi (= coi /No), ... >We note that?ni=1 fyi =?ni=1 foi = 1.We then compute the relative proportions pertrigram, comparing now across the groups.
Thisprepares for the step which redistributes the rawtrigram counts to compensate for differences insentence length.py = < ..., pyi (= fyi /(fyi + foi )), ... >po = < ..., poi (= foi /(fyi + foi )), ... >We might also define a sum of py + po:p = < ..., pi(= (poi + pyi ) = 1), ... >We do not actually use p below, only py and po,but we mention it for the sake of the check it al-lows that pyi + poi = 1,?i.We then re-introduce the raw frequencies percategory to obtain the normalized, redistributedcounts Cyn,Con.
Note that we use the total countof the trigram in both samples to redistribute (thusredistributing these counts based on the trigram to-tals in both samples):Cyn = < ..., pyi ?
ci, ... >Con = < ..., poi ?
ci, ... >Up to this point the normalization has correctedfor differences in sentence length, or to be moreprecise, for differences in the numbers of n-gramswhich may appear as a result of permuting sen-tences.
For larger numbers of trigrams the situ-ation will become: Ny =?ni=1 cyi ?
?ni=1 Cyiso that we have effectively neutralized the in-crease or decrease in the number of n-grams whichmight have arisen due to sentence length.
With-out this normalization a skew in sentence lengthin the base case would cause changed, in theworst case increased, and perhaps even extreme,significance.
During random permutation, wherelonger sentences will tend to be distributed moreevenly between the sub-corpora, a disproportion-ately larger number of n-grams would be found inthe sub-corpus corresponding to the base corpuswith shorter sentences.
We have now normalizedso that that effect will no longer appear.We illustrate the normalizations up to this pointin Table 1.
We see already that the overall effectis to shift mass to the smaller sample.
Notice alsothat if we were to define C = Cy + Co, thenC = c, since Cy and Co are a redistribution ofc using py and po, whose sum p is 1 under all cir-cumstances, as was noted above.
At the same timecy 6= Cy and co 6= Co (if there were differencesin sentence lengths).
The values obtained at thispoint may be measured by the vector comparisonmeasure (cosine or R(sq)).We use this redistributing normalization insteadof just the relative frequency because using rel-ative frequency would cause trigrams occurringmainly and frequently in the short-sentence groupto become extremely significant.
This is especially86Group y Group o Group y?
Group o?T1 T2 T1 T2 T1 T2 T1 T2counts c 15 10 90 10 10 10 17 0rel.
freq.
f 0.6 0.4 0.9 0.1 0.5 0.5 1 0norm.
prop.
p 0.4 0.8 0.6 0.2 0.33 1 0.67 0trigram ci 105 20 105 20 27 10 27 10redistrib.
C 42 16 63 4 9 10 18 0Table 1: Two examples of the normalizations applied before each measurement of vector difference.On the left groups y and o are compared on the basis of the two trigrams T1 and T2.
The counts areshown in the first row, then relative frequencies (within the group), normalized relative proportions, andfinally redistributed normalized counts.
The two numbers in boldface in the ?count?
line are comparedto calculate the underlined relative frequency (on the left) in the ?relative frequency?
line (in generalcounts are compared within groups to obtain relative frequencies).
Next, the two underlined fractions ofthe ?relative frequency?
row are compared to obtain the corresponding fractions (immediately below) ofthe ?normalized proportions?
row.
Thus relative frequencies are compared across groups (sub-corpora)to obtain the relative proportions.
The trigram count row shows the counts per trigram type, and the?redistributed?
row is simply the product of the last two.
The second example (on the right) demonstratesthat missing data finds no compensation in this procedure (although we might experiment with smoothingin the future).distorting if one calculates the per trigram type p-value (R or Rsq for a single i).The normalization does not eliminate all the ir-relevant effects of differing sentence lengths.
Toobtain further precision we iterate the steps abovea few times, re-applying the normalization to itsown output.
We are motivated to iterate the pro-cedure for the following reason.
If a trigram isrelatively more frequent in the smaller sub-corpus,it must then also be relatively less frequent withinthe entire corpus (less frequent within the two sub-corpora together), so there is less frequency massto re-distribute for these trigrams than for trigramsthat are relatively more frequent in the larger sub-corpus (those will be more frequent within the en-tire corpus).
A special case of this are n-gramsthat occur only in one sub-corpus.
If they oc-cur only in the larger sub-corpus then their masswill never be re-distributed in the direction of thesmaller sub-corpus, since zero-frequencies withinone sub-corpus will always result in zero relativeweight (in the current set-up).2 This means that af-ter normalization the larger sub-corpus will alwaysstill be a bit larger than the smaller one.
After onenormalization the effect of these factors is small,but we can reduce it yet further by iterating thenormalization.
This is worthwhile since we wish2Alternatively, we might have explored a Good-Turingestimation of unseen items (Manning and Schu?tze, 1999,p.
212).to be certain.
After five iterations the relative size-difference between our normalized sub-corpora isless than 0.1% for trigrams of the full ICE-tagset(and even a thousand times smaller for the reducedtagset).
We regard this as small enough to effec-tively eliminate corpus size differences as poten-tial problems.For the purposes of interpretation we also scaleeverything down so that the average redistributedcount is 1.
We do this by dividing each Cyi , Coi byN/2n, where N is the total count of all trigramsand n is the number of trigram categories beingcounted.
Note that N/2n is the average count of agiven trigram in one of the groups.sy = cy ?
2n/N = < ..., Cyi ?
2n/N, ... >so = co ?
2n/N = < ..., Coi ?
2n/N, ... >These values might just as well be submitted to thevector comparison measure since they are just lin-ear transformations of the redistributed C values.The scaling expresses the trigram count as a valuewith respect to the total 2n of counts involved inthe comparison, and, since?ni=1 cyi +?ni=1 coi =N ,?ni=1 syi +?ni=1 soi = 2n.
As there are n sortsof trigrams being compared in two groups, it isclear that the average value in these last vectorswill be 1.Similarly, this normalized value will be higherthan 1 for trigrams that are more frequent than av-erage.
Now if we sort the trigrams by frequency?87or more precisely, by the weight that they havewithin the total R(sq) value, so by their per tri-gram R(sq) value?we get a listing of the POStrigrams that distinguish the groups most sharply.This list can be made even more telling by addingthe raw frequency and a per-trigram p-value.
It al-lows us to directly see significant under and over-use of POS trigrams, and thereby of syntax.3.3 Between-Permutations NormalizationThe purpose of this normalization is the identifica-tion of n-gram types which are typical in the twooriginal sub-corpora.
It is applied after comparingall the results of all the Monte Carlo re-shufflings.The BETWEEN-PERMUTATIONS NORMALIZA-TION is similar to the last step of the within-permutation normalization, except that the lineartransformation is applied across permutations, in-stead of across groups (sub-corpora): for eachPOS trigram type i in each group (sub-corpora)g ?
{o, y}, the redistributed count Cgi is dividedby the average redistributed count for that type inthat group (across all permutations)Cgi .
Note thatthe average redistributed count is ci/2 for largenumbers of permutations.
The values thus normal-ized will be 1 on average across permutations.Trigrams with large average counts betweenpermutations are those with high frequencies inthe original sub-corpora, and these contribute mostheavily toward statistical significance.
The nor-malization under discussion strips away the roleof frequency, allowing us to see which POS tri-grams are most (a)typical for a group.
We noteadditionally that this normalization is useful onlytogether with information on frequency (or statis-tical significance).
Infrequent trigrams are espe-cially likely to have high values with respect toCgi .
For example a trigram occurring only once,in one sub-corpus, gets the maximum value of1/0.5 = 2 (as it is indeed very typical for thissub-corpus), while with a count of one it clearlycannot be statistically significant (moving betweenequally sized sub-corpora with a chance of 50 %during permutations).
So it?s best to calculatethis normalization together with the per trigram p-values.4 A Test CaseWe tested this procedure on data transcribed fromfree interviews with Finnish emigrants to Aus-tralia.
The emigrants were farmers and skilled orsemi-skilled working class Finns who left Finlandin the 1960?s at the age of 25-40 years old, somewith children.
Greg Watson of Joensuu Univer-sity interviewed these people between 1995 and1998, publishing about his corpus in ICAME 20,1996 (Watson).
He included both interviews withthose who emigrated as adults (at seventeen yearsor older) and those who emigrated as children (be-fore their seventeenth birthday).
There are sixtyconversations with adult-age emigrants and thirtywith those who emigrated as children, totaling305,000 words of relatively free conversation.It is well established in the literature on second-language learning that the language of people wholearned the second language as children is supe-rior to that of adult learners.
We will test our ideaabout measuring syntactic differences by apply-ing the measure to the two samples language fromadult vs. child emigrants.
The issue is not remark-able, but it allows us to verify whether the measureis functioning.4.1 ResultsThe two sub-corpora had 221,000 words for theolder group and 84,000 words for the youngergroup, respectively.
The sentences of the child-hood immigrants were indeed substantially longer(27.1 tokens) than those of the older immigrants(16.3 tokens).
So the within-permutation normal-ization was definitely needed in this case.
Thegroups clearly differed in the distribution of POStrigrams they contain (p < 0.001).
This meansthat the difference between the original two sub-corpora was in the largest 0.1% of the Monte Carlopermutations.In addition we find genuinely deviant syntaxpatterns if we inspect the trigrams most respon-sible for the difference between the two sub-corpora.it ?s low tax in herePRO COP ADJ N/COM PREP ADVand I was professional fishermanCONJ PRO COP ADJ N/COMBoth COP-ADJ-N/COM and N/COM-PREP-ADV accounted for a substantial degree of aggre-gate syntactic difference.
The first pattern nor-mally corresponds to an error, as it does in thetwo (!)
examples of it above (there is a sepa-rate tag for plural and mass nouns).
These arecases where English normally requires an article.88Since Finnish has no articles, these are clear casesof transfer, i.e., the (incorrect) imposition of thefirst language?s structure on a second language.The N/COM-PREP-ADV pattern (correspondingto the use of in here) is also worth noting, as itfalls into the class of expressions which is not ab-solutely in error (The material is in here), but itis clearly being overused in the example above.Presumably this is a case of hypercorrection fromFinnish, a language without prepositions.
We con-clude from this experiment that the procedure ispromising.On the other hand there were also problems,perhaps most seriously with the use of the tagsdenoting pauses and hesitations, where we foundthat the tag trigrams most responsible for the de-viant measures in the corpora involved disfluen-cies of one sort or another.
These tended to occurmore frequently in the speech of the older emi-grants.
With the pauses removed (hesitations stillin place) a list of the ten most frequent, significanttrigrams for the older group is shown.
Two ran-dom examples from the corpus are given for eachin Table 2.We suspect additionally that the low accuracyrate of the tagger when applied to this material alsostems from the large number of disfluencies.5 Conclusions and ProspectsWeinreich (1953) regretted that there was no wayto ?measure or characterize the total impact onelanguage on another in the speech of bilinguals,?(p.
63) and speculated that there could not be.
Thispaper has proposed a way of going beyond countsof individual phenomena to a measure of aggre-gate syntactic difference.
The technique may beimplemented effectively, and its results are subjectto statistical analysis using permutation statistics.The technique proposed follow Aarts andGranger (1998) in using part-of-speech trigrams.We argue that such lexical categories are likelyto reflect a great deal of syntactic structure giventhe tenets of linguistic theory according to whichmore abstract structure is, in general, projectedfrom lexical categories.
We go beyond Aarts andGranger in Showing how entire histograms of POStrigrams may be used to characterize aggregatesyntactic distance, in particular by showing howthis can be analyzed.We fall short of Weinreich?s goal of assaying?total impact?
in that we focus on syntax, but we1 roadworks and uhhill and ahN CONJUNC INTERJEC2 I reckon itthat take lotPRON V PRON3 enjoy to takingmy machine breakINTERJEC PRON V4 but that ?sthat I cleanCONJUNC PRON V5 I ?m uhit ?s uhPRON V INTERJEC6 now what whatchanging but someCONJUNC INTERJEC PRON7 said it ?sall everybody hasPRON PRON V8 bought that carlead glass windowsV PRON N9 that was differentI was fitPRON V ADJ10 Oh lake lakeuh money productionINTERJEC N NTable 2: The most significant and most frequenttrigrams that were typical for the speech of thegroup of older Finnish emigrants to Australia com-pared to the speech of those who emigrated beforetheir 17th birthday.
The tag trigrams indicatingpauses were removed before comparing the cor-pora, as these appear to dominate the differences.The examples illustrating the trigrams were cho-sen at random, and we note that the examples ofthe third sort of trigram involved tagging errors inthe first and second elements of the trigram, andthat other errors are noticeable at the seventh andeight positions in the list (where ?said?
and ?glass?are marked as pronouns).
We reserve the linguisticinterpretation of the error patterns for future work,but we note that we will also want to filter inter-jections before drawing definite conclusions.89take a large step in this direction by showing howto aggregate and test for significance, using thesorts of counts he worked with.The software implementing the permutationtest, including the normalizations, is avail-able freely at http://en.logilogi.org/HomE/WyboWiersma/FiAuImEnRe.
It is de-veloped to allow easy generalization to more thantwo sub-corpora and longer n-grams.Several further steps would be useful.
Weshould like to repeat the analysis here, eliminat-ing the effect of hesitation tags, etc.
Second,we should like to experiment systematically withthe inclusion of n-grams for n > 3; to-date wehave experimented with this, but not systemati-cally enough.
Third, we would like to test theanalysis on other cases of putative syntactic dif-ferences, and in particular in cases where taggingaccuracy might be less an issue.AcknowledgmentsWe are grateful to Lisa Lena Opas-Ha?nninen,Pekka Hirvonen and Timo Lauttamus of the Uni-versity of Oulu, who made the data available andconsulted extensively on its analysis.
We alsothank audiences at the 2005 TaBu Dag, Gronin-gen; at the Workshop Finno-Ugric Languages inContact with English II held in conjunction withMethods in Dialectology XII at the Universite?
deMoncton, Aug. 2005; the Sonderforschungsbere-ich 441, ?Linguistic Data Structures?, Tu?bingenin Jan. 2006; and the Seminar on Methodologyand Statistics in Linguistic Research, University ofGroningen, Spring, 2006, and especially Livi Ruf-fle, for useful comments and discussion.
Finally,two referees for the 2006 ACL/COLING work-shop on ?Linguistic Distances?
also commentedusefully.ReferencesJan Aarts and Sylviane Granger.
1998.
Tag sequencesin learner corpora: A key to interlanguage grammarand discourse.
In Sylviane Granger, editor, LearnerEnglish on Computer, pages 132?141.
Longman,London.Alan Agresti.
1996.
An Introduction to CategoricalData Analysis.
Wiley, New York.Thorsten Brants.
2000.
TnT ?
a statistical partof speech tagger.
In 6th Applied Natural Lan-guage Processing Conference, pages 224?231, Seat-tle.
ACL.Eugenio Coseriu.
1970.
Probleme der kontrastivenGrammatik.
Schwann, Du?sseldorf.Kees de Bot, Wander Lowie, and Marjolijn Verspoor.2005.
Second Language Acquisition: An AdvancedResource Book.
Routledge, London.Charles Fillmore and Paul Kay.
1999.
Grammati-cal constructions and linguistic generalizations: thewhat?s x doing y construction.
Language, 75(1):1?33.Roger Garside, Geoffrey Leech, and Tony McEmery.1997.
Corpus Annotation: Linguistic Informa-tion from Computer Text Corpora.
Longman, Lon-don/New York.Phillip Good.
1995.
Permutation Tests.
Springer, NewYork.
2nd, corr.
ed.1949.
The Principles of the International Phonetic As-sociation.
International Phonetics Association, Lon-don, 1949.Brett Kessler.
2001.
The Significance of Word Lists.CSLI Press, Stanford.Chris Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press, Cambridge.Shana Poplack and David Sankoff.
1984.
Borrowing:the synchrony of integration.
Linguistics, 22:99?135.Shana Poplack, David Sankoff, and Christopher Miller.1988.
The social correlates and linguistic processesof lexical borrowing and assimilation.
Linguistics,26:47?104.William C. Ritchie and Tej K. Bhatia, editors.
1998.Handbook of Child Language Acquisition.
Aca-demic, San Diego.Peter Sells.
1982.
Lectures on Contemporary SyntacticTheories.
CSLI, Stanford.Sarah Thomason and Terrence Kaufmann.
1988.
Lan-guage Contact, Creolization, and Genetic Linguis-tics.
University of California Press, Berkeley.Frans van Coetsem.
1988.
Loan Phonology and theTwo Transfer Types in Language Contact.
Publica-tions in Language Sciences.
Foris Publications, Dor-drecht.Greg Watson.
1996.
The Finnish-Australian Englishcorpus.
ICAME Journal: Computers in English Lin-guistics, 20:41?70.Uriel Weinreich.
1953.
Languages in Contact.
Mou-ton, The Hague.
(page numbers from 2nd ed.
1968).90
