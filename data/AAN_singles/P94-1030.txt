GENERAL IZED CHART ALGORITHM:AN EFF IC IENT PROCEDURE FORCOST-BASED ABDUCTIONYasuharu  DenATR Interpret ing Telecommunicat ions Research Laboratories2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto  619-02, JAPANTel: +81-7749-5-1328, Fax: +81-7749-5-1308, e-mail: denQit l .atr .co.
jpAbst rac tWe present an efficient procedure for cost-based ab-duction, which is based on the idea of using chartparsers as proof procedures.
We discuss in de-tail three features of our algorithm - -  goal-drivenbottom-up derivation, tabulation of the partial re-sults, and agenda control mechanism - -  and reportthe results of the preliminary experiments, whichshow how these features improve the computationalefficiency of cost-based abduction.IntroductionSpoken language understanding is one of the mostchallenging research areas in natural language pro-cessing.
Since spoken language isincomplete in var-ious ways, i.e., containing speech errors, ellipsis,metonymy, etc., spoken language understandingsystems hould have the ability to process incom-plete inputs by hypothesizing the underlying infor-mation.
The abduction-based approach (Hobbs etal., 1988) has provided asimple and elegant way torealize such a task.Consider the following 3apanese sentence:(1) Sfseki kat- ta(a famous writer) buy PASTThis sentence contains two typical phenomena aris-ing in spoken language, i.e., metonymy and the el-lipsis of a particle.
When this sentence is utteredunder the situation where the speaker eports hisexperience, its natural interpretation is the speakerbought a SSseki novel.
To derive this interpreta-tion, we need to resolve the following problems:?
The metonymy implied by the noun phraseS6seki is expanded to a S6seki novel, based onthe pragmatic knowledge that the name of awriter is sometimes used to refer to his novel.?
The particle-less thematic relation between theverb katta and the noun phrase SSseki is deter-mined to be the object case relation, based on thesemantic knowledge that the object case relationbetween a trading action and a commodity canbe linguistically expressed as a thematic relation.This interpretation is made by abduction.
Forinstance, the above semantic knowledge is stated,in terms of the predicate logic, as follows:(2) sem(e,x) C trade(e) A commodity(x) A obj(e,x)Then, the inference process derives the consequentsem(e,x) by hypothesizing an antecedent obj(e,x),which is never proved from the observed facts.
Thisprocess is called abduction.Of course, there may be several other possibili-ties that support the thematic relation sem(e,x).For instance, the thematic relation being deter-mined to be the agent case relation, sentence (1)can have another interpretation, i.e., Sfseki boughtsomething, which, under some other situations,might be more feasible than the first interpretation.To cope with feasibility, the abduction-based modelusually manages the mechanism for evaluating thegoodness of the interpretation.
This is known ascost-based abduction (Hobbs et al, 1988).In cost-based abduction, each assumptionbears a certain cost.
For instance, the assump-tion obj(e,x), introduced by applying rule (2), isspecified to have a cost of, say, $2.
The goodness ofthe interpretation is evaluated by accumulating thecosts of all the assumptions involved.
The wholeprocess of interpreting an utterance is depicted inthe following schema:1.
Find all possible interpretations, and2.
Select the one that has the lowest cost.In our example, the interpretation that as-sumes the thematic relation to be the object caserelation, with the metonymy being expanded toa S6seki novel, is cheaper than the interpretationthat assumes the thematic relation to be the agentcase relation; hence, the former is selected.An apparent problem here is the high compu-tational cost; because abduction allows many pos-sibilities, the schema involves very heavy compu-tation.
Particularly in the spoken language under-standing task, we need to consider a great numberof possibilities when hypothesizing various underly-ing information.
This makes the abduction process218computationally demanding, and reduces the prac-ticality of abduction-based systems.
The existingmodels do not provide any basic solution to thisproblem.
Charniak (Charniak and Husain, 1991;Charniak and Santos Jr., 1992) dealt with the prob-lem, but those solutions are applicable only to thepropositional case, where the search space is rep-resented as a directed graph over ground formulas.In other words, they did not provide a way to buildsuch graphs from rules, which, in general, containvariables and can be recursive.This paper provides a basic and practical so-lution to the computation problem of cost-basedabduction.
The basic idea comes from the naturallanguage parsing literature.
As Pereira and War-ren (1983) pointed out, there is a strong connec-tion between parsing and deduction.
They showedthat parsing of DCG can be seen as a special caseof deduction of Horn clauses; conversely, deductioncan be seen as a generalization of parsing.
Theiridea of using chart parsers as deductive-proof pro-cedures can easily be extended to the idea of usingchart parsers as abductive-proof procedures.
Be-cause chart parsers have many advantages from theviewpoint of computational efficiency, chart-basedabductive-proof procedures are expected to nicelysolve the computation problem.
Our algorithm,proposed in this paper, has the following features,which considerably enhance the computational ef-ficiency of cost-based abduction:1.
Goal-driven bottom-up derivation, which reducesthe search space.2.
Tabulation of the partial results, which avoids therecomputation of the same goal.3.
Agenda control mechanism, which realizes var-ious search strategies to find the best solutionefficiently.The rest of the paper is organized as follows.First, we explain the basic idea of our algorithm,and then present he details of the algorithm alongwith simple examples.
Next, we report the resultsof the preliminary experiments, which clearly showhow the above features of our algorithm improvethe computational efficiency.
Then, we compareour algorithm with Pereira and Warren's algorithm,and finally conclude the paper.Head-driven DerivationPereira and Warren showed that chart parserscan be used as proof procedures; they presented theEarley deduction proof procedure, that is a gener-alization of top-down chart parsers.
However, theymentioned only top-down chart parsers, which isnot always very efficient compared to bottom-up(left-corner) chart parsers.
It seems that using left-corner parsers as proof procedures i not so easy,...........':"'"'"Figure 1: Concept of Head-driven Derivationunless the rules given to the provers have a certainproperty.
Here, we describe under what conditionsleft-corner parsers can be used as proof procedures.Let us begin with the general problems of Hornclause deduction with naive top-down and bottom-up derivations:?
Deduction with top-down derivation is affectedby the frequent backtracking necessitated by theinadequate selection of rules to be applied.?
Deduction with bottom-up derivation is affectedby the extensive vacuous computation, whichnever contributes to the proof of the initial goal.These are similar to the problems that typi-cally arise in natural language parsing with naivetop-down and bottom-up arsers.
In natural lan-guage parsing, these problems are resolved by intro-ducing a more sophisticated derivation mechanism,i.e., left-corner parsing.
We have attempted to ap-ply such a sophisticated mechanism to deduction.Suppose that the proof of a goal g(x,y) canbe represented in the manner in Figure 1; the firstargument x of the goal g(x,y) is shared by all theformulas along the path from the goal g(z,y) tothe left corner am(z,zm).
In such a case, we canthink of a derivation process that is similar to left-corner parsing.
We call this derivation head-drivenderivation, which is depicted as follows:Step 1 Find a fact a(w,z) whose first argumentw unifies with the first argument x of the goalg(x,y), and place it on the left corner.Step 2 Find a rule am-l(W,Zrn-l) C a(W,Zm)/~BZ ^ ... A Bn whose leftmost antecedenta(W,Zm) unifies with the left-corner key a(x,z),and introduce the new goals B1, ..., and Bn.
Ifall these goals are recursively derived, then cre-ate the consequent a,,~_ 1( z ,zm_ 1 ), which domi-nates a(x,zm), B1, ..., and Bn, and place it onthe left corner instead of a(x,z).S tep3  If the consequent am-l (x ,zm_l)  unifieswith the goal g(z,y), then finish the pro-cess.
Otherwise, go back to s tep2  witham- 1 (x,zm_l) being the new left-corner key.219Left-corner parsing of DCG is just a specialcase of head-driven derivation, in which the in-put string is shared along the left border, i.e., thepath from a nonterminal to the leftmost word inthe string that is dominated by that nonterminal.Also, semantic-head-driven generation (Shieber elal., 1989; van Noord, 1990) and head-corner pars-ing ivan Noord, 1991; Sikkel and op den Akker,1993) can be seen as head-driven derivation, whenthe semantic-head/syntactic-head is moved to theleftmost position in the body of each rule and theargument representing the semantic-feature/head-feature is moved to the first position in the argu-ment list of each formula.To apply the above procedures, all rules mustbe in chain form arn--l(W,Zrn-~) C arn(W,Zm) A B1A ... A Bn; that is, in every rule, the first argu-ment of the leftmost antecedent must be equal tothe first argument of the consequent.
This is thecondition under which left-corner parsers can beused as proof procedures.
Because this condition isoverly restrictive, we extend the procedures so thatthey allow non-chain rules, i.e., rules not in chainform.
Step 1 is replaced by the following:Step 1 Find a non-chain rule a(w,z) C B1 A. .
.
AB~ such that the first argument w of the con-sequent a(w,z) unifies with the first argumentz of the goal g(x,y), and introduce the newgoals B1, ..., and /3,.
A fact is regarded asa non-chain rule with an empty antecedent.
Ifall these goals are recursively derived, then cre-ate the consequent a(z,z), which dominates B1,..., and B, ,  and place it on the left corner.Generalized Chart AlgorithmThe idea given in the previous section realizes thegoal-driven bottom-up derivation, which is the firstfeature of our algorithm.
Then, we present a morerefined algorithm based upon the idea, which real-izes the other two features as well as the first one.Char t  Pars ing  and  i ts  Genera l i za t ionLike left-corner parsing, which has the drawback ofrepeatedly recomputing partial results, head-drivenderivation will face the same problem when it isexecuted in a depth-first manner with backtrack-ing.
In the case of left-corner parsing, the prob-lem is resolved by using the tabulation method,known as chart parsing (Kay, 1980).
A recentstudy by Haruno et al (1993) has shown thatthe same method is applicable to semantic-head-driven generation.
The method is also applicableto head-driven derivation, which is more generalthan semantic-head-driven generation.To generalize charts to use in proof procedures,m( <\[AJ,\[B\]>,\[A,B\]) ,oO.O*?"?"?O?Oo..,.....""
...../ / ',..., ,../ / .. ki /.
'-.
\ / \ \h( <IA\],\[BI> A~> ) I <II.\[BI>~.. m( <\[\],~f~..){\])g-" .
.
.
.
.
-%- "A .
.
.
.
.
C'" "" ........ ":':~~\[A1JBI~(" -- ~ /  <\[l \ [ l>~Z \ u".... .....
I(Some labels ..... m(<\[A\],\[_\]..~',tA\])are  omitted) .................... ..m( <\[A\],IB\]>,\[B,A\])Figure 2: Example of Generalized Chartswe first define the chart lexicons.
In chart pars-ing, lexicons are the words in the input string,each of which is used as the index for a subsetof the edges in the chart; each edge incident from(the start-point of) lexicon w represents the sub-structure dominating the sub-string starting fromw.
In our case, from the-similarity between left-corner parsing and head-driven derivation, lexiconsare the terms that occur in the first argument po-sition of any formula; each edge incident from (thestart-point of) lexicon x represents the substruc-ture dominating the successive sequence of the de-rived formulas starting from the fact in which zoccupies the first argument position.
For example,in the chart representing the proof in Figure 1, allthe edges corresponding to the formulas on the leftborder, i.e.
am(X,Zrn), am--l(Z,Zm--1),..., al(x,zl)and g(z,y), are incident from (the start-point of)lexicon z, and, hence, x is the index for these edges.Following this definition of the chart lexicons,there are two major differences between chartparsing and proof procedures, which Haruno alsoshowed to be the differences between chart parsingand semantic-head-driven generation.1.
In contrast o chart parsing, where lexicons aredetermined immediately upon input, in proofprocedures lexicons should be incrementally in-troduced.2.
In contrast o chart parsing, where lexicons areconnected one by one in a linear sequence, inproof procedures lexicons hould be connected inmany-to-many fashion.In proof procedures, the chart lexicons are notdetermined at the beginning of the proof (because220we don't know which formulas are actually used inthe proof), rather they are dynamically extractedfrom the subgoals as the process goes.
In addi-tion, if the rules are nondeterministic, it sometimeshappens that there are introduced, from one left-corner key, a(x,z), two or more distinct succes-sive subgoals, bl(wl,y~), b2(w2,y2), etc., that havedifferent first arguments, w 1, w 2, etc.
In such acase, one lexicon x should be connected to two ormore distinct lexicons, w 1, w 2, etc.
Furthermore,it can happen that two or more distinct left-cornerkeys, al(xl,zl), a2(x2,z2), etc., incidentally intro-duce the successive subgoals, bl(w,yl), b2(w,y~),etc., with the same first argument w. In such acase, two or more distinct lexicons, x 1, x 2, etc.,should be connected to one lexicon w. Therefore,the connections among lexicons should be many-to-many.
Figure 2 shows an example of charts withmany-to-many connections, where the connectionsare represented by pointers A, B; etc.The  A lgor i thmWe, so far, have considered eduction but not ab-duction.
Here, we extend our idea to apply to ab-duction, and present he definition of the algorithm.The extension for abduction is very simple.First, we add a new procedure, which introducesan assumption G for a given goal G. An assump-tion is treated as if it were a fact.
This means thatan assumption, as well as a fact, is represented as apassive edge in terms of the chart algorithm.
Sec-ond, we associate a set S of assumptions with eachedge e in the chart; S consists of all the assump-tions that are contained in the completed part ofthe (partial) proof represented by the edge e. Moreformally, the assumption set 5 associated with anedge e is determined as follows:1.
If e is a passive edge representing an assumptionA, then S - -  {A}.2.
If e is a passive/active edge introduced from anon-chain rule, including fact, then S is empty.3.
If e is a passive/active edge predicted from achain rule with a passive edge e' being the left-corner key, then S is equal to the assumption setS'  of e'.4.
If e is a passive/active edge created by combiningan active edge el and a passive edge e2, then,-q = $1 U $2 where 81 and ~q2 are the assumptionsets of el and e2, respectively.Taking these into account, the definition of ouralgorithm is as follows, f is a function that assignsa unique vertex to each chart lexicon.
The notationA:S stands for the label of an edge e, where A isthe label of e in an ordinary sense and S is theassumption set associated with e.In i t ia l i za t ion  Add an active edge \[\[?IG\]-I-:?
tothe chart, looping at vertex 0, where G is theinitial goal.Apply the following procedures repeatedly untilno procedures are applicable.In t roduct ion  Let e be an active edge labeled\[...\[?\]Bj...\]A:S incident from vertex s to t,where Bj = bj (zj,yj) is the first open box in e.1.
If the lexicon xj is never introduced in thechart, then introduce it and run a pointerfrom t to f (z j ) .
Then, do the following:(a) For every non-chain rule a(w,z)C B1 A. .
.
A Bn, including fact, such that w uni-fies with zi,  create an active edge la-beled \[\[?\]Bl'"\[?lS,~\]a(xj,z):?
between ver-tex f(xj) and f(zj) + 1.
(Create, instead,a passive edge labeled a(xj,z):?
when therule is a fact, i.e.
n = 0.
)(b) Create a passive edge labeled Bj:{Bj} be-tween vertex f(xj) and f(zj)  + 1.2.
If the lexicon zj was previously introduced inthe chart, then run a pointer from t to f(xj).In addition, if the passive dge Bj :{Bj } neverexists in the chart, create it between vertexf ( r j )  and f(xj) + 1.Pred ic t ion  Let e be a passive edge labeled C:Sincident from vertex s to t. For every chainrule A' C A A B1 A .
.
.
A Bn such that Aunifies with C, create an active edge labeled\[A\[?\]B1...\[?\]Bn\]A':,~ between vertex s and t.(Create, instead, a passive edge labeled A':Swhen A is the single antecedent, i.e., n = 0.
)Combinat ion Let ez be an active edge labeled\['" "\[?\]Bj\[?\]Bj+I'" .\[?\]B,~\]A:$1 incident from ver-tex s to t, where Bj is the first open box in ezand let e2 be a passive edge labeled C:S~ inci-dent from vertex u to v. If Bj and C unify andthere is a pointer from t to u, then create an ac-tive edge labeled \[-..Bj\[?\]Bj+I...\[?\]Bn\]A:S1 US2between vertex s and v. (Create, instead, a pas-sive edge labeled A:S1 U S: when B 1 is the lastelement, i.e., j = n.)Each passive edge T:S represents an answer.ExamplesHere, we present a simple example of the appli-cation of our algorithm to spoken language un-derstanding.
Figure 3 provides the rules for spo-ken Japanese understanding, with which the sen-tence (1) is parsed and interpreted.
They includethe pragmatic, semantic and knowledge rules aswell as the syntactic and lexical rules.The syntactic rules allow the connection be-tween a verb and a noun phrase with or with-221Syntactic Ruless(i,k,e)Cvp(i,k,e)vp(i,k,e)Cnp(i,j,c,x) A vp(j,k,e) A depend( (c,e,x)d)vp( i,k,e)C np( i,j,x) A vp(j,k,e) A depend( (c,e,X)d)np(i,k,c,x)Cnp(i,j,x) A p(j,k,c)depend( (c,e,x)d)Cprag( (x)p,y) ^  sem( c,e,y), )Lexical  Rulesnp(\[S6seki\]k\],k,x)C soseki(x) $~vp(\[katta\]k\],k,e)C buy(e) *1p(\[galk\],k,c)c ga(e)p(\[ olk \],k,c)C wo( c) .1Pragmatic Rulesprag((x)p, )prag( (x)p,y)C r ter(x) ^ wr te( ^ novel(y)SlSemantic Rulessem( s)C ga( s,e) A ga(e) $3sem(s)Cwo(s,e) ^ o(e) .3ga( ( c,e,x) 8 ,c)C intend( e) A person(x) A agt( ( e,x) e ) $2?wo( (c,e,x), c)C trade(e) A commodity(z) ^ obj( (e,x),) $~Knowledge Rulesperson( x)C soseki( x)w~ter(x)Csoseki(x)book(x)Cnovd(x)eommodity( ~ )C book(z)trade(e)Cbuy(e)intend( e)C trade( e)Figure 3: Example of Rulesout a particle, which permit structures like\[VP\[NpS6sek2\]\[vpkatla\]\].
Such a structure is evalu-ated by the pragmatic and semantic riteria.
Thatis, the dependency between a verbal concept eand anominal concept x is supported if there is an entityy such that x and y have a pragmatic relation, i.e.,a metonymy relation, and e and y have a semanticrelation, i.e., a thematic relation.
The metonymyrelation is defined by the pragmatic rules, based oncertain knowledge, such as that the name of a writeris sometimes used to refer to his novel.
Also, thethematic relation is defined by the semantic rules,based on certain knowledge, such as that the objectcase relation between a trading action and a com-modity can be linguistically expressed as a thematicrelation.The subscript $c of a formula A representsthe cost of assuming formula A.
A is easy to as-sume when c is small, while A is difficult to as-sume when c is large.
For instance, the cost ofinterpreting the thematic relation between a trad-ing action and a commodity as the object case re-lation is low, say $2, while the cost of interpret-ing the thematic relation between an intentionalaction and a third person as the agent case rela-tion is high, say $20.
This assignment of costs issuitable for a situation in which the speaker re-ports his experience.
In spite of the difficulty ofassigning suitable costs in general, the cost-basedinterpretation is valuable, because it provides a uni-form criteria for syntax, semantics and pragmat-ics.
Hopefully, several techniques, independentlydeveloped in these areas, e.g., stochastic parsing,example-based/corpus-based techniques for wordsense/structural disambiguation, etc., will be us-able for better cost assignment.
Probability willalso be a key technique for the cost assignment(Charniak and Shimony, 1990).Figure 4 and Table 1 show the chart that iscreated when a sentence (1) is parsed and inter-preted using our algorithm.
Although the diagramseems complicated, it is easy to understand if webreak down the diagram.
Included are the syntac-tic parsing of the sentence (indicated by edges 2, 6,7, 14, 52 and 53), the pragmatic interpretation ofthe metonymy by S6seki S (indicated by edges 17,18, 20 and 24), the semantic interpretation of thethematic relation between a buying event B and anovel N written by S6seki (indicated by edges 42,44, 45, 47, 48 and 50), and so on.
In the pragmaticinterpretation, assumption ovel(N) (edge 21) isintroduced, which is reused in the semantic inter-pretation.
In other words, a single assumption isused more than once.
Such a tricky job is naturallyrealized by the nature of the chart algorithm.Agenda ControlSince the aim of cost-based abduction is to findout the best solution, not all solutions, it is reason-able to consider combining heuristic search strate-gies with our algorithm to find the best solutionefficiently.
Our algorithm facilitates uch an exten-sion by using the agenda control mechanism, whichis broadly used in advanced chart parsing systems.The agenda is a storage for the edges created byany of the three procedures of the chart algorithm,out of which edges to be added to the chart areselected, one by one, by a certain criterion.
Thesimplest strategy is to select the edge which hasthe minimal cost at that time, i.e., ordered search.Although ordered search guarantees that thefirst solution is the best one, it is not always very ef-ficient.
We can think of other search strategies, likebest first search, beam search, etc., which are morepractical than ordered search.
To date, we have notinvestigated any of these practical search strategies.However, it is apparent hat our chart algorithm,together with the agenda control mechanism, willprovide a good way to realize these practical searchstrategies.222\ [Sosek i ,kat ta \ ]iIIiIIIiIIIIl39,,,5..4.
- .
-  ~?..?.?-?
??o?
?g~gllIRmBSml~00 -?
--.......6"7 ,8%\f .
l  -*-o.~i ............... !i 35.. .... 34,49 " i!a F <B,S>si~ 1.~iii .
.
.
.
.
.
.
.
<s.vI " .
. '
L _3,4,5 ~.
SQ.-"20 /24g/k "\ _ X Z:" .
.
.
.
.
L25\ [kat ta \ ]???'~???'o0o?
'......28,29IW-.IIIIIi D!P n\ \13 ~.1-'-) 2i-" I" J...10,11,1pass ive edgeact ive edgepointerFigure 4: Chart Diagram for SSseki kattaPre l iminary  Exper imentsWe conducted preliminary experiments o comparefour methods of cost-based abduction: top-down al-gorithm (TD), head-driven algorithm (HD), gener-alized chart algorithm with full-search (GCF), andgeneralized chart algorithm with ordered search(GCO).
The rules used for the experiments are inthe spoken language understanding task, and theyare rather small (51 chain rules + 35 non-chainrules).
The test sentences include one verb and 1-4noun phrases, e.g., sentence (1).Table 2 shows the results.
The performance ofeach method is measured by the number of compu-tation steps, i.e., the number of derivation stepsin TD and HD, and the number of passive andactive edges in GCF and GCO.
The decimals inparentheses show the ratio of the performance ofeach method to the performance of TD.
The tableclearly shows how the three features of our algo-rithm improve the computational efficiency.
Theimprovement from TD to HD is due to the first fea-ture, i.e., goal-driven bottom-up derivation, whicheliminates about 50% of the computation steps; theimprovement from HD to GCF is due to the sec-ond feature, i.e., tabulation of the partial results,Table 2: Comp.
among TD, HD, GCF, and GCONs II TDI  ttD GCF1 215 112 (0.52) 83 (0.39)2 432 218 (0.50) 148 (0.34)3 654 330 (0.50) 193 (0.30)4 876 442 (0.50) 238 (0.27)GCO75 (0.35)113 (0.26)160 (0.24)203 (0.23)which decreases the number of steps another 13%-23%; the improvement from GCF to GCO is due tothe last feature, i.e., the agenda control mechanism,which decreases the number of steps another 4%-8%.
In short, the efficiency is improved, maximally,about four times.Compar i son  w i th  Ear ley  Deduct ionWe describe, here, some differences between our al-gorithm and Earley deduction presented by Pereiraand Warren.
First, as we mentioned before, our al-gorithm is mainly based on bottom-up (left-corner)derivation rather than top-down derivation, thatEarley deduction is based on.
Our experimentsshowed the superiority of this approach in our par-223titular, though not farfetched, example.Second, our algorithm does not use sub-sumption-checking of edges, which causes a seriouscomputation problem in Earley deduction.
Our al-gorithm needs subsumption-checking o ly when anew edge is introduced by the combination proce-dure.
In the parsing of augmented grammars, evenwhen two edges have the same nonterminal symbol,they are different in the annotated structures asso-ciated with those edges, e.g., feature structures; insuch a case, we cannot use one edge in place ofanother.
Likewise, in our algorithm, edges are al-ways annotated by the assumption sets, which, inmost cases, prevent those edges from being reused.Therefore, in this case, subsumption-checking is oteffective.
In our algorithm, reuse of edges only be-comes possible when a new edge is introduced bythe introduction procedure.
However, this is doneonly by adding a pointer to the edge to be reused,and, to invoke this operation, equality-checking oflexicons, not edges, is sufficient.Finally, our algorithm has a stronger connec-tion with chart parsing than Earley deduction does.Pereira and Warren noted that the indexing of for-mulas is just an implementation technique to in-crease efficiency.
However, indexing plays a con-siderable role in chart parsing, and how to indexformulas in the case of proof procedures i not soobvious.
In our algorithm, from the considerationof head-driven derivation, the index of a formulais determined to be the first argument of that for-mula.
All formulas with the same index are derivedthe first time that index is introduced in the chart.Pointers among lexicons are also helpful in avoidingnonproductive attempts at applying the combina-tion procedure.
All the devices that were originallyused in chart parsers in a restricted way are in-cluded in the formalism, not in the implementation,of our algorithm.Conc lud ing  RemarksIn this paper, we provided a basic and practi-cal solution to the computation problem of cost-based abduction.
We explained the basic conceptof our algorithm and presented the details of thealgorithm along with simple examples.
We alsoshowed how our algorithm improves computationalefficiency on the basis of the results of the prelimi-nary experiments.We are now developing an abduction-basedspoken language understanding system using ouralgorithm.
The main problem is how to find a goodsearch strategy that can be implemented with theagenda control mechanism.
We are investigatingthis issue using both theoretical nd empirical ap-proaches.
We hope to report good results alongthese lines in the future.AcknowledgmentsThe author would like to thank Prof. Yuji Matsu-moto of Nara Institute of Science and Technologyand Masahiko Haruno of NTT Communication Sci-ence Laboratories for their helpful discussions.References\[Charniak and Husain, 1991\] Eugene Charniakand Saadia Husain.
A new admissible heuristicfor minimal-cost proofs.
Proceedings of the 12thIJCAI, pages 446-451, 1991.\[Charniak and Santos Jr., 1992\] Eugene Charniakand Eugene Santos Jr.
Dynamic MAP calcu-lations for abduction.
Proceedings of the lOthAAAI, pages 552-557, 1992.\[Charniak and Shimony, 1990\] Eugene Charniakand Solomon E. Shimony.
Probabilistic seman-tics for cost based abduction.
Proceedings of the8th AAAI, pages 106-111, 1990.\[Haruno et al, 1993\] Masahiko Haruno, YasuharuDen, Yuji Matsumoto, and Makoto Nagao.
Bidi-rectional chart generation of natural languagetexts.
Proceedings of the 11th AAAI, pages 350-356, 1993.\[Hobbs et at., 1988\] Jerry R. Hobbs, Mark Stickel,Paul Martin, and Douglas Edwards.
Interpreta-tion as abduction.
Proceedings of the 26th An-nual Meeting of ACL, pages 95-103, 1988.\[Kay, 1980\] Martin Kay.
Algorithm schemata anddata structures insyntactic processing.
TechnicalReport CSL-80-12, XEROX Palo Alto ResearchCenter, 1980.\[Pereira nd Warren, 1983\] Fernando C.N.
Pereiraand David H.D.
Warren.
Parsing as deduction.Proceedings of the 21st Annual Meeting of A CL,pages 137-144, 1983.\[Shieber et at., 1989\] Stuart M. Shieber, Gertjanvan Noord, Robert C. Moore, and Fernando C.N.Pereira.
A semantic-head-driven g eration al-gorithm for unification-based formalisms.
Pro-ceedings of the 27th Annual Meeting of ACL,pages 7-17, 1989.\[Sikkel and op den Akker, 1993\] Klaas Sikkel andRieks op den Akker.
Predictive head-corner chartparsing.
The 3rd International Workshop onParsing Technologies, pages 267-276, 1993.\[van Noord, 1990\] Gertjan van Noord.
An over-view of head-driven bottom-up generation.
Cur-rent Research in Natural Language Generation,chapter 6, pages 141-165.
Academic Press, 1990.\[van Noord, 1991\] Gertjan van Noord.
Head cor-ner parsing for discontinuous constituency.
Pro-ceedings of the 29th Annual Meeting of ACL,pages 114-121, 1991.224Tab le  1: Tab le  Representat ion  of  the  Char tI1# I # I Arc A-Set  \[ From1 \[\[?\]s(,~,\[\],e)\]T ?
1 -' 2 ' \[\[?\]s?seki(S) $1\] d ' 1np( ~, q2,S)' 3 ' s?seki(S)~l {~} l 2' 4 I person(S) {a} , 3' 5 ' writer(S) {c~} , 3' 6 ' np(Cb,ffl,S) {c~} 2+B+3' ' 6\[?lvp(@,k,e)\ [?\]depend((c,e,S)a)\]' 8 I \ [np(~, f f2 ,S )  {a} ' 6\[?lp( kO,k,e ) \]np( d2 ,k,c,S)9 I \[\[?\]buy(B)Sl\] ? }
7. vp(~,~,B)10 ' buy(B)~l {fl} i 9!
11 ' t rade(B)  {/3} I 10' i '2 l intend(B) {/3} I 11i3 I vP(ql'~'B) {/3} I 9+D+10141\[np(rb,ff2,S) {c~,/3} 7+C+13vp(~,~,B)\[?\]depend( ( P ,B ,S )d ) \]vp( q~,~,B )15 \[\[?\]prag((S)p,X) ?
14\[?\]sem((P,S,x)..)\]depend(<P,S,S)a)16 prag((S)p,S) ?
, 15i7 J \[\[?\]writer(S) ' ?
15\[?\]write( ( S,x )p ) $1?\[7\]novel(c) .1\] \[ prag((s)~,~)as ' \[writer(S) ' {a} ' 17+G+5\[?\]write( (S,N)p) ~1?\[?\]novel(N) ~1\]prag((S>v'N) i19 i write((S,N)p)'a? '
{7} ' 18 '20 \[writer(S) ' {oqT} ' 18+H+19write({S,N>.
)\[?\]novel(N) ~1\]prag((S)p,N) , ,'' nove l (N)"  2022 book(N) t 6 } 2123 ' commodity(N) ' {6} ' 2224 ' prag(IS)v,N ) ' {a,7,8} ' 20+I+2125 \[prag((S)v,S" ) ?
15+F+16t?\]sem((P,B,S>~,)\] ,depend((P,B,S)d)26 \[\[?\]intend(By 25\[?\]person(S)\[?\]agt( ( B,S) , )~2?\]ga(<P,B,S),,P)27'\[\[?\]trade(B) !
~b ,~ 25\[?\]commodity(S)wo((P,B,S), ,P)Arc  \[ A-Set  I From J28 \[intend(B) ,{ /3}  26+K+12\[?\]person(S)\[?\]agt( < B,S),  ) .2?\]ga(<P,B,S)~,P)29 I \[trade(B) I {/3} I 27+K+11\[?\]commodity(S) \[e\]obj((B,S)~) ~2\]wo(<P,B,S),,P)'30 ' \[intend(B) ' {a,/3} ' 28+L+4 'person(S) i t?\]agt( ( B,S), ) ~2?\]ga(<P,B,S)~,P) , , ,31 ' agt((B S)s)  ~2?
{e} 3032 ~ ga((P,B,S)s P) ' {a, fl, e} '33 I \ [ga((P ,B,S) , ,P)  ' {a,/3, e} I 30+M+31 I 32\[?\]ga(P) ~3\]sem((P,B,S),) ,34 ' ga(P) ~3 I ,35 \[ sem((P,B,S)s) I {a,  {?}
I 33 \[ /3, e,?}
t 33+N+34 I36 I depe~d((P,B,S)d) {a,/3, e,~'} I 25+J+35 l37 I vp((~,~,B) ' {a,/3, e, ~} , , 14+E+3638 ' ~?,~,B) {or,/3, e, ~} , 37 ,39 ' ' {a,  fl, e, (~} 1+A+3840 \[prag((S)p,N) {a,7 ,5}  15+F+24\[?\]sem((P,S,g)s)\]depend((P,B,S)a)41 \[\[?\]intend(B) ~ 40\[?\]person(N) \[?\]agt((B,N),)'2?\] :ga((P,B,N I ~,P))42 '\[\[?\]trade(B) ~b ' 40j \[?\]commodity(N)\[?\]obj((B,N),) $2\]wo( (P ,B , i ) s ,P )' \[intend(B) {/3} ' 41+P+12 '43 i \[?\]person(N) i\[?\]agt((s,g)s)$2?\] !i ga((P,B,Nl~,P )44 ' \[trade(B) ' {/3} ' 42+P+11 '\[?\]commodity(N)\[?\]obj((B,N),) ~2\]wo((P,B,N).,P)45 \[trade(B) {/3, 8} 44+Q+23 :commodity(N) \[?\]obj(<B,N) s) '2\]wo((P,B,N) , ,P)46 obj((B,g)~3 s2 {r/} , 45 t47 'wo( (P ,B ,N)~,P)  t {fl, 6, r/} 45+R+4648 \[wo((P,B,lg)s,P) {/3, 6, r/} 47\[?\]wo(P) s3\] sem(<P,B,NL )49 ' wo(P) ~3 " {0} i 48 '50 ,I sem((P,B N)s)  t {/3,6, r/,0} 48+S+49 t51 t depend((P,B,S)a) ' {o~,/3, 7,6, rt, 0} t 40+0+50 't52 J vp(?~B)  ' "  t {a , /3 ,7 ,6 ,  r/,0} i 14+E+5153 s(?,D,B ) {a,/3, v, e,,, 0} 52 ', {a, fl, 7 ,5 ,n ,  0} , 1+A+53 , ?
= iS6seki,katta\], ?
= \[katta\] ~ 54 , T , .. 0}o~ = soseki(S) $1, fl = buy(B)$1, 7 = wr i te((S,N)p)  $1?, 6 = novel(N)  $1,e = agt( (B ,S) , )  $2?, ?
= ga(P)$3, , = obj((B,N)s)$2, 0 = wo(P)  $3225
