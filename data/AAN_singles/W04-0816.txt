Word Sense Disambiguation based onTerm to Term Similarity in a Context SpaceJavier ArtilesDpto.
Lenguajes ySistemas Informa?ticosUNED, Spainjavart@bec.uned.esAnselmo Pen?asDpto.
Lenguajes ySistemas Informa?ticosUNED, Spainanselmo@lsi.uned.esFelisa VerdejoDpto.
Lenguajes ySistemas Informa?ticosUNED, Spainfelisa@lsi.uned.esAbstractThis paper describes the exemplar based ap-proach presented by UNED at Senseval-3.
In-stead of representing contexts as bags of termsand defining a similarity measure between con-texts, we propose to represent terms as bagsof contexts and define a similarity measure be-tween terms.
Thus, words, lemmas and sensesare represented in the same space (the contextspace), and similarity measures can be definedbetween them.
New contexts are transformedinto this representation in order to calculatetheir similarity to the candidate senses.
Weshow how standard similarity measures obtainbetter results in this framework.
A new similar-ity measure in the context space is proposed forselecting the senses and performing disambigua-tion.
Results of this approach at Senseval-3 arehere reported.1 IntroductionWord Sense Disambiguation (WSD) is the taskof deciding the appropriate sense for a partic-ular use of a polysemous word, given its tex-tual or discursive context.
A previous non triv-ial step is to determine the inventory of mean-ings potentially attributable to that word.
Forthis reason, WSD in Senseval is reformulated asa classification problem where a dictionary be-comes the class inventory.
The disambiguationprocess, then, consists in assigning one or moreof these classes to the ambiguous word in thegiven context.
The Senseval evaluation forumprovides a controlled framework where differentWSD systems can be tested and compared.Corpus-based methods have offered encour-aging results in the last years.
This kind ofmethods profits from statistics on a trainingcorpus, and Machine Learning (ML) algorithmsto produce a classifier.
Learning algorithmscan be divided in two main categories: Super-vised (where the correct answer for each piece oftraining is provided) and Unsupervised (wherethe training data is given without any answerindication).
Tests at Senseval-3 are made invarious languages for which two main tasks areproposed: an all-words task and a lexical sam-ple task.
Participants have available a trainingcorpus, a set of test examples and a sense inven-tory in each language.
The training corpora areavailable in a labelled and a unlabelled format;the former is mainly for supervised systems andthe latter mainly for the unsupervised ones.Several supervised ML algorithms have beenapplied to WSD (Ide and Ve?ronis, 1998), (Es-cudero et al, 2000): Decision Lists, Neural Net-works, Bayesian classifiers, Boosting, Exemplar-based learning, etc.
We report here theexemplar-based approach developed by UNEDand tested at the Senseval-3 competition in thelexical sample tasks for English, Spanish, Cata-lan and Italian.After this brief introduction, Sections 2 and3 are devoted, respectively, to the training dataand the processing performed over these data.Section 4 characterizes the UNED WSD system.First, we describe the general approach based onthe representation of words, lemmas and sensesin a Context Space.
Then, we show how resultsare improved by applying standard similaritymeasures as cosine in this Context Space.
Oncethe representation framework is established, wedefine the criteria underlying the final similar-ity measure used at Senseval-3, and we com-pare it with the previous similarity measures.Section 5 reports the official results obtained atthe Senseval-3 Lexical Sample tasks for English,Spanish, Italian and Catalan.
Finally, we con-clude and point out some future work.2 DataEach Lexical Sample Task has a relatively largetraining set with disambiguated examples.
Thetest examples set has approximately a half ofthe number of the examples in the training data.Each example offers an ambiguous word and itsAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemssurrounding context, where the average contextwindow varies from language to language.
Eachtraining example gives one or more semantic la-bels for the ambiguous word corresponding tothe correct sense in that context.Senseval-3 provided the training data and thetest data in XML format.
The XML taggingconventions provides an excellent ground for thecorpora processing, allowing a simple way forthe data browsing and transformation.
How-ever, some of the XML well-formedness con-straints are not completely satisfied.
For exam-ple, there is no XML declaration and no rootelement in the English Lexical Sample docu-ments.
Once these shortcomings are fixed anyXML parser can normally read and process thedata.Despite the similarity in the structure of thedifferent corpora at the lexical sample taskin different languages, we had found a het-erogeneous vocabulary both in the XML tagsand the attributes, forcing to develop ?ad hoc?parsers for each language.
We missed a commonand public document type definition for all thetasks.Sense codification is another field where dif-ferent solutions had been taken.
In the Englishcorpus nouns and adjectives are annotated usingthe WordNet 1.7.1. classification1 (Fellbaum,1998), while the verbs are based on Wordsmyth2(Scott, 1997).
In the Catalan and Spanish tasksthe sense inventory gives a more coarse-grainedclassification than WordNet.
Both tasks haveprovided a dictionary with additional informa-tion as examples, typical collocations and theequivalent synsets at WordNet 1.5.
Finally, theItalian sense inventory is based on the Multi-Wordnet dictionary3 (Pianta et al, 2002).
Un-like the other mentioned languages , the Italiantask doesn?t provide a separate file with the dic-tionary.Besides the training data provided by Sen-seval, we have used the SemCor (Miller et al,1993) collection in which every word is alreadytagged in its part of speech, sense and synset ofWordNet.3 PreprocessingA tokenized version of the Catalan, Spanish andItalian corpora has been provided.
In this ver-sion every word is tagged with its lemma and1http://www.cogsci.princeton.edu/ wn/2http://www.wordsmyth.net3http://multiwordnet.itc.it/part of speech tag.
This information has beenmanually annotated by human assessors both inthe Catalan and the Spanish corpora.
The Ital-ian corpus has been processed automatically bythe TnT POStagger4 (Brants, 2000) includingsimilar tags.The English data lacked of this information,leading us to apply the TreeTagger5 (Schmid,1994) tool to the training and test data as aprevious step to the disambiguation process.Since the SemCor collection is already tagged,the preprocessing consisted in the segmentationof texts by the paragraph tag, obtaining 5382different fragments.
Each paragraph of Semcorhas been used as a separate training examplefor the English lexical sample task.
We appliedthe mapping provided by Senseval to representverbs according to the verb inventory used inSenseval-3.4 ApproachThe supervised UNED WSD system is an ex-emplar based classifier that performs the disam-biguation task measuring the similarity betweena new instance and the representation of somelabelled examples.
However, instead of repre-senting contexts as bags of terms and defininga similarity measure between the new contextand the training contexts, we propose a rep-resentation of terms as bags of contexts andthe definition of a similarity measure betweenterms.
Thus, words, lemmas and senses arerepresented in the same space, where similar-ity measures can be defined between them.
Wecall this space the Context Space.
A new disam-biguation context (bag of words) is transformedinto the Context Space by the inner product,becoming a kind of abstract term suitable to becompared with singular senses that are repre-sented in the same Context Space.4.1 RepresentationThe training corpus is represented in the usualtwo-dimension matrix A as shown in Figure 1,where?
c1, ..., cN is the set of examples or con-texts in the training corpus.
Contexts aretreated as bags of words or lemmas.?
lem1, ..., lemT is the set of different wordsor lemmas in all the training contexts.4http://www.coli.uni-sb.de/ thorsten/tnt/5http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/?
wi,j is the weight for lemi in the trainingcontext cj .A new instance q, represented with the vec-tor of weights (w1q, ..., wiq, ..., wTq), is trans-formed into a vector in the context space ~q =(q1, ..., qj , ..., qN ), where ~q is given by the usualinner product ~q = q ?
A (Figure 1):qj =T?i=1wiqwijFigure 1: Representation of terms in the Con-text Space, and transformation of new in-stances.If vectors cj (columns of matrix A) and vectorq (original test context) are normalized to havea length equal to 1, then qj become the cosinebetween vectors q and cj .
More formally,~q = q.A = (cos(q, c1), ..., cos(q, cj), ..., cos(q, cN ))wherecos(q, cj) =T?i=1wiq?q?wij?cj?and?x?
=?
?ix2iAt this point, both senses and the representa-tion of the new instance ~q are represented in thesame context space (Figure 2) and a similaritymeasure can be defined between them:sim( ~senik, ~q)where senik is the k candidate sense for theambiguous lemma lemi.
Each component j of~senik is set to 1 if lemma lemi is used with sensesenik in the training context j, and 0 otherwise.Figure 2: Similarity in the Context Space.For a new context of the ambiguous lemmalemi, the candidate sense with higher similarityis selected:argmaxk sim( ~senik, ~q)4.2 Bag of words versus bag of contextsTable 1 shows experimental results over the En-glish Lexical Sample test of Senseval-3.
Sys-tem has been trained with the Senseval-3 dataand the SemCor collection.
The Senseval train-ing data has been lemmatized and tagged withTreeTagger.
Only nouns and adjectives havebeen considered in their canonical form.Three different weights wij have been tested:?
Co-occurrence: wij and wiq are set to {0,1}depending on whether lemi is present ornot in context cj and in the new instance qrespectively.
After the inner product q ?
A,the components qj of ~q get the number ofco-occurrences of different lemmas in bothq and the training context cj .?
Term Frequency: wij is set to tfij , the num-ber of occurrences of lemi in the context cj .?
tf.idf : wij = (1 + log(tfij)) ?
(log( Ndfi )),a standard tf.idf weight where dfi is thenumber of contexts that contain lemi.These weights have been normalized ( wij||cj ||)and so, the inner product q?A generates a vector~q of cosines as described above, where qj is thecosine between q and context cj .Two similarity measures have been compared.The first one (maximum) is a similarity of q asbag of words with the training contexts of sensesen.
The second one (cosine) is the similarityof sense sen with ~q in the context space:?
Maximum: sim( ~sen, ~q) == MaxNj=1 (senj ?
qj) == Max{j/sen?cj}qj == Max{j/sen?cj}cos(q, cj)Weight Similarity Nouns Adjectives Verbs TotalCo-occurrences Maximum 60.76% 35.85% 60.75% 59.75%(normalized) Cosine 59.99% 55.97% 63.88% 61.78%Term frequency Maximum 56.83% 50.31% 56.85% 56.58%(normalized) Cosine 60.76% 53.46% 63.83% 62.01%tf.idf Maximum 59.82% 48.43% 59.94% 59.42%(normalized) Cosine 60.27% 53.46% 64.29% 62.01%Most frequent(baseline) 54.01% 54.08% 56.45% 55.23%Table 1: Bag of words versus bag of contexts, precision-recallSimilarity with sense sen is the high-est similarity (cosine) between q (as bag ofwords) and each of the training contexts(as bag of words) for sense sen.?
Cosine: sim( ~sen, ~q) = cos( ~sen, ~q) ==?
{j/sen?cj}senj|| ~sen|| ?cos(q,cj)||~q||Similarity with sense sen is the co-sine in the Context Space between ~q and~senTable 1 shows that almost all the results areimproved when the similarity measure (cosine)is applied in the Context Space.
The exceptionis the consideration of co-ocurrences to disam-biguate nouns.
This exception led us to explorean alternative similarity measure aimed to im-prove results over nouns.
The following sectionsdescribe this new similarity measure and the cri-teria underlying it.4.3 Criteria for the similarity measureCo-occurrences behave quite good to disam-biguate nouns as it has been shown in the exper-iment above.
However, the consideration of co-occurrences in the Context Space permits acu-mulative measures: Instead of selecting the can-didate sense associated to the training contextwith the maximum number of co-occurrences,we can consider the co-occurences of q with allthe contexts.
The weights and the similarityfunction has been set out satisfying the follow-ing criteria:1.
Select the sense senk assigned to moretraining contexts ci that have the maxi-mum number of co-occurrences with thetest context q.
For example, if sense sen1has two training contexts with the highestnumber of co-occurrences and sense sen2has only one with the same number of co-occurrences, sen1 must receive a highervalue than sen2.2.
Try to avoid label inconsistencies in thetraining corpus.
There are some trainingexamples where the same ambiguous wordis used with the same meaning but taggedwith different sense by human assessors.Table 2 shows an example of this kind ofinconsistencies.4.4 Similarity measureWe assign the weights wij and wiq to have ~q avector of co-occurrences, where qj is the numberof different nouns and adjectives that co-occurrin q and the training context cj .
In this way, wijis set to 1 if lemi is present in the context cj .Otherwise wij is set to 0.
Analogously for thenew instance q, wiq is set to 1 if lemi is presentin q and it is set to 0 otherwise.According to the second criterium, if thereis only one context c1 with the higher num-ber of co-occurrences with q, then we reducethe value of this context by reducing artifi-cially its number of co-occurrences: Being c2a context with the second higher number of co-occurrences with q, then we assign to the firstcontext c1 the number of co-occurrences of con-text c2.After this slight modification of ~q we imple-ment the similarity measure between ~q and asense senk according to the first criterium:sim( ~sen, ~q) =N?j=1senj ?
NqjFinally, for a new context of lemi we selectthe candidate sense that gives more value to thesimilarity measure:argmaxk sim( ~senk, ~q)<answer instance=?grano.n.1?
senseid=?grano.4?/><previous> La Federacin Nacional de Cafeteros de Colombia explic que el nuevo valor fue estable-cido con base en el menor de los precios de reintegro mnimo de grano del pas de los ltimos tres das,y que fue de 1,3220 dlares la libra, que fue el que alcanz hoy en Nueva York, y tambin en la tasa rep-resentativa del mercado para esta misma fecha (1.873,77 pesos por dlar).
</previous> <target>El precio interno del caf colombiano permaneci sin modificacin hasta el 10 de noviembre de 1999,cuando las autoridades cafetaleras retomaron el denominado ?sistema de ajuste automtico?, quetiene como referencia la cotizacin del <head>grano</head> nacional en los mercados interna-cionales.
</target><answer instance=?grano.n.9?
senseid=?grano.3?/><previous> La carga qued para maana en 376.875 pesos (193,41 dlares) frente a los 375.000 pesos(192,44 dlares) que rigi hasta hoy.
</previous> <target> El reajuste al alza fue adoptado porel Comit de Precios de la Federacin que fijar el precio interno diariamente a partir de este lunestomando en cuenta la cotizacin del <head>grano</head> en el mercado de Nueva York y la tasade cambio del da, que para hoy fueron de 1,2613 dlares libra y1.948,60 pesos por dlar </target>Table 2: Example of inconsistencies in human annotationWeight Similarity Nouns Adjectives Verbs TotalCo-occurrences Without criterium 2 65.6% 45.9% 62.5% 63.3%(not normalized) With criterium 2 66.5% 45.9% 63.4% 64.1%Table 3: Precision-recall for the new similarity measureTable 3 shows experimental results over theEnglish Lexical Sample test under the same con-ditions than experiments in Table 1.Comparing results in both tables we observethat the new similarity measure only behavesbetter for the disambiguation of nouns.
How-ever, the difference is big enough to improveoverall results.
The application of the secondcriterium (try to avoid label inconsistencies)also improves the results as shown in Tables 3and 4.
Table 4 shows the effect of applying thissecond criterium to all the languages we haveparticipated in.
With the exception of Cata-lan, all results are improved slightly (about 1%)after the filtering of singular labelled contexts.Although it is a regular behavior, this improve-ment is not statistically significative.With WithoutCriterium 2 Criterium 2Spanish 81.8% 80.9%Catalan 81.8% 82.0%English 64.1% 63.3%Italian 49.8% 49.3%Table 4: Incidence of Criterium 2, precision-recall5 Results at Senseval-3The results submited to Senseval-3 were gener-ated with the system described in Section 4.4.Since one sense is assigned to every test con-text, precison and recall have equal values.
Ta-ble 4 shows official results for the Lexical Sam-ple Task at Senseval-3 in the four languages wehave participated in: Spanish, Catalan, Englishand Italian.Fine Coarse Baselinegrained grained (most frequent)Spanish 81.8% - 67%Catalan 81.8% - 66%English 64.1% 72% 55%Italian 49.8% - -Table 5: Official results at Senseval-3, precision-recallDifferences between languages are quite re-markable and show the system dependence onthe training corpora and the sense inventory.In the English task, 16 test instances havea correct sense not present in the training cor-pus.
Since we don?t use the dictionary informa-tion our system was unable to deal with none ofthem.
In the same way, 68 test instances havebeen tagged as ?Unasignable?
sense and againthe system was unable to detect none of them.6 Conclusion and work in progressWe have shown the exemplar-based WSD sys-tem developed by UNED for the Senseval-3 lexi-cal sample tasks.
The general approach is basedon the definition of a context space that be-comes a flexible tool to prove quite differentsimilarity measures between training contextsand new instances.
We have shown that stan-dard similarity measures improve their resultsapplied inside this context space.
We have es-tablished some criteria to instantiate this gen-eral approach and the resulting system has beenevaluated at Senseval-3.
The new similaritymeasure improves the disambiguation of nounsand obtains better overall results.
The work inprogress includes:?
the study of new criteria to lead us to al-ternative measures,?
the development of particular disambigua-tion strategies for verbs, nouns and adjec-tives,?
the inclusion of the dictionary information,and?
the consideration of WordNet semantic re-lationships to extend the training corpus.AcknowledgementsSpecial thanks to Julio Gonzalo for the lendingof linguistic resources, and to V?
?ctor Peinado forhis demonstrated sensibility.This work has been supported by the SpanishMinistry of Science and Technology through thefollowing projects:?
Hermes (TIC-2000-0335-C03-01)?
Syembra (TIC-2003-07158-C04-02)?
R2D2 (TIC 2003-07158-104-01)ReferencesThorsten Brants.
2000.
Tnt - a statistical part-of-speech tagger.
In In Proceedings of theSixth Applied Natural Language ProcessingConference ANLP-2000.G.
Escudero, L. Ma`rquez, and G. Rigau.
2000.A comparison between supervised learning al-gorithms for word sense disambiguation.
InIn Proceedings of the 4th Computational Nat-ural Language Learning Workshop, CoNLL.Christiane Fellbaum, editor.
1998.
WordNetAn Electronic Lexical Database.
The MITPress.N.
Ide and J.
Ve?ronis.
1998.
Introduction to thespecial issue on word sense disambiguation:The state of the art.
Computational Linguis-tics.G.
Miller, C. Leacock, T. Randee, andR.
Bunker.
1993.
A semantic concordance.In In Procedings of the 3rd DARPA Work-shop on Human Language Technology.E.
Pianta, L. Bentivogli, and C. Girardi.
2002.Multiwordnet: developing an aligned mul-tilingual database.
In In Proceedings ofthe First International Conference on GlobalWordNet.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In Inter-national Conference on New Methods in Lan-guage Processing.M.
Scott.
1997.
Wordsmith tools lexical analy-sis software for data driven learning and re-search.
Technical report, The University ofLiverpool.
