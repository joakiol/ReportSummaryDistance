Proceedings of NAACL-HLT 2013, pages 1110?1119,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsLabeling the Languages of Words in Mixed-Language Documents usingWeakly Supervised MethodsBen KingDepartment of EECSUniversity of MichiganAnn Arbor, MIbenking@umich.eduSteven AbneyDepartment of LinguisticsUniversity of MichiganAnn Arbor, MIabney@umich.eduAbstractIn this paper we consider the problem of label-ing the languages of words in mixed-languagedocuments.
This problem is approached in aweakly supervised fashion, as a sequence la-beling problem with monolingual text sam-ples for training data.
Among the approachesevaluated, a conditional random field modeltrained with generalized expectation criteriawas the most accurate and performed consis-tently as the amount of training data was var-ied.1 IntroductionLanguage identification is a well-studied problem(Hughes et al 2006), but it is typically only studiedin its canonical text-classification formulation, iden-tifying a document?s language given sample textsfrom a few different languages.
But there are sev-eral other interesting and useful formulations of theproblem that have received relatively little attention.Here, we focus on the problem of labeling the lan-guages of individual words within a multilingualdocument.
To our knowledge, this is the first paperto specifically address this problem.Our own motivation for studying this problemstems from issues encountered while attempting tobuild language resources for minority languages.
Intrying to extend parts of Kevin Scannell?s Cru?bada?nproject (Scannell, 2007), which automatically buildsminority language corpora from the Web, we foundthat the majority of webpages that contain text ina minority language also contain text in other lan-guages.
Since Scannell?s method builds these cor-pora by bootstrapping from the pages that were re-trieved, the corpus-building process can go disas-trously wrong without accounting for this problem.And any resources, such as a lexicon, created fromthe corpus will also be incorrect.In this paper, we explore techniques for per-forming language identification at the word level inmixed language documents.
Our results show thatone can do better than independent word languageclassification, as there are clues in a word?s context:words of one language are frequently surrounded bywords in the same language, and many documentshave patterns that may be marked by the presence ofcertain words or punctuation.
The methods in thispaper also outperform sentence-level language iden-tification, which is too coarse to capture most of theshifts between language.To evaluate our methods, we collected and man-ually annotated a corpus of over 250,000 wordsof bilingual (though mostly non-parallel) text fromthe web.
After running several different weakly-supervised learning methods, we found that a condi-tional random field model trained with generalizedexpectation criteria is the most accurate and per-forms quite consistently as the amount of trainingdata is varied.In section 2, we review the related work.
In sec-tion 3, we define the task and describe the data andits annotation.
Because the task of language identi-fication for individual words has not been explicitlystudied in the literature, and because of its impor-tance to the overall task, we examine the featuresand methods that work best for independent wordlanguage identification in section 4.
We begin to ex-1110amine the larger problem of labeling the languageof words in context in section 5 by describing ourmethods.
In section 6, we describe the evaluationand present the results.
We present our error analy-sis in section 7 and conclude in section 8.2 Related WorkLanguage identification is one of the older NLPproblems (Beesley, 1988), especially in regards tospoken language (House and Neuburg, 1977), andhas received a fair share of attention through theyears (Hughes et al 2006).
In its standard formu-lation, language identification assumes monolingualdocuments and attempts to classify each documentaccording to its language from some closed set ofknown languages.Many approaches have been proposed, such asMarkov models (Dunning, 1994), Monte Carlomethods (Poutsma, 2002), and more recently sup-port vector machines with string kernels, but nearlyall approaches use the n-gram features first sug-gested by (Cavnar and Trenkle, 1994).
Performanceof language identification is generally very high withlarge documents, usually in excess of 99% accuracy,but Xia et al(2009) mention that current methodsstill can perform quite poorly when the class of po-tential languages is very large or the texts to be clas-sified are very short.This paper attempts to address three of the on-going issues specifically mentioned by Hughes etal.
(2006) in their survey of textual language iden-tification: supporting minority languages, sparse orimpoverished training data, and multilingual docu-ments.A number of methods have been proposed in re-cent years to apply to the problems of unsuper-vised and weakly-supervised learning.
Excludingself- and co-training methods, these methods canbe categorized into two broad classes: those whichbootstrap from a small number of tokens (some-times called prototypes) (Collins and Singer, 1999;Haghighi and Klein, 2006), and those which imposeconstraints on the underlying unsupervised learningproblem (Chang et al 2007; Bellare et al 2009;Druck et al 2008; Ganchev et al 2010).Constraint-based weakly supervised learning hasbeen applied to some sequence labeling problems,through such methods as contrastive estimation(Smith and Eisner, 2005), generalized expectationcriteria (Mann and McCallum, 2008), alternatingprojections (Singh et al 2010), and posterior reg-ularization (Ganchev et al 2010).Perhaps the work that is most similar to this workis the study of code-switching within NLP literature.Most of the work done has been on automaticallyidentifying code-switch points (Joshi, 1982; Solorioand Liu, 2008).
The problem of identifying lan-guage in the presence of code-switching has seenthe most attention in the realm of speech process-ing (Chu et al 2007; Lyu and Lyu, 2008), amongmany others.
Though code-switching has been well-studied linguistically, it is only one possible rea-son to explain why a document contains multiplelanguages, and is actually one of the less commoncauses observed in our corpus.
For that reason, weapproach this problem more generally, assuming nospecific generative process behind multilingual text.3 Task DefinitionThe task we describe in this paper is a sequencelabeling problem, labeling a word in running textaccording to the language to which it belongs.
Inthe interest of being able to produce reliable hu-man annotations, we limit ourselves to texts withexactly two languages represented, though the tech-niques developed in this paper would certainly beapplicable to documents with more than two lan-guages.
The two languages represented in the paperare known a priori by the labeler and the only train-ing data available to the labeler is a small amountof sample text in each of the two languages repre-sented.In most NLP sequence labeling problems, the re-searchers can safely assume that each sequence (butnot each item in the sequence) is independent andidentically distributed (iid) according to some un-derlying distribution common to all the documents.For example, it is safe to assume that a sentencedrawn from WSJ section 23 can be labeled by amodel trained on the other sections.
With the taskof this paper we cannot assume that sequences fromdifferent documents are iid, (e.g.
One documentmay have 90% of its words in Basque, while anotheronly has 20%), but we do make the simplifying as-1111sumption that sequences within the same documentare iid.Because of this difference, the labeler is presentedeach document separately and must label its wordsindependently of any other document.
And the train-ing data for this task is not in the form of labeledsequences.
Rather, the models in this task are giventwo monolingual example texts which are used onlyto learn a model for individual instances.
Any se-quential dependencies between words must be boot-strapped from the document.
It is this aspect ofthe problem that makes it well-suited for weakly-supervised learning.It is worth considering whether this problem isbest approached at the word level, or if perhapssentence- or paragraph-level language identificationwould suffice for this task.
In those cases, we couldeasily segment the text at the sentence or paragraphlevel and feed those segments to an existing lan-guage identifier.
To answer this question we seg-mented our corpus into sentences by splitting at ev-ery period, exclamation point, or question mark (anoverly agressive approximation of sentence segmen-tation).
Even if every sentence was given the cor-rect majority label under this sentence segmentation,the maximum possible word-level accuracy that asentence-level classifier could achieve is 85.8%, andeven though this number reflects quite optimisticconditions, it is still much lower than the methodsof this paper are able to achieve.3.1 Evaluation DataTo build a corpus of mixed language documents, weused the BootCat tool (Baroni and Bernardini, 2004)seeded with words from a minority language.
Boot-Cat is designed to automatically collect webpageson a specific topic by repeatedly searching for key-words from a topic-specific set of seed words.
Wefound that this method works equally well for lan-guages as for topics, when seeded with words froma specific language.
Once BootCat returned a col-lection of documents, we manually identified docu-ments from the set that contained text in both the tar-get language and in English, but did not contain textin any other languages.
Since the problem becomestrivial when the languages do not share a characterset, we limited ourselves to languages with a Latinorthography.Language # words Language # wordsAzerbaijani 4114 Lingala 1359Banjar 10485 Lombard 18512Basque 5488 Malagasy 6779Cebuano 17994 Nahuatl 1133Chippewa 15721 Ojibwa 24974Cornish 2284 Oromo 28636Croatian 17318 Pular 3648Czech 886 Serbian 2457Faroese 8307 Slovak 8403Fulfulde 458 Somali 11613Hausa 2899 Sotho 8198Hungarian 9598 Tswana 879Igbo 11828 Uzbek 43Kiribati 2187 Yoruba 4845Kurdish 531 Zulu 20783Table 1: Languages present in the corpus and theirnumber of words before separating out English text.We found that there was an important balance tobe struck concerning the popularity of a language.
Ifa language is not spoken widely enough, then thereis little chance of finding any text in that language onthe Web.
Conversely if a language is too widely spo-ken, then it is difficult to find mixed-language pagesfor it.
The list of languages present in the corpusand the number of words in each language reflectsthis balance as seen in Table 1.For researchers who wish to make use this data,the set of annotations used in this paper is availablefrom the first author?s website1.3.2 AnnotationBefore the human annotators were presented withthe mixed-language documents fetched by Boot-Cat, the documents were first stripped of all HTMLmarkup, converted to Unicode, and had HTML es-cape sequences replaced with the proper Unicodecharacters.
Documents that had any encoding er-rors (e.g.
original page used a mixture of encodings)were excluded from the corpus.1http://www-personal.umich.edu/?benking/resources/mixed-language-annotations-release-v1.0.tgz1112ENG: because of LUTARU.Thank you ntate T.T!
Sevice...SOT: Retselisitsoemonethi ekare jwale hotla sebetswa ...ENG: Lesotho is heading 4 development #big-ups Mr ...SOT: Basotho bare monoana hao its?upe.ENG: Just do the job and lets see what you are made ...SOT: Malerato Mokoena Ntate Thabane, molimo ...ENG: It is God who reigns and if God is seen in your ...SOT: Mathabo Letsie http://www.facebook.com/taole.
...ENG: As Zuma did he should introduce a way of we can ...SOT: Msekhotho Matona a rona ha a hlomamisoe, re ...Table 2: An example of text from an annotatedEnglish-Sotho web page.Since there are many different reasons that thelanguage in a document may change (e.g.
code-switching, change of authors, borrowing) and manyvariations thereof, we attempted to create a broadset of annotation rules that would cover many cases,rather than writing a large number of very specificrules.
In cases when the language use was ambigu-ous, the annotators were instructed simply to maketheir best guess.
Table 2 shows an example of anannotated document.Generally, only well-digested English loanwordsand borrowings were to be marked as belonging tothe foreign language.
If a word appeared in the con-text of both languages, it was permissible for thatword to receive different labels at different times,depending on its context.Ordinary proper names (like ?John Williams?
or?Chicago?)
were to be marked as belonging to thelanguage of the context in which they appear.
Thisrule also applied to abbreviations (like ?FIFA?
or?BBC?).
The exception to this rule was propernames composed of common nouns (like ?Stairwayto Heaven?
or ?American Red Cross?)
and to abbre-viations that spelled out English words, which wereto be marked as belonging to the language of thewords they were composed of.The annotators were instructed not to assign la-bels to numbers or punctuation, but they were al-lowed to use numbers as punctuation as clues for as-signing other labels.3.3 Human AgreementTo verify that the annotation rules were reasonableand led to a problem that could potentially be solvedby a computer, we had each of the annotators markLanguage # words Language # wordsAzerbaijani 211 Lingala 1816Banjar 450 Lombard 2955Basque 1378 Malagasy 4038Cebuano 1898 Nahuatl 3544Chippewa 92 Ojibwa 167Cornish 2096 Oromo 1443Croatian 1505 Pular 1285Czech 1503 Serbian 1515English 16469 Slovak 1504Faroese 1585 Somali 1871Fulfulde 1097 Sotho 2154Hausa 2677 Tswana 2191Hungarian 1541 Uzbek 1533Igbo 2079 Yoruba 2454Kiribati 1891 Zulu 1075Kurdish 1674Table 3: Number of total words of training data foreach language.up a small shared set of a few hundred words fromeach of eight documents, in order to measure theinter-annotator agreement.The average actual agreement was 0.988, with 0.5agreement expected by chance for a kappa of 0.975.3.4 Training DataFollowing Scannell (2007), we collected smallmonolingual samples of 643 languages from foursources: the Universal Declaration of HumanRights2, non-English Wikipedias3, the Jehovah?sWitnesses website4, and the Rosetta project (Lands-bergen, 1989).Only 30 of these languages ended up being usedin experiments.
Table 3 shows the sizes of the mono-lingual samples of the languages used in this paper.2The Universal Declaration of Human Rights is a documentcreated by the United Nations and translated into many lan-guages.
As of February 2011 there were 365 versions availablefrom http://www.unicode.org/udhr/3As of February 2011, there were 113 Wikipedias in differ-ent languages.
Current versions of Wikipedia can be accessedfrom http://meta.wikimedia.org/wiki/List ofWikipedias4As of February 2011, there were 310 versions of the siteavailable at http://www.watchtower.org1113They range from 92 for Chippewa to 16469 for En-glish.
Most of the languages have between 1300 and1600 words in their example text.
To attempt to mit-igate variation caused by the sizes of these languagesamples, we sample an equal number of words withreplacement from each of English and a second lan-guage to create the training data.4 Word-level Language ClassificationWe shift our attention momentarily to a subproblemof the overall task: independent word-level languageclassification.
While the task of language identifica-tion has been studied extensively at the document,sentence, and query level, little or no work has beendone at the level of an individual word.
For this rea-son, we feel it is prudent to formally evaluate the fea-tures and classifiers which perform most effectivelyat the task of word language classification (ignoringany sequential dependencies at this point).4.1 FeaturesWe used a logistic regression classifier to experimentwith combinations of the following features: charac-ter unigrams, bigrams, trigrams, 4-grams, 5-grams,and the full word.
For these experiments, the train-ing data consisted of 1000 words sampled uniformlywith replacement from the sample text in the appro-priate languages.
Table 4 shows the accuracies thatthe classifier achieved when using different sets offeatures averaged over 10 independent runs.Features AccuracyUnigrams 0.8056Bigrams 0.8783Trigrams 0.84914-grams 0.78465-grams 0.6977{1,2,3,4,5}-grams 0.8817{1,2,3,4,5}-grams, word 0.8819Table 4: Logistic regression accuracy when trainedusing varying features.The use of all available features seems to be thebest option, and we use the full set of features inall proceeding experiments.
This result also concurswith the findigs of (Cavnar and Trenkle, 1994), who0 200 400 600 800 1,0000.70.80.9Sampled WordsAccuracylogistic regressionna?
?ve Bayesdecision treewinnow2Figure 1: Learning curves for logistic regression,na?
?ve Bayes, decision tree, and Winnow2 on the in-dependent word classification problem as the num-ber of sampled words in each training examplechanges from 10 to 1000.found 1-5-grams to be most effective for documentlanguage classification.4.2 ClassifiersUsing all available features, we compare four MAL-LET (McCallum, 2002) classifiers: logistic regres-sion, na?
?ve Bayes, decision tree, and Winnow2.
Fig-ure 1 shows the learning curves for each classifier asthe number of sampled words comprising each train-ing example is varied from 10 to 1000.Since a na?
?ve Bayes classifier gave the best per-formance in most experiments, we use na?
?ve Bayesas a representative word classifier for the rest of thepaper.5 MethodsMoving onto the main task of this paper, labelingsequences of words in documents according to theirlanguages, we use this section to describe our meth-ods.Since training data for this task is limited and isof a different type than the evaluation data (labeledinstances from monolingual example texts vs. la-beled sequences from the multilingual document),we approach the problem with weakly- and semi-supervised methods.1114The sequence labeling methods are presentedwith a few new sequence-relevant features, whichare not applicable to independent word classification(since these features do not appear in the trainingdata):?
a feature for the presence of each possible non-word character (punctuation or digit) betweenthe previous and the current words?
a feature for the presence of each possible non-word character between the current and nextwordsIn addition to independent word classification,which was covered in section 4, we also imple-ment a conditional random field model trained withgeneralized expectation criteria, a hidden Markovmodel (HMM) trained with expectation maximiza-tion (EM), and a logistic regression model trainedwith generalized expectation criteria.We had also considered that a semi-Markov CRF(Sarawagi and Cohen, 2004) could be useful ifwe could model segment lengths (a non-Markovianfeature), but we found that gold-standard segmentlengths did not seem to be distributed according toany canonical distribution, and we did not have a re-liable way to estimate these segment lengths.5.1 Conditional Random Field Model trainedwith Generalized ExpectationGeneralized expectation (GE) criteria (Druck et al2008) are terms added to the objective function ofa learning algorithm which specify preferences forthe learned model.
When the model is a linearchain conditional random field (CRF) model, we canstraightforwardly express these criteria in the objec-tive function with a KL-divergence term between theexpected values of the current model p?
and the pre-ferred model p?
(Mann and McCallum, 2008).O(?
;D,U) =?dlog p?(y(d)|x(d))?
?k ?k2?2?
?D(p?||p??
)Practically, to compute these expectations, weproduce the smoothed MLE on the output label dis-tribution for every feature observed in the trainingdata.
For example, the trigram ?ter?
may occur27 times in the English sample text and 34 timesin the other sample text, leading to an MLE ofp?
(eng|ter) ?
0.44.Because we do not expect the true marginal labeldistribution to be uniform (i.e.
the document maynot have equal numbers of words in each language),we first estimate the expected marginal label distri-bution by classifying each word in the document in-dependently using na?
?ve Bayes and taking the result-ing counts of labels produced by the classifier as anMLE estimate for it: p?
(eng) and p?
(non).We use these terms to bias the expected label dis-tributions over each feature.
Let Feng and Fnon re-spectively be the collections of all training data fea-tures with the two labels.
For every label l ?
L ={eng,non} and every feature f ?
Feng?Fnon, wecalculatep(l|f) =count(f,Fl) + ?count(f,?iFi) + ?|L|?p?
(l)puniform(l),the biased maximum likelihood expected outputlabel distribution.
To avoid having p(l|f) = 0,which can cause the KL-divergence to be undefined,we perform additive smoothing with ?
= 0.5 on thecounts before multiplying with the biasing term.We use the implementation of CRF with GE cri-teria from MALLET (McCallum, 2002), which usesa gradient descent algorithm to optimize the objec-tive function.
(Mann and McCallum, 2008; Druck,2011)5.2 Hidden Markov Model trained withExpectation MaximizationA second method we used was a hidden Markovmodel (HMM) trained iteratively using the Expec-tation Maximization algorithm (Dempster et al1977).
Here an HMM is preferable to a CRF be-cause it is a generative model and therefore uses pa-rameters with simple interpretations.
In the case ofan HMM, it is easy to estimate emission and transi-tion probabilities using an external method and thenset these directly.To initialize the HMM, we use a uniform distri-bution for transition probabilities, and produce theemission probabilities by using a na?
?ve Bayes clas-sifier trained over the two small language samples.1115In the expectation step, we simply pass the docu-ment through the HMM and record the labels it pro-duces for each word in the document.In the maximization step, we produce maximum-likelihood estimates for transition probabilities fromthe transitions between the labels produced.
Toestimate emission probabilities, we retrain a na?
?veBayes classifier on the small language samples alongthe set of words from the document that were labeledas being in the respective language.
We iterated thisprocess until convergence, which usually took fewerthan 10 iterations.We additionally experimented with a na?
?ve Bayesclassifier trained by EM in the same fashion, exceptthat it had no transition probabilities to update.
Thisclassifier?s performance was almost identical to thatof the GE-trained MaxEnt method mentioned in thefollowing section, so we omit it from the results andanalysis for that reason.5.3 Logistic Regression trained withGeneralized ExpectationGE criteria can also be straightforwardly applied tothe weakly supervised training of logistic regressionmodels.
The special case where the constraints spec-ified are over marginal label distributions, is calledlabel regularization.As with the CRF constraint creation, here we firstuse an ordinary supervised na?
?ve Bayes classifier inorder to estimate the marginal label distributions forthe document, which can be used to create more ac-curate output label expectations that are biased tothe marginal label distributions over all words in thedocument.We use the MALLET implementation of a GE-trained logistic regression classifier, which opti-mizes the objective function using a gradient descentalgorithm.5.4 Word-level ClassificationOur fourth method served as a baseline and didnot involve any sequence labeling, only independentclassification of words.
Since na?
?ve Bayes was thebest performer among word classification methods,we use that the representative of independent wordclassification methods.
The implementation of thena?
?ve Bayes classifier is from MALLET.0 200 400 600 800 1,0000.70.750.80.850.90.95Sampled WordsAccuracyna?
?ve BayesGE-trained logistic regressionEM-trained HMMGE-trained CRFFigure 2: Learning curves for na?
?ve Bayes, logisticregression trained with GE, HMM trained with EM,and CRF trained with GE as the number of sampledwords in each training example changes from 10 to1000.We also implemented a self-trained CRF, initiallytrained on the output of this na?
?ve Bayes classifier,and trained on its own output in subsequent itera-tions.
This method was not able to consistently out-perform the na?
?ve Bayes classifier after any numberof iterations.6 Evaluation and ResultsWe evaluated each method using simple token-levelaccuracy, i.e.
whether the correct label was assignedto a word in the document.
Word boundaries weredefined by punctuation or whitespace, and no tokenscontaining a digit were included.
Figure 2 displaysthe accuracy for each method as the number of sam-pled words from each language example is variedfrom 10 to 1000.In all the cases we tested, CRF trained with GEis clearly the most accurate option among the meth-ods examined, though the EM-trained HMM seemedto be approaching a similar accuracy with largeamounts of training data.
With a slight edge in ef-ficiency also in its favor, we think the GE+CRF ap-proach, rather than EM+HMM, is the best approachfor this problem because of its consistent perfor-mance across a wide range of training data sizes.In its favor, the EM+HMM approach has a slightly1116lower variance in its performance across differentfiles, though not at a statistically significant level.Contrary to most of the results in (Mann and Mc-Callum, 2010), a logistic regression classifier trainedwith GE did not outperform a standard supervisedna?
?ve Bayes classifier.
We suspect that this is dueto the different nature of this problem as comparedto most other sequence labeling problems, with theclassifier bootstrapping over a single document only.In the problems studied by Mann and McCallum, theGE-trained classifier was able to train over the entiretraining set, which was on average about 50,000 in-stances, far more than the number of words in theaverage document in this set (2,500).7 Error AnalysisIn order to analyze the types of mistakes that themodels made we performed an error analysis on tenrandomly selected files, looking at each mislabeledword and classifying the error according to its type.The results of this analysis are in Table 5.
The threeclasses of errors are (1) named entity errors, whena named entity is given a label that does not matchthe label it was given in the original annotation, (2)shared word errors, when a word that could belongto either language is classified incorrectly, and (3)other, a case that covers all other types of errors.Method NE SW OtherGE+CRF 41% 10% 49%EM+HMM 50% 14% 35%GE+MaxEnt 37% 12% 51%Na?
?ve Bayes 42% 17% 40%Table 5: Types of errors and their proportions amongthe different methods.
NE stands for Named Entity,SW stands for Shared Word, and Other covers allother types of errors.Our annotation rules for named entities specifiedthat named entities should be given a label match-ing their context, but this was rather arbitrary, andnot explicitly followed by any of the methods, whichtreat a named entity as if it was any other token.
Thiswas the one of most frequent types of error made byeach of the methods and in our conclusion in sec-tion 8, we discuss ways to improve it.In a regression analysis to determine which fac-tors had the greatest correlations with the GE-trained CRF performance, the estimated proportionof named entities in the document had by far thegreatest correlation with CRF accuracy of anythingwe measured.
Following that in decreasing order ofcorrelation strength were the cosine similarity be-tween English and the document?s second language,the number of words in the monolingual exampletext (even though we sampled from it), and the aver-age length of gold-standard monolingual sequencesin the document.The learning curve for GE-trained CRF in Fig-ure 2 is somewhat atypical as far as most machinelearning methods are concerned: performance istypically non-decreasing as more training data ismade available.We believe that the model is becoming over-constrained as more words are used to create theconstraints.
The GE method does not have a wayto specify that some of the soft constraints (for thelabels observed most frequently in the sample text)should be more important than other constraints(those observed less frequently).
When we mea-sure the KL-divergence between the label distribu-tions predicted by the constraints and the true la-bel distribution, we find that this divergence seemsto reach its minimum value between 600 and 800words, which is where the GE+CRF also seems toreach its maximum performance.The step with a na?
?ve Bayes classifier estimatingthe marginal label distribution ended up being quiteimportant overall.
Without it, the accuracy droppedby more than a full percentage point absolute.
Butthe problem of inaccurate constraint estimation isone that needs further consideration.
Some possibleways to address it may be to prune the constraintsaccording to their frequency or perhaps according toa metric like entropy, or to vary the GE-criteria coef-ficient in the objective function in order to penalizethe model less for varying from the expected model.8 ConclusionThis paper addresses three of the ongoing issuesspecifically mentioned by Hughes et al(2006) intheir survey of textual language identification.
Ourapproach is able to support minority languages; in1117fact, almost all of the languages we tested on wouldbe considered minority languages.
We also addressthe issue of sparse or impoverished training data.Because we use weakly-supervised methods, we areable to successfully learn to recognize a languagewith as few as 10 words of training data5.
The lastand most obvious point we address is that of multi-lingual documents, which is the focus of the paper.We present a weakly-supervised system for iden-tifying the languages of individual words in mixed-language documents.
We found that across a broadrange of training data sizes, a CRF model trainedwith GE criteria is an accurate sequence classifierand is preferable to other methods for several rea-sons.One major issue to be improved upon in futurework is how named entities are handled.
A straight-forward way to approach this may be to create an-other label for named entities, which (for the pur-poses of evaluation) would be considered not to be-long to any of the languages in the document.
Wecould simply choose not to evaluate a system on thenamed entity tokens in a document.
Alternatively,the problem of language-independent named entityrecognition has received some attention in the past(Tjong Kim Sang and De Meulder, 2003), and it maybe beneficial to incorporate such a system in a robustword-level language identification system.Going forward, an issue that needs to be ad-dressed with this method is its dependence on know-ing the set of possible languages a priori.
Becausewe don?t see an easy way to adapt this method to ac-curately label words in documents from a possibleset of thousands of languages when the documentitself may only contain two or three languages, wewould propose the following future work.We propose a two-step approach to general word-level language identification.
The first step would beto examine a multilingual document, and with highaccuracy, list the languages that are present in thedocument.
The second step would be identical to theapproach described in this paper (but with the two-language restriction lifted), and would be responsi-ble for labeling the languages of individual words,using the set of languages provided by the first step.5With only 10 words of each language as training data, theCRF approach correctly labels 88% of wordsReferencesMarco Baroni and Silvia Bernardini.
2004.
Bootcat:Bootstrapping corpora and terms from the web.
InProceedings of the Fourth International Conferenceon Langauge Resources and Evaluation (LREC 2004),volume 4, pages 1313?1316, Lisbon, Portugal.Kenneth R. Beesley.
1988.
Language identifier: A com-puter program for automatic natural-language identifi-cation of on-line text.
In Proceedings of the 29th An-nual Conference of the American Translators Associa-tion, volume 47, page 54.Kedar Bellare, Gregory Druck, and Andrew McCallum.2009.
Alternating projections for learning with expec-tation constraints.
In Proceedings of the Twenty-FifthConference on Uncertainty in Artificial Intelligence,pages 43?50.
AUAI Press.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of theThird Annual Symposium on Document Analysis andInformation (SDAIR 94), pages 161?175, Las Vegas,Nevada.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 280?287, Prague, Czech Republic, June.Association for Computational Linguistics.Chyng-Leei Chu, Dau-cheng Lyu, and Ren-yuan Lyu.2007.
Language identification on code-switchingspeech.
In Proceedings of ROCLING.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 100?110.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the em algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 1?38.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using general-ized expectation criteria.
In Proceedings of the 31stannual international ACM SIGIR conference on Re-search and development in information retrieval (SI-GIR 2008), pages 595?602.
ACM.Gregory Druck.
2011.
Generalized Expectation Criteriafor Lightly Supervised Learning.
Ph.D. thesis, Univer-sity of Massachusetts Amherst.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical report.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.1118Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA, June.
Association for Computational Lin-guistics.A.S.
House and E.P.
Neuburg.
1977.
Toward automaticidentification of the language of an utterance.
i. pre-liminary methodological considerations.
The Journalof the Acoustical Society of America, 62:708.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proc.
International Conference on Lan-guage Resources and Evaluation, pages 485?488.Aravind K. Joshi.
1982.
Processing of sentences withintra-sentential code-switching.
In Proceedings of the9th conference on Computational linguistics-Volume1, pages 145?150.
Academia Praha.Jan Landsbergen.
1989.
The rosetta project.
pages 82?87, Munich, Germany.Dau-Cheng Lyu and Ren-Yuan Lyu.
2008.
Languageidentification on code-switching utterances using mul-tiple cues.
In Ninth Annual Conference of the Interna-tional Speech Communication Association.Gideon S. Mann and Andrew McCallum.
2008.
Gener-alized expectation criteria for semi-supervised learn-ing of conditional random fields.
In Proceedings ofACL-08: HLT, pages 870?878, Columbus, Ohio, June.Association for Computational Linguistics.Gideon S. Mann and Andrew McCallum.
2010.
Gener-alized expectation criteria for semi-supervised learn-ing with weakly labeled data.
The Journal of MachineLearning Research, pages 955?984.Andrew McCallum.
2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.Arjen Poutsma.
2002.
Applying monte carlo techniquesto language identification.
Language and Computers,45(1):179?189.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
Advances in Neural Information ProcessingSystems (NIPS 2004), 17:1185?1192.Kevin P. Scannell.
2007.
The cru?bada?n project: Cor-pus building for under-resourced languages.
In Build-ing and Exploring Web Corpora: Proceedings of the3rd Web as Corpus Workshop, volume 4, pages 5?15,Louvain-la-Neuve, Belgium.Sameer Singh, Dustin Hillard, and Chris Leggetter.
2010.Minimally-supervised extraction of entities from textadvertisements.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 73?81, Los Angeles, California, June.
As-sociation for Computational Linguistics.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 354?362, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Thamar Solorio and Yang Liu.
2008.
Learning to pre-dict code-switching points.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 973?981, Honolulu, Hawaii,October.
Association for Computational Linguistics.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In WalterDaelemans and Miles Osborne, editors, Proceed-ings of the Seventh Conference on Natural LanguageLearning at HLT-NAACL 2003, pages 142?147.Fei Xia, William Lewis, and Hoifung Poon.
2009.
Lan-guage ID in the context of harvesting language dataoff the web.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL (EACL 2009),pages 870?878, Athens, Greece, March.
Associationfor Computational Linguistics.1119
