Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 265?277,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsError-Driven Analysis of Challenges in Coreference ResolutionJonathan K. Kummerfeld and Dan KleinComputer Science DivisionUniversity of California, BerkeleyBerkeley, CA 94720, USA{jkk,klein}@cs.berkeley.eduAbstractCoreference resolution metrics quantify errorsbut do not analyze them.
Here, we consideran automated method of categorizing errors inthe output of a coreference system into intu-itive underlying error types.
Using this tool,we first compare the error distributions acrossa large set of systems, then analyze commonerrors across the top ten systems, empiricallycharacterizing the major unsolved challengesof the coreference resolution task.1 IntroductionMetrics produce measurements that concisely sum-marize performance on the full range of error types,and for coreference resolution there has been ex-tensive work on developing effective metrics (Luo,2005; Recasens and Hovy, 2011).
However, it is alsovaluable to tease apart the errors to understand theirrelative importance.Previous investigations of coreference errors havefocused on quantifying the importance of subtaskssuch as named entity recognition and anaphoricitydetection, typically by measuring accuracy improve-ments when partial gold annotations are provided(Stoyanov et al 2009; Pradhan et al 2011; Prad-han et al 2012).
For coreference resolution thedrawback of this approach is that decisions are ofteninterdependent, and so even partial gold informationis extremely informative.
Also, previous work onlyconsidered errors by counting links, which does notcapture certain errors in a natural way, e.g.
whena system incorrectly divides a large entity into twoparts, each with multiple mentions.
Recent work hasconsidered some of these issues, but only with smallscale manual analysis (Holen, 2013).We present a new tool that automatically classifieserrors in the standard output of any coreference res-olution system.
Our approach is to identify changesthat convert the system output into the gold annota-tions, and map the steps in the conversion onto lin-guistically intuitive error types.
Since our tool usesonly system output, we are able to classify errorsmade by systems of any architecture, including bothsystems that use link-based inference and systemsthat use global inference methods.Using our tool we perform two studies to un-derstand similarities and differences between sys-tems.
First, we compare the error distributions oncoreference resolution of all of the systems from theCoNLL 2011 shared task plus several publicly avail-able systems.
This comparison adds to the analy-sis from the shared task by illustrating the substan-tial variation in the types of errors different systemsmake.
Second, we investigate the aggregate behav-ior of ten state-of-the-art systems, providing a de-tailed characterization of each error type.
This in-vestigation identifies key outstanding challenges andpresents the impact that solving each of them wouldhave in terms of changes in the standard coreferenceresolution metrics.We find that the best systems are not best acrossall error types, that a large proportion of span errorsare due to superficial parse differences, and that thebiggest performance loss is on missed entities thatcontain a small number of mentions.This work presents a comprehensive investiga-tion of common errors in coreference resolution,identifying particular issues worth focusing on infuture research.
Our analysis tool is available atcode.google.com/p/berkeley-coreference-analyser/.2652 BackgroundMost coreference work focuses on accuracy im-provements, as measured by metrics such as MUC(Vilain et al 1995), B3 (Bagga and Baldwin, 1998),CEAF (Luo, 2005), and BLANC (Recasens andHovy, 2011).
The only common forms of furtheranalysis are results for anaphoricity detection andscores for each mention type (nominal, pronoun,proper).
Two exceptions are: the detailed analysis ofthe Reconcile system by Stoyanov et al(2009), andthe multi-system comparisons in the CoNLL sharedtask reports (Pradhan et al 2011, 2012).A common approach to performance analysis is tocalculate scores for nominals, pronouns and propernames separately, but this is a very coarse division(Ng and Cardie, 2002; Haghighi and Klein, 2009).More fine consideration of some subtasks does oc-cur, for example, anaphoricity detection, which hasbeen recognized as a key challenge in coreferenceresolution for decades and regularly has separate re-sults reported (Paice and Husk, 1987; Sobha et al2011; Yuan et al 2012; Bjo?rkelund and Farkas,2012; Zhekova et al 2012).
Some work has alsoincluded anecdotal discussion of specific error typesor manual classification of a small set of errors, butthese approaches do not effectively quantify the rel-ative impact of different errors (Chen and Ng, 2012;Martschat et al 2012; Haghighi and Klein, 2009).In a recent paper, Holen (2013) presented a detailedmanual analysis that considered a more comprehen-sive set of error types, but their focus was on explor-ing the shortcomings of current metrics, rather thanunderstanding the behavior of current systems.The detailed investigation presented by Stoyanovet al(2009) is the closest to the work we presenthere.
First, they measured accuracy improvementswhen their system was given gold annotations forthree subtasks of coreference resolution: mentiondetection, named entity recognition, and anaphoric-ity detection.
To isolate other types of errors they de-fined resolution classes, based on both the type of amention, and properties of possible antecedents (forexample, nominals that have a possible antecedentthat is an exact string match).
For each resolutionclass they measured performance while giving thesystem gold annotations for all other classes.
Whilethis approach is effective at characterizing variationsPresident Clinton1 is questioning the legitimacyof George W. Bush?s election victory.
Speakinglast night to Democratic supporters in Chicago,he said Bush won the election only because Re-publicans stopped the vote-counting in Florida,and Mr. Clinton1 praised Al Gore?s campaignmanager, Bill Daley, for the way he handled theelection.
?I2 want to thank Bill Daley for his ex-emplary service as Secretary of Commerce.
Hewas brilliant.
I2 think he did a brilliant job inleading Vice President Gore to victory myself2.
?Figure 1: Two coreference errors.
Mentions are under-lined and subscripts indicate entities.
One error is a men-tion missing from the system output, he.
The other is thedivision of references to Bill Clinton into two entities.between the nine classes they defined, it misses thecascade effect of errors that only occur when allmentions are being resolved at once.The only multi-system comparisons are theCoNLL task reports (Pradhan et al 2011, 2012),which explored the impact of mention detection andanaphoricity detection through subtasks with differ-ent types of gold annotation.
With a large set of sys-tems, and well controlled experimental conditions,the tasks provided a great snapshot of progress in thefield, which we aim to supplement by characterizingthe major outstanding sources of error.This work adds to previous investigations by pro-viding a comprehensive and detailed analysis of er-rors.
Our tool can automatically analyze any sys-tem?s output, giving a reliable estimate of the rela-tive importance of different error types.3 Error ClassificationWhen inspecting the output of coreference resolu-tion systems, several types of errors become imme-diately apparent: entities that have been divided intopieces, spurious entities, non-referential pronounsthat have been assigned antecedents, and so on.
Ourgoal in this work is to automatically assign intuitivelabels like these to errors in system output.A simple approach, refining results by measur-ing the accuracy of subsets of the mentions, can bemisleading.
For example, in Figure 1, we can in-tuitively see two pronoun related mistakes: a miss-ing mention (he), and a divided entity where the twopieces are the blue pronouns (I2, I2, myself2) and thered proper names (President Clinton1, Mr. Clinton1).266Simply counting the number of incorrect pronounlinks would miss the distinction between the twotypes of mistakes present.One question in designing an error analysis toollike ours is whether to operate on just system output,or to also consider intermediate system decisions.We focused on using system output because othermethods cannot uniformly apply to the full range ofcoreference resolution decoding methods, from linkbased methods to global inference methods.Our overall approach is to transform the sys-tem output into the gold annotations, then mapthe changes made in the conversion process to er-rors.
The transformation process is presented in Sec-tion 3.1 and Figure 2, and the mapping process isdescribed in Section 3.2 and Figure 3.3.1 TransformationsThe first part of our error classification process de-termines the changes needed to transform the systemoutput into the gold annotations.
This five stage pro-cess is described below, and an abstract example ispresented in Figure 2.1.
Alter Span transforms an incorrect systemmention into a gold mention that has the samehead token.
In Figure 2 this stage is demon-strated by a mention in the leftmost entity,which has its span altered, indicated by thechange from an X to a light blue circle.2.
Split breaks the system entities into pieces,each containing mentions from a single goldentity.
In Figure 2 there are three changes inthis stage: the leftmost entity is split into a redpiece and a light blue piece, the middle entityis split into a dark red piece and an X, and therightmost entity is split into singletons.3.
Remove deletes every mention that is notpresent in the gold annotations.
In Figure 2 thismeans the four singleton X?s are removed.4.
Introduce creates a singleton entity for eachmention that is missing from the system output.In Figure 2 this stage involves the introductionof a light blue mention and two white mentions.5.
Merge combines entities to form the final,completely correct, set of entities.
In Figure 2the two red entities are merged, the singletonMentionsSpurious mentionEntity1.
Alter Span2.
Split3.
Remove4.
Introduce5.
MergeXXXXXX XXXX XXXGold entities indicated using common shadingXKeySystemOutputGoldEntitiesFigure 2: Abstract example of the transformation processthat converts system output (at the top) to gold annota-tions (at the bottom).blue entity is merged with the rest of the blueentity, and the two white mentions are merged.267Operation(s) Error System Goldi) Alter Span Span error Gorbachev Soviet leader Gorbachevii)Multiple IntroducesMissing Entity- the pillsand Merges - the tranquilizing pillsiii)Multiple SplitsExtra Entityhuman rights -and Removes Human Rights -Introduceand Mergethe Arab region the Arab regioniv) Missing Mention the region the region- itSplit andRemoveher story her storyv) Extra Mention this thisit -vi) Merge Divided EntityIraq1 Iraq1this nation2 this nation1the nation2 the nation1its1 its1vii) Split Conflated EntitiesMohammed Rashid1 Mohammed Rashid1the Rashid case1 the Rashid case2Rashid1 Rashid1the case1 the case2Figure 3: Examples of the error types.
In examples (i) - (iv) and (vi) the system output contains a single entity.
Whenmultiple entities are involved, they are marked with subscripts.
Mentions are in the order in which they appear in thetext.
All examples are from system output on the dev set of the CoNLL task.One subtle point in the split stage is how to recordan entity being split into several pieces.
This couldeither be a single operation, one entity being splitinto N pieces, or N ?1 operations, each involving asingle piece being split off from the rest of the entity.We use the second approach, as it fits more naturallywith the error mapping we describe in the follow-ing section.
Similarly, for the merge operation, werecord N entities being merged as N?1 operations.3.2 MappingThe operations in Section 3.1 are mapped onto sevenerror types.
In some cases, a single change mapsonto a single error, while in others a single error rep-resents several closely related operations from adja-cent stages in the error correction process.
The map-ping is described below and in Figure 3.1.
Span Error.
Each Alter Span operation ismapped to a Span Error, e.g.
in Figure 3(i), thesystem mention Gorbachev is replaced by theannotated mention Soviet leader Gorbachev.2.
Missing Entity.
A set of Introduce and Mergeoperations that forms an entirely new entity,e.g.
the white entity in Figure 2, and the pillsin Figure 3(ii).
This error is still assigned ifthe new entity includes pronouns that were al-ready present in the system output.
The rea-soning for this is that most pronouns in the cor-pus are coreferent, so including just the pro-nouns from an entity is not meaningfully dif-ferent from missing the entity entirely.3.
Extra Entity.
A set of Split and Remove oper-ations that completely remove an entity, e.g.
therightmost entity in Figure 2, and Figure 3(iii).As for the Missing Entity error type, this erroris still assigned if the original entity containedpronouns that were valid.4.
Missing Mention.
An Introduce and a Mergethat apply to the same mention, e.g.
it in Fig-ure 3(iv), and the blue mention in Figure 2.5.
Extra Mention.
A Split and a Remove that ap-ply to the same mention, e.g.
it in Figure 3(v),and the X in the red entity in Figure 2.6.
Divided Entity.
Each remaining Merge oper-ation is mapped to a Divided Entity error, e.g.Figure 3(vi), and the red entity in Figure 2.7.
Conflated Entities.
Each remaining Split op-eration is mapped to a Conflated Entity error,e.g.
Figure 3(vii), and the blue and red entitiesin Figure 2.2684 MethodologyOur tool processes the CoNLL task output, with noother information required.
During development,and when choosing examples for this paper, weused the development set of the CoNLL shared task(Hovy et al 2006; Pradhan et al 2007; Pradhan etal., 2011).
The results we present in the rest of thepaper are all for the test set.
Using the developmentset would have been misleading, as the entrants inthe shared task used it to tune their systems.4.1 SystemsWe analyzed all of the 2011 CoNLL task systems, aswell as several publicly available systems.
For theshared task systems we used the output data fromthe task itself, provided by the organizers.
For thepublicly available systems we used the default con-figurations.
Finally, we included another run of theStanford system, with their OntoNotes-tuned param-eters (STANFORD-T).The publicly available systems we used are:BERKELEY (Durrett and Klein, 2013), IMS(Bjo?rkelund and Farkas, 2012), STANFORD (Leeet al 2013), RECONCILE (Stoyanov et al 2010),BART (Versley et al 2008), UIUC (Bengtson andRoth, 2008), and CHERRYPICKER (Rahman andNg, 2009).
The systems from the shared task arelisted in Table 1 and in the references.5 Broad System ComparisonTable 1 presents the frequency of errors for each sys-tem and F-Scores for standard metrics1 on the testset of the 2011 CoNLL shared task.
Each bar isfilled in proportion to the number of errors the sys-tem made, with a full bar corresponding to the num-ber of errors listed in the bottom row.The metrics provide an effective overall rank-ing, as the systems with high scores generally makefewer errors.
However, the metrics do not conveythe significant variation in the types of errors sys-tems make.
For example, YANG and CHARTON areassigned almost the same scores, but YANG makesmore than twice as many Extra Mention errors.1CEAF and BLANC are not included as the most recent ver-sion of the CoNLL scorer (v5) is incorrect, and there are nostandard implementations available.The most frequent error across all systems is Di-vided Entity.
Unlike parsing errors (Kummerfeld etal., 2012), improvements are not monotonic, withbetter systems often making more errors of one typewhen decreasing the frequency of another type.One outlier is the Irwin et al(2011) system,which makes very few mistakes in five categories,but many in the last two.
This reflects a high pre-cision, low recall approach, where clusters are onlyformed when there is high confidence.The third section of Table 1 shows results for sys-tems that were run with gold noun phrase span in-formation.
This reduces all errors slightly, thoughmost noticeably Extra Mention, Missing Mention,and Span Error.
On inspection of the remainingSpan Errors we found that many are due to incon-sistencies regarding the inclusion of the possessive.The final section of the table shows results for sys-tems that were provided with the set of mentions thatare coreferent.
In this setting, three of the error typesare not present, but there are still Missing Mentionsand Missing Entities because systems do not alwayschoose an antecedent, leaving a mention as a single-ton, which is then ignored.While this broad comparison gives a completeview of the range of errors present, it is still a coarserepresentation.
In the next section, we characterizethe common errors on a finer level by breaking downeach error type by a range of properties.6 Common ErrorsTo investigate the aggregate state of the art, in thissection we consider results averaged over the topten systems: CAI, CHANG, IMS, NUGUES, SAN-TOS, SAPENA, SONG, STANFORD-T, STOYANOV,URYUPINA-OPEN.2 These systems represent a broadrange of approaches, all of which are effective.In each section below, we focus on one or twoerror types, characterizing the mistakes by a rangeof properties.
We then consider a few questions thatapply across multiple error types.6.1 Span ErrorsTo characterize the Span Errors, we considered thetext that is in the gold mention, but not the system2For systems that occur multiple times in Table 1, we onlyuse the best instance.
The BERKELEY system was not includedas it had not been published at submission time.269Metric F-Scores Span Conflated Extra Extra Divided Missing MissingSystem Mention MUC B3 Error Entities Mention Entity Entity Mention EntityPUBLICLY AVAILABLE SYSTEMSBERKELEY 75.57 66.43 66.17IMS 72.96 64.71 64.73STANFORD-T 71.21 61.40 63.06STANFORD 58.56 48.37 56.42RECONCILE 46.45 49.40 54.90BART 56.61 46.00 52.56UIUC 50.60 45.21 52.88CHERRYPICKER 41.10 40.71 51.39CONLL, PREDICTED MENTIONSLEE-OPEN 70.94 61.03 62.96LEE 70.70 59.56 61.88SAPENA 43.20 59.54 61.28SONG 67.26 59.95 60.08CHANG 64.86 57.13 61.75CAI-OPEN 67.45 57.86 60.89NUGUES 68.96 58.61 59.75URYUPINA-OPEN 68.39 57.63 58.74SANTOS 65.45 56.65 59.48STOYANOV 67.78 58.43 57.35HAO 64.30 54.46 55.82YANG 63.93 52.31 55.85CHARTON 64.36 52.49 55.61KLENNER-OPEN 62.28 49.86 55.62SOBHA 64.83 50.48 54.85ZHOU 62.31 48.96 53.42KOBDANI 61.03 48.62 53.00ZHANG 61.13 47.88 52.76XINXIN 61.92 46.62 51.50KUMMERFELD 62.72 42.70 50.05IRWIN-OPEN 35.27 27.21 44.29ZHEKOVA 48.29 24.08 41.42IRWIN 26.67 19.98 42.73CONLL, GOLD NP SPANSLEE-OPEN 75.39 65.39 65.88LEE 75.16 63.90 64.70NUGUES 72.42 62.12 61.67CHANG 67.91 59.77 62.97SANTOS 67.80 59.52 61.35STOYANOV 70.29 61.53 59.07SONG 66.68 55.48 58.04KOBDANI 66.08 53.94 55.82ZHANG 64.89 51.64 54.77ZHEKOVA 62.67 35.22 45.80CONLL, GOLD MENTIONSLEE-OPEN 90.93 81.56 75.95CHANG 99.97 82.52 73.68Most Errors 2410 3849 2744 5290 4789 2026 3237Table 1: Counts for each error type on the test set of the 2011 CoNLL task.
Bars indicate the number of errors, withwhite as zero and fully filled as the number in the Most Errors row.
-OPEN indicates a system using external resources.270Type Missing ExtraNP 65.8 45.0POS 12.4 96.9, 71.2 22.4SBAR 55.9 1.9PP 46.2 10.3DT 17.0 35.9Total 271.1 224.6Table 2: Counts of Span Errors grouped by the label overthe extra/missing part of the mention.mention (missing text), and vice versa (extra text).We then found nodes in the gold parse that cov-ered just this extra/missing text, e.g.
in Figure 3(i)we would consider the node over Soviet leader.
InTable 2 we show the most frequent parse nodes.Some of these differences are superficial, such asthe possessive and the punctuation.
Others, such asthe missing PP and SBAR cases, may be due to parseerrors.
Of the system mentions involved in span er-rors, 27.0% do not correspond to a node in the goldparse.
The frequency of punctuation errors couldalso be parse related, because punctuation is not con-sidered in the standard parser evaluation.Overall it seems that span errors can best be dealtwith by improving parsing, though it is not possi-ble to completely eliminate these errors because ofinconsistent annotations.6.2 Extra Mention and Missing MentionWe consider Extra and Missing Mentions togetheras they mirror each other, forming a precision-recalltradeoff, where a high precision system will havefewer Extra Mentions and more Missing Mentions,and a high recall system will have the opposite.Table 3 divides these errors by the type of men-tion involved and presents some of the most fre-quent Extra Mentions and Missing Mentions.
Forthe corpus statistics we count as mentions all NPspans in the gold parse plus any word tagged withPRP, WP, WDT, or WRB (following the definitionof gold mention boundaries for the CoNLL tasks).The mentions it and you are the most commonerrors, matching observations from several of thepapers cited in Section 2.
However, there is a sur-prising imbalance between Extra and Missing cases,e.g.
it accounts for a third of the extra errors, butonly 12% of the Missing errors.
This imbalance mayAv.
Errors Corpus StatsMention Extra Missing Count % Coref.Proper Name 281.6 297.7 6915 59.0Nominal 484.2 516.5 33328 15.9Pronoun 390.7 323.3 9926 69.7it 130.4 38.9 1211 57.1you 85.2 55.9 1028 44.9we 39.6 19.6 691 64.7us 23.2 3.2 242 23.6that 13.8 13.4 2010 11.5they 9.6 39.5 738 94.3their 8.6 21.5 410 95.1Total 1156.5 1137.5 50169 32.5Table 3: Counts of Missing and Extra Mention errors bymention type, and the most common mentions.Proper Name NominalExtra Missing Extra MissingText match 145.2 163.6 171.2 96.1Head match 56.8 70.7 149.6 166.0Other 79.6 63.4 163.4 254.4NER Matches 143.4 174.4 23.0 32.0NER Differs 6.6 6.1 2.4 0.0NER Unknown 131.6 117.2 458.8 484.5Total 281.6 297.7 484.2 516.5Table 4: Counts of Extra and Missing Mentions, groupedby properties of the mention and the entity it is in.be the result of systems being tuned to the metrics,which seem to penalize Missing Mentions more thanExtra Mentions (shown in Section 6.7).In Table 4 we consider the Extra Mention er-rors and Missing Mention errors involving propernames and nominals.
The top section counts errorsin which the mention involved in the error has anexact string match with a mention in the cluster, orwhether it has just a head match.
The second sec-tion of the table considers the named entity anno-tations in OntoNotes, counting how often the men-tion?s type matches the type of the cluster.In all cases shown in the table it appears that sys-tems are striking a balance between these two typesof errors.
One exception may be the use of exactstring matching for nominals, which seems to be bi-ased towards Extra Mentions.For these two error types, our observations agreewith previous work: the most common specific erroris the identification of pleonastic pronouns, namedentity types are of limited use, and head matching isalready being used about as effectively as it can be.271Composition Av.
ErrorsName Nom Pro Extra Missing0 1 1 70.7 271.61 0 1 13.2 28.11 1 0 26.6 86.22 0 0 61.3 89.30 2 0 512.0 347.90 0 2 110.9 13.63+ 0 0 14.7 14.40 3+ 0 154.8 65.90 0 3+ 91.0 18.1Other 51.8 216.4Total 1107.0 1151.5Table 5: Counts of Extra and Missing Entity errors,grouped by the composition of the entity (Names, Nomi-nals, Pronouns).Match Type Extra MissingProper Name 51.4 42.2Exact Nominal 338.3 49.5Pronoun 141.9 10.3HeadProper Name 14.4 27.3Nominal 234.7 129.0Proper Name 10.2 34.2None Nominal 92.8 235.3Pronoun 60.0 21.4Table 6: Counts of Extra and Missing Entity errorsgrouped by properties of the mentions in the entity.6.3 Extra Entities and Missing EntitiesIn this section, we consider the errors that involve anentire entity that was either missing from the systemoutput or does not exist in the annotations.Table 5 counts these errors based on the compo-sition of the entity.
There are several noticeable dif-ferences between the two error types, e.g.
for entitiescontaining one nominal and one pronoun (row 0 1 1)there are far more Missing errors than Extra errors,while entities containing two pronouns (row 0 0 2)have the opposite trend.It is clear that entities consisting of a single typeof mention are the primary source of these errors,accounting for 85.3% of the Extra Entity errors,and 47.7% of Missing Entity errors.
Table 6 showscounts for these cases divided into three groups:when all mentions are identical, when all mentionshave the same head, and the rest.Nominals are the most frequent type in Table 6,and have the greatest variation across the three sec-Mention Extra Missingthat 6.9 99.7it 47.7 47.8this 0.9 36.2they 3.8 29.1their 2.1 23.5them 0.9 13.8Any pronoun 83.9 299.7Table 7: Counts of common Missing and Extra Entityerrors where the entity has just two mentions: a pronounand either a nominal or a proper name.tions of the table.
For the Extra column, Exact matchcases are a major challenge, accounting for over halfof the nominal errors.
These errors include caseslike the example below, where two mentions are notconsidered coreferent because they are generic:everybody tends to mistake the part for the whole.Here, mistaking the part for the whole is ...For missing entities we see the opposite trend,with Exact match cases accounting for less than 12%of nominal errors.
Instead, cases with no match arethe greatest challenge, such as this example, whichrequires semantic knowledge to correctly resolve:The charges related to her sale of ImClone stock.She sold the share a day before ...The other common case in Table 5 is an entitycontaining a pronoun and a nominal.
In Table 7 wepresent the most frequent pronouns for this case andthe similar case involving a pronoun and a name.One way of interpreting these errors is fromthe perspective of the pronoun, which is eitherincorrectly coreferent (Extra), or incorrectly non-coreferent (Missing).
From this perspective, theseerrors are similar in nature to those described by Ta-ble 3.
However, the distribution of errors is quite dif-ferent, with it being balanced here where previouslyit skewed heavily towards extra mentions, while thatwas balanced in Table 3 but is skewed towards beingpart of Missing Entities here.Extra Entity errors and Missing Entity errors areparticularly challenging because they are dominatedby entities that are either just nominals, or a nominaland a pronoun, and for these cases the string match-ing features are often misleading.
This implies thatreducing Extra Entity and Missing Entity errors willrequire the use of discourse, context, and semantics.272Incorrect Part Rest of Entity Av.
ErrorsNa No Pr Na No Pr Conflated Divided- - 1+ - - 1+ 312.7 69.9- - 1+ - 1+ 1+ 238.5 179.8- - 1+ - 1+ - 189.6 549.3- 1+ - - 1+ - 181.5 156.5- - 1+ 1+ 1+ 1+ 143.6 181.5- - 1+ 1+ - 1+ 109.7 150.5- - 1+ 1+ - - 60.0 136.5Other 454.8 657.7Total 1690.4 2081.7Table 8: Counts of Conflated and Divided entities errorsgrouped by the Name / Nominal / Pronoun compositionof the parts involved.6.4 Conflated Entities and Divided EntitiesTable 8 breaks down the Conflated Entities errorsand Divided Entity errors by the composition of thepart being split/merged and the rest of the entity in-volved.
Each 1+ indicates that at least one mentionof that type is present (Name / Nominal / Pronoun).Clearly pronouns being placed incorrectly is thebiggest issue here, with almost all of the commonerrors involving a part with just pronouns.
It is alsoclear that not having proper names in the rest ofthe entity presents a challenge.
One particularly no-ticeable issue involves entities composed entirely ofpronouns, which are often created by systems con-flating the pronouns of two entities together.Table 8 aggregates errors by the presence of dif-ferent types of mentions.
Aggregating instead by theexact composition of the incorrect part being con-flated or divided we found that instances with a partcontaining a single pronoun account for 38.9% ofconflated cases and 35.8% of divided cases.Finally, it is worth noting that in many cases a partis both conflated with the wrong entity, and dividedfrom its true entity.
Only 12.6% of Conflated Entityerrors led to a complete gold entity with no other er-rors, and only 21.3% of Divided Entity errors camefrom parts that were not involved in another error.Conflated Entities and Divided Entities are domi-nated by pronoun link errors: cases where a pronounwas placed in the wrong entity.
Finding finer charac-terizations of these errors is difficult, as almost anydivision produces sparse counts, reflecting the longtail of mistakes that make up these two error types.Gold System Decision CountCataphoricSame referent 10.6Different referent 13.4Not cataphoric 208.2Not present 42.8Not cataphoric Cataphoric 46.2Not present Cataphoric 186.8Table 9: Occurrence of mistakes involving cataphora.6.5 CataphoraCataphora (when an anaphor precedes its an-tecedent) is a pronoun-specific problem that doesnot fit easily in the common left-to-right coreferenceresolution approach.
In the CoNLL test set, 2.8% ofthe pronouns are cataphoric.
In Table 9 we showhow well systems handle this challenge by countingmentions based on whether they are cataphoric inthe annotations, are cataphoric in the system output,and whether the antecedents match.Systems handle cataphora poorly, missing almostall of the true instances, and introducing a largenumber of extra cases.
However, this issue is a fairlysmall part of the task, with limited metric impact.6.6 Entity PropertiesGender, number, person, and named entity type areproperties commonly used in coreference resolutionsystems.
In some cases, two mentions with differ-ent properties are placed in the same entity.
Someof these cases are correct, such as variation in per-son between mentions inside and outside of quotes.However, many of these cases are errors.
In Table 11we present the percentage of entities that containmentions with properties of more than one type.
Fornamed entity types we considered the annotations inOntoNotes; for the other properties we derive themfrom the pronouns in each cluster.For all of the properties, there are many entitiesthat we could not assign a value to, either becauseno named entity information was available, or be-cause no pronouns with an unambiguous value forthe property were present.
For named entity infor-mation, OntoNotes only has annotations for 68% ofgold entities, suggesting that named entity taggersare of limited usefulness, matching observations onthe MUC and ACE corpora (Stoyanov et al 2009).The results in the ?Gold?
column of Table 11 in-273Mentions MUC B3Error type P R F P R F P R FSpan Error 2.8 2.8 2.7 2.8 2.8 2.8 1.0 2.0 1.6Conflated Entities 1.7 0.0 0.8 9.9 0.0 4.5 15.9 0.0 6.2Extra Mention 5.5 0.0 2.6 6.4 0.0 3.0 5.3 0.0 2.2Extra Entity 15.3 0.0 7.0 11.4 0.0 5.2 6.1 0.0 2.4Divided Entity 1.8 6.8 4.3 5.7 16.8 10.9 -10.0 21.6 4.5Missing Mention 1.8 7.0 4.4 3.2 9.2 6.1 -1.3 7.3 3.4Missing Entity 3.8 16.2 9.8 5.3 13.7 9.3 1.7 11.4 7.0Table 10: Average accuracy improvement if all errors of a particular type are corrected.
Each row in the lower sectionis calculated independently, relative to the change after the span errors have been corrected.
Some values are negativebecause the merge operations involved in fixing the errors are applying to clusters that contain mentions from morethan one gold entity.Property System GoldNamed Entity 1.7% 0.7%Gender 0.8% 0.1%Number 2.1% 0.8%Person 6.4% 5.1%Table 11: Percentage of entities that contain mentionswith properties that disagree.dicate possible errors in the annotations, e.g.
in the0.7% of entities with a mixture of named entity typesthere may be mistakes in the coreference annota-tions, or mistakes in the named entity annotations.3However, even after taking into consideration caseswhere the mixture is valid and cases of annotationerrors, current systems are placing mentions withdifferent properties in the same clusters.6.7 Impact of Errors on Metric ScoresTable 10 shows the performance impact of correct-ing errors of each type.
The Span Error row givesimprovements over the original scores, while allother rows are relative to the scores after Span Er-rors are corrected.4 By fixing each of the other errortypes in isolation, we can get a sense of the gain ifjust that error type is addressed.
However, it alsomeans some mentions are incorrectly placed in thesame cluster, causing some negative scores.Interaction between the error types and the waythe metrics are defined means that the deltas do not3This kind of cross-annotation analysis may be a useful wayof detecting annotation errors.4This difference was necessary as the later errors makechanges relative to the state of the entities after the Span Errorsare corrected, e.g.
in Figure 2 a blue and red entity is split thatpreviously contained an X instead of one of the blue mentions.add up to the overall average gap in performance, butit is still clear that every error type has a noticeableimpact.
Missing Entity errors have the most sub-stantial impact, which reflects the precision orientednature of many coreference resolution systems.7 ConclusionWhile the improvement of metrics and the organiza-tion of shared tasks have been crucial for progressin coreference resolution, there is much insight to begained by performing a close analysis of errors.We have presented a new means of automaticallyclassifying coreference errors that provides an ex-haustive view of error types.
Using our tool we haveanalyzed the output of a large set of coreference res-olution systems and investigated the common chal-lenges across state-of-the-art systems.We find that there is considerable variability inthe distribution of errors, and the best systems arenot best across all error types.
No single sourceof errors stands out as the most substantial chal-lenge today.
However, it is worth noting thatwhile confidence measures can be used to reduceprecision-related errors, no system has been able toeffectively address the recall-related errors, such asMissed Entities.
Our analysis tool is available atcode.google.com/p/berkeley-coreference-analyser/.AcknowledgmentsWe would like to thank the CoNLL task organizersfor providing us with system outputs.
This work wassupported by a General Sir John Monash fellowshipto the first author and by BBN under DARPA con-tract HR0011-12-C-0014.274ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedingsof The First International Conference on LanguageResources and Evaluation Workshop on LinguisticsCoreference, pages 563?566.Eric Bengtson and Dan Roth.
2008.
Understanding thevalue of features for coreference resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 294?303.Anders Bjo?rkelund and Richa?rd Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Joint Conference on EMNLP andCoNLL - Shared Task, pages 49?55.Anders Bjo?rkelund and Pierre Nugues.
2011.
Explor-ing lexicalized features for coreference resolution.
InProceedings of the Fifteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 45?50.Jie Cai, Eva Mujdricza-Maydt, and Michael Strube.2011.
Unrestricted coreference resolution via globalhypergraph partitioning.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 56?60.Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,Nick Rizzolo, Mark Sammons, and Dan Roth.
2011.Inference protocols for coreference resolution.
InProceedings of the Fifteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 40?44.Eric Charton and Michel Gagnon.
2011.
Poly-co: a mul-tilayer perceptron approach for coreference detection.In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning: Shared Task,pages 97?101.Chen Chen and Vincent Ng.
2012.
Combining the best oftwo worlds: A hybrid approach to multilingual coref-erence resolution.
In Joint Conference on EMNLP andCoNLL - Shared Task, pages 56?63.Weipeng Chen, Muyu Zhang, and Bing Qin.
2011.Coreference resolution system using maximum en-tropy classifier.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 127?130.Greg Durrett and Dan Klein.
2013.
Easy victories anduphill battles in coreference resolution.
In Proceed-ings of the 2013 Conference on Empirical Methods inNatural Language Processing.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1152?1161.Gordana Ilic Holen.
2013.
Critical reflections on evalu-ation practices in coreference resolution.
In Proceed-ings of the 2013 NAACL HLT Student Research Work-shop, pages 1?7.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Com-panion Volume: Short Papers, pages 57?60.Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.2011.
Narrative schema as world knowledge forcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 86?92.Manfred Klenner and Don Tuggener.
2011.
An incre-mental model for coreference resolution with restric-tive antecedent accessibility.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 81?85.Hamidreza Kobdani and Hinrich Schuetze.
2011.
Super-vised coreference resolution with sucre.
In Proceed-ings of the Fifteenth Conference on ComputationalNatural Language Learning: Shared Task, pages 71?75.Jonathan K. Kummerfeld, Mohit Bansal, David Burkett,and Dan Klein.
2011.
Mention detection: Heuristicsfor the ontonotes annotations.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 102?106.Jonathan K. Kummerfeld, David Hall, James R. Cur-ran, and Dan Klein.
2012.
Parser showdown at thewall street corral: An empirical investigation of er-ror types in parser output.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 1048?1059.Sobha Lalitha Devi, Pattabhi Rao, Vijay Sundar Ram R,M.
C S, and A.
A.
2011.
Hybrid approach for corefer-ence resolution.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 93?96.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanfords multi-pass sieve coreference resolution sys-tem at the conll-2011 shared task.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28?34.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Lin-guistics, 39(4).Xinxin Li, Xuan Wang, and Shuhan Qi.
2011.
Coref-erence resolution with loose transitivity constraints.275In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning: Shared Task,pages 107?111.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 25?32.Sebastian Martschat, Jie Cai, Samuel Broscheit, E?vaMu?jdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
In JointConference on EMNLP and CoNLL - Shared Task,pages 100?106.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 104?111.Cicero Nogueira dos Santos and Davi Lopes Carvalho.2011.
Rule and tree ensembles for unrestrictedcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 51?55.C.
D. Paice and G. D. Husk.
1987.
Towards the auto-matic recognition of anaphoric features in english text:the impersonal pronoun ?it?.
Computer Speech & Lan-guage, 2(2):109?132.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Un-restricted coreference: Identifying entities and eventsin ontonotes.
In Proceedings of the International Con-ference on Semantic Computing, pages 446?453.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the15th Conference on Computational Natural LanguageLearning (CoNLL 2011), pages 1?27.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012shared task: Modeling multilingual unrestricted coref-erence in ontonotes.
In Joint Conference on EMNLPand CoNLL - Shared Task, pages 1?40.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977.M.
Recasens and E. Hovy.
2011.
BLANC: Implement-ing the rand index for coreference evaluation.
NaturalLanguage Engineering, 17:485?510, 9.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2011.
Re-laxcor participation in conll shared task on coreferenceresolution.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 35?39.Lalitha Devi.
Sobha, RK.
Rao.
Pattabhi, R. Vijay SundarRam, CS.
Malarkodi, and A. Akilandeswari.
2011.Hybrid approach for coreference resolution.
In Pro-ceedings of the Fifteenth Conference on Computa-tional Natural Language Learning: Shared Task,pages 93?96.Yang Song, Houfeng Wang, and Jing Jiang.
2011.
Linktype based pre-cluster pair model for coreference reso-lution.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 131?135.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: making sense of the state-of-the-art.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 656?664.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with reconcile.
In Proceedings of theACL 2010 Conference Short Papers, pages 156?161.Veselin Stoyanov, Uday Babbar, Pracheer Gupta, andClaire Cardie.
2011.
Reconciling ontonotes: Unre-stricted coreference resolution in ontonotes with rec-oncile.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 122?126.Olga Uryupina, Sriparna Saha, Asif Ekbal, and MassimoPoesio.
2011.
Multi-metric optimization for coref-erence: The unitn / iitp / essex submission to the 2011conll shared task.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 61?65.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-aofeng Yang, and Alessandro Moschitti.
2008.
Bart:a modular toolkit for coreference resolution.
In Pro-ceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics on Human Lan-guage Technologies: Demo Session, pages 9?12.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the Sixth Message Uunderstanding Conference,pages 45?52.Hao Xiong, Linfeng Song, Fandong Meng, Yang Liu,Qun Liu, and Yajuan Lv.
2011.
Ets: An error tolera-ble system for coreference resolution.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 76?80.Yaqin Yang, Nianwen Xue, and Peter Anick.
2011.
Amachine learning-based coreference detection system276for ontonotes.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 117?121.Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,Liping Ge, Zengjian Liu, Meng Liao, and XianboSi.
2012.
A mixed deterministic model for corefer-ence resolution.
In Joint Conference on EMNLP andCoNLL - Shared Task, pages 76?82.Desislava Zhekova and Sandra Ku?bler.
2011.
Ubiu: Arobust system for resolving unrestricted coreference.In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning: Shared Task,pages 112?116.Desislava Zhekova, Sandra Ku?bler, Joshua Bonner,Marwa Ragheb, and Yu-Yin Hsu.
2012.
Ubiu for mul-tilingual coreference resolution in ontonotes.
In JointConference on EMNLP and CoNLL - Shared Task,pages 88?94.Huiwei Zhou, Yao Li, Degen Huang, Yan Zhang, Chun-long Wu, and Yuansheng Yang.
2011.
Combin-ing syntactic and semantic features by svm for unre-stricted coreference resolution.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 66?70.277
