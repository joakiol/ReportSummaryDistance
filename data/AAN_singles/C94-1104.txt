SYNTACTIC  ANALYS IS  OF  NATURAL LANGUAGE US INGL INGUIST IC  RULES AND CORPUS-BASED PATTER.NSPasi Tapanainen* Timo J~rvinenRank Xerox Research CentreGrenoble LaboratoryUniversity of tIelsinkiResearch Unit for Computational LinguisticsAbstractWe are concerned with the syntactic annotationof unrestricted text.
We combine a rule-basedanalysis with subsequent exploitation of empiri-cal data.
The rule~based surface syntactic anal-yser leaves ome amount of ambiguity in the out-put that is resolved using empirical patterns.
Wehave implemented a system for generating andapplying corpus-based patterns.
Somc patternsdescribe the main constituents in the sentenceand some the local context of the each syntac-tic function.
There are several (partly) redml-tant patterns, and the  "pattern" parser selectsanalysis of the sentence ttmt matches the strictestpossible pattern(s).
The system is applied to anexperimeutal corpus.
We present he results anddiscuss possible refinements of the method froma linguistic point of view.1 INTRODUCTIONWe discuss surface-syntactic analysis of running text.Our purpose is to mark each word with a syntactictag.
The tags denote subjects, object, main verbs,adverbials, etc.
They are listed in Appendix A.Our method is roughly following?
Assign to each word all the possible syntactic tags.?
Disambiguate words as much as possible using lin-guistic information (hand-coded rules).
Ilere weavoid risks; we rather leave words ambiguous thanguess wrong.?
Use global patterns to form alternative sentencelevel readings.
Those alternatiw~" analyses are se-lected that match the strictest global pattern.
\[f itdoes not accept any of the remaining readings, thesecond strictest pattern is used, and so on.?
Use local patterns to rank the remaining readings.The local patterns contain possible contexts for syn-tactic functions.
The ranking of the readings de-pends on the length of the contexts associated withthe syntactic functions of the sentece.We use both linguistic knowledge, represented asrules, and empirical data collected from tagged cor-pora.
We describe a new way to collect informationfrom a tagged corpus and a way to apply it.
In thispaper, we are mainly concerned with exploiting theempirical data and combining two different kinds ofparsers.
*This work was done when the author worked in theResearch Unit for Computational Linguistics at the Uni-versity of Itelsinki.Our work is based on work done with ENGCG,the Constraint Grammar Parser of English \[Karls-son, 1990; Karlsson, 1994; Karlsson et al, 1994;Voutilainen, 1994\].
It is a rule-h~ed tagger andsurface-syntactic parser that makes a very small num-her of errors but leaves some words ambiguous i.e.
itprefers ambiguity to guessing wrong.
The morpholog-ical part-of-speech analyser leaves \[Voutilainen et al,1992\] only 0.3 % of all words in running text withoutthe correct analysis when 3-6 % of words still havetwo or Inore I analyses.Vontilainen, Ileikkil'5.
and Anttila \[1992\] reportedthat the syntactic analyser leaves :3-3.5 % of wordswithout the correct syntactic tag, and 15-20 % ofwords remain amhiguos.
Currently, the error rate hasbeen decreased to 2-2.5 % and ambiguity rate to 15 %by Tirao Jiirvinen \[1994\], who is responsible for tag-ging a 200 million word corpus using I'\]NGCG in theBank of English project.Althought, the ENGCG parser works very well inpart-of-speech tagging, the syntactic descriptions arestill problematic.
In the constraint grammar frame-work, it is quite hard to make linguistic generalisationsthat can be applied reliably.
To resolve the remainingambiguity we generate, by using a tagged corpus, aknowledge-base that contains information about boththe general structure of the sentences and the localcontexts of tim syntactic tags.
The general structurecontains information about where, for example, sub-jects, objects and main verbs appear and how theyfollow one another.
It does not pay any attention totheir potential modiliers.
The modifier-head relationsare resolved by using the local context i.e.
by lookingat what kinds of words there are in the neighbour-hood.The method is robust in the sense that it is ahleto handle very large corpora.
Although rule-b~medparsers usually perlbrrn slowly, 0rot is not the ca.qewith ENGCG.
With the English grammar, the Con-straint Granun;~r Parser implementation by Pasi Ta-panainen analyses 400 words 2 per second on a Spare-Station 10/:30. q'hat is, one million words are pro-cessed in about 40 minutes.
'l'he pattern parser forempirical patterns runs somewhat slower, about 100words per second.1 But even then some of tile original ,xlternative analysesare removed'2InchMing all steps of preprocessing, morphologlcManalysis, disambiguation and syntactic analysis.
Thespeed of morphological disamblguation alone exceeds 1000words per second.
(,292 KNOWLEDGE ACQUIS IT IONWe have used two schemes to extract knowledge fromcorpora.
Both produce readable patterns that can beverified by a linguist.
In the first scheme, sentencesare handled as units and information about the struc-ture of the sentence is extracted.
0n ly  the main con-stituents (like subject, objects) of the sentence aretreated at this stage.
The second scheme works withlocal context and looks only a few words to the rightand to the left.
It is used to resolve the nmdifier-t,eaddependencies in the phrases.First, we form an axis of the sentence using somegiven set of syntactic tags.
We collect several ayersof patterns that may be partly redundant with eachother.
For instance, simplifying a little, we can saythat a sentence can be of the form subjecl --  mainverb and there may be other words before and afterthe subject and main verb.
We may also say thata sentence can be of the form subject - -  main verb- -  object.
The latter is totally covered by the formerbecause the former statement does not prohibit theappearance of an object but does not require it either.The redundant patterns are collected on purpose.During parsing we try to find the strictest frame forthe sentence.
If we can not apply some pattern be-cause it conflicts with the sentence, we may use other,possibly more general, pattern.
For instance, an axisthat describes all accepted combinations of subject,objects and main verbs in the sentence, is stricterthan an axis that  describes all accepted combinationsof subjects and main verbs.After applying the axes, the parser's output is usu-ally still ambiguous because all syntactic tags are nottaken into account yet (we do not handle, for instance,determiners and adjective premodifiers here).
The re-maining ambiguity is resolved using local informationderived from a corpus.
The second phase has a moreprobabilistie fiavour, although no actual probabilitiesare computed.
We represent information in a readableform, where all possible contexts, that are commonenough, are listed for each syntactic tag.
The lengthof the contexts may vary.
The common contexts arclonger than the rare ones.
In parsing, we try to finda match for each word in a maximally long context.Briefly, the relation between tim axes and the jointsis following.
The axes force sentences to comply withthe established frames.
If more than one possibility isfound, the joints are used to rank them.2.1 The  sentence  ax isIn this section we present a new method to collectinformation from a tagged corpus.
We define a newconcept, a sentence axis.
The sentence axis is a pat-tern that  describes the sentence structure at an ap-propriate level.
We use it to select a group of possibleanalyses for the sentence.
In our implementation, weform a group of sentence axes and the parser selects,using the axes, those analyses of the sentence thatmatch all or as many as possible sentence axes.We define the sentence axis in the following way.Let S be a set of sentences and T a, set of syntactictags.
The sentence axis of S according to tags T showsthe order of appearance of any tag in T for every sen-tence in S.Itere, we will demonstrate the usage of a sentenceaxis with one sentence.
In our real application we,of course, use more text to build up a database ofsentence axes.
Consider the following sentence aISUBJ  would_+FAUXV also_ADVLincrease_-FMAINV child NN> benefiLOBa ,give_-FMAINV some_QN> help OBJt0_AI)VL the 1)N> car_NN> industry <Pand CC relax_-FMAINV r~,les OBagoverning_<NOM-FMAiNV Iocal AN>avthority_NN> capital_AN> reeeipts OBJ ,alIowing_-FMAINV councils SUBJ/o_INFMAI{K> spend_-FMAINV more ADVL .The axis according to the manually defined set T ={ SUBJ +FAUXV +FMAINV }is?
.. SUBJ +FAUXV .. .
SUBJ ...which shows what order the elements of set T ap-pear in the sentence above, and where three (lotsmean that there may be something between words,e.g.
+FAUXV is not followed (in ttfis c~e) immedi-ately by SUBJ.
When we have more than one.
sen-tence, ttm axis contains more than one possible orderfor the elements of set T.The axis we have extracted is quite general.
It de-fines the order in which the finite verbs and subjectsin the sentence may occur but it does not say anythingabout nmdlnite verbs in the sentence.
Notice that thesecond subject is not actually tt,e subject of the fi-nite clause, but the subject of nontinite constructioncouncils to spend more.
This is inconvenient, and aquestion arises whether there should be a specific tagto mark suhjects of the nonllnite clauses.
Voutilainenand Tapanaincn \[1993\] argued that the richer set oftags could make parsing more accurate in a rule-basedsystem.
It may be true he.re as well.We can also specify an axis for verbs of the sentence.
'Fhus the axis according to tim set{ +FAUXV +FMAINV-FMAINV INFMAI{,K> }is.
.
.
.
kFAUXV .
.
.
.
.
FMAINV .
.
.
.
.
FMAINV.
.
.
.
FMAINV .
.
.
.
.
FMAINV .,.
INFMAR, K>-FMAINV ?
..The nonlinite verbs occur in this axis four times oneafter another.
We do not want just to list how manytimes a nonllnite verb may occur (or occurs in a cor-pus) in this kind of position, so we clearly need somegeneralisations.The fundamental rule ofgeneralisation that we usedis the following: Anything that is repeated may berepeated any number of times.We mark this using l)rackets and a plus sign.
Thegeneralised axis for the above axis is?
.. +FAUXV \[ .
.
.
.
FMAINV \]+?
.. INI,'MARK> -FMAINV ...aThe tag set is adapted from the Constraint Grammarof English as it is.
It is more extensive than commonlyused in tagged corpora projects (see Appendix A).630We can also repeat longer sequences, for instance theset{ --FMAINV <N()M-I,'MAINV +FAUXVSUBJ OBJ }provides the axisSUBJ +FAUXV .
.
.
.
FMAINV .
.
.
oBa.
.
.
.
.
FMAINV .
.
.
oBa .
.
.
.
FMAINV OBJ?
.. <NOM-FMAINV .
.
.
OBa .
.
.-FMAINV SUBJ .
.
.
.
FMAINV .. .And we lbrm a generalisationSUBJ +FAUXV \[ .
.
.
.
.
FMAINV .
.
.
OBJ \]+?
.. <NOM-FMAINV .
.
.
OBJ .
.
.-FMAINV SUBJ .
.
.
.
FMAINV .
.
.Note that we added silently an extra (tot be.tweeuone -FMAINV and OBY in order not to make, dis-tinctions between -FMAINV OBg and -FMAINV.
.
.OBJ here.Another generalisation can be made using equiva-lence clauses.
We can ,assign several syntactic tags tothe same equivalence class (for instance -I"MAINV,< NOM-FMAIN V arrd < P-FMA \[N V), and then gen-.crate axes as above.
'l'he result would beSUBJ +FAUXV \[ .
.
.
nonfinv .
.
.
OBJ \]-I-?
.. nontinv SUBJ .
.
.
nonfinv .
.
.where nonfinv denotes both -FMAINV ;u,d <NOMFMAINV (and also <P-.1,'MAINV).The equivalence classes are essential in the presenttag set because the syntactic arguments of finite verbsare not distinguished from the arguments of nontlniteverbs.
Using equivalence classes for the finite attd non-finite verbs, we may tmiht an generallsation that ;tl)-plies to both types of clauses.
Another way to solvethe problem, is to add new tags for the arguments ofthe nontinite clauses, arid make several axes for them.2.2 Loca l  pat ternsIn the second phase of the pattern parsing scheme weapply local patterns, the joints.
They contain iofor-mation about what kinds of modifiers have what kindsof heads, and vice versa.For instance, in the following sentence 4 the wordsfair and crack are both three ways ambiguous beforethe axes are applied.He_SUBJ gives_-t-FMAINV us I-OBaa l)N> fa irAN>/SUBJ/NN>crack_OBJ/+FMAINV/SUIU theT, fl l)VLwe.
SUBJ wiII+FAUXVbe_--FMAINV/-FAUXV in_AI)VL with_A1)VLa_l)N> chance<P of <NOM-OFca~ying<P-FMAINV off <NOM/AI)VLthe DN> World <P/NN> Cvp_<P/Ol3J .After the axes have been applied, the noun phr,xse afair crack has the analysesa DN> fa i rAN>/NN> crack OBJ.The word fairis still left partly ambiguous.
We resolvethis ambiguity using the joints.4This analysis is comparable to the output of I'3NCCG.The ambiguity is marked here using the slash.
The mor.-phological information is not printed.in an ideal case we have only one head in eachl)hrase, although it may not be in its exact locationyet.
r\['he following senLencv, fragment (temonstrate.sthisThey SUlta have...+ FAUXV been -VMAINVmuch AD-A> less P(X)Mlq,--,qfAD--A>attentive <NOM/PCOMPl,-.Sto <NOM/AI)VI, theft)N> .. .In tit(.'
analysis, the head of the l)hr~me mvch lessattentive may be less or altenlive.
If it is less theword attentive is a postn,odifier, and if the head is at-tentive then less is a premodilier.
Tim sentence isrepresented internally in the parser in such a waythat if the axes make this distinction, i.e.
forcethe.re to be exactly one subject complement, hereare only two possil)le paths which the joints can se-lect from: less AD-A> attenlive_l'COMPL- S andless J'COMPl,- S attentive <NOM.Generating the joints is quite straightforward.
Weproduce different alternative variants for each syntac:tic tag and select some of them.
Wc use a couple ofparameters to validate possible joint candidates.
* q'he error margin provides the probability for check-ing if the context is relevant, i.e., there is enottg\]tevidence for it among the existing contexts of thetag.
This probability may be used in two ways:l,'or a syntactic tag, generate all contexts (oflength n) tl, at appear in the corpora.
Select allthose contexts that are frequent enough.
Do thiswith all n's wdues: 1, 2, ...-- First generate all contexts of length t. Selectthose contexts that are fregnent enough amongthe generated contexts.
Next,, lengtlmn all con-texts selected in the previous tep by one word.Select those contexts that are frequent enoughamong the new generated context, s. R.epeat hissulficient malty times.lloth algorithms l)roduce a set of contexts of differ-ent lengths.
Characteristic for t)oth the algorithmsis that if they haw; gene.rated a context of lengthn that matches a syntactic function in a sentence,there is also a context of length n - 1 that matches.?
The absolute, margin mmd)er of cases that isneeded for the evidence, of till: generated context.If therc is less cvidencc, it is not taken into accountarm a shorter context is generated.
'.\['his is used toprevent strange behaviour with syntactic tags thatare not very common or with a corpus that is notbig enough.?
'l'he maximum length of the context to be gener-ated.l)uring the parsing, longer contexts are preferredto shorter ones.
The parsing problem is thus a kindof pattern matching problem: we have to match apattern (context) arouml each tag and tlnd a sequenceof syntactic tags (analysis of the sentence) that h~mthe best score.
The scoring fimetion depends on thelengths of the matched patterns.631I text .
II words\ ]ambigu i ty  rate I error rate \]bbl 1734' 12.4 % 2.4 %bb2 1674 14.2 % 2.8 %1599 18.6 % 1.6 %wsj " 2309 16.2 % 2.9 %\].
to ta l l \ ]  7316 I 15.3 % \] 2.2 %-1Figure 1: Test corpora after syntactical analysis ofENGCG.3 EXPERIMENTS WITH REALCORPORAInformation concerning the axes was acquired from amanually checked and fully disambiguated corpus 5 ofabout 30,000 words and 1,300 sentences.
Local con-text information was derived from corpora that wereanalysed by ENGCG.
We generated three differentparsers using three different corpora 6.
Each corpuscontains about 10 million words.For evaluation we used four test samples (in Fig-ure 1).
Three of them were taken frmn corpora thatwe used to generate the parsers and one is an addi-tional sample.
The samples that are named bbl, todayand wsj belong to the corpora from which three dif-ferent joint parsers, called BB1, TODAY and WSJrespectively, were generated.
Sample bb~ is the addi-tional sample that is not used during development ofthe parsers.The ambiguity rate tells us how much ambiguity isleft after ENGCG analysis, i.e.
how many words stillhave one or more alternative syntactic tags.
The errorrate shows us how many syntactic errors ENGCG hasmade while analysing the texts.
Note that the ambi-guity denotes the amount of work to be done, and theerror rate denotes the number of errors that alreadyexist in the input of our parser.All the samples were analysed with each generatedparser (in Figure 2).
The idea is to find out aboutthe effects of different text types on the generationof the parsers.
The present method is applied to re-duce the syntactic ambiguity to zero.
Success ratesvariate from 88.5 % to 94.3 % in ditferent samples.There is maximally a 0.5 percentage points differencein the success rate between the parsers when appliedto the same data.
Applying a parser to a sample fromthe same corpus of which it was generated oes notgenerally show better results.Some of the distinctions left open by ENGCG maynot be structurally resolvable (see \[Karlsson et al,1994\]).
A case in point is the prepositional attach-ment ambiguity, which alone represents about 20 %of the ambiguity in the ENGCG output.
The properway to deal with it in the CG framework is probablyusing lexical information.Therefore, as long as there still is structurally un-resolvable ambiguity in the ENGCG output, a cer-tain amount of processing before the present systemSOonsisting of 15 individual texts from the Bank ofEnglish project \[J~.rvinen, 1994\].
The texts were chosento cover a variety of text types but due to small size andintuitive sampling it cannot be truly representative.6We use here Today newspaper, The t",conomist -kWallStreet Journal and British Books._T_Text s ~ s  e r s \[ riB1 .I TODAY~ ~  92.5% \ [92~%__1 91.9-W-oFigure 2: Overall parsing success rate in syntacticallyamdysed samplesmight improve the results considerably, e.g., convert-ins structurally unresolvable syntactic tags to a singleunderspecified tag.
\[,'or instance, resolving preposi-tional attachment ambiguity by other means wouldiruprove the success rate of the current system to90.5 % - 95.5 %.
In the wsj sample ttLe improvementwould be as much a.s 2.0 percentage points.The differences between success rates in differentsamples are partly explained by tile error types thatare characteristic of the samples.
For example, inthe Wall Street Journal adverbials of time are easilyparsed erroneously.
This may cause an accumulationeffect, ms happens in tile following sentenceMAN AG Tuesday said fiscal 1989 net incomerose 25% and said it, will raise its dividend forlhe year ended June 30 by about the samepercentage.Tile phrase the year ended June 30 gets the analysisthe_DN> year_NN> ended_AN>June_NN> 30_<Pwhile the correct (or wanted) result islhe DN> year_<P ended_<NOM-FMAINVJune_ADVL 30 <NOMDifferent kind of errors appear in text bb!
which con-tains incomplete sentences.
The parser prefers com-plete sentences and produces errors in sentences likeThere w~s Provence in mid-autumn.
Gold Zints.Air so serene you could look out over the sea fortens of miles.
Rehabilitalion walks with himalong tim woodland l)aths.The errors are: gold tints is parsed a.s svbjeel - mainverb ~s well ~m r'ehabililation walks, and air is analysed,as a main verb, Other words have the appropriateanalyses.The strict sequentiality of morphological nd syn-tactic analysis in ENGCG does not allow the use ofsyntactic information in morphological disambigua-tion.
The present method makes it possible to prunethe remaining morphological mbiguities, i.e.
do somepart-of-speech tagging.
Morphological ambiguity re-mains unresoNed if the chosen syntactic tag is presentin two or more morphological readings of the sameword.
Morphological ambiguity 7 is reduced close tozero (about 0.3 % in all the samples together) and theoverall success rate of ENGCG + our pattern parseris 98.7 %.r After ENGCG the amount of nmrphologic',d ambiguityin the test data was 2.9 %, with au error rate of 0.4 %.6324 CONCLUSIONWe discussed combining a linguistic rule-based parserand a corpus-based empirical parser.
We divide theparsing process into two parts: applying linguistic in-formation and applying corpus-based patterns.
Thelinguistic rules are regarded ms more reliable than thecorpus-based generalisations.
They are therefore ap-plied first.The idea is to use reliable linguistic information aslong as it is possible.
After certain phase it comesharder and harder to make new linguistic onstraintsto eliminate the remaining ambiguity.
Therefore weuse corpus-based patterns to do the remaining dis-and)iguation.
The overall success rate of the com-bination of the linguistic rule-based parser and thecorpus-based pattern parser is good.
If some unrc-solvable ambiguity is left pending (like prepositionalattachment), the total success rate of our morpho-logical and surface-syntactic analysis is only slightlyworse than that of many probabilistic part-of-speechtaggers.
It is a good result because we do more thanjust label each word with a morphological tags (i.e.noun, verb, etc.
), we label them also with syntacticfimction tags (i.e.
subject, object, subject comple-ment, etc.
).Some improvements might be achieved by modi-fying the syntactic tag set of ENGCG.
As discussedabove, the (syntactic) tag set of the ENGCG is notprobably optimal.
Some ambiguity is not resolvable(like prepositional ttachment) and some distinctionsarc not made (like subjects of the finite and the non-finite clauses).
A better tag set for surface-syntacticparsing is presented in \[Voutilainen and Tapanainen,1993\].
But we have not modified the present ag setbecause it is not clear whether small changes wouldimprove the result significantly when compared to theeffort needed.Although it is not possible to fully disambiguate thesyntax in ENGCG, the rate of disambiguation can beimproved using a more powerful linguistic rule tbrmal-ism (see \[Koskenniemi el al., 1992; Koskenniemi, 1990;Tapanainen, 1991\]).
The results reported in this sudycan most likely be improved by writing a syntacticgrammar in the finite-state framework.
The samekind of pattern parser could then be used for disam-biguating the resulting analyses.5 ACKNOWLEDGEMENTSThe Constraint Grammar framework was originallyproposed by Fred Karlsson \[1990\].
The extensive workon the description of English was (tone by Atro Vouti-lainen, Juha tleikkil~ and Arto Anttila \[1992\].
TimoJ~rvinen \[1994\] has developed the syntactic onstraintsystem further.
ENGCG uses Kimmo Koskenniemi's\[1983\] two-level morphological nalyser and Past Ta-panainen's implementation of Constraint Grammarparser.We want to thank Fred Karlsson, Lauri Karttunen,Annie Za~nen, Atro Voutilainen and Gregory Grefen-stette for commenting this paper.Re ferences\[J~rvinen, 1994\] Timo J~rvinen.
Annotating 200 mil-lion words: The Bank of English project.
\[n pro-ceedings of COLING-9\]~.
Kyoto, 1994.\[Karlsson, 1990\] Fred Karlsson.
Constraint Grammaras a framework for parsing running text.
in tIansKarlgren (editor), COLING-90.
Papers presentedto the 13th International Conference on Compv-tational Linguistics.
Vol.
3, pp.
168-173, IIelsinki,1990.\[Karlsson, 1994\] Fred Karlsson.
Robust parsing of un-constrained text.
In Nelleke Oostdijk and Pieterde IIaan (eds.
), Corpus-based Research Into Lan-guage., pp.
121-142, l~odopi, Amsterdam-Atlanta,1994.\[Karlsson et at., 1994\] Fred Karlsson, Atro Voutilai-hen, Juha Ileikkilii.
and Arto Anttila (eds.)
Con-straint Grammar: a Language-Independent Systemfor Parsing Unrestricted Text.
Mouton de Gruyter,Berlin, 1994.\[Koskenniemi, 1983\] Kimmo Koskenniemi.
Two-levelmorphology: a general computational model tbrword-form recognition and production.
Publica-tions nro.
11.
Dept.
of General Linguistics, Univer-sity of Ilelsinki.
1983.\[Koskenniemi, 1990\] Kimmo Koskenniemi.
Finite-state parsing and disambiguation.
In lians Karl-gren (editor), COLING-90.
Papers presented to the13th International Conference on ComputationalLinguistics.
Vol.
2 pages 229-232, ll\[elsinki, 1999.\[Koskenniemi el al., 1992\] Kimmo Koskenniemi, P~iTapanainen and Atro Voutilainen.
Compiling andusing finite-state syntactic rules.
In Proceedings ofthe fifteenth International Conference on Computa-tional Linguistics.
COLING-92.
Vol.
I, pp.
156-102,Nantes, France.
1992.\[Tapanainen, 1991\] Past Tapanainen.
~.Srellisinii au-tomaatteina esitettyjen kielioppis~i?ntSjen sovelta-minen hmnnollisen kielen j,isentKj~sK (Natural an-guage parsing with finite-state syntactic rules).Master's thesis.
Dept.
of computer science, Univer-sity of Ilelslnki, i991.\[Voutilainen, 1994\] Afro Voutitainen.
Three studiesof grammar-based surface parsing of unrestrictedenglish text.
Publications nr.
24.
Dept.
of GeneralLinguistics.
University of llelsinki.
19!
)4.\[Voutilainen el al., 1992\]Atro Voutilainen, Juha lteikkilii and Arto Anttila.Constraint grammar of English - -  A l'erformance-Oriented Introduction.
Publications nr.
21.
Dept.of General Linguistics, University of Ilelsinki, 1992.\[Voutilainen and Tapanainen, 199q\] Atro Voutilainenand Past Tapanainen.
Ambiguity resolution in a re-dnctionistic parser.
In Proceedings of of Sixth Con-ference of the European Chapter of the Associationfor Computational Linguistics.
EACL-93.
pp.
394-403, Utrecht, Netherlands.
1993.A T I IE  TAG SET2'his appendix contains the syntactic tags we haveused.
Tbe list is adopted from \[Voutilainen et al,1992\].
To obtain also the morphological "part-of-speech" tags you can send an empty e-mail messageto engcg-info@ling.helsinki.fi.633+FAUXV = Finite Auxiliary Predicator: lie ca_~nread.,-FAUXV = Nonfinite Auxiliary Predicator: Fie mayhave_ read.,+FMAINV = Finite Main Predicator: He reads.,-FMAINV = Nonfinite Main Predicator: He hasre a~d.,NPHIL = Stray NP: Volume I: Syntax,SUBJ = Subject: H__~e reads.,F-SUBJ = Formal Subject: There was some argu-ment about that.
I_tt is raining.,OBJ = Object: He read a book.,I -OBJ = Indirect Object: He gave Mary a book.,PCOMPL-S = Subject Complement~ is a fool.,PCOMPL-O = Object Complement: I eonsid~--hima fool.,AI~V-L = Adverbial: He came home late.
He is in theCa r.,O-ADVL = Object Adverbial: lie ran two miles.APP = Apposition: Helsinki, the capital of Finland,N = Title: King George and Mr.DN> - Dete~ner :  He read the book.,NN> = Premodifying Noun: The car park was full.,AN> -- Premodifying Adjective: The bh, e car ismine.,QN> --- Premodifying Quantifier: He had two sand-wiches and some coffee.,GN> = Premodifying Genitive: M._yy car and flill'sbike are blue.,AD-A> = Premodifying Ad-Adjective: She is veryintelligent.,<NOM-OF = Postmodifying Of: Five of you willpass.,<NOM-FMAINV = Postmodifying Nonfinite Verb:He has the licence lo kill.
John is easy to please.
Theman drinking coffee is my uncle.,<AD---A---~= Postmodifying Ad-Adjective: This is goodenough.,< ~  = Other Postmodifier: The man with glassesis my uncle.
He is the president elect.
The man i_~nthe moon fell down too soon.,INFMAP~K> = Infinitive Marker: John wants t~read.,<P-FMAINV = Nonfinite Verb as Complement ofPreposition: This is a brush for cleaning.,<P = Other Complement of P re l )~ He is in tilecar.,CC --- Coordinator: John and Bill are friends.,CS = Subordinator: / f  Johu is there, we shall go, too.,634
