Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 29?38,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsModeling Infant Word SegmentationConstantine LignosDepartment of Computer and Information ScienceUniversity of Pennsylvanialignos@cis.upenn.eduAbstractWhile many computational models have beencreated to explore how children might learnto segment words, the focus has largely beenon achieving higher levels of performance andexploring cues suggested by artificial learningexperiments.
We propose a broader focus thatincludes designing models that display prop-erties of infants?
performance as they beginto segment words.
We develop an efficientbootstrapping online learner with this focus inmind, and evaluate it on child-directed speech.In addition to attaining a high level of perfor-mance, this model predicts the error patternsseen in infants learning to segment words.1 IntroductionThe last fifteen years have seen an increased inter-est in the problem of how infants learn to segmenta continuous stream of speech into words.
Much ofthis work has been inspired by experiments with in-fants focusing on what capabilities infants have andwhich cues they attend to.
While experimental workprovides insight into the types of cues infants may beusing, computational modeling of the task providesa unique opportunity to test proposed cues on rep-resentative data and validate potential approaches tousing them.While there are many potential approaches to theproblem, a desirable solution to the problem shoulddemonstrate acceptable performance in a simula-tion of the task, rely on cues in the input that aninfant learner is able to detect at the relevant age,and exhibit learning patterns similar to those of in-fant learners.
Most work in computational model-ing of language acquisition has primarily focusedon achieving acceptable performance using a sin-gle cue, transitional probabilities, but little effort hasbeen made in that work to try to connect these learn-ing solutions to the actual learning patterns observedin children outside of performance on short artificiallanguage learning experiments.In this work we present a simple, easily extendedalgorithm for unsupervised word segmentation that,in addition to achieving a high level of performancein the task, correlates with the developmental pat-terns observed in infants.
We discuss the connec-tions between the design and behavior of our algo-rithm and the cognitive capabilities of infants at theage at which they appear to begin segmenting words.We also discuss how our technique can easily be ex-tended to accept additional cues to word segmenta-tion beyond those implemented in our learner.2 Related WorkAs this paper examines the intersection of infants?capabilities and computational modeling, we discusswork in both domains, beginning with experimentalapproaches to understanding how infants may per-form the task of word segmentation.2.1 Infant Word SegmentationA potential account of how infants learn to iden-tify words in fluent speech is that they learn wordsin isolation and then use those words to segmentlonger utterances (Peters, 1983; Pinker et al, 1984).It is not clear, however, that infant-directed speechprovides enough detectable words in isolation for29such a strategy (Aslin et al, 1996).
Whatever iso-lated words children do hear, they appear to attend tothem; whether a word is heard in isolation is a betterpredictor of whether a child has learned a word thanthe word?s frequency (Brent and Siskind, 2001).A more plausible alternative account to assumechildren attend to patterns in the input, using them toidentify likely word units.
Much experimental workhas followed from the finding that in artificial learn-ing tasks, infants and adults appear to prefer word-like units that match statistical patterns in the input(Saffran et al, 1996b; Saffran et al, 1996a).
Saffranet al and the authors of following studies (Aslin etal., 1998; Saffran, 2001, among many others) sug-gest that participants used transitional probabilitiesto succeed in these experiments, but the actual strat-egy used is unclear and may even be an artifact ofthe perceptual system (Perruchet and Vinter, 1998;Hewlett and Cohen, 2009).More recent work using real language data hasnot shown transitional probabilities to be as usefula cue as originally suggested.
Lew-Williams et al(2011) found that 9-month-old English-learning in-fants were not able to learn high-transitional prob-ability words in fluent Italian speech unless thosewords were also presented in isolation.
Given thisfinding and the extensive exisiting modeling workfocusing on the used of transitional probabilities, webelieve it is crucial to additionally explore segmen-tation strategies that rely on other cues in the input.2.2 Modeling Word SegmentationWhile experimental work has posited simple algo-rithms that infants might use to accomplish the taskof word segmentation, when applied to real languagedata these techniques have yielded very poor results(Yang, 2004).
This problem has created a chal-lenge for researchers modeling language acquisitionto suggest more sophisticated strategies that infantsmight use.
These approaches have fallen into twoprimary categories: optimization-based and boot-strapping algorithm strategies.Optimization-based strategies have focused ontechniques that a learner might use to arrive at anoptimal segmentation, either through a dynamic pro-gramming approach (Brent, 1999), online learning(Venkataraman, 2001), or nonparametric Bayesianinference (Goldwater et al, 2009; Johnson andGoldwater, 2009).
These approaches fit within stan-dard statistical approaches to natural language pro-cessing, defining statistical objectives and inferencestrategies, with the learners trying to optimize somecombination of the quality of its lexicon and repre-sentations of the corpus.In contrast, bootstrapping approaches (Gambelland Yang, 2004; Lignos and Yang, 2010) to wordsegmentation have focused on simple heuristics forpopulating a lexicon and strategies for using the con-tents of the lexicon to segment utterances.
These ap-proaches have focused on a procedure for segmen-tation rather than defining an optimal segmentationexplicitly, and do not define a formal objective thatis to be optimized.While bootstrapping approaches have generallymade stronger attempts to align with infants abili-ties to process the speech signal (Gambell and Yang,2004) than other approaches, little effort has beenmade to connect the details of an implemented seg-mentation strategy with children?s learning patternssince the earliest computational models of the task(Olivier, 1968).
It is important to draw a con-trast here between attempts to match patterns of hu-man development with regard to word segmentationwith attempts to model performance in artificial lan-guage learning experiments whose goal is to probeword segmentation abilities in humans (Frank et al,2010).
In this paper we are focused on matching theprogression of development and performance in nat-uralistic experiments to characteristics of a segmen-tation strategy, an approach similar to that employedin English past tense learning (Rumelhart and Mc-Clelland, 1986; Pinker, 2000; Yang, 2002).We will now discuss the patterns of developmentfor children learning to segment English words,which form the motivation for the design of our seg-menter.3 Infant Performance in WordSegmentationWhile the developmental patterns of English-learning infants have been broadly studied, it hasbeen difficult to identify errors that must be causedby failures to correctly segment words and not othercognitive limitations, issues of morphological pro-ductivity, or syntactic competency issues.30Brown (1973) offers one of the most compre-hensive examinations of the types of errors thatyoung infants make regarding word segmentation.He notes that Adam?s common errors included treat-ing it?s-a, that-a, get-a, put-a, want-to, and at-thatas single words, as judged by various misproduc-tions that involved these items.
A possible analysisof these errors is that in addition to the high level offrequency with which those syllables co-occur, ele-ments such as a and to do not carry any identifiableamount of stress in natural speech.In addition to the undersegmentations that Brownidentifies, Peters (1983) identifies the pattern ofoversegmenting function words begin other words,including this famous dialog between a parent andchild, where in the child?s response have is pro-nounced in the same way as the second syllable ofbehave: Parent: Behave!
Child: I am have!The response by the child indicates that they haveanalyzed behave as be have.
There are two majorfactors that could contribute to such an analysis: thehigh frequency of be leading to it being treated as aseparate word (Saffran et al, 1996b), and the lack ofstress on be but stress on have which forms a wordcontrary to the dominant pattern of stress in English(Cutler and Butterfield, 1992).Infants appear to use the ends of utterances to aidsegmentation, and as early at 7.5 months old theyare able to recognize novel words in fluent speech ifthe novel words are presented at the ends of an utter-ance and not utterance medially (Seidl and Johnson,2006).
Thus the reliable boundaries presented by theedge of an utterance should be treated as informativefor a learner.Most crucially, the syllable seems to be the unitchildren use to form words.
Experiments that havebeen performed to gauge adult and infant compe-tency in word segmentation have been designed withthe assumption that the only possible segmentationpoints are at syllable boundaries.
That infants shouldbe able to operate on syllables is unsurprising; in-fants as young as 4-days-old are able to discrimi-nate words based on syllable length (Bijeljac-Babicet al, 1993) and phonotactic cues to syllable bound-aries seem to be rapidly acquired by infants (On-ishi et al, 2002).
The use of the syllable in exper-imental work on word segmentation stands in con-trast to many computational models that have oper-ated at the phoneme level (Brent, 1999; Venkatara-man, 2001; Goldwater et al, 2009).
An exceptionto the focus on phoneme-based segmentation is thejoint learning model proposed by Johnson (2008)that learns syllabification and other levels of repre-sentation jointly with word segmentation, but thatmodel poses problems as a developmentally relevantapproach in that it predicts unattested joint syllabifi-cation/segmentation errors by infants and problemsas a linguistically relevant approach due to its non-phonotactic approach to learning syllabification.From this survey, we see some relevant phenom-ena that a good model of infant word segmentationshould replicate.
(1) The learner should operate onsyllables.
(2) At some stage of learning, underseg-mentation function word collocations (e.g., that-ashould occur.
(3) At some stage of learning, over-segmentation of function words that may begin otherwords (e.g., be-have) should occur.
(4) The learnershould attend to the ends of utterances as use themto help identify novel words.4 An Algorithm for SegmentationThe algorithm we propose is similar in style to previ-ous online bootstrapping segmenters (Gambell andYang, 2004; Lignos and Yang, 2010) but varies in afew crucial aspects.
First, it inserts word boundariesin a left-to-right fashion as it processes each utter-ance (i.e., in temporal order), unlike previous mod-els which have worked from the outside in.
Second,it can handle cases where the segmentation is am-biguous given the current lexicon and score multiplepossible segmentations.
Finally, the use of word-level stress information is an optional part of themodel, and not an essential part of the segmenta-tion process.
This allows us to examine the addi-tional power that stress provides on top of a sub-tractive segmentation system and allows the modelto generalize to languages where word-level stressis not present in the same fashion as English (e.g.,French).
We first discuss the individual operationsthe algorithm uses to segment an utterance, and thendiscuss how they are combined in the segmenter.4.1 The LexiconThe learner we propose will primarily use items inits lexicon to help identify new possible words.
The31structure of the lexicon is as follows:Lexicon.
The lexicon contains the phonological ma-terial of each word that the learner has previouslyhypothesized.
The lexicon stores a score along witheach word, which the segmenter may increment ordecrement.The score assigned to each entry in the lexiconrepresents the relative confidence that it is a trueword of the language.
Each increment simply addsto the score of an individual word and each decre-ment subtracts from it.4.2 Subtractive SegmentationSubtractive segmentation is the process of usingknown words to segment the speech signal, whichinfants appear to be able to do as young as at sixmonths of age (Bortfeld et al, 2005).Subtractive Segmentation.
When possible, removea known word in the lexicon from the front of theutterance being segmented.One way to apply subtractive segmentation is agreedy score-based heuristic for subtractive segmen-tation (Lignos and Yang, 2010), such that whenevermultiple words in the lexicon could be subtractedfrom an utterance, the entry with the highest scorewill deterministically be used.
This greedy approachresults in a ?rich get richer?
effect of the sort seen inDirichlet processes (Goldwater et al, 2009).
We willfirst discuss this approach and then later extend thisgreedy search to a beam search.Figure 1 gives the implementation of subtractivesegmentation in our algorithm.
This algorithm re-sults in the following properties:Initially, utterances are treated as words in isola-tion.
When the lexicon is empty, no word bound-aries will be inserted and the full contents of eachutterance will be added to the lexicon as a word.High-frequency words are preferred.
When pre-sented with a choice of multiple words to subtract,the highest scored word will be subtracted, whichwill prefer higher frequency words over lower fre-quency words in segmentation.Syllables between words are not necessarily con-sidered words.
Syllables that occur between sub-tractions are not added as words in the lexicon.
Forexample, if play and please are in the lexicon butcheckers is not, the utterance play checkers pleasewill be correctly segmented, but checkers will notbe added to the lexicon.
Much like infants appearto do, the learner does not place as much weight onless reliable boundaries hypothesized in the middleof an utterance (Seidl and Johnson, 2006).4.3 Incorporating Stress InformationA particularly useful constraint for defining a word,introduced to the problem of word segmentation byYang (2004) but previously discussed by Halle andVergnaud (1987), is as follows:Unique Stress Constraint (USC): A word can bearat most one primary stress.Yang (2004) evaluated the effectiveness of theUSC in conjunction with a simple approach to us-ing transitional probabilities, showing significantperformance improvements.
The availability ofsuch stress cues is not, however, an uncontroversialassumption; there are no language-universal cuesto stress and even within a single language auto-matic detection of word-level stress is still unreli-able (Van Kuijk and Boves, 1999), making auto-matic capture of such data for simulation purposesdifficult.Before taking advantage of word-level stress in-formation, the infant learner would need to iden-tify the acoustic correlates to word-level stress in herlanguage, and we will not address the specific mech-anisms that an infant learner may use to accomplishthe task of identifying word-level stress in this paper.Based on strong experimental evidence that infantsdiscriminate between weakly and strongly stressedsyllables and use it to group syllables into word-likeunits (Jusczyk et al, 1999), we assume that an infantmay attend to this cue and we evaluate our modelwith and without it.We adopt the USC for segmentation in the follow-ing fashion:Unique Stress Segmentation (USS).
Insert wordboundaries such that no word contains two strongstresses.
Do so in a lazy fashion, inserting bound-aries as a last resort just before adding another syl-lable to the current would cause it to contain twostrong stresses.32u?
the syllables of the utterance, initially with no word boundariesi?
0while i < len(u) doif u starts with one or more words in the lexicon thenChoose the highest scoring word w and remove it from the front of u by inserting a word boundary before and after it.Increment the score of wAdvance i to the last word boundary insertedelseAdvance i by one syllableend ifend whileAdd the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and theend of the utterance as a word in the lexicon with a score of 1Figure 1: Subtractive segmentation procedureu?
the syllables of the utterance, initally with no word boundariesi?
0seenStress?
Falsewhile i < len(u)?
1 doif u[i] is stressed thenseenStress?
Trueend ifif seenStress and u[i+ 1] is stressed thenInsert a word boundary between u[i] and u[i+ 1]w ?
the syllables between the previous boundary inserted (or the beginning of the utterance if no boundaries were inserted)and the boundary just insertedIncrement w?s score in the lexicon, adding it to the lexicon if neededseenStress?
Falseend ifi?
i+ 1end whilew ?
the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and theend of the utteranceIncrement w?s score in the lexicon, adding it to the lexicon if neededFigure 2: A Unique Stress Segmentation AlgorithmThis strategy is expressed in an algorithmic formin Figure 2.
The learner uses USS as a last resortto prevent creating a segmentation with an impossi-ble amount of stress in a single word.
For exampleconsider an unsegmented English utterance with thestressed syllables underlined: Givemetheball.
Ap-plying USS would create the following segmenta-tion: Givemethe ball.A USS-based algorithm would note the stress onthe first syllable, then keep scanning until anotherstress is located on the fourth syllable, inserting abreak between the two.
Givemethe and ball wouldbe added to the lexicon.
While this is not a per-fect segmentation, it can be used to aid subtractivesegmentation by seeding the lexicon, even if not allentries added to the lexicon are not correct.4.4 Combining Subtraction and StressInformationGiven our bootstrapping methodology, it is highlydesirable to be able to integrate USS along with sub-tractive segmentation.
An algorithm that combinesboth is shown in Figure 3.4.5 Extending to Beam SearchThe greedy segmentation proposed is limited in itsability to find a good segmentation by its reliance onlocal decisions.
A frequent undersegmentation errorof the greedy segmenter is of this type: partof anapple.
Because partof has a higher score than partat the point in learning where this utterance is en-countered, the greedy segmenter will always choosepartof.An alternative approach is to let the segmenter33u?
the syllables of the utterance, initally with no word boundariesi?
0while i < len(u) doif USS requires a word boundary thenInsert a word boundary and advance i, updating the lexicon as neededelse if Subtractive Segmentation can be performed thenSubtract the highest scoring word and advance i, updating the lexicon as neededelseAdvance i by one syllableend ifend whilew ?
the syllables between the last boundary inserted (or the beginning of the utterance if no boundaries were inserted) and theend of the utteranceIncrement w?s score in the lexicon, adding it to the lexicon if neededFigure 3: An algorithm combining USS and Subtractive Segmentationexplore multiple hypotheses at once, using a sim-ple beam search.
New hypotheses are added tosupport multiple possible subtractive segmentations.For example, using the utterance above, at the be-ginning of segmentation either part or partof couldbe subtracted from the utterance, and both possi-ble segmentations can be evaluated.
The learnerscores these hypotheses in a fashion similar to thegreedy segmentation, but using a function based onthe score of all words used in the utterance.
Thegeometric mean has been used in compound split-ting (Koehn and Knight, 2003), a task in many wayssimilar to word segmentation, so we adopt it as thecriterion for selecting the best hypothesis.
For ahypothesized segmentation H comprised of wordswi .
.
.
wn, a hypothesis is chosen as follows:argmaxH(?wi?Hscore(wi))1nFor any w not found in the lexicon we must assigna score; we assign it a score of one as that wouldbe its value assuming it had just been added to thelexicon, an approach similar to Laplace smoothing.Returning to the previous example, while thescore of partof is greater than that of part, the scoreof of is much higher than either, so if both partofan apple and part of an apple are considered, thehigh score of of causes the latter to be chosen.When beam search is employed, only words used inthe winning hypothesis are rewarded, similar to thegreedy case where there are no other hypotheses.In addition to preferring segmentations that usewords of higher score, it is useful to reduce theAlgorithm Word BoundariesPrecision Recall F-ScoreNo Stress InformationSyllable Baseline 81.68 100.0 89.91Subtractive Seg.
91.66 89.13 90.37Subtractive Seg.
+ Beam 2 92.74 88.69 90.67Word-level StressUSS Only 91.53 18.82 31.21USS + Subtractive Seg.
93.76 92.02 92.88USS + Subtractive Seg.
+Beam 294.20 91.87 93.02Table 1: Learner and baseline performancescore of words that led to the consideration of a los-ing hypothesis.
In the previous example we maywant to penalize partof so that we are less likely tochoose a future segmentation that includes it.
Set-ting the beam size to be two, forcing each hypothesisto develop greedily after an ambiguous subtractioncauses two hypotheses to form, we are guaranteeda unique word to penalize.
In the previous examplepartof causes the split between the two hypothesesin the beam, and thus the learner penalizes it to dis-courage using it in the future.5 Results5.1 EvaluationTo evaluate the performance of our model, we mea-sured performance on child-directed speech, usingthe same corpus used in a number of previous stud-ies that used syllabified input (Yang, 2004; Gambelland Yang, 2004; Lignos and Yang, 2010).
The eval-34uation set was comprised of adult utterances fromthe Brown (1973) data of the CHILDES database(MacWhinney, 2000).1 Phonemic transcriptions ofwords from the Carnegie Mellon Pronouncing Dic-tionary (CMUdict) Version 0.7 (Weide, 1998), us-ing the first pronunciation for each word and mark-ing syllables with level 1 stress as strong syllables.The corpus was syllabified using onset maximiza-tion.
Any utterance in which a word could not betranscribed using CMUDICT was excluded, leaving55,840 utterances.
We applied a probabilistic re-call function to the lexicon to simulate the fact thata child learner will not perfectly recall all hypothe-sized words either due to memory limitations, vari-ability in the input, or any other possible source offailure.
We used the same function and constant asused by Lignos and Yang (2010).To adjust the word-level stress information to bet-ter reflect natural speech, the stress information ob-tained from CMUdict was post-processed in the con-text of each utterance using the technique of Lig-nos and Yang (2010).
For any n adjacent primary-stress syllables, only the nth syllable retains primarystress; all others are made into weak syllables.
Thisreflects the fact that stress clash is avoided in Englishand that infants may not reliably detect acoustic cor-relates of stress in the input.In addition to variations of our algorithm, we eval-uated a baseline segmenter which marks every syl-lable boundary as a word boundary, treating eachsyllable as a word.
We tested five variants of ouralgorithm, adding combinations of USS, subtractivesegmentation, and adding beam search with a beamsize of two2 to subtractive segmentation.Precision and recall metrics were calculated overall word boundaries over all utterances in the cor-pus.
The segmenter?s task is effectively to classifyeach syllable boundary as a word boundary or not.As single-syllable utterances are unambiguously asingle word with no possible boundaries, they are1A separate set of previous studies have used a corpus se-lected by Brent (1999) for evaluation.
Due to length limitationsand the fact that the results presented here cannot be meaning-fully compared to those studies, we only present results on theBrown (1973) data here.2As larger beam sizes did not lead to any benefits, partlybecause they do not straightforwardly allow for penalization,we do not report results for larger beam sizes.excluded from evaluation but still given as input.Evaluation was performed by giving each algo-rithm a single pass over the data set, with the perfor-mance on every utterance included in the total score.This is the most challenging metric for an onlinesegmenter, as early mistakes made when the learnerhas been exposed to no data still count against it.5.2 PerformanceThe performance of several variations of our algo-rithm is given in Table 1.
The most surprising re-sult is the high performance provided by the sylla-ble baseline.
This good performance is both an arti-fact of English and the metrics used to evaluate thesegmenters.
In English, there are larger number ofmonosyllabic words than in other languages, result-ing in high precision in addition to the guaranteed100% recall because it predicts every possible wordboundary.
The standard metric of evaluating pre-cision and recall over word boundaries rather thanwords identified in each utterance also contributesto this performance; when this baseline is evaluatedwith a word-level precision and recall it does notperform as well (Lignos and Yang, 2010).Subtractive Segmentation provides an improve-ment in utterance evaluation over the Syllable Base-line, and adding beam search to it slightly improvesF-score, sacrificing precision for recall.
This is to beexpected from the penalization step in beam search;as the penalization penalizes some good words in ad-dition to undesirable ones, the purification of the ut-terance segmentation and the lexicon comes at thecost of recall from over-penalization.While USS alone is clearly not a sufficiently richsegmentation technique, it is important to note thatit is a high precision indicator of word boundaries,suggesting that stress information can be useful tothe learner even when used in this simple way.
Moreimportantly, USS contributes unique information tosubtractive segmentation, as the utterance F-scoreof subtractive segmentation improves from 90.37 to92.88.While the performance numbers show that thesegmenter performs competently at the task, themore significant question at hand is whether the er-rors committed by the learner match developmentalpatterns of infants.
As the design of the segmenterpredicts, the main error types of the Subtractive Seg-35mentation + USS algorithm fall into two classes:Function word collocations.
For example, thethird highest-scored non-word in the lexicon isthat?sa, congruent with observations of functionword collocations seen in children (Brown, 1973).Oversegmentation of function words.
Thegreedy approach used for segmenting the wordsof highest score results in function words beingaggressively segmented off the front of words, forexample a nother.
The highest scored non-word inthe lexicon is nother as a result.Adding beam search reduces the number of func-tion word collocations in the segmenter?s output; thelearner?s most commonly penalized lexicon entry isisthat.
However, beam search also penalizes a lot ofwords, such as another.
Thus the strategy used inbeam search predicts an early use of function wordcollocations, followed by later oversegmentation.6 DiscussionIn the discussion of related work, we identified twomajor paradigms in modeling word segmentation:optimization and bootstrapping approaches.
The al-gorithm presented here combines elements of both.Its behavior over time and across utterances is that ofa bootstrapping learner, but when processing eachutterance it selects a segmentation based on a sim-ple, cognitively plausible beam search.By using a beam search of the kind suggested, itis easy to see how a variety of other cues could beintegrated into the learning process.
We have given asimple function for selecting the best hypothesis thatonly relies on lexicon scores, but more sophisticatedfunctions could take multiple cues into account.
Forexample it has been observed that 7-month-olds at-tend more to distributional cues while 9-month-oldsattend more to stress cues (Thiessen and Saffran,2003).
A learner in which the weight placed onstress cues increases as the learner receives moredata would match this pattern.
Other research hassuggested a more complex hierarchy of cues (Mat-tys et al, 2005), but how the weighting of the vari-ous cues can be adjusted with more input remains anopen question.A crucial frontier in word segmentation is the ex-pansion of evaluation to include other languages.
Aswith many other tasks, creating solutions that per-form well in a broad variety of languages is im-portant but has not yet been pursued.
Future workshould attempt to match developmental patterns inother languages, which will require adding morpho-logical complexity to the system; the techniquesdeveloped for English are unlikely to succeed un-changed in other languages.Comparing with other algorithms?
published re-sults is difficult because of varying choices of datasets and metrics.
For example, other syllable-basedalgorithms have evaluated their performance usingword-level, as opposed to boundary-level, precisionand recall (Gambell and Yang, 2004; Lignos andYang, 2010).
We have adopted the more popularboundary-based metric here, but there is no way todirectly compare with work that does not use syllab-ified input.
The variety of possible evaluation met-rics obviates the need for a longer-form explorationof how existing approaches perform when evaluatedagainst varying metrics.
Additionally, a more stan-dard set of evaluation data in many languages wouldgreatly improve the ability to compare different ap-proaches to this task.7 ConclusionThe work presented here represents a step towardbringing together developmental knowledge regard-ing word segmentation and computational model-ing.
Rather than focusing on cues in artificial learn-ing experiments which may or may not generalizeto the natural development of word segmentation inchildren, we have shown how a simple algorithmfor segmentation mimics many of the patterns seenin infants?
developing competence.
We believe thiswork opens the door to a promising line of researchthat will make a stronger effort to see simulationsof language acquisition as not just an unsupervisedlearning task but rather a modeling task that musttake into account a broad variety of phenomena.8 AcknowledgmentsI would like to thank Charles Yang and Mitch Mar-cus for many enlightening discussions regarding thiswork.
The author was supported by an NSF IGERTgrant to the University of Pennsylvania Institute forResearch in Cognitive Science.36ReferencesR.N.
Aslin, J.Z.
Woodward, N.P.
LaMendola, and T.G.Bever.
1996.
Models of word segmentation in fluentmaternal speech to infants.
Signal to syntax: Boot-strapping from speech to grammar in early acquisi-tion, pages 117?134.R.N.
Aslin, J.R. Saffran, and E.L. Newport.
1998.Computation of conditional probability statistics by 8-month-old infants.
Psychological Science, 9(4):321.R.
Bijeljac-Babic, J. Bertoncini, and J. Mehler.
1993.How do 4-day-old infants categorize multisyllabic ut-terances?
Developmental Psychology, 29:711?711.H.
Bortfeld, J.L.
Morgan, R.M.
Golinkoff, and K. Rath-bun.
2005.
Mommy and me.
Psychological Science,16(4):298.M.R.
Brent and J.M.
Siskind.
2001.
The role of ex-posure to isolated words in early vocabulary develop-ment.
Cognition, 81(2):B33?B44.M.R.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34(1):71?105.R.
Brown.
1973.
A First Language: The EarlyStages.
Harvard Univ.
Press, Cambridge, Mas-sachusetts 02138.A.
Cutler and S. Butterfield.
1992.
Rhythmic cuesto speech segmentation: Evidence from juncturemisperception.
Journal of Memory and Language,31(2):218?236.M.C.
Frank, S. Goldwater, T.L.
Griffiths, and J.B. Tenen-baum.
2010.
Modeling human performance in statis-tical word segmentation.
Cognition.T.
Gambell and C. Yang.
2004.
Statistics learning anduniversal grammar: Modeling word segmentation.
InFirst Workshop on Psycho-computational Models ofHuman Language Acquisition, page 49.S.
Goldwater, T.L.
Griffiths, and M. Johnson.
2009.
ABayesian framework for word segmentation: Explor-ing the effects of context.
Cognition.M.
Halle and J.R. Vergnaud.
1987.
An essay on stress.MIT Press.D.
Hewlett and P. Cohen.
2009.
Word segmentationas general chunking.
In Psychocomputational Modelsof Language Acquisition Workshop (PsychoCompLA),July 29, 2009.M.
Johnson and S. Goldwater.
2009.
Improving non-parameteric Bayesian inference: experiments on unsu-pervised word segmentation with adaptor grammars.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 317?325.
Association for ComputationalLinguistics.M.
Johnson.
2008.
Using adaptor grammars to identifysynergies in the unsupervised acquisition of linguisticstructure.
In 46th Annual Meeting of the ACL, pages398?406.
Citeseer.P.W.
Jusczyk, D.M.
Houston, and M. Newsome.
1999.The Beginnings of Word Segmentation in English-Learning Infants.
Cognitive Psychology, 39(3-4):159?207.P.
Koehn and K. Knight.
2003.
Empirical methods forcompound splitting.
In Proceedings of the tenth con-ference on European chapter of the Association forComputational Linguistics-Volume 1, pages 187?193.Association for Computational Linguistics.C.
Lew-Williams, B. Pelucchi, and J. Saffran.
2011.
Iso-lated words enhance statistical learning by 9-month-old infants.
In Budapest CEU Conference on Cogni-tive Development 2011.C.
Lignos and C. Yang.
2010.
Recession Segmenta-tion: Simpler Online Word Segmentation Using Lim-ited Resources.
In Proceedings of CoNLL-2010, pages88?97.B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum Associates.S.L.
Mattys, L. White, and J.F.
Melhorn.
2005.
In-tegration of multiple speech segmentation cues: Ahierarchical framework.
Journal of ExperimentalPsychology-General, 134(4):477?500.D.C.
Olivier.
1968.
Stochastic grammars and languageacquisition mechanisms: a thesis.
Ph.D. thesis, Har-vard University.K.H.
Onishi, K.E.
Chambers, and C. Fisher.
2002.Learning phonotactic constraints from brief auditoryexperience.
Cognition, 83(1):B13?B23.P.
Perruchet and A. Vinter.
1998.
PARSER: A modelfor word segmentation.
Journal of Memory and Lan-guage, 39:246?263.A.M.
Peters.
1983.
The units of language acquisition.CUP Archive.S.
Pinker, Harvard University.
The President, and Fellowsof Harvard College.
1984.
Language learnabilityand language development.
Harvard University PressCambridge, MA.S.
Pinker.
2000.
Words and rules: The ingredients oflanguage.
Harper Perennial.D.E.
Rumelhart and J.L.
McClelland.
1986.
Parallel dis-tributed processing: Explorations in the microstruc-ture of cognition.
MIT Press, Cambridge, MA.J.R.
Saffran, R.N.
Aslin, and E.L. Newport.
1996a.Statistical Learning by 8-month-old Infants.
Science,274(5294):1926.J.R.
Saffran, E.L. Newport, and R.N.
Aslin.
1996b.
WordSegmentation: The Role of Distributional Cues.
Jour-nal of Memory and Language, 35(4):606?621.37J.R.
Saffran.
2001.
Words in a sea of sounds: The outputof infant statistical learning.
Cognition, 81(2):149?169.A.
Seidl and E.K.
Johnson.
2006.
Infant word segmenta-tion revisited: edge alignment facilitates target extrac-tion.
Developmental Science, 9(6):565?573.E.D.
Thiessen and J.R. Saffran.
2003.
When cues col-lide: Use of stress and statistical cues to word bound-aries by 7-to 9-month-old infants.
Developmental Psy-chology, 39(4):706?716.D.
Van Kuijk and L. Boves.
1999.
Acoustic character-istics of lexical stress in continuous telephone speech.Speech Communication, 27(2):95?111.A.
Venkataraman.
2001.
A statistical model for worddiscovery in transcribed speech.
Computational Lin-guistics, 27(3):351?372.R.L.
Weide.
1998.
The Carnegie Mellon PronouncingDictionary [cmudict.
0.6].C.D.
Yang.
2002.
Knowledge and learning in naturallanguage.
Oxford University Press, USA.C.D.
Yang.
2004.
Universal Grammar, statistics or both?Trends in Cognitive Sciences, 8(10):451?456.38
