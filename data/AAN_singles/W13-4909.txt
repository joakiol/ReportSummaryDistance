Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 78?85,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsThe AI-KU System at the SPMRL 2013 Shared Task : UnsupervisedFeatures for Dependency ParsingVolkan Cirik Husnu SensoyArtificial Intelligence LaboratoryKoc?
University, I?stanbul, Turkey{vcirik,hsensoy}@ku.edu.trAbstractWe propose the use of the word categories andembeddings induced from raw text as auxil-iary features in dependency parsing.
To in-duce word features, we make use of contex-tual, morphologic and orthographic propertiesof the words.
To exploit the contextual infor-mation, we make use of substitute words, themost likely substitutes for target words, gen-erated by using a statistical language model.We generate morphologic and orthographicproperties of word types in an unsupervisedmanner.
We use a co-occurrence model withthese properties to embed words onto a 25-dimensional unit sphere.
The AI-KU sys-tem shows improvements for some of the lan-guages it is trained on for the first Shared Taskof Statistical Parsing of Morphologically RichLanguages.1 IntroductionFor the first shared task of Workshop on StatisticalParsing of Morphologically Rich Languages (Sed-dah et al 2013), we propose to use unsupervisedfeatures as auxillary features for dependency pars-ing.We induce the unsupervised features using con-textual, morphological and orthographic propertiesof the words.
We use possible substitutes of the tar-get word which are generated by a statistical lan-guage model to exploit the contextual information.We induce morphological features with a HMM-based model (Creutz and Lagus, 2005).
We combinecontextual, morphological and orthographic featuresof co-occurring words within the co-occurrencedata embedding framework (Maron et al 2010).The framework embeds word types sharing simi-lar context, morphological and orthographic prop-erties closely on a 25-dimensional sphere.
Thus, itprovides the word embeddings on a 25 dimensionalsphere.
We conduct experiments using these wordembeddings with MaltParser (Nivre et al 2007) andMaltOptimizer (Ballesteros and Nivre, 2012).
Inaddition to CONLL features (Buchholz and Marsi,2006a), they are added as additional features and theparsers are configured such that they are able to ex-ploit these additional features.
As a first step we usereal valued word embeddings as they are.
Secondly,we discretize the real valued word embeddings.
Fi-nally, we cluster them and find fine-grained wordcategories for word types.Our experiments show that, the AI-KU systemleads to better results than the baseline experimentsfor some languages.
We claim that with the cor-rect parameter settings, these unsupervised featurescould be useful for dependency parsing.In the following sections, we introduce the relatedwork, the algorithm, experiments, results and pro-vide a conclusion.2 Related WorkThe features extracted from unlabeled corpora arealready used for all major NLP tasks.
Early stud-ies mainly use clustering based representations (es-pecially Brown clustering (Brown et al 1992)) toobtain those features.
Miller et al(2004; Freitag(2004) utilized Brown Clusters to improve NamedEntity Recognition (NER) performance whereasBiemann et al(2007) used them for NER, WordSense Disambiguation(WSD), and chunking.
Ush-ioda (1996) extended Brown Clustering to cluster78not only words but also phrases using hierarcicalclustering and uses them to improve supervised part-of-speech (PoS) tagging.
More recently, BrownClusters are used for Chinese word segmentationand NER (Liang, 2005).Just like other tasks, clustering based representa-tions are used to improve parser performance.
Kooet al(2008; Suzuki et al(2009) improved depen-dency parsing by using Brown clusters.
While Can-dito and Seddah (2010; Candito and Crabbe?
(2009)improved PCFG parsing by using them and Gold-berg et al(2009) improved PCFG parser for He-brew by using HMM generated features.
More re-cently Socher et al(2010) used word embeddingscomputed using method explained in (Collobert andWeston, 2008) for syntactic parsing.3 AlgorithmIn this section, the general flow of the algorithm willbe presented.
First, we explain how we generatethe substitute vectors.
Then, we explain the induc-tion procedure of morphological features.
In the fol-lowing subsection, we explain how we use substi-tute vectors and morphological features and gener-ate word embeddings.
The same flow is followedfor all languages we work on.3.1 Substitute VectorsA target word?s substitute vector is represented bythe vocabulary of words and their correspondingprobabilities of occurring in the position of the targetword.
(1) ?
Nobody thought you could just in-ject DNA into someone ?s body and theywould just suck it up.
?Probability Substitute Word0.123 thought0.091 knew0.064 felt0.062 said0.052 believed0.037 wishTable 1: Substitute Vector for ?thought?
in above sen-tence.Table 1 illustrates the substitute vector of?thought?
in (1).
There is a row for each word inthe vocabulary.
For instance, probability of ?knew?occurring in the position of ?thought?
is 9.1% in thiscontext.To calculate these probabilities, as described in(Yatbaz et al 2012), a 4-gram language model isbuilt with SRILM (Stolcke, 2002) on the corpora ofthe target languages.
For French, Hungarian, Pol-ish and Swedish we used Europarl Corpus1(Koehn,2005).
For German, CONLL-X German Corpusis used (Buchholz and Marsi, 2006b).
For He-brew, we combined HaAretz and Arutz 7 corpora ofMILA2(Itai and Wintner, 2008).
For the tokens seenless than 5 times we replace them with an unknowntag to handle unseen words in training and test data.We should note that these corpora are not providedto the other participants.To estimate probabilities of lexical substitutes, forevery token in our datasets, we use three tokens eachon the left and the right side of the token as a con-text.
Using Fastsubs (Yuret, 2012) we generated top100 most likely substitute words.
Top 100 substi-tute probabilities are then normalized to represent aproper probability distribution.We should emphasize that a substitute vector is afunction of the context and does not depend on thetarget word.3.2 Morphological FeaturesIn order to generate unsupervised word features, thesecond set of features that we use are morphologicaland orthographic features.The orthographic feature set used is similar to theone defined in (Berg-Kirkpatrick et al2010)INITIAL-CAP Capitalized words with theexception of sentence initialwords.NUMBER The token starts with adigit.CONTAINS-HYPHEN Lowercase words with aninternal hyphen.INITIAL-APOSTROPHE Tokens that start with anapostrophe.The morpological features are obtained using theunlabeled corpora that are used for the generation1http://www.statmt.org/europarl/2http://www.mila.cs.technion.ac.il79Figure 1: The Flow of The Modification for Handling New Featuresof substitute vectors, using Morfessor defined in(Creutz and Lagus, 2005).
We will only give abrief sketch of the model used.
Morfessor splitseach word into morphemes (word itself may also bea morpheme) which can be categorized under fourgroups, namely prefix, stem, suffix, non-morpheme.The model is defined as a maximum a posteriori(MAP) estimate which maximizes the lexicon (setof morphemes) over the corpus.The maximization problem is solved by using agreedy algorithm that iteratively splits and mergesmorphemes, then re-segments corpus using Viterbialgorithm and reestimates probabilities until conver-gence.
Finally, a final merge step takes place to re-move all non-morphemes.3.3 Co-occurence EmbeddingFor a pair of categorical variables, the Spherical Co-occurrence Data Embedding (S-CODE) framework(Maron et al 2010) represents each of their valueson a sphere such that frequently co-occurring valuesare positioned closely on this sphere.The input of S-CODE are tuples of values of cate-gorical variables.
In our case, these are word tokens,their substitutes, morphological and orthograpic fea-tures.
We construct the tuples by sampling substitutewords using substitute vectors, their correspondingmorphological and orthographic features of the to-kens.
On each row of the co-occurrence input, thereare the target token, its substitute sampled from itssubstitute vector, morphological and orthographicfeatures.
Tokens having the similar substitutes, mor-phological and orthographic features will be closelylocated on the sphere at the end of this process.
Asin (Yatbaz et al 2012), the dimension of the sphereis 25, in other words for each word type seen in thecorpora we have a 25 dimensional vector3.4 ExperimentsWe conduct experiments using MaltParser (Nivreet al 2007) and MaltOptimizer (Ballesteros andNivre, 2012) with features provided in CONLL for-mat and the additional unsupervised features that wegenerated with default settings of the parsers.
Tomake use of additional features, we need to modifyMaltParser accordingly.
Figure 1 shows that howwe use MaltOptimizer and MaltParser with new fea-tures.
In order to handle auxiliary features, the fea-ture model file is modified in two different ways.
Wehandle new features with feature functions Input[0]and Stack[0]4.
We should note that other featurefunctions should also be experimented as a futurework.The following subsections explain the details ofthe experiments.4.1 Experiment IOur first approach was trying to use word embed-dings as they are with the MaltParser.
For each tokenin the training and the test set, we added the corre-sponding 25-dimensional word vector from the wordembeddings file to the training and test sets.
If theword type is not present in the word embeddings,then, we use the unknown word vector.3The vectors can be downloaded here :https://github.com/wolet/sprml13-word-embeddings4Thanks for Joakim Nivre for his suggestions on this80Stack[0] Input[0]LAS UAS LaA LAS UAS Labeled AccuracyReal Valued Vectors 80.56 84.33 85.78 80.63 84.38 85.92Binning, b=5 80.25 84.07 85.58 80.45 84.20 85.79Binning, b=2 80.41 84.19 85.79 80.47 84.26 85.77Clustering, k = 50 80.48 84.29 85.79 80.50 84.24 85.78Clustering k = 300 80.49 84.23 85.83 80.58 84.31 85.82LAS UAS LaSBaseline 80.36 84.11 85.72Table 2: Results on German with MaltParser of Development Set with Default SettingsStack[0] Input[0]LAS UAS LaS LAS UAS LaSReal Valued Vectors 87.30 89.33 93.35 87.29 89.30 93.32Binning, b =2 87.12 89.20 93.20 87.04 89.11 93.16Clustering, k = 300 90.30 91.80 95.09 90.49 91.94 95.19LAS UAS LaSBaseline 90.38 91.88 95.14Table 3: Results on German with MaltOptimizer of Development SetGold PredictedLAS UAS LaS LAS UAS LaS Predicted (Unofficial)Best System 90.29 91.92 95.95 85.86 89.19 92.20 LAS UAS LaSAI-KU 1 86.39 88.21 94.07 72.57 78.54 82.39 AI-KU 1 79.92 83.94 88.51AI-KU 2 86.31 88.14 94.05 72.55 78.55 82.36 AI-KU 2 79.84 83.85 88.45Baseline 85.71 87.50 93.70 79.00 83.35 87.73Table 4: Results on FrenchGold PredictedLAS UAS LaS LAS UAS LaS Predicted (Unofficial)Best System 91.83 93.20 96.06 86.95 91.64 94.38 LAS UAS LaSAI-KU 1 86.98 88.71 93.70 82.32 85.31 89.95 AI-KU 1 84.08 86.71 91.13AI-KU 2 86.95 88.67 93.67 82.29 85.30 89.95 AI-KU 2 83.93 86.54 91.05Baseline 86.96 87.67 93.67 82.75 85.38 90.15Table 5: Results on GermanGold PredictedLAS UAS LaS LAS UAS LaSBest System 83.87 88.95 89.19 80.89 86.7 86.93AI-KU 1 79.42 84.48 86.52 69.01 75.84 79.01AI-KU 2 78.73 83.79 85.98 62.27 75.84 79.01Baseline 80.03 84.9 86.97 73.01 79.89 81.28Table 6: Results on Hebrew81Gold PredictedLAS UAS LaS LAS UAS LaS Predicted (Unofficial)Best System 88.06 91.14 92.58 86.13 89.81 90.92 LAS UAS LaSAI-KU 1 83.67 87.08 89.64 78.92 83.77 85.98 AI-KU 1 79.98 84.42 87.12AI-KU 2 83.63 87.06 89.58 78.76 83.60 85.95 AI-KU 2 79.74 84.12 86.93Baseline 83.14 86.56 89.20 79.63 83.71 85.89Table 7: Results on HungarianGold PredictedLAS UAS LaS LAS UAS LaSBest System 89.58 93.24 93.42 87.07 91.75 91.24AI-KU 1 85.16 88.86 90.87 81.86 86.96 88.06AI-KU 2 85.12 88.79 90.84 78.31 84.18 85.64Baseline 80.49 86.41 86.94 79.89 85.80 86.24Table 8: Results on PolishGold PredictedLAS UAS LaS LAS UAS LaSBest System 83.97 89.11 87.63 82.13 88.06 85.93AI-KU 1 78.87 85.19 83.44 76.35 83.30 81.37AI-KU 2 78.57 85.12 83.25 76.35 83.24 81.35Baseline 77.67 84.6 82.36 75.82 83.20 80.88Table 9: Results on SwedishGold PredictedPrecision Recall F1 Precision Recal F1Best System 99.41 99.38 99.39 81.68 79.97 80.81AI-KU 1 99.41 99.38 99.39 74.47 71.51 72.96AI-KU 2 99.38 99.36 99.37 74.34 71.51 72.89MaltOptimizer Baseline 98.77 99.18 99.26 72.64 68.09 70.29Table 10: Results of Multi Word Expressions on French824.2 Experiment IIThe second approach is discretizing the real valuedvectors.
For each dimension of word embeddings,we separate b equal sized bins.
Then, for each vec-tor?s dimensions, we assign their corresponding binnumbers.4.3 Experiment IIIThe third approach is clustering the word embed-dings.
We use a modified k-means algorithm (Arthurand Vassilvitskii, 2007).
We experiment with vary-ing number of clusters k.For each token in training and test file, we useword type?s cluster id as an auxiliary feature.
Again,if the token is not in the word embeddings file, weused the unknown word?s cluster id.5 ResultsIn Table 2, the experiments on German with Malt-Parser without the optimization step are demon-strated.
We use the default settings of the MaltParseras our baseline.
We use training data consisting of5000 sentences with gold tags as training set and theprovided development data as test set.When we use real valued word embeddings asan auxiliary feature, we observe slight improvementcompared to MaltParser baseline.
The large bin-ning size results in worse results compared to base-line due to sparsity.
Clustering again leads to someimprovement compared to MaltParser baseline.
Wealso observe that increasing the number of clustersresult in better scores compared to smaller k.In Table 3, the results on German with MaltOp-timizer can be seen.
As a baseline, again, we usetraining data consisting of 5000 sentences with goldtags as training set and the provided developmentdata as test set.
We use the baseline experiment?sparsing algorithm, feature model and learning algo-rithm to experiment with word embedding, binningand clustering on MaltParser.Unlike in Table 2, in Table 3 we observe that onlythe clustering experiment outperforms the baselinebut not significantly.
Since clustering is leads tobest results, for all other languages, we apply thesame optimization and clustering pipeline.
The onlydifference is that when the MaltOptimizer suggestsStack Projective as the best algorithm, instead of In-put[0] ve use Stack[0], Stack[1], Stack[2] as featurefunctions.
The two systems of AI-KU only differ inthese feature functions.In Table 3-7, the results of the best system, base-line MaltOptimizer result and our two submittedsystems can be seen.
For Polish, our system outper-foms the MaltOptimizer baseline significantly.
Forthe rest of the languages, our systems are not signif-icantly better or worse than the baseline.
We makean assumption that we need to find the optimum set-tings, for instance the number of clusters, for eachlanguage separately, instead of using the fixed set-tings for all languages.For French, German, Hungarian the model trainedon the data with gold features is mistakenly used fortesting on the data with predicted features.
To cor-rect these, for those languages, we report the unoffi-cial results that are obtained by training on predictedfeatures.For French, there is also another evaluation met-ric.
It is about capturing the Multi Word Expres-sions(MWE).
Table 10 reports the results of MWEand it shows that our system is significantly betterthan MaltOptimizer baseline.6 ConclusionWe can speculate on these results in couple of ways.First, for all languages we used the same numberof clusters.
The optimum number of clusters mayvary with the syntactic properties of these languages.Similarly, the optimum dimension of the word em-beddings may vary with the languages.
In addition,for co-occurence embedding and morphological in-duction we use the parameter settings of (Yatbaz etal., 2012) which is optimized for Part-of-Speech in-duction on Penn Treebank data.
We suggest to findthe optimum parameter settings for co-occurrenceembedding and morphological induction as a futurework.We only experimented with simple feature func-tions, namely Input and Stack functions.
Other con-figuration of these functions may lead to better re-sults.
Lastly, as a future direction, we propose touse real valued word embeddings and unsupervisedword categories as auxiliary features in the trainingphase of the MaltOptimizer.83AcknowledgmentsWe would like to thank Joakim Nivre and DenizYuret for valuable suggestions and their support.ReferencesD.
Arthur and S. Vassilvitskii.
2007. k-means++: Theadvantages of careful seeding.
In Proceedings of theeighteenth annual ACM-SIAM symposium on Discretealgorithms, pages 1027?1035.
Society for Industrialand Applied Mathematics.Miguel Ballesteros and Joakim Nivre.
2012.
Maltop-timizer: A system for maltparser optimization.
InLREC, pages 2757?2763.Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.2007.
Unsupervised part-of-speech tagging support-ing supervised methods.
In Proceedings of RANLP,volume 7.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.S.
Buchholz and E. Marsi.
2006a.
CoNLL-X shared taskon multilingual dependency parsing.
SIGNLL.Sabine Buchholz and Erwin Marsi.
2006b.
Conll-xshared task on multilingual dependency parsing.
InProceedings of the Tenth Conference on Computa-tional Natural Language Learning, CoNLL-X ?06,pages 149?164, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Marie Candito and Beno?
?t Crabbe?.
2009.
Improving gen-erative statistical parsing with semi-supervised wordclustering.
In Proceedings of the 11th InternationalConference on Parsing Technologies, pages 138?141.Association for Computational Linguistics.Marie Candito and Djame?
Seddah.
2010.
Parsing wordclusters.
In Proceedings of the NAACL HLT 2010 FirstWorkshop on Statistical Parsing of Morphologically-Rich Languages, pages 76?84.
Association for Com-putational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference onMachine learn-ing, pages 160?167.
ACM.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural language fromunannotated text.
In Proceedings of AKRR?05, Inter-national and Interdisciplinary Conference on Adap-tive Knowledge Representation and Reasoning, pages106?113, Espoo, Finland, June.Dayne Freitag.
2004.
Trained named entity recogni-tion using distributional clusters.
In Proceedings ofEMNLP, pages 262?269.Yoav Goldberg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and em-hmm-based lexical probabilities.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 327?335.
Association for ComputationalLinguistics.Alon Itai and Shuly Wintner.
2008.
Language resourcesfor Hebrew.
Language Resources and Evaluation,42(1):75?98, March.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
Colum-bus, Ohio USA, June.
ACL.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, MIT, May.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere embedding: An application to part-of-speech induction.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems23, pages 1567?1575.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In In Proceedings of HLT-NAACL,pages 337?342.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Djame Seddah, Reut Tsarfaty, Sandra Kubler, Marie Can-dito, Jinho Choi, Richard Farkas, Jennifer Foster, IakesGoenaga, Koldo Gojenola, Yoav Goldberg, SpenceGreen, Nizar Habash, Marco Kuhlmann, WolfgangMaier, Joakim Nivre, Adam Przepiorkowski, RyanRoth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Wolinski, Alina Wroblewska, and EricVillemonte de la Clergerie.
2013.
Overview of thespmrl 2013 shared task: A cross-framework evalua-tion of parsing morphologically rich languages.
InProceedings of the 4th Workshop on Statistical Pars-ing of Morphologically Rich Languages: Shared Task,Seattle, WA.Richard Socher, Christopher D Manning, and Andrew YNg.
2010.
Learning continuous phrase representa-tions and syntactic parsing with recursive neural net-works.
In Proceedings of the NIPS-2010 Deep Learn-ing and Unsupervised Feature Learning Workshop.84Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings International Con-ference on Spoken Language Processing, pages 257?286, November.Jun Suzuki, Hideki Isozaki, Xavier Carreras, and MichaelCollins.
2009.
An empirical study of semi-supervisedstructured conditional models for dependency parsing.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume 2-Volume 2, pages 551?560.
Association for Computa-tional Linguistics.Akira Ushioda.
1996.
Hierarchical clustering of wordsand applications to nlp tasks.
In Proceedings of theFourth Workshop on Very Large Corpora, pages 28?41.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmatic rep-resentations of word context.
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 940?951, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Deniz Yuret.
2012.
Fastsubs: An efficient and exact pro-cedure for finding the most likely lexical substitutesbased on an n-gram language model.
Signal Process-ing Letters, IEEE, 19(11):725?728, Nov.85
