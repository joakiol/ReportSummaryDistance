Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840Manchester, August 2008Prediction of Maximal Projection for Semantic Role LabelingWeiwei Sun?, Zhifang SuiInstitute of Computational LinguisticsPeking UniversityBeijing, 100871, China{ws, szf}@pku.edu.cnHaifeng WangToshiba (China) R&D Center501, Tower W2, Oriental PlazaBeijing, 100738, Chinawanghaifeng@rdc.toshiba.com.cnAbstractIn Semantic Role Labeling (SRL), argu-ments are usually limited in a syntax sub-tree.
It is reasonable to label arguments lo-cally in such a sub-tree rather than a wholetree.
To identify active region of argu-ments, this paper models Maximal Pro-jection (MP), which is a concept in D-structure from the projection principle ofthe Principle and Parameters theory.
Thispaper makes a new definition of MP in S-structure and proposes two methods to pre-dict it: the anchor group approach and thesingle anchor approach.
The anchor groupapproach achieves an accuracy of 87.75%and the single anchor approach achieves83.63%.
Experimental results also indicatethat the prediction of MP improves seman-tic role labeling.1 IntroductionSemantic Role Labeling (SRL) has gained the in-terest of many researchers in the last few years.SRL consists of recognizing arguments involvedby predicates of a given sentence and labeling theirsemantic types.
As a well defined task of shallowsemantic parsing, SRL has a variety of applicationsin many kinds of NLP tasks.A variety of approaches has been proposedfor the different characteristics of SRL.
More re-cent approaches have involved calibrating features(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;?This work was partial completed while this author was atToshiba (China) R&D Center.?c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Pradhan et al, 2005), analyzing the complex input?
syntax trees (Moschitti, 2004; Liu and Sarkar,2007), exploiting the complicated output ?
thepredicate-structure (Toutanova et al, 2005), aswell as capturing paradigmatic relations betweenpredicates (Gordon and Swanson, 2007).In prior SRL methods, role candidates are ex-tracted from a whole syntax tree.
Though sev-eral pruning algorithms have been raised (Xue andPalmer, 2004), the policies are all in global style.In this paper, a statistical analysis of Penn Prop-Bank indicates that arguments are limited in a localsyntax sub-tree rather than a whole one.
Prior SRLmethods do not take such locality into account andseek roles in a wider area.
The neglect of local-ity of arguments may cause labeling errors suchas constituents outside active region of argumentsmay be falsely recognized as roles.This paper uses insights from generative lin-guistics to guide the solution of locality of argu-ments.
In particular, Maximal Projection (MP)which dominates1active region of arguments ac-cording to the projection principle of principle andparameters.
Two methods, the anchor group ap-proach and the single anchor approach, are pro-posed to find the active sub-tree which is rooted byMP and covers all roles.
The solutions put forwardin this paper borrow ideas from NP-movementprinciple in generative linguistics and are in statis-tical flavor.
The anchor group approach achievesan accuracy of 87.75%, and the single anchor ap-proach achieves 83.63%.
Though the accuracy islower, the single anchor approach fits SRL better.1Dominate is an concept in X-bar theory are modeled.
As-suming ?
and ?
are two nodes in a syntax tree: ?
dominates?
means ?
is ancestor of ?.833Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task2 Maximal Projection and ItsGovernment of Arguments2.1 Maximal ProjectionPrinciple and parameters theory is a framework ofgenerative grammar.
X-bar theory, as a moduleof principle and parameters, restricts context-freephrase structure rules as follows:1. a phrase always contains a head of the sametype, i.e.
NPs Ns, VPs Vs, PPs Ps, etc.2.
XP(X?)
?
specifier X?3.
X?
?X complement(s)These structural properties are conventionally rep-resented as shown in figure 2.Figure 2: X-bar structureX is the head of the phrase XP.
X?
and XP(X?
)are called projections of X.
The head is also calledthe zero projection.
X-bar structure is integratedwith the properties of lexical items via the Projec-tion Principle of principle and parameters.
Thisprinciple is summed up as the properties of lexi-cal information project onto the syntax of the sen-tence.
For instance:?
Sue likes Picasso?
*Sue likesThe subcategorization frame of the lexical itemlike [ ,NP] ensures that the verb is followed by anNP and the second sentence is of ungrammaticalform.Maximal Projection (MP) is the constituentwhich is projected to the highest level of an X-barstructure from lexical entities and is therefore thetop node XP of the X-bar structure.Take figure 1 for instance, S is the MP of thepredicate come.
Though the syntax tree is not in D-structure (deep structure), the S-structure (surfacestructure) headed by come is similar to its genuineD-structure.
In a latter part of this section, a spe-cific definition of MP in S-structure will be givenfor application.2.2 MP Limits Active Region of ArgumentsMP holds all lexical properties of heads.
In partic-ular, the MP of a predicate holds predicate struc-ture information and the constituents out of its do-main cannot occupy argument positions.
?-theoryand government are two modules of principle andparameters.
They both suggest that the possi-ble positions of semantic roles are in the sub-treerooted by MP.834Concerning assignment of semantic roles toconstituents, ?-theory suggests that semantic rolesare assigned by predicates to their sisters (Chom-sky, 1986).
Furthermore, in a X-bar theory, com-plements are assigned semantic roles by the pred-icate and specifiers get roles from the V?.
In bothsituations the process of roles assignment is in sis-terhood condition and limited in the sub-structurewhich is dominated by the MP.
Only constituentsunder MP can get semantic roles.
The Case As-signment Principle also points out: Case is as-signed under government (Chomsky, 1981).
Takefigure 1 for instance, only NP-1 and PP-2 can getsemantic roles of the head come.From generative linguists?
point, MP limits sub-tree of arguments.
Therefore, finding the MP isequivalent to finding the active region of predicatestructure.2.3 Definition of MP in S-structureThough a clear enough definition of MP in D-structure has been previously illustrated, it is stillnecessary to define a specific one in S-structurefor application, especially for automatic parsingwhich are not exactly correct.
This paper de-fines MP in S-structure (hereinafter denote MPfor short) as following: for every predicate p in thesyntax tree T , there exists one and only one MPmp s.t.1.
mp dominates all arguments of p;2. all descendent nodes of mp don?t satisfy theformer condition.Due to its different characteristics from argu-ments, adjunct-like arguments are excluded fromthe set of arguments in generative grammar andmany other linguistic theories.
For this reason, thispaper does not take them into account.For gold syntax tree, there exists a one-to-onemapping between arguments and nodes of syn-tax trees, whereas automatic syntactic parsing con-tains no such mapping.
This paper do not takearguments which cannot get corresponding con-stituents into account to reduce the influence of au-tomatic parsing error.Take the sentence of figure 1 to illustrate ourdefinition of MP: S is MP of come since NP-1 andPP-2 are arguments of it.
There is no node map-ping to the argument Wall Street professionals inthe parsing tree.
Instead of covering argument?sfragments, we simply take it PP-4 as MP.2.4 Using MP Information in SRLThe boundaries of a predicate structure are twoword positions of the sentence.
It is difficult tomodel these two words.
On the contrary, MP, asone ancestor of predicate, has a clear-cut meaningand is ideal for modeling.
In this paper, the pol-icy to predict MP rather than two word positions iscarried out to deal with locality of arguments.Automatic prediction of MP can be viewed as apreprocessing especially a pruning preprocessingfor SRL.
Given a sentence and its parsing, SRLsystems can take seeking the active sub-tree rootedby MP as the first step.
Then SRL systems canwork on the shrunk syntax tree, and follow-up la-beling processes can be in a various form.
Mostof previous SRL methods still work without spe-cial processing.
Take figure 1 for example: whenlabeling include, as the MP is PP-4, just NP-7 willbe extracted as argument candidate.3 Analysis of Locality of ArgumentsPrinciple and parameters suggests that MP boundsarguments.
Additionally, a statistical analysisshows that possible positions of arguments are lim-ited in a narrow region of syntax tree.
An oppositeexperiment also shows that MP information is use-ful for SRL.3.1 Data and Baseline SystemIn this paper, CoNLL-2005 SRL shared taskdata (Carreras and M`arquez, 2005) is used as cor-pus.
The data consists of the Wall Street Jour-nal (WSJ) part of the Penn TreeBank with infor-mation on predicate argument structures extractedfrom the PropBank corpus.
In addition, the testset of the shared task includes three sections of theBrown corpus.
Statistical analysis is based on sec-tion 02-21 of WSJ.
Experiments are conducted onWSJ and Brown corpus.
As defined by the sharedtask, section 02-21 of PropBank are used for train-ing models while section 23 and Brown corpus areused for test.
In terms of syntax information, weuse Charniak parser for POS tagging and full pars-ing.A majority of prior SRL approaches formulatethe SRL propblem as a multi-class classificationpropblem.
Generally speaking, these SRL ap-proaches use a two-stage architecture: i) argumentidentification; ii) argument classification, to solvethe task as a derivation of Gildea and Jurafsky?spioneer work (Gildea and Jurafsky, 2002).
UIUC835Precision Recall F?=1Arg0 86.28% 87.01% 86.64Arg1 79.37% 75.06% 77.15Arg2 69.48% 62.97% 66.07Arg3 69.01% 56.65% 62.22Arg4 72.64% 75.49% 74.04Table 1: SRL performance of UIUC SRLerPrecision Recall F?=1Arg0 91.84% 89.98% 90.90Arg1 81.73% 75.93% 78.72Arg2 69.86% 63.06% 66.29Arg3 71.13% 58.38% 64.13Arg4 73.08% 74.51% 73.79Table 2: SRL performance of UIUC SRLer usinginformation of gold MPSemantic Role Labeler2(UIUC SRLer) is a state-of-the-art SRL system that based on the championsystem of CoNLL-2005 shared task (Carreras andM`arquez, 2005).
It is utilized as a baseline systemin this paper.
The system participated in CoNLL-2005 is based on several syntactic parsing results.However, experiments of this paper just use thebest parsing result from Charniak parser.
Param-eters for training SRL models are the same as de-scribed in (Koomen, 2005).3.2 Active Region of ArgumentsAccording to a statistical analysis, the averagedepth from a target predicate to the root of a syntaxtree is 5.03, and the average depth from a predicateto MP is just 3.12.
This means about 40% of an-cestors of a predicate do not dominate argumentsdirectly.
In addition, the quantity of leaves in syn-tax tree is another measure to analyze the domain.On average, a syntax tree covers 28.51 leaves, andMP dominates only 18.19.
Roughly speaking, onlyabout 60% of words are valid for semantic roles.Statistics of corpora leads to the following conclu-sion: arguments which are assigned semantic rolesare in a local region of a whole syntax tree.3.3 Typical Errors Caused by Neglect ofLocality of ArgumentsThe neglect of the locality of arguments in priorSRL methods shows that it may cause errors.Some constituents outside active region of argu-ments may be falsely labeled as roles especially forthose being arguments of other predicates.
A sta-tistical analysis shows 20.62% of falsely labeledarguments are constituents out of MP domain inlabeling results of UIUC SRLer.
Take figure 1 forinstance, UIUC SRLer makes a mistake when la-beling NP-1 which is Arg1 of the predicate comefor the target include; it labels Arg0 to NP.
In fact,the active region of include is the sub-tree rooted2http://l2r.cs.uiuc.edu/ cogcomp/srl-demo.phpby PP-4.
Since NP-1 is an argument of anotherpredicate, some static properties of NP-1 make itconfusing as an argument.3.4 SRL under Gold MPIf MP has been found before labeling semanticroles, the set of role candidates will be shrunk,and the capability to identify semantic roles maybe improved.
An opposite experiment verifies thisidea.
In the first experiment, UIUC SRLer is re-trained as a baseline.
For comparison, during thesecond experiment, syntax sub-trees dominated bygold MP are used as syntactic information.
Bothtraining and test data are preprocessed with goldMP information.
That is to say we use pruned datafor training, and test is conducted on pruned syntaxsub-trees.Table 1 and 2 show that except for Arg4, all ar-guments get improved labeling performance, espe-cially Arg0.
Since arguments except for Arg0 arerealized as objects on the heel of predicate in mostcase, the information of MP is not so useful forthem as Arg0.
The experiment suggests that highperformance prediction of MP can improve SRL.4 Prediction of MPConforming to government and ?-theory, MP isnot too difficult to predict in D-structure.
Unfor-tunately, sentences being looked at are in their sur-face form and region of arguments has been ex-panded.
Simple rules alone are not adequate forfinding MP owing to a variety of movement be-tween D-structure and S-structure.
This paper de-signs two data driven algorithms based on move-ment principles for prediction of MP.4.1 NP-movement and Prediction of MP4.1.1 NP-movement in Principle andParametersThe relationship between D-structure and S-structure is movement: S-structure equals D-836structure plus movement.
NP-movement prin-ciple in principle and parameters indicates thatnoun phrases only move from A-positions (argu-ment position) which have been assigned rolesto A-positions which have not, leaving an NP-trace.
On account of ?-theory and government, A-positions are nodes m-commanded3by predicatesin D-structure.
In NP-movement, arguments moveto positions which are C-commanded4by targetpredicate and m-commanded by other predicates.Broadly speaking, A-positions are C-commandedby predicates after NP-movement.
The key of thewell-known pruning algorithm raised in (Xue andPalmer, 2004) is extracting sisters of ancestors asrole candidates.
Those candidate nodes are all C-commanders of a predicate.
NP-movement cangive an explanation why the algorithm works.4.1.2 Definition of Argument AnchorTo capture the characteristics of A-positions, wemake definition of A-anchor as following.
For ev-ery predicate p in the syntax tree T , denote A theset of C-commanders of p:?
a left-A-anchor satisfies:1. left-A-anchor belongs to A;2. left-A-anchor is a noun phrase (includ-ing NNS, NNP, etc.)
or simple declara-tive clause (S);3. left-A-anchor is on the left hand of p.?
a right-A-anchor satisfies:1. right-A-anchor belongs to A;2. right-A-anchor is a noun phrase (includ-ing NNS, NNP, etc.);3.
right-A-anchor is on the right hand of p.Take figure 1 for example, NP-1, NP-4 and NP-6 are left-A-anchors of include, and no right-A-anchor.
There is a close link between A-positionand the A-anchor that we defined, since A-anchorsoccupy A-positions.4.1.3 Anchor Model for Prediction of MPParents of A-anchors and first branching ances-tor of the predicate can cover 96.25% of MP andthe number of those ancestors is 2.78 times of the3M-command is an concept in X-bar syntax.
Assuming?
and ?
are two nodes in a syntax tree: ?
m-commands ?means ?
C-commands ?
and the MP of ?
dominates ?4C-command is an concept in X-bar theory.
Assuming ?and ?
are two nodes in a syntax tree: ?
C-commands ?
meansevery parent of ?
is ancestor of ?.number of MP.
The number of all ancestors is 6.65times.
The data suggests that taking only thesekinds of ancestors as MP candidates can shrink thecandidate set with a relatively small loss.4.2 Anchor Group ApproachMP is one ancestor of a predicate.
An natural ap-proach to predict MP is searching the set of allancestors.
This idea encounters the difficulty thatthere are too many ancestors.
In order to reducethe noise brought by non-anchors?
parents, the an-chor group approach prunes away useless ances-tors which are neither parents of A-anchors norfirst branching node upon predicate from MP can-didate set.
Then the algorithm scores all candidatesand chooses the MP in argmax flavor.
Formally,we denote the set of MP candidates C and the scorefunction S(.
).m?p = argmaxc?CS(mp|c)Probability function is chosen as score func-tion in this paper.
In estimating of the probabilityP (MP |C), log-linear model is used.
This model isoften called maximum entropy model in researchof NLP.
Let the set {1,-1} denotes whether a con-stituent is MP and ?
(c, {?1, 1}) ?
Rsdenotesa feature map from a constituent and the possibleclass to the vector space Rs.
Formally, the modelof our system is defined as:m?p = argmaxc?Ce<?(c,1),?>e<?(c,1),?>+e<?
(c,0),?>The algorithm is also described in pseudo codeas following.Ancestor Algorithm:1: collect parents of anchors and the firstbranching ancestor, denote them set C2: for every c ?
C3: calculate P (mp|c)4: return c?
that gets the maximal P (mp|c)4.2.1 FeaturesWe use some features to represent various as-pects of the syntactic structure as well as lexicalinformation.
The features are listed as follows:Path The path features are similar to the pathfeature which is designed by (Gildea and Jurafsky,2002).A path is a sequential collection of phrasetags.
There are two kinds of path features here: oneis from target predicate through to the candidate;the other is from the candidate to the root of thesyntax tree.
For include in the sentence of figure 1,the first kind of path of PP-2 is VBG+PP+NP+PPand the second is PP+VP+S.837C-commander Thread As well as path features,C-commander threads are other features whichreflect aspects of the syntactic structures.
C-commander thread features are sequential contain-ers of constituents which C-command the targetpredicate.
We design three kinds of C-commanderthreads: 1) down thread collects C-commandersfrom the anchor to the target predicate; 2) upthread collects C-commanders from the anchor tothe left/right most C-commander; 3) full threadcollects all C-commanders in the left/right direc-tion from the target predicate.
Direction is depen-dent on the type of the anchor - left or right anchor.Considering the grammatical characteristics ofphrase, we make an equivalence between suchphrase types:?
JJ, JJR, JJS, ADJP?
NN, NNP, NNS, NNPS, NAC, NX, NPBesides the equivalent constituents, we discardthese types of phrases:?
MD, RB, RBS, RBR, ADVPFor include in figure 1, the up thread ofNP-4 is VBG+,+NP+NP; the down threadis NP+IN+VBD+NP; the full thread isVBG+,+NP+NP+IN+VBD+NP.The phrase type of candidate is an important fea-ture for predictionCandidate of MP.
We also select the rank num-ber of the current candidate and the number of allcandidates as features.
For the former example,the two features for PP-2 are 2 and 3, since NP-4 is the second left-A-anchor and there are threeA-anchors of include.Anchor Features of anchor include the headword of the anchor, the boundary words and theirPOS, and the number of the words in the anchor.Those features are clues of judgment of whetherthe anchor?s position is an A-position.Forward predicate For the former example, theforward predicate of NP-4 is come.
The featuresinclude the predicate itself, the Levin class and theSCF of the predicate.predicate Features of predicate include lemma,Levin class, POS and SCF of the predicate.Figure 3: Flow diagram of the single anchor ap-proachFormal Subject An anchor may be formal sub-ject.
Take It is easy to say the specialist is not do-ing his job for example, the formal subject will berecognized as anchor of do.
We use a heuristic ruleto extract this feature: if the first NP C-commanderof the anchor is ?it?
and the left word of predicateis ?to?, the value of this feature is 1; otherwise 0.The Maximal Length of C-commanders Con-stituent which consists of many words may be abarrier between the predicate and an A-position.For the former example, if the target predicate isinclude, this feature of NP-1 is 2, since the largestconstituent NP-4 is made up of two words.4.3 Single Anchor ApproachAmong all A-anchors, the right most left-A-anchorsuch as NP-6 of include in figure 1 is the most im-portant one for MP prediction.
The parent of thiskind of left-A-anchor is the MP of the predicate,obtaining a high probability of 84.59%.
The singleanchor approach is designed based on right mostleft-A-anchor.
The key of this approach is an ac-tion prediction that when right most left-A-anchoris found, the algorithm predicts next action to re-turn which node of syntax tree as MP.
There isa label set of three types for learning ?
here, up,down.
After action is predicted, several simplerules are executed as post process of this predic-tion: i) if there is no left-A-anchor, return the rootof the whole syntax tree as MP; ii)if the predictedlabel is here, return the parent of right most left-A-anchor; iii) if the predicted label is down, return838Prediction AccuracyCorpus Action MPWSJ ?
87.75%Brown ?
88.84%Table 3: Accuracy of the anchor group ap-proachPrediction AccuracyCorpus Action MPWSJ 88.45% 83.63%Brown 90.10% 85.70%Table 4: Accuracy of the single anchor ap-proachPrecision Recall F?=1Arg0 86.23% 87.90% 87.06Arg1 80.21% 74.79% 77.41Arg2 70.09% 62.70% 66.19Arg3 71.74% 57.23% 63.67Arg4 74.76% 75.49% 75.12Table 5: SRL performance of UIUC SRLer us-ing information of predicted MP; the anchorgroup approach; WSJ test corpusPrecision Recall F?=1Arg0 87.03% 87.59% 87.31Arg1 80.24% 74.77% 77.41Arg2 70.35% 63.06% 66.51Arg3 71.43% 57.80% 63.90Arg4 73.33% 75.49% 74.40Table 6: SRL performance of UIUC SRLer us-ing information of predicted MP; the single an-chor approach; WSJ test corpusthe first branching node upon the predicate; iv) ifthe predicted label is up, return the root.
The ac-tion prediction also uses maximum entropy model.Figure 3 is the flow diagram of the single anchorapproach.
Features for this approach are similarto the former method.
Features of the verb whichis between the anchor and the predicate are added,including the verb itself and the Levin class of thatverb.5 Experiments and ResultsExperiment data and toolkit have been illustratedin section 3.
Maxent5, a maximum entropy model-ing toolkit, is used as a classifier in the experimentsof MP prediction.5.1 Experiments of Prediction of MPThe results are reported for both the anchor groupapproach and the single anchor approach.
Table 3summaries the accuracy results of MP predictionfor the anchor group approach; table 4 summariesresults of both action prediction and MP predictionfor the single anchor approach.
Both the anchorgroup approach and the single anchor approachhave better prediction performance in Brown testset, though the models are trained on WSJ cor-pus.
These results illustrate that anchor approacheswhich are based on suitable linguistic theories haverobust performance and overcome limitations oftraining corpus.5http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html5.2 Experiments of SRL Using MP PredictionLike the experiments in the end of section 3, weperform similar experiments under predicted MP.Both training and test corpus make use of predictedMP information.
It is an empirical tactic that pre-dicted information of maximal projection, insteadof gold information, is chosen for a training set.Experiments suggest predicted information is bet-ter.
Table 5 is SRL performance using the anchorgroup approach to predict MP; Table 6 is SRL per-formance using the single anchor approach.Compared with table 1 on page 4, table 5 andtable 6 both indicate the predicted MP can help tolabel semantic roles.
However, there is an interest-ing phenomenon.
Even though the anchor groupapproach achieves a higher performance of MP,the single anchor approach is more helpful to SRL.18.56% of falsely labeled arguments are out of MPdomain using the single anchor approach to predictMP, compared to 20.62% of the baseline system.In order to test robustness of the contributionof MP prediction to SRL, another opposite exper-iment is performed using the test set from Browncorpus.
Table 7 is the SRL performance of UIUCSRLer on Brown test set.
Table 8 is the corre-sponding performance using MP information pre-dicted by the single anchor approach.
Comparisonbetween table 7 and table 8 indicates the approachof MP prediction proposed in this paper adapts toother genres of corpora.Capability of labeling Arg0 gets significant im-provement.
Subject selection rule, a part of the-839Precision Recall F?=1Arg0 82.88% 85.51% 84.17Arg1 66.30% 63.17% 64.70Arg2 50.00% 45.58% 47.69Arg3 0.00% 0.00% 0.00Arg4 60.00% 20.00% 30.00Table 7: SRL performance of UIUC SRLer;Brown test corpusPrecision Recall F?=1Arg0 83.85% 86.22% 85.02Arg1 66.67% 63.02% 64.79Arg2 50.38% 44.90% 47.48Arg3 0.00% 0.00% 0.00Arg4 60.00% 20.00% 30.00Table 8: SRL performance of UIUC SRLer us-ing information of predicted MP; the single an-chor approach; Brown test corpusmatic hierarchy theory, states that the argumentthat the highest role (i.e.
proto-agent, Arg0 inPropBank) is the subject.
This means that Arg0 isusually realized as a constituent preceding a predi-cate and has a long distance from the predicate.
Asa solution of finding active region of arguments,MP prediction is helpful to shrink the searchingrange of arguments preceding the predicate.
Fromthis point, we give a rough explanation why exper-iment results for Arg0 are better.6 ConclusionInspired by the locality phenomenon that argu-ments are usually limited in a syntax sub-tree, thispaper proposed to label semantic roles locally inthe active region arguments dominated by maximalprojection, which is a concept in D-structure fromthe projection principle of the principle and param-eters theory.
Statistical analysis showed that MPinformation was helpful to avoid errors in SRL,such as falsely recognizing constituents outside ac-tive region as arguments.
To adapt the projectionconcept to label semantic roles, this paper definedMP in S-structure and proposed two methods topredict MP, namely the anchor group approach andthe single anchor approach.
Both approaches werebased on NP-movement principle of principle andparameters.
Experimental results indicated thatour MP prediction methods improved SRL.AcknowlegementsThe work is supported by the National Natu-ral Science Foundation of China under GrantsNo.
60503071, 863 the National High Technol-ogy Research and Development Program of Chinaunder Grants No.2006AA01Z144, 973 NaturalBasic Research Program of China under GrantsNo.2004CB318102.ReferencesCarreras, Xavier and Llu?
?s M`arquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: semantic rolelabeling.
In Proceedings of Conference on NaturalLanguage Learning.Chomsky, Noam.
1981.
Lectures on Government andBinding.
Foris Publications, Dordrecht.Chomsky, Noam.
1986.
Barriers.
MIT Press, Barriers.Gildea, Daniel and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computional Linguis-tics, 28(3):245?288.Gordon, Andrew and Reid Swanson.
2007.
Generaliz-ing Semantic Role Annotations Across SyntacticallySimilar Verbs.
In Proceedings of Conference on As-sociation for Computational Linguistics.Koomen, Peter, Vasina Punyakanok, Dan Roth andWen-tau Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
In Pro-ceedings of Conference on Natural Language Learn-ing.Liu, Yudong and Anoop Sarkar.
2004.
Experimen-tal Evaluation of LTAG-Based Features for SemanticRole Labeling.
In Proceedings of Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning.Mocshitti, Alessandro.
2004.
A Study on Convolu-tion Kernels for Shallow Semantic Parsing.
In Pro-ceedings of Conference on Association for Compu-tational Linguistics.Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin and Daniel Jurafsky.2005.
Support Vector Learning for Semantic Argu-ment Classification.
In Proceedings of Conferenceon Association for Computational Linguistics.Toutanova, Kristina, Aria Haghighi and ChristopherManning.
2005.
Joint Learning Improves Seman-tic Role Labeling.
In Proceedings of Conference onAssociation for Computational Linguistics.Xue, Nianwen and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceed-ings of Empirical Methods in Natural Language Pro-cessing.840
