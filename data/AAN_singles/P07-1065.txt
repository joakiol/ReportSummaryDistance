Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512?519,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsRandomised Language Modelling for Statistical Machine TranslationDavid Talbot and Miles OsborneSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh, EH8 9LW, UKd.r.talbot@sms.ed.ac.uk, miles@inf.ed.ac.ukAbstractA Bloom filter (BF) is a randomised datastructure for set membership queries.
Itsspace requirements are significantly belowlossless information-theoretic lower boundsbut it produces false positives with somequantifiable probability.
Here we explore theuse of BFs for language modelling in statis-tical machine translation.We show how a BF containing n-grams canenable us to use much larger corpora andhigher-order models complementing a con-ventional n-gram LM within an SMT sys-tem.
We also consider (i) how to include ap-proximate frequency information efficientlywithin a BF and (ii) how to reduce the er-ror rate of these models by first checking forlower-order sub-sequences in candidate n-grams.
Our solutions in both cases retain theone-sided error guarantees of the BF whiletaking advantage of the Zipf-like distributionof word frequencies to reduce the space re-quirements.1 IntroductionLanguage modelling (LM) is a crucial component instatistical machine translation (SMT).
Standard n-gram language models assign probabilities to trans-lation hypotheses in the target language, typically assmoothed trigram models, e.g.
(Chiang, 2005).
Al-though it is well-known that higher-order LMs andmodels trained on additional monolingual corporacan yield better translation performance, the chal-lenges in deploying large LMs are not trivial.
In-creasing the order of an n-gram model can result inan exponential increase in the number of parameters;for corpora such as the English Gigaword corpus, forinstance, there are 300 million distinct trigrams andover 1.2 billion 5-grams.
Since a LMmay be queriedmillions of times per sentence, it should ideally re-side locally in memory to avoid time-consuming re-mote or disk-based look-ups.Against this background, we consider a radicallydifferent approach to language modelling: insteadof explicitly storing all distinct n-grams, we store arandomised representation.
In particular, we showthat the Bloom filter (Bloom (1970); BF), a sim-ple space-efficient randomised data structure for rep-resenting sets, may be used to represent statisticsfrom larger corpora and for higher-order n-grams tocomplement a conventional smoothed trigrammodelwithin an SMT decoder.
1The space requirements of a Bloom filter are quitespectacular, falling significantly below information-theoretic error-free lower bounds while query timesare constant.
This efficiency, however, comes at theprice of false positives: the filter may erroneouslyreport that an item not in the set is a member.
Falsenegatives, on the other hand, will never occur: theerror is said to be one-sided.In this paper, we show that a Bloom filter can beused effectively for language modelling within anSMT decoder and present the log-frequency Bloomfilter, an extension of the standard Boolean BF that1For extensions of the framework presented here to stand-alone smoothed Bloom filter language models, we refer thereader to a companion paper (Talbot and Osborne, 2007).512takes advantage of the Zipf-like distribution of cor-pus statistics to allow frequency information to beassociated with n-grams in the filter in a space-efficient manner.
We then propose a mechanism,sub-sequence filtering, for reducing the error ratesof these models by using the fact that an n-gram?sfrequency is bound from above by the frequency ofits least frequent sub-sequence.We present machine translation experiments us-ing these models to represent information regardinghigher-order n-grams and additional larger mono-lingual corpora in combination with conventionalsmoothed trigram models.
We also run experimentswith these models in isolation to highlight the im-pact of different order n-grams on the translationprocess.
Finally we provide some empirical analysisof the effectiveness of both the log frequency Bloomfilter and sub-sequence filtering.2 The Bloom filterIn this section, we give a brief overview of theBloom filter (BF); refer to Broder andMitzenmacher(2005) for a more in detailed presentation.
A BF rep-resents a set S = {x1, x2, ..., xn} with n elementsdrawn from a universe U of size N .
The structure isattractive when N  n. The only significant stor-age used by a BF consists of a bit array of size m.This is initially set to hold zeroes.
To train the filterwe hash each item in the set k times using distincthash functions h1, h2, ..., hk.
Each function is as-sumed to be independent from each other and to mapitems in the universe to the range 1 to m uniformlyat random.
The k bits indexed by the hash valuesfor each item are set to 1; the item is then discarded.Once a bit has been set to 1 it remains set for the life-time of the filter.
Distinct items may not be hashedto k distinct locations in the filter; we ignore col-lisons.
Bits in the filter can, therefore, be shared bydistinct items allowing significant space savings butintroducing a non-zero probability of false positivesat test time.
There is no way of directly retrieving orennumerating the items stored in a BF.At test time we wish to discover whether a givenitem was a member of the original set.
The filter isqueried by hashing the test item using the same khash functions.
If all bits referenced by the k hashvalues are 1 then we assume that the item was amember; if any of them are 0 then we know it wasnot.
True members are always correctly identified,but a false positive will occur if all k correspondingbits were set by other items during training and theitem was not a member of the training set.
This isknown as a one-sided error.The probability of a false postive, f , is clearly theprobability that none of k randomly selected bits inthe filter are still 0 after training.
Letting p be theproportion of bits that are still zero after these n ele-ments have been inserted, this gives,f = (1?
p)k.As n items have been entered in the filter by hashingeach k times, the probability that a bit is still zero is,p?=(1?1m)kn?
e?knmwhich is the expected value of p. Hence the falsepositive rate can be approximated as,f = (1?
p)k ?
(1?
p?
)k ?(1?
e?knm)k.By taking the derivative we find that the number offunctions k?
that minimizes f is,k?
= ln 2 ?mn.which leads to the intuitive result that exactly halfthe bits in the filter will be set to 1 when the optimalnumber of hash functions is chosen.The fundmental difference between a Bloom fil-ter?s space requirements and that of any lossless rep-resentation of a set is that the former does not dependon the size of the (exponential) universe N fromwhich the set is drawn.
A lossless representationscheme (for example, a hash map, trie etc.)
must de-pend on N since it assigns a distinct representationto each possible set drawn from the universe.3 Language modelling with Bloom filtersIn our experiments we make use of both standard(i.e.
Boolean) BFs containing n-gram types drawnfrom a training corpus and a novel BF scheme, thelog-frequency Bloom filter, that allows frequencyinformation to be associated efficiently with itemsstored in the filter.513Algorithm 1 Training frequency BFInput: Strain, {h1, ...hk} and BF = ?Output: BFfor all x ?
Strain doc(x)?
frequency of n-gram x in Strainqc(x)?
quantisation of c(x) (Eq.
1)for j = 1 to qc(x) dofor i = 1 to k dohi(x)?
hash of event {x, j} under hiBF [hi(x)]?
1end forend forend forreturn BF3.1 Log-frequency Bloom filterThe efficiency of our scheme for storing n-gramstatistics within a BF relies on the Zipf-like distribu-tion of n-gram frequencies in natural language cor-pora: most events occur an extremely small numberof times, while a small number are very frequent.We quantise raw frequencies, c(x), using a loga-rithmic codebook as follows,qc(x) = 1 + blogb c(x)c. (1)The precision of this codebook decays exponentiallywith the raw counts and the scale is determined bythe base of the logarithm b; we examine the effect ofthis parameter in experiments below.Given the quantised count qc(x) for an n-gram x,the filter is trained by entering composite events con-sisting of the n-gram appended by an integer counterj that is incremented from 1 to qc(x) into the filter.To retrieve the quantised count for an n-gram, it isfirst appended with a count of 1 and hashed underthe k functions; if this tests positive, the count is in-cremented and the process repeated.
The procedureterminates as soon as any of the k hash functions hitsa 0 and the previous count is reported.
The one-sidederror of the BF and the training scheme ensure thatthe actual quantised count cannot be larger than thisvalue.
As the counts are quantised logarithmically,the counter will be incremented only a small numberof times.
The training and testing routines are givenhere as Algorithms 1 and 2 respectively.Errors for the log-frequency BF scheme are one-sided: frequencies will never be underestimated.Algorithm 2 Test frequency BFInput: x, MAXQCOUNT , {h1, ...hk} and BFOutput: Upper bound on qc(x) ?
Strainfor j = 1 to MAXQCOUNT dofor i = 1 to k dohi(x)?
hash of event {x, j} under hiif BF [hi(x)] = 0 thenreturn j ?
1end ifend forend forThe probability of overestimating an item?s fre-quency decays exponentially with the size of theoverestimation error d (i.e.
as fd for d > 0) sinceeach erroneous increment corresponds to a singlefalse positive and d such independent events mustoccur together.3.2 Sub-sequence filteringThe error analysis in Section 2 focused on the falsepositive rate of a BF; if we deploy a BF within anSMT decoder, however, the actual error rate will alsodepend on the a priori membership probability ofitems presented to it.
The error rate Err is,Err = Pr(x /?
Strain|Decoder)f.This implies that, unlike a conventional lossless datastructure, the model?s accuracy depends on othercomponents in system and how it is queried.We take advantage of the monotonicity of the n-gram event space to place upper bounds on the fre-quency of an n-gram prior to testing for it in the filterand potentially truncate the outer loop in Algorithm2 when we know that the test could only return pos-tive in error.Specifically, if we have stored lower-order n-grams in the filter, we can infer that an n-gram can-not present, if any of its sub-sequences test nega-tive.
Since our scheme for storing frequencies cannever underestimate an item?s frequency, this rela-tion will generalise to frequencies: an n-gram?s fre-quency cannot be greater than the frequency of itsleast frequent sub-sequence as reported by the filter,c(w1, ..., wn) ?
min {c(w1, ..., wn?1), c(w2, ..., wn)}.We use this to reduce the effective error rate of BF-LMs that we use in the experiments below.5143.3 Bloom filter language model testsA standard BF can implement a Boolean ?languagemodel?
test: have we seen some fragment of lan-guage before?
This does not use any frequency in-formation.
The Boolean BF-LM is a standard BFcontaining all n-grams of a certain length in thetraining corpus, Strain.
It implements the followingbinary feature function in a log-linear decoder,?bool(x) ?
?
(x ?
Strain)Separate Boolean BF-LMs can be included fordifferent order n and assigned distinct log-linearweights that are learned as part of a minimum errorrate training procedure (see Section 4).The log-frequency BF-LM implements a multino-mial feature function in the decoder that returns thevalue associated with an n-gram by Algorithm 2.?logfreq(x) ?
qc(x) ?
StrainSub-sequence filtering can be performed by usingthe minimum value returned by lower-order modelsas an upper-bound on the higher-order models.By boosting the score of hypotheses containing n-grams observed in the training corpus while remain-ing agnostic for unseen n-grams (with the exceptionof errors), these feature functions have more in com-mon with maximum entropy models than conven-tionally smoothed n-gram models.4 ExperimentsWe conducted a range of experiments to explore theeffectiveness and the error-space trade-off of Bloomfilters for language modelling in SMT.
The space-efficiency of these models also allows us to inves-tigate the impact of using much larger corpora andhigher-order n-grams on translation quality.
Whileour main experiments use the Bloom filter models inconjunction with a conventional smoothed trigrammodel, we also present experiments with these mod-els in isolation to highlight the impact of differentorder n-grams on the translation process.
Finally,we present some empirical analysis of both the log-frequency Bloom filter and the sub-sequence filter-ing technique which may be of independent interest.Model EP-KN-3 EP-KN-4 AFP-KN-3Memory 64M 99M 1.3Ggzip size 21M 31M 481M1-gms 62K 62K 871K2-gms 1.3M 1.3M 16M3-gms 1.1M 1.0M 31M4-gms N/A 1.1M N/ATable 1: Baseline and Comparison Models4.1 Experimental set-upAll of our experiments use publically available re-sources.
We use the French-English section of theEuroparl (EP) corpus for parallel data and languagemodelling (Koehn, 2003) and the English Giga-word Corpus (LDC2003T05; GW) for additionallanguage modelling.Decoding is carried-out using the Moses decoder(Koehn and Hoang, 2007).
We hold out 500 test sen-tences and 250 development sentences from the par-allel text for evaluation purposes.
The feature func-tions in our models are optimised using minimumerror rate training and evaluation is performed usingthe BLEU score.4.2 Baseline and comparison modelsOur baseline LM and other comparison models areconventional n-gram models smoothed using modi-fied Kneser-Ney and built using the SRILM Toolkit(Stolcke, 2002); as is standard practice these modelsdrop entries for n-grams of size 3 and above whenthe corresponding discounted count is less than 1.The baseline language model, EP-KN-3, is a trigrammodel trained on the English portion of the parallelcorpus.
For additional comparisons we also trained asmoothed 4-gram model on this Europarl data (EP-KN-4) and a trigram model on the Agence FrancePress section of the Gigaword Corpus (AFP-KN-3).Table 1 shows the amount of memory these mod-els take up on disk and compressed using the gziputility in parentheses as well as the number of dis-tinct n-grams of each order.
We give the gzip com-pressed size as an optimistic lower bound on the sizeof any lossless representation of each model.22Note, in particular, that gzip compressed files do not sup-port direct random access as required by our application.515Corpus Europarl Gigaword1-gms 61K 281K2-gms 1.3M 5.4M3-gms 4.7M 275M4-gms 9.0M 599M5-gms 10.3M 842M6-gms 10.7M 957MTable 2: Number of distinct n-grams4.3 Bloom filter-based modelsTo create Bloom filter LMs we gathered n-gramcounts from both the Europarl (EP) and the wholeof the Gigaword Corpus (GW).
Table 2 shows thenumbers of distinct n-grams in these corpora.
Notethat we use no pruning for these models and thatthe numbers of distinct n-grams is of the same or-der as that of the recently released Google Ngramsdataset (LDC2006T13).
In our experiments we cre-ate a range of models referred to by the corpus used(EP or GW), the order of the n-gram(s) entered intothe filter (1 to 10), whether the model is Boolean(Bool-BF) or provides frequency information (Freq-BF), whether or not sub-sequence filtering was used(FTR) and whether it was used in conjunction withthe baseline trigram (+EP-KN-3).4.4 Machine translation experimentsOur first set of experiments examines the relation-ship between memory allocated to the BF and BLEUscore.
We present results using the Boolean BF-LM in isolation and then both the Boolean and log-frequency BF-LMS to add 4-grams to our baseline3-gram model.Our second set of experiments adds3-grams and 5-grams from the Gigaword Corpus toour baseline.
Here we constrast the Boolean BF-LM with the log-frequency BF-LM with differentquantisation bases (2 = fine-grained and 5 = coarse-grained).
We then evaluate the sub-sequence fil-tering approach to reducing the actual error rate ofthese models by adding both 3 and 4-grams from theGigaword Corpus to the baseline.
Since the BF-LMseasily allow us to deploy very high-order n-grammodels, we use them to evaluate the impact of dif-ferent order n-grams on the translation process pre-senting results using the Boolean and log-frequencyBF-LM in isolation for n-grams of order 1 to 10.Model EP-KN-3 EP-KN-4 AFP-KN-3BLEU 28.51 29.24 29.17Memory 64M 99M 1.3Ggzip size 21M 31M 481MTable 3: Baseline and Comparison Models4.5 Analysis of BF extensionsWe analyse our log-frequency BF scheme in termsof the additional memory it requires and the errorrate compared to a non-redundant scheme.
The non-redundant scheme involves entering just the exactquantised count for each n-gram and then searchingover the range of possible counts at test time startingwith the count with maximum a priori probability(i.e.
1) and incrementing until a count is found orthe whole codebook has been searched (here the sizeis 16).We also analyse the sub-sequence filteringscheme directly by creating a BF with only 3-gramsand a BF containing both 2-grams and 3-grams andcomparing their actual error rates when presentedwith 3-grams that are all known to be negatives.5 Results5.1 Machine translation experimentsTable 3 shows the results of the baseline (EP-KN-3) and other conventional n-gram models trained onlarger corpora (AFP-KN-3) and using higher-orderdependencies (EP-KN-4).
The larger models im-prove somewhat on the baseline performance.Figure 1 shows the relationship between space al-located to the BF models and BLEU score (left) andfalse positive rate (right) respectively.
These experi-ments do not include the baseline model.
We can seea clear correlation between memory / false positiverate and translation performance.Adding 4-grams in the form of a Boolean BF or alog-frequency BF (see Figure 2) improves on the 3-gram baseline with little additional memory (around4MBs) while performing on a par with or abovethe Europarl 4-gram model with around 10MBs;this suggests that a lossy representation of the un-pruned set of 4-grams contains more useful informa-tion than a lossless representation of the pruned set.33An unpruned modified Kneser-Ney 4-gram model on theEurpoparl data scores slightly higher - 29.69 - while taking up489MB (132MB gzipped).516292827262510864210.80.60.40.20BLEU ScoreFalse positive rateMemory inMBEuroparl BooleanBF 4-gram(alone)BLEUScore Bool-BF-EP-4Falsepositive rateFigure 1: Space/Error vs. BLEU Score.30.5  3029.5  2928.5  2897531BLEU ScoreMemory inMBEP-Bool-BF-4 and Freq-BF-4 (withEP-KN-3)EP-Bool-BF-4 +EP-KN-3EP-Freq-BF-4 +EP-KN-3EP-KN-4 comparison(99M /31M gzip)EP-KN-3 baseline (64M/ 21Mgzip)Figure 2: Adding 4-grams with Bloom filters.As the false positive rate exceeds 0.20 the perfor-mance is severly degraded.
Adding 3-grams drawnfrom the whole of the Gigaword corpus rather thansimply the Agence France Press section results inslightly improved performance with signficantly lessmemory than the AFP-KN-3 model (see Figure 3).Figure 4 shows the results of adding 5-gramsdrawn from the Gigaword corpus to the baseline.
Italso contrasts the Boolean BF and the log-frequencyBF suggesting in this case that the log-frequency BFcan provide useful information when the quantisa-tion base is relatively fine-grained (base 2).
TheBoolean BF and the base 5 (coarse-grained quan-tisation) log-frequency BF perform approximatelythe same.
The base 2 quantisation performs worse30.5  3029.5  2928.5  2827.5  2710.80.60.40.20.1BLEU ScoreMemory inGBGW-Bool-BF-3 and GW-Freq-BF-3(with EP-KN-3)GW-Bool-BF-3 +EP-KN-3GW-Freq-BF-3 +EP-KN-3AFP-KN-3+ EP-KN-3Figure 3: Adding GW 3-grams with Bloom filters.30.5  3029.5  2928.5  2810.80.60.40.20.1BLEU ScoreMemory inGBGW-Bool-BF-5 and GW-Freq-BF-5(base2 and 5) (withEP-KN-3)GW-Bool-BF-5 +EP-KN-3GW-Freq-BF-5 (base 2) +EP-KN-3GW-Freq-BF-5 (base 5) +EP-KN-3AFP-KN-3+ EP-KN-3Figure 4: Comparison of different quantisation rates.for smaller amounts of memory, possibly due to thelarger set of events it is required to store.Figure 5 shows sub-sequence filtering resulting ina small increase in performance when false positiverates are high (i.e.
less memory is allocated).
Webelieve this to be the result of an increased a pri-ori membership probability for n-grams presentedto the filter under the sub-sequence filtering scheme.Figure 6 shows that for this task the most usefuln-gram sizes are between 3 and 6.5.2 Analysis of BF extensionsFigure 8 compares the memory requirements ofthe log-frequencey BF (base 2) and the Boolean5173130292810.80.60.40.2BLEU ScoreMemory inMBGW-Bool-BF-3-4-FTRand GW-Bool-BF-3-4 (with EP-KN-3)GW-Bool-BF-3-4-FTR+ EP-KN-3GW-Bool-BF-3-4+ EP-KN-3Figure 5: Effect of sub-sequence filtering.2726252410987654321BLEU ScoreN-gram orderEP-Bool-BF andEP-Freq-BF withdifferent order N-grams (alone)EP-Bool-BFEP-Freq-BFFigure 6: Impact of n-grams of different sizes.BF for various order n-gram sets from the Giga-word Corpus with the same underlying false posi-tive rate (0.125).
The additional space required byour scheme for storing frequency information is lessthan a factor of 2 compared to the standard BF.Figure 7 shows the number and size of frequencyestimation errors made by our log-frequency BFscheme and a non-redundant scheme that stores onlythe exact quantised count.
We presented 500K nega-tives to the filter and recorded the frequency of over-estimation errors of each size.
As shown in Section3.1, the probability of overestimating an item?s fre-quency under the log-frequency BF scheme decaysexponentially in the size of this overestimation er-ror.
Although the non-redundant scheme requires0102030405060708016151413121110987654321Frequency (K)Size of overestimationerrorFrequencyEstimationErrorson 500K NegativesLog-frequency BF(Bloomerror =0.159)Non-redundant scheme(Bloomerror =0.076)Figure 7: Frequency estimation errors.0100200300400500600700  1234567Memory (MB)N-gram order (Gigaword)Memory requirements for 0.125 false positiverateBool-BFFreq-BF (logbase-2quantisation)Figure 8: Comparison of memory requirements.fewer items be stored in the filter and, therefore, hasa lower underlying false positive rate (0.076 versus0.159), in practice it incurs a much higher error rate(0.717) with many large errors.Figure 9 shows the impact of sub-sequence filter-ing on the actual error rate.
Although, the false pos-itive rate for the BF containing 2-grams, in addition,to 3-grams (filtered) is higher than the false positiverate of the unfiltered BF containing only 3-grams,the actual error rate of the former is lower for mod-els with less memory.
By testing for 2-grams priorto querying for the 3-grams, we can avoid perform-ing some queries that may otherwise have incurrederrors using the fact that a 3-gram cannot be presentif one of its constituent 2-grams is absent.51800.10.20.30.40.50.60.7  0.511.522.533.5Error rateMemory (MB)Errorratewith sub-sequence filteringFiltered false positiverateUnfilteredfalsepos rate / actual error rateFiltered actual error rateFigure 9: Error rate with sub-sequence filtering.6 Related WorkWe are not the first people to consider building verylarge scale LMs: Kumar et al used a four-gramLM for re-ranking (Kumar et al, 2005) and in un-published work, Google used substantially larger n-grams in their SMT system.
Deploying such LMsrequires either a cluster of machines (and the over-heads of remote procedure calls), per-sentence fil-tering (which again, is slow) and/or the use of someother lossy compression (Goodman and Gao, 2000).Our approach can complement all these techniques.Bloom filters have been widely used in databaseapplications for reducing communications over-heads and were recently applied to encode wordfrequencies in information retrieval (Linari andWeikum, 2006) using a method that resembles thenon-redundant scheme described above.
Exten-sions of the BF to associate frequencies with itemsin the set have been proposed e.g., (Cormode andMuthukrishn, 2005); while these schemes are moregeneral than ours, they incur greater space overheadsfor the distributions that we consider here.7 ConclusionsWe have shown that Bloom Filters can form the ba-sis for space-efficient language modelling in SMT.Extending the standard BF structure to encode cor-pus frequency information and developing a strat-egy for reducing the error rates of these models bysub-sequence filtering, our models enable higher-order n-grams and larger monolingual corpora to beused more easily for language modelling in SMT.In a companion paper (Talbot and Osborne, 2007)we have proposed a framework for deriving con-ventional smoothed n-gram models from the log-frequency BF scheme allowing us to do away en-tirely with the standard n-gram model in an SMTsystem.
We hope the present work will help estab-lish the Bloom filter as a practical alternative to con-ventional associative data structures used in compu-tational linguistics.
The framework presented hereshows that with some consideration for its workings,the randomised nature of the Bloom filter need notbe a significant impediment to is use in applications.ReferencesB.
Bloom.
1970.
Space/time tradeoffs in hash coding withallowable errors.
CACM, 13:422?426.A.
Broder and M. Mitzenmacher.
2005.
Network applicationsof bloom filters: A survey.
Internet Mathematics, 1(4):485?509.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics (ACL?05), pages 263?270, Ann Arbor, Michigan.G.
Cormode and S. Muthukrishn.
2005.
An improved datastream summary: the count-min sketch and its applications.Journal of Algorithms, 55(1):58?75.J.
Goodman and J. Gao.
2000.
Language model size reductionby pruning and clustering.
In ICSLP?00, Beijing, China.Philipp Koehn and Hieu Hoang.
2007.
Factored translationmodels.
In Proc.
of the 2007 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP/Co-NLL).P.
Koehn.
2003.
Europarl: A multilingual corpus for eval-uation of machine translation philipp koehn, draft.
Availableat:http://people.csail.mit.edu/ koehn/publications/europarl.ps.S.
Kumar, Y. Deng, and W. Byrne.
2005.
Johns Hopkins Uni-versity - Cambridge University Chinese-English and Arabic-English 2005 NIST MT Evaluation Systems.
In Proceedingsof 2005 NIST MT Workshop, June.Alessandro Linari and Gerhard Weikum.
2006.
Efficient peer-to-peer semantic overlay networks based on statistical lan-guage models.
In Proceedings of the International Workshopon IR in Peer-to-Peer Networks, pages 9?16, Arlington.Andreas Stolcke.
2002.
SRILM ?
An extensible language mod-eling toolkit.
In Proc.
of the Intl.
Conf.
on Spoken Lang.Processing, 2002.David Talbot and Miles Osborne.
2007.
Smoothed Bloom fil-ter language models: Tera-scale LMs on the cheap.
In Pro-ceedings of the 2007 Conference on Empirical Methods inNatural Language Processing (EMNLP/Co-NLL), June.519
