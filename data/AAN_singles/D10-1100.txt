Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024?1034,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAutomatic Detection and Classification of Social EventsApoorv AgarwalDepartment of Computer ScienceColumbia UniversityNew York, U.S.A.apoorv@cs.columbia.eduOwen RambowCCLSColumbia UniversityNew York, U.S.A.rambow@ccls.columbia.eduAbstractIn this paper we introduce the new task ofsocial event extraction from text.
We distin-guish two broad types of social events depend-ing on whether only one or both parties areaware of the social contact.
We annotate partof Automatic Content Extraction (ACE) data,and perform experiments using Support Vec-tor Machines with Kernel methods.
We use acombination of structures derived from phrasestructure trees and dependency trees.
A char-acteristic of our events (which distinguishesthem from ACE events) is that the participat-ing entities can be spread far across the parsetrees.
We use syntactic and semantic insightsto devise a new structure derived from depen-dency trees and show that this plays a role inachieving the best performing system for bothsocial event detection and classification tasks.We also use three data sampling approachesto solve the problem of data skewness.
Sam-pling methods improve the F1-measure for thetask of relation detection by over 20% abso-lute over the baseline.1 IntroductionThis paper introduces a novel natural language pro-cessing (NLP) task, social event extraction.
We areinterested in this task because it contributes to ouroverall research goal, which is to extract a socialnetwork from written text.
The extracted social net-work can be used for various applications such assummarization, question-answering, or the detectionof main characters in a story.
For example, we man-ually extracted the social network of characters inAlice in Wonderland and ran standard social networkanalysis algorithms on the network.
The most influ-ential characters in the story were correctly detected.Moreover, characters occurring in a scene togetherwere given same social roles and positions.
Socialnetwork extraction has recently been applied to lit-erary theory (Elson et al, 2010) and has the potentialto help organize novels that are becoming machinereadable.We take a ?social network?
to be a network con-sisting of individual human beings and groups of hu-man beings who are connected to each other by thevirtue of participating in social events.
We definesocial events to be events that occur between peo-ple where at least one person is aware of the otherand of the event taking place.
For example, in thesentence John talks to Mary, entities John and Maryare aware of each other and the talking event.
Inthe sentence John thinks Mary is great, only John isaware of Mary and the event is the thinking event.In the sentence Rabbit ran by Alice there is no evi-dence about the cognitive states of Rabbit and Alice(because the Rabbit could have run by Alice withoutany one of them noticing each other).
A text can de-scribe a social network in two ways: explicitly, bystating the type of relationship between two individ-uals (e.g.
husband-wife), or implicitly, by describingan event which creates or perpetuates a social rela-tionship (e.g.
John talked to Mary).
We will callthese types of events social events.
We define twotypes of social events: interaction, in which bothparties are aware of the social event (e.g., a conver-sation), and observation, in which only one partyis aware of the interaction (e.g., thinking about or1024spying on someone).
Note that the notion of cogni-tive state is crucial to our definition.
This paper isthe first attempt to detect and classify social eventspresent in text.Our task is different from related tasks, notablyfrom the Automated Content Extraction (ACE) rela-tion and event extraction tasks because the events aredifferent (they are a class of events defined throughthe effect on participants?
cognitive state), and thelinguistic realization is different.
Mentions of enti-ties1 engaged in a social event are often quite distantfrom each other in the sentence (unlike in ACE rela-tions where about 70% of relations are local, in oursocial event annotation, only 25% of the events arelocal.
In fact, the average number of words betweenentities participating in any social event is 9.
)We use tree kernel methods (on structures derivedfrom phrase structure trees and dependency trees) inconjunction with Support Vector Machines (SVMs)to solve our tasks.
For the design of structures andtype of kernel, we take motivation from a systemproposed by Nguyen et al (2009) which is a state-of-the-art system for relation extraction.
Data skew-ness turns out to be a big challenge for the task ofrelation detection since there are many more pairsof entities without a relation as compared to pairs ofentities that have a relation.
In this paper we dis-cuss three data sampling techniques that deal withthis skewness and allow us to gain over 20% in F1-measure over our baseline system.
Moreover, weintroduce a new sequence kernel that outperformspreviously proposed sequence kernels for the task ofsocial event detection and plays a role to achieve thebest performing system for the task of social eventdetection and classification.The paper is structured as follows.
In Section 2,we compare our work to existing work, notably theACE extraction literature.
In Section 3, we presentour task in detail, and explain how we annotated ourcorpus.
We also show why this is a novel task, andhow it is different from the ACE extraction tasks.We then discuss kernel methods and the structureswe use, and introduce our new structure in Section 4.In Section 5, we present the sampling methods usedfor experiments.
In Section 6 we present our exper-1An entity mention is a reference of an entity in text.
Also,we use entities and people interchangeably since the only enti-ties we are interested in are people or groups of people.iments and results for social event detection and so-cial event classification tasks.
We conclude in Sec-tion 7 and mention our future direction of research.2 Literature SurveyThere has not been much work in developing tech-niques for ACE event extraction as compared toACE relation extraction.
The most salient work forevent extraction is Grishman et al (2005) and Ji andGrishman (2008).
To solve the task for event ex-traction, Grishman et al (2005) mainly use a combi-nation of pattern matching and statistical modelingtechniques.
They extract two kinds of patterns: 1)the sequence of constituent heads separating anchorand its arguments and 2) a predicate argument sub-graph of the sentence connecting anchor to all theevent arguments.
In conjunction they use a set ofMaximum Entropy based classifiers for 1) Triggerlabeling, 2) Argument classification and 3) Eventclassification.
Ji and Grishman (2008) further ex-ploit a correlation between senses of verbs (that aretriggers for events) and topics of documents.Our work shares some similarities.
However, in-stead of building different classifiers, we use kernelmethods with SVMs that ?naturally?
combine vari-ous patterns.
The structures we use for kernel meth-ods are a super-set of the patterns used by Grishmanet al (2005).
Moreover, in our work, we take goldannotation for entity mentions, and do not deal withthe task of named entity detection or resolution.
Fi-nally, our social events are a broad class of eventtypes, and they involve linguistic expressions for ex-pressing interactions and cognition that do not seemto have a correlation with the topics of documents.There has been much work in extracting ACE re-lations.
The supervised approaches used for relationextraction can broadly be divided into three maincategories: 1) feature-based approaches 2) kernel-based approaches and 3) a combination of featureand kernel based approaches.
The state-of-the-artfeature based approach is that of GuoDong et al(2005).
They use diverse lexical, syntactic and se-mantic knowledge for the task.
The lexical fea-tures they use are words between, before, and af-ter target entity mentions, the type of entity (Per-son, Organization etc.
), the type of mention (named,nominal or pronominal) and a feature called overlap1025that counts the number of other entity mentions andwords between the target entities.
To incorporatesyntactic features they use features extracted frombase phrase chunking, dependency trees and phrasestructure trees.
To incorporate semantic features,their approach uses resources like a country list andWordNet.
GuoDong et al (2005) report that 70% ofthe entities are embedded within each other or sep-arated by just one word.
This is a major differenceto our task because most of our relations span over along distance in a sentence.Collins and Duffy (2002) are among the earliestresearchers to propose the use of tree kernels forvarious NLP tasks.
Since then kernels have beenused for the task of relation extraction (Zelenko etal., 2002; Zhao and Grishman, 2005; Zhang et al,2006; Moschitti, 2006b; Nguyen et al, 2009).
Foran excellent review of these techniques, see Nguyenet al (2009).
In addition, there has been some workthat combines feature and kernel based methods(Harabagiu et al, 2005; Culotta and Jeffrey, 2004;Zhou et al, 2007).
Apart from using kernels over de-pendency trees, Culotta and Jeffrey (2004) incorpo-rate features like words, part of speech (POS) tags,syntactic chunk tag, entity type, entity level, rela-tion argument and WordNet hypernym.
Harabagiuet al (2005) leverage this approach by adding moresemantic feature derived from semantic parsers forFrameNet and PropBank.
Zhou et al (2007) use acontext sensitive kernel in conjunction with featuresthey used in their earlier publication (GuoDong etal., 2005).
However, we take an approach similarto Nguyen et al (2009).
This is because it incorpo-rates many of the features suggested in feature-basedapproaches by using combinations of various struc-tures derived from phrase structure trees and depen-dency trees.
In addition we use data sampling tech-niques to deal with the problem of data skewness.We not only try the structures suggested by Nguyenet al (2009) but also introduce a new sequence struc-ture on dependency trees.
We discuss their struc-tures and kernel method in detail in Section 4.3 Social Event Annotation Data3.1 Social Event AnnotationThere has been much work in the past on annotat-ing entities, relations and events in free text, mostnotably the ACE effort (Doddington et al, 2004).We leverage this work by annotating social events onthe English part of ACE 2005 Multilingual TrainingData2 that has already been annotated for entities,relations and events.
In Agarwal et al (2010), we in-troduce a comprehensive set of social events whichare conceptually different from the event annotationthat already exists for ACE.
Since our annotationtask is complex and layered, in Agarwal et al (2010)we present confusion matrices, Cohen?s Kappa, andF-measure values for each of the decision points thatthe annotators go through in the process of select-ing a type and subtype for an event.
Our annota-tion scheme is reliable, achieving a moderate kappafor relation detection (0.68) and a high kappa for re-lation classification (0.86).
We also achieve a highglobal agreement of 69.7% using a measure whichis inspired by Automated Content Extraction (ACE)inter-annotator agreement measure.
This comparesfavorably to the ACE annotation effort.Following are the two broad types of social eventsthat were annotated:Interaction event (INR): When both entities par-ticipating in an event are aware of each other and ofthe social event, we say they have an INR relation.Consider the following Example (1).
(1) [Toujan Faisal], 54, {said} [she] was{informed} of the refusal by an [InteriorMinistry committee] overseeing electionpreparations.
INRAs is intuitive, if one person informs the otherabout something, both have to be cognizant of eachother and of the informing event in which they areboth participating.Observation event (OBS): When only one person(out of the two people that are participating in anevent) is aware of the other and of the social event,we say they have an OBS relation.
Of the type OBS,there are three subtypes: Physical Proximity (PPR),Perception (PCR) and Cognition (COG).
PPR re-quires that one entity can observe the other entity inreal time not through a broadcast medium, in con-trast to the subtype PCR, where one entity observesthe other through media (TV, radio, magazines etc.
)Any other observation event that is not PPR or PCR2Version: 6.0, Catalog number: LDC2005E181026is COG.
Consider the aforementioned Example (1).In this sentence, the event said marks a COG re-lation between Toujan Faisal and the committee.This is because, when one person talks about anotherperson, the other person must be present in the firstperson?s cognitive state without any requirement onphysical proximity or external medium.As the annotations revealed, PPR and PCR oc-curred only twice and once, respectively, in the partof ACE corpus we annotated.
(They occur more fre-quently in another genre we are investigating suchas literary texts.)
We omit these extremely low-frequency categories from our current study; in thispaper we build classifiers to detect and classify onlyINR and COG events.3.2 Comparison Between Social Events andACE AnnotationsThe ACE effort is about entity, relation and eventannotation.
We use their annotations for entity typesPER.Individual and PER.Group and add our socialevent annotations.
Our event annotations are dif-ferent from ACE event annotations because we an-notate text that expresses the cognitive states of thepeople involved, or allows the annotator to infer it.Therefore, at the top level of classification we dif-ferentiate between events in which only one entityis cognizant of the other (observation) versus eventswhen both entities are cognizant of each other (in-teraction).
This distinction is, we believe, novel inevent or relation annotation.Now we present statistics and examples to makeclear how our annotations are different from ACEevent annotations.
The statistics are based on 62documents from the ACE corpus.
These files con-tain a total of 212 social events.
We found a total of63 candidate ACE events that had at least two Personentities involved.
Out of these 63 candidate events,54 match our annotations.
The majority of socialevents that match the ACE events are of type INR.On analysis, we found that most of these correspondto the ACE event type CONTACT.
Specifically, the?meeting?
event, which is an ACE CONTACT eventand an INR event according to our definition, is themajor cause of overlap.
However, our type INR hasa broader definition than ACE type CONTACT.
Forexample, in Example 1, we recorded an INR eventbetween Toujan Faisal and committee (event span:informed).
ACE does not record any event betweenthese two entities because informed does not entaila CONTACT event for ACE event annotations.
An-other example that will clarify the difference is thefollowing:(2) In central Baghdad, [a Reuters cameraman] and[a cameraman for Spain?s Telecinco] died whenan American tank fired on the Palestine HotelACE has annotated the above example as an eventof type CONFLICT in which there are two entitiesthat are of type person: the Reuters cameramanand the cameraman for Spain?s Telecinco, both ofwhich are arguments of type ?Victim?.
Being anevent that has two person entities involved makesthe above sentence a potential social event.
How-ever, we do not record any event between these enti-ties since the text does not reveal the cognitive statesof the two entities; we do not know whether one wasaware of the other.ACE defines a class of social relations (PER-SOC) that records named relations like friendship,co-worker, long lasting etc.
Also, there already existsystems that detect and classify these relations well.Therefore, even though these relations are directlyrelevant to our overall goal of social event extrac-tion, we do not annotate, detect or classify these re-lations in this paper.4 Tree Kernels, Discrete Structures, andLanguageIn this section, we give details of the structures andkernel we use for our classification tasks.
We alsodiscuss our motivation behind using these methods.Linear learning machines are one of the most popu-lar machines used for classification problems.
Theobjective of a typical classification problem is tolearn a function that separates the data into differ-ent classes.
The data is usually in the form of fea-tures extracted from abstract objects like strings,trees, etc.
A drawback of learning by using com-plex functions is that complex functions do not gen-eralize well and thus tend to over-fit.
The researchcommunity therefore prefers linear classifiers overother complex classifiers.
But more often than not,the data is not linearly separable.
It can be madelinearly separable by increasing the dimensionalityof data but then learning suffers from the curse of1027dimensionality and classification becomes computa-tionally intractable.
This is where kernels come tothe rescue.
The well-known kernel trick aids us infinding similarity between feature vectors in a highdimensional space without having to write down theexpanded feature space.
The essence of kernel meth-ods is that they compare two feature vectors in highdimensional space by using a dot product that is afunction of the dot product of feature vectors in thelower dimensional space.
Moreover, ConvolutionKernels (first introduced by Haussler (1999)) canbe used to compare abstract objects instead of fea-ture vectors.
This is because these kernels involvea recursive calculation over the ?parts?
of a discretestructure.
This calculation is usually made computa-tionally efficient using Dynamic Programming tech-niques.
Therefore, Convolution Kernels alleviate theneed of feature extraction (which usually requiresdomain knowledge, results in extraction of incom-plete information and introduces noise in the data).Therefore, we use convolution kernels with a linearlearning machine (Support Vector Machines) for ourclassification task.Now we present the ?discrete?
structures followedby the kernel we used.
We use the structures pre-viously used by Nguyen et al (2009), and proposeone new structure.
Although we experimented withall of their structures,3 here we only present the onesthat perform best for our classification task.
All thestructures and their combinations are derived from avariation of the underlying structures, Phrase Struc-ture Trees (PST) and Dependency Trees (DT).
Forall trees we first extract their Path Enclosed Tree,which is the smallest common subtree that containsthe two target entities (Moschitti, 2004).
We use theStanford parser (Klein and Manning, 2003) to getthe basic PSTs and DTs.
Following are the struc-tures that we refer to in our experiments and resultssection:PET: This refers to the smallest common phrasestructure tree that contains the two target entities.Dependency Words (DW) tree: This is the smallestcommon dependency tree that contains the two tar-get entities.
In Figure 1, since the target entities areat the leftmost and rightmost branch of the depen-3We omitted SK6, which is the worst performing sequencekernel in (Nguyen et al, 2009).committee...byT2-GrouppobjrefusalthedetT1-IndividualToujan_FaisalnsubjsaidinformedccompofpobjIndividualshensubjpassprepprepwasauxpass54apposFigure 1: Dependency parse tree for the sentence (inthe ACE corpus): ?
[Toujan Faisal], 54, {said} [she]was {informed} of the refusal by an [Interior Min-istry committee] overseeing election preparations.
?dency tree, this is in fact a DW (ignoring the gram-matical relations on the arcs).Grammatical Relation (GR) tree: If we replace thewords at the nodes by their relation to their corre-sponding parent in DW, we get a GR tree.
For exam-ple, in Figure 1, replacing Toujan Faisal by nsubj,54 by appos, she by nsubjpass and so on.Grammatical Relation Word (GRW) tree: We getthis tree by adding the grammatical relations as sep-arate nodes between a node and its parent.
For ex-ample, in Figure 1, adding nsubj as a node betweenT1-Individual and Toujan Faisal, appos as a nodebetween 54 and Toujan Faisal, and so on.Sequence Kernel of words (SK1): This is the se-quence of words between the two entities, includingtheir tags.
For our example in Figure 1, it wouldbe T1-Individual Toujan Faisal 54 said she was in-formed of the refusal by an T2-Group Interior Min-istry committee.Sequence in GRW tree (SqGRW): This is the newstructure that we introduce which, to the best of1028our knowledge, has not been used before for sim-ilar tasks.
It is the sequence of nodes from onetarget to the other in the GRW tree.
For example,in Figure 1, this would be Toujan Faisal nsubj T1-Individual said ccomp informed prep by T2-Grouppobj committee.We also use combinations of these structures(which we refer to as ?combined-structures?).
Forexample, PET GR SqGRW means we used the threestructures (PET, GR and SqGRW) together with akernel that calculates similarity between forests.We use the Partial Tree (PT) kernel, first proposedby Moschitti (2006a), for structures derived from de-pendency trees and Subset Tree (SST) kernel, pro-posed by Collins and Duffy (2002), for structuresderived from phrase structure trees.
PT is a relaxedversion of the SST; SST measures the similarity be-tween two PSTs by counting all subtrees common tothe two PSTs.
However, there is one constraint: alldaughter nodes of a node must be included.
In PTsthis constraint is removed.
Therefore, in contrastto SSTs, PT kernels compare many more substruc-tures.
They have been used successfully by (Mos-chitti, 2004) for the task of semantic role labeling.The choices we have made are motivated bythe following considerations.
We are interestedin modeling classes of events which are charac-terized by the cognitive states of participants?whois aware of whom.
The predicate-argument struc-ture of verbs can encode much of this informationvery efficiently, and classes of verbs express theirpredicate-argument structure in similar ways.
Forexample, many verbs of communication can ex-press their arguments using the same pattern: Johntalked/spoke/lectured/ranted/testified to Mary aboutPercy.
Independently of the verb, John is in a COGrelation with Percy and in an INR relation withMary.
All these verbs allow us to drop either orboth of the prepositional phrases, without alteringthe interpretation of the remaining constituents.
Andeven more strikingly, any verb that can be put in thatposition is likely to have this interpretation; for ex-ample, we are likely to interpret the neologistic Johngazooked to Mary about Percy as a similarly struc-tured social event.The regular relation between verb alternations andmeaning components has been extensively studied(Levin, 1993; Schuler, 2005).
This regularity inthe syntactic predicate-argument structure allows usto overcome lexical sparseness.
However, in or-der to exploit such regularities, we need to have ac-cess to a representation which makes the predicate-argument structure clear.
Dependency representa-tions do this.
Phrase structure representations alsorepresent predicate-argument structure, but in an in-direct way through the structural configurations, andwe expect this to increase the burden on the learner.
(In some phrase structure representations, some ar-guments and adjuncts are not disambiguated.)
Whenusing dependency structures, the SST kernel is farless appealing, since it forces us to always considerall daughter nodes of a node.
However, as we haveseen, it is certain daughter nodes, such as the pres-ence of a to PP and a about PP, which are important,while other daughters, such as temporal or locativeadjuncts, should be disregarded.
The PT kernel al-lows us to do this.5 Sampling MethodsIn this section we present the data sampling meth-ods we use to deal with data skewness.
We em-ploy two well-known data sampling methods on thetraining data before creating a model for test data;random under-sampling and random over-sampling(Kotsiantis et al, 2006; Japkowicz, 2000; Weiss andProvost, 2001).
These techniques are non-heuristicsampling methods that aim at balancing the classproportions by removing examples of the major-ity class and by duplicating instances of the minor-ity class respectively.
The reason for using thesetechniques is that learning is usually optimized toachieve high accuracy.
Therefore, when presentedwith skewed training data, a classifier may learn thetarget concept with a high accuracy by only predict-ing the majority class.
But if one looks at the preci-sion, recall, and F-measure, of such a classifier, theywill be very low for the minority class.
Since, likeother researchers, we are evaluating the goodness ofa model based on its precision, recall and F-measureand not on the accuracy on the test set, either weshould change the optimization function of the clas-sifier or employ data sampling techniques.
We em-ploy the latter because by balancing the class ratio,we are presenting the classifier with a more chal-lenging task of achieving a good accuracy when the1029majority base class is about 50%.
The major draw-backs of the two techniques is that under-samplingthrows away important information whereas over-sampling is prone to over-fitting (due to data dupli-cation).
As our results show, throwing away infor-mation about the majority class is much better thanthe system that tries to learn in an unbalanced sce-nario, but it performs worse than an approach usingdata duplication.
Since we are using SVMs as a clas-sifier, over-fitting is unlikely as reported by Kolcz etal.
(2003).In order to be sure that we are not over-fitting,we tried another sampling method proposed by Haand Bunke (1997), which is shown to be good so-lution to avoid over-fitting by Chawla et al (2002).This sampling technique proposes to generate syn-thetic examples of the minority class by ?perturb-ing?
the training data.
Specifically, Ha and Bunke(1997) produced new synthetic examples for the taskof handwritten character recognition by doing op-erations like rotation and skew on characters.
Thebasic idea is to produce synthetic examples that are?close?
to the real example from which these syn-thetic points are generated.
Analogously, we triedtwo transformations on our dependency tree struc-tures to produce synthetic examples.
The first trans-formation is based on the observation that in con-trol verb constructions, the matrix verb typicallydoes not contribute to the interpretation as a socialevent or not.
In this transformation, we lower thesubject to an argument verb if it does not have asubject, and repeat this procedure iteratively.
Asit turned out, this transformation only occurred 15times, and therefore it does not serve the purposeof over-sampling.
We tried a more relaxed trans-formation on the rightmost target in the tree.
Here,the observation is that for the COG social events,the second target may be very deeply embedded inthe tree.
For example, in Example 1, Toujan Faisaland the Interior Ministry Committee participate in aCOG event (because Faisal is aware of the Commit-tee during the saying event).
However, the contentsof what Faisal said is only relevant to the extent thatit pertains to the committee.
The depth of the em-bedding of the second target creates issues of datasparseness, as the path-enclosed trees become verylarge and very diverse.
Our transformation, there-fore, is to move the second target to its grandmothernode, attaching it on the left, and to recalculate thepath-enclosed tree, which is now smaller.
This is re-peated iteratively, so that a sentence with a deeplyembedded second target can yield a large number ofsynthesized structures.6 Experiments And ResultsIn this section we present experiments and results forour two tasks: social event detection and classifica-tion.
For the social event detection task, we wish tovalidate the following research hypotheses.
First, weaim to show the importance of using data samplingwhen evaluating on F-measure; specifically, we ex-pect under-sampling to outperform no sampling,over-sampling to outperform under-sampling, andover-sampling with transformations to out performover-sampling without transformations.
In contrast,the social event classification task does not sufferfrom data skewness because the INR and COG rela-tions; both occur almost the same number of times.Therefore, sampling methods may not be applied forthis task.
Second, for both tasks, we expect that acombination of kernels will out-perform individualkernels.
Moreover, we expect that dependency treeswill have a crucial role in achieving the best perfor-mance.6.1 Experimental Set-upWe use part of ACE data that we annotated for socialevents.
In all, we annotated 138 ACE documents.We retained the ACE entity annotations.
We con-sider all entity mention pairs in a sentence.
If ourannotators recorded a relation between a pair of en-tity mentions, we say there is a relation between thecorresponding entities.
If there are any other pairs ofentity mentions for the same pair of entity, we dis-card those.
For all other pairs of entity mentions,we say there is no relation.
Out of 138 files, fourfiles did not have any positive or negative examples(because there were very few and sparse entity men-tions in these four files).
We found a total of 1291negative examples, 172 examples belonging to classINR and 174 belonging to class COG.We use Jet?s sentence splitter4 and the StanfordParser (Klein and Manning, 2003) for phrase struc-ture trees and dependency parses.
For classifica-4http://cs.nyu.edu/grishman/jet/jetDownload.html1030tion, we used Alessandro Moschitti?s SVM-Light-TK package (Moschitti, 2006b) which is built onthe SVM-Light implementation of Joakhims (1999).For all our experiments, we perform 5-fold cross-validation.
We randomly divide the whole corpusinto 5 equal parts, such that no news story (or docu-ment) gets divided among two parts.
For each fold,we then merge 4 parts to create a training corpus andtreat the remaining part as a test corpus.
By keep-ing individual news stories intact, we make sure thatvocabulary specific to one story does not unrealisti-cally improve the performance.6.2 Social Event DetectionSocial event detection is the task of detecting if anysocial event exists between a pair of entities in a sen-tence.
We formulate the problem as a binary classi-fication task by labeling an example that does nothave a social event as class -1 and by labeling an ex-ample that either has an INR or COG social eventas class 1.
First we present results for our baselinesystem.
Our baseline system uses various structuresand their combinations but without any data balanc-ing.
5Kernel P R F1PET 70.28 21.46 32.38GR 87.79 15.21 25.55GRW 76.42 8.26 14.8SqGRW 48.78 6.08 10.38PET GR 70.21 27.76 38.89PET GR SqGRW 71.06 26.74 38.02GR SqGRW 82.0 24.47 36.12GRW SqGRW 68.19 17.01 25.06GR GRW SqGRW 79.81 21.99 32.57Table 1: Baseline System for the task of social eventdetection.
The proportion of positive data in trainingand test set is 21.1% and 20.6% respectively.Table 1 presents results for our baseline system.Grammatical relation tree structure (GR), a struc-ture derived from dependency tree by replacing thewords by their grammatical relations achieves thebest precision.
This is probably because the clas-5Although we experimented with many more structures andtheir combinations, due to space restrictions we mention onlythe top results.sifier learns that if both the arguments of a predi-cate contain target entities then it is a social event.Among kernels for single structures, the path en-closed tree for PSTs (PET) achieves the best re-call.
Furthermore, a combination of structures de-rived from PSTs and DTs performs best.
The se-quence kernels, perform much worse than SqGRW(F1-measure as low as 0.45).
Since it is the samecase for all subsequent experiments, we omit themfrom the discussion.Kernel P R F1PET 28.89 77.06 41.96GR 35.68 72.47 47.37GRW 29.7 83.6 43.6SqGRW 34.31 84.15 48.61PET GR 34.38 83.94 48.52PET GR SqGRW 34.34 83.66 48.52GR SqGRW 33.45 81.73 47.27GRW SqGRW 32.87 84.44 47.11GR GRW SqGRW 32.73 83.26 46.82Table 2: Under-sampled system for the task of rela-tion detection.
The proportion of positive examplesin the training and test corpus is 50.0% and 20.6%respectively.We now turn to experiments involving sampling.Table 2 presents results for under-sampling, i.e.
ran-domly removing examples belonging to the negativeclass until its size matches the positive class.
Table 2shows a large gain in F1-measure of 9.72% abso-lute over the baseline system (Table 1).
We foundthat worst performing kernel with under-samplingis SK1 with an F1-measure of 39.2% which isbetter than the best performance without under-sampling.
These results make it clear that doingunder-sampling greatly improves the performance ofthe classifier, despite the fact that we are using lesstraining data (fewer negative examples).
This is asexpected because we are evaluating on F1-measureand the classifier is optimizing for accuracy.Table 3 presents results for over-sampling i.e.replicating positive examples to achieve an equalnumber of examples belonging to the positive andnegative class.
Table 3 shows that the gain overthe baseline system now is 22.2% absolute.
Also,the gain over the under-sampled system is 12.5%1031Kernel P R F1PET 50.9 57.21 53.62GR 43.57 67.21 52.59GRW 46.05 64.15 53.31SqGRW 42.4 72.75 53.5PET GR 56.42 66.2 60.63PET GR SqGRW 57.28 66.26 61.11GR SqGRW 44.35 71.17 54.52GRW SqGRW 44.77 68.79 54.12GR GRW SqGRW 46.79 71.54 56.45Table 3: Over-sampled system for the task of rela-tion detection.
The proportion of positive examplesin the training and test corpus is 50.0% and 20.6%respectively.absolute.
As in the baseline system, a combina-tion of structures performs best.
As in the under-sampled system, when the data is balanced, SqGRW(sequence kernel on dependency tree in which gram-matical relations are inserted as intermediate nodes)achieves the best recall.
Here, the PET and GR ker-nel perform similar: this is different from the resultsof (Nguyen et al, 2009) where GR performed muchworse than PET for ACE data.
This exemplifiesthe difference in the nature of our event annotationsfrom that of ACE relations.
Since the average dis-tance between target entities in the surface word or-der is higher for our events, the phrase structure treesare bigger.
This means that implicit feature space ismuch sparser and thus not the best representation.PET 37.04 66.49 47.28GR 40.39 71.14 51.27GRW 45.16 66.82 53.47SqGRW 42.88 70.67 53.22PET GR 45.33 70.26 54.71PET GR SqGRW 45.26 72.97 55.67GR SqGRW 43.73 71.47 54.06GRW SqGRW 45.70 71.30 55.32GR GRW SqGRW 45.91 71.90 55.70Table 4: Over-sampled System with transformationfor relation detection.
The proportion of positive ex-amples in the training and test corpus is 51.7% and20.6% respectively.Table 4 presents results for using the over-sampling method with transformation that producessynthetic positive examples by using a transforma-tion on dependency trees such that the new syn-thetic examples are ?close?
to the original exam-ples.
This method achieves a gain 16.78% over thebaseline system.
We expected this system to per-form better than the over-sampled system but it doesnot.
This suggests that our over-sampled system isnot over-fitting; a concern with using oversamplingtechniques.6.3 Social Event ClassificationFor the social event classification task, we only con-sider pairs of entities that have an event.
Since theseevents could only be INR or COG, this is a binaryclassification problem.
However, now we are inter-ested in both outcomes of the classification, whileearlier we were only interested in knowing how wellwe were finding relations (and not in how well wewere finding ?non-relations?).
Therefore, accuracyis the relevant metric (Table 5).Kernel AccPET 76.85GR 71.04GRW 76.22SqGRW 75.78PET GR 76.34PET GR SqGRW 78.72GR SqGRW 75.60GRW SqGRW 76.96GR GRW SqGRW 77.29Table 5: System for the task of relation classifica-tion.
The two classes are INR and COG, and weevaluate using accuracy (Acc.).
The proportion ofINR relations in training and test set is 49.7% and49.63% respectively.Even though the task of reasoning if an eventis about one-way or mutual cognition seems hard,our system beats the chance baseline by 28.72%.These results show that there are significant cluesin the lexical and syntactic structures that help indifferentiating between interaction and cognition so-cial events.
Once again we notice that the combi-nation of kernels works better than single kernels1032alone, though the difference here is less pronounced.Among the combined-structure approaches, com-binations with dependency-derived structures con-tinue to outperform those not including dependency(the best all-phrase structure performer is PET SK1with 75.7% accuracy, not shown in Table 5).7 Conclusion And Future WorkIn this paper, we have introduced the novel tasks ofsocial event detection and classification.
We showthat data sampling techniques play a crucial rolefor the task of relation detection.
Through over-sampling we achieve an increase in F1-measure of22.2% absolute over a baseline system.
Our exper-iments show that as a result of how language ex-presses the relevant information, dependency-basedstructures are best suited for encoding this informa-tion.
Furthermore, because of the complexity ofthe task, a combination of phrase based structuresand dependency-based structures perform the best.This revalidates the observation of Nguyen et al(2009) that phrase structure representations and de-pendency representations add complimentary valueto the learning task.
We also introduced a new se-quence structure (SqGRW) which plays a role inachieving the best accuracy for both, social event de-tection and social event classification tasks.In the future, we will use other parsers (such assemantic parsers) and explore new types of linguis-tically motivated structures and transformations.
Wewill also investigate the relation between classes ofsocial events and their syntactic realization.AcknowledgmentsThe work was funded by NSF grant IIS-0713548.We thank Dr. Alessandro Moschitti and Truc-VienT.
Nguyen for helping us with re-implementing theirsystem.
We acknowledge Boyi Xie for his assistancein implementing the system.
We would also like tothank Dr. Claire Monteleoni and Daniel Bauer foruseful discussions and feedback.ReferencesApoorv Agarwal, Owen Rambow, and Rebecca J Passon-neau.
2010.
Annotation scheme for social networkextraction from text.
In Fourth Linguistic AnnotationWorkshop, ACL.N V Chawla, L O Hall, K W Bowyer, and W PKegelmeyer.
2002.
Smote: Synthetic minority over-sampling technique.
In Journal of Artificial Intelli-gence Research.M.
Collins and N. Duffy.
2002.
Convolution kernels fornatural language.
In Advances in neural informationprocessing systems.Aron Culotta and Sorensen Jeffrey.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 423?429,Barcelona, Spain, July.G Doddington, A Mitchell, M Przybocki, L Ramshaw,S Strassel, and R Weischedel.
2004.
The automaticcontent extraction (ace) program?tasks, data, and eval-uation.
LREC, pages 837?840.David K. Elson, Nicholas Dames, and Kathleen R. McK-eown.
2010.
Extracting social networks from literaryfiction.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics (ACL2010), Uppsala, Sweden.Ralph Grishman, David Westbrook, and Adam MeyersProc.
2005.
Nyu?s english ace 2005 system descrip-tion.
In ACE Evaluation Workshop.Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.2005.
Exploring various knowledge in relation extrac-tion.
In Proceedings of 43th Annual Meeting of theAssociation for Computational Linguistics.T.
M. Ha and H Bunke.
1997.
Off-line, handwritten nu-merical recognition by perturbation method.
In Pat-tern Analysis and Machine Intelligence.Sanda Harabagiu, Cosmin Adrian Bejan, and PaulMorarescu.
2005.
Shallow semantics for relation ex-traction.
In International Joint Conference On Artifi-cial Intelligence.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, University of Californiaat Santa Cruz.Nathalie Japkowicz.
2000.
Learning from imbalanceddata sets: Comparison of various strategies.
In AAAIWorkshop on Learning from Imbalanced Data Sets.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through unsupervised cross-document infer-ence.
In Proceedings of ACL.Thorsten Joakhims.
1999.
Making large-scale svmlearning practical.
In B. Scholkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.1033Dan Klein and Chistopher D. Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In In Advances in Neural Information Pro-cessing Systems 15 (NIPS).Aleksander Kolcz, Abdur Chowdhury, and Joshua Al-spector.
2003.
Data duplication: An imbalanceproblem.
In Workshop on Learning from ImbalancedDatasets, ICML.Sotiris Kotsiantis, Dimitris Kanellopoulos, and Panayio-tis Pintelas.
2006.
Handling imbalanced datasets: Areview.
In GESTS International Transactions on Com-puter Science and Engineering.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In Proceedingsof the 42nd Conference on Association for Computa-tional Linguistic.Alessandro Moschitti.
2006a.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InProceedings of the 17th European Conference on Ma-chine Learning.Alessandro Moschitti.
2006b.
Making tree kernels prac-tical for natural language learning.
In Proceedings ofEuropean chapter of Association for ComputationalLinguistics.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution kernels onconstituent, dependency and sequential structures forrelation extraction.
Conference on Empirical Methodsin Natural Language Processing.Karin Kipper Schuler.
2005.
Verbnet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,upenncis.Gary M Weiss and Foster Provost.
2001.
The effect ofclass distribution on classifier learning: an empiricalstudy.
Technical Report ML.TR-44, Rutgers Univer-sity, August.D.
Zelenko, C. Aone, and A. Richardella.
2002.
Kernelmethods for relation extraction.
In Proceedings of theEMNLP.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations betweenentities with both flat and structured features.
In Pro-ceedings of COLING-ACL.Shubin Zhao and Ralph Grishman.
2005.
Extracting re-lations with integrated information using kernel meth-ods.
In Proceedings of the 43rd Meeting of the ACL.GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-ing Zhu.
2007.
Tree kernel-based relation extractionwith context-sensitive structured parse tree informa-tion.
In Proceedings of EMNLP-CoNLL.1034
