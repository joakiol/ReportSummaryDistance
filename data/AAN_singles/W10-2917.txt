Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135?143,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsImproving Word Alignment by Semi-supervised EnsembleShujian Huang1, Kangxi Li2, Xinyu Dai1, Jiajun Chen11State Key Laboratory for Novel Software Technology at Nanjing UniversityNanjing 210093, P.R.China{huangsj,daixy,chenjj}@nlp.nju.edu.cn2School of Foreign Studies, Nanjing UniversityNanjing 210093, P.R.Chinarichardlkx@126.comAbstractSupervised learning has been recentlyused to improve the performance of wordalignment.
However, due to the limitedamount of labeled data, the performanceof ?pure?
supervised learning, which onlyused labeled data, is limited.
As a re-sult, many existing methods employ fea-tures learnt from a large amount of unla-beled data to assist the task.
In this pa-per, we propose a semi-supervised ensem-ble method to better incorporate both la-beled and unlabeled data during learning.Firstly, we employ an ensemble learningframework, which effectively uses align-ment results from different unsupervisedalignment models.
We then propose touse a semi-supervised learning method,namely Tri-training, to train classifiers us-ing both labeled and unlabeled data col-laboratively and further improve the result.Experimental results show that our meth-ods can substantially improve the qualityof word alignment.
The final translationquality of a phrase-based translation sys-tem is slightly improved, as well.1 IntroductionWord alignment is the process of learning bilin-gual word correspondences.
Conventional wordalignment process is treated as an unsupervisedlearning task, which automatically learns the cor-respondences between bilingual words using anEM style algorithm (Brown et al, 1993; Vogelet al, 1996; Och and Ney, 2003).
Recently, su-pervised learning methods have been used to im-prove the performance.
They firstly re-formalizeword alignment as some kind of classificationtask.
Then the labeled data is used to train theclassification model, which is finally used to clas-sify unseen test data (Liu et al, 2005; Taskar etal., 2005; Moore, 2005; Cherry and Lin, 2006;Haghighi et al, 2009).It is well understood that the performance ofsupervised learning relies heavily on the fea-ture set.
As more and more features are addedinto the model, more data is needed for train-ing.
However, due to the expensive cost of la-beling, we usually cannot get as much labeledword alignment data as we want.
This may limitthe performance of supervised methods (Wu etal., 2006).
One possible alternative is to usefeatures learnt in some unsupervised manner tohelp the task.
For example, Moore (2005) usesstatistics like log-likelihood-ratio and conditional-likelihood-probability to measure word associa-tions; Liu et al (2005) and Taskar et al (2005)use results from IBM Model 3 and Model 4, re-spectively.Ayan and Dorr (2006) propose another way ofincorporating unlabeled data.
They first train someexisting alignment models, e.g.
IBM Model4 andHidden Markov Model, using unlabeled data.
Theresults of these models are then combined using amaximum entropy classifier, which is trained us-ing labeled data.
This method is highly efficientin training because it only makes decisions onalignment links from existing models and avoidssearching the entire alignment space.In this paper, we follow Ayan and Dorr (2006)?sidea of combining multiple alignment results.
Andwe use more features, such as bi-lexical features,which help capture more information from unla-beled data.
To further improve the decision mak-ing during combination, we propose to use a semi-supervised strategy, namely Tri-training (Zhouand Li, 2005), which ensembles three classifiersusing both labeled and unlabeled data.
Morespecifically, Tri-training iteratively trains threeclassifiers and labels all the unlabeled instances.It then uses some instances among the unlabeledones to expand the labeled training set of each in-135dividual classifier.
As word alignment task usuallyfaces a huge parallel corpus, which contains mil-lions of unlabeled instances, we develop specificalgorithms to adapt Tri-training for this large scaletask.The next section introduces the supervisedalignment combination framework; Section 3presents our semi-supervised learning algorithm.We show the experiments and results in Section4; briefly overview related work in Section 5 andconclude in the last section.2 Word Alignment as a ClassificationTask2.1 ModelingGiven a sentence pair (e, f), where e =e1, e2, .
.
.
, eI and f = f1, f2, .
.
.
, fJ , an align-ment link ai,j indicates the translation correspon-dence between words ei and fj .
Word alignmentis to learn the correct alignment A between e andf , which is a set of such alignment links.As the number of possible alignment linksgrows exponentially with the length of e and f , werestrict the candidate set using results from severalexisting alignment models.
Note that, all the mod-els we employ are unsupervised models.
We willrefer to them as sub-models in the rest of this pa-per.Let A = {A1, A2, .
.
.
, An} be a set of align-ment results from sub-models; AI and AU be theintersection and union of these results, respec-tively.
We define our learning task as: for eachalignment link ai,j in the candidate set AC =AU?AI , deciding whether ai,j should be includedin the alignment result.
We use a random variableyi,j (or simply y) to indicate whether an alignmentlink ai,j ?
AC is correct.
A Maximum Entropymodel is employed to directly model the distribu-tion of y.
The probability of y is defined in For-mula 1, where hm(y, e, f ,A, i, j) is the mth fea-ture function, and ?m is the corresponding weight.p(y|e, f ,A, i, j)=exp?Mm=1 ?mhm(y, e, f ,A, i, j)?y??
{0,1} exp?Mm=1 ?mhm(y?, e, f ,A, i, j)(1)While Ayan and Dorr (Ayan and Dorr, 2006)make decisions on each alignment link in AU , wetake a different strategy by assuming that all thealignment links in AI are correct, which meansalignment links in AI are always included in thecombination result.
One reason for using thisstrategy is that it makes no sense to exclude analignment link, which all the sub-models vote forincluding.
Also, links in AI usually have a goodquality (In our experiment, AI can always achievean accuracy higher than 96%).
On the other hand,because AI is decided before the supervised learn-ing starts, it will be able to provide evidence formaking decisions on candidate links.Also note that, Formula 1 is based on the as-sumption that given AI , the decision on each y isindependent of each other.
This is the crucial pointthat saves us from searching the whole alignmentspace.
We take this assumption so that the Tri-training strategy can be easily applied.2.2 FeaturesFor ensemble, the most important features are thedecisions of sub-models.
We also use some otherfeatures, such as POS tags, neighborhood infor-mation, etc.
Details of the features for a given linkai,j are listed below.Decision of sub-models: Whether ai,j exists inthe result of kth sub-model Ak.
Besides in-dividual features for each model, we also in-clude features describing the combination ofsub-models?
decisions.
For example, if wehave 3 sub-models, there will be 8 featuresindicating the decisions of all the sub-modelsas 000, 001, 010, .
.
.
, 111.Part of speech tags: POS tags of previous, cur-rent and next words in both languages.
Wealso include features describing the POS tagpairs of previous, current and next word pairsin the two languages.Neighborhood: Whether each neighbor link ex-ists in the intersection AI .
Neighbor links re-fer to links in a 3*3 window with (i, j) in thecenter.Fertilities: The number of words that ei (or fj) isaligned to in AI .Relative distance: The relative distance betweenei and fj , which is calculated as abs(i/I ?j/J).Conditional Link Probability (CLP) : The con-ditional link probability (Moore, 2005) of ei136and fj .
CLP of word e and f is estimated onan aligned parallel corpus using Formula 2,CLPd(e, f) =link(e, f)?
dcooc(e, f)(2)where link(e, f) is the number of times e andf are linked in the aligned corpus; cooc(e, f)is the number of times e and f appear inthe same sentence pair; d is a discount-ing constant which is set to 0.4 followingMoore (2005).
We estimate these counts onour set of unlabeled data, with the union ofall sub-model results AU as the alignment.Union is used in order to get a better link cov-erage.
Probabilities are computed only forthose words that occur at least twice in theparallel corpus.bi-lexical features: The lexical word pair ei-fj .Lexical features have been proved to be useful intasks such as parsing and name entity recognition.Taskar et al (2005) also employ similar bi-lexicalfeatures of the top 5 non-punctuation words forword alignment.
Using bi-lexicons for arbitraryword pairs will capture more evidence from thedata; although it results in a huge feature set whichmay suffer from data sparseness.
In the next sec-tion, we introduce a semi-supervised strategy willmay alleviate this problem and further improve thelearning procedure.3 Semi-supervised methodsSemi-supervised methods aim at using unlabeledinstances to assist the supervised learning.
Oneof the prominent achievements in this area isthe Co-training paradigm proposed by Blum andMitchell (1998).
Co-training applies when the fea-tures of an instance can be naturally divided intotwo sufficient and redundant subsets.
Two weakclassifiers can be trained using each subset of fea-tures and strengthened using unlabeled data.
Blumand Mitchell (1998) prove the effectiveness of thisalgorithm, under the assumption that features inone set is conditionally independent of features inthe other set.
Intuitively speaking, if this condi-tional independence assumption holds, the mostconfident instance of one classifier will act as arandom instance for the other classifier.
Thus itcan be safely used to expand the training set of theother classifier.The standard Co-training algorithm requires anaturally splitting in the feature set, which is hardto meet in most scenarios, including the task ofword alignment.
Variations include using randomsplit feature sets or two different classification al-gorithms.
In this paper, we use the other Co-training style algorithm called Tri-training, whichrequires neither sufficient and redundant views nordifferent classification algorithms.3.1 Tri-trainingSimilar with Co-training, the basic idea of Tri-training (Zhou and Li, 2005) is to iteratively ex-pand the labeled training set for the next-roundtraining based on the decisions of the current clas-sifiers.
However, Tri-training employs three clas-sifiers instead of two.
To get diverse initial classi-fiers, the training set of each classifier is initiallygenerated via bootstrap sampling from the origi-nal labeled training set and updated separately.
Ineach round, these three classifiers are used to clas-sify all the unlabeled instances.
An unlabeled in-stance is added to the training set of any classifierif the other two classifiers agree on the labelingof this example.
So there is no need to explicitlymeasure the confidence of any individual classi-fier, which might be a problem for some learningalgorithms.
Zhou and Li (2005) also give a termi-nate criterion derived from PAC analysis.
As thealgorithm goes, the number of labeled instancesincreases, which may bring in more bi-lexical fea-tures and alleviate the problem of data sparseness.3.2 Tri-training for Word AlignmentOne crucial problem for word alignment is thehuge amount of unlabeled instances.
Typical par-allel corpus for word alignment contains at leasthundreds of thousands of sentence pairs, with eachsentence pair containing tens of instances.
Thatmakes a large set of millions of instances.
There-fore, we develop a modified version of Tri-trainingalgorithm using sampling techniques, which canwork well with such large scale data.
A sketch ofour algorithm is shown in Figure 1.The algorithm takes original labeled instanceset L, unlabeled sentence set SU , sub-model re-sults As for each s in SU and a sampling ratio r asinput.
Fk represents the kth classifier.
Variableswith superscript i represent their values during theith iteration.Line 2 initializes candidate instance set AC,s ofeach sentence s to be the difference set between137Input: L, SU , As for each s and sampling ratio r.1: for all sentence s in SU do2: A0C,s ?
AU,s ?AI,s //initializing candidate set3: end for4: for all l ?
{1, 2, 3} do5: L0l ?
Subsample(L, 0.33)6: F 0l ?
Train(L0l )7: end for8: repeat9: for all l ?
{1, 2, 3} do10: Let m,n ?
{1, 2, 3} and m ?= n ?= l; Lil = ?11: for all sentence s in SU do12: for all instance a in Ai?1C,s do13: if F i?1m (a) = F i?1n (a) then14: Ai?1C,s ?
Ai?1C,s ?
{(a, Fi?1m (a))}15: Lil ?
Lil ?
{(a, F i?1m (a))}16: end if17: end for18: end for19: end for20: for all l ?
{1, 2, 3} do21: Lil ?
Subsampling(Lil, r) ?
Li?1l22: F il ?
Train(Lil)23: AiC,s ?
Ai?1C,s24: end for25: until all AiC,s are unchanged or emptyOutput: F (x)?
argmaxy?
{0,1}?l:Fl(x)=y1Figure 1: Modified Tri-training AlgorithmAU,s and AI,s.
In line 5-6, sub-samplings are per-formed on the original labeled set L and the ini-tial classifier F 0l is trained using the sampling re-sults.
In each iteration, the algorithm labels eachinstance in the candidate set AiC,s for each clas-sifier with the other two classifiers trained in lastiteration.
Instances are removed from the candi-date set and added to the labeled training set (Lil)of classifier l, if they are given the same label bythe other two classifiers (line 13-16).A sub-sampling is performed before the labeledtraining set is used for training (line 21), whichmeans all the instances in Lil are accepted as cor-rect, but only part of them are added into the train-ing set.
The sampling rate is controlled by a pa-rameter r, which we empirically set to 0.01 in allour experiments.
The classifier is then re-trainedusing the augmented training set Lil (line 22).
Thealgorithm iterates until all instances in the candi-date sets get labeled or the candidate sets do notchange since the last iteration (line 25).
The result-ing classifiers can be used to label new instancesvia majority voting.Our algorithm differs from Zhou and Li (2005)in the following three aspects.
First of all, com-paring to the original bootstrap sampling initial-ization, we use a more aggressive strategy, whichSource Usage Sent.
Pairs Cand.
LinksLDC Train 288111 8.8MNIST?02 Train 200 5,849NIST?02 Eval 291 7,797Table 1: Data used in the experimentactually divides the original labeled set into threeparts.
This strategy ensures that initial classifiersare trained using different sets of instances andmaximizes the diversity between classifiers.
Wewill compare these two initializations in the ex-periments section.
Secondly, we introduce sam-pling techniques for the huge number of unlabeledinstances.
Sampling is essential for maintain-ing a reasonable growing speed of training dataand keeping the computation physically feasible.Thirdly, because the original terminate criterionrequires an error estimation process in each iter-ation, we adapt the much simpler terminate cri-terion of standard Co-training into our algorithm,which iterates until all the unlabeled data are fi-nally labeled or the candidate sets do not changesince the last iteration.
In other words, our algo-rithm inherits both the benefits of using three clas-sifiers and the simplicity of using Co-training styletermination criterion.
Parallel computing tech-niques are also used during the processing of un-labeled data to speed up the computation.4 Experiments and Results4.1 Data and Evaluation MethodologyAll our experiments are conducted on the lan-guage pair of Chinese and English.
For trainingalignment systems, a parallel corpus coming fromLDC2005T10 and LDC2005T14 is used as un-labeled training data.
Labeled data comes fromNIST Open MT Eval?02, which has 491 labeledsentence pairs.
The first 200 labeled sentence pairsare used as labeled training data and the rest areused for evaluation (Table 1).
The number of can-didate alignment links in each data set is also listedin Table 1.
These candidate alignment links aregenerated using the three sub-models described inSection 4.2.The quality of word alignment is evaluated interms of alignment error rate (AER) (Och and Ney,2003), classifier?s accuracy and recall of correctdecisions.
Formula 3 shows the definition of AER,where P and S refer to the set of possible and surealignment links, respectively.
In our experiments,138ModelName AER Dev AER Test Accuracy Recall F1Model4C2E 0.4269 0.4196 0.4898 0.3114 0.3808Model4E2C 0.3715 0.3592 0.5642 0.5368 0.5502BerkeleyAl.
0.3075 0.2939 0.7064 0.6377 0.6703Model4GDF 0.3328 0.3336 0.6059 0.6184 0.6121Supervised 0.2291 0.2430 0.8124 0.7027 0.7536Table 2: Experiments of Sub-modelsModelName AER Dev AER Test Accuracy Recall F1Supervised 0.2291 0.2430 0.8124 0.7027 0.7536BerkeleyAl.
0.3075 0.2939 0.7064 0.6377 0.6703Tri-Bootstrap0 0.2301 0.2488 0.8030 0.6858 0.7398Tri-Divide0 0.2458 0.2525 0.8002 0.6630 0.7251Tri-Bootstrap 0.2264 0.2468 0.7934 0.7449 0.7684Tri-Divide 0.2416 0.2494 0.7832 0.7605 0.7717Table 3: Experiments of Semi-supervised Modelswe treat all alignment links as sure links.AER = 1?
|A ?
P |+ |A ?
S||A|+ |S|(3)We also define a F1 score to be the harmonic meanof classifier?s accuracy and recall of correct deci-sions (Formula 4).F1 =2 ?
accuracy ?
recallaccuracy + recall(4)We also evaluate the machine translation qualityusing unlabeled data (in Table 1) and these align-ment results as aligned training data.
We usemulti-references data sets from NIST Open MTEvaluation as development and test data.
The En-glish side of the parallel corpus is trained intoa language model using SRILM (Stolcke, 2002).Moses (Koehn et al, 2003) is used for decoding.Translation quality is measured by BLEU4 scoreignoring the case.4.2 Experiments of Sub-modelsWe use the following three sub-models: bidi-rectional results of Giza++ (Och and Ney,2003) Model4, namely Model4C2E andModel4E2C, and the joint training result ofBerkeleyAligner (Liang et al, 2006) (Berke-leyAl.).
To evaluate AER, all three data setslisted in Table 1 are combined and used for theunsupervised training of each sub-model.Table 2 presents the alignment quality of thosesub-models, as well as a supervised ensemble ofthem, as described in Section 2.1.
We use the sym-metrized IBM Model4 results by the grow-diag-final-and heuristic as our baseline (Model4GDF).Scores in Table 2 show the great improvementof supervised learning, which reduce the align-ment error rate significantly (more than 5% AERpoints from the best sub-model, i.e.
Berke-leyAligner).
This result is consistent with Ayanand Dorr (2006)?s experiments.
It is quite reason-able that supervised model achieves a much higherclassification accuracy of 0.8124 than any unsu-pervised sub-model.
Besides, it also achieves thehighest recall of correct alignment links (0.7027).4.3 Experiments of Semi-supervised ModelsWe present our experiment results on semi-supervised models in Table 3.
The two strategiesof generating initial classifiers are compared.
Tri-Bootstrap is the model using the original boot-strap sampling initialization; and Tri-Divide isthe model using the dividing initialization as de-scribed in Section 3.2.
Items with superscripts 0indicate models before the first iteration, i.e.
ini-tial models.
The scores of BerkeleyAligner andthe supervised model are also included for com-parison.In general, all supervised and semi-supervisedmodels achieve better results than the best sub-model, which proves the effectiveness of labeledtraining data.
It is also reasonable that initial mod-els are not as good as the supervised model, be-cause they only use part of the labeled data fortraining.
After the iterative training, both the two1390 1000 2000 3000 4000 5000 60000.40.50.60.70.80.9Training Instances NumberScores(F?1,Accuracy, Recall)F?1RecallAccuracy(a)0 0.5 1 1.5 2 2.5 3x 1050.730.740.750.760.770.780.79Number of sentencesF?1 scoresTri?DivideSupervisedTri?Bootstrap(b)Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b) Experimentson the Size of Unlabeled Data in Tri-trainingTri-training models get a significant increase inrecall.
We attribute this to the use of bi-lexicalfeatures described in Section 2.2.
Analysis ofthe resulting model shows that the number ofbi-lexical features increases from around 300 tonearly 7,800 after Tri-training.
It demonstratesthat semi-supervised algorithms are able to learnmore bi-lexical features automatically from theunlabeled data, which may help recognize moretranslation equivalences.
However, we also noticethat the accuracy drops a little after Tri-training.This might also be caused by the large set of bi-lexical features, which may contain some noises.In the comparison of initialization strategies,the dividing strategy achieves a much higher re-call of 0.7605, which is also the highest amongall models.
It also achieves the best F1 score of0.7717, higher than the bootstrap sampling strat-egy (0.7684).
This result confirms that diversity ofinitial classifiers is important for Co-training stylealgorithms.4.4 Experiments on the Size of Data4.4.1 Size of Labeled DataWe design this experiment to see how the size oflabeled data affects the supervised training proce-dure.
Our labeled training set contains 5,800 train-ing instances.
We randomly sample different setsof instances from the whole set and perform thesupervised training.The alignment results are plotted in Figure 2a.Basically, both accuracy and recall increase withthe size of labeled data.
However, we also find thatthe increase of all the scores gets slower when thenumber of training instances exceeds 3,000.
Onepossible explanation for this is that the trainingset itself is too small and contains redundant in-stances, which may prevent further improvement.We can see in the Section 4.4.2 that the scores canbe largely improved when more data is added.4.4.2 Size of Unlabeled DataFor better understanding the effect of unlabeleddata, we run the Tri-training algorithm on unla-beled corpus of different sizes.
The original un-labeled corpus contains about 288 thousand sen-tence pairs.
We create 12 sub-corpus of it withdifferent sizes by selecting certain amounts of sen-tences from the beginning.
Our smallest sub-corpus consists of the first 5,000 sentence pairs ofthe original corpus; while the largest sub-corpuscontains the first 275 thousand sentence pairs.
Thealignment results on these different sub-corpus areevaluated (See Figure 2b).The result shows that as the size of unlabeleddata grows, the F1 score of Tri-Divide increasesfrom around 0.74 to 0.772.
The F1 score of Tri-Bootstrap also gets a similar increase.
This provesthat adding unlabeled data does help the learningprocess.
The result also suggests that when thesize of unlabeled data is small, both Tri-Bootstrapand Tri-Divide get lower scores than the super-vised model.
This is because the Tri-training mod-els only use part of the labeled data for the trainingof each individual classifier, while the supervisedmodel use the whole set.
We can see that whenthere are more than 50 thousand unlabeled sen-tence pairs, both Tri-training models outperformthe supervised model significantly.140ModelName Dev04 Test05 Test06 Test08Model4C2E 24.54 17.10 17.52 14.59Model4E2C 26.54 19.00 20.18 16.56BerkeleyAl.
26.19 20.08 19.65 16.70Model4GDF 26.75 20.67 20.58 17.05Supervised 27.07 20.00 19.47 16.13Tri-Bootstrap 26.88 20.49 20.76 17.31Tri-Divide 27.04 20.96 20.79 17.18Table 4: Experiments on machine translation (BLEU4 scores in percentage)Note that, both experiments on data size showsome unsteadiness during the learning process.We attribute this mainly to the random samplingwe use in the algorithm.
As there are, in all, about8.8 million instances , it is highly possible thatsome of these instances are redundant or noisy.And because our random sampling does not dis-tinguish different instances, the quality of result-ing model may get affected if these redundant ornoisy instances are selected and added to the train-ing set.4.5 Experiments on Machine TranslationWe compare the machine translation results ofeach sub-models, supervised models and semi-supervised models in Table 4.
Among sub-models,BerkeleyAligner gets better BLEU4 scores in al-most all the data sets except TEST06, whichagrees with its highest F1 score among all sub-models.
The supervised method gets the highestBLEU score of 27.07 on the dev set.
However, itsperformance on the test sets is a bit lower than thatof BerkeleyAligner.As we expect, our two semi-supervised mod-els achieve highest scores on almost all the datasets, which are also higher than the commonlyused grow-diag-final-and symmetrization of IBMModel 4.
More specifically, Tri-Divide is thebest of all systems.
It gets a dev score of 27.04,which is comparable with the highest one (27.07).Tri-Divide also gets the highest BLEU scoreson Test05 and Test06 (20.96 and 20.79, respec-tively), which are nearly 1 point higher than allsub-models.
The other Tri-training model, Tri-Bootstrap, gets the highest score on Test08, whichis also significantly better than those sub-models.Despite the large improvement in F1 score, ourtwo Tri-training models only get slightly betterscore than the well-known Model4GDF.
This kindof inconsistence between AER or F1 scores andBLEU scores is a known issue in machine trans-lation community (Fraser and Marcu, 2007).
Onepossible explanation is that both AER or F1 are0-1 loss functions, which means missing one linkand adding one redundant link will get the samepenalty.
And more importantly, every wrong linkreceives the same penalty under these metrics.However, these different errors may have differenteffects on the machine translation quality.
Thus,improving alignment quality according to AER orF1 may not directly lead to an increase of BLEUscores.
The relationship among these metrics arestill under investigation.5 Related workPrevious work mainly focuses on supervisedlearning of word alignment.
Liu et al (2005)propose a log-linear model for the alignment be-tween two sentences, in which different featurescan be used to describe the alignment quality.Moore (2005) proposes a similar framework, butwith more features and a different search method.Other models such as SVM and CRF are alsoused (Taskar et al, 2005; Cherry and Lin, 2006;Haghighi et al, 2009).
For alignment ensemble,Wu and Wang (2005) introduce a boosting ap-proach, in which the labeled data is used to cal-culate the weight of each sub-model.These researches all focus on the modeling ofalignment structure and employ some strategy tosearch for the optimal alignment.
Our main con-tribution here is the use Co-training style semi-supervised methods to assist the ensemble learn-ing framework of Ayan and Dorr (2006).
Althoughwe use a maximum entropy model in our experi-ment, other models like SVM and CRF can alsobe incorporated into our learning framework.In the area of semi-supervised learning of wordalignment, Callison-Burch et al (2004) comparethe results of interpolating statistical machine141translation models learnt from labeled and unla-beled data, respectively.
Wu et al (2006) proposea modified boosting algorithm, where two differ-ent models are also trained using labeled and un-labeled data respectively and interpolated.
Fraserand Marcu (2006) propose an EMD algorithm,where labeled data is used for discriminative re-ranking.
It should be pointed out that these piecesof work all use two separate processes for learn-ing with labeled and unlabeled data.
They eithertrain and interpolate two separate models or re-rank previously learnt models with labeled dataonly.
Our proposed semi-supervised strategy isable to incorporate both labeled and unlabeled datain the same process, which is in a different line ofthinking.6 Conclusions and Future WorkSemi-supervised techniques are useful when thereis a large amount of unlabeled data.
In thispaper, we introduce a semi-supervised learningmethod, called Tri-training, to improve the wordalignment combination task.
Although experi-ments have proved the effectiveness of our meth-ods, there is one defect that should be mentioned.As we previously assume that all the decisionson alignment links are independent of each other(in Section 2.1), our model are only able to cap-ture link level evidence like bi-lexical features.Some global features, such as final word fertil-ity, cannot be integrated into the current frame-work.
In the future, we plan to apply our semi-supervised strategy in more complicated learningframeworks, which are able to capture those globalfeatures.Currently we use a random sampling to handlethe 8.8 million instances.
We will also explorebetter and more aggressive sampling techniques,which may lead to more stable training results andalso enable us to process larger corpus.AcknowledgmentsThe authors would like to thank Dr. Ming Li,Mr.
Junming Xu and the anonymous reviewers fortheir valuable comments.
This work is supportedby the National Fundamental Research Programof China(2010CB327903) and the Scientific Re-search Foundation of Graduate School of NanjingUniversity(2008CL08).ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
A max-imum entropy approach to combining word align-ments.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 96?103, Morristown,NJ, USA.
Association for Computational Linguis-tics.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the 11th Annual Conference on Com-putational Learning Theory, pages 92?100.
MorganKaufmann Publishers.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matic of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, David Talbot, and Miles Os-borne.
2004.
Statistical machine translation withword- and sentence-aligned parallel corpora.
In ACL?04: Proceedings of the 42nd Annual Meeting on As-sociation for Computational Linguistics, page 175,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Colin Cherry and Dekang Lin.
2006.
Soft syntacticconstraints for word alignment through discrimina-tive training.
In Proceedings of the COLING/ACLon Main conference poster sessions, pages 105?112,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Alexander Fraser and Daniel Marcu.
2006.
Semi-supervised training for statistical word alignment.In ACL-44: Proceedings of the 21st InternationalConference on Computational Linguistics and the44th annual meeting of the Association for Com-putational Linguistics, pages 769?776, Morristown,NJ, USA.
Association for Computational Linguis-tics.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine trans-lation.
Comput.
Linguist., 33(3):293?303.Aria Haghighi, John Blitzer, and Dan Klein.
2009.Better word alignments with supervised itg models.In Association for Computational Linguistics, Sin-gapore.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL.Percy Liang, Benjamin Taskar, and Dan Klein.
2006.Alignment by agreement.
In Robert C. Moore,Jeff A. Bilmes, Jennifer Chu-Carroll, and MarkSanderson, editors, HLT-NAACL.
The Associationfor Computational Linguistics.142Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In ACL ?05:Proceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, pages 459?466, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Robert C. Moore.
2005.
A discriminative frameworkfor bilingual word alignment.
In HLT ?05: Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 81?88, Morristown, NJ, USA.Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51.A.
Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing, page901 904.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A discriminative matching approach to wordalignment.
In HLT ?05: Proceedings of the confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages73?80, Morristown, NJ, USA.
Association for Com-putational Linguistics.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1996.
Hmm-based word alignment in sta-tistical translation.
In Proceedings of the 16th Inter-national Conference on Computational Linguistics,pages 836?841.Hua Wu and Haifeng Wang.
2005.
Boosting statisticalword alignment.
In Proceedings of MT SUMMIT X,pages 364?371, Phuket Island, Thailand, September.HuaWu, HaifengWang, and Zhanyi Liu.
2006.
Boost-ing statistical word alignment using labeled and un-labeled data.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions, pages 913?920, Sydney, Australia, July.
Association for Com-putational Linguistics.Zhi-Hua Zhou and Ming Li.
2005.
Tri-training: Ex-ploiting unlabeled data using three classifiers.
vol-ume 17, pages 1529?1541, Piscataway, NJ, USA.IEEE Educational Activities Department.143
