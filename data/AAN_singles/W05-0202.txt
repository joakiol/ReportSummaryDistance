Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 9?16, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Automatic Short Answer MarkingStephen G. Pulman                  Jana Z. SukkariehComputational Linguistics Group, Computational Linguistics Group,University of Oxford.
University of Oxford.Centre for Linguistics and Philology, Centre for Linguistics and Philology,Walton St., Oxford, OX1 2HG, UK Walton St., Oxford,  OX1 2HG, UKsgp@clg.ox.ac.uk  Jana.Sukkarieh@clg.ox.ac.ukAbstractOur aim is to investigate computational lin-guistics (CL) techniques in marking short freetext responses automatically.
Successful auto-matic marking of free text answers would seemto presuppose an advanced level of perform-ance in automated natural language under-standing.
However, recent advances in CLtechniques have opened up the possibility ofbeing able to automate the marking of free textresponses typed into a computer without hav-ing to create systems that fully understand theanswers.
This paper describes  some of thetechniques we have tried so far vis-?-vis thisproblem with results, discussion and descrip-tion of the main issues encountered.11.
IntroductionOur aim is to investigate computational linguisticstechniques in marking short free text responsesautomatically.
The free text responses we are deal-ing with are answers ranging from a few words upto 5 lines.
These answers are for factual sciencequestions that typically ask candidates to state, de-scribe, suggest, explain, etc.
and where there is anobjective criterion for right and wrong.
Thesequestions are from an exam known as GCSE (Gen-eral Certificate of Secondary Education): most 161This is a 3-year project funded by the University of Cam-bridge Local Examinations Syndicate.year old students take up to 10 of these in differentsubjects in the UK school system.2.
The DataConsider the following GCSE biology question:Statement of thequestionThe blood vesselshelp to maintainnormal body tem-perature.
Explainhow the blood ves-sels reduce heatloss if the bodytemperature fallsbelow normal.Marking Scheme (full mark 3)2any three:vasoconstriction; explanation (ofvasoconstriction); less bloodflows to / through the skin / closeto the surface; less heat loss toair/surrounding/from the blood /less radiation / conduction / con-vection;Here is a sample of real answers:1. all the blood move faster and dose not go near thetop of your skin they stay close to the moses2.
The blood vessels stops a large ammount of bloodgoing to the blood capillary and sweat gland.This prents the presonne from sweating and loos-ing heat.3.
When the body falls below normal the blood ves-sels 'vasoconstrict' where the blood supply to theskin is cut off, increasing the metabolism of the2X;Y/D/K;V is equivalent to saying that each of X, [L]={Y,D,K}, and V deserves 1 mark.
The student has to write only 2of these to get the full mark.
[L] denotes an equivalence classi.e.
Y, D, K are equivalent.
If the student writes Y and D s/hewill get only 1 mark.9body.
This prevents heat loss through the skin,and causes the body to shake to increase metabo-lism.It will be obvious that many answers are ungram-matical with many spelling mistakes, even if theycontain more or less the right content.
Thus usingstandard syntactic and semantic analysis methodswill be difficult.
Furthermore, even if we had fullyaccurate syntactic and semantic processing, manycases require a degree of inference that is beyondthe state of the art, in at least the following re-spects:?
The need for reasoning and making infer-ences:  a student may answer with we do nothave to wait until Spring,which only impliesthe marking key it can be done at any time.Similarly, an answer such as don?t have spermor egg will get a 0 incorrectly if there is nomechanism to infer no fertilisation.?
Students tend to use a negation of a negation(for an affirmative):  An answer like won?t bedone only at a specific time is the equivalent towill be done at any time.
An answer like it isnot formed from more than one egg and spermis the same as saying formed from one egg andsperm.
This category is merely an instance ofthe need for more general reasoning and infer-ence outlined above.
We have given this casea separate category because here, the wordingof the answer is not very different, while in thegeneral case, the wording can be completelydifferent.?
Contradictory or inconsistent information:Other than logical contradiction like needs fer-tilisation and does not need fertilisation, an an-swer such as identical twins have the samechromosomes but different DNA holds incon-sistent scientific information that needs to bedetected.Since we were sceptical that existing deep process-ing NL systems would succeed with our data,we chose to adopt a shallow processing approach,trading robustness for complete accuracy.
Afterlooking carefully at the data we also discoveredother issues which will affect assessment of  theaccuracy of any automated system, namely:?
Unconventional expression for scientificknowledge: Examiners sometimes accept un-conventional or informal ways of expressingscientific knowledge, for example, ?sperm andegg get together?
for ?fertilisation?.?
Inconsistency across answers: In some cases,there is inconsistency in marking across an-swers.
Examiners sometimes make mistakesunder pressure.
Some biological information isconsidered relevant in some answers and ir-relevant in others.In the following, we describe various implementedsystems and report on their accuracy.We  conclude with some current work and suggesta road map.3.
Information Extraction for Short An-swersIn our initial experiments, we adopted an Informa-tion Extraction approach (see also Mitchell et al2003).
We used an existing Hidden Markov Modelpart-of-speech (HMM POS) tagger trained on thePenn Treebank corpus, and a Noun Phrase (NP)and Verb Group (VG) finite state machine (FSM)chunker.
The NP network was induced from thePenn Treebank, and then tuned by hand.
The VerbGroup FSM (i.e.
the Hallidayean constituent con-sisting of the verbal cluster without its comple-ments) was written by hand.
Relevant missingvocabulary was added to the tagger from thetagged British National Corpus (after mappingfrom their tag set to ours), and from examples en-countered in our training data.
The tagger also in-cludes some suffix-based heuristics for guessingtags for unknown words.In real information extraction, template mergingand reference resolution are important components.Our answers display little redundancy, and aretypically less than 5 lines long, and so templatemerging is not necessary.
Anaphors do not occurvery frequently, and when they do, they often referback to entities introduced in the text of  the ques-tion (to which the system does not have access).
Soat the cost of missing some correct answers, theinformation extraction components really consistsof little more than a set of patterns applied to thetagged and chunked text.We wrote our initial patterns by hand, although weare currently working on the development of a toolto take most of the tedious effort out of this task.We base the patterns on recurring head words orphrases, with syntactic annotation where neces-10sary,  in the training data.
Consider the followingexample training answers:the egg after fertilisationsplits in twothe fertilised egg has di-vided into twoThe egg was fertilised itsplit in twoOne fertilised egg splitsinto twoone egg fertilised whichsplit into two1 sperm has fertilized anegg.. that split into twoThese are all paraphrases of It is the same fertilisedegg/embryo, and variants of what is written abovecould be captured by a pattern like:singular_det + <fertilised egg> +{<split>; <divide>;<break>} + {in, into} + <two_halves>, where<fertilised egg>  = NP with the content of ?fertilisedegg?singular_det       = {the, one, 1, a, an}<split>               = {split, splits, splitting, has split, etc.
}<divide>            = {divides, which divide, has gone,being broken...}<two_halves>    = {two, 2, half, halves}etc.The pattern basically is all the paraphrases col-lapsed into one.
It is essential that the patterns usethe linguistic knowledge we have at the moment,namely, the part-of-speech tags, the noun phrasesand verb groups.
In our previous example, the re-quirement that <fertilised egg>  is an NP will ex-clude something like ?one sperm has fertilized anegg?
while accept something like ?an egg which isfertilized ...?.System Architecture:?When the caterpillars are feeding on the tomato plants, a chemical isreleased from the plants?.When/WRB [the/DT caterpillars/NNS]/NP[are/VBP feed-ing/VBG]/VG on/IN [the/DT tomato/JJ plants/NNS] /NP,/,  [a/DTchemical/NN]/NP[is/VBZ released/VBN]/VG from/IN [the/DT plants/NNS]/NP./.Table 1 gives results for the current version of thesystem.
For each of 9 questions, the patterns weredeveloped using a training set of about 200marked answers, and tested on 60 which werenot released to us until the patterns had been writ-ten.
Note that the full mark for each questionranges between 1-4.Question Full Mark %  ExaminerAgreement%  Mark SchemeAgreement1 2 89.4   93.82 2 91.8 96.53 2 84 94.24 1 91.3 94.25 2 76.4 93.46 3 75 87.87 1 95.6 97.58 4 75.3 86.19 2 86.6 92Average ---- 84 93Table 1.
Results for the manually-written IE approach.Column 3 records the percentage agreement be-tween our system and the marks assigned by a hu-man examiner.
As noted earlier, we detected acertain amount of inconsistency with the markingscheme in the grades actually awarded.
Column 4reflects the degree of agreement between thegrades awarded by our system and those whichwould have been awarded by following the mark-ing scheme consistently.
Notice that agreement iscorrelated with the mark scale: the system appearsless accurate on multi-part questions.
We adoptedan extremely strict measure, requiring an exactmatch.
Moving to a pass-fail criterion producesmuch higher agreement for questions 6 and 8.4.
Machine LearningOf course, writing patterns by hand  requires ex-pertise both in the domain of the examination, andin computational linguistics.
This requirementmakes the commercial deployment of a system likethis problematic, unless specialist staff are takenon.
We have therefore been experimenting withways in which a short answer marking systemmight be developed rapidly using machine learningmethods on a training set of marked answers.Previously (Sukkarieh et al 2003) we reported theresults we obtained using  a simple  NearestHMM Pos TaggerNP & VG Chunker SpecializedlexiconPattern MatcherScore and JustificationMarkerGenerallexiconPatternsGrammar11Neighbour Classification techniques.
In the follow-ing, we report our results using three different ma-chine learning methods: Inductive Logicprogamming (ILP), decision tree learning(DTL)and Naive Bayesian learning (Nbayes).
ILP(Progol, Muggleton 1995)  was chosen as a repre-sentative symbolic learning method.
DTL andNBayes were chosen following the Weka (Wittenand Frank, 2000) injunction to `try the simplethings first?.
With ILP, only 4 out of the 9 ques-tions shown in the previous section were tested,due to resource limitations.
With DTL and Nbayes,we conducted two experiments on all 9 questions.The first experiments show the results with non-annotated data; we then repeat the experimentswith annotated data.
Annotation in this context is alightweight activity, simply consisting of a domainexpert highlighting the part of the answer that de-serves a mark.
Our idea was to make this as simplea process as possible, requiring minimal software,and being exactly analogous to what some markersdo with pencil and paper.
As it transpired, this wasnot always straightforward, and does not mean thatthe training data is noiseless since sometimes an-notating the data accurately requires non-adjacentcomponents to be linked: we could not take ac-count of this.4.1 Inductive Logic ProgrammingFor our problem, for every question, the set oftraining data consists of students?
answers, to thatquestion, in a Prologised version of their textualform, with no syntactic analysis at all initially.
Wesupplied some `background knowledge?
predicatesbased on the work of  (Junker et al 1999).
Insteadof using their 3 Prolog basic predicates, however,we only defined 2, namely, word-pos(Text,Word,Pos) which represents words andtheir position in the text and window(Pos2-Pos1,Word1,Word2) which represents two wordsoccurring within a Pos2-Pos1 window distance.After some initial experiments, we believed that astemmed and tagged training data should give bet-ter results and that window should be made inde-pendent to occur in the logic rules learned byProgol.
We used our POS tagger mentioned aboveand the Porter stemmer (Porter 1980).
We set theProgol noise parameter to 10%, i.e.
the rules do nothave to fit the training data perfectly.
They can bemore general.
The percentages of agreement areshown in table 23.
The results reported are on a 5-fold cross validation testing and the agreement ison whether an answer is marked 0 or a mark >0,i.e.
pass-fail, against the human examiner scores.The baseline is the number of answers with themost common mark multiplied by 100 over thetotal number of answers.Question Baseline % of agreement6 51,53 74,877 73,63 90,508 57,73 74,309 70,97 65,77Average 71,15 77,73Table 2.
Results using ILP.The results of the experiment are not very promis-ing.
It seems very hard to learn the rules with ILP.Most rules state that an answer is correct if it con-tains a certain word, or two certain words within apredefined distance.
A question such as 7, though,scores reasonably well.
This is because Progollearns a rule such as mark(Answer) only if word-pos(Answer,?shiver?, Pos) which is, according toits marking scheme, all it takes to get its full mark,1.
ILP has in effect found the single keyword thatthe examiners were looking for.Recall that we only have ~200 answers for train-ing.
By training on a larger set, the learning algo-rithm may be able to find more structure in theanswers and may come up with better results.However, the rules learned may still be basic since,with the background knowledge we have suppliedthe ILP learner always tries to find simple andsmall predicates over (stems of) keywords.4.2 Decision Tree Learning and BayesianLearningIn our marking problem, seen as a machine learn-ing problem, the outcome or target attribute iswell-defined.
It is the mark for each question andits values are {0,1, ?, full_mark}.
The input at-tributes could vary from considering each word tobe an attribute or considering deeper linguistic fea-tures like a head of a noun phrase or a verb groupto be an attribute, etc.
In the following experi-ments, each word in the answer was considered tobe an attribute.
Furthermore, Rennie et al (2003)3Our thanks to our internship student, Leonie IJzereef for theresults in table 2.12propose simple heuristic solutions to some prob-lems with na?ve classifiers.
In Weka, Complementof Na?ve Bayes (CNBayes) is a refinement to theselection process that Na?ve Bayes makes whenfaced with instances where one outcome value hasmore training data than another.
This is true in ourcase.
Hence, we ran our experiments using thisalgorithm also to see if there were any differences.The results reported are on a 10-fold cross valida-tion testing.4.2.1 Results on Non-Annotated dataWe first considered the non-annotated data, that is,the answers given by students in their raw form.The first experiment considered the values of themarks to be {0,1, ?, full_mark} for each question.The results of decision tree learning and Bayesianlearning are reported in the columns titled DTL1and NBayes/CNBayes1.
The second experimentconsidered the values of the marks to be either 0 or>0, i.e.
we considered two values only, pass andfail.
The results are reported in columns DTL2 andNBayes2/CNBayes2.
The baseline is calculated thesame way as in the ILP case.
Obviously, the resultof the baseline differs in each experiment onlywhen the sum of the answers with marks greaterthan 0 exceeds that of those with mark 0.
This af-fected questions 8 and 9 in Table 3 below.
Hence,we took the average of both results.
It was no sur-prise that the results of the second experiment werebetter than the first on questions with the full mark>1, since the number of target features is smaller.In both experiments, the complement of Na?veBayes did slightly better or equally well on ques-tions with a full mark of 1, like questions 4 and 7in the table, while it resulted in a worse perform-ance on questions with full marks >1.Ques.
Base-lineDTL1 N/CNBayes1 N/CNBayes2 DTL21 69 73.52 73.52 / 66.47 81.17 / 73.52 76.472 54 62.01 65.92  /61.45 73.18/  68.15 62.563 46 68.68 72.52 / 61.53 93.95 / 92.85 93.44 58 69.71 75.42 /  76 75.42 / 76 69.715 54 60.81 66.66 / 53.21 73.09 / 73.09 67.256 51 47.95 59.18 / 52.04 81.63  /77.55 67.347 73 88.05 88.05 / 88.05 88.05 / 88.05 88.058 42   41.75 43.29 / 37.62 70.10/ 69.07 72.689 60  61.82 67.20 / 62.36 79.03 / 76.88 76.34Ave.
60.05 63.81 67.97/62.1 79.51/77.3 74.86Table 3.
Results for Bayesian learning and decision tree learningon non-annotated data.Since we were using the words as attributes, weexpected that in some cases stemming the words inthe answers would improve the results.
Hence, weexperimented with the answers of 6, 7, 8 and 9from the list above but there was only a tiny im-provement (in question 8).
Stemming does notnecessarily make a difference if the attrib-utes/words that make a difference appear in a rootform already.
The lack of any difference or worseperformance may also be due to the error rate inthe stemmer.4.2.2 Results on Annotated dataWe repeated the second experiments with the an-notated answers.
The baseline for the new data dif-fers and the results are shown in Table 4.Question Baseline DTL NBayes/CNBayes1 58 74.87 86.69  /  81.282 56 75.89 77.43   /  73.333 86 90.68 95.69   /  96.774 62 79.08 79.59   /  82.655 59 81.54 86.26   /  81.976 69 85.88 92.19   /  93.997 79 88.51 91.06   /  89.788 78 94.47 96.31   /   93.949 79 85.6 87.12   /   87.87Average 69.56 84.05  88.03  /  86.85Table 4.
Results for Bayesian learning and decision tree learningon annotated data.As we said earlier, annotation in this context sim-ply  means highlighting the part of the answer thatdeserves 1 mark (if the answer has >=1 mark), sofor e.g.
if an answer was given a 2 mark then atleast two pieces of information should be high-lighted and answers with 0 mark stay the same.Obviously, the first experiments could not be con-ducted since with the annotated answers the markis either 0 or 1.
Bayesian learning is doing betterthan DTL and 88% is a promising result.
Further-more, given the results of CNBayes in Table 3, weexpected that CNBayes would do better on ques-tions 4 and 7.
However, it actually did better onquestions 3, 4, 6 and 9.
Unfortunately, we cannotsee a pattern or a reason for this.5.
Comparison of ResultsIE did best on all the questions before annotatingthe data as it can be seen in Fig.
1.
Though, thetraining data for the machine learning algorithms is13tiny relative to what usually such algorithms con-sider, after annotating the data, the performance ofNBayes on questions 3, 6 and 8 were better thanIE.
This is seen in Fig.
2.
However, as we said ear-lier in section 2, the percentages shown for IEmethod are on the whole mark while the results ofDTL and Nbayes, after annotation,  are  calculatedon pass-fail.F ig.
1.
IE vs D T L & N bayes pre-anno tat io n0204060801001201 2 3 4 5 6 7 8 9Quest ionIEDTL1NBayes1DTL2NBayes2In addition, in the pre-annotation experiments re-ported in Fig.
1, the NBayes algorithm did betterthan that of DTL.
Post-annotation, results in Fig.
2show, again, that NBayes is doing better than theDTL algorithm.
It is worth noting that, in the anno-tated data, the number of answers whose marks are0 is less than in the answers whose mark is 1, ex-cept for questions 1 and 2.
This may have an effecton the results.Fig.2.
IE vs DTL & NBayes post-annotation0204060801001201 2 3 4 5 6 7 8 9Question%PerformanceIEDTLNBayesMoreover, after getting the worse performance inNBayes2 before annotation, question 8 jumps tobest performance.
The rest of the questions main-tained the same position more or less, with ques-tion 3 always coming nearest to the top (see Fig.3).
We noted that Count(Q,1)-Count(Q,0) is high-est for questions 8 and 3, where Count(Q,N) is, forquestion Q, the number of answers whose mark isN.
Also, the improvement of performance forquestion 8 in relation to Count(8,1) was not sur-prising, since question 8 has a full-mark of 4 andthe annotation?s role was an attempt at a one-to-one correspondence between an answer and 1mark.Fig.
3.
NBayes before and after annotation0204060801001201 2 3 4 5 6 7 8 9Question%PerformanceNbayes1_beforeNbayes2_beforeNbayes_afterOn the other hand, question 1 that was in seventhplace in DTL2 before annotation, jumps down tothe worst place after annotation.
In both cases,namely, NBayes2 and DTL2 after annotation, itseems reasonable to hypothesize that P(Q1) is bet-ter than P(Q2) if Count(Q1,1)-Count(Q1,0) >>Count(Q2,1)-Count(Q2,0), where P(Q) is the per-centage of agreement for question Q.As they stand, the results of agreement with givenmarks are encouraging.
However, the models thatthe algorithms are learning are very na?ve in thesense that they depend on words only.
Unlike theIE approach, it would not be possible to provide areasoned justification for a student as to why theyhave got the mark they have.
One of the advan-tages to the pattern-matching approach is that it isvery easy, knowing which patterns have matched,to provide some simple automatic feed-back to thestudent as to which components of the answer wereresponsible for the mark awarded.We began experimenting with machine learningmethods in order to try to overcome the IE cus-tomisation bottleneck.
However, our experience sofar has been that in short answer marking (as op-posed to essay marking) these methods are, whilepromising, not accurate enough at present to be areal alternative to the hand-crafted, pattern-14matching approach.
We should instead think ofthem either as aids to the pattern writing process ?for example, frequently the decision trees that arelearned are quite intuitive, and suggestive of usefulpatterns ?
or perhaps as complementary supportingassessment techniques to give extra confirmation.6.
Other workSeveral other groups are working on this problem,and we have learned from all of them.
Systemswhich share properties with ours are C-Rater, de-veloped by Leacock et al (2003) at the Educa-tional Testing Service(ETS),  the IE-based systemof Mitchell et al (2003) at Intelligent AssessmentTechnologies, and Ros?
et al (2003) at CarnegieMellon University.
The four systems are being de-veloped independently, yet it seems they sharesimilar characteristics.
Commercial and resourcepressures currently make it impossible to try thesedifferent systems on the same data, and so per-formance comparisons are meaningless: this is areal hindrance to progress in this area.
The field ofautomatic marking really needs a MUC-style com-petition to be able to develop and assess these tech-niques and systems in a controlled and objectiveway.7.
Current and Future WorkThe manually-engineered IE approach requiresskill, much labour, and familiarity with both do-main and tools.
To save time and labour, variousresearchers have investigated machine-learningapproaches to learn IE patterns (Collins et al 1999,Riloff 1993).
We are currently investigating ma-chine learning algorithms to learn the patterns usedin IE (an initial skeleton-like algorithm can befound in Sukkarieh et al 2004).We are also in the process of evaluating our systemalong two dimensions: firstly, how long it takes,and how difficult it is, to customise to new ques-tions; and secondly, how easy it is for students touse this kind of system for formative assessment.In the first trial, a domain expert (someone otherthan us) is annotating some new training data forus.
Then we will measure how long it takes us (ascomputational linguists familiar with the system)to write IE patterns for this data, compared to thetime taken by a computer scientist who is familiarwith the domain and with general concepts of pat-tern matching but with no computational linguis-tics expertise.
We will also assess the performanceaccuracy of the resulting patterns.For the second evaluation, we have collaboratedwith UCLES to build a web-based demo whichwill be trialled during May and June 2005 in agroup of schools in the Cambridge (UK) area.
Stu-dents will be given access to the system as amethod of self-assessment.
Inputs and other as-pects of the transactions will be logged and used toimprove the IE pattern accuracy.
Students?
reac-tions to the usefulness of the tool will also be re-corded.
Ideally, we would go on to compare thefuture examination performance of students withand without access to the demo, but that is someway off at present.ReferencesCollins, M. and Singer, Y.
1999.
Unsupervised modelsfor named entity classification.
Proceedings JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing  and Very Large Corpora, pp.
189-196.Junker, M, M. Sintek & M. Rinck 1999.
Learning forText Categorization and Information Extraction withILP.
In: Proceedings of the 1st Workshop on LearningLanguage in Logic, Bled, Slovenia, 84-93.Leacock, C. and Chodorow, M. 2003.
C-rater: Auto-mated Scoring of Short-Answer Questions.
Computersand Humanities 37:4.Mitchell, T. Russell, T. Broomhead, P. and Aldridge, N.2003.
Computerized marking of short-answer free-textresponses.
Paper presented at the 29th annual confer-ence of the International Association for EducationalAssessment (IAEA), Manchester, UK.Muggleton, S. 1995.
Inverting Entailment and Progol.In: New Generation Computing, 13:245-286.Porter, M.F.
1980.
An algorithm for suffix stripping,Program, 14(3):130-137.Rennie, J.D.M., Shih, L., Teevan, J. and Karger, D.2003 Tackling the Poor Assumptions of Na?ve BayesTextClassifiers.http://haystack.lcs.mit.edu/papers/rennie.icml03.pdf.15Riloff, E. 1993.
Automatically constructing a dictionaryfor information extraction tasks.
Proceedings 11th Na-tional Conference on Artificial Intelligence, pp.
811-816.Ros?, C. P. Roque, A., Bhembe, D. and VanLehn, K.2003.
A hybrid text classification approach for analysisof student essays.
In Building Educational ApplicationsUsing Natural Language Processing, pp.
68-75.Sukkarieh, J.
Z., Pulman, S. G. and Raikes N. 2003.Auto-marking: using computational linguistics to scoreshort, free text responses.
Paper presented at the 29thannual conference of the International Association forEducational Assessment (IAEA), Manchester, UK.Sukkarieh, J.
Z., Pulman, S. G. and Raikes N. 2004.Auto-marking2: An update on the UCLES-OXFORDUniversity research into using computational linguisticsto score short, free text responses.
Paper presented atthe 30th annual conference of the International Associa-tion for Educational Assessment (IAEA), Philadelphia,USA.Witten, I. H. Eibe, F. 2000.
Data Mining.
AcademicPress.16
