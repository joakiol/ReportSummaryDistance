Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 230?235,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDiscrete vs.
Continuous Rating Scales for Language Evaluation in NLPAnja Belz Eric KowSchool of Computing, Engineering and MathematicsUniversity of BrightonBrighton BN2 4GJ, UK{A.S.Belz,E.Y.Kow}@brighton.ac.ukAbstractStudies assessing rating scales are very com-mon in psychology and related fields, butare rare in NLP.
In this paper we as-sess discrete and continuous scales used formeasuring quality assessments of computer-generated language.
We conducted six sep-arate experiments designed to investigate thevalidity, reliability, stability, interchangeabil-ity and sensitivity of discrete vs. continuousscales.
We show that continuous scales are vi-able for use in language evaluation, and offerdistinct advantages over discrete scales.1 Background and IntroductionRating scales have been used for measuring hu-man perception of various stimuli for a long time,at least since the early 20th century (Freyd, 1923).First used in psychology and psychophysics, theyare now also common in a variety of other disci-plines, including NLP.
Discrete scales are the onlytype of scale commonly used for qualitative assess-ments of computer-generated language in NLP (e.g.in the DUC/TAC evaluation competitions).
Contin-uous scales are commonly used in psychology andrelated fields, but are virtually unknown in NLP.While studies assessing the quality of individualscales and comparing different types of rating scalesare common in psychology and related fields, suchstudies hardly exist in NLP, and so at present little isknown about whether discrete scales are a suitablerating tool for NLP evaluation tasks, or whether con-tinuous scales might provide a better alternative.A range of studies from sociology, psychophys-iology, biometrics and other fields have compareddiscrete and continuous scales.
Results tend to dif-fer for different types of data.
E.g., results from painmeasurement show a continuous scale to outperforma discrete scale (ten Klooster et al, 2006).
Otherresults (Svensson, 2000) from measuring students?ease of following lectures show a discrete scale tooutperform a continuous scale.
When measuringdyspnea, Lansing et al (2003) found a hybrid scaleto perform on a par with a discrete scale.Another consideration is the types of data pro-duced by discrete and continuous scales.
Parametricmethods of statistical analysis, which are far moresensitive than non-parametric ones, are commonlyapplied to both discrete and continuous data.
How-ever, parametric methods make very strong assump-tions about data, including that it is numerical andnormally distributed (Siegel, 1957).
If these as-sumptions are violated, then the significance of re-sults is overestimated.
Clearly, the numerical as-sumption does not hold for the categorial data pro-duced by discrete scales, and it is unlikely to be nor-mally distributed.
Many researchers are happier toapply parametric methods to data from continuousscales, and some simply take it as read that such datais normally distributed (Lansing et al, 2003).Our aim in the present study was to system-atically assess and compare discrete and continu-ous scales when used for the qualitative assess-ment of computer-generated language.
We start withan overview of assessment scale types (Section 2).We describe the experiments we conducted (Sec-tion 4), the data we used in them (Section 3), andthe properties we examined in our inter-scale com-parisons (Section 5), before presenting our results230Q1: Grammaticality The summary should have no date-lines, system-internal formatting, capitalization errors or obvi-ously ungrammatical sentences (e.g., fragments, missing com-ponents) that make the text difficult to read.1.
Very Poor2.
Poor3.
Barely Acceptable4.
Good5.
Very GoodFigure 1: Evaluation of Readability in DUC?06, compris-ing 5 evaluation criteria, including Grammaticality.
Eval-uation task for each summary text: evaluator selects oneof the options (1?5) to represent quality of the summaryin terms of the criterion.
(Section 6), and some conclusions (Section 7).2 Rating ScalesWith Verbal Descriptor Scales (VDSs), partici-pants give responses on ordered lists of verbally de-scribed and/or numerically labelled response cate-gories, typically varying in number from 2 to 11(Svensson, 2000).
An example of a VDS used in NLPis shown in Figure 1.
VDSs are used very widely incontexts where computationally generated languageis evaluated, including in dialogue, summarisation,MT and data-to-text generation.Visual analogue scales (VASs) are far less com-mon outside psychology and related areas thanVDSs.
Responses are given by selecting a point ona typically horizontal line (although vertical lineshave also been used (Scott and Huskisson, 2003)),on which the two end points represent the extremevalues of the variable to be measured.
Such linescan be mono-polar or bi-polar, and the end pointsare labelled with an image (smiling/frowning face),or a brief verbal descriptor, to indicate which endof the line corresponds to which extreme of the vari-able.
The labels are commonly chosen to represent apoint beyond any response actually likely to be cho-sen by raters.
There is only one examples of a VASin NLP system evaluation that we are aware of (Gattet al, 2009).Hybrid scales, known as a graphic rating scales,combine the features of VDSs and VASs, and are alsoused in psychology.
Here, the verbal descriptors arealigned along the line of a VAS and the endpoints aretypically unmarked (Svensson, 2000).
We are awareof one example in NLP (Williams and Reiter, 2008);Q1: Grammaticality The summary should have no datelines,system-internal formatting, capitalization errors or obviouslyungrammatical sentences (e.g., fragments, missing compo-nents) that make the text difficult to read.extremelyexcellentbadFigure 2: Evaluation of Grammaticality with alternativeVAS scale (cf.
Figure 1).
Evaluation task for each sum-mary text: evaluator selects a place on the line to repre-sent quality of the summary in terms of the criterion.we did not investigate this scale in our study.We used the following two specific scale designsin our experiments:VDS-7: 7 response categories, numbered (7 =best) and verbally described (e.g.
7 = ?perfectly flu-ent?
for Fluency, and 7 = ?perfectly clear?
for Clar-ity).
Response categories were presented in a verti-cal list, with the best category at the bottom.
Eachcategory had a tick-box placed next to it; the rater?stask was to tick the box by their chosen rating.VAS: a horizontal, bi-polar line, with no ticks onit, mapping to 0?100.
In the image description tests,statements identified the left end as negative, theright end as positive; in the weather forecast tests,the positive end had a smiling face and the label?statement couldn?t be clearer/read better?
; the neg-ative end had a frowning face and the label ?state-ment couldn?t be more unclear/read worse?.
Theraters?
task was to move a pointer (initially in themiddle of the line) to the place corresponding totheir rating.3 DataWeather forecast texts: In one half of our evalua-tion experiments we used human-written and auto-matically generated weather forecasts for the sameweather data.
The data in our evaluations was for 22different forecast dates and included outputs from 10generator systems and one set of human forecasts.This data has also been used for comparative sys-tem evaluation in previous research (Langner, 2010;Angeli et al, 2010; Belz and Kow, 2009).
The fol-lowing are examples of weather forecast texts fromthe data:1: SSE 28-32 INCREASING 36-40 BY MID AF-TERNOON2: S?LY 26-32 BACKING SSE 30-35 BY AFTER-231NOON INCREASING 35-40 GUSTS 50 BY MIDEVENINGImage descriptions: In the other half of our eval-uations, we used human-written and automaticallygenerated image descriptions for the same images.The data in our evaluations was for 112 differentimage sets and included outputs from 6 generatorsystems and 2 sets of human-authored descriptions.This data was originally created in the TUNA Project(van Deemter et al, 2006).
The following is an ex-ample of an item from the corpus, consisting of a setof images and a description for the entity in the redframe:the small blue fan4 Experimental Set-up4.1 Evaluation criteriaFluency/Readability: Both the weather forecast andimage description evaluation experiments used aquality criterion intended to capture ?how well apiece of text reads?, called Fluency in the latter,Readability in the former.Adequacy/Clarity: In the image description ex-periments, the second quality criterion was Ade-quacy, explained as ?how clear the description is?,and ?how easy it would be to identify the image fromthe description?.
This criterion was called Clarity inthe weather forecast experiments, explained as ?howeasy is it to understand what is being described?.4.2 RatersIn the image experiments we used 8 raters (nativespeakers) in each experiment, from cohorts of 3rd-year undergraduate and postgraduate students doinga degree in a linguistics-related subject.
They werepaid and spent about 1 hour doing the experiment.In the weather forecast experiments, we used 22raters in each experiment, from among academicstaff at our own university.
They were not paid andspent about 15 minutes doing the experiment.4.3 Summary overview of experimentsWeather VDS-7 (A): VDS-7 scale; weather forecastdata; criteria: Readability and Clarity; 22 raters (uni-versity staff) each assessing 22 forecasts.Weather VDS-7 (B): exact repeat of WeatherVDS-7 (A), including same raters.Weather VAS: VAS scale; 22 raters (universitystaff), no overlap with raters in Weather VDS-7 ex-periments; other details same as in Weather VDS-7.Image VDS-7: VDS-7 scale; image descriptiondata; 8 raters (linguistics students) each rating 112descriptions; criteria: Fluency and Adequacy.Image VAS (A): VAS scale; 8 raters (linguisticsstudents), no overlap with raters in Image VAS-7;other details same as in Image VDS-7 experiment.Image VAS (B): exact repeat of Image VAS (A),including same raters.4.4 Design features common to all experimentsIn all our experiments we used a Repeated LatinSquares design to ensure that each rater sees thesame number of outputs from each system and foreach text type (forecast date/image set).
Followingdetailed instructions, raters first did a small numberof practice examples, followed by the texts to berated, in an order randomised for each rater.
Eval-uations were carried out via a web interface.
Theywere allowed to interrupt the experiment, and in thecase of the 1 hour long image description evaluationthey were encouraged to take breaks.5 Comparison and Assessment of ScalesValidity is to the extent to which an assessmentmethod measures what it is intended to measure(Svensson, 2000).
Validity is often impossible to as-sess objectively, as is the case of all our criteria ex-cept Adequacy, the validity of which we can directlytest by looking at correlations with the accuracy withwhich participants in a separate experiment identifythe intended images given their descriptions.A standard method for assessing Reliability isKendall?s W, a coefficient of concordance, measur-ing the degree to which different raters agree in theirratings.
We report W for all 6 experiments.Stability refers to the extent to which the resultsof an experiment run on one occasion agree withthe results of the same experiment (with the same232raters) run on a different occasion.
In the presentstudy, we assess stability in an intra-rater, test-retestdesign, assessing the agreement between the sameparticipant?s responses in the first and second runsof the test with Pearson?s product-moment correla-tion coefficient.
We report these measures betweenratings given in Image VAS (A) vs. those given in Im-age VAS (B), and between ratings given in WeatherVDS-7 (A) vs. those given in Weather VDS-7 (B).We assess Interchangeability, that is, the extentto which our VDS and VAS scales agree, by comput-ing Pearson?s and Spearman?s coefficients betweenresults.
We report these measures for all pairs ofweather forecast/image description evaluations.We assess the Sensitivity of our scales by de-termining the number of significant differences be-tween different systems and human authors detectedby each scale.We also look at the relative effect of the differ-ent experimental factors by computing the F-Ratiofor System (the main factor under investigation, soits relative effect should be high), Rater and TextType (their effect should be low).
F-ratios were de-termined by a one-way ANOVA with the evaluationcriterion in question as the dependent variable andSystem, Rater or Text Type as grouping factors.6 Results6.1 Interchangeability and Reliability forsystem/human authored image descriptionsInterchangeability: Pearson?s r between the meansper system/human in the three image descriptionevaluation experiments were as follows (Spearman?s?
shown in brackets):Adeq.
VAS (A) VAS (B)VDS-7 .957**(.958**) .819* (.755*)VAS (A) ?
.874** (.810*)Flue.
VDS-7 .948**(.922**) .864** (.850**)VAS (A) ?
.937** (.929**)For both Adequacy and Fluency, correlations be-tween Image VDS-7 and Image VAS (A) (the mainVAS experiment) are extremely high, meaning thatthey could substitute for each other here.Reliability: Inter-rater agreement in terms ofKendall?s W in each of the experiments:VDS-7 VAS (A) VAS (B)K?s W Adequacy .598** .471** .595*K?s W Fluency .640** .676** .729**W was higher in the VAS data in the case of Fluency,whereas for Adequacy, W was the same for the VDSdata and VAS (B), and higher in the VDS data thanin the VAS (A) data.6.2 Interchangeability and Reliability forsystem/human authored weather forecastsInterchangeability: The correlation coefficients(Pearson?s r with Spearman?s ?
in brackets) betweenthe means per system/human in the image descrip-tion experiments were as follows:Clar.
VDS-7 (B) VASVDS-7 (A) .995** (.989**) .942** (.832**)VDS-7 (B) ?
.939**( .836**)Read.
VDS-7 (A) .981** (.870**) .947** (.709*)VDS-7 (B) ?
.951** (.656*)For both Adequacy and Fluency, correlations be-tween Weather VDS-7 (A) (the main VDS-7 experi-ment) and Weather VAS (A) are again very high, al-though rank-correlation is somewhat lower.Reliability: Inter-rater agreement in terms ofKendall?s W was as follows:VDS-7 (A) VDS-7 (B) VASW Clarity .497** .453** .485**W Read.
.533** .488** .480**This time the highest agreement for both Clarity andReadability was in the VDS-7 data.6.3 Stability tests for image and weather dataPearson?s r between ratings given by the same ratersfirst in Image VAS (A) and then in Image VAS (B)was .666 for Adequacy, .593 for Fluency.
Betweenratings given by the same raters first in WeatherVDS-7 (A) and then in Weather VDS-7 (B), Pearson?sr was .656 for Clarity, .704 for Readability.
(All sig-nificant at p < .01.)
Note that these are computedon individual scores (rather than means as in the cor-relation figures given in previous sections).6.4 F-ratios and post-hoc analysis for imagedataThe table below shows F-ratios determined by a one-way ANOVA with the evaluation criterion in question(Adequacy/Fluency) as the dependent variable andSystem/Rater/Text Type as the grouping factor.
Note233that for System a high F-ratio is desirable, but a lowF-ratio is desirable for other factors.Image descriptionsVDS-7 VAS (A)AdequacySystem 8.822** 6.371**Rater 12.623** 13.136**Text Type 1.193 1.519**FluencySystem 13.312** 17.207**Rater 27.401** 17.479**Text Type .894 1.091Out of a possible 28 significant differences for Sys-tem, the main factor under investigation, VDS-7found 8 for Adequacy and 14 for Fluency; VAS (A)found 7 for Adequacy and 15 for Fluency.6.5 F-ratios and post-hoc analysis for weatherdataThe table below shows F-ratios analogous to the pre-vious section (for Clarity/Readability).Weather forecastsVDS-7 (A) VASClaritySystem 23.507** 23.468**Rater 4.832** 6.857**Text Type 1.467 1.632*Read.System 24.351** 22.538**Rater 4.824** 5.560**Text Type 1.961** 2.906**Out of a possible 55 significant differences for Sys-tem, VDS-7 (A) found 24 for Clarity, 23 for Read-ability; VAS found 25 for Adequacy, 26 for Fluency.6.6 Scale validity test for image dataOur final table of results shows Pearson?s correla-tion coefficients (calculated on means per system)between the Adequacy data from the three imagedescription evaluation experiments on the one hand,and the data from an extrinsic experiment in whichwe measured the accuracy with which participantsidentified the intended image described by a descrip-tion:ID Acc.Image VAS (A) Adequacy .870**Image VAS (B) Adequacy .927**Image VDS-7 Adequacy .906**The correlation between Adequacy and ID Accuracywas strong and highly significant in all three imagedescription evaluation experiments, but strongest inVAS (B), and weakest in VAS (A).
For comparison,Pearson?s between Fluency and ID Accuracy rangedbetween .3 and .5, whereas Pearson?s between Ade-quacy and ID Speed (also measured in the same im-age identfication experiment) ranged between -.35and -.29.7 Discussion and ConclusionsOur interchangeability results (Sections 6.1 and 6.2)indicate that the VAS and VDS-7 scales we havetested can substitute for each other in our presentevaluation tasks in terms of the mean system scoresthey produce.
Where we were able to measure va-lidity (Section 6.6), both scales were shown to besimilarly valid, predicting image identification ac-curacy figures from a separate experiment equallywell.
Stability (Section 6.3) was marginally betterfor VDS-7 data, and Reliability (Sections 6.1 and6.2) was better for VAS data in the image descrip-tion evaluations, but (mostly) better for VDS-7 datain the weather forecast evaluations.
Finally, the VASexperiments found greater numbers of statisticallysignificant differences between systems in 3 out of 4cases (Section 6.5).Our own raters strongly prefer working with VASscales over VDSs.
This has also long been clear fromthe psychology literature (Svensson, 2000)), whereraters are typically found to prefer VAS scales overVDSs which can be a ?constant source of vexationto the conscientious rater when he finds his judg-ments falling between the defined points?
(Champ-ney, 1941).
Moreover, if a rater?s judgment falls be-tween two points on a VDS then they must make thefalse choice between the two points just above andjust below their actual judgment.
In this case weknow that the point they end up selecting is not anaccurate measure of their judgment but rather justone of two equally accurate ones (one of which goesunrecorded).Our results establish (for our evaluation tasks) thatVAS scales, so far unproven for use in NLP, are atleast as good as VDSs, currently virtually the onlyscale in use in NLP.
Combined with the fact thatraters strongly prefer VASs and that they are regardedas more amenable to parametric means of statisti-cal analysis, this indicates that VAS scales should beused more widely for NLP evaluation tasks.234ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approach togeneration.
In Proceedings of the 15th Conference onEmpirical Methods in Natural Language Processing(EMNLP?10).Anja Belz and Eric Kow.
2009.
System building costvs.
output quality in data-to-text generation.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation, pages 16?24.H.
Champney.
1941.
The measurement of parent behav-ior.
Child Development, 12(2):131.M.
Freyd.
1923.
The graphic rating scale.
BiometricalJournal, 42:83?102.A.
Gatt, A. Belz, and E. Kow.
2009.
The TUNA Chal-lenge 2009: Overview and evaluation results.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation (ENLG?09), pages 198?206.Brian Langner.
2010.
Data-driven Natural LanguageGeneration: Making Machines Talk Like HumansUsing Natural Corpora.
Ph.D. thesis, LanguageTechnologies Institute, School of Computer Science,Carnegie Mellon University.Robert W. Lansing, Shakeeb H. Moosavi, and Robert B.Banzett.
2003.
Measurement of dyspnea: word la-beled visual analog scale vs. verbal ordinal scale.
Res-piratory Physiology & Neurobiology, 134(2):77 ?83.J.
Scott and E. C. Huskisson.
2003.
Vertical or hori-zontal visual analogue scales.
Annals of the rheumaticdiseases, (38):560.Sidney Siegel.
1957.
Non-parametric statistics.
TheAmerican Statistician, 11(3):13?19.Elisabeth Svensson.
2000.
Comparison of the qualityof assessments using continuous and discrete ordinalrating scales.
Biometrical Journal, 42(4):417?434.P.
M. ten Klooster, A. P. Klaar, E. Taal, R. E. Gheith,J.
J. Rasker, A. K. El-Garf, and M. A. van de Laar.2006.
The validity and reliability of the graphic ratingscale and verbal rating scale for measuing pain acrosscultures: A study in egyptian and dutch women withrheumatoid arthritis.
The Clinical Journal of Pain,22(9):827?30.Kees van Deemter, Ielka van der Sluis, and Albert Gatt.2006.
Building a semantically transparent corpus forthe generation of referring expressions.
In Proceed-ings of the 4th International Conference on NaturalLanguage Generation, pages 130?132, Sydney, Aus-tralia, July.S.
Williams and E. Reiter.
2008.
Generating basic skillsreports for low-skilled readers.
Natural Language En-gineering, 14(4):495?525.235
