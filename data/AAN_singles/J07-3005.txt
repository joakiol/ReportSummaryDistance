Classifying Non-Sentential Utterances inDialogue: A Machine Learning ApproachRaquel Ferna?ndez?Potsdam UniversityJonathan Ginzburg?
?King?s College LondonShalom Lappin?King?s College LondonIn this article we use well-known machine learning methods to tackle a novel task, namelythe classification of non-sentential utterances (NSUs) in dialogue.
We introduce a fine-grainedtaxonomy of NSU classes based on corpus work, and then report on the results of several machinelearning experiments.
First, we present a pilot study focused on one of the NSU classes in thetaxonomy?bare wh-phrases or ?sluices?
?and explore the task of disambiguating betweenthe different readings that sluices can convey.
We then extend the approach to classify thefull range of NSU classes, obtaining results of around an 87% weighted F-score.
Thus ourexperiments show that, for the taxonomy adopted, the task of identifying the right NSU classcan be successfully learned, and hence provide a very encouraging basis for the more generalenterprise of fully processing NSUs.1.
IntroductionNon-sentential utterances (NSUs)?fragmentary utterances that do not have the formof a full sentence according to most traditional grammars, but that nevertheless conveya complete clausal meaning?are a common phenomenon in spoken dialogue.
Thefollowing are two examples of NSUs taken from the dialogue transcripts of the BritishNational Corpus (BNC) (Burnard 2000):(1) a.
A: Who wants Beethoven music?B: Richard and James.
[BNC: KB8 1024?1025]1b.
A: It?s Ruth?s birthday.B: When?
[BNC: KBW 13116?13117]?
Karl-Liebknecht Strasse 24-25, 14476 Golm, Germany.
E-mail: raquel@ling.uni-potsdam.de.??
The Strand, London WC2R 2LS, UK.
E-mail: jonathan.ginzburg@kcl.ac.uk.?
The Strand, London WC2R 2LS, UK.
E-mail: shalom.lappin@kcl.ac.uk.1 This notation indicates the name of the file and the sentence numbers in the BNC.Submission received: 24 September 2004; revised submission received: 10 November 2006; accepted forpublication: 9 March 2007.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 3Arguably the most important issue in the processing of NSUs concerns their resolu-tion, that is, the recovery of a full clausal meaning from a form which is standardlyconsidered non-clausal.
In the first of the examples, the NSU in bold face is a typical?short answer,?
which despite having the form of a simple NP would most likely beunderstood as conveying the proposition Richard and James want Beethoven music.
TheNSU in (1b) is an example of what has been called a ?sluice.?
Again, despite beingrealized by a bare wh-phrase, the meaning conveyed by the NSU could be paraphrasedas the question When is Ruth?s birthday?Although short answers and short queries like those in (1) are perhaps two of themost prototypical NSU classes, recent corpus studies (Ferna?ndez and Ginzburg 2002;Schlangen 2003) show that other less well-known types of NSUs?each with its ownresolution constraints?are also pervasive in real conversations.
This variety of NSUclasses, together with their inherent concise form and their highly context-dependentmeaning, often make NSUs ambiguous.
Consider, for instance, example (2):(2) a.
A: I left it on the table.B: On the table.b.
A: Where did you leave it?B: On the table.c.
A: I think I put it er.
.
.B: On the table.d.
A: Should I put it back on the shelf?B: On the table.An NSU like B?s response in (2a) can be understood either as a clarification questionor as an acknowledgment, depending on whether it is uttered with raising intonationor not.
In (2b), on the other hand, the NSU is readily understood as a short answer,whereas in (2c) it fills a gap left by the previous utterance.
Yet in the context of (2d) itwill most probably be understood as a sort of correction or a ?helpful rejection,?
as weshall call this kind of NSU later on in this article.As different NSU classes are typically related to different resolution constraints, inorder to resolve NSUs appropriately systems need to be equipped in the first place withthe ability of identifying the intended kind of NSU.
How this ability can be developed isprecisely the issue we address in this article.
We concentrate on the task of automaticallyclassifying NSUs, which we approach using machine learning (ML) techniques.
Our aimin doing so is to develop a classification model whose output can be fed into a dialogueprocessing system?be it a full dialogue system or, for instance, an automatic dialoguesummarization system?to boost its NSU resolution capability.As we shall see, to run the ML experiments we report in this article, we an-notate our data with small sets of meaningful features, instead of using large setsof arbitrary features as is common in some stochastic approaches.
We do thiswith the aim of obtaining a better understanding of the different classes of NSUs,their distribution, and their properties.
For training, we use four machine learn-ing systems: the rule induction learner SLIPPER (Cohen and Singer 1999), thememory-based learner TiMBL (Daelemans et al 2003), the maximum entropy algo-rithm MaxEnt (Le 2003), and the Weka toolkit (Witten and Frank 2000).
From theWeka toolkit we use the J4.8 decision tree learner, as well as a majority class pre-dictor and a one-rule classifier to derive baseline systems that help us to evaluate398Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialoguethe difficulty of the classification task and the ML results obtained.
The mainadvantage of using several systems that implement different learning techniques is thatthis allows us to factor out any algorithm-dependent effects that may influence ourresults.The article is structured as follows.
In Section 2, we introduce the taxonomy of NSUclasses we adopt, present a corpus study done using the BNC, and give an overviewof the theoretical approach to NSU resolution we assume.
After these introductorysections, in Section 3 we present a pilot study that focuses on bare wh-phrases or sluices.This includes a small corpus study and a preliminary ML experiment that concentrateson disambiguating between the different interpretations that sluices can convey.
Weobtain very encouraging results: around 80% weighted F-score (an 8% improvementover a simple one-rule baseline).
After this, in Section 4, we move on to the full rangeof NSUs.
We present our main experiments, whereby the ML approach is extended tothe task of classifying the full range of NSU classes in our taxonomy.
The results weachieve on this task are decidedly positive: around an 87% weighted F-score (a 25%improvement over a four-rule baseline where only four features are used).
Finally, inSection 5, we offer conclusions and some pointers for future work.2.
A Taxonomy of NSUsWe propose a taxonomy that offers a comprehensive inventory of the kinds of NSUsthat can be found in conversation.
The taxonomy includes 15 NSU classes.
With a fewmodifications, these follow the corpus-based taxonomy proposed by Ferna?ndez andGinzburg (2002).
In what follows we exemplify each of the categories we use in ourwork and characterize them informally.Clarification Ellipsis (CE).
We use this category to classify reprise fragments used toclarify an utterance that has not been fully comprehended.
(3) a.
A: There?s only two people in the classB: Two people?
[BNC: KPP 352?354]b.
A: [.
.
. ]
You lift your crane out, so this part would come up.B: The end?
[BNC: H5H 27?28]Check Question.
This NSU class refers to short queries, usually realized by convention-alized forms like alright?
and okay?, that are requests for explicit feedback.
(4) A: So <pause> I?m allowed to record you.Okay?B: Yes.
[BNC: KSR 5?7]Sluice.
We consider as sluices all wh-question NSUs, thereby conflating under this form-based NSU class reprise and direct sluices like those in (5a) and (5b), respectively.2 In thetaxonomy of Ferna?ndez and Ginzburg (2002) reprise sluices are classified as CE.
In thetaxonomy used in the experiments we report in this article, however, CE only includesclarification fragments that are not bare wh-phrases.2 This distinction is due to Ginzburg and Sag (2001).
More on it will be discussed in Section 2.2.399Computational Linguistics Volume 33, Number 3(5) a.
A: Only wanted a couple weeks.B: What?
[BNC: KB1 3311?3312]b.
A: I know someone who?s a good kisser.B: Who?
[BNC: KP4 511?512]Short Answer.
This NSU class refers to typical responses to (possibly embedded) wh-questions (6a)/(6b).
Sometimes, however, wh-questions are not explicit, as in the contextof a short answer to a CE question, for instance (6c).
(6) a.
A: Who?s that?B: My Aunty Peggy.
[BNC: G58 33?35]b.
A: Can you tell me where you got that information from?B: From our wages and salary department.
[BNC: K6Y 94?95]c. A: Vague and?B: Vague ideas and people.
[BNC: JJH 65?66]Plain Affirmative Answer and Plain Rejection.
The typical context of these two classesof NSUs is a polar question (7a), which can be implicit as in CE questions like (7b).
Asshown in (7c), rejections can also be used to respond to assertions.
(7) a.
A: Did you bring the book I told you?B: Yes./ No.b.
A: That one?B: Yeah.
[BNC: G4K 106?107]c. A: I think I left it too long.B: No no.
[BNC: G43 26?27]Both plain affirmative answers and rejections are strongly indicated by lexicalmaterial, characterized by the presence of a ?yes?
word ( yeah, aye, yep.
.
. )
or the negativeinterjection no.Repeated Affirmative Answer.
We distinguish plain affirmative answers like the onesin (7) from repeated affirmative answers like the one in (8), which respond affirmativelyto a polar question by verbatim repetition or reformulation of (a fragment of) thequery.
(8) A: Did you shout very loud?B: Very loud, yes.
[BNC: JJW 571-572]Helpful Rejection.
The context of helpful rejections can be either a polar question oran assertion.
In the first case, they are negative answers that provide an appropriatealternative (9a).
As responses to assertions, they correct some piece of information inthe previous utterance (9b).
(9) a.
A: Is that Mrs. John <last or full name>?B: No, Mrs. Billy.
[BNC: K6K 67-68]400Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialogueb.
A: Well I felt sure it was two hundred pounds a, a week.B: No fifty pounds ten pence per person.
[BNC: K6Y 112?113]Plain Acknowledgment.
The class plain acknowledgment refers to utterances (likeyeah, mhm, ok) that signal that a previous declarative utterance was understood and/oraccepted.
(10) A: I know that they enjoy debating these issues.B: Mhm.
[BNC: KRW 146?147]Repeated Acknowledgment.
This class is used for acknowledgments that, as repeatedaffirmative answers, also repeat a part of the antecedent utterance, which in this case isa declarative.
(11) A: I?m at a little place called Ellenthorpe.B: Ellenthorpe.
[BNC: HV0 383?384]Propositional and Factual Modifiers.
These two NSU classes are used to classify propo-sitional adverbs like (12a) and factual adjectives like (12b), respectively, in stand-aloneuses.
(12) a.
A: I wonder if that would be worth getting?B: Probably not.
[BNC: H61 81?82]b.
A: There?s your keys.B: Oh great!
[BNC: KSR 137?138]Bare Modifier Phrase.
This class refers to NSUs that behave like adjuncts modifying acontextual utterance.
They are typically PPs or AdvPs.
(13) A: [.
.
. ]
they got men and women in the same dormitory!B: With the same showers!
[BNC: KST 992?996]Conjunct.
This NSU class is used to classify fragments introduced by conjunctions.
(14) A: Alistair erm he?s, he?s made himself coordinator.B: And section engineer.
[BNC: H48 141?142]Filler.
Fillers are NSUs that fill a gap left by a previous unfinished utterance.
(15) A: [.
.
. ]
twenty two percent is er <pause>B: Maxwell.
[BNC: G3U 292?293]2.1 The Corpus StudyThe taxonomy of NSUs presented herein has been tested in a corpus study carried outusing the dialogue transcripts of the BNC.
The study, which we describe here briefly,supplies the data sets used in the ML experiments we will present in Section 4.The present corpus of NSUs includes and extends the subcorpus used in Ferna?ndezand Ginzburg (2002).
It was created by manual annotation of a randomly selectedsection of 200-speaker-turns from 54 BNC files.
Of these files, 29 are transcripts of401Computational Linguistics Volume 33, Number 3conversations between two dialogue participants, and 25 files are multi-party tran-scripts.
The total of transcripts used covers a wide variety of domains, from free conver-sation to meetings, tutorials and training sessions, as well as interviews and transcriptsof medical consultations.
The examined subcorpus contains 14,315 sentences.
Sentencesin the BNC are identified by the CLAWS segmentation scheme (Garside 1987) and eachunit is assigned an identifier number.We found a total of 1,299 NSUs, which make up 9% of the total of sentences in thesubcorpus.
These results are in line with the rates reported in other recent corpus studiesof NSUs: 11.15% in (Ferna?ndez and Ginzburg 2002), 10.2% in (Schlangen and Lascarides2003), 8.2% in (Schlangen 2005).3The NSUs found were labeled according to the taxonomy presented previouslytogether with an additional class Other introduced to catch all NSUs that did not fallin any of the classes in the taxonomy.
All NSUs that could be classified with the tax-onomy classes were additionally tagged with the sentence number of their antecedentutterance.
The NSUs not covered by the classification only make up 1.2% (16 instances)of the total of NSUs found.
Thus, with a rate of 98.8% coverage, the present taxonomyoffers a satisfactory coverage of the data.The labeling of the entire corpus of NSUs was done by one expert annotator.
Toassess the reliability of the annotation, a small study with two additional, non-expertannotators was conducted.
These annotated a total of 50 randomly selected instances(containing a minimum of two instances of each NSU class as labeled by the expertannotator) with the classes in the taxonomy.
The agreement obtained by the threeannotators is reasonably good, yielding a ?
score of 0.76.
The non-expert annotatorswere also asked to identify the antecedent sentence of each NSU.
Using the expertannotation as a gold standard, they achieved 96% and 92% accuracy in this task.The distribution of NSU classes that emerged after the annotation of the subcorpusis shown in detail in Table 1.
By far the most common class can be seen to be PlainAcknowledgment, which accounts for almost half of all NSUs found.
This is followedin frequency by Short Answer (14.5%) and Plain Affirmative Answer (8%).
CE is themost common class among the NSUs that denote questions (i.e., CE, Sluice, and CheckQuestion), making up 6.3% of all NSUs found.2.2 Resolving NSUs: Theoretical Background and ImplementationThe theoretical background we assume with respect to the resolution of NSUs derivesfrom the proposal presented in Ginzburg and Sag (2001), which in turn is based on thetheory of context developed by Ginzburg (1996, 1999).Ginzburg and Sag (2001) provide a detailed analysis of a number of classes ofNSUs?including Short Answer, Sluice, and CE?couched in the framework of Head-driven Phrase Structure Grammar (HPSG).
They take NSUs to be first-class gram-matical constructions whose resolution is achieved by combining the contribution ofthe NSU phrase with contextual information?concretely, with the current questionunder discussion, or QUD, which roughly corresponds to the current conversationaltopic.43 For a comparison of our NSU taxonomy and the one proposed by Schlangen (2003), see Ferna?ndez (2006).4 An anonymous reviewer asked about the distinction between NSUs that are meaning complete and thosewhich are not.
In fact we take all NSUs to be interpreted as full propositions or questions.402Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 1Distribution of NSU classes.NSU class Total %Plain Acknowledgment 599 46.1Short Answer 188 14.5Plain Affirmative Answer 105 8.0Repeated Acknowledgment 86 6.6Clarification Ellipsis 82 6.3Plain Rejection 49 3.7Factual Modifier 27 2.0Repeated Affirmative Answer 26 2.0Helpful Rejection 24 1.8Check Question 22 1.7Filler 18 1.4Bare Modifier Phrase 15 1.1Propositional Modifier 11 0.8Sluice 21 1.6Conjunct 10 0.7Other 16 1.2Total 1,299 100The simplest way of exemplifying this strategy is perhaps to consider a direct shortanswer to an explicit wh-question, like the one shown in (16a).
(16) a.
A: Who?s making the decisions?B: The fund manager.
(= The fund manager is making the decisions.
)[BNC: JK7 119?120]b. QUD: ?
(x).Make decision(x, t)Resolution: Make decision( fm,t)In this dialogue, the current QUD corresponds to the content of the previous utterance?the wh-question Who?s making the decisions?
Assuming a representation of questions aslambda abstracts, the resolution of the short answer amounts to applying this questionto the phrasal content of the NSU, as shown in (16b) in an intuitive notation.5Ginzburg and Sag (2001) distinguish between direct and reprise sluices.
For directsluicing, the current QUD is a polar question p?, where p is required to be a quan-tified proposition.6 The resolution of the direct sluice consists in constructing a wh-question by a process that replaces the quantification with a simple abstraction.
Forinstance:(17) a.
A: A student phoned.B: Who?
(= Which student phoned?
)5 To simplify matters, throughout the examples in this section we use lambda abstraction for wh-questionsand a simple question mark operator for polar questions.
For a far more accurate representation ofquestions in HPSG and Type Theory with Records, see Ginzburg and Sag (2001) and Ginzburg (2005),respectively.6 In Ginzburg?s theory of context an assertion of a proposition p raises the polar question p?
for discussion.403Computational Linguistics Volume 33, Number 3b.
QUD: ?
?xPhone(x, t)Resolution: ?
(x).Phone(x, t)In the case of reprise sluices and CE, the current QUD arises in a somewhat less directway, via a process of utterance coercion or accommodation (Larsson 2002; Ginzburg andCooper 2004), triggered by the inability to ground the previous utterance (Traum 1994;Clark 1996).
The output of the coercion process is a question about the content of a(sub)utterance which the addressee cannot resolve.
For instance, if the original utteranceis the question Did Bo leave?
in (18a), with Bo as the unresolvable sub-utterance, onepossible output from the coercion operations defined by Ginzburg and Cooper (2004) isthe question in (18b), which constitutes the current QUD, as well as the resolved contentof the reprise sluice in (18a).
(18) a.
A: Did Bo leave?B: Who?
(= Who are you asking if s/he left?)b.
QUD: ?
(b).Ask(A, ?Leave(b, t))Resolution: ?
(b).Ask(A, ?Leave(b, t))The interested reader will find further details of this approach to NSU resolution and itsextension to other NSU classes in Ginzburg (forthcoming) and Ferna?ndez (2006).The approach sketched here has been implemented as part of the SHARDS system(Ginzburg, Gregory, and Lappin 2001; Ferna?ndez et al, in press), which provides a pro-cedure for computing the interpretation of some NSU classes in dialogue.
The systemcurrently handles short answers, direct and reprise sluices, as well as plain affirmativeanswers to polar questions.
SHARDS has been extended to cover several types ofclarification requests and used as a part of the information-state-based dialogue systemCLARIE (Purver 2004b).
The dialogue system GoDiS (Larsson et al 2000; Larsson 2002)also uses a QUD-based approach to handle short answers.3.
Pilot Study: Sluice Reading ClassificationThe first study we present focuses on the different interpretations or readings thatsluices can convey.
We first describe a corpus study that aims at providing empiricalevidence about the distribution of sluice readings and establishing possible correlationsbetween these readings and particular sluice types.
After this, we report the results ofa pilot machine learning experiment that investigates the automatic disambiguation ofsluice interpretations.3.1 The Sluicing Corpus StudyWe start by introducing the corpus of sluices.
The next subsections describe the annota-tion scheme, the reliability of the annotation, and the corpus results obtained.Because sluices have a well-defined surface form?they are bare wh-words?wewere able to use an automatic mechanism to reliably construct our subcorpus of sluices.This was created using SCoRE (Purver 2001), a tool that allows one to search the BNCusing regular expressions.404Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 2Total of sluices in the BNC.what why who where which N when how which Total3,045 1,125 491 350 160 107 50 15 5,343The dialogue transcripts of the BNC contain 5,183 bare sluices (i.e., 5,183 sentencesconsisting of just a wh-word).
We distinguish between the following classes of baresluices: what, who, when, where, why, how, and which.
Given that only 15 bare which werefound, we also considered sluices of the form which N. Including which N, the corpuscontains a total of 5,343 sluices, whose distribution is shown in Table 2.For our corpus study, we selected a sample of sluices extracted from the total foundin the dialogue transcripts of the BNC.
The sample was created by selecting all instancesof bare how (50) and bare which (15), and arbitrarily selecting 100 instances of each of theremaining sluice classes, making up a total of 665 sluices.Note that the sample does not reflect the frequency of sluice types found in the fullcorpus.
The inclusion of sufficient instances of the lesser frequent sluice types wouldhave involved selecting a much larger sample.
Consequently it was decided to abstractover the true frequencies to create a balanced sample whose size was manageableenough to make the manual annotation feasible.
We will return to the issue of the truefrequencies in Section 3.1.3.3.1.1 Sluice Readings.
The sample of sluices was classified according to a set of foursemantic categories?drawn from the theoretical distinctions introduced by Ginzburgand Sag (2001)?corresponding to different sluice interpretations.
The typology reflectsthe basic direct/reprise divide and incorporates other categories that cover additionalreadings, including an Unclear class intended for those cases that cannot easily beclassified by any of the other categories.
The typology of sluice readings used was thefollowing:Direct.
Sluices conveying a direct reading query for additional information that wasexplicitely or implicitly quantified away in the antecedent, which is understood withoutdifficulty.
The sluice in (19) is an example of a sluice with direct reading: It asks foradditional temporal information that is implicitly quantified away in the antecedentutterance.
(19) A: I?m leaving this school.B: When?
[BNC: KP3 537?538]Reprise.
Sluices conveying a reprise reading emerge as a result of an understandingproblem.
They are used to clarify a particular aspect of the antecedent utterance corre-sponding to one of its constituents, which was not correctly comprehended.
In (20) thereprise sluice has as antecedent constituent the pronoun he, whose reference could notbe adequately grounded.
(20) A: What a useless fairy he was.B: Who?
[BNC: KCT 1752?1753]405Computational Linguistics Volume 33, Number 3Clarification.
As reprise, this category also corresponds to a sluice reading that dealswith understanding problems.
In this case the sluice is used to request clarification ofthe entire antecedent utterance, indicating a general breakdown in communication.
Thefollowing is an example of a sluice with a clarification interpretation:(21) A: Aye and what money did you get on it?B: What?A: What money does the government pay you?
[BNC: KDJ 1077?1079]Wh-anaphor.
This category is used for the reading conveyed by sluices like (22), whichare resolved to a (possibly embedded) wh-question present in the antecedent utterance.
(22) A: We?re gonna find poison apple and I know where that one is.B: Where?
[BNC: KD1 2370?2371]Unclear.
We use this category to classify those sluices whose interpretation is difficultto grasp, possibly because the input is too poor to make a decision as to its resolution,as in the following example:(23) A: <unclear> <pause>B: Why?
[BNC: KCN 5007]3.1.2 Reliability.
The coding of sluice readings was done independently by three dif-ferent annotators.
Agreement was moderate (?
= 0.59).
There were important differ-ences among sluice classes: The lowest agreement was on the annotation of how (0.32)and what (0.36), whereas the agreement on classifying who was substantially higher(0.74).Although the three coders may be considered ?experts,?
their training and famil-iarity with the data were not equal.
This resulted in systematic differences in theirannotations.
Two of the coders had worked more extensively with the BNC dialoguetranscripts and, crucially, with the definition of the categories to be applied.
Leaving thethird annotator out of the coder pool increases agreement very significantly (?
= 0.71).The agreement reached by the more expert pair of coders was acceptable and, we believe,provides a solid foundation for the current classification.73.1.3 Distribution Patterns.
The sluicing corpus study shows that the distribution of read-ings is significantly different for each class of sluice.
The distribution of interpretationsis shown in Table 3, presented as row counts and percentages of those instances where7 Besides the difficulty of annotating fine-grained semantic distinctions, we think that one of the reasonswhy the ?
score we obtain is not too high is that, as shall become clear in the next section, the presentannotation is strongly affected by the prevalence problem, which occurs when the distributions forcategories are skewed (highly unequal instantiation across categories).
In order to control for differencesin prevalence, Di Eugenio and Glass (2004) propose an additional measure called PABAK(prevalence-adjusted bias-adjusted kappa).
In our case, we obtain a PABAK score of 0.60 for agreementamongst the three coders, and a PABAK score of 0.80 for agreement between the pair of more expertcoders.
A more detailed discussion of these issues can be found in Ferna?ndez (2006).406Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 3Distribution patterns.Sluice Direct n (%) Reprise n (%) Clarification n (%) Wh-anaphor n (%)what 7 (9.60) 17 (23.3) 48 (65.7) 1 (1.3)why 55 (68.7) 24 (30.0) 0 (0) 1 (1.2)who 10 (13.0) 65 (84.4) 0 (0) 2 (2.6)where 31 (34.4) 56 (62.2) 0 (0) 3 (3.3)when 50 (63.3) 27 (34.1) 0 (0) 2 (2.5)which 1 (8.3) 11 (91.6) 0 (0) 0 (0)whichN 19 (21.1) 71 (78.8) 0 (0) 0 (0)how 23 (79.3) 3 (10.3) 3 (10.3) 0 (0)at least two annotators agree, labeled taking the majority class and leaving aside casesclassified as Unclear.Table 3 reveals significant correlations between sluice classes and preferred interpre-tations (a chi square test yields ?2 = 438.53, p ?
0.001).
The most common interpretationfor what is Clarification, making up more than 65%.
Why sluices have a tendency to beDirect (68.7%).
The sluices with the highest probability of being Reprise are who (84.4%),which (91.6), which N (78.8%), and where (62.2%).
On the other hand, when (63.3%) andhow (79.3%) have a clear preference for Direct interpretations.As explained in Section 3.1, the sample used in the corpus study does not reflectthe overall frequencies of sluice types found in the BNC.
Now, in order to gain acomplete perspective on sluice distribution in the full corpus, it is therefore appropriateto combine the percentages in Table 3 with the absolute number of sluices contained inthe BNC.
The number of estimated tokens is displayed in Table 4.For instance, the combination of Tables 3 and 4 allows us to see that althoughalmost 70% of why sluices are Direct, the absolute number of why sluices that are Repriseexceeds the total number of when sluices by almost 3 to 1.
Another interesting patternrevealed by this data is the low frequency of when sluices, particularly by comparisonwith what one might expect to be its close cousin, where.
Indeed the Direct/Reprisesplits are almost mirror images for when versus where.
Explicating the distribution inTable 4 is important in order to be able to understand among other issues whether wewould expect a similar distribution to occur in a Spanish or Mandarin dialogue corpus;similarly, whether one would expect this distribution to be replicated across differentdomains.Table 4Sluice class frequency (estimated tokens).whatcla 2,040 whichNrep 135whydir 775 whendir 90whatrep 670 whodir 70whorep 410 wheredir 70whyrep 345 howdir 45whererep 250 whenrep 35whatdir 240 whichNdir 24407Computational Linguistics Volume 33, Number 3We will not attempt to provide an explanation for these patterns here.
The readeris invited to check a sketch of such an explanation for some of the patterns exhibited inTable 4 in Ferna?ndez, Ginzburg, and Lappin (2004).3.2 Automatic DisambiguationIn this section, we report a pilot study where we use machine learning to automaticallydisambiguate between the different sluice readings using data obtained in the corpusstudy presented previously.3.2.1 Data.
The data set used in this experiment was selected from our classified corpusof sluices.
To generate the input data for the ML experiments, all three-way agreementinstances plus those instances where there is agreement between the two coders with thehighest agreement were selected, leaving out cases classified as Unclear.
The total dataset includes 351 datapoints.
Of these, 106 are classified as Direct, 203 as Reprise, 24 asClarification, and 18 as Wh-anaphor.
Thus, the classes in the data set have significantlyskewed distributions.
However, as we are faced with a very small data set, we cannotafford to balance the classes by leaving out a subset of the data.
Hence, in this pilot studythe 351 data points are used in the ML experiments with their original distributions.3.2.2 Features and Feature Annotation.
In this pilot study?as well as in the extendedexperiment we will present later on?instances were annotated with a small set offeatures extracted automatically using the POS information encoded in the BNC.
Theannotation procedure involves a simple algorithm which employs string searching andpattern matching techniques that exploit the SGML mark-up of the corpus.
The BNCwas automatically tagged using the CLAWS system developed at Lancaster University(Garside 1987).
The ?100 million words in the corpus were annotated according to aset of 57 POS codes (known as the C5 tag-set) plus 4 codes for punctuation tags.
Alist of these codes can be found in Burnard (2000).
The BNC POS annotation process isdescribed in detail in Leech, Garside, and Bryant (1994).Unfortunately the BNC mark-up does not include any coding of intonation.
Our fea-tures can therefore not use any intonational data, which would presumably be a usefulTable 5Sluice features and values.Feature Description Valuessluice type of sluice what, why, who, .
.
.mood mood of the antecedent utterance decl, n declpolarity polarity of the antecedent utterance pos, neg, ?quant presence of a quantified expression yes, no, ?deictic presence of a deictic pronoun yes, no, ?proper n presence of a proper name yes, no, ?pro presence of a pronoun yes, no, ?def desc presence of a definite description yes, no, ?wh presence of a wh word yes, no, ?overt presence of any other potential antecedent expression yes, no, ?408Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in Dialoguesource of information to distinguish, for instance, between question- and proposition-denoting NSUs, between Plain Acknowledgment and Plain Affirmative Answer, andbetween Reprise and Direct sluices.To annotate the sluicing data, a set of 11 features was used.
An overview of thefeatures and their values is shown in Table 5.
Besides the feature sluice, which indicatesthe sluice type, all the other features are concerned with properties of the antecedentutterance.
The features mood and polarity refer to syntactic and semantic propertiesof the antecedent utterance as a whole.
The remaining features, on the other hand,focus on a particular lexical type or construction contained in the antecedent.
Thesefeatures (quant, deictic, proper n, pro, def desc, wh, and overt) are not annotatedindependently, but conditionally on the sluice type.
That is, they will take yes as avalue if the element or construction in question appears in the antecedent and it matchesthe semantic restrictions imposed by the sluice type.
For instance, when a sluice withvalue where for the feature sluice is annotated, the feature deictic, which encodesthe presence of a deictic pronoun, will take value yes only if the antecedent utterancecontains a locative deictic like here or there.
Similarly the feature wh takes a yes valueonly if there is a wh-word in the antecedent that is identical to the sluice type.Unknown or irrelevant values are indicated by a question mark (?)
value.
Thisallows us to express, for instance, that the presence of a proper name is irrelevant todetermining the interpretation of say a when sluice, although it is crucial when the sluicetype is who.
The feature overt takes no as value when there is no overt antecedentexpression.
It takes yes when there is an antecedent expression not captured by anyother feature, and it is considered irrelevant (?
value) when there is an antecedentexpression defined by another feature.The 351 data points were automatically annotated with the 11 features shown inTable 5.
The automatic annotation procedure was evaluated against a manual goldstandard, achieving an accuracy of 86%.3.2.3 Baselines.
Because sluices conveying a Reprise reading make up more than 57%of our data set, relatively high results can already be achieved with a majority classbaseline that always predicts the class Reprise.
This yields a 42.4% weighted F-score.A slightly more interesting baseline can be obtained by using a one-rule classifier.We use the implementation of a one-rule classifier provided in the Weka toolkit.
For eachfeature, the classifier creates a single rule which generates a decision tree where the rootis the feature in question and the branches correspond to its different values.
The leavesare then associated with the class that occurs most often in the data, for which that valueholds.
The classifier then chooses the feature which produces the minimum error.Figure 1One-rule tree.409Computational Linguistics Volume 33, Number 3Table 6Baselines?
results.Sluice reading Recall Precision F1Majority class baseline Reprise 100 57.80 73.30weighted score 57.81 33.42 42.40One-rule baseline Direct 72.60 67.50 70.00Reprise 79.30 80.50 79.90Clarification 100 64.90 78.70weighted score 73.61 71.36 72.73In this case the feature with the minimum error chosen by the one-rule classifier issluice.
The classifier produces the one-rule tree in Figure 1.
The branches of the treecorrespond to the sluice types; the interpretation with the highest probability for eachtype of sluice is then predicted.By using the feature sluice the one-rule tree implements the correlations betweensluice type and preferred interpretation that were discussed in Section 3.1.3.
There, wepointed out that these correlations were statistically significant.
We can see now thatthey are indeed a good rough guide for predicting sluice readings.
As shown in Table 6,the one-rule baseline dependent on the distribution patterns of the different sluice typesyields a 72.73% weighted F-score.All results reported (here and in the remainder of the article) were obtained byperforming 10-fold cross-validation.
They are presented as follows: The tables showthe recall, precision, and F-measure for each class.
To calculate the overall performanceof the algorithm, these scores are normalized according to the relative frequency ofeach class.
This is done by multiplying each score by the total of instances of thecorresponding class and then dividing by the total number of datapoints in the dataset.
The weighted overall recall, precision, and F-measure, shown in boldface for eachbaseline in Table 6, is then the sum of the corresponding weighted scores.
For each ofthe baselines, the sluice readings not shown in the table obtain null scores.3.2.4 ML Results.
Finally, the four machine learning algorithms were run on the dataset annotated with the 11 features.
Here, as well as in the more extensive experimentwe will present in Section 4, we use the following parameter settings with each of thelearners.
Weka?s J4.8 decision tree learner is run using the default parameter settings.With SLIPPER we use the option unordered, which finds a rule set that separates eachclass from the remaining classes using growing and pruning techniques and in our caseyields slightly better results than the default setting.
As for TiMBL, we run it using themodified value difference metric (which performs better than the default overlap metric),and keep the default settings for the number of nearest neighbors (k = 1) and featureweighting method (gain ratio).
Finally, with MaxEnt we use 40 iterations of the defaultL-BFGS parameter estimation (Malouf 2002).Overall, in this pilot study we obtain results of around 80% weighted F-score,although there are some significant differences amongst the learners.
MaxEnt gives thelowest score (73.24% weighted F-score)?hardly over the one-rule baseline, and morethan 8 points lower than the best results, obtained with Weka?s J4.8 (81.80% weightedF-score).
The size of the data set seems to play a role in these differences, indicating thatMaxEnt does not perform so well with small data sets.
A summary of weighted F-scoresis given in Table 7.410Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 7Comparison of weighted F-scores.System Weighted F-scoreMajority class baseline 42.40One rule baseline 72.73MaxEnt 73.24TiMBL 79.80SLIPPER 81.62J4.8 81.80Detailed recall, precision, and F-measure results for each learner are shown inAppendix A.
The results yielded by MaxEnt are almost equivalent to the ones achievedwith the one-rule baseline.
With the other three learners, the use of contextual featuresimproves the results for Reprise and Direct by around 5 points each with respect to theone-rule baseline.
The results obtained with the one-rule baseline for the Clarificationreading, however, are hardly improved upon by any of the learners.
In the case ofTiMBL the score is in fact lower?72.16 versus 78.70 weighted F-score.
This leads us toconclude that the best strategy is to interpret al what sluices as conveying a Clarificationreading.The class Wh-anaphora, which, not being the majority interpretation for any sluicetype, was not predicted by the one-rule baseline nor by MaxEnt, now gives positiveresults with the other three learners.
The best result for this class is obtained with Weka?sJ4.8: 80% F-score.The decision tree generated by Weka?s J4.8 algorithm is displayed in Figure 2.
Theroot of the tree corresponds to the feature wh, which makes a first distinction betweenFigure 2Weka?s J4.8 tree.411Computational Linguistics Volume 33, Number 3Wh-anaphor and the other readings.
If the value of this feature is yes, the class Wh-anaphor is predicted.
A negative value for this feature leads to the feature sluice.
Theclass with the highest probability is the only clue used to predict the interpretationof the sluice types what, where, which, and whichN in a way parallel to the one-rulebaseline.
Additional features are used for when, why, and who.
A Direct reading ispredicted for a when sluice if there is no overt antecedent expression, whereas a Reprisereading is preferred if the feature overt takes as value yes.
For why sluices the moodof the antecedent utterance is used to disambiguate between Reprise and Direct: If theantecedent is declarative, the sluice is classified as Direct; if it is non-declarative it isinterpreted as Reprise.
In the classification of who sluices three features are taken intoaccount: quant, pro, and proper n. The basic strategy is as follows: If the antecedentutterance contains a quantifier and neither personal pronouns nor proper names appear,the predicted class is Direct, otherwise the sluice is interpreted as Reprise.3.2.5 Feature Contribution.
Note that not all features are used in the tree generated byWeka?s J4.8.
The missing features are polarity, deictic, and def desc.
Although theydon?t make any contribution to the model generated by the decision tree, examinationof the rules generated by SLIPPER shows that they are all used in the rule set induced bythis algorithm, albeit in rules with low confidence level.
Despite the fact that SLIPPERuses all features, the contribution of polarity, deictic, and def desc does not seemto be very significant.
When they are eliminated from the feature set, SLIPPER yieldsvery similar results to the ones obtained with the full set of features: 81.22% weighted F-score versus the 81.66% obtained before.
TiMBL on the other hand goes down a couple ofpoints, from 79.80% to 77.32% weighted F-score.
No variation is observed with MaxEnt,which seems to be using just the sluice type as a clue for classification.4.
Classifying the Full Range of NSUsSo far we have presented a study that has concentrated on fine-grained semantic dis-tinctions of one of the classes in our taxonomy, namely Sluice, and have obtained veryencouraging results?around 80% weighted F-score (an improvement of 8 points overa simple one-rule baseline).
In this section we show that the ML approach taken canbe successfully extended to the task of classifying the full range of NSU classes in ourtaxonomy.We first present an experiment run on a restricted data set that excludes the classesPlain Acknowledgement and Check Question, and then, in Section 4.6, report on afollow-up experiment where all NSU classes are included.4.1 DataThe data used in the experiments was selected from the corpus of NSUs following somesimplifying restrictions.
Firstly, we leave aside the 16 instances classified as Other in thecorpus study (see Table 1).
Secondly, we restrict the experiments to those NSUs whoseantecedent is the immediately preceding utterance.
This restriction, which makes thefeature annotation task easier, does not pose a significant coverage problem, given thatthe immediately preceding utterance is the antecedent for the vast majority of NSUs(88%).
The set of all NSUs, excluding those classified as Other, whose antecedent is theimmediately preceding utterance, contains a total of 1123 datapoints.
See Table 8.412Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 8NSU subcorpus.NSU class TotalPlain Acknowledgment 582Short Answer 105Affirmative Answer 100Repeated Acknowledgment 80CE 66Rejection 48Repeated Affirmative Answer 25Factual Modifier 23Sluice 20Helpful Rejection 18Filler 16Check Question 15Bare Modifier Phrase 10Propositional Modifier 10Conjunct 5Total data set 1,123Finally, as mentioned previously, the last restriction adopted concerns the instancesclassified as Plain Acknowledgment and Check Question.
Taking the risk of end-ing up with a considerably smaller data set, we decided to leave aside these meta-communicative NSU classes given that (1) plain acknowledgments make up more than50% of the subcorpus leading to a data set with very skewed distributions; (2) checkquestions are realized by the same kind of expressions as plain acknowledgments (okay,right, etc.)
and would presumably be captured by the same feature; and (3) a priori thesetwo classes seem two of the easiest types to identify (a hypothesis that was confirmedafter a second experiment?see Section 4.6).
We therefore exclude plain acknowledg-ments and check questions and concentrate on a more interesting and less skewed dataset containing all remaining NSU classes.
This makes up a total of 526 data points(1123 ?
582 ?
15).
In Subsection 4.6 we shall compare the results obtained using thisrestricted data set with those of a second experiment in which plain acknowledgementsand check questions are incorporated.4.2 FeaturesA small set of features that capture the contextual properties that are relevant forNSU classification was identified.
In particular three types of properties that play animportant role in the classification task were singled out.
The first one has to do withsemantic, syntactic, and lexical properties of the NSUs themselves.
The second onerefers to the properties of its antecedent utterance.
The third concerns relations betweenthe antecedent and the fragment.
Table 9 shows an overview of the nine features used.4.2.1 NSU Features.
A set of four features are related to properties of the NSUs.
Theseare nsu cont, wh nsu, aff neg, and lex.
The feature nsu cont is intended to distin-guish between question-denoting (q value) and proposition-denoting (p value) NSUs.The feature wh nsu encodes the presence of a wh-phrase in the NSU?it is primarilyintroduced to identify Sluices.
The features aff neg and lex signal the appearance of413Computational Linguistics Volume 33, Number 3Table 9NSU features and values.Feature Description Valuesnsu cont content of the NSU (either prop or question) p, qwh nsu presence of a wh word in the NSU yes, noaff neg presence of a yes/no word in the NSU yes, no, e(mpty)lex presence of different lexical items in the NSU p mod, f mod, mod, conj, eant mood mood of the antecedent utterance decl, n declwh ant presence of a wh word in the antecedent yes, nofinished (un)finished antecedent fin, unfrepeat repeated words in NSU and antecedent 0-3parallel repeated tag sequences in NSU and antecedent 0-3particular lexical items.
They include a value e(mpty) which allows us to encode theabsence of the relevant lexical items as well.
The values of the feature aff neg indicatethe presence of either a yes or a no word in the NSU.
The values of lex are invoked by theappearance of modal adverbs (p mod), factual adjectives (f mod), and prepositions (mod)and conjunctions (conj) in initial positions.
These features are expected to be crucial tothe identification of Plain/Repeated Affirmative Answer and Plain/Helpful Rejectionon the one hand, and Propositional Modifiers, Factual Modifiers, Bare Modifier Phrases,and Conjuncts on the other.Note that the feature lex could be split into four binary features, one for each of itsnon-empty values.
This option, however, leads to virtually the same results.
Hence, weopt for a more compact set of features.
This also applies to the feature aff neg.4.2.2 Antecedent Features.
We use the features ant mood, wh ant, and finished to encodeproperties of the antecedent utterance.
The first of these features distinguishes betweendeclarative and non-declarative antecedents.
The feature wh ant signals the presenceof a wh-phrase in the antecedent utterance, which seems to be the best cue for classi-fying Short Answers.
As for the feature finished, it should help the learners identifyFillers.
The value unf is invoked when the antecedent utterance has a hesitant ending(indicated, for instance, by a pause) or when there is no punctuation mark signalling afinished utterance.4.2.3 Similarity Features.
The last two features, repeat and parallel, encode similarityrelations between the NSU and its antecedent utterance.
They are the only numer-ical features in the feature set.
The feature repeat, which indicates the appearanceof repeated words between NSU and antecedent, is introduced as a clue to identifyRepeated Affirmative Answers and Repeated Acknowledgments.
The feature parallel,on the other hand, is intended to capture the particular parallelism exhibited by HelpfulRejections.
It signals the presence of sequences of POS tags common to the NSU and itsantecedent.As in the sluicing experiment, all features were extracted automatically from thePOS information encoded in the BNC mark-up.
However, as with the feature moodin the sluicing study, some features like nsu cont and ant mood are high level featuresthat do not have straightforward correlates in POS tags.
Punctuation tags (that wouldcorrespond to intonation patterns in spoken input) help to extract the values of thesefeatures, but the correspondence is still not unique.
For this reason the automatic414Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueFigure 3One-rule tree.feature annotation procedure was again evaluated against a small sample of manuallyannotated data.
The feature values were extracted manually for 52 instances (?10% ofthe total) randomly selected from the data set.
In comparison with this gold standard,the automatic feature annotation procedure achieves 89% accuracy.
Only automaticallyannotated data is used for the learning experiments.4.3 BaselinesWe now turn to examine some baseline systems that will help us to evaluate theclassification task.
As before, the simplest baseline we can consider is a majority classbaseline that always predicts the class with the highest probability in the data set.
Inthe restricted data set used for the first experiment, this is the class Short Answer.
Themajority class baseline yields a 6.7% weighted F-score.When a one-rule classifier is run, we see that the feature that yields the minimumerror is aff neg.
The one-rule baseline produces the one-rule decision tree in Fig-ure 3, which yields a 32.5% weighted F-score (see Table 10).
Plain Affirmative Answeris the class predicted when the NSU contains a yes-word, Rejection when it contains ano-word, and Short Answer otherwise.Finally, we consider a more substantial baseline that uses the four NSU features.Running Weka?s J4.8 decision tree classifier with these features creates a decision treewith four rules, one for each feature used.
The tree is shown in Figure 4.Table 10Baselines?
results.NSU Class Recall Precision F1Majority class baseline ShortAns 100.00 20.10 33.50weighted score 19.92 4.00 6.67One-rule baseline ShortAns 95.30 30.10 45.80AffAns 93.00 75.60 83.40Reject 100.00 69.60 82.10weighted score 45.93 26.73 32.50Four-rule baseline CE 96.97 96.97 96.97Sluice 100.00 95.24 97.56ShortAns 94.34 47.39 63.09AffAns 93.00 81.58 86.92Reject 100.00 75.00 85.71PropMod 100.00 100.00 100.00FactMod 100.00 100.00 100.00BareModPh 80.00 72.73 76.19Conjunct 100.00 71.43 83.33weighted score 70.40 55.92 62.33415Computational Linguistics Volume 33, Number 3Figure 4Four-rule tree.The root of the tree corresponds to the feature nsu cont.
It makes a first distinctionbetween question-denoting (q branch) and proposition-denoting NSUs (p branch).
Notsurprisingly, within the q branch the feature wh nsu is used to distinguish betweenSluice and CE.
The feature lex is the first node in the p branch.
Its different valuescapture the classes Conjunct, Propositional Modifier, Factual Modifier, and Bare Modi-fier Phrase.
The e(mpty) value for this feature takes us to the last, most embedded nodeof the tree, realized by the feature aff neg, which creates a sub-tree parallel to the one-rule tree in Figure 3.
This four-rule baseline yields a 62.33% weighted F-score.
Detailedresults for the three baselines considered are shown in Table 10.4.4 Feature ContributionAs can be seen in Table 10, the classes Sluice, CE, Propositional Modifier, and FactualModifier achieve very high F-scores with the four-rule baseline?between 97% and100%.
These results are not improved upon by incorporating additional features norby using more sophisticated learners, which indicates that NSU features are sufficientindicators to classify these NSU classes.
This is in fact not surprising, given that thedisambiguation of Sluice, Propositional Modifier, and Factual Modifier is tied to thepresence of particular lexical items that are relatively easy to identify (wh-phrases andcertain adverbs and adjectives), whereas CE acts as a default category within question-denoting NSUs.There are, however, four NSU classes that are not predicted at all when only NSUfeatures are used.
These are Repeated Affirmative Answer, Helpful Rejection, RepeatedAcknowledgment, and Filler.
Because they are not associated with any leaf in thetree, they yield null scores and therefore don?t appear in Table 10.
Examination ofthe confusion matrices shows that around 50% of Repeated Affirmative Answers wereclassified as Plain Affirmative Answers, whereas the remaining 50%?as well as theoverwhelming majority of the other three classes just mentioned?were classified asShort Answer.
Acting as the default class, Short Answers achieves the lowest score:63.09% F-score.In order to determine the contribution of the antecedent features (ant mood, wh ant,finished), as a next step these were added to the NSU features used in the four-rule tree.
When the antecedent features are incorporated, two additional NSU classesare predicted.
These are Repeated Acknowledgment and Filler, which achieve rather416Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueFigure 5Node on a tree using NSU and antecedent features.positive results: 74.8% and 64% F-score, respectively.
We do not show the full resultsobtained when NSU and antecedent features are used together.
Besides the additionof these two NSU classes, the results are very similar to those achieved with just NSUfeatures.
The tree obtained when the antecedent features are incorporated to the NSUfeatures can be derived by substituting for the last node in the tree in Figure 4 thetree in Figure 5.
As can be seen in Figure 5, the features ant mood and finished con-tribute to distinguish Repeated Acknowledgment and Filler from Short Answer, whoseF-score consequently rises, from 63.09% to 79%, due to an improvement in precision.Interestingly, the feature wh ant does not have any contribution at this stage (althoughit will be used by the learners when the similarity features are added).
The generalweighted F-score obtained when NSU and antecedent features are combined is 77.87%.A comparison of all weighted F-scores obtained will be shown in the next section, inTable 11.The use of NSU features and antecedent features is clearly not enough to accountfor Repeated Affirmative Answer and Helpful Rejection, which obtain null scores.4.5 ML ResultsIn this section we report the results obtained when the similarity features are included,thereby using the full feature set, and the four machine learning algorithms are trainedon the data.Although the classification algorithms implement different machine learning tech-niques, they all yield very similar results: around an 87% weighted F-score.
The max-imum entropy model performs best, although the difference between its results andTable 11Comparison of weighted F-scores.System Weighted F-scoreMajority class baseline 6.67One rule baseline 32.50Four rule baseline (NSU features) 62.33NSU and antecedent features 77.83Full feature set:- SLIPPER 86.35- TiMBL 86.66- J4.8 87.29- MaxEnt 87.75417Computational Linguistics Volume 33, Number 3those of the other algorithms is not statistically significant.
Detailed recall, precision,and F-measure scores are shown in Appendix B.As seen in previous sections, the four-rule baseline algorithm that uses only NSUfeatures yields a 62.33% weighted F-score, whereas the incorporation of antecedentfeatures yields a 77.83% weighted F-score.
The best result, the 87.75% weighted F-scoreobtained with the maximal entropy model using all features, shows a 10% improvementover this last result.
As promised, a comparison of the scores obtained with the differentbaselines considered and all learners used is given in Table 11.Short Answers achieve high recall scores with the baseline systems (more than90%).
In the three baselines considered, Short Answer acts as the default category.Therefore, even though the recall is high (given that Short Answer is the class withthe highest probability), precision tends to be quite low.
The precision achieved forShort Answer when only NSU features are used is ?47%.
When antecedent featuresare incorporated precision goes up to ?72%.
Finally, the addition of similarity featuresraises the precision for this class to ?82%.
Thus, by using features that help to identifyother categories with the machine learners, the precision for Short Answers is improvedby around 36%, and the precision of the overall classification system by almost 33%:from 55.90% weighted precision obtained with the four-rule baseline, to the 88.41%achieved with the maximum entropy model using all features.With the addition of the similarity features (repeat and parallel), the classesRepeated Affirmative Answer and Helpful Rejection are predicted by the learners.Although this contributes to the improvement of precision for Short Answer, the scoresyielded by these two categories are lower than the ones achieved with other classes.
Re-peated Affirmative Answer achieves nevertheless decent F-score, ranging from 56.96%with SLIPPER to 67.20% with MaxEnt.
The feature wh ant, for instance, is used todistinguish Short Answer from Repeated Affirmative Answer.
Figure 6 shows one ofthe sub-trees generated by the feature repeat when Weka?s J4.8 is used with the fullfeature set.The class with the lowest scores is clearly Helpful Rejection.
TiMBL achieves a39.92% F-score for this class.
The maximal entropy model, however, yields only a 10.37%F-score.
Examination of the confusion matrices shows that ?27% of Help Rejectionswere classified as Rejection, ?26% as Short Answer, and ?15% as Repeated Acknowl-edgement.
This indicates that the feature parallel, introduced to identify this type ofNSUs, is not a good enough cue.Figure 6Node on a tree using the full feature set.418Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueFigure 7One-rule tree.4.6 Incorporating Plain Acknowledgment and Check QuestionAs explained in Section 4.1, the data set used in the experiments reported in the previoussection excluded the instances classified as Plain Acknowledgment and Check Questionin the corpus study.
The fact that Plain Acknowledgment is the category with the highestprobability in the subcorpus (making up more than 50% of our total data set?seeTable 8), and that it does not seem particularly difficult to identify could affect theperformance of the learners by inflating the results.
Therefore it was left out in orderto work with a more balanced data set and to minimize the potential for misleadingresults.
As the expressions used in plain acknowledgments and check questions arevery similar and they would in principle be captured by the same feature values, checkquestions were left out as well.
In a second phase the instances classified as PlainAcknowledgment and Check Question were incorporated to measure their effect onthe results.
In this section we discuss the results obtained and compare them with theones achieved in the initial experiment.To generate the annotated data set an additional value ack was added to the featureaff neg.
This value is invoked to encode the presence of expressions typically used inplain acknowledgments and/or check questions (mhm, right, okay, etc.).
The total dataset (1,123 data points) was automatically annotated with the features modified in thisway, and the machine learners were then run on the annotated data.4.6.1 Baselines.
Given the high probability of Plain Acknowledgment, a simple majorityclass baseline gives relatively high results: 35.31% weighted F-score.
The feature withthe minimum error used to derive the one-rule baseline is again aff neg, this time withthe new value ack as part of its possible values (see Figure 7).
The one-rule baselineyields a weighted F-score of 54.26%.The four-rule tree that uses only NSU features goes up to a weighted F-scoreof 67.99%.
In this tree the feature aff neg is now also used to distinguish betweenCE and Check Question.
Figure 8 shows the q branch of the tree.
As the last node ofFigure 8Node on the four-rule tree.419Computational Linguistics Volume 33, Number 3the four-rule tree now corresponds to the tree in Figure 7, the class Plain AffirmativeAnswer is not predicted when only NSU features are used.When antecedent features are incorporated, Plain Affirmative Answers, RepeatedAcknowledgments, and Fillers are predicted, obtaining very similar scores to the onesachieved in the experiment with the restricted data set.
The feature ant mood is nowalso used to distinguish between Plain Acknowledgment and Plain Affirmative Answer.The last node in the tree is shown in Figure 9.
The combined use of NSU features andantecedent features yields a weighted F-score of 85.44%.4.6.2 ML Results.
As in the previous experiment, when all features are used the resultsobtained are very similar across learners (around 92% weighted F-score), if slightlylower with Weka?s J4.8 (89.53%).
Detailed scores for each class are shown in Appen-dix C. As expected, the class Plain Acknowledgment obtains a high F-score (?95% withall learners).
The F-score for Check Question ranges from 73% yielded by MaxEnt to90% obtained with SLIPPER.
The high score of Plain Acknowledgment combined withits high probability raises the overall performance of the systems almost four pointsover the results obtained in the previous experiment: from ?87% to ?92% weightedF-score.
The improvement with respect to the baselines, however, is not as large: wenow obtain a 55% improvement over the simple majority class baseline (from 35.31% to92.21%), whereas in the experiment with the restricted data set the improvement withrespect to the majority class baseline is 81% (from 6.67% to 87.75% weighted F-score.
).Table 12 shows a comparison of all weighted F-scores obtained in this secondexperiment.It is interesting to note that even though the overall performance of the algorithmsis slightly higher than before (due to the reasons mentioned previously), the scores forsome NSU classes are actually lower.
The most striking cases are perhaps the classesHelpful Rejection and Conjunct, for which the maximum entropy model now gives nullscores (see Appendix C).
We have already pointed out the problems encountered withHelpful Rejection.
As for the class Conjunct, although it yields good results with theother learners, the proportion of this class (0.4%, 5 instances only) is now probably toolow to obtain reliable results.A more interesting case is the class Affirmative Answer, which in TiMBL goes downmore than 10 points (from 93.61% to 82.42% F-score).
The tree in Figure 7 provides aclue to the reason for this.
When the NSU contains a yes-word (second branch of thetree) the class with the highest probability is now Plain Acknowledgment, instead ofPlain Affirmative Answer as before (see tree in Figure 3).
This is due to the fact that,Figure 9Node on a tree using NSU and antecedent features.420Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueTable 12Comparison of weighted F-scores.System Weighted F-scoreMajority class baseline 35.31One rule baseline 53.03Four rule baseline (NSU features) 67.99NSU and antecedent features 85.44Full feature set:- J4.8 89.53- SLIPPER 92.01- TiMBL 92.02- MaxEnt 92.21at least in English, expressions like yeah (considered here as yes-words) are potentiallyambiguous between acknowledgments and affirmative answers.8 This ambiguity andthe problems it entails are also noted by Schlangen (2005), who addresses the problemof identifying NSUs automatically.
As he points out, the ambiguity of yes-words isone of the difficulties encountered when trying to distinguish between backchannels(plain acknowledgments in our taxonomy) and non-backchannel fragments.
This is atricky problem for Schlangen as his NSU identification procedure does not have accessto the context.
Although in the present experiments we do use features that capturecontextual information, determining whether the antecedent utterance is declarative orinterrogative (which one would expect to be the best clue to disambiguate between PlainAcknowledgement and Plain Affirmative Answer) is not always trivial.5.
ConclusionsIn this article we have presented results of several machine learning experiments wherewe have used well-known machine learning techniques to address the novel task ofclassifying NSUs in dialogue.We first introduced a comprehensive NSU taxonomy based on corpus work carriedout using the dialogue transcripts of the BNC, and then sketched the approach to NSUresolution we assume.We then presented a pilot study focused on sluices, one of the NSU classes in ourtaxonomy.
We analyzed different sluice interpretations and their distributions in a smallcorpus study and reported on a machine learning experiment that concentrated onthe task of disambiguating between sluice readings.
This showed that the observedcorrelations between sluice type and preferred interpretation are a good rough guidefor predicting sluice readings, which yields a 72% weighted F-score.
Using a small setof features that refer to properties of the antecedent utterance, we were able to improvethis result by 8%.In the second part of this article we extended the machine learning approach usedin the sluicing experiment to the full range of NSU classes in our taxonomy.
In orderto work with a more balanced set of data, the first run of this second experiment wascarried out using a restricted data set that excluded the classes Plain Acknowledgment8 Arguably this ambiguity would not arise in French given that, according to Beyssade (2005), in Frenchthe expressions used to acknowledge an assertion are different from those used in affirmative answers topolar questions.421Computational Linguistics Volume 33, Number 3and Check Question.
We identified a small set of features that capture properties ofthe NSUs, their antecedents and relations between them, and employed a series ofsimple baseline methods to evaluate the classification task.
The most successful of theseconsists of a four-rule decision tree that only uses features related to properties ofthe NSUs themselves.
This gives a 62% weighted F-score.
Not surprisingly, with thisbaseline very high scores (over 95%) could be obtained for NSU classes that are definedin terms of lexical or construction types, like Sluice and Propositional/Factual Modifier.We then applied four learning algorithms to the data set annotated with all featuresand improved the result of the four-rule baseline by 25%, obtaining a weighted F-scoreof around 87% for all learners.
The experiment showed that the classes that are mostdifficult to identify are those that rely on relational features, like Repeated AffirmativeAnswer and especially Helpful Rejection.In a second run of the experiment we incorporated the instances classified as PlainAcknowledgment and Check Question in the data set and ran the machine learnersagain.
The results achieved are very similar to those obtained in the previous run, ifslightly higher due to the high probability of the class Plain Acknowledgment.
Theexperiment did show however a potential confusion between Plain Acknowledgmentand Plain Affirmative Answer (observed elsewhere in the literature) that obviously hadnot shown up in the previous run.As typically different NSU classes are subjected to different resolution constraints,identifying the correct NSU class is a necessary step towards the goal of fully processingNSUs in dialogue.
Our results show that, for the taxonomy we have considered, this taskcan be successfully learned.There are, however, several aspects that deserve further investigation.
One of themis the choice of features employed to characterize the utterances.
In this case we haveopted for rather high-level features instead of using simple surface features, as is com-mon in robust approaches to language understanding.
As pointed out by an anonymousreviewer, it would be worth exploring to what extent the performance of our currentapproach could be improved by incorporating more low-level features, for instance bythe presence of closed-class function words.Besides identifying the right NSU class, the processing and resolution of NSUs in-volves other tasks that have not been addressed in this article and that are subjects of ourfuture research.
For instance, we have abstracted here from the issue of distinguishingNSUs from other sentential utterances.
In our experiments the input fed to the learnerswas in all cases a vector of features associated with an utterance that had already beensingled out as an NSU.
Deciding whether an utterance is or is not an NSU is not an easytask.
This has for instance been addressed by Schlangen (2005), who obtains rather lowscores (42% F-measure).
There is therefore a lot of room for improvement in this respect,and indeed in the future we plan to explore ways of combining the classification taskaddressed here with the NSU identification task.Identifying and classifying NSUs are necessary conditions for resolving them.
Inorder to actually resolve them, however, the output of the classifier needs to be fed intosome extra module that takes care of this task.
A route we plan to take in the futureis to integrate our classification techniques with the information state-based dialoguesystem prototype CLARIE (Purver 2004a), which implements a procedure for NSUresolution based on the theoretical assumptions sketched in Section 2.2.
The taxonomywhich we have tested and presented here will provide the basis for classifying NSUs inthis dialogue processing system.
The classification system will determine the templatesand procedures for interpretation that the system will apply to an NSU once it hasrecognized its fragment type.422Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueAppendix A: Detailed ML Results for the Sluice Reading Classification TaskLearner Sluice Reading Recall Precision F1Weka?s J4.8 Direct 71.70 79.20 75.20Reprise 85.70 83.70 84.70Clarification 100.00 68.60 81.40Wh anaphor 66.70 100.00 80.00weighted score 81.47 82.14 81.80SLIPPER Direct 81.01 71.99 76.23Reprise 83.85 86.49 85.15Clarification 71.17 94.17 81.07Wh anaphor 77.78 62.96 69.59weighted score 81.81 81.43 81.62TiMBL Direct 78.72 75.24 76.94Reprise 83.08 83.96 83.52Clarification 75.83 68.83 72.16Wh anaphor 55.56 77.78 64.81weighted score 79.85 79.98 79.80MaxEnt Direct 65.22 75.56 70.01Reprise 85.74 76.38 80.79Clarification 89.17 70.33 78.64Wh anaphor 0.00 0.00 0.00weighted score 75.38 76.93 73.24423Computational Linguistics Volume 33, Number 3Appendix B: Detailed ML Results for the Restricted NSU Classification TaskWeka?s J4.8 SLIPPERNSU Class Recall Precision F1 Recall Precision F1CE 97.00 97.00 97.00 93.64 97.22 95.40Sluice 100.00 95.20 97.60 96.67 91.67 94.10ShortAns 89.60 82.60 86.00 83.93 82.91 83.41AffAns 92.00 95.80 93.90 93.13 91.63 92.38Reject 95.80 80.70 87.60 83.60 100.00 91.06RepAffAns 68.00 63.00 65.40 53.33 61.11 56.96RepAck 85.00 89.50 87.20 85.71 89.63 87.62HelpReject 22.20 33.30 26.70 28.12 20.83 23.94PropMod 100.00 100.00 100.00 100.00 90.00 94.74FactMod 100.00 100.00 100.00 100.00 100.00 100.00BareModPh 80.00 100.00 88.90 100.00 80.56 89.23ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00Filler 56.30 100.00 72.00 100.00 62.50 76.92weighted score 87.62 87.68 87.29 86.21 86.49 86.35TiMBL MaxEntNSU Class Recall Precision F1 Recall Precision F1CE 94.37 91.99 93.16 96.11 96.39 96.25Sluice 94.17 91.67 92.90 100.00 95.83 97.87ShortAns 88.21 83.00 85.52 89.35 83.59 86.37AffAns 92.54 94.72 93.62 92.79 97.00 94.85Reject 95.24 81.99 88.12 100.00 81.13 89.58RepAffAns 63.89 60.19 61.98 68.52 65.93 67.20RepAck 86.85 91.09 88.92 84.52 81.99 83.24HelpReject 35.71 45.24 39.92 5.56 77.78 10.37PropMod 90.00 100.00 94.74 100.00 100.00 100.00FactMod 97.22 100.00 98.59 97.50 100.00 98.73BareModPh 80.56 100.00 89.23 69.44 100.00 81.97ConjFrag 100.00 100.00 100.00 100.00 100.00 100.00Filler 48.61 91.67 63.53 62.50 90.62 73.98weighted score 86.71 87.25 86.66 87.11 88.41 87.75424Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueAppendix C: Detailed ML Results for the Full NSU Classification TaskWeka?s J4.8 SLIPPERNSU Class Recall Precision F1 Recall Precision F1Ack 95.00 96.80 95.90 96.67 95.71 96.19CheckQu 100.00 83.30 90.90 86.67 100.00 92.86CE 92.40 95.30 93.80 96.33 93.75 95.02Sluice 100.00 95.20 97.60 94.44 100.00 97.14ShortAns 83.00 80.70 81.90 85.25 84.46 84.85AffAns 86.00 82.70 84.30 82.79 87.38 85.03Reject 100.00 76.20 86.50 77.60 100.00 87.39RepAffAns 68.00 65.40 66.70 67.71 72.71 70.12RepAck 86.30 84.10 85.20 84.04 92.19 87.93HelpReject 33.30 46.20 38.70 29.63 18.52 22.79PropMod 60.00 100.00 75.00 100.00 100.00 100.00FactMod 91.30 100.00 95.50 100.00 100.00 100.00BareModPh 70.00 100.00 82.40 83.33 69.44 75.76ConjFrag 100.00 71.40 83.30 100.00 100.00 100.00Filler 37.50 50.00 42.90 70.00 56.33 62.43weighted score 89.67 89.78 89.53 91.57 92.70 92.01TiMBL MaxEntNSU Class Recall Precision F1 Recall Precision F1Ack 95.71 95.58 95.64 95.54 94.59 95.06CheckQu 77.78 71.85 74.70 63.89 85.19 73.02CE 93.32 94.08 93.70 88.89 94.44 91.58Sluice 100.00 94.44 97.14 88.89 94.44 91.58ShortAns 87.79 88.83 88.31 88.46 84.91 86.65AffAns 85.00 85.12 85.06 86.83 81.94 84.31Reject 98.33 80.28 88.39 100.00 78.21 87.77RepAffAns 58.70 55.93 57.28 69.26 62.28 65.58RepAck 86.11 80.34 83.12 86.95 77.90 82.18HelpReject 22.67 40.00 28.94 00.00 00.00 00.00PropMod 100.00 100.00 100.00 44.44 100.00 61.54FactMod 97.50 100.00 98.73 93.33 100.00 96.55BareModPh 69.44 83.33 75.76 58.33 100.00 73.68ConjFrag 100.00 100.00 100.00 00.00 00.00 00.00Filler 44.33 55.00 49.09 62.59 100.00 76.99weighted score 91.49 90.75 91.02 91.96 93.17 91.21425Computational Linguistics Volume 33, Number 3AcknowledgmentsThis work was funded by grantRES-000-23-0065 from the Economic andSocial Council of the United Kingdom and itwas undertaken while all three authors weremembers of the Department of ComputerScience at King?s College London.
We wishto thank Lief Arda Nielsen and MatthewPurver for useful discussion and suggestionsregarding machine learning algorithms.
Weare grateful to two anonymous reviewers forvery helpful comments on an earlier draft ofthis article.
Their insights and suggestionshave resulted in numerous improvements.Of course we remain solely responsible forthe ideas presented here, and for any errorsthat may remain.ReferencesBeyssade, Claire and Jean-Marie Marandin.2005.
Contour meaning and dialoguestructure.
Paper presented at theworkshop Dialogue Modelling andGrammar, Paris, France.Burnard, Lou.
2000.
Reference Guide for theBritish National Corpus (World Edition).Oxford University Computing Services.Available from ftp://sable.ox.ac.uk/pub/ota/BNC/.Clark, Herbert H. 1996.
Using Language.Cambridge University Press, Cambridge.Cohen, William and Yoram Singer.
1999.A simple, fast, and effective rule learner.In Proceedings of the 16th NationalConference on Artificial Intelligence,pages 335?342, Orlando, FL.Daelemans, Walter, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
2003.TiMBL: Tilburg Memory-Based Learner,v.
5.0, Reference Guide.
TechnicalReport ILK-0310, University ofTilburg.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Ferna?ndez, Raquel.
2006.
Non-SententialUtterances in Dialogue: Classification,Resolution and Use.
Ph.D. thesis,Department of Computer Science, King?sCollege London, University of London.Ferna?ndez, Raquel and Jonathan Ginzburg.2002.
Non-sentential utterances: A corpusstudy.
Traitement automatique des languages,43(2):13?42.Ferna?ndez, Raquel, Jonathan Ginzburg,Howard Gregory, and Shalom Lappin.In press.
SHARDS: Fragment resolutionin dialogue.
In H. Bunt and R. Muskens,editors, Computing Meaning, volume 3.Kluwer, Amsterdam.Ferna?ndez, Raquel, Jonathan Ginzburg, andShalom Lappin.
2004.
Classifying Ellipsisin Dialogue: A Machine LearningApproach.
In Proceedings of the 20thInternational Conference on ComputationalLinguistics, pages 240?246, Geneva,Switzerland.Garside, Roger.
1987.
The CLAWSword-tagging system.
In R. Garside,G.
Leech, and G. Sampson, editors, TheComputational Analysis of English: ACorpus-Based Approach.
Longman, Harlow,pages 30?41.Ginzburg, Jonathan.
1996.
Interrogatives:Questions, facts, and dialogue.
In ShalomLappin, editor, Handbook of ContemporarySemantic Theory.
Blackwell, Oxford,pages 385?422.Ginzburg, Jonathan.
1999.
Ellipsis resolutionwith syntactic presuppositions.
InH.
Bunt and R. Muskens, editors,Computing Meaning: Current Issues inComputational Semantics.
Kluwer,Amsterdam, pages 255?279.Ginzburg, Jonathan.
2005.
Abstraction andontology: Questions as propositionalabstracts in type theory with records.Journal of Logic and Computation,2(15):113?118.Ginzburg, Jonathan.
forthcoming.
Semanticsand Interaction in Dialogue.
CSLIPublications and University of ChicagoPress, Stanford, California.
Draft chaptersavailable from http://www.dcs.kcl.ac.uk/staff/ginzburg.Ginzburg, Jonathan and Robin Cooper.2004.
Clarification, ellipsis, and the natureof contextual updates.
Linguistics andPhilosophy, 27(3):297?366.Ginzburg, Jonathan, Howard Gregory,and Shalom Lappin.
2001.
SHARDS:Fragment resolution in dialogue.
InH.
Bunt, I. van der Suis, and E. Thijsse,editors, Proceedings of the FourthInternational Workshop on ComputationalSemantics, pages 156?172, Tilburg,The Netherlands.Ginzburg, Jonathan and Ivan Sag.
2001.Interrogative Investigations.
CSLIPublications, Stanford, California.Larsson, Staffan.
2002.
Issue-based DialogueManagement.
Ph.D. thesis, Go?teborgUniversity, Sweden.Larsson, Staffan, Peter Ljunglo?f, RobinCooper, Elisabet Engdahl, and StinaEricsson.
2000.
GoDiS: An accommodatingdialogue system.
In Proceedings of426Ferna?ndez, Ginzburg, and Lappin Classifying NSUs in DialogueANLP/NAACL-2000 Workshop onConversational Systems, pages 7?10,Seattle, WA.Le, Zhang.
2003.
Maximum entropymodeling toolkit for Python andC++.
Available from http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.Leech, Geoffrey, Roger Garside, andMichael Bryant.
1994.
The large-scalegrammatical tagging of text: Experiencewith the British National Corpus.
InN.
Oostdijk and P. de Haan, editors,Corpus-Based Research into Language.Rodopi, Amsterdam, pages 47?63.Malouf, Robert.
2002.
A comparision ofalgorithm for maximum entropyparameter estimation.
In Proceedingsof the Sixth Conference on NaturalLanguage Learning, pages 49?55, Taipei,Taiwan.Purver, Matthew.
2001.
SCoRE: A tool forsearching the BNC.
Technical ReportTR-01-07, Department of ComputerScience, King?s College London.Purver, Matthew.
2004a.
CLARIE: TheClarification Engine.
In J. Ginzburg andE.
Vallduv?
?, editors, Proceedings of the 8thWorkshop on the Semantics and Pragmatics ofDialogue (Catalog), pages 77?84, Barcelona,Spain.Purver, Matthew.
2004b.
The Theory andUse of Clarification Requests in Dialogue.Ph.D.
thesis, King?s College, Universityof London.Schlangen, David.
2003.
A Coherence-BasedApproach to the Interpretation ofNon-Sentential Utterances in Dialogue.
Ph.D.thesis, University of Edinburgh, Scotland.Schlangen, David.
2005.
Towards finding andfixing fragments: Using ML to identifynon-sentential utterances and theirantecedents in multi-party dialogue.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics,pages 247?254, Ann Arbor, MI.Schlangen, David and Alex Lascarides.
2003.The interpretation of non-sententialutterances in dialogue.
In Proceedings of the4th SIGdial Workshop on Discourse andDialogue, pages 62?71, Sapporo, Japan.Traum, David.
1994.
A Computational Theoryof Grounding in Natural LanguageConversation.
Ph.D. thesis, University ofRochester, Department of ComputerScience, Rochester, NY.Witten, Ian H. and Eibe Frank.
2000.Data Mining: Practical MachineLearning Tools with Java Implementations.Morgan Kaufmann, San Francisco.Available from http://www.cs.waikato.ac.nz/ml/weka.427
