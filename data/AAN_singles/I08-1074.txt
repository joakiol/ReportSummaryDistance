Benchmarking Noun Compound InterpretationSu Nam Kim and Timothy BaldwinDepartment of Computer Science and Software EngineeringandNICTA Victoria LabUniversity of Melbourne, VIC 3010 Australia{snkim,tim}@csse.unimelb.edu.auAbstractIn this paper we provide benchmark resultsfor two classes of methods used in inter-preting noun compounds (NCs): semanticsimilarity-based methods and their hybrids.We evaluate the methods using 7-way andbinary class data from the nominal pair in-terpretation task of SEMEVAL-2007.1 Wesummarize and analyse our results, withthe intention of providing a framework forbenchmarking future research in this area.1 IntroductionThis paper reviews a range of simple and hybridapproaches to noun compound (NC) interpretation.The interpretation of NCs such as computer scienceand paper submission involves predicting the se-mantic relation (SR) that underlies a given NC.
Forexample, student price conventionally expresses themeaning that a student benefits from the price (SR= BENEFICIARY), while student protest conven-tionally means a student undertaking a protest (SR= AGENT).2NCs are formed from simplex nouns with highproductivity.
The huge number of possible NCs andpotentially large number of SRs makes NC interpre-tation a very difficult problem.
In the past, much NCinterpretation work has been carried out which tar-gets particular NLP applications such as informationextraction, question-answering and machine trans-lation.
Unfortunately, much of it has not gained1The 4th International Workshop on Semantic Evaluation2SRs used in the examples are taken from Barker and Sz-pakowicz (1998).traction in real-world applications as the accuracyof the methods has not been sufficiently high overopen-domain data.
Most prior work has been car-ried out under specific assumptions and with one-off datasets, which makes it hard to analyze perfor-mance and to build hybrid methods.
Additionally,disagreement in the inventory of SRs and a lack ofresource sharing has hampered comparative evalua-tion of different methods.The first step in NC interpretation is to define a setof SRs.
Levi (1979), for example, proposed a systemof 9 SRs, while others have proposed classificationswith 20-30 SRs (Finin, 1980; Barker and Szpakow-icz, 1998; Moldovan et al, 2004).
Smaller sets tendto have reduced coverage due to coarse granularity,whereas larger sets tend to be too fine grained andsuffer from low inter-annotator agreement.
Addi-tionally pragmatic/contextual differentiation leads todifficulties in defining and interpreting SRs (Down-ing, 1977; SparckJones, 1983).Recent attempts in the area of NC interpretationhave taken two basic approaches: analogy-base in-terpretation (Rosario, 2001; Moldovan et al, 2004;Kim and Baldwin, 2005; Girju, 2007) and seman-tic disambiguation relative to an underlying predi-cate or semantically-unambiguous paraphrase (Van-derwende, 1994; Lapata, 2002; Kim and Baldwin,2006; Nakov, 2006).
Most methods employ rich on-tologies and ignore the context of use, supportingthe claim by Fan (2003) that axioms and ontologicaldistinctions are more important than detailed knowl-edge of specific nouns for NC interpretation.
Addi-tionally, most approaches use supervised learning,raising questions about the generality of the test and569training data sets and the effectiveness of the algo-rithms in different domains (coverage of SRs overthe NCs is another issue).Our aim in this paper is to compare and analyzeexisting NC interpretation methods over a common,publicly available dataset.
While recent researchhas made significant progress, bringing us one stepcloser to practical applicability in NLP applications,no direct comparison or analysis of the approacheshas been attempted to date.
As a result, it is hard todetermine which approach is appropriate in a givendomain or build hybrid methods based on prior ap-proaches.
We also investigate the impact on perfor-mance of relaxing assumptions made in the origi-nal research, to compare different approaches in anidentical setting.In the remainder of the paper, we review the re-search background and NC interpretation methodsin Section 2, describe the methods and system archi-tectures in Section 3, detail the datasets used in ourexperiments in Section 4, carry out a system evalu-ation in Section 5 and Section 6, and finally presenta discussion and conclusions in Section 7 and Sec-tion 8, respectively.2 Background and Methods2.1 Research BackgroundIn this study, we selected three semantic similar-ity based models which had been found to performstrongly in previous research, and which were easyto re-implement: SENSE COLLOCATION (Moldovanet al, 2004), CONSTITUENT SIMILARITY (Kimand Baldwin, 2005) and CO-TRAINING, e.g.
usingSENSE COLLOCATION or CONSTITUENT SIMILAR-ITY (Kim and Baldwin, 2007).
These approacheswere evaluated over a 7-way classification usingopen-domain data from the nominal pair interpre-tation task of SEMEVAL-2007 (Girju et al, 2007).We test their performance in both 7-way and binary-class classification settings.2.2 Sense Collocation MethodThe SENSE COLLOCATION method of Moldovan etal.
(2004) is based on the pair of word senses of NCconstituents.
The basic idea is that NCs which havethe same or similar sense collocation tend to havethe same SR. As an example, car factory and auto-mobile factory share the conventional interpretationof MAKE, which is predicted by car and automo-bile having the same sense across the two NCs, andfactory being used with the same sense in each in-stance.
This intuition is formulated in Equations 1and 2 below.The probability P (r|fifj) (simplified toP (r|fij)) of a SR r for word senses fi and fjis calculated based on simple maximum likelihoodestimation:P (r|fij) = n(r, fij)n(fij) (1)The preferred SR r?
for the given sense combina-tion is that which maximises the probability:r?
= argmaxr?RP (r|fij)= argmaxr?RP (fij |r)P (r) (2)2.3 Constituent Similarity MethodThe intuition behind the CONSTITUENT SIMILAR-ITY method is similar to the SENSE COLLOCATIONmethod, in that NCs made up of similar words tendto share the same SR.
The principal difference is thatit doesn?t presuppose that we know the word senseof each constituent word (i.e.
the similarity is cal-culated at the word rather than sense level).
Themethod takes the form of a 1-nearest neighbour clas-sifier, with the best-matching training instance foreach test instance predicting its SR. For example,we may find that test instance chocolate milk mostclosely matches apple juice and hence predict thatthe SR is MATERIAL.This idea is formulated in Equation 3 below.
For-mally, SA is the similarity between NCs (Ni,1, Ni,2)and (Bj,1, Bj,2):SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =((?S1 + S1)?
((1?
?
)S2 + S2))2 (3)where S1 is the modifier similarity (i.e.S(Ni,1, Bj1)) and S2 is the head noun similarity(i.e.
S(Ni,2, Bj2)); ?
?
[0, 1] is a weighting factor.The similarity scores are calculated across the bagof WordNet senses (without choosing between570them) using the method of Wu and Palmer (1994) asimplemented in WordNet::Similarity (Pat-wardhan et al, 2003).
This is done for each pairingof WordNet senses of the two words in question,and the overall lexical similarity is calculated as theaverage across the pairwise sense similarities.2.4 Co-Training by Sense CollocationCo-training by sense collocation (SCOLL CO-TRAINING) is based on the SENSE COLLOCATIONmethod and lexical substitution (Kim and Baldwin,2007).
It expands the set of training NCs froma relatively small number of manually-tagged seedinstances.
That is, it makes use of extra train-ing instances fashioned through a bootstrap process.For example, assuming automobile factory with theSR MAKE were a seed instance, NCs generatedfrom synonyms, hypernyms and sister words of itsconstituents would be added as extra training in-stances, with the same SR of MAKE.
That is, wewould add car factory (SYNONYM), vehicle fac-tory (HYPERNYM) and truck factory (SISTERWORD), for example.
Note that the substitutiontakes place only for one constituent at a time to avoidextreme variation.2.5 Co-training by Constituent SimilarityCo-training by Constituent Similarity (CS CO-TRAINING) is also a co-training method, but basedon CONSTITUENT SIMILARITY rather than SENSECOLLOCATION.
The basic idea is that when NCsare interpreted using the CONSTITUENT SIMILAR-ITY method, the predictions are more reliable whenthe lexical similarity is higher.
Hence, we progres-sively reduce the similarity threshold, and incorpo-rate higher-similarity instances into our training dataearlier in the bootstrap process.
That is, we runthe CONSTITUENT SIMILARITY method and acquireNCs with similarity equal to or greater than a fixedthreshold.
Then in the next iteration, we add the ac-quired NCs into the training dataset for use in clas-sifying more instances.
As a result, in each step,the number of training instances increases monoton-ically.
We ?cascade?
through a series of decreas-ing similarity thresholds until we reach a saturationpoint.
As our threshold, we used a starting value of0.90, which was decremented down to 0.65 in stepsof 0.05.Method DescriptionSCOLL sense collocationSCOLLCT sense collocation + SCOLL co-trainingCSIM constituent similarityCSIM +SCOLLCT constituent similarity + SCOLL co-trainingHYBRID SCOLL + CSIM + SCOLLCTCSIMCT constituent similarity + CSIM co-trainingTable 1: Systems used in our experimentsTESTuntagged test datauntagged test datauntagged test datauntagged test datatagged datatagged datatagged datatagged datatagged dataTRAINExtension ofTraining databy similar words?
Synonym?
Hypernym?
Sister wordExtended TRAINSense CollcationStep 1SimilarityStep 2Step 3Step 4SimilarityStep 5Sense CollcationSimilarityFigure 1: Architecture of the HYBRID method3 Systems and ArchitecturesWe tested the original methods of Moldovan et al(2004) and Kim and Baldwin (2005), and combinedthem with the co-training methods of Kim and Bald-win (2007) to come up with six different hybrid sys-tems for evaluation, as detailed in Table 1.
To buildthe classifiers, we used the TIMBL5.0 memory-based learner (Daelemans et al, 2004).The HYBRID method consists of five interpreta-tion steps.
The first step is to use the SENSE COL-LOCATION method over the original training data.When the sense collocation of the test and train-ing instances is the same, we judge the predictedSR to be correct.
The second step is to apply theCONSTITUENT SIMILARITY method over the origi-nal training data.
In order to confirm that the pre-dicted SR is correct, we use a threshold of 0.8 tointerpret the test instances.
The third step is to ap-ply SENSE COLLOCATION over the expanded train-571TRAIN#of Tagged>= 10% of testThresholdTaggedfinalize currenttags and endreduce ThresholdTESTget SimilaritySim >= TN YYNif T == 0.6 &(#of Tagged <10% of test)NYFigure 2: Architecture of the CSIMCT systeming data through the advent of hypernyms and sis-ter words, using the SCOLL CO-TRAINING method.This step benefits from a larger amount of trainingdata (17,613 vs. 937).
The fourth step is to applythe CONSTITUENT SIMILARITY method (EXTCS)over the consolidated training data, with the thresh-old unchanged at 0.8.
The final step is to apply theCONSTITUENT SIMILARITY (CSTT) method overthe combined training data without any restrictionon the threshold (to guarantee a SR prediction forevery test instance).
We select SRs from the traininginstances whose similarity is higher than the origi-nal training data and expanded training data.
How-ever, since the generated training instances are morelikely to contain errors, we apply a linear weight of0.8 to the similarity values for the expanded train-ing instances.
This gives preferential treatment topredictions based on the original training instances.Note that this weight was based on analysis of theerror rate in the expanded training instances.
In pre-vious work (Kim and Baldwin, 2007), we found theoverall classification accuracy rate after the first it-eration to be 70-80%.
Hence, we settled on a weightof 0.8.The CSIMCT system is based solely on the CON-STITUENT SIMILARITY method with cascading.
Weperform iterative CS co-training as described in Sec-tion 2.5, with the slight variation that we hold offBinary 7-waySR Test Train Train* Test Train Train*CE 80 136 2,588 36 71 1,854IA 78 135 1,400 36 68 1,001PP 93 126 2,591 55 78 2,089OE 81 136 3,085 35 52 1,560TT 71 129 2,994 27 50 1,718PW 72 138 2,577 28 64 1,510CC 74 137 2,378 37 63 1,934Total 549 937 17,613 254 446 11,664Table 3: Number of instances associated with eachSR (Train* is the number of expanded train in-stances)on reducing the threshold if less than 10% of thetest instances are tagged on a given iteration, givingother test instances a chance to be tagged at a higherthreshold level relative to newly generated traininginstances.
The residue of test instances on comple-tion of the final iteration (threshold = 0.6) are taggedaccording to the best-matching training instance, ir-respective of the magnitude of the similarity.4 DataWe used the dataset from the SEMEVAL-2007nominal pair interpretation task, which is basedon 7 SRs: CAUSE-EFFECT (CE), INSTRUMENT-AGENCY (IA), PRODUCT-PRODUCER (PP),ORIGIN-ENTITY (OE), THEME-TOOL (TT),PART-WHOLE (PW), CONTENT-CONTAINER(CC).
The task in SEMEVAL-2007 was to identifythe compatibility of a given SR for each testinstances using word senses retrieved from WORD-NET 3.0 (Fellbaum, 1998) and queries.
Table 2shows the definition of the SRs.In our research, we interpret the dataset in twoways: (1) as a binary classification task for each SRbased on the original data; and (2) as a 7-way clas-sification task, combining together all positive testand training instances for each of the 7 SR datasetsinto a single dataset.
Hence, the size of the datasetfor 7-way classification is much smaller than that ofthe original dataset.
We also expand the training in-stances using SCOLL CO-TRAINING.
Table 3 de-scribes the number of test and train instances for NCinterpretation for the binary and 7-way classificationtasks.Our analysis shows that only 5 NCs are repeated572Semantic relation Definition ExamplesCause-Effect (CE) N1 is the cause of N2 virus flu, hormone growthInstrument-Agency (IA) N1 is the instrument of N2; N2 uses N1 laser printer, axe murdererProduct-Producer (PP) N1 is a product of N2; N2 produces N1 honey bee, music clockOrigin-Entity (OE) N1 is the origin of N2 bacon grease, desert stormTheme-Tool (TT) N2 is intended for N1 reorganization process, copyright lawPart-Whole (PW) N1 is part of N2 table leg, daisy flowerContent-Container (CC) N1 is stored or carried inside N2 apple basket, wine bottleTable 2: The set of 7 semantic relations, where N1 is the modifier and N2 is the head nounacross multiple SR datasets (i.e.
occur as an instancein more than one of the 7 datasets), none of whichoccur as positive instances for multiple SRs.
Assuch, no NC instances in the 7-way classificationtask end up with a multiclass classification.
Alsonote that some of NCs are contained within ternaryor higher-order NCs: 40 test NCs and 81 trainingNCs for the binary classification task, and 24 testNCs and 42 training NCs for the 7-way classificationtask.
For these NCs, we extracted a ?base?
binaryNC based on the provided bracketing.
The follow-ing are examples of extraction of binary NCs fromternary or higher-order NCs.
((billiard table) room) ?
table room(body (bath towel)) ?
body towelIn order to extract a binary NC, we take the headnoun of each embedded NC and combine this withthe corresponding head noun or modifier.
E.g., tableis the head noun of billiard table, which combineswith the head noun of the complex NC room to formtable room.5 Experiment 1: 7-way classificationOur first experiment was carried out over the 7-wayclassification task?i.e.
all 7 SRs in a single classifi-cation task?using the 6 systems from Section 3.
Inour results in Table 4, we use the system categoriesfrom SEMEVAL-2007 of A4 and B4, where A4 sys-tems use none of the provided word senses, and B4systems use the word senses.3 We categorized oursystems into these two groups in order to evaluatethem separately within the bounds of the originalSEMEVAL-2007 task.
In each case, the baseline isa majority class classifier.3In the original SEMEVAL-2007 task, there were two fur-ther categories, which incorporated the ?query?
with or withoutthe sense information.Class Method P R F1 A?
Majority .217A4 CSIM .518 .522 .449 .528CSIMCT .517 .511 .426 .522B4 SCOLL .705 .444 .477 .496SCOLLCT .646 .466 .498 .508CSIM +SCOLLCT .523 .520 .454 .528HYBRID .500 .505 .416 .516Table 4: Experiment 1: Results (P=precision,R=recall, F1=F-score, A=accuracy)Step Method Tagged Ai Untagged1 SCOLL 12 1.000 2422 CSIM 57 .719 1853 extSCOLL 0 .000 1854 extCSIM 78 .462 1075 CSIMREST 107 .393 0Table 5: Experiment 1: Classifications for eachstep of the HYBRID method (CSREST=the final ap-plication of CS over the remaining test instances,Ai=accuracy for classifications made at step i)Tables 5 and 6 show the results at each step forthe HYBRID and CSIMCT methods, respectively.
Aseach method proceeds, the amount of tagged data in-creases but the classification accuracy of the systemdecreases, due to the inclusion of increasingly noisytraining instances in the previous step.
The perfor-mance of each individual relation is shown in Fig-ure 3, which largely mirrors the findings of the sys-tems in the original SEMEVAL-2007 task in termsof the relative difficulty to predict each of the 7 SRs.6 Experiment 2: binary classificationIn the second experiment, we performed a separatebinary classification task for each of the 7 SRs, inthe manner of the original SEMEVAL-2007 task.Table 7 shows the three baselines provided by theSEMEVAL-2007 organisers and performance of our573Iteration ?
Tagged Ai Untagged1 .90 29 .897 2252 .85 12 .750 2133 .80 31 .613 1824 .75 43 .535 1395 .70 63 .540 766 .65 26 .346 507 <.65 49 .250 1Table 6: Experiment 1: Classifications at each stepof the CSIMCT method (?=threshold, Ai=accuracyfor classifications made at iteration i)CE IA OEPP TT PW CCRelationsAccuracy(%)KE w/ multiple classes020406080100 precisionrecallFscoreFigure 3: Experiment 1: Performance over each SR(CSIM +SCOLLCT method)6 systems.
We also present the best-performing sys-tem within each group from the SEMEVAL-2007task.
The methods for computing the baselines aredescribed in Girju et al (2007).As with the first experiment, we analyzed thenumber of tagged instances and accuracy for the HY-BRID and CSIMCT methods, as shown in Tables 8and 9, respectively.
The overall results are similar tothose for the 7-way classification task.Figures 4 and 5 show the performance for posi-tive and negative classifications for each individualSR.
The performance when the classifier outputs aremapped onto the 7-way classification task are simi-lar to those in Figure 3.7 Discussion and ConclusionWe compared the performance of the 6 systems inTables 4 and 7 over the 7-way and binary clas-sification tasks, respectively.
The performance ofall methods exceeded the baseline.
The CON-STITUENT SIMILARITY (CSIM) system performedthe best in group A4 and CONSTITUENT SIMILAR-Class Method P R F1 A?
All True .485 1.000 .648 .485?
Probability .485 .485 .485 .517?
Majority .813 .429 .308 .570A4 Best .661 .667 .648 .660CSIM .632 .628 .627 .650CSIMCT .615 .557 .578 .627B4 Best .797 .698 .724 .763SCOLL .672 .584 .545 .634SCOLLCT .602 .571 .554 .619CSIM +SCOLLCT .660 .657 .654 .669HYBRID .617 .568 .587 .625Table 7: Experiment 2: Binary classification results(P=precision, R=recall, F1=F-score, A=accuracy)Step Method Tagged Ai Untagged1 SCOLL 21 .810 5262 CSIM 106 .689 4203 extSCOLL 0 .000 4204 extCSIM 61 .607 3595 CSIMREST 359 .619 0Table 8: Experiment 2: Classifications for eachstep of the HYBRID method (CSREST=the final ap-plication of CS over the remaining test instances,Ai=accuracy for classifications made at step i)ITY + SCOLLCT (CSIM +SCOLLCT ) system per-formed the best in group B4 for both classificationtasks.
In general, the performance of CONSTITUENTSIMILARITY is marginally better than that of SENSECOLLOCATION.
Also, the utility of co-training isconfirmed by it outperforming both CONSTITUENTSIMILARITY and SENSE COLLOCATION.In order to compare the original methods withthe hybrid methods, we observed that the originalmethods, SCOLL and K, and their co-training vari-ants performed consistently better than the hybridmethods, HYBRID and CSIMCT .
We found that thecombination of the methods lowers overall perfor-mance.
We also found that the number of traininginstances contributes to improved performance, pre-dictably in the sense that the methods are supervised,but encouraging in the sense that the extra trainingdata is generated automatically.
As expected, thestep-wise performance of HYBRID and CSIMCT de-grades with each iteration, although there were in-stances where the performance didn?t drop from oneiteration to the next (e.g.
iteration 3 = 59.46% vs. it-eration 4 = 72.23% in Experiment 2).
This confirms574Iteration ?
Tagged Ai Untagged1 .90 21 .810 5262 .85 52 .726 4743 .80 56 .714 4184 .75 74 .595 3445 .70 101 .722 2436 .65 222 .572 217 <.65 21 .996 0Table 9: Experiment 2: Classifications at each stepof the CSIMCT method (?=threshold, Ai=accuracyfor classifications made at iteration i)CE IA PP OE TT PW CCrelationsAccuracy(%)KE w/ binary classes & tagged as "true"Fscorerecallprecision020406080100Figure 4: TPR for each SR for the binary task (pos-itive instances, CSIM +SCOLLCT method)our expectation that: (a) the similarity threshold isstrongly correlated with the quality of the resultantdata; and (b) the method is susceptible to noisy train-ing data.Our performance comparison over the binaryclassification task from the SEMEVAL-2007 taskshows that our 6 systems performed below the bestperforming system in the competition, to varying de-grees.
This is partly because the methods were origi-nally designed for multi-way (positive) classificationand require adjustment for the binary task reformu-lation, although their performance is competitive.Finally, comparing the SCOLL and CSIM meth-ods, we found that the methods interpret SRs with100% accuracy when the sense collocations arefound in both the test and training data.
However,the CSIM method is more sensitive than the SCOLLmethod to variation in the sense collocations, whichleads to better performance.
Also, the CSIM methodinterprets NCs with high accuracy when the com-puted similarity is sufficiently high (e.g.
with simi-larity ?
0.9 the accuracy is 89.7%).
Another benefitCE IA PP OE TT PW CCRelationsAccuracy(%)KE w/ binary classes & tagged as "false"020406080100 precisionrecallFscoreFigure 5: TNR for each SR for the binary task (neg-ative instances, CSIM +SCOLLCT method)of this method is that it interprets NCs without wordsense information.
As a result, we conclude that theCSIM method is more flexible and robust.
One pos-sible weakness of CSIM is its reliance on the simi-larity measure.8 Conclusions and Future WorkIn this paper, we have benchmarked and hybridisedexisting NC interpretation methods over data fromthe SEMEVAL-2007 nominal pair interpretationtask.
In this, we have established guidelines for theuse of the different methods, and also for the rein-terpretation of the SEMEVAL-2007 data as a moreconventional multi-way classification task.
We con-firmed that CONSTITUENT SIMILARITY is the bestmethod due to its insensitivity to varied sense col-locations.
We also confirmed that co-training im-proves the performance of the methods by expand-ing the number of training instances.Looking to the future, there is room for improve-ment for all the methods through such factors asthreshold tweaking and expanding the training in-stances further.ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the Eighteenth International JointConference on Artificial Intelligence, pp.
805?810,Acapulco, Mexico.Ken Barker and Stan Szpakowicz.
1998.
Semi-automatic recognition of noun modifier relationships.In Proceedings of the 17th International Conference575on Computational Linguistics, pp.
96?102, Montreal,Canada.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
TiMBL: Tilburg Mem-ory Based Learner, version 5.1, Reference Guide.
ILKTechnical Report 04-02.Pamela Downing.
1977.
On the Creation and Use of En-glish Compound Nouns.
Language, 53(4):810?842.James Fan and Ken Barker and Bruce W. Porter.
2003.The knowledge required to interpret noun compounds.In In Proceedings of the 7th International Joint Con-ference on Artificial Intelligence, Acapulco, Mexico,1483?1485.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,USA.Timothy W. Finin.
1980.
The Semantic Interpretationof Compound Nominals.
Ph.D. thesis, University ofIllinois.Roxana Girju.
2007.
Improving the Interpretation ofNoun Phrases with Cross-linguistic Information.
InProceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pp.
568?575,Prague, Czech Republic.Roxana Girju and Preslav Nakov and Vivi Nastase andStan Szpakowicz and Peter Turney and Deniz Yuret.2007.
SemEval-2007 Task 04: Classification of Se-mantic Relations between Nominals.
In Proceedingsof the 4th Semantic Evaluation Workshop (SemEval-2007), Prague, Czech Republic, pp.13?18.Su Nam Kim and Timothy Baldwin.
2005.
Auto-matic interpretation of Noun Compounds using Word-Net similarity.
In Proceedings of the 2nd InternationalJoint Conference On Natural Language Processing,pp.
945?956, JeJu, Korea.Su Nam Kim and Timothy Baldwin.
2006.
InterpretingSemantic Relations in Noun Compounds via Verb Se-mantics.
In Proceedings of the 44th Annual Meetingof the Association for Computational Linguistics and21st International Conference on Computational Lin-guistics (COLING/ACL-2006).
pp.
491?498, Sydney,Australia.Su Nam Kim and Timothy Baldwin.
2007.
InterpretingNoun Compound Using Bootstrapping and Sense Col-location.
In Proceedings of the Pacific Association forComputational Linguistics (PACLING), pp.
129?136,Melbourne, Australia.Maria Lapata.
2002.
The disambiguation of nominaliza-tions.
Computational Linguistics, 28(3):357?388.Judith Levi.
1979.
The syntax and semantics of complexnominals.
In The Syntax and Semantics of ComplexNominals.
New York:Academic Press.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the se-mantic classification of noun phrases.
In Proceedingsof the HLT-NAACL 2004 Workshop on ComputationalLexical Semantics, pp.
60?67, Boston, USA.Preslav Nakov and Marti Hearst.
2006.
Using Verbs toCharacterize Noun-Noun Relations.
In Proceedings ofthe 12th International Conference on Artificial Intelli-gence: Methodology, Systems, Applications (AIMSA),Bularia.Diarmuid O?
Se?aghdha and Ann Copestake.
2007.
Co-occurrence Contexts for Noun Compound Interpre-tation.
In Proc.
of the ACL-2007 Workshop onA Broader Perspective on Multiword Expressions,Prague, Czech Republic.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2003.
Using measures of semantic related-ness for word sense disambiguation.
In Proceedingsof the Fourth International Conference on IntelligentText Processing and Computational Linguistics.Barbara Rosario and Hearst Marti.
2001.
Classify-ing the Semantic Relations in Noun Compounds viaa Domain-Specific Lexical Hierarchy.
In In Proceed-ings of the 6th Conference on Empirical Methods inNatural Language Processing (EMNLP-2001), 82?90.Karen Sparck Jones.
1983.
Compound noun interpre-tation problems.
Computer Speech Processing, FrankFallside and William A.
Woods, Prentice-Hall, Engle-wood Cliffs, NJ.Lucy Vanderwende.
1994.
Algorithm for automaticinterpretation of noun sequences.
In Proceedings ofthe 15th Conference on Computational linguistics, pp.782?788.Zhibiao Wu and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
In Proceedings of the 32ndAnnual Meeting of the Association for ComputationalLinguistics, pp.
133?138, Las Cruces, USA.576
