Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 217?222,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJudging Grammaticality with Tree Substitution Grammar DerivationsMatt PostHuman Language Technology Center of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211AbstractIn this paper, we show that local features com-puted from the derivations of tree substitutiongrammars ?
such as the identify of particu-lar fragments, and a count of large and smallfragments ?
are useful in binary grammaticalclassification tasks.
Such features outperformn-gram features and various model scores bya wide margin.
Although they fall short ofthe performance of the hand-crafted featureset of Charniak and Johnson (2005) developedfor parse tree reranking, they do so with anorder of magnitude fewer features.
Further-more, since the TSGs employed are learnedin a Bayesian setting, the use of their deriva-tions can be viewed as the automatic discov-ery of tree patterns useful for classification.On the BLLIP dataset, we achieve an accuracyof 89.9% in discriminating between grammat-ical text and samples from an n-gram languagemodel.1 IntroductionThe task of a language model is to provide a measureof the grammaticality of a sentence.
Language mod-els are useful in a variety of settings, for both humanand machine output; for example, in the automaticgrading of essays, or in guiding search in a machinetranslation system.
Language modeling has provedto be quite difficult.
The simplest models, n-grams,are self-evidently poor models of language, unableto (easily) capture or enforce long-distance linguis-tic phenomena.
However, they are easy to train, arelong-studied and well understood, and can be ef-ficiently incorporated into search procedures, suchas for machine translation.
As a result, the outputof such text generation systems is often very poorgrammatically, even if it is understandable.Since grammaticality judgments are a matter ofthe syntax of a language, the obvious approach formodeling grammaticality is to start with the exten-sive work produced over the past two decades inthe field of parsing.
This paper demonstrates theutility of local features derived from the fragmentsof tree substitution grammar derivations.
Follow-ing Cherry and Quirk (2008), we conduct experi-ments in a classification setting, where the task is todistinguish between real text and ?pseudo-negative?text obtained by sampling from a trigram languagemodel (Okanohara and Tsujii, 2007).
Our primarypoints of comparison are the latent SVM trainingof Cherry and Quirk (2008), mentioned above, andthe extensive set of local and nonlocal feature tem-plates developed by Charniak and Johnson (2005)for parse tree reranking.
In contrast to this latter setof features, the feature sets from TSG derivationsrequire no engineering; instead, they are obtaineddirectly from the identity of the fragments used inthe derivation, plus simple statistics computed overthem.
Since these fragments are in turn learned au-tomatically from a Treebank with a Bayesian model,their usefulness here suggests a greater potential foradapting to other languages and datasets.2 Tree substitution grammarsTree substitution grammars (Joshi and Schabes,1997) generalize context-free grammars by allow-ing nonterminals to rewrite as tree fragments of ar-bitrary size, instead of as only a sequence of one or217.S.NP .VP.VBD.said.NP .SBAR..Figure 1: A Tree Substitution Grammar fragment.more children.
Evaluated by parsing accuracy, thesegrammars are well below state of the art.
However,they are appealing in a number of ways.
Larger frag-ments better match linguists?
intuitions about whatthe basic units of grammar should be, capturing, forexample, the predicate-argument structure of a verb(Figure 1).
The grammars are context-free and thusretain cubic-time inference procedures, yet they re-duce the independence assumptions in the model?sgenerative story by virtue of using fewer fragments(compared to a standard CFG) to generate a tree.3 A spectrum of grammaticalityThe use of large fragments in TSG grammar deriva-tions provides reason to believe that such grammarsmight do a better job at language modeling tasks.Consider an extreme case, in which a grammar con-sists entirely of complete parse trees.
In this case,ungrammaticality is synonymous with parser fail-ure.
Such a classifier would have perfect precisionbut very low recall, since it could not generalizeat all.
On the other extreme, a context-free gram-mar containing only depth-one rules can basicallyproduce an analysis over any sequence of words.However, such grammars are notoriously leaky, andthe existence of an analysis does not correlate withgrammaticality.
Context-free grammars are too poormodels of language for the linguistic definition ofgrammaticality (a sequence of words in the languageof the grammar) to apply.TSGs permit us to posit a spectrum of grammati-cality in between these two extremes.
If we have agrammar comprising small and large fragments, wemight consider that larger fragments should be lesslikely to fit into ungrammatical situations, whereassmall fragments could be employed almost any-where as a sort of ungrammatical glue.
Thus, onaverage, grammatical sentences will license deriva-tions with larger fragments, whereas ungrammaticalsentences will be forced to resort to small fragments.This is the central idea explored in this paper.This raises the question of what exactly the largerfragments are.
A fundamental problem with TSGs isthat they are hard to learn, since there is no annotatedcorpus of TSG derivations and the number of possi-ble derivations is exponential in the size of a tree.The most popular TSG approach has been Data-Oriented Parsing (Scha, 1990; Bod, 1993), whichtakes all fragments in the training data.
The largesize of such grammars (exponential in the size of thetraining data) forces either implicit representations(Goodman, 1996; Bansal and Klein, 2010) ?
whichdo not permit arbitrary probability distributions overthe grammar fragments ?
or explicit approxima-tions to all fragments (Bod, 2001).
A number of re-searchers have presented ways to address the learn-ing problems for explicitly represented TSGs (Zoll-mann and Sima?an, 2005; Zuidema, 2007; Cohn etal., 2009; Post and Gildea, 2009a).
Of these ap-proaches, work in Bayesian learning of TSGs pro-duces intuitive grammars in a principled way, andhas demonstrated potential in language modelingtasks (Post and Gildea, 2009b; Post, 2010).
Our ex-periments make use of Bayesian-learned TSGs.4 ExperimentsWe experiment with a binary classification task, de-fined as follows: given a sequence of words, deter-mine whether it is grammatical or not.
We use twodatasets: the Wall Street Journal portion of the PennTreebank (Marcus et al, 1993), and the BLLIP ?99dataset,1 a collection of automatically-parsed sen-tences from three years of articles from the WallStreet Journal.For both datasets, positive examples are obtainedfrom the leaves of the parse trees, retaining their to-kenization.
Negative examples were produced froma trigram language model by randomly generatingsentences of length no more than 100 so as to matchthe size of the positive data.
The language modelwas built with SRILM (Stolcke, 2002) using inter-polated Kneser-Ney smoothing.
The average sen-tence lengths for the positive and negative data were23.9 and 24.7, respectively, for the Treebank data1LDC Catalog No.
LDC2000T43.218dataset training devel.
testTreebank 3,836 2,690 3,39891,954 65,474 79,998BLLIP 100,000 6,000 6,0002,596,508 155,247 156,353Table 1: The number of sentences (first line) and words(second line) using for training, development, and test-ing of the classifier.
Each set of sentences is evenly splitbetween positive and negative examples.and 25.6 and 26.2 for the BLLIP data.Each dataset is divided into training, develop-ment, and test sets.
For the Treebank, we trainedthe n-gram language model on sections 2 - 21.
Theclassifier then used sections 0, 24, and 22 for train-ing, development, and testing, respectively.
Forthe BLLIP dataset, we followed Cherry and Quirk(2008): we randomly selected 450K sentences totrain the n-gram language model, and 50K, 3K, and3K sentences for classifier training, development,and testing, respectively.
All sentences have 100or fewer words.
Table 1 contains statistics of thedatasets used in our experiments.To build the classifier, we used liblinear (Fanet al, 2008).
A bias of 1 was added to each featurevector.
We varied a cost or regularization parame-ter between 1e ?
5 and 100 in orders of magnitude;at each step, we built a model, evaluating it on thedevelopment set.
The model with the highest scorewas then used to produce the result on the test set.4.1 Base models and featuresOur experiments compare a number of different fea-ture sets.
Central to these feature sets are featurescomputed from the output of four language models.1.
Bigram and trigram language models (the sameones used to generate the negative data)2.
A Treebank grammar (Charniak, 1996)3.
A Bayesian-learned tree substitution grammar(Post and Gildea, 2009a)22The sampler was run with the default settings for 1,000iterations, and a grammar of 192,667 fragments was then ex-tracted from counts taken from every 10th iteration betweeniterations 500 and 1,000, inclusive.
Code was obtained fromhttp://github.com/mjpost/dptsg.4.
The Charniak parser (Charniak, 2000), run inlanguage modeling modeThe parsing models for both datasets were built fromsections 2 - 21 of the WSJ portion of the Treebank.These models were used to score or parse the train-ing, development, and test data for the classifier.From the output, we extract the following featuresets used in the classifier.?
Sentence length (l).?
Model scores (S).
Model log probabilities.?
Rule features (R).
These are counter featuresbased on the atomic unit of the analysis, i.e., in-dividual n-grams for the n-gram models, PCFGrules, and TSG fragments.?
Reranking features (C&J).
From the Char-niak parser output we extract the complete setof reranking features of Charniak and Johnson(2005), and just the local ones (C&J local).3?
Frontier size (Fn,F ln).
Instances of this fea-ture class count the number of TSG fragmentshaving frontier size n, 1 ?
n ?
9.4 Instancesof F ln count only lexical items for 0 ?
n ?
5.4.2 ResultsTable 2 contains the classification results.
The firstblock of models all perform at chance.
We exper-imented with SVM classifiers instead of maximumentropy, and the only real change across all the mod-els was for these first five models, which saw classi-fication rise to 55 to 60%.On the BLLIP dataset, the C&J feature sets per-form the best, even when the set of features is re-stricted to local ones.
However, as shown in Table 3,this performance comes at a cost of using ten timesas many features.
The classifiers with TSG featuresoutperform all the other models.The (near)-perfect performance of the TSG mod-els on the Treebank is a result of the large numberof features relative to the size of the training data:3Local features can be computed in a bottom-up manner.See Huang (2008, ?3.2) for more detail.4A fragment?s frontier is the number of terminals and non-terminals among its leaves, also known its rank.
For example,the fragment in Figure 1 has a frontier size of 5.219feature set Treebank BLLIPlength (l) 50.0 46.43-gram score (S3) 50.0 50.1PCFG score (SP ) 49.5 50.0TSG score (ST ) 49.5 49.7Charniak score (SC) 50.0 50.0l + S3 61.0 64.3l + SP 75.6 70.4l + ST 82.4 76.2l + SC 76.3 69.1l + R2 62.4 70.6l + R3 61.3 70.7l + RP 60.4 85.0l + RT 99.4 89.3l + C&J (local) 89.1 92.5l + C&J 88.6 93.0l + RT + F?
+ F l?
100.0 89.9Table 2: Classification accuracy.feature set Treebank BLLIPl + R3 18K 122Kl + RP 15K 11Kl + RT 14K 60Kl + C&J (local) 24K 607Kl + C&J 58K 959Kl + RT + F?
14K 60KTable 3: Model size.the positive and negative data really do evince dif-ferent fragments, and there are enough such featuresrelative to the size of the training data that very highweights can be placed on them.
Manual examina-tion of feature weights bears this out.
Despite hav-ing more features available, the Charniak & John-son feature set has significantly lower accuracy onthe Treebank data, which suggests that the TSG fea-tures are more strongly associated with a particular(positive or negative) outcome.For comparison, Cherry and Quirk (2008) reporta classification accuracy of 81.42 on BLLIP.
We ex-clude it from the table because a direct comparison isnot possible, since we did not have access to the spliton the BLLIP used in their experiments, but only re-peated the process they described to generate it.5 AnalysisTable 4 lists the highest-weighted TSG features as-sociated with each outcome, taken from the BLLIPmodel in the last row of Table 2.
The learnedweights accord with the intuitions presented in Sec-tion 3.
Ungrammatical sentences use smaller, ab-stract (unlexicalized) rules, whereas grammaticalsentences use higher rank rules and are more lexical-ized.
Looking at the fragments themselves, we seethat sensible patterns such as balanced parentheticalexpressions or verb predicate-argument structuresare associated with grammaticality, while many ofthe ungrammatical fragments contain unbalancedquotations and unlikely configurations.Table 5 contains the most probable depth-onerules for each outcome.
The unary rules associatedwith ungrammatical sentences show some interest-ing patterns.
For example, the rule NP?
DT occurs2,344 times in the training portion of the Treebank.Most of these occurrences are in subject settingsover articles that aren?t required to modify a noun,such as that, some, this, and all.
However, in theBLLIP n-gram data, this rule is used over the defi-nite article the 465 times ?
the second-most commonuse.
Yet this rule occurs only nine times in the Tree-bank where the grammar was learned.
The smallfragment size, together with the coarseness of thenonterminal, permit the fragment to be used in dis-tributional settings where it should not be licensed.This suggests some complementarity between frag-ment learning and work in using nonterminal refine-ments (Johnson, 1998; Petrov et al, 2006).6 Related workPast approaches using parsers as language modelsin discriminative settings have seen varying degreesof success.
Och et al (2004) found that the scoreof a bilexicalized parser was not useful in distin-guishing machine translation (MT) output from hu-man reference translations.
Cherry and Quirk (2008)addressed this problem by using a latent SVM toadjust the CFG rule weights such that the parserscore was a much more useful discriminator be-tween grammatical text and n-gram samples.
Mut-ton et al (2007) also addressed this problem by com-bining scores from different parsers using an SVMand showed an improved metric of fluency.220grammatical ungrammatical(VP VBD (NP CD)PP)F l0(S (NP PRP) VP) (NP (NP CD) PP)(S NP (VP TO VP)) (TOP (NP NP NP .
))F l2 F5(NP NP (VP VBGNP))(S (NP (NNP UNK-CAPS-NUM)))(SBAR (S (NP PRP)VP))(TOP (S NP VP (.
.
)))(SBAR (IN that) S) (TOP (PP IN NP .
))(TOP (S NP (VP (VBDsaid) NP SBAR) .
))(TOP (S ?
NP VP (.
.
)))(NP (NP DT JJ NN)PP)(TOP (S PP NP VP .
))(NP (NP NNP NNP) ,NP ,)(TOP (NP NP PP .
))(TOP (S NP (ADVP(RB also)) VP .
))F4(VP (VB be) VP) (NP (DT that) NN)(NP (NP NNS) PP) (TOP (S NP VP .
?
))(NP NP , (SBARWHNP (S VP)) ,)(TOP (NP NP , NP .
))(TOP (S SBAR , NPVP .
))(QP CD (CD million))(ADJP (QP $ CD (CDmillion)))(NP NP (CC and) NP)(SBAR (IN that) (S NPVP))(PP (IN In) NP)F8 (QP $ CD (CD mil-lion))Table 4: Highest-weighted TSG features.Outside of MT, Foster and Vogel (2004) arguedfor parsers that do not assume the grammaticality oftheir input.
Sun et al (2007) used a set of templatesto extract labeled sequential part-of-speech patternstogether with some other linguistic features) whichwere then used in an SVM setting to classify sen-tences in Japanese and Chinese learners?
Englishcorpora.
Wagner et al (2009) and Foster and An-dersen (2009) attempt finer-grained, more realistic(and thus more difficult) classifications against un-grammatical text modeled on the sorts of mistakesmade by language learners using parser probabili-ties.
More recently, some researchers have shownthat using features of parse trees (such as the rulesgrammatical ungrammatical(WHNP CD) (NN UNK-CAPS)(NP JJ NNS) (S VP)(PRT RP) (S NP)(WHNP WP NN) (TOP FRAG)(SBAR WHNP S) (NP DT JJ)(WHNP WDT NN) (NP DT)Table 5: Highest-weighted depth-one rules.used) is fruitful (Wong and Dras, 2010; Post, 2010).7 SummaryParsers were designed to discriminate among struc-tures, whereas language models discriminate amongstrings.
Small fragments, abstract rules, indepen-dence assumptions, and errors or peculiarities in thetraining corpus allow probable structures to be pro-duced over ungrammatical text when using modelsthat were optimized for parser accuracy.The experiments in this paper demonstrate theutility of tree-substitution grammars in discriminat-ing between grammatical and ungrammatical sen-tences.
Features are derived from the identities ofthe fragments used in the derivations above a se-quence of words; particular fragments are associatedwith each outcome, and simple statistics computedover those fragments are also useful.
The most com-plicated aspect of using TSGs is grammar learning,for which there are publicly available tools.Looking forward, we believe there is significantpotential for TSGs in more subtle discriminativetasks, for example, in discriminating between finergrained and more realistic grammatical errors (Fos-ter and Vogel, 2004; Wagner et al, 2009), or in dis-criminating among translation candidates in a ma-chine translation framework.
In another line of po-tential work, it could prove useful to incorporate intothe grammar learning procedure some knowledge ofthe sorts of fragments and features shown here to behelpful for discriminating grammatical and ungram-matical text.ReferencesMohit Bansal and Dan Klein.
2010.
Simple, accurateparsing with an all-fragments grammar.
In Proc.
ACL,Uppsala, Sweden, July.221Rens Bod.
1993.
Using an annotated corpus as a stochas-tic grammar.
In Proc.
ACL, Columbus, Ohio, USA.Rens Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy?
In Proc.
ACL,Toulouse, France, July.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
ACL, Ann Arbor, Michigan, USA, June.Eugene Charniak.
1996.
Tree-bank grammars.
In Proc.of the National Conference on Artificial Intelligence.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
NAACL, Seattle, Washington, USA,April?May.Colin Cherry and Chris Quirk.
2008.
Discriminative,syntactic language modeling through latent svms.
InProc.
AMTA, Waikiki, Hawaii, USA, October.Trevor Cohn, Sharon.
Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In Proc.
NAACL, Boulder, Colorado, USA,June.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Jennifer Foster and ?istein E. Andersen.
2009.
Gen-errate: generating errors for use in grammatical errordetection.
In Proceedings of the fourth workshop oninnovative use of nlp for building educational appli-cations, pages 82?90.
Association for ComputationalLinguistics.Jennifer Foster and Carl Vogel.
2004.
Good reasonsfor noting bad grammar: Constructing a corpus of un-grammatical language.
In Pre-Proceedings of the In-ternational Conference on Linguistic Evidence: Em-pirical, Theoretical and Computational Perspectives.Joshua Goodman.
1996.
Efficient algorithms for pars-ing the DOP model.
In Proc.
EMNLP, Philadelphia,Pennsylvania, USA, May.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), Columbus, Ohio, June.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors,Handbook of Formal Languages: BeyondWords, volume 3, pages 71?122.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
Computationallinguistics, 19(2):330.Andrew Mutton, Mark Dras, Stephen Wan, and RobertDale.
2007.
Gleu: Automatic evaluation of sentence-level fluency.
In Proc.
ACL, volume 45, page 344.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng, et al2004.
A smorgasbord of features for statistical ma-chine translation.
In Proc.
NAACL.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
Adiscriminative language model with pseudo-negativesamples.
In Proc.
ACL, Prague, Czech Republic, June.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
COLING/ACL, Syd-ney, Australia, July.Matt Post and Daniel Gildea.
2009a.
Bayesian learningof a tree substitution grammar.
In Proc.
ACL (shortpaper track), Suntec, Singapore, August.Matt Post and Daniel Gildea.
2009b.
Language modelingwith tree substitution grammars.
In NIPS workshop onGrammar Induction, Representation of Language, andLanguage Learning, Whistler, British Columbia.Matt Post.
2010.
Syntax-based Language Models forStatistical Machine Translation.
Ph.D. thesis, Univer-sity of Rochester.Remko Scha.
1990.
Taaltheorie en taaltechnologie; com-petence en performance.
In R. de Kort and G.L.J.Leerdam, editors, Computertoepassingen in de neer-landistiek, pages 7?22, Almere, the Netherlands.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
International Conferenceon Spoken Language Processing.Ghihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee, and Chin-Yew Lin.2007.
Detecting erroneous sentences using automat-ically mined sequential patterns.
In Proc.
ACL, vol-ume 45.JoachimWagner, Jennifer Foster, and Josef van Genabith.2009.
Judging grammaticality: Experiments in sen-tence classification.
CALICO Journal, 26(3):474?490.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parserfeatures for sentence grammaticality classification.
InProc.
Australasian Language Technology AssociationWorkshop, Melbourne, Australia, December.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for Data-Oriented Parsing.Journal of Automata, Languages and Combinatorics,10(2/3):367?388.Willem Zuidema.
2007.
Parsimonious Data-OrientedParsing.
In Proc.
EMNLP, Prague, Czech Republic,June.222
