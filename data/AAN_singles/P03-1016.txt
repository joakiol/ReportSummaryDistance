Synonymous Collocation Extraction Using Translation InformationHua WU, Ming ZHOUMicrosoft Research Asia5F Sigma Center, No.49 Zhichun Road, Haidian DistrictBeijing, 100080, Chinawu_hua_@msn.com, mingzhou@microsoft.comAbstractAutomatically acquiring synonymous col-location pairs such as <turn on, OBJ, light>and <switch on, OBJ, light> from corporais a challenging task.
For this task, we can,in general, have a large monolingual corpusand/or a very limited bilingual corpus.Methods that use monolingual corporaalone or use bilingual corpora alone areapparently inadequate because of low pre-cision or low coverage.
In this paper, wepropose a method that uses both these re-sources to get an optimal compromise ofprecision and coverage.
This method firstgets candidates of synonymous collocationpairs based on a monolingual corpus and aword thesaurus, and then selects the ap-propriate pairs from the candidates usingtheir translations in a second language.
Thetranslations of the candidates are obtainedwith a statistical translation model which istrained with a small bilingual corpus and alarge monolingual corpus.
The translationinformation is proved as effective to selectsynonymous collocation pairs.
Experi-mental results indicate that the averageprecision and recall of our approach are74% and 64% respectively, which outper-form those methods that only use mono-lingual corpora and those that only use bi-lingual corpora.1 IntroductionThis paper addresses the problem of automaticallyextracting English synonymous collocation pairsusing translation information.
A synonymous col-location pair includes two collocations which aresimilar in meaning, but not identical in wording.Throughout this paper, the term collocation refersto a lexically restricted word pair with a certainsyntactic relation.
For instance, <turn on, OBJ,light> is a collocation with a syntactic relationverb-object, and <turn on, OBJ, light> and <switchon, OBJ, light> are a synonymous collocation pair.In this paper, translation information means trans-lations of collocations and their translation prob-abilities.Synonymous collocations can be considered asan extension of the concept of synonymous ex-pressions which conventionally include synony-mous words, phrases and sentence patterns.
Syn-onymous expressions are very useful in a number ofNLP applications.
They are used in informationretrieval and question answering (Kiyota et al,2002; Dragomia et al, 2001) to bridge the expres-sion gap between the query space and the documentspace.
For instance, ?buy book?
extracted from theusers?
query should also in some way match ?orderbook?
indexed in the documents.
Besides, thesynonymous expressions are also important inlanguage generation (Langkilde and Knight, 1998)and computer assisted authoring to produce vividtexts.Up to now, there have been few researcheswhich directly address the problem of extractingsynonymous collocations.
However, a number ofstudies investigate the extraction of synonymouswords from monolingual corpora (Carolyn et al,1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al,2001).
The methods used the contexts around theinvestigated words to discover synonyms.
Theproblem of the methods is that the precision of theextracted synonymous words is low because itextracts many word pairs such as ?cat?
and ?dog?,which are similar but not synonymous.
In addition,some studies investigate the extraction of synony-mous words and/or patterns from bilingual corpora(Barzilay and Mckeown, 2001; Shimohata andSumita, 2002).
However, these methods can onlyextract synonymous expressions which occur in thebilingual corpus.
Due to the limited size of thebilingual corpus, the coverage of the extractedexpressions is very low.Given the fact that we usually have large mono-lingual corpora (unlimited in some sense) and verylimited bilingual corpora, this paper proposes amethod that tries to make full use of these differentresources to get an optimal compromise of preci-sion and coverage for synonymous collocationextraction.
We first obtain candidates of synony-mous collocation pairs based on a monolingualcorpus and a word thesaurus.
We then select thoseappropriate candidates using their translations in asecond language.
Each translation of the candidatesis assigned a probability with a statistical translationmodel that is trained with a small bilingual corpusand a large monolingual corpus.
The similarity oftwo collocations is estimated by computing thesimilarity of their vectors constructed with theircorresponding translations.
Those candidates withlarger similarity scores are extracted as synony-mous collocations.
The basic assumption behindthis method is that two collocations are synony-mous if their translations are similar.
For example,<turn on, OBJ, light> and <switch on, OBJ, light>are synonymous because both of them are translatedinto < , OBJ, > (<kai1, OBJ, deng1>) and < ,OBJ, > (<da3 kai1, OBJ, deng1>)  in Chinese.In order to evaluate the performance of ourmethod, we conducted experiments on extractingthree typical types of synonymous collocations.Experimental results indicate that our approachachieves 74% average precision and 64% recallrespectively, which considerably outperform thosemethods that only use monolingual corpora or onlyuse bilingual corpora.The remainder of this paper is organized as fol-lows.
Section 2 describes our synonymous colloca-tion extraction method.
Section 3 evaluates theproposed method, and the last section draws ourconclusion and presents the future work.2 Our ApproachOur method for synonymous collocation extractioncomprises of three steps: (1) extract collocationsfrom large monolingual corpora; (2) generate can-didates of synonymous collocation pairs with aword thesaurus WordNet; (3) select synonymouscollocation candidates using their translations.2.1 Collocation ExtractionThis section describes how to extract English col-locations.
Since Chinese collocations will be usedto train the language model in Section 2.3, they arealso extracted in the same way.Collocations in this paper take some syntacticalrelations (dependency relations), such as <verb,OBJ, noun>, <noun, ATTR, adj>, and <verb, MOD,adv>.
These dependency triples, which embody thesyntactic relationship between words in a sentence,are generated with a parser?we use NLPWIN inthis paper1.
For example, the sentence ?She ownedthis red coat?
is transformed to the following fourtriples after parsing: <own, SUBJ, she>, <own, OBJ,coat>, <coat, DET, this>, and <coat, ATTR, red>.These triples are generally represented in the formof <Head, Relation Type, Modifier>.The measure we use to extract collocationsfrom the parsed triples is weighted mutual infor-mation (WMI) (Fung and Mckeown, 1997), asdescribed as)()|()|(),,(log),,(),,(21212121rprwprwpwrwpwrwpwrwWMI =Those triples whose WMI values are larger than agiven threshold are taken as collocations.
We do notuse the point-wise mutual information because ittends to overestimate the association between twowords with low frequencies.
Weighted mutualinformation meliorates this effect by add-ing ),,( 21 wrwp .For expository purposes, we will only look intothree kinds of collocations for synonymous collo-cation extraction: <verb, OBJ, noun>, <noun,ATTR, adj> and <verb, MOD, adv>.Table 1.
English CollocationsClass #Type  #Tokenverb, OBJ, noun 506,628 7,005,455noun, ATTR, adj 333,234 4,747,970verb, Mod, adv 40,748 483,911Table 2.
Chinese CollocationsClass #Type #Tokenverb, OBJ, noun 1,579,783 19,168,229noun, ATTR, adj 311,560 5,383,200verb, Mod, adv 546,054 9,467,103The English collocations are extracted fromWall Street Journal (1987-1992) and AssociationPress (1988-1990), and the Chinese collocations are1The NLPWIN parser is developed at Microsoft Re-search, which parses several languages including Chi-nese and English.
Its output can be a phrase structureparse tree or a logical form which is represented withdependency triples.extracted from People?s Daily (1980-1998).
Thestatistics of the extracted collocations are shown inTable 1 and 2.
The thresholds are set as 5 for bothEnglish and Chinese.
Token refers to the totalnumber of collocation occurrences and Type refersto the number of unique collocations in the corpus.2.2 Candidate GenerationCandidate generation is based on the followingassumption: For a collocation <Head, RelationType, Modifier>, its synonymous expressions alsotake the form of <Head, Relation Type, Modifier>although sometimes they may also be a single wordor a sentence pattern.The synonymous candidates of a collocation areobtained by expanding a collocation <Head, Rela-tion Type, Modifier> using the synonyms of Headand Modifier.
The synonyms of a word are obtainedfrom WordNet 1.6.
In WordNet, one synset consistsof several synonyms which represent a single sense.Therefore, polysemous words occur in more thanone synsets.
The synonyms of a given word areobtained from all the synsets including it.
For ex-ample, the word ?turn on?
is a polysemous wordand is included in several synsets.
For the sense?cause to operate by flipping a switch?, ?switch on?is one of its synonyms.
For the sense ?be contingenton?, ?depend on?
is one of its synonyms.
We takeboth of them as the synonyms of ?turn on?
regard-less of its meanings since we do not have sense tagsfor words in collocations.If we use Cw to indicate the synonym set of aword w and U to denote the English collocation setgenerated in Section 2.1.
The detail algorithm ongenerating candidates of synonymous collocationpairs is described in Figure 1.
For example, given acollocation <turn on, OBJ, light>, we expand ?turnon?
to ?switch on?, ?depend on?, and then expand?light?
to ?lump?, ?illumination?.
With thesesynonyms and the relation type OBJ, we generatesynonymous collocation candidates of <turn on,OBJ, light>.
The candidates are <switch on, OBJ,light>, <turn on, OBJ, lump>, <depend on, OBJ,illumination>, <depend on, OBJ, light> etc.
Boththese candidates and the original collocation <turnon, OBJ, light> are used to generate the synony-mous collocation pairs.With the above method, we obtained candidatesof synonymous collocation pairs.
For example,<switch on, OBJ, light> and <turn on, OBJ, light>are a synonymous collocation pair.
However, thismethod also produces wrong synonymous colloca-tion candidates.
For example, <depend on, OBJ,illumination> and <turn on, OBJ, light> is not asynonymous pair.
Thus, it is important to filter outthese inappropriate candidates.Figure 1.
Candidate Set Generation Algorithm2.3 Candidate SelectionIn synonymous word extraction, the similarity oftwo words can be estimated based on the similarityof their contexts.
However, this method cannot beeffectively extended to collocation similarity esti-mation.
For example, in sentences ?They turned onthe lights?
and ?They depend on the illumination?,the meaning of two collocations <turn on, OBJ,light> and <depend on, OBJ, illumination> aredifferent although their contexts are the same.Therefore, monolingual information is not enoughto estimate the similarity of two collocations.However, the meanings of the above two colloca-tions can be distinguished if they are translated intoa second language (e.g., Chinese).
For example,<turn on, OBJ, light> is translated into < , OBJ,> (<kai1, OBJ, deng1) and < , OBJ, > (<da3kai1, OBJ, deng1>) in Chinese while <depend on,OBJ, illumination> is translated into < , OBJ,> (qu3 jue2 yu2, OBJ, guang1 zhao4 du4>).Thus, they are not synonymous pairs because theirtranslations are completely different.In this paper, we select the synonymous collo-cation pairs from the candidates in the followingway.
First, given a candidate of synonymous col-location pair generated in section 2.2, we translatethe two collocations into Chinese with a simplestatistical translation model.
Second, we calculatethe similarity of two collocations with the featurevectors constructed with their translations.
A can-didate is selected as a synonymous collocation pair(1) For each collocation (Co1i=<Head, R, Modi-fier>) U, do the following:a.
Use the synonyms in WordNet 1.6 to expandHead and Modifier and get their synonymsets CHead and CModifierb.
Generate the candidate set of its synonymouscollocations Si={<w1, R, w2> | w1 {Head}CHead  & w2 {Modifier}  CModifier  &<w1, R, w2> U & <w1, R, w2> ?
Co1i }(2) Generate the candidate set of synonymouscollocation  pairs  SC= {(Co1i, Co1j)| Co1iCo1j Siif its similarity exceeds a certain threshold.2.3.1 Collocation TranslationFor an English collocation ecol=<e1, re, e2>, wetranslate it into Chinese collocations 2  using anEnglish-Chinese dictionary.
If the translation sets ofe1 and e2 are represented as CS1 and CS2 respec-tively, the Chinese translations can be representedas S={<c1, rc, c2>| c1 CS1 , c2 CS2 , rc  },  with Rdenoting the relation set.Given an English collocation ecol=<e1, re, e2>and one of its Chinese collocation ccol=<c1, rc,c2> S, the probability that ecol is translated into ccolis calculated as in Equation (1).
)(),,(),,|,,()|( 212121colccecolcolepcrcpcrcerepecp =  (1)According to Equation (1), we need to calculate thetranslation probability p(ecol|ccol) and the targetlanguage probability p(ccol).
Calculating the trans-lation probability needs a bilingual corpus.
If theabove equation is used directly, we will run into thedata sparseness problem.
Thus, model simplifica-tion is necessary.2.3.2 Translation ModelOur simplification is made according to the fol-lowing three assumptions.Assumption 1: For a Chinese collocation ccol and re,we assume that e1 and e2 are conditionally inde-pendent.
The translation model is rewritten as:)|(),|(),|()|,,()|(2121colecolecolecolecolcolcrpcrepcrepcerepcep==(2)Assumption 2: Given a Chinese collocation <c1, rc,c2>, we assume that the translation probabilityp(ei|ccol) only depends on ei and ci (i=1,2), andp(re|ccol) only depends on re and rc.
Equation (2) isrewritten as:)|()|()|()|()|()|()|(221121cecolecolcolcolcolrrpcepcepcrpcepcepcep==(3)It is equal to a word translation model if we takethe relation type in the collocations as an elementlike a word, which is similar to Model 1 in (Brownet al, 1993).Assumption 3: We assume that one type of English2Some English collocations can be translated into Chi-nese words, phrases or patterns.
Here we only considerthe case of being translated into collocations.collocation can only be translated to the same typeof Chinese collocations3.
Thus, p(re| rc) =1 in ourcase.
Equation (3) is rewritten as:)|()|()|()|()|()|(22112211cepceprrpcepcepcepcecolcol==(4)2.3.3 Language ModelThe language model p(ccol) is calculated with theChinese collocation database extracted in section2.1.
In order to tackle with the data sparsenessproblem, we smooth the language model with aninterpolation method.When the given Chinese collocation occurs inthe corpus, we calculate it as in (5).Nccountcp colcol)()( =                      (5)where )(colccount represents the count of the Chi-nese collocationcolc .
N represents the total countsof all the Chinese collocations in the training cor-pus.For a collocation <c1, rc, c2>, if we assume thattwo words c1 and c2 are conditionally independentgiven the relation rc, Equation (5) can be rewrittenas in (6).
)()|()|()( 21 ccccol rprcprcpcp =                          (6)where,*)(*,,*),()|( 11cccrcountrccountrcp =,*)(*,),(*,)|( 22cccrcountcrcountrcp = ,Nrcountrp cc,*)(*,)( =,*),( 1 crccount : frequency of the collocations with c1as the head and rc as the relation type.
),(*, 2crcount c : frequency of the collocations withc2 as the modifier and rc as the relation type,*)(*,crcount : frequency of the collocations with rcas the relation type.With Equation (5) and (6), we get the interpolatedlanguage model as shown in (7).
)()|()|()-(1 )()( 21 ccccolcol rprcprcpNccountcp ??
+=(7)where 10 << ?
.
?
is a constant so that the prob-abilities sum to 1.3Zhou et al (2001) found that about 70% of the Chinesetranslations have the same relation type as the sourceEnglish collocations.2.3.4 Word Translation Probability EstimationMany methods are used to estimate word translationprobabilities from unparallel or parallel bilingualcorpora (Koehn and Knight, 2000; Brown et al,1993).
In this paper, we use a parallel bilingualcorpus to train the word translation probabilitiesbased on the result of word alignment with a bi-lingual Chinese-English dictionary.
The alignmentmethod is described in (Wang et al, 2001).
In orderto deal with the problem of data sparseness, weconduct a simple smoothing by adding 0.5 to thecounts of each translation pair as in (8).|_|*5.0)(5.0),()|(etransccountcecountcep++=       (8)where |_| etrans  represents the number of Eng-lish translations for a given Chinese word c.2.3.5 Collocation Similarity CalculationFor each synonymous collocation pair, we get itscorresponding Chinese translations and calculatethe translation probabilities as in section 2.3.1.These Chinese collocations with their correspond-ing translation probabilities are taken as featurevectors of the English collocations, which can berepresented as:>=< ),(, ... ),,(),,( 2211 imcolimcolicolicolicolicolicol pcpcpcFeThe similarity of two collocations is defined as in(9).
The candidate pairs whose similarity scoresexceed a given threshold are selected.
( ) ( )===jjcoliicoljcolcicolcjcolicolcolcolcolcolppppFeFeeesim222121212121**),cos(),((9)For example, given a synonymous collocationpair <turn on, OBJ, light> and <switch on, OBJ,light>, we first get their corresponding featurevectors.The feature vector of <turn on, OBJ, light>:< (< , OBJ, >, 0.04692), (< , OBJ, >,0.01602), ?
, (< , OBJ, >, 0.0002710), (< ,OBJ, >, 0.0000305) >The feature vector of <switch on, OBJ, light>:< (< , OBJ, >, 0.04238), (< , OBJ, >,0.01257), (< , OBJ, >, 0.002531), ?
, (< ,OBJ, >, 0.00003542) >The values in the feature vector are translationprobabilities.
With these two vectors, we get thesimilarity of <turn on, OBJ, light> and <switch on,OBJ, light>, which is 0.2348.2.4 Implementation of our ApproachWe use an English-Chinese dictionary to get theChinese translations of collocations, which includes219,404 English words.
Each source word has 3translation words on average.
The word translationprobabilities are estimated from a bilingual corpusthat obtains 170,025 pairs of Chinese-English sen-tences, including about 2.1 million English wordsand about 2.5 million Chinese words.With these data and the collocations in section2.1, we produced 93,523 synonymous collocationpairs and filtered out 1,060,788 candidate pairs withour translation method if we set the similaritythreshold to 0.01.3 EvaluationTo evaluate the effectiveness of our methods, twoexperiments have been conducted.
The first one isdesigned to compare our method with two methodsthat use monolingual corpora.
The second one isdesigned to compare our method with a method thatuses a bilingual corpus.3.1 Comparison with Methods usingMonolingual CorporaWe compared our approach with two methods thatuse monolingual corpora.
These two methods alsoemployed the candidate generation described insection 2.2.
The difference is that the two methodsuse different strategies to select appropriate candi-dates.
The training corpus for these two methods isthe same English one as in Section 2.1.3.1.1 Method DescriptionMethod 1: This method uses monolingual contextsto select synonymous candidates.
The purpose ofthis experiment is to see whether the contextmethod for synonymous word extraction can beeffectively extended to synonymous collocationextraction.The similarity of two collocations is calculatedwith their feature vectors.
The feature vector of acollocation is constructed by all words in sentenceswhich surround the given collocation.
The contextvector for collocation i is represented as in (10).>=< ),(),...,,(),,( 2211 imimiiiiicol pwpwpwFe   (10)whereNewcountpicolijij),(=ijw : context word j of collocation i.ijp : probability of ijw  co-occurring with icole .
),( icolij ewcount : frequency of the context word ijwco-occurring with the collocation icoleN: all counts of the words in the training corpus.With the feature vectors, the similarity of two col-locations is calculated as in (11).
Those candidateswhose similarities exceed a given threshold areselected as synonymous collocations.
( ) ( )===jjiijwiwjicolcolcolcolppppFeFeeesim222121212121**),cos(),((11)Method 2: Instead of using contexts to calculate thesimilarity of two words, this method calculates thesimilarity of collocations with the similarity of theircomponents.
The formula is described in Equation(12).
),(*),(*),(),(212212211121relrelsimeesimeesimeesimcolcol=(12)where ),,( 21 iiiicol erelee = .
We assume that the rela-tion type keeps the same, so 1),( 21 =relrelsim .The similarity of the words is calculated with thesame method as described in (Lin, 1998), which isrewritten in Equation (13).
The similarity of thewords is calculated through the surrounding contextwords which have dependency relationships withthe investigated words.),,(),,()),,(),,((),(2)2(),(1)1(),(21)2()1(),(21erelewerelewerelewereleweeSimeTereleTereleTeTerel??
?++=(13)where T(ei) denotes the set of words which have thedependency relation rel with ei.
)()|()|(),,(log),,(),,(relprelepreleperelepereleperelewjijijiji=3.1.2 Test SetWith the candidate generation method as depictedin section 2.2, we generated 1,154,311 candidatesof synonymous collocations pairs for 880,600collocations, from which we randomly selected1,300 pairs to construct a test set.
Each pair wasevaluated independently by two judges to see if it issynonymous.
Only those agreed upon by two judgesare considered as synonymous pairs.
The statisticsof the test set is shown in Table 3.
We evaluatedthree types of synonymous collocations: <verb,OBJ, noun>, <noun, ATTR, adj>, <verb, MOD,adv>.
For the type <verb, OBJ, noun>, among the630 synonymous collocation candidate pairs, 197pairs are correct.
For <noun, ATTR, adj>, 163 pairs(among 324 pairs) are correct, and for <verb, MOD,adv>, 124 pairs (among 346 pairs) are correct.Table 3.
The Test SetType #total #correctverb, OBJ, noun 630 197noun, ATTR, adj 324 163verb, MOD, adv 346 1243.1.3 Evaluation ResultsWith the test set, we evaluate the performance ofeach method.
The evaluation metrics are precision,recall, and f-measure.A development set including 500 synonymouspairs is used to determine the thresholds of eachmethod.
For each method, the thresholds for gettinghighest f-measure scores on the development set areselected.
As the result, the thresholds for Method 1,Method 2 and our approach are 0.02, 0.02, and 0.01respectively.
With these thresholds, the experi-mental results on the test set in Table 3 are shown inTable 4, Table 5 and Table 6.Table 4.
Results for <verb, OBJ, noun>Method Precision Recall F-measureMethod 1 0.3148 0.8934 0.4656Method 2 0.3886 0.7614 0.5146Ours 0.6811 0.6396 0.6597Table 5.
Results for <noun, ATTR, adj>Method Precision Recall F-measureMethod 1 0.5161 0.9816 0.6765Method 2 0.5673 0.8282 0.6733Ours 0.8739 0.6380 0.7376Table 6.
Results for <verb, MOD, adv>Method Precision Recall F-measureMethod 1 0.3662 0.9597 0.5301Method 2 0.4163 0.7339 0.5291Ours 0.6641 0.7016 0.6824It can be seen that our approach gets the highestprecision (74% on average) for all the three types ofsynonymous collocations.
Although the recall (64%on average) of our approach is below other methods,the f-measure scores, which combine both precisionand recall, are the highest.
In order to compare ourmethods with other methods under the same recallvalue, we conduct another experiment on the type<verb, OBJ, noun>4.
We set the recalls of the twomethods to the same value of our method, which is0.6396 in Table 4.
The precisions are 0.3190,0.4922, and 0.6811 for Method 1, Method 2, andour method, respectively.
Thus, the precisions ofour approach are higher than the other two methodseven when their recalls are the same.
It proves thatour method of using translation information toselect the candidates is effective for synonymouscollocation extraction.The results of Method 1 show that it is difficultto extract synonymous collocations with monolin-gual contexts.
Although Method 1 gets higher re-calls than the other methods, it brings a largenumber of wrong candidates, which results in lowerprecision.
If we set higher thresholds to get com-parable precision, the recall is much lower than thatof our approach.
This indicates that the contexts ofcollocations are not discriminative to extract syn-onymous collocations.The results also show that Model 2 is not suit-able for the task.
The main reason is that both highscores of ),( 2111 eesim and ),( 2212 eesim  does not meanthe high similarity of the two collocations.The reason that our method outperforms theother two methods is that when one collocation istranslated into another language, its translationsindirectly disambiguate the words?
senses in thecollocation.
For example, the probability of <turnon, OBJ, light> being translated into < , OBJ,> (<da3 kai1, OBJ, deng1>) is much higher thanthat of it being translated into < , OBJ,> (<qu3 jue2 yu2, OBJ, guang1 zhao4 du4>) whilethe situation is reversed for <depend on, OBJ, il-lumination>.
Thus, the similarity between <turn on,OBJ, light> and <depend on, OBJ, illumination> islow and, therefore, this candidate is filtered out.4The results of the other two types of collocations are thesame as <verb, OBJ, noun>.
We omit them because ofthe space limit.3.2 Comparison with Methods usingBilingual CorporaBarzilay and Mckeown (2001), and Shimohata andSumita (2002) used a bilingual corpus to extractsynonymous expressions.
If the same source ex-pression has more than one different translation inthe second language, these different translations areextracted as synonymous expressions.
In order tocompare our method with these methods that onlyuse a bilingual corpus, we implement a method thatis similar to the above two studies.
The detail proc-ess is described in Method 3.Method 3: The method is described as follows:(1) All the source and target sentences (here Chi-nese and English, respectively) are parsed; (2)extract the Chinese and English collocations in thebilingual corpus; (3) align Chinese collocationsccol=<c1, rc, c2> and English collocations ecol=<e1, re,e2> if c1 is aligned with e1 and c2  is aligned with e2;(4) obtain two English synonymous collocations iftwo different English collocations are aligned withthe same Chinese collocation and if they occur morethan once in the corpus.The training bilingual corpus is the same onedescribed in Section 2.
With Method 3, we get9,368 synonymous collocation pairs in total.
Thenumber is only 10% of that extracted by our ap-proach, which extracts 93,523 pairs with the samebilingual corpus.
In order to evaluate Method 3 andour approach on the same test set.
We randomlyselect 100 collocations which have synonymouscollocations in the bilingual corpus.
For these 100collocations, Method 3 extracts 121 synonymouscollocation pairs, where 83% (100 among 121) arecorrect 5.
Our method described in Section 2 gen-erates 556 synonymous collocation pairs with athreshold set in the above section, where 75% (417among 556) are correct.If we set a higher threshold (0.08) for ourmethod, we get 360 pairs where 295 are correct(82%).
If we use |A|, |B|, |C| to denote correct pairsextracted by Method 3, our method, both Method 3and our method respectively, we get |A|=100,|B|=295, and 78|||| =?= BAC .
Thus, the syn-onymous collocation pairs extracted by our methodcover 78% ( |||| AC ) of those extracted by Method5These synonymous collocation pairs are evaluated bytwo judges and only those agreed on by both are selectedas correct pairs.3 while those extracted by Method 3 only cover26% ( |||| BC ) of those extracted by our method.It can be seen that the coverage of Method 3 ismuch lower than that of our method even when theirprecisions are set to the same value.
This is mainlybecause Method 3 can only extract synonymouscollocations which occur in the bilingual corpus.
Incontrast, our method uses the bilingual corpus totrain the translation probabilities, where the trans-lations are not necessary to occur in the bilingualcorpus.
The advantage of our method is that it canextract synonymous collocations not occurring inthe bilingual corpus.4 Conclusions and Future WorkThis paper proposes a novel method to automati-cally extract synonymous collocations by usingtranslation information.
Our contribution is that,given a large monolingual corpus and a very limitedbilingual corpus, we can make full use of theseresources to get an optimal compromise of preci-sion and recall.
Especially, with a small bilingualcorpus, a statistical translation model is trained forthe translations of synonymous collocation candi-dates.
The translation information is used to selectsynonymous collocation pairs from the candidatesobtained with a monolingual corpus.
Experimentalresults indicate that our approach extracts syn-onymous collocations with an average precision of74% and recall of 64%.
This result significantlyoutperforms those of the methods that only usemonolingual corpora, and that only use a bilingualcorpus.Our future work will extend synonymous ex-pressions of the collocations to words and patternsbesides collocations.
In addition, we are also inter-ested in extending this method to the extraction ofsynonymous words so that ?black?
and ?white?,?dog?
and ?cat?
can be classified into differentsynsets.AcknowledgementsWe thank Jianyun Nie, Dekang Lin, Jianfeng Gao,Changning Huang, and Ashley Chang for theirvaluable comments on an early draft of this paper.ReferencesBarzilay R. and McKeown K. (2001).
Extracting Para-phrases from a Parallel Corpus.
In Proc.
ofACL/EACL.Brown P.F., S.A. Della Pietra, V.J.
Della Pietra, and R.L.Mercer (1993).
The mathematics of statistical machinetranslation: Parameter estimation.
ComputationalLinguistics, 19(2), pp263- 311.Carolyn J. Crouch and Bokyung Yang (1992).
Experi-ments in automatic statistical thesaurus construction.In Proc.
of the Fifteenth Annual International ACMSIGIR conference on Research and Development inInformation Retrieval, pp77-88.Dragomir R. Radev, Hong Qi, Zhiping Zheng, SashaBlair-Goldensohn, Zhu Zhang, Waiguo Fan, and JohnPrager (2001).
Mining the web for answers to naturallanguage questions.
In ACM CIKM 2001: Tenth In-ternational Conference on Information and KnowledgeManagement, Atlanta, GA.Fung P. and Mckeown K. (1997).
A Technical Word- andTerm- Translation Aid Using Noisy Parallel Corporaacross Language Groups.
In: Machine Translation,Vol.1-2 (special issue), pp53-87.Gasperin C., Gamallo P, Agustini A., Lopes G., and Verade Lima  (2001) Using Syntactic Contexts for Meas-uring Word Similarity.
Workshop on KnowledgeAcquisition & Categorization, ESSLLI.Grefenstette G. (1994) Explorations in Automatic The-saurus Discovery.
Kluwer Academic Press, Boston.Kiyota Y., Kurohashi S., and Kido F. (2002) "DialogNavigator":  A Question Answering System based onLarge Text Knowledge Base.
In Proc.
of the 19th In-ternational Conference on Computational Linguistics,Taiwan.Koehn.
P and Knight K. (2000).
Estimating WordTranslation Probabilities from Unrelated Monolin-gual Corpora using the EM Algorithm.
NationalConference on Artificial Intelligence (AAAI 2000)Langkilde I. and Knight K. (1998).
Generation thatExploits Corpus-based Statistical Knowledge.
In Proc.of the COLING-ACL 1998.Lin D. (1998) Automatic Retrieval and Clustering ofSimilar Words.
In Proc.
of the 36th Annual Meeting ofthe Association for Computational Linguistics.Shimohata M. and Sumita E.(2002).
Automatic Para-phrasing Based on Parallel Corpus for Normalization.In Proc.
of the Third International Conference onLanguage Resources and Evaluation.Wang W., Huang J., Zhou M., and Huang C.N.
(2001).Finding Target Language Correspondence for Lexi-calized EBMT System.
In Proc.
of the Sixth NaturalLanguage Processing Pacific Rim Symposium.Zhou M., Ding Y., and Huang C.N.
(2001).
ImprovingTranslation Selection with a New Translation ModelTrained by Independent Monolingual Corpora.Computational Linguistics & Chinese LanguageProcessing.
Vol.
6 No, 1, pp1-26.
