Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 119?124,Prague, June 2007. c?2007 Association for Computational LinguisticsTextual Entailment Through Extended Lexical Overlap andLexico-Semantic MatchingRod Adams, Gabriel Nicolae, Cristina Nicolae and Sanda HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, Texas{rod, gabriel, cristina, sanda}@hlt.utdallas.eduAbstractThis paper presents two systems for textualentailment, both employing decision treesas a supervised learning algorithm.
Thefirst one is based primarily on the con-cept of lexical overlap, considering a bag ofwords similarity overlap measure to form amapping of terms in the hypothesis to thesource text.
The second system is a lexico-semantic matching between the text and thehypothesis that attempts an alignment be-tween chunks in the hypothesis and chunksin the text, and a representation of the textand hypothesis as two dependency graphs.Their performances are compared and theirpositive and negative aspects are analyzed.1 IntroductionTextual entailment is the task of taking a pair of pas-sages, referred to as the text and the hypothesis, andlabeling whether or not the hypothesis (H) can befully inferred from the text (T), as is illustrated inPair 1.
In Pair 1, the knowledge that an attorney rep-resenting someone?s interests entails that they workfor that person.Pair 1 (RTE2 IE 58)T: ?A force majeure is an act of God,?
said attorney PhilWittmann, who represents the New Orleans Saints and ownerTom Benson?s local interests.H: Phil Wittmann works for Tom Benson.The Third PASCAL Recognizing Textual Entail-ment Challenge1 follows the experience of the sec-1http://www.pascal-network.org/Challenges/RTE3/ond challenge (Bar-Haim et al, 2006), whose maintask was to automatically detect if a hypothesis His entailed by a text T. To increase the ?reality?
ofthe task, the text-hypothesis examples were takenfrom outputs of actual systems that solved appli-cations like Question Answering (QA), Informa-tion Retrieval (IR), Information Extraction (IE) andSummarization (SUM).In the challenge, there are two corpora, each con-sisting of 800 annotated pairs of texts and hypothe-ses.
Pairs are annotated as to whether there existsa positive entailment between them and from whichapplication domain each example came from.
In-stances are distributed evenly among the four tasksin both corpora, as are the positive and negative ex-amples.
One corpus was designated for developmentand training, while the other was reserved for test-ing.In the Second PASCAL RTE Challenge (Bar-Haim et al, 2006), one of the best performing sub-missions was (Adams, 2006), which focused onstrict lexical methods so that the system could re-main relatively simple and be easily applied to var-ious entailment applications.
However, this simpleapproach did not take into account details like thesyntactic structure, the coreference or the semanticrelations between words, all necessary for a deeperunderstanding of natural language text.
Thus, a newsystem, based on the same decision tree learning al-gorithm, was designed in an attempt to gain perfor-mance by adding alignment and dependency rela-tions information.
The two systems will be com-pared and their advantages and disadvantages dis-cussed.119This paper is organized as follows: The first sys-tem is discussed in Section 2, followed by the sec-ond system in Section 3.
The experimental resultsare presented in Section 4, and the paper concludesin Section 5.2 Textual entailment through extendedlexical overlapThe first system (Adams, 2006) follows a four stepframework.
The first step is a tokenization processthat applies to the content words of the text andhypothesis.
The second step is building a ?tokenmap?
of how the individual tokens in the hypoth-esis are tied to those in the text, as explained inSection 2.1.
Thirdly, several features, as describedin Section 2.2, are extracted from the token map.Finally, the extracted features are fed into Weka?s(Witten and Frank, 2005) J48 decision tree for train-ing and evaluation.2.1 The token mapCentral to this system is the concept of the tokenmap.
This map is inspired by (Glickman et al,2005)?s use of the most probable lexical entailmentfor each hypothesis pair, but has been modified inhow each pair is evaluated, and that the mappingis stored for additional extraction of features.
Thecomplete mapping is a list of (Hi, Tj) mappings,where Hi represents the ith token in the hypothesis,and Tj is similarly the jth token in the text.
Eachmapping has an associated similarity score.
There isone mapping per token in the hypothesis.
Text to-kens are allowed to appear in multiple mappings.The mappings are created by considering each hy-pothesis token and comparing it to each token in thetext and keeping the one with the highest similarityscore.Similarity scores A similarity score ranging from0.0 to 1.0 is computed for any two tokens via a com-bination of two scores.
This score can be thought ofas the probability that the text token implies the hy-pothesis one, even though the methods used to pro-duce it were not strictly probabilistic in nature.The first score is derived from the cost of a Word-Net (Fellbaum, 1998) path.
The WordNet pathsbetween two tokens are built with the method re-ported in (Hirst and St-Onge, 1998), and designatedas SimWN (Hi, Tj).
Exact word matches are al-ways given a score of 1.0, words that are morpho-logically related or that share a common sense are0.9 and other paths give lower scores down to 0.0.This method of obtaining a path makes use of threegroups of WordNet relations: Up (e.g.
hypernym,member meronym), Down (e.g.
hyponym, cause)and Horizontal (e.g.
nominalization, derivation).The path can only follow certain combinations ofthese groupings, and assigns penalties for each linkin the path, as well as for changing from one direc-tion group to another.The secondary scoring routine is the lexical en-tailment probability, lep(u, v), from (Glickman etal., 2005).
This probability is estimated by takingthe page counts returned from the AltaVista2 searchengine for a combined u and v search term, and di-viding by the count for just the v term.
This can beprecisely expressed as:SimAV (Hi, Tj) =AVCount(Hi &Tj)AVCount(Tj)The two scores are combined such that the sec-ondary score can take up whatever slack the domi-nant score leaves available.
The exact combinationis:Sim(Hi, Tj) = SimWN (Hh, Tt)+ ?
?
(1 ?
SimWN (Hh, Tt)) ?
SimAV (Hh, Tt)where ?
is a tuned constant (?
?
[0, 1]).
Empiricalanalysis found the best results with very low valuesof ?3.
This particular combination was chosen overa strict linear combination, so as to more strongly re-late to SimWN when it?s values are high, but allowSimAV to play a larger role when SimWN is low.2.2 Feature extractionThe following three features were constructed fromthe token map for use in the training of the decisiontree, and producing entailment predictions.Baseline score This score is the product of thesimilarities of the mapped pairs, and is an extensionof (Glickman et al, 2005)?s notion of P (H|T ).
This2http://www.av.com3The results reported here used ?
= 0.1120is the base feature of entailment.ScoreBASE =?
(Hi,Tj)?MapSim(Hi, TJ )One notable characteristic of this feature is thatthe overall score can be no higher than the lowestscore of any single mapping.
The failure to locate astrong similarity for even one token will produce avery low base score.Unmapped negations A token is considered un-mapped if it does not appear in any pair of the tokenmap, or if the score associated with that mapping iszero.
A token is considered a negation if it is in a setlist of terms such as no or not.
Both the text andthe hypothesis are searched for unmapped negations,and total count of them is kept, with the objective ofdetermining whether there is an odd or even num-ber of them.
A (possibly) modified, or flipped, scorefeature is generated:n = # of negations found.ScoreNEG ={ScoreBASE if n is even,1 ?
ScoreBASE if n is odd.Task The task domain used for evaluating entail-ment (i.e.
IE, IR, QA or SUM) was also used as afeature to allow different thresholds among the do-mains.3 Textual entailment throughlexico-semantic matchingThis second system obtains the probability of entail-ment between a text and a hypothesis from a su-pervised learning algorithm that incorporates lexi-cal and semantic information extracted from Word-Net and PropBank.
To generate learning examples,the system computes features that are based uponthe alignment between chunks from the text and thehypothesis.
In the preliminary stage, each instancepair of text and hypothesis is processed by a chunker.The resulting chunks can be simple tokens or com-pound words that exist in WordNet, e.g., pick up.They constitute the lexical units in the next stages ofthe algorithm.identity 1.0 coreference 0.8synonymy 0.8 antonymy -0.8hypernymy 0.5 hyponymy -0.5meronymy 0.4 holonymy -0.4entailment 0.6 entailed by -0.6cause 0.6 caused by -0.6Table 2: Alignment relations and their scores.3.1 AlignmentOnce all the chunks have been identified, the sys-tem searches for alignment candidates between thechunks of the hypothesis and those of the text.
Thesearch pairs all the chunks of the hypothesis, in turn,with all the text chunks, and for each pair it ex-tracts all the relations between the two nodes.
Stopwords and auxiliary verbs are discarded, and onlytwo chunks with the same part of speech are com-pared (a noun must be transformed into a verb tocompare it with another verb).
The alignments ob-tained in this manner constitute a one-to-many map-ping between the chunks of the hypothesis and thechunks of the text.The following relations are identified: (a) iden-tity (between the original spellings, lowercase formsor stems), (b) coreference and (c) WordNet relations(synonymy, antonymy, hypernymy, meronymy, en-tailment and causation).
Each of these relations isattached to a score between -1 and 1, which is hand-crafted by trial and error on the development set (Ta-ble 2).The score is positive if the relation from the textword to the hypothesis word is compatible with anentailment, e.g., identity, coreference, synonymy,hypernymy, meronymy, entailment and causation,and negative in the opposite case, e.g., antonymy,hyponymy, holonymy, reverse entailment and re-verse causation.
This is a way of quantifying in-tuitions like: ?The cat ate the cake?
entails ?Theanimal ate the cake?.
To identify these relations,no word sense disambiguation is performed; instead,all senses from WordNet are considered.
Negationspresent in text or hypothesis influence the sign ofthe score; for instance, if a negated noun is alignedwith a positive noun through a negative link likeantonymy, the two negations cancel each other andthe score of the relation will be positive.
The scoreof an alignment is the sum of the scores of all the121detarrestednumarrestedpolicemanBelgianA posingdealerSwedesThreeSwedesThreewereBelgianadetamod partmodnsubjprep?asdobjartnsubjpassnumauxpassprep_indet amodsting operationpolicean Brusselsprep_inFigure 1: The dependency graphs and alignment candidates for Pair 2 (RTE3 SUM 633).Category Feature name Feature descriptionalignment (score) totaligscore the total alignment score (sum of all scores)totminaligscore the total alignment score when considering only the minimum scored relationfor any two chunks alignedtotmaxaligscore the total alignment score when considering only the maximum scored relationfor any two chunks alignedalignment (count) allaligs the number of chunks aligned considering all alignmentsposaligs the number of chunks aligned considering only positive alignmentsnegaligs the number of chunks aligned considering only negative alignmentsminposaligs the number of alignments that have the minimum of their scores positivemaxposaligs the number of alignments that have the maximum of their scores positiveminnegaligs the number of alignments that have the minimum their scores negativemaxnegaligs the number of alignments that have the maximum of their scores negativedependency edgelabels the pair of labels of non matching edgesmatch the number of relations that match when comparing the two edgesnonmatch the number of relations that don?t match when comparing the two edgesTable 1: Features for lexico-semantic matching.relations between the two words, and if the sum ispositive, the alignment is considered positive.3.2 Dependency graphsThe system then creates two dependency graphs, onefor the text and one for the hypothesis.
The de-pendency graphs are directed graphs with chunks asnodes, interconnected by edges according to the re-lations between them, which are represented as edgelabels.
The tool used is the dependency parser de-veloped by (de Marneffe et al, 2006), which as-signs some of 48 grammatical relations to each pairof words within a sentence.
Further informationis added from the predicate-argument structures inPropBank, e.g., a node can be the ARG0 of anothernode, which is a predicate.Because the text can have more than one sentence,the dependency graphs for each of the sentences arecombined into a larger one.
This is done by col-lapsing together nodes (chunks) that are coreferent,identical or in an nn relation (as given by the parser).The relations between the original nodes and the restof the nodes in the text (dependency links) and nodesin the hypothesis (alignment links) are all inheritedby the new node.
Again, each edge can have multi-ple relations as labels.3.3 FeaturesWith the alignment candidates and dependencygraphs obtained in the previous steps, the systemcomputes the values of the feature set.
The featuresused are of two kinds (Table 1):(a) The alignment features are based on the scoresand counts of the candidate alignments.
All thescores are represented as real numbers between -1and 1, normalized by the number of concepts in thehypothesis.
(b) The dependency features consider each posi-tively scored aligned pair with each of the other pos-itively scored aligned pairs, and compare the set of122detBerlinguersucceedednsubjNattadobjelected drew1984 party Berlinguerdeathsecretary he/Natta 1969 up reportproposingtheexpulsion partythe grouptheauxpasswasprep_in nnprep_afternsubjpasspossnsubjprep_indobjdet partmoddobj prep_fromdet det prep_ofprep_as prtManifestotheFigure 2: The dependency graphs and alignment candidates for Pair 3 (RTE3 IE 19).relations between the two nodes in the text with theset of relations between the two nodes in the hypoth-esis.
This comparison is performed on the depen-dency graphs, on the edges that immediately connectthe two text chunks and the two hypothesis chunks,respectively.
They have numerical values between 0and 1, normalized by the square of the total numberof aligned chunks.3.4 ExamplesPair 2 (RTE3 SUM 633)T: A Belgian policeman posing as an art dealer in Brussels ar-rested three Swedes.H: Three Swedes were arrested in a Belgian police sting opera-tion.Figure 1 illustrates the dependency graphs and align-ment candidates extracted for the instance in Pair 2.There is no merging of graphs necessary here, be-cause the text is made up of a single sentence.
Thevertical line in the center divides the graph corre-sponding to the text from the one corresponding tothe hypothesis.
The dependency relations in the twographs are represented as labels of full lines, whilethe alignment candidate pairs are joined by dottedlines.
As can be observed, the alignment was donebased on identity of spelling, e.g., Swedes-Swedes,and stem, e.g., policeman-police.
For the sake ofsimplicity, the predicate-argument relations have notbeen included in the drawing.
This is a case of a pos-itive instance, and the dependency and alignment re-lations strongly support the entailment.Pair 3 (RTE3 IE 19)T: In 1969, he drew up the report proposing the expulsion fromthe party of the Manifesto group.
In 1984, after Berlinguer?sdeath, Natta was elected as party secretary.H: Berlinguer succeeded Natta.Figure 2 contains an example of a negative in-stance (Pair 3) that cannot be solved through thesimple analysis of alignment and dependency rela-tions.
The graphs corresponding to the two sen-tences of the text have been merged into a singlegraph because of the coreference between the pro-noun he in the first sentence and the proper nameNatta in the second one.
This merging has enrichedthe overall information about relations, but the algo-rithm does not take advantage of this.
To correctlysolve this problem of entailment, one needs addi-tional information delivered by a temporal relationssystem.
The chain of edges between Berlinguer andNatta in the text graph expresses the fact that theevent of Natta?s election happened after Berlinguer?sdeath.
Since the hypothesis states that Berlinguersucceeded Natta, the entailment is obviously false.The system presented in this section will almost cer-tainly solve this kind of instance incorrectly.4 ResultsThe experimental results are summarized in Ta-bles 3 and 4.
The first table presents the accu-racy scores obtained by running the two systemsthrough 10-fold crossvalidation on incremental RTEdatasets.
The first system, based on extended lexicaloverlap (ELO), almost consistently outperforms thesecond system, lexico-semantic matching (LSM),123Evaluation set ELO LSM ELO+LSMJ48 J48 J48 JRipRTE3Dev 66.38 63.63 65.50 67.50+RTE2Dev 64.38 59.19 61.56 62.50+RTE1Dev 62.11 56.67 60.36 59.62+RTE2Test 61.04 57.77 61.51 61.20+RTE1Test 60.07 56.57 59.04 60.42Table 3: Accuracy for the two systems on variousdatasets.Task IE IR QA SUM AllAccuracy 53.50 73.50 80.00 61.00 67.00Table 4: Accuracy by task for the Extended LexicalOverlap system tested on the RTE3Test corpus.and the combination of the two.
The only casewhen the combination gives the best score is on theRTE3 development set, using the rule-based classi-fier JRip.
It can be observed from the table that themore data is added to the evaluation set, the poorerthe results are.
This can be explained by the fact thateach RTE dataset covers a specific kind of instances.Because of this variety in the data, the results ob-tained on the whole collection of RTE datasets avail-able are more representative than the results reportedon each set, because they express the way the sys-tems would perform in real-life natural languageprocessing as opposed to an academic setup.Since the ELO system was clearly the better ofthe two, it was the one submitted to the Third PAS-CAL Challenge evaluation.
Table 4 contains thescores obtained by the system on the RTE3 testingset.
The overall accuracy is 67%, which representsan increase from the score the system achieved at theSecond PASCAL Challenge (62.8%).
The task withthe highest performance was Question Answering,while the task that ranked the lowest was Informa-tion Extraction.
This is understandable, since IE in-volves a very deep understanding of the text, whichthe ELO system is not designed to do.5 ConclusionsThis paper has presented two different approaches ofsolving textual entailment: one based on extendedlexical overlap and the other on lexico-semanticmatching.
The experiments have shown that the firstapproach, while simpler in concept, yields a greaterperformance when applied on the PASCAL RTE3development set.
At first glance, it seems puzzlingthat a simple approach has outperformed one thattakes advantage of a deeper analysis of the text.However, ELO system treats the text naively, as abag of words, and does not rely on any preprocess-ing application.
The LSM system, while attemptingan understanding of the text, uses three other sys-tems that are not perfect: the coreference resolver,the dependency parser and the semantic parser.
Theperformance of the LSM system is limited by theperformance of the tools it uses.
It will be of interestto evaluate this system again once they increase inaccuracy.ReferencesRod Adams.
2006.
Textual entailment through extendedlexical overlap.
In The Second PASCAL RecognisingTextual Entailment Challenge (RTE-2).Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In PASCAL RTE Challenge.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In 5thInternational Conference on Language Resources andEvaluation (LREC 2006).Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.Web based probabilistic textual entailment.
In PAS-CAL RTE Challenge.Graeme Hirst and David St-Onge.
1998.
Lexical chainsas representations of context for the detection and cor-rection of malapropisms.
In Christiane Fellbaum, ed-itor, WordNet: An electronic lexical database, pages305?332.
The MIT Press.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, 2nd edition.124
