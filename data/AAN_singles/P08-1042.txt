Proceedings of ACL-08: HLT, pages 362?370,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAd Hoc Treebank StructuresMarkus DickinsonDepartment of LinguisticsIndiana Universitymd7@indiana.eduAbstractWe outline the problem of ad hoc rules intreebanks, rules used for specific construc-tions in one data set and unlikely to be usedagain.
These include ungeneralizable rules,erroneous rules, rules for ungrammatical text,and rules which are not consistent with the restof the annotation scheme.
Based on a sim-ple notion of rule equivalence and on the ideaof finding rules unlike any others, we developtwo methods for detecting ad hoc rules in flattreebanks and show they are successful in de-tecting such rules.
This is done by examin-ing evidence across the grammar and withoutmaking any reference to context.1 Introduction and MotivationWhen extracting rules from constituency-based tree-banks employing flat structures, grammars oftenlimit the set of rules (e.g., Charniak, 1996), dueto the large number of rules (Krotov et al, 1998)and ?leaky?
rules that can lead to mis-analysis (Fothand Menzel, 2006).
Although frequency-based cri-teria are often used, these are not without problemsbecause low-frequency rules can be valid and po-tentially useful rules (see, e.g., Daelemans et al,1999), and high-frequency rules can be erroneous(see., e.g., Dickinson and Meurers, 2005).
A keyissue in determining the rule set is rule generaliz-ability: will these rules be needed to analyze newdata?
This issue is of even more importance whenconsidering the task of porting a parser trained onone genre to another genre (e.g., Gildea, 2001).
In-frequent rules in one genre may be quite frequent inanother (Sekine, 1997) and their frequency may beunrelated to their usefulness for parsing (Foth andMenzel, 2006).
Thus, we need to carefully considerthe applicability of rules in a treebank to new text.Specifically, we need to examine ad hoc rules,rules used for particular constructions specific to onedata set and unlikely to be used on new data.
This iswhy low-frequency rules often do not extend to newdata: if they were only used once, it was likely fora specific reason, not something we would expect tosee again.
Ungeneralizable rules, however, do notextend to new text for a variety of reasons, not all ofwhich can be captured strictly by frequency.While there are simply phenomena which, for var-ious reasons, are rarely used (e.g., long coordinatedlists), other ungeneralizable phenomena are poten-tially more troubling.
For example, when ungram-matical or non-standard text is used, treebanks em-ploy rules to cover it, but do not usually indicate un-grammaticality in the annotation.
These rules areonly to be used in certain situations, e.g., for ty-pographical conventions such as footnotes, and thefact that the situation is irregular would be usefulto know if the purpose of an induced grammar isto support robust parsing.
And these rules are out-right damaging if the set of treebank rules is in-tended to accurately capture the grammar of a lan-guage.
This is true of precision grammars, whereanalyses can be more or less preferred (see, e.g.,Wagner et al, 2007), and in applications like in-telligent computer-aided language learning, wherelearner input is parsed to detect what is correct ornot (see, e.g., Vandeventer Faltin, 2003, ch.
2).
If atreebank grammar is used (e.g., Metcalf and Boyd,3622006), then one needs to isolate rules for ungram-matical data, to be able to distinguish grammaticalfrom ungrammatical input.Detecting ad hoc rules can also reveal issues re-lated to rule quality.
Many ad hoc rules exist be-cause they are erroneous.
Not only are errors in-herently undesirable for obtaining an accurate gram-mar, but training on data with erroneous rules canbe detrimental to parsing performance (e.g., Dickin-son and Meurers, 2005; Hogan, 2007) As annotationschemes are not guaranteed to be completely con-sistent, other ad hoc rules point to non-uniform as-pects of the annotation scheme.
Thus, identifying adhoc rules can also provide feedback on annotationschemes, an especially important step if one is touse the treebank for specific applications (see, e.g.,Vadas and Curran, 2007), or if one is in the processof developing a treebank.Although statistical techniques have been em-ployed to detect anomalous annotation (Ule andSimov, 2004; Eskin, 2000), these methods do notaccount for linguistically-motivated generalizationsacross rules, and no full evaluation has been doneon a treebank.
Our starting point for detecting adhoc rules is also that they are dissimilar to the restof the grammar, but we rely on a notion of equiva-lence which accounts for linguistic generalizations,as described in section 2.
We generalize equivalencein a corpus-independent way in section 3 to detectad hoc rules, using two different methods to deter-mine when rules are dissimilar.
The results in sec-tion 4 show the success of the method in identifyingall types of ad hoc rules.2 Background2.1 Equivalence classesTo define dissimilarity, we need a notion of simi-larity, and, a starting point for this is the error de-tection method outlined in Dickinson and Meurers(2005).
Since most natural language expressions areendocentric, i.e., a category projects to a phrase ofthe same category (e.g., X-bar Schema, Jackendoff,1977), daughters lists with more than one possiblemother are flagged as potentially containing an er-ror.
For example, IN NP1 has nine different mothersin the Wall Street Journal (WSJ) portion of the Penn1Appendix A lists all categories used in this paper.Treebank (Marcus et al, 1993), six of which are er-rors.This method can be extended to increase recall, bytreating similar daughters lists as equivalent (Dick-inson, 2006, 2008).
For example, the daughters listsADVP RB ADVP and ADVP , RB ADVP in (1) canbe put into the same equivalence class, because theypredict the same mother category.
With this equiv-alence, the two different mothers, PP and ADVP,point to an error (in PP).
(1) a. to slash its work force in the U.S. , [PP[ADV P as] soon/RB [ADV P as next month]]b. to report ... [ADV P [ADV P immediately] ,/,not/RB [ADV P a month later]]Anything not contributing to predicting themother is ignored in order to form equivalenceclasses.
Following the steps below, 15,989 daugh-ters lists are grouped into 3783 classes in the WSJ.1.
Remove daughter categories that are alwaysnon-predictive to phrase categorization, i.e., al-ways adjuncts, such as punctuation and the par-enthetical (PRN) category.2.
Group head-equivalent lexical categories, e.g.,NN (common noun) and NNS (plural noun).3.
Model adjacent identical elements as a singleelement, e.g., NN NN becomes NN.While the sets of non-predictive and head-equivalentcategories are treebank-specific, they require only asmall amount of manual effort.2.2 Non-equivalence classesRules in the same equivalence class not only pre-dict the same mother, they provide support that thedaughters list is accurate?the more rules within aclass, the better evidence that the annotation schemelegitimately licenses that sequence.
A lack of simi-lar rules indicates a potentially anomalous structure.Of the 3783 equivalence classes for the wholeWSJ, 2141 are unique, i.e., have only one uniquedaughters list.
For example, in (2), the daughterslist RB TO JJ NNS is a daughters list with no corre-lates in the treebank; it is erroneous because close towholesale needs another layer of structure, namelyadjective phrase (ADJP) (Bies et al, 1995, p. 179).363(2) they sell [merchandise] for [NP close/RBto/TO wholesale/JJ prices/NNS ]Using this strict equivalence to identify ad hocrules is quite successful (Dickinson, 2008), butit misses a significant number of generalizations.These equivalences were not designed to assist indetermining linguistic patterns from non-linguisticpatterns, but to predict the mother category, and thusmany correct rules are incorrectly flagged.
To pro-vide support for the correct rule NP ?
DT CD JJSNNP JJ NNS in (3), for instance, we need to lookat some highly similar rules in the treebank, e.g.,the three instances of NP ?
DT CD JJ NNP NNS,which are not strictly equivalent to the rule in (3).
(3) [NP the/DT 100/CD largest/JJS Nasdaq/NNPfinancial/JJ stocks/NNS ]3 Rule dissimilarity and generalizability3.1 Criteria for rule equivalenceWith a notion of (non-)equivalence as a heuristic, wecan begin to detect ad hoc rules.
First, however, weneed to redefine equivalence to better reflect syntac-tic patterns.Firstly, in order for two rules to be in thesame equivalence class?or even to be similar?themother must also be the same.
This captures theproperty that identical daughters lists with differ-ent mothers are distinct (cf.
Dickinson and Meurers,2005).
For example, looking back at (1), the oneoccurrence of ADVP?
ADVP , RB ADVP is verysimilar to the 4 instances of ADVP ?
RB ADVP,whereas the one instance of PP?ADVP RB ADVPis not and is erroneous.
Daughters lists are thus nowonly compared to rules with the same mother.Secondly, we use only two steps to determineequivalence: 1) remove non-predictive daughter cat-egories, and 2) group head-equivalent lexical cat-egories.2 While useful for predicting the samemother, the step of Kleene reduction is less usefulfor our purposes since it ignores potential differ-ences in argument structure.
It is important to knowhow many identical categories can appear within agiven rule, to tell whether it is reliable; VP ?
VB2See Dickinson (2006) for the full mappings.NP and VP?
VB NP NP, for example, are two dif-ferent rules.3Thirdly, we base our scores on token counts, in or-der to capture the fact that the more often we observea rule, the more reliable it seems to be.
This is notentirely true, as mentioned above, but this preventsfrequent rules such as NP?
EX (1075 occurrences)from being seen as an anomaly.With this new notion of equivalence, we can nowproceed to accounting for similar rules in detectingad hoc rules.3.2 Reliability scoresIn order to devise a scoring method to reflect simi-lar rules, the simplest way is to use a version of editdistance between rules, as we do under the Wholedaughters scoring below.
This reflects the intuitionthat rules with similar lists of daughters reflect thesame properties.
This is the ?positive?
way of scor-ing rules, in that we start with a basic notion ofequivalence and look for more positive evidence thatthe rule is legitimate.
Rules without such evidenceare likely ad hoc.Our goal, though, is to take the results and exam-ine the anomalous rules, i.e., those which lack strongevidence from other rules.
We can thus more di-rectly look for ?negative?
evidence that a rule is adhoc.
To do this, we can examine the weakest partsof each rule and compare those across the corpus, tosee which anomalous patterns emerge; we do this inthe Bigram scoring section below.Because these methods exploit different proper-ties of rules and use different levels of abstraction,they have complementary aspects.
Both start withthe same assumptions about what makes rules equiv-alent, but diverge in how they look for rules whichdo not fit well into these equivalences.Whole daughters scoring The first method to de-tect ad hoc rules directly accounts for similar rulesacross equivalence classes.
Each rule type is as-signed a reliability score, calculated as follows:1.
Map a rule to its equivalence class.2.
For every rule token within the equivalenceclass, add a score of 1.3Experiments done with Kleene reduction show that the re-sults are indeed worse.3643.
For every rule token within a highly similarequivalence class, add a score of 12 .Positive evidence that a rule is legitimate is ob-tained by looking at similar classes in step #3, andthen rules with the lowest scores are flagged as po-tentially ad hoc (see section 4.1).
To determinesimilarity, we use a modified Levenshtein distance,where only insertions and deletions are allowed; adistance of one qualifies as highly similar.4 Allow-ing two or more changes would be problematic forunary rules (e.g., (4a), and in general, would allowus to add and subtract dissimilar categories.
We thusremain conservative in determining similarity.Also, we do not utilize substitutions: while theymight be useful in some cases, it is too problematicto include them, given the difference in meaning ofeach category.
Consider the problematic rules in (4).In (4a), which occurs once, if we allow substitutions,then we will find 760 ?comparable?
instances of VP?
VB, despite the vast difference in category (verbvs.
adverb).
Likewise, the rule in (4b), which occurs8 times, would be ?comparable?
to the 602 instancesof PP ?
IN PP, used for multi-word prepositionslike because of.5 To maintain these true differences,substitutions are not allowed.
(4) a. VP?
RBb.
PP?
JJ PPThis notion of similarity captures many general-izations, e.g., that adverbial phrases are optional.For example, in (5), the rule reduces to S ?
PPADVP NP ADVP VP.
With a strict notion of equiv-alence, there are no comparable rules.
However, theclass S ?
PP NP ADVP VP, with 198 members,is highly similar, indicating more confidence in thiscorrect rule.
(5) [S [PP During his years in Chiriqui] ,/, [ADV Phowever] ,/, [NP Mr. Noriega] [ADV P also][V P revealed himself as an officer as perverseas he was ingenious] ./.
]4The score is thus more generally 11+distance , although weascribe no theoretical meaning to this5Rules like PP ?
JJ PP might seem to be correct, but thisdepends upon the annotation scheme.
Phrases starting with dueto are sometimes annotated with this rule, but they also occuras ADJP or ADVP or with due as RB.
If PP?
JJ PP is correct,identifying this rule actually points to other erroneous rules.Bigram scoring The other method of detecting adhoc rules calculates reliability scores by focusingspecifically on what the classes do not have in com-mon.
Instead of examining and comparing rules intheir entirety, this method abstracts a rule to its com-ponent parts, similar to features using informationabout n-grams of daughter nodes in parse rerankingmodels (e.g., Collins and Koo, 2005).We abstract to bigrams, including added STARTand END tags, as longer sequences risk missing gen-eralizations; e.g., unary rules would have no compa-rable rules.
We score rule types as follows:1.
Map a rule to its equivalence class, resulting ina reduced rule.2.
Calculate the frequency of each<mother,bigram> pair in a reduced rule:for every reduced rule token with the samepair, add a score of 1 for that bigram pair.3.
Assign the score of the least-frequent bigram asthe score of the rule.We assign the score of the lowest-scoring bigrambecause we are interested in anomalous sequences.This is in the spirit of Kve?ton and Oliva (2002),who define invalid bigrams for POS annotation se-quences in order to detect annotation errors..As one example, consider (6), where the reducedrule NP?
NP DT NNP is composed of the bigramsSTART NP, NP DT, DT NNP, and NNP END.
All ofthese are relatively common (more than a hundredoccurrences each), except for NP DT, which appearsin only two other rule types.
Indeed, DT is an in-correct tag (NNP is correct): when NP is the firstdaughter of NP, it is generally a possessive, preclud-ing the use of a determiner.
(6) (NP (NP ABC ?s) (??
??)
(DT This) (NNPWeek))The whole daughters scoring misses such prob-lematic structures because it does not explicitly lookfor anomalies.
The disadvantage of the bigram scor-ing, however, is its missing of the big picture: forexample, the erroneous rule NP?NNP CC NP getsa large score (1905) because each subsequence isquite common.
But this exact sequence is rather rare(NNP and NP are not generally coordinated), so thewhole daughters scoring assigns a low score (4.0).3654 EvaluationTo gauge our success in detecting ad hoc rules, weevaluate the reliability scores in two main ways: 1)whether unreliable rules generalize to new data (sec-tion 4.1), and, more importantly, 2) whether the un-reliable rules which do generalize are ad hoc in otherways?e.g., erroneous (section 4.2).
To measurethis, we use sections 02-21 of the WSJ corpus astraining data to derive scores, section 23 as testingdata, and section 24 as development data.4.1 Ungeneralizable rulesTo compare the effectiveness of the two scoringmethods in identifying ungeneralizable rules, we ex-amine how many rules from the training data do notappear in the heldout data, for different thresholds.In figure 1, for example, the method identifies 3548rules with scores less than or equal to 50, 3439 ofwhich do not appear in the development data, result-ing in an ungeneralizability rate of 96.93%.To interpret the figures below, we first need toknow that of the 15,246 rules from the training data,1832 occur in the development data, or only 12.02%,corresponding to 27,038 rule tokens.
There are also396 new rules in the development data, making for atotal of 2228 rule types and 27,455 rule tokens.4.1.1 Development data resultsThe results are shown in figure 1 for the wholedaughters scoring method and in figure 2 for the bi-gram method.
Both methods successfully identifyrules with little chance of occurring in new data, thewhole daughters method performing slightly better.Thresh.
Rules Unused Ungen.1 311 311 100.00%25 2683 2616 97.50%50 3548 3439 96.93%100 4596 4419 96.15%Figure 1: Whole daughter ungeneralizability (devo.
)4.1.2 Comparing across dataIs this ungeneralizability consistent over differentdata sets?
To evaluate this, we use the whole daugh-ters scoring method, since it had a higher ungener-alizability rate in the development data, and we useThresh.
Rules Unused Ungen.1 599 592 98.83%5 1661 1628 98.01%10 2349 2289 97.44%15 2749 2657 96.65%20 3120 2997 96.06%Figure 2: Bigram ungeneralizability (devo.
)section 23 of the WSJ and the Brown corpus portionof the Penn Treebank.Given different data sizes, we now report the cov-erage of rules in the heldout data, for both type andtoken counts.
For instance, in figure 3, for a thresh-old of 50, 108 rule types appear in the developmentdata, and they appear 141 times.
With 2228 totalrule types and 27,455 rule tokens, this results in cov-erages of 4.85% and 0.51%, respectively.In figures 3, 4, and 5, we observe the same trendsfor all data sets: low-scoring rules have little gener-alizability to new data.
For a cutoff of 50, for exam-ple, rules at or below this mark account for approxi-mately 5% of the rule types used in the data and halfa percent of the tokens.Types TokensThresh.
Used Cov.
Used Cov.10 23 1.03% 25 0.09%25 67 3.01% 78 0.28%50 108 4.85% 141 0.51%100 177 7.94% 263 0.96%All 1832 82.22% 27,038 98.48%Figure 3: Coverage of rules in WSJ, section 24Types TokensThresh.
Used Cov.
Used Cov.10 33 1.17% 39 0.08%25 82 2.90% 117 0.25%50 155 5.49% 241 0.51%100 242 8.57% 416 0.88%All 2266 80.24% 46,375 98.74%Figure 4: Coverage of rules in WSJ, section 23Note in the results for the larger Brown corpusthat the percentage of overall rule types from the366Types TokensThresh.
Used Cov.
Used Cov.10 187 1.51% 603 0.15%25 402 3.25% 1838 0.45%50 562 4.54% 2628 0.64%100 778 6.28% 5355 1.30%All 4675 37.75% 398,136 96.77%Figure 5: Coverage of rules in Brown corpustraining data is only 37.75%, vastly smaller than theapproximately 80% from either WSJ data set.
Thisillustrates the variety of the grammar needed to parsethis data versus the grammar used in training.We have isolated thousands of rules with littlechance of being observed in the evaluation data, and,as we will see in the next section, many of the ruleswhich appear are problematic in other ways.
Theungeneralizabilty results make sense, in light of thefact that reliability scores are based on token counts.Using reliability scores, however, has the advantageof being able to identify infrequent but correct rules(cf.
example (5)) and also frequent but unhelpfulrules.
For example, in (7), we find erroneous casesfrom the development data of the rules WHNP ?WHNP WHPP (five should be NP) and VP?
NNPNP (OKing should be VBG).
These rules appear 27and 16 times, respectively, but have scores of only28.0 and 30.5, showing their unreliability.
Futurework can separate the effect of frequency from theeffect of similarity (see also section 4.3).
(7) a.
[WHNP [WHNP five] [WHPP of whom]]b. received hefty sums for * [V P OKing/NNP[NP the purchase of ...]]4.2 Other ad hoc rulesThe results in section 4.1 are perhaps unsuprising,given that many of the identified rules are simplyrare.
What is important, therefore, is to figure outwhy some rules appeared in the heldout data atall.
As this requires qualitative analysis, we hand-examined the rules appearing in the developmentdata.
We set out to examine about 100 rules, andso we report only for the corresponding threshold,finding that ad hoc rules are predominant.For the whole daughters scoring, at the 50 thresh-old, 55 (50.93%) of the 108 rules in the developmentdata are errors.
Adding these to the ungeneralizablerules, 98.48% (3494/3548) of the 3548 rules are un-helpful for parsing, at least for this data set.
An ad-ditional 12 rules cover non-English or fragmentedconstructions, making for 67 clearly ad hoc rules.For the bigram scoring, at the 20 threshold, 67(54.47%) of the 123 rules in the development dataare erroneous, and 8 more are ungrammatical.
Thismeans that 97.88% (3054/3120) of the rules at thisthreshold are unhelpful for parsing this data, stillslightly lower than the whole daughters scoring.4.2.1 Problematic casesBut what about the remaining rules for both meth-ods which are not erroneous or ungrammatical?First, as mentioned at the outset, there are severalcases which reveal non-uniformity in the annota-tion scheme or guidelines.
This may be justifiable,but it has an impact on grammars using the annota-tion scheme.
Consider the case of NAC (not a con-stituent), used for complex NP premodifiers.
Thedescription for tagging titles in the guidelines (Bieset al, 1995, p. 208-209) covers the exact case foundin section 24, shown in (8a).
This rule, NAC?
NPPP, is one of the lowest-scoring rules which occurs,with a whole daughters score of 2.5 and a bigramscore of 3, yet it is correct.
Examining the guide-lines more closely, however, we find examples suchas (8b).
Here, no extra NP layer is added, and it isnot immediately clear what the criteria are for hav-ing an intermediate NP.
(8) a. a ?
[NAC [NP Points] [PP of Light]] ?
foun-dationb.
The Wall Street Journal ?
[NAC AmericanWay [PP of Buying]] ?
SurveySecondly, rules with mothers which are simplyrare are prone to receive lower scores, regardless oftheir generalizability.
For example, the rules dom-inated by SINV, SQ, or SBARQ are all correct (6in whole daughters, 5 in bigram), but questions arenot very frequent in this news text: SQ appearsonly 350 times and SBARQ 222 times in the train-ing data.
One might thus consider normalizing thescores based on the overall frequency of the parent.Finally, and most prominently, there are issueswith coordinate structures.
For example, NP?
NNCC DT receives a low whole daughters score of 7.0,367despite the fact that NP ?
NN and NP ?
DT arevery common rules.
This is a problem for both meth-ods: for the whole daughters scoring, of the 108,28 of them had a conjunct (CC or CONJP) in thedaughters list, and 18 of these were correct.
Like-wise, for the bigram scoring, 18 had a conjunct, and12 were correct.
Reworking similarity scores to re-flect coordinate structures and handle each case sep-arately would require treebank-specific knowledge:the Penn Treebank, for instance, distinguishes unlikecoordinated phrases (UCP) from other coordinatedphrases, each behaving differently.4.2.2 Comparing the methodsThere are other cases in which one method out-performs the other, highlighting their strengths andweaknesses.
In general, both methods fare badlywith clausal rules, i.e., those dominated by S, SBAR,SINV, SQ, or SBARQ, but the effect is slightlygreater on the bigram scoring, where 20 of the 123rules are clausal, and 16 of these are correct (i.e.,80% of them are misclassified).
To understand this,we have to realize that most modifiers are adjoinedat the sentence level when there is any doubt abouttheir attachment (Bies et al, 1995, p. 13), leading tocorrect but rare subsequences.
In sentence (9), forexample, the reduced rule S ?
SBAR PP NP VParises because both the introductory SBAR and thePP are at the same level.
This SBAR PP sequence isfairly rare, resulting in a bigram score of 13.
(9) [S [SBAR As the best opportunities for corpo-rate restructurings are exhausted * of course],/, [PP at some point] [NP the market] [V P willstart * to reject them] ./.
]Whole daughters scoring, on the other hand, assignsthis rule a high reliability score of 2775.0, due tothe fact that both SBAR NP VP and PP NP VPsequences are common.
For rules with long mod-ifier sequences, whole daughters scoring seems tobe more effective since modifiers are easily skippedover in comparing to other rules.
Whole daughtersscoring is also imprecise with clausal rules (10/12are misclassified), but identifies less of them, andthey tend to be for rare mothers (see above).Various cases are worse for the whole daughtersscoring.
First are quantifier phrases (QPs), whichhave a highly varied set of possible heads and argu-ments.
QP is ?used for multiword numerical expres-sions that occur within NP (and sometimes ADJP),where the QP corresponds frequently to some kindof complex determiner phrase?
(Bies et al, 1995, p.193).
This definition leads to rules which look dif-ferent from QP to QP.
Some of the lowest-scoring,correct rules are shown in (10).
We can see that thereis not a great deal of commonality about what com-prises quantifier phrases, even if subparts are com-mon and thus not flagged by the bigram method.
(10) a.
[QP only/RB three/CD of/IN the/DTnine/CD] justicesb.
[QP too/RB many/JJ] cooksc.
10 % [QP or/CC more/JJR]Secondly, whole daughters scoring relies on com-plete sequences, and thus whether Kleene reduction(step #3 in section 2) is used makes a marked dif-ference.
For example, in (11), the rule NP?
DT JJNNP NNP JJ NN NN is completely correct, despiteits low whole daughters score of 15.5 and one oc-currence.
This rule is similar to the 10 occurrencesof NP ?
DT JJ NNP JJ NN in the training set, butwe cannot see this without performing Kleene re-duction.
For noun phrases at least, using Kleene re-duction might more accurately capture comparabil-ity.
This is less of an issue for bigram scoring, asall the bigrams are perfectly valid, resulting here ina relatively high score (556).
(11) [NP the/DT basic/JJ Macintosh/NNPPlus/NNP central/JJ processing/NN unit/NN ]4.3 Discriminating rare rulesIn an effort to determine the effectiveness of thescores on isolating structures which are not linguis-tically sound, in a way which factors out frequency,we sampled 50 rules occurring only once in thetraining data.
We marked for each whether it wascorrect or how it was ad hoc, and we did this blindly,i.e., without knowledge of the rule scores.
Of these50, only 9 are errors, 2 cover ungrammatical con-structions, and 8 more are unclear.
Looking at thebottom 25 scores, we find that the whole daughtersand bigrams methods both find 6 errors, or 67% ofthem, additionally finding 5 unclear cases for thewhole daughters and 6 for the bigrams method.
Er-roneous rules in the top half appear to be ones which368happened to be errors, but could actually be correctin other contexts (e.g.,NP ?
NN NNP NNP CD).Although it is a small data set, the scores seem to beeffectively sorting rare rules.5 Summary and OutlookWe have outlined the problem of ad hoc rules intreebanks?ungeneralizable rules, erroneous rules,rules for ungrammatical text, and rules which are notnecessarily consistent with the rest of the annotationscheme.
Based on the idea of finding rules unlikeany others, we have developed methods for detectingad hoc rules in flat treebanks, simply by examiningproperties across the grammar and without makingany reference to context.We have been careful not to say how to usethe reliability scores.
First, without 100% accu-racy, it is hard to know what their removal froma parsing model would mean.
Secondly, assign-ing confidence scores to rules, as we have done,has a number of other potential applications.
Parsereranking techniques, for instance, rely on knowl-edge about features other than those found in thecore parsing model in order to determine the bestparse (e.g., Collins and Koo, 2005; Charniak andJohnson, 2005).
Active learning techniques also re-quire a scoring function for parser confidence (e.g.,Hwa et al, 2003), and often use uncertainty scoresof parse trees in order to select representative sam-ples for learning (e.g., Tang et al, 2002).
Both couldbenefit from more information about rule reliability.Given the success of the methods, we can striveto make them more corpus-independent, by remov-ing the dependence on equivalence classes.
In someways, comparing rules to similar rules already natu-rally captures equivalences among rules.
In this pro-cess, it will also be important to sort out the impactof similarity from the impact of frequency on iden-tifying ad hoc structures.AcknowledgmentsThanks to the three anonymous reviewers for theirhelpful comments.
This material is based upon worksupported by the National Science Foundation underGrant No.
IIS-0623837.A Relevant Penn Treebank categoriesCC Coordinating conjunctionCD Cardinal numberDT DeterminerEX Existential thereIN Preposition or subordinating conjunctionJJ AdjectiveJJR Adjective, comparativeJJS Adjective, superlativeNN Noun, singular or massNNS Noun, pluralNNP Proper noun, singularRB AdverbTO toVB Verb, base formVBG Verb, gerund or present participleFigure 6: POS tags in the PTB (Santorini, 1990)ADJP Adjective PhraseADVP Adverb PhraseCONJP Conjunction PhraseNAC Not A ConstituentNP Noun PhrasePP Prepositional PhrasePRN ParentheticalQP Quantifier PhraseS Simple declarative clauseSBAR Clause introduced by subordinating conjunctionSBARQ Direct question introduced by wh-word/phraseSINV Inverted declarative sentenceSQ Inverted yes/no questionUCP Unlike Coordinated PhraseVP Verb PhraseWHNP Wh-noun PhraseWHPP Wh-prepositional PhraseFigure 7: Syntactic categories in the PTB (Bies et al,1995)ReferencesBies, Ann, Mark Ferguson, Karen Katz and RobertMacIntyre (1995).
Bracketing Guidelines forTreebank II Style Penn Treebank Project.
Univer-sity of Pennsylvania.Charniak, Eugene (1996).
Tree-Bank Grammars.Tech.
Rep. CS-96-02, Department of ComputerScience, Brown University, Providence, RI.369Charniak, Eugene and Mark Johnson (2005).Coarse-to-fine n-best parsing and MaxEnt dis-criminative reranking.
In Proceedings of ACL-05.Ann Arbor, MI, USA, pp.
173?180.Collins, Michael and Terry Koo (2005).
Discrim-inative Reranking for Natural Language Parsing.Computational Linguistics 31(1), 25?69.Daelemans, Walter, Antal van den Bosch and JakubZavrel (1999).
Forgetting Exceptions is Harmfulin Language Learning.
Machine Learning 34, 11?41.Dickinson, Markus (2006).
Rule Equivalence forError Detection.
In Proceedings of TLT 2006.Prague, Czech Republic.Dickinson, Markus (2008).
Similarity and Dissim-ilarity in Treebank Grammars.
In 18th Interna-tional Congress of Linguists (CIL18).
Seoul.Dickinson, Markus and W. Detmar Meurers (2005).Prune Diseased Branches to Get Healthy Trees!How to Find Erroneous Local Trees in a Treebankand Why It Matters.
In Proceedings of TLT 2005.Barcelona, Spain.Eskin, Eleazar (2000).
Automatic Corpus Correc-tion with Anomaly Detection.
In Proceedings ofNAACL-00.
Seattle, Washington, pp.
148?153.Foth, Kilian and Wolfgang Menzel (2006).
RobustParsing: More with Less.
In Proceedings of theworkshop on Robust Methods in Analysis of Nat-ural Language Data (ROMAND 2006).Gildea, Daniel (2001).
Corpus Variation and ParserPerformance.
In Proceedings of EMNLP-01.Pittsburgh, PA.Hogan, Deirdre (2007).
Coordinate Noun PhraseDisambiguation in a Generative Parsing Model.In Proceedings of ACL-07.
Prague, pp.
680?687.Hwa, Rebecca, Miles Osborne, Anoop Sarkar andMark Steedman (2003).
Corrected Co-training forStatistical Parsers.
In Proceedings of ICML-2003.Washington, DC.Jackendoff, Ray (1977).
X?
Syntax: A Study ofPhrase Structure.
Cambridge, MA: MIT Press.Krotov, Alexander, Mark Hepple, Robert J.Gaizauskas and Yorick Wilks (1998).
Compact-ing the Penn Treebank Grammar.
In Proceedingsof ACL-98.
pp.
699?703.Kve?ton, Pavel and Karel Oliva (2002).
Achievingan Almost Correct PoS-Tagged Corpus.
In Text,Speech and Dialogue (TSD).
pp.
19?26.Marcus, M., Beatrice Santorini and M. A.Marcinkiewicz (1993).
Building a large annotatedcorpus of English: The Penn Treebank.
Compu-tational Linguistics 19(2), 313?330.Metcalf, Vanessa and Adriane Boyd (2006).
Head-lexicalized PCFGs for Verb Subcategorization Er-ror Diagnosis in ICALL.
In Workshop on Inter-faces of Intelligent Computer-Assisted LanguageLearning.
Columbus, OH.Santorini, Beatrice (1990).
Part-Of-Speech TaggingGuidelines for the Penn Treebank Project (3rd Re-vision, 2nd printing).
Tech.
Rep. MS-CIS-90-47,The University of Pennsylvania, Philadelphia, PA.Sekine, Satoshi (1997).
The Domain Dependence ofParsing.
In Proceedings of ANLP-96.
Washing-ton, DC.Tang, Min, Xiaoqiang Luo and Salim Roukos(2002).
Active Learning for Statistical NaturalLanguage Parsing.
In Proceedings of ACL-02.Philadelphia, pp.
120?127.Ule, Tylman and Kiril Simov (2004).
UnexpectedProductions May Well be Errors.
In Proceedingsof LREC 2004.
Lisbon, Portugal, pp.
1795?1798.Vadas, David and James Curran (2007).
AddingNoun Phrase Structure to the Penn Treebank.
InProceedings of ACL-07.
Prague, pp.
240?247.Vandeventer Faltin, Anne (2003).
Syntactic error di-agnosis in the context of computer assisted lan-guage learning.
The`se de doctorat, Universite?
deGene`ve, Gene`ve.Wagner, Joachim, Jennifer Foster and Josef vanGenabith (2007).
A Comparative Evaluation ofDeep and Shallow Approaches to the AutomaticDetection of Common Grammatical Errors.
InProceedings of EMNLP-CoNLL 2007. pp.
112?121.370
