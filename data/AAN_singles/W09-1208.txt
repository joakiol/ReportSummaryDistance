Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55?60,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultilingual Dependency Learning:A Huge Feature Engineering Method to Semantic Dependency Parsing ?Hai Zhao(??)?
?, Wenliang Chen(???
)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and LinguisticsCity University of Hong Kong83 Tat Chee Avenue, Kowloon, Hong Kong, China?Language Infrastructure Group, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289?School of Computer Science and TechnologySoochow University, Suzhou, China 215006haizhao@cityu.edu.hk, chenwl@nict.go.jpAbstractThis paper describes our system about mul-tilingual semantic dependency parsing (SR-Lonly) for our participation in the shared taskof CoNLL-2009.
We illustrate that semanticdependency parsing can be transformed intoa word-pair classification problem and im-plemented as a single-stage machine learningsystem.
For each input corpus, a large scalefeature engineering is conducted to select thebest fit feature template set incorporated with aproper argument pruning strategy.
The systemachieved the top average score in the closedchallenge: 80.47% semantic labeled F1 for theaverage score.1 IntroductionThe syntactic and semantic dependency parsing inmultiple languages introduced by the shared taskof CoNLL-2009 is an extension of the CoNLL-2008 shared task (Hajic?
et al, 2009).
Seven lan-guages, English plus Catalan, Chinese, Czech, Ger-man, Japanese and Spanish, are involved (Taule?
etal., 2008; Palmer and Xue, 2009; Hajic?
et al, 2006;Surdeanu et al, 2008; Burchardt et al, 2006; Kawa-hara et al, 2002).
This paper presents our researchfor participation in the semantic-only (SRLonly)challenge of the CoNLL-2009 shared task, with a?This study is partially supported by CERG grant 9040861(CityU 1318/03H), CityU Strategic Research Grant 7002037,Projects 60673041 and 60873041 under the National NaturalScience Foundation of China and Project 2006AA01Z147 un-der the ?863?
National High-Tech Research and Developmentof China.highlight on our strategy to select features from alarge candidate set for maximum entropy learning.2 System SurveyWe opt for the maximum entropy model with Gaus-sian prior as our learning model for all classificationsubtasks in the shared task.
Our implementation ofthe model adopts L-BFGS algorithm for parameteroptimization as usual.
No additional feature selec-tion techniques are applied.Our system is basically improved from its earlyversion for CoNLL-2008 (Zhao and Kit, 2008).
Byintroducing a virtual root for every predicates, Thejob to determine both argument labels and predicatesenses is formulated as a word-pair classificationtask in four languages, namely, Catalan, Spanish,Czech and Japanese.
In other three languages, Chi-nese, English and German, a predicate sense clas-sifier is individually trained before argument labelclassification.
Note that traditionally (or you maysay that most semantic parsing systems did so) ar-gument identification and classification are handledin a two-stage pipeline, while ours always tacklesthem in one step, in addition, predicate sense classi-fication are also included in this unique learning/teststep for four of all languages.3 Pruning Argument CandidatesWe keep using a word-pair classification procedureto formulate semantic dependency parsing.
Specif-ically, we specify the first word in a word pair as apredicate candidate (i.e., a semantic head, and notedas p in our feature representation) and the next as anargument candidate (i.e., a semantic dependent, and55noted as a).
We do not differentiate between verbaland non-verbal predicates and our system handlesthem in the exactly same way.When no constraint available, however, all wordpairs in the an input sequence must be considered,leading to very poor efficiency in computation forno gain in effectiveness.
Thus, the training sampleneeds to be pruned properly.
As predicates overtlyknown in the share task, we only consider how toeffectively prune argument candidates.We adopt five types of argument pruning strate-gies for seven languages.
All of them assume that asyntactic dependency parsing tree is available.As for Chinese and English, we continue to usea dependency version of the pruning algorithm of(Xue and Palmer, 2004) as described in (Zhao andKit, 2008).
The pruning algorithm is readdressed asthe following.Initialization: Set the given predicate candidateas the current node;(1) The current node and all of its syntactic chil-dren are selected as argument candidates.
(2) Reset the current node to its syntactic head andrepeat step (1) until the root is reached.Note that the given predicate candidate itself isexcluded from the argument candidate list for Chi-nese, that is slightly different from English.The above pruning algorithm has been shown ef-fective.
However, it is still inefficient for a single-stage argument identification/classification classifi-cation task.
Thus we introduce an assistant argumentlabel ?
NoMoreArgument?
to alleviate this difficulty.If an argument candidate in the above algorithm islabeled as such a label, then the pruning algorithmwill end immediately.
In training, this assistant labelmeans no more samples will be generated for thecurrent predicate, while in test, the decoder will notsearch more argument candidates any more.
Thisadaptive technique more effectively prunes the ar-gument candidates.
In fact, our experiments show1/3 training memory and time may be saved from it.As for Catalan and Spanish, only syntactic chil-dren of the predicate are considered as the argumentcandidates.As for Czech, only syntactic children, grandchil-dren, great-grandchildren, parent and siblings of thepredicate are taken as the argument candidates.As for German, only syntactic children, grand-children, parent, siblings, siblings of parent and sib-lings of grandparent of the predicate are taken as theargument candidates.The case is somewhat sophisticated for Japanese.As we cannot identify a group of simple predicate-argument relations from the syntactic tree.
Thuswe consider top frequent 28 syntactic relations be-tween the predicate and the argument.
The parserwill search all words before and after the predicate,and only those words that hold one of the 28 syn-tactic relations to the predicate are considered asthe argument candidate.
Similar to the pruning al-gorithm for Chinese/English/German, we also in-troduce two assistant labels ?
leftNoMoreArgument?and ?
rightNoMoreArgument?
to adaptively prunewords too far away from the predicate.4 Feature TemplatesAs we don?t think that we can benefit from know-ing seven languages, an automatic feature templateselection is conducted for each language.About 1000 feature templates (hereafter this tem-plate set is referred to FT ) are initially considered.These feature templates are from various combina-tions or integrations of the following basic elements.Word Property.
This type of elements includeword form, lemma, part-of-speech tag (PoS), FEAT(additional morphological features), syntactic de-pendency label (dprel), semantic dependency label(semdprel) and characters (char) in the word form(only suitable for Chinese and Japanese)1.Syntactic Connection.
This includes syntactichead (h), left(right) farthest(nearest) child (lm, ln,rm, and rn), and high(low) support verb or noun.We explain the last item, support verb(noun).
Fromthe predicate or the argument to the syntactic rootalong the syntactic tree, the first verb(noun) that ismet is called as the low support verb(noun), and thenearest one to the root is called as the high supportverb(noun).Semantic Connection.
This includes semantic1All lemmas, PoS, and FEAT for either training or test arefrom automatically pre-analyzed columns of every input files.56FEATn 1 2 3 4 5 6 7 8 9 10 11Catalan/Spanish postype gen num person mood tense punctCzech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per TenTable 1: Notations of FEATshead (semhead), left(right) farthest(nearest) seman-tic child (semlm, semln, semrm, semrn).
We saya predicate is its argument?s semantic head, and thelatter is the former?s child.
Features related to thistype may track the current semantic parsing status.Path.
There are two basic types of path betweenthe predicate and the argument candidates.
One isthe linear path (linePath) in the sequence, the otheris the path in the syntactic parsing tree (dpPath).
Forthe latter, we further divide it into four sub-typesby considering the syntactic root, dpPath is the fullpath in the syntactic tree.
Leading two paths to theroot from the predicate and the argument, respec-tively, the common part of these two paths will bedpPathShare.
Assume that dpPathShare starts froma node r?, then dpPathPred is from the predicate tor?, and dpPathArgu is from the argument to r?.Family.
Two types of children sets for the predi-cate or argument candidate are considered, the firstincludes all syntactic children (children), the secondalso includes all but excludes the left most and theright most children (noFarChildren).Concatenation of Elements.
For all collected el-ements according to linePath, children and so on, weuse three strategies to concatenate all those stringsto produce the feature value.
The first is seq, whichconcatenates all collected strings without doing any-thing.
The second is bag, which removes all dupli-cated strings and sort the rest.
The third is noDup,which removes all duplicated neighbored strings.In the following, we show some feature templateexamples derived from the above mentioned items.a.lm.lemma The lemma of the left most child ofthe argument candidate.p.h.dprel The dependant label of the syntactichead of the predicate candidate.a.pos+p.pos The concatenation of PoS of the ar-gument and the predicate candidates.p?1.pos+p.pos PoS of the previous word of thepredicate and PoS of the predicate itself.a:p|dpPath.lemma.bag Collect all lemmas alongthe syntactic tree path from the argument to the pred-icate, then removed all duplicated ones and sort therest, finally concatenate all as a feature string.a:p.highSupportNoun|linePath.dprel.seq Collectall dependant labels along the line path from the ar-gument to the high support noun of the predicate,then concatenate all as a feature string.
(a:p|dpPath.dprel.seq)+p.FEAT1 Collect all de-pendant labels along the line path from the argumentto the predicate and concatenate them plus the firstFEAT of the predicate.An important feature for the task is dpTreeRela-tion, which returns the relationship of a and p in asyntactic parse tree and cannot be derived from com-bining the above basic elements.
The possible valuesfor this feature include parent, sibling etc.5 Automatically Discovered FeatureTemplate SetsFor each language, starting from a basic feature tem-plate set (a small subset of FT ) according to ourprevious result in English dependency parsing, eachfeature template outside the basic set is added andeach feature template inside the basic set is removedone by one to check the effectiveness of each fea-ture template following the performance change inthe development set.
This procedure will be contin-uously repeated until no feature template is added orremoved or the performance is not improved.There are some obvious heuristic rules that helpus avoid trivial feature template checking, for ex-ample, FEAT features are only suitable for Cata-lan, Czech and Spanish.
Though FEAT features arealso available for Japanese, we don?t adopt them forthis language due to the hight training cost.
To sim-plify feature representation, we use FEAT1, FEAT2,and so on to represent different FEAT for every lan-guages.
A lookup list can be found in Table 1.
Ac-cording to the list, FEAT4 represents person forCatalan or Spanish, but Cas for Czech.As we don?t manually interfere the selection pro-cedure for feature templates, ten quite different fea-57Ca Ch Cz En Gr Jp SpCa 53Ch 5 75Cz 11 10 76En 11 11 12 73Gr 7 7 7 14 45Jp 6 22 13 15 10 96Sp 22 9 18 15 9 12 66Table 2: Feature template set: argument classifierCh En GrCh 46En 5 9Gr 17 2 40Table 3: Feature template set: sense classifierture template sets are obtained at last.
Statistical in-formation of seven sets for argument classifiers is inTable 2, and those for sense classifiers are in Table 3.Numbers in the diagonals of these two tables meanthe numbers of feature templates, and others meanhow many feature templates are identical for everylanguage pairs.
The most matched feature templatesets are for Catalan/Spanish and Chinese/Japanese.As for the former, it is not so surprised because thesetwo corpora are from the same provider.Besides the above statistics, these seven featuretemplate sets actually share little in common.
Forexample, the intersection set from six languages, asChinese is excluded, only includes one feature tem-plate, p.lemma (the lemma of the predicate candi-date).
If all seven sets are involved, then such an in-tersection set will be empty.
Does this mean humanlanguages share little in semantic representation?
:)It is unlikely to completely demonstrate full fea-ture template sets for all languages in this short re-port, we thus only demonstrate two sets, one for En-glish sense classification in Table 4 and the other forCatalan argument classification in Table 52.6 Word Sense DeterminationThe shared task of CoNLL-2009 still asks for thepredicate sense.
In our work for CoNLL-2008 (Zhaoand Kit, 2008), this was done by searching for a right2Full feature lists and their explanation for all languages willbe available at the website, http://bcmi.sjtu.edu.cn/?zhaohai.p.lm.posp.rm.posp.lemmap.lemma + p.lemma1p.lemma + p.children.dprel.noDupp.lemma + p.currentSensep.formp.form?1 + p.formp.form + p.form1Table 4: Feature set for English sense classificationexample in the given dictionary.
Unfortunately, welate found this caused a poor performance in sensedetermination.
This time, an individual classifier isused to determine the sense for Chinese, English orGerman, and this is done by the argument classifierby introducing a virtual root for every predicates forthe rest four languages3.
Features used for sensedetermination are also selected following the sameprocedure in Section 5.
The difference is only pred-icate related features are used for selection.7 DecodingThe decoding for four languages, Catalan, Czech,Japanese and Spanish is trivial, each word pairs willbe checked one by one.
The first word of the pairis the virtual root or the predicate, the second is thepredicate or every argument candidates.
Argumentcandidates are checked in the order of different syn-tactic relations to their predicate, which are enumer-ated by the pruning algorithms in Section 3, or fromleft to right for the same syntactic relation.
Afterthe sense of the predicate is determined, the label ofeach argument candidate will be directly classified,or, it is proved non-argument.As for the rest languages, Chinese, English orGerman, after the sense classifier outputs its result,an optimal argument structure for each predicate isdetermined by the following maximal probability.Sp = argmax?iP (ai|ai?1, ai?2, ...), (1)where Sp is the argument structure, P (ai|ai?1...)is the conditional probability to determine the la-bel of the i-th argument candidate label.
Note that3For Japanese, no senses for predicates are defined.
Thus itis actually a trivial classification task in this case.58p.currentSense + p.lemmap.currentSense + p.posp.currentSense + a.posp?1.FEAT1p.FEAT2p1.FEAT3p.semrm.semdprelp.lm.dprelp.form + p.children.dprel.bagp.lemman (n = ?1, 0)p.lemma + p.lemma1p.pos?1 + p.posp.pos1p.pos + p.children.dprel.baga.FEAT1 + a.FEAT3 + a.FEAT4+ a.FEAT5 + a.FEAT6a?1.FEAT2 + a.FEAT2a.FEAT3 + a1.FEAT3a.FEAT3 + a.h.FEAT3a.children.FEAT1.noDupa.children.FEAT3.baga.h.lemmaa.lm.dprel + a.forma.lm.forma.lm?1.lemmaa.lmn.pos (n=0,1)a.noFarChildren.pos.bag + a.rm.forma.pphead.lemmaa.rm.dprel + a.forma.rm?1.forma.rm.lemmaa.rn.dprel + a.forma.lowSupportVerb.lemmaa?1.forma.form + a1.forma.form + a.children.posa.lemma + a.h.forma.lemma + a.pphead.forma1.lemmaa1.pos + a.pos.seqa.pos + a.children.dprel.baga.lemma + p.lemma(a:p|dpPath.dprel) + p.FEAT1a:p|linePath.distancea:p|linePath.FEAT1.baga:p|linePath.form.seqa:p|linePath.lemma.seqa:p|linePath.dprel.seqa:p|dpPath.lemma.seqa:p|dpPath.lemma.baga:p|dpPathArgu.lemma.seqa:p|dpPathArgu.lemma.bagTable 5: Feature set for Catalan argument classificationP (ai|ai?1, ...) in equation (1) may be simplified asP (ai) if the input feature template set does not con-cerned with the previous argument label output.
Abeam search algorithm is used to find the parsing de-cision sequence.8 Evaluation ResultsOur evaluation is carried out on two computationalservers, (1) LEGA, a 64-bit ubuntu Linux installedserver with double dual-core AMD Opteron proces-sors of 2.8GHz and 24GB memory.
This server wasalso used for our previous participation in CoNLL-2008 shared task.
(2) MEGA, a 64-bit ubuntu Linuxinstalled server with six quad-core Intel Xeon pro-cessors of 2.33GHz and 128GB memory.Altogether nearly 60,000 machine learning rou-tines were run to select the best fit feature templatesets for all seven languages within two months.
BothLEGA and MEGA were used for this task.
How-ever, training and test for the final submission ofChinese, Czech and English run in MEGA, and therest in LEGA.
As we used multiple thread trainingand multiple routines run at the same time, the exacttime cost for either training or test is hard to esti-mate.
Here we just report the actual time and mem-ory cost in Table 7 for reference.The official evaluation results of our system are inTable 6.
Numbers in bold in the table stand for thebest performances for the specific languages.
Theresults in development sets are also given.
The firstrow of the table reports the results using golden in-put features.Two facts as the following suggest that our systemdoes output robust and stable results.
The first is thattwo results for development and test sets in the samelanguage are quite close.
The second is about out-of-domain (OOD) task.
Though for each OOD task, wejust used the same model trained from the respectivelanguage and did nothing to strengthen it, this doesnot hinder our system to obtain top results in Czechand English OOD tasks.In addition, the feature template sets from auto-matical selection procedure in this task were usedfor the joint task of this shared task, and also outputtop results according to the average score of seman-tic labeled F1 (Zhao et al, 2009).59average Catalan Chinese Czech English German Japanese SpanishDevelopment with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46Out-of-domain 74.34 85.44 73.31 64.26Table 6: Semantic labeled F1Catalan Chinese Czech English German Japanese SpanishSense Training memory (MB) 418.0 136.0 63.0Training time (Min.)
11.0 2.5 1.7Test time (Min.)
0.7 0.2 0.03Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4Test time (Min.)
3.0 144.0 27.1 88.0 1.0 4.2 7.0Table 7: Computational cost9 ConclusionAs presented in the above sections, we have tackledsemantic parsing for the CoNLL-2009 shared taskas a word-pair classification problem.
Incorporatedwith a proper argument candidate pruning strategyand a large scale feature engineering for each lan-guage, our system produced top results.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008).Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.Nianwen Xue and Martha Palmer.
2004.
Calibrating fea-tures for semantic role labeling.
In 2004 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-2004), pages 88?94, Barcelona, Spain,July 25-26.Hai Zhao and Chunyu Kit.
2008.
Parsing syntac-tic and semantic dependencies with two single-stagemaximum entropy models.
In Twelfth Conference onComputational Natural Language Learning (CoNLL-2008), pages 203?207, Manchester, UK, August 16-17.Hai Zhao, Wenliang Chen, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009.
Multilin-gual dependency learning: Exploiting rich features fortagging syntactic and semantic dependencies.
In Pro-ceedings of the 13th Conference on ComputationalNatural Language Learning (CoNLL-2009), June 4-5,Boulder, Colorado, USA.60
