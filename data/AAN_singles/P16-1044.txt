Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 464?473,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsImproved Representation Learning for Question Answer MatchingMing Tan, Cicero dos Santos, Bing Xiang & Bowen ZhouIBM Watson Core TechnologiesYorktown Heights, NY, USA{mingtan,cicerons,bingxia,zhou}@us.ibm.comAbstractPassage-level question answer matching isa challenging task since it requires effec-tive representations that capture the com-plex semantic relations between questionsand answers.
In this work, we propose aseries of deep learning models to addresspassage answer selection.
To match pas-sage answers to questions accommodat-ing their complex semantic relations, un-like most previous work that utilizes a sin-gle deep learning structure, we develophybrid models that process the text us-ing both convolutional and recurrent neu-ral networks, combining the merits on ex-tracting linguistic information from bothstructures.
Additionally, we also developa simple but effective attention mechanismfor the purpose of constructing better an-swer representations according to the in-put question, which is imperative for bet-ter modeling long answer sequences.
Theresults on two public benchmark datasets,InsuranceQA and TREC-QA, show thatour proposed models outperform a varietyof strong baselines.1 IntroductionPassage-level answer selection is one of the es-sential components in typical question answering(QA) systems.
It can be defined as follows: Givena question and a pool of candidate passages, se-lect the passages that contain the correct answer.The performance of the passage selection task isnot only crucial to non-factoid QA systems, wherea question is expected to be answered with a se-quence of descriptive text (e.g.
the question in Ta-ble 1), but also very important to factoid QA sys-tems, where the answer passage selection step isQuestion: Does Medicare cover my spouse?Ground-truth answer: If your spouse has workedand paid Medicare taxes for the entire required 40quarters, or is eligible for Medicare by virtue ofbeing disabled or some other reason, your spousecan receive his/her own medicare benefits.
If yourspouse has not met those qualifications, if you havemet them, and if your spouse is age 65, he/she canreceive Medicare based on your eligibility.Another candidate answer: If you were married toa Medicare eligible spouse for at least 10 years, youmay qualify for Medicare.
If you are widowed, andhave not remarried, and you were married to yourspouse at least 9 months before your spouse?s death,you may be eligible for Medicare benefits under aspouse provision.Table 1: An example of a question with theground-truth answer and a negative answer ex-tracted from the InsuranceQA dataset.also known as passage scoring.
In factoid QA, ifthe sentences selected by the passage scorer mod-ule do not contain the answer, it will definitely leadto an incorrect response from the QA system.One central challenge of this task lies in thecomplex and versatile semantic relations observedbetween questions and passage answers.
For ex-ample, while the task of supporting passage selec-tion for factoid QA may be largely cast as a textualentailment problem, what makes an answer betterthan another in the real world for non-factoid QAoften depends on many factors.Specifically, different from many other pair-matching NLP tasks, the linguistic similarities be-tween questions and answers may or may not beindicative for our task.
This is because, dependingon what the question is looking for, a good answermay come in different forms: sometimes a correct464answer completes the question precisely with themissing information, and in other scenarios, goodanswers need to elaborate part of the question torationalize it, and so on.
For instance, the ques-tion in Table 1 only contains five words, while thebest answer uses 60 words for elaboration.
On theother hand, the best answers from a pool can alsobe noisy and include extraneous information irrel-evant to the question.
Additionally, while a goodanswer must relate to the question, they often donot share common lexical units.
For instance, inthe example question, ?cover?
is not directly men-tioned in the answer.
This issue may confuse sim-ple word-matching systems.These challenges consequently make hand-crafting features much less desirable compared todeep learning based methods.
Furthermore, theyalso require our systems to learn how to distin-guish useful pieces from irrelevant ones, and fur-ther, to focus more on the former.Finally, the system should be capable of cap-turing the nuances between the best answer andan acceptable one.
For example, the second an-swer in Table 1 is suitable for a questioner, whosespouse is Medicare eligible, asking about his/herown coverage, while the example question is morelikely asked by a person, who is Medicare eligible,asking about his/her spouse?
coverage.
Clearly,the first answer is more appropriate for the ques-tion, although the second one implicitly answersit.
A good system should reflect this preference.While this task is usually approached as apairwise-ranking problem, the best strategy to cap-ture the association between the questions and an-swers is still an open problem.
Established ap-proaches normally suffer from two weaknesses atthis point.
First, prior work, such as (Feng etal., 2015; Wang and Nyberg, 2015), resort to ei-ther convolutional neural network (CNN) or re-current neural network (RNN) respectively.
How-ever, each structure describes only one semanticperspective of the text.
CNN emphasizes the lo-cal interaction within n-gram, while RNN is de-signed to capture long range information and for-get unimportant local information.
How to com-bine the merits from both has not been sufficientlyexplored.
Secondly, previous approaches are usu-ally based on independently generated questionand answer embeddings; the quality of such rep-resentations, however, usually degrades as the an-swer sequences grow longer.In this work, we propose a series of deeplearning models in order to address such weak-nesses.
We start with the basic discriminativeframework for answer selection.
We first proposetwo independent models, Convolutional-poolingLSTM and Convolution-based LSTM, which aredesigned to benefit from both of the two popu-lar deep learning structures to distinguish betterbetween useful and irrelevant pieces presented inquestions and answers.
Next, by breaking the in-dependence assumption of the question and an-swer embedding, we introduce an effective atten-tion mechanism to generate answer representa-tions according to the question, such that the em-beddings do not overlook informative parts of theanswers.
We report experimental results for twoanswer selection datasets: (1) InsuranceQA (Fenget al, 2015)1, a recently released large-scale non-factoid QA dataset from the insurance domain,and (2) TREC-QA2, which was created by Wanget al (2007) based on Text REtrieval Conference(TREC) QA track data.The contribution of this paper is hence three-fold: 1) We propose hybrid neural networks,which learn better representations for both ques-tions and answers by combining merits of bothRNN and CNN.
2) We prove the effectiveness ofattention on the answer selection task, which hasnot been sufficiently explored in prior work.
3) Weachieve the state-of-the-art results on both TREC-QA and InsuranceQA datasets.The rest of the paper is organized as follows:Section 2 describes the related work for answer se-lection; Section 3 provides the details of the pro-posed models; Experimental settings and resultsare discussed in Section 4 and 5; Finally, we drawconclusions in Section 6.2 Related workPrevious work on answer selection normally usedfeature engineering, linguistic tools, or external re-sources.
For example, semantic features were con-structed based on WordNet in (Yih et al, 2013).This model pairs semantically related words basedon word semantic relations.
In (Wang and Man-ning, 2010; Wang et al, 2007), the answer se-lection problem was transformed to a syntacti-1git clone https://github.com/shuzi/insuranceQA.git (Weuse the V1 version of this dataset).2The data is obtained from (Yao et al, 2013)http://cs.jhu.edu/?xuchen/packages/jacana-qa-naacl2013-data-results.tar.bz2465cal matching between the question/answer parsetrees.
Some work tried to fulfill the matching us-ing minimal edit sequences between dependencyparse trees (Heilman and Smith, 2010; Yao et al,2013).
Discriminative tree-edit feature extractionand engineering over parsing trees were automatedin (Severyn and Moschitti, 2013).
Such methodsmight suffer from the availability of additional re-sources, the effort of feature engineering and thesystematic complexity introduced by the linguis-tic tools, such as parse trees and dependency trees.Some recent work has used deep learning meth-ods for the passage-level answer selection task.The approaches normally pursue the solution onthe following directions.
First, a joint feature vec-tor is constructed based on both the question andthe answer, and then the task can be converted intoa classification or ranking problem (Wang and Ny-berg, 2015; Hu et al, 2014).
Second, recentlyproposed models for text generation can intrinsi-cally be used for answer selection and generation(Bahdanau et al, 2015; Vinyals and Le, 2015).
Fi-nally, the question and answer representations canbe learned and then matched by certain similaritymetrics (Feng et al, 2015; Yu et al, 2014; dosSantos et al, 2015; Qiu and Huang, 2015).
Fun-damentally, our proposed models belong to the lastcategory.Meanwhile, attention-based systems haveshown very promising results on a variety of NLPtasks, such as machine translation (Bahdanau etal., 2015; Sutskever et al, 2014), machine readingcomprehension (Hermann et al, 2015), text sum-marization (Rush et al, 2015) and text entailment(Rockt?aschel et al, 2016).
Such models learnto focus their attention to specific parts of theirinput and most of them are based on a one-wayattention, in which the attention is basicallyperformed merely over one type of input basedon another (e.g.
over target languages based onthe source languages for machine translation, orover documents according to queries for readingcomprehension).
Most recently, several two-wayattention mechanisms are proposed, where the in-formation from the two input items can influencethe computation of each others representations.Rockt?aschel et al (2016) develop a two-wayattention mechanism including another one-wayattention over the premise conditioned on thehypothesis, in addition to the one over hypothesisconditioned on premise.
dos Santos et al (2016)and Yin et al (2015) generate interactive attentionweights on both inputs by assignment matrices.Yin et al (2015) use a simple Euclidean distanceto compute the interdependence between the twoinput texts, while dos Santos et al (2016) resort toattentive parameter matrices.3 ApproachesIn this section, we first present our basic discrim-inative framework for answer selection based onlong short-term memory (LSTM), which we callQA-LSTM.
Next, we detail the proposed hybridand attentive neural networks that are built on topof the QA-LSTM framework.3.1 LSTM for Answer SelectionOur LSTM implementation is similar to theone in (Graves et al, 2013) with minor mod-ifications.
Given an input sequence X ={x(1),x(2), ?
?
?
,x(n)}, where x(t) is an E-dimension word vector in this paper, the hiddenvector h(t) (with size H) at the time step t is up-dated as follows.it= ?
(Wix(t) +Uih(t?
1) + bi) (1)ft= ?
(Wfx(t) +Ufh(t?
1) + bf) (2)ot= ?
(Wox(t) +Uoh(t?
1) + bo) (3)?Ct= tanh(Wcx(t) +Uch(t?
1) + bc)(4)Ct= it?
?Ct+ ft?
Ct?1(5)ht= ot?
tanh(Ct) (6)There are three gates (input i, forget f and out-put o), and a cell memory vector Ct. ?
is thesigmoid function.
W ?
RH?E, U ?
RH?Hand b ?
RH?1are the network parameters.Single-direction LSTMs suffer from the weak-ness of not making use of the contextual informa-tion from the future tokens.
Bidirectional LSTMs(biLSTMs) use both the previous and future con-text by processing the sequence in two directions,and generate two sequences of output vectors.
Theoutput for each token is the concatenation of thetwo vectors from both directions, i.e.
ht=??ht??
?ht.QA-LSTM: Our basic answer selection frame-work is shown in Figure 1.
Given an input pair(q,a), where q is a question and a is a candidate an-swer, first we retrieve the word embeddings (WEs)of both q and a.
Then, we separately apply abiLSTM over the two sequences of WEs.
Next,466we generate a fixed-sized distributed vector rep-resentations using one of the following three ap-proaches: (1) the concatenation of the last vec-tors on both directions of the biLSTM; (2) averagepooling over all the output vectors of the biLSTM;(3) max pooling over all the output vectors.
Fi-nally, we use cosine similarity sim(q, a) to scorethe input (q, a) pair.
It is important to note that thesame biLSTM is applied to both q and a.Similar to (Feng et al, 2015; Weston et al,2014; Hu et al, 2014), we define the training ob-jective as a hinge loss.L = max{0,M?sim(q, a+)+sim(q, a?)}
(7)where a+is a ground truth answer, a?is an incor-rect answer randomly chosen from the entire an-swer space, andM is a margin.
We treat any ques-tion with more than one ground truth as multipletraining examples.
During training, for each ques-tion we randomly sample K negative answers, butonly use the one with the highest L to update themodel.
Finally, dropout operation is performed onthe representations before cosine similarity match-ing.The same scoring function, loss function andnegative sampling procedure is also used in theNN architectures presented in what follows.3.2 Convolutional LSTMsThe pooling strategies used in QA-LSTM sufferfrom the incapability of filtering important localinformation, especially when dealing with longanswer sequences.Also, it is well known that LSTM models suc-cessfully keep the useful information from long-range dependency.
But the strength has a trade-off effect of ignoring the local n-gram coherence.This can be partially alleviated with bidirectionalarchitectures.Meanwhile, the convolutional structures havebeen widely used in the question answering tasks,Figure 1: Basic Model: QA-LSTMsuch as (Yu et al, 2014; Feng et al, 2015; Huet al, 2014).
Classical convolutional layers usu-ally emphasize the local lexical connections of then-gram.
However, the local pieces are associatedwith each other only at the pooling step.
No long-range dependencies are taken into account duringthe formulation of convolution vectors.Fundamentally, recurrent and convolutionalneural networks have their own pros and cons, dueto their different topologies.
How to keep bothmerits motivates our studies of the following twohybrid models.3.2.1 Convolutional-pooling LSTMsIn Figure 2 we detail the convolutional-poolingLSTM architecture.
In this NN architecture, wereplace the simple pooling layers (average/max-pooling) by a convolutional layer, which allowsto capture richer local information by applying aconvolution over sequences of LSTM output vec-tors.
The number of output vectors k (contextwindow size) considered by the convolution is ahyper-parameter of the model.The convolution structure adopted in this workis as follows: Z ?
Rk|h|?Lis a matrix wherethe m-th column is the concatenation of k hiddenvectors generated from biLSTM centralized in them-th word of the sequence, L is the length of thesequence after wide convolution (Kalchbrenner etal., 2014).
The output of the convolution with cfilters is,C = tanh(WcpZ) (8)where Wcpare network parameters, and C ?Rc?L.
The j-th element of the representation vec-tors (oqand oa) is computed as follows,[oj] = max1<l<L[Cj,l] (9)Figure 2: Convolutional-pooling LSTM4673.2.2 Convolution-based LSTMsIn Figure 3, we detail our second hybrid NN ar-chitecture.
The aim of this approach is to capturethe local n-gram interaction at the lower level us-ing a convolution.
At the higher level, we buildbidirectional LSTMs, which extract the long rangedependency based on convoluted n-gram.
Com-bining convolutional and recurrent structures havebeen investigated in prior work other than questionanswering (Donahue et al, 2015; Zuo et al, 2015;Sainath et al, 2015).As shown in Figure 3, the model first retrievesword vectors for each token in the sequence.
Next,we compose the matrix D ?
RkE?L, where eachcolumn l in D consists of the concatenation of kword vectors of size E centered at the l-th word.The matrix X ?
Rc?L, which is the output of theconvolution with c filters is computed as follows:X = tanh(WcbD) (10)The matrix X is the input to the biLSTM structurein Eqs.
1-6.
After the biLSTM step, we use max-pooling over the biLSTM output vectors to obtainthe representations of both q and a.3.3 Attentive LSTMsIn the previous subsections, the two most populardeep learning architectures are integrated to gen-erate semantic representations for questions andanswers from both the long-range sequential andlocal n-gram perspectives.QA-LSTM and the two proposed hybrid mod-els are basically siamese networks (Chopra et al,2005).
These structures overlook another poten-tial issue.
The answers might be extremely longand contain lots of words that are not related to thequestion at hand.
No matter what advanced neuralnetworks are exploited at the answer side, the re-sulting representation might still be distracted bynon-useful information.
A typical example is theFigure 3: Convolution-based LSTMsecond candidate answer in Table 1.
If the con-struction of the answer representation is not awareof the input question, the representation might bestrongly influenced by n-grams such as ?are wid-owed?
and ?your spouse?s death?, which are in-formative if we only look at the candidate answer,but are not so important for the input question.We address this problem by developing a simpleattention model for the answer vector generation,in order to alleviate this weakness by dynamicallyaligning the more informative parts of answers tothe questions.Inspired by the work in (Hermann et al, 2015),we develop a very simple but efficient word-levelattention on the basic model.
In Figure 4, we detailour Attentive LSTM architecture.
Prior to the av-erage or mean pooling, each biLSTM output vec-tor is multiplied by a softmax weight, which is de-termined by the question representation from biL-STM.
Specifically, given the output vector of biL-STM on the answer side at time step t, ha(t), andthe question representation, oq, the updated vector?ha(t) for each answer token are formulated below.ma,q(t) = Wamha(t) +Wqmoq(11)sa,q(t) ?
exp(wTmstanh(ma,q(t))) (12)?ha(t) = ha(t)sa,q(t) (13)where Wam, Wqmand wmsare attention pa-rameters.
Conceptually, the attention mechanismgives more weight to certain words of the can-didate answer, where the weights are computedby taking into consideration information from thequestion.
The expectation is that words in the can-didate answer that are more important with regardto the input question should receive larger weights.The attention mechanism in this paper is con-ceptually analogous to the one used in one-layerFigure 4: Attentive LSTM468Train Validation Test1 Test2# of Qs 12887 1000 1800 1800# of As 18540 1454 2616 2593Table 2: Numbers of Qs and As in InsuranceQA.memory network (Sukhbaatar et al, 2015).
Thefundamental difference is that the transformedquestion vector and answer unit vectors are com-bined in an inner-product pattern in order to gener-ate attentive weights in memory network, whereasthis work adopts a summation operation (Eq.
11).4 InsuranceQA ExperimentsThe first dataset we use to evaluate the proposedapproaches is the InsuranceQA, which has beenrecently proposed by Feng et al (2015).
We usethe first version of this dataset.
This dataset con-tains question and answer pairs from the insurancedomain and is already divided into a training set, avalidation set, and two test sets.
We do not see anyobvious categorical differentiation between twotests?
questions.
We list the numbers of questionsand answers of the dataset in Table 2.
We referthe reader to (Feng et al, 2015), for more detailsregarding the InsuranceQA data.
In this dataset, aquestion may have multiple correct answers, andnormally the questions are much shorter than an-swers.
The average length of questions in tokens is7, while the average length of answers is 94.
Suchdifference posts additional challenges for the an-swer selection task.
This corpus contains 24981unique answers in total.
For the development andtest sets, the InsuranceQA also includes an answerpool of 500 candidate answers for each question.These answer pools were constructed by includingthe correct answer(s) and randomly selected can-didates from the complete set of unique answers.The top-1 accuracy of the answer selection is re-ported.4.1 SetupThe proposed models are implemented withTheano (Bastien et al, 2012) and all experimentsare conducted in a GPU cluster.
We use the accu-racy on validation set to select the best epoch andbest hyper-parameter settings for testing.The word embeddings are pre-trained, usingword2vec (Mikolov et al, 2013)3.
The trainingdata for the word embeddings is a Wikipedia cor-3https://code.google.com/p/word2vec/pus of 164 million tokens combined with the ques-tions and answers in the InsuranceQA training set.The word vector size is set to 100.
Word embed-dings are also part of the parameters and are op-timized during the training.
Stochastic GradientDescent (SGD) is the optimization strategy.
Thelearning rate ?
is 1.1.
We get the best perfor-mances when the negative answer count K = 50.We also tried different margins in the hing lossfunction, and finally fixed the margin as M=0.2.We train our models in mini-batches (with batchsize as 20), and the maximum length L of ques-tions and answers is 200.
Any tokens out of thisrange are discarded.
In order to get more obviouscomparison between the proposed models and thebasic framework, with respect to the ground-truthanswer length in Fig.
5, we also provide the resultsof K = 1.
In this case, we set M = 0.1, ?
= 0.1and mini-batches as 100 to get the best perfor-mance on the validation set.
Also, the dimen-sion of LSTM output vectors is 141x2 for bidirec-tional LSTM in QA-LSTM, Attentive LSTM andConvolutional-pooling LSTM, such that biLSTMhas a comparable number of parameters with asingle-direction LSTM with 200 dimensions.
ForConvolution-based LSTM, since LSTM structureis built on the top of CNN, we fixed the CNN out-put as 282 dimensions and tune the biLSTM hid-den vector size in the experiments.Because the sequences within a mini-batch havedifferent lengths, we use a mask matrix to indicatethe real length of each sequence.4.2 BaselinesFor comparison, we report the performances offour baselines in the top group in Table 3: twostate-of-the-art non-DL approaches and two varia-tions of a strong DL approach based on CNN.Bag-of-word: The idf-weighted sum of wordvectors is used as a feature vector.
The candidatesare ranked by the cosine similarity to the question.Metzler-Bendersky IR model: A state-of-the-art weighted dependency model (Bendersky et al,2010; Bendersky et al, 2011), which employsa weighted combination of term-based and termproximity-based features to score each candidate.Architecture-II in (Feng et al, 2015): A CNNmodel is employed to learn distributed representa-tions of questions and answers.
Cosine similarityis used to rank answers.469Model Validation Test1 Test2Bag-of-word 31.9 32.1 32.2Metzler-Bendersky IR model 52.7 55.1 50.8CNN (Feng et al, 2015) 61.8 62.8 59.2CNN with GESD (Feng et al, 2015) 65.4 65.3 61.0A QA-LSTM (head/tail) 54.8 53.6 51.0B QA-LSTM (avg pooling,K=50) 55.0 55.7 52.4C QA-LSTM (max pooling,K=1) 64.3 63.1 58.0D QA-LSTM (max pooling,K=50) 66.6 66.6 63.7E Conv-pooling LSTM (c=4000,K=1) 66.2 64.6 62.2F Conv-pooling LSTM (c=200,K=50) 66.4 67.4 63.5G Conv-pooling LSTM (c=400,K=50) 67.8 67.5 64.4H Conv-based LSTM (|h|=200,K=50) 66.0 66.1 63.0I Conv-based LSTM (|h|=400,K=50) 67.1 67.6 64.4J QA-CNN (max-pooling, k = 3) 61.6 62.2 57.9K Attentive CNN (max-pooling, k = 3) 62.3 63.3 60.2L Attentive LSTM (avg-pooling K=1) 68.4 68.1 62.2M Attentive LSTM (avg-pooling K=50) 68.4 67.8 63.2N Attentive LSTM (max-pooling K=50) 68.9 69.0 64.8Table 3: The experimental results of InsuranceQA.Architecture-II with Geometricmean of Eu-clidean and Sigmoid Dot product (GESD): Co-sine similarity is replaced by GESD, which got thebest performance in (Feng et al, 2015).4.3 Results and discussionsIn this section, we provide detailed analysis on theexperimental results.
Table 3 summarizes the re-sults of our models on InsuranceQA.
From Row(A) to (D), we list QA-LSTM without either CNNstructure or attention.
They vary on the poolingmethod used.
We can see that by concatenatingthe last vectors from both directions, (A) performsthe worst.
We see that using max-pooling (C) ismuch better than average pooling (B).
The poten-tial reason may be that the max-pooling extractsmore local values for each dimension.
Comparedto (C), (D) is better, showing the need of multiplenegative answers in training.Row (E) to (I) show the results ofConvolutional-pooling LSTMs and Convolution-based LSTMs with different filter sizes c, biLSTMhidden sizes |h| and negative answer pool sizeK.
Increasing the negative answer pool size,we are allowed to use less filter counts (F vs E).Larger filter counts help on the test accuracies(G vs F) for Convolutional-pooling LSTMs.
Wehave the same observation with larger biLSTMhidden vector size for Convolution-based LSTMs.Both convolutional models outperform the plainQA-LSTM (D) by about 1.0% on test1, and 0.7%on test2.Rows (L-N) correspond to QA-LSTM with theattention model, with either max-pooling or aver-age pooling.
We observe that max-pooling is bet-ter than avg-pooling, which is consistent with QA-LSTMs.
In comparison to Model (D), Model (N)shows over 2% improvement on both validationand Test1 sets.
And (N) gets improvements overthe best baseline in Table 3 by 3.5%, 3.7% and3.8% on the validation, Test1 and Test2 sets, re-spectively.
Compared to Architecture II in (Fenget al, 2015), which involved a large number ofCNN filters, (N) model also has fewer parameters.We also test the proposed attention mechanismon convolutional networks.
(J) replaces the LSTMin QA-LSTM with a convolutional layer.
We setthe filter size c = 400 and window size k = 3 ac-cording to the validation accuracy.
(K) performsthe similar attention on the convolutional outputof the answers.
Similar to biLSTM, the attentionon the convolutional layer gives over 2% accu-racy improvement on both test sets, which provesthe attention?s efficiency on both CNN and RNNstructures.Finally, we investigate the proposed models onhow they perform with respect to long answers.To better illustrate the performance difference, we470Models MAP MRR(Yao et al, 2013) 0.631 0.748(Severyn and Moschitti, 2013) 0.678 0.736(Yih et al, 2013)-BDT 0.694 0.789(Yih et al, 2013)-LCLR 0.709 0.770(Wang and Nyberg, 2015) 0.713 0.791Architecture-II (Feng et al, 2015) 0.711 0.800(Severyn and Moschitti, 2015) 0.671 0.728w/o additional features(Severyn and Moschitti, 2015) 0.746 0.808with additional featuresA.
QA-CNN 0.714 0.807B.
QA-LSTM (max-pooling) 0.733 0.819C.
Conv-pooling LSTM 0.742 0.819D.
Conv-based LSTM 0.737 0.827E.
Attentive LSTM 0.753 0.830Table 4: The test set results on TREC-QAcompare the models with K = 1 (i.e.
the mod-els C, E, L).
We divide the questions of Test1 andTest2 sets into eleven buckets, according to theaverage length of their ground truth answers.
Asshown in Figure 5, QA-LSTM gets better or simi-lar performance compared to the proposed mod-els on buckets with shorter answers (L ?
50,50 < L ?55, 55 < L ?60).
As the answerlengths increase, the gap between QA-LSTM andother models becomes more obvious.
It suggeststhe effectiveness of Convolutional-pooling LSTMand Attentive LSTM for long-answer questions.In (Feng et al, 2015), GESD outperforms co-sine similarity in their models.
However, the pro-posed models with GESD as similarity scores donot provide any improvement on the accuracy.5 TREC-QA ExperimentsIn this section we detail our experimental setupand results using the TREC-QA dataset.5.1 Data, metrics and baselinesWe test the models on TREC-QA dataset, cre-ated based on Text REtrieval Conference (TREC)QA track (8-13) data.
More detail of the gener-ation steps for this data can be found in (Wanget al, 2007).
We follow the exact approach oftrain/dev/test questions selection in (Wang andNyberg, 2015), in which all questions with onlypositive or negative answers are removed.
Finally,we have 1162 training, 65 development and 68test questions.
Similar to previous work, we useMean Average Precision (MAP) and Mean Recip-rocal Rank (MRR) as evaluation metrics, whichare evaluated using the official scripts.In the top part of Table 4, we list the perfor-mance of recent prior work on this dataset.
Weimplemented the Architecture II in (Feng et al,2015) from scratch.
The CNN structure in (Sev-eryn and Moschitti, 2015) combined with addi-tional human-designed features achieved the bestMAP and MRR.5.2 SetupWe keep the configurations same as those in Insur-anceQA in section 4.1, except the following differ-ences: 1) Following Wang and Nyberg (2015), weuse 300-dimensional vectors that were trained andprovided by word2vec (Mikolov et al, 2013) us-ing a part of the Google News dataset4.
2) Sincethe word vectors of TREC-QA have a greater di-mension than InsuranceQA, we accordingly havelarger biLSTM hidden vectors and CNN filters, inorder not to lose information from word vectors.Here we set both of them as 600.
3) We use themodels from the epoch with the best MAP on thevalidation set.
4) We also observe that becauseof the smaller data size, we need a decayed learn-ing rate ?
in order to stablize the models?
training.Specifically, we set the initial ?0= 1.1, and de-crease it for each epoch T > 1 as ?T= ?0/T .5) We fix the negative answer size K = 50 duringtraining.5.3 ResultsThe bottom part of Table 4 shows the perfor-mance of the proposed models.
For the compar-ison purpose, we replace biLSTM with a convo-lution in Model (A), and also use max-pooling toget question and answer embeddings, and call thismodel QA-CNN.
QA-LSTM (B) improves MAPand MRR in more than 1% when compared toQA-CNN (A).
Compared to (B), convolutional-pooling (C) performs better on MAP by 0.9%,and convolution-based models on MAP by 0.4%and MRR by 0.8%.
Attentive LSTM is the bestproposed model, and outperforms the best base-line (Severyn and Moschitti, 2015) by 0.7% onMAP and 2.2% on MRR.
Note that the best re-sult in (Severyn and Moschitti, 2015) was obtainedby combining CNN-based features with additionalhuman-defined features.
In contrast, our attentiveLSTM model achieves higher performance with-out using any human-defined features.6 ConclusionIn this paper, we address the following problemfor the answer passage selection: how can we con-struct the embeddings for questions and candidate4https://code.google.com/archive/p/word2vec/471Figure 5: The accuracy of Test1 and Test2 of InsuranceQA sets for three models, i.e.
maxpooling QA-LSTM (C), Convolutional-pooling LSTM (E) and Attentive LSTM (L) in Table 3, on different levels ofground truth answer lengths on each test set.
The figures show the accuracy of each bucket.answers, in order to better distinguish the correctanswers from other candidates?
We propose threeindependent models in two directions.
First, wedevelop two hybrid models which combine thestrength of both recurrent and convolutional neu-ral networks.
Second, we introduce a simple one-way attention mechanism, in order to generate an-swer embeddings influenced by the question con-text.
Such attention fixes the issue of independentgeneration of the question and answer embeddingsin previous work.
All proposed models are de-parted from a basic architecture, built on bidirec-tional LSTMs.
We conduct experiments on Insur-anceQA and TREC-QA datasets, and the experi-mental results demonstrate that the proposed mod-els outperform a variety of strong baselines.
Po-tential future work include: 1) Evaluating the pro-posed approaches for different tasks, such as com-munity QA and textual entailment; 2) Includingthe sentential attention mechanism; 3) Integratingthe hybrid and the attentive mechanisms into a sin-gle framework.ReferencesDzmitry Bahdanau, KyungHyun Cho, and Yoshua.Bengio.
2015.
Neural machine translation byjointly learning to align and translate.
Proceedingsof International conference of learning representa-tions.Frederic Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.Deep Learning and Unsupervised Feature LearningNIPS 2012 Workshop.Michael Bendersky, Donald Metzler, and W. BruceCroft.
2010.
Learning concept importance usinga weighted dependence model.
In in Proceedingsof the Third ACM International Conference on WebSearch and Data Mining (WSDM).Michael Bendersky, Donald Metzler, and W. BruceCroft.
2011.
Parameterized concept weighting inverbose queries.
In in Proceedings of the 34th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR).Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, withapplication to face verification.
Computer Visionand Pattern Recognition (CVPR).Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2015.
Long-termrecurrent convolutional networks for visual recogni-tion and description.
In The IEEE Conference onComputer Vision and Pattern Recognition (CVPR),June.C?
?cero dos Santos, Luciano Barbosa, Dasha Bog-danova, and Bianca Zadrozny.
2015.
Learning hy-brid representations to retrieve semantically equiva-lent questions.
In Proceedings of ACL, pages 694?699, Beijing, China, July.C?
?cero dos Santos, Ming Tan, Bing Xiang, and BowenZhou.
2016.
Attentive pooling networks.
CoRR,abs/1602.03609.Minwei Feng, Bing Xiang, Michael Glass, LidanWang, and Bowen Zhou.
2015.
Applying deeplearning to answer selection: A study and an opentask.
IEEE Automatic Speech Recognition and Un-derstanding Workshop (ASRU).Alex Graves, Abdel-rahman Mohamed, and GeoffreyHinton.
2013.
Speech recognition with deep recur-rent neural networks.
In IEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP).472Michael Heilman and Noah A. Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics.
Association forComputational Linguistics (NAACL).Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems (NIPS).Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.Advances in Neural Information Processing Systems(NIPS).Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In ACL.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their composition-ality.
Advances in Neural Information ProcessingSystems (NIPS).Xipeng Qiu and Xuanjing Huang.
2015.
Con-volutional neural tensor network architecture forcommunity-based question answering.
Proceedingsof the 24th International Joint Conference on Artifi-cial Intelligence (IJCAI).Tim Rockt?aschel, Edward Grefenstette, Karl MoritzHermann, Tom?as Kocisk?y, and Phil Blunsom.
2016.Reasoning about entailment with neural attention.International Conference on Learning Representa-tions (ICLR).Alexander Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for sentence sum-marization.
Proceedings of the 2015 Conference onEmpirical Methods in Natural Language Processing(EMNLP).Tara N. Sainath, Andrew Senior Oriol Vinyals, andHasim Sak.
2015.
Convolutional, long short-termmemory, fully connected deep neural networks.
InAcoustics, Speech and Signal Processing (ICASSP),IEEE International Conference.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Aliaksei Severyn and Alessandro Moschitti.
2015.Learning to rank short text pairs with convolutionaldeep neural networks.
In SIGIR.Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,and Rob Fergus.
2015.
End-to-end memory net-works.
In Advances in Neural Information Process-ing Systems (NIPS).Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems (NIPS).Oriol Vinyals and Quoc V. Le.
2015.
A neural con-versational model.
Proceedings of the 31st Interna-tional Conference on Machine Learning.Mengqiu Wang and Christopher Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
The Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING).Di Wang and Eric Nyberg.
2015.
A long short-term memory model for answer sentence selectionin question answering.
Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Confer-ence on Natural Language Processing.Mengqiu Wang, Noah Smith, and Mitamura Teruko.2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
The Proceedings ofEMNLP-CoNLL.Jason Weston, Sumit Chopra, and Keith Adams.2014.
#tagspace: Semantic embeddings from hash-tags.
Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing(EMNLP).Xuchen Yao, Benjamin Durme, and Peter Clark.
2013.Answer extraction as sequence tagging with tree editdistance.
Proceedings of NAACL-HLT.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguist (ACL).Wenpeng Yin, Hinrich Schutze, Bing Xiang, andBowen Zhou.
2015.
Abcnn: attention-based convo-lutional neural network for modeling sentence pairs.CoRR, abs/1512.05193.Lei Yu, Karl M. Hermann, Phil Blunsom, and StephenPulman.
2014.
Deep learning for answer sentenceselection.
NIPS Deep Learning Workshop.Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingx-ing Wang, Bing Wang, and Yushi Chen.
2015.
Con-volutional recurrent neural networks: Learning spa-tial dependencies for image representation.
In Pro-ceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition Workshops.473
