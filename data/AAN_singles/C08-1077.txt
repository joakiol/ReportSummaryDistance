Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 609?616Manchester, August 2008Detecting multiple facets of an event using graph-based unsupervisedmethodsPradeep MuthukrishnanDept of EECSUniversity of Michiganmpradeep@umich.eduJoshua GerrishSchool of InformationUniversity of Michiganjgerrish@umich.eduDragomir R. RadevDept of EECS &School of Information,University of Michiganradev@umich.eduAbstractWe propose a new unsupervised methodfor topic detection that automatically iden-tifies the different facets of an event.
Weuse pointwise Kullback-Leibler divergencealong with the Jaccard coefficient to builda topic graph which represents the com-munity structure of the different facets.The problem is formulated as a weightedset cover problem with dynamically vary-ing weights.
The algorithm is domain-independent and generates a representa-tive set of informative and discriminativephrases that cover the entire event.
Weevaluate this algorithm on a large collec-tion of blog postings about different newsevents and report promising results.1 IntroductionFinding a list of topics that a collection of docu-ments cover is an important problem in informa-tion retrieval.
Topics can be used to describe orsummarize the collection, or they can be used tocluster the collection.
Topics provide a short andinformative description of the documents that canbe used for quickly browsing and finding relateddocuments.Inside a given corpus, there may be multiple top-ics.
Individual documents can also contain multi-ple topics.Traditionally, information retrieval systems re-turn a ranked list of query results based on thesimilarity between the user?s query and the docu-ments.
Unfortunately, the results returned will of-ten be redundant.
Users may need to reformulatec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.their search to find the specific topic they are in-terested in.
This active searching process leads toinefficiencies, especially in cases where queries orinformation needs are ambiguous.For example, auser wants to get an overview of the Virginia techshootings, then the first query he/she might try is?Virginia tech shooting?.
Most of the results re-turned would be posts just mentioning the shoot-ings and the death toll.
But the user might wanta more detailed overview of the shootings.
Thusthis leads to continuously reformulating the searchquery to discover all the facets of the event.2 Related WorkTopic detection and tracking was studied exten-sively on newswire and broadcast collections bythe NIST TDT research program (Allan et al, ).The large number of people blogging on the webprovides a new source of information for topic de-tection and tracking.The TDT task defines topics as ?an event or ac-tivity, along with all directly related events and ac-tivities.?
In this paper we will stay with this defini-tion of topic.Zhai et al proposed several methods for dealingwith a related task, which they called subtopic re-trieval (Zhai et al, 2003).
This is an informationretrieval task where the goal is to retrieve and re-turn documents that cover the different subtopicsof a given query.
As they point out, the utilityof each document is dependent on the other doc-uments in the ranking, which violates the indepen-dent relevance assumption traditionally used in IR.Blei et al (Blei et al, 2003) proposed LatentDirichlet Allocation (LDA), a generative modelthat allows sets of documents to be explained byunobserved groups of documents, each based ona single topic.
The LDA model assumes the bag-609of-words model and posits that each document iscomposed of different topics.
Specifically, eachword?s existence is attributed to one of the docu-ment?s topic.
This algorithm outputs a set of n-grams for each topic whereas our algorithm mod-els each subtopic using a single n-gram.
Due tolimitations of time we were not able to comparethis approach with ours.
We plan to have this com-parison in our future work.To reduce the complexity of this task, a candi-date set of subtopics needs to be generated thatcover the document collection.
We choose touse a keyphrase detection algorithm to generatetopic labels.
Several keyphrase extraction algo-rithms have been discussed in the literature, in-cluding ones based on machine learning methods(Turney, 2000), (Hulth, 2003) and tf-idf ((Franket al, 1999)).
Our method uses language modelsand pointwise mutual information expressed as theKullback-Leibler divergence.Kullback-Leibler divergence has been found tobe an effective method of finding keyphrases intext collections.
But identification of keyphrasesis not enough to find topics in document.
Thekeyphrases identified may describe the entire col-lection, or aspects of the collection.
We wish tosummarize subtopics within these collections.The problem of subtopic detection is also relatedto novelty detection in (Allan et al, ).
In this prob-lem, given a set of previously seen documents, thetask is to determine whether a new document con-tains new or novel content.
The TREC 2002 nov-elty track, the task was to discard sentences thatdid not contain new material.
This is similar to ourgoal of reducing redundancy in the list of returnedsubtopics.In most cases, novelty detection is implementedas an online algorithm.
The system has a set of ex-isting documents they have seen up until a certainpoint.
The task is to determine whether a new doc-ument is novel based on the previous documents.Once a decision has been made, the status of thatdocument is fixed.
The subtopic detection task dif-fers from this because it is an offline task.
The al-gorithm typically has access to the entire documentset.
Our method differs from this novelty detectiontask in that it has access to the entire document col-lection.2.1 Existing redundancy measuresZhang et al examine five different redundancymeasures for adaptive information filtering (Zhanget al, ).
Information filtering systems return rel-evant documents in a document stream to a user.Examples of information filtering systems includetraditional information retrieval systems that returnrelevant documents depending on the user?s query.The redundancy measures Zhang et al examineare based on online analysis of documents.
Theyidentify two methods of measuring redundancy:?
Given n documents, they are considered oneby one, and suppose we have processed i doc-uments and we have k clusters.
Now we needto process the i+1th document.
We computethe distance of the i + 1th document with thek clusters and add the document to the clos-est cluster if the distance is above a certainthreshold, else we create a new cluster withonly the i+ 1th document.?
Measure distance between the new documentand each previously seen document.They evaluate several measures like set difference,geometric distance, Distributional similarity andmixture models.
Evaluating the five systems, theyfound that cosine similarity was the most effectivemeasure, followed by the new mixture model mea-sure.3 DataWe choose several news events that occurred in2007 and 2008 based on the popularity in the bl-ogosphere.
We were looking for events that werewidely discussed and commented on.
The eventsin our collection are the top-level events that wehave gathered.
Table 1 lists the events that werechosen for analysis:To help illustrate our subtopic detection method,we will use the Virginia Tech tragedy as an ex-ample throughout the rest of this paper.
Peoplethroughout the blogosphere posted responses ex-pressing support and condolences for the peopleinvolved, along with their own opinions on whatcaused it.Figures 1 and 2 show two different responses tothe event.
The quote in figure 1 shows an examplepost from LiveJournal, a popular blogging com-munity.
In this post, the user is discussing his viewon gun control, a hotly debated topic in the after-math of the shooting.
Figure 2 expresses anotherperson?s emotional response to this event.
Bothposts show different aspects of the same story.
Oursubtopic detection system seeks to automatically610Event Description Posts DatesiPhone iPhone release hype 48810 June 20 , 2007 - July 7, 2007petfoodrecall Melamine tainted petfood recall 4285 March 10, 2007 - May 10, 2007spitzer Eliot Spitzer prostitution scandal 10379 March 6, 2008 - March 23, 2008vtech Virginia Tech shooting 12256 April 16, 2007 - April 30, 2007Table 1: Major events summarizedidentify these and other distinct discussions thatoccur around an event.After the Virginia Tech murders, there?sthe usual outcry for something to bedone, and in particular, for more guncontrol.
As usual, I am not persuaded.The Virginia Tech campus had gun con-trol, which meant that Cho Seung-Huiwas in violation of the law even beforehe started shooting, and also that no law-abiding citizens were able to draw.Figure 1: Example blog post from LiveJournal dis-cussing gun control (Rosen, 2007)... Predictably, there have been rum-blings in the media that video gamescontributed to Cho Seung-Hui?s mas-sacre at Virginia Tech.
Jack Thomp-son has come out screaming, referringto gamers as ?knuckleheads?
and callingvideo games ?mental masturbation?
allthe while referring to himself as an ?ed-ucator?
and ?pioneer?
out to ?right?
so-ciety.
...Figure 2: Example blog post discussing videogames (hoopdog, 2007)Figure 3 shows a generalized Venn diagram(Kestler et al, 2005) of the cluster overlap betweendifferent keyphrases from the Virginia Tech event.3.1 PreprocessingData was collected from the Blogocenter blog-lines database.
The Blogocenter group at UCLAhas been retrieving RSS feeds from the Bloglines,Blogspot, Microsoft Live Spaces, and syndic8 ag-gregators for the past several years.
They currentlyhave over 192 million blog posts collected.For each news item, relevant posts were re-trieved, based on keyword searching and date ofblog post.
Posts from the date of occurrence ofthe item to two weeks after the event occurredFigure 3: Generalized Venn diagram of topic over-lap in the Virginia Tech collectionwere gathered, regardless of the actual length ofthe event.Since many RSS feeds indexed by Bloglines arefrom commercial news organizations or commer-cial sites, we had to clean up the retrieved data.Table 1 lists the event we analyzed along with ba-sic statistics.4 MethodOur algorithm should find discriminative labels forthe different topics that exist in a collection of doc-uments.
Taken together, these labels should satisfythe following conditions:?
Describe a large portion of the collection?
The overlap between the topics should beminimalThis task is similar to Minimum Set Cover,which is NP-complete (Garey and Johnson, 1990).Therefore, trying to find the optimal solution byenumerating all possible phrases in the corpuswould be impossible, instead we propose a two-step method for subtopic detection.The first step is to generate a list of candidatephrases.
These phrases should be informative andrepresentative of all of the different subtopics.
Thesecond step should select from these phrases con-sistent with the two conditions stated above.6114.1 Generating Candidate PhrasesWe want to generate a list of phrases that have ahigh probability of covering the document space.There are many methods that could be used to findinformative keyphrases.
One such method is usingthe standard information retrieval TF-IDF model(Salton and McGill, 1986).Witten et al(Witten et al, 1999) proposedKEA, an algorithm which generates a list of can-didate keyphrases using lexical features.
Theykeyphrases are then selected from these candidatesusing a supervised machine learning algorithm.This approach is not plausible for our purposes be-cause of the following two reasons.1.
The algorithm is domain-dependent andneeds a training set of documents with anno-tated keyphrases.
But our data sets come fromvarious domains and it is not a very viable op-tion to create a training set for each domain.2.
The algorithm generates keyphrases for a sin-gle document, but for our purposes we needkeyphrases for a corpus.Another method is using Kullback-Leibler di-vergence to find informative keyphrases.
We foundthat KL divergence generated good candidate top-ics.Tomokiyo and Hurst (2003) developed a methodof extracting keyphrases using statistical languagemodels.
They considered keyphrases as consistingof two features, phraseness and informativeness.Phraseness is described by them as the ?degree towhich a given word sequence is considered to be aphrase.?
For example, collocations could be con-sidered sequences with a high phraseness.
Infor-mativeness is the extent to which a phrase capturesthe key idea or main topic in a set of documents.To find keyphrases, they compared two lan-guage models, the target document set and a back-ground corpus.
Pointwise KL divergence was cho-sen as the method of finding the difference be-tween two language models.The KL divergence D(p||q) between two prob-ability mass functions p(x) and q(x) with alphabet?
is given in equation 1.D(p||q) =?x?
?p(x)logp(x)q(x)(1)KL divergence is an asymmetric function.D(p||q) may not equal D(q||p).Pointwise KL divergence is the individual con-tribution of x to the loss of the entire distribution.The pointwise KL divergence of a single phrase wis ?w(p||q):?w(p||q) = p(w)logp(w)q(w)(2)The phraseness of a phrase can be found bycomparing the foreground n-gram language modelagainst the background unigram model.
For ex-ample, if we were judging the phraseness of ?guncontrol?, we would find the pointwise KL diver-gence of ?gun control?
between the foreground bi-gram language model and the foreground unigramlanguage model.
?p= ?w(LMfgN||LMfg1) (3)The informativeness of a phrase can be found byfinding the pointwise KL divergence of the fore-ground model against the background model.
?i= ?w(LMfgN||LMbgN) (4)A unified score can be formed by adding thephraseness and informative score: ?
= ?p+ ?i4.2 Selecting Topic LabelsOnce keyphrases have been extracted from thedocument set, they are sorted based on theircombined score.
We select the top n-rankedkeyphrases as candidate phrases.
This step willhereafter be referred to as ?KL divergence mod-ule?.Based on our chosen task conditions regardingcoverage of the documents and minimized overlapbetween topics, we need an undirected mappingbetween phrases and documents.
A natural repre-sentation for this is a bipartite graph where the twosets of nodes are phrases and documents.
Let thegraph be: G = (W,D,E) where W is the set ofcandidate phrases generated by the first step and Dis the entire set of documents.
E is the set of edgesbetween W and D where there is an edge betweena phrase and a document if the document containsthe phrase.We formulate the task as a variation of WeightedSet Cover problem in theoretical computer science.In normal Set Cover we are given a collection ofsets S over a universe U , and the goal is to select aminimal subset of S such that the whole universe,U is covered.
Unfortunately this problem is NP-complete (Garey and Johnson, 1990), so we must612d1w3w1d4d2w2d5d3Figure 4: Bipartite graph representation of topicdocument coverage, where the di?s are the docu-ments and the wi?s are the n-gramssettle for an approximate solution.
But fortunatelythere exist very good ?-approximation algorithmsfor this problem (Cui, 2007).The difference in Weighted Set Cover is thateach set has an associated real-valued weight orcost and the goal is to find the minimal or maximalcost subset which covers the universe U .In our problem, each phrase can be thought ofas a set of the documents which contain it.
Theuniverse is the set of all documents.4.3 Greedy AlgorithmTo solve the above problem, we propose a greedyalgorithm.
This algorithm computes a cost for eachnode iteratively and selects the node with the low-est cost at every iteration.
The cost of a keyphraseshould be such that we do not choose a phrase withvery high coverage, like ?Virginia?
and at the sametime not choose words with very low documentfrequency since a very small collection of docu-ments can not be judged a topic.Based on these two conditions we have come upwith a linear combination of two cost components,similar to Maximal Marginal Relevance (MMR)(Carbonell and Goldstein, 1998).1.
Relative Document Size:f1(wi) =|adj(wi)|N(5)where |adj(wi)| is the document frequency ofthe word.This factor takes into account that we do notwant to choose words which cover the wholedocument collection.
For example, phrasessuch as ?Virginia?
or ?Virginia tech?
are badsubtopics, because they cover most of thedocument set.2.
Redundancy Penalty:We want to choose elements that do not have alot of overlap with other elements.
One mea-sure of set overlap is the Jaccard similarity co-efficient:J(A,B) =|A ?B||A ?B|(6)f2(wi) = 1?
?wj?W?wiJ(wi, wj)|W | ?
1(7)This component is essentially 1?
averageJaccard similarity.We calculate the pairwise Jaccard coefficientbetween the target keyphrase and every otherkeyphrase.
The pairwise coefficient vectorprovides information on how much overlapthere is between a keyphrase and every otherkeyphrase.
Phrases with a high average Jac-card coefficient are general facets that coverthe entire collection.
Phrases with a low Jac-card coefficient are facets that cover specifictopics with little overlap.3.
Subtopic Redundancy Memory EffectOnce a keyphrase has been chosen we alsowant to penalize other keyphrases that coverthe same content or documents.
Equation 8represents a redundancy ?memory?
for eachkeyphrase or subtopic.
This memory is up-dated for every step in the greedy algorithm.R(wi) = R(wi) + J(wi, wj) (8)where wjis the newly selected phrase.A general cost function can be formed from alinear combination of the three cost components.We provide two parameters, ?
and ?
to representthe trade-off between coverage, cohesiveness andintersection.
For our experiments, we found thatan ?
value of 0.7 and a ?
value of 0.2 performedwell.cost(wi) = ??
f1(wi)+?
?
f2(wi)+(1?
(?
+ ?
))?R(wi)(9)613The pseudocode for the greedy algorithm isgiven in Figure 5.
It should be noted that the al-gorithm requires the costs to be recomputed af-ter every iteration.
This is because the cost of akeyphrase may change due to a change in any ofthe three components.
This is because after select-ing a keyphrase, it might make another keyphraseredundant, that is, covering the same content.
Thismakes the whole problem a dynamic weighted setcover problem.
Hence, the performance guaran-tees associated with the greedy algorithm for theWeighted Set Cover problem do not hold true forthe dynamic version.Algorithm Greedy algorithm for weighted set-coverInput: Graph G = (W,D,E)1.
N: number of documents to cover2.Output: Set of discriminative phrases for the different topics3.
W = {w1, w2, .
.
.
, wn}4.
Wchosen= ?5.
num docs covered = 06. while num docs covered < N7.
do for wi?
W8.
do cost(wi) = ??
f1(wi)9.
+?
?
f2(wi)10.
+(1?
(?+ ?))?R(wi)11.
wselected= argmaxwcost(wi)12. for wi?
W13.
do R(wi) = R(wi) + J(wselected, wi)14. num docs covered = num docs covered +|adj(wselected)|15.
Wchosen= Wchosen?
{wselected}16.
W = W ?
{wselected}17.
D = D ?
adj(selected)18. return WchosenFigure 5: A greedy set-cover algorithm for detect-ing sub-topics5 ExperimentsAs a baseline measure, we extracted the top kphrases from the word distribution as the topic la-bels.
As a gold standard, we manually annotatedthe four different collections of blog posts.
Eachannotator generated a list of subtopics.6 EvaluationIn evaluating topic detection, there exist two cate-gories of methods, intrinsic and extrinsic (Liddy,2001).
Extrinsic methods evaluate the labelsagainst a particular task whereas intrinsic methodsmeasure the quality of the labels directly.
We pro-vide intrinsic and extrinsic evaluations of our algo-rithm.To evaluate our facet detection algorithm, wecreated a gold standard list of facets for each dataset.
A list of the top 300 keyphrases generated bythe KL divergence module was given to two eval-uators.
The evaluators were the first and secondauthor of this paper.
The evaluators labeled eachkeyphrase as a positive example of a subtopic ora negative example of a subtopic.
The positiveexamples taken together form the gold standard.For this evaluation process we defined a positivesubtopic as a cohesive collection of documents dis-cussing the same topic.Cohen?s Kappa coefficient (Cohen, 1960) wascalculated for the gold standard.
Table 6 lists the ?value for the four data sets.iPhone petfoodrecall spitzer vtech0.62 0.86 0.77 0.88Table 2: Kappa scores for the gold standardThe kappa scores for the petfoodrecall and vtechdatasets showed good agreement among the raters,while the spitzer data set had only fair agreement.For the iPhone data set, both evaluators had a largeamount of disagreement on what they consideredsubtopics.A separate group of two evaluators was giventhe output from our graph-based algorithm, a listof the top KL divergence keyphrases of the samelength, and the gold standard for all four data sets.Evaluators were asked to rate the keyphrases on ascale from one to five, with one indicating a poorsubtopic, and five indicating a good subtopic.
Thenumber k of subtopics for the algorithm was cutoffwhere the f-score is maximized.
The same numberof phrases was chosen for KL divergence as well.Table 3 lists the cutoffs for the four data sets.iPhone Petfood recall Spitzer Vtech25 30 24 18Table 3: Number of generated subtopics for eachcollection.In addition, the precision, F-score, coverage andaverage pairwise Jaccard coefficient were calcu-lated for the four data sets.
Precision, recall andthe F-score are given in table 4.
The precision,recall and F-score for the gold standards is one.The others are shown in table 5.
Average pairwiseJaccard coefficient is calculated by finding the Jac-card coefficient for every pair of subtopics in theoutput and averaging this value.
This value is ameasure of the redundancy.
The average relevanceis a normalized version of the combined ?phrase-614ness?
and ?informativeness?
score calculated bythe keyphrase detection module.
This value is nor-malized by dividing by the KL divergence for theentire 300 phrase list.
This provides a relevancyscore for the output.Data set Precision Recall F-scoreiphoneKL-Divergence 0.08 0.10 0.09Graph-based method 0.52 0.60 0.56petfoodrecallKL-Divergence 0.37 0.39 0.38Graph-based method 0.61 0.57 0.59spitzerKL-Divergence 0.10 0.08 0.09Graph-based method 0.79 0.59 0.68vtechKL-Divergence 0.05 0.06 0.05Graph-based method 0.72 0.76 0.74Table 4: Precision, recall and F-score for the base-line and graph-based algorithm.Data setCoverage Average Normalized Humanpairwise KL ratingJC divergenceiphoneKL-Divergence 40168 0.08 18.19 1.92Gold standard 12977 0.02 2.81 3.13Graph-based 9850 0.01 1.98 2.82petfoodrecallKL-Divergence 4280 0.18 19.53 1.82Gold standard 2659 0.05 4.30 3.43Graph-based 2055 0.01 1.75 2.81spitzerKL-Divergence 9291 0.19 22.90 1.33Gold standard 4036 0.03 2.29 3.31Graph-based 2468 0.01 1.60 2.88vtechKL-Divergence 12215 0.29 24.61 1.61Gold standard 5058 0.03 2.79 3.76Graph-based 4342 0.01 1.66 3.28Table 5: Coverage, overlap and relevance and eval-uation scores for the gold standard, baseline andgraph-based method.7 ResultsTable 6 shows some of the different subtopics cho-sen by our algorithm for the different data sets.There is no manual involvement required in the al-gorithm except for the intial preprocessing to re-move commercial news feeds and spam posts.
Ourgraph-based method performs very well and al-most achieves the gold standard?s rating.
The F-score for the iPhone data set was only 0.56, but webelieve part of this may be because this data set didnot have clearly defined subtopics, as shown by thelow agreement (0.62) among human evaluators.Spitzer Petfood recallAshley Alexandra Dupre Under Wal-MartOberweis Xuzhou AnyingEmperor?s club People who buyGovernor of New Cuts and GravySpitzer?s resignation Cat and DogDr Laura Cats and DogsMayflower hotel Food and DrugSex workers Cyanuric acidformer New york recent petHigh priced prostitution industrial chemicalMcGreevey massive pet foodGeraldine Ferraro Royal caninHigh priced call Iams and Eukanubalegally blind Dry foodmoney launderingVirginia Tech shooting iPhoneKorean American Photo sent fromGun Ownership Waiting in lineHolocaust survivor About the iPhoneMentally ill Unlimited dataShooting spree From my iPhoneDon Imus cell phonesVideo Games Multi-touchGun free zone Guided tourWest Ambler Johnston iPhone launchColumbine High school Walt MossbergSelf defense Apple IncTwo hours later Windows MobileGun violence June 29thSeung Hui Cho Web browserSecond Amendment ActivationSouth KoreanTable 6: Different topics chosen by the graph-based algorithm for the different data setsFigure 6 shows the trade off between coverageand redundancy.
This graph clearly shows thatthe overlap between the subtopics increases veryslowly as compared to the number of documentscovered.
The slope of the curves increases slowlywhen the number of documents to be covered issmall and later increases rapidly.
This means thatinitially there are a lot of small focused subtopicsand once we have selected all the focused ones thealgorithm is forced to pick the bigger topics andhence the average pairwise intersection increases.0 0.5 1 1.5 2 2.5 3 3.5 4x 10401020304050607080Number of documents to be coveredAveragepairwiseintersectionoftopicsiPhoneSpitzerpetfoodrecallvtechFigure 6: Subtopic redundancy vs. coverage6158 ConclusionWe have presented a new algorithm based onweighted set cover for finding subtopics in acorpus of selected blog postings.
The algo-rithm performs very well in practice comparedto the baseline standard, which outputs the topkeyphrases according to the Kullback-Leibler di-vergence method.
While the baseline standard out-puts keyphrases which are redundant, in the sense,they cover the same documents, the graph-basedmethod outputs keyphrases which have very littleintersection.
We provide a new method of rankingkeyphrases that can help users find different facetsof an event.The identification of facets has many applica-tions to natural language processing.
Once facetshave been identified in a collection, documents canbe clustered based on these facets.
These clusterscan be used to generate document summaries orfor visualization of the event space.The keyphrases themselves provide a succinctsummary of the different subtopics.
In futurework, we intend to investigate summarization ofdocuments based on subtopic clustering using thismethod.9 AcknowledgmentsThis work was supported by NSF grants IIS0534323 ?Collaborative Research: BlogoCenter -Infrastructure for Collecting, Mining and Access-ing Blogs?
awarded to The University of Michiganand ?iOPENER: A Flexible Framework to SupportRapid Learning in Unfamiliar Research Domains?,jointly awarded to U. of Michigan and U. of Mary-land as IIS 0705832.?
Also we would like to thankVahed Qazvinian and Arzucan ?Ozgu?r for helpingwith the evaluation and their valuable suggestions.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.ReferencesAllan, James, Courtney Wade, and Alvaro Bolivar.
Re-trieval and novelty detection at the sentence level.SIGIR 2003, pages 314?321.Blei, D., A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research,3:993?1022, January.Carbonell, Jaime G. and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
SIGIR 1998,pages 335?336.Cohen, J.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Measure-ment, 20:37.Cui, Peng.
2007.
A tighter analysis of set cover greedyalgorithm for test set.
In ESCAPE, pages 24?35.Frank, Eibe, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
Sixteenth In-ternational Joint Conference on Artificial Intelli-gence, pages 668?673.Garey, Michael R. and David S. Johnson.
1990.
Com-puters and Intractability; A Guide to the Theory ofNP-Completeness.
W. H. Freeman.hoopdog.
2007.
Follow-up: Blame game.http://hoopdogg.livejournal.com/39060.html.Hulth, Anette.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
EMNLP2003, pages 216?223.Kestler, Hans A., Andre Muller, Thomas M. Gress, andMalte Buchholz.
2005.
Generalized venn diagrams:a new method of visualizing complex genetic set re-lations.
Bioinformatics, 21:1592?1595, April.Liddy, Elizabeth.
2001.
Advances in automatic textsummarization.
Information Retrieval, 4:82?83.Rosen, Nicholas D. 2007.
Gun control and mentalhealth.
http://ndrosen.livejournal.com/128715.html.Salton, G. and M. J. McGill.
1986.
Introduction toModern Information Retrieval.
McGraw-Hill, Inc.New York, NY, USA.Tomokiyo, Takashi and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
ACL2003 workshop on Multiword expressions: analysis,acquisition and treatment - Volume 18, pages 33?40.Turney, Peter D. 2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval, 2:303?336.Witten, Ian H., Gordon W. Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning.
1999.
KEA:Practical automatic keyphrase extraction.
In 1stACM/IEEE-CS joint conference on Digital libraries,pages 254?255.Zhai, Chengxiang, William W. Cohen, and John Laf-ferty.
2003.
Beyond independent relevance: meth-ods and evaluation metrics for subtopic retrieval.
SI-GIR 2003, pages 10?17.Zhang, Yi, James P. Callan, and Thomas P. Minka.Novelty and redundancy detection in adaptive filter-ing.
In SIGIR 2002, pages 81?88.616
