Text Recognition using Collocationsand Domain CodesT.G.
Rose & L.J.
EvettDept.
of Computing, Nottingham Trent University, Nottingham, Englandphone: 0602 418418 email tgr@uk.ac.trent.doc fax: 0602 484266Keywords: handwriting recognition, OCR, collocation, sense taggingAbstract: Text recognition systems require the use of contextual information inorder to maximise the accuracy of their output.
However, the acquisition of suchknowledge for a realistically sized vocabulary presents a major problem.
This paperdescribes methods for extracting contextual knowledge from text corpora, anddemonstrates it  contribution to the performance of handwriting recognition systems.IntroductionSuch is the visual ambiguity of handwriting that a number of possibleinterpretations may be made for any written word.
Indeed, this is true of any text, butparticularly handwritten text since the segmentation between the individual charactersis often indistinct.
Human readers cope with this by making selective use of visualcues and using an understanding of the text to compensate for any degradation orambiguity within the visual stimulus.
Word images occur within a meaningfulcontext, and human readers are able to exploit he syntactic and semantic onstraintsof the textual material \[Just & Carpenter, 1987\].
Analogously, computerised textrecognition systems would be enhanced by using higher level knowledge.
Characterrecognition techniques alone are insufficient o unambiguously identify the input,particularly that of handwritten data.Ideally, this higher-level knowledge would be acquired by the creation of alexical database that contains all the relevant information.
However, to create adatabase of such information "from scratch" for a realistically sized vocabulary is anenormous task - which is a major reason why so many theories of language~The research reported in this paper was supported by the European Commissionunder the ESPRIT initiative.65processing fail to "scale up" from the small, artificial domains in which they weredeveloped.
An alternative approach is to exploit existing sources of information, suchas machine-readable dictionaries \[Rose & Evett, 1992\] and text corpora.
Corpora canbe used to provide empirical information (such as collocations) concerning word useacross a wide range of subject areas \[Smadja, 1989\].
A further source of information,known as domain coding, can be acquired either from a machine-readable dictionaryor generated as a further product of corpus analysis.
This paper is concerned with theacquisition of collocations and domain codes, and their contribution to textrecognition systems.Text Recognition SystemsDue to its greater inherent degree of ambiguity, handwritten text is seen as themain application of the following techniques.
However, the methods may also beapplied to OCR data, or indeed to any recognition system that produces wordalternatives as its output (e.g.
speech recognition systems).
The system to which thecurrent efforts are applied operates in the following way: input is written on a datapad using an electronic pen, and data is captured ynamically in the form of x-y co-ordinates.
The co-ordinates are translated into a set of vector codes that are thenmatched against a database to produce candidate characters for the input.
Thesecharacters are combined to produce candidate l tter strings, which are checked againsta list of acceptable words (as many as 71,000), and those strings not on the list arerejected from further processing.
The remaining strings are then combined to producepossible phrases.For example, consider the sentence "this is a new savings account which youcan open with one pound" written as input to the system.
This could produce theoutput shown in Figure 1 (in which the alternative candidates are shown in separatecolumns).
The problem is now to select from these alternatives those words that aremost likely to be correct.th is  is a hen sav ings  ga l lant  wh ich  you can open w i th  one roundta i l  new account  boy car oxen p ick  ore poundta l l  see accept  nos oar  oven l ick  due foundt r io  our bra houndFigure 1: TvDical outout  from a handwrit in~ recognition svstemCollocationsIntroduction: There are certain classes of English word combinations that cannot beexplained purely by existing syntactic or semantic theories.
For example, consider theuse of "strong" and "powerful" in the phrases "to drink strong tea" and "to drive apowerful car".
They fulfil the same syntactic role, and both make a similar semantic66modification to the subject.
However, to interchange them ("powerful tea" & "strongcar") would undoubtedly be judged anomalous by most English speakers.
Thesepredisposed combinations are called co-occurrence relations or collocations, andaccount for a large part of English word combinations.An algorithm was developed to analyse a given corpus and transform thedistributional patterns of the constituent words into a set of collocations.
Thisalgorithm was based on the work of Lancashire \[1987\], although modifications weremade to reformat he output as a sorted, lemmatised, ictionary-like structure.
Thisinformation could now be used to measure the plausibility of individual collocationsin data such as the above, and thereby identify the correct word candidates.
Forexample, the word "savings" should collocate more strongly with "account" than with"gallant" or "accept", and "account" should collocate more strongly with "open" thanwith "oxen" or "oven".The collocation analysis technique proceeds by comparing the"neighbourhoods" of each word candidate (up to a distance of four words) with theirlikely collocates (as defined by the results of corpus analysis).
Each candidate isassigned a score according to the overlap between its neighbourhood and its list oflikely collocates.
Once a complete sentence has been processed in this manner, thecandidates with the highest scores in each position are deemed to be the correctwords.
The "window size" of four words reflects both the results of empiricalinvestigation \[Rose, 1993\] and the findings of other researchers (e.g.
Jones &Sinclair, \[ 1974\]).Invest~atiotl I1.1 Method: Test data consisted of fifteen documents each of 500 words, taken fromseparate domains, with alternative word candidates in each position as in the aboveexample.
Two types of coUocation were investigated: (a) general, and (b) domain-specific.
To this end, it was necessary to create a num~r  of "collocationdictionaries".
The first of these was the General Collocation Dictionary (GCD),which was derived from 5 million words of text, taken from all subject areas withinthe Longman Corpus.
The remainder were domain-specific collocation dictionaries,derived from 500,000-word omain-specific corpora.
No part of any test documenthad been included in the corpora used for the creation of any collocation dictionary.For each of the fifteen documents, the collocations were analysed, once using theGCD and once using the appropriate domain-specific dictionary.1.2 Results: Table 1 shows the percentage of correct words idendfied by eachcollocation dictionary for each test document.
N.B.
- Since this data only concernsword positions in which there were two or more "competing" candidates, it does NOTdirectly reflect he overaU (system) recognition rate.674ComputingEnergyEngineeringBusinessEmploymentFinanceBiologyChemistryGENERAL SPECIFIC84.7 82.976.3 66.770.3 68.479.573.473.275.361.563.675.2 77.383.8 83.063.9 Maths 70.5Education 68.7 88.7Medicine 69.1 83.664.1 Sociology 73.1Economics 83.6 94.4History 70.8 80.0Politics 77.4 88.6AVERAGE 74.7 76.7STD.
DEV.
5.95 9.95Table 1: Percentage correct bv domain for each collocation dictionarv1.3 Discussion: The average performances of the general and the domain-specificdictionaries are extremely close (they differ by only two per cent).
This is somewhatsurprising, since it would be reasonable to assume that domain-specific dictionarieswould contain the most appropriate collocations for domain-specific documents.However, for 8 of the 15 documents, the general dictionary is more effective (by asmuch as 11.9% in one case).Explanations for this inevitably concern (a) the content of the textual materialused as data, and (b) the content of the collocation dictionaries.
Evidently, any givendocument will consist of a variety of language structures, some of which will begeneral (i.e.
not exclusively associated with any particular domain) and some domain-specific (i.e.
with restrictions on word senses, etc.).
This ratio of "general" to"specific" material will vary between documents and domains, such that a highproportion of "general" material may render the use of a domain-specific collocationdictionary less appropriate, and vice-versa.However, the specific dictionaries were derived from smaller corpora than theGCD and therefore contained fewer entries: 5,545 (on average) compared to 12,475in the GCD.
Furthermore, although the domain-specific corpora were all the samelength, due to variations in the type:token ratio the resultant dictionaries variedgreatly in size (from 3,960 entries to 7,748 entries).
Indeed, this variation in size veryclosely matches their performance: those larger than average tend to do better thanthe GCD, and those smaller tend to do worse.
This variation in performance is furtherreflected by the higher standard eviation of the specific dictionaries.68The performance level that could be expected from a random choice ofcandidates i 30.4% correct for this data.
Clearly, the use of collocations represents asignificant improvement on this baseline.
Although the handwriting recogniser itselfprovides a ranking of the alternative candidates, its accuracy is variable (dependingon the identity of the writer, the extent of training, the handwriting sample used, etc.
)and it is clear that contextual information is needed to disambiguate many wordpositions \[Powaika ctal, 1993\].
In this respect, collocations are just one of a numberof sources of higher-level knowledge that may be independently applied to textrecognition data.
However, the question of how to combine these knowledge sourcesremains highly problematic, since it is unclear how much influence should beallocated to each of them.
It is desirable therefore to measure their contribution inisolation before attempting to combine them within an integrated system \[Evett et al1993\].Evidently, it would seem that "big is beautiful" when it comes to acquiringcollocations from text corpora.
The analysis of a single domain may be fruitful only ifthe size and type:token ratio of the domain corpus are such that collocates for asufficiently wide variety of types can be acquired.
A more reliable approach is toanalyse as large and varied a corpus as possible to maximise the coverage of theresultant dictionary.
Additionally, good coverage is required to process all thealternative candidates produced by text recognition systems.
However, it must beappreciated that for a real-time application such as handwriting recognition,processing and storage requirements constitute an overhead that must be rninimised.Consequently, if the implementation is restricted to a single domain, then a specificdictionary may represent the best compromise between performance and efficiency.Domain CodesIntroduction: Domain codes are essentially labels that may be associated with wordsto describe the domain or subject area with which they are usually associated.
Thecodes themselves can be organised as a simple series of subject areas, or as ahierarchy whereby specific domain codes imply inheritance of a more general subjectarea.
Using them as an aid to recognition involves firstly determining the domain ofthe data, and then using the codes to favour those word candidates whose senses areappropriate to that domain.A system of domain codes can be either created from scratch or obtained from amachine-readable dictionary (e.g.
LDOCE).
The first method is impractical due to thesheer size of the task; the second is derivative and produces a domain coding systemthat may not necessarily bc the most suitable for a particular application.
A thirdmethod, based on corpus analysis, has been developed that does not suffer from eitherof the above drawbacks.
This method proceeds on a domain-by-domain ( .e.
corpus-by-corpus) basis according to the following algorithm:696(1) Start with the raw domain corpus and reduce it to its uninflected root forms;(2) Produce a type-frequency distribution for this corpus;(3) Obtain a corresponding distribution from an undifferentiated (general) corpus;(4) Normalise these frequency distributions so that each type's frequency is expressed as aproportion of the total number of tokens within that corpus;(5) Calculate the comparative frequency of each type (i.e.
its "distinctiveness");(6) Select those words which have a distinctiveness of 3.0 or above, i.e.
their frequency is atleast three times greater in the domain corpus than in the general corpus (this threshold hasbeen selected arbitrarily and needs to be investigated empirically);(7) Normalise these comparative frequencies by expressing them as natural logarithms.
Theresultant file now contains those words distinctive to the domain, and a measure of theirdistinctiveness within that domain;(8) Repeat steps (1)-(7) for all domains for which corpora are available.
(9) Merge the domain codes from each domain into a single file.
This file now contains thatsection of the lexicon that displays domain-based behaviour, and identifies the domains withwhich each word is associated, with a measure of the strength of that association.Essentially, the codes thus produced reflect the relative frequency of wordswithin a domain-specific corpus compared to their frequency in an undifferentiatedcorpus.
The domain code lexicon can never be exhaustive: their coverage can only beas complete as the corpora from which they are derived.
However, the codesproduced by this technique have a distinct advantage over those of LDOCE: they arequantitative rather than qualitative.
Instead of just labelling words with a code to saywhether they belong to a given domain or not (such distinctions are not always clear-cut), they also provide a measure of the strength of this association.Invesffgation 22.1 Method: A set of domain codes was derived from LDOCE, and a further set (ofcomparable size) derived from a number of domain-based corpora according to theabove algorithm.
However, since a given word must display specialised omain-basedbehaviour to justify the possession of a domain code, high frequency words tend to beexcluded.
For this reason, using domain code information for text recognition tends toleave many word positions in the data um~solved.
Consequently, it was preferable toapply each set of codes as a "supplement" o the collocational analysis technique(using the GCD).
Test data consisted of the five documents that had produced thepoorest performance in Investigation 1 (as they left the most room for improvement).2.2 Results: Table 2 shows the percentage of correct words identified using (a) justcollocations (i.e.
the GCD), (b) collocations plus corpus-based codes, and (c)collocations plus LDOCE codes.2.3 Discussion: Both the LDOCE domain codes \[Walker & Amsler, 1986\] and thecorpus-based domain codes \[Evett & Rose, Note 1\] have been shown to be effectivefor topic identification.
Conscquendy, ff the domain of a document is known, it isreasonable to assume that such codes could contribute to recognition.
However, theiraverage contribution to the test documents i minimal.
Since the corpus-based codescover only ten broad subject areas, they may be simply not distinctive nough toidentify the correct word.70DomainEngineeringMathsSociologyHistoryFinanceAVERAGESTD.
DEV.Collocations70.370.5+ Corpus Codes75.268.8+LDOCE Codes70.374.164.1 65.8 64.170.8 72.9 70.873.269.83.0373.2 73.271.2 70.53.40 3.85Table 2: Percentage correct for each qqmlpinationConversely, the LDOCE, with its elaborate coding system of 120 major subjectareas and 212 subfields may be too fine, as the multitude of categories createsconfusion rather than effective discrimination.
It may transpire that an alternativecoding system (possibly designed around an intermediate l vel of representation) maybe optimal for recognition applications.
The lack of coverage of both current sets ofcodes renders them ineffective for certain documents (e.g.
Finance).
The onlysuccesses seem to be in Engineering (with the corpus-based codes) and Mathematics(with the LDOCE codes).
This result may reflect he propensity of these domains forusing distinctive specialised terminology.SummaryInvestigation 1 has shown that collocations can be used to identify the correctwords within text recognition data taken from a number of domains.
Investigation 2has demonstrated the limited improvement obtained by the use of two sets of domaincodes.Each of the techniques described above has been applied to handwritingrecognition data.
However, they are equally appropriate to other recognitionapplications, uch as OCR or some speech systems.
In the case of the former, it hasbeen possible to test the various techniques using output from an existing system.
Thedata source consisted of a collection of 22 scanned ocuments, and the OCR outputconsisted of the recognise, d characters that had been post-processed by a lexicallookup to identify word candidates.
In most cases, the correct word had been uniquelyidentified, but for 38 word positions there was one or more alternative candidates.Collocation analysis identified the correct word for 31 of these, chose an incorrectcandidate for 4 cases and left 3 positions unresolved.
This represents a performanceof 81.58% correct.Evidently, there are a number of limitations to the above methods.
Firsdy, sinceprocessing takes place within an integrated recognition architecture (i.e.
working inreal time, with a pattern recogniser, lexical analyser and syntax analyser),computational overheads and memory requirements must be minimised whereverpossible.
For this reason, both collocation analysis and domain code analysis are71based on lernmatised (root) forms rather than inflections.
However, it is clear thatsome collocations only exist in particular inflected forms \[Schuetze, forthcoming\].Consequently, it is intended to acquire inflected versions of the above collocationdictionaries and compare these with their lemmatised equivalents (using the same textrecognition data).Secondly, the collocation analysis makes no use of function words (again torninimise processing overheads).
However, these are an essential part of a number ofimportant linguistic phenomena such as phrasal verbs \[Sinclair, 1987\].
It is intendedtherefore to incorporate such information into future acquisition methods, andcompare the results with the "content-word only" predecessors.
Thirdly, no use ismade of word order information.
However, linear precedence has been shown to be asignificant factor affecting the manner in which words associate with each other\[Church & Hanks, 1989\].
Indeed, this is particularly relevant to a run-timerecognition application, since data is usually input in one direction anyway (i.e.
left-to-right).
Consequently, the next phase of collocation acquisition will be to create aset of uni-directional collocations and compare them with their bi-directionalequivalent.
Finally, the collocation analysis makes no use of distance information.Clearly, some collocations are independent of distance, but there are others whosebehaviour is highly distance dependent \[Jones & Sinclair, 1974\].
It is appropriate thatfuture system development should exploit his constraint.Likewise, there are ways in which domain code acquisition and analysis can beimproved.
For example, the acquisition algorithm has associated with it a number ofparameters (e.g.
the number of domains covered, the "specificity" of the domains, theoptimal value for the threshold of distinctiveness); all of which need to be empiricallyinvestigated.
Moreover, it is hoped that the current limitations concerning coveragewill be eliminated by the availability of larger corpora.
Consequently, their coveragemay be such that domain code analysis becomes a viable aid to recognition in its ownright, i.e.
without needing supplementary collocation information.
Part of thisinvestigation will be the development of alternative coding systems, based on varyinglevels of domain-specificity.Clearly, many of these extensions will involve the need to store and process agreater amount of data, which could compromise the efficiency of real-timeapplications uch as handwriting recognition.
It is suspected that the trade-offbetween performance and run-time fficiency will form the basis of further empiricalinvestigation.NOte 1."
L.J.
Evett & T.G.
Rose (1993) "Automatic Document TopicIdentification", paper submitted to 2nd IAPR Conf.
on Document Analysis andRecognition, Tsukuba Science City, Japan.72ReferencesK.
Church & P. Hanks (1989) "Word association orms, mutual information andlexicography", Proc.
27th Meeting of the ACL, pp.
76-83.L.J.
Evett, T.G.
Rose, F.G. Keenan & R.J. Whitrow (1993) "Linguistic ContextualConstraints for Text Recognition", ESPRIT Deliverable DLP 2.1, Project 5203.S.
Jones & J. Sinclair (1974) "English Lexical Collocations", Cahiers deLexicologie, 24, pp.
15-61.M.A.
Just & P.A.
Carpenter (1987) "The Paychology of Reading and LanguageComprehension", Allyn & Bacon Inc., Boston.I.
Lancashire (1987) "Using a Textbase for English-language r search", Proc.
3rdAnn.
Conf.
of the UWC for the New Oxford English Dictionary, Waterloo.R.K.
Powalka, N. Sherkat, L.J.
Evett & R.J. Whitrow (forthcoming) "Dynamiccursive script recognition: a hybrid approach to recognition", Sixth InternationalConference on Handwriting and Drawing, Paris, July 1993.T.G, Rose (1993) "Large Vocabulary Semantic Analysis for Text Recognition",Unpublished PhD thesis, Dept.
of Computing, Nottingham Trent University.T.G.
Rose & L.J.
Evett (1992) "A Large Vocabulary Semantic Analyser forHandwriting Recognition", AISB Quarterly, No.
80, Brighton, England.H.
Schuetze (forthcoming) "Word space", in S. Hanson, J. Cowan & C. Giles (Eds.
)"Advances in Neural Information Processing Systems", San Mateo CA, MorganKaufmann.F.
Smadja (1989) "Macrocoding the lexicon with co-occurrence knowledge", Proc.1 st International Lexieal Acquisition Workshop, Detroit, Michigan, pp.
197-204.D.E.
Walker & R.A. Amsler (1986) "The use of machine-readable dictionaries insublanguage analysis", in R. Gfishman & R. Kittredge (Eds.)
"Analyzing Languagein Restricted Domains", LEA, HiUsdale, N.J.73
