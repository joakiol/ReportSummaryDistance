Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531?537,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsApplying a Naive Bayes Similarity Measure toWord Sense DisambiguationTong WangUniversity of Torontotong@cs.toronto.eduGraeme HirstUniversity of Torontogh@cs.toronto.eduAbstractWe replace the overlap mechanism of theLesk algorithm with a simple, general-purpose Naive Bayes model that mea-sures many-to-many association betweentwo sets of random variables.
Even withsimple probability estimates such as max-imum likelihood, the model gains signifi-cant improvement over the Lesk algorithmon word sense disambiguation tasks.
Withadditional lexical knowledge from Word-Net, performance is further improved tosurpass the state-of-the-art results.1 IntroductionTo disambiguate a homonymous word in a givencontext, Lesk (1986) proposed a method that mea-sured the degree of overlap between the glossesof the target and context words.
Known as theLesk algorithm, this simple and intuitive methodhas since been extensively cited and extended inthe word sense disambiguation (WSD) commu-nity.
Nonetheless, its performance in several WSDbenchmarks is less than satisfactory (Kilgarriffand Rosenzweig, 2000; Vasilescu et al, 2004).Among the popular explanations is a key limita-tion of the algorithm, that ?Lesk?s approach is verysensitive to the exact wording of definitions, so theabsence of a certain word can radically change theresults.?
(Navigli, 2009).Compounding this problem is the fact that manyLesk variants limited the concept of overlap tothe literal interpretation of string matching (withtheir own variants such as length-sensitive match-ing (Banerjee and Pedersen, 2002), etc.
), and itwas not until recently that overlap started to takeon other forms such as tree-matching (Chen et al,2009) and vector space models (Abdalgader andSkabar, 2012; Raviv et al, 2012; Patwardhan andPedersen, 2006).
To address this limitation, aNaive Bayes model (NBM) is proposed in thisstudy as a novel, probabilistic treatment of over-lap in gloss-based WSD.2 Related WorkIn the extraordinarily rich literature on WSD, wefocus our review on those closest to the topic ofLesk and NBM.
In particular, we opt for the ?sim-plified Lesk?
(Kilgarriff and Rosenzweig, 2000),where inventory senses are assessed by gloss-context overlap rather than gloss-gloss overlap.This particular variant prevents proliferation ofgloss comparison on larger contexts (Mihalceaet al, 2004) and is shown to outperform the origi-nal Lesk algorithm (Vasilescu et al, 2004).To the best of our knowledge, NBMs have beenemployed exclusively as classifiers in WSD ?that is, in contrast to their use as a similarity mea-sure in this study.
Gale et al (1992) used NBclassifier resembling an information retrieval sys-tem: a WSD instance is regarded as a document d,and candidate senses are scored in terms of ?rel-evance?
to d. When evaluated on a WSD bench-mark (Vasilescu et al, 2004), the algorithm com-pared favourably to Lesk variants (as expectedfor a supervised method).
Pedersen (2000) pro-posed an ensemble model with multiple NB clas-sifiers differing by context window size.
Hristea(2009) trained an unsupervised NB classifier usingthe EM algorithm and empirically demonstratedthe benefits of WordNet-assisted (Fellbaum, 1998)feature selection over local syntactic features.Among Lesk variants, Banerjee and Pedersen(2002) extended the gloss of both inventory sensesand the context words to include words in their re-lated synsets in WordNet.
Senses were scored bythe sum of overlaps across all relation pairs, andthe effect of individual relation pairs was evalu-ated in a later work (Banerjee and Pedersen, 2003).Overlap was assessed by string matching, with thenumber of matching words squared so as to assign531higher scores to multi-word overlaps.Breaking away from string matching, Wilkset al (1990) measured overlap as similarity be-tween gloss- and context-vectors, which were ag-gregated word vectors encoding second order co-occurrence information in glosses.
An extensionby Patwardhan and Pedersen (2006) differentiatedcontext word senses and extended shorter glosseswith related glosses in WordNet.
Patwardhan et al(2003) measured overlap by concept similarity(Budanitsky and Hirst, 2006) between each inven-tory sense and the context words.
Gloss overlapsfrom their earlier work actually out-performed allfive similarity-based methods.More recently, Chen et al (2009) pro-posed a tree-matching algorithm that measuredgloss-context overlap as the weighted sum ofdependency-induced lexical distance.
Abdalgaderand Skabar (2012) constructed a sentential simi-larity measure (Li et al, 2006) using lexical simi-larity measures (Budanitsky and Hirst, 2006), andoverlap was measured by the cosine of their re-spective sentential vectors.
A related approach(Raviv et al, 2012) also used Wikipedia-inducedconcepts to encoded sentential vectors.
These sys-tems compared favourably to existing methods inWSD performance, although by using sense fre-quency information, they are essentially super-vised methods.Distributional methods have been used in manyWSD systems in quite different flavours than thecurrent study.
Kilgarriff and Rosenzweig (2000)proposed a Lesk variant where each gloss word isweighted by its idf score in relation to all glosses,and gloss-context association was incremented bythese weights rather than binary, overlap counts.Miller et al (2012) used distributional thesauri as aknowledge base to increase overlaps, which were,again, assessed by string matching.In conclusion, the majority of Lesk variantsfocused on extending the gloss to increase thechance of overlapping, while the proposed NBMaims to make better use of the limited lexicalknowledge available.
In contrast to string match-ing, the probabilistic nature of our model offersa ?softer?
measurement of gloss-context associa-tion, resulting in a novel approach to unsupervisedWSD with state-of-the-art performance in morethan one WSD benchmark (Section 4).3 Model and Task Descriptions3.1 The Naive Bayes ModelFormally, given two sets e = {ei} and f = { fj}each consisting of multiple random events, theproposed model measures the probabilistic asso-ciation p(f|e) between e and f. Under the assump-tion of conditional independence among the eventsin each set, a Naive Bayes treatment of the mea-sure can be formulated as:p(f|e) =?jp( fj|{ei}) =?jp({ei}| fj)p( fj)p({ei})=?j[p( fj)?ip(ei| fj)]?j?ip(ei),(1)In the second expression, Bayes?s rule is appliednot only to take advantage of the conditional inde-pendence among ei?s, but also to facilitate proba-bility estimation, since p({ei}| fj) is easier to esti-mate in the context of WSD, where sample spacesof e and f become asymmetric (Section 3.2).3.2 Model Application in WSDIn the context of WSD, e can be regarded as aninstance of a polysemous word w, while f repre-sents certain lexical knowledge about the sense sof w manifested by e.1WSD is thus formulated asidentifying the sense s?in the sense inventory Sof w s.t.
:s?= argmaxs?Sp(f|e) (2)In one of their simplest forms, ei?s correspondto co-occurring words in the instance of w, andfj?s consist of the gloss words of sense s. Conse-quently, p(f|e) is essentially measuring the asso-ciation between context words of w and definitiontexts of s, i.e., the gloss-context association in thesimplified Lesk algorithm (Kilgarriff and Rosen-zweig, 2000).
A major difference, however, is thatinstead of using hard, overlap counts between thetwo sets of words from the gloss and the context,this probabilistic treatment can implicitly modelthe distributional similarity among the elements eiand fj(and consequently between the sets e andf) over a wider range of contexts.
The result is a?softer?
proxy of association than the binary viewof overlaps in existing Lesk variants.The foregoing discussion offers a second mo-tivation for applying Bayes?s rule on the second1Think of the notations e and f mnemonically as exem-plars and features, respectively.532Senses Hypernyms Hyponyms Synonymsfactory buildingcomplex,complexbrewery,factory,mill, ...works,industrialplantlife form organism,beingperennial,crop...flora,plant lifeTable 1: Lexical knowledge for the word plant un-der its two meanings factory and life form.expression in Equation (1): it is easier to estimatep(ei| fj) than p( fj|ei), since the vocabulary for thelexical knowledge features ( fj) is usually morelimited than that of the contexts (ei) and hence esti-mation of the former suffices on a smaller amountof data than that of the latter.3.3 Incorporating Additional LexicalKnowledgeThe input of the proposed NBM is bags of words,and thus it is straightforward to incorporate var-ious forms of lexical knowledge (LK) for wordsenses: by concatenating a tokenized knowledgesource to the existing knowledge representation f,while the similarity measure remains unchanged.The availability of LK largely depends on thesense inventory used in a WSD task.
WordNetsenses are often used in Senseval and SemEvaltasks, and hence senses (or synsets, and possiblytheir corresponding word forms) that are seman-tic related to the inventory senses under WordNetrelations are easily obtainable and have been ex-ploited by many existing studies.As pointed out by Patwardhan et al (2003),however, ?not all of these relations are equallyhelpful.?
Relation pairs involving hyponyms wereshown to result in better F-measure when usedin gloss overlaps (Banerjee and Pedersen, 2003).The authors attributed the phenomenon to the themultitude of hyponyms compared to other rela-tions.
We further hypothesize that, beyond sheernumbers, synonyms and hyponyms offer strongersemantic specification that helps distinguish thesenses of a given ambiguous word, and thus aremore effective knowledge sources for WSD.Take the word plant for example.
Selected hy-pernyms, hyponyms, and synonyms pertaining toits two senses factory and life form are listed inTable 1.
Hypernyms can be overly general terms(e.g., being).
Although conceptually helpful forhumans in coarse-grained WSD, this generality islikely to inflate the hypernyms?
probabilistic esti-mation.
Hyponyms, on the other hand, help spec-ify their corresponding senses with informationthat is possibly missing from the often overly briefglosses: the many technical terms as hyponymsin Table 1 ?
though rare ?
are likely to occurin the (possibly domain-specific) contexts that arehighly typical of the corresponding senses.
Par-ticularly for the NBM, the co-occurrence is likelyto result in stronger gloss-definition associationswhen similar contexts appear in a WSD instance.We also observe that some semantically relatedwords appear under rare senses (e.g., still as analcohol-manufacturing plant, and annual as a one-year-life-cycle plant; omitted from Table 1).
Thisis a general phenomenon in gloss-based WSD andis beyond the scope of the current discussion.2Overall, all three sources of LK may complementeach other in WSD tasks, with hyponyms particu-larly promising in both quantity and quality com-pared to hypernyms and synonyms.33.4 Probability EstimationA most open-ended question is how to estimate theprobabilities in Equation (1).
In WSD in particu-lar, the estimation concerns the marginal and con-ditional probabilities of and between word tokens.Many options are available to this end in statis-tical machine learning (MLE, MAP, etc.
), infor-mation theory (Church and Hanks, 1990; Turney,2001), as well as the rich body of research in lex-ical semantic similarity Resnik, 1995; Jiang andConrath, 1997; Budanitsky and Hirst, 2006).Here we choose maximum likelihood ?
notonly for its simplicity, but also to demonstratemodel strength with a relatively crude probabilityestimation.
To avoid underflow, Equation (1) isestimated as the following log probability:?ilogc( fj)c(?
)+?i?jlogc(ei, fj)c( fj)?|f|?jlogc(ei)c(?
)=(1?|e|)?ilogc( fj)?|f|?jlogc(ei)+?i?jlogc(ei, fj)+ |f|(|e|?1) logc(?
),where c(x) is the count of word x, c(?)
is the corpus2We do, however, refer curious readers to the work of Ra-viv et al (2012) for a novel treatment of a similar problem.3Note that LK expansion is a feature of our model ratherthan a requirement.
What type of knowledge to include iseventually a decision made by the user based on the applica-tion and LK availability.533size, c(x,y) is the joint count of x and y, and |v| isthe dimension of vector v.Nonetheless, we do investigate how model per-formance responds to estimation quality.
Specif-ically in WSD, a source corpus is defined as thesource of the majority of the WSD instances in agiven dataset, and a baseline corpus of a smallersize and less resemblance to the instances is usedfor all datasets.
The assumption is that a sourcecorpus offers better estimates for the model thanthe baseline corpus, and difference in model per-formance is expected when using probability esti-mation of different quality.4 Evaluation4.1 Data, Scoring, and Pre-processingVarious aspects of the model discussed in Section3 are evaluated in the English lexical sample tasksfrom Senseval-2 (Edmonds and Cotton, 2001) andSemEval-2007 (Pradhan et al, 2007).
Trainingsections are used as development data and testsections held out for final testing.
Model perfor-mance is evaluated in terms of WSD accuracy us-ing Equation (2) as the scoring function.
Accu-racy is defined as the number of correct responsesover the number of instances.
Because it is a rareevent for the NBM to produce identical scores,4the model always proposes a unique answer andaccuracy is thus equivalent to F-score commonlyused in existing reports.Multiword expressions (MWEs) in theSenseval-2 sense inventory are not explicitlymarked in the contexts.
Several of the top-rankingsystems implemented their own MWE detectionalgorithms (Kilgarriff and Rosenzweig, 2000;Litkowski, 2002).
Without digressing to thedetails of MWE detection ?
and meanwhile,to ensure fair comparison with existing systems?
we implement two variants of the predictionmodule, one completely ignorant of MWE anddefaulting to INCORRECT for all MWE-relatedanswers, while the other assuming perfect MWEdetection and performing regular disambiguationalgorithm on the MWE-related senses (not de-faulting to CORRECT).
All results reported forSenseval-2 below are harmonic means of the twooutcomes.Each inventory sense is represented by a set ofLK tokens (e.g., definition texts, synonyms, etc.
)4This has never occurred in the hundreds of thousands ofruns in our development process.from their corresponding WordNet synset (or inthe coarse-grained case, a concatenation of tokensfrom all synsets in a sense group).
The MIT-JWIlibrary (Finlayson, 2014) is used for accessingWordNet.
Usage examples in glosses (included bythe library by default) are removed in our experi-ments.5Basic pre-processing is performed on the con-texts and the glosses, including lower-casing, stop-word removal, lemmatization on both datasets,and tokenization on the Senseval-2 instances.6Stanford CoreNLP7is used for lemmatization andtokenization.
Identical procedures are applied toall corpora used for probability estimation.Binomial test is used for significance testing,and with one exception explicitly noted in Sec-tion 4.3, all differences presented are statisticallyhighly significant (p < 0.001).4.2 Comparing Lexical Knowledge SourcesTo study the effect of different types of LK inWSD (Section 3.3), for each inventory sense, wechoose synonyms (syn), hypernyms (hpr), and hy-ponyms (hpo) as extended LK in addition to itsgloss.
The WSD model is evaluated with gloss-only (glo), individual extended LK sources, andthe combination of all four sources (all).
The re-sults are listed in Table 2 together with existing re-sults (1st and 2nd correspond to the results of thetop two unsupervised methods in each dataset).8By using only glosses, the proposed modelalready shows statistically significant improve-ment over the basic Lesk algorithm (92.4%and 140.5% relative improvement in Senseval-2 coarse- and fine-grained tracks, respectively).9Moreover, comparison between coarse- and fine-grained tracks reveals interesting properties of dif-ferent LK sources.
Previous hypotheses (Section3.3) are empirically confirmed that WSD perfor-5We also compared the two Lesk baselines (with andwithout usage examples) on the development data but did notobserve significant differences as reported by Kilgarriff andRosenzweig (2000).6The SemEval-2007 instances are already tokenized.7http://nlp.stanford.edu/software/corenlp.shtml.8We excluded the results of UNED (Fern?andez-Amor?oset al, 2001) in Senseval-2 because, by using sense frequencyinformation that is only obtainable from sense-annotated cor-pora, it is essentially a supervised system.9Comparisons are made against the simplified Lesk al-gorithm (Kilgarriff and Rosenzweig, 2000) without usageexamples.
The comparison is unavailable in SemEval2007since we have not found existing experiments with this exactconfiguration.534Dataset glo syn hpr hpo all 1st 2nd LeskSenseval-2 Coarse .475 .478 .494 .518 .523 .469 .367 .262Senseval-2 Fine .362 .371 .326 .379 .388 .412 .293 .163SemEval-2007 .494 .511 .507 .550 .573 .538 .521 ?Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine- andcoarse-grained) and the SemEval-2007 dataset.Figure 1: Model response to probability esti-mates of different quality on the SemEval-2007dataset.
Error bars indicate confidence intervals(p < .001), and the dashed line corresponds to thebest reported result.mance benefits most from hyponyms and leastfrom hypernyms.
Specifically, highly similar, fine-grained sense candidates apparently share morehypernyms in the fine-grained case than in thecoarse-grained case; adding to the generality ofhypernyms (both semantic and distributional), wepostulate that their probability in the NBM is uni-formly inflated among many sense candidates, andhence they decrease in distinguishability.
Syn-onyms might help with regard to semantic spec-ification, though their limited quantity also limitstheir benefits.
These patterns on the LK types areconsistent in all three experiments.When including all four LK sources, our modeloutperforms the state-of-the-art systems with sta-tistical significance in both coarse-grained tasks.For the fine-grained track, it achieves 2nd placeafter that of Tugwell and Kilgarriff (2001), whichused a decision list (Yarowsky, 1995) on manu-ally selected corpora evidence for each inventorysense, and thus is not subject to loss of distin-guishability in the glosses as Lesk variants are.4.3 Probability EstimationTo evaluate model response to probability esti-mation of different quality (Section 3.4), sourcecorpora are chosen as the majority value of thedoc-source attribute of instances in each dataset,namely, the British National Corpus for Senseval-2 (94%) and the Wall Street Journal for SemEval-2007 (86%).
The Brown Corpus is shared by bothdatasets as the baseline corpus.
Figure 1 shows thecomparison on the SemEval-2007 dataset.
Acrossall experiments, higher WSD accuracy is consis-tently witnessed using the source corpus; differ-ences are statistically highly significant except forhpo (which is significant with p < 0.01).5 Conclusions and Future WorkWe have proposed a general-purpose Naive Bayesmodel for measuring association between two setsof random events.
The model replaced stringmatching in the Lesk algorithm for word sense dis-ambiguation with a probabilistic measure of gloss-context overlap.
The base model on average morethan doubled the accuracy of Lesk in Senseval-2on both fine- and coarse-grained tracks.
With ad-ditional lexical knowledge, the model also outper-formed state of the art results with statistical sig-nificance on two coarse-grained WSD tasks.For future work, we plan to apply the modelin other shared tasks, including open-text WSD,so as to compare with more recent Lesk variants.We would also like to explore how to incorpo-rate syntactic features and employ alternative sta-tistical methods (e.g., parametric models) to im-prove probability estimation and inference.
OtherNLP problems involving compositionality in gen-eral might also benefit from the proposed many-to-many similarity measure.AcknowledgmentsThis study is funded by the Natural Sciences andEngineering Research Council of Canada.
Wethank Afsaneh Fazly, Navdeep Jaitly, and VaradaKolhatkar for the many inspiring discussions, aswell as the anonymous reviewers for their con-structive advice.535ReferencesKhaled Abdalgader and Andrew Skabar.
Unsupervisedsimilarity-based word sense disambiguation using contextvectors and sentential word importance.
ACM Transac-tions on Speech and Language Processing, 9(1):2:1?2:21,May 2012.Satanjeev Banerjee and Ted Pedersen.
An adapted Lesk al-gorithm for word sense disambiguation using WordNet.In Computational Linguistics and Intelligent Text Process-ing, pages 136?145.
Springer, 2002.Satanjeev Banerjee and Ted Pedersen.
Extended gloss over-laps as a measure of semantic relatedness.
In Proceedingsof the 18th International Joint Conference on Artificial In-telligence, volume 3, pages 805?810, 2003.Alexander Budanitsky and Graeme Hirst.
EvaluatingWordNet-based measures of lexical semantic relatedness.Computational Linguistics, 32(1):13?47, 2006.Ping Chen, Wei Ding, Chris Bowes, and David Brown.
Afully unsupervised word sense disambiguation method us-ing dependency knowledge.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association for Com-putational Linguistics, pages 28?36, Stroudsburg, PA,USA, 2009.Kenneth Ward Church and Patrick Hanks.
Word associationnorms, mutual information, and lexicography.
Computa-tional Linguistics, 16(1):22?29, 1990.Philip Edmonds and Scott Cotton.
Senseval-2: Overview.
InProceedings of the 2nd International Workshop on Eval-uating Word Sense Disambiguation Systems, pages 1?5.Association for Computational Linguistics, 2001.Christiane Fellbaum.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA, 1998.David Fern?andez-Amor?os, Julio Gonzalo, and Felisa Verdejo.The UNED systems at Senseval-2.
In The Proceedings ofthe Second International Workshop on Evaluating WordSense Disambiguation Systems, pages 75?78.
Associationfor Computational Linguistics, 2001.Mark Alan Finlayson.
Java libraries for accessing the Prince-ton WordNet: Comparison and evaluation.
In Proceed-ings of the 7th Global Wordnet Conference, Tartu, Estonia,2014.William Gale, Kenneth Church, and David Yarowsky.
Amethod for disambiguating word senses in a large corpus.Computers and the Humanities, 26(5-6):415?439, 1992.Florentina Hristea.
Recent advances concerning the usage ofthe Na?
?ve Bayes model in unsupervised word sense dis-ambiguation.
International Review on Computers & Soft-ware, 4(1), 2009.Jay Jiang and David Conrath.
Semantic similarity based oncorpus statistics and lexical taxonomy.
Proceedings ofInternational Conference on Research in ComputationalLinguistics, 1997.Adam Kilgarriff and Joseph Rosenzweig.
Framework andresults for English Senseval.
Computers and the Humani-ties, 34(1-2):15?48, 2000.Michael Lesk.
Automatic sense disambiguation using ma-chine readable dictionaries: how to tell a pine cone froman ice cream cone.
In Proceedings of the 5th Annual In-ternational Conference on Systems Documentation, pages24?26, New York, New York, USA, 1986.Yuhua Li, David McLean, Zuhair A Bandar, James DO?Shea, and Keeley Crockett.
Sentence similarity basedon semantic nets and corpus statistics.
IEEE Transactionson Knowledge and Data Engineering, 18(8):1138?1150,2006.Kenneth C. Litkowski.
Sense information for disambigua-tion: Confluence of supervised and unsupervised methods.In Proceedings of the ACL-02 Workshop on Word SenseDisambiguation: Recent Successes and Future Directions,pages 47?53.
Association for Computational Linguistics,July 2002.Rada Mihalcea, Paul Tarau, and Elizabeth Figa.
PageRank onsemantic networks, with application to word sense disam-biguation.
In Proceedings of the 20th International Con-ference on Computational Linguistics, 2004.Tristan Miller, Chris Biemann, Torsten Zesch, and IrynaGurevych.
Using distributional similarity for lexical ex-pansion in knowledge-based word sense disambiguation.In Proceedings of the 24th International Conference onComputational Linguistics, pages 1781?1796, 2012.Roberto Navigli.
Word sense disambiguation: A survey.ACM Computing Surveys, 41(2):10:1?10:69, 2009.Siddharth Patwardhan and Ted Pedersen.
Using WordNet-based context vectors to estimate the semantic relatednessof concepts.
Proceedings of the EACL 2006 WorkshopMaking Sense of Sense-Bringing Computational Linguis-tics and Psycholinguistics Together, 1501:1?8, 2006.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-sen.
Using measures of semantic relatedness for wordsense disambiguation.
In Proceedings of the 4th Interna-tional Conference on Intelligent Text Processing and Com-putational Linguistics, pages 241?257, 2003.Ted Pedersen.
A simple approach to building ensembles ofNaive Bayesian classifiers for word sense disambiguation.In Proceedings of the 1st Conference of North AmericanChapter of the Association for Computational Linguistics,pages 63?69.
Association for Computational Linguistics,2000.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
SemEval-2007 task 17: English lexicalsample, SRL and all words.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations, pages87?92.
Association for Computational Linguistics, 2007.Ariel Raviv, Shaul Markovitch, and Sotirios-EfstathiosManeas.
Concept-based approach to word sense disam-biguation.
In Proceedings of the 26th Conference on Arti-ficial Intelligence, 2012.Philip Resnik.
Using information content to evaluate seman-tic similarity in a taxonomy.
In Proceedings of the 14thInternational Joint Conference on Artificial Intelligence -Volume 1, IJCAI?95, pages 448?453, San Francisco, CA,USA, 1995.David Tugwell and Adam Kilgarriff.
Wasp-bench: a lex-icographic tool supporting word sense disambiguation.In The Proceedings of the Second International Work-shop on Evaluating Word Sense Disambiguation Systems,pages 151?154.
Association for Computational Linguis-tics, 2001.Peter Turney.
Mining the web for synonyms: PMI-IR versusLSA on TOEFL.
In Proceedings of the 12th EuropeanConference on Machine Learning, pages 491?502, 2001.Florentina Vasilescu, Philippe Langlais, and Guy Lapalme.Evaluating variants of the Lesk approach for disambiguat-ing words.
In Proceedings of the 4th International Con-ference on Language Resources and Evaluation, 2004.536Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDon-ald, Tony Plate, and Brian M. Slator.
Providing machinetractable dictionary tools.
Machine Translation, 5(2):99?154, 1990.David Yarowsky.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proceedings of the 33rdannual meeting on Association for Computational Lin-guistics, pages 189?196, 1995.537
