Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 648?653,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsComparative News Summarization Using Linear ProgrammingXiaojiang Huang Xiaojun Wan?
Jianguo XiaoInstitute of Computer Science and Technology, Peking University, Beijing 100871, ChinaKey Laboratory of Computational Linguistic (Peking University), MOE, China{huangxiaojiang, wanxiaojun, xiaojianguo}@icst.pku.edu.cnAbstractComparative News Summarization aims tohighlight the commonalities and differencesbetween two comparable news topics.
Inthis study, we propose a novel approach togenerating comparative news summaries.
Weformulate the task as an optimization problemof selecting proper sentences to maximize thecomparativeness within the summary and therepresentativeness to both news topics.
Weconsider semantic-related cross-topic conceptpairs as comparative evidences, and con-sider topic-related concepts as representativeevidences.
The optimization problem isaddressed by using a linear programmingmodel.
The experimental results demonstratethe effectiveness of our proposed model.1 IntroductionComparative News Summarization aims to highlightthe commonalities and differences between twocomparable news topics.
It can help users to analyzetrends, draw lessons from the past, and gain insightsabout similar situations.
For example, by comparingthe information about mining accidents in Chile andChina, we can discover what leads to the differentendings and how to avoid those tragedies.Comparative text mining has drawn much atten-tion in recent years.
The proposed works differin the domain of corpus, the source of comparisonand the representing form of results.
So far, mostresearches focus on comparing review opinions ofproducts (Liu et al, 2005; Jindal and Liu, 2006a;?Corresponding authorJindal and Liu, 2006b; Lerman and McDonald,2009; Kim and Zhai, 2009).
A reason is that theaspects in reviews are easy to be extracted and thecomparisons have simple patterns, e.g.
positivevs.
negative.
A few other works have alsotried to compare facts and views in news article(Zhai et al, 2004) and Blogs (Wang et al, 2009).The comparative information can be extracted fromexplicit comparative sentences (Jindal and Liu,2006a; Jindal and Liu, 2006b; Huang et al, 2008),or mined implicitly by matching up features ofobjects in the same aspects (Zhai et al, 2004; Liuet al, 2005; Kim and Zhai, 2009; Sun et al,2006).
The comparisons can be represented bycharts (Liu et al, 2005), word clusters (Zhai et al,2004), key phrases(Sun et al, 2006), and summarieswhich consist of pairs of sentences or text sections(Kim and Zhai, 2009; Lerman and McDonald,2009; Wang et al, 2009).
Among these forms,the comparative summary conveys rich informationwith good readability, so it keeps attracting interestin the research community.
In general, documentsummarization can be performed by extraction orabstraction (Mani, 2001).
Due to the difficultyof natural sentence generation, most automaticsummarization systems are extraction-based.
Theyselect salient sentences to maximize the objectivefunctions of generated summaries (Carbonell andGoldstein, 1998; McDonald, 2007; Lerman andMcDonald, 2009; Kim and Zhai, 2009; Gillick et al,2009).
The major difference between the traditionalsummarization task and the comparative summa-rization task is that traditional summarization taskplaces equal emphasis on all kinds of information in648the source, while comparative summarization taskonly focuses on the comparisons between objects.News is one of the most important channels foracquiring information.
However, it is more difficultto extract comparisons in news articles than inreviews.
The aspects are much diverse in news.They can be the time of the events, the personinvolved, the attitudes of participants, etc.
Theseaspects can be expressed explicitly or implicitly inmany ways.
For example, ?storm?
and ?rain?
bothtalk about ?weather?, and thus they can form apotential comparison.
All these issues raise greatchallenges to comparative summarization in thenews domain.In this study, we propose a novel approach forcomparative news summarization.
We considercomparativeness and representativeness as well asredundancy in an objective function, and solve theoptimization problem by using linear programmingto extract proper comparable sentences.
Morespecifically, we consider a pair of sentencescomparative if they share comparative concepts;we also consider a sentence representative if itcontains important concepts about the topic.
Thusa good comparative summary contains importantcomparative pairs, as well as important conceptsabout individual topics.
Experimental resultsdemonstrate the effectiveness of our model, whichoutperforms the baseline systems in quality ofcomparison identification and summarization.2 Problem Definition2.1 ComparisonA comparison identifies the commonalities ordifferences among objects.
It basically consistsof four components: the comparee (i.e.
what iscompared), the standard (i.e.
to what the compareis compared), the aspect (i.e.
the scale on whichthe comparee and standard are measured), and theresult (i.e.
the predicate that describes the positionsof the comparee and standard).
For example, ?Chileis richer than Haiti.?
is a typical comparison, wherethe comparee is ?Chile?
; the standard is ?Haiti?
; thecomparative aspect is wealth, which is implied by?richer?
; and the result is that Chile is superior toHaiti.A comparison can be expressed explicitly in acomparative sentence, or be described implicitlyin a section of text which describes the individualcharacteristics of each object point-by-point.
Forexample, the following textHaiti is an extremely poor country.Chile is a rich country.also suggests that Chile is richer than Haiti.2.2 Comparative News SummarizationThe task of comparative news summarization is tobriefly sum up the commonalities and differencesbetween two comparable news topics by usinghuman readable sentences.
The summarizationsystem is given two collections of news articles,each of which is related to a topic.
The systemshould find latent comparative aspects, and generatedescriptions of those aspects in a pairwise way, i.e.including descriptions of two topics simultaneouslyin each aspect.
For example, when comparingthe earthquake in Haiti with the one in Chile,the summary should contain the intensity of eachtemblor, the damages in each disaster area, thereactions of each government, etc.Formally, let t1 and t2 be two comparable newstopics, and D1 and D2 be two collections ofarticles about each topic respectively.
The task ofcomparative summarization is to generate a shortabstract which conveys the important comparisons{< t1, t2, r1i, r2i >}, where r1i and r2i aredescriptions about topic t1 and t2 in the samelatent aspect ai respectively.
The summary can beconsidered as a combination of two components,each of which is related to a news topic.
It can alsobe subdivided into several sections, each of whichfocuses on a major aspect.
The comparisons shouldhave good quality, i.e., be clear and representative toboth topics.
The coverage of comparisons should beas wide as possible, which means the aspects shouldnot be redundant because of the length limit.3 Proposed ApproachIt is natural to select the explicit comparativesentences as comparative summary, because theyexpress comparison explicitly in good qualities.However, they do not appear frequently in regularnews articles so that the coverage is limited.
Instead,649it is more feasible to extract individual descriptionsof each topic over the same aspects and thengenerate comparisons.To discover latent comparative aspects, weconsider a sentence as a bag of concepts, each ofwhich has an atom meaning.
If two sentences havesame concepts in common, they are likely to discussthe same aspect and thus they may be comparablewith each other.
For example,Lionel Messi named FIFA Word Player ofthe Year 2010.Cristiano Ronalo Crowned FIFA WordPlayer of the Year 2009.The two sentences compare on the ?FIFA WordPlayer of the Year?, which is contained in bothsentences.
Furthermore, semantic related conceptscan also represent comparisons.
For example,?snow?
and ?sunny?
can indicate a comparisonon ?weather?
; ?alive?
and ?death?
can imply acomparison on ?rescue result?.
Thus the pairsof semantic related concepts can be considered asevidences of comparisons.A comparative summary should contain as manycomparative evidences as possible.
Besides, itshould convey important information in the originaldocuments.
Since we model the text with acollection of concept units, the summary shouldcontain as many important concepts as possible.An important concept is likely to be mentionedfrequently in the documents, and thus we use thefrequency as a measure of a concept?s importance.Obviously, the more accurate the extractedconcepts are, the better we can represent themeaning of a text.
However, it is not easy to extractsemantic concepts accurately.
In this study, weuse words, named entities and bigrams to simplyrepresent concepts, and leave the more complexconcept extraction for future work.Based on the above ideas, we can formulatethe summarization task as an optimization problem.Formally, letCi = {cij} be the set of concepts in thedocument set Di, (i = 1, 2).
Each concept cij has aweight wij ?
R. ocij ?
{0, 1} is a binary variableindicating whether the concept cij is presented in thesummary.
A cross-topic concept pair < c1j , c2k >has a weight ujk ?
R that indicates whether itimplies a important comparison.
opjk is a binaryvariable indicating whether the pair is presented inthe summary.
Then the objective function score of acomparative summary can be estimated as follows:?|C1|?j=1|C2|?k=1ujk ?opjk +(1??
)2?i=1|Ci|?j=1wij ?ocij (1)The first component of the function estimates thecomparativeness within the summary and the secondcomponent estimates the representativeness to bothtopics.
?
?
[0, 1] is a factor that balances these twofactors.
In this study, we set ?
= 0.55.The weights of concepts are calculated as follows:wij = tfij ?
idfij (2)where tfij is the term frequency of the concept cijin the document set Di, and idfij is the inversedocument frequency calculated over a backgroundcorpus.The weights of concept pairs are calculated asfollows:ujk ={(w1j + w2k)/2, if rel(c1j , c2k) > ?0, otherwise(3)where rel(c1j , c2k) is the semantic relevance be-tween two concepts, and it is calculated using thealgorithms basing on WordNet (Pedersen et al,2004).
If the relevance is higher than the threshold?
(0.2 in this study), then the concept pair isconsidered as an evidence of comparison.Note that a concept pair will not be presented inthe summary unless both the concepts are presented,i.e.opjk ?
oc1j (4)opjk ?
oc2k (5)In order to avoid bias towards the concepts whichhave more related concepts, we only count the mostimportant relation of each concept, i.e.
?kopjk ?
1, ?j (6)?jopjk ?
1, ?k (7)The algorithm selects proper sentences to max-imize the objective function.
Formally, let Si =650{sik} be the set of sentences in Di, ocsijk bea binary variable indicating whether concept cijoccurs in sentence sik, and osik be a binary variableindicating whether sik is presented in the summary.If sik is selected in the summary, then all theconcepts in it are presented in the summary, i.e.ocij ?
ocsijk ?
osik, ?1 ?
j ?
|Ci| (8)Meanwhile, a concept will not be present in thesummary unless it is contained in some selectedsentences, i.e.ocij ?|Si|?k=1ocsijk ?
osik (9)Finally, the summary should satisfy a lengthconstraint:2?i=1|Si|?k=1lik ?
osik ?
L (10)where lik is the length of sentence sik, and L is themaximal summary length.The optimization of the defined objective functionunder above constraints is an integer linear program-ming (ILP) problem.
Though the ILP problemsare generally NP-hard, considerable works havebeen done and several software solutions have beenreleased to solve them efficiently.14 Experiment4.1 DatasetBecause of the novelty of the comparative newssummarization task, there is no existing data setfor evaluating.
We thus create our own.
We firstchoose five pairs of comparable topics, then retrieveten related news articles for each topic using theGoogle News2 search engine.
Finally we write thecomparative summary for each topic pair manually.The topics are showed in table 1.4.2 Evaluation MetricsWe evaluate the models with following measures:Comparison Precision / Recall / F-measure:let aa and am be the numbers of all aspects1We use IBM ILOG CPLEX optimizer to solve the problem.2http://news.google.comID Topic 1 Topic 21 Haiti Earth quake Chile Earthquake2 Chile Mining Acci-dentNew Zealand MiningAccident3 Iraq Withdrawal AfghanistanWithdrawal4 Apple iPad 2 BlackBerry Playbook5 2006 FIFAWorld Cup 2010 FIFAWorld CupTable 1: Comparable topic pairs in the dataset.involved in the automatically generated summaryand manually written summary respectively; cabe the number of human agreed comparativeaspects in the automatically generated summary.The comparison precision (CP ), comparison recall(CR) and comparison F-measure (CF ) are definedas follows:CP = caaa; CR = caam; CF = 2 ?
CP ?
CRCP + CRROUGE: the ROUGE is a widely used metricin summarization evaluation.
It measures summaryquality by counting overlapping units between thecandidate summary and the reference summary (Linand Hovy, 2003).
In the experiment, we reportthe f-measure values of ROUGE-1, ROUGE-2 andROUGE-SU4, which count overlapping unigrams,bigrams and skip-4-grams respectively.
To evaluatewhether the summary is related to both topics,we also split each comparative summary into twotopic-related parts, evaluate them respectively, andreport the mean of the two ROUGE values (denotedas MROUGE).4.3 Baseline SystemsNon-Comparative Model (NCM): Thenon-comparative model treats the task as atraditional summarization problem and selects theimportant sentences from each document collection.The model is adapted from our approach by setting?
= 0 in the objection function 1.Co-Ranking Model (CRM): The co-rankingmodel makes use of the relations within eachtopic and relations across the topics to reinforcescores of the comparison related sentences.
Themodel is adapted from (Wan et al, 2007).
The651SS, WW and SW relationships are replaced byrelationships between two sentences within eachtopic and relationships between two sentences fromdifferent topics.4.4 Experiment ResultsWe apply all the systems to generate comparativesummaries with a length limit of 200 words.
Theevaluation results are shown in table 2.
Comparedwith baseline models, our linear programming basedcomparative model (denoted as LPCM) achievesbest scores over all metrics.
It is expected to findthat the NCM model does not perform well in thistask because it does not focus on the comparisons.The CRM model utilizes the similarity betweentwo topics to enhance the score of comparisonrelated sentences.
However, it does not guaranteeto choose pairwise sentences to form comparisons.The LPCM model focus on both comparativenessand representativeness at the same time, and thusit achieves good performance on both comparisonextraction and summarization.
Figure 1 showsan example of comparative summary generated byusing the CLPM model.
The summary describesseveral comparisons between two FIFA World Cupsin 2006 and 2010.
Most of the comparisons are clearand representative.5 ConclusionIn this study, we propose a novel approach tosumming up the commonalities and differencesbetween two news topics.
We formulate thetask as an optimization problem of selectingsentences to maximize the score of comparative andrepresentative evidences.
The experiment resultsshow that our model is effective in comparisonextraction and summarization.In future work, we will utilize more semanticinformation such as localized latent topics to helpcapture comparative aspects, and use machinelearning technologies to tune weights of concepts.AcknowledgmentsThis work was supported by NSFC (60873155),Beijing Nova Program (2008B03) and NCET(NCET-08-0006).Model CP CR CF ROUGE-1 ROUGE-2 ROUGE-su4 MROUGE-1 MROUGE-2 MROUGE-su4NCM 0.238 0.262 0.247 0.398 0.146 0.174 0.350 0.122 0.148CRM 0.313 0.285 0.289 0.426 0.194 0.226 0.355 0.146 0.175LPCM 0.359 0.419 0.386 0.427 0.205 0.234 0.380 0.171 0.192Table 2: Evaluation results of systemsWorld Cup 2006 World Cup 2010The 2006 Fifa World Cup drew to a close on Sundaywith Italy claiming their fourth crown after beatingFrance in a penalty shoot-out.Spain have won the 2010 FIFA World Cup South Africafinal, defeating Netherlands 1-0 with a wonderful goalfrom Andres Iniesta deep into extra-time.Zidane won the Golden Ball over Italians FabioCannavaro and Andrea Pirlo.Uruguay star striker Diego Forlan won the GoldenBall Award as he was named the best player of thetournament at the FIFA World Cup 2010 in SouthAfrica.Lukas Podolski was named the inaugural Gillette BestYoung Player.German youngster Thomas Mueller got double delightafter his side finished third in the tournament as he wasnamed Young Player of the World CupGermany striker Miroslav Klose was the Golden Shoewinner for the tournament?s leading scorer.Among the winners were goalkeeper and captain IkerCasillas who won the Golden Glove Award.England?s fans brought more colour than their team.
Only four of the 212 matches played drew more that40,000 fans.Figure 1: A sample comparative summary generated by using the LPCM model652ReferencesJaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofthe 21st annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 335?336.
ACM.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tur.
2009.
A global optimizationframework for meeting summarization.
In Proceed-ings of the 2009 IEEE International Conference onAcoustics, Speech and Signal Processing, ICASSP?09, pages 4769?4772, Washington, DC, USA.
IEEEComputer Society.Xiaojiang.
Huang, Xiaojun.
Wan, Jianwu.
Yang, andJianguo.
Xiao.
2008.
Learning to IdentifyComparative Sentences in Chinese Text.
PRICAI2008: Trends in Artificial Intelligence, pages 187?198.Nitin Jindal and Bing Liu.
2006a.
Identifying compar-ative sentences in text documents.
In Proceedings ofthe 29th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 244?251.
ACM.Nitin Jindal and Bing Liu.
2006b.
Mining comparativesentences and relations.
In proceedings of the 21stnational conference on Artificial intelligence - Volume2, pages 1331?1336.
AAAI Press.Hyun Duk Kim and ChengXiang Zhai.
2009.
Generatingcomparative summaries of contradictory opinions intext.
In Proceeding of the 18th ACM conferenceon Information and knowledge management, pages385?394.
ACM.Kevin Lerman and Ryan McDonald.
2009.
Contrastivesummarization: an experiment with consumer reviews.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, pages113?116.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrencestatistics.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology - Volume 1, NAACL ?03, pages 71?78,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opinionson the Web.
In Proceedings of the 14th internationalconference on World Wide Web, pages 342?351.
ACM.Inderjeet Mani.
2001.
Automatic summarization.
Natu-ral Language Processing.
John Benjamins PublishingCompany.Ryan McDonald.
2007.
A study of global inferencealgorithms in multi-document summarization.
InProceedings of the 29th European conference on IR re-search, ECIR?07, pages 557?564, Berlin, Heidelberg.Springer-Verlag.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet:: Similarity: measuring therelatedness of concepts.
In Demonstration Papers atHLT-NAACL 2004 on XX, pages 38?41.
Associationfor Computational Linguistics.Jian-Tao Sun, Xuanhui Wang, Dou Shen, Hua-Jun Zeng,and Zheng Chen.
2006.
CWS: a comparativeweb search system.
In Proceedings of the 15thinternational conference on World Wide Web, pages467?476.
ACM.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.Towards an iterative reinforcement approach forsimultaneous document summarization and keywordextraction.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages552?559, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Dingding Wang, Shenghuo Zhu, Tao Li, and YihongGong.
2009.
Comparative document summarizationvia discriminative sentence selection.
In Proceedingof the 18th ACM conference on Information andknowledge management, pages 1963?1966.
ACM.ChengXiang Zhai, Atulya Velivelli, and Bei Yu.
2004.A cross-collection mixture model for comparative textmining.
In Proceedings of the tenth ACM SIGKDDinternational conference on Knowledge discovery anddata mining, pages 743?748.
ACM.653
