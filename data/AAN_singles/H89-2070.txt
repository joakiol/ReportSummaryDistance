PILOT PROJECT ON SPEECH RECOGNITION USINGLAYERED ABDUCTION AND MULTIPLE KNOWLEDGE TYPESJ.
Josephson, M. Beckman, R. Fox, A. Krishnamurthy,T.
Patten, L. Feth, B. ChandrasekaranThe Ohio State University, Columbus, OH 43210Our long-term goal is to test the feasibility of basing a recognition systemon a model of human speech processing.
Key hypotheses are: (1) Computationalmodels for reasoning from incomplete knowledge provide a useful metaphor formany aspects of human speech understanding even at levels where highlyautomatic perceptual processes are at work; (2) A computat ional  model ofhuman cochlear processing must be used as the s ignal -processing front end;(3) Some representation of articulation should mediate between the acousticsand the phonology in order to accommodate contextual var iat ion of varioussorts; (4) The phonological representation must encode the prosodic structureand intonation pattern as wel l  as the phoneme string.
Accordingly ourspecif ic short-term goal is to build a small prototype system that uses alayered-abduction architecture, in which there are many stages of processing,corresponding to different levels of knowledge.
The common information-processing task at each stage is to form a coherent, composite (multi-part)hypothesis that explains the data presented from the preceding levels.
Theinput signal will be the digitized speech processed by Patterson's StabilizedAuditory Image system.
An articulatory representation based on Browman andGoldstein's gestural score wi l l  mediate between the auditory representationand the phonology, and Pierrehumbert  and Beckman's prosodic tree and tonestring will be used for the phonological organization and intonational melody.We are nearing complet ion of an intial two-level  system incorporat ing astratum of acoustic features, and a stratum of phonological  features, for asmall vocabulary of CV monosyllables produced by one speaker.
An abductionmachine, with distinct elements of recognition, classification, and compositehypothesis formation, controls all the processing beyond some rudimentaryinitial signal processing; it even does the formant tracking.
Our next stepis to incorporate the Stabil ized Auditory Image system as the signal-processing front end for the acoustic stratum in our initial two-level system.We have also begun to write gestural scores for a small vocabulary ofmonosyl lables and disyl lables on the basis of art iculatory data that werecorded for our one speaker at the University of Wisconsin's X-ray microbeamfacility.
We will use these scores in building a separate system with threestrata representing articulation, phonology, and a lexicon.
This will providea first test of a three-level machine; it wi l l  incorporate some top-downprocessing, and will provide the first opportunities to test the computationaleff ic iency of the model.
We are using these art iculatory data also todetermine the mapping between auditory features and articulatory gestures inpreparation for merging the two systems into a single four-level system.454
