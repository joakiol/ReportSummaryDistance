Experience with an Easily Computed Metricfor Ranking Alternative ParsesGeorge E. HeidornComputer Sciences DepartmentIBM Thomas J. Watson Research CenterYorktown Heights, New York 10598AbstractThis brief paper, which is itself an extended abstract for aforthcoming paper, describes a metric that can be easily com-puted during either bottom-up or top-down construction of aparse tree for ranking the desirability of alternative parses.
Inits simplest form, the metric tends to prefer  trees in whichconstituents are pushed as far down as possible, but by appro-priate modification of a constant in the formula other behaviorcan be obtained also.
This paper includes an introduction tothe EPISTLE system being developed at IBM Research and adiscussion of the results of using this metric with that system.Introduct ionHeidorn (1976) described a technique for computing anumber for each node during the bottom-up construction of aparse tree, such that a node with a smaller number is to bepreferred to a node with a larger number covering the sameportion of text.
At the time, this scheme was used primarily toselect among competing noun phrases in queries to a programexplanation system.
Although it appeared to work well, it wasnot extensively tested.
Recently, as part of our research onthe EPISTLE system, this idea has been modified and extend-ed to work over entire sentences and to provide for top-downcomputation.
Also, we have done an analysis of 80 sentenceswith multiple parses from our data base to evaluate the per-formance of this metric, and have found that it is producingvery good results.This brief paper, which is actually an extended abstractfor a forthcoming paper, begins with an introduction to theEPISTLE system, to set the stage for the current application ofthis metric.
Then the metrie's computation is described, fol-lowed by a discussion of the results of the 80-sentence analy-sis.
Finally, some comparisons are made to related work byothers.The EPISTLE SystemIn its current form, the EPISTLE system (Miller, Heidornand Jensen 1981) is intended to do critiquing of a writer's useof English in business correspondence, and can do someamount of grammar and style checking.
The central compo-nent of the system is a parser for assigning rammatical struc-tures to input sentences.
This is done with NLP, a LISP-basednatural language processing system which uses augmentedphrase structure grammar ~APSG) rules (Heidorn 1975) tospecify how text is to be converted into a network of nodesconsisting of attribute-value pairs and how such a network canbe converted into text.
The first process, decoding, is done ina bottom-up, parallel processing fashion, and the inverse proc-ess, encoding, is done in a top-down, serial manner.
In thecurrent application the network which is constructed is simplya decorated parse tree, rather than a meaning representation.Because EPISTLE must deal with unrestricted input (bothin terms of vocabulary and syntactic constructions),  we aretrying to see how far we can get initially with almost no se-mantic information.
In particular, our information aboutwords is pretty much limited to parts-of-speech t at come froman on-line version of a standard dictionary of over 100,000entries, and the conditions in our 250 decoding rules are basedprimarily on syntactic cues.
We strive for what we call aunique approximate parse for each sentence, a parse that is notnecessarily semantically accurate (e.g., prepositional phraseattachments are not always done right) but one which is ade-quate for the text critiquing tasks, nevertheless.One of the things we do periodically to test the perform-anee of our parsing component is to run it on a set of 400actual business letters, consisting of almost 2,300 sentenceswhich range in length up to 63 words, averaging 19 words persentence.
In two recent runs of this data base, the followingresults were obtained:No.
of parses June 1981 Dec. 19810 57% 36%1 31% 41%2 6% 11%>2 6% 12%The improvement in performance from June to Decembercan be attributed both to writing additional grammar ules andto relaxing overly restrictive conditions in other rules.
It canbe seen that this not only had the desirable effect of reducingthe percentage of no-parse sentences (from 57% to 36%)  andincreasing the percentage of single-parse sentences (from 31%to 41%) ,  but it also had the undesirable side effect of inerez., ?
,ing the multiple-parse sentences (from 12% to 23%).
Be-cause we expect th!
:; ~;';~.ation to continue as we further in-crease our grammatical coverage, the need for a method ofranking multiple parses in order to select the best one onwhich to base our grammar and style critiques is acutely felt,82The Metric and Its ComputationThe metric can be stated by the following recursive for-mula:Scorephrase = ~ KMod(Sc?reMod+l)Modswhere the lowest score is considered to be the best.
This for-mula says that the score associated with a phrase is equal tothe sum of the scores of the modifying phrases of that phraseadjusted in a particular way, namely that the score of eachmodifier is increased by 1 and then multiplied by a constant Kappropriate for that type of modifier.
A phrase with no modi-fiers, such as an individual word, has a score of 0.
This metricis based on a flat view of syntactic structure which says thateach phrase consists of a head word and zero or more pre- andpost-modifying phrases.
(In this view a sentence is just a bigverb phrase, with modifiers such as subject, objects, adverbs,and subordinate clauses.
)In its simplest form this metric can be considered to benothing more than the numerical realization of Kimbatl's Prin-ciple Number Two (Kimball 1972): "Terminal symbols opti-mally associate to the lowest nonterminal node."
(AlthoughKimball calls this principle right association and il lustrates itwith right-branching examples, it can often apply equally wellto left-branching structures.)
One way to achieve this simplestform is to use a K of 0.1 for all types of modifiers.An example of the application of the metric in this sim-plest form is given in Figure 1.
Two parse trees are shown forthe sentence, "See the man with the telescope," with a scoreattached to each node (other than those that are zero).
Anode marked with an asterisk is the head of its respectivephrase.
In this form of flat parse tree a prepositional phrase isdisplayed as a noun phrase with the preposition as an addition-al premodifier.
As an example of the calculation, the score ofthe PP here is computed as 0.1(0+ 1)+0.1(0+1) ,  because thescores of its modifiers m the ADJ and the PREP m are each0.
Similarly, the score of the NP in the second parse tree iscomputed as 0.1(0+ 1)+0.1(0.2+ 1), where the 0.2 within it isthe score of the PP.It can be seen from the example that in this simplest formthe individual digits of the score after the decimal point tellhow many modifiers appear at each level in the phrase (as longas there are no more than nine modifiers at any level).
Thefarther down in the parse tree a constituent is pushed, thefarther to the right in the final score its contribution will ap-pear.
Hence, a deeper structure will tend to have a smallerscore than a shallower structure, and, therefore, be preferred.In the example, this is the second tree, with a score of 0.122vs.
0.23.
That is not to say that this would be the semanticallycorrect tree for this sentence in all contexts, but only that if achoice cannot be made on any other grounds, this tree is to bepreferred.Applying the metric in its simplest form does not producethe desired result for all grammatical constructions, so thatvalues for K other than 0.1 must be used for some types ofmodifiers.
It basically boils down to a system of rewards andpenalties to make the metric reflect preferences determinedheuristically.
For example, the preference that a potentialauxiliary verb is to be used as an auxiliary rather than as amain verb when both parses are possible can be realized byusing a K of 0, a reward, when picking up an auxiliary verb.Similarly, a K of 2, a penalty, can be used to increase the score(thereby lessening the preference) when attaching an adverbialphrase as a premodifier in a lower level clause (rather than asa postmodifier in a higher level clause).
When semantic infor-mation is available, it can be used to select appropriate valuesfor K, too, such as using 100 for an anomalous combination.Straightforward application of the formula given aboveimplies that the computation of the score can be done in abottom-up fashion, as the modifiers of each phrase are pickedup.
However, it can also be done in a top-down manner afterdoing a little bit of algebra on the formula to expand it andregroup the terms.
In the EPISTLE application it is the latterapproach that is being used.
There is actually a set of tenNLP encoding rules that do the computation in a downwardtraversal of a completed parse tree, determining the appropri-ate constant o use at each node.
The top-down method ofcomputation could be done during top-down parsing of thesort typically used with ATN's,  also.SENT(0 .23)~ .
.
.
.
VERB*I .
.
.
.
NP(0 .1 )ii .
.
.
.
PP (0 .2 )"SEE"ADJ  "THE"NOUN * "MAN"PREP  .
.
.
.
.
.
"WITH"ADJ  "THE"I .
.
.
.
NOUN*  "TELESCOPE"SENT(0 .122)  l - - -  VERB*  .
.
.
.
.
.
"SEE"i - - -  NP(0 .22) i - - -  ADJ  "THE"I - - -  NOUN*  "MAN"i - - -  pp(0 .2 )  I - - -  PREPI - - -  ADJ  .
.
.
.
.
.i - - -  NOUN*"WITH""THE""TELESCOPE"Figure 1.
Two alternative parses with their scores.83Performance of the MetricTo test the performance of the metric in our EPISTLEapplication, the parse trees of 80 multiple-parse ntences wereanalyzed to determine if the metric favored what we consid-ered to he the best tree for our purposes.
A raw calculationsaid it was right in 65% of the cases.
However, further analy-sis of those cases where it was wrong showed that in half ofthem the parse that it favored was one which will not even beproduced when we further refine our grammar ules.
If weeliminate these from consideration, our success rate increasesto 80%.
Out of the remaining "failures," more than half arecases where semantic information is required to make thecorrect choice, and our system simply does not yet haveenough such information to deal with these.
The others, about7%, will require further tuning of the constant K in the for-mula.
(In fact, they all seem to involve VP conjunction, forwhich the metric has not been tuned at all yet.
)The analysis just described was based on multiple parsesof order 2 through 6.
Another analysis was done separately onthe double parses (i.e.
order 2).
The results were similar, butwith an adjusted success rate of 85%, and with almost all ofthe remainder due to the need for more semantic information.It is also of interest to note that significant right-branching occurred in about 75% of the eases for which themetric selected the best parse.
Most of these were situationsin which the grammar ules would allow a constituent to beattached at more than one level, but simply pushing it down tothe lowest possible level with the metric turned out to producethe best parse.Related ResearchThere has not been much in the literature about usingnumerical scores to rank alternative analyses of segments oftext.
One notable exception to this is the work at SRI (e.g.,Paxton 1975 and Robinson 1975, 1980), where factorstatements may be attached to an APSG rule to aid in thecalculation of a score for a phrase formed by applying the rule.The score of a phrase is intended to express the likelihood thatthe phrase is a correct interpretation of the input.
Thesescores apparently can be integers in the range 0 to 100 orsymbols such as GOOD or POOR.
This method of scoringphrases provides more flexibility than the metric of this paper,but also puts more of a burden on the grammar writer.Another place in which scoring played an important role isthe syntactic component of the BBN SPEECHLIS system(Bates 1976), where ,an integer score is assigned to eachconfiguration during the processing of a sentence to reflect thelikelihood that the path which terminates on that configurationis correct.
The grammar writer must assign weights to each areof the ATN grammar, but the rest of the computation appearsto be done by the system, utilizing such information as thenumber of words in a constituent.
Although this scoringmechanism worked very well for its intended purpose, it maynot be more generally applicable.A very specialized scoring scheme was used in theJIMMY3 system (Maxwell and Tuggle 1977), where eachparse network is given an integer score calculated by rewardingthe finding of the actor, object, modifiers, and prepositionalphrases and punishing the ignoring of words and terms.
Final-ly, there is Wilks' counting of dependencies to find the analysiswith the greatest semantic density in his Preference Semanticswork (eg., Wilks 1975).
Neither of these purports to proposescoring methods that are more generally applicable, either.AcknowledgementsI would like to thank.Karen Jensen, Martin Chodorow andLance Miller for the help that they have given me in the devel-opment and testing of this parsing metric, and John Sowa forhis comments on an earlier draft of this paper.ReferencesBates, M. 1976.
"Syntax in Automatic Speech Understanding"Am.
J. Comp.
Ling.
Microfiche 45.Heidorn, G.E.
1975.
"Augmented Phrase Structure Gram-mars" Theoretical Issues in Natural Language Processing,B.L.
Webber and R.C.
Schank (Eds.
), Assoc.
for Comp.Ling., June 1975, 1-5.Heidorn, G.E.
1976.
"An Easily Computed Metric for Rank-ing Alternative Parses" Presented at the Fourteenth AnnualMeeting of  the Assoc.
for Comp.
Ling., San Francisco, Octo-ber 1976.Kimball, J.
1972.
"Seven Principles of Surface Structure Pars-ing in Natural Language" Cognition 2, 1, 15-47.Maxwell, B.D.
and F.D.
Tuggle 1977.
"Toward a 'Natural'Language Question-Answering Facility" Am.
J. Comp.
Ling.Microfiche 61.Miller, L.A., G.E.
Heidorn and K. Jensen 1981.
"Text-Critiquing with the EPISTLE System: An Author's Aid toBetter Syntax" AFIPS - Conference Proceedings, Vol.
50,May 1981, 649-655.Paxton, W.H.
1975.
"The Definition System" in Speech Un-derstanding Research, SRI Annual Technical Report, June1975, 20-25.Robinson, J.J. 1975.
"A Tuneable Performance Grammar"Am.
J. Comp.
Ling., Microfiche 34, 19-33.Robinson, J.J. 1980.
"DIAGRAM: A Grammar for Dia-logues" SRI  Technical Note 205, Feb. 1980.Wilks, Y.
1975.
"An Intelligent Analyzer and Understander ofEnglish" Comm.
ACM 18, 5 (May 1975), 264-274.84
