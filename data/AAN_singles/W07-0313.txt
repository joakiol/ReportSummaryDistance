Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 89?96,NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsDifferent measurements metrics to evaluate a chatbot systemBayan Abu ShawarIT departmentArab Open University[add]b_shawar@arabou-jo.edu.joEric AtwellSchool of ComputingUniversity of LeedsLS2 9JT, Leeds-UKeric@comp .leeds.ac.ukAbstractA chatbot is a software system, which caninteract or ?chat?
with a human user innatural language such as English.
For theannual Loebner Prize contest, rival chat-bots have been assessed in terms of abilityto fool a judge in a restricted chat session.We are investigating methods to train andadapt a chatbot to a specific user?s lan-guage use or application, via a user-supplied training corpus.
We advocateopen-ended trials by real users, such as anexample Afrikaans chatbot for Afrikaans-speaking researchers and students inSouth Africa.
This is evaluated in terms of?glass box?
dialogue efficiency metrics,and ?black box?
dialogue quality metricsand user satisfaction feedback.
The otherexamples presented in this paper are theQur'an and the FAQchat prototypes.
Ourgeneral conclusion is that evaluationshould be adapted to the application andto user needs.1 Introduction?Before there were computers, we could distin-guish persons from non-persons on the basis of anability to participate in conversations.
But now, wehave hybrids operating between person and nonpersons with whom we can talk in ordinary lan-guage.?
(Colby 1999a).
Human machine conversa-tion as a technology integrates different areaswhere the core is the language, and the computa-tional methodologies facilitate communication be-tween users and computers using natural language.A related term to machine conversation is thechatbot, a conversational agent that interacts withusers turn by turn using natural language.
Differentchatbots or human-computer dialogue systemshave been developed using text communicationsuch as Eliza (Weizenbaum 1966), PARRY (Colby1999b), CONVERSE (Batacharia etc 1999),ALICE1.
Chatbots have been used in different do-mains such as: customer service, education, website help, and for fun.Different mechanisms are used to evaluateSpoken Dialogue Systems (SLDs), ranging fromglass box evaluation that evaluates individualcomponents, to black box evaluation that evaluatesthe system as a whole McTear (2002).
For exam-ple, glass box evaluation was applied on the(Hirschman 1995) ARPA Spoken Language sys-tem, and it shows that the error rate for sentenceunderstanding was much lower than that for sen-tence recognition.
On the other hand black boxevaluation evaluates the system as a whole basedon user satisfaction and acceptance.
The black boxapproach evaluates the performance of the systemin terms of achieving its task, the cost of achievingthe task in terms of time taken and number ofturns, and measures the quality of the interaction,normally summarised by the term ?user satisfac-tion?, which indicates whether the user ?
gets theinformation s/he wants, is s/he comfortable withthe system, and gets the information within accept-able elapsed time, etc.?
(Maier et al1996).The Loebner prize2 competition has been usedto evaluate machine conversation chatbots.
TheLoebner Prize is a Turing test, which evaluates theability of the machine to fool people that they aretalking to human.
In essence, judges are allowed ashort chat (10 to 15 minutes) with each chatbot,and asked to rank them in terms of ?naturalness?.ALICE (Abu Shawar and Atwell 2003) is theArtificial Linguistic Internet Computer Entity, first1 http://www.alicebot.org/2 http://www.loebner.net/Prizef/loebner-prize.html89implemented by Wallace in 1995.
ALICE knowl-edge about English conversation patterns is storedin AIML files.
AIML, or Artificial IntelligenceMark-up Language, is a derivative of ExtensibleMark-up Language (XML).
It was developed byWallace and the Alicebot free software communityduring 1995-2000 to enable people to input dia-logue pattern knowledge into chatbots based on theA.L.I.C.E.
open-source software technology.In this paper we present other methods toevaluate the chatbot systems.
ALICE chtabot sys-tem was used for this purpose, where a Java pro-gram has been developed to read from a corpusand convert the text to the AIML format.
The Cor-pus of Spoken Afrikaans (Korpus Gesproke Afri-kaans, KGA), the corpus of the holy book of Islam(Qur?an), and the FAQ of the School of Computingat University of Leeds3 were used to produce twoKGA prototype, the Qur?an prototype and theFAQchat one consequently.Section 2 presents Loebner Prize contest, sec-tion 3 illustrates the ALICE/AIMLE architecture.The evaluation techniques of the KGA prototype,the Qur?an prototype, and the FAQchat prototypeare discussed in sections 4, 5, and 6 consequently.The conclusion is presented in section 7.2 The Loebner Prize CompetitionThe story began with the ?imitation game?
whichwas presented in Alan Turing?s paper ?Can Ma-chine think??
(Turing 1950).
The imitation gamehas a human observer who tries to guess the sex oftwo players, one of which is a man and the other isa woman, but while screened from being able totell which is which by voice, or appearance.
Turingsuggested putting a machine in the place of one ofthe humans and essentially playing the same game.If the observer can not tell which is the machineand which is the human, this can be taken as strongevidence that the machine can think.Turing?s proposal provided the inspiration forthe Loebner Prize competition, which was an at-tempt to implement the Turing test.
The first con-test organized by Dr. Robert Epstein was held on1991, in Boston?s Computer Museum.
In this in-carnation the test was known as the Loebner con-test, as Dr. Hugh Loebner pledged a $100,000grand prize for the first computer program to pass3 http://www.comp.leeds.ac.ukthe test.
At the beginning it was decided to limitthe topic, in order to limit the amount of languagethe contestant programs must be able to cope with,and to limit the tenor.
Ten agents were used, 6were computer programs.
Ten judges would con-verse with the agents for fifteen minutes and rankthe terminals in order from the apparently leasthuman to most human.
The computer with thehighest median rank wins that year?s prize.
JosephWeintraub won the first, second and third LoebnerPrize in 1991, 1992, and 1993 for his chatbots, PCTherapist, PC Professor, which discusses men ver-sus women, and PC Politician, which discussesLiberals versus Conservatives.
In 1994 ThomasWhalen (Whalen 2003) won the prize for his pro-gram TIPS, which provides information on a par-ticular topic.
TIPS provides ways to store,organize, and search the important parts of sen-tences collected and analysed during system tests.However there are sceptics who doubt the ef-fectiveness of the Turing Test and/or the LoebnerCompetition.
Block, who thought that ?the Turingtest is a sorely inadequate test of intelligence be-cause it relies solely on the ability to fool people?
;and Shieber (1994), who argued that intelligence isnot determinable simply by surface behavior.Shieber claimed the reason that Turing chose natu-ral language as the behavioral definition of humanintelligence is ?exactly its open-ended, free-wheeling nature?, which was lost when the topicwas restricted during the Loebner Prize.
Epstein(1992) admitted that they have trouble with thetopic restriction, and they agreed ?every fifth yearor so ?
we would hold an open-ended test - onewith no topic restriction.?
They decided that thewinner of a restricted test would receive a smallcash prize while the one who wins the unrestrictedtest would receive the full $100,000.Loebner in his responses to these arguments be-lieved that unrestricted test is simpler, less expen-sive and the best way to conduct the Turing Test.Loebner presented three goals when constructingthe Loebner Prize (Loebner 1994):?
?No one was doing anything about theTuring Test, not AI.?
The initial LoebnerPrize contest was the first time that theTuring Test had ever been formally tried.?
Increasing the public understanding of AIis a laudable goal of Loebner Prize.
?I be-lieve that this contest will advance AI and90serve as a tool to measure the state of theart.??
Performing a social experiment.The first open-ended implementation of theTuring Test was applied in the 1995 contest, andthe prize was granted to Weintraub for the fourthtime.
For more details to see other winners overyears are found in the Loebner Webpage4.In this paper, we advocate alternative evalua-tion methods, more appropriate to practical infor-mation systems applications.
We have investigatedmethods to train and adapt ALICE to a specificuser?s language use or application, via a user-supplied training corpus.
Our evaluation takes ac-count of open-ended trials by real users, rather thancontrolled 10-minute trials.3 The ALICE/AIML chatbot architectureAIML consists of data objects called AIML ob-jects, which are made up of units called topics andcategories.
The topic is an optional top-level ele-ment; it has a name attribute and a set of categoriesrelated to that topic.
Categories are the basic unitsof knowledge in AIML.
Each category is a rule formatching an input and converting to an output, andconsists of a pattern, which matches against theuser input, and a template, which is used in gener-ating the Alice chatbot answer.
The format struc-ture of AIML is shown in figure 1.< aiml version=?1.0?
>< topic name=?
the topic?
><category><pattern>PATTERN</pattern><that>THAT</that><template>Template</template></category>....</topic></aiml>The <that> tag is optional and means that the cur-rent pattern depends on a  previous bot output.Figure 1.
AIML format4 http://www.loebner.net/Prizef/loebner-prize.htmlThe AIML pattern is simple, consisting only ofwords, spaces, and the wildcard symbols _ and *.The words may consist of letters and numerals, butno other characters.
Words are separated by a sin-gle space, and the wildcard characters function likewords.
The pattern language is case invariant.
Theidea of the pattern matching technique is based onfinding the best, longest, pattern match.
Threetypes of AIML categories are used: atomic cate-gory, are those with patterns that do not have wild-card symbols, _ and   *; default categories arethose with patterns having wildcard symbols * or_.
The wildcard symbols match any input but candiffer in their alphabetical order.
For example,given input ?hello robot?, if ALICE does not find acategory with exact matching atomic pattern, thenit will try to find a category with a default pattern;The third type, recursive categories are those withtemplates having <srai> and <sr> tags, which referto simply recursive artificial intelligence and sym-bolic reduction.
Recursive categories have manyapplications: symbolic reduction that reduces com-plex grammatical forms to simpler ones; divideand conquer that splits an input into two or moresubparts, and combines the responses to each; anddealing with synonyms by mapping different waysof saying the same thing to the same reply.The knowledge bases of almost all chatbots areedited manually which restricts users to specificlanguages and domains.
We developed a Java pro-gram to read a text from a machine readable text(corpus) and convert it to AIML format.
The chat-bot-training-program was built to be general, thegenerality in this respect implies, no restrictions onspecific language, domain, or structure.
Differentlanguages were tested: English, Arabic, Afrikaans,French, and Spanish.
We also trained with a rangeof different corpus genres and structures, includ-ing: dialogue, monologue, and structured textfound in the Qur?an, and FAQ websites.The chatbot-training-program is composed offour phases as follows:?
Reading module which reads the dialoguetext from the basic corpus and inserts itinto a list.?
Text reprocessing module, where all cor-pus and linguistic annotations such asoverlapping, fillers and others are filtered.?
Converter module, where the pre-processed text is passed to the converter toconsider the first turn as a pattern and the91second as a template.
All punctuation isremoved from the patterns, and the pat-terns are transformed to upper case.?
Producing the AIML files by copying thegenerated categories from the list to theAIML file.An example of a sequence of two utter-ances from an English spoken corpus is:<u who=F72PS002><s n="32"><w ITJ>Hello<c PUN>.</u><u who=PS000><s n="33"><w ITJ>Hello <w NP0>Donald<cPUN>.</u>After the reading and the text processingphase, the text becomes:F72PS002: HelloPS000: Hello DonaldThe corresponding AIML atomic category thatis generated from the converter modules looks like:<category><pattern>HELLO</pattern><template>Hello Donald</template></category>As a result different prototypes were developed,in each prototype, different machine-learning tech-niques were used and a new chatbot was tested.The machine learning techniques ranged from aprimitive simple technique like single word match-ing to more complicated ones like matching theleast frequent words.
Building atomic categoriesand comparing the input with all atomic patterns tofind a match is an instance based learning tech-nique.
However, the learning approach does notstop at this level, but it improved the matchingprocess by using the most significant words (leastfrequent word).
This increases the ability of find-ing a nearest match by extending the knowledgebase which is used during the matching process.Three prototypes will be discussed in this paper aslisted below:?
The KGA prototype that is trained by acorpus of spoken Afrikaans.
In this proto-type two learning approaches wereadopted.
The first word and the most sig-nificant word (least frequent word) ap-proach;?
The Qur?an prototype that is trained by theholy book of Islam (Qur?an): where in ad-dition to the first word approach, two sig-nificant word approaches (least frequentwords) were used, and the system wasadapted to deal with the Arabic languageand the non-conversational nature ofQur?an as shown in section 5;?
The FAQchat prototype that is used in theFAQ of the School of Computing at Uni-versity of Leeds.
The same learning tech-niques were used, where the questionrepresents the pattern and the answer rep-resents the template.
Instead of chatting forjust 10 minutes as suggested by the Loeb-ner Prize, we advocate alternative evalua-tion methods more attuned to andappropriate to practical information sys-tems applications.
Our evaluation takes ac-count of open-ended trials by real users,rather than artificial 10-minute trials as il-lustrated in the following sections.The aim of the different evaluations method-ologies is as follows:?
Evaluate the success of the learning tech-niques in giving answers, based on dia-logue efficiency, quality and users?satisfaction applied on the KGA.?
Evaluate the ability to use the chatbot as atool to access an information source, and auseful application for this, which was ap-plied on the Qur'an corpus.?
Evaluate the ability of using the chatbot asan information retrieval system by com-paring it with a search engine, which wasapplied on FAQchat.4 Evaluation of the KGA prototypeWe developed two versions of the ALICE thatspeaks Afrikaans language, Afrikaana that speaksonly Afrikaans and AVRA that speaks English andAfrikaans; this was inspired by our observationthat the Korpus Gesproke Afrikaans actually in-cludes some English, as Afrikaans speakers aregenerally bilingual and ?code-switch?
comfortably.We mounted prototypes of the chatbots on web-sites using Pandorabot service5, and encouraged5 http://www.pandorabots.com/pandora92open-ended testing and feedback from remote us-ers in South Africa; this allowed us to refine thesystem more effectively.We adopted three evaluation metrics:?
Dialogue efficiency in terms of matchingtype.?
Dialogue quality metrics based on re-sponse type.?
Users' satisfaction assessment based on anopen-ended request for feedback.4.1 Dialogue efficiency metricWe measured the efficiency of 4 sample dia-logues in terms of atomic match, first word match,most significant match, and no match.
We wantedto measure the efficiency of the adopted learningmechanisms to see if they increase the ability tofind answers to general user input as shown in ta-ble 1.Matching Type D1 D2 D3 D4Atomic 1 3 6 3First word 9 15 23 4Most significant 13 2 19 9No match 0 1 3 1Number of turns 23 21 51 17Table 1.
Response type frequencyThe frequency of each type in each dialoguegenerated between the user and the Afrikaanschatbot was calculated; in Figure 2, these absolutefrequencies are normalised to relative probabilities.No significant test was applied, this approach toevaluation via dialogue efficiency metrics illus-trates that the first word and the most significantapproach increase the ability to generate answersto users and let the conversation continue.Figure 2.
Dialogue efficiency: Response TypeRelative Frequencies4.2 Dialogue quality metricIn order to measure the quality of each re-sponse, we wanted to classify responses accordingto an independent human evaluation of ?reason-ableness?
: reasonable reply, weird but understand-able, or nonsensical reply.
We gave the transcriptto an Afrikaans-speaking teacher and asked her tomark each response according to these classes.
Thenumber of turns in each dialogue and the frequen-cies of each response type were estimated.
Figure 3shows the frequencies normalised to relative prob-abilities of each of the three categories for eachsample dialogue.
For this evaluator, it seems that?nonsensical?
responses are more likely than rea-sonable or understandable but weird answers.4.3 Users' satisfactionThe first prototypes were based only on literalpattern matching against corpus utterances: we hadnot implemented the first word approach and least-frequent word approach to add ?wildcard?
defaultcategories.
Our Afrikaans-speaking evaluatorsfound these first prototypes disappointing and frus-trating: it turned out that few of their attempts atconversation found exact matches in the trainingcorpus, so Afrikaana replied with a default ?ja?most of the time.
However, expanding the AIMLpattern matching using the first-word and least-frequent-word approaches yielded more favorablefeedback.
Our evaluators found the conversationsless repetitive and more interesting.
We measureuser satisfaction based on this kind of informaluser feed back.Response Types0.000.200.400.600.801.00Dialogue1Dialogue2Dialogue3Dialogue4Repetion(%)reasonableWeirdNon sensicalMatching Types00.20.40.60.8Dialogu1Dialogue 2Dialogue 3Dialogue 4repetition(%) AtomicFirst wordMostsignificantMatchnothingFigure 3.
The quality of the Dialogue: Responsetype relative probabilities935 Evaluation of the Qur'an prototypeIn this prototype a parallel corpus of Eng-lish/Arabic of the holy book of Islam was used, theaim of the Qur?an prototype is to explore the prob-lem of using the Arabic language and of using atext which is not conversational in its nature likethe Qur?an.
The Qur?an is composed of 114 soora(chapters), and each soora is composed of differentnumber of verses.
The same learning technique asthe KGA prototype were applied, where in thiscase if an input was a whole verse, the responsewill be the next verse of the same soora; or if aninput was a question or a statement, the output willbe all verses which seems appropriate based on thesignificant word.
To measure the quality of theanswers of the Qur?an chatbot version, the follow-ing approach was applied:1.
Random sentences from Islamic sites wereselected and used as inputs of the Eng-lish/Arabic version of the Qur?an.2.
The resulting transcripts which have 67turns were given to 5 Muslims and 6 non-Muslims students, who were asked to labeleach turn in terms of:?
Related (R), in case the answer was correctand in the same topic as the input.?
Partially related (PR), in case the answerwas not correct, but in the same topic.?
Not related (NR), in case the answer wasnot correct and in a different topic.Proportions of each label and each class of us-ers (Muslims and non-Muslims) were calculated asthe total number over number of users times num-ber of turns.
Four out of the 67 turns returned noanswers, therefore actually 63 turns were used aspresented in figure 4.In the transcripts used, more than half of the re-sults were not related to their inputs.
A small dif-ference can be noticed between Muslims and non-Muslims proportions.
Approximately one half ofanswers in the sample were not related from non-Muslims?
point of view, whereas this figure is 58%from the Muslims?
perspective.
Explanation forthis includes:?
The different interpretation of the answers.The Qur?an uses traditional Arabic lan-guage, which is sometimes difficult to un-derstand without knowing the meaning ofsome words, and the historical story be-hind each verse.?
The English translation of the Qur?an isnot enough to judge if the verse is relatedor not, especially given that non-Muslimsdo not have the background knowledge ofthe Qur?an.Using chatting to access the Qur?an looks likethe use of a standard Qur?an search tool.
In fact itis totally different; a searching tool usuallymatches words not statements.
For example, if theinput is: ?How shall I pray??
using chatting: therobot will give you all ayyas where the word?pray?
is found because it is the most significantword.
However, using a search tool6 will not giveyou any match.
If the input was just the word?pray?, using chatting will give you the same an-swer as the previous, and the searching tool willprovide all ayyas that have ?pray?
as a string orsubstring, so words such as: ?praying, prayed, etc.
?will match.Another important difference is that in thesearch tool there is a link between any word andthe document it is in, but in the chatting systemthere is a link just for the most significant words,so if it happened that the input statement involves asignificant word(s), a match will be found, other-wise the chatbot answer will be: ?I have no answerfor that?.Answer types0%10%20%30%40%50%60%70%Related PartialyRelatedNot relatedAnswersProportionMuslimsNon MuslimsOverallFigure4.
The Qur?an proportion of each answertype denoted by users6 Evaluation of the FAQchat prototypeTo evaluate FAQchat, an interface was built,which has a box to accept the user input, and a but-ton to send this to the system.
The outcomes ap-6 http://www.islamicity.com/QuranSearch/94pear in two columns: one holds the FAQchat an-swers, and the other holds the Google answers af-ter filtering Google to the FAQ database only.Google allows search to be restricted to a givenURL, but this still yields all matches from thewhole SoC website (http://www.comp.leeds.ac.uk)so a Perl script was required to exclude matchesnot from the FAQ sub-pages.An evaluation sheet was prepared which con-tains 15 information-seeking tasks or questions ona range of different topics related to the FAQ data-base.
The tasks were suggested by a range of usersincluding SoC staff and research students to coverthe three possibilities where the FAQchat couldfind a direct answer, links to more than one possi-ble answer, and where the FAQchat could not findany answer.
In order not to restrict users to thesetasks, and not to be biased to specific topics, theevaluation sheet included spaces for users to try 5additional tasks or questions of their own choosing.Users were free to decide exactly what input-stringto give to FAQchat to find an answer: they werenot required to type questions verbatim; users werefree to try more than once: if no appropriate an-swer was found; users could reformulate the query.The evaluation sheet was distributed among 21members of the staff and students.
Users wereasked to try using the system, and state whetherthey were able to find answers using the FAQchatresponses, or using the Google responses; andwhich of the two they preferred and why.Twenty-one users tried the system; nine mem-bers of the staff and the rest were postgraduates.The analysis was tackled in two directions: thepreference and the number of matches found perquestion and per user.Which tool do you prefer?0%10%20%30%40%50%60%FAQchat GoogleToolAveargepercentagenumber StaffStudentTotal6.1 Number of matches per questionThe number of evaluators who managed to findanswers by FAQchat and Google was counted, foreach question.Results in table 2 shows that 68% overall of oursample of users managed to find answers using theFAQchat while 46% found it by Google.
Sincethere is no specific format to ask the question,there are cases where some users could find an-swers while others could not.
The success in find-ing answers is based on the way the questions werepresented to FAQchat.Users/ToolMean of users find-ing answersProportion of find-ing answersFAQchat Google FAQchat GoogleStaff 5.53 3.87 61% 43%Student 8.8 5.87 73% 49%Overall 14.3 9.73 68% 46%Table 2: Proportion of users finding answersOf the overall sample, the staff outcome showsthat 61% were able to find answers by FAQchatwhere 73% of students managed to do so; studentswere more successful than staff.6.2 The preferred tool per each questionFor each question, users were asked to statewhich tool they preferred to use to find the answer.The proportion of users who preferred each toolwas calculated.
Results in figure 5 shows that 51%of the staff, 41% of the students, and 47% overallpreferred using FAQchat against 11% who pre-ferred the Google.Figure5.
Proportion of preferred tool6.3 Number of matches and preference foundper userThe number of answers each user had foundwas counted.
The proportions found were thesame.
The evaluation sheet ended with an opensection inviting general feedback.
The following isa summary of the feedback we obtained:?
Both staff and students preferred using theFAQchat for two main reasons:1.
The ability to give direct answers some-times while Google only gives links.2.
The number of links returned by theFAQchat is less than those returned byGoogle for some questions, which savestime browsing/searching.95?
Users who preferred Google justified theirpreference for two reasons:1.
Prior familiarity with using Google.2.
FAQchat seemed harder to steer with care-fully chosen keywords, but more often didwell on the first try.
This happens becauseFAQchat gives answers if the keywordmatches a significant word.
The same willoccur if you reformulate the question andthe FAQchat matches the same word.However Google may give different an-swers in this case.To test reliability of these results, the t=Testwere applied, the outcomes ensure the previousresults.7 ConclusionThe Loebner Prize Competition has been usedto evaluate the ability of chatbots to fool peoplethat they are speaking to humans.
Comparing thedialogues generated from ALICE, which won theLoebner Prize with real human dialogues, showsthat ALICE tries to use explicit dialogue-act lin-guistic expressions more than usual to re enforcethe impression that users are speaking to human.Our general conclusion is that we should NOTadopt an evaluation methodology just because astandard has been established, such as the LoebnerPrize evaluation methodology adopted by mostchatbot developers.
Instead, evaluation should beadapted to the application and to user needs.
If thechatbot is meant to be adapted to provide a specificservice for users, then the best evaluation is basedon whether it achieves that service or taskReferencesAbu Shawar B and Atwell E. 2003.
Using dialoguecorpora to retrain a chatbot system.
In Proceedings ofthe Corpus Linguistics 2003 conference, LancasterUniversity, UK, pp681-690.Batacharia, B., Levy, D., Catizone R., Krotov A. andWilks, Y.
1999.
CONVERSE: a conversational com-panion.
In Wilks, Y.
(ed.
), Machine Conversations.Kluwer, Boston/Drdrecht/London, pp.
205-215.Colby, K. 1999a.
Comments on human-computer con-versation.
In Wilks, Y.
(ed.
), Machine Conversations.Kluwer, Boston/Drdrecht/London, pp.
5-8.Colby, K. 1999b.
Human-computer conversation in acognitive therapy program.
In Wilks, Y.
(ed.
), Ma-chine Conversations.
Kluwer, Bos-ton/Drdrecht/London, pp.
9-19.Epstein R. 1992.
Can Machines Think?.
AI magazine,Vol 13, No.
2, pp80-95Garner R. 1994.
The idea of RED, [Online],http://www.alma.gq.nu/docs/ideafred_garner.htmHirschman L. 1995.
The Roles of language processingin a spoken language interface.
In Voice Communi-cation Between Humans and Machines, D. Roe and J.Wilpon (Eds), National Academy Press Washinton,DC, pp217-237.Hutchens, J.
1996.
How to pass the Turing test bycheating.
[Onlin], http://ciips.ee.uwa.edu.au/Papers/,1996Hutchens, T., Alder, M. 1998.
Introducing MegaHAL.
[Online],http://cnts.uia.ac.be/conll98/pdf/271274hu.pdfLoebner H. 1994.
In Response to lessons from a re-stricted Turing Test.
[Online],http://www.loebner.net/Prizef/In-response.htmlMaier E, Mast M, and LuperFoy S. 1996.
Overview.In Elisabeth Maier, Marion Mast, and Susan Luper-Foy (Eds), Dialogue Processing in Spoken LanguageSystems, , Springer, Berlin, pp1-13.McTear M. 2002.
Spoken dialogue technology: ena-bling the conversational user interface.
ACM Com-puting Surveys.
Vol.
34, No.
1, pp.
90-169.Shieber S. 1994.
Lessons from a Restricted TuringTest.
Communications of the Association for Com-puting Machinery, Vol 37, No.
6, pp70-78Turing A.
1950.
Computing Machinery and intelli-gence.
Mind 59, 236, 433-460.Weizenbaum, J.
1966.
ELIZA-A computer programfor the study of natural language communication be-tween man and machine.
Communications of theACM.
Vol.
10, No.
8, pp.
36-45.Whalen T.  2003.
My experience with 1994 Loebnercompetition, [Online],http://hps.elte.hu/~gk/Loebner/story94.htm96
