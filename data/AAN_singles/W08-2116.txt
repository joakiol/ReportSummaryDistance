CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 119?126Manchester, August 2008Easy as ABC?
Facilitating Pictorial Communicationvia Semantically Enhanced LayoutAndrew B. Goldberg, Xiaojin Zhu, Charles R. Dyer, Mohamed Eldawy, Lijie HengDepartment of Computer SciencesUniversity of Wisconsin, Madison, WI 53706, USA{goldberg, jerryzhu, dyer, eldawy, ljheng}@cs.wisc.eduAbstractPictorial communication systems convertnatural language text into pictures to as-sist people with limited literacy.
We definea novel and challenging problem: picturelayout optimization.
Given an input sen-tence, we seek the optimal way to lay outword icons such that the resulting picturebest conveys the meaning of the input sen-tence.
To this end, we propose a familyof intuitive ?ABC?
layouts, which organizeicons in three groups.
We formalize layoutoptimization as a sequence labeling prob-lem, employing conditional random fieldsas our machine learning method.
Enabledby novel applications of semantic role la-beling and syntactic parsing, our trainedmodel makes layout predictions that agreewell with human annotators.
In addition,we conduct a user study to compare ourABC layout versus the standard linear lay-out.
The study shows that our semanticallyenhanced layout is preferred by non-nativespeakers, suggesting it has the potential tobe useful for people with other forms oflimited literacy, too.1 IntroductionA picture is worth a thousand words?especiallywhen you are someone with communicative dis-orders, a foreign language speaker, or a youngchild.
Pictorial communication systems aim to au-tomatically convert general natural language textinto meaningful pictures.
A perfect pictorialc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.communication system can turn signs and opera-tion instructions into easy-to-understand graphicalforms; combined with optical character recogni-tion input, a personal assistant device could createsuch visual translations on-the-fly without the helpof a caretaker.
Pictorial communication may alsofacilitate literacy development and rapid browsingof documents through pictorial summaries.Pictorial communication research is in its in-fancy with a spectrum of experimental systems,which we review in Section 2.
At one end ofthe spectrum, some systems render highly realis-tic 3D scenes but require specific scene-descriptivelanguage.
At the other end, some systems per-form dictionary-based iconic transliteration (turn-ing words into icons1 one by one) on arbitrary textbut the pictures can be hard to understand.
We areinterested in using pictorial communication as anassistive communication tool.
Thus, our systemneeds to be able to handle general text yet produceeasy-to-understand pictures, which is in the middleof the spectrum.
To this end, our system adoptsa ?collage?
approach (Zhu et al, 2007).
Given apiece of text (e.g., a sentence), it first identifies im-portant and easy-to-depict words (or phrases) withnatural language processing (NLP) techniques.
Itthen finds one good icon per word, either from amanually created picture-dictionary, or via imageanalysis on image search results.
Finally, it laysout the icons to create the picture.
Each step in-volves several interesting research problems.This paper focuses exclusively on the picturelayout component and addresses the followingquestion: Can we use machine learning and NLPtechniques to learn a good picture layout that im-1In this paper, an icon refers to a small thumbnail imagecorresponding to a word or phrase.
A picture refers to theoverall large image corresponding to the whole text.119proves picture comprehension for our target audi-ences of limited literacy?
We first propose a sim-ple yet novel picture layout scheme called ?ABC.
?Next, we design a Conditional Random Field-based semantic tagger for predicting the ABC lay-out.
Finally, we conduct a user study contrastingour ABC layout to the linear layout used in iconictransliteration.
The main contribution of this paperis to introduce the novel task of layout prediction,learned using linguistic features including Prop-Bank role labels, part-of-speech tags, and lexicalfeatures.2 Prior Pictorial Communication WorkAt one extreme, there has been significant priorwork on ?text-to-scene?
type systems, which wereoften intended to aid graphic designers in placingobjects in a 3D environment.
Example systems in-clude NALIG (Adorni et al, 1983), SPRINT (Ya-mada et al, 1992), Put (Clay and Wilhelms,1996), and others (Brown and Chandrasekaran,1981).
Perhaps the best known system of this type,WordsEye (Coyne and Sproat, 2001), uses a largemanually tagged collection of 3D polyhedral mod-els to create photo-realistic scenes.
Similarly, Car-Sim (Johansson et al, 2005) can create animatedscenes, but operates exclusively in the limited do-main of reconstructing road accidents from trafficreports.
These systems cater to detailed descriptivetext with visual and spatial elements.
They are notintended as assistive tools to communicate generaltext, which is our goal.Several systems (Zhu et al, 2007; Mihalcea andLeong, 2006; Joshi et al, 2006) attempt to bal-ance language coverage versus picture sophistica-tion.
They perform some form of keyword selec-tion, and select corresponding icons automaticallyfrom a 2D image database.
The result is a pictorialsummary representing the main idea of the origi-nal text, but precisely determining the original textby looking at the picture can be difficult.At the other extreme, augmentative and alterna-tive communication software allows users to in-put arbitrary text.
The words, and sometimescommon phrases, are semi-automatically translit-erated into icons, and displayed in sequential or-der.
Users must learn special icons, which corre-spond to function words, before the resulting pic-tures can be fully understood.
Examples includeSymWriter (Widgit Software, 2007) and Blissym-bols (Hehner, 1980).Other than explicit scene-descriptive languages,pictorial communication systems have not suffi-ciently addressed the issue of picture layout forgeneral text.
We believe a good layout can bettercommunicate the text a picture is trying to convey.The present work studies the use of a semanticallyinspired layout to enhance pictorial communica-tion.
For simplicity, we restrict our attention to thelayout of a single sentence.
We anticipate the useof text simplification (Chandrasekar et al, 1996;Vickrey and Koller, 2008) to convert complex textinto a set of appropriate inputs for our system.3 The ABC LayoutA good picture layout scheme must be intuitive tohumans and easy to generate by computers.
Todesign such a layout, we conducted a pilot study.Five human annotators produced free-hand pic-tures of many sentences.
Analyzing these pictures,we found a large amount of agreement in the useof arrows to mark actions and to provide structureto what would otherwise be a jumble of icons.Motivated by the pilot study, we propose a sim-ple layout scheme called ABC.
It features threepositions, referred to as A, B, and C. In addition,an arrow points from A through B to C (Figure 1).These positions are meant to denote certain seman-tic roles: roughly speaking, A denotes ?who,?
Bdenotes ?what action,?
and C denotes ?to whom,for what.?
Each position can contain any numberof icons, each representing a word or phrase in thetext.
Words that do not play a significant role inthe text will be omitted from the ABC layout.There are two main advantages of the ABC lay-out:1.
The ABC positioning of icons allows users toinfer the semantic role of the corresponding con-cepts.
In particular, we found that verbs can be dif-ficult to depict and understand without such hints.The B position serves as an action indicator to dis-ambiguate between multiple senses of the sameicon.
For example, in Figure 1, the school bus iconclearly represents the verb phrase ?rides the bus,?rather than just the noun ?bus.?2.
Such a layout is particularly amenable to ma-chine learning.
Specifically, we can turn the prob-lem of finding the optimal layout for an input sen-tence into a sequence tagging problem, which iswell-studied in NLP.120The girl rides the bus to school in the morningO A B B B O C O O BFigure 1: Example ABC picture layout, originaltext, and tag sequence.3.1 ABC Layout as Sequence TaggingGiven an input sentence, one can assign each worda tag from the set {A, B, C, O}.
The bottom row inFigure 1 shows an example tag sequence.
The tagspecifies the ABC layout position of the icon cor-responding to that word.
Tag O means ?other?
andmarks words not included in the picture.
Withineach position, icons appear in the word order in theinput sentence.
Therefore, a tag sequence uniquelydetermines an ABC layout of the picture.Finding the optimal ABC layout of the inputsentence is thus equivalent to computing the mostlikely tag sequence given the input sentence.
Weadopt a machine learning approach by training asequence tagger for this task.
To do so, we needto collect labeled training data in the form of sen-tences with manually annotated tag sequences.
Wediscuss our annotation effort next, and present ourmachine learning models in Section 4.3.2 Human Annotated Training DataWe asked the five annotators to manually label 571sentences compiled from several online sources,including grade school texts about history and sci-ence, children?s books, and recent news headlines.Some sentences were written by the annotators anddescribe daily activities.
The annotators taggedeach sentence using a Web-based tool to drag-and-drop icons into the desired positions in the layout2.To gauge the quality of the manually labeleddata, and to understand the difficulty of the ABC2The manual tagging actually employs a more detailed tagset to denote phrase structure: Each A, B, or C tag is com-bined with a modifier of b (begin phrase) or i (inside phrase).For example, the phrase ?rides the bus?
in Figure 1 is taggedwith Bb Bi Bi, and shares one icon.
The icons were alsomanually selected by the annotator from a list of Web imagesearch results.layout, we computed inter annotator agreementamong three of the five annotators on a commonset of 48 sentences.
Considering all pair-wise com-parisons of the three annotators, the overall aver-age tag agreement was 77%.
This measures the to-tal number of matching tags (across all sentences)divided by the total number of tags.
Matchingstrictly requires both the correct tag and the correctmodifier.
We also computed Fleiss?
kappa, whichmeasures the degree of inter-annotator agreementbeyond the amount expected by chance (Fleiss,1971).
The values range from 0 to 1, with 1 indi-cating perfect agreement.
The kappa statistic was0.71, which is often considered moderate to highagreement.Further inspection revealed that most disagree-ment was due to annotators reversing A and Ctags.
This could arise from interpreting passivesentences in different ways or trying to representphysical movement.
For example, some annotatorsfound it more natural to depict eating by placing afood item in A and the eater in C, treating the ar-row as the transfer of food.
It was also common forannotators to disagree on whether certain adverbsand time modifiers belong in B or in C. These dif-ferences all suggest the highly subjective nature ofconceptualizing pictures from text.4 A Conditional Random Field Model forABC Layout PredictionWe now introduce our approach to automaticallypredicting the ABC layout of an input sentence.While it was most natural for human annotators toannotate text at the word level, early experimentsquickly revealed that predicting tags at this level isquite challenging.
Most of this stems from the factthat human annotators tend to fragment the textinto many small segments based on the availabilityof good icons.
For example, the phrase ?the whitepygmy elephant?
may be tagged as ?O A O A?
be-cause it is difficult for the annotator to find an iconof this exact phrase or the word ?pygmy,?
but easyto find icons of ?white?
and ?elephant?
separately.Essentially, human annotation combines two tasksin one: deciding where each phrase goes in the lay-out, and deciding which words within a phrase canbe depicted with icons.To rectify this situation, we make layout predic-tions at the level of chunks (phrases); that is, weautomatically break the text into chunks, then pre-dict one A, B, C, or O tag for each chunk.
Since the121tag choices made for different chunks may dependon each other, we employ Conditional RandomFields (CRF) (Lafferty et al, 2001), which are fre-quently used in sequential labeling tasks like infor-mation extraction.
Our choice of chunking is de-scribed in Section 4.1, and the CRF models and in-put features are described in Section 4.2.
The taskof deciding which words within a chunk should ap-pear in the picture is addressed by a ?word pictura-bility?
model, and is discussed in a separate paper.For training, we automatically map the word-level tags in our annotated data to chunk-level tagsbased on the majority ABC tag within a chunk.4.1 Chunking by Semantic Role LabelingIdeally, we would like semantically coherent textchunks to be represented pictorially in the samelayout position.
To obtain such chunks, we lever-age existing semantic role labeling (SRL) tech-nology (Palmer et al, 2005; Gildea and Jurafsky,2002).
SRL is an active NLP task in which wordsor phrases in a sentence are assigned a label indi-cating the role they play with respect to a particu-lar verb (also known as the target predicate).
SRLsystems like FrameNet (Baker et al, 1998) andPropBank (Palmer et al, 2005) aim to provide arich representation for applications requiring somedegree of natural language understanding, and arethus perfectly suited for our needs.
We shall fo-cus on PropBank labels because they are easier touse for our task.
To obtain semantic role labels,we use the automatic statistical semantic role la-beler ASSERT (Pradhan et al, 2004), trained toidentify PropBank arguments through the use ofsupport vector machines and full syntactic parses.To understand how SRL can be useful for deriv-ing pictorial layouts, consider the sentence ?Theboy gave the ball to the girl.?
PropBank marksthe semantic role labels of the ?arguments?
ofverbs.
The target verb ?give?
is part of the frameset?transfer,?
with core arguments ?Arg0: giver?
(theboy), ?Arg1: thing given?
(the ball), and ?Arg2:entity given to?
(the girl).
Verbs can also in-volve non-core modifier arguments, such as ArgM-TMP (time), ArgM-LOC (location), ArgM-CAU(cause), etc.
The entities playing semantic rolesare likely to be entities we want to portray in apicture.
For PropBank, Arg0 often represents anAgent, and Arg1 the Patient or Theme.
If we couldmap the different semantic role labels to ABC tagswith simple rules, then we would be done.Unfortunately, it is not this simple, as Prop-Bank roles are verb-specific.
As Palmer et alpointed out, ?No consistent generalizations can bemade across verbs for the higher-numbered argu-ments?
(Palmer et al, 2005).
In the above exam-ple, we might expect a layout rule of [Arg0]?A,[Target, Arg1]?B, [Arg2]?C.
However, this ruledoes not generalize to other verbs, such as ?drive,?as in the sentence ?The boy drives his parentscrazy,?
which also has three core arguments ?Arg0:driver,?
?Arg1: thing in motion,?
and ?Arg2: sec-ondary predication on Arg1.?
However, here theaction is figurative, and we would expect a lay-out rule that puts Arg1 in position C: [Arg0]?A,[Target]?B, [Arg1,Arg2]?C.In addition, while modifier arguments have thesame meaning across verbs, their pictorial repre-sentation may differ based on context.
Considerthe sentences ?Polar bears live in the Arctic.?
and?Yesterday at the zoo, the students saw a polarbear.?
In the former, a human annotator is likelyto place an icon for the ArgM-LOC ?in the Arc-tic?
in position C (e.g., following a polar bear iconin A and a house icon in B).
However, the ArgM-LOC in the second sentence, ?at the zoo,?
seemsmore appropriately placed in position B since it de-scribes where this particular action occurred.Finally, the situation is further complicatedwhen a sentence contains multiple verbs.
SRLtreats each verb in isolation, producing multiplesets of role labels, yet our goal is to produce a sin-gle picture.
Clearly, the mapping from semanticroles to layout positions is non-trivial.
We describeour statistical machine learning approach next.4.2 Our CRF Models and FeaturesWe use a linear-chain CRF as our sequence tag-ging model.
A CRF is a discriminative model ofthe conditional probability p(y|x), where y is thesequence of layout tags in Y ={A,B,C,O}, and xis the sequence of SRL chunks produced by theprocess described in Section 4.1.
Our CRF has thegeneral formp(y|x) =1Z(x)exp?
?|x|?t=1K?k=1?kfk(yt, yt?1, x, t)?
?where the model parameters are {?k}.
Weuse binary features fk(yt, yt?1, x, t) detailed be-low.
Finally, we use an isotropic Gaussian priorN(0,?2I) on parameters as regularization.122We explored three versions of the above modelby specializing the weighted feature function?kfk().
Model 1 ignores the pairwise label poten-tials and treats each labeling prediction indepen-dently: ?jk1{yt=j}fk(x, t), where 1{z} is an indi-cator function on z.
This is equivalent to a multi-class logistic regression classifier.
Model 2 resem-bles a Hidden Markov Model (HMM) by factoringpairwise label potentials and emission potentials:?ij1{yt?1=i}1{yt=j}+?jk1{yt=j}fk(x, t).
Finally,Model 3 has the most general linear-chain poten-tial: ?ijk1{yt?1=i}1{yt=j}fk(x, t).
Model 3 is themost flexible, but has the most weights to learn.We use the following binary predicate featuresfk(x, t) in all our models, evaluated on each chunkproduced by the semantic role labeler:1.
PropBank role label(s) of the chunk (e.g., Tar-get, Arg0, Arg1, ArgM-LOC).
A chunk can havemultiple role labels if the sentence contains multi-ple verbs; in this case, we merge the multiple SRLresults by taking their union.2.
Part-of-speech tags of all the words in thechunk.
All syntactic parsing results are obtainedfrom the Stanford Parser (Klein and Manning,2003), using the default PCFG model.3.
Phrase type (e.g., NP, VP, PP) of the deepestsyntactic parse tree node covering the entire chunk.We also include a feature indicating whether thephrase is nested within an ancestor VP.4.
Lexical features: individual word identities inthe top 5000 most frequent words in the Google 1T5gram corpus (Brants and Franz, 2006).
For otherwords, we use their automatically predicted Word-Net supersenses (Ciaramita and Altun, 2006).
Su-persenses are 41 broad semantic categories (e.g.,noun.location, verb.communication).
By dividinglexical features in this way, we hope to learn spe-cific qualities of common words, but generalizeacross rarer words.We also experimented with features derivedfrom typed dependency relations, but these did notimprove our models.
We suspect the PropBankrole labels capture much of the same information.In addition, the Google 5000-word list was the bestamong several word lists that we explored for split-ting up the lexical features.4.3 CRF Experimental ResultsWe trained our CRF models using the MAL-LET toolkit (McCallum, 2002).
Our completedataset consists of the 571 manually annotated sen-10?1 100 1010.710.720.730.740.750.760.770.78VarianceAccuracyandF1AccuracyF1Model 1Model 2Model 3Figure 2: 5-fold cross validation results for dif-ferent values of the regularization parameter (vari-ance ?2) and three CRF models predicting A, B,C, or O layout tags.tences (tags mapped to chunk-level).
The onlytuning parameter is the Gaussian prior variance,?2.
We performed 5-fold cross validation, vary-ing ?2 and comparing performance across models.Figure 2 demonstrates that peak per-chunk accu-racy (77.6%) and macro-averaged F1 scores areachieved using the most general sequence labelingmodel.
As a result, the user study in the next sec-tion is based on layouts predicted by Model 3 with?2 = 1.0, trained on all the data.To understand which features contribute mostto performance, we experimented with removingeach of the four types (individually).
Peak accu-racy drops the most when lexical features are re-moved (76.4%), followed by PropBank features(76.5%), phrase features (76.9%), and POS fea-tures (77.1%).The features in the final learned model make in-tuitive sense.
It prefers tag transitions A?B andB?C, but not A?C or C?A.
The model likes theword ?I?
and noun phrases (not nested in a verbphrase) to have tag A. Verbs and ArgM-NEGs arefrequently tagged B, while noun.object?s, Arg4s,and ArgM-CAUs are typically C. The model dis-courages Arg0s and conjunctions in B, and dislikesadverbial phrases and noun.time?s in C.While 77.6% cross validation accuracy mayseem low, it is in fact close to the 81% inter an-notator agreement3, and thus close to optimal.
Theconfusion matrix (not shown) reveals that most er-3The 81% agreement is on mapped chunk-level tags with-out modifiers (Fleiss?
kappa 0.74), while the 77% agreementin Section 3.2 is on word-level tags with modifiers.123rors probably arise from disagreements in the in-dividual annotators.
The most common errors arepredicting B for chunks labeled O and confusingtags B and C. Manually inspecting the pictures inour training set shows that annotators often omit-ted the verb (such as ?is?
or ?has?)
and left the Bposition empty, since it could be inferred by thepresence of the arrow and the images in A and C.Also, annotators tended to disagree on the locationof adverbial expressions, dividing them betweenpositions B and C. Finally, only 3.3% of chunkswere incorrectly omitted from the pictures.
There-fore, we conclude that our CRFmodels are capableof predicting the ABC layouts.5 User StudyWe have proposed the ABC layout, and showedthat we can learn to predict it reasonably well.
Butan important question remains: Can the proposedABC layout help a target audience of limited lit-eracy understand pictures better, compared to thelinear layout used in state-of-the-art augmentativeand alternative communication software?
We de-scribe a user study as our first attempt to answerthis question.
This line of work has two main chal-lenges: one is the practical difficulty of workingwith human subjects of limited literacy; the other isthe lack of a quantitative measure of picture com-prehension.
[Subjects]: To partially overcome the first chal-lenge, we recruited two groups of subjects withmedium and high literacy respectively, in hopesof extrapolating our findings towards the low lit-eracy group.
Specifically, the medium group con-sisted of seven non-native English speakers whospeak some degree of English?
?medium literacy?refers to their English fluency; twelve native En-glish speakers comprised the high literacy group.All subjects were adults and did not include theauthors of this paper or the five annotators.
Thesubjects had no prior exposure to pictorial com-munication systems.
[Material]: We randomly chose 90 test sen-tences from three sources4 representing ourtarget application domains: short narrativeswritten by and for individuals with commu-nicative disorders (symbolworld.org);one-sentence news synopses written in simpleEnglish targeting foreign language learners(simpleenglishnews.com); and the child4Distinct from the sources of the 571 training sentences.writing sections of the LUCY corpus (Sampson,2003).
We created two pictures for each testsentence: one using a linear layout and oneusing an ABC layout.
For the linear layout,we used SymWriter.
Typing text in SymWriterautomatically produces a left-to-right sequenceof icons, chosen from an icon database.
In caseswhere SymWriter suggests several possible iconsfor a word, we manually selected the best one.
Forwords not in the database, we found appropriatethumbnail images using Web image search.
Thisis how a typical user would use SymWriter.
Toproduce the ABC layout, we applied the trainedCRF tagger Model 3 to the test sentence.
Afterobtaining A, B, C, and O tags for text chunks, weplaced the corresponding icons (from SymWriter?slinear layout) in the correct layout positions.
Iconsfor words tagged O did not appear in the ABCversion of the picture.
Aside from this difference,both pictures of each test sentence containedexactly the same icons?the only difference wasthe layout.
[Protocol]: All 19 subjects observed each ofthe 90 test sentences exactly once: 45 with thelinear layout and 45 with the ABC layout.
Thelayouts and the order of sentences were both ran-domized throughout the sequence, and the subjectswere counter-balanced so each sentence?s linearand ABC layouts were viewed by roughly equalnumbers of subjects.
At the start of the study,each subject read a brief introduction describingthe task and saw an example of each layout style.Then for each test sentence, we displayed a pic-ture, and the subject typed a guess of the underly-ing sentence.
Finally, the subject provided a confi-dence rating (2=?almost sure,?
1=?maybe correct,?or 0=?no idea?).
We measured response time asthe time from image display until sentence/ratingsubmission.
Figure 3 shows a test sentence in bothlayouts, together with several subjects?
guesses.
[Evaluation metrics]: As noted above, thesecond main challenge is measuring picturecomprehension?we need a way to compare theoriginal sentences with the subjects?
guesses.
Inmany ways, this is like machine translation (viapictures), so we turned to two automatic eval-uation metrics: BLEU-1 (Papineni et al, 2002)and METEOR (Lavie and Agarwal, 2007).
BLEU-1computes unigram precision (i.e., fraction of re-sponse words that exactly match words in the orig-inal), multiplied by a brevity penalty for omit-124?we sing a song about a farm.?
?i sing about the farm and animals?
?we sang for the farmer and he gave us animals.?
?Someone went to his grandfather?s farmand played with the animals?
?i can?t sing in the choir because i have to tendto the animals.?
?twins sing old macdonald has a farm?
?they sang about a farm?
?they sing old mcdonald had a farm.?
?we have a farm with a sheep, a pig and a cow.?
?two people sing old mcdonald had a farm?
?we sang old mcdonald on the farm.?
?they both sing ?old macdonald had a farm?.
?Figure 3: The linear and ABC layout pictures for the test sentence ?We sang Old MacDonald had afarm.?
and some subjects?
guesses.
Note the predicted ABC layout omits the ambiguous ?had?
icon.ting words.
In contrast, METEOR finds a one-to-one word alignment between the texts that allowspartial matches (after stemming and by consider-ing WordNet-based synonyms) and optionally ig-nores stop words.
Based on this alignment, uni-gram precision, recall, and weighted F measure arecomputed, and the final METEOR score is obtainedby scaling F to account for word-order preserva-tion.
We computed METEOR using its default pa-rameters and the stop word list from the Snowballproject (Porter, 2001).
[Results]: We report average METEOR and BLEUscores, confidence ratings, and response time forthe 4 conditions (native vs. non-native, ABC vs.linear) in Table 1.
The most striking observationis that native speakers perform better (in terms ofMETEOR and BLEU) with the linear layout, whilenon-native speakers do better with ABC.
5To explain this finding, it is worth noting thatSymWriter pictures include function words, whoseicons are abstract but distinct.
We speculate thateven though none of our subjects were trained torecognize these function-word icons, the nativespeakers are more accustomed to the English syn-tactic structure, so they may be able to transliter-ate those icons back to words.
In an ABC lay-5Using a Mann-Whitney rank sum test, the difference innative speakers?
METEOR scores is statistically significant(p = 0.003), though the other differences are not (nativeBLEU, p = 0.085; non-native METEOR, p = 0.172; non-native BLEU, p = 0.170).
Nevertheless, we observe someevidence to support our hypothesis that non-native speak-ers benefit from the ABC layout, and we intend to conductfollow-up experiments to test the claim further.Non-native NativeABC Linear ABC LinearMETEOR 0.1975 0.1800 0.2955 0.3335BLEU 0.1497 0.1456 0.2710 0.3011Conf.
0.50 0.47 0.90 0.89Time 47.4s 47.8s 38.1s 38.6sTable 1: User study results.out, the sentence order is mostly removed, andsome phrases might be omitted due to the O tag.Thus native speakers do not get as many syntactichints.
On the other hand, non-native speakers donot have the same degree of built-in English syn-tactic knowledge.
As such, they do not gain muchfrom seeing the whole sentence sequence includ-ing function-word icons.
Instead, they may havebenefited from the ABC layout?s added organiza-tion and potential exclusion of irrelevant icons.If this reasoning holds, it has interesting impli-cations for viewers who have lower English liter-acy: they might take away more meaning from asemantically structured layout like ABC.
Verifyingthis is a direction for future work.Finally, it is interesting that all subjects feelmore confident in their responses to ABC layoutsthan linear layouts, and, despite their added com-plexity, ABC layouts do not require more responsetime than linear layouts.1256 ConclusionsWe proposed a semantically enhanced picture lay-out for pictorial communication.
We formulatedour ABC layout prediction problem as sequencetagging, and trained CRF models with linguisticfeatures including semantic role labels.
A userstudy indicated that our ABC layout has the poten-tial to facilitate picture comprehension for peoplewith limited literacy.
Future work includes incor-porating ABC layouts into our pictorial communi-cation system, improving other components, andverifying our findings with additional user studies.AcknowledgmentsThis work is supported by NSF IIS-0711887, andby the Wisconsin Alumni Research Foundation.ReferencesAdorni, G., M. Di Manzo, and G. Ferrari.
1983.
Natu-ral language input for scene generation.
In ACL.Baker, C. F., C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley FrameNet Project.
In COLING.Brants, T. and A. Franz.
2006.
Web 1T 5-gram version1.1.
Linguistic Data Consortium, Philadelphia.Brown, D. C. and B. Chandrasekaran.
1981.
Designconsiderations for picture production in a natural lan-guage graphics system.
SIGGRAPH, 15(2).Chandrasekar, R., C. Doran, and B. Srinivas.
1996.Motivations and methods for text simplification.
InCOLING.Ciaramita, M. and Y. Altun.
2006.
Broad-coveragesense disambiguation and information extractionwith a supersense sequence tagger.
In EMNLP.Clay, S. R. and J. Wilhelms.
1996.
Put: Language-based interactive manipulation of objects.
IEEEComputer Graphics and Applications, 16(2).Coyne, B. and R. Sproat.
2001.
WordsEye: An au-tomatic text-to-scene conversion system.
In SIG-GRAPH.Fleiss, J. L. 1971.
Measuring nominal scale agreementamong many raters.
Psychological Bulletin, 76(5).Gildea, D. and D. Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics, 28(3).Hehner, B.
1980.
Blissymbols for use.
BlissymbolicsCommunication Institute.Johansson, R., A. Berglund, M. Danielsson, andP.
Nugues.
2005.
Automatic text-to-scene conver-sion in the traffic accident domain.
In IJCAI.Joshi, D., J.
Z. Wang, and J. Li.
2006.
The story pictur-ing engine?a system for automatic text illustration.ACM Transactions on Multimedia Computing, Com-munications, and Applications, 2(1).Klein, D. and C. D. Manning.
2003.
Accurate unlexi-calized parsing.
In ACL.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML.Lavie, A. and A. Agarwal.
2007.
METEOR: An au-tomatic metric for MT evaluation with high levels ofcorrelation with human judgments.
In Second Work-shop on Statistical Machine Translation, June.McCallum, A. K. 2002.
Mallet: A machine learningfor language toolkit.
http://mallet.cs.umass.edu.Mihalcea, R. and B. Leong.
2006.
Toward commu-nicating simple sentences using pictorial representa-tions.
In Association of Machine Translation in theAmericas.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).Papineni, K., S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In ACL.Porter, M. F. 2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/.Pradhan, S., W. Ward, K. Hacioglu, J. Martin, andD.
Jurafsky.
2004.
Shallow semantic parsing usingsupport vector machines.
In HLT/NAACL.Sampson, G. 2003.
The structure of children?s writ-ing: Moving from spoken to adult written norms.
InGranger, S. and S. Petch-Tyson, editors, Extendingthe Scope of Corpus-Based Research.
Rodopi.Vickrey, D. and D. Koller.
2008.
Sentence simplifica-tion for semantic role labeling.
In ACL.
To appear.Widgit Software.
2007.
SymWriter.http://www.mayer-johnson.com.Yamada, A., T. Yamamoto, H. Ikeda, T. Nishida, andS.
Doshita.
1992.
Reconstructing spatial imagefrom natural language texts.
In COLING.Zhu, X., A.
B. Goldberg, M. Eldawy, C. Dyer, andB.
Strock.
2007.
A Text-to-Picture synthesis systemfor augmenting communication.
In AAAI.126
