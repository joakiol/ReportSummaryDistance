Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 815?824,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsNamed Entity Disambiguation in Streaming DataAlexandre Davis1, Adriano Veloso1, Altigran S. da Silva2Wagner Meira Jr.1, Alberto H. F. Laender11Computer Science Dept.
?
Federal University of Minas Gerais2Computer Science Dept.
?
Federal University of Amazonas{agdavis,adrianov,meira,laender}@dcc.ufmg.bralti@dcc.ufam.edu.brAbstractThe named entity disambiguation task is to re-solve the many-to-many correspondence be-tween ambiguous names and the unique real-world entity.
This task can be modeled as aclassification problem, provided that positiveand negative examples are available for learn-ing binary classifiers.
High-quality sense-annotated data, however, are hard to be ob-tained in streaming environments, since thetraining corpus would have to be constantlyupdated in order to accomodate the fresh datacoming on the stream.
On the other hand, fewpositive examples plus large amounts of un-labeled data may be easily acquired.
Produc-ing binary classifiers directly from this data,however, leads to poor disambiguation per-formance.
Thus, we propose to enhance thequality of the classifiers using finer-grainedvariations of the well-known Expectation-Maximization (EM) algorithm.
We conducteda systematic evaluation using Twitter stream-ing data and the results show that our clas-sifiers are extremely effective, providing im-provements ranging from 1% to 20%, whencompared to the current state-of-the-art biasedSVMs, being more than 120 times faster.1 IntroductionHuman language is not exact.
For instance, an en-tity1 may be referred by multiple names (i.e., poly-semy), and also the same name may refer to differententities depending on the surrounding context (i.e.,1The term entity refers to anything that has a distinct, sepa-rate (materialized or not) existence.homonymy).
The task of named entity disambigua-tion is to identify which names refer to the same en-tity in a textual collection (Sarmento et al, 2009;Yosef et al, 2011; Hoffart et al, 2011).
The emer-gence of new communication technologies, such asmicro-blog platforms, brought a humongous amountof textual mentions with ambiguous entity names,raising an urgent need for novel disambiguation ap-proaches and algorithms.In this paper we address the named entity disam-biguation task under a particularly challenging sce-nario.
We are given a stream of messages from amicro-blog channel such as Twitter2 and a list ofnames n1, n2, .
.
.
, nN used for mentioning a spe-cific entity e. Our problem is to monitor the streamand predict whether an incoming message contain-ing ni indeed refers to e (positive example) or not(negative example).
This scenario brings challengesthat must be overcome.
First, micro-blog messagesare composed of a small amount of words and theyare written in informal, sometimes cryptic style.These characteristics make hard the identification ofentities and the semantics of their relationships (Liuet al, 2011).
Further, the scarcity of text in the mes-sages makes it even harder to properly characterize acommon context for the entities.
Second, as we needto monitor messages that keep coming at a fast pace,we cannot afford to gather information from externalsources on-the-fly.
Finally, fresh data coming in thestream introduces new patterns, quickly invalidatingstatic disambiguation models.2Twitter is one of the fastest-growing micro-blog channels,and an authoritative source for breaking news (Jansen et al,2009).815We hypothesize that the lack of information ineach individual message and from external sourcescan be compensated by using information obtainedfrom the large and diverse amount of text in a streamof messages taken as a whole, that is, thousands ofmessages per second coming from distinct sources.The information embedded in such a stream ofmessages may be exploited for entity disambigua-tion through the application of supervised learningmethods, for instance, with the application of bi-nary classifiers.
Such methods, however, suffer froma data acquisition bottleneck, since they are basedon training datasets that are built by skilled hu-man annotators who manually inspect the messages.This annotation process is usually lengthy and la-borious, being clearly unfeasible to be adopted indata streaming scenarios.
As an alternative to suchmanual process, a large amount of unlabeled data,augmented with a small amount of (likely) posi-tive examples, can be collected automatically fromthe message stream (Liu et al, 2003; Denis, 1998;Comite?
et al, 1999; Letouzey et al, 2000).Binary classifiers may be learned from such databy considering unlabeled data as negative examples.This strategy, however, leads to classifiers with poordisambiguation performance, due to a potentiallylarge number of false-negative examples.
In this pa-per we propose to refine binary classifiers iteratively,by performing Expectation-Maximization (EM) ap-proaches (Dempster et al, 1977).
Basically, a partialclassifier is used to evaluate the likelihood of an un-labeled example being a positive example or a nega-tive example, thus automatically and (continuously)creating a labeled training corpus.
This process con-tinues iteratively by changing the label of some ex-amples (an operation we call label-transition), sothat, after some iterations, the combination of la-bels is expected to converge to the one for whichthe observed data is most likely.
Based on such anapproach, we introduce novel disambiguation algo-rithms that differ among themselves on the granu-larity in which the classifier is updated, and on thelabel-transition operations that are allowed.An important feature of the proposed approach isthat, at each iteration of the EM-process, a new clas-sifier (an improved one) is produced in order to ac-count for the current set of labeled examples.
Weintroduce a novel strategy to maintain the classifiersup-to-date incrementally after each iteration, or evenafter each label-transition operation.
Indeed, we the-oretically show that our classifier needs to be up-dated just partially and we are able to determine ex-actly which parts must be updated, making our dis-ambiguation methods extremely fast.To evaluate the effectiveness of the proposed al-gorithms, we performed a systematic set of ex-periments using large-scale Twitter data containingmessages with ambiguous entity names.
In orderto validate our claims, disambiguation performanceis investigated by varying the proportion of false-negative examples in the unlabeled dataset.
Ouralgorithms are compared against a state-of-the-arttechnique for named entity disambiguation basedon classifiers, providing performance gains rangingfrom 1% to 20% and being roughly 120 times faster.2 Related WorkIn the context of databases, traditional entity dis-ambiguation methods rely on similarity functionsover attributes associated to the entities (de Car-valho et al, 2012).
Obviously, such an approachis unfeasible for the scenario we consider here.Still on databases, Bhattacharya and Getoor (2007)and Dong et.
al (2005) propose graph-based dis-ambiguation methods that generate clusters of co-referent entities using known relationships betweenentities of several types.
Methods to disambiguateperson names in e-mail (Minkov et al, 2006) andWeb pages (Bekkerman and McCallum, 2005; Wanet al, 2005) have employed similar ideas.
In e-mails, information taken from the header of the mes-sages leads to establish relationships between usersand building a co-reference graph.
In Web pages,reference information come naturally from links.Such graph-based approach could hardly be appliedto the context we consider, in which the implied re-lationships between entities mentioned in a givenmicro-blog message are not clearly defined.In the case of textual corpora, traditional disam-biguation methods represent entity names and theircontext (Hasegawa et al, 2004) (i.e., words, phrasesand other names occurring near them) as weightedvectors (Bagga and Baldwin, 1998; Pedersen et al,2005).
To evaluate whether two names refer tothe same entity, these methods compute the similar-816ity between these vectors.
Clusters of co-referentnames are then built based on such similarity mea-sure.
Although effective for the tasks considered inthese papers, the simplistic BOW-based approachesthey adopt are not suitable for cases in which thecontext is harder to capture due to the small num-ber of terms available or to informal writing style.To address these problems, some authors argue thatcontextual information may be enriched with knowl-edge from external sources, such as search resultsand the Wikipedia (Cucerzan, 2007; Bunescu andPasca, 2006; Han and Zhao, 2009).
While such astrategy is feasible in an off-line setting, two prob-lems arise when monitoring streams of micro-blogmessages.
First, gathering information from exter-nal sources through the Internet can be costly and,second, informal mentions to named entities make ithard to look for related information in such sources.The disambiguation methods we propose fall intoa learning scenario known as PU (positive and un-labeled) learning (Liu et al, 2003; Denis, 1998;Comite?
et al, 1999; Letouzey et al, 2000), in whicha classifier is built from a set of positive examplesplus unlabeled data.
Most of the approaches for PUlearning, such as the biased-SVM approach (Li andLiu, 2003), are based on extracting negative exam-ples from unlabeled data.
We notice that existing ap-proaches for PU learning are not likely to scale giventhe restrictions imposed by streaming data.
Thus,we propose highly incremental approaches, whichare able to process large-scale streaming data.3 Disambiguation in Streaming DataConsider a stream of messages from a micro-blogchannel such as Twitter and let n1, n2, .
.
.
, nN benames used for mentioning a specific entity e inthese messages.
Our problem is to continually moni-tor the stream and predict whether an incoming mes-sage containing ni indeed refers to e or not.This task may be accomplished through the appli-cation of classification techniques.
In this case, weare given an input data set called the training cor-pus (denoted as D) which consists of examples ofthe form <e,m, c>, where e is the entity, m is amessage containing the entity name (i.e., any ni),and c ?
{,} is a binary variable that specifieswhether or not the entity name in m refers to thedesired real-world entity e. The training corpus isused to produce a classifier that relates textual pat-terns (i.e., terms and sets of terms) in m to the valueof c. The test set (denoted as T ) consists of a setof records of the form <e,m, ?>, and the classifieris used to indicate which messages in T refer to (ornot) the desired entity.Supervised classifiers, however, are subject to adata acquisition bottleneck, since the creation of atraining corpus requires skilled human annotators tomanually inspect the messages.
The cost associ-ated with this annotation process may render vastamounts of examples unfeasible.
In many cases,however, the acquisition of some positive examplesis relatively inexpensive.
For instance, as we aredealing with messages collected from micro-blogchannels, we may exploit profiles (or hashtags) thatare known to be strongly associated with the desiredentity.
Let us consider, as an illustrative example,the profile associated with a company (i.e., @bayer).Although the entity name is ambiguous, the sense ofmessages that are posted in this profile is biased to-wards the entity as being a company.
Clearly, othertricks like this one can be used, but, unfortunately,they do not guarantee the absence of false-positives,and they are not complete since the majority of mes-sages mentioning the entity name may appear out-side its profile.
Thus, the collected examples arenot totally reliable, and disambiguation performancewould be seriously compromised if classifiers werebuilt upon these uncertain examples directly.3.1 Expectation-Maximization ApproachIn this paper we hypothesize that it is worthwhileto enhance the reliability of unlabeled examples,provided that this type of data is inexpensive andthe enhancement effort will be then rewarded withan improvement in disambiguation performance.Thus, we propose a new approach based on theExpectation-Maximization (EM) algorithm (Demp-ster et al, 1977).
We assume two scenarios:?
the training corpusD is composed of a small setof truly positive examples plus a large amountof unlabeled examples.?
the training corpus D is composed of a smallset of potentially positive examples plus a largeamount of unlabeled examples.817In both scenarios, unlabeled examples are ini-tially treated as negative ones, so that classifiers canbe built from D. Therefore, in both scenarios, Dmay contain false-negatives.
In the second scenario,however, D may also contain false-positives.Definition 3.1: The label-transition operationx? turns a negative example x ?
D into apositive one x.
The training corpus D becomes{(D ?
x) ?
x}.
Similarly, the label-transitionoperation x?, turns a positive example x ?
Dinto a negative one x.
The training corpus D be-comes {(D ?
x) ?
x}.Our Expectation Maximization (EM) methodsemploy a classifier which assigns to each examplex ?
D a probability ?
(x,) of being negative.Then, as illustrated in Algorithm 1, label-transitionoperations are performed, so that, in the end of theprocess, it is expected that the assigned labels con-verge to the combination for which the data is mostlikely.
In the first scenario only operations x?are allowed, while in the second scenario operationsx? are also allowed.
In both cases, a crucial issuethat affects the effectiveness of our EM-based meth-ods concerns the decision of whether or not perform-ing the label-transition operation.
Typically, a tran-sition threshold ?min is employed, so that a label-transition operation x? is always performed if xis a negative example and ?
(x,) ?
?min.
Simi-larly, operation x? is always performed if x is apositive example and ?
(x,) > ?min.Algorithm 1 Expectation-Maximization Approach.Given:D: training corpusR: a binary classifier learned from DExpectation step:perform transition operations on examples in DMaximization step:update R and ?
(x,) ?x ?
DThe optimal value for ?min is not known in ad-vance.
Fortunately, data distribution may providehints about proper values for ?min.
In our ap-proach, instead of using a single value for ?min,which would be applied to all examples indistinctly,we use a specific ?xmin threshold for each exam-ple x ?
D. Based on such an approach, we in-troduce fine-grained EM-based methods for namedentity disambiguation under streaming data.
A spe-cific challenge is that the proposed methods performseveral transition operations during each EM itera-tion, and each transition operation may invalidateparts of the current classifier, which must be prop-erly updated.
We take into consideration two possi-ble update granularities:?
the classifier is updated after each EM iteration.?
the classifier is updated after each label-transition operation.Incremental Classifier: As already discussed, theclassifier must be constantly updated during the EMprocess.
In this case, well-established classifiers,such as SVMs (Joachims, 2006), have to be learnedentirely from scratch, replicating work by large.Thus, we propose as an alternative the use of LazyAssociative Classifiers (Veloso et al, 2006).Definition 3.2: A classification rule is a specializedassociation rule {X ??
c} (Agrawal et al, 1993),where the antecedent X is a set of terms (i.e., atermset), and the consequent c indicates if the pre-diction is positive or negative (i.e., c ?
{,}).The domain for X is the vocabulary of D. The car-dinality of rule {X ?
c} is given by the number ofterms in the antecedent, that is |X |.
The support ofX is denoted as ?
(X ), and is the number of exam-ples in D having X as a subset.
The confidence ofrule {X ?
c} is denoted as ?
(X ??
c), and is theconditional probability of c given the termsetX , thatis, ?
(X ??
c) = ?(X?c)?
(X ) .In this context, a classifier is denoted as R, andit is composed of a set of rules {X ??
c} ex-tracted from D. Specifically, R is represented asa pool of entries with the form <key, properties>,where key={X , c} and properties={?
(X ), ?
(X ?c), ?
(X ?
c)}.
Each entry in the pool correspondsto a rule, and the key is used to facilitate fast accessto rule properties.Once the classifier R is extracted from D, rulesare collectively used to approximate the likelihoodof an arbitrary example being positive () or neg-ative ().
Basically, R is interpreted as a poll, inwhich each rule {X ?
c} ?
R is a vote given by Xfor  or .
Given an example x, a rule {X ?
c} isonly considered a valid vote if it is applicable to x.818Definition 3.3: A rule {X ?
c} ?
R is said to beapplicable to example x ifX ?
x, that is, if all termsin X are present in example x.We denote as Rx the set of rules in R that are ap-plicable to example x.
Thus, only and all the rules inRx are considered as valid votes when classifying x.Further, we denote as Rcx the subset of Rx contain-ing only rules predicting c. Votes in Rcx have differ-ent weights, depending on the confidence of the cor-responding rules.
Weighted votes for c are averaged,giving the score for c with regard to x (Equation 1).Finally, the likelihood of x being a negative exampleis given by the normalized score (Equation 2).s(x, c) =?
?
(X ?
c)|Rcx|,with c ?
{,} (1)?
(x,) = s(x,)s(x,) + s(x,) (2)Training Projection and Demand-Driven RuleExtraction: Demand-driven rule extraction (Velosoet al, 2006) is a recent strategy used to avoid thehuge search space for rules, by projecting the train-ing corpus according to the example being pro-cessed.
More specifically, rule extraction is delayeduntil an example x is given for classification.
Then,terms in x are used as a filter that configures thetraining corpus D so that just rules that are appli-cable to x can be extracted.
This filtering processproduces a projected training corpus, denoted asDx,which contains only terms that are present in x. Asshown in (Silva et al, 2011), the number of rules ex-tracted using this strategy grows polynomially withthe size of the vocabulary.Extending the Classifier Dynamically: Withdemand-driven rule extraction, the classifierR is ex-tended dynamically as examples are given for clas-sification.
Initially R is empty; a subset Rxi is ap-pended to R every time an example xi is processed.Thus, after processing a sequence of m examples{x1, x2, .
.
.
, xm}, R = {Rx1 ?Rx2 ?
.
.
.
?Rxm}.Before extracting rule {X ?
c}, it is checkedwhether this rule is already in R. In this case, whileprocessing an example x, if an entry is found witha key matching {X , c}, then the rule in R is usedinstead of extracting it from Dx.
Otherwise, the ruleis extracted from Dx and then it is inserted into R.Incremental Updates: Entries in R may becomeinvalid when D is modified due to a label-transitionoperation.
Given thatD has been modified, the clas-sifier R must be updated properly.
We propose tomaintain R up-to-date incrementally, so that the up-dated classifier is exactly the same one that wouldbe obtained by re-constructing it from scratch.Lemma 3.1: Operation x? (or x?) does notchange the value of ?
(X ), for any termset X .Proof: The operation x? changes only the labelassociated with x, but not its terms.
Thus, the num-ber of examples in D having X as a subset is essen-tially the same as in {(D ?
x) ?
x.
The sameholds for operation x?.
Lemma 3.2: Operation x? (or x?) changesthe value of ?
(X ?
c) iff termset X ?
x.Proof: For operation x?, if X ?
x, then {X ?} appears once less in {(D ?
x) ?
x} than inD.
Similarly, {X ?} appears once more in {(D?x)?x} than inD.
Clearly, if X 6?
x, the numberof times {X ?} (and {X ?}) appears in {(D ?x)?x} remains the same as inD.
The same holdsfor operation x?.
Lemma 3.3: Operation x? (or x?) changesthe value of ?
(X ?
c) iff termset X ?
x.Proof: Comes directly from Lemmas 3.1 and 3.2.
From Lemmas 3.1 to 3.3, the number of rules thathave to be updated due to a label-transition operationis bounded by the number of possible termsets in x.The following theorem states exactly the rules in Rthat have to be updated due to a transition operation.Theorem 3.4: All rules in R that must be updateddue to x? (or x?) are those in Rx.Proof: From Lemma 3.3, all rules {X ??
c} ?
Rthat have to be updated due to operation x? (orx?) are those for which X ?
x.
By definition,Rx contains only and all such rules.
Updating ?
(X ?
) and ?
(X ?
) is straight-forward.
For operation x?, it suffices to iterateon Rx, incrementing ?
(X ?
) and decrementing?
(X ?
).
Similarly, for operation x?, it suf-fices to iterate on Rx, incrementing ?
(X ?
) anddecrementing ?
(X ?
).
The corresponding valuesfor ?
(X ?
) and ?
(X ?
) are simply obtainedby computing ?(X?)?
(X ) and?(X?)?
(X ) , respectively.8193.2 Best Entropy Cut MethodIn this section we propose a method for finding theactivation threshold, ?xmin, which is a fundamentalstep of our Expectation-Maximization approach.Definition 3.4: Let cy ?
{,} be the label asso-ciated with an example y ?
Dx.
Consider N(Dx)the number of examples inDx for which cy=.
Sim-ilarly, consider N(Dx) the number of examples inDx for which cy=.Entropy Minimization: Our method searches for athreshold ?xmin that provides the best entropy cut inthe probability space induced by Dx.
Specifically,given examples {y1, y2, .
.
.
, yk} in Dx, our methodfirst calculates ?
(yi,) for all yi ?
Dx.
Then, thevalues for ?
(yi,) are sorted in ascending order.
Inan ideal case, there is a cut ?xmin such that:cyi ={ if ?
(yi,) ?
?xmin otherwiseHowever, there are more difficult cases, for whichit is not possible to obtain a perfect separation in theprobability space.
Thus, we propose a more generalmethod to find the best cut in the probability space.The basic idea is that any value for ?xmin induces twopartitions over the space of values for ?
(yi,) (i.e.,one partition with values that are lower than ?xmin,and another partition with values that are higher than?xmin).
Our method sets ?xmin to the value that min-imizes the average entropy of these two partitions.Once ?xmin is calculated, it can be used to activate alabel-transition operation.
Next we present the basicdefinitions in order to detail this method.Definition 3.5: Consider a list of pairs O ={.
.
.
, <cyi , ?
(yi,)>, <cyj , ?
(yj ,)>, .
.
.
}, suchthat ?
(yi,) ?
?
(yj ,).
Also, consider f a candi-date value for ?xmin.
In this case,Of (?)
is a sub-listof O, that is, Of (?)={.
.
., <cy, ?
(yi,)>, .
.
.
},such that for all pairs in Of (?
), ?
(y,) ?
f .
Sim-ilarly, Of (>)={.
.
., <cy, ?
(y,)>, .
.
.
}, such thatfor all pairs inOf (>), ?
(y,) > f .
In other words,Of (?)
andOf (>) are partitions ofO induced by f .Our method works as follows.
Firstly, it calculatesthe entropy in O, as shown in Equation 3.
Then,it calculates the sum of the entropies in each par-tition induced by f , according to Equation 4.
Fi-nally, it sets ?xmin to the value of f that minimizesE(O)?E(Of ), as illustrated in Figure 1.?
(y3 ,)?
(y4 ,)?
(y1 ,)?
(y2 ,)?
(y7 ,)?
(y6 ,)0.00 1.00     lowentropyhighentropy0.00 1.00     highentropy lowentropy0.00 1.00     lowentropy lowentropybest entropy cutFigure 1: Looking for the minimum entropy cut point.E(O) = ?
(N(O)|O| ?
logN(O)|O|)?
(N(O)|O| ?
logN(O)|O|)(3)E(Of ) =|Of (?
)||O| ?
E(Of (?))
+|Of (>)||O| ?
E(Of (>)) (4)3.3 Disambiguation AlgorithmsIn this section we discuss four algorithms basedon our incremental EM approach and following ourBest Entropy Cut method.
They differ among them-selves on the granularity in which the classifier is up-dated and on the label-transition operations allowed:?
A1: the classifier is updated incrementally aftereach EM iteration (which may comprise sev-eral label-transition operations).
Only opera-tion x? is allowed.?
A2: the classifier is updated incrementally aftereach EM iteration.
Both operations x? andx? are allowed.?
A3: the classifier is updated incrementally aftereach label-transition operation.
Only operationx? is allowed.?
A4: the classifier is updated incrementally af-ter each label-transition operation.
Both opera-tions x? and x? are allowed.8204 Experimental EvaluationIn this section we analyze our algorithms usingstandard measures such as AUC values.
For eachpositive+unlabeled (PU) corpus used in our evalu-ation we randomly selected x% of the positive ex-amples (P) to become unlabeled ones (U).
This pro-cedure enables us to control the uncertainty levelof the corpus.
For each level we have a differentTPR-FPR combination, enabling us to draw ROCcurves.We repeated this procedure five times, so thatfive executions were performed for each uncertaintylevel.
Tables 2?5 show the average for the fiveruns.
Wilcoxon significance tests were performed(p<0.05) and best results, including statistical ties,are shown in bold.4.1 Baselines and CollectionsOur baselines include namely SVMs (Joachims,2006) and Biased SVMs (B-SVM (Liu et al, 2003)).Although the simple SVM algorithm does not adaptitself with unlabeled data, we decided to use it inorder to get a sense of the performance achievedby simple baselines (in this case, unlabeled data issimply used as negative examples).
The B-SVM al-gorithm uses a soft-margin SVM as the underlyingclassifier, which is re-constructed from scratch aftereach EM iteration.
B-SVM employs a single tran-sition threshold ?min for the entire corpus, insteadof a different threshold ?xmin for each x ?
D. Itis representative of the state-of-the-art for learningclassifiers from PU data.We employed two different Twitter collections.The first collection, ORGANIZATIONS, is com-posed of 10 corpora3 (O1 to O10).
Each corpus con-tains messages in English mentioning the name ofan organization (Bayer, Renault, among others).
Allmessages were labeled by five annotators.
Label means that the message is associated with the orga-nization, whereas label  means the opposite.The other collection, SOCCER TEAMS, contains6 large-scale PU corpora (ST1 to ST6), taken from aplatform for real time event monitoring (the link tothis platform is omitted due to blind review).
Eachcorpus contains messages in Portuguese mentioningthe name/mascot of a Brazilian soccer team.
Bothcollections are summarized in Table 1.3http://nlp.uned.es/weps/Table 1: Characteristics of each collection.P U P UO1 404 10 ST1 216,991 251,198O2 404 55 ST2 256,027 504,428O3 349 116 ST3 160,706 509,670O4 329 119 ST4 147,706 633,357O5 335 133 ST5 35,021 168,669O6 314 143 ST6 5,993 351,882O7 292 148 ?
?
?O8 295 172 ?
?
?O9 273 165 ?
?
?O10 33 425 ?
?
?4.2 ResultsAll experiments were performed on a Linux PC withan Intel Core 2 Duo 2.20GHz and 4GBytes RAM.Next we discuss the disambiguation performanceand the computational efficiency of our algorithms.ORGANIZATIONS Corpora: Table 2 shows av-erage AUC values for each algorithm.
AlgorithmA4 was the best performer in all cases, suggest-ing the benefits of (i) enabling both types of label-transition operations and (ii) keeping the classifierup-to-date after each label-transition operation.
Fur-ther, algorithm A3 performed better than algorithmA2 in most of the cases, indicating the importance ofkeeping the classifier always up-to-date.
On averageA1 provides gains of 4% when compared against B-SVM, while A4 provides gains of more than 20%.SOCCER TEAMS Corpora: Table 3 shows aver-age AUC values for each algorithm.
Again, algo-rithm A4 was the best performer, providing gainsthat are up to 13% when compared against the base-line.
Also, algorithm A3 performed better than al-gorithm A2, and the effectiveness of Algorithm A1is similar to the effectiveness of the baseline.Since the SOCCER TEAMS collection is com-posed of large-scale corpora, in addition to higheffectiveness, another important issue to be evalu-ated is computational performance.
Table 4 showsthe results obtained for the evaluation of our algo-rithms.
As it can be seen, algorithm A1 is the fastestone, since it is the simplest one.
Even though beingslower than algorithm A1, algorithm A4 runs, on av-erage, 120 times faster than B-SVM.821Table 2: Average AUC values for each algorithm.A1 A2 A3 A4 SVM B-SVMO1 0.74 ?
0.02 0.76 ?
0.02 0.74 ?
0.03 0.79 ?
0.01 0.71 ?
0.03 0.76 ?
0.01O2 0.77 ?
0.02 0.78 ?
0.02 0.70 ?
0.03 0.82 ?
0.02 0.73 ?
0.03 0.75 ?
0.02O3 0.68 ?
0.02 0.70 ?
0.01 0.69 ?
0.02 0.69 ?
0.02 0.64 ?
0.03 0.65 ?
0.02O4 0.68 ?
0.02 0.68 ?
0.02 0.70 ?
0.01 0.72 ?
0.02 0.63 ?
0.02 0.66 ?
0.02O5 0.71 ?
0.01 0.72 ?
0.01 0.71 ?
0.01 0.72 ?
0.01 0.69 ?
0.01 0.71 ?
0.01O6 0.73 ?
0.01 0.73 ?
0.01 0.75 ?
0.01 0.75 ?
0.01 0.68 ?
0.02 0.70 ?
0.01O7 0.69 ?
0.01 0.72 ?
0.01 0.74 ?
0.01 0.74 ?
0.01 0.66 ?
0.02 0.69 ?
0.02O8 0.65 ?
0.02 0.68 ?
0.02 0.69 ?
0.02 0.72 ?
0.01 0.61 ?
0.03 0.63 ?
0.03O9 0.70 ?
0.01 0.70 ?
0.01 0.72 ?
0.01 0.72 ?
0.01 0.65 ?
0.01 0.70 ?
0.01O10 0.70 ?
0.01 0.74 ?
0.02 0.71 ?
0.02 0.75 ?
0.02 0.61 ?
0.03 0.66 ?
0.02Table 3: Average AUC values for each algorithm.A1 A2 A3 A4 SVM B-SVMST1 0.62 ?
0.02 0.63 ?
0.02 0.64 ?
0.01 0.67 ?
0.02 0.59 ?
0.03 0.61 ?
0.03ST2 0.55 ?
0.01 0.58 ?
0.01 0.59 ?
0.01 0.59 ?
0.01 0.54 ?
0.01 0.57 ?
0.01ST3 0.65 ?
0.02 0.67 ?
0.01 0.67 ?
0.01 0.69 ?
0.01 0.61 ?
0.03 0.64 ?
0.03ST4 0.57 ?
0.01 0.59 ?
0.01 0.59 ?
0.01 0.59 ?
0.01 0.50 ?
0.04 0.55 ?
0.02ST5 0.74 ?
0.01 0.74 ?
0.01 0.77 ?
0.02 0.77 ?
0.01 0.67 ?
0.02 0.72 ?
0.03ST6 0.68 ?
0.02 0.70 ?
0.01 0.71 ?
0.01 0.72 ?
0.01 0.63 ?
0.01 0.68 ?
0.02Table 4: Average execution time (secs) for each algo-rithm.
The time spent by algorithm A1 is similar to thetime spent by algorithm A2.
The time spent by algorithmA3 is similar to the time spent by algorithm A4.A1(?A2) A3(?
A4) SVM B-SVMST1 1,565 2,102 9,172 268,216ST2 2,086 2,488 11,284 297,556ST3 2,738 3,083 14,917 388,184ST4 847 1,199 6,188 139,100ST5 1,304 1,604 9,017 192,576ST6 1,369 1,658 9,829 196,9225 ConclusionsIn this paper we have introduced a novel EM ap-proach, which employs a highly incremental un-derlying classifier based on association rules, com-pletely avoiding work replication.
Further, twolabel-transition operations are allowed, enabling thecorrection of false-negatives and false-positives.
Weproposed four algorithms based on our EM ap-proach.
Our algorithms employ an entropy min-imization method, which finds the best transitionthreshold for each example in D. All these prop-erties make our algorithms appropriate for namedentity disambiguation under streaming data scenar-ios.
Our experiments involve Twitter data mention-ing ambiguous named entities.
These datasets wereobtained from real application scenarios and fromplatforms currently in operation.
We have shownthat three of our algorithms achieve significantlyhigher disambiguation performance when comparedagainst a strong baseline (B-SVM), providing gainsranging from 1% to 20%.
Also importantly, forlarge-scale streaming data, our algorithms are morethan 120 times faster than the baseline.6 AcknowledgmentsThis research is supported by InWeb ?
The Brazil-ian National Institute of Science and Technology forthe Web (CNPq grant no.
573871/2008-6), by UOL(www.uol.com.br) through its UOL Bolsa Pesquisaprogram (process number 20110215172500), and bythe authors?
individual grants from CAPES, CNPqand Fapemig.822ReferencesR.
Agrawal, T. Imielinski and A. Swami.
1993.
Min-ing association rules between sets of items in largedatabases.
In Proceedings of the 18th ACM SIGMODInternational Conference on Management of Data,Washington, D.C., pages 207?216.A.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.In Proceedings of the 17th International Conferenceon Computational Linguistics, Montreal, Canada,pages 79?85.R.
Bekkerman and A. McCallum.
2005.
Disambiguat-ing web appearances of people in a social network.
InProceedings of the 14th International Conference onthe World Wide Web, Chiba, Japan, pages 463?470.I.
Bhattacharya and L. Getoor.
2007.
Collective entityresolution in relational data.
ACM Transactions onKnowledge Discovery from Data, 1.R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of the 11st Conference of the European Chap-ter of the Association for Computational Linguistics,Proceedings of the Conference, Trento, Italy, pages 9?16.F.
De Comite?, F. Denis, R. Gilleron and F. Letouzey.1999.
Positive and unlabeled examples help learning.In Proceedings of the 10th International Conferenceon Algorithmic Learning Theory, Tokyo, Japan, pages219?230.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedings ofthe 4th Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning, Prague, Czech Republic, pages708?716.M.
G. de Carvalho, A. H. F. Laender, M. A. Gonc?alves,and A. S. da Silva.
2006.
Learning to deduplicate.Proceedings of the 6th ACM/IEEE Joint Conference onDigital Libraries, Chapel Hill, NC, USA.
pages 41?50.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EM al-gorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.F.
Denis.
1998.
PAC learning from positive statisticalqueries.
In Proceedings of the Algorithmic LearningTheory, 9th International Conference, Otzenhausen,Germany, pages 112?126.X.
Dong, A. Y. Halevy, and J. Madhavan.
2005.
Refer-ence reconciliation in complex information spaces.
InProceedings of the 24th ACM SIGMOD InternationalConference on Management of Data, Baltimore, USA,pages 85?96.X.
Han and J. Zhao.
2009.
Named entity disambigua-tion by leveraging wikipedia semantic knowledge.
InProceedings of the 18th ACM conference on Informa-tion and knowledge management, Hong Kong, China,pages 215?224.T.
Hasegawa, S. Sekine and R. Grishman.
2004.
Dis-covering Relations among Named Entities from LargeCorpora.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain, pages 415?422.J.
Hoffart, M. Yosef, I. Bordino, H. Fu?rstenau, M. Pinkal,M.
Spaniol, B. Taneva, S. Thater and G. Weikum.2011.
Robust Disambiguation of Named Entities inText.
In Proceedings of the 8th Conference on Empir-ical Methods in Natural Language Processing, Edin-burgh, UK, pages 782?792.B.
J. Jansen, M. Zhang, K. Sobel, and A. Chowdury.2009.
Twitter power: Tweets as electronic word ofmouth.
JASIST, 60(11):2169?2188.T.
Joachims.
2006.
Training linear SVMs in linear time.In Proceedings of the 12th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining, Philadelphia, USA, pages 217?226.F.
Letouzey, F. Denis, and R. Gilleron.
2000.
Learningfrom positive and unlabeled examples.
In Proceedingsof the 11th International Conference on AlgorithmicLearning Theory, Sydney, Australia, pages 71?85.X.
Li and B. Liu.
2003.
Learning to classify texts us-ing positive and unlabeled data.
In Proceedings of the18th International Joint Conference on Artificial Intel-ligence, Acapulco, Mexico, pages 587?592.B.
Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu.
2003.Building text classifiers using positive and unlabeledexamples.
In Proceedings of the 3rd IEEE Interna-tional Conference on Data Mining, Melbourne, USA,pages 179?188.X.
Liu, S. Zhang, F. Wei and M. Zhou 2011.
Recogniz-ing Named Entities in Tweets.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,Portland, Oregon, USA, pages 359?367.E.
Minkov, W. W. Cohen, and A. Y. Ng.
2006.
Contex-tual search and name disambiguation in email usinggraphs.
In Proceedings of the 29th International ACMSIGIR Conference on Research and Development inInformation Retrieval, Seattle, USA, pages 27?34.T.
Pedersen, A. Purandare, and A. Kulkarni.
2005.
Namediscrimination by clustering similar contexts.
In Pro-ceedings of the 6th International Conference on Com-putational Linguistics and Intelligent Text Processing,Mexico City, Mexico, pages 226?237.I.
S. Silva, J. Gomide, A. Veloso, W. Meira Jr. and R. Fer-reira 2011.
Effective sentiment stream analysis with823self-augmenting training and demand-driven projec-tion.
In Proceedings of the 34th International ACMSIGIR Conference on Research and Development inInformation Retrieval, Beijing, China, pages 475?484.L.
Sarmento, A. Kehlenbeck, E. Oliveira, and L. Ungar.2009.
An approach to web-scale named-entity dis-ambiguation.
In Proceedings of the 6th InternationalConference on Machine Learning and Data Mining inPattern Recognition, Leipzig, Germany, pages 689?703.A.
Veloso, W. Meira Jr., M. de Carvalho, B. Po?ssas,S.
Parthasarathy, and M. J. Zaki.
2002.
Mining fre-quent itemsets in evolving databases.
In Proceedingsof the Second SIAM International Conference on DataMining, Arlington, USA.A.
Veloso, W. Meira Jr., and M. J. Zaki.
2006.
Lazyassociative classification.
In Proceedings of the 6thIEEE International Conference on Data Mining, HongKong, China, pages 645?654.X.
Wan, J. Gao, M. Li, and B. Ding.
2005.
Person reso-lution in person search results: Webhawk.
In Proceed-ings of the 14th ACM International Conference on In-formation and Knowledge Management, Bremen, Ger-many, pages 163?170.M.
Yosef, J. Hoffart, I. Bordino, M. Spaniol andG.
Weikum 2011.
AIDA: An Online Tool for Ac-curate Disambiguation of Named Entities in Text andTables.
PVLDB, 4(12):1450?1453.824
