Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1?4,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLanguage Dynamics and Capitalization using Maximum EntropyFernando Batistaa,b, Nuno Mamedea,c and Isabel Trancosoa,ca L2F ?
Spoken Language Systems Laboratory - INESC ID LisboaR.
Alves Redol, 9, 1000-029 Lisboa, Portugalhttp://www.l2f.inesc-id.pt/b ISCTE ?
Instituto de Ci?ncias do Trabalho e da Empresa, Portugalc IST ?
Instituto Superior T?cnico, Portugal.
{fmmb,njm,imt}@l2f.inesc-id.ptAbstractThis paper studies the impact of written lan-guage variations and the way it affects the cap-italization task over time.
A discriminativeapproach, based on maximum entropy mod-els, is proposed to perform capitalization, tak-ing the language changes into consideration.The proposed method makes it possible to uselarge corpora for training.
The evaluation isperformed over newspaper corpora using dif-ferent testing periods.
The achieved resultsreveal a strong relation between the capital-ization performance and the elapsed time be-tween the training and testing data periods.1 IntroductionThe capitalization task, also known as truecasing(Lita et al, 2003), consists of rewriting each wordof an input text with its proper case information.The capitalization of a word sometimes depends onits current context, and the intelligibility of texts isstrongly influenced by this information.
Differentpractical applications benefit from automatic capi-talization as a preprocessing step: when applied tospeech recognition output, which usually consistsof raw text, automatic capitalization provides rele-vant information for automatic content extraction,named entity recognition, and machine translation;many computer applications, such as word process-ing and e-mail clients, perform automatic capital-ization along with spell corrections and grammarcheck.The capitalization problem can be seen as a se-quence tagging problem (Chelba and Acero, 2004;Lita et al, 2003; Kim and Woodland, 2004), whereeach lower-case word is associated to a tag that de-scribes its capitalization form.
(Chelba and Acero,2004) study the impact of using increasing amountsof training data as well as a small amount of adap-tation.
This work uses a Maximum Entropy MarkovModel (MEMM) based approach, which allows tocombine different features.
A large written news-paper corpora is used for training and the test dataconsists of Broadcast News (BN) data.
(Lita et al,2003) builds a trigram language model (LM) withpairs (word, tag), estimated from a corpus with caseinformation, and then uses dynamic programming todisambiguate over all possible tag assignments on asentence.
Other related work includes a bilingualcapitalization model for capitalizing machine trans-lation (MT) outputs, using conditional random fields(CRFs) reported by (Wang et al, 2006).
This workexploits case information both from source and tar-get sentences of the MT system, producing betterperformance than a baseline capitalizer using a tri-gram language model.
A preparatory study on thecapitalization of Portuguese BN has been performedby (Batista et al, 2007).One important aspect related with capitalizationconcerns the language dynamics: new words are in-troduced everyday in our vocabularies and the usageof some other words decays with time.
Concerningthis subject, (Mota, 2008) shows that, as the timegap between training and test data increases, the per-formance of a named tagger based on co-training(Collins and Singer, 1999) decreases.This paper studies and evaluates the effects of lan-guage dynamics in the capitalization of newspaper1corpora.
Section 2 describes the corpus and presentsa short analysis on the lexicon variation.
Section 3presents experiments concerning the capitalizationtask, either using isolated training sets or by retrain-ing with different training sets.
Section 4 concludesand presents future plans.2 Newspaper CorpusExperiments here described use the RecPub news-paper corpus, which consists of collected editionsof the Portuguese ?P?blico?
newspaper.
The corpuswas collected from 1999 to 2004 and contains about148Million words.
The corpus was split into 59 sub-sets of about 2.5 Million words each (between 9 to11 per year).
The last subset is only used for testing,nevertheless, most of the experiments here describeduse different training and test subsets for better un-derstanding the time effects on capitalization.
Eachsubset corresponds to about five weeks of data.2.1 Data AnalysisThe number of unique words in each subset isaround 86K but only about 50K occur more thanonce.
In order to assess the relation between theword usage and the time gap, we created a numberof vocabularies with the 30K more frequent wordsappearing in each training set (roughly correspondsto a freq > 3).
Then, the first and last corpora subsetswere checked against each one of the vocabularies.Figure 1 shows the correspondent results, revealingthat the number of OOVs (Out of VocabularyWords)decreases as the time gap between the train and testperiods gets smaller.???????????????????????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
???????????
?????????
???
?Figure 1: Number of OOVs using a 30K vocabulary.3 CapitalizationThe present study explores only three ways ofwriting a word: lower-case, all-upper, and first-capitalized, not covering mixed-case words such as?McLaren?
and ?SuSE?.
In fact, mixed-case wordsare also being treated by means of a small lexicon,but they are not evaluated in the scope of this paper.The following experiments assume that the capi-talization of the first word of each sentence is per-formed in a separated processing stage (after punc-tuation for instance), since its correct graphical formdepends on its position in the sentence.
Evaluationresults may be influenced when taking such wordsinto account (Kim and Woodland, 2004).The evaluation is performed using the met-rics: Precision, Recall and SER (Slot Error Rate)(Makhoul et al, 1999).
Only capitalized words (notlowercase) are considered as slots and used by thesemetrics.
For example: Precision is calculated by di-viding the number of correct capitalized words bythe number of capitalized words in the testing data.The modeling approach here described is discrim-inative, and is based on maximum entropy (ME)models, firstly applied to natural language problemsin (Berger et al, 1996).
An ME model estimatesthe conditional probability of the events given thecorresponding features.
Therefore, all the infor-mation must be expressed in terms of features ina pre-processing step.
Experiments here describedonly use features comprising word unigrams and bi-grams: wi (current word), ?wi?1, wi?
and ?wi, wi+1?(bigrams).
Only words occurring more than oncewere included for training, thus reducing the numberof misspelled words.
All the experiments used theMegaM tool (Daum?
III, 2004), which uses conju-gate gradient and a limited memory optimization oflogistic regression.
The following subsections de-scribe the achieved results.3.1 Isolated TrainingIn order to assess how time affects the capitalizationperformance, the first experiments consist of pro-ducing six isolated language models, one for eachyear of training data.
For each year, the first 8 sub-sets were used for training and the last one was usedfor evaluation.
Table 1 shows the correspondingcapitalization results for the first and last testing sub-2Train 1999-12 test set 2004-12 test setPrec Rec SER Prec Rec SER1999 94% 81% 0.240 92% 76% 0.2962000 94% 81% 0.242 92% 77% 0.2912001 94% 79% 0.262 93% 76% 0.2912002 93% 79% 0.265 93% 78% 0.2772003 94% 77% 0.276 93% 78% 0.2732004 93% 77% 0.285 93% 80% 0.264Table 1: Using 8 subsets of each year for training.??????????????????
???
???
???
???
???????????????????????
????????
?Figure 2: Performance for different training periods.sets, revealing that performance is affected by thetime lapse between the training and testing periods.The best results were always produced with nearbythe testing data.
A similar behavior was observed onthe other four testing subsets, corresponding to thelast subset of each year.
Results also reveal a degra-dation of performance when the training data is froma time period after the evaluation data.Results from previous experiment are still worsethan results achieved by other work on the area(Batista et al, 2007) (about 94% precision and 88%recall), specially in terms of recall.
This is causedby a low coverage of the training data, thus reveal-ing that each training set (20Million words) does notprovide sufficient data for the capitalization task.One important problem related with this discrim-inative approach concerns memory limitations.
Thememory required increases with the size of the cor-pus (number of observations), preventing the useof large corpora, such as RecPub for training, withEvaluation Set Prec Rec SER2004-12 test set 93% 82% 0.233Table 2: Training with all RecPub training data.Checkpoint LM #lines Prec Rec SER1999-12 1.27 Million 92% 77% 0.2902000-12 1.86 Million 93% 79% 0.2662001-12 2.36 Million 93% 80% 0.2572002-12 2.78 Million 93% 81% 0.2472003-12 3.10 Million 93% 82% 0.2362004-08 3.36 Million 93% 83% 0.225Table 3: Retraining from Jan. 1999 to Sep. 2004.available computers.
For example, four millionevents require about 8GB of RAM to process.
Thisproblem can be minimized using a modified train-ing strategy, based on the fact that scaling the eventby the number of occurrences is equivalent to multi-ple occurrences of that event.
Accordingly to this,our strategy to use large training corpora consistsof counting all n-gram occurrences in the trainingdata and then use such counts to produce the cor-responding input features.
This strategy allows usto use much larger corpora and also to remove lessfrequent n-grams if desired.
Table 2 shows the per-formance achieved by following this strategy withall the RecPub training data.
Only word frequen-cies greater than 4 were considered, minimizing theeffects of misspelled words and reducing memorylimitations.
Results reveal the expected increase ofperformance, specially in terms of recall.
However,these results can not be directly compared with pre-vious work on this subject, because of the differentcorpora used.3.2 RetrainingResults presented so far use isolated training.
A newapproach is now proposed, which consists of train-ing with new data, but starting with previously cal-culated models.
In other words, previously trainedmodels provide initialized models for the new train.As the training is still performed with the new data,the old models are iteratively adjusted to the newdata.
This approach is a very clean framework forlanguage dynamics adaptation, offering a number ofadvantages: (1) new events are automatically con-sidered in the new models; (2) with time, unusedevents slowly decrease in weight; (3) by sorting thetrained models by their relevance, the amount of dataused in next training stage can be limited withoutmuch impact in the results.
Table 3 shows the re-3??????????????????????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
????
??????????????????????????
??????????
?Figure 3: Training forward and backwardssults achieved with this approach, revealing higherperformance as more training data is available.The next experiment shows that the training or-der is important.
In fact, from previous results, theincrease of performance may be related only withthe number of events seen so far.
For this reason,another experiment have been performed, using thesame training data, but retraining backwards.
Corre-sponding results are illustrated in Figure 3, revealingthat: the backwards training results are worse thanforward training results, and that backward trainingresults do not allways increase, rather stabilize af-ter a certain amount of data.
Despite the fact thatboth training use all training data, in the case of for-ward training the time gap between the training andtesting data gets smaller for each iteration, while inthe backwards training is grows.
From these resultswe can conclude that a strategy based on retrainingis suitable for using large amounts of data and forlanguage adaptation.4 Conclusions and Future WorkThis paper shows that maximum entropy modelscan be used to perform the capitalization task, spe-cially when dealing with language dynamics.
Thisapproach provides a clean framework for learningwith new data, while slowly discarding unused data.The performance achieved is almost as good as us-ing generative approaches, found in related work.This approach also allows to combine different datasources and to explore different features.
In termsof language changes, our proposal states that differ-ent capitalization models should be used for differ-ent time periods.Future plans include the application of this workto BN data, automatically produced by our speechrecognition system.
In fact, subtitling of BN has ledus into using a baseline vocabulary of 100K wordscombined with a daily modification of the vocabu-lary (Martins et al, 2007) and a re-estimation of thelanguage model.
This dynamic vocabulary providesan interesting scenario for our experiments.AcknowledgmentsThis work was funded by PRIME National ProjectTECNOVOZ number 03/165, and FCT projectCMU-PT/0005/2007.ReferencesF.
Batista, N. J. Mamede, D. Caseiro, and I. Trancoso.2007.
A lightweight on-the-fly capitalization systemfor automatic speech recognition.
In Proc.
of theRANLP 2007, Borovets, Bulgaria, September.A.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.C.
Chelba and A. Acero.
2004.
Adaptation of maxi-mum entropy capitalizer: Little data can help a lot.EMNLP04.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In Proc.
of the JointSIGDAT Conference on EMNLP.H.
Daum?
III.
2004.
Notes on CG and LM-BFGS opti-mization of logistic regression.J.
Kim and P. C. Woodland.
2004.
Automatic capitalisa-tion generation for speech input.
Computer Speech &Language, 18(1):67?90.L.
V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.2003.
tRuEcasIng.
In Proc.
of the 41st annual meet-ing on ACL, pages 152?159, Morristown, NJ, USA.J.
Makhoul, F. Kubala, R. Schwartz, and R. Weischedel.1999.
Performance measures for information extrac-tion.
In Proceedings of the DARPA Broadcast NewsWorkshop, Herndon, VA, Feb.C.
Martins, A. Teixeira, and J. P. Neto.
2007.
Dynamiclanguage modeling for a daily broadcast news tran-scription system.
In ASRU 2007, December.Cristina Mota.
2008.
How to keep up with languagedynamics?
A case study on Named Entity Recognition.Ph.D.
thesis, IST / UTL.Wei Wang, Kevin Knight, and Daniel Marcu.
2006.
Cap-italizing machine translation.
In HLT-NAACL, pages1?8, Morristown, NJ, USA.
ACL.4
