AUTOMATIC  LEX ICON ENHANCEMENT BY  MEANS OFCORPUS TAGGINGFrdddr ic  Bdchet  , Th ier ry  Spr ie t  , Marc  E1-B~zeLaboratoire Informat ique d 'Avignon LIA - CER I339, chemin des Meinajaries - BP 122884911 Avignon Cedex 9 - Francefrederic, bechet@univ-avignon, frAbst rac tUsing specialised text corpus to automati-cally enhance a general lexicon is the aim ofthis study.
Indeed, having lexicons whichoffer maximal cover on a specific topic is animportant benefit in many applications ofAutomatic Speech and Natural LanguageProcessing.
The enhancement of these lex-icons can be made automatic as big corporaof specialised texts are available.A syntactic tagging process, based on 3-class and 3-gram language models, allowsus to automatically allocate possible syn-tactic categories to the Out-Of-Vocabulary(OOV) words which are found in the cor-pus processed.
These OOV words generallyoccur several times in the corpus, and anumber of these occurrences can be impor-tant.
By taking into account all the occur-rences of an OOV word in a given text asa whole, we propose here a method for au-tomatically extracting a specialised lexiconfrom a text corpus which is representativeof a specific topic.1 IntroductionWith both Automatic Speech Processing and Nat-ural Language Processing it is necessary to use alexicon which associates each item with a certainnumber of characteristics ( yntactic, morphologic,frequency, phonetic, etc.).
In Speech Recognition,these lexicons are necessary in the lexical accessphases and the language modelisation as they allowthe association between lexical items and recognisedsounds while maintaining syntactic oherence withinthe sentence under analysis.
In Speech Synthesis,the grapheme-to-phoneme transcription phase usesmorphological nd syntactical information to con-stjrain the phonetic transcription of the graphemes.In both cases, using lexicons which have the maxi-mum information about the subject is an importantbenefit.The actual performance of Automatic SpeechTreatment systems often limits their application tosmaller subject-areas of language (medical texts,economic articles, etc.).
It is important to have spe-cialised lexicons which cover these smaller subject-areas in order to optimise the synthesis or recogni-tion applications.
But although general lexicons arereadily available now, this is not the case for spe-cialised lexicons which contain, for example, techni-cal terms relevant to a subject, or family and brandnames as can be found in journalistic texts.When working with corpora we are faced bythe evolutionary aspects of a given language.
Thequicker the evolution of a specialised area, the morethe dictionary will lack the ability to cover the sub-ject, because a dictionary represents the state ofa language at a given time.
The words missingfrom a lexicon (which we refer to here as Out-Of-Vocabulary words or OOV words) represent a signif-icant problem.
In effect, whatever the size of the lex-icon used, one can always find OOV words in texts.If, for a given word, the lexical access fails, this fail-ure can affect the processing of the word as well asthe processing of the contextual words.It would be useful to have dynamic lexicons whichevolve in accordance with the corpora processed inorder to limit, as much as possible, the OOV words.Such an enhancement of lexicons could be automaticif big corpora of specialised texts were available :medical reports in an electronic form, newspaperavailable in CD-ROM, etc.This interesting idea of automatically enhancingspecialised lexicons from a general lexicon and a bigcorpus, is the aim of this paper.
By using statisti-cal language models, we show how to automaticallyassign one or several categories to the OOV wordswhich are found in our corpora.
Then, by taking29into account all the occurences of each OOV word,we are able to automatically extract a new lexiconof OOV word with reliable labels associated to eachword.2 P rocess ing  OOV wordsVarious applications at LIA need a large lexicon,such as the automatic generation of graphical ac-cents in a French text, language models for a dicta-tion machine, the grapheme-to-phoneme transcrip-tion system, etc.
As most of these applications pro-cess text corpus, the lexicon is mainly used througha syntactic labelling system developed at the labora-tory (E1-Btze, 1995).
This tagging system is basedon a 3-class probabilistic language model which hasbeen trained on a corpus of 39 million words con-tained in articles of the french newspaper Le Monde.The lexicon used is composed of 230 000 items.The use of a big general dictionary allows us tolimit most of the OOV words to one of these cate-gories : proper names, composit words, unused flex-ions, neologisms, mistakes.
The problem of missingroots becomes important when the texts processedbelong to a different area than the one used duringthe building of the lexicon.
This is the case in cor-pus dedicated to sub-areas of language, such as intechnical documentation, forexample.Previous studies (Ueberla, 1995; Maltese, 1991)show that the modelling of OOV words improvessignificantly the performance of a language model.The presence of OOV words in the corpus can pro-duce errors, not only in the form itself, but also in itscontext in the sentence.
This is the reason why thesyntactic tagging system has been endowed with amodule, called Devin (Spriet, 1996), which proposesa category for each OOV word that is found.The modules described here take into account allthe simple OOV words, which are those composedwith only alphabetical characters (no space, hyphen,digits, or special characters).
A specific module ded-icated to composite words is currently being devel-oped.
We classify these simple OOV words in twocategories : the "proper-names", and the "common-words" which represent all the others !
By applyingsimple heuristics to a sentence we can separate theOOV words into proper-names and common-words.3 P rocess ing  OOV common-wordsw i th  the  morpho-syntact i c  Dev in3.1 Out-of-context processThe goal of this module is to give a probabil-ity to syntactic labels which can represent heOOV common-words.
These labels are distributedamongst 21 syntactic lasses (adverbs, adjectives,names, verbs).
It is commonly accepted that theending of a word belonging to one of these classesinfluences trongly its syntactic ategory (Vergne,1989; Guillet, 1989).
Using this idea, we trained astatistical model with all the words from our dictio-nary.
We make the hypothesis that this model willcorrectly work on unknown words, since these wordsshould be governed by the same morphological prin-ciples.The approach chosen is based on decision-trees(Breiman, 1984).
An out-of-context evaluation ofthe morpho-syntactic Devin is presented in (Spriet,1996).3.2 Context analysisThe context analysis of OOV words permits thechoice, from all the possible categories proposed bythe Devin, of the one which best fits with the con-text of the OOV word.
The hypotheses produced foreach OOV word are inserted in the graph of possiblecategories generated by the language model.
The 3-class analysis allows us to find the label which hasthe best probability.We decided to test the module on a corpus con-taining "forced" OOV words.
This means that wevoluntarily removed from the lexicon a set of testwords.
The text corpus chosen contained 313 690words of which 10 850 were "forced" OOV words(these 10 850 occurrences represent 3430 differentforms).In the first stage, we labelled this corpus withoutusing the Devin.
1771 errors of context (as comparedto the initial reference) were induced by the additionof 10 850 OOV words.
Then we labelled again thesame corpus, this time using the Devin.
88.3% ofOOV words were correctly labelled (as compared tothe initial reference) and 86.2% of induced contex-tual errors were corrected ue to attributing a syn-tactic category to each OOV word.
Thus, 87.5% oflabelling differences with the initial reference werecorrected by using the Devin.It is important to point out that this type of evalu-ation does not take into account the errors which areintrinsic to the tagging system employed (about 4%as mentioned in (EI-B~ze, 1995)).
Indeed, the syn-tactic categories calculated by the Devin were com-pared to those produced by the tagger when thesewords belonged to the lexicon.
Nevertheless the ben-efit of this technique is that it is automatic, whichallows us to test our module on an important corpusof tests.
A manual verification of a small corpus of"true" OOV words has also been carried out (Spriet,1996), the results are appreciably similar.304 Proper -names  processThe second category of OOV words represents theforms which have been identified as proper-names.We separate these words into the following classes :family name, first name, town name, company name,country name.
It is not possible to simply makea morphological module which allows us to processproper-names.
Thus, the estimation of an out-of-context probability for each of these classes is inde-pendent of the graphical form of the proper-names.It is therefore the consideration of the context hatallows us to attribute a reliable probability to thelikelihood of an OOV proper-name belonging to aspecific class.
We present here a method based ona statistic 3-class model dedicated to OOV propernames.4.1 Contextual Tagging using the Devin forproper-namesThe general 3-class language model is, most of thetime, unable to choose between the different cate-gories of proper-names.
In fact, when you have todecide whether an OOV word is a family name ora town name, the word-context of the OOV wordis more useful than its syntactic-class-context.
A 3-gram model seems natural for solving this problem.But, because we want to process OOV words, weuse a 3-gram model specific to proper names wheresome categories of words are represented by theirclasses (all the proper names as well as punctuationand non-alphabetical words) while others are repre-sented by their graphical form (all the other classes).In the labelling process, when an OOV proper-name Xi appears at position i in the sentence, thelabel which is given to Xi represents the class whichmaximize P(t/Xi), the probability of Xi belongingto the class t.---- Arg max P(t/Mi ... Xi ...M,~)t-- Argmax Pt(M,.
.
.t .
.
.
/PI~)~i  P(M,.. .
j .
.
.
M,)We carried out similar experiments to those pre-sented above.
The test corpus was the same andwe voluntarily removed 970 proper-names from thelexicon, which represented 5000 occurrences in thecorpus.
86% of the OOV words had been correctlytagged by the proper-names language model.It is important o point out that the average num-ber of classes which can be attributed to a proper-name is very close to 1 (1.07 in our test corpus and1.08 in the general exicon).
This shows that thecomparison between the reference labels and the la-bels calculated is a true evaluation.5 Automat ic  lex icon  product ionIn studying all the occurrences, in all their contexts,of the OOV words of a corpus, we aim to automati-cally obtain new lexicons which represent the corpusstudied.As we have mentioned already, the syntactic tag-ger used was trained on a journalistic text corpusfrom the newspaper Le Monde.
The test corpus cho-sen to validate our automatic lexicon enhancementmethod was composed with articles of the newspa-per Le Monde Diplomatique from 1990 until 1995.This 6-million-word corpus contains a large amountof proper-names and technical terms relative to var-ious subjects.The test corpus contains 110 000 OOV words com-posed as follows :?
22 766 OOV common-words (20.7%)?
63 194 OOV proper-names (57.4%)?
24 040 OOV composite words (21.8%)The lack of static coverage of our general exiconis 1.85% (0.38% for the OOV common-words and1.06% for the OOV proper-names).By tagging the corpus using Devin modules (forcommon-words and proper-names) we are able to au-tomatically extract a lexicon of OOV words whichcontains, for each word, its number of occurrences aswell as the list of labels which have been attributedto it during the tagging process.
The list of labelsgiven to each word of the lexicon is classified by fre-quency, as shown in the example below.OOV word Nb C1 C2 C3 C4tchdtch~ne 41 AFS AMS NFS ~NMS, 54% 32% 8% 6%This frequency information allows us to filter thelexicon according to 2 criteria : number of oc-curences of each word ; percentage of occurences foreach label given to a word.5.1 Lexicon of common-wordsFor the OOV common-words, we reduce the lexiconto the words which have at least 4 occurences inthe corpus, then we keep, for each word, only thesyntactic labels which represent 80% of all the oc-curences of the word.
We obtain a lexicon of 1032items representing 44% of all the occurences of OOVcommon-words in our corpus.315.2 Lexicon of proper -namesThe lexicon of OOV proper-names is limited to thewords which have at least 4 occurences in the cor-pus and for which the most frequent label has a fre-quency of at least 90%.
Then we keep, for each word,only the most frequent label.
The lexicon contains2250 words representing 28.5% of all the occurencesof OOV proper-names in our corpus.5.3 ResultsWe verified manually the first 1000 most frequentOOV words of each filtered lexicon.
The results arepresented as follows : Table 1 shows, in the column"Correct", the percentage of OOV words where allthe labels were correct ; the column "Wrong" indi-cates the percentage of words which were labelledwith at least one incorrect ag.Table 1 Correct WrongCommon-words 95.6% 4.4%Proper-names 92.4% 7.6%Table 2 details, for the common-words lexicon, theresults obtained on the correct words.
The column"All classes" shows the percentage of correct wordswhich had all their possible syntactic ategories inthe lexicon.
The column "Missing classes" indicatesthe percentage of correct words which could havereceived more syntactic ategories than those storedin the lexicon.Table 2 \] All classes I Missing classesCommon-words I 79% \] 21%These results show that the criteria used to fil-ter the OOV lexicons allows us to produce reliablelexicons (only 4% of the OOV common-words con-tained label errors).
By keeping the 1000 most fre-quent words of each lexicon, we reduced by 20% thelack of coverage of our general lexicon on all the testcorpus.6 Conclus ionThe aim of this study was the automatic productionof a lexicon from corpus dedicated to some specificareas.
The results obtained satisfy this goal.
In-deed, taking into account all the occurrences of theunknown words of a text corpus permits us to auto-matically produce lexicons containing, for each en-try, a list of possible syntactic lasses with frequencyinformation.The integration of these lexicons within a linguis-tic module, points out the problem of the dynamicadaptation of the language model.
This shouldbe dealt with by means of a cache-based languagemodel (Kuhn, 1990).
The resultant lexicons pro-duced contain very few incorrect syntactic lassesfor each item which is represented in the corpus bya sufficient number of occurrences.This lexicon-extraction module has been usedwithin the Text-To-Speech system developed atLIA : before the grapheme-to-phoneme transcriptionphase, we first extract a lexicon of all the OOV wordsof the text to process.
Then, we add this lexicon toour general lexicon and we use the syntactic labelsgiven to each word to constrain the grapheme-to-phoneme transcription rules as well as the liaison-generation rules.Finally, it is important o point out that the ap-proach chosen in this study remains independent ofthe processed language, as long as the hypothesesmade by the morpho-syntactic Devin are satisfied.Re ferencesBreiman L., Friedman J., Olshen It., Stone C. 1984.Classification and Itegression Trees WadsworthInc.EI-B~ze M., Sprier T. 1995.
Integration de Con-traintes Syntaxiques dans un Systeme d'Etique-tage Probabifiste In TAL , Vol.
6 N 1-2.Guillet A.
1989.
Reconnaissance d s formes verbalesavec un dictionnaire minimal In Revue Langage ,Paris.Kuhn It., De Mori It.
1990.
A Cache-Based NaturalLanguage Model for Speech Recognition In IEEE, Vol.
12 N.6, Juin 1990.Maltese G., Mancini F. 1991.
A technique to auto-matically assign parts-of-speech to words takinginto account word-ending information through aprobabilistic model In Eurospeech 91 Genova, pp.753-756.Spriet T., B~chet F., EI-B~ze M., de Loupy C,Khouri L. 1996.
Traitement Automatique desMots Inconnus In TALN 96, Marseille juin 1996.Ueberla J.P. 1995.
Analysing weaknesses of lan-guage models for speech recognition In ICASSP1995, pp.
205-208.Vergne J.
1989.
Analyse morpho-syntaxique au-tomatique sans dictionnaire These de doctorat del'Universite de Paris 6, 8 juin 1989.32
