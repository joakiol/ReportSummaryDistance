Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 371?378, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Semantic Approach to Recognizing Textual EntailmentMarta Tatu and Dan MoldovanLanguage Computer CorporationRichardson, TX 75080, USAmarta,moldovan@languagecomputer.comAbstractExhaustive extraction of semantic infor-mation from text is one of the formidablegoals of state-of-the-art NLP systems.
Inthis paper, we take a step closer to thisobjective.
We combine the semantic in-formation provided by different resourcesand extract new semantic knowledge toimprove the performance of a recognizingtextual entailment system.1 Recognizing Textual EntailmentWhile communicating, humans use different expres-sions to convey the same meaning.
Therefore, nu-merous NLP applications, such as, Question An-swering, Information Extraction, or Summarizationrequire computational models of language that rec-ognize if two texts semantically overlap.
Trying tocapture the major inferences needed to understandequivalent semantic expressions, the PASCAL Net-work proposed the Recognizing Textual Entailment(RTE) challenge (Dagan et al, 2005).
Given two textfragments, the task is to determine if the meaning ofone text (the entailed hypothesis, H) can be inferredfrom the meaning of the other text (the entailing text,T ).Given the wide applicability of this task, thereis an increased interest in creating systems whichdetect the semantic entailment between two texts.The systems that participated in the Pascal RTEchallenge competition exploit various inference el-ements which, later, they combine within statisti-cal models, scoring methods, or machine learningframeworks.
Several systems (Bos and Markert,2005; Herrera et al, 2005; Jijkoun and de Rijke,2005; Kouylekov and Magnini, 2005; Newman etal., 2005) measured the word overlap between thetwo text strings.
Using either statistical or Word-Net?s relations, almost all systems considered lexicalrelationships that indicate entailment.
The degree ofsimilarity between the syntactic parse trees of thetwo texts was also used as a clue for entailment byseveral systems (Herrera et al, 2005; Kouylekov andMagnini, 2005; de Salvo Braz et al, 2005; Rainaet al, 2005).
Several groups used logic provers toshow the entailment between T and H (Bayer etal., 2005; Bos and Markert, 2005; Fowler et al,2005; Raina et al, 2005) and some of them madeuse of world knowledge axioms to increase the logicprover?s power of inference (Bayer et al, 2005; Bosand Markert, 2005; Fowler et al, 2005).In this paper, we describe a novel technique whichemploys a set of semantic axioms in its attempt toexhaustively extract semantic knowledge from texts.In order to show the contribution that our semanticinformation extraction method brings, we append itas an additional module to an already existing sys-tem that participated in the RTE challenge.
Our sys-tem (Fowler et al, 2005), first, transforms the textT and the hypothesis H into semantically enhancedlogic forms, and, then, the integrated logic provertries to prove or disprove the entailment using aset of world-knowledge axioms (die of blood loss?
bleed to death), linguistic rewriting rules whichbreak down complex syntactic structures, like co-ordinating conjunctions, and WordNet-based lexicalchains axioms (buy/VB/1 ?
pay/VB/1).3712 ApproachWe believe that a logic-based semantic approach ishighly appropriate for the RTE task1.
Text T seman-tically entails H if its meaning logically implies themeaning of H .
Because the set of semantic relationsencoded in a text represents its meaning, we need toidentify all the semantic relations that hold betweenthe constituents of T and, subsequently, between theconstituents of H to understand the meaning of eachtext.
It should be noted that state-of-the-art seman-tic parsers extract only some of the semantic rela-tions encoded in a given text.
To complete this in-formation, we need semantic axioms that augmentthe extracted knowledge and, thus, provide a bettercoverage of the text?s semantics.
Once we gatherthis information, we state that text T entails hypoth-esis H if and only if we find similar relations be-tween a concept from T and a semantically analo-gous concept from H .
By analogous concepts, wemean identical concepts, or words connected by achain of SYNONYMY, HYPERNYMY or morphologi-cal derivation relations in WordNet.Because the set of semantic elements identified bya semantic parser does not necessarily convey thecomplete meaning of a sentence, we shall use a setof semantic axioms to infer the missing pieces of in-formation.
By combining two semantic relations orby using the FrameNet?s frame elements identifiedin a given text, we derive new semantic information.In order to show if T entails H , we analyze theirmeanings.
Our approach to semantic entailment in-volves the following stages:1.
We convert each text into logic form (Moldovanand Rus, 2001).
This conversion includes part-of-speech tagging, parse tree generation, and name en-tity recognition.2.
Using our semantic parser, we identify some ofthe semantic relations encoded in the analyzed texts.We note that state-of-the-art semantic parsers can-not discover all the semantic relations conveyed im-plicitly or explicitly by the text.
This problem com-promises our system?s performance.
To obtain thecomplete set of semantic relations that represents themeaning of the given texts, we introduce a new stepin our algorithm.1After all, the entailment, inference, and equivalence termsoriginated from logic.3.
We add semantic axioms to the already createdset of world knowledge, NLP, and WordNet-basedlexical chain (Moldovan and Novischi, 2002) ax-ioms that assist the logic prover in its search forproofs.
We developed semantic axioms that showhow two semantic relations can be combined.
Thiswill allow the logic prover to combine, wheneverpossible, semantic instances in order to infer new se-mantic relationships.
The instances of relations thatparticipate in semantic combinations can be eitherprovided by the text or annotated between WordNetsynsets.
We also exploit other sources of semanticinformation from the text.
For example, the framesencoded in the text sentence provide informationwhich complements the meaning given by the se-mantic relations.
Our second type of axioms derivesemantic relations between the frame elements of agiven FrameNet frame.We claim that the process of applying the seman-tic axioms, given the semantic relations detected bya semantic parser, will capture the complete seman-tic information expressed by a text fragment.
In thispaper, we show the usefulness of this procedure forthe RTE task, but we are convinced that it can be usedby any system which plans to extract the entire se-mantic information from a given text.4.
We load the COGEX logic prover (Moldovanet al, 2003) which operates by ?reductio ad absur-dum?
with H?s negated form and T ?s predicates.These clauses are weighted in the order in whichthey should be chosen to participate in the search.To ensure that H will be the last clause to partici-pate, we assign it the largest value.
The logic proversearches for new inferences that can be made usingthe smallest weight clauses.
It also assigns a valueto each inference based on the axiom it used to de-rive it.
This process continues until the set of clausesis empty.
If a refutation is found, the proof is com-plete.
If a contradiction cannot be found, then thepredicate arguments are relaxed and, if the argumentrelaxation fails, then predicates are dropped until aproof by refutation is found.
Its score will be com-puted by deducting points for each argument relax-ation and predicate removal.
If this value falls belowa threshold, then T does not entail H .
Otherwise, the(T, H) pair is a true entailment.We present a textual entailment example to showthe steps of our approach.
This proof will not372T John and his son, George, emigrated with Mike, John?s uncle, to US in 1969.LFT John(x1) ?
son(x2) ?
George(x3) ?
ISA(x3, x2) ?
KIN(x1, x3) ?
emigrate(e1) ?
AGT(x1, e1) ?
AGT(x2, e1) ?
Mike(x4) ?uncle(x5) ?
ISA(x4, x5) ?
KIN(x1, x4) ?
US(x6) ?
LOC(e1, x6) ?
1969(x7) ?
TMP(e1, x7).TDeparting [John and his son, George,]T heme.fe emigrated [with Mike, John?s uncle,]Cotheme.fe to [US]Goal.fe in [1969]T ime.fe.TKinship [John]Ego.fe and his son, [George,]Alter.fe emigrated with [Mike]Alter.fe, [John]Ego.fe?s uncle, to US in 1969.TAxiom1 KIN(w1, w2)?
KIN(w2, w1)KIN(x1, x3)?
KIN(x3, x1) (KIN(John, George)?
KIN(George, John))TAxiom2 KIN ?
KIN = KIN (KIN(w1, w2) ?
KIN(w2, w3)?
KIN(w1, w3))KIN(x3, x1) ?
KIN(x1, x4)?
KIN(x3, x4) (KIN(George, Mike))TAxiom3 DEPARTING F ?
LOC(Theme.fe, Goal.fe) (LOC(John, US) ?
LOC(George, US))TAxiom4 DEPARTING F ?
LOC(Cotheme.fe, Goal.fe) (LOC(Mike, US))TSemantics KIN(John, George), KIN(John, Mike), KIN(George, Mike), LOC(John, US), LOC(George, US), LOC(Mike, US),TMP(emigrate, 1969), AGT(John, emigrate), AGT(George, emigrate)H George and his relative, Mike, came to America.LFH George(x1) ?
relative(x2) ?
Mike(x3) ?
ISA(x3, x2) ?
KIN(x1, x3) ?
come(e1) ?
AGT(x1, e1) ?
AGT(x2, e1) ?
America(x4)?
LOC(e1, x2)HArriving [George and his relative, Mike,]T heme.fe came to [America]Goal.fe.HKinship [George]Ego.fe and his relative, [Mike]Alter.fe, came to America.HAxiom1 ARRIVING F ?
LOC(Theme.fe, Goal.fe) (LOC(George, America) ?
LOC(Mike, America))HSemantics KIN(George, Mike), LOC(George, America), LOC(Mike, America)Table 1: Entailment proof example.
Table 2 lists the semantic relations and their abbreviations.
Sections 3.2and 4.1 will detail the semantics behind the axioms TAxiom1 , TAxiom2 , TAxiom3 , TAxiom4 , and HAxiom1 .make use of any world knowledge axioms.
Letthe text T be John and his son, George, emigratedwith Mike, John?s uncle, to US in 1969 and the en-tailed hypothesis H George and his relative, Mike,came to America.
Our system transforms eachtext into its corresponding semantically enhancedlogic form (LFT and LFH in Table 1).
Then,the logic prover uses the newly added semantic ax-ioms to derive extra semantic information from Tand H (for example, George and Mike are rela-tives, but T does not explicitly specify this), af-ter another preprocessing step which identifies theframe elements of each frame encoded in the twotexts (TDeparting , TKinship, HArriving, HKinship).In our example, the axioms TAxiom1 and TAxiom2denote the symmetry and the transitivity of the KIN-SHIP relation.
TAxiom3 , TAxiom4 and HAxiom1 arethe frame-related axioms used by the logic prover.The TSemantics and HSemantics rows (Table 1) sum-marize the meaning of T and H .
We note that half ofthese semantic instances were extracted using the se-mantic axioms.
Once the lexical chains between theconcepts in T and the ones from H are computed,the entailment becomes straightforward.
We repre-sented, graphically, the meaning of the two texts inFigure 1.
We also show the links between the analo-gous concepts that help prove the entailment.In the coming sections of the paper, we detail theprocess of semantic axiom generation.
We start witha summary of the axioms that combine two semanticrelations.Figure 1: TSemantics and HSemantics.
The solid ar-rows represent the relations identified by the seman-tic parser.
The dotted arrows symbolize the lexicalchains between concepts in T and their analogousconcepts in H (UST and AmericaH belong to thesame WordNet synset).
The dash arrows denote therelations inferred by combining two semantic rela-tions.
The long dash arrows indicate the relationsbetween frame elements.3 Semantic Calculus3.1 Semantic relationsFor this study, we adopt a revised version of the se-mantic relation set proposed by (Moldovan et al,2004).
Table 2 enumerates the semantic relationsthat we consider2.2See (Moldovan et al, 2004) for definitions and examples.373POSSESSION (POS) MAKE-PRODUCE (MAK) RECIPIENT (REC) THEME-PATIENT (THM)KINSHIP (KIN) INSTRUMENT (INS) FREQUENCY (FRQ) RESULT (RSL)PROPERTY-ATTRIBUTE (PAH) LOCATION-SPACE (LOC) INFLUENCE (IFL) STIMULUS (STI)AGENT (AGT) PURPOSE (PRP) ASSOCIATED WITH (OTH) EXTENT (EXT)TEMPORAL (TMP) SOURCE-FROM (SRC) MEASURE (MEA) PREDICATE (PRD)DEPICTION (DPC) TOPIC (TPC) SYNONYMY-NAME (SYN) CAUSALITY (CSL)PART-WHOLE (PW) MANNER (MNR) ANTONYMY (ANT) JUSTIFICATION (JST)HYPERNYMY (ISA) MEANS (MNS) PROBABILITY OF EXISTENCE (PRB) GOAL (GOL)ENTAIL (ENT) ACCOMPANIMENT (ACC) POSSIBILITY (PSB) BELIEF (BLF)CAUSE (CAU) EXPERIENCER (EXP) CERTAINTY (CRT) MEANING (MNG)Table 2: The set of semantic relations3.2 Combinations of two semantic relationsOur goal is to devise semantic axioms for combina-tions of two relations, R1 and R2, by observing thesemantic connection between the w1 and w3 wordsfor which there exists at least one other word, w2,such that R1(w1, w2) and R2(w2, w3) hold true3.Harabagiu and Moldovan (1998) tackled the prob-lem of semantic combinations, for the first time.Their set of relations included the WordNet1.5 anno-tations and 12 relationships derived from the Word-Net glosses4.
In our research, unlike (Harabagiu andMoldovan, 1998), the semantic combinations use therelations identified in text with a rather minimal con-tribution from the WordNet relations.Harabagiu and Moldovan (1998) also investi-gate the number of possible semantic combinations.Based on their properties, we can have up to eightcombinations between any two semantic relationsand their inverses, not counting the combinationsbetween a semantic relation and itself5.
For in-stance, given an asymmetric relation and a sym-metric one which share the same part-of-speech fortheir arguments, we can produce four combinations.ISA ?
ANT, ISA?1 ?
ANT, ANT ?
ISA, and ANT ?ISA?1 are the four possible distinct combinationsbetween HYPERNYMY and ANTONYMY.
???
sym-bolizes the semantic composition between two rela-tions compatible with respect to the part-of-speechof their arguments: for any two concepts, w1 and w3,(Ri?Rj)(w1, w3) if and only if ?w2, a third concept,such that Ri(w1, w2) and Rj(w2, w3) hold.
By R?1,3R(x, y) indicates that relation R holds between x and y.4This set includes the AGENT, OBJECT, INSTRUMENT, BEN-EFICIARY, PURPOSE, ATTRIBUTE, REASON, STATE, LOCA-TION, THEME, TIME, and MANNER relations.5Harabagiu and Moldovan (1998) lists the exact number ofpossible combinations for several WordNet relations and part-of-speech classes.we denote the inverse of relation R: if R(x, y), thenR?1(y, x).While analyzing the combinations, we observedsome regularities within the semantic compositionprocess.
For example, R?11 ?
R?12 = (R2 ?
R1)?1for any, not necessarily distinct, semantic relationsR1 and R26.
If one of the relations is symmet-ric (R?1 = R), the statement is still valid.
Using(R?1)?1 = R and the previous equality, we can re-duce by half the number of semantic combinationsthat we have to compute for R1 6= R2.We plan to create a 40 ?
40 matrix with all thepossible combinations between any two semanticrelations from the set we consider.
Theoretically,we can have up to 27,556 semantic combinations,but only 25.79% of them are possible7 (for exam-ple, MNR(r, v) and SYN(n, n) cannot be combined).Many combinations are not semantically significanteither because they are very rare, like, KIN(n, n)?
TMP(n, v), or because they do not result intoone of the 40 relations, for instance, PAH(a, n) ?AGT(n, v)8.
We identified two approaches to theproblem mentioned above.
The first tries to fill onematrix cell at a time in a consecutive manner.
Thesecond approach tries to solve the semantic combi-nations we come upon in text corpora.
As a result,we analyzed the RTE development corpus and we de-vised rules for some of the Ri?Rj combinations thatwe encountered.
We validated these axioms by man-6The equality holds only if the two composition terms exist.7On average, each semantic relation has 2.075 pairs of argu-ments.
For example, SRC can connect two nouns (US investor),or an adjective and a noun (American investor) and, dependingon its arguments, SRC will participate in different combinations.Out of the 27,556 combinations, only 7,109 are syntacticallypossible.8n, v, a, and r stand for noun, verb, adjective, and adverb,respectively.
As an example, R(n, n) means that relation R canconnect two nouns.374LOCATION ?
PART-WHOLE = LOCATION (LOCATION(x, l1) ?
PART-WHOLE(l1, l2) ?
LOCATION(x, l2))Example: John lives in Dallas, Texas.LOCATION(John, Dallas) and PART-WHOLE(Dallas, Texas) imply that LOCATION(John, Texas).ISA ?
ATTRIBUTE = ATTRIBUTE (ISA(x, y) ?
ATTRIBUTE(y, a) ?
ATTRIBUTE(x, a))Example: Mike is a rich man.If ISA(Mike, man) and ATTRIBUTE(man, rich), then ATTRIBUTE(Mike, rich).Similar statements can be made for other ?attributes?
: LOCATION, TIME, SOURCE, etc.ISA ?
LOCATION = LOCATION (ISA(x, y) ?
LOCATION(y, l) ?
LOCATION(x, l))Example: The man in the car, George, is an old friend of mine.ISA(George, man) and LOCATION(man, car) ?
LOCATION(George, car)KINSHIP ?
KINSHIP = KINSHIP (KINSHIP(x, y) ?
KINSHIP(y, z) ?
KINSHIP(x, z))See example in Section 2.THEME ?
ISA?1 = THEME (THEME(e, y) ?
ISA(x, y) ?
THEME(e, x))Example: Yesterday, John ate some fruits: an apple and two oranges.THEME(eat, fruit) ?
ISA(apple, fruit) ?
THEME(eat, apple)THEME ?
PART-WHOLE?1 = THEME (THEME(e, y) ?
PART-WHOLE(x, y) ?
THEME(e, x))Example: Five Israelis, including two children, were killed yesterday.THEME(kill, Israeli) ?
PART-WHOLE(child, Israeli) ?
THEME(kill, child)Similar statements can be made for all the thematic roles: AGENT, EXPERIENCER, INSTRUMENT, CAUSE, LOCATION, etc.AGENT ?
ISA?1 = AGENT (AGENT(e, y) ?
ISA(x, y) ?
AGENT(e, x))AGENT ?
PART-WHOLE?1 = AGENT (AGENT(e, y) ?
PART-WHOLE(x, y) ?
AGENT(e, x))Table 3: Examples of semantic combination axiomsually checking all the LA Times corpus (w1, w3)pairs which satisfy (Ri?Rj)(w1, w3).
We have iden-tified 64 semantic axioms that show how semanticrelations can be combined.
These axioms use re-lations such as PART-WHOLE, ISA, LOCATION, AT-TRIBUTE, or AGENT.
We listed several examplerules in Table 3.
The 64 axioms can be applied in-dependent of the concepts involved in the semanticcomposition.
We have also identified rules that canbe applied only if the concepts that participate sat-isfy a certain condition or if the relations are of acertain type.
For example, LOC ?
LOC = LOC only ifthe LOC relation shows inclusion (John is in the carin the garage ?
LOC(John, garage).
John is nearthe car behind the garage 6?
LOC(John, garage)).4 FrameNet Can HelpThe Berkeley FrameNet project9 (Baker et al, 1998)is a lexicon-building effort based on the theory offrame semantics which defines the meanings of lexi-cal units with respect to larger conceptual structures,called frames.
Individual lexical units point to spe-cific frames and establish a binding pattern to spe-cific elements within the frame.
FrameNet describesthe underlying frames for different lexical units andexamines sentences related to the frames using theBNC corpus.
The result is an XML database that9http://framenet.icsi.berkeley.educontains a set of frames, a set of frame elements foreach frame, and a set of frame annotated sentences.4.1 Frame-based semantic axiomsWith respect to a given target, the frame ele-ments contribute to the understanding of the sen-tence.
But they only link each argument to thetarget word (for example, THM(theme, target)or AGT(theme, target), LOC(place, target), etc.
).Often enough, we can find relations between theframe elements of a given frame.
These new in-stances of semantic relations take as arguments theframe elements of a certain frame, when they are ex-pressed in the text.
For example, given the DEPART-ING frame, we can say that the origin of the themeis the source (SRC(theme, source)) and that thenew location of the theme is the goal frame element(LOC(theme, goal)).
Moreover, if the text speci-fies the cotheme frame element, then we can makesimilar statements about it (SRC(cotheme, source)and LOC(cotheme, goal)).
These new relation in-stances increase the semantic information that canbe derived from text.So far, we manually inspected 54 frames and ana-lyzed the relationships between their frame elementsby examining their definitions and the annotated cor-pus provided with the FrameNet data.
For eachframe, we retained only the rules independent of the375CLOTHING PARTS F ?
PW(subpart, clothing)CLOTHING PARTS F ?
PW(material, subpart)Example: ?Hello, Hank?
they said from the depths of the [fur]Material [collars]Subpart,Target of [their]Wearer [coats]Clothing .PW(fur, collar) and PW(collar, coat)CLOTHING F ?
PAH(descriptor, garment) ?
PAH(descriptor, material)Example: She didn?t bring heels with her so she decided on [gold]Descriptor [leather]Material [flip-flops]Garment,Target.PAH(gold, leather) ?
PAH(gold, flip ?
flop)KINSHIP F ?
KIN(ego, alter)Example: The new subsidiary is headed by [Rupert Soames]Alter , [son]Target [of the former British Ambassador to France andEC vice-president]Ego.KIN(Rupert Soames, the former British Ambassador to France and EC vice-president)GETTING F ?
POS(recipient, theme)GETTING F ??
POS(source, theme) (only if the source is a person)Example: In some cases, [the BGS libraries]Recipient had [obtained]Target [copies of theses]Theme [from the authors]Source[by purchase or gift]Means, and no loan records were available for such copies.POS(the BGS libraries, copies of theses) and ?
POS(authors, copies of theses)GETTING F ?
SRC(theme, source) (if the source is not a person)Example: He also said that [Iran]Recipient [acquired]Target [fighter-bomber aircraft]Theme [from countries other than the USAand the Soviet Union]Source.SRC(fighter-bomber aircraft, countries other than the USA and the Soviet Union)Table 4: Frames-related semantic rulesframe?s lexical units.
We identified 132 semantic ax-ioms that hold in most cases10.
We show some ex-amples in Table 4.4.2 Context importanceThere are cases when the rules that we identifiedshould not be applied.
Let?s examine the sen-tence John intends to leave the kitchen.
If weconsider only the DEPARTING frame and its cor-responding rules, without looking at the context,then our conclusions (?
LOC(John, kitchen) andSRC(John, kitchen)) will be false.
This sentencestates an intention of motion, not the actual action.Therefore, our semantic axioms apply only when thecontext they are in, allows it.
To overcome this prob-lem, we do not apply the axioms for target wordsfound in planning contexts, contexts related to be-liefs, intentions, desires, etc.
As an alternative, wekeep track of plans, intentions, desires, etc.
and,if, later on, we confirm them, then we apply thesemantic axioms.
Also, when we analyze a sen-tence, the frame whose rules we apply needs to bechosen carefully.
For example, in the sentence [Aboat]Agent [carrying]Target [would-be Moroccan il-legal emigrants]Theme [from UK]Path start [to Spain]Path end sank in the Strait of Gibraltar on June 8,the CARRYING frame?s axioms do not apply.
Theboat nor the emigrants reach Spain (the path end of10Section 4.2 lists some exception cases.the motion) because the boat sank.
Here, the rulesgiven by sink.v?s frame should be given priority overthe carry.v?s rules.
We can generalize and concludethat, given a sentence that contains more than onetarget (therefore, maybe multiple frames), the dom-inant frame, the one whose rules should be applied,is the frame given by the predicative verb.
In theprevious sentence, the dominant frame is the onegiven by sink.v and its rules should be applied beforethe axioms of the CARRYING frame.
It should benoted that some of the axioms semantically relatedto the CARRYING frame still apply (for example,SRC(emigrants, UK) or SRC(boat, UK)).
UnlikeLOC(emigrants, Spain), the previous relations donot conflict with the semantics given by sink.v andits location (the Strait of Gibraltar).5 Experimental Results5.1 The RTE dataThe benchmark corpus for the RTE task consists ofseven subsets with a 50%-50% split between thepositive entailment examples and the negative ones.Each subgroup corresponds to a different NLP appli-cation: Information Retrival (IR), Comparable Doc-uments (CD), Reading Comprehension (RC), Ques-tion Answering (QA), Information Extraction (IE),Machine Translation (MT), and Paraphrase Acquisi-tion (PP).
The RTE data set includes 1367 English(T, H) pairs from the news domain (political, eco-376Semantic Axioms CD IE IR MT PP QA RCT F T F T F T F T F T F T FTest data (%)applied to all T s 13.33 21.33 26.66 10 6.66 4.44 11.66 10 8 0 15.38 7.69 21.43 17.14applied to all Hs 1.33 9.33 5 10 0 0 1.66 1.66 8 0 1.53 0 0 1.43solution for (T, H) 9.33 0 20 0 4.44 0 10 1.66 0 0 10.77 1.53 10 5.71Development data (%)applied to all T s 22 27.08 34.28 5.71 8.57 8.57 18.51 18.51 5.12 9.3 28.88 0 9.61 9.8applied to all Hs 4 8.33 5.71 2.85 0 2.85 7.4 3.7 0 0 2.22 0 0 1.96solution for (T, H) 10 2.08 22.85 5.71 5.71 2.85 18.51 3.7 2.56 0 20 0 7.69 0Table 5: The impact of the semantic axioms on each NLP application data set.
T and F stand for True andFalse entailments, respectively.nomical, etc.).
The development set consists of 567examples and the test set contains the remaining 800pairs.5.2 Semantic axiom applicabilityWe measured the applicability of our set of semanticrules, by counting the number of times they extractnew semantic information from text.
Table 6 shows,in percentages, the coverage of the semantic axiomswhen applied to the texts T and the hypotheses H .We also show the number of times the semantic rulessolve a (T, H) entailment without employing anyother type of axioms.Semantic Axioms True False Overall(True and False)Test data (%)applied to all T s 15.75 11.75 13.75applied to all Hs 2.00 3.74 2.87both T s and Hs 8.87 7.75 8.31solution for (T, H) 10.25 1.50 5.87Development data (%)applied to all T s 18.02 11.26 14.64applied to all Hs 2.47 2.81 2.65both T s and Hs 10.25 7.04 8.64solution for (T, H) 12.36 1.76 7.05Table 6: Applicability on the RTE dataClearly, because the texts T convey much moreinformation than H , they are the ones that benefitthe most from our semantic axioms.
The hypothesesH are more straightforward and a semantic parsercan extract all their semantic information.
Also,the rules tend to solve more positive (T, H) entail-ments.
Because there are seven subsets correspond-ing to different NLP applications that make up theRTE data, we analyzed the contribution of our se-mantic axioms to each of the seven tasks.
Table 5shows the axioms?
impact on each type of data.
Thelogic-based approach proves to be useful to taskslike Information Extraction, Reading Comprehen-sion, or Comparable Documents, and it doesn?t seemto be the right choice for the more lexical-orientatedapplications like Paraphrase Acquisition, MachineTranslation, and Information Retrieval.5.3 RTE performanceTo show the impact of our semantic axioms, wemeasured the contribution they bring to a system thatparticipated in the RTE challenge.
The ACC and Fcolumns (Table 7) show the performance of the sys-tem before and after we added our semantic rules tothe list of axioms needed by the logic prover.Task Original EnhancedACC F ACC FTest-IR .478 .472 .5 .505Test-CD .78 .736 .847 .819Test-RC .514 .558 .6 .636Test-QA .485 .481 .523 .537Test-IE .483 .603 .575 .687Test-MT .542 .444 .567 .49Test-PP .45 .585 .44 .576Test .551 .561 .604 .621Development .63 .619 .718 .714Table 7: The accuracy(ACC) and f-measure(F) per-formance values of our systemThe results show that richer semantic connectiv-ity between text concepts improve the performanceof a semantic entailment system.
The overall accu-racy increases with around 5% on the test data andalmost 8% on the development set.
We obtained per-formance improvements for all application settings,except for the Paraphrase Acquisition task.
For thisapplication, we obtained the smallest axiom cover-age (Table 5).
The impact of the semantic axiomson each NLP application data set correlates with the377improvement that the addition of the rules broughtto the system?s accuracy.Our error analysis showed that the system did nottake full advantage of our semantic axioms, becausethe semantic parser did not identify all the seman-tic relations needed as building blocks by the ax-ioms.
We noticed a significant decrease in the logicprover?s usage of world-knowledge axioms.6 ConclusionIn this paper, we present a logic-based semantic ap-proach for the recognizing textual entailment task.The system participating in the RTE competitionused a set of world-knowledge, NLP, and lexicalchain-based axioms and an in-house logic proverwhich received as input the logic forms of the twotexts enhanced with semantic relation instances.
Be-cause the state-of-the-art semantic parsers cannotextract the complete semantic information encodedin text, the need for semantic calculus in NLP be-came evident.
We introduce semantic axioms thateither combine two semantic instances or label rela-tions between the frame elements of a given frame.Preliminary statistical results show that incorporat-ing semantic rules into the logic prover can doublethe semantic connectivity between the concepts ofthe analyzed text.
Our process of identifying moresemantic instances leads to a smaller dependency ofthe logic-based RTE system on world knowledge ax-ioms, while improving its overall accuracy.ReferencesCollin Baker, Fillmore Charles, and John Love.
1998.The Berkeley FrameNet project.
In Proceedings ofCOLING/ACL.Samuel Bayer, John Burger, Lisa Ferro, John Henderson,and Alexander Yeh.
2005.
MITRE?s Submissions tothe EU Pascal RTE Challenge.
In Proceedings of thePASCAL RTE Challenge.Johan Bos and Katja Markert.
2005.
Combining Shal-low and Deep NLP Methods for Recognizing TextualEntailment.
In Proceedings of the PASCAL RTE Chal-lenge.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL Recognising Textual EntailmentChallenge.
In Proceedings of the PASCAL RTE Chal-lenge.Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-yakanok, Dan Roth, and Mark Sammons.
2005.
AnInference Model for Semantic Entailment in NaturalLanguage.
In Proceedings of the PASCAL RTE Chal-lenge.Abraham Fowler, Bob Hauser, Daniel Hodges, Ian Niles,Adrian Novischi, and Jens Stephan.
2005.
ApplyingCOGEX to Recognize Textual Entailment.
In Pro-ceedings of the PASCAL RTE Challenge.Sanda Harabagiu and Dan Moldovan.
1998.
KnowledgeProcessing on Extended WordNet.
In WordNet: anElectronic Lexical Database and Some of its Applica-tions.Jess Herrera, Anselmo Peas, and Felisa Verdejo.
2005.Textual Entailment Recognision Based on Depen-dency Analysis and WordNet.
In Proceedings of thePASCAL RTE Challenge.Valentin Jijkoun and Maarten de Rijke.
2005.
Recogniz-ing Textual Entailment Using Lexical Similarity.
InProceedings of the PASCAL RTE Challenge.Milen Kouylekov and Bernardo Magnini.
2005.
Rec-ognizing Textual Entailment with Tree Edit DistanceAlgorithms.
In Proceedings of the PASCAL RTE Chal-lenge.Dan Moldovan and Adrian Novischi.
2002.
LexicalChains for Question Answering.
In Proceedings ofCOLING.Dan Moldovan and Vasile Rus.
2001.
Logic Form Trans-formation of WordNet and its Applicability to Ques-tion Answering.
In Proceedings of ACL.Dan Moldovan, Christine Clark, Sanda Harabagiu, andSteve Maiorano.
2003.
COGEX A Logic Proverfor Question Answering.
In Proceedings of theHLT/NAACL.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the Se-mantic Classification of Noun Phrases.
In Proceed-ings of HLT/NAACL, Computational Lexical Seman-tics workshop.Eamonn Newman, Nicola Stokes, John Dunnion, and JoeCarthy.
2005.
UCD IIRG Approach to the TextualEntailment Challenge.
In Proceedings of the PASCALRTE Challenge.Rajat Raina, Aria Haghighi, Christopher Cox, JennyFinkel, Jeff Michels, Kristina Toutanova, Bill Mac-Cartney, Marie-Catherine de Marneffe, ChristopherManning, and Andrew Ng.
2005.
Robust TextualInference using Diverse Knowledge Sources.
In Pro-ceedings of the PASCAL RTE Challenge.378
