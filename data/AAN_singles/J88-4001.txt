LFP:  A LOGIC FOR LINGUISTIC DESCRIPTIONS ANDAN ANALYSIS OF ITS COMPLEXITYWilliam C. RoundsElectrical Engineering and Computer Science DepartmentUniversity of MichiganAnn Arbor, MI 48109We investigate the weak expressive power of a notation using first-order logic, augmented with a facilityfor recursion, to give linguistic descriptions.
The notation is precise and easy to read, using ordinaryconventions of logic.
Two versions of the notation are presented.
One, called CLFP, speaks about stringsand concatenation, and generates the class EXPTIME of languages accepted by Turing machines in time2 c" for some c > 0.
The other, called ILFP, speaks about integer positions in strings, and generates theclass PTIME of languages recognizable in polynomial time.
An application is given, showing how to codeHead Grammars in ILFP, showing why these grammars generate only polynomial time languages.1 FIRST-ORDER LOGIC AS A TOOL FOR SYNTACTICDESCRIPTIONIn this paper we investigate the properties of a newnotation for specifying syntax for natural languages.
Itis based on the simple idea that first-order logic, thoughinadequate as a semantics for natural language, is quiteadequate to express relations between syntactic onstit-uents.
This is the insight behind definite clause gram-mars (DCGs) (Pereira nd Warren 1980) and, in fact, ournotation is in some ways a generalization f that nota-tion.
However, we have tried to keep our formalism asmuch as possible like that of standard textbook first-order logic.
There are actually two versions of ournotation.
The first works with strings of symbols anduses concatenation as a primitive operation.
The secondworks with integers and takes the standard arithmeticoperations as primitive.
These integers can be regardedas indexing positions of morphemes in a sentence, butthe sentence itself is not explicitly referenced.
Bothversions allow the recursive definition of predicatesover strings and integers.
This capacity for recursivedefinition is what gives our grammars their generativeability, and our notation has this feature in commonwith DCGs.
However, we liberate DCGs from the Hornclause format, and we do not base the semantics of ournotation on the semantics for Prolog or for logic pro-grams.
We hope that making the logic more familiar andreadable will encourage more people to use logic as ameans for specifying desired syntactic relations be-tween sentential constituents in grammars.
Anyoneknowing the standard conventions of first-order logicshould be able to read or to specify a grammar in ournotation.We also provide a precise semantics for our twonotations.
This involves using the least-fixed-point op-erator from denotational semantics for programminglanguages to explain the recursive definition of predi-cates.
It involves as well using restricted universal andexistential quantification to restrict he class of defin-able predicates ( ets of strings).
We prove a complexitytheoretic characterization for both grammar formal-isms: (1) the formalism using strings and concatenationdefines exactly the class EXPTIME of formal anguagesrecognizable by deterministic Turing machines withintime T(n) = 2 en for some positive c; and (2) theformalism using integers defines exactly the classPTIME of languages recognizable in time T(n) = n k forsome integer k.As an application of the second notation we sketch anatural way to write Head Grammars (Pollard 1984).Because these grammars can be expressed in this way,we immediately obtain the result that head languagescan be recognized in polynomial time.
We even obtainan estimate of the degree of the polynomial that isrequired, derived directly from the form of the gram-matical description.
Unfortunately, the estimated e-gree is at least twice as large as is actually necessary ifone uses the algorithm of Pollard (1984), or Vija-Copyright 1988 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/88/01001-9503.00Computational Linguistics, Volume 14, Number 4, December 1988 1William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexityyashanker and Joshi (1985).
We conjecture that in fact,this factor of (2) can be removed.Our complexity theoretic haracterizations are ver-sions of theorems already appearing in the literature.Shapiro (1984) characterizes the class of languagesdefinable by logic programs with a linear space restric-tion as the class EXPTIME.
The proof of our firsttheorem is very much like his.
Our second theoremcharacterizing PTIME can be viewed as a specializationof the results of Chandra nd Harel (1982), Immerman(1982), and Vardi (1982), who show that the class of setsof finite mathematical structures definable by formulasof first-order logic augmented with a least-fixed-pointoperator is just the class of sets of structures recogniz-able in polynomial time.
We prove both of our results inthe same way, and thus show how these apparentlyunconnected theorems are related.
The proof uses thenotion of alternating Turing machines, and therebyexplains the significance of this idea for the science offormal inguistics.We should also note that our notation will not findimmediate application in current linguistic theory, be-cause it does not allow structural descriptions to bedescribed.
We are in the process of extending andmodifying the notation for this purpose.
However, wethink it is important to explicate the properties of theindividual operations used in building strings and struc-tures.
Our first attempt is therefore to understand howconcatenation f strings can be expressed ina restrictedlogic.
We can then consider other predicates or func-tions on both strings and treelike structures in the sameuniform way.2 CLFP  GRAMMARS" GRAMMARS BASED ONCONCATENATION THEORY2.1 SYNTAX OF CLFPWe present a standard version of the first-order theoryof concatenation, augmented with the least-fixed-pointoperator.
Before proceeding with the formal descrip-tion, we give an example to illustrate the scheme wehave in mind.
Consider the following context-free frag-ment, adapted irectly from Johnson (1985).S --~ NP VPNP --~ Det NounVP--> V NPDet--> NP\[+Gen\] \[ theHere is the corresponding CLFP fragment:S(x) <:~ :4yz.NP\[-Gen\](y) A VP(z) /k x = yz;NP\[case\](x) <:~ :lyz.Det(y) /k Noun\[case\](z)/k x = yz;VP(x) ?
:~ 3yz.V(y) /k NP\[ -Gen\] (z)  /k x = yz;Det(x) <:> NP\[+Gen\](x) V x = the.In this formulation, x,y, and z range over strings ofsymbols (morphemes) and NP,VP,  etc.
are predicatesover strings.
The second clause is here an abbreviationfor two clauses, where case can take two values,namely + Gen and -Gen. At present we do not treat heproblem of calculating complex feature structures, butthere seems to be no reason that the notation cannot besuitably extended.This; example illustrates the most complex case of aCLFP formula.
It is a recursion scheme, which assignsto predicate variables, S,NP,  etc.
certain formulas (theright-hand sides of the corresponding clauses in thedefinition).
The whole scheme is understood as thesimultaneous recursive definition of the predicate vari-ables in the left sides of the definition.
To handle thefact that string variables occur on the left-hand side ofeach clause, we will understand each clause as a func-tion assigning both the formula on its right and the set ofindividual variables mentioned on the left to the givenpredicate symbol.We now proceed with the formal definition of CLFP.Let Ivar be a set {Xo,X i .
.
.
.  }
of individual variablesranging over strings.
Let ~ be a finite set of terminalsymbols.
These are the constants of our theory.
A isanother constant denoting the null string.
Terms arebuilt from variables and constants using the binaryoperation of concatenation.
We also require a set Pvarof predicate variables, listed as the set {PI,PE .
.
.
.
}.Each predicate variable P is equipped with an arityar(P), indicating the number of individual argumentsthat a relation assigned to this variable will have.
(Theexample CLFP scheme given above employs only unarypredicate variables S,NP,VP,  and Det.)
The set ofCLFP formulas is given by the following inductiveclauses.1.
If P ~ Pvar and (x I .
.
.
.
.
x n) is a sequence of Ivarwith length n = ar(P) then P(x~ .
.
.
.
.
xn) is in CLFP;2.
If t~ and t a are terms, then t I = t a is in CLFP;3.
If x E Ivar and ~b is in CLFP then 3x~b and Vx~b arein CLFP;4.
The usual Boolean combinations of CLFP formulasare in CLFP.5.
This clause requires more definitions.
Let f i  be afinite nonempty subset of Pvar with a distinguishedelement S ~ fit.
Let (I) : f i  --) ~'(Ivar) x CLFP.
((I)(R)is going to be the defining clause for the predicate R.)If (9(R) = (X,~b), then we define B~(R)  = X,  andC(I~(R) = (h- We require that IB~(R)I = at(R) andthus be a finite set of individual variables.
Now wesay that the whole map (I) is a recursion scheme iffeach P E fi occurs only positively in cI)(R) for anyR E fi; that is, within the scope of an even numberof negation signs.
Finally, condition 5 states that if (I)is a recursion scheme, with distinguished variable S,then/zS~ (the least fixed point of ~) is in CLFP.Example 1.
Consider the following scheme, which de-fines a regular language.2 Computational Linguistics, Volume 14, Number 4, December 1988William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its ComplexityS(x) ?~ 3y((x  = ay /~ (S(y)) V (x = by A T(y))) V x = aT(v) ?~ 3w(v = cw A S(w)).In this example, fit = {S, T}, BOP(S) = {x}, andCOP(S) = 3y( (x  = ay A (S(y))  V (x = by /~ T(y)))k~/X :-- a.Similarly, BOP(T) = {v}, and COP(T) is the second formulain the scheme.In the example, we have written our recursionscheme in a conventional style to emphasize its directconnection to the usual grammatical presentations.Thus the variable x is bound by the left-hand side of (1),so this clause has been written with S(x)  on the left tomake this fact apparent.
Also, the use of the <:?, sign isconventional in writing out OP.
In our example, the firstclause is taken as defining the distinguished predicate Sof our scheme.
Finally, there are no occurrences of freepredicate variables in this example, but there are in ourfirst example (e.g., noun) .The usual rules for calculating free individual varia-bles apply; if Fvar(qb) is the set of free variables of ~b,then F lvar (P (x l  .
.
.
.
.
xn)) = {xl .
.
.
.
.
x,,}.
The quantifierand Boolean cases are handled as in standard textpresentations.
However, if OP is a recursion scheme thenFlvar(/zSOP) will be calculated as follows.
For each Rfit, find Fvar(COP(R)).
Remove from this set any varia-bles in BOP(R).
The union of the resulting sets for eachR E fit is defined to be the set Flvar(/zSOP).The rules for free predicate variables are a bit sim-pler.
In the atomic formula P(x~ .
.
.
.
.
x , ) ,  P is a freepredicate variable.
In a recursion scheme OP with do-main fit, the set FPvar(/zSOP)) is the union of the setsFPvar(/zSOP(R))),  minus the set fit.A final remark on notation: we will use the notation~t~, .
.
.
,  t,) to stand for the formula3x l  .
.
.
3x , (~x l  .
.
.
.
.
xn) A xl  = tl A .
.
.
A x~ = t ,)where the ti are terms, and the xl are individual variablesnot appearing in any ts.
This will not affect our com-plexity results in any way.2.2 SEMANTICS FOR CLFPWe borrow some notation from the Oxford school ofdenotational semantics to help us explain the meaningof our logic grammars.
I fX  and Y are sets, then IX--> Y\]is the set of functions from X to Y.
Let A = \[Ivar ~ ~*\]be the set of assignments of values to individual varia-bles.
We wish to define when a given assignment, say or,satisfies a given CLFP formula ~b.
This will depend onthe meaning assigned to the free predicate variables in~b, so we need to consider such assignments.
Let PA bethe set of maps p from Pvar to the class of relations onI?
* such that the arity of p(P) is ar(P).
We are now readyto define for each formula ~b and predicate assignment p,the set At\[\[~b\]\]p C A of individual assignments satisfying ~bwith respect o p.1.
~e(x , , .
.
.
, x , )np  = (~ I (a(x,) .
.
.
.
.
a(x,,)) ~ p(e)};2.
A~t, = t21\]O = (a l  tl a = t2a}, where ta is theevaluation of t with variables assigned values by a;3.
~t\[\[3x~b\]lp = {a\[ 3u E E* : a(x/u) ~ ~tff~bl\]p}, andsimilarly for universal quantification;4. kt\[dpVqJ\]\] p = At\[\[4~\]\]pUAt\[\[~b\]\]p, and similarly for otherBoolean connectives.5.
a~/zs~p = {a I (ak)(~ e ~c~k(s)~p)}where, for each k, opk is a recursion scheme with thesame domain fit as OP, and is defined as follows byinduction on k. First, we stipulate that for each P e fit,the set BOPr(P) = BOP(P).
Then we setCOP?
(P) = COP(P)\[R * -  FALSE : R ~ fit\];COPk+t(P) = COP(P)\[R ~ COPk(R) : R E fit\]where the notation ~R ~ 0(R) : R E fit\] denotes thesimultaneous replacement of atomic subformulasR(wl  .
.
.
.
.
wk) in q, (where R is a free occurrence) by theformula O(R)(wl .
.
.
.
.
wk), in such a way that no freeoccurrences of other variables in 0(R) are captured by aquantifier or a/z-operator in ~b.
(We may always changethe bound variables of qJ first, to accomplish this.
)This definition appears much more clumsy than itreally is, and we continue our example to illustrate it.Refer to the example of a regular grammar in theprevious ection.
In the clause for S we are required tosubstitute the formula FALSE for occurrences of both Sand T. This gives, after simplification,COP?
(S)(x) = (x = a).Similarly, substitution of FALSE into the clause for Tgives OP?
(T)(v) = FALSE.
Now substitution of thesenew formulae for S and T into OP gives (after simplifica-tion):COPI(S)(x) = 3y(x  = ay A x = a) V x = a;COPI(T)(v) = 3w(v  = cw A w = a).It is easy to see that continuing this process willsimulate all possible derivations in the grammar, andalso that it explains the meaning of the scheme OP interms of the meaning of subformulas.Some remarks are in order to explain why we use theterm "least-fixed-point", and to explain why, in arecursion scheme, all occurrences of recursively calledpredicates are required to be positive.
Let OP : fit --->CLFP be a recursion scheme.
Define the map /~OP\] :PA --> PA  as follows.
If R E fit, then(u~ .
.
.
.
.
u,,> ~ ~qopIlo(R) ?~ (3,~ ~ ,,~op(R)~p)(a(x~)= ui)where (xl .
.
.
.
.
x,) is the sequence of variables inBOP(R), listed in increasing order of subscripts.
If Rfit, then ~OP\]p(R) = p(R).
Next, let~/~/zROP\]\]p = k->\["Jl I~OP\]\](k)(P\[R ~'- O : R ~ fit\])Computational Linguistics, Volume 14, Number 4, December 1988 3William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexitywhere unions are coordinatewise, F ~k) is the k-th iterateof F, and p\[R <--- ~ : R ~ ~\ ]  is p with the empty relationassigned to each predicate variable in fit.
This formula isjust the familiar least-fixed-point formula Uk_>~ F(~)(-1-)from denotational semantics.
Then we can check that~tzSgP\]\]p is in PA,  and is the least fixed point of thecontinuous map ~qb\]~.
It is then possible to prove thatJC \ [~S~p = (~S,~p) (S )where S is the distinguished predicate variable in fit.If we had no conditions on negative formulas inrecursion schemes, then we could entertain schemeslikeS(x) ~ 7 S(x)which, although they would receive an interpretation iour first definition, would give a T which was notcontinuous, or even monotonic.
We therefore xcludesuch cases for reasons of smoothness.Next we come to the definition of the language orrelation denoted by a formula.
A k-ary relation P on E*is said to be definable in CLFP iff there is a CLFPformula th with no free predicate variables such that(Ul .
.
.
.
.
/'/k) E P <:~ 3a  E kt\[\[th\]\]:(a(x0 .
.
.
.
.
a(xk)) =(ul .
.
.
.
.
Uk), where xl .
.
.
.
.
xk is the list of free variablesin ~b arranged in increasing order of subscript.
(Noticethat the parameter p has been omitted since there are nofree predicate variables in ~b.
)So far, we have not restricted quantification i  ourformulas, and every r.e.
predicate is definable.
We needto add one other parameter to the definition of J(, whichwill limit the range of quantification.
This will be aninteger n, which will be the length of an input sentenceto be recognized.
The occurrences of the formula .~t\[\[~b\]pwill thus be changed everywhere in the above clauses toAt\[~b\]pn.
The only change in the substance of the clausesis in the rule for existential and universal quantification.~t~3x~b\]pn = {~ I 3u  ~ :~* : \[ul -< n A ,~(x/u)A predicate P is said to be boundedly definable iff forsome 4~:(Ul .
.
.
.
.
uk) ~ P ?
:> 3a  E kt~?\]\]n : (a(x 0 .
.
.
.
.
a(xk))= (ul .
.
.
.
.
u~)where n = max(\[uiD.
(To abbreviate the right-handcondition, we write (ul .
.
.
.
.
uk) ~ ~b).
Our first theoremcan now be stated.Theorem 1.
A language (unary predicate) is bound-edly definable in CLFP iff it is in EXPTIME.We defer the proof to the next section.3 EXPT IME AND CLFP3.1 ALTERNATIONBefore proving Theorem 1, we need to discuss themethod of proof both for this result and for the IntegerLFP characterization of PTIME in the next section.This material is repeated from the fundamental rticle ofChandra, Kozen, and Stockmeyer (1981).
Their papershould be consulted for the full details of what we statehere.An alternating Turing machine can be regarded as aTuring machine with unbounded parallelism.
In a givenstate, and with given tape contents, the machine canspawn a finite number of successor configurations ac-cording to its transition rules.
These configurations arethought of as separate processes, each of which runs tocompletion in the same way.
A completed process isone which is in a special accepting state with nosuccessors.
The results of the spawned processes arereported back to the parent, which combines the resultsto pass on to its own parent, and so forth.
How theparent does this depends on the state of the finitecontrol.
These states are classified as being eitherexistential (OR), universal (AND), negating (NOT), oraccepting.
If the parent is in an existential state, itreports back the logical OR of the results of its off-spring.
If it is in a universal state, it reports back thelogical AND; if the state is negating, the parent reportsthe negation of its one offspring.
An accepting stategenerates a logical 1 (TRUE) to be reported back.
Thus anondeterministic TM can be regarded as an alternatingTM with purely existential states.An alternating TM is defined as a tuple in a standardway.
It has a read-only input tape with a head capable oftwo-way motion.
It also has a fixed number of worktapes.
The input tape contains a string u E E*, while thework tapes can use a tape alphabet F. The transitionrelation 6 is defined as for ordinary nondeterministicTMs.
The state set is partitioned as described aboveinto universal, existential, negating, and acceptingstates.
The relation 6 is constrained so that existentialand universal states have at least one successor, negat-ing states have exactly one successor, and acceptingstates have no successors.
A configuration is then just atuple describing the current state, positions of theheads, and tape contents as is familiar.
The initialconfiguration is the one with the machine in its initialstate, all the work tapes empty, and the input head atthe left end of the input u.
The successor elation I-between configurations i defined again as usual.To determine whether or not a configuration is ac-cepting, we proceed as follows.
Imagine the configura-tions that succeed the given one arranged in a tree, withthe given configuration at the root.
At each node, theimmediate descendants of the configuration are thesuccessors given by F. The tree is truncated at a leveldetermined by the length of the input tape (this trunca-tion is not part of the general definition but will sufficefor our results.)
The leaf nodes of this tree are labeledwith (0) if the configuration at that node is not accept-ing, and with (1) if the configuration is accepting.
Thetree is then evaluated according to the description givenabove.
The configuration at the root is accepting iff it islabeled (1) by this method.
Thus an input is accepted by4 Computational Linguistics, Volume 14, Number 4, December 1988William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexitythe machine iff the initial configuration with that input isaccepting.
In our application, it will always suffice tocut off the tree at level 2 on, where n is the length of theinput string, and c is a positive constant depending onlyon the description of the machine.We say that an alternating TM is S(n) space boundediffin the above tree, for any initial configuration labelingthe root, no auxiliary tape length ever exceeds S(n)where n is the length of the input.
We are concernedonly with the functions S(n) = log n and S(n) = n in thispaper.
We let the class ASPACE(S(n))  be the class oflanguages accepted by space-bounded ATMs in thisway.
We then have the following theorem (Chandra,Kozen, Stockmeyer 1987):Lemma 1.
If S(n) >- log n, thenASPACE(S(n))  = \[,.J DTIME(2 cs?m)c>0where DTIME(T(n)) is the class of languages accepteddeterministically by ordinary Turing machines withinT(n) steps.Our problem in the rest of this section is to show howlinear space bounded ATMs and CLFP grammars im-ulate each other.
To facilitate the construction of thenext section, it is convenient to add one feature to thedefinition of alternating Turing machines.
Let U be thename of a k-ary relation on E*.
We allow machines tohave oracle states of the form U?
(i I .
.
.
.
.
ik), where the ijare tape numbers.
If now the predicate U is interpretedby an actual relation on E*, then when M executes uchan instruction, it will accept or reject according towhether the strings on the specified tapes are in therelation U.
We will need such states to simulate recur-sive invocations of recursion schemes.
It is not hard tomodify the definition of acceptance for ordinary ATMsto that for oracle ATMs.
The language or relationaccepted by the ATM will now of course be relative toan assignment p of relations to the predicate names U.The next subsections contain our constructions forthe CLFP characterizations.
Then, in Section 4 we willtreat Integer LFP grammars and show how these gram-mars and logspace bounded ATMs simulate ach other.As a consequence of the above lemma, we will thenhave our main results.3.2 PROOF OF THEOREM 1Our first task is to show that if a language L is(boundedly) CLFP-definable, then it can be recognizedby a linear space bounded ATM.
The idea is simple.Given an input string, our machine will try to executethe logical description of the grammar.
Its states willcorrespond to the logical structure of the CLFP for-mula.
If that formula is, for example, the logical AND oftwo subformulas, then the part of our machine for thatformula will have an AND state.
A recursion schemewill be executed with states corresponding to the pred-icate variables involved in the recursion, and so forth.To give an explicit construction of an ATM corre-sponding to a formula 4, of CLFP we need to be preciseabout the number of work tapes required.
This will bethe sum of the number of free individual variables of 4,,and the number of "declarat ions" of bound variables in4,.
A "declaration" is either the occurrence of a univer-sal or existential quantifier in 4,, or one of the individualvariables bound on the left side of a (non-S) clause in arecursion scheme.
If that clause defines the predicate R,then the number of variables declared at that point isar(R) = \[Bqb(R)l. We thus define the number /3(4,) ofdeclarations of bound variables in 4' by induction asfollows:1.
\]3(R(x I .
.
.
.
.
Xn)) = 0,2.
13(/1 = /2) = 0,3.
/3(4,Vq,) = /3(4,A~) = /3(4,) + /3(qJ);4.
/3(q4,) = /3(4,);5.
/3(3x4,) =/3(Vx4,) = 1 +/3(4,);6. fl(tzS~) = ~(C~(S)) + En~\{s~(ar(R ) + \[3(C~(R))).The number 7(4,) counts the maximum number of tapesneeded, and is defined to be/3(4,) + IFivar(4,) I + 1.We can now state the inductive lemma which allowsthe construction of ATMs.Lemma 2.
Let 4, be a CLFP formula, with IFlvar(4,)l= k, and T:Flvar(4,) ~ {1 .
.
.
.
.
k}.
Let m = y(4,).Then we may construct an m-tape ATM M(4,,T)having the following properties: (i) M has oraclestates P?
for each free predicate variable of 4,, and (ii)For any a:Flvar(4,) ---> Z*, and any environment p,we have the following.
Let n = max{\[a(xi)\[}.
Then Mwith oracle states for the p(P), started with a(xO ontape T(x,) .
.
.
.
.
and a(xk) on tape T(xk), and the othertapes blank, will accept without ever writing morethan n symbols on any tape, if and only if(a(x l) .
.
.
.
.
e~(xk)) E Jl4.\[\[4,\]\]pn.Proof: This lemma formalizes the intuitive idea, statedabove, that to calculate the membership of a string x inthe language defined by a recursion scheme, it sufficesto execute the scheme recursively.
The full proof woulduse the formal definition of the semantics of ATMs,which themselves are given by least-fixed-point defini-tions.
We have chosen not to give the full proof,because the amount of explanation would be over-whelming relative to the actual content of the proof.Instead we give a reasonably complete account of theinductive construction involved, and illustrate with theregular set example of the previous section.To start the induction over formulas 4,, suppose that4, is R(x~ .
.
.
.
.
xk).
Then we may take M to be a machinewith k = 7(4,) tapes, with one oracle state P, and thesingle instruction P?
(T(x0 .
.
.
.
.
T(xk)).If 4, is t~ = tz, then we let M be a simple machineevaluating t~ and t z, using perhaps an extra tape forbookkeeping.
It does a letter-by-letter comparison, sothat it never has to copy more than the maximum lengthof any one tape.If 4, is ~0, then M(4,) consists of adding a negatingComputational Linguistics, Volume 14, Number 4, December 1988 5William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexitystate before the initial state of M(q,), and transferringcontrol to that initial state.If ~b is qq V ~02, we construct MI and M2 by inductivehypothesis.
Then M(~b) is constructed by having disjointinstruction sets corresponding to each M;, prefixed byan OR state which transfers control to either of the twoformerly initial states.
The free individual "variables ofthe disjunction are those occurring free in either dis-junct.
Let T be an assignment of tapes to the freevariables of the disjunction.
Then we construct MI witha T~ such that Tl(x) = T(x), and similarly for M 2, wherex is a free individual variable.
Otherwise, any tapesreferenced in M~ are distinct from any tapes referencedin M2.
In other words, the machine M has sharedstorage for the free variables, and private storage forvariables bound in either disjunct.
The oracle states inthe two pieces of code are not made disjoint, however,because a predicate variable is free in the disjunction iffit is free in either disjunct.
It is clear that the number oftapes of the ~ V 1~2 is just ~(I\]/I V 1~2).
For the case ofth = ~l /k  ~b,, we make exactly the same construction,only using an AND state as the new initial state.If ~b is 3x~b, and T is a tape assignment for the freevariables of ~b, then we construct M(~,) using the ex-tended tape assignment which assigns a new tape k + 1to the variable x, and otherwise is the same as T. NowM is constructed to go through an initial loop of exis-tential states, which fills tape k + l with a string nolonger than the maximum length of any string on tapes1 through k. It then transfers control to the initial stateof M(~,).
The same construction is used for the universalquantifier, using an initial loop of universal states.Finally, we need to treat the case of a recursionscheme/.~Sqb.
Suppose that ?
has domain ~,  and let Tbe a tape assignment for/xSqb.
For each clause CO(Q),where Q E ~,  we construct a machine M(Q) by induc-tive hypothesis.
The global free variables of each M(Q)will have tapes assigned by T. However, we constructthe M(Q) all in such a way that the local tape numbersdo not overlap the tape numbers for any other M(R).This procedure will give tape numbers to all the varia-bles in the set Bdp(Q).
Let this set be {zl .
.
.
.
.
Zk} inincreasing order.
Define TQ(Zi) to be the tape assigned tozi in M(Q).The machine M(lxSd~) will consist of the code for theM(Q), arranged as blocks; the initial state of each suchblock will be labeled Q.
In all the blocks, recursiveoracle calls to Q?
will be replaced by statements rans-ferring control to Q.
Thus, consider an oracle callQ?
(il .
.
.
.
.
ik), in any block M(R).
Replace this call bycode which copies tape i z to tape TQ(ZO .
.
.
.
.
and tapei k to tape TQ(Zk).
Insert code that empties all other tapeslocal to M(Q), and insert a statement "go to Q.
"This completes the construction, and we now illus-trate it with an example.
Consider the recursion schemeintroduced in the first section.S(x) ~',, 3y((x = ay A (S(y)) V (x = by A T(y))) V x = aT(v) ?~ 3w(v = cw A S(w)).We construct he machine M(S) as follows :ltape 1 :xtape 2 : y (initially blank)Initially : guess a value of y, such that lyl -< Ixl, andstore y on tape 2; go to (ql or q2 or q7);ql : go to (q3 and q4);q3 : check x = ay on tapes I and 2, and accept orreject as appropriate;q4 : S?
(tape 2)q2 : go to (q5 and q6);q5 : check x = by on tapes 1 and 2, and accept orreject as appropriate;q6 : T?
(tape 2)q7 : check x = a and accept or reject.Similarly, we can construct a machine M(T) for the Tclause.
Then the result of pasting together the twoconstructions i shown in Figure 1.tape 1 : xtape 2 : y (initially blank)tape 3 : v (initially blank)tape 4 : w (initially blank)S : guess a value of y, such that lyl ~ Ixl, on tape2;go to (ql or q2 or q7);q l : go to (q3 and q4);q3 : check x = ay on tapes 1 and 2 , andaccept or reject as appropriate;q4 : copy tape 2 to tape I;Empty tape 2;Go to S.q2 : go to (q5 and q6);q5 : check x = by on tapes 1 and 2 , andaccept or reject as appropriate;q6 : copy tape 2 to tape 3;empty tape 4;go to T.q7 : check x = a and accept or reject.T : guess a w on tape 4 no longer than v on tape3;go to (q9 and ql0);q9 : Check v = cw on tapes 3 and 4, andreturn appropriately;ql0 : copy tape 4 (w) to tape 1;empty tape 2;go to S.Figure 1.
ATM Program for the Recursion Scheme.6 Computational Linguistics, Volume 14, Number 4, December 1988William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its ComplexityAs we remarked, we cannot give a full proof of thecorrectness of our construction.
However, the con-struction does correspond to the formal semantics ofCLFP.
In particular, the semantics of recursion corre-sponds to the iterated schemes Ok. Iterating the schemek times roughly corresponds to developing the compu-tation tree of the ATM to k levels, and replacing theoracle states at the leaves of the k-level tree withrejecting states corresponds tosubstituting FALSE intothe kth iteration.With these remarks, the proof is complete.Lemma 3.
Suppose L is accepted by a S(n) =n-bounded ATM.
Then there is a CLFP formula ~bsuch that for all u E ~*, we have u E L ?
:> u ~ ~b.Proof: We may assume that M is an ATM with one worktape, if we allow M to print symbols in an auxiliary tapealphabet F. By a result in Chandra, Kozen, and Stock-meyer (1981) M has no negating states.
We show how toconstruct a formula ~b, which has constants ranging overF, but which has the property stated in the conclusion ofthe lemma: for each string x over ~, M accepts x iffx~b.
The formula ~b will be given as a recursion scheme/xS~.
Each state q of M will become a binary predicatevariable q(x,y) in ~.
The meaning of q(u, v), where u andv are specific strings in F*, is that M is in state q,scanning the first symbol of v, and that u and v are theportions of the work tape to the left and the right of thehead, respectively.We give a perfectly general example to illustrate theconstruction of ~.
In this example, the tape alphabetF is {a,b}.
Suppose that q is a universal state of M andthat 8(q,a) = {(r,b,right),(s,a,left)}, and 8(q,b) ={(p,b,left),(q,a,right)}.
Then dp(q)(x,y) is the followingformula:/~  Vwt\[(x = wtr A y = at ~ r(xb,t) A s(w,o~at))o~{a,b}A(x  = wtr /~ y = bt ~ p(w,trbt) /~ q(xa,t))\]The distinguished element of 2~ is qo, the start state ofM.
Notice that all predicate variables in R occur posi-tively in ~, and that the search for w and t is limited tostrings no longer than the length of the original input toM.
If q is an accepting state of M, then we have a clausein ~ of the form q(x,y) <==> TRUE, where TRUE is sometautology.Technically speaking, the explicit substitutionsr(xb,t) are not allowed in our formulas, but these can beexpressed by suitable sentences like (3z)(z = xb /~r(z,t)), as remarked in the first section.
The cases forq(x,y) when x and y are null must also be handledseparately because M fails if it tries to leave the originalregion.Finally, we can obtain a formula over the constantalphabet E by a more complicated construction.
If weencode F into E by a homomorphic mapping, then amachine N can be constructed to simulate M. N willhave tape alphabet E, but will have a number n of worktapes bounded linearly by the constant involved in theencoding.
We now make a formula corresponding to N,but the predicates will have to be 2n-ary, one pair ofarguments for each tape of N. With these remarks, theproof of the lemma is complete.Theorem I follows immediately from the above lem-mas.4 ILFP :  GRAMMARS WITH INTEGER INDEXING4.1 SYNTAX OF ILFPOur characterization f the defining power of CLFPrelied on the result EXPT IME = ASPACE(n) .
We alsoknow that PT IME = ASPACE( Iog  n).
Is there a similarlogical notation that gives a grammatical characteriza-tion of PTIME?
This section is devoted to giving anaffirmative answer to this question.
As stated in theintroduction, this result is already known (Immerman1982, Vardi 1982), but the result fits well with the CLFPtheorem, and may in the linguistic domain have somereal applications other than ours to Head Grammars.
Toexplain the logic, it helps to consider acceptance by alogspace bounded ATM.
In this case, the machine has aread-only input tape, which can be accessed by atwo-way read head.
Writing is strictly disallowed on theinput tape, in contrast o the linear space boundedATMs of the previous ection.
There is also a number kof work tapes on which computation occurs.
Supposethat these work tapes use a binary alphabet.
If their sizealways is less than or equal to I-log2 n-I, then they arealways capable of representing the numbers from 0through n - 1.
We thus think of the contents of the worktapes as indices of specific positions in the read-onlyinput string, though in fact they may not serve thispurpose in an arbitrary computation.
Since the input isoff-line, substrings of the input will not be quantified.Instead, we quantify over the integer subscripts, and theinput simply becomes a global parameter appearing inthe semantics.
Instead of having equations betweenstrings as atomic formulas, we will have equationsbetween integer terms.
In order to access the input, wewill have, for each symbol a E E, an atomic predicatesymbol a(i) of one argument, which will be true iff in thegiven input x, the symbol x(i) at position i is a.
(Wenumber the positions from 0 through n - 1).
We allowindividual constant symbols 0,1, and last, which will beinterpreted as 0, 1, and n - 1, respectively, when theinput has size n. As primitive arithmetic operations weallow addition and subtraction, and multiplication andinteger division by 2.
All of these operations are inter-preted modulo n when the input is given.We need not give the formal definition of ILFPformulas, as it is the same as for CLFP, except hatindividual variables come from a set {io,i ~ .
.
.
.
}, termsare formed as above from arithmetic ombinations ofindividual variables and constants, and the unary pred-icates a(/) are atomic formulas.Example 2.
Consider the CFGS---> aSb \[bSa\[ SS \[ab \[ baComputational Linguistics, Volume 14, Number 4, December 1988 7William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its ComplexityThis is represented in ILFP as follows:S(id) ?~ a(i) A S(i + 1,j - 1) A bq)V b(i) A S(i + I j  - 1) A a(\])V 3k < j  : S( i ,k)/~ S(k + ld)V J = i + 1 A ((a(i) A bq)) V (b(i) A aq))(Again, the explicit substitution of terms for variables isnot officially allowed but can be introduced by defini-tion.
)The meaning of the above scheme should be clear.The predicate S(id) is intended to mean that node Sdominates positions i through j in the input.
Thus theassertion S(0,1ast), with no free variables, will be satis-fied by a string x iff x is generated by the given CFG.The relation of this descriptive formalism to the CKYalgorithm for context-free recognition should also sug-gest itself.Our definition of the meaning function At\[\[4'\]\] is likethat in Section 2, except hat the parameter n is replacedby a string x E X*.
Thus1.
At~p(i I .
.
.
.
.
ik)\]px = {t~ \[ (a(i 0 .
.
.
.
a(ik) ) ~ /9(/9)};2.
At~a(i)\]px = {a I x(a(i)) = a};3..~t~t 1 = tz\]pX = {a I tl?t = tEa};4.
At~3i4'\]px = {a \[ (3m < Ixl)(~(i/m) ~ ~q4'\]px)};5.
Boolean combinations are as before;6.
At~SdP\]px = {a I (3k)(a E At\[C~pk(S)\]px)}The schemes qb k are defined for recursion schemes asabove.If 4' is a formula of ILFP with no free individual orpredicate variables then S\[4'\]\]px is either A, the set of allindividual assignments, or 0, independent of p, butdepending onx.
We say thatx ~ 4'iffS~4'~px is all of A.A language L _C X* is ILFP-definable iff for some 4' inILFP, L = {x I x D 4'}.
Our objective is nowTheorem 2.
A language is ILFP-definable iff it is inPTIME.The proof appears in the next subsection.4.2 PROOF OF THEOREM 2The idea of our proof is the same as that for Theorem 1,and only a sketch of the proof is necessary.
We firstrestate Lemma 2 for ILFP, using the same definition for/3 and %Lemma 4.
Let 4' be an ILFP formula, with IFIvar(4')\]= k, and T : Flvar(4') ---> {1 .
.
.
.
.
k}.
Let m = 7(4').Then we may construct an m-tape ATM M(4',T)having the following properties: (i) M has oraclestates P?
for each free predicate variable of 4', and (ii)For any x ~ E*, any a mapping Flvar(4') to naturalnumbers, and any environment p, we have the fol-lowing: M with oracle states for the p(P), started withx on the input tape, binary representations of theintegers a(il) on tape T(il) .
.
.
.
.
and a(ik) on tapeT(ik), and the other tapes blank, will accept withoutever writing a valuej  > I x \[ on any tape, if and onlyif (ot(iO .
.
.
.
.
t~(ik)) E At\[4'\]pX.Proof: The proof is almost identical to that of Lemma 2.To evaluate quations M may have to use an extra tape,because otherwise the given nonblank tapes would beoverwritten by the arithmetic operations.
If 4' is a(i) (theonly case not covered in (2), then tape 1 is used as acounter to Mcate the input head at the position of thecontents of tape 1.
Since arithmetic is modulo Ixl, themachine never writes too great a value in these cases.The other cases are proved exactly as in (2), so thiscompletes the proof.Lemma 5.
If L ~ ASPACE(Iog n), then L is ILFP-definable.Proof: We may assume that L is accepted by an ATMwith p binary work tapes and one input tape.
(If the tapealphabet is not binary, encode with a homomorphismand expand the number of tapes as necessary.)
We mayfurther assume that the machine M never writes a stringlonger than L..log2(n)__\] - 1 on any work tape (rememberone bit on each tape in finite control if necessary).
Eachwork tape, or portion thereof, is thus guaranteed torepresent a binary number strictly less than n in value,where n is the length of the input string.We now proceed as in the proof of Lemma 3, butcoding the contents of the work tapes as binary num-bers.
We need a number h, which tells the position ofthe input head.
We also have two numbers l and r,which are the binary values of the tape contents to theleft and right of the work tape head (here we describethe case of just one work tape).
The number r willactually be the binary value of the reversal of the stringto the right of the tape head, because this makes theoperation of shifting the head a simple multiplication ordivision by 2.
Since a string may have leading zeroes,we also need to keep two auxiliary numbers II and rr,which are the actual engths of the strings to the left andright of the head.
For each state q of the ATM we thushave a predicate q(h,l,r, ll, rr) of five integer variables.The reader should have no difficulty in encoding thetransition rules of M exactly as in Lemma 3.
Forexample, a test as to whether the scanned symbol on thework tape is 0 or 1 becomes a test of the parity of r, andso on.
Finally, it can be seen that the case of p worktapes requires 4p + l-ary predicates.
This completes theproof of our lemma and thus the theorem.4.3 WHICH POLYNOMIAL?We can get a rough estimate of the degree of thepolynomial time algorithm, which will recognize stringsin the language defined by an ILFP grammar.
We saw inthe proof of Lemma 4 that if a scheme 4' has 7(4') = P,then an ATM with p + 1 binary work tapes can beconstructed to recognize the associated language.
Thenumber of configurations of each tape is thus log n *2log n+l If there are p + 2 tapes, this gives O(logp+ln *n p+IIP := O(n p+2) possible tape configurations.
Multiply-ing by n for the position of the input head gives O(n p+3)possible ATM configurations.
From an analysis of theproof of Lemma 1 in Chandra, Kozen, and Stockmeyer8 Computational Linguistics, Volume 14, Number 4, December 1988William C. Rounds LFP: A Logic for Linguistic Descriptions and an Analysis of its Complexity(1981), we can see that the polynomial in our determin-istic TM algorithm is bounded by the square of thenumber of ATM configurations.
This leads to anO(n 2p+6) recognition algorithm.
Since this bound wouldgive an O(n ~2) algorithm for context-free language rec-ognition, we conjecture that the general estimate can beimproved.
In particular, We would like to remove thefactor of 2 from 2p.5 APPLICATIONS TO HEAD GRAMMARSIn this section we express head grammars (Pollard 1984)in ILFP,  and thus show that head languages can berecognized in polynomial time.
Since the class of headlanguages is the same as the class of tree adjunctlanguages (Vijayashankar, Joshi 1985), we get the sameresult for this class.
We will actually give only asimplified version of head grammars to make our ILFPformulas easy to write.
This version corresponds ex-actly to the Modified Head Grammars of Vijayashankarand Joshi (1985), and differs only from the originalversion in that it does not treat the empty string.
(Roach(1988) has an extended iscussion of head languages.
)We define a head grammar as a tuple G = (N,~,,P,S),where N and E are finite nonterminal and terminalalphabets, P is a finite set of productions, and S is thestart nonterminal.
The productions are of the form COp(A,B), where A,B,  and C are nonterminals and Op ischosen from a fixed set of head-wrapping operations.Productions can also have the form C ~ (x,y), where xand y are terminal strings.We view nonterminals in N as deriving pairs ofstrings (u, v).
In the original formulation, this meant hatthe head of the string uv occurred at the end of u. Thewrapping operations come from the set {LLI,LL2,LC ~,LC2}.
We consider LL  2 and LC !
as examples.
We defineLL2((w,x),(u,v)) = (wu, vx).
Thus if A derives (w,x) andB derives (u, v), and C ~ LL2(A,B) is a production, thenC derives (wu,vx).
Similarly, LCl((W,X),(u,v)) =(w,xuv), so in the corresponding case, we would have Cderives (w,xuv) if C ~ LCI(A,B) were a production.
Astring t is in L(G) iff for some u and v, t = uv and Sderives (u,v).Given a head grammar, we write an ILFP  recursionscheme as follows.
For each nonterminal C, we intro-duce a predicate C(i j ,  k,l).
We think of these fourintegers as indexing the positions of symbols in a string,starting at the left with 0.
Then C(i j ,  k,l) means that thenonterminal symbol C can derive the pair of substringsof the input string between i and j,  and between k and linclusive.
Thus, if C ~ LL2(A,B) is a production, ourscheme would include a clauseC(ij ,  k,l) ?
:~ (3pq)(A(i,p,q + 1,l) A B(p + ld',k,q))Similarly, if C --~ LCI(A,B) were a production, wewould haveC(ij ,  k,l) ?~ (3pq)(A(i j ,  k,p) /~ B(p + 1,q,q + 1,/))Finally, if C--* (a, bb)were a terminating production, wewould haveC(i j ,  k,l) ?
:~ a(i) /~ i = j /~  k = i + 1/~ b(k)Ab(k+ l) A l=  k+ 1The grammar would be defined by the recursion schemeand the assertion 3 jS (0 j j  + 1 ,last), where S is the startsymbol of G.It can be seen from this formulation that every headgrammar can be written as an ILFP  scheme with at mostsix total variables.
Section 4 thus gives us an O(n 18)algorithm.
However,  the algorithm of Vijayashankerand Joshi (1985) is at most n 6.
It would seem that a ruleof thumb for the order of the polynomial algorithm is touse the number y(th) for the ILFP  scheme th, but wehave no proof for this conjecture.ACKNOWLEDGMENTResearch supported by NSF Grant MCS-8301022.REFERENCESChandra, A.K.
and Harel, D. 1982 Structure and Complexity ofRelational Queries.
Journal of Computer Systems Science 25: 99-128.Chandra, A.K.
; Kozen, D.C.; and Stockmeyer, L.J.
1981 Alterna-tion.
Journal of Associated Computer Machines 28:114-133.Immerman, N. 1982 Relational Queries Computable in PolynomialTime.
In Proceedings of the 14th ACM Symposium on Theory ofComputing: 147-152.Johnson, M. 1985 Parsing with Discontinuous Constituents.
In Pro-ceedings of the 23rd Annual Meeting of the Association forComputational Linguistics, Chicago, IL: 127-132.Pereira, F.C.N.
and Warren, D.H.D.
1980 Definite Clause Grammarsfor Language Analysis--A Survey of the Formalism and a Com-parison with Augmented Transition Networks.
Artificial Intelli-gence 13: 231-278.Pollard, C. 1984 Generalized Phrase Structure Grammars, HeadGrammars, and Natural Language.
Ph.D thesis, Stanford Univer-sity, Stanford, CA.Roach, K. (1988) Formal Properties of Head Grammars.
Manuscript,Stanford University.
In Manaster-Ramer, A.
(ed.)
Mathematics ofLanguage.
John Benjamins, Amsterdam, Holland.Shapiro, E.Y.
1984 On the Complexity of Logic Programs.
Journal ofLogic Programming 1.Vardi, M. 1982 The Complexity of Relational Query Languages.
InProceedings of the I4th ACM Symposium on Theory of Comput-ing: 137-146.Vijayashankar, K. and Joshi, A.K.
1985 Some Computational Prop-erties of Tree Adjoining Languages.
In Proceedings of the 23rdMeeting of the Association for Computational Linguistics, Chi-cago, IL: 82-93.NOTE1.
Notice that the machines are presented inATMGOL, a syntacti-cally ill-defined variant of ATM transition functions.
Also, ATMsand ATNs are not to be confused.Computational Linguistics, Volume 14, Number 4, December 1988 9
