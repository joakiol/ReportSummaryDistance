Tree Kernels for Semantic Role LabelingAlessandro Moschitti?University of TrentoDaniele Pighin?
?University of TrentoRoberto Basili?University of Rome ?Tor Vergata?The availability of large scale data sets of manually annotated predicate?argument struc-tures has recently favored the use of machine learning approaches to the design of automatedsemantic role labeling (SRL) systems.
The main research in this area relates to the design choicesfor feature representation and for effective decompositions of the task in different learning models.Regarding the former choice, structural properties of full syntactic parses are largely employed asthey represent ways to encode different principles suggested by the linking theory between syntaxand semantics.
The latter choice relates to several learning schemes over global views of theparses.
For example, re-ranking stages operating over alternative predicate?argument sequencesof the same sentence have shown to be very effective.In this article, we propose several kernel functions to model parse tree properties in kernel-based machines, for example, perceptrons or support vector machines.
In particular, we definedifferent kinds of tree kernels as general approaches to feature engineering in SRL.
Moreover, weextensively experiment with such kernels to investigate their contribution to individual stagesof an SRL architecture both in isolation and in combination with other traditional manuallycoded features.
The results for boundary recognition, classification, and re-ranking stages providesystematic evidence about the significant impact of tree kernels on the overall accuracy, especiallywhen the amount of training data is small.
As a conclusive result, tree kernels allow for a generaland easily portable feature engineering method which is applicable to a large family of naturallanguage processing tasks.1.
IntroductionMuch attention has recently been devoted to the design of systems for the automaticlabeling of semantic roles (SRL) as defined in two important projects: FrameNet (Baker,Fillmore, and Lowe 1998), based on frame semantics, and PropBank (Palmer, Gildea,?
Department of Information Engineering and Computer Science, Via Sommarive, 14 I-38050 Povo (TN).E-mail: moschitti@dit.unitn.it.??
Fondazione Bruno Kessler, Center for Scientific and Technological Research, Department of InformationEngineering and Computer Science, Via Sommarive, 18 I-38050 Povo (TN).
E-mail: pighin@itc.it.?
Department of Computer Science, Systems and Production, Via del Politecnico, 1 I-00133 RM.E-mail: basili@info.uniroma2.it.Submission received: 15 July 2006; revised submission received: 1 May 2007; accepted for publication:19 June 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 2and Kingsbury 2005), inspired by Levin?s verb classes.
To annotate natural languagesentences, such systems generally require (1) the detection of the target word em-bodying the predicate and (2) the detection and classification of the word sequencesconstituting the predicate?s arguments.Previous work has shown that these steps can be carried out by applying machinelearning techniques (Carreras and Ma`rquez 2004, 2005; Litkowski 2004), for which themost important features encoding predicate?argument relations are derived from (shal-low or deep) syntactic information.
The outcome of this research brings wide empiricalevidence in favor of the linking theories between semantics and syntax, for example,Jackendoff (1990).
Nevertheless, as no such theory provides a sound and completetreatment, the choice and design of syntactic features to represent semantic structuresrequires remarkable research effort and intuition.For example, earlier studies on feature design for semantic role labeling were car-ried out by Gildea and Jurafsky (2002) and Thompson, Levy, and Manning (2003).
Sincethen, researchers have proposed several syntactic feature sets, where the more recentsets slightly enhanced the older ones.A careful analysis of such features reveals that most of them are syntactic treefragments of training sentences, thus a viable way to alleviate the feature design com-plexity is the adoption of syntactic tree kernels (Collins and Duffy 2002).
For example, inMoschitti (2004), the predicate?argument relation is represented by means of the min-imal subtree that includes both of them.
The similarity between two instances is eval-uated by a tree kernel function in terms of common substructures.
Such an approachis in line with current research on kernels for natural language learning, for example,syntactic parsing re-ranking (Collins and Duffy 2002), relation extraction (Zelenko,Aone, and Richardella 2003), and named entity recognition (Cumby and Roth 2003;Culotta and Sorensen 2004).Furthermore, recent work (Haghighi, Toutanova, and Manning 2005; Punyakanoket al 2005) has shown that, to achieve high labeling accuracy, joint inference shouldbe applied on the whole predicate?argument structure.
For this purpose, we need toextract features from the sentence syntactic parse tree that encodes the relationshipsgoverning complex semantic structures.
This task is rather difficult because we donot exactly know which syntactic clues effectively capture the relation between thepredicate and its arguments.
For example, to detect the interesting context, themodelingof syntax-/semantics-based features should take into account linguistic aspects likeancestor nodes or semantic dependencies (Toutanova, Markova, and Manning 2004).In this scenario, the automatic feature generation/selection carried out by tree kernelscan provide useful insights into the underlying linguistic phenomena.
Other advantagescoming from the use of tree kernels are the following.First, we can implement them very quickly as the feature extractor module onlyrequires the writing of a general procedure for subtree extraction.
In contrast, traditionalSRL systems use more than thirty features (e. g., Pradhan, Hacioglu, Krugler et al 2005),each of which requires the writing of a dedicated procedure.Second, their combination with traditional attribute?value models produces moreaccurate systems, also when using the same machine learning algorithm in the combi-nation, because the feature spaces are very different.Third, we can carry out feature engineering using kernel combinations andmarkingstrategies (Moschitti et al 2005a; Moschitti, Pighin, and Basili 2006).
This allows us toboost the SRL accuracy in a relatively simple way.Next, tree kernels generate large tree fragment sets which constitute back-offmodels for important syntactic features.
Using them, the learning algorithm generalizes194Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingbetter and produces a more accurate classifier, especially when the amount of trainingdata is scarce.Finally, once the learning algorithm using tree kernels has converged, we can iden-tify the most important structured features of the generated model.
One approach forsuch a reverse engineering process relies on the computation of the explicit featurespace, at least for the highest-weighted features (Kudo and Matsumoto 2003).
Oncethe most relevant fragments are available, they can be used to design novel effectiveattribute?value features (which in turn can be used to design more efficient classifiers,e.
g., with linear kernels) and inspire new linguistic theories.These points suggest that tree kernels should always be applied, at least for an initialstudy of the problem.
Unfortunately, they suffer from two main limitations: (a) poorimpact on boundary detection as, in this task, correct and incorrect arguments mayshare a large portion of the encoding trees (Moschitti 2004); and (b) more expensiverunning time and limited contribution to the overall accuracy if compared with manu-ally derived features (Cumby and Roth 2003).
Point (a) has been addressed byMoschitti,Pighin, and Basili (2006) by showing that a strategy ofmarking relevant parse-tree nodesmakes correct and incorrect subtrees for boundary detection quite different.
Point (b)can be tackled by studying approaches to kernel engineering that allow for the designof efficient and effective kernels.In this article, we provide a comprehensive study of the use of tree kernels for se-mantic role labeling.
For this purpose, we define tree kernels based on the compositionof two different feature functions: canonical mappings, which map sentence-parse treesin tree structures encoding semantic information, and feature extraction functions,which encode these trees in the actual feature space.
The latter functions explode thecanonical trees into all their substructures and, in the literature, are usually referred to astree kernels.
For instance, in Collins and Duffy (2002), Vishwanathan and Smola (2002),and Moschitti (2006a) different tree kernels extract different types of tree fragments.Given the heuristic nature of canonical mappings, we studied their propertiesby experimenting with them within support vector machines and with the data setprovided by CoNLL shared tasks (Carreras and Ma`rquez 2005).
The results show thatcarefully engineered tree kernels always boost the accuracy of the basic systems.
Mostimportantly, in complex tasks such as the re-ranking of semantic role annotations, theyprovide an easy way to engineer new features which enhance the state-of-the-art in SRL.In the remainder of this article, Section 2 presents traditional architectures for SRLand the typical features proposed in literature.
Tree kernels are formally introducedin Section 3, and Section 4 describes our modular architecture employing supportvector machines along with manually designed features, tree kernels (feature extractionfunctions), and their combinations.
Section 5 presents our structured features (canonicalmappings) inducing different kernels that we used for different SRL subtasks.
Theextensive experimental results obtained on the boundary recognition, role classification,and re-ranking stages are presented in Section 6.
Finally, Section 7 summarizes theconclusions.2.
Automatic Shallow Semantic ParsingThe recognition of semantic structures within a sentence relies on lexical and syntacticinformation provided by early stages of anNLP process, such as lexical analysis, part-of-speech tagging, and syntactic parsing.
The complexity of the SRL task mostly dependson two aspects: (a) the information is generally noisy, that is, in a real-world scenariothe accuracy and reliability of NLP subsystems are generally not very high; and (b) the195Computational Linguistics Volume 34, Number 2lack of a sound and complete linguistic or cognitive theory about the links betweensyntax and semantics does not allow an informed, deductive approach to the problem.Nevertheless, the large amount of available lexical and syntactic information favors theapplication of inductive approaches to the SRL task, which indeed is generally treatedas a combination of statistical classification problems.The next sections define the SRL task more precisely and summarize the mostrelevant work carried out to address these two problems.2.1 Problem DefinitionThe most well-known shallow semantic theories are studied in two different projects:PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, andLowe 1998).
The former is based on a linguistic model inspired by Levin?s verb classes(Levin 1993), focusing on the argument structure of verbs and on the alternation pat-terns that describe movements of verbal arguments within a predicate structure.
Thelatter refers to the application of frame semantics (Fillmore 1968) in the annotation ofpredicate?argument structures based on frame elements (semantic roles).
These theorieshave been investigated in two CoNLL shared tasks (Carreras and Ma`rquez 2004, 2005)and a Senseval-3 evaluation (Litkowski 2004), respectively.Given a sentence and a predicate word, an SRL system outputs an annotation of thesentence in which the sequences of words that make up the arguments of the predicateare properly labeled, for example:[Arg0 He] got [Arg1 his money] [C-V back]1in response to the input He got his money back.
This processing requires that: (1) thepredicates within the sentence are identified and (2) the word sequences that span theboundaries of each predicate argument are delimited and assigned the proper role label.The first sub-task can be performed either using statistical methods or hand-craftedlexical and syntactic rules.
In the case of verbal predicates, it is quite easy to writesimple rules matching regular expressions built on POS tags.
The second task is morecomplex and is generally viewed as a combination of statistical classification problems:The learning algorithms are trained to recognize the extension of predicate argumentsand the semantic role they play.2.2 Models for Semantic Role LabelingAn SRL model and the resulting architecture are largely influenced by the kind of dataavailable for the task.
As an example, a model relying on a shallow syntactic parserwould assign roles to chunks, whereas with a full syntactic parse of the sentence itwould be straightforward to establish a correspondence between nodes of the parse treeand semantic roles.
We focused on the latter as it has been shown to be more accurateby the CoNLL 2005 shared task results.According to the deep syntactic formulation, the classifying instances are pairsof parse-tree nodes which dominate the exact span of the predicate and the targetargument.
Such pairs are usually represented in terms of attribute?value vectors, where1 In PropBank notation, Arg0 and Arg1 represent the logical subject and the logical object of the targetverbal predicate, respectively.
C-V represents the particle of a phrasal-verb predicate.196Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingthe attributes describe properties of predicates, arguments, and theway they are related.There is large agreement on an effective set of linguistic features (Gildea and Jurafsky2002; Pradhan, Hacioglu, Krugler, et al 2005) that have been employed in the vastmajority of SRL systems.
The most relevant features are summarized in Table 1.Once the representation for the predicate?argument pairs is available, a multi-classifier is used to recognize the correct node pairs, namely, nodes associated withcorrect arguments (given a predicate), and assign them a label (which is the label of theargument).
This can be achieved by training a multi-classifier on n+ 1 classes, wherethe first n classes correspond to the different roles and the (n+ 1)th is a NARG (non-argument) class to which non-argument nodes are assigned.A more efficient solution consists in dividing the labeling process into two steps:boundary detection and argument classification.
A Boundary Classifier (BC) is a binaryclassifier that recognizes the tree nodes that exactly cover a predicate argument, thatis, that dominate all and only the words that belong to target arguments.
Then, suchnodes are classified by a Role Multi-classifier (RM) that assigns to each example themost appropriate label.
This two-step approach (Gildea and Jurafsky 2002) has theadvantage of only applying BC on all parse-tree nodes.
RM can ignore non-boundarynodes, resulting in a much faster classification.
Other approaches have extended thissolution and suggested other multi-stage classification models (e. g., Moschitti et al2005b in which a four-step hierarchical SRL architecture is described).After node labeling has been carried out, it is possible that the output of the argu-ment classifier does not result in a consistent annotation, as the labeling scheme maynot be compatible with the underlying linguistic model.
As an example, PropBank-styleannotations do not allow arguments to be nested.
This happens when two or moreTable 1Standard linguistic features employed by most SRL systems.Feature Name DescriptionPredicate Lemmatization of the predicate wordPath Syntactic path linking the predicate and an argument,e.
g., NN?NP?VP?VBXPartial path Path feature limited to the branching of the argumentNo-direction path Like Path, but without traversal directionsPhrase type Syntactic type of the argument nodePosition Relative position of the argument with respect to the predicateVoice Voice of the predicate, i. e., active or passiveHead word Syntactic head of the argument phraseVerb subcategorization Production rule expanding the predicate parent nodeNamed entities Classes of named entities that appear in the argument nodeHead word POS POS tag of the argument node head word (less sparse thanHead word)Verb clustering Type of verb?
direct object relationGoverning Category Whether the candidate argument is the verb subject or objectSyntactic Frame Position of the NPs surrounding the predicateVerb sense Sense information for polysemous verbsHead word of PP Enriched POS of prepositional argument nodes (e. g., PP-for, PP-in)First and last word/POS First and last words and POS tags of candidate argument phrasesOrdinal position Absolute offset of a candidate argument within a propositionConstituent tree distance Distance from the predicate with respect to the parse treeConstituent features Description of the constituents surrounding the argument nodeTemporal Cue Words Temporal markers which are very distinctive of some roles197Computational Linguistics Volume 34, Number 2overlapping tree nodes, namely, one dominating the other, are classified as positiveboundaries.The simplest solution relies on the application of heuristics that take into accountthe whole predicate?argument structure to remove the incorrect labels (e. g., Moschittiet al 2005a; Tjong Kim Sang et al 2005).
A much more complex solution consists in theapplication of some joint inference model to the whole predicate?argument structure,as in Pradhan et al (2004).
As an example, Haghighi, Toutanova, and Manning (2005)associate a posterior probability with each argument node role assignment, estimate thelikelihood of the alternative labeling schemes, and employ a re-ranking mechanism toselect the best annotation.Additionally, the most accurate systems participating in CoNLL 2005 shared task(Pradhan, Hacioglu, Ward et al 2005; Punyakanok et al 2005) use different syntacticviews of the same input sentence.
This allows the SRL system to recover from syntacticparser errors; for example, a prepositional phrase specifying the direct object of thepredicate would be attached to the verb instead of the argument.
This kind of errorprevents some arguments of the proposition from being recognized, as: (1) there maynot be a node of the parse tree dominating (all and only) the words of the correct se-quence; (2) a badly attached tree node may invalidate other argument nodes, generatingunexpected overlapping situations.The manual design of features which capture important properties of completepredicate?argument structures (also coming from different syntactic views) is quitecomplex.
Tree kernels are a valid alternative to manual design as the next sectionpoints out.3.
Tree KernelsTree kernels have been applied to reduce the feature design effort in the context ofseveral natural language tasks, for example, syntactic parsing re-ranking (Collins andDuffy 2002), relation extraction (Zelenko, Aone, and Richardella 2003), named entityrecognition (Cumby and Roth 2003; Culotta and Sorensen 2004), and semantic rolelabeling (Moschitti 2004).On the one hand, these studies show that the kernel ability to generate large featuresets is useful to quickly model new and not well understood linguistic phenomenain learning machines.
On the other hand, they show that sometimes it is possible tomanually design features for linear kernels that produce higher accuracy and fastercomputation time.
One of the most important causes of such mixed behavior is theinappropriate choice of kernel functions.
For example, in Moschitti, Pighin, and Basili(2006) andMoschitti (2006a), several kernels have been designed and shown to producedifferent impacts on the training algorithms.In the next sections, we briefly introduce the kernel trick and describe the subtree(ST) kernel devised in Vishwanathan and Smola (2002), the subset tree (SST) kerneldefined in Collins and Duffy (2002), and the partial tree (PT) kernel proposed inMoschitti (2006a).3.1 Kernel TrickThe main concept underlying machine learning for classification tasks is the automaticlearning of classification functions based on examples labeled with the class informa-tion.
Such examples can be described by means of feature vectors in an n dimensional198Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingspace over the real numbers, namely, n.
The learning algorithm uses space metricsover vectors, for example, the scalar product, to learn an abstract representation of allinstances belonging to the target class.For example, support vector machines (SVMs) are linear classifiers which learn ahyperplane f (x) = w?
x+ b = 0, separating positive from negative examples.
x is thefeature vector representation of a classifying object o, whereas w ?
n and b ?
 areparameters learned from the data by applying the Structural Risk Minimization principle(Vapnik 1998).
The object o is mapped to x via a feature function ?
: O ?
n, O beingthe set of the objects that we want to classify.
o is categorized in the target class only iff (x) ?
0.The kernel trick allows us to rewrite the decision hyperplane as:f (x) =(?i=1..lyi?ixi)?
x+ b =?i=1..lyi?ixi ?
x+ b =?i=1..lyi?i?
(oi) ??
(o)+ b = 0where yi is equal to 1 for positive examples and ?1 for negative examples, ?i ?
 with?i ?
0, oi ?i ?
{1, .., l} are the training instances and the product K(oi, o) = ??
(oi) ??
(o)?is the kernel function associated with the mapping ?.Note that we do not need to apply the mapping ?
; we can use K(oi, o) directly.This allows us, underMercer?s conditions (Shawe-Taylor and Cristianini 2004), to defineabstract kernel functions which generate implicit feature spaces.
A traditional exampleis given by the polynomial kernel: Kp(o1, o2) = (c+ x1 ?
x2)d, where c is a constant and dis the degree of the polynomial.
This kernel generates the space of all conjunctions offeature groups up to d elements.Additionally, we can carry out two interesting operations: kernel combinations, for example, K1 + K2 or K1 ?
K2 feature mapping compositions, for example,K(o1, o2) = ??
(o1) ??(o2)?
= ?
?B(?A(o1)) ?
?B(?A(o2))?Kernel combinations are very useful for integrating the knowledge provided by themanually defined features with the knowledge automatically obtained with structuralkernels; feature mapping compositions are useful methods to describe diverse kernelclasses (see Section 5).
In this perspective, we propose to split themapping?
by definingour tree kernel as follows: Canonical Mapping, ?M(), in which a linguistic object (e. g., a syntacticparse tree) is transformed into a more meaningful structure (e. g., thesubtree corresponding to a verb subcategorization frame). Feature Extraction, ?S(), which maps the canonical structure in all itsfragments according to different fragment spaces S (e. g., ST, SST, and PT).For example, given the kernel KST = ?ST(o1) ?
?ST(o2), we can apply a canonicalmapping ?M(), obtaining KMST = ?ST(?M(o1)) ?
?ST(?M(o2)) =(?ST ?
?M)(o1) ?
(?ST ?
?M)(o2), which is a noticeably different kernel, which is induced by the mapping(?ST ?
?M).199Computational Linguistics Volume 34, Number 2In the remainder of this section we start the description of our engineered kernelsby defining three different feature extraction mappings based on three different kernelspaces (i. e., ST, SST, and PT).3.2 Tree Kernel SpacesThe kernels that we consider represent trees in terms of their substructures (fragments).The kernel function detects if a tree subpart (common to both trees) belongs to the fea-ture space that we intend to generate.
For this purpose, the desired fragments need to bedescribed.
We consider three main characterizations: the subtrees (STs) (Vishwanathanand Smola 2002), the subset trees (SSTs) or all subtrees (Collins and Duffy 2002), and thepartial trees (PTs) (Moschitti 2006a).As we consider syntactic parse trees, each node with its children is associated witha grammar production rule, where the symbol on the left-hand side corresponds to theparent and the symbols on the right-hand side are associated with the children.
Theterminal symbols of the grammar are always associated with tree leaves.A subtree (ST) is defined as a tree rooted in any non-terminal node along withall its descendants.
For example, Figure 1a shows the parse tree of the sentence Marybrought a cat together with its six STs.
A subset tree (SST) is a more general structurebecause its leaves can be non-terminal symbols.
For example, Figure 1(b) shows tenSSTs (out of 17) of the subtree in Figure 1a rooted in VP.
SSTs satisfy the constraint thatgrammatical rules cannot be broken.
For example, [VP [V NP]] is an SST which hastwo non-terminal symbols, V and NP, as leaves.
On the contrary, [VP [V]] is not anSST as it violates the production VP?V NP.
If we relax the constraint over the SSTs, weobtain a more general form of substructures that we call partial trees (PTs).
These can begenerated by the application of partial production rules of the grammar; consequently[VP [V]] and [VP [NP]] are valid PTs.
It is worth noting that PTs consider the positionof the children as, for example, [A [B][C][D]] and [A [D][C][B]] only share singlechildren, i.e., [A [B]], [A [C]], and [A [D]].Figure 1c shows that the number of PTs derived from the same tree as before is stillhigher (i. e., 30 PTs).
These numbers provide an intuitive quantification of the differentdegrees of information encoded by each representation.Figure 1Example of (a) ST, (b) SST, and (c) PT fragments.200Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling3.3 Feature Extraction FunctionsThe main idea underlying tree kernels is to compute the number of common substruc-tures between two trees T1 and T2 without explicitly considering the whole fragmentspace.
In the following, we report on the Subset Tree (SST) kernel proposed in Collinsand Duffy (2002).
The algorithms to efficiently compute it along with the ST and PTkernels can be found in Moschitti (2006a).Given two trees T1 and T2, let { f1, f2, ..} = F be the set of substructures (fragments)and Ii(n) be equal to 1 if fi is rooted at node n, 0 otherwise.
Collins and Duffy?s kernel isdefined asK(T1,T2) =?n1?NT1?n2?NT2?
(n1,n2) (1)where NT1 and NT2 are the sets of nodes in T1 and T2, respectively, and ?
(n1,n2) =?|F|i=1 Ii(n1)Ii(n2).
The latter is equal to the number of common fragments rooted in nodesn1 and n2.?
can be computed as follows:1.
If the productions (i.e.
the nodes with their direct children) at n1 and n2are different, then?
(n1,n2) = 0.2.
If the productions at n1 and n2 are the same, and n1 and n2 only haveleaf children (i.e., they are pre-terminal symbols), then ?
(n1,n2) = 1.3.
If the productions at n1 and n2 are the same, and n1 and n2 arenot pre-terminals, then?
(n1,n2) =?nc(n1 )j=1 (1+?
(cjn1 , cjn2 )), wherenc(n1) is the number of children of n1 and cjn is the j-th child of n.Such tree kernels can be normalized and a ?
factor can be added to reduce theweight of large structures (refer to Collins and Duffy [2002] for a complete description).3.4 Related WorkAlthough the literature on SRL is extensive, there is almost no study of the use of treekernels for its solution.
Consequently, the reported research is mainly based on diversenatural language learning problems tackled by means of tree kernels.In Collins and Duffy (2002), the SST kernel was experimented with using the votedperceptron for the parse tree re-ranking task.
A combination with the original PCFGmodel improved the syntactic parsing.
Another interesting kernel for re-ranking wasdefined in Toutanova, Markova, andManning (2004).
This represents parse trees as listsof paths (leaf projection paths) from leaves to the top level of the tree.
It is worth notingthat the PT kernel includes tree fragments identical to such paths.In Kazama and Torisawa (2005), an interesting algorithm that speeds up the averagerunning time is presented.
This algorithm looks for node pairs in which the rootedsubtrees share many substructures (malicious nodes) and applies a transformation tothe trees rooted in such nodes to make the kernel computation faster.
The results showa several-hundred-fold speed increase with respect to the basic implementation.201Computational Linguistics Volume 34, Number 2In Zelenko, Aone, and Richardella (2003), two kernels over syntactic shallowparser structures were devised for the extraction of linguistic relations, for example,person-affiliation.
To measure the similarity between two nodes, the contiguous stringkernel and the sparse string kernelwere used.
In Culotta and Sorensen (2004) such kernelswere slightly generalized by providing a matching function for the node pairs.
Thetime complexity for their computation limited the experiments to a data set of just200 news items.In Shen, Sarkar, and Joshi (2003), a tree kernel based on lexicalized tree adjoininggrammar (LTAG) for the parse re-ranking task was proposed.
The subtrees induced bythis kernel are built using the set of elementary trees as defined by LTAG.In Cumby and Roth (2003), a feature description language was used to extract struc-tured features from the syntactic shallow parse trees associated with named entities.Their experiments on named entity categorization showed that when the descriptionlanguage selects an adequate set of tree fragments the voted perceptron algorithmincreases its classification accuracy.
The explanationwas that the complete tree fragmentset contains many irrelevant features and may cause overfitting.In Zhang, Zhang, and Su (2006), convolution tree kernels for relation extractionwere applied in a way similar to the one proposed in Moschitti (2004).
The combina-tion of standard features along with several tree subparts, tailored according to theirimportance for the task, produced again an improvement on the state of the art.Such previous work, as well as that described previously, show that tree kernelscan efficiently represent syntactic objects, for example, constituent parse trees, in hugefeature spaces.
The next section describes our SRL system adopting tree kernels withinSVMs.4.
A State-of-the-Art Architecture for Semantic Role LabelingAmeaningful study of tree kernels for SRL cannot be carried out without a comparisonwith a state-of-the-art architecture: Kernel models that improve average performingsystems are just a technical exercise whose findings would have a reduced value.
Astate-of-the-art architecture, instead, can be used as a basic system upon which treekernels should improve.
Because kernel functions in general introduce a sensible slow-down with respect to the linear approach, we also have to consider efficiency issues.These aims drove us in choosing the following components for our SRL system: SVMs as our learning algorithm; these provide both a state-of-the-artlearning model (in terms of accuracy) and the possibility of usingkernel functions a two-stage role labeling module to improve learning and classificationefficiency; this comprises:?
a feature extractor that can represent candidate arguments usingboth linear and structured features?
a boundary classifier (BC)?
a role multi-classifier (RM), which is obtained by applying the OVA(One vs. All) approach a conflict resolution module, that is, a software component that resolvesinconsistencies in the annotations using either a rule-based approachor a tree kernel classifier; the latter allows experimentation with202Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingthe classification of complete predicate?argument annotations in correctand incorrect structures a joint inference re-ranking module, which employs a combination ofstandard features and tree kernels to rank alternative candidate labelingschemes for a proposition; this module, as shown in Gildea and Jurafsky(2002), Pradhan et al (2004), and Haghighi, Toutanova, and Manning(2005), is mandatory in order to achieve state-of-the-art accuracyWe point out that we did not use any heuristic to filter out the nodes which arelikely to be incorrect boundaries, for example, as done in Xue and Palmer (2004).
On theone hand, this makes the learning and classification phases more complex because theyinvolve more instances.
On the other hand, our results are not biased by the quality ofthe heuristics, leading to more meaningful findings.In the remainder of this section, we describe the main functional modules of ourarchitecture for SRL and introduce some basic concepts about the use of structuredfeatures for SRL.
Specific feature engineering for the above SRL subtasks is describedand discussed in Section 5.4.1 A Basic Two-Stage Role Labeling SystemGiven a sentence in natural language, our SRL system identifies all the verb predicatesand their respective arguments.
We divide this step into three subtasks: (a) predicatedetection, which can be carried out by simple heuristics based on part-of-speech infor-mation, (b) the detection of predicate?argument boundaries (i. e., the span of their wordsin the sentence), and (c) the classification of the argument type (e. g., Arg0 or ArgM inPropBank).The standard approach to learning both the detection and the classification ofpredicate arguments is summarized by the following steps:1.
Given a sentence from the training set, generate a full syntactic parse tree;2. let P and A be the set of predicates and the set of parse-tree nodes (i. e., thepotential arguments), respectively;3. for each pair ?p, a?
?
P ?A: extract the feature representation, ?
(p, a), (e. g., attribute?values ortree fragments [see Section 3.1]); if the leaves of the subtree rooted in a correspond to all andonly the words of one argument of p (i. e., a exactly covers anargument), add ?
(p, a) in E+ (positive examples), otherwiseadd it in E?
(negative examples).For instance, given the example in Figure 2(a), we would consider all the pairs ?p, a?where p is the node associated with the predicate took and a is any other tree node notoverlapping with p. If the node a exactly covers the word sequences John or the book,then ?
(p, a) is added to the set E+, otherwise it is added to E?, as in the case of the node(NN book).The E+ and E?
sets are used to train the boundary classifier.
To train the rolemulticlassifier, the elements of E+ can be reorganized as positive E+argi and negative E?argiexamples for each role type i.
In this way, a binary OVA classifier for each argument203Computational Linguistics Volume 34, Number 2Figure 2Positive (framed) and negative (unframed) examples of candidate argument nodes for thepropositions (a) [Arg0 John] took [Arg1 the book] and read its title and (b) [Arg0 John] took thebook and read [Arg1 its title].i can be trained.
We adopted this solution following Pradhan, Hacioglu, Krugler et al(2005) because it is simple and effective.
In the classification phase, given an unseensentence, all the pairs ?p, a?
are generated and classified by each individual role classifierCi.
The argument label associated with the maximum among the scores provided by Ciis eventually selected.The feature extraction function ?
can be implemented according to different lin-guistic theories and intuitions.
From a technical point of view, we can use ?
to map?p, a?
in feature vectors or in structures to be used in a tree kernel function.
The nextsection describes our choices in more detail.4.2 Linear and Structured RepresentationOur feature extractor module and our learning algorithms are designed to cope withboth linear and structured features, used for the different stages of the SRL process.The standard features that we adopted are shown in Table 1.
They include: the Phrase Type, Predicate Word, Head Word, Governing Category, Position,and Voice defined in Gildea and Jurafsky (2002); the Partial Path, No Direction Path, Constituent Tree Distance, Head WordPOS, First and Last Word/POS, Verb Subcategorization, and Head Word of theNoun Phrase in the Prepositional Phrase proposed in Pradhan, Hacioglu,Krugler et al (2005); and the Syntactic Frame defined in Xue and Palmer (2004).We indicate with structured features the basic syntactic structures extracted fromthe sentence-parse tree or their canonical transformation (see Section 3.1).
In particular,we focus on the minimal spanning tree that includes the predicate along with all of itsarguments.More formally, given a parse tree t, a node set spanning tree (NST) over a set ofnodes Nt = {n1, .
.
.
,nk} is a partial tree of t that (1) is rooted at the deepest level and (2)contains all and only the nodes ni ?
Nt, along with their ancestors and descendants.
AnNST can be built as follows.
For any choice of Nt, we call r the lowest common ancestor204Moschitti, Pighin, and Basili Tree Kernels for Semantic Role LabelingFigure 3(a) A sentence parse tree, the correct ASTns associated with two different predicates (b,c), and (d)a correct AST1 relative to the argument Arg1 its title of the predicate read.of n1, .
.
.
,nk.
Then, from the set of all the descendants of r, we remove all the nodes njthat: (1) do not belong to Nt and (2) are neither ancestors nor descendants of any nodebelonging to Nt.Because predicate arguments are associatedwith tree nodes, we can define the pred-icate argument spanning tree (ASTn) of a predicate argument node setAp = {a1, .
.
.
, an}as the NST over these nodes and the predicate node, that is, the node exactly coveringthe predicate p.2 An ASTn corresponds to the minimal parse subtree whose leaves areall and only the word sequences belonging to the arguments and the predicate.
Forexample, Figure 3a shows the parse tree of the sentence: John took the book and read itstitle.
took{ARG0,ARG1} and read{ARG0,ARG1} are two ASTn structures associated with the twopredicates took and read, respectively, and are shown in Figure 3b and 3c.For each predicate, only one NST is a valid ASTn.
Careful manipulations of an ASTncan be employed for those tasks that require a representation of the whole predicate?argument structure, for example, overlap resolution or proposition re-ranking.It is worth noting that the predicate?argument feature, or PAF in Moschitti (2004),is a canonical transformation of the ASTn in the subtree including the predicate p andonly one of its arguments.
For the sake of uniform notation, PAF will be referred to asAST1 (argument spanning tree), the subscript 1 stressing the fact that the structure onlyencompasses one of the predicate arguments.
An example AST1 is shown in Figure 3d.Manipulations of an AST1 structure can lead to interesting tree kernels for local learningtasks, such as boundary detection and argument classification.Regardless of the adopted feature space, our multiclassification approach suffersfrom the problem of selecting both boundaries and argument roles independently ofthe whole structures.
Thus, it is possible that (a) two labeled nodes refer to the samearguments (node overlaps) and (b) invalid role sequences are generated (e. g., Arg0,Arg0, Arg0, .
.
.
).
Next, we describe our approach to solving such problems.4.3 Conflict ResolutionWe call a conflict, or ambiguity, or overlap resolution a stage of the SRL processwhich resolves annotation conflicts that invalidate the underlying linguistic model.
This2 The ASTn of a predicate p and its argument nodes {a1, .
.
.
, an}, will also be referred to as p{a1,..., an}.205Computational Linguistics Volume 34, Number 2happens, for example, when both a node and one of its descendants are classified aspositive boundaries, namely, they received a role label.
We say that such nodes areoverlapping as their leaf (i. e., word) sequences overlap.
Because this situation is notallowed by the PropBank annotation definition, we need a method to select the mostappropriate word sequence.
Our system architecture can employ one of three differentdisambiguation strategies: a basic solution which, given two overlapping nodes, randomly selectsone to be removed; the following heuristics:1.
The node causing the major number of overlaps is removed, forexample, a node which dominates two nodes labeled as arguments2.
Core arguments (i. e., arguments associated with thesubcategorization frame of the target verb) are always preferredover adjuncts (i. e., arguments that are not specific to verbs orverb senses)3.
In case the two previous rules do not eliminate all conflicts, thenodes located deeper in the tree are discarded; and a tree kernel?based overlap resolution strategy consisting of an SVMtrained to recognize non-clashing configurations that often correspondto correct propositions.The latter approach consists of: (1) a softwaremodule that generates all the possible non-overlapping configurations of nodes.
These are built using the output of the local nodeclassifiers by generating all the permutations of argument nodes of a predicate and re-moving the configurations that contain at least one overlap; (2) an SVM trained on suchnon-overlapping configurations, where the positive examples are correct predicate?argument structures (although eventually not complete) and negative ones are not.
Attesting time, we classify all the alternative non-clashing configurations.
In case morethan one structure is selected as correct, we choose the one associated with the highestSVM score.These disambiguationmodules can be invoked after either the BC or the RM classifi-cation.
The different information available after each phase can be used to design differ-ent kinds of features.
For example, the knowledge of the candidate role of an argumentnode can be a key issue in the design of effective conflict resolution methodologies, forexample, by eliminating ArgX, ArgX, ArgX, .
.
.
sequences.
These different approachesare discussed in Section 5.2.The next section describes a more advanced approach that can eliminate overlapsand choose the most correct annotation for a proposition among a set of alternativelabeling schemes.4.4 A Joint Model for Re-RankingThe heuristics considered in the previous sections only act when a conflict is detected.In a real situation, many incorrect annotations are generated with no overlaps.
To dealwith such cases, we need a re-ranking module based on a joint BC and RM model assuggested in Haghighi, Toutanova, and Manning (2005).
Such a model is based on (1)206Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingan algorithm to evaluate the most likely labeling schemes for a given predicate, and (2)a re-ranker that sorts the labeling schemes according to their correctness.Step 1 uses the probabilities associated with each possible annotation of parse treenodes, hence requiring a probabilistic output from BC and RM.
As the SVM learningalgorithm produces metric values, we applied Platt?s algorithm (Platt 1999) to convertthem into probabilities, as already proposed in Pradhan, Ward et al (2005).
Theseposterior probabilities are then combined to generate the n labelings that maximize alikelihood measure.
Step 2 requires the training of an automatic re-ranker.
This can bedesigned using a binary classifier that, given two annotations, decides which one ismore accurate.
We modeled such a classifier by means of three different kernels basedon standard features, structured features, and their combination.4.4.1 Evaluation of the N-best Annotations.
First, we converted the output of each node-classifier into a posterior probability conditioned by its output scores (Platt 1999).This method uses a parametric model to fit onto a sigmoid distribution the posteriorprobability P(y = 1, f ), where f is the output of the classifier and the parameters aredynamically adapted to give the best probability output.3 Second, we selected the nmost likely sequences of node labelings.
Given a predicate, the likelihood of a labelingscheme (or state) s for the K candidate argument nodes is given by:p(s) =K?i=1p?i (l), p?i (l) ={pi(li)pi(ARG) if li = NARG(1?
pi(ARG))2 otherwise(2)where pi(l) is the probability of node i being assigned the label l, and p?i (l) is the sameprobability weighted by the probability pi(ARG) of the node being an argument.
If l =NARG (not an argument) then both terms evaluate to (1?
pi(ARG)) and the likelihoodof the NARG label assignment is given by (1?
pi(ARG))2.To select the n states associated with the highest probability, we cannot evaluatethe likelihood of all possible states because they are exponential in number.
In orderto reduce the search space we (a) limit the number of possible labelings of each nodeto m and (b) avoid traversing all the states by applying a Viterbi algorithm to searchfor the most likely labeling schemes.
From each state we generate the states in whicha candidate argument is assigned different labels.
This operation is bound to output atmost n states which are generated by traversing a maximum of n?m states.
Therefore,in the worst case scenario the number of traversed states is V = n?m?
k, k being thenumber of candidate argument nodes in the tree.During the search we also enforce overlap resolution policies.
Indeed, for any givenstate in which a node nj is assigned a label l = NARG, we generate all; and only thestates in which all the nodes that are dominated by nj are assigned the NARG label.4.4.2 Modeling an Automatic Re-Ranker.
The Viterbi algorithm generates the nmost likelyannotations for the proposition associated with a predicate p. These can be used to buildannotation pairs, ?si, sj?, which, in turn, are used to train a binary classifier that decides if3 We actually implemented the pseudo-code proposed in Lin, Lin, and Weng (2003) which, with respectto Platt?s original formulation, is theoretically demonstrated to converge and avoids some numericaldifficulties that may arise.207Computational Linguistics Volume 34, Number 2si is more accurate that sj.
Each candidate proposition si can be described by a structuredfeature ti and a vector of standard features vi.
As a whole, an example ei is described bythe tuple ?t1i , t2i , v1i , v2i ?, where t1i and v1i refer to the first candidate annotation, whereas t2iand v2i refer to the second one.
Given such data, we can define the following re-rankingkernels:Ktr(e1, e2) = Kt(t11, t12)+ Kt(t21, t22)?
Kt(t11, t22)?
Kt(t21, t12)Kpr(e1, e2) = Kp(v11, v12)+ Kp(v21, v22)?
Kp(v11, v22)?
Kp(v21, v12)where Kt is one of the tree kernel functions defined in Section 3 and Kp is a polynomialkernel applied to the feature vectors.
The final kernel that we use is the followingcombination:K(e1, e2) =Ktr(e1, e2)|Ktr(e1, e2)|+Kpr(e1, e2)|Kpr(e1, e2)|Previous sections have shown how our SRL architecture exploits tree kernel func-tions to a large extent.
In the next section, we describe in more detail our structuredfeatures and the engineering methods applied for the different subtasks of the SRLprocess.5.
Structured Feature EngineeringStructured features are an effective alternative to standard features in many aspects.
Animportant advantage is that the target feature space can be completely changed evenby small modifications of the applied kernel function.
This can be exploited to identifyfeatures relevant to learning problems lacking a clear and sound linguistic or cognitivejustification.As shown in Section 3.1, a kernel function is a scalar product ?
(o1) ??
(o2), where?
is a mapping in an Euclidean space, and o1 and o2 are the target data, for example,parse trees.
To make the engineering process easier, we decompose ?
into a canonicalmapping, ?M, and a feature extraction function, ?S, over the set of incoming parsetrees.
?M transforms a tree into a canonical structure equivalent to an entire class ofinput parses and ?S shatters an input tree into its subparts (e. g., subtrees, subset trees,or partial trees as described in Section 3).
A large number of different feature spaces canthus be explored by suitable combinations ?
= ?S ?
?M of mappings.We study different canonical mappings to capture syntactic/semantic aspects usefulfor SRL.
In particular, we define structured features for the different phases of the SRLprocess, namely, boundary detection, argument classification, conflict resolution, andproposition re-ranking.5.1 Structures for Boundary Detection and Argument ClassificationThe AST1 or PAF structures, already mentioned in Section 4.2, have shown to be veryeffective for argument classification but not for boundary detection.
The reason is thattwo nodes that encode correct and incorrect boundaries may generate very similarAST1s and, consequently, have many fragments in common.
To solve this problem, we208Moschitti, Pighin, and Basili Tree Kernels for Semantic Role LabelingFigure 4Parse tree of the example proposition [Arg0 Paul] delivers [Arg1 a talk in formal style].Figure 5(a) AST1, (b) ASTm1 , and (c) ASTcm1 structures relative to the argument Arg1 a talk in formal style ofthe predicate delivers of the example parse tree shown in Figure 4.specify the node that exactly covers the target argument node by simply marking it (ormarking all its descendants) with the label B, denoting the boundary property.For example, Figure 4 shows the parse tree of the sentence Paul delivers a talk informal style, highlighting the predicate with its two arguments, that is, Arg0 and Arg1.Figure 5 shows the AST1, ASTm1 , and ASTcm1 , that is, the basic structure, the structurewith the marked argument node, and the completely marked structure, respectively.To understand the usefulness of node-marking strategies, we can examine Figure 6.This reports the case in which a correct and an incorrect argument node are chosen byalso showing the corresponding AST1 and ASTm1 representations ((a) and (b)).
Figure 6cshows that the number of common fragments of two AST1 structures is 14.
This ismuch larger than the number of common ASTm1 fragments, that is, only 3 substructures(Figure 6d).Additionally, because the type of a target argument strongly depends on the typeand number of the other predicate arguments4 (Punyakanok et al 2005; Toutanova,4 This is true at least for core arguments.209Computational Linguistics Volume 34, Number 2Figure 6(a) AST1s and (b) ASTm1 s extracted for the same target argument with their respective (c,b)common fragment spaces.Haghighi, and Manning 2005), we should extract features from the whole predicateargument structure.
In contrast, AST1s completely neglect the information (i. e., the treeportions) related to non-target arguments.One way to use this further information with tree kernels is to use the minimumsubtree that spans all the predicate?argument structures, that is, the ASTn defined inSection 4.2.However, ASTns pose two problems.
First, we cannot use them for the boundarydetection task since we do not know the predicate?argument structure yet.
We canderive the ASTn (its approximation) from the nodes selected by a boundary classifier,that is, the nodes that correspond to potential arguments.
Such approximated ASTnscan be easily used in the argument classification stage.Second, an ASTn is the same for all the arguments in a proposition, thus we need away to differentiate it for each target argument.
Again, we canmark the target argumentnode as shown in the previous section.
We refer to this subtree as a marked targetASTn (ASTmtn ).
However, for large arguments (i. e., spread over a large part of thesentence tree) the substructures?
likelihood of being part of different arguments is quitehigh.To address this problem, we can mark all the nodes that descend from the targetargument node.
We refer to this structure as a completely marked targetASTn (ASTcmtn ).ASTcmtn s may be seen as AST1s enriched with new information coming from the otherarguments (i. e., the non-marked subtrees).
Note that if we only consider the AST1subtree from a ASTcmtn , we obtain ASTcm1 .210Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling5.2 Structured Features for Conflict ResolutionThis section describes structured features employed by the tree kernel?based conflictresolution module of the SRL architecture described in Section 4.3.
This subtask isperformed by means of:1.
A first annotation of potential arguments using a high recall boundaryclassifier and, eventually, the role information provided by a rolemulticlassifier (RM).2.
An ASTn classification step aiming at selecting, among the substructuresthat do not contain overlaps, those that are more likely to encode thecorrect argument set.The set of argument nodes recognized by BC can be associated with a subtree of thecorresponding sentence parse, which can be classified using tree kernel functions.
Theseshould evaluate whether a subtree encodes a correct predicate?argument structure ornot.
As it encodes features from the whole predicate?argument structure, the ASTn thatwe introduced in Section 4.2 is a structure that can be employed for this task.Let Ap be the set of potential argument nodes for the predicate p output by BC; theclassifier examples are built as follows: (1) we look for node pairs ?n1,n2?
?
Ap ?
Apwhere n1 is the ancestor of n2 or vice versa; (2) we create two node sets A1 = A?
{n1}and A2 = A?
{n2} and classify the two NSTs associated with A1 and A2 with the treekernel classifier to select the most correct set of argument boundaries.
This procedurecan be generalized to a set of overlapping nodes Owith more than two elements, as wesimply need to generate all and only the permutations of A?s nodes that do not containoverlapping pairs.Figure 7 shows a working example of such amulti-stage classifier.
In (Figure 7a), theBC labels as potential arguments four nodes (circled), three of which are overlappingFigure 7An overlap situation (a) and the candidate solutions resulting from the employment of thedifferent marking strategies.211Computational Linguistics Volume 34, Number 2(in bold circles).
The overlap resolution algorithm proposes two solutions (Figure 7b)of which only one is correct.
In fact, according to the second solution, the preposi-tional phrase of the book would incorrectly be attached to the verbal predicate, thatis, in contrast with the parse tree.
The ASTn classifier, applied to the two NSTs,should detect this inconsistency and provide the correct output.
Figure 7 also high-lights a critical problem the ASTn classifier has to deal with: as the two NSTs areperfectly identical, it is not possible to distinguish between them using only theirfragments.In order to engineer novel features, we simply add the boundary information pro-vided by BC to the NSTs.
We mark with a progressive number the phrase type cor-responding to an argument node, starting from the leftmost argument.
We call theresulting structure an ordinal predicate?argument spanning tree (ASTordn ).
For example,in the first NST of Figure 7c, we mark as NP-0 and NP-1 the first and second argumentnodes, whereas in the second NST, we have a hypothesis of three arguments on threenodes that we transform as NP-0, NP-1, and PP-2.This simple modification enables the tree kernel to generate features useful for dis-tinguishing between two identical parse trees associated with different argument struc-tures.
For example, for the first NST the fragments [NP-1 [NP PP]], [NP [DT NN]], and[PP [IN NP]] are generated.
They no longer match with the fragments of the secondNST [NP-0 [NP PP]], [NP-1 [DT NN]], and [PP-2 [IN NP]].We also experimented with another structure, the marked predicate?argumentspanning tree (ASTmn ), in which each argument node is marked with a role label as-signed by a role multi-classifier (RM).
Of course, this model requires a RM to classify allthe nodes recognized by BC first.
An example ASTmn is shown in Figure 7d.5.3 Structures for Proposition Re-RankingIn Section 4.4, we presented our re-ranking mechanism, which is inspired by the jointinference model described in Haghighi, Toutanova, and Manning (2005).
Designingstructured features for the re-ranking classifier is complex in many aspects.
Unlikethe other structures that we have discussed so far, the defined mappings should:(1) preserve as much information as possible about the whole predicate?argumentstructure; (2) focus the learning algorithm on the whole structure; and (3) be ableto identify those small differences that distinguish more or less accurate labelingschemes.
Among the possible solutions that we have explored, three are especiallyinteresting in terms of accuracy improvement or linguistic properties, and are describedhereinafter.The ASTcmn (completely marked ASTn, see Figure 8a) is an ASTn in which eachargument node label is enriched with the role assigned to the node by RM.
The la-bels of the descendants of each argument node are modified accordingly, down topre-terminal nodes.
The ASTcmtn is a variant of ASTcmn in which only the target ismarked.
Marking a node descendant is meant to force substructures matching onlyamong homogeneous argument types.
This representation should provide rich syn-tactic and lexical information about the parse tree encoding the predicate?argumentstructure.The PAS (predicate?argument structure, see Figure 8b) is a completely differentstructure that preserves the parse subtrees associated with each argument node whilediscarding the intra-argument syntactic parse information.
Indeed, the syntactic linksbetween the argument nodes are represented as a dummy 1-level tree, which appearsin any PAS and therefore does not influence the evaluation of similarity between pairs212Moschitti, Pighin, and Basili Tree Kernels for Semantic Role LabelingFigure 8Different representations of the same proposition.of structures.
This structure accommodates the predicate and all the arguments of anannotation in a sequence of seven slots.5 To each slot is attached an argument label towhich in turn is attached the subtree rooted in the argument node.
The predicate isrepresented by means of a pre-terminal node labeled rel to which the lemmatization ofthe predicate word is attached as a leaf node.
In general, a proposition consists of marguments, with m ?
6, where m varies according to the predicate and the context.
Toguarantee that predicate structures with a different number of arguments are matchedin the SST kernel function, we attach a dummy descendant marked null to the slots notfilled by an argument.The PAStl (type-only, lemmatized PAS, see Figure 8c) is a specialization of the PASthat only focuses on the syntax of the predicate?argument structure, namely, the typeand relative position of each argument, minimizing the amount of lexical and syntacticinformation derived from the parse tree.
The differences with the PAS are that: (1) eachslot is attached to a pre-terminal node representing the argument type and a terminalnode whose label indicates the syntactic type of the argument; and (2) the predicateword is lemmatized.The next section presents the experiments used to evaluate the effectiveness of theproposed canonical structures in SRL.5 We assume that predicate?argument structures cannot be composed by more than six arguments, whichis generally true.213Computational Linguistics Volume 34, Number 26.
ExperimentsThe experiments aim to measure the contribution and the effectiveness of our proposedkernel engineering models and of the diverse structured features that we designed(Section 5).
From this perspective, the role of feature extraction functions is notfundamental because the study carried out in Moschitti (2006a) strongly suggests thatthe SST (Collins and Duffy 2002) kernel produces higher accuracy than the PT kernelwhen dealing with constituent parse trees, which are adopted in our study.6 We thenselected the SST kernel and designed the following experiments:(a) A study of canonical functions based on node marking for boundary detectionand argument classification, that is, ASTm1 (Section 6.2).
Moreover, as the standardfeatures have shown to be effective, we combined them with ASTm1 based kernels onthe boundary detection and classification tasks (Section 6.2).
(b) We varied the amount of training data to demonstrate the higher generalizationability of tree kernels (Section 6.3).
(c) Given the promising results of kernel engineering, we also applied it to solve a morecomplex task, namely, conflict resolution in SRL annotations (see Section 6.4).
As thisinvolves the complete predicate?argument structure, we could test advanced canonicalfunctions generating ASTn, ASTordn , and ASTmn .
(d) Previous work has shown that re-ranking is very important in boosting the accuracyof SRL.
Therefore, we tested advanced canonical mappings, that is, those based onASTcmn , PAS, and PAStl, on such tasks (Section 6.5).6.1 General SetupThe empirical evaluations were mostly carried out within the setting defined in theCoNLL 2005 shared task (Carreras and Ma`rquez 2005).
As a target data set, weused the PropBank7 and the automatic Charniak parse trees of the sentences of PennTreeBank 2 corpus8 (Marcus, Santorini, and Marcinkiewicz 1993) from the CoNLL 2005shared-task data.9 We employed the SVM-light-TK software10, which encodes fast treekernel evaluation (Moschitti 2006b), and combinations betweenmultiple feature vectorsand trees in the SVM-light software (Joachims 1999).
We used the default regularizationparameter (option -c) and ?
= 0.4 (see Moschitti [2004]).6.2 Testing Canonical Functions Based on Node MarkingIn these experiments, we measured the impact of node marking strategies on boundarydetection (BD) and the complete SRL task, that is, BD and role classification (RC).
Weemployed a configuration of the architecture described in Section 4 and previously6 Of course the PT kernel may be much more accurate in processing PAS and PAStl because these are notsimply constituent parse trees.
Nevertheless, a study of the PT kernel potential is beyond the purposeof this article.7 http://www.cis.upenn.edu/?ace.8 http://www.cis.upenn.edu/?treebank.9 http://www.lsi.upc.edu/?srlconll/.10 http://ai-nlp.info.uniroma2.it/moschitti/.214Moschitti, Pighin, and Basili Tree Kernels for Semantic Role LabelingTable 2Number of arguments (Arguments) and of unrecoverable arguments (Unrecoverable) due toparse tree errors in Sections 2, 3, and 24 of the Penn TreeBank/PropBank.Sec.
Arguments Unrecoverable2 198,373 454 (0.23%)3 147,193 347 (0.24%)24 139,454 731 (0.52%)Table 3Comparison between different models on Boundary Detection and the complete Semantic RoleLabeling tasks.
The training set is constituted by the first 1 million instances from Sections 02?06for the boundary classifier and all arguments from Sections 02?21 for the role multiclassifier(253,129 instances).
The performance is measured against Section 24 (149,140 instances).Boundary Detection Semantic Role LabelingKernels P R F1 P R F1AST1 75.75% 71.68% 73.66 64.71% 61.71% 63.17ASTm1 77.32% 74.80% 76.04 66.58% 64.87% 65.71Poly 82.18% 79.19% 80.66 75.86% 72.60% 73.81Poly+AST1 81.74% 80.71% 81.22 74.23% 73.62% 73.92Poly+ASTm1 81.64% 80.73% 81.18 74.36% 73.87% 74.11adopted in Moschitti et al (2005b), in which the simple conflict resolution heuristic isapplied.
The results were derived within the CoNLL setting by means of the relatedevaluator.In more detail, in the BD experiments, we used the first million instances from thePenn TreeBank Sections 2?6 for training11 and Section 24 for testing.
Our classificationmodel applied to this data replicates the results obtained in the CoNLL 2005 sharedtask, that is, the highest accuracy in BD among the systems using only one parsetree and one learning algorithm.
For the complete SRL task, we used the previousBC and all the available data, that is, the sections from 2 to 21, for training the rolemulticlassifier.It is worth mentioning that, as the automatic parse trees contain errors, somearguments cannot be associated with any covering node; thus we cannot extract atree representation for them.
In particular, Table 2 shows the number of arguments(column 2) for sections 2, 3, and 24 as well as the number of arguments that we could nottake into account (Unrecoverable) due to the lack of parse tree nodes exactly coveringtheir word spans.
Note how Section 24 of the Penn TreeBank (which is not part of theCharniak training set) is much more affected by this problem.Given this setting, the impact of node marking can be measured by comparing theAST1 and the ASTm1 based kernels.
The results are reported in the rows AST1 and ASTm1of Table 3.
Columns 2, 3, and 4 show their Precision, Recall, and F1 measure on BD andcolumns 5, 6, and 7 report the performance on SRL.
We note that marking the argument11 This was the most expensive process in terms of training time, requiring more than one week.215Computational Linguistics Volume 34, Number 2node simplifies the generalization process as it improves both tasks by about 3.5 and 2.5absolute percentage points, respectively.However, Row Poly shows that the polynomial kernel using state-of-the-art fea-tures (Moschitti et al 2005b) outperforms ASTm1 by about 4.5 percentage points in BDand 8 points in the SRL task.
The main reason is that the employed tree structuresdo not explicitly encode very important features like the passive voice or predicateposition.
In Moschitti (2004), these are shown to be very effective especially when usedin polynomial kernels.
Of course, it is possible to engineer trees including these andother standard features with a canonical mapping, but the aim here is to provide newinteresting representations rather than to abide by the simple exercise of representingalready designed features within tree kernel functions.
In other words, we follow theidea presented in Moschitti (2004), where tree kernels were suggested as a means toderive new features rather than generate a stand-alone feature set.Rows Poly+AST1 and Poly+ASTm1 investigate this possibility by presenting thecombination of polynomial and tree kernels.
Unfortunately, the results on both BD andSRL do not show enough improvement to justify the use of tree kernels; for example,Poly+ASTm1 improves Poly by only 0.52 in BD and 0.3 in SRL.
The small improvementis intuitively due to the use of (1) a state-of-the-art model as a baseline and (2) a verylarge amount of training data which decreases the contribution of tree features.
In thenext section an analysis in terms of training data will shed some light on the role of treekernels for BD and RC in SRL.6.3 The Role of Tree Kernels for Boundary Detection and Argument ClassificationThe previous section has shown that if a state-of-the-art model12 is adopted, then thetree kernel contribution is marginal.
On the contrary, if a non state-of-the-art model isadopted tree kernels can play a significant role.
To verify this hypothesis, we tested thepolynomial kernel over the standard feature vector proposed in Gildea and Jurafsky(2002) obtaining an F1 of 67.3, which is comparable with the ASTm1 model, that is 65.71.Moreover, a kernel combination produced a significant improvement of both modelsreaching an F1 of 70.4.Thus, the role of tree kernels relates to the design of features for novel linguistictasks for which the optimal data representation has not yet been developed.
For exam-ple, although SRL has been studied for many years and many effective features havebeen designed, representations for languages like Arabic are still not very well under-stood and raise challenges in the design of effective predicate?argument descriptions.However, this hypothesis on the usefulness of tree kernels is not completely satis-factory as the huge feature space produced by them should play a more important rolein predicate?argument representation.
For example, the many fragments extracted byan AST1 provide a very promising back-off model for the Path feature, which shouldimprove the generalization process of SVMs.As back-off models show their advantages when the amount of training datais small, we experimented with Poly, AST1, ASTm1 , Poly+AST1, and Poly+ASTm1 and12 The adopted model is the same as used in Moschitti et al (2005b), which is the most accurate among thesystems that use a single learning model, a single source of syntactic information, and no accurateinference mechanism.
If tree kernels improved this basic model they would likely improve theaccuracy of more complex systems as well.216Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingdifferent bins of training data, starting from a very small set, namely, 10,000 instances(1%) to 1 million (100%) of instances.
The results from the BD classifiers and thecomplete SRL task are very interesting and are illustrated by Figure 9.
We note severalthings.First, Figure 9a shows that with only 1% of data (i.e., 640 arguments) as positiveexamples, the F1 on BD of the ASTm1 kernel is surprisingly about 3 percentage pointshigher than the one obtained by the polynomial kernel (Poly) (i. e., the state of the art).When ASTm1 is combined with Poly the improvement reaches 5 absolute percentagepoints.
This suggests that tree kernels should always be used when small training datasets are available.Second, although the performance of AST1 is much lower than all the other models,its combination with Poly produces results similar to Poly+ASTm1 , especially whenthe amount of training data increases.
This, in agreement with the back-off property,indicates that the number of tree fragments is more relevant than their quality.Third, Figure 9b shows that as we increase training data, the advantage of usingtree kernels decreases.
This is rather intuitive as (i) in general less accurate data machinelearning models trained with enough data can reach the accuracy of the most accuratemodels, and (ii) if the hypothesis that tree kernels provide back-off models is true, a lotof training data makes them less critical, for example, the probability of finding the Pathfeature of a test instance in the training set becomes high.Figure 9Learning curves for BD (a and b) and the SRL task (c and d), where 100% of data corresponds to1 million candidate argument nodes for boundary detection and 64,000 argument nodes for roleclassification.217Computational Linguistics Volume 34, Number 2Table 4Boundary detection accuracy (F1) on gold-standard parse trees and ambiguous structuresemploying the different conflict resolution methodologies described in Section 4.3.RND HEU ASTordn73.13 71.50 91.11Finally, Figures 9c and 9d show learning curves13 similar to Figures 9a and 9b, butwith a reduced impact of tree kernels on the Poly model.
This is due to the reducedimpact of ASTm1 on role classification.
Such findings are in agreement with the resultsin Moschitti (2004), which show that for argument classification the SCF structure (avariant of the ASTmn ) is more effective.
Thus a comparison between learning curves ofPoly and SCF on RC may show a behavior similar to Poly and ASTm1 for BD.6.4 Conflict Resolution ResultsIn these experiments, we are interested in (1) the evaluation of the accuracy of ourtree kernel?based conflict resolution strategy and (2) studying the most appropriatestructured features for the task.A first evaluation was carried out over gold-standard Penn TreeBank parses andPropBank annotations.
We compared the alternative conflict resolution strategies imple-mented by our architecture (see Section 4.3), namely the random (RND), the heuristic(HEU), and a tree kernel?based disambiguator working with ASTordn structures.
Thedisambiguators were run on the output of BC, that is, without any information about thecandidate arguments?
roles.
BC was trained on Sections 2 to 7 with a high-recall linearkernel.
We applied it to classify Sections 8 to 21 and obtained 2,988 NSTs containing atleast one overlapping node.
These structures generated 3,624 positive NSTs (i. e., correctstructures) and 4,461 negative NSTs (incorrect structures) in which no overlap is present.We used them to train the ASTordn classifier.
The F1 measure on the boundary detectiontask was evaluated on the 385 overlapping annotations of Section 23, consisting of 642argument and 15,408 non-argument nodes.The outcome of this experiment is summarized in Table 4.
We note two points.
(1) The RND disambiguator (slightly) outperforms the HEU.
This suggests that theheuristics that we implemented were inappropriate for solving the problem.
It alsounderlines how difficult it is to explicitly choose the aspects that are relevant for acomplex, non-local task such as overlap resolution.
(2) The ASTordn classifier outperformsthe other strategies by about 20 percentage points, that is, 91.11 vs. 73.13 and 71.50.This datum along with the previous one is a good demonstration of how tree kernelscan be effectively exploited to describe phenomena whose relevant features are largelyunknown or difficult to represent explicitly.
It should be noted that a more accuratebaseline can be provided by using the Viterbi-style search (see Section 4.4.1).
However,the experiments in Section 6.5 show that the heuristics produce the same accuracy (atleast when the complete task is carried out).13 Note that using all training data, all the models reach lower F1s than the respective values shown inTable 3.
This happens because the data for training the role multiclassifier is restricted to the firstmillion instances, in other words, about 64,000 out of the total 253,129 arguments.218Moschitti, Pighin, and Basili Tree Kernels for Semantic Role LabelingTable 5SRL accuracy on different PropBank target sections in terms of F1 measure of the differentstructured features employed for conflict resolution.Target section ASTn ASTordn ASTmn21 73.7 77.3 78.723 68.9 71.2 72.1These experiments suggest that tree kernels are promising methods for resolvingannotation conflicts; thus, we tried to also select the most representative structuredfeatures (i. e., ASTn, ASTordn , or ASTmn ) when automatic parse trees are used.
We trainedBC on Sections 2?8, whereas, to achieve a very accurate argument classifier, we traineda role multi-classifier (RM) on Sections 2?21.
Then, we trained the ASTn, ASTordn , andASTmn classifiers on the output of BC.
To test BC, RM, and the tree kernel classifiers, weran two evaluations on Section 23 and Section 21.14Table 5 shows the F1 measure for the different tree kernels (columns 2, 3, and 4) forconflict resolution over the NSTs of Sections 21 and 23.
Several points should be noted.
(1) The general performance is much lower than that achieved on gold-standardtrees, as shown in Table 4.
This datum and the gap of about 6 percentage points betweenSections 21 and 23 confirm the impact of parsing accuracy on the subtasks of the SRLprocess.
(2) The ordinal numbering of arguments (ASTordn ) and the role type information(ASTmn ) provide tree kernels with more meaningful fragments because they improvethe basic model by about 4 percentage points.
(3) The deeper semantic information generated by the argument labels providesuseful clues for selecting correct predicate?argument structures because the ASTmnmodel improves ASTordn performance on both sections.6.5 Proposition Re-Ranking ResultsIn these experiments, Section 23 was used for testing our proposition re-ranking.
Weemployed a BC trained on Sections 2 to 8, whereas RMwas trained on Sections 2?12.15 Inorder to provide a probabilistic interpretation of the SVM output (see Section 4.4.1), weevaluated each classifier distribution parameter based on its output on Section 12.
Forcomputational complexity reasons, we decided to consider the five most likely labelingsfor each node and the five first alternatives output by the Viterbi algorithm (i. e., m = 5and n = 5).With this set-up, we evaluated the accuracy lower and upper bounds of our system.As our baseline, we consider the accuracy of a re-ranker that always chooses the firstalternative output from the Viterbi algorithm, that is, the most likely according to thejoint inference model.
This accuracy has been measured as 75.91 F1 percentage points;this is practically identical to the 75.89 obtained by applying heuristics to removeoverlaps generated by BC.14 As Section 21 of the Penn TreeBank is part of the Charniak parser training set, the performance derivedon its parse trees represents an upper bound for our classifiers, i. e., the results using a nearly idealsyntactic parser and role multiclassifier.15 In these experiments we did not use tree kernels for BC and RM as we wanted to measure the impact oftree kernels only on the re-ranking stage.219Computational Linguistics Volume 34, Number 2This does not depend on the bad quality of the five top labelings.
Indeed, weselected the best alternative produced by the Viterbi algorithm according to the gold-standard score, and we obtained an F1 of 84.76 for n = 5.
Thus, the critical aspect residesin the selection of the best annotations, which should be carried out by an automaticre-ranker.Rows 2 and 3 of Table 6 show the number of distinct propositions and alternativeannotations output by the Viterbi algorithm for each of the employed sections.
In row3, the number of pair comparisons (i. e., the number of training/test examples for theclassifier) is shown.Using this data, we carried out a complete SRL experiment, which is summarized inTable 7.
First, we compared the accuracy of the ASTcmn , PAS, and PAStlclassifiers trainedon Section 24 (in row 3, columns 2, 3, and 4) and discovered that the latter structureproduces a noticeable F1 improvement, namely, 78.15 vs. 76.47 and 76.77, whereas theaccuracy gap between the PAS and the ASTcmn classifiers is very small, namely, 76.77vs.
76.47 percentage points.
We selected the most interesting structured feature, thatis, the PAStl, and extended it with the local (to each argument node) standard featurescommonly employed for the boundary detection and argument classification tasks, asin Haghighi, Toutanova, and Manning (2005).
This richer kernel (PAStl+STD, column 5)was compared with the PAStl one.
The comparison was performed on two differenttraining sets (rows 2 and 3): In both cases, the introduction of the standard featuresproduced a performance decrement, most notably in the case of Section 12 (i. e., 82.07vs.
75.06).
Our best re-ranking kernel (i. e., the PAStl) was then employed in a largerexperiment, using both Sections 12 and 24 for testing (row 4), achieving an F1 measureof 78.44.First, we note that the accuracy of the ASTcmn and PAS classifiers is very similar (i. e.,76.77 vs. 76.47).
This datum suggests that the intra-argument syntactic information isnot critical for the re-ranking task, as including it or not in the learning algorithm doesnot lead to noticeable differences.Second, we note that the PAStl kernel is much more effective than those based onASTcmn and PAS, which are always outperformed.
This may be due to the fact thatTable 6Number of propositions, alternative annotations (as output by the Viterbi algorithm), and paircomparisons (i. e., re-ranker input examples) for the PropBank sections used for the experiments.Section 12 Section 23 Section 24Propositions 4,899 5,267 3,248Alternatives 24,494 26,325 16,240Comparisons 74,650 81,162 48,582Table 7Summary of the proposition re-ranking experiments with different training sets.Training Section ASTcmn PAS PAStl PAStl+STD12 ?
?
78.27 77.6124 76.47 76.77 78.15 77.7712+24 ?
?
78.44 ?220Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingtwo ASTcmn s (or PASs) always share a large number of substructures, because mostalternative annotations tend to be very similar and the small differences among themonly affect a small part of the encoding of syntactic information; on the other hand,the small amount of local parsing information encoded in the PAStls enables a goodgeneralization process.Finally, the introduction of the standard, local standard features in our re-rankingmodel caused a performance loss of about 0.5 percentage points on both Sections 12 and24.
This fact, which is in contrast with what has been shown in Haghighi, Toutanova,and Manning (2005), might be the consequence of the small training sets that weemployed.
Indeed, local standard features tend to be very sparse and their effectivenessshould be evaluated against a larger data set.7.
Discussions and ConclusionsThe design of automatic systems for the labeling of semantic roles requires the solutionof complex problems.
Among other issues, feature engineering is made difficult by thestructured nature of the data, that is, features should represent information expressedby automatically generated parse trees.
This raises two main problems: (1) the mod-eling of effective features, partially solved for some subtasks in previous works, and(2) the implementation of the software for the extraction of a large number of suchfeatures.A system completely (or largely) based on tree kernels alleviates both problemsas (1) kernel functions automatically generate features and (2) only a procedure for theextraction of subtrees is needed.
Although some of themanually designed features seemto be superior to those derived with tree kernels, their combination still seems worthapplying.
Moreover, tree kernels provide a back-off model that greatly outperformsstate-of-the-art SRL models when the amount of training data is small.To demonstrate these points, we carried out a comprehensive study of the use oftree kernels for semantic role labeling by designing several canonical mappings.
Thesecorrespond to the application of innovative tree kernel engineering techniques tailoredto different stages of an SRL process.
The experiments with these methods and SVMs onthe data set provided by the CoNLL 2005 shared task (Carreras andMa`rquez 2005) showthat, first, tree kernels are a valid support tomanually designed features for many stagesof the SRL process.
We have shown that our improved tree kernel (i.e., the one basedon ASTm1 ) highly improves accuracy in both boundary detection and the SRL task whenthe amount of training data is small (e.g., 5 absolute percentage points over a state-of-the-art boundary classifier).
In the case of argument classification the improvement isless evident but still consistent, at about 3%.Second, appropriately engineered tree kernels can replace standard features inmany SRL subtasks.
For example, in complex tasks such as conflict resolution or re-ranking, they provide an easy way to build new features that would be difficult todescribe explicitly.
More generally, tree kernels can be used to combine different sourcesof information for the design of complex learning models.Third, in the specific re-ranking task, our structured features show a noticeable im-provement over our baseline (i. e., about 2.5 percentage points).
This could be increasedconsidering that we have not been able to fully exploit the potential of our re-rankingmodel, whose theoretical upper bound is 6 percentage points away.
Still, although weonly used a small fraction of the available training data (i. e., only 2 sections out of 22were used to train the re-ranker) our system?s accuracy is in line with state-of-the-artsystems (Carreras and Ma`rquez 2005) that do not employ tree kernels.221Computational Linguistics Volume 34, Number 2Finally, although the study carried out in this article is quite comprehensive, severalissues should be considered in more depth in the future:(a) The tree feature extraction functions ST, SST, and PT should be studied in com-bination with the proposed canonical mappings.
For example, as the PT kernel seemsmore suitable for the processing of dependency information, it would be interestingto apply it in an architecture using these kinds of syntactic parse trees (e. g., Chenand Rambow 2003).
In particular, the combination of different extraction functions ondifferent syntactic views may lead to very good results.
(b) Once the set of the most promising kernels is established, it would be interestingto use all the available CoNLL 2005 data.
This would allow us to estimate the potentialof our approach by comparing it with previous work on a fairer basis.
(c) The use of fast tree kernels (Moschitti 2006a) along with the proposed tree repre-sentations makes the learning and classification much faster, so that the overall runningtime is comparable with polynomial kernels.
However, when used with SVMs theirrunning time on very large data sets (e. g., millions of instances) becomes prohibitive.Exploiting tree kernel?derived features in a more efficient way (e. g., by selecting themost relevant fragments and using them in an explicit space) is thus an interestingline of future research.
Note that such fragments would be the product of a reverseengineering process useful to derive linguistic insights on semantic role theory.
(d) As CoNLL 2005 (Punyakanok et al 2005) has shown that multiple parse treesprovide the most important boost to the accuracy of SRL systems, we would like toextend our model to work with multiple syntactic views of each input sentence.AcknowledgmentsThis article is the result of research on kernelmethods for Semantic Role Labeling whichstarted in 2003 and went under the review ofseveral program committees of differentscientific communities, from which it highlybenefitted.
In this respect, we would like tothank the reviewers of the SRL special issueas well as those of the ACL, CoNLL, EACL,ECAI, ECML, HLT-NAACL, and ICMLconferences.
We are indebted to SilviaQuarteroni for her help in reviewing theEnglish formulation of an earlier version ofthis article.ReferencesBaker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The BerkeleyFrameNet project.
In COLING-ACL ?98:Proceedings of the Conference, pages 86?90,Montre?al, Canada.Carreras, Xavier and Llu?
?s Ma`rquez.2004.
Introduction to the CoNLL-2004shared task: Semantic role labeling.In HLT-NAACL 2004 Workshop: EighthConference on Computational NaturalLanguage Learning (CoNLL-2004),pages 89?97, Boston, MA.Carreras, Xavier and Llu?
?s Ma`rquez.2005.
Introduction to the CoNLL-2005shared task: Semantic role labeling.In Proceedings of the Ninth Conferenceon Computational Natural LanguageLearning (CoNLL-2005), pages 152?164,Ann Arbor, MI.Chen, John and Owen Rambow.
2003.Use of deep linguistic features forthe recognition and labeling ofsemantic arguments.
In Proceedingsof the 2003 Conference on EmpiricalMethods in Natural Language Processing,pages 41?48, Sapporo, Japan.Collins, Michael and Nigel Duffy.
2002.New ranking algorithms for parsing andtagging: Kernels over discrete structures,and the voted perceptron.
In ACL02,pages 263?270, Philadelphia, PA.Culotta, Aron and Jeffrey Sorensen.
2004.Dependency tree kernels for relationextraction.
In ACL04, pages 423?429,Barcelona, Spain.Cumby, Chad and Dan Roth.
2003.
Kernelmethods for relational learning.
InProceedings of ICML 2003, pages 107?114,Washington, DC.Fillmore, Charles J.
1968.
The case for case.
InEmmon Bach and Robert T. Harms,222Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labelingeditors, Universals in Linguistic Theory.Holt, Rinehart, and Winston, New York,pages 1?210.Gildea, Daniel and Daniel Jurafsky.
2002.Automatic labeling of semantic roles.Computational Linguistics, 28(3): 245?288.Haghighi, Aria, Kristina Toutanova, andChristopher Manning.
2005.
A joint modelfor semantic role labeling.
In Proceedings ofthe Ninth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 173?176, Ann Arbor, MI.Jackendoff, Ray.
1990.
Semantic Structures,Current Studies in Linguistics Series.
TheMIT Press, Cambridge, MA.Joachims, Thorsten.
1999.
Making large-scaleSVM learning practical.
In B. Scho?lkopf,C.
Burges, and A. Smola, editors,Advances in Kernel Methods?Support VectorLearning.
MIT Press, Cambridge, MA,pages 169?184.Kazama, Jun?ichi and Kentaro Torisawa.2005.
Speeding up training with treekernels for node relation labeling.In Proceedings of EMNLP 2005,pages 137?144, Toronto, Canada.Kudo, Taku and Yuji Matsumoto.
2003.
Fastmethods for kernel-based text analysis.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics,pages 24?31, Sapporo, Japan.Levin, Beth.
1993.
English Verb Classesand Alternations.
The University ofChicago Press, Chicago, IL.Lin, H.-T., C.-J.
Lin, and R. C. Weng.
2003.A note on Platt?s probabilistic outputsfor support vector machines.
Technicalreport, National Taiwan University.Litkowski, Kenneth.
2004.
Senseval-3 task:Automatic labeling of semantic roles.In Senseval-3: Third International Workshopon the Evaluation of Systems for theSemantic Analysis of Text, pages 9?12,Barcelona, Spain.Marcus, M. P., B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a largeannotated corpus of English: The Penntreebank.
Computational Linguistics,19:313?330.Moschitti, Alessandro.
2004.
A studyon convolution kernels for shallowsemantic parsing.
In Proceedings ofthe 42th Conference on Association forComputational Linguistic (ACL-2004),pages 335?342, Barcelona, Spain.Moschitti, Alessandro.
2006a.
Efficientconvolution kernels for dependencyand constituent syntactic trees.In Proceedings of The 17th EuropeanConference on Machine Learning,pages 318?329, Berlin, Germany.Moschitti, Alessandro.
2006b.
Making treekernels practical for natural languagelearning.
In Proceedings of 11th Conferenceof the European Chapter of the Association forComputational Linguistics (EACL2006),pages 113?120, Treato, Italy.Moschitti, Alessandro, BonaventuraCoppola, Daniele Pighin, and RobertoBasili.
2005a.
Engineering of syntacticfeatures for shallow semantic parsing.In Proceedings of the ACL Workshop onFeature Engineering for Machine Learning inNatural Language Processing, pages 48?56,Ann Arbor, MI.Moschitti, Alessandro, Ana-Maria Giuglea,Bonaventura Coppola, and RobertoBasili.
2005b.
Hierarchical semanticrole labeling.
In Proceedings of theNinth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 201?204, Ann Arbor, MI.Moschitti, Alessandro, Daniele Pighin,and Roberto Basili.
2006.
Tree kernelengineering in semantic role labelingsystems.
In Proceedings of the Workshop onLearning Structured Information in NaturalLanguage Applications, EACL 2006,pages 49?56, Trento, Italy.Palmer, Martha, Daniel Gildea, and PaulKingsbury.
2005.
The Proposition Bank:An annotated corpus of semantic roles.Computational Linguistics, 31(1): 71?106.Platt, J.
1999.
Probabilistic outputsfor support vector machines andcomparison to regularized likelihoodmethods.
In A. J. Smola, P. Bartlett,B.
Schoelkopf, and D. Schuurmans,editors, Advances in Large MarginClassifiers.
MIT Press, Cambridge, MA,pages 61?74.Pradhan, Sameer, Kadri Hacioglu, ValerieKrugler, Wayne Ward, James H. Martin,and Daniel Jurafsky.
2005a.
Supportvector learning for semantic argumentclassification.Machine Learning,60(1?3):11?39.Pradhan, Sameer, Kadri Hacioglu, WayneWard, James H. Martin, and DanielJurafsky.
2005b.
Semantic role chunkingcombining complementary syntacticviews.
In Proceedings of the Ninth Conferenceon Computational Natural LanguageLearning (CoNLL-2005), pages 217?220,Ann Arbor, MI.Pradhan, Sameer, Wayne Ward, KadriHacioglu, James Martin, and DanielJurafsky.
2005c.
Semantic role labeling223Computational Linguistics Volume 34, Number 2using different syntactic views.
InProceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL?05), pages 581?588,Ann Arbor, MI.Pradhan, Sameer S., Wayne H. Ward,Kadri Hacioglu, James H. Martin, andDan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.In HLT-NAACL 2004: Main Proceedings,pages 233?240, Boston, MA.Punyakanok, Vasin, Peter Koomen,Dan Roth, and Wen-tau Yih.
2005.Generalized inference with multiplesemantic role labeling systems.
InProceedings of the Ninth Conferenceon Computational Natural LanguageLearning (CoNLL-2005), pages 181?184,Ann Arbor, MI.Shawe-Taylor, John and Nello Cristianini.2004.
Kernel Methods for PatternAnalysis.
Cambridge University Press,Cambridge, UK.Shen, Libin, Anoop Sarkar, and Aravind K.Joshi.
2003.
Using LTAG based features inparse reranking.
In Empirical Methods forNatural Language Processing (EMNLP),pages 89?96, Sapporo, Japan.Thompson, Cynthia A., Roger Levy, andChristopher Manning.
2003.
A generativemodel for semantic role labeling.
In 14thEuropean Conference on Machine Learning,pages 397?408, Cavtat, Croatia.Tjong Kim Sang, Erik, Sander Canisius,Antal van den Bosch, and Toine Bogers.2005.
Applying spelling error correctiontechniques for improving semanticrole labelling.
In Proceedings of theNinth Conference on ComputationalNatural Language Learning (CoNLL-2005),pages 229?232, Ann Arbor, MI.Toutanova, Kristina, Aria Haghighi, andChristopher Manning.
2005.
Joint learningimproves semantic role labeling.
InProceedings of the 43rd Annual Meetingof the Association for ComputationalLinguistics (ACL?05), pages 589?596,Ann Arbor, MI.Toutanova, Kristina, Penka Markova, andChristopher Manning.
2004.
The leaf pathprojection view of parse trees: Exploringstring kernels for HPSG parse selection.
InProceedings of EMNLP 2004, pages 166?173,Barcelona, Spain.Vapnik, Vladimir N. 1998.
Statistical LearningTheory.
John Wiley and Sons, New York.Vishwanathan, S. V. N. and A. J. Smola.2002.
Fast kernels on strings and trees.In Proceedings of Neural InformationProcessing Systems, pages 569?576,Vancouver, British Columbia.Xue, Nianwen and Martha Palmer.
2004.Calibrating features for semantic rolelabeling.
In Proceedings of EMNLP 2004,pages 88?94, Barcelona, Spain.Zelenko, D., C. Aone, and A. Richardella.2003.
Kernel methods for relationextraction.
Journal of Machine LearningResearch, 3:1083?1106.Zhang, Min, Jie Zhang, and Jian Su.
2006.Exploring syntactic features for relationextraction using a convolution treekernel.
In Proceedings of the HumanLanguage Technology Conference of theNAACL, Main Conference, pages 288?295,New York, NY.224
