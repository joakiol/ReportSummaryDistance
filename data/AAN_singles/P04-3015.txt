Hierarchy Extraction based on Inclusion of AppearanceEiko Yamamoto Kyoko Kanzaki Hitoshi IsaharaComputational Linguistics Group,National Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan.eiko@nict.go.jp kanzaki@nict.go.jp isahara@nict.go.jpAbstractIn this paper, we propose a method of auto-matically extracting word hierarchies based onthe inclusion relation of appearance patternsfrom corpora.
We apply a complementarysimilarity measure to find a hierarchical wordstructure.
This similarity measure was devel-oped for the recognition of degraded machine-printed text in the field and can be applied toestimate one-to-many relations.
Our purpose isto extract word hierarchies from corporaautomatically.
As the initial task, we attemptto extract hierarchies of abstract nouns co-occurring with adjectives in Japanese andcompare with hierarchies in the EDR elec-tronic dictionary.1 IntroductionThe hierarchical relations of words are useful aslanguage resources.
Hierarchical semantic lexicaldatabases such as WordNet (Miller et al, 1990)and the EDR electronic dictionary (1995) are usedfor NLP research worldwide to fully understand aword meaning.
In current thesauri in the form ofhierarchical relations, words are categorized manu-ally and classified in a top-down manner based onhuman intuition.
This is a good way to make alexical database for users having a specific purpose.However, word hierarchies based on human intui-tion tend to vary greatly depending on the lexicog-rapher.
In addition, hierarchical relations based onvarious data may be needed depending on eachuser.Accordingly, we try to extract a hierarchical re-lation of words automatically and statistically.
Inprevious research, ways of extracting from defini-tion sentences in dictionaries (Tsurumaru et al,1986; Shoutsu et al, 2003) or from a corpus byusing patterns such as ?a part of?, ?is-a?, or ?and?
(Berland and Charniak, 1999; Caraballo, 1999)have been proposed.
Also, there is a method thatuses the dependence relation between words takenfrom a corpus (Matsumoto et al, 1996).
In contrast,we propose a method based on the inclusion rela-tion of appearance patterns from corpora.In this paper, to verify the suitability of ourmethod, we attempt to extract hierarchies of ab-stract nouns co-occurring with adjectives in Japa-nese.
We select two similarity measures to estimatethe inclusion relation between word appearancepatterns.
One is a complementary similarity meas-ure; i.e., a similarity measure developed for therecognition of degraded machine-printed text in thefield (Hagita and Sawaki, 1995).
This measure canbe used to estimate one-to-many relations such assuperordinate?subordinate relations from appear-ance patterns (Yamamoto and Umemura, 2002).The second similarity measure is the overlap coef-ficient, which is a similarity measure to calculatethe rate of overlap between two binary vectors.Using each measure, we extract hierarchies from acorpus.
After that, we compare these with the EDRelectronic dictionary.2 Experiment CorpusA good deal of linguistic research has focused onthe syntactic and semantic functions of abstractnouns (Nemoto, 1969; Takahashi, 1975; Schmid,2000; Kanzaki et al, 2003).
In the example, ?Yagi(goat) wa seishitsu (nature) ga otonashii (gentle)(The nature of goats is gentle).
?, Takahashi (1975)recognized that the abstract noun ?seishitsu (na-ture)?
is a hypernym of the attribute that the predi-cative adjective ?otonashi (gentle)?
expresses.Kanzaki et al (2003) defined such abstract nounsthat co-occur with adjectives as adjective hy-pernyms, and extracted these co-occurrence rela-tions between abstract nouns and adjectives frommany corpora such as newspaper articles.
In thelinguistic data, there are sets of co-occurringadjectives for each abstract noun ?
the total num-ber of abstract noun types is 365 and the number ofadjective types is 10,525.
Some examples are asfollows.OMOI  (feeling): ureshii (glad), kanashii (sad),shiawasena (happy), ?KANTEN (viewpoint): igakutekina (medical),rekishitekina (historical), ...3 Complementary Similarity MeasureThe complementary similarity measure (CSM) isused in a character recognition method for binaryimages which is robust against heavy noise orgraphical designs (Sawaki and Hagita, 1996).
Ya-mamoto et al (2002) applied CSM to estimate one-to-many relations between words.
They estimatedone-to-many relations from the inclusion relationsbetween the appearance patterns of two words.The appearance pattern is expressed as an n-dimensional binary feature vector.
Now, let F = (f1,f2, ?, fn) and T = (t1, t2, ?, tn) (where fi, ti = 0 or1) be the feature vectors of the appearance patternsfor a word and another word, respectively.
TheCSM of F to T is defined asdcbantfdtfctfbtfadbcabcadTFCSMni iini iini iini ii+++=???=??=??=?=++?=???
?====,)1()1(,)1(,)1(,))((),(1111The CSM of F to T represents the degree towhich F includes T; that is, the inclusion relationbetween the appearance patterns of two words.In our experiment, each ?word?
is an abstractnoun.
Therefore, n is the number of adjectives inthe corpus, a indicates the number of adjectives co-occurring with both abstract nouns, b and c indi-cate the number of adjectives co-occurring witheither abstract noun, and d indicates the number ofadjectives co-occurring with neither abstract noun.4 Overlap CoefficientThe overlap coefficient (OVLP) is a similaritymeasure for binary vectors (Manning and Schutze,1999).
OVLP is essentially a measure of inclusion.It has a value of 1.0 if every dimension with a non-zero value for the first vector is also non-zero forthe second vector or vice versa.
In other words, thevalue is 1.0 when the first vector completely in-cludes the second vector or vice versa.
OVLP of Fand T is defined as),(),(),(cabaMINaTFMINTFTFOVLP ++==I5 EDR hierarchyThe EDR Electronic Dictionary (1995) was de-veloped for advanced processing of natural lan-guage by computers and is composed of elevensub-dictionaries.
The sub-dictionaries include aconcept dictionary, word dictionaries, bilingualdictionaries, etc.
We verify and analyse the hierar-chies that are extracted based on a comparison withthe EDR dictionary.
However, the hierarchies inEDR consist of hypernymic concepts representedby sentences.
On the other hand, our extracted hi-erarchies consist of hypernyms such as abstractnouns.
Therefore, we have to replace the conceptcomposed of a sentence with the sequence of thewords.
We replace the description of concepts withentry words from the ?Word List by SemanticPrinciples?
(1964) and add synonyms.
We also addto abstract nouns in order to reduce any differencein representation.
In this way, conceptual hierar-chies of adjectives in the EDR dictionary are de-fined by the sequence of words.6 Hierarchy Extraction ProcessThe processes for hierarchy extraction from thecorpus are as follows.
?TH?
is a threshold value foreach pair under consideration.
If TH is low, we canobtain long hierarchies.
However, if TH is too low,the number of word pairs taken into considerationincreases overwhelmingly and the measurementreliability diminishes.
In this experiment, we set0.2 as TH.1.
Compute the similarity between appear-ance patterns for each pair of words.
Thehierarchical relation between the twowords in a pair is determined by the simi-larity value.
We express the pair as (X, Y),where X is a hypernym of Y and Y is ahyponym of X.2.
Sort the pairs by the normalized similari-ties and reduce the pairs where the simi-larity is less than TH.3.
For each abstract noun,A) Choose a pair (B, C) where word B isthe hypernym with the highest value.The hierarchy between B and C is setto the initial hierarchy.B) Choose a pair (C, D) where hyponymD is not contained in the current hier-archy and has the highest value in pairswhere the last word of the current hier-archy C is a hypernym.C) Connect hyponym D with the tail ofthe current hierarchy.D) While such a pair can be chosen, repeatB) and C).E) Choose a pair (A, B) where hypernymA is not contained in the current hier-archy and has the highest value in pairswhere the first word of the current hi-erarchy B is a hypernym.F) Connect hypernym A with the head ofthe current hierarchy.G) While such a pair can be chosen, repeatE) and F).4.
For the hierarchies that are built,A) If a short hierarchy is included in alonger hierarchy with the order of thewords preserved, the short one isdropped from the list of hierarchies.B) If a hierarchy has only one or a fewdifferent words from another hierarchy,the two hierarchies are merged.7 Extracted HierarchySome extracted hierarchies are as follows.
In ourexperiment, we get koto (matter) as the commonhypernym.koto (matter) -- joutai (state) -- kankei (relation)-- kakawari (something to do with) -- tsukiai(have an acquaintance with)koto (matter) -- toki (when) -- yousu (aspect) --omomochi (one?s face) -- manazashi (a look) --iro (on one?s face) -- shisen (one?s eye)8 ComparisonWe analyse extracted hierarchies by using thenumber of nodes that agree with the EDR hierar-chy.
Specifically, we count the number of nodes(nouns) which agree with a word in the EDR hier-archy, preserving the order of each hierarchy.
Here,two hierarchies are ?A - B - C - D - E?
and ?A - B- D - F - G.?
They have three agreement nodes; ?A- B - D.?Table 1 shows the distribution of the depths of aCSM hierarchy, and the number of nodes thatagree with the EDR hierarchy at each depth.
Table2 shows the same for an OVLP one.
?AgreementLevel?
is the number of agreement nodes.
The boldfont represents the number of hierarchies com-pletely included in the EDR hierarchy.8.1 Depth of HierarchyThe number of hierarchies made from the EDRdictionary (EDR hierarchy) is 932 and the deepestlevel is 14.
The number of CSM hierarchies is 105and the depth is from 3 to 14 (Table 1).
The num-ber of OVLP hierarchies is 179 and the depth isfrom 2 to 9 (Table 2).
These results show thatCSM builds a deeper hierarchy than OVLP, thoughthe number of hierarchies is less than OVLP.
Also,the deepest level of CSM equals that of EDR.Therefore, comparison with the EDR dictionary isan appropriate way to verify the hierarchies that wehave extracted.In both tables, we find most hierarchies have anagreement level from 2 to 4.
The deepest agree-ment level is 6.
For an agreement level of 5 or bet-ter, the OVLP hierarchy includes only two hierar-chies while the CSM hierarchy includes nine hier-archies.
This means CSM can extract hierarchieshaving more nodes which agree with the EDR hi-erarchy than is possible with OVLP.Depth ofHierarchyAgreement Level1        2        3       4       5       63 1 4 14 8 6 25 9 8  16 8 9 4 17 2 6 1 18 1 5 2 29 3 2 3 110  1  211  4 112  1  113  1  214    1Table 1: Distribution of CSM hierarchy for eachdepthDepth ofHierarchyAgreement Level1        2         3        4       5       62 13 2 8 14 25 9 15 24 13 76 21 31 57 5 12 1 18 3 5 2 19 1 3 1Table 2: Distribution of OVLP hierarchy foreach depthAlso, many abstract nouns agree with the hy-peronymic concept around the top level.
In currentthesauri, the categorization of words is classified ina top-down manner based on human intuition.Therefore, we believe the hierarchy that we havebuilt is consistent with human intuition, at leastaround the top level of hyperonymic concepts.9 ConclusionWe have proposed a method of automatically ex-tracting hierarchies based on an inclusion relationof appearance patterns from corpora.
In this paper,we attempted to extract objective hierarchies ofabstract nouns co-occurring with adjectives inJapanese.
In our experiment, we showed that com-plementary similarity measure can extract a kind ofhierarchy from corpora, though it is a similaritymeasure developed for the recognition of degradedmachine-printed text.
Also, we can find interestinghierarchies which suit human intuition, thoughthey are different from exact hierarchies.
Kanzakiet al (2004) have applied our approach to verifyclassification of abstract nouns by using self-organization map.
We can look a suitability of ourresult at that work.In our future work, we will use our approach forother parts of speech and other types of word.Moreover, we will compare with current alterna-tive approaches such as those based on sentencepatterns.ReferencesBerland, M. and Charniak, E. 1999.
Finding Partsin Very Large Corpora, In Proceedings of the37th Annual Meeting of the Association for Com-putational Linguistics, pp.57-64.Caraballo, S. A.
1999.
Automatic Construction of aHypernym-labeled Noun Hierarchy from Text,In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics,pp.120-126.EDR Electronic Dictionary.
1995.http://www2.nict.go.jp/kk/e416/EDR/index.htmlHagita, N. and Sawaki, M. 1995.
Robust Recogni-tion of Degraded Machine-Printed Characters us-ing Complementary Similarity Measure and Er-ror-Correction Learning?In Proceedings of theSPIE ?The International Society for Optical En-gineering, 2442: pp.236-244.Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M.,and Isahara, H. 2003.
Adjectives and their Ab-stract concepts --- Toward an objective thesaurusfrom Semantic Map.
In Proceedings of the Sec-ond International Workshop on Generative Ap-proaches to the Lexicon, pp.177-184.Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M.,and Isahara, H. 2004.
Extraction of Hyperonymyof Adjectives from Large Corpora by using theNeural Network Model.
In Proceedings of theFourth International Conference on LanguageResources and Evaluation, Volume II, pp.423-426.Kay, M. 1986.
Parsing in Functional UnificationGrammar.
In ?Readings in Natural LanguageProcessing?, Grosz, B. J., Spark Jones, K. andWebber, B. L., ed., pp.125-138, Morgan Kauf-mann Publishers, Los Altos, California.Manning, C. D. and Schutze, H. 1999.
Foundationsof Statistical Natural Language Processing, TheMIT Press, Cambridge MA.Matsumoto, Y. and Sudo, S., Nakayama, T., andHirao, T. 1996.
Thesaurus Construction fromMultiple Language Resources, In IPSJ SIGNotes NL-93, pp.23-28 (In Japanese).Miller, A., Beckwith, R., Fellbaum, C., Gros, D.,Millier, K., and Tengi, R. 1990.
Five Papers onWordNet, Technical Report CSL Report 43,Cognitive Science Laboratory, Princeton Univer-sity.Mosteller, F. and Wallace, D. 1964.
Inference andDisputed Authorship: The Federalist.
Addison-Wesley, Reading, Massachusetts.Nemoto, K. 1969.
The combination of the nounwith ?ga-Case?
and the adjective, Language re-search2 for the computer, National LanguageResearch Institute, pp.63-73 (In Japanese).Shmid, H-J.
2000.
English Abstract Nouns as Con-ceptual Shells, Mouton de Gruyter.Shoutsu, Y., Tokunaga, T., and Tanaka, H. 2003.The integration of Japanese dictionary and the-saurus, In IPSJ SIG Notes NL-153, pp.141-146(In Japanese).Sparck Jones, K. 1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28(1): pp.11-21.Takahashi, T. 1975.
A various phase related to thepart-whole relation investigated in the sentence,Studies in the Japanese language 103, TheSociety of Japanese Linguistics, pp.1-16 (InJapanese).Tsurumaru, H., Hitaka, T., and Yoshita, S. 1986.Automatic extraction of hierarchical relation be-tween words, In IPSJ SIG Notes NL-83, pp.121-128 (In Japanese).Yamamoto, E. and Umemura, K. 2002.
A Similar-ity Measure for Estimation of One?to-Many Re-lationship in Corpus, In Journal of Natural Lan-guage Processing, pp.45-75 (In Japanese).Word List by Semantic Principles.
1964.
NationalLanguage Research Institute Publications, ShueiShuppan (In Japanese).
