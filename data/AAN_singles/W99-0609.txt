IDetermining the specificity of nouns from textSharon A. Caraballo and Eugene CharniakDept.
of Computer ScienceBrown UniversityProvidence, RI 02912{ SC, ec}@cs, brown, eduAbstractIn this work, we use a large text corpus toorder nouns by their level of specificity.
Thissemantic information can for most nouns bedetermined with over 80% accuracy usingsimple statistics from a text corpus with-out using any additional sources of seman-tic knowledge.
This kind of semantic in-formation can be used to help in automat-ically constructing or augmenting a lexicaldatabase such as WordNet.1 IntroductionLarge lexical databases such as Word-Net (see Fellbaum (1998)) are in com-mon research use.
However, there are cir-cumstances, particularly involving domain-specific text, where WordNet does not havesufficient coverage.
Various automatic meth-ods have been proposed to automaticallybuild lexical resources or augment existingresources.
(See, e.g., Riloff and Shepherd(1997), Roark and Charniak (1998), Cara-hallo (1999), and Berland and Charniak(1999).)
In this paper, we describe a methodwhich can be used to assist in this problem.We present here a way to determine therelative specificity of nouns; that is, whichnouns are more specific (or more general)than others, using only a large text cor-pus and no additional sources of semanticknowledge.
By gathering simple statistics,we are able to decide which of two nouns ismore specific to over 80% accuracy for nounsat "basic level" or below (see, e.g., Lakoff(1987)), and about 59% accuracy for nounsabove basic level.It should be noted that specificity by it-self is not enough information from whichto construct a noun hierarchy.
This projectis meant o provide a tool to support othermethods.
See Caraballo (1999) for a detaileddescription of a method to construct such ahierarchy.2 Previous workTo the best of our knowledge, this is the firstattempt o automatically rank nouns basedon specificity.Hearst (1992) found individual pairs ofhypernyms and hyponyms from text usingpattern-matching techniques.
The sparse-ness of these patterns prevents this from be-ing an effective approach to the problem weaddress here.In Caraballo (1999), we construct a hierar-chy of nouns, including hypernym relations.However, there are several areas where thatwork could benefit from the research pre-sented here.
The hypernyms used to labelthe internal nodes of that hierarchy are cho-sen in a simple fashion; pattern-matching asin Hearst (1992) is used to identify candi-date hypernyms ofthe words dominated by aparticular node, and a simple voting schemeselects the hypernyms to be used.
The hy-pernyms tend to lose quality as one lookscloser to the root of the tree, generally bybeing too specific.
This work could help tochoose more general hypernyms from amongthe candidate words.
In addition, it couldbe used to correct places where a more spe-cific word is listed as a hypernym of a moregeneral word, and to select between hyper-nyms which seem equally good by the votingscheme (which is currently done arbitrarily).633 Methods for determiningspecificityWe tested several methods for orderingnouns by their specificity.
Each of thesemethods was trained on the text of the 1987Wall Street Journal corpus, about 15 mil-lion words.
When parsed text was needed,it was obtained using a parser recently de-veloped at Brown which performs at aboutthe 88% level on the standard precision andrecall measures.One possible indicator of specificity is howoften the noun is modified.
It seems rea-sonable to suppose that very specific nounsare rarely modified, while very general nounswould usually be modified.
Using the parsedtext, we collected statistics on the probabil-ity that a noun is modified by a prenomi-nal adjective, verb, or other noun.
(In all ofthese measures, when we say "noun" we arereferring only to common ouns, tagged NNor NNS, not proper nouns tagged NNP orNNPS.
Our results were consistently betterwhen proper nouns were eliminated, prob-ably since the proper nouns may conflictwith identically-spelled common ouns.)
Welooked at both the probability that the nounis modified by any of these modifiers andthe probability that the noun is modifiedby each specific category.
The nouns, ad-jectives, and verbs are all stemmed beforecomputing these statistics.Po  (noun) =count(noun with a prenominal djective)count(noun)Pub(noun) =count(noun with a prenominal verb)count(noun)Pn (noun) =count(noun with a prenominal noun)count(noun)Brood(noun) =count(noun with prenom, adj, vb, or nn)count(noun)However, if a noun almost always appearswith exactly the same modifiers, this maybe an indication of an expression (e.g., "uglyduckling"), rather than a very general noun.For this reason, we also collected entropy-based statistics.
For each noun, we com-puted the entropy of the rightmost prenom-inal modifier.Hmod(noun) =-  ~ \[P(modifierlnoun)modifier* log 2 P(modifier Inoun)\]where P(modifierlnoun ) is the probabilitythat a (possibly null) modifier is the right-most modifier of noun.
The higher the en-tropy, the more general we believe the nounto be.
In other words, we are considering notjust how often the noun is modified, but howmuch these modifiers vary.
A great varietyof modifiers uggests that the noun is quitegeneral, while a noun that is rarely modifiedor modified in only a few different ways isprobably fairly specific.We also looked at a simpler measure whichcan be computed from raw text rather thanparsed text.
(For this experiment we usedthe part-of-speech tags determined by theparser, but that was only to produce theset of nouns for testing.
If one wanted tocompute this measure for all words, or for aspecific list of words, tagging would be un-necessary.)
We simply looked at all wordsappearing within an n-word window of anyinstance of the word being evaluated, andthen computed the entropy measure:Hn(noun) = - E \[P(w?rdln?un)Word* log 2 P(word\[noun)\]where P(word\]noun) is the probability thata word appearing within an n-word win-dow of noun is word.
Again, a higher en-tropy indicates a more general noun.
Inthis measure, the nouns being evaluated arestemmed, but the words in its n-word win-dow are not.Finally, we computed the very simple mea-sure of frequency (freq(noun)).
The higher64the frequency, the more general we expectthe noun to be.
(Recall that we are usingtagged text, so it is not merely the frequencyof the word that is being measured, but thefrequency of the word or its plural tagged asa common noun.
)This assumed inverse relationship betweenfrequency and the semantic content of aword is used, for example, to weight the im-portance of terms in the standard IDF mea-sure used in information retrieval (see, e.g.,Sparck Jones (1972)), and to weight the im-portance of context words to compare thesemantic similarity of nouns in Grefenstette(1993).4 EvaluationTo evaluate the performance of these mea-sures, we used the hypernym data in Word-Net (1998) as our gold standard.
(A wordX is considered a hypernym of a word Y ifnative speakers accept the sentence "Y is a(kind of) X.
")'We constructed three smallhierarchies of nouns and looked at how of-ten our measures found the proper relation-ships between the hypernym/hyponym pairsin these hierarchies.To select the words for our three hierar-chies, we wanted to use sets of words forwhich there would be enough information inthe Wall Street Journal corpus.
We chosethree clusters produced by a program similarto Roark and Charniak (1998) except hat itis based on a generative probability modeland tries to classify all nouns rather thanjust those in pre-selected clusters.
(All datasets are given in the Appendix.)
The clus-ters we selected represented vehicles (car,truck, boat, ...), food (bread, pizza, wine,...), and occupations (journalist, engineer,biochemist, ...).
From the clustered ata weremoved proper nouns and words that werenot really in our target categories.
We thenlooked up the remaining words in Word-Net, and added their single-word hypernymsto the categories in the correct hierarchicalstructure.
(Many WordNet categories aredescribed by multiple words, e.g., "motor-ized vehicle", and these were omitted for ob-vious reasons.
)For each of these three hierarchies,we looked at each hypernym/hyponympair within the hierarchy and determinedwhether each specificity measure placed thewords in the proper order.
The percentageeach specificity measure placed correctly arepresented in Table 1.Clearly the better measures are perform-ing much better than a random-guess algo-rithm which would give 50% performance.Among the measures based on the parsedtext (Pmod and its components and Hmod),the entropy-based measure Hmod is clearlythe best performer, as would be expected.However, it is interesting to note that thestatistics based on adjectives alone (Padj)somewhat outperform those based on all ofour prenominal modifiers (P~od).
The rea-sons for this are not entirely clear.Although Hmod is the best performeron the vehicles data, freq and Hs0 domarginally better overall, with each havingthe best results on one of the data sets.
Allthree of these measures, as well as H2 andH10, get above 80% correct on average.In these evaluations, it became clear that asingle bad node high in the hierarchy couldhave a large effect on the results.
For ex-ample, in the "occupations" hierarchy, theroot node is "person," however, this is not avery frequent word in the Wall Street Jour-nal corpus and rates as fairly specific acrossall of our measures.
Since this node haseight children, a single bad value at thisnode can cause eight errors.
We thereforeconsidered another evaluation measure: foreach internal node in the tree, we evalu-ated whether each specificity measure ratedthis word as more general than all of its de-scendants.
(This is somewhat akin to theidea of edit distance.
If we sufficiently in-creased the generality measure for each nodemarked incorrect in this system, the hierar-chy would match WordNet's exactly.)
Theresults for this evaluation are presented inTable 2.
Although this is a harsher measure,it isolates the effect of individual difficult in-ternal nodes.Although the numbers are lower in Table65Specificity measure Vehicles\]Pmod 65.2/:~djPvbPnnHmod65.273.965.291.3H2 87.0H10 87.0Hso 87.0Freq 87.0 \[Food63.367.342.957.179.679.679.685.783.7Occupations Average66.7 65.069.7 67.451.5 56.151.5 58.072.7 81.275.8 8O.875.8 8O.875.8 82.878.8 83.1Table 1: Percentage of parent-child relationships whichsure.Specificity measure Vehicles FoodPmod 44.4Padj 33.3Pvb 33.3Pn~ 55.6Hmod 77.8H2 66.7H lo  66.7Hso 66.7Freq 66.757.952.621.121.163.257.963.273.763.2Table 2: Percentage of internal nodesdants.are ordered correctly by each mea-Occupations Average53.3 51.960.0 48.740.0 31.533.3 36.666.7 69.260.0 61.560.0 63.360.060.0having the66.863.3correct relationship to all of their descen-2, the same measures as in Table 1 performrelatively well.
However, here Hmod has thebest performance both on average and ontwo of three data sets, while the freq mea-sure does a bit less well, now performing atabout the level of Hi0 rather than Hs0.
Thefact that some of the numbers in Table 2 arebelow 50% should not be alarming, as the av-erage number of descendants of an internalnode is over 5, implying that random chancewould give performance well below the 50%level on this measure.Some of these results are negatively af-fected by word-sense problems.
Some of thewords added from the WordNet data aremuch more common in the Wall Street Jour-nal data for a different word sense than theone we are trying to evaluate.
For example,the word "performer" is in the occupationshierarchy, but in the Wall Street Journal thisword generally refers to stocks or funds (as"good performers", for example) rather thanto people.
Since it was our goal not to useany outside sources of semantic knowledgethese words were included in the evaluations.However, if we eliminate those words, the re-sults are as shown in Tables 3 and 4.It is possible that using some kind ofautomatic word:sense disambiguation whilegathering the statistics would help reducethis problem.
This is also an area for fu-ture work.
However, it should be noted thaton the evaluation measures in Tables 3 and4, as in the first two tables, the best resultsare obtained with Hmod, Hso and freq.The above results are primarily for nounsat "basic level" and below, which includesthe vast majority of nouns.
We also consid-ered a data set at basic level and above, with"entity" at its root.
Table 5 presents the66Specificity measure VehiclesPmod 65.0Padj 70.0Pvb 80.0P~n 70.0Hmo d 100.0H2 95.0H10 95.0H~0 95.0Freq 95.0Food62.566.743.856.381.3 I79.279.285.483.31Table 3:dominant sense are removed.Percentage of correct parent-childSpecificity measure VehiclesBroodPadjVvbHmod 100.0H2Hlo"-H5 0I Occupations Average67.7 65.171.0 69.248.4 57.451.6 59.3Freq74.2 85.177.4 83.977.4 83.977.4 85.980.6 86.3relationships when words with the wrong pre-Food50.0 55.6 61.533.3 50.0 61.533.3 16.7 38.566.7 22.2 30.855.6 I 76.983.3 55.6 69.283.3 61.1 61.583.3 72.2 69.283.3 61.1 I 69.2I Occupations Average55.748.329.539.977.569.468.774.971.2Table 4: Percentage of internal nodes with the correct relationship to all descendants whenwords with the wrong predominant sense are removed.results of testing on this data set and eachmeasure, for the evaluation measures de-scribed above, percentage of correct parent-child relationships and percentage of nodesin the correct relationship to all of their de-scendants.Note that on these nouns, freq and H~0are among the worst performers; in fact, bylooking at the parent-child results, we cansee that these measures actually do worsethan chance.
As nouns start to get extremelygeneral, their frequency appears to actuallydecrease, so these are no longer useful mea-sures.
On the: other hand, Hmod is still oneof the best performers; although it does per-form worse here than on very specific nouns,it still assigns the correct relationship to apair of nouns about 59% of the time.5 Conc lus ionDetermining the relative specificity of nounsis a task which can help in automaticallybuilding or augmenting a semantic lexicon.We have identified various measures whichcan identify which of two nouns is more spe-cific with over 80% accuracy on basic-level ormore specific nouns.
The best among thesemeasures eem to be Hmod, the entropy ofthe rightmost modifier of a noun, Hs0, theentropy of words appearing within a 50-wordwindow of a target noun, and freq, the fre-quency of the noun.
These three measuresperform approximately equivalently.If the task requires handling very gen-eral nouns as well as those at or below thebasic level, we recommend using the Hmodmeasure.
This measure performs nearly aswell as the other two on specific nouns, andmuch better on general nouns.
However,67Specificity measure Parent-child All descendantsPmod 59.1 46.4Padj 60.2 46.4Pvb 50.0 35.7Pnn 50.0 28.6Hmod 59.1 39.3H2 53.4 25.0H10 45.5 32.1Hs0 i 46.6 32.1Freq 45.5 32.1Table 5: Evaluation of the various specificity measures on a test set of more general nouns.if it is known that the task will only in-volve fairly specific nouns, such as addingdomain-specific terms to an existing hier-archy which already has the more generalnouns arranged appropriately, the easily-computed freq measure can be used instead.6 AcknowledgmentsThanks to Mark Johnson and to the anony-mous reviewers for many helpful sugges-tions.
This research is supported in part byNSF grant IRI-9319516 and by ONR grantN0014-96-1-0549.ReferencesMatthew Berland and Eugene Charniak.1999.
Finding parts in very large corpora.In Proceedings of the 37th Annual Meet-ing of the Association for ComputationalLinguistics.Sharon A. Caraballo.
1999.
Automatic on:struction of a hypernym-labeled noun hi-erarchy from text.
In Proceedings of the37th Annual Meeting of the Associationfor Computational Linguistics.Christiane Fellbaum, editor.
1998.
Word-Net: An Electronic Lexical Database.
MITPress.Gregory Grefenstette.
1993.
SEXTANT:Extracting semantics from raw text imple-mentation details.
Heuristics: The Jour-nal of Knowledge Engineering.Marti A. Hearst.
1992.
Automatic acquisi-tion of hyponyms from large text corpora.In Proceedings of the Fourteenth Interna-tional Conference on Computational Lin-guistics.George Lakoff.
1987.
Women, Fire, andDangerous Things: What Categories Re-veal about he Mind.
University of ChicagoPress.Ellen Riloff and Jessica Shepherd.
1997.A corpus-based approach for building se-mantic lexicons.
In Proceedings of the Sec-ond Conference on Empirical Methods inNatural Language Processing, pages 117-124.Brian Roark and Eugene Charniak.
1998.Noun-phrase co-occurrence statistics forsemi-automatic semantic lexicon construc-tion.
In COLING-ACL '98: 36th An-nual Meeting of the Association for Com-putational Linguistics and 17th Interna-tional Conference on Computational Lin-guistics: Proceedings of the Conference,pages 1110-1116.Karen Sparck Jones.
1972.
A statistical in-terpretation of term specificity and its ap-plication in retrieval.
Journal of Docu-mentation, 28:11-21.68Append ixBelow are the data sets used in these exper-iments.
Words shown in italics are omittedfrom the results in Tables 3 and 4 becausethe predominant sense in the Wall StreetJournal text is not the one represented bythe word's position in this hierarchy.Food:fo )~(,verage- -  alcg.holnquor| gin| rum| vodkai brandy cognacI Winc~ampagnecoladessertbreadmuffin 'crackercheesemeatliverveal, ham ~ ingredientrelishI L_  olives' ketchupdishsandwich souppizzasaladbutteri cakecookieeggcandyI mint~pPr  apastry s tanoodlesoduce~e~ vegetable tomatoesshroomumepeafruitpineapplepeachesi oerrystrawberryVehicles:vehicle--1 truck~-- \ ]  van  .
.mlnlvanscarcompactlimousinesjeepwagoncabsedan coupehatchbacktrailer campersi craft~sse lyachtb?~argesmotorcycleI motorbike wagonI cartOccupations:personworkerI editortechnicianriterjournalistcolumnistcommentatornovelistbiographerintellectualscientistI sociologistt--7 chemist\[____L_~ biochemistphysicist' scholar' .
historianprofessionalphysicianI i speciali_stpsychiatristveterinarianeducatorI teachernursedentist~p.e  l ader administratortertainer r formercomedianI engineer' homemaker69Entities (data used for Table 5):entityorganismpersonanimalverminmammal?
horsedogcatcattlebirdchickenduckfiSl~erringsalmontroutreptileturtlesnakelizardalligatorvirusbacteriamicrobe-, cause\] i dangerhazard- -  menaceobjectsubstancefood" metalalloysteel' bronzegoldsilverir.oncarcinogenliquidwaterlocatio.nI regionI countrystatecitycom mq&ty\[ clothingapplianceartifactcoveringpaintroofcurtaincreationI artmusicpublication\[ book,, articledecorationsedativeinterferonenclosurefabricnylonwoolfacilityairportheadquartersstationfixturestructureOUSectorystore~partorgan\[ \]---- ~eart \] ~-a~: ngI cornerI fragmentsliceneedvariable70
