Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215?223,Honolulu, October 2008. c?2008 Association for Computational LinguisticsProbabilistic Inference for Machine TranslationPhil Blunsom and Miles OsborneSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh, EH8 9AB, UK{pblunsom,miles}@inf.ed.ac.ukAbstractWe advance the state-of-the-art for discrimi-natively trained machine translation systemsby presenting novel probabilistic inferenceand search methods for synchronous gram-mars.
By approximating the intractable spaceof all candidate translations produced by inter-secting an ngram language model with asynchronous grammar, we are able to trainand decode models incorporating millions ofsparse, heterogeneous features.
Further, wedemonstrate the power of the discriminativetraining paradigm by extracting structuredsyntactic features, and achieving increases intranslation performance.1 IntroductionThe goal of creating statistical machine translation(SMT) systems incorporating rich, sparse, featuresover syntax and morphology has consumed muchrecent research attention.
Discriminative approachesare widely seen as a promising technique, poten-tially allowing us to further the state-of-the-art.Most work on discriminative training for SMT hasfocussed on linear models, often with margin basedalgorithms (Liang et al, 2006; Watanabe et al,2006), or rescaling a product of sub-models (Och,2003; Ittycheriah and Roukos, 2007).Recent work by Blunsom et al (2008) has shownhow translation can be framed as a probabilisticlog-linear model, where the distribution over trans-lations is modelled in terms of a latent variableon derivations.
Their approach was globally opti-mised and discriminative trained.
However, a lan-guage model, an information source known to becrucial for obtaining good performance in SMT, wasnotably omitted.
This was because adding a lan-guage model would mean that the normalising parti-tion function could no longer be exactly calculated,thereby preventing efficient parameter estimation.Here, we show how language models can beincorporated into large-scale discriminative transla-tion models, without losing the probabilistic inter-pretation of the model.
The key insight is that wecan use Monte-Carlo methods to approximate thepartition function, thereby allowing us to tackle theextra computational burden associated with addingthe language model.
This approach is theoreti-cally justified and means that the model contin-ues to be both probabilistic and globally optimised.As expected, using a language model dramaticallyincreases translation performance.Our second major contribution is an exploita-tion of syntactic features.
By encoding source syn-tax as features allows the model to use, or ignore,this information as it sees fit, thereby avoiding theproblems of coverage and sparsity associated withdirectly incorporating the syntax into the grammar(Huang et al, 2006; Mi et al, 2008).
We report ontranslation gains using this approach.We begin by introducing the synchronous gram-mar approach to SMT in Section 2.
In Section3 we define the parametric form of our modeland describe techniques for approximating theintractable space of all translations for a givensource sentence.
In Section 4 we evaluate the abil-ity of our model to effectively estimate the highlydependent weights for the sparse features and real-valued language model.
In addition we describe how215X??
???
, Brown?X??
??
X???
X?, arrived in X?from X??X??
???
, Shanghai?X??
???
, Beijing?X??
?X??
??
??
X?, X?X?late last night?S ?
?X???
, X?.
?Figure 1.
An example SCFG derivation from a Chi-nese source sentence which yields the English sentence:?Brown arrived in Shanghai from Beijing late last night.
?our model can easily integrate rich features oversource syntax trees and compare our training meth-ods to a state-of-the-art benchmark.2 Synchronous context free grammarA synchronous context free grammar (SCFG,(Lewis II and Stearns, 1968)) describes the gener-ation of pairs of strings.
A string pair is generatedby applying a series of paired context-free rewriterules of the form,X ?
?
?, ?,?
?, whereX is a non-terminal, ?
and ?
are strings of terminals and non-terminals and ?
specifies a one-to-one alignmentbetween non-terminals in ?
and ?.
In the context ofSMT, by assigning the source and target languagesto the respective sides of a SCFG it is possible todescribe translation as the process of parsing thesource sentence, while generating the target trans-lation (Chiang, 2007).In this paper we only consider grammarsextracted using the heuristics described for the HieroSMT system (Chiang, 2007).
Note however that ourapproach is general and could be used with othersynchronous grammar transducers (e.g., (Galley etal., 2006)).
SCFG productions can specify that theorder of the child non-terminals is the same in bothlanguages (a monotone production), or is reversed (areordering production).
Without loss of generality,here we add the restriction that non-terminals on thesource and target sides of the grammar must have thesame category.
Figure 1 shows an example deriva-tion for Chinese to English translation.3 ModelWe start by defining a log-linear model for the con-ditional probability distribution over target transla-tions of a given source sentence.
A sequence ofSCFG rule applications which produce a translationfrom a source sentence is referred to as a derivation,and each translation may be produced by many dif-ferent derivations.
As the training data only providessource and target sentences, the derivations are mod-elled as a latent variable.The conditional probability of a derivation, d, fora target translation, e, conditioned on the source, f ,is given by:p?
(d, e|f) =exp?k ?kHk(d, e, f)Z?
(f)(1)where Hk(d, e, f) =?r?dhk(f , r, q(r,d)) (2)Using Equation (1), the conditional probability ofa target translation given the source is the sum overall of its derivations:p?
(e|f) =?d??(e,f)p?
(d, e|f)where ?
(e, f) is the set of all derivations of thetarget sentence e from the source f.Here k ranges over the model?s features, and?
= {?k} are the model parameters (weights fortheir corresponding features).
The function q(r,d)returns the target ngram context, for a languagemodel with order m, of rule r in derivation d.For a rule which spans the target words (i, j) andtarget yield(d) = {t0, ?
?
?
, tl}:q(r,d) ={ti???ti+m?2?tj?m+2??
?tj if j ?
i > mti??
?tj otherwiseThe feature functions hk are real-valued functionsover the source and target sentences, and can includeoverlapping and non-independent features of thedata.
The features must decompose with the deriva-tion and the ngram context defined by the function q,as shown in Equation (2).
The features can referencethe entire source sentence coupled with each rule, r,and its target context, in a derivation.By directly incorporating the language modelcontext q into the model formulation, we will not216be able to exactly compute the partition functionZ?
(f), which sums over all possible derivations.Even though a dynamic program over this spacewould still run in polynomial time, as shown by Chi-ang (2007), a packed chart representation of the par-tition function for the binary Hiero grammars usedin this work would require O(n3|T |4(m?1)) space,1which is far too large to be practical.Instead we approximate the partition functionusing a sum over a large subset of the possiblederivations (?
(e, f)):Z?
(f) ??e?d?{??
(e,f)}exp?k?kHk(d, e, f)= Z??
(f)This model formulation raises the questions ofwhat an appropriate large subset of derivations fortraining is, and how to efficiently calculate the sumover all derivations in decoding.
In the followingsections we elucidate and evaluate our solutions tothese problems.3.1 Sampling DerivationsThe training and decoding algorithms presented inthe following sections rely upon Monte-Carlo tech-niques, which in turn require the ability to drawderivation samples from the probability distributiondefined by our log-linear model.
Here we adaptpreviously presented algorithms for sampling froma PCFG (Goodman, 1998) for use with our syn-chronous grammar model.
Algorithm 1 describes thealgorithm for sampling derivations.
The samplingalgorithm assumes the pre-existance of a packedchart representation of all derivations for a givensource sentence.
The inside algorithm is then usedto calculate the scores needed to define a multino-mial distribution over all partial derivations associ-ated with expanding a given child rule.
These ini-tial steps are performed once and then an unlim-ited number of samples can be drawn by calling therecursive SAMPLE procedure.
MULTI draws a samplefrom the distribution over rules for a given chart cell,CHILDREN enumerates the chart cells connected toa rule as variables, and DERIVATION is a recursivetree data structure for derivations.
The algorithm is1where |T | is the size of the terminal alphabet, i.e.
the num-ber of unique English words.Algorithm 1 Top-down recursive derivation sam-pling algorithm.1: procedure SAMPLE(X, i, k)2: rule?
MULTI(inside chart(X, i, k))3: c = ?4: for (child category, x, y) ?
CHILDREN(rule)do5: c?
c ?
SAMPLE(child category, x, y)6: end for7: return DERIVATION(X, children)8: end procedurefirst called on a category and chart cell spanning theentire chart, and then proceeds top down by usingthe function MULTI to draw the next rule to expandfrom the distribution defined by the inside scores.3.2 Approximate InferenceApproximating the partition function with Z??
(f)could introduce biases into inference and in the fol-lowing discussion we describe measures taken tominimise the effects of the approximation bias.An obvious approach to approximating the parti-tion function, and the feature expectations requiredfor calculating the derivative in training, is to usethe packed chart of derivations produced by runningthe cube pruning beam search algorithm of Chiang(2007) on the source sentence.
In this case Z??
(f)includes all the derivations that fall within the cubepruning beam, hopefully representing the majorityof the probability mass.
We denote the partitionfunction estimated with this cube beam approxima-tion as Z?cb?
(f).
This approach has the advantage ofusing the same beam search dynamic program dur-ing training as is used for decoding.
As the approxi-mated partition function does not contain all deriva-tions, it is possible that some, or all, of the deriva-tions of the reference translation from the parallelcorpus may be excluded.
We must therefore intersectthe packed chart built from the cube beam with thatof the reference derivations to ensure consistency.Although, as would be done using cube-pruning,it would seem intuitively sensible to approximatethe partition function using only high probabilityderivations, it is possible that doing so will biasour model in odd ways.
The space of derivationscontained within the beam will be tightly clusteredabout a maximum, and thus a model trained withsuch an approximation will only see a very small217Alles /Everythingund /andjedes /anythingist /isvorstellbar /possibleXXXSAlles /Everythingund /andjedes /everyoneist /isvorstellbar /conceivableXXXSX[1,2]EverythingX[3,4]anythingX[1,4]EverythingandsX[4,5]isX[5,6]possibleX[1,5]Everything *isS[1,6]Everything *possibleX[5,6]conceivableX[2,3]andX[1,3]Everything *anythingX[1,3]Everything *everyoneX[3,4]everyoneS[1,6]Everything *conceivableAlles /Everythingund /andjedes /anythingist /isvorstellbar /conceivableXXXS(a)(c)(b)213456213456213456Figure 2.
A German-English translation example of building Z?sam?
(f) from samples.
(a) Two sample derivations aredrawn from the model, (b) these samples are then combined into a packed representation, here represented by ahypergraph with target translations elided for a bigram language model.
The derivation in (c) is contained within thehypergraph even though it was never explicitly inserted.part of the overall distribution, possibly leading itastray.
Consider the example of a language modelfeature: as this is a very strong indicator of transla-tion quality, we would expect all derivations withinthe beam to have a similar (high) language modelscore, thereby robbing this feature of its discriminat-ing power.
However if our model could also see thelow probability derivations it would be clear that thisfeature is indeed very strongly correlated with goodtranslations.
Thus a good approximation of the spaceof derivations is one that includes both good and badexamples, not just a cluster around the maximum.A principled solution to the problem of approx-imating the partition function would be to use aMarkov Chain Monte Carlo (MCMC) sampler toestimate the sum with a large number of samples.Most of the sampled derivations would be in thehigh probability region of the distribution, howeverthere would also be a number of samples drawnfrom the rest of the space, giving the model a moreglobal view of the distribution, avoiding the pit-falls of the narrow view obtained by a beam search.Although effective, the computational cost of suchan approach is prohibitive as we would need to drawhundreds of thousands of samples to obtain conver-gence, for every training iteration.Here we mediate between the computationaladvantages of a beam and the broad view of the dis-tribution provided by sampling.
Using the algorithmoutlined in Section 3.1 we draw samples from thedistribution of derivations and then insert these sam-ples into a packed chart representation.
This processis illustrated in Figure 2.
The packed chart createdby intersecting the sample derivations represents aspace of derivations much greater than the originalsamples.
In Figure 2 the chart is built from the firsttwo sampled derivations, while the third derivationcan be extracted from the chart even though it wasnever explicitly entered.
This approximation of thepartition function (denoted Z?sam?
(f)) allows us tobuild an efficient packed chart representation of alarge number of derivations, centred on those withhigh probability while still including a significantrepresentation of the low probability space.
Deriva-tions corresponding to the reference can be detectedduring sampling and thus we can build the chartfor the reference derivations at the same time asthe one approximating the partition function.
Thiscould lead to some, or none of, the possible ref-erence derivations being included, as they may nothave been sampled.
Although we could intersect allof the reference derivations with the sampled chart,this could distort the distribution over derivations,218and we believe it to be advantageous to keep thedistributions between the partition function and ref-erence charts consistent.Both of the approximations proposed above,Z?cb?
(f) and Z?sam?
(f), rely on the pre-existence of atrained translation model in order to either guide thecube-pruning beam, or define the probability distri-bution from which we draw samples.
We solve thischicken and egg problem by first training an exacttranslation model without a language model, andthen use this model to create the partition functionapproximations for training with a language model.We denote the distribution without a language modelas p?LM?
(e|f) and that with as p+LM?
(e|f).A final training problem that we need to addressis the appropriate initialisation of the model param-eters.
In theory we could simply randomly initialise?
for p+LM?
(e|f), however in practice we found thatthis resulted in poor performance on the develop-ment data.
This is due to the complex non-convexoptimisation function, and the fact that many fea-tures will fall outside the approximated charts result-ing in random, or zero, weights in testing.
We intro-duce a novel solution in which we use the Gaus-sian prior over model weights to tie the exact modeltrained without a language model, which assignssensible values to all rule features, with the approx-imated model.
The prior over model parameters forp+LM?
(e|f) is defined as:p+LM(?k) ?
e???k??
?LMk ?22?2Here we have set the mean parameters of the Gaus-sian distribution for the approximated model tothose learnt for the exact one.
This has the effectthat any features that fall outside the approximatedmodel will simply retain the weight assigned by theexact model.
While for other feature weights theprior will penalise substantial deviations away from?
?LM , essentially encoding the intuition that therule rule parameters should not change substantiallywith the inclusion of language model features.This results in the following log-likelihood objec-tive and corresponding gradient:L =?
(ei,fi)?Dlog p+LM?
(ei|fi) +?klog p+LM0 (?k)?L?
?k= Ep+LM?
(d|ei,fi)[hk]?
Ep+LM?
(e|fi)[hk]?
?+LMk ?
?
?LMk?23.3 DecodingAs stated in Equation 3 the probability of a giventranslation string is calculated as the sum of theprobabilities of all the derivations that yield thatstring.
In decoding, where the reference translationis not known, the exact calculation of this summa-tion is NP-Hard.
This problem also arises in mono-lingual parsing with probabilistic tree substitutiongrammars and has been tackled in the literatureusing Monte-Carlo sampling methods (Chappelierand Rajman, 2000).
Their approach is directly appli-cable to our SCFG decoding problem and we can useAlgorithm 1 to draw sample translation derivationsfor the source sentence.
The probability of a trans-lation can be calculated simply from the number oftimes a derivation that yields it was sampled, dividedby the total number of samples.
For the p?LM?
(e|f)model we can build the full chart of all possiblederivations and thus sample from the true distribu-tion over derivations.
For the p+LM?
(e|f) model wesuffer the same problem as in training and cannotbuild the full chart.
Instead a chart is built usingthe cube-pruning algorithm with a wide beam andwe then draw samples from this chart.
Althoughsampling from a reduced chart will result in biasedsamples, in Section 4 we show this approach to beeffective in practice.2 In Section 4 we compare oursampling approach to the heuristic beam search pro-posed by Blunsom et al (2008).It is of interest to compare our proposed decodingalgorithms to minimum Bayes risk (MBR) decoding(Kumar and Byrne, 2004), a commonly used decod-ing method.
From a theoretical standpoint, the sum-ming of derivations for a given translation is exactly2We have experimented with using a Metropolis Hastingssampler, with p?LM?
(e|f) as the proposal distribution, to sam-ple from the true distribution with the language model.
Unfor-tunately the sample rejection rate was very high such that thismethod proved infeasibly slow.219equivalent to performing MBR with a 0/1 loss func-tion over derivations.
From a practical perspective,MBR is normally performed with BLEU as the lossand approximated using n-best lists.
These n-bestlists are produced using algorithms tuned to removemultiple derivations of the same translation (whichhave previously been seen as undesirable).
However,it would be simple to extend our sampling baseddecoding algorithm to calculate the MBR estimateusing BLEU , in theory providing a lower varianceestimate than attained with n-best lists.4 EvaluationWe evaluate our model on the IWSLT 2005 Chineseto English translation task (Eck and Hori, 2005),using the 2004 test set as development data fortuning the hyperparameters and MERT training thebenchmark systems.
The statistics for this data arepresented in Table 1.3 The training data made avail-able for this task consisted of 40k pairs of tran-scribed utterances, drawn from the travel domain.The development and test data for this task are some-what unusual in that each sentence has a singlehuman translated reference, and fifteen paraphrasesof this reference, provided by monolingual anno-tators.
Model performance is evaluated using thestandard BLEU metric (Papineni et al, 2002) whichmeasures average n-gram precision, n ?
4, and weuse the NIST definition of the brevity penalty formultiple reference test sets.
We provide evaluationagainst both the entire multi-reference sets, and thesingle human translation.Our translation grammar is induced using thestandard alignment and rule extraction heuristicsused in hierarchical translation models (Chiang,2007).4 As these heuristics aren?t based on a genera-tive model, and don?t guarantee that the target trans-lation will be reachable from the source, we discardthose sentence pairs for which we cannot produce aderivation, leaving 38,405 sentences for training.Our base model contains a single feature for eachrule which counts the number of times it appeared ina particular derivation.
For models which include a3Development and test set statistics are for the single humantranslation reference.4With the exception that we allow unaligned words at theboundary of rules.
This improves training set coverage.language model, we train a standard Kneser-Ney tri-gram model on the target side of the training corpus.We also include a word penalty feature to compen-sate for the shortening effect of the language model.In total our model contains 2.9M features.The aims of our evaluation are: (1) to deter-mine that our proposed training regimes are able torealise performance increase when training sparserule features and a real valued language model fea-ture together, (2) that the model is able to effectivelyuse rich features over the source sentence, and (3)to confirm that our model obtains performance com-petitive with the current state-of-the-art.4.1 Inference and DecodingWe have described a number of modelling choiceswhich aim to compensate for the training biasesintroduced by incorporating a language model fea-ture through approximate inference.
Our a prioriknowledge from other SMT systems suggests thatincorporating a language model should lead to largeincreases in BLEU score.
In this evaluation we aimto determine whether our training regimes are ableto realises these expected gains.Table 2 shows a matrix of development BLEUscores achieved by varying the approximation of thepartition function in training, and varying the decod-ing algorithm.
If we consider the vertical axis wecan see that the sampling method for approximat-ing the partition function has a small but consistentadvantage over using the cube-pruning beam.
Thecharts produced by the sampling approach occupyroughly half the disc space as those produced bythe beam search, so in subsequent experiments wepresent results using the Z?sam?
(f) approximation.Comparing the decoding algorithms on the hori-zontal axis we can reconfirm the findings of Blun-som et al (2008) that the max-translation decod-ing outperforms the Viterbi max-derivation approx-imation.
It is also of note that this BLEU increaseis robust to the introduction of the language modelfeature, assuaging fears that the max-translationapproach may have been doing the job of the lan-guage model.
We also compare using Monte-Carlosampling for decoding with the previously pro-posed heuristic beam search algorithm.
The differ-ence between the two algorithms is small, however220Training Development TestChinese English Chinese English Chinese EnglishUtterances 38405 500 506Segments/Words 317621 353116 3464 3752 3784 3823Av.
Utterances Length 8 9 6 7 7 7Longest Utterance 55 68 58 62 61 56Table 1.
IWSLT Chinese to English translation corpus statistics.Model Max-derivation Max-translation(Beam) Max-translation(Sampling)p?LM?
(e|f) 31.0 32.5 32.6p+LM?
(e|f) (Z?cb?
(f)) 39.1 39.8 39.8p+LM?
(e|f) (Z?sam?
(f)) 39.9 40.5 40.6Table 2.
Development set results for varying the approximation of the partition function in training, Z?cb?
(f) vs. Z?sam?
(f),and decoding using the Viterbi max-derivation algorithm, or the max-translation algorithm with either a beam approxi-mation or Monte-Carlo sampling.we feeling the sampling approach is more theoreti-cally justified and adopt it for our later experiments.The most important result from this evaluationis that both our training regimes realise substantialgains from the introduction of the language modelfeature.
Thus we can be confident that our modelis capable of modelling the distribution over trans-lations even when the space over all derivations isintractable to dynamically program exactly.4.2 A Discriminative Syntactic TranslationModelIn the previous sections we?ve described and evalu-ated a statistical model of translation that is able toestimate a probability distribution over translationsusing millions of sparse features.
A prime motiva-tion for such a model is the ability to define com-plex features over more than just the surface formsof the source and target strings.
There are limit-less options for such features, and previous workhas focused on defining token based features suchas part-of-speech and morphology (Ittycheriah andRoukos, 2007).
Although such features are applica-ble to our model, here we attempt to test the model?sability to incorporate complex features over source-side syntax trees, essentially subsuming and extend-ing previous work on tree-to-string translation mod-els (Huang et al, 2006; Mi et al, 2008).We first parse the source side of our training,development and test corpora using the Stanfordparser.5 Next, while building the synchronous charts5http://nlp.stanford.edu/software/lex-parser.shtmlrequired for training, whenever a rule is used in aderivation a feature is activated which captures: (1)the constituent spanning the rule?s source side in thesyntax tree (if any) (2) constituents spanning anyvariables in the rule, and (3) the rule?s target sidesurface form.
Figure 3 illustrates this process.These syntactic features are equivalent to thegrammar rules extracted for tree-to-string translationsystems.
The key difference in our model is that thesource syntax tree is treated as conditioning contextand it?s information encoded as features, thus thisinformation can be used or ignored as the model seesfit.
This avoids the problems associated with explic-itly encoding the source syntax in the grammar, suchas sparsity and overly constraining the model.
Inaddition we could easily incorporate features overmultiple source trees, for example mixing labelledsyntax trees with dependency graphs.We limit the extraction of syntactic features tothose that appear in at least two training derivations,giving a total of 4.2M syntactic features, for an over-all total of 7.1M features.4.3 DiscussionTable 3 shows the results from applying ourdescribed models to the test set.
We benchmark ourresults against a model (Hiero) which was directlytrained to optimise BLEUNIST using the standardMERT algorithm (Och, 2003) and the full set oftranslation and lexical weight features describedfor the Hiero model (Chiang, 2007).
As well as221Model BLEUNIST BLEUIBM BLEUHumanRefp?LM?
(e|f) 33.5 35.2 25.2p+LM?
(e|f) 44.6 44.6 31.2p+LM?
(e|f) + syntax 45.3 45.2 31.8MERT (BLEUNIST ) 46.2 44.5 30.2Table 3.
Test set results.??????
??
?V WH ?NNNPVPSQWhere is the currency exchange office ?
(Step 2) X2-> < [X1] ?
??
?, Where is the [X1] ?>(Step 1) X1-> <??
??
?, currency exchange office>NPSQ?
??
?Where is the [X1] ?
Example Syntax feature =for Step 2Example Derivation:X1(a) (b)(c)Figure 3.
Syntax feature example: For the parsed source and candidate translation (a), with the derivation (b), weextract the syntax feature in (c) by combining the grammar rule with the source syntax of the constituents containedwithin that rule.Source ???????????????????p?LM?
(e|f) don ?t have enough bag on me change please go purchase a new by plane .p+LM?
(e|f) i have enough money to buy a new one by air .p+LM?
(e|f) + syntax i don ?t have enough money to buy a new airline ticket .MERT (BLEUNIST ) i don ?t have enough money to buy a new ticket .Reference i do n?t have enough money with me to buy a new airplane ticket .Table 4.
Example test set output produced when: not using a language model, using a language model, also usingsyntax, output optimised using MERT and finally the referenceBLEUNIST (brevity penalty uses the shortest ref-erence), we also include results from the IBM(BLEUIBM ) metric (brevity penalty uses the closestreference), and using only the actual human transla-tion in the test set, not the monolingual paraphrasemultiple references (BLEUHumanRef ).The first result of interest is that we see anincrease in performance through the incorporationof the source syntax features.
This is an encourag-ing result as the transcribed speech source sentencesare well out of the domain of the data on which theparser was trained, suggesting that our model is ableto sift the good information from the noisy in theunreliable source syntax trees.
Table 4 shows illus-trative system output on the test set.On the BLEUNIST metric we see that our mod-els under-perform the MERT trained system.
Wehypothesise that this is predominately due to theinteraction of the brevity penalty with the unusualnature of the multiple paraphrase reference train-ing and development data.
Their performance ishowever quite consistent across the different inter-pretations of the brevity penalty (NIST vs. IBM).This contrasts with the MERT trained model, whichclearly over-fits to the NIST metric that it wastrained on and underperforms our models when eval-uated on the single human test translations.
If wedirectly compare the brevity penalties of the MERTmodel (0.868) and our discriminative model incor-porating source syntax (0.942), on the these single222references, we can see that the MERT training hasoptimised to the shortest paraphrase reference.From these results it is difficult to draw any hardconclusions on the relative performance of the dif-ferent training regimes.
However we feel confidentin claiming that we have achieved our goal of train-ing a probabilistic model on millions of sparse fea-tures which obtains performance competitive withthe current state-of-the-art training algorithm.5 ConclusionIn this paper we have shown that statistical machinetranslation can be effectively modelled as a wellposed machine learning task.
In doing so we havedescribed a model capable of estimating a probabil-ity distribution over translations using sparse com-plex features, and achieving performance compara-ble to the state-of-the-art on standard metrics.
Withfurther work on scaling these models to large datasets, and engineering high performance features, webelieve this research has the potential to provide sig-nificant increases in translation quality.AcknowledgementsThe authors acknowledge the support of the EPSRCgrant EP/D074959/1.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
of the 46th Annual Con-ference of the Association for Computational Linguis-tics: Human Language Technologies (ACL-08:HLT),pages 200?208, Columbus, Ohio, June.Jean-Ce?dric Chappelier and Martin Rajman.
2000.Monte-carlo sampling for np-hard maximization prob-lems in the framework of weighted parsing.
In NLP?00: Proceedings of the Second International Confer-ence on Natural Language Processing, pages 106?117, London, UK.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Matthias Eck and Chiori Hori.
2005.
Overview of theIWSLT 2005 evaluation campaign.
In Proc.
of theInternational Workshop on Spoken Language Trans-lation, Pittsburgh, October.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of the 44th Annual Meeting of the ACL and 21stInternational Conference on Computational Linguis-tics (COLING/ACL-2006), pages 961?968, Sydney,Australia, July.Joshua T. Goodman.
1998.
Parsing inside-out.
Ph.D.thesis, Cambridge, MA, USA.
Adviser-Stuart Shieber.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In In Proceedings of the 7th Bien-nial Conference of the Association for Machine Trans-lation in the Americas (AMTA), Boston, MA.Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In Proc.
of the 7th InternationalConference on Human Language Technology Researchand 8th Annual Meeting of the NAACL (HLT-NAACL2007), pages 57?64, Rochester, USA.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Proc.
of the 4th International Conference on HumanLanguage Technology Research and 5th Annual Meet-ing of the NAACL (HLT-NAACL 2004), pages 169?176.Philip M. Lewis II and Richard E. Stearns.
1968.
Syntax-directed transduction.
J. ACM, 15(3):465?488.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminativeapproach to machine translation.
In Proc.
of the44th Annual Meeting of the ACL and 21st Inter-national Conference on Computational Linguistics(COLING/ACL-2006), pages 761?768, Sydney, Aus-tralia, July.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of the 46th Annual Confer-ence of the Association for Computational Linguistics:Human Language Technologies (ACL-08:HLT), pages192?199, Columbus, Ohio, June.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the 41stAnnual Meeting of the ACL (ACL-2003), pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proc.
of the 40thAnnual Meeting of the ACL and 3rd Annual Meeting ofthe NAACL (ACL-2002), pages 311?318, Philadelphia,Pennsylvania.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of the 44th AnnualMeeting of the ACL and 21st International Conferenceon Computational Linguistics (COLING/ACL-2006),pages 777?784, Sydney, Australia.223
