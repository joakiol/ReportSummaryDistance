Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 485?494,Honolulu, October 2008. c?2008 Association for Computational LinguisticsImproving Interactive Machine Translation via Mouse ActionsGerma?n Sanchis-Trilles and Daniel Ortiz-Mart?
?nez and Jorge CiveraInstituto Tecnolo?gico de Informa?ticaUniversidad Polite?cnica de Valencia{gsanchis,dortiz,jorcisai}@iti.upv.esFrancisco Casacuberta and Enrique VidalDepartamento de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de Valencia{fcn,evidal}@dsic.upv.esHieu HoangUniversity of Edinburghhhoang@sms.ed.ac.ukAbstractAlthough Machine Translation (MT) is a veryactive research field which is receiving an in-creasing amount of attention from the researchcommunity, the results that current MT sys-tems are capable of producing are still quitefar away from perfection.
Because of this,and in order to build systems that yield correcttranslations, human knowledge must be inte-grated into the translation process, which willbe carried out in our case in an Interactive-Predictive (IP) framework.
In this paper, weshow that considering Mouse Actions as a sig-nificant information source for the underly-ing system improves the productivity of thehuman translator involved.
In addition, wealso show that the initial translations that theMT system provides can be quickly improvedby an expert by only performing additionalMouse Actions.
In this work, we will be usingword graphs as an efficient interface betweena phrase-based MT system and the IP engine.1 IntroductionInformation technology advances in modern societyhave led to the need of more efficient methods oftranslation.
It is important to remark that currentMT systems are not able to produce ready-to-usetexts (Kay, 1997; Hutchins, 1999; Arnold, 2003).Indeed, MT systems are usually limited to specificsemantic domains and the translations provided re-quire human post-editing in order to achieve a cor-rect high-quality translation.A way of taking advantage of MT systems is tocombine them with the knowledge of a human trans-lator, constituting the so-called Computer-AssistedTranslation (CAT) paradigm.
CAT offers differentapproaches in order to benefit from the synergy be-tween humans and MT systems.An important contribution to interactive CATtechnology was carried out around the TransType(TT) project (Langlais et al, 2002; Foster et al,2002; Foster, 2002; Och et al, 2003).
This projectentailed an interesting focus shift in which interac-tion directly aimed at the production of the targettext, rather than at the disambiguation of the sourcetext, as in former interactive systems.
The ideaproposed was to embed data driven MT techniqueswithin the interactive translation environment.Following these TT ideas, (Barrachina and oth-ers, 2008) propose the usage of fully-fledged statis-tical MT (SMT) systems to produce full target sen-tence hypotheses, or portions thereof, which can bepartially or completely accepted and amended by ahuman translator.
Each partial correct text segmentis then used by the SMT system as additional infor-mation to achieve further, hopefully improved sug-gestions.
In this paper, we also focus on the inter-active and predictive, statistical MT (IMT) approachto CAT.
The IMT paradigm fits well within the In-teractive Pattern Recognition framework introducedin (Vidal and others, 2007).485SOURCE (x): Para encender la impresora:REFERENCE (y): To power on the printer:ITER-0 (p) ( )(s?h) To switch on:ITER-1(p) To(sl) switch on:(k) power(s?h) on the printer:ITER-2(p) To power on the printer:(sl) ( )(k) (#)(s?h) ( )FINAL (p ?
y) To power on the printer:Figure 1: IMT session to translate a Spanish sentence into English.
Non-validated hypotheses are displayed in italics,whereas accepted prefixes are printed in normal font.Figure 1 illustrates a typical IMT session.
Ini-tially, the user is given an input sentence x to betranslated.
The reference y provided is the trans-lation that the user would like to achieve at the endof the IMT session.
At iteration 0, the user does notsupply any correct text prefix to the system, for thisreason p is shown as empty.
Therefore, the IMT sys-tem has to provide an initial complete translation sh,as it were a conventional SMT system.
At the nextiteration, the user validates a prefix p as correct bypositioning the cursor in a certain position of sh.
Inthis case, after the words ?To print a?.
Implicitly, heis also marking the rest of the sentence, the suffix sl,as potentially incorrect.
Next, he introduces a newword k, which is assumed to be different from thefirst word sl1 in the suffix sl which was not validated,k 6= sl1 .
This being done, the system suggests a newsuffix hypothesis s?h, subject to s?h1 = k. Again, theuser validates a new prefix, introduces a new wordand so forth.
The process continues until the wholesentence is correct that is validated introducing thespecial word ?#?.As the reader could devise from the IMT sessiondescribed above, IMT aims at reducing the effortand increasing the productivity of translators, whilepreserving high-quality translation.
For instance, inFigure 1, only three interactions were necessary inorder to achieve the reference translation.In this paper, we will show how Mouse Actionsperformed by the human expert can be taken advan-tage of in order to further reduce this effort.2 Statistical interactive-predictive MTIn this section we will briefly describe the statisticalframework of IMT.
IMT can be seen as an evolutionof the SMT framework, which has proved to be anefficient framework for building state-of-the-art MTsystems with little human effort, whenever adequatecorpora are available (Hutchings and Somers, 1992).The fundamental equation of the statistical approachto MT isy?
= argmaxyPr(y |x) (1)= argmaxyPr(x |y)Pr(y) (2)where Pr(x |y) is the translation model modellingthe correlation between source and target sentenceand Pr(y) is the language model representing thewell-formedness of the candidate translation y.In practise, the direct modelling of the posteriorprobability Pr(y|x) has been widely adopted.
Tothis purpose, different authors (Papineni et al, 1998;Och and Ney, 2002) propose the use of the so-calledlog-linear models, where the decision rule is givenby the expressiony?
= argmaxyM?m=1?mhm(x,y) (3)where hm(x,y) is a score function representing animportant feature for the translation of x into y, Mis the number of models (or features) and ?m are theweights of the log-linear combination.486One of the most popular instantiations of log-linear models is that including phrase-based (PB)models (Zens et al, 2002; Koehn et al, 2003).Phrase-based models allow to capture contextual in-formation to learn translations for whole phrases in-stead of single words.
The basic idea of phrase-based translation is to segment the source sentenceinto phrases, then to translate each source phraseinto a target phrase, and finally to reorder the trans-lated target phrases in order to compose the tar-get sentence.
Phrase-based models were employedthroughout this work.In log-linear models, the maximisation problemstated in Eq.
3 is solved by means of the beam searchalgorithm1 which was initially introduced in (Low-erre, 1976) for its application in the field of speechrecognition.
The beam search algorithm attempts togenerate partial solutions, called hypotheses, untila complete sentence is found; these hypotheses arestored in a stack and ordered by their score.
Such ascore is given by the log-linear combination of fea-ture functions.However, Eq.
1 needs to be modified according tothe IMT scenario in order to take into account partof the target sentence that is already translated, thatis p and ks?h = argmaxshPr(sh|x,p, k) (4)where the maximisation problem is defined over thesuffix sh.
This allows us to rewrite Eq.
4, by decom-posing the right side appropriately and eliminatingconstant terms, achieving the equivalent criterions?h = argmaxshPr(p, k, sh|x).
(5)An example of the intuition behind these variablescan be seen in Figure 1.Note that, since (p k sh) = y, Eq.
5 is very simi-lar to Eq.
1.
The main difference is that the argmaxsearch is now performed over the set of suffixes shthat complete (p k) instead of complete sentences(y in Eq.
1).
This implies that we can use the samemodels if the search procedures are adequately mod-ified (Barrachina and others, 2008).1Also known as stack decoding algorithm.3 Phrase-based IMTThe phrase-based approach presented above can beeasily adapted for its use in an IMT scenario.
Themost important modification is to rely on a wordgraph that represents possible translations of thegiven source sentence.
The use of word graphsin IMT has been studied in (Barrachina and oth-ers, 2008) in combination with two different trans-lation techniques, namely, the Alignment Templatestechnique (Och et al, 1999; Och and Ney, 2004),and the Stochastic Finite State Transducers tech-nique (Casacuberta and Vidal, 2007).3.1 Generation of word graphsA word graph is a weighted directed acyclic graph,in which each node represents a partial translationhypothesis and each edge is labelled with a word ofthe target sentence and is weighted according to thescores given by an SMT model (see (Ueffing et al,2002) for more details).
In (Och et al, 2003), theuse of a word graph is proposed as interface betweenan alignment-template SMT model and the IMT en-gine.
Analogously, in this work we will be usinga word graph built during the search procedure per-formed on a PB SMT model.During the search process performed by the abovementioned beam search algorithm, it is possible tocreate a segment graph.
In such a graph, each noderepresents a state of the SMT model, and each edgea weighted transition between states labelled with asequence of target words.
Whenever a hypothesis isextended, we add a new edge connecting the stateof that hypothesis with the state of the extended hy-pothesis.
The new edge is labelled with the sequenceof target words that has been incorporated to the ex-tended hypothesis and is weighted appropriately bymeans of the score given by the SMT model.Once the segment graph is generated, it can beeasily converted into a word graph by the introduc-tion of artificial states for the words that composethe target phrases associated to the edges.3.2 IMT using word graphsDuring the process of IMT for a given source sen-tence, the system makes use of the word graph gen-erated for that sentence in order to complete the pre-fixes accepted by the human translator.
Specifically,487SOURCE (x): Para encender la impresora:REFERENCE (y): To power on the printer:ITER-0 (p) ( )(s?h) To switch on:ITER-1(p) To(sl) |switch on:(s?h) power on the printer:ITER-2(p) To power on the printer:(sl) ( )(k) (#)(s?h) ( )FINAL (p ?
y) To power on the printer:Figure 2: Example of non-explicit positioning MA which solves an error of a missing word.
In this case, the systemproduces the correct suffix sh immediately after the user validates a prefix p, implicitly indicating that we wants thesuffix to be changed, without need of any further action.
In ITER-1, character | indicates the position where a MAwas performed, sl is the suffix which was rejected by that MA, and s?h is the new suffix that the system suggests afterobserving that sl is to be considered incorrect.
Character # is a special character introduced by the user to indicate thatthe hypothesis is to be accepted.the system finds the best path in the word graph as-sociated with a given prefix so that it is able to com-plete the target sentence, being capable of providingseveral completion suggestions for each prefix.A common problem in IMT arises when the usersets a prefix which cannot be found in the wordgraph, since in such a situation the system is un-able to find a path through the word graph and pro-vide an appropriate suffix.
The common procedureto face this problem is to perform a tolerant searchin the word graph.
This tolerant search uses the wellknown concept of Levenshtein distance in order toobtain the most similar string for the given prefix(see (Och et al, 2003) for more details).4 Enriching user?machine interactionAlthough the IMT paradigm has proved to offer in-teresting benefits to potential users, one aspect thathas not been reconsidered as of yet is the user?machine interface.
Hence, in traditional IMT thesystem only received feedback whenever the usertyped in a new word.
In this work, we show howto enrich user?machine interaction by introducingMouse Actions (MA) as an additional informationsource for the system.
By doing so, we will considertwo types of MAs, i.e.
non-explicit (or positioning)MAs and interaction-explicit MAs.4.1 Non-explicit positioning MAsBefore typing in a new word in order to correct a hy-pothesis, the user needs to position the cursor in theplace where he wants to type such a word.
In thiswork, we will assume that this is done by perform-ing a MA, although the same idea presented can alsobe applied when this is done by some other means.It is important to point out that, by doing so, the useris already providing some very useful information tothe system: he is validating a prefix up to the posi-tion where he positioned the cursor, and, in addition,he is signalling that whatever word is located afterthe cursor is to be considered incorrect.
Hence, thesystem can already capture this fact and provide anew translation hypothesis, in which the prefix re-mains unchanged and the suffix is replaced by a newone in which the first word is different to the firstword of the previous suffix.
We are aware that thisdoes not mean that the new suffix will be correct, butgiven that we know that the first word in the previ-ous suffix was incorrect, the worst thing which canhappen is that the the first word of the new suffix isincorrect as well.
However, if the new suffix hap-pens to be correct, the user will happily find that hedoes not need to correct that word any more.An example of such behaviour can be seen inFigure 2.
In this example, the SMT system firstprovides a translation which the user does not488like.
Hence, he positions the cursor before word?postscript?, with the purpose of typing in ?lists?.By doing so, he is validating the prefix ?To printa?, and signalling that he wants ?postscript?
to bereplaced.
Before typing in anything, the system re-alises that he is going to change the word locatedafter the cursor, and replaces the suffix by anotherone, which is the one the user had in mind in thefirst place.
Finally, the user only has to accept thefinal translation.We are naming this kind of MA non-explicit be-cause it does not require any additional action fromthe user: he has already performed a MA in order toposition the cursor at the place he wants, and we aretaking advantage of this fact to suggest a new suffixhypothesis.Since the user needs to position the cursor beforetyping in a new word, it is important to point outthat any improvement achieved by introducing non-explicit MAs does not require any further effort fromthe user, and hence is considered to have no cost.Hence, we are now considering two different situ-ations: the first one, the traditional IMT framework,in which the system needs to find a suffix accordingto Eq.
5, and a new one, in which the system needsto find a suffix in which the first word does not needto be a given k, but needs to be different to a givensl1.
This constraint can be expressed by the follow-ing equation:s?h = argmaxsh:sh1 6=sl1Pr(p, sh|x, sl) (6)where sl is the suffix generated in the previous iter-ation, already discarded by the user, and sl1 is thefirst word in sl.
k is omitted in this formula becausethe user did not type any word at all.4.2 Interaction-explicit MAsIf the system is efficient and provides suggestionswhich are good enough, one could easily picture asituation in which the expert would ask the systemto replace a given suffix, without typing in any word.We will be modelling this as another kind of MA,interaction-explicit MA, since the user needs to in-dicate explicitly that he wants a given suffix to bereplaced, in contrast to the non-explicit positioningMA.
However, if the underlying MT engine provid-ing the suffixes is powerful enough, the user wouldquickly realise that performing a MA is less costlythat introducing a whole new word, and would takeadvantage of this fact by systematically clicking be-fore introducing any new word.
In this case, aswell, we assume that the user clicks before an in-correct word, hence demanding a new suffix whosefirst word is different, but by doing so he is adoptinga more participative and interactive attitude, whichwas not demanded in the case of non-explicit posi-tioning MAs.
An example of such an explicit MAcorrecting an error can be seen in Figure 3In this case, however, there is a cost associated tothis kind of MAs, since the user does need to per-form additional actions, which may or may not bebeneficial.
It is very possible that, even after askingfor several new hypothesis, the user will even thoughneed to introduce the word he had in mind, hencewasting the additional MAs he had performed.If we allow the user to perform n MAs before in-troducing a word, this problem can be formalised inan analogous way as in the case of non-explicit MAsas follows:s?h= argmaxsh:sh1 6=sil1?i?
{1..n}Pr(p, sh|x, s1l , s2l , .
.
.
, snl ) (7)where sil1 is the first word of the i-th suffix dis-carded and s1l , s2l , .
.
.
, snl is the set of all n suffixesdiscarded.Note that this kind of MA could also be imple-mented with some other kind of interface, e.g.
bytyping some special key such as F1 or Tab.
How-ever, the experimental results would not differ, andin our user interface we found it more intuitive toimplement it as a MA.5 Experimental setup5.1 System evaluationAutomatic evaluation of results is a difficult problemin MT.
In fact, it has evolved to a research field withown identity.
This is due to the fact that, given aninput sentence, a large amount of correct and differ-ent output sentences may exist.
Hence, there is nosentence which can be considered ground truth, as isthe case in speech or text recognition.
By extension,this problem is also applicable to IMT.In this paper, we will be reporting our results asmeasured by Word Stroke Ratio (WSR) (Barrachina489SOURCE (x): Seleccione el tipo de instalacio?n.REFERENCE (y): Select the type of installation.ITER-0 (p) ( )(s?h) Select the installation wizard.ITER-1(p) Select the(sl) |installation wizard.
(s?h) install script.ITER-2(p) Select the(k) type(s?h) installation wizard.ITER-3(p) Select the type(sl) |installation wizard.
(s?h) of installation.ITER-4(p) Select the type of installation.
(sl) ( )(k) (#)(s?h) ( )FINAL (p ?
y) Select the type of installation.Figure 3: Example of explicit interactive MA which corrects an erroneous suffix.
In this case, a non-explicit MA isperformed in ITER-1 with no success.
Hence, the user introduces word ?type?
in ITER-2, which leaves the cursorposition located immediately after word ?type?.
In this situation the user would not need to perform a MA to re-position the cursor and continue typing in order to further correct the remaining errors.
However, since he has learntthe potential benefit of MAs, he performs an interaction-explicit MA in order to ask for a new suffix hypothesis, whichhappens to correct the error.and others, 2008), which is computed as the quotientbetween the number of word-strokes a user wouldneed to perform in order to achieve the translationhe has in mind and the total number of words inthe sentence.
In this context, a word-stroke is in-terpreted as a single action, in which the user typesa complete word, and is assumed to have constantcost.
Moreover, each word-stroke also takes into ac-count the cost incurred by the user when reading thenew suffix provided by the system.In the present work, we decided to use WSR in-stead of Key Stroke Ratio (KSR), which is used inother works on IMT such as (Och et al, 2003).
Thereason for this is that KSR is clearly an optimisticmeasure, since in such a scenario the user is oftenoverwhelmed by receiving a great amount of trans-lation options, as much as one per key stroke, andit is not taken into account the time the user wouldneed to read all those hypotheses.In addition, and because we are also introducingMAs as a new action, we will also present results interms of Mouse Action Ratio (MAR), which is thequotient between the amount of explicit MAs per-formed and the number of words of the final trans-lation.
Hence, the purpose is to elicit the number oftimes the user needed to request a new translation(i.e.
performed a MA), on a per word basis.Lastly, we will also present results in terms ofuMAR (useful MAR), which indicates the amountof MAs which were useful, i.e.
the MAs that actu-ally produced a change in the first word of the suffixand such word was accepted.
Formally, uMAR isdefined as follows:uMAR = MAC ?
n ?WSCMAC (8)where MAC stands for ?Mouse Action Count?,WSC for ?Word Stroke Count?
and n is the max-imum amount of MAs allowed before the user typesin a word.
Note that MAC?n ?WSC is the amountof MAs that were useful since WSC is the amountof word-strokes the user performed even though hehad already performed n MAs.Since we will only use single-reference WSR andMAR, the results presented here are clearly pes-simistic.
In fact, it is relatively common to have theunderlying SMT system provide a perfectly correct490Table 1: Characteristics of Europarl for each of the sub-corpora.
OoV stands for ?Out of Vocabulary?
words,Dev.
for Development, K for thousands of elements andM for millions of elements.De En Es En Fr EnTraining Sentences 751K 731K 688KRun.
words 15.3M16.1M 15.7M15.2M 15.6M13.8MAvg.
len.
20.3 21.4 21.5 20.8 22.7 20.1Voc.
195K 66K 103K 64K 80K 62KDev.Sentences 2000 2000 2000Run.
words 55K 59K 61K 59K 67K 59KAvg.
len.
27.6 29.3 30.3 29.3 33.6 29.3OoV 432 125 208 127 144 138TestSentences 2000 2000 2000Run.
words 54K 58K 60K 58K 66K 58KAvg.
len.
27.1 29.0 30.2 29.0 33.1 29.3OoV 377 127 207 125 139 133translation, which is ?corrected?
by the IMT proce-dure into another equivalent translation, increasingWSR and MAR significantly by doing so.5.2 CorporaOur experiments were carried out on the Eu-roparl (Koehn, 2005) corpus, which is a corpuswidely used in SMT and that has been used in sev-eral MT evaluation campaigns.
Moreover, we per-formed our experiments on the partition establishedfor the Workshop on Statistical Machine Translationof the NAACL 2006 (Koehn and Monz, 2006).
TheEuroparl corpus (Koehn, 2005) is built from the pro-ceedings of the European Parliament.
Here, we willfocus on the German?English, Spanish?English andFrench?English tasks, since these were the languagepairs selected for the cited workshop.
The corpus isdivided into three separate sets: one for training, onefor development, and one for test.
The characteris-tics of the corpus can be seen in Table 1.5.3 Experimental resultsAs a first step, we built a SMT system for each ofthe language pairs cited in the previous subsection.This was done by means of the Moses toolkit (Koehnand others, 2007), which is a complete system forbuilding Phrase-Based SMT models.
This toolkit in-volves the estimation from the training set of fourdifferent translation models, which are in turn com-Table 2: WSR improvement when considering non-explicit MAs.
?rel.?
indicates the relative improvement.All results are given in %.pair baseline non-explicit rel.Es?En 63.0?0.9 59.2?0.9 6.0?1.4En?Es 63.8?0.9 60.5?1.0 5.2?1.6De?En 71.6?0.8 69.0?0.9 3.6?1.3En?De 75.9?0.8 73.5?0.9 3.2?1.2Fr?En 62.9?0.9 59.2?1.0 5.9?1.6En?Fr 63.4?0.9 60.0?0.9 5.4?1.4bined in a log-linear fashion by adjusting a weightfor each of them by means of the MERT (Och, 2003)procedure, optimising the BLEU (Papineni et al,2002) score obtained on the development partition.This being done, word graphs were generatedfor the IMT system.
For this purpose, we used amulti-stack phrase-based decoder which will be dis-tributed in the near future together with the Thottoolkit (Ortiz-Mart?
?nez et al, 2005).
We discardedthe use of the Moses decoder because preliminaryexperiments performed with it revealed that the de-coder by (Ortiz-Mart?
?nez et al, 2005) performsclearly better when used to generate word graphsfor use in IMT.
In addition, we performed an ex-perimental comparison in regular SMT with the Eu-roparl corpus, and found that the performance dif-ference was negligible.
The decoder was set toonly consider monotonic translation, since in realIMT scenarios considering non-monotonic transla-tion leads to excessive waiting time for the user.Finally, the word graphs obtained were usedwithin the IMT procedure to produce the referencetranslation contained in the test set, measuring WSRand MAR.
The results of such a setup can be seen inTable 2.
As a baseline system, we report the tradi-tional IMT framework, in which no MA is taken intoaccount.
Then, we introduced non-explicit MAs, ob-taining an average improvement in WSR of about3.2% (4.9% relative).
The table also shows theconfidence intervals at a confidence level of 95%.These intervals were computed following the boot-strap technique described in (Koehn, 2004).
Sincethe confidence intervals do not overlap, it can bestated that the improvements obtained are statisti-cally significant.491404550556065700  1  2  3  4  550100150200250300WSRMARmax.
MAs per incorrect wordSpanish -> EnglishWSRMAR404550556065700  1  2  3  4  54681012WSRuMARmax.
MAs per incorrect wordSpanish -> EnglishWSRuMAR404550556065700  1  2  3  4  550100150200250300WSRMARmax.
MAs per incorrect wordGerman -> EnglishWSRMAR404550556065700  1  2  3  4  54681012WSRuMARmax.
MAs per incorrect wordGerman -> EnglishWSRuMAR404550556065700  1  2  3  4  550100150200250300WSRMARmax.
MAs per incorrect wordFrench -> EnglishWSRMAR404550556065700  1  2  3  4  54681012WSRuMARmax.
MAs per incorrect wordFrench -> EnglishWSRuMARFigure 4: WSR improvement when considering one to five maximum MAs.
All figures are given in %.
The leftcolumn lists WSR improvement versus MAR degradation, and the right column lists WSR improvement versus uMAR.Confidence intervals at 95% confidence level following (Koehn, 2004).Once the non-explicit MAs were considered andintroduced into the system, we analysed the effectof performing up to a maximum of 5 explicit MAs.Here, we modelled the user in such a way that, incase a given word is considered incorrect, he willalways ask for another translation hypothesis untilhe has asked for as many different suffixes as MAsconsidered.
The results of this setup can be seen inFigure 4.
This yielded a further average improve-ment in WSR of about 16% (25% relative improve-ment) when considering a maximum of 5 explicitMAs.
However, relative improvement in WSR and492uMAR increase drop significantly when increasingthe maximum allowed amount of explicit MAs from1 to 5.
For this reason, it is difficult to imagine thata user would perform more than two or three MAsbefore actually typing in a new word.
Nevertheless,just by asking twice for a new suffix before typingin the word he has in mind, the user might be savingabout 15% of word-strokes.Although the results in Figure 4 are onlyfor the translation direction ?foreign?
?English,the experiments in the opposite direction (i.e.English??foreign?)
were also performed.
How-ever, the results were very similar to the ones dis-played here.
Because of this, and for clarity pur-poses, we decided to omit them and only display thedirection ?foreign?
?English.6 Conclusions and future workIn this paper, we have considered new input sourcesfor IMT.
By considering Mouse Actions, we haveshown that a significant benefit can be obtained, interms of word-stroke reduction, both when consid-ering only non-explicit MAs and when consideringMAs as a way of offering the user several suffix hy-potheses.
In addition, we have applied these ideason a state-of-the-art SMT baseline, such as phrase-based models.
To achieve this, we have first ob-tained a word graph for each sentence which is to betranslated.
Experiments were carried out on a refer-ence corpus in SMT.Note that there are other systems (Esteban andothers, 2004) that, for a given prefix, provide n-best lists of suffixes.
However, the functionality ofour system is slightly (but fundamentally) different,since the suggestions are demanded to be differentin their first word, which implies that the n-best listis scanned deeper, going directly to those hypothe-ses that may be of interest to the user.
In addition,this can be done ?on demand?, which implies thatthe system?s response is faster and that the user isnot confronted with a large list of hypotheses, whichoften results overwhelming.As future work, we are planning on performing ahuman evaluation that assesses the appropriatenessof the improvements described.AcknowledgementsThis work has been partially supported by the Span-ish MEC under scholarship AP2005-4023 and un-der grants CONSOLIDER Ingenio-2010 CSD2007-00018, and by the EC (FEDER) and the SpanishMEC under grant TIN2006-15694-CO2-01.ReferencesD.
J. Arnold, 2003.
Computers and Translation: A trans-lator?s guide, chapter 8, pages 119?142.S.
Barrachina et al 2008.
Statistical approaches tocomputer-assisted translation.
Computational Lin-guistics, page In press.F.
Casacuberta and E. Vidal.
2007.
Learning finite-statemodels for machine translation.
Machine Learning,66(1):69?91.J.
Esteban et al 2004.
Transtype2 - an innovativecomputer-assisted translation system.
In The Compan-ion Volume to the Proc.
ACL?04, pages 94?97.G.
Foster, P. Langlais, and G. Lapalme.
2002.
User-friendly text prediction for translators.
In Proc.
ofEMNLP?02, pages 148?155.G.
Foster.
2002.
Text Prediction for Translators.
Ph.D.thesis, Universite?
de Montre?al.J.
Hutchings and H. Somers.
1992.
An introduction tomachine translation.
In Ed.
Academic Press.J.
Hutchins.
1999.
Retrospect and prospect in computer-based translation.
In Proc.
of MT Summit VII, pages30?44.M.
Kay.
1997.
It?s still the proper place.
Machine Trans-lation, 12(1-2):35?38.P.
Koehn and C. Monz, editors.
2006.
Proc.
of the Work-shop on SMT.P.
Koehn et al 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of the ACL?07.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
HLT/NAACL?03,pages 48?54.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proc.
of EMNLP?04, pages388?395, Barcelona, Spain.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In Proc.
of the MT Summit X,pages 79?86.P.
Langlais, G. Lapalme, and M. Loranger.
2002.Transtype: Development-evaluation cycles to boosttranslator?s productivity.
Machine Translation,15(4):77?98.Bruce T. Lowerre.
1976.
The harpy speech recogni-tion system.
Ph.D. thesis, Carnegie Mellon University,Pittsburgh, PA, USA.493F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of the ACL?02, pages 295?302.F.J.
Och and H. Ney.
2004.
The alignment template ap-proach to statistical machine translation.
Comput.
Lin-guist., 30(4):417?449.F.
Och, C. Tillmann, and H. Ney.
1999.
Improved align-ment models for statistical machine translation.
InProc.
of EMNLP/WVLC?99, pages 20?28.F.J.
Och, R. Zens, and H. Ney.
2003.
Efficient search forinteractive statistical machine translation.
In Proc.
ofEACL?03, pages 387?393.F.J.
Och.
2003.
Minimum error rate training for statis-tical machine translation.
In Proc.
of ACL?03, pages160?167.D.
Ortiz-Mart?
?nez, I.
Garc?
?a-Varea, and F. Casacuberta.2005.
Thot: a toolkit to train phrase-based statisti-cal translation models.
In Proc.
of the MT Summit X,pages 141?148.K.
Papineni, S. Roukos, and T. Ward.
1998.
Maximumlikelihood and discriminative training of direct transla-tion models.
In Proc.
of ICASSP?98, pages 189?192.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.Bleu: A method for automatic evaluation of machinetranslation.
In Proc.
of ACL?02.N.
Ueffing, F. Och, and H. Ney.
2002.
Generation ofword graphs in statistical machine translation.
In Proc.of EMNLP?02, pages 156?163.E.
Vidal et al 2007.
Interactive pattern recognition.
InProc.
of MLMI?07, pages 60?71.R.
Zens, F.J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Proc.
of KI?02, pages18?32.494
