Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575?584,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSentence and Expression Level Annotation of Opinions in User-GeneratedDiscourseCigdem Toprak and Niklas Jakob and Iryna GurevychUbiquitous Knowledge Processing LabComputer Science Department, Technische Universita?t Darmstadt, Hochschulstra?e 10D-64289 Darmstadt, Germanywww.ukp.tu-darmstadt.deAbstractIn this paper, we introduce a corpus ofconsumer reviews from the rateitall andthe eopinions websites annotated withopinion-related information.
We presenta two-level annotation scheme.
In thefirst stage, the reviews are analyzed atthe sentence level for (i) relevancy to agiven topic, and (ii) expressing an eval-uation about the topic.
In the secondstage, on-topic sentences containing eval-uations about the topic are further investi-gated at the expression level for pinpoint-ing the properties (semantic orientation,intensity), and the functional componentsof the evaluations (opinion terms, targetsand holders).
We discuss the annotationscheme, the inter-annotator agreement fordifferent subtasks and our observations.1 IntroductionThere has been a huge interest in the automaticidentification and extraction of opinions from freetext in recent years.
Opinion mining spans a va-riety of subtasks including: creating opinion wordlexicons (Esuli and Sebastiani, 2006; Ding et al,2008), identifying opinion expressions (Riloff andWiebe, 2003; Fahrni and Klenner, 2008), identi-fying polarities of opinions in context (Breck etal., 2007; Wilson et al, 2005), extracting opiniontargets (Hu and Liu, 2004; Zhuang et al, 2006;Cheng and Xu, 2008) and opinion holders (Kimand Hovy, 2006; Choi et al, 2005).Data-driven approaches for extracting opinionexpressions, their holders and targets require re-liably annotated data at the expression level.
Inprevious research, expression level annotation ofopinions was extensively investigated on newspa-per articles (Wiebe et al, 2005; Wilson and Wiebe,2005; Wilson, 2008b) and on meeting dialogs (So-masundaran et al, 2008; Wilson, 2008a).Compared to the newspaper and meeting dialoggenres, little corpus-based work has been carriedout for interpreting the opinions and evaluations inuser-generated discourse.
Due to the high popular-ity of Web 2.0 communities1, the amount of user-generated discourse and the interest in the analysisof such discourse has increased over the last years.To the best of our knowledge, there are two cor-pora of user-generated discourse which are anno-tated for opinion related information at the expres-sion level: The corpus of Hu & Liu (2004) consistsof customer reviews about consumer electronics,and the corpus of Zhuang et al (2006) consists ofmovie reviews.
Both corpora are tailored for ap-plication specific needs, therefore, do not containcertain related information explicitly annotated inthe discourse, which we consider important (seeSection 2).
Furthermore, none of these works pro-vide inter-annotator agreement studies.Our goal is to create sentence and expressionlevel annotated corpus of customer reviews whichfulfills the following requirements: (1) It filtersindividual sentences regarding their topic rele-vancy and the existence of an opinion or factualinformation which implies an evaluation.
(2) Itidentifies opinion expressions including the re-spective opinion target, opinion holder, modi-fiers, and anaphoric expressions if applicable.
(3)The semantic orientation of the opinion expres-sion is identified while considering negation, andthe opinion expression is linked to the respectiveholder and target in the discourse.
Such a re-source would (i) enable novel applications of opin-ion mining such as a fine-grained identification ofopinion properties, e.g.
opinion modification de-tection including negation, and (ii) enhance opin-ion target extraction and the polarity assignmentby linking the opinion expression with its target1http://blog.nielsen.com/nielsenwire/wp-content/uploads/2008/10/press_release24.pdf575and providing anaphoric resolutions in discourse.We present an annotation scheme which fulfillsthe mentioned requirements, an inter-annotatoragreement study, and discuss our observations.The rest of this paper is structured as follows:Section 2 presents the related work.
In Sections3, we describe the annotation scheme.
Section 4presents the data and the annotation study, whileSection 5 summarizes the main conclusions.2 Previous Opinion Annotated Corpora2.1 Newspaper Articles and Meeting DialogsMost prominent work concerning the expres-sion level annotation of opinions is the Multi-Perspective Question Answering (MPQA) corpus2(Wiebe et al, 2005).
It was extended several timesover the last years, either by adding new docu-ments or annotating new types of opinion relatedinformation (Wilson and Wiebe, 2005; Stoyanovand Cardie, 2008; Wilson, 2008b).
The MPQAannotation scheme builds upon the private statenotion (Quirk et al, 1985) which describes men-tal states including opinions, emotions, specula-tions and beliefs among others.
The annotationscheme strives to represent the private states interms of their functional components (i.e.
expe-riencer holding an attitude towards a target).
Itconsists of frames (direct subjective, expressivesubjective element, objective speech event, agent,attitude, and target frames) with slots represent-ing various attributes and properties (e.g.intensity,nested source) of the private states.Wilson (2008a) adapts and extends the conceptsfrom the MPQA scheme to annotate subjectivecontent in meetings (AMI corpus), and creates theAMIDA scheme.
Besides subjective utterances,the AMIDA scheme contains objective polar ut-terances which annotates evaluations without ex-pressing explicit opinion expressions.Somasundaran et al (2008) proposes opinionframes for representing discourse level associa-tions in meeting dialogs.
The annotation schemefocuses on two types of opinions, sentiment andarguing.
It annotates the opinion expression andtarget spans.
The link and link type attributes asso-ciate the target with other targets in the discoursethrough same or alternative relations.
The opinionframes are built based on the links between tar-gets.
Somasundaran et al (2008) show that opin-ion frames enable a coherent interpretation of the2http://www.cs.pitt.edu/mpqa/opinions in discourse and discover implicit evalu-ations through link transitivity.Similar to Somasundaran et al (2008), Asheret al (2008) performs discourse level analysis ofopinions.
They propose a scheme which first iden-tifies and assigns categories to the opinion seg-ments as reporting, judgment, advice, or senti-ment; and then links the opinion segments witheach other via rhetorical relations including con-trast, correction, support, result, or continuation.However, in contrast to our scheme and otherschemes, instead of marking expression bound-aries without any restriction they annotate an opin-ion segment only if it contains an opinion wordfrom their lexicon, or if it has a rhetorical relationto another opinion segment.2.2 User-generated DiscourseThe two annotated corpora of user-generated con-tent and their corresponding annotation schemesare far less complex.
Hu & Liu (2004) presenta dataset of customer reviews for consumer elec-tronics crawled from amazon.com.
The follow-ing example shows two annotations taken from thecorpus of Hu & Liu (2004):camera[+2]##This is my first digital camera and what a toyit is...size[+2][u]##it is small enough to fit easily in a coat pocketor purse.The corpus provides only target and polarity anno-tations, and do not contain opinion expression oropinion modifier annotations which lead to thesepolarity scores.
The annotation scheme allows theannotation of implicit features (indicated with thethe attribute [u] ).
Implicit features are not re-solved to any actual product feature instances indiscourse.
In fact, the actual positions of the prod-uct features (or any anaphoric references to them)are not explicitly marked in the discourse, i.e, it isunclear to which mention of the feature the opin-ion refers to.In their paper on movie review mining and sum-marization, Zhuang et al (2006) introduce an an-notated corpus of movie reviews from the InternetMovie Database.
The corpus is annotated regard-ing movie features and corresponding opinions.The following example shows an annotated sen-tence:?Sentence?I have never encountered a movie whosesupporting cast was so perfectly realized.
?FOFword=?supporting cast?
Ftype=?PAC?
Oword=?perfect?Otype=?PRO?/?
?/Sentence?576The movie features (Fword) are attributed to oneof 20 predefined categories (Ftype).
The opin-ion words (Oword) and their semantic orientations(Otype) are identified.
Possible negations are di-rectly reflected by the semantic orientation, but notexplicitly labeled in the sentence.
(PD) in the fol-lowing example indicates that the movie feature isreferenced by anaphora:?Sentence?It is utter nonsense and insulting to myintelligence and sense of history.
?FO Fword=?film(PD)?Ftype=?OA?
Oword=?nonsense, insulting?Otype=?CON?/?
?/Sentence?However, similar to the corpus of Hu & Liu (2004)the referring pronouns are not explicitly marked indiscourse.
It is therefore neither possible to au-tomatically determine which pronoun creates thelink if there are more than one in a sentence, nor itis denoted which antecedent, i.e.
the actual men-tion of the feature in the discourse it relates to.3 Annotation Scheme3.1 Opinion versus Polar FactsThe goal of the annotation scheme is to capture theevaluations regarding the topics being discussed inthe consumer reviews.
The evaluations in con-sumer reviews are either explicit expressions ofopinions, or facts which imply evaluations as dis-cussed below.Explicit expressions of opinions: Opinions areprivate states (Wiebe et al, 2005; Quirk et al,1985) which are not open to objective observationor verification.
In this study, we focus on the opin-ions stating the quality or value of an entity, ex-perience or a proposition from one?s perspective.
(1) illustrates an example of an explicit expressionof an opinion.
Similar to Wiebe et al (2005), weview opinions in terms of their functional compo-nents, as opinion holders, e.g., the author in (1),holding attitudes (polarity), e.g., negative attitudeindicated with the word nightmare, towards possi-ble targets, e.g., Capella University.
(1) I had a nightmare with Capella University.3Facts implying evaluations: Besides opinions,there are facts which can be objectively verified,but still imply an evaluation of the quality or valueof an entity or a proposition.
For instance, con-sider the snippet below:3We use authentic examples from the corpus without cor-recting grammatical or spelling errors.
(2) In a 6-week class, I counted 3 comments from theprofessors directly to me and two directed to my team.
(3) I found that I spent most of my time learning from myfellow students.
(4) A standard response from my professors would be that ofa sentence fragment.The example above provides an evaluation aboutthe professors without stating any explicit expres-sions of opinions.
We call such objectively verifi-able, but evaluative sentences polar facts.
Explicitexpressions of opinions typically contain specificcues, i.e.
opinion words, loaded with a positive ornegative connotation (e.g., nightmare).
Even whenthey are taken out of the context in which they ap-pear, they evoke an evaluation.
However, evalu-ations in polar facts can only be inferred withinthe context of the review.
For instance, the targetsof the implied evalution in the polar facts (2), (3)and (4) are the professors.
However, (3) may havebeen perceived as a positive statement if the re-view was explaining how good the fellow studentswere or how the course enforced team work etc.The annotation scheme consists of two levels.First, the sentence level scheme analyses each sen-tence in terms of (i) its relevancy to the overalltopic of the review, and (ii) whether it containsan evaluation (an opinion or a polar fact) aboutthe topic.
Once the on-topic sentences contain-ing evaluations are identified, the expression levelscheme first focuses either on marking the textspans of the opinion expressions (if the sentencecontains an explicit expression of an opinion) ormarking the targets of the polar facts (if the sen-tence is a polar fact).
Upon marking an opin-ion expression span, the target and holder of theopinion is marked and linked to the marked opin-ion expression.
Furthermore, the expression levelscheme allows assigning polarities to the markedopinion expression spans and targets of the polarfacts.The following subsections introduce the sen-tence and the expression level annotation schemesin detail with examples.3.2 Sentence Level AnnotationThe sentence annotation strives to identify the sen-tences containing evaluations about the topic.
Inconsumer reviews people occasionally drift off theactual topic being reviewed.
For instance, as in(5) taken from a review about an online university,they tend to provide information about their back-ground or other experiences.
(5) I am very fortunate and almost right out of high school577Figure 1: The sentence level annotation schemewith a very average GPA and only 20; I already make above$45,000 a year as a programmer with a large health carecompany for over a year and have had 3 promotions up inthe first year and a half.Such sentences do not provide information aboutthe actual topic, but typically serve for justifyingthe user?s point of view or provide a better under-standing about her circumstances.
However, theyare not valuable for an application aiming to ex-tract opinions about a specific topic.Reviews given to the annotators contain metainformation stating the topic, for instance, thename of the university or the service being re-viewed.
A markable (i.e.
an annotation unit) iscreated for each sentence prior to the annotationprocess.
At this level, the annotation process istherefore a sentence labeling task.
The annotatorsare able to see the whole review, and instructed tolabel sentences in the context of the whole review.Figure 1 presents the sentence level scheme.
At-tribute names are marked with oval circles and thepossible values are given in parenthesis.
The fol-lowing attributes are used:topic relevant attribute is labeled as yes if thesentence discusses the given topic itself or its as-pects, properties or features as in examples (1)-(4).
Other possible values for this attribute includenone given which can be chosen in the absence ofmeta data, or no if the sentence drifted off the topicas in example (5).opinionated attribute is labeled as yes if thesentence contains any explicit expressions of opin-ions about the given topic.
This attribute is pre-sented if the topic relevant attribute has been la-beled as none given or yes.
In other words, onlythe on-topic sentences are considered in this step.Examples (6)-(8) illustrate examples labeled astopic relevant=yes and opinionated=yes.
(6) Many people are knocking Devry but I have seen them tobe a very great school.
[Topic: Devry University](7) University of Phoenix was a surprising disappointment.
[Topic: University of Phoenix](8) Assignments were passed down, but when asked toclarify the assignment because the syllabus hadcontradicting, poorly worded, information, my professorsregularly responded....?refer to the syllabus?....but wait, thesyllabus IS the question.
[Topic: University of Phoenix]polar fact attribute is labeled as yes if the sen-tence is a polar fact.
This attribute is presentedif the opinionated attribute has been labeled asno.
Examples (2)-(4) demonstrate sentences la-beled as topic relevant=yes, opinionated=no andpolar fact=yes.polar fact polarity attribute represents the po-larity of the evaluation in a polar fact sentence.The possible values for this attribute include posi-tive, negative, both.
The value both is intended forthe polar fact sentences containing more than oneevaluation with contradicting polarities.
At theexpression level analysis, the targets of the con-tradicting polar fact evaluations are identified dis-tinctly and assigned polarities of positive or neg-ative later on.
Examples (9)-(11) demonstrate ex-amples of polar fact sentences with different val-ues of the attribute polar fact polarity.
(9) There are students in the first programming class andafter taking this class twice they cannot write a single line ofcode.
[polar fact polarity=negative](10) The same class (i.e.
computer class) being teach at IvyLeague schools are being offered at Devry.
[polar fact polarity=positive](11) The lectures are interactive and recorded, but you needa consent from the instructor each time.
[polar fact polarity=both]3.3 Expression Level AnnotationAt the expression level, we focus on the topicrelevant sentences containing evaluations, i.e.,sentences labeled as topic relevant=yes, opinion-ated=yes or topic relevant=yes, opinionated=no,polar fact=yes.
If the sentence is a polar fact, thenthe aim is to mark the target and label the polarityof the evaluation.
If the sentence is opinionated,then, the aim is to mark the opinion expressionspan, and label its polarity and strength (i.e.
in-tensity), and to link it to the target and the holder.Figure 2 presents the expression level scheme.At this stage, annotators mark text spans, and areallowed to assign one of the five labels to themarked span:The polar target is used to label the targets ofthe evaluations implied by polar facts.
The is-Reference attribute labels polar targets which areanaphoric references.
The polar target polarity578Figure 2: The expression level annotation schemeattribute is used to label the polarity as positiveor negative.
If the isReference attribute is labeledas true, then the referent attribute appears whichenables the annotator to resolve the reference toits antecedent.
Consider the example sentences(12) and (13) below.
The polar target in (13),written bold, is labeled as isReference=true, po-lar target polarity=negative.
To resolve the ref-erence, annotator first creates another polar targetmarkable for the antecedent, namely the bold textspan in (12), then, links the antecedent to the ref-erent attribute of the polar target in (13).
(12) Since classes already started, CTU told me they wouldextend me so that I could complete the classes and get creditonce I got back.
(13) What they didn?t tell me is in order to extend, I also hadto be enrolled in the next semester.The target annotation represents what the opin-ion is about.
Both polar targets and targets can bethe topic of the review or different aspects, i.e.
fea-tures of the topic.
Similar to the polar targets, theisReference attribute allows the identification ofthe targets which are anaphoric references and thereferent attribute links them to their antecedents inthe discourse.
Bold span in (14) shows an exampleof a target in an opinionated sentence.
(14) Capella U has incredible faculty in the Harold AbelSchool of Psychology.The holder type represents the holder of anopinion in the discourse and is labeled in the samemanner as the targets and polar targets.
In con-sumer reviews, holders are most of the time theauthors of the reviews.
To ease the annotation pro-cess, the holder is not labeled when this is the au-thor.The modifier annotation labels the lexical items,such as not, very, hardly etc., which affect thestrength of an opinion or shift its polarity.
Uponcreation of a modifier markable, annotators areasked to choose between negation, increase, de-crease for identifying the influence of the modifieron the opinion.
For instance, the marked span in(15) is labeled as modifier=increase as it gives theimpression that the author is really offended by thenegative comments about her university.
(15) I am quite honestly appauled by some of the negativecomments given for Capella University on this website.The opinionexpression annotation is used to la-bel the opinion terms in the sentence.
This mark-able type has five attributes, three of which, i.e.,modifier, holder, and target are pointer attributesto the previously defined markable types.
The po-larity attribute assesses the semantic orientation ofthe attitude, where the strength attribute marks theintensity of this attitude.
The polarity and strengthattributes focus solely on the marked opinionex-pression span, not the whole evaluation impliedin the sentence.
For instance, the opinionexpres-sion span in (16) is labeled as polarity=negative,strength=average.
We infer the polarity of theevaluation only after considering the modifier, po-larity and the strength attributes together.
In (16),the evaluation about the target is strongly negativeafter considering all three attributes of the opinion-expression annotation.
In (17), the polarity of theopinionexpression1 itself (complaints) is labeledas negative.
It is linked to the modifier1 whichis labeled as negation.
Target1 (PhD journey) islinked to the opinionexpression1.
The overall eval-uation regarding the target1 is positive after ap-plying the affect of the modifier1 to the polarityof the opinionexpression1, i.e., after negating thenegative polarity.
(16) I am quite honestly[modifier] appauledby[opinionexpression] some of the negative commentsgiven for Capella University on this website[target].
(17) I have no[modifier1]complaints[opinionexpression1] about the entire PhDjourney[target1] and highly[modifier2]recommend[opinionexpression2] this school[target2].Finally, Figure 3 demonstrates all expressionlevel markables created for an opinionated sen-tence and how they relate to each other.579Figure 3: Expression level annotation example4 Annotation StudyEach review has been annotated by two annotatorsindependently according to the annotation schemeintroduced above.
We used the freely availableMMAX24 annotation tool capable of stand-offmulti-level annotations.
Annotators were nativespeaker linguistic students.
They were trained on15 reviews after reading the annotation manual.5In the training stage, the annotators discussed witheach other if different decisions have been madeand were allowed to ask questions to clarify theirunderstanding of the scheme.
Annotators had ac-cess to the review text as a whole while makingtheir decisions.4.1 DataThe corpus consists of consumer reviews col-lected from the review portals rateitall6 and eopin-ions7.
It contains reviews from two domains in-cluding online universities, e.g., Capella Univer-sity, Pheonix, University of Maryland UniversityCollege etc.
and online services, e.g., PayPal,egroups, eTrade, eCircles etc.
These two domainswere selected with the project-relevant, domain-specific research goals in mind.
We selected a spe-cific topic, e.g.
Pheonix, if there were more than 3reviews written about it.
Table 1 shows descriptivestatistics regarding the data.We used 118 reviews containing 1151 sentencesfrom the university domain for measuring the sen-tence and expression level agreements.
In the fol-lowing subsections, we report the inter-annotatoragreement (IAA) at each level.4http://mmax2.sourceforge.net/5http://www.ukp.tu-darmstadt.de/research/data/sentiment-analysis6http://www.rateitall.com7http://www.epinions.comUniversity Service AllReviews 240 234 474Sentences 2786 6091 8877Words 49624 102676 152300Avg sent./rev.
11.6 26 18.7Std.
dev.
sent./rev.
8.2 16 14.6Avg.
words/rev.
206.7 438.7 321.3Std.
dev.
words/rev.
159.2 232.1 229.8Table 1: Descriptive statistics about the corpus4.2 Sentence Level AgreementSentence level markables were already created au-tomatically prior to the annotation, i.e., the set ofannotation units were the same for both annota-tors.
We use Cohen?s kappa (?)
(Cohen, 1960)for measuring the IAA.
The sentence level anno-tation scheme has a hierarchical structure.
A newattribute is presented based on the decision madefor the previous attribute, for instance, opinionatedattribute is only presented if the topic relevant at-tribute is labeled as yes or none given; polar factattribute is only presented if the opinionated at-tribute is labeled as no etc.
We calculate ?
for eachattribute considering only the markables whichwere labeled the same by both annotators in thepreviously required step.
Table 2 shows the ?
val-ues for each attribute, the size of the markable seton which the value was calculated, and the per-centage agreement.Attribute Markables Agr.
?topic relevant 1151 0.89 0.73opinionated 682 0.80 0.61polar fact 258 0.77 0.56polar fact polarity 103 0.96 0.92Table 2: Sentence level inter-annotator agreementThe agreement for topic relevancy shows thatit is possible to label this attribute reliably.
Thesentences labeled as topic relevant by both anno-tators correspond to 59% of all sentences, suggest-ing that people often drift off the topic in consumerreviews.
This is usually the case when they pro-vide information about their backgrounds or alter-natives to the given topic.On the other hand, we obtain moderate agree-ment levels for the opinionated and polar fact at-tributes.
62% of the topic relevant sentences werelabeled as opinionated by at least one annotator,and the rest 38% constitute the topic relevant sen-tences labeled as not opinionated by both anno-tators.
Nonetheless, they still contain evaluations(polar facts), as 15% of the topic relevant sen-580tences were labeled as polar facts by both anno-tators.
When we merge the attributes opinionatedand polar fact into a single category, we obtain ?of 0.75 and a percentage agreement of 87%.
Thus,we conclude that opinion-relevant sentences, ei-ther in the form of an explicit expression of opin-ion or a polar fact, can be labeled reliably in con-sumer reviews.
However, there is a thin border be-tween polar facts and explicit expressions of opin-ions.To the best of our knowledge, similar annotationefforts on consumer or movie reviews do not pro-vide any agreement figures for direct comparison.However, Wiebe et al (2005) present an annota-tion study where they mark textual spans for sub-jective expressions in a newspaper corpus.
Theyreport pairwise ?
values for three annotators rang-ing between 0.72 - 0.84 for the sentence level sub-jective/objective judgments.
Wiebe et al (2005)mark subjective spans, and do not explicitly per-form the sentence level labeling task.
They calcu-late the sentence level ?
values based on the ex-istence of a subjective expression span in the sen-tence.
Although the task definitions, approachesand the corpora have quite disparate characteris-tics in both studies, we obtain comparable resultswhen we merge opinionated and polar fact cate-gories.4.3 Expression Level AgreementAt the expression level, annotators focus only onthe sentences which were labeled as opinionatedor polar fact by both annotators.
Annotators wereinstructed to mark text spans, and then, assignthem the annotation types such as polar target,opinionexpression etc.
(see Figure 2).
For calcu-lating the text span agreement, we use the agree-ment metric presented by Wiebe et al (2005) andSomasundaran et al (2008).
This metric corre-sponds to the precision (P) and recall (R) metricsin information retrieval where the decisions of oneannotator are treated as the system; the decisionsof the other annotator are treated as the gold stan-dard; and the overlapping spans correspond to thecorrectly retrieved documents.Somasundaran et al (2008) present a discourselevel annotation study in which opinion and tar-get spans are marked and linked with each otherin a meeting transcript corpus.
Following Soma-sundaran et al (2008), we compute three differ-ent measures for the text span agreement: (i) exactmatching in which the text spans should perfectlymatch; (ii) lenient (relaxed) matching in which theoverlap between spans is considered as a match,and (iii) subset matching in which a span has tobe contained in another span in order to be consid-ered as a match.8 Agreement naturally increasesas we relax the matching constraints.
However,there were no differences between the lenient andthe subset agreement values.
Therefore, we reportonly the exact and lenient matching agreement re-sults for each annotation type in Table 3.
Thesame agreement results for the lenient and subsetmatching indicates that inexact matches are stillvery similar to each other, i.e., at least one span istotally contained in the other.Somasundaran et al (2008) do not report anyF-measure.
However, they report span agreementresults in terms of precision and recall rangingbetween 0.44 - 0.87 for opinion spans and be-tween 0.74 - 0.90 for the target spans.
Wiebe etal.
(2005) use the lenient matching approach forreporting text span agreements ranging between0.59 - 0.81 for subjective expressions.
We ob-tain higher agreement values for both opinion ex-pression and target spans.
We attribute this to thefact that the annotators look for opinion expressionand target spans within the opinionated sentenceswhich they agreed upon.
Sentence level analysisindeed increases the reliability at the expressionlevel.
Compared to the high agreement on mark-ing target spans, we obtain lower agreement val-ues on marking polar target spans.
We observethat it is easier to attribute explicit expressions ofevaluations to topic relevant entities compared toattributing evaluations implied by experiences tospecific topic relevant entities in the reviews.We calculated the agreement on identifyinganaphoric references using the method introducedin (Passonneau, 2004) which utilizes Krippen-dorf?s ?
(Krippendorff, 2004) for computing reli-ability for coreference annotation.
We consideredthe overlapping target and polar target spans to-gether in this calculation, and obtained an ?
valueof 0.29.
Compared to Passonneau (?
values from0.46 to 0.74), we obtain a much lower agreementvalue.
This may be due to the different definitionsand organizations of the annotation tasks.
Passon-neau requires prior marking of all noun phrases (orinstances which needs to be processed by the an-8An example of subset matching: waste of time vs. totalwaste of time581SpanExact LenientP R F P R Fopinionexpression 0.70 0.80 0.75 0.82 0.93 0.87modifier 0.80 0.82 0.81 0.86 0.86 0.86target 0.80 0.81 0.80 0.91 0.90 0.91holder 0.75 0.72 0.73 0.93 0.88 0.91polar target 0.67 0.42 0.51 0.75 0.49 0.59Table 3: Inter-annotator agreement on text spans at the expression levelnotator).
Annotator?s task is to identify whetheran instance refers to another marked entity in thediscourse, and then, to identify corefering entitychains.
However, in our annotation process anno-tators were tasked to identify only one entity as thereferent, and was free to choose it from anywherein the discourse.
In other words, our chains con-tain only one entity.
It is possible that both annota-tors performed correct resolutions, but still did notoverlap with each other, as they resolve to differ-ent instances of the same entity in the discourse.We plan to further investigate reference resolutionannotation discrepancies and perform correctionsin the future.Some annotation types require additional at-tributes to be labeled after marking the span.For instance, upon marking a text span as a po-lar target or an opinionexpression, one has to la-bel the polarity and strength.
We consider theoverlapping spans for each annotation type anduse ?
for reporting the agreement on these at-tributes.
Table 4 shows the ?
values.Attribute Markables Agr.
?polarity 329 0.97 0.94strength 329 0.74 0.55modifier 136 0.88 0.77polar target polarity 63 0.80 0.67Table 4: Inter-annotator agreement at the expres-sion levelWe observe that the strength of the opinionex-pression and the polar target polarity cannot belabeled as reliably as the polarity of the opinion-expression.
61% of the agreed upon polar targetswere labeled as negative by both annotators.
Onthe other hand, only 35% of the agreed upon opin-ionexpressions were labeled as negative by bothannotators.
There were no neutral instances.
Thisindicates that reviewers tend to report negative ex-periences using polar facts, probably objectivelydescribing what has happened, but report posi-tive experiences with explicit opinion expressions.Distribution of the strength attribute was as fol-lows: weak 6%, average 54%, and strong 40%.The majority of the modifiers were annotated asintensifiers (70%), while 20% of the modifierswere labeled as negation.4.4 DiscussionWe analyzed the discrepancies in the annotationsto gain insights about the challenges involved invarious opinion related labeling tasks.
At the sen-tence level, there were several trivial cases of dis-agreement, for instance, failing to recognize topicrelevancy when the topic was not mentioned orreferenced explicitly in the sentence, as in (18).Occasionally, annotators disagreed about whethera sentence that was written as a reaction to theother reviewers, as in (19), should be consideredas topic relevant or not.
Another source of dis-agreement included sentences similar to (20) and(21).
One annotator interpreted them as univer-sally true statements regardless of the topic, whilethe other attributed them to the discussed topic.
(18) Go to a state university if you know whats good for you!
(19) Those with sour grapes couldnt cut it, have an ax togrind, and are devoting their time to smearing the school.
(20) As far as learning, you really have to WANT to learnthe material.
(21) On an aside, this type of education is not for theundisciplined learner.Annotators easily distinguished the evaluationsat the sentence level.
However, they had diffi-culties distinguishing between a polar fact and anopinion.
For instance, both annotators agreed thatthe sentences (22) and (23) contain evaluations re-garding the topic of the review.
However, one an-notator interpreted both sentences as objectivelyverifiable facts giving a positive impression aboutthe school, while the other one treated them asopinions.
(22) All this work in the first 2 Years!
(23) The school has a reputation for making students workreally hard.Sentence level annotation increases the relia-bility of the expression level annotation in termsof marking text spans.
However, annotators of-ten had disagreements on labeling the strength at-tribute.
For instance, one annotator labeled the582opinion expression in (24) as strong, while theother one labeled it as average.
We observe thatit is not easy to identify trivial causes of disagree-ments regarding strength as its perception by eachindividual is highly subjective.
However, most ofthe disagreements occurred between weak and av-erage cases.
(24) the experience that i have when i visit student finance ismuch like going to the dentist, except when i leave, nothingis ever fixed.We did not apply any consolidation steps duringour agreement studies.
However, a final version ofthe corpus will be produced by the third judge (oneof the co-authors) by consolidating the judgementsof the two annotators.5 ConclusionsWe presented a corpus of consumer reviews fromthe rateitall and eopinions websites annotatedwith opinion related information.
Existing opin-ion annotated user-generated corpora suffer fromseveral limitations which result in difficulties forinterpreting the experimental results and for per-forming error analysis.
To name a few, they donot explicitly link the functional components ofthe opinions like targets, holders, or modifiers withthe opinion expression; some of them do not markopinion expression spans, none of them resolvesanaphoric references in discourse.
Therefore, weintroduced a two level annotation scheme consist-ing of the sentence and expression levels, whichovercomes the limitations of the existing reviewcorpora.
The sentence level annotation labels sen-tences for (i) relevancy to a given topic, and (ii)expressing an evaluation about the topic.
Similarto (Wilson, 2008a), our annotation scheme allowscapturing evaluations made with factual (objec-tive) sentences.
The expression level annotationfurther investigates on-topic sentences containingevaluations for pinpointing the properties (polar-ity, strength), and marking the functional com-ponents of the evaluations (opinion terms, modi-fiers, targets and holders), and linking them withina discourse.
We applied the annotation schemeto the consumer review genre and presented anextensive inter-annotator study providing insightsto the challenges involved in various opinion re-lated labeling tasks in consumer reviews.
Simi-lar to the MPQA scheme, which is successfullyapplied to the newspaper genre, the annotationscheme treats opinions and evaluations as a com-position of functional components and it is eas-ily extendable.
Therefore, we hypothesize that thescheme can also be applied to other genres withminor extensions or as it is.
Finally, the corpusand the annotation manual will be made availableat http://www.ukp.tu-darmstadt.de/research/data/sentiment-analysis.AcknowledgementsThis research was funded partially by the German Fed-eral Ministry of Economy and Technology under grant01MQ07012 and partially by the German Research Founda-tion (DFG) as part of the Research Training Group on Feed-back Based Quality Management in eLearning under grant1223.
We are very grateful to Sandra Ku?bler for her help inorganizing the annotators, and to Lizhen Qu for his program-ming support in harvesting the data.ReferencesNicholas Asher, Farah Benamara, and Yvette YannickMathieu.
2008.
Distilling opinion in discourse: Apreliminary study.
In Coling 2008: Companion vol-ume: Posters, pages 7?10, Manchester, UK.Eric Breck, Yejin Choi, and Claire Cardie.
2007.Identifying expressions of opinion in context.
InProceedings of the Twentieth International JointConference on Artificial Intelligence (IJCAI-2007),pages 2683?2688, Hyderabad, India.Xiwen Cheng and Feiyu Xu.
2008.
Fine-grained opin-ion topic and polarity identification.
In Proceedingsof the 6th International Conference on LanguageResources and Evaluation, pages 2710?2714, Mar-rekech, Morocco.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opin-ions with conditional random fields and extractionpatterns.
In HLT ?05: Proceedings of the confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages355?362, Morristown, NJ, USA.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.In Proceedings of the International Conference onWeb Search and Web Data Mining, WSDM 2008,pages 231?240, Palo Alto, California, USA.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Eval-uation, pages 417?422, Genova, Italy.583Angela Fahrni and Manfred Klenner.
2008.
Old wineor warm beer: Target-specific sentiment analysis ofadjectives.
In Proceedings of the Symposium onAffective Language in Human and Machine, AISB2008 Convention, pages 60 ?
63, Aberdeen, Scot-land.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In KDD?04: Proceed-ings of the Tenth ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 168?177, Seattle, Washington.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In Proceedings of the Work-shop on Sentiment and Subjectivity in Text at thejoint COLING-ACL Conference, pages 1?8, Sydney,Australia.Klaus Krippendorff.
2004.
Content Analysis: AnIntroduction to Its Methology.
Sage Publications,Thousand Oaks, California.Rebecca J. Passonneau.
2004.
Computing reliabilityfor coreference.
In Proceedings of LREC, volume 4,pages 1503?1506, Lisbon.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Gram-mar of the English Language.
Longman, New York.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In EMNLP-03: Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages105?112.Swapna Somasundaran, Josef Ruppenhofer, and JanyceWiebe.
2008.
Discourse level opinion relations:An annotation study.
In In Proceedings of SIGdialWorkshop on Discourse and Dialogue, pages 129?137, Columbus, Ohio.Veselin Stoyanov and Claire Cardie.
2008.
Topicidentification for fine-grained opinion analysis.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages817?824, Manchester, UK.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39:165?210.Theresa Wilson and Janyce Wiebe.
2005.
Annotat-ing attributions and private states.
In Proceedings ofthe Workshop on Frontiers in Corpus Annotations II:Pie in the Sky, pages 53?60, Ann Arbor, Michigan.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT ?05: Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 347?354, Vancouver, BritishColumbia, Canada.Theresa Wilson.
2008a.
Annotating subjective con-tent in meetings.
In Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08), Marrakech, Morocco.Theresa Ann Wilson.
2008b.
Fine-grained Subjectiv-ity and Sentiment Analysis: Recognizing the Inten-sity, Polarity, and Attitudes of Private States.
Ph.D.thesis, University of Pittsburgh.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In CIKM?06: Proceedings of the 15th ACM internationalconference on Information and knowledge manage-ment, pages 43?50, Arlington, Virginia, USA.584
