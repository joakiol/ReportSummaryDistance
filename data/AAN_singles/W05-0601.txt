Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 1?8, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsEffective use of WordNet semantics via kernel-based learningRoberto Basili and Marco Cammisa and Alessandro MoschittiDepartment of Computer ScienceUniversity of Rome ?Tor Vergata?, Rome, Italy{basili,cammisa,moschitti}@info.uniroma2.itAbstractResearch on document similarity hasshown that complex representations arenot more accurate than the simple bag-of-words.
Term clustering, e.g.
using latentsemantic indexing, word co-occurrencesor synonym relations using a word ontol-ogy have been shown not very effective.In particular, when to extend the similar-ity function external prior knowledge isused, e.g.
WordNet, the retrieval systemdecreases its performance.
The critical is-sues here are methods and conditions tointegrate such knowledge.In this paper we propose kernel func-tions to add prior knowledge to learn-ing algorithms for document classifica-tion.
Such kernels use a term similaritymeasure based on the WordNet hierarchy.The kernel trick is used to implement suchspace in a balanced and statistically co-herent way.
Cross-validation results showthe benefit of the approach for the SupportVector Machines when few training data isavailable.1 IntroductionThe large literature on term clustering, term sim-ilarity and weighting schemes shows that docu-ment similarity is a central topic in Information Re-trieval (IR).
The research efforts have mostly beendirected in enriching the document representationby using clustering (term generalization) or addingcompounds (term specifications).
These studies arebased on the assumption that the similarity betweentwo documents can be expressed as the similarity be-tween pairs of matching terms.
Following this idea,term clustering methods based on corpus term dis-tributions or on external prior knowledge (e.g.
pro-vided by WordNet) were used to improve the basicterm matching.An example of statistical clustering is given in(Bekkerman et al, 2001).
A feature selection tech-nique, which clusters similar features/words, calledthe Information Bottleneck (IB), was applied to TextCategorization (TC).
Such cluster based representa-tion outperformed the simple bag-of-words on onlyone out of the three experimented collections.
Theeffective use of external prior knowledge is evenmore difficult since no attempt has ever been suc-cessful to improve document retrieval or text classi-fication accuracy, (e.g.
see (Smeaton, 1999; Sussna,1993; Voorhees, 1993; Voorhees, 1994; Moschittiand Basili, 2004)).The main problem of term cluster based represen-tations seems the unclear nature of the relationshipbetween the word and the cluster information lev-els.
Even if (semantic) clusters tend to improve thesystem recall, simple terms are, on a large scale,more accurate (e.g.
(Moschitti and Basili, 2004)).To overcome this problem, hybrid spaces containingterms and clusters were experimented (e.g.
(Scottand Matwin, 1999)) but the results, again, showedthat the mixed statistical distributions of clusters andterms impact either marginally or even negatively onthe overall accuracy.In (Voorhees, 1993; Smeaton, 1999), clusters ofsynonymous terms as defined in WordNet (WN)(Fellbaum, 1998) were used for document retrieval.The results showed that the misleading informationdue to the wrong choice of the local term sensescauses the overall accuracy to decrease.
Word sensedisambiguation (WSD) was thus applied beforehandby indexing the documents by means of disam-biguated senses, i.e.
synset codes (Smeaton, 1999;1Sussna, 1993; Voorhees, 1993; Voorhees, 1994;Moschitti and Basili, 2004).
However, even thestate-of-the-art methods for WSD did not improvethe accuracy because of the inherent noise intro-duced by the disambiguation mistakes.
The abovestudies suggest that term clusters decrease the pre-cision of the system as they force weakly related orunrelated (in case of disambiguation errors) terms togive a contribution in the similarity function.
Thesuccessful introduction of prior external knowledgerelies on the solution of the above problem.In this paper, a model to introduce the semanticlexical knowledge contained in the WN hierarchyin a supervised text classification task has been pro-posed.
Intuitively, the main idea is that the docu-ments d are represented through the set of all pairsin the vocabulary < t, t?
>?
V ?
V originating bythe terms t ?
d and all the words t?
?
V , e.g.
theWN nouns.
When the similarity between two docu-ments is evaluated, their matching pairs are used toaccount for the final score.
The weight given to eachterm pair is proportional to the similarity that the twoterms have in WN.
Thus, the term t of the first docu-ment contributes to the document similarity accord-ing to its relatedness with any of the terms of thesecond document and the prior external knowledge,provided by WN, quantifies the single term to termrelatedness.
Such approach has two advantages: (a)we obtain a well defined space which supports thesimilarity between terms of different surface formsbased on external knowledge and (b) we avoid toexplicitly define term or sense clusters which in-evitably introduce noise.The class of spaces which embeds the above pairinformation may be composed by O(|V |2) dimen-sions.
If we consider only the WN nouns (about105), our space contains about 1010 dimensionswhich is not manageable by most of the learning al-gorithms.
Kernel methods, can solve this problem asthey allow us to use an implicit space representationin the learning algorithms.
Among them SupportVector Machines (SVMs) (Vapnik, 1995) are kernelbased learners which achieve high accuracy in pres-ence of many irrelevant features.
This is another im-portant property as selection of the informative pairsis left to the SVM learning.Moreover, as we believe that the prior knowledgein TC is not so useful when there is a sufficientamount of training documents, we experimented ourmodel in poor training conditions (e.g.
less equalthan 20 documents for each category).
The improve-ments in the accuracy, observed on the classificationof the well known Reuters and 20 NewsGroups cor-pora, show that our document similarity model isvery promising for general IR tasks: unlike previousattempts, it makes sense of the adoption of semanticexternal resources (i.e.
WN) in IR.Section 2 introduces the WordNet-based termsimilarity.
Section 3 defines the new document simi-larity measure, the kernel function and its use withinSVMs.
Section 4 presents the comparative resultsbetween the traditional linear and the WN-basedkernels within SVMs.
In Section 5 comparative dis-cussion against the related IR literature is carriedout.
Finally Section 6 derives the conclusions.2 Term similarity based on generalknowledgeIn IR, any similarity metric in the vector space mod-els is driven by lexical matching.
When small train-ing material is available, few words can be effec-tively used and the resulting document similaritymetrics may be inaccurate.
Semantic generaliza-tions overcome data sparseness problems as con-tributions from different but semantically similarwords are made available.Methods for the induction of semantically in-spired word clusters have been widely used in lan-guage modeling and lexical acquisition tasks (e.g.
(Clark and Weir, 2002)).
The resource employedin most works is WordNet (Fellbaum, 1998) whichcontains three subhierarchies: for nouns, verbs andadjectives.
Each hierarchy represents lexicalizedconcepts (or senses) organized according to an ?is-a-kind-of ?
relation.
A concept s is described bya set of words syn(s) called synset.
The wordsw ?
syn(s) are synonyms according to the senses.For example, the words line, argumentation, logi-cal argument and line of reasoning describe a synsetwhich expresses the methodical process of logicalreasoning (e.g.
?I can?t follow your line of reason-ing?).
Each word/term may be lexically related tomore than one synset depending on its senses.
Theword line is also a member of the synset line, divid-ing line, demarcation and contrast, as a line denotes2also a conceptual separation (e.g.
?there is a nar-row line between sanity and insanity?).
The Wordnetnoun hierarchy is a direct acyclic graph1 in whichthe edges establish the direct isa relations betweentwo synsets.2.1 The Conceptual DensityThe automatic use of WordNet for NLP and IR taskshas proved to be very complex.
First, how the topo-logical distance among senses is related to their cor-responding conceptual distance is unclear.
The per-vasive lexical ambiguity is also problematic as it im-pacts on the measure of conceptual distances be-tween word pairs.
Second, the approximation of aset of concepts by means of their generalization inthe hierarchy implies a conceptual loss that affectsthe target IR (or NLP) tasks.
For example, blackand white are colors but are also chess pieces andthis impacts on the similarity score that should beused in IR applications.
Methods to solve the aboveproblems attempt to map a priori the terms to spe-cific generalizations levels, i.e.
to cuts in the hier-archy (e.g.
(Li and Abe, 1998; Resnik, 1997)), anduse corpus statistics for weighting the resulting map-pings.
For several tasks (e.g.
in TC) this is unsatis-factory: different contexts of the same corpus (e.g.documents) may require different generalizations ofthe same word as they independently impact on thedocument similarity.On the contrary, the Conceptual Density (CD)(Agirre and Rigau, 1996) is a flexible semantic simi-larity which depends on the generalizations of wordsenses not referring to any fixed level of the hier-archy.
The CD defines a metrics according to thetopological structure of WordNet and can be seem-ingly applied to two or more words.
The measureformalized hereafter adapt to word pairs a more gen-eral definition given in (Basili et al, 2004).We denote by s?
the set of nodes of the hierarchyrooted in the synset s, i.e.
{c ?
S|c isa s}, where Sis the set of WN synsets.
By definition ?s ?
S, s ?s?.
CD makes a guess about the proximity of thesenses, s1 and s2, of two words u1 and u2, accord-ing to the information expressed by the minimal sub-hierarchy, s?, that includes them.
Let Si be the set of1As only the 1% of its nodes own more than one parent inthe graph, most of the techniques assume the hierarchy to be atree, and treat the few exception heuristically.generalizations for at least one sense si of the wordui, i.e.
Si = {s ?
S|si ?
s?, ui ?
syn(si)}.
TheCD of u1 and u2 is:CD(u1, u2) =??????
?0 iff S1 ?
S2 = ?maxs?S1?S2?hi=0(?(s?))i|s?|otherwise(1)where:?
S1?S2 is the set of WN shared generalizations(i.e.
the common hypernyms) of u1 and u2?
?(s?)
is the average number of children per node(i.e.
the branching factor) in the sub-hierarchys?.
?(s?)
depends on WordNet and in some casesits value can approach 1.?
h is the depth of the ideal, i.e.
maximallydense, tree with enough leaves to cover thetwo senses, s1 and s2, according to an averagebranching factor of ?(s?).
This value is actuallyestimated by:h ={ blog?(s?
)2c iff ?(s?)
6= 12 otherwise (2)When ?
(s)=1, h ensures a tree with at least 2nodes to cover s1 and s2 (height = 2).?
|s?| is the number of nodes in the sub-hierarchys?.
This value is statically measured on WN andit is a negative bias for the higher level general-izations (i.e.
larger s?
).CD models the semantic distance as the densityof the generalizations s ?
S1 ?
S2.
Such density isthe ratio between the number of nodes of the idealtree and |s?|.
The ideal tree should (a) link the twosenses/nodes s1 and s2 with the minimal numberof edges (isa-relations) and (b) maintain the samebranching factor (bf ) observed in s?.
In other words,this tree provides the minimal number of nodes (andisa-relations) sufficient to connect s1 and s2 accord-ing to the topological structure of s?.
For example, ifs?
has a bf of 2 the ideal tree connects the two senseswith a single node (their father).
If the bf is 1.5, toreplicate it, the ideal tree must contain 4 nodes, i.e.the grandfather which has a bf of 1 and the fatherwhich has bf of 2 for an average of 1.5.
When bf is1 the Eq.
1 degenerates to the inverse of the numberof nodes in the path between s1 and s2, i.e.
the sim-ple proximity measure used in (Siolas and d?AlchBuc, 2000).3It is worth noting that for each pair CD(u1, u2)determines the similarity according to the closestlexical senses, s1, s2 ?
s?
: the remaining senses of u1and u2 are irrelevant, with a resulting semantic dis-ambiguation side effect.
CD has been successfullyapplied to semantic tagging ((Basili et al, 2004)).As the WN hierarchies for other POS classes (i.e.verb and adjectives) have topological properties dif-ferent from the noun hyponimy network, their se-mantics is not suitably captured by Eq.
1.
In thispaper, Eq.
1 has thus been only applied to nounpairs.
As the high number of such pairs increasesthe computational complexity of the target learn-ing algorithm, efficient approaches are needed.
Thenext section describes how kernel methods can makepractical the use of the Conceptual Density in TextCategorization.3 A WordNet Kernel for documentsimilarityTerm similarities are used to design document simi-larities which are the core functions of most TC al-gorithms.
The term similarity proposed in Eq.
1is valid for all term pairs of a target vocabulary andhas two main advantages: (1) the relatedness of eachterm occurring in the first document can be com-puted against all terms in the second document, i.e.all different pairs of similar (not just identical) to-kens can contribute and (2) if we use all term paircontributions in the document similarity we obtain ameasure consistent with the term probability distri-butions, i.e.
the sum of all term contributions doesnot penalize or emphasize arbitrarily any subset ofterms.
The next section presents more formally theabove idea.3.1 A semantic vector spaceGiven two documents d1 and d2 ?
D (the document-set) we define their similarity as:K(d1, d2) =?w1?d1,w2?d2(?1?2)?
?
(w1, w2) (3)where ?1 and ?2 are the weights of the words (fea-tures) w1 and w2 in the documents d1 and d2, re-spectively and ?
is a term similarity function, e.g.the conceptual density defined in Section 2.
Toprove that Eq.
3 is a valid kernel is enough toshow that it is a specialization of the general defi-nition of convolution kernels formalized in (Haus-sler, 1999).
Hereafter, we report such definition.
LetX,X1, .., Xm be separable metric spaces, x ?
Xa structure and ~x = x1, ..., xm its parts, wherexi ?
Xi ?i = 1, ..,m. Let R be a relation onthe set X?X1?
..?Xm such that R(~x, x) is ?true?if ~x are the parts of x.
We indicate with R?1(x) theset {~x : R(~x, x)}.
Given two objects x and y ?
Xtheir similarity K(x, y) is defined as:K(x, y) =?~x?R?1(x)?~y?R?1(y)m?i=1Ki(xi, yi) (4)If X defines the document set (i.e.
D = X),and X1 the vocabulary of the target document corpus(X1 = V ), it follows that: x = d (a document), ~x =x1 = w ?
V (a word which is a part of the documentd) and R?1(d) defines the set of words in the doc-ument d. As ?mi=1 Ki(xi, yi) = K1(x1, y1), thenK1(x1, y1) = K(w1, w2) = (?1?2) ?
?
(w1, w2),i.e.
Eq.
3.The above equation can be used in support vectormachines as illustrated by the next section.3.2 Support Vector Machines and KernelmethodsGiven the vector space in R?
and a set of positiveand negative points, SVMs classify vectors accord-ing to a separating hyperplane, H(~x) = ~?
?~x+b = 0,where ~x and ~?
?
R?
and b ?
R are learned by apply-ing the Structural Risk Minimization principle (Vap-nik, 1995).
From the kernel theory we have that:H(~x) =( ?h=1..l?h ~xh)?~x+b =?h=1..l?h~xh?~x+b =?h=1..l?h?
(dh) ?
?
(d) + b =?h=1..l?hK(dh, d) + b(5)where, d is a classifying document and dh are all thel training instances, projected in ~x and ~xh respec-tively.
The product K(d, dh) =<?
(d) ?
?
(dh)> isthe Semantic WN-based Kernel (SK) function asso-ciated with the mapping ?.Eq.
5 shows that to evaluate the separating hy-perplane in R?
we do not need to evaluate the entirevector ~xh or ~x.
Actually, we do not know even themapping ?
and the number of dimensions, ?.
Asit is sufficient to compute K(d, dh), we can carryout the learning with Eq.
3 in the Rn, avoiding to4use the explicit representation in the R?
space.
Thereal advantage is that we can consider only the wordpairs associated with non-zero weight, i.e.
we canuse a sparse vector computation.
Additionally, tohave a uniform score across different document size,the kernel function can be normalized as follows:SK(d1,d2)?SK(d1,d1)?SK(d2,d2)4 ExperimentsThe use of WordNet (WN) in the term similarityfunction introduces a prior knowledge whose impacton the Semantic Kernel (SK) should be experimen-tally assessed.
The main goal is to compare the tradi-tional Vector Space Model kernel against SK, bothwithin the Support Vector learning algorithm.The high complexity of the SK limits the sizeof the experiments that we can carry out in a fea-sible time.
Moreover, we are not interested to largecollections of training documents as in these train-ing conditions the simple bag-of-words models arein general very effective, i.e.
they seems to modelwell the document similarity needed by the learningalgorithms.
Thus, we carried out the experimentson small subsets of the 20NewsGroups2 (20NG)and the Reuters-215783 corpora to simulate criticallearning conditions.4.1 Experimental set-upFor the experiments, we used the SVM-light software (Joachims, 1999) (available atsvmlight.joachims.org) with the default linearkernel on the token space (adopted as the baselineevaluations).
For the SK evaluation we imple-mented the Eq.
3 with ?
(?, ?)
= CD(?, ?)
(Eq.
1)inside SVM-light.
As Eq.
1 is only defined fornouns, a part of speech (POS) tagger has been previ-ously applied.
However, also verbs, adjectives andnumerical features were included in the pair space.For these tokens a CD = 0 is assigned to pairsmade by different strings.
As the POS-tagger couldintroduce errors, in a second experiment, any tokenwith a successful look-up in the WN noun hierarchywas considered in the kernel.
This approximationhas the benefit to retrieve useful information even2Available at www.ai.mit.edu/people/jrennie/20Newsgroups/.3The Apte?
split available at kdd.ics.uci.edu/databases/reuters21578/reuters21578.html.for verbs and capture the similarity between verbsand some nouns, e.g.
to drive (via the noun drive)has a common synset with parkway.For the evaluations, we applied a careful SVMparameterization: a preliminary investigation sug-gested that the trade-off (between the training-set er-ror and margin, i.e.
c option in SVM-light) parame-ter optimizes the F1 measure for values in the range[0.02,0.32]4.
We noted also that the cost-factor pa-rameter (i.e.
j option) is not critical, i.e.
a value of10 always optimizes the accuracy.
The feature se-lection techniques and the weighting schemes werenot applied in our experiments as they cannot be ac-curately estimated from the small available trainingdata.The classification performance was evaluated bymeans of the F1 measure5 for the single category andthe MicroAverage for the final classifier pool (Yang,1999).
Given the high computational complexity ofSK we selected 8 categories from the 20NG6 and 8from the Reuters corpus7To derive statistically significant results with fewtraining documents, for each corpus, we randomlyselected 10 different samples from the 8 categories.We trained the classifiers on one sample, parameter-ized on a second sample and derived the measureson the other 8.
By rotating the training sample weobtained 80 different measures for each model.
Thesize of the samples ranges from 24 to 160 documentsdepending on the target experiment.4.2 Cross validation resultsThe SK (Eq.
3) was compared with the linear kernelwhich obtained the best F1 measure in (Joachims,1999).
Table 1 reports the first comparative resultsfor 8 categories of 20NG on 40 training documents.The results are expressed as the Mean and the Std.Dev.
over 80 runs.
The F1 are reported in Column 2for the linear kernel, i.e.
bow, in Column 3 for SKwithout applying POS information and in Column 44We used all the values from 0.02 to 0.32 with step 0.02.5F1 assigns equal importance to Precision P and Recall R,i.e.
F1 = 2P ?RP+R .6We selected the 8 most different categories (in terms oftheir content) i.e.
Atheism, Computer Graphics, Misc Forsale,Autos, Sport Baseball, Medicine, Talk Religions and Talk Poli-tics.7We selected the 8 largest categories, i.e.
Acquisition, Earn,Crude, Grain, Interest, Money-fx, Trade and Wheat.5for SK with the use of POS information (SK-POS).The last row shows the MicroAverage performancefor the above three models on all 8 categories.
Wenote that SK improves bow of 3%, i.e.
34.3% vs.31.5% and that the POS information reduces the im-provement of SK, i.e.
33.5% vs. 34.3%.To verify the hypothesis that WN information isuseful in low training data conditions we repeatedthe evaluation over the 8 categories of Reuters withsamples of 24 and 160 documents, respectively.
Theresults reported in Table 2 shows that (1) again SKimproves bow (41.7% - 37.2% = 4.5%) and (2) asthe number of documents increases the improvementdecreases (77.9% - 75.9% = 2%).
It is worth notingthat the standard deviations tend to assume high val-ues.
In general, the use of 10 disjoint training/testingsamples produces a higher variability than the n-fold cross validation which insists on the same docu-ment set.
However, this does not affect the t-studentconfidence test over the differences between the Mi-croAverage of SK and bow since the former has ahigher accuracy at 99% confidence level.The above findings confirm that SK outperformsthe bag-of-words kernel in critical learning condi-tions as the semantic contribution of the SK recov-ers useful information.
To complete this study wecarried out experiments with samples of differentsize, i.e.
3, 5, 10, 15 and 20 documents for eachcategory.
Figures 1 and 2 show the learning curvesfor 20NG and Reuters corpora.
Each point refers tothe average on 80 samples.As expected the improvement provided by SKdecreases when more training data is available.However, the improvements are not negligible yet.The SK model (without POS information) pre-serves about 2-3% of improvement with 160 trainingdocuments.
The matching allowed between noun-verb pairs still captures semantic information whichis useful for topic detection.
In particular, duringthe similarity estimation, each word activates 60.05pairs on average.
This is particularly useful to in-crease the amount of information available to theSVMs.Finally, we carried out some experiments with160 Reuters documents by discarding the stringmatching from SK.
Only words having differentsurface forms were allowed to give contributions tothe Eq.
3.Category bow SK SK-POSAtheism 29.5?19.8 32.0?16.3 25.2?17.2Comp.Graph 39.2?20.7 39.3?20.8 29.3?21.8Misc.Forsale 61.3?17.7 51.3?18.7 49.5?20.4Autos 26.2?22.7 26.0?20.6 33.5?26.8Sport.Baseb.
32.7?20.1 36.9?22.5 41.8?19.2Sci.Med 26.1?17.2 18.5?17.4 16.6?17.2Talk.Relig.
23.5?11.6 28.4?19.0 27.6?17.0Talk.Polit.
28.3?17.5 30.7?15.5 30.3?14.3MicroAvg.
F1 31.5?4.8 34.3?5.8 33.5?6.4Table 1: Performance of the linear and Semantic Kernel with40 training documents over 8 categories of 20NewsGroups col-lection.Category 24 docs 160 docsbow SK bow SKAcq.
55.3?18.1 50.8?18.1 86.7?4.6 84.2?4.3Crude 3.4?5.6 3.5?5.7 64.0?20.6 62.0?16.7Earn 64.0?10.0 64.7?10.3 91.3?5.5 90.4?5.1Grain 45.0?33.4 44.4?29.6 69.9?16.3 73.7?14.8Interest 23.9?29.9 24.9?28.6 67.2?12.9 59.8?12.6Money-fx 36.1?34.3 39.2?29.5 69.1?11.9 67.4?13.3Trade 9.8?21.2 10.3?17.9 57.1?23.8 60.1?15.4Wheat 8.6?19.7 13.3?26.3 23.9?24.8 31.2?23.0Mic.Avg.
37.2?5.9 41.7?6.0 75.9?11.0 77.9?5.7Table 2: Performance of the linear and Semantic Kernel with40 and 160 training documents over 8 categories of the Reuterscorpus.30.033.036.039.042.045.048.051.054.040 60 80 100 120 140 160# Training DocumentsMicro-Average F1bowSKSK-POSFigure 1: MicroAverage F1 of SVMs using bow, SK andSK-POS kernels over the 8 categories of 20NewsGroups.The important outcome is that SK converges to aMicroAverage F1 measure of 56.4% (compare withTable 2).
This shows that the word similarity pro-vided by WN is still consistent and, although in theworst case, slightly effective for TC: the evidenceis that a suitable balancing between lexical ambigu-ity and topical relatedness is captured by the SVMlearning.635.040.045.050.055.060.065.070.075.080.020 40 60 80 100 120 140 160# Training DocumentsMicro-Average F1bowSKFigure 2: MicroAverage F1 of SVMs using bow and SK overthe 8 categories of the Reuters corpus.5 Related WorkThe IR studies in this area focus on the term similar-ity models to embed statistical and external knowl-edge in document similarity.In (Kontostathis and Pottenger, 2002) a Latent Se-mantic Indexing analysis was used for term cluster-ing.
Such approach assumes that values xij in thetransformed term-term matrix represents the simi-larity (> 0) and anti-similarity between terms i andj.
By extension, a negative value represents an anti-similarity between i and j enabling both positive andnegative clusters of terms.
Evaluation of query ex-pansion techniques showed that positive clusters canimprove Recall of about 18% for the CISI collection,2.9% for MED and 3.4% for CRAN.
Furthermore,the negative clusters, when used to prune the resultset, improve the precision.The use of external semantic knowledge seemsto be more problematic in IR.
In (Smeaton, 1999),the impact of semantic ambiguity on IR is stud-ied.
A WN-based semantic similarity function be-tween noun pairs is used to improve indexing anddocument-query matching.
However, the WSD al-gorithm had a performance ranging between 60-70%, and this made the overall semantic similaritynot effective.Other studies using semantic information for im-proving IR were carried out in (Sussna, 1993) and(Voorhees, 1993; Voorhees, 1994).
Word seman-tic information was here used for text indexing andquery expansion, respectively.
In (Voorhees, 1994)it is shown that semantic information derived di-rectly from WN without a priori WSD producespoor results.The latter methods are even more problematic inTC (Moschitti and Basili, 2004).
Word senses tendto systematically correlate with the positive exam-ples of a category.
Different categories are bettercharacterized by different words rather than differ-ent senses.
Patterns of lexical co-occurrences in thetraining data seem to suffice for automatic disam-biguation.
(Scott and Matwin, 1999) use WN sensesto replace simple words without word sense disam-biguation and small improvements are derived onlyfor a small corpus.
The scale and assessment pro-vided in (Moschitti and Basili, 2004) (3 corpora us-ing cross-validation techniques) showed that eventhe accurate disambiguation of WN senses (about80% accuracy on nouns) did not improve TC.In (Siolas and d?Alch Buc, 2000) was proposedan approach similar to the one presented in this ar-ticle.
A term proximity function is used to designa kernel able to semantically smooth the similaritybetween two document terms.
Such semantic ker-nel was designed as a combination of the Radial Ba-sis Function (RBF) kernel with the term proximitymatrix.
Entries in this matrix are inversely propor-tional to the length of the WN hierarchy path link-ing the two terms.
The performance, measured overthe 20NewsGroups corpus, showed an improvementof 2% over the bag-of-words.
Three main differ-ences exist with respect to our approach.
First, theterm proximity does not fully capture the WN topo-logical information.
Equidistant terms receive thesame similarity irrespectively from their generaliza-tion level.
For example, Sky and Location (directhyponyms of Entity) receive a similarity score equalto knife and gun (hyponyms of weapon).
More ac-curate measures have been widely discussed in lit-erature, e.g.
(Resnik, 1997).
Second, the kernel-based CD similarity is an elegant combination oflexicalized and semantic information.
In (Siolas andd?Alch Buc, 2000) the combination of weightingschemes, the RBF kernel and the proximitry matrixhas a much less clear interpretation.
Finally, (Siolasand d?Alch Buc, 2000) selected only 200 featuresvia Mutual Information statistics.
In this way rareor non statistically significant terms are neglectedwhile being source of often relevant contributions inthe SK space modeled over WN.Other important work on semantic kernel for re-trieval has been developed in (Cristianini et al,72002; Kandola et al, 2002).
Two methods for in-ferring semantic similarity from a corpus were pro-posed.
In the first a system of equations were de-rived from the dual relation between word-similaritybased on document-similarity and viceversa.
Theequilibrium point was used to derive the semanticsimilarity measure.
The second method models se-mantic relations by means of a diffusion process ona graph defined by lexicon and co-occurrence in-formation.
The major difference with our approachis the use of a different source of prior knowledge.Similar techniques were also applied in (Hofmann,2000) to derive a Fisher kernel based on a latent classdecomposition of the term-document matrix.6 ConclusionsThe introduction of semantic prior knowledge inIR has always been an interesting subject as theexamined literature suggests.
In this paper, weused the conceptual density function on the Word-Net (WN) hierarchy to define a document similar-ity metric.
Accordingly, we defined a semantickernel to train Support Vector Machine classifiers.Cross-validation experiments over 8 categories of20NewsGroups and Reuters over multiple sampleshave shown that in poor training data conditions, theWN prior knowledge can be effectively used to im-prove (up to 4.5 absolute percent points, i.e.
10%)the TC accuracy.These promising results enable a number of futureresearches: (1) larger scale experiments with differ-ent measures and semantic similarity models (e.g.
(Resnik, 1997)); (2) improvement of the overall ef-ficiency by exploring feature selection methods overthe SK, and (3) the extension of the semantic sim-ilarity by a general (i.e.
non binary) application ofthe conceptual density model.ReferencesE.
Agirre and G. Rigau.
1996.
Word sense disambiguationusing conceptual density.
In Proceedings of COLING?96,Copenhagen, Danmark.R.
Basili, M. Cammisa, and F. M. Zanzotto.
2004.
A similar-ity measure for unsupervised semantic disambiguation.
InIn Proceedings of Language Resources and Evaluation Con-ference, Lisbon, Portugal.Ron Bekkerman, Ran El-Yaniv, Naftali Tishby, and Yoad Win-ter.
2001.
On feature distributional clustering for text cat-egorization.
In Proceedings of SIGIR?01 , New Orleans,Louisiana, US.Stephen Clark and David Weir.
2002.
Class-based probabilityestimation using a semantic hierarchy.
Comput.
Linguist.,28(2):187?206.Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002.Latent semantic kernels.
J. Intell.
Inf.
Syst., 18(2-3):127?152.Christiane Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.D.
Haussler.
1999.
Convolution kernels on discrete struc-tures.
Technical report ucs-crl-99-10, University of Califor-nia Santa Cruz.Thomas Hofmann.
2000.
Learning probabilistic models ofthe web.
In Research and Development in Information Re-trieval.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning.J.
Kandola, J. Shawe-Taylor, and N. Cristianini.
2002.
Learn-ing semantic similarity.
In NIPS?02) - MIT Press.A.
Kontostathis and W. Pottenger.
2002.
Improving retrievalperformance with positive and negative equivalence classesof terms.Hang Li and Naoki Abe.
1998.
Generalizing case frames usinga thesaurus and the mdl principle.
Computational Linguis-tics, 23(3).Alessandro Moschitti and Roberto Basili.
2004.
Complexlinguistic features for text classification: a comprehensivestudy.
In Proceedings of ECIR?04, Sunderland, UK.P.
Resnik.
1997.
Selectional preference and sense disambigua-tion.
In Proceedings of ACL Siglex Workshop on TaggingText with Lexical Semantics, Why, What and How?, Wash-ington, 1997.Sam Scott and Stan Matwin.
1999.
Feature engineering fortext classification.
In Proceedings of ICML?99, Bled, SL.Morgan Kaufmann Publishers, San Francisco, US.Georges Siolas and Florence d?Alch Buc.
2000.
Support vectormachines based on a semantic kernel for text categorization.In Proceedings of IJCNN?00.
IEEE Computer Society.Alan F. Smeaton.
1999.
Using NLP or NLP resources for in-formation retrieval tasks.
In Natural language informationretrieval, Kluwer Academic Publishers, Dordrecht, NL.M.
Sussna.
1993.
Word sense disambiguation for free-text in-dexing using a massive semantic network.
In CKIM?93,.V.
Vapnik.
1995.
The Nature of Statistical Learning Theory.Springer.Ellen M. Voorhees.
1993.
Using wordnet to disambiguate wordsenses for text retrieval.
In Proceedings SIGIR?93 Pitts-burgh, PA, USA.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of SIGIR?94,ACM/Springer.Y.
Yang.
1999.
An evaluation of statistical approaches to textcategorization.
Information Retrieval Journal.8
