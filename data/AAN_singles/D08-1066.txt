Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630?639,Honolulu, October 2008. c?2008 Association for Computational LinguisticsPhrase Translation Probabilities with ITG Priorsand Smoothing as Learning ObjectiveMarkos MylonakisLanguage and Computation, ILLCFaculty of ScienceUniversity of Amsterdamm.mylonakis@uva.nlKhalil Sima?anLanguage and Computation, ILLCFaculty of ScienceUniversity of Amsterdamk.simaan@uva.nlAbstractThe conditional phrase translation probabil-ities constitute the principal components ofphrase-based machine translation systems.These probabilities are estimated using aheuristic method that does not seem to opti-mize any reasonable objective function of theword-aligned, parallel training corpus.
Ear-lier efforts on devising a better understoodestimator either do not scale to reasonablysized training data, or lead to deterioratingperformance.
In this paper we explore a newapproach based on three ingredients (1) Agenerative model with a prior over latentsegmentations derived from Inversion Trans-duction Grammar (ITG), (2) A phrase ta-ble containing all phrase pairs without lengthlimit, and (3) Smoothing as learning ob-jective using a novel Maximum-A-Posterioriversion of Deleted Estimation working withExpectation-Maximization.
Where othersconclude that latent segmentations lead tooverfitting and deteriorating performance,we show here that these three ingredientsgive performance equivalent to the heuristicmethod on reasonably sized training data.1 MotivationA major component in phrase-based statistical Ma-chine translation (PBSMT) (Zens et al, 2002;Koehn et al, 2003) is the table of conditional prob-abilities of phrase translation pairs.
The pervadingmethod for estimating these probabilities is a sim-ple heuristic based on the relative frequency of thephrase pair in the multi-set of the phrase pairs ex-tracted from the word-aligned corpus (Koehn et al,2003).
While this heuristic estimator gives good em-pirical results, it does not seem to optimize any intu-itively reasonable objective function of the (word-aligned) parallel corpus (see e.g., (DeNero et al,2006)) The mounting number of efforts attackingthis problem over the last few years (DeNero et al,2006; Marcu and Wong, 2002; Birch et al, 2006;Moore and Quirk, 2007; Zhang et al, 2008) exhibitsits difficulty.
So far, none has lead to an alternativemethod that performs as well as the heuristic on rea-sonably sized data (approx.
1000k sentence pair).Given a parallel corpus, an estimator for phrase-tables in PBSMT involves two interacting decisions(1) which phrase pairs to extract, and (2) how to as-sign probabilities to the extracted pairs.
The heuris-tic estimator employs word-alignment (Giza++)(Och and Ney, 2003) and a few thumb rules fordefining phrase pairs, and then extracts a multi-setof phrase pairs and estimates their conditional prob-abilities based on the counts in the multi-set.
Us-ing this method for extracting a set of phrase pairs,(DeNero et al, 2006; Moore and Quirk, 2007) aimat defining a better estimator for the probabilities.Generally speaking, both efforts report deterioratingtranslation performance relative to the heuristic.Instead of employing word-alignment to guidephrase pair extraction, it is theoretically more ap-pealing to aim at phrase alignment as part of the esti-mation process (Marcu and Wong, 2002; Birch et al,2006).
This way, phrase pair extraction goes hand-in-hand with estimating the probabilities.
How-ever, in practice, due to the huge number of possi-ble phrase pairs, this task is rather challenging, bothcomputationally and statistically.
It is hard to define630both a manageable phrase pair translation model anda well-founded training regime that would scale upto reasonably sized parallel corpora (see e.g., (Birchet al, 2006)).
It remains to be seen whether this the-oretically interesting approach will lead to improvedphrase probability estimates.In this paper we also start out from a stan-dard phrase extraction procedure based on word-alignment and aim solely at estimating the condi-tional probabilities for the phrase pairs and theirreverse translation probabilities.
Unlike precedingwork, we extract all phrase pairs from the trainingcorpus and estimate their probabilities, i.e., withoutlimit on length.
We present a novel formulation ofa conditional translation model that works with aprior over segmentations and a bag of conditionalphrase pairs.
We use binary Synchronous Context-Free Grammar (bSCFG), based on Inversion Trans-duction Grammar (ITG) (Wu, 1997; Chiang, 2005a),to define the set of eligible segmentations for analigned sentence pair.
We also show how the num-ber of spurious derivations per segmentation in thisbSCFG can be used for devising a prior probabil-ity over the space of segmentations, capturing thebias in the data towards monotone translation.
Theheart of the estimation process is a new smoothingestimator, a penalized version of Deleted Estima-tion, which averages the temporary probability es-timates of multiple parallel EM processes at eachjoint iteration.For evaluation we use a state-of-the-art baselinesystem (Moses) (Hoang and Koehn, 2008) whichworks with a log-linear interpolation of feature func-tions optimized by MERT (Och, 2003).
We sim-ply substitute our own estimates for the heuristicphrase translation estimates (both directions and thephrase penalty score) and compare the two withinthe Moses decoder.
While our estimates differ sub-stantially from the heuristic, their performance is onpar with the heuristic estimates.
This is remark-able given the fact that comparable previous work(DeNero et al, 2006; Moore and Quirk, 2007) didnot match the performance of the heuristic estima-tor using large training sets.
We find that smooth-ing is crucial for achieving good estimates.
Thisis in line with earlier work on consistent estimationfor similar models (Zollmann and Sima?an, 2006),and agrees with the most up-to-date work that em-ploys Bayesian priors over the estimates (Zhang etal., 2008).2 Related workMarcu and Wong (Marcu and Wong, 2002) realizethat the problem of extracting phrase pairs shouldbe intertwined with the method of probability esti-mation.
They formulate a joint phrase-based modelin which a source-target sentence pair is generatedjointly.
However, the huge number of possiblephrase-alignments prohibits scaling up the estima-tion by Expectation-Maximization (EM) (Dempsteret al, 1977) to large corpora.
Birch et al(Birch etal., 2006) provide soft measures for including word-alignments in the estimation process and obtain im-proved results only on small data sets.Coming up-to-date, (Blunsom et al, 2008) at-tempt a related estimation problem to (Marcu andWong, 2002), using the expanded phrase pair setof (Chiang, 2005a), working with an exponentialmodel and concentrating on marginalizing out thelatent segmentation variable.
Also most up-to-date,(Zhang et al, 2008) report on a multi-stage model,without a latent segmentation variable, but with astrong prior preferring sparse estimates embedded ina Variational Bayes (VB) estimator and concentrat-ing the efforts on pruning both the space of phrasepairs and the space of (ITG) analyses.
The latter twoefforts report improved performance, albeit again ona limited training set (approx.
140k sentences up toa certain length).DeNero et al(2006) have explored estimation us-ing EM of phrase pair probabilities under a con-ditional translation model based on the originalsource-channel formulation.
This model involves ahidden segmentation variable that is set uniformly(or to prefer shorter phrases over longer ones).
Fur-thermore, the model involves a reordering compo-nent akin to the one used in IBM model 3.
De-spite this, the heuristic estimator remains superiorbecause ?EM learns overly determinized segmen-tations and translation parameters, overfitting thetraining data and failing to generalize?.
More re-cently, (Moore and Quirk, 2007) devise a estimatorworking with a model that does not include a hid-den segmentation variable but works with a heuris-tic iterative procedure (rather than MLE or EM).
The631translation results remain inferior to the heuristic butthe authors note an interesting trade-off between de-coding speed and the various settings of this estima-tor.Our work expands on the general approach takenby (DeNero et al, 2006; Moore and Quirk, 2007)but arrives at insights similar to those of the mostrecent work (Zhang et al, 2006), albeit in a com-pletely different manner.
The present work differsfrom all preceding work in that it employs the setof all phrase pairs during training.
It differs from(Zhang et al, 2008) in that it does postulate a la-tent segmentation variable and puts the prior di-rectly over that variable rather than over the ITGsynchronous rule estimates.
Our method neitherexcludes phrase pairs before estimation nor does itprune the space of possible segmentations/analysesduring training/estimation.
As well as smoothing,we find (in the same vein as (Zhang et al, 2008))that setting effective priors/smoothing is crucial forEM to arrive at better estimates.3 The Translation ModelGiven a word-aligned parallel corpus of source-target sentences, it is common practice to extract aset of phrase pairs using extraction heuristics (cf.
(Koehn et al, 2003; Och and Ney, 2004)).
Theseheuristics define a phrase pair to consist of a sourceand target ngrams of a word-aligned source-targetsentence pair such that if one end of an alignmentis in the one ngram, the other end is in the otherngram (and there is at least one such alignment)(Och and Ney, 2004; Koehn et al, 2003).
For ef-ficiency and sparseness, the practitioners of PBSMTconstrain the length of the source phrase to a certainmaximum number of words.An All Phrase Pairs Model: In this work we traina phrase-translation table that consists of all phrase-pairs that can be extracted from the word-alignedtraining data according to the standard phrase ex-traction heuristic.
After training, we can still limitthe set of phrase pairs to those selected by a cut-offon phrase length.
The reason for using all phrasepairs during training is that it gives a clear point ofreference for an estimator, without implicit, acciden-tal biases that might emerge due to length cut-off1.The Generative Model: Given a word-alignedsource-target sentence pair ?f , e,a?, the generativestory underlying our model goes as follows:1.
Abiding by the word-alignments in a, segmentthe source-target sentence pair ?f , e?
into a se-quence of I containers ?I1 , and a bag of Iphrase pairs ?I1(f , e) = {?fj , ej?}Ij=1.
Eachcontainer ?j = ?lf , rf , le, re?
consists of thestart lf and end rf positions2 for a phrase inf and the start le and end re positions for analigned phrase in e.2.
For a given segmentation ?I1 , for every con-tainer ?j (1 ?
j ?
I) generate the phrase-pair?fj, ej?, independently from all other phrase-pairs.This leads to the following probabilistic model:P (f | e;a) =??I1??
(a)P (?I1)?
?fj ,ej??
?I1 (f ,e)P (fj | ej) (1)Where ?
(a) is the set of binarizable segmenta-tions (defined next) that are eligible according to theword-alignments a between f and e. These segmen-tations into bilingual containers (where segmenta-tions are taken inside the containers) are differentfrom the monolingual segmentations used in earliercomparable conditional models (e.g., (DeNero et al,2006)) which must generate the alignment on top ofthe segmentations.
Note how the different phrasepairs ?fj, ej?
are generated from their bilingual con-tainers in the given segmentation ?I1 .
We will dis-cuss our choice of prior probability over segmenta-tions P (?I1) after we discuss the definition of the bi-narizable segmentations ?
(a).3.1 Binarizable segmentations ?
(a)Following (Zhang et al, 2006; Huang et al, 2008),every sequence of phrase alignments can be viewed1For example, if the cut-off on phrase pairs is ten words, allsentence pairs smaller than ten words in the training data willbe included as phrase pairs as well.
These sentences are treateddifferently from longer sentences, which are not allowed to bephrase pairs.2The NULL alignments (word-to-NULL) in the trainingdata can also be marked with actual positions on both sides inorder to allow for this definition of containers.632as a sequence of integers 1, .
.
.
I together with apermuted version of this sequence pi(1), .
.
.
, pi(I),where the two copies of an integer in the two se-quences are assumed aligned/paired together.
Forexample, possible permutations of {1, 2, 3, 4} are{2, 1, 3, 4} and {2, 4, 1, 3}.
Because a segmenta-tion ?I1 of a sentence pair is also a sequence ofaligned phrases, it also constitutes a permuted se-quence.
A binarizable permutation x is either oflength one, or can be properly split into two binariz-able sub-sequences y and z such that either3 z < yor y < z.
For example, one way to binarize thepermutation {2, 1, 3, 4} is to introduce a proper splitinto {2, 1; 3, 4}, then recursively another proper splitof {2, 1} into {2; 1} and {3, 4} into {3; 4}.
In con-trast, the permutation {2, 4, 1, 3} is non-binarizable.<><>2 1[]3 4[][]<>2 134Figure 1: Multiple ways to binarize a permutationGraphically speaking, the recursive definition ofbinarizable permutations can be depicted as a bi-nary tree structure where the nodes correspond torecursive proper splits of the permutation, and theleaves are decorated with the naturals.
Figure 1 ex-hibits two possible binarizations of the same permu-tation where <> and [] denote inverted and mono-tone proper splits respectively.
Note that the numberof possible binarizations of a binarizable permuta-tion is a recursive function of the number of possi-ble proper splits and reaches its maximum for fullymonotone permutations (all binary trees, which is afactorial function of the length of the permutation).By definition (cf.
(Zhang et al, 2006; Huang etal., 2008)), a binarizable segmentation/permutationcan be recognized by a binarized SynchronousContext-Free Grammar (SCFG), i.e., an SCFG inwhich the right hand sides of all non-lexical rulesconstitute binarizable permutations.
In particular,this holds for the SCFG implementing Inversion3For two sequences of numbers, the notation y < z standsfor ?y ?
y,?z ?
z : y < z.Transduction Grammar (Wu, 1997).
This SCFG(Chiang, 2005b) has two binary synchronous rulesthat correspond resp.
to the contiguous monotoneand inverted alignments:XP ?
XP 1 XP 2 , XP 1 XP 2 (2)XP ?
XP 1 XP 2 , XP 2 XP 1The boxed integers in the superscripts on the non-terminal XP denote synchronized rewritings.
Inthis work, we employ a binary SCFG (bSCFG)working with these two synchronous rules to-gether with a set of lexical rules {XP ?f, e | ?f, e?
is a phrase pair}.In this bSCFG, every derivation corresponds to abinarization of a segmentation of the input.
Notethat the bSCFG defined in equation 2 generates allpossible binarizations for every segmentation of theinput.
It is possible to constrain this bSCFG suchthat it generates a single, canonical derivation persegmentation.
However, in section 3.2 we show thatthe number of such derivations is a good measure ofphrase pair productivity.It is well known that there are alignments andsegmentations that this bSCFG does not cover (see(Huang et al, 2008)).
Recently, strong evidenceemerged (e.g., (Huang et al, 2008)) showing thatmost word-alignments of actual parallel corpora canbe covered by a binarized SCFG of the ITG type.Furthermore, because our model employs the set ofall phrase-pairs that can be extracted from a giventraining set, it will always find segmentations thatcover every sentence pair in the training data4.
Thisimplies that while our model might discard non-binarizable segmentations for certain complex wordalignments, we do manage to train the model on thebinarizable segementations of all sentence pairs.Up to the prior over segmentations (see next), weimplement the above model using a weighted ver-sion of the binary SCFG as follows:?
The weight for lexical rules is given byP (XP ?
f, e) := P (f | e), where ?f, e?
isa phrase-pair.
These are the trainable parame-ters of our model.4In the worst case the whole sentence pair is a phrase pairwith a trivial segmentation.633115511552 3 43 4 23 4 22 43Figure 2: Two segmentations of an align-ment/permutation.
Both segmentations have thesame number of binarizations despite differences incontainer sizes.?
The weights for the two non-lexical rules inequation 2 are fixed at 1.0.
These weights arenot trained at all.Where we use the notation P (.)
for the weight of asynchronous rule.3.2 Prior over segmentationsAs it has been found out by (DeNero et al, 2006),it is not easy to come up with a simple, effec-tive prior distribution over segmentations that al-lows for improved phrase pair estimates.
Within aMaximum-Likelihood estimator, preference for seg-mentations ?I1 consisting of longer containers couldlead to overfitting as we will explain in section 4.Alternatively, it is tempting to have preference forsegmentations ?I1 that consist of shorter contain-ers, because (generally speaking) shorter contain-ers have higher expected coverage of new sentencepairs.
However, mere bias for shorter containerswill not give better estimates as observed by (DeN-ero et al, 2006).
One case where this bias clearlyfails is the case of a contiguous sequence of con-tainers with a complex alignment structure (cross-ing alignments).
For example (see figure 2), forthe alignment {1, 3, 4, 2, 5} there is a segmentationinto five containers {1; 3; 4; 2; 5}, and another intothree {1; 3, 4, 2; 5}.
The first segmentation involvesshorter containers that have crossing brackets amongthem, while the second one consists of three con-tainers including a longer container {3, 4, 2}.
Inthe first segmentation, due to their crossing align-ments, each of the containers {3}, {4} and {2} willnot combine with the surrounding context ({1} and{5}) on its own, i.e., without the other two contain-ers.
Furthermore, there is only a single binariza-tion of {3, 4, 2}.
Hence, while the first segmen-tation involves shorter containers than the secondone, these shorter containers are as productive asthe large container {3, 4, 2}, i.e., they combine withsurrounding containers in the same number of waysas the large container.
In such and similar cases,there are no grounds for the bias towards shorterphrases/containers.The notion of container productivity (the num-ber of ways in which it combines with surroundingcontainers during training) seems to correlate withthe expected number of ways a container can beused during decoding, which should be correlatedwith expected coverage.
During training, contain-ers that are often surrounded by other, monotoni-cally aligned containers are expected to be more pro-ductive than alternative containers that are often sur-rounded by crossing alignments.
Hence, the num-ber of binarizations that a segmentation has underthe bSCFG is a direct function of the ways in whichthe containers combine among themselves (mono-tone vs. inverted/crossing) within segmentations,and provides a more accurate measure of containerproductivity than container length.
Hence, the finalmodel we employ is the following:P (f | e;a) =??I1??(a)N(?I1)Z(?(a))?
?fj ,ej??
?I1(f ,e)P (fj | ej) (3)Where N(?I1) is the number of binary deriva-tions/trees that ?I1 has in the binary SCFG (bSCFG),and Z(?
(a)) = ?
?J1 ??
(a) N(?J1 ), i.e., this prior isthe ratio of number of derivations of ?I1 to the to-tal number of derivations that ?f , e,a?
has under thebSCFG.3.3 Contrast with similar models:In contrast with the model of (DeNero et al, 2006),who define the segmentations over the source sen-tence f alone, our model employs bilingual con-tainers thereby segmenting both source and targetsides simultaneously.
Therefore, unlike (DeNeroet al, 2006), our model does not need to gener-ate the word-alignments explicitly, as these are em-bedded in the segmentations.
Similarly, our modeldoes not include explicit penalty terms for reorder-634ing/inversion but includes a related bias in the priorprobabilities over segmentations P (?I1).In a way, the segmentations and bilingual contain-ers we use can be viewed as similar to the conceptsused in the Joint Model of Marcu and Wong (Marcuand Wong, 2002).
Unlike (Marcu and Wong, 2002),however, our model works with conditional proba-bilities and starts out from the word-alignments.The novel aspects of our model are three (1) It de-fines the set of segmentations using a bSCFG, (2) Itincludes a novel, refined prior probability over seg-mentations, and (3) It employs all phrase pairs thatcan be extracted from a word-aligned training par-allel corpus.
For these novel elements to producereasonable estimates, we devise our own estimator.4 Estimation by SmoothingIn principle, we are dealing here with a translationmodel that employs all phrase pairs (of unboundedsize), extracted from a word-aligned parallel cor-pus.
Under this model, where a phrase pair andits sub-phrase pairs are included in the model, theMLE can be expected to overfit the data5 unless asuitable prior probability over segmentations is em-ployed.
Indeed, the prior over segmentations definedin the preceding section prevents the MLE fromcompletely overfitting the training data.
However,we find empirical evidence that this prior is insuffi-cient for avoiding overfitting.Our model behaves like a memory-based modelbecause it memorizes all extractable phrase pairsfound in the training data including the training sen-tence pairs themselves.
Such memory-based mod-els are related to nonparametric models such asK-NN and kernel methods (Hastie et al, 2001).For memory-based models, consistent estimation fornovel instances proceeds by local density estimationfrom the surroundings of the instance, which is akinto smoothing for parametric models.
Hence, next wedescribe our own version of a smoothed Maximum-Likelihood estimator for phrase translation probabil-5One trivial MLE solution would give the longest container,consisting of the longest phrase pairs, a probability of one, atthe cost of all shorter alternatives.
A similar problem arises inData-Oriented Parsing, see (Sima?an and Buratto, 2003; Zoll-mann and Sima?an, 2006).
Note that models that employ anupperbound on phrase pair length will still risk overfitting train-ing sentences of lengths that fall within this upperbound.??????????????????
?-INPUT: Word-aligned parallel training data TOUTPUT: Estimates pi for all P (f | e){Split training data T into equal parts H1, .
.
.
,H10.For 1 ?
i ?
10 doExtract from Ei = ?j 6=iHj all phrase pairs piiInitialize p?i0i to uniform conditional probsLet j = 0RepeatLet j = j + 1 // EM iteration counterFor 1 ?
i ?
10 doE-step: calculate expected counts for pairsin piji on Hi using counts from p?ij?1i .M-step: calculate probabilities for pairs inpiji from the expected countsFor 1 ?
i ?
10 do p?iji := 110?10i=1 pijiUntil pi := {p?ij1, .
.
.
, p?ij10} has converged}??????????????????
?-Figure 3: Penalized Deleted Estimationities.For a latent variable model, it is usually commonto employ Expectation-Maximization (EM) (Demp-ster et al, 1977) as a search method for a (local)maximum-likelihood estimate (MLE) of the train-ing data.
Instead of mere EM we opt for a smoothedversion: we present a new method, that combinesDeleted Estimation (Jelinek and Mercer, 1980) withthe Jackknife (Duda et al, 2001) as the core estima-tor.Figure 3 shows the pseudo-code for our estima-tor.
Like in Deleted Estimation, we split the trainingdata into ten equal portions.
This way we create tendifferent splits of extraction/heldout sets of respec-tively 90%/10% of the training set.
For every split1 ?
i ?
10, we extract a set of phrase pairs pii fromthe extraction set Ei and train it (under our model)on the heldout set Hi.
Naturally, the phrase pair setspii (1 ?
i ?
10) are subsets of (or equal to) the setof phrase pairs pi = ?ipii extracted from the totaltraining data (i.e., pi is the set of model parameters).The training of the different pii?s, each on its corre-sponding heldout set Hi, is done by ten separate EMprocesses, which are synchronized in their initializa-635tion, their iterations as well as stop condition.
TheEM processes start out from uniform conditional es-timates of the phrase pairs in all pii.
After every EMiteration j, when the M-step has finished, the esti-mates in all piji (1 ?
i ?
10) are set to the average(over 1 ?
i ?
10) of the estimates in piji leading top?iji (following the Jackknife method).
The resultingaveraged probabilities in p?iji are then used as the cur-rent phrase pair estimates, which feed into the nextiteration j + 1 of the different EM processes (eachworking on a different heldout set Hi with a differ-ent set of phrase pairs pii).There are two special boundary cases which de-mand special attention during estimation:Sparse distributions: For a phrase e that does oc-cur both in Hi and Ei, there could be a phrasepair ?f, e?
that does occur in Hi but does notoccur in pii.
To prevent EM from giving theextra probability mass to all other pairs ?f, e?
?unjustifiably, we apply smoothing.
We add themissing pair ?f, e?
to pii and set its probabilityto a fixed number 10?5?len, where len is thelength of the phrase pair.
In effect, we backoffour model (equation 1) to a word-level modelwith fixed word translation probability (10?5).Zero distributions: When a phrase e does not oc-cur in Hi, all its pairs ?f, e?
in pii will havezero counts.
During each EM iteration, whenthe M-step is applied, the distribution P (?
| e)is undefined by MLE, since it is irrelevant forthe likelihood of Hi.
In this case any choiceof proper distribution P (?
| e) will constitute anMLE solution.
We choose to set this case to auniform distribution every time again.Since our model and estimator are implementedwithin the bSCFG framework, we use a bilingualCYK parser (Younger, 1967) under the grammarin equation 2.
This parser builds for every input?f ,a, e?
all binarizations/derivations for every seg-mentation in ?(a).
For implementing EM, we em-ploy the Inside-Outside algorithm (Lari and Young,1990; Goodman, 1998).
During estimation, becausethe input, output and word-alignment are knownin advance, the time and space requirements re-main manageable despite the worst-case complexityO(n6) in target sentence length n.Penalized Deleted Estimation: In contrast withour method, Deleted Estimation sums the expectedcounts (rather than probabilities) obtained fromthe different splits before applying the M-step(normalization).
While the rationale behind DeletedEstimation comes from MLE over the originaltraining data, our method has a smoothing objective(inspired by the Jackknife ): generally speaking, theaverages over different heldout sets (under differentsubsets of the model) give less sharp estimates thanMLE.
By averaging the different heldout estimates,this estimator employs a penalty term that dependson the marginal count of e in the heldout set6.Interestingly, when the phrase e is very frequent7,it will approximately occur almost as often in thedifferent heldout sets.
In this case, our methodreduces to Deleted Estimation, where it effectivelysums the counts8.
Yet, when the target phrase edoes occur only very few times, it is likely that itscount in some splits will be zero.
In our method, atevery EM iteration, during the Maximization step,we set such cases back to uniform.
By averaging theprobabilities from the different splits over many EMiterations, setting these cases to uniform constitutesa kind of prior that prevents the final estimatesfrom falling too far from uniform.
In contrast, inDeleted Interpolation the zero counts are simplysummed with the other corresponding counts of thesame phrase pair, which leads to sharper probabilitydistributions.
In all experiments that we conducted,our method (which we call Penalized DeletedEstimation) gave more successful estimates thanmere Deleted Estimation.On the theoretical side, the choice for a fixed6Define county(x) to be the count of event xin data y.
The Deleted Estimation (DE) estimate is?H countH (f, e)/countT (e), which can be written as?H [countH (f, e)/countH (e)][countH(e)/countT (e)] =?H piH(f |e)[countH (e)/countT (e)] where piH(f |e) is theestimate from heldout set H .
Hence, DE linearly interpolatedpiH with factors countH (e)/countT (e).
Our estimator em-ploys uniform interpolation factors instead, thereby penalizingthe DI counts (hence Penalized DI).7Theoretically speaking, when the training data is unbound-edly large, our estimator will converge to the same estimatesas the Deleted Estimation.
When the data is still sparse, ourestimator is biased, unlike the MLE which will overfit.8When calculating the conditional probabilities, the denom-inators used are approximately equal to one another.636prior over segmentations (ITG prior) implies that ourmodel cannot be estimated to converge (in proba-bility) to the relative frequency estimates (RFE) ofsource-target sentence pairs in the limit of the train-ing data (a sufficiently large parallel corpus).
A priorprobability over segmentations that would allow ourestimator to converge in the limit to the RFE mustgradually prefer segmentations consisting of largercontainers as the data grows large.
We set the de-sign and estimation of such a prior aside for futurework.5 Empirical experimentsDecoding and Baseline Model: In this workwe employ an existing decoder, Moses (Hoangand Koehn, 2008), which defines a log-linearmodel interpolating feature functions, with interpo-lation scores ?f e?
= argmaxe?f??
?fHf (f , e).The ?f are optimized by Minimum-Error Training(MERT) (Och, 2003).
The set ?
consists of thefollowing feature functions (see (Hoang and Koehn,2008)): a 5-gram target language model, the stan-dard reordering scores, the word and phrase penaltyscores, the conditional lexical estimates obtainedfrom the word-alignment in both directions, and theconditional phrase translation estimates in both di-rections P (f | e) and P (e | f).
Keeping the otherfive feature functions fixed, we compare our esti-mates of P (f | e) and P (e | f) (and the phrasepenalty) to the commonly used heuristic estimates.Because our model employs a latent segmenta-tion variable, this variable should be marginalizedout during decoding to allow selecting the highestprobability translation given the input.
This turnsout crucial for improved results (cf.
(Blunsom et al,2008)).
However, such a marginalization can be NP-Complete, in analogy to a similar problem in Data-Oriented Parsing (Sima?an, 2002)9.
We do not havea decoder yet that can approximate this marginaliza-tion efficiently and we employ the standard Mosesdecoder for this work.Experimental Setup: The training, developmentand test data all come from the French-Englishtranslation shared task of the ACL 2007 Second9A reduction of simple instances of the first problem to in-stances of the latter problem should be possible.Phrases System BLEU?
7 Baseline PBSMT 33.03?
10 Baseline PBSMT 33.03All Baseline PBSMT 33.00?
7 EM + ITG Prior 32.50?
7 EM + Del.
Est.
32.67?
7 EM + Del.
Est.
+ ITG Prior 32.73?
7 EM + Pen.
Del.
Est.
+ ITG Prior 33.02?
10 EM + Pen.
Del.
Est.
+ ITG Prior 33.14All EM + Pen.
Del.
Est.
+ ITG Prior 32.98Table 1: Results: data from ACL07 2nd Wkshp on SMTWorkshop on Statistical Machine Translation 10.
Af-ter pruning sentence pairs with word length morethan 40 on either side, we are left with 949K sen-tence pairs for training.
The development and testdata are composed of 2K sentence pairs each.
Alldata sets are lower-cased.For both the baseline system and our method,we produce word-level alignments for the paralleltraining corpus using GIZA++.
We use 5 iterationsof each IBM Model 1 and HMM alignment mod-els, followed by 3 iterations of each Model 3 andModel 4.
From this aligned training corpus, we ex-tract the phrase pairs according to the heuristics in(Koehn et al, 2003).
The baseline system extractsall phrase-pairs upto a certain maximum length onboth sides and employs the heuristic estimator.
Thelanguage model used in all systems is a 5-gram lan-guage model trained on the English side of the paral-lel corpus.
Minimum-Error Rate Training (MERT)is applied on the development set to obtain opti-mal log-linear interpolation weights for all systems.Performance is measured by computing the BLEUscores (Papineni et al, 2002) of the system?s trans-lations, when compared against a single referencetranslation per sentence.Results: We compare different versions of oursystem against the baseline system using the heuris-tic estimator.
We observe the effects of the ITG priorin the translation model as well as the method of es-timation (Deleted Estimation vs. Penalized DeletedEstimation).Table 1 exhibits the BLEU scores for the sys-10http://www.statmt.org/wmt07637tems.
Our own system (with ITG prior and Pe-nalized Deleted Estimation and maximum phrase-length ten words) scores (33.14), slightly outper-forming the best baseline system (33.03).
When us-ing straight Deleted Estimation over EM, this leadsto deterioration (32.73).
When also the ITG prior isexcluded (by having a single derivation per segmen-tation) this leads to further deterioration (32.67).
Byusing mere EM with an ITG prior, performance goesdown to 32.50, exhibiting the crucial role of the es-timation by smoothing.
Clearly, Penalized DeletedEstimation and the ITG prior are important for theimproved phrase translation estimates.As table 1 shows we also varied the phrase lengthcutoff (seven, ten or none=all phrase pairs).
Thelength cutoff pertains to both sides of a phrase-pair.For our estimator, we always train all phrase pairs,applying the length cutoff only after training (no re-normalization is applied at that point).Interestingly, we find out that the heuristic estima-tor cannot benefit performance by including longerphrase pairs.
Our estimator does benefit perfor-mance by including phrase pairs of length upto tenwords, but then it degrades again when includingall phrase pairs.
We take the latter finding to sig-nal remaining overfitting that proved resistant to thesmoothing applied by our estimator.
The heuristicestimator exhibits a similar degradation.We also tried to vary the treatment of Sparse Dis-tributions (section 4, page 7) during heldout estima-tion from fixed word-translation probabilities to thelexical model probabilities.
This lead to slight dete-rioration of results (32.94).
It is unclear whether thisdeterioration is meaningful or not.
We did not ex-plore mere EM without any smoothing or ITG prior,as we expect it will directly overfit the training dataas reported by (DeNero et al, 2006).We note that for French-English translation it ishard to outperform the heuristic within the PBSMTframework, since it already performs very well.Preliminary, most recent experiments on German-English (also WMT07 data) exhibit that our estima-tor outperforms the heuristic.6 Discussion and Future ResearchThe most similar efforts to ours, mainly (DeNeroet al, 2006), conclude that segmentation variablesin the generative translation model lead to overfit-ting while attaining higher likelihood of the train-ing data than the heuristic estimator.
Based on thisadvise (Moore and Quirk, 2007) exclude the latentsegmentation variables and opt for a heuristic train-ing procedure.
In this work we also start out from agenerative model with latent segmentation variables.However, we find out that concentrating the learningeffort on smoothing is crucial for good performance.For this, we devise ITG-based priors over segmenta-tions and employ a penalized version of Deleted Es-timation working with EM at its core.
The fact thatour results (at least) match the heuristic estimates ona reasonably sized data set (947k parallel sentencepairs) is rather encouraging.The work in (Zhang et al, 2008) has a simi-lar flavor to our work, yet the two differ substan-tially.
Both depart from Maximum-Likelihood to-wards non-overfitting estimators.
Where Zhang et alchoose for sparse priors (leading to sharp phrase dis-tributions) and put the smoothing burden on the ITGrule parameters and a pruning strategy, we choosefor a prior over segmentations determined by theITG derivation space and smooth the MLE directlywith a penalized version of Deleted Estimation.
Itremains to be seen how the two biases compare toone another on the same task.There are various strands of future research.Firstly, we plan to explore our estimator on otherlanguage pairs in order to obtain more evidence onits behavior.
Secondly, as (Blunsom et al, 2008)show, marginalizing out the different segmentationsduring decoding leads to improved performance.
Weplan to build our own decoder (based on ITG) wheredifferent ideas can be tested including tractable waysfor achieving a marginalization effect.
Apart from anew decoder, it will be worthwhile adapting the priorprobability in our model to allow for consistent es-timation.
Finally, it would be interesting to studyproperties of the penalized Deleted Estimation usedin this paper.Acknowledgments: Both authors are supportedby a VIDI grant (nr.
639.022.604) from The Nether-lands Organization for Scientific Research (NWO).David Chiang and Andy Way are acknowledged forstimulating discussions on machine translation andparsing.638ReferencesA.
Birch, Ch.
Callison-Burch, M. Osborne, and Ph.Koehn.
2006.
Constraining the phrase-based, jointprobability statistical translation model.
In Proceed-ings on the Workshop on Statistical Machine Trans-lation, pages 154?157.
Association for ComputationalLinguistics.P.
Blunsom, T. Cohn, and M. Osborne.
2008.
A discrim-inative latent variable model for statistical machinetranslation.
In Proceedings of ACL-08: HLT, pages200?208.
Association for Computational Linguistics.D.
Chiang.
2005a.
A hierarchical phrase-based modelfor statistical machine translation.
In In Proceedingsof ACL 2005, pages 263?270.D.
Chiang.
2005b.
An introduction to synchronousgrammars.
Technical report, Univeristy of Maryland.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the em al-gorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.J.
DeNero, D. Gillick, J. Zhang, and D. Klein.
2006.Why generative phrase models underperform surfaceheuristics.
In Proceedings on the Workshop on Sta-tistical Machine Translation, pages 31?38, New YorkCity.
Association for Computational Linguistics.R.O.
Duda, P.E.
Hart, and D.G.
Stork.
2001.
PatternClassification.
John Wiley & Sons, NY, USA.J.T.
Goodman.
1998.
Parsing Inside-Out.
PhD thesis,Departement of Computer Science, Harvard Univer-sity, Cambridge, Massachusetts.T.
Hastie, R. Tibshirani, and J. H. Friedman.
2001.
TheElements of Statistical Learning.
Springer.H.
Hoang and Ph.
Koehn.
2008.
Design of the moses de-coder for statistical machine translation.
In ACL Work-shop on Software engineering, testing, and quality as-surance for NLP 2008.L.
Huang, H. Zhang, D. Gildea, and K. Knight.2008.
Binarization of synchronous context-freegrammars.
Submitted to Computational Linguistics.http://www.cis.upenn.edu/ lhuang3/opt.pdf.F.
Jelinek and R. L. Mercer.
1980.
Interpolated estima-tion of markov source parameters from sparse data.
InIn Proceedings of the Workshop on Pattern Recogni-tion in Practice.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In HLT-NAACL.K.
Lari and S.J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer, Speech and Language, 4:35?56.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proceedings of Empirical methods in natural lan-guage processing, pages 133?139.
Association forComputational Linguistics.R.
Moore and Ch.
Quirk.
2007.
An iteratively-trainedsegmentation-free phrase translation model for statisti-cal machine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages112?119, Prague, Czech Republic.
Association forComputational Linguistics.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30(4):417?449.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In ACL, pages 311?318.K.
Sima?an and L. Buratto.
2003.
Backoff Parame-ter Estimation for the DOP Model.
In H. BlockeelN.
Lavra ?C, D. Gamberger and L. Todorovski, editors,Proceedings of the 14th European Conference on Ma-chine Learning (ECML?03), Lecture Notes in Artifi-cial Intelligence (LNAI 2837), pages 373?384, Cavtat-Dubrovnik, Croatia.
Springer.K.
Sima?an.
2002.
Computational complexity of proba-bilistic disambiguation.
Grammars, 5(2):125?151.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403.D.H.
Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In Matthias Jarke, JanaKoehler, and Gerhard Lakemeyer, editors, KI 2002:Advances in Artificial Intelligence, 25th Annual Ger-man Conference on AI (KI 2002), volume 2479 ofLecture Notes in Computer Science, pages 18?32.Springer.H.
Zhang, L. Huang, D. Gildea, and K. Knight.
2006.Synchronous binarization for machine translation.
InHLT-NAACL.H.
Zhang, Ch.
Quirk, R. C. Moore, and D. Gildea.2008.
Bayesian learning of non-compositional phraseswith synchronous parsing.
In Proceedings of ACL-08:HLT, pages 97?105, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.A.
Zollmann and K. Sima?an.
2006.
An efficient andconsistent estimator for data-oriented parsing.
Journalof Automata, Languages and Combinatorics (JALC),10 (2005) Number 2/3:367?388.639
