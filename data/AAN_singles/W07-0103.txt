Proceedings of the Workshop on Computational Approaches to Figurative Language, pages 13?20,Rochester, NY, April 26, 2007. c?2007 Association for Computational LinguisticsHunting Elusive Metaphors Using Lexical ResourcesSaisuresh Krishnakumaran?Computer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, WI 53706ksai@cs.wisc.eduXiaojin ZhuComputer Sciences DepartmentUniversity of Wisconsin-MadisonMadison, WI 53706jerryzhu@cs.wisc.eduAbstractIn this paper we propose algorithmsto automatically classify sentences intometaphoric or normal usages.
Our algo-rithms only need the WordNet and bigramcounts, and does not require training.
Wepresent empirical results on a test set de-rived from the Master Metaphor List.
Wealso discuss issues that make classificationof metaphors a tough problem in general.1 IntroductionMetaphor is an interesting figure of speech whichexpresses an analogy between two seemingly un-related concepts.
Metaphoric usages enhance theattributes of the source concept by comparing itwith the attributes of the target concept.
Abstrac-tions and enormously complex situations are rou-tinely understood via metaphors (Lakoff and John-son, 1980).
Metaphors begin their lives as NovelPoetic Creations with marked rhetoric effects whosecomprehension requires special imaginative leap.As time goes by, they become part of general useand their comprehension becomes automatic andidiomatic and rhetoric effect is dulled (Nunberg,1987).
We term such metaphors whose idiomatic ef-fects are dulled because of common usage as deadmetaphors while metaphors with novel usages aslive metaphors.
In this paper we are interested onlyin identifying live metaphors.?
The first author is currently affiliated with Google Inc,Mountain View, CA.Metaphors have interesting applications in manyNLP problems like machine translation, text sum-marization, information retrieval and question an-swering.
Consider the task of summarizing a parablewhich is a metaphoric story with a moral.
The bestsummary of a parable is the moral.
Paraphrasing ametaphoric passage like a parable is difficult with-out understanding the metaphoric uses.
The per-formance of the conventional summarizing systemswill be ineffective because they cannot identify suchmetaphoric usages.
Also it is easy to create noveland interesting uses of metaphors as long as one con-cept is explained in terms of another concept.
Theperformance of machine translation systems will beaffected in such cases especially if they have not en-countered such metaphoric uses before.Metaphor identification in text documents is,however, complicated by issues including con-text sensitiveness, emergence of novel metaphoricforms, and the need for semantic knowledge aboutthe sentences.
Metaphoric appeal differs across lan-guage or people?s prior exposure to such usages.
Inaddition, as (Gibbs, 1984) points out, literal and fig-urative expressions are end points of a single con-tinuum along which metaphoricity and idiomaticityare situated, thereby making clear demarcation ofmetaphoric and normal usages fuzzy.We discuss many such issues that make the taskof classifying sentences into metaphoric or non-metaphoric difficult.
We then focuses on a sub-set of metaphoric usages involving the nouns in asentence.
In particular, we identify the subject-object, verb-noun and adjective-noun relationshipsin sentences and classify them as metaphoric or13non-metaphoric.
Extensions to other metaphorictypes will be part of future work.
Our algorithmsuse the hyponym relationship in WordNet (Fell-baum, 1998), and word bigram counts, to predict themetaphors.
In doing so we circumvent two issues:the absence of labeled training data, and the lack ofclear features that are indicative of metaphors.The paper is organized as follows.
Section 2presents interesting observations that were madeduring the initial survey, and presents examplesthat makes metaphor identification hard.
Sec-tion 3 discusses our main techniques for identifyingmetaphors in text documents.
Section 4 analyzes theeffect of the techniques.
Section 5 discusses relevantprior work in the area of metaphor processing andidentification.
Finally we conclude in Section 6.2 Challenges in Metaphor IdentificationIn this section we present some issues that makemetaphor identification hard.2.1 Context SensitivitySome metaphoric usages are sensitive to the contextin which they occur.
For example, the followingsentence can act as a normal sentence as well as ametaphoric sentence.Men are animals.It is a normal sentence in a biology lecture becauseall human beings fall under the animal kingdom.However this is a metaphoric sentence in a socialconversation when it refers to animal qualities.
Alsothe word ?Men?
has two different senses in WordNetand hence it is necessary to disambiguate the sensesbased on the context.
Sense disambiguation is be-yond the scope of this paper.2.2 Pronoun ResolutionConsider the following sentence,This homework is a breeze.
The previousone was on calculus.
It was a tornado.The techniques we discuss in this paper can clas-sify the reference to ?breeze?
as metaphoric.
In or-der to correctly classify the reference to ?tornado?as metaphoric, however, the system needs to resolvethe reference to the pronoun ?It?.
Strictly speak-ing, this example might be solved without resolu-tion because any of the potential antecedents renderthe sentence metaphoric, but in general resolution isnecessary.2.3 Word UsagesConsider the following two sentences,He is a Gandhi.
vs.
He is Gandhi.The first sentence is a metaphor which attributes thequalities of Gandhi to the actor, while the secondsentence is a normal one.
Here the article ?a?
dis-tinguishes the first sentence from the second.
Sim-ilarly, in the following example, the phrase ?amongmen?
helps in making the second usage metaphoric.He is a king.
vs.
He is a king among men.A comprehensive list of such uses are not known andincorporating all such grammatical features wouldmake the system quite complex.2.4 Parser IssuesThe techniques that we propose work on the parsedsentences.
Hence the accuracy of our technique ishighly dependent on the accuracy of the parser.2.5 Metaphoric Usages in WordNetSome metaphoric senses of nouns are already part ofthe WordNet.He is a wolf.The metaphoric sense of ?wolf?
is directly men-tioned in the WordNet.
We call such usages as ?deadmetaphors?
because they are so common and are al-ready part of the lexicon.
In this paper we are inter-ested in identifying only novel usages of metaphors.3 Noun-Form MetaphorsWe restrict ourselves to metaphoric usages involvingnouns.
In particular, we study the effect of verbs andadjectives on the nouns in a sentence.
We categorizethe verb-noun relationship in sentences as Type I andType II based on the verb.
We call the adjective-noun relationship as Type III, see Table 1.For Type I, the verb is one of the ?be?
form verbslike ?is?, ?are?, ?am?, ?was?, etc.
An example of TypeI form metaphor is14Table 1: TerminologySentence Type RelationshipType I Subject IS-A ObjectType II Verb acting on Noun(verb not ?be?
)Type III Adjective acting on NounHe is a brave lion.An example of Type II form metaphor isHe planted good ideas in their minds.An example for Type III form metaphor isHe has a fertile imagination.We use two different approaches for Type I vs.Types II, III.
In Type I form we are interested inthe relationship between the subject and the object.We use a hyponym heuristic.
In Types II and III,we are interested in the subject-verb, verb-object,or adjective-noun relations.
We use hyponym to-gether with word co-occurrence information, in thiscase bigrams from the Web 1T corpus (Brants andFranz, 2006).
Sections 3.1 and 3.2 discuss the twoalgorithms, respectively.
We use a parser (Klein andManning, 2003) to obtain the relationships betweennouns, verbs and adjectives in a sentence.3.1 Identifying Type I metaphorsWe identify the WordNet hyponym relationship (orthe lack thereof) between the subject and the objectin a Type I sentence.
We classify the sentence asmetaphoric, if the subject and object does not havea hyponym relation.
A hyponym relation exists be-tween a pair of words if and only if one word is asubclass of another word.
We motivate this idea us-ing some examples.
Let us consider a normal sen-tence with a subject-object relationship governed bya ?be?
form verb, ?is?.A lion is a wild animal.The subject-verb-object relationship of this normalsentence is shown in Figure 1.The subject and the object in the above example isgoverned by ?IS-A?
relationship.
Thus, Lion ?IS-A?type of animal.
The ?IS-A?
relationship is capturedFigure 1: The Subject-Verb-Object relationship for?A lion is a wild animal.
?as the ?hyponym?
relationship in WordNet, where?Lion?
is the hyponym of ?animal?.
Consider anotherexample,He is a scientist.Here the object ?scientist?
is the occupation of thesubject ?He?, which we change to ?person?.
?Sci-entist?
is a hyponym of ?person?
in WordNet.
Theabove two examples show that we expect a subject-object hyponym relation for normal Type I relations.On the other hand, consider a metaphoric example inType I form,All the world?s a stage.- William ShakespeareThe subject-verb-object relationship is representedby Figure 2.Figure 2: The Subject-Verb-Object relationship for?All the world is a stage.
?There is a subject-object relation between ?World?and ?Stage?, but they do not hold a hyponym re-lation in WordNet.
This is an important observa-tion which we use in classifying relationships of this15form.
Consider another example with complex sen-tences,Men are April when they woo, Decemberwhen they wed. Maids are May whenthey are maids, but the sky changeswhen they are wives.-Shakespeare?s ?As You Like It?.In this case, there are two explicit subject-objectrelations, namely Men-April and Maids-May.
TheWordNet hyponym relation does not exist betweeneither pair.From the examples considered above, it is seemsthat when a hyponym relation exists between thesubject and the object, the relationship is normal,and metaphoric otherwise.
The effectiveness of thisapproach is analyzed in detail in Section 4.
Thepseudo code for classifying Type I relations is givenbelow:1.
Parse the sentences and get al R ?
{subject,be, object} relations in those sentences.2.
for each relation Rsub,objif Hyponym(sub,obj) = truethen Rsub,obj is normal usageelse Rsub,obj is a metaphoric relation3.
All sentences with at least one metaphoric rela-tion is classified as metaphoric.3.2 Identifying Type II and Type III metaphorsWe use a two dimensional V/A-N co-occurrence ma-trix, in addition to WordNet, for detecting Type IIand Type III metaphors.
V/A-N matrix stands forVerb/Adjective-Noun matrix, which is a two dimen-sional matrix with verbs or adjectives along one di-mension, and nouns along the other.
The entriesare co-occurrence frequency of the word pair, fromwhich we may estimate the conditional probabil-ity p(wn|w) for a noun wn and a verb or adjec-tive w. Ideally the matrix should be constructedfrom a parsed corpus, so that we can identify V/A-N pairs from their syntactic roles.
However pars-ing a large corpus would be prohibitively expensive.As a practical approximation, we use bigram countsfrom the Web 1T corpus (Brants and Franz, 2006).Web 1T corpus consists of English word n-grams(up to 5-grams) generated from approximately 1 tril-lion word tokens of text from public Web pages.
Inthis paper we use the bigram data in which a nounfollows either a verb or an adjective.
We note thatthis approximation thus misses, for example, the pair(plant, idea) in phrases like ?plant an idea?.
Nonethe-less, the hope is that the corpus makes it up by sheersize.3.2.1 Type II metaphorsWe discuss the metaphoric relationship betweena verb-noun pair (wv, wn).
The idea is that if nei-ther wn nor its hyponyms or hypernyms co-occurfrequently with wv, then the pair is a novel usage,and we classify the pair as metaphoric.
To this end,we estimate the conditional probability p(wh|wv) =count(wv, wh)/count(wv) from the V/A-N matrix,where wh is wn itself, or one of its hyponyms / hy-pernyms.
If at least one of these wh has high enoughconditional probability as determined by a thresh-old, we classify it as normal usage, and metaphoricotherwise.
Consider the following exampleHe planted good ideas in their minds.The verb ?planted?
acts on the noun ?ideas?
andmakes the sentence metaphoric.
In our corpus theobjects that occur more frequently with the verb?planted?
are ?trees?, ?bomb?
and ?wheat?, etc.
Nei-ther the noun ?ideas?
nor its hyponyms / hypernymsoccurs frequently enough with ?planted?.
Hence wepredict this verb-object relationship as metaphoric.The pseudo code for classifying Type II metaphorsis given below:1.
Parse the sentences and obtain all R ?
{verb,noun} relations in those sentences.2.
for each relation Rverb,nounSort all nouns w in the vocabulary by de-creasing p(w|verb).
Take the smallest set oftop k nouns whose conditional probability sum?
threshold T .if ?wh such that wh is related to noun bythe hyponym relation in WordNet, and wh ?top k words above,then Rverb,noun is normal usageelse Rverb,noun is a Type II metaphoric re-lation163.
All sentences with at least one metaphoric rela-tionship is classified as a metaphor.3.2.2 Type III metaphorsThe technique for detecting the Type IIImetaphors is the same as the technique for detectingthe Type II metaphors except that it operates on dif-ferent relationship.
Here we compare the Adjective-Noun relationship instead of the Verb-Noun rela-tionship.
For example,He has a fertile imagination.Here the adjective ?fertile?
acts on the noun ?imagi-nation?
to make it metaphoric.
The nouns that occurfrequently with the ?fertile?
in our corpus are ?soil?,?land?, ?territory?, and ?plains?, etc.
Comparison ofthe WordNet hierarchies of the noun ?imagination?with each of these nouns will show that there doesnot exist any hyponym relation between ?imagina-tion?
and any of these nouns.
Hence we classifythem as metaphors.
As another example,TV is an idiot box.The adjective ?idiot?
qualifies nouns related to peo-ple such as ?boy?, ?man?, etc.
that are unrelated tothe noun ?box?.
Thus we classify it as a Type IIImetaphor.4 Experimental ResultsWe experimented with the Berkeley MasterMetaphor List (Lakoff and Johnson, 1980) tocompute the performance of our techniques.
TheBerkeley Master Metaphor List is a collection ofnearly 1728 unique sentences and phrases.
Wecorrected some typos and spelling errors in theMaster list and expanded phrases to completesentences.
The list has many metaphoric useswhich has become very common usages in today?sstandards, and thus no longer have any rhetoriceffects.
Therefore, we manually label the sentencesin the Master List into 789 ?live metaphors?
andthe remaining ones ?dead metaphors?
as the groundtruth1.Table 2 shows the initial performance of theType I algorithm.
There are 129 sentences in the1Our processed and labeled dataset is available athttp://www.cs.wisc.edu/?ksai/publications/2007/HLT_NAACL_metaphors/metaphors.htmlMaster List that contain subject-be-object form.
Ouralgorithm has a precision of 70% and a recall of 61%with respect to the live/dead labels.
Note that al-though the accuracy is 58%, the algorithm is bet-ter than a random classification in terms of precisionand recall.
One thing to note is that our negativeexamples are (subjectively labeled) dead metaphors.We thus expect the task to be harder than with ran-dom non-metaphoric sentences.
Another point tonote here is that the live/dead labels are on sentencesand not on particular phrases with type I relations.A sentence can contain more than one phrases withvarious types.
Therefore this result does not give acomplete picture of our algorithm.Table 2: Type I PerformancePredicted as Predicted asMetaphoric NormalAnnotated as live 50 32Annotated as dead 22 25A few interesting metaphors detected by our algo-rithm are as follows:Lawyers are real sharks.Smog pollution is an environmentalmalaise.Some false negatives are due to phrases qualifyingthe object of the sentence as in the following exam-ple,He is a budding artist.There is a Type I relation in this sentence becausethe subject ?He?
and the object ?artist?
are relatedby the ?be?
form verb ?is?.
In this case, the Type Ialgorithm compares the hyponyms relation between?person?
and ?artist?
and declares it as a normal sen-tence.
However the adjective ?budding?
adds TypeIII figurative meaning to this sentence.
Therefore al-though the Type I relation is normal, there are otherfeatures in the sentences that make it metaphoric.We observed that most of false negatives that arewrongly classified because of the above reason havepronoun subject like ?he?, ?she?
etc.Another major source of issue is the occurrencesof pronoun ?it?
which is hard to resolve.
We replacedit by ?entity?, which is the root of WordNet, when17comparing the hyponyms.
?Entity?
matches the hy-ponym relation with any other noun and hence allthese sentences with ?it?
as the subject are classifiedas normal sentences.Table 3: Type I Performance for sentences with non-pronoun subjectPredicted as Predicted asMetaphoric NormalAnnotated as live 40 1Annotated as dead 19 4Table 3 shows the performance of our Type I al-gorithm for sentences with non-pronoun subjects.It clearly shows that the performance in Table 2 isaffected by sentences with pronoun subjects as ex-plained in the earlier paragraphs.In some cases, prepositional phrases affects theperformance of our algorithm.
Consider the follow-ing example,He is the child of evil.Here the phrase ?child of evil?
is metaphoric.
But theparser identifies a subject-be-object relationship be-tween ?He?
and ?child?
and our algorithm comparesthe hyponym relation between ?person?
and ?child?and declares it as a normal sentence.Our current algorithm does not deal with caseslike the following exampleThe customer is a scientist.
vs.
The cus-tomer is king.Since there is no direct hyponym relation betweenscientist/king with customer we declare both thesesentences as metaphors although only the latter is.Unlike the algorithm for Type I, there is a thresh-old T to be set for Type II and III algorithm.
Bychanging T , we are able to plot a precision recallcurve.
Figure 3 and figure 4 show the precision re-call graph for Type II and Type III relations respec-tively.
Figure 5 shows the overall precision recallgraph for all three types put together.False positives in Type II and Type III were dueto very general verbs and adjectives.
These verbsand adjectives can occur with a large number ofnouns, and tend to produce low conditional prob-abilities even for normal nouns.
Thereby they are0.40.450.50.550.60.650.70.750.80  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallType II Precision  Recall graphType IIFigure 3: Precision Recall curve for Type II rela-tions.0.350.40.450.50.550.60.650.70  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallType III Precision  Recall graphType IIIFigure 4: Precision Recall curve for Type III rela-tions.often mistakenly classified as metaphoric relations.We expect the performance to improve if these gen-eral verbs and adjectives are handled properly.
Somegeneral verbs include ?gave?, ?made?, ?has?, etc., andsimilarity general adjectives include ?new?, ?good?,?many?, ?more?, etc.
The plot for Type III is morerandom.Most errors can be attributed to some of the fol-lowing reasons:?
As mentioned in the challenges section, theparser is not very accurate.
For example,They battled each other over thechess board every week.Here the parser identifies the verb-object rela-tion as ( battled , week ), which is not correct.180.450.50.550.60.650.70.750.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallCombined Precision  Recall graphAll types combinedFigure 5: Overall Precision Recall curve for all threetypes combined.?
Pronoun resolution: As discussed earlier, thepronoun ?it?
is not resolved and hence they in-troduce additional source of errors.?
Manual annotations could be wrong.
In our ex-periment we have used only two annotators, buthaving more would have increased the confi-dence in the labels.?
Many of the verb-noun forms are most natu-rally captured by trigrams instead of bigram.For example, (developed , attachment) mostlikely occurs in a corpus as ?developed an at-tachment?
or ?developed the attachment?.
Ourbigram approach can fail here.?
Sense disambiguation: We don?t disambiguatesenses while comparing the WordNet relations.This increases our false negatives.?
Also as mentioned earlier, the labels are onsentences and not on the typed relationships.Therefore even though a sentence has one ormore of the noun form types, those may benormal relationships while the whole sentencemay be metaphoric because of other types.Note, however, that some of these mismatchesare corrected for the ?All types combined?
re-sult.5 Related WorkThere has been a long history of research inmetaphors.
We briefly review some of them here.One thing that sets our work apart is that most pre-vious literatures in this area tend to give little em-pirical evaluation of their approaches.
In contrast, inthis study we provide detailed analysis of the effec-tiveness of our approaches.
(Fass and Wilks, 1983) proposes the use of pref-erence semantics for metaphor recognition.
Tech-niques for automatically detecting selections prefer-ences have been discussed in (McCarthy and Car-rol, 2003) and (Resnik, 1997).
Type II and Type IIIapproaches discussed in this paper uses both theseideas for detecting live metaphors.
Fass (Fass, 1991)uses selectional preference violation technique todetect metaphors.
However they rely on hand-codeddeclarative knowledge bases.
Our technique de-pends only on WordNet and we use selection prefer-ence violation based on the knowledge learned fromthe bigram frequencies on the Web.Markert and Nissim (Markert and Nissim, 2002)presents a supervised classification algorithm for re-solving metonymy.
Metonymy is a closely relatedfigure of speech to metaphors where a word is sub-stituted by another with which it is associated.
Ex-ample,A pen is mightier than a sword.Here sword is a metonymy for war and pen is ametonymy for articles.
They use collocation, co-occurrence and grammatical features in their clas-sification algorithm.MetaBank (Martin, 1994) is a large knowledgebase of metaphors empirically collected.
The de-tection technique compares new sentences with thisknowledge base.
The accuracy is dependent on thecorrectness of the knowledge base and we expectthat some of these metaphors would be dead in thepresent context.
The techniques we discuss in thiswork will drastically reduce the need for manuallyconstructing such a large collection.Goatly (Goatly, 1997) proposes using analogymarkers such as ?like?, ?such as?, ?illustrated by?and lexical markers like ?literally?, ?illustrating?,?metaphorically?
etc.
These would be useful foridentifying simile and explicit metaphoric relationsbut not metaphors where the relation between thetarget concept and the source concept is not explicit.The CorMet system (Mason, 2004) dynamicallymines domain specific corpora to find less frequent19usages and identifies conceptual metaphors.
How-ever the system is limited to extracting only selec-tional preferences of verbs.
Verbal selectional pref-erence is the verb?s preference for the type of argu-ment it takes.Dolan (Dolan, 1995) uses the path and path lengthbetween words in the knowledge base derived fromlexical resources for interpreting the interrelation-ship between the component parts of a metaphor.The effectiveness of this technique relies on whetherthe metaphoric sense is encoded in the dictionar-ies.
This approach however will not be effective fornovel metaphoric usages that are not encoded in dic-tionaries.6 ConclusionIn this paper we show that we can use the hyponymrelation in WordNet and word co-occurrence infor-mation for detecting metaphoric uses in subject-object, verb-noun and adjective-noun relationships.According to (Cameron and Deignan, 2006), nonliteral expressions with relatively fixed forms andhighly specific semantics are over-represented in themetaphor literature in comparison to corpora occur-rences.
Therefore as part of future work we wouldbe studying the effect of our algorithms for naturallyoccurring text.
We are also interested in increasingthe confidence of the labels using more and diverseannotators and see how the techniques perform.
Thestudy can then be extended to incorporate the role ofprepositions in metaphoric uses.7 AcknowledgmentWe would like to thank our anonymous reviewers fortheir constructive suggestions that helped improvethis paper.
We would also like to thank Mr. Kr-ishna Kumaran Damodaran for annotating the Mas-ter Metaphor List.ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.
Linguistic Data Consortium, Philadelphia.Lynne Cameron and Alice Deignan.
2006.
The emer-gence of metaphor in discourse.
Applied Linguistics,27(4):671?690.William B. Dolan.
1995.
Metaphor as an emergentproperty of machine-readable dictionaries.
AAAI 1995Spring Symposium, 95(1):27?32.Dan Fass and Yorick Wilks.
1983.
Preference semantics,ill-formedness, and metaphor.
American Journal ofComputational Linguistics, 9(3):178?187.Dan Fass.
1991.
Met: A method for discriminatingmetonymy and metaphor by computer.
ComputationalLinguistics, 17(1):49?90.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Raymond Gibbs.
1984.
Literal meaning and psychologi-cal theory.
Cognitive Science, 8:275?304.Andrew Goatly.
1997.
The Language of Metaphors.Routledge,London.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
University of Chicago Press, Chicago, Illi-nois.Katja Markert and Malvina Nissim.
2002.
Metonymyresolution as a classification task.
In Proceedings ofACL-02 conference on Empirical Methods in NaturalLanguage Processing, pages 204?213.James H. Martin.
1994.
Metabank: a knowledge-baseof metaphoric language conventions.
ComputationalIntelligence, 10(2):134?149.Zachary J. Mason.
2004.
Cormet: A computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Diana McCarthy and John Carrol.
2003.
Disambiguat-ing nouns, verbs and adjectives using automaticallyacquired selectional preferences.
Computational Lin-guistics, 29(4):639?654.Geoffrey Nunberg.
1987.
Poetic and prosaic metaphors.In Proceedings of the 1987 workshop on Theoreticalissues in natural language processing, pages 198?201.Philip Resnik.
1997.
Selectional preferences and wordsense disambiguation.
In Proceedings of ACL SiglexWorkshop on Tagging Text with Lexical Semantics,Why, What and How?, Washington, D.C., pages 52?57.20
