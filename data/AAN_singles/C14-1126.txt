Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1331?1340, Dublin, Ireland, August 23-29 2014.Sentiment Classification with Graph Co-RegularizationGuangyou Zhou, Jun Zhao, and Daojian ZengNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences95 Zhongguancun East Road, Beijing 100190, China{gyzhou,jzhao,djzeng}@nlpr.ia.ac.cnAbstractSentiment classification aims to automatically predict sentiment polarity (e.g., positive or neg-ative) of user-generated sentiment data (e.g., reviews, blogs).
To obtain sentiment classifica-tion with high accuracy, supervised techniques require a large amount of manually labeled data.The labeling work can be time-consuming and expensive, which makes unsupervised (or semi-supervised) sentiment analysis essential for this application.
In this paper, we propose a novelalgorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from thegeometric perspective.
GNMTF assumes that if two words (or documents) are sufficiently closeto each other, they tend to share the same sentiment polarity.
To achieve this, we encode thegeometric information by constructing the nearest neighbor graphs, in conjunction with a non-negative matrix tri-factorization framework.
We derive an efficient algorithm for learning thefactorization, analyze its complexity, and provide proof of convergence.
Our empirical study ontwo open data sets validates that GNMTF can consistently improve the sentiment classificationaccuracy in comparison to the state-of-the-art methods.1 IntroductionRecently, sentiment classification has gained a wide interest in natural language processing (NLP) com-munity.
Methods for automatically classifying sentiments expressed in products and movie reviews canroughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis.
Super-vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,2002; Pang and Lee, 2008; Liu, 2012).
However, the performance of these methods relies on manuallylabeled training data.
In some cases, the labeling work may be time-consuming and expensive.
Thismotivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)paradigm.A traditional way to perform unsupervised sentiment analysis is the lexicon-based method (Turney,2002; Taboada et al., 2011).
Lexicon-based methods employ a sentiment lexicon to determine overallsentiment orientation of a document.
However, it is difficult to define a universally optimal sentimentlexicon to cover all words from different domains (Lu et al., 2011a).
Besides, most semi-automatedlexicon-based methods yield unsatisfactory lexicons, with either high coverage and low precision orvice versa (Ng et al., 2006).
Thus it is challenging for lexicon-based methods to accurately identifythe overall sentiment polarity of users generated sentiment data.
Recently, Li et al.
(2009) proposed aconstrained non-negative matrix tri-factorization (CNMTF) approach to sentiment classification, witha domain-independent sentiment lexicon as prior knowledge.
Experimental results show that CNMTFachieves state-of-the-art performance.From the geometric perspective, the data points (words or documents) may be sampled from a distribu-tion supported by a low-dimensional manifold embedded in a high-dimensional space (Cai et al., 2011).This geometric structure, meaning that two words (or documents) sufficiently close to each other tend toshare the same sentiment polarity, should be preserved during the matrix factorization.
Research studiesThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http:// creativecommons.org/licenses/by/4.0/1331have shown that learning performance can be significantly enhanced in many real applications (e.g., textmining, computer vision, etc.)
if the geometric structure is exploited (Roweis and Saul, 2000; Tenen-baum et al., 2000).
However, CNMTF fails to exploit the geometric structure, it is not clear whether thisgeometric information is useful for sentiment classification, which remains an under-explored area.
Thispaper is thus designed to fill the gap.In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF).
We construct two affinity graphs to encode the geometric information under-lying the word space and the document space, respectively.
Intuitively, if two words or documents aresufficiently close to each other, they tend to share the same sentiment polarity.
Taking these two graphsas co-regularization for the non-negative matrix tri-factorization, leading to the better sentiment polarityprediction which respects to the geometric structures of the word space and document space.
We also de-rive an efficient algorithm for learning the tri-factorization, analyze its complexity, and provide proof ofconvergence.
Empirical study on two open data sets shows encouraging results of the proposed methodin comparison to state-of-the-art methods.The remainder of this paper is organized as follows.
Section 2 introduces the basic concept of matrixtri-factorization.
Section 3 describes our graph co-regularized non-negative matrix tri-factorization (GN-MTF) for sentiment classification.
Section 4 presents the experimental results.
Section 5 introduces therelated work.
In section 6, we conclude the paper and discuss future research directions.2 Preliminaries2.1 Non-negative Matrix Tri-factorizationLi et al.
(2009) proposed a matrix factorization based framework for unsupervised (or semi-supervised)sentiment analysis.
The proposed framework is built on the orthogonal non-negative matrix tri-factorization (NMTF) (Ding et al., 2006).
In these models, a term-document matrixX = [x1, ?
?
?
,xn] ?Rm?nis approximated by three factor matrices that specify cluster labels for words and documents bysolving the following optimization problem:minU,H,V?0O =??X?UHVT?
?2F+ ?1??UTU?
I?
?2F+ ?2?
?VTV ?
I?
?2F(1)where ?1and ?2are the shrinkage regularization parameters, U = [u1, ?
?
?
,uk] ?
Rm?k+is the word-sentiment matrix, V = [v1, ?
?
?
,vn] ?
Rn?k+is the document-sentiment matrix, and k is the number ofsentiment classes for documents.
Our task is polarity sentiment classification (positive or negative), i.e.,k = 2.
For example,Vi1= 1 (orUi1= 1) represents that the sentiment polarity of document i (or wordi) is positive, andVi2= 1 (orUi2= 1) represents that the sentiment polarity of document i (or word i)is negative.
Vi?= 0 (orUi?= 0) represents unknown, i.e., the document i (or word i) is neither positiveor negative.
H ?
Rk?k+provides a condensed view of X; ?
?
?Fis the Frobenius norm and I is a k ?
kidentity matrix with all entries equal to 1.
Based on the shrinkage methodology, we can approximatelysatisfy the orthogonality constraints forU andV by preventing the second and third terms from gettingtoo large.2.2 Constrained NMTFLexical knowledge in the form of the polarity of words in the lexicon can be introduced in matrix tri-factorization.
By partially specifying word polarity viaU, the lexicon influences the sentiment predictionV over documents.
Following the literature (Li et al., 2009), let U0represent lexical prior knowledgeabout sentiment words in the lexicon, e.g., if word i is positive (U0)i1= 1 while if it is negative(U0)i2= 1, and if it does not exist in the lexicon (U0)i?= 0.
Li et al.
(2009) also investigated that wehad a few documents manually labeled for the purpose of capturing some domain-specific connotations.LetV0denote the manually labeled documents, if the document expresses positive sentiment (V0)ii= 1,and (V0)i2= 1 for negative sentiment.
Therefore, the semi-supervised learning with lexical knowledgecan be written as:minU,H,V?0O + ?Tr[(U?U0)TCu(U?U0)]+ ?Tr[(V ?V0)TCv(V ?V0)](2)1332where Tr(?)
denotes the trace of a matrix, ?
> 0 and ?
> 0 are the parameters which control thecontribution of lexical prior knowledge and manually labeled documents.
Cu?
{0, 1}m?mis a diagonalmatrix whose entry Cuii= 1 if the category of the i-th word is known and Cuii= 0 otherwise.
Cv?
{0, 1}n?nis a diagonal matrix whose entry Cvii= 1 if the category of the i-th document is labeled andCvii= 0 otherwise.3 Graph Co-regularized Non-negative Matrix Tri-factorizationIn this section, we introduce our proposed graph co-regularized non-negative matrix tri-factorization(GNMTF) algorithm which avoids this limitation by incorporating the geometrically based co-regularization.3.1 Model FormulationBased on the manifold assumption (Belkin and Niyogi, 2001), if two documents xiand xjare sufficientlyclose to each other in the intrinsic geometric of the documents distribution, then their sentiment polarityviand vjshould be close.
In order to model the geometric structure, we construct a document-documentgraphGv.
In the graph, nodes represent documents in the corpus and edges represent the affinity betweenthe documents.
The affinity matrixWv?
Rn?nof the graph Gvis defined asWvij={cos(xi,xj) if xi?
Np(xj) or xj?
Np(xi)0 otherwise(3)where Np(xi) represents the p-nearest neighbors of document xi.
Many matrices, e.g., 0-1 weighting,textual similarity and heat kernel weighting (Belkin and Niyogi, 2001), can be used to obtain nearestneighbors of a document, and further define the affinity matrix.
Since Wvijin our paper is only formeasuring the closeness, we only use the simple textual similarity and do not treat the different weightingschemes separately due to the limited space.
For further information, please refer to (Cai et al., 2011).Preserving the geometric structure in the document space is reduced to minimizing the following lossfunction:Rv=12n?i,j=1??vi?
vj?
?22Wvij=n?i=1vTiviDvii?n?i,j=1vTivjWvij= Tr(VTDvV)?
Tr(VTWvV) = Tr(VTLvV)(4)whereDv?
Rn?nis a diagonal matrix whose entries are column (or row, sinceDvis symmetric) sumsofWv,Dvii=?nj=1Wvij, and Lv= Dv?Wvis the Laplacian matrix (Chung, 1997) of the constructedgraph Gv.Similarly to document-document geometric structure, if two words wi= [xi1, ?
?
?
,xin] and wj=[xj1, ?
?
?
,xjn] are sufficiently close to each other in the intrinsic geometric of the words distribution,then their sentiment polarity uiand ujshould be close.
In order to model the geometric structure in theword space, we construct a word-word graph Gu.
In the graph, nodes represent distinct words and edgesrepresent the affinity between words.
The affinity matrixWu?
Rm?mof the graph Guis defined asWuij={cos(wi,wj) ifwi?
Np(wj) orwj?
Np(wi)0 otherwise(5)where Np(wj) represents the p-nearest neighbor of word wj.
Here, we represent a term wjas a docu-ment vector [xj1, ?
?
?
,xjn].
To measure the closeness of two words, a common way is to calculate thesimilarity of their vector representations.
Although there are several ways (e.g., co-occurrence infor-mation, semantic similarity computed by WordNet, Wikipedia, or search engine have been empiricallystudied in NLP literature (Hu et al., 2009)) to define the affinity matrixWu, we do not treat the differentways separately and leave this investigation for future work.Preserving the geometric structure in the word space is reduced to minimizing the following lossfunction:Ru=12m?i,j=1??ui?
uj?
?22Wuij= Tr(UTLuU) (6)1333where Lu= Du?Wuis the Laplacian matrix of the constructed graph Gu, and Du?
Rm?mis adiagonal matrix whose entries areDuii=?mj=1Wuij.Finally, we treat unsupervised (or semi-supervised) sentiment classification as a clustering problem,employing lexical prior knowledge and partial manually labeled data to guide the learning process.
More-over, we introduce the geometric structures from both document and word sides as co-regularization.Therefore, our proposed unsupervised (or semi-supervised) sentiment classification framework can bemathematically formulated as solving the following optimization problem:minU,H,V?0L =??X?UHVT?
?2F+ ?1??UTU?
I?
?2F+ ?2?
?VTV ?
I?
?2F+ ?Tr[(U?U0)TCu(U?U0)]+ ?Tr(UTLuU)+ ?Tr[(V ?V0)TCv(V ?V0)]+ ?Tr(VTLvV)(7)where ?
> 0 and ?
> 0 are parameters which control the contributions of document space and wordspace geometric information, respectively.
With the optimization results, the sentiment polarity of a newdocument xican be easily inferred by f(xi) = argmaxj?
{p, n}Vij.3.2 Learning AlgorithmWe present the solution to the GNMTF optimization problem in equation (7) as the following theorem.The theoretical aspects of the optimization are presented in the next subsection.Theorem 3.1.
Updating U, H and V using equations (8)?
(10) will monotonically decrease the objec-tive function in equation (7) until convergence.U?
U ?
[XVHT+ ?1U+ ?CuU0+ ?WuU][UHVTVHT+ ?1UUTU+ ?CuU+ ?DuU](8)H?
H ?[UTXV][UTUHVTV](9)V?
V ?
[XTUH+ ?2V + ?CvV0+ ?WvV][VHTUTUH+ ?2VVTV + ?CvV + ?DvV](10)where operator ?
is element-wise product and[?][?
]is element-wise division.Based on Theorem 3.1, we note that the multiplicative update rules given by equations (8)?
(10) areobtained by extending the updates of standard NMTF (Ding et al., 2006).
A number of techniques canbe used here to optimize the objective function in equation (7), such as alternating least squares (Kimand Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin,2007).
Nonetheless, the multiplicative updates derived in this paper has reasonably fast convergencebehavior as shown empirically in the experiments.3.3 Theoretical AnalysisIn this subsection, we give the theoretical analysis of the optimization, convergence and computationalcomplexity.
Without loss of generality, we only show the optimization ofU and formulate the Lagrangefunction with constraints as follows:L(U) =??X?UHVT?
?2F+ ?1??UTU?
I?
?2F+ ?Tr[(U?U0)TCu(U?U0)]+ Tr(?UT)(11)where ?
is the Lagrange multiplier for the nonnegative constraintU ?
0.The partial derivative of L(U) w.r.t.
U is?UL(U) = ?2XVHT+ 2UHVTVHT+ 2?1UUTU?
2?1U+ 2?CuU?
2?CuU0+ 2?DuU?
2?WuU+?1334Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ?
?U = 0, we canobtain?UL(U) ?U =[UHVTVHT+ ?1UUTU+ ?CuU+ ?DuU]?U?
[XVHT+ ?1U+ ?CuU0+ ?WuU]?U = 0This leads to the update rule in equation (8).
Following the similar derivations as shown above, wecan obtain the updating rules for all the other variables H and V in GNMTF optimization, as shown inequations (9) and (10).3.3.1 Convergence AnalysisIn this subsection, we prove the convergence of multiplicative updates given by equations (8)?(10).
Wefirst introduce the definition of auxiliary function as follows.Definition 3.1.
F(Y,Y?)
is an auxiliary function for L(Y) if L(Y) ?
F(Y,Y?)
and equality holds ifand only if L(Y) = F(Y,Y).Lemma 3.1.
(Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under theupdateY(t+1)= argminYF(Y,Y(t))Proof.
By Definition 3.1, L(Y(t+1)) ?
F(Y(t+1),Y(t)) ?
F(Y(t),Y(t)) = L(Y(t))Theorem 3.2.
Let functionF(Uij,U(t)ij) = L(U(t)ij) + L?
(U(t)ij)(Uij?U(t)ij)+[UHVTVHT+ ?1UUTU+ ?CuU+ ?DuU]ijUij(Uij?U(t)ij)(12)be a proper auxiliary function for L(Uij), where L?
(Uij) = [?UL(U)]ijis the first-order derivativesof L(Uij) with respect toUij.Theorem 3.2 can be proved similarly to (Ding et al., 2006).
Due to limited space, we omit the detailsof the validation.
Based on Lemmas 3.1 and Theorem 3.2, the update rule for U can be obtained byminimizing F(U(t+1)ij,U(t)ij).
When setting ?U(t+1)ijF(U(t+1)ij,U(t)ij), we can obtainU(t+1)ij= U(t)ij[XVHT+ ?1U+ ?CuU0+ ?WuU]ij[UHVTVHT+ ?1UUTU+ ?CuU+ ?DuU]ijBy Lemma 3.1 and Theorem 3.2, we have L(U(0)) = F(U(0),U(0)) ?
F(U(1),U(0)) ?F(U(1),U(1)) = L(U(1)) ?
?
?
?
?
L(U(Iter)), where Iter denotes the number of iteration number.Therefore, U is monotonically decreasing.
Since the objective function L is lower bounded by 0, thecorrectness and convergence of Theorem 3.1 is validated.3.3.2 Time Complexity AnalysisIn this subsection, we discuss the time computational complexity of the proposed algorithm GNMTF.Besides expressing the complexity of the algorithm using big O notation, we also count the number ofarithmetic operations to provide more details about running time.
We show the results in Table 1, wherem ?
k and n ?
k.Based on the updating rules summarized in Theorem 3.1, it it not hard to count the arithmetic operatorsof each iteration in GNMTF.
It is important to note thatCuis a diagonal matrix, the nonzero elements oneach row of Cuis 1.
Thus, we only need zero addition and mk multiplications to compute CuU.
Simi-larly, forCuU0,CvV,CvV0,DuU andDvV, we also only need zero addition and mk multiplicationsfor each of them.
Besides, we also note thatWuis a sparse matrix, if we use a p-nearest neighbor graph,the average nonzero elements on each row of Wuis p. Thus, we only need mpk additions and mpkmultiplications to compute WuU.
Similarly, for WvV, we need the same operation counts as WuU.Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates thenbecomes O(Iter ?
mnk).
Therefore, the overall running time of GNMTF is similar to the standardNMTF and CNMTF.1335addition multiplication division overallGNMTF:U 2k3+ (2m+ n)k2+m(n+ p)k 2k3+ (2m+ n)k2+m(n+ p+ 7)k mk O(mnk)GNMTF:H 2k3+ (m+ n+ 2)k2+mnk 2k3+ (m+ n+ 1)k2+mnk k2O(mnk)GNMTF:V 2k3+ (2n+m)k2+ n(m+ p)k 2k3+ (2n+m)k2+ n(m+ p+ 7)k nk O(mnk)Table 1: Computational operation counts for each iteration in GNMTF.4 Experiments4.1 Data SetsSentiment classification has been extensively studied in the literature.
Among these, a large majorityproposed experiments performed on the benchmarks made of Movies Reviews (Pang et al., 2002) andAmazon products (Blitzer et al., 2007).Movies data This data set has been widely used for sentiment analysis in the literature (Pang etal., 2002), which consists of 1000 positive and 1000 negative reviews drawn from the IMDB archive ofrec.arts.movies.reviews.newsgroups.Amazon data This data set is heterogeneous, heavily unbalanced and large-scale, a smaller ver-sion has been released.
The reduced data set contains 4 product types: Kitchen, Books, DVDs, andElectronics (Blitzer et al., 2007).
There are 4000 positive and 4000 negative reviews.1For these two data sets, we select 8000 words with highest document-frequency to generate the vo-cabulary.
Stopwords2are removed and a normalized term-frequency representation is used.
In order toconstruct the lexical prior knowledge matrixU0, we use the sentiment lexicon generated by (Hu and Liu,2004).
It contains 2,006 positive words (e.g., ?beautiful?)
and 4,783 negative words (e.g., ?upset?
).4.2 Unsupervised Sentiment ClassificationOur first experiment is to explore the benefits of incorporating the geometric information in the unsu-pervised paradigm (that is Cv= 0).
Therefore, the third part in equation (7) will be ignored.
For thisunsupervised paradigm of GNMTF, we empirically set ?
= ?
= ?
= 1, ?1= ?2= 1, Iter = 100 andrun GNMTF 10 repeated times to remove any randomness caused by the random initialization.
Due tolimited space, we do not present the impacts of the parameters on the learning model.
Now we compareour proposed GNMTF with the following four categories of methods:(1) Lexicon-Based Methods (LBM in short): Taboada et al.
(2011) proposed to incorporate intensifi-cation and negation to refine the sentiment score for each document.
This is the state-of-the-art lexicon-based method for unsupervised sentiment classification.
(2) Document Clustering Methods: We choose the most representative cluster methods, K-means,NMTF, Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clusteringmethod (ECC) (Cho et al., 2004).
We set the number of clusters as two in these methods.
Note that allthese methods do not make use of the sentiment lexicon.
(3) Constrained NMTF (CNMTF in short): Li et al.
(2009) incorporated the sentiment lexicon intoNMTF as a domain-independent prior constraint.
(4) Graph co-regularized Non-negative Matrix Tri-factorization (GNMTF in short): It is a new algo-rithm proposed in this paper.
We use cosine similarity for constructing the p-nearest neighbor graph forits simplicity.
The number of nearest neighbor p is set to 10 empirically both on document and wordspaces.4.2.1 Sentiment Classification ResultsThe experimental results are reported in Table 2.
We perform a significant test, i.e., a t-test with a defaultsignificant level of 0.05.
From Table 2, we can see that (1) Both CNMTF and GNMTF consider thelexical prior knowledge from off-the-shelf sentiment lexicon and achieve better performance than NMTF.This suggests the importance of the lexical prior knowledge in learning the sentiment classification (row1The data set can be freely downloaded from http://www.cs.jhu.edu/ mdredze/datasets/sentiment/.2http://truereader.com/manuals/onix/stopwords1.html1336# Methods Movies Amazon1 LBM 0.632 0.5802 K-means 0.543 (-8.9%) 0.535 (-4.5%)3 NMTF 0.561 (-7.1%) 0.547 (-3.3%)4 ECC 0.678 (+4.6%) 0.642 (+6.2%)5 ITCC 0.714 (+8.2%) 0.655 (+7.5%)6 CNMTF 0.695 (+6.3%) 0.658 (+7.8%)7 GNMTF 0.736 (+10.4%) 0.705 (+12.5%)Table 2: Sentiment classification accuracy of unsupervised paradigm on the data sets.
Improvements ofK-means, NMTF, ITCC, ECC, CNMTF and GNMTF over baseline LBM are shown in parentheses.0 20 40 60 80 1000.20.250.30.350.40.450.5(a) Movies dataObjective functionvalue0 20 40 60 80 1000.460.480.50.520.540.560.580.60.620.640.66(b) Amazon dataObjective functionvalueFigure 1: Convergence curves of GNMTF on both data sets.3 vs. row 6 and row 7); (2) Regardless of the data sets, our GNMTF significantly outperforms state-of-the-art CNMTF and achieves the best performance.
This shows the superiority of geometric informationand graph co-regularization framework (row 4 vs. row 5, the improvements are statistically significant atp < 0.05).4.2.2 Convergence BehaviorIn subsection 3.3.1, we have shown that the multiplicative updates given by equations (8)?
(10) areconvergent.
Here, we empirically show the convergence behavior of GNMTF.Figure 1 shows the convergence curves of GNMTF on Movies and Amazon data sets.
From the figure,y-axis is the value of objective function and x-axis denotes the iteration number.
We can see that themultiplicative updates for GNMTF converge very fast, usually within 50 iterations.4.3 Semi-supervised Sentiment ClassificationIn this subsection, we describe our proposed GNMTF with a few labeled documents.
For this semi-supervised paradigm of GNMTF, we empirically set Iter = 100, ?1= ?2= 2, ?
= ?
= ?
= ?
= 1 andp = 10 on document and word spaces and also run 10 repeated times to remove any randomness causedby the random initialization.
Due to limited space, we do not give an in-depth parameter analysis.
ForCNMTF, we set ?
= ?
= 1 for fair comparison.
We also compare our proposed GNMTF with somerepresentative semi-supervised approaches described in (Li et al., 2009): (1) Semi-supervised learningwith local and global consistency (Consistency Method in short) (Zhou et al., 2004); (2) Semi-supervisedlearning using gaussian fields and harmonic functions (GFHF in short) (Zhu et al., 2003).
Besides,we also compare the results of our proposed GNMTF with the representative supervised classificationmethod: support vector machine (SVM), which has been widely used in sentiment classification (Panget al., 2002).The results are presented in Figure 2.
From the figure, we can see that GNMTF outperforms othermethods over the entire range of number of labeled documents on both data sets.
By this observation,we can conclude that taking the geometric information can still improve the sentiment classificationaccuracy in semi-supervised paradigm.13370.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.450.50.550.60.650.70.750.8(a) Movies dataSentimentclassification accuracySVMConsistency MethodGFHFCNMTFGNMTF 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.550.60.650.70.750.80.85(b) Amazon dataSentimentclassification accuracySVMConsistency MethodGFHFCNMTFGNMTFFigure 2: Sentiment classification accuracy vs. different percentage of labeled documents, where x-axisdenotes the number of documents labeled as a fraction of the original labeled documents.5 Related WorkSentiment classification has gained widely interest in NLP community, we point the readers to recentbooks (Pang and Lee, 2008; Liu, 2012) for an in-depth survey of literature on sentiment analysis.Methods for automatically classifying sentiments expressed in products and movie reviews canroughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis.
Super-vised techniques have been proved promising and widely used in sentiment classification (Pang et al.,2002; Pang and Lee, 2008; Liu, 2012).
However, the performance of these methods relies on manuallylabeled training data.
In some cases, the labeling work may be time-consuming and expensive.
Thismotivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised)paradigm.The most representative way to perform semi-supervised paradigm is to employ partial labeled data toguide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009;Li et al., 2011).
However, we do not have any labeled data at hand in many situations, which makesthe unsupervised paradigm possible.
The most representative way to perform unsupervised paradigmis to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al., 2011)or learn sentiment orientation via a matrix factorization clustering framework (Li et al., 2009; ?
; Huet al., 2013).
In contrast, we perform sentiment classification with the different model formulation andlearning algorithm, which considers both word-level and document-level sentiment-related contextualinformation (e.g., the neighboring words or documents tend to share the same sentiment polarity) intoa unified framework.
The proposed framework makes use of the valuable geometric information tocompensate the problem of lack of labeled data for sentiment classification.
In addition, some researchersalso explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Pengand Park, 2013) and question answering (Zhou et al., 2013)Besides, many studies address some other aspects of sentiment analysis, such as cross-domain senti-ment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al., 2011; Glorotet al., 2011), cross-lingual sentiment classification (Wan, 2009; Lu et al., 2011b; Meng et al., 2012) andimbalanced sentiment classification (Li et al., 2011), which are out of scope of this paper.6 Conclusion and Future WorkIn this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from a geometric perspective.
GNMTF assumes that if two words (or docu-ments) are sufficiently close to each other, they tend to share the same sentiment polarity.
To achievethis, we encode the geometric information by constructing the nearest neighbor graphs, in conjunctionwith a non-negative matrix tri-factorization framework.
We derive an efficient algorithm for learningthe factorization, analyze its complexity, and provide proof of convergence.
Our empirical study on twoopen data sets validates that GNMTF can consistently improve the sentiment classification accuracy incomparison to state-of-the-art methods.1338There are some ways in which this research could be continued.
First, some other ways should beconsidered to construct the graphs (e.g., hyperlinks between documents, synonyms or co-occurrencesbetween words).
Second, we will try to extend the proposed framework for other aspects of sentimentanalysis, such as cross-domain or cross-lingual settings.AcknowledgmentsThis work was supported by the National Natural Science Foundation of China (No.
61303180 andNo.
61272332), the Beijing Natural Science Foundation (No.
4144087), CCF Opening Project of Chi-nese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund.
We thank theanonymous reviewers for their insightful comments.ReferencesM.
Belkin and P. Niyogi.
2001.
Laplacian eigenmaps and spectral techniques for embedding and clustering.
InProceedings of NIPS, pages 585-591.J.
Blitzer, M. Dredze and F. Pereira.
2007.
Biographies, bollywood, boom-boxes and blenders: domain adaptationfor sentiment classification.
In Proceedings of ACL, pages 440-447.D.
Bollegala, D. Weir, and J. Carroll.
2011.
Using multiples sources to construct a sentiment sensitive thesaurus.In Proceedings of ACL, pages 132-141.S.
Boyd and L. Vandenberghe.
2004.
Convex Optimization.
Cambridge university press.D.
Cai, X.
He, J. Han, and T. Huang.
2011.
Graph regularized non-negative matrix factorization for data represen-tation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8): 1548-1560.H.
Cho, I. Dhillon, Y. Guan, and S. Sra.
2004.
Minimum sum squared residue co-clutering of gene expressiondata.
In Proceedings of SDM, pages 22-24.F.
Chung.
1997.
Spectral graph theory.
Regional Conference Series in Mathematics, Volume 92.I.
Dhillon, S. Mallela, and D. Modha.
2003.
Information-theoretic Co-clustering.
In Proceedings of KDD, pages89-98.C.
Ding, T. Li, W. Peng, and H. Park.
2006.
Orthogonal non-negative matrix tri-factorization for clustering.
InProceedings of KDD, pages 126-135.X.
Glorot, A. Bordes, and Y. Bengio.
2011.
Domain adaptation for larage-scale sentiment classification: a deeplearning approach.
In Proceedings of ICML.A.
Goldberg and X. Zhu.
2006.
Seeing stars when there aren?t many stars: graph-based semi-supervised learningfor sentiment categorization.
In Proceedings of NAACL Workshop.Y.
He, C. Lin and H. Alani.
2011.
Automatically extracting polarity-bearing topics for cross-domain sentimentclassification.
In Proceedings of ACL, pages 123-131.M.
Hu and B. Liu.
2004.
Mining and summarizing customer reviews.
In Proceedings of KDD.X.
Hu, J. Tang, H. Gao, and H. Liu.
2013.
Unsupervised sentiment analysis with emotional signals.
In Proceedingsof WSDM.X.
Hu, N. Sun, C. Zhang, and T. Chua.
2009.
Exploiting internal and external semantics for the clustering of shorttexts using world knowldge.
In Proceedings of CIKM, pages 919-928.H.
Kim and H. Park.
2008.
Non-negative matrix factorization based on alternating non-negativity constrainedleast squares and active set method.
SIAM J Matrix Anal Appl, 30(2):713-730.D.
Lee and H. Seung.
2001.
Algorithms for non-negative matrix factorization.
In Proceedings of NIPS.S.
Li, Z. Wang, G. Zhou, and S. Lee.
2011.
Semi-supervised learning for imbalanced sentiment classification.
InProceedings of IJCAI, pages 1826-1831.1339T.
Li, Y. Zhang, and V. Singhwani.
2009.
A non-negative matrix tri-factorization approach to sentiment classifica-tion with lexical prior knowledge.
In Proceedings of ACL, pages 244-252.C.
Lin.
2007.
Projected gradient methods for nonnegative matrix factorization.
Neural Comput, 19(10):2756-2779.B.
Liu.
2012.
Sentiment analysis and opinion mining.
Morgan & Claypool Publishers.B.
Lu, C. Tan, C. Cardie, and B. Tsou.
2011.
Joint bilingual sentiment classification with unlabeled parallelcorpora.
In Proceedings of ACL, pages 320-330.Y.
Lu, M. Castellanos, U. Dayal, and C. Zhai.
2011.
Automatic construction of a context-aware sentiment lexicon:an optimization approach.
In Proceedings of WWW, pages 347-356.X.
Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang.
2012.
Cross-lingual mixture model for sentimentclassification.
In Proceedings of ACL, pages 572-581.V.
Ng, S. Dasgupta, and S. Arifin.
2006.
Examing the role of linguistic knowlege sources in the automaticidentification and classificaton of reviews.
In Proceedings of ACL.S.
Pan, X. Ni, J.
Sun, Q. Yang, and Z. Chen.
2010.
Cross-domain sentiment classification via spectral featurealignment.
In Proceedings of WWW.B.
Pang and L. Lee.
2008.
Opinion mining and sentiment analysis.
Foundations and Trends in InformaitonRetrieval, pages 1-135.B.
Pang, L. Lee, S. Vaithyanathan.
2002.
Thumbs up?
sentiment classification using machine learning techniques.In Proceedings of EMNLP, pages 79-86.S.
Riedel, L. Yao, A. McCallum, and B. Marlin.
2013.
Relation extraction with matrix factorization and universalschemas.
In Proceedings of NAACL.W.
Peng and D. Park.
2011.
Generative adjective sentiment dictionary for social media sentiment analysis usingconstrained nonnegative matrix factorization.
In Proceedings of ICWSM.S.
Roweis and L. Saul.
2000.
Nonlinear dimensionality reduction by locally linear embedding.
Science,290(5500):2323-2326.V.
Sindhwani and P. Melville.
2008.
Document-word co-regulariztion for semi-supervised sentiment analysis.
InProceedings of ICDM, pages 1025-1030.J.
Tenenbaum, V. Silva, and J. Langford.
2000.
A global geometric framework for nonlinear dimensionalityreduction.
Science, 290(5500):2319-2323.M.
Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede.
2011.
Lexicon-based methods for sentiment analysis.Computational Linguistics.P.
Turney.
2002.
Thumbs up or thumbs down?
: semantic orientation applied to unsupervised classification ofreviews.
In Proceedings of ACL, pages 417-424.X.
Wan.
2009.
Co-training for cross-lingual sentiment classification.
In Proceedings of ACL, pages 235-243.D.
Zhou, Q. Bousquet, T. Lal, J. Weston, and B. Scholkopf.
2004.
Learning with local and global consistency.
InProceedings of NIPS.G.
Zhou, F. Liu, Y. Liu, S. He, and J. Zhao.
2013.
Statistical machine translation improves question retrieval incommunity question answering via matrix factorization.
In Proceedings of ACL, pages 852-861.X.
Zhu, Z. Ghahramani, and J. Lafferty.
2003.
Semi-supervised learning using gaussian fields and harmonicfunctions.
In Proceedings of ICML.1340
