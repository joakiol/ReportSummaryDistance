Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
976?985, Prague, June 2007. c?2007 Association for Computational LinguisticsHierarchical Phrase-Based Translation with Suffix ArraysAdam LopezComputer Science DepartmentInstitute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742 USAalopez@cs.umd.eduAbstractA major engineering challenge in statisticalmachine translation systems is the efficientrepresentation of extremely large translationrulesets.
In phrase-based models, this prob-lem can be addressed by storing the trainingdata in memory and using a suffix array asan efficient index to quickly lookup and ex-tract rules on the fly.
Hierarchical phrase-based translation introduces the added wrin-kle of source phrases with gaps.
Lookupalgorithms used for contiguous phrases nolonger apply and the best approximate pat-tern matching algorithms are much too slow,taking several minutes per sentence.
Wedescribe new lookup algorithms for hierar-chical phrase-based translation that reducethe empirical computation time by nearlytwo orders of magnitude, making on-the-flylookup feasible for source phrases with gaps.1 IntroductionCurrent statistical machine translation systems relyon very large rule sets.
In phrase-based systems,rules are extracted from parallel corpora containingtens or hundreds of millions of words.
This can re-sult in millions of rules using even the most conser-vative extraction heuristics.
Efficient algorithms forrule storage and access are necessary for practicaldecoding algorithms.
They are crucial to keeping upwith the ever-increasing size of parallel corpora, aswell as the introduction of new data sources such asweb-mined and comparable corpora.Until recently, most approaches to this probleminvolved substantial tradeoffs.
The common prac-tice of test set filtering renders systems impracti-cal for all but batch processing.
Tight restrictionson phrase length curtail the power of phrase-basedmodels.
However, some promising engineering so-lutions are emerging.
Zens and Ney (2007) use adisk-based prefix tree, enabling efficient access tophrase tables much too large to fit in main memory.An alternative approach introduced independentlyby both Callison-Burch et al (2005) and Zhang andVogel (2005) is to store the training data itself inmemory, and use a suffix array as an efficient in-dex to look up, extract, and score phrase pairs on thefly.
We believe that the latter approach has severalimportant applications (?7).So far, these techniques have focused on phrase-based models using contiguous phrases (Koehn etal., 2003; Och and Ney, 2004).
Some recent modelspermit discontiguous phrases (Chiang, 2007; Quirket al, 2005; Simard et al, 2005).
Of particular in-terest to us is the hierarchical phrase-based model ofChiang (2007), which has been shown to be supe-rior to phrase-based models.
The ruleset extractedby this model is a superset of the ruleset in an equiv-alent phrase-based model, and it is an order of mag-nitude larger.
This makes efficient rule representa-tion even more critical.
We tackle the problem usingthe online rule extraction method of Callison-Burchet al (2005) and Zhang and Vogel (2005).The problem statement for our work is: Givenan input sentence, efficiently find all hierarchicalphrase-based translation rules for that sentence inthe training corpus.976We first review suffix arrays (?2) and hierarchicalphrase-based translation (?3).
We show that the ob-vious approach using state-of-the-art pattern match-ing algorithms is hopelessly inefficient (?4).
Wethen describe a series of algorithms to address thisinefficiency (?5).
Our algorithms reduce computa-tion time by two orders of magnitude, making theapproach feasible (?6).
We close with a discussionthat describes several applications of our work (?7).2 Suffix ArraysA suffix array is a data structure representing all suf-fixes of a corpus in lexicographical order (Manberand Myers, 1993).
Formally, for a text T , the ithsuffix of T is the substring of the text beginning atposition i and continuing to the end of T .
This suf-fix can be uniquely identified by the index i of itsfirst word.
The suffix array SAT of T is a permuta-tion of [1, |T |] arranged by the lexicographical orderof the corresponding suffixes.
This representationenables fast lookup of any contiguous substring us-ing binary search.
Specifically, all occurrences of alength-m substring can be found in O(m + log |T |)time (Manber and Myers, 1993).
1Callison-Burch et al (2005) and Zhang and Vogel(2005) use suffix arrays as follows.1.
Load the source training text F , the suffix arraySAF , the target training text E, and the align-ment A into memory.2.
For each input sentence, look up each substring(phrase) f?
of the sentence in the suffix array.3.
For each instance of f?
found in F , find itsaligned phrase e?
using the phrase extractionmethod of Koehn et al (2003).4.
Compute the relative frequency score p(e?|f?)
ofeach pair using the count of the extracted pairand the marginal count of f?
.5.
Compute the lexical weighting score of thephrase pair using the alignment that gives thebest score.1Abouelhoda et al (2004) show that lookup can be done inoptimal O(m) time using some auxiliaray data structures.
Forour purposes O(m + log |T |) is practical, since for the 27M-word corpus used to carry out our experiments, log |T | ?
25.6.
Use the scored rules to translate the input sen-tence with a standard decoding algorithm.A difficulty with this approach is step 3, which canbe quite slow.
Its complexity is linear in the num-ber of occurrences of the source phrase f?
.
BothCallison-Burch et al (2005) and Zhang and Vogel(2005) solve this with sampling.
If a source phraseappears more than k times, they sample only k oc-currences for rule extraction.
Both papers reportthat translation performance is nearly identical to ex-tracting all possible phrases when k = 100.
23 Hierarchical Phrase-Based TranslationWe consider the hierarchical translation model ofChiang (2007).
Formally, this model is a syn-chronous context-free grammar.
The lexicalizedtranslation rules of the grammar may contain a sin-gle nonterminal symbol, denoted X .
We will use a,b, c and d to denote terminal symbols, and u, v, andw to denote (possibly empty) sequences of these ter-minals.
We will additionally use ?
and ?
to denote(possibly empty) sequences containing both termi-nals and nonterminals.A translation rule is written X ?
?/?.
This rulestates that a span of the input matching ?
is replacedby ?
in translation.
We require that ?
and ?
con-tain an equal number (possibly zero) of coindexednonterminals.
An example rule with coindexes isX ?
uX 1 vX 2w/u?X 2 v?X 1w?.
When discussingonly the source side of such rules, we will leave outthe coindexes.
For instance, the source side of theabove rule will be written uXvXw.
3For the purposes of this paper, we adhere to therestrictions described by Chiang (2007) for rules ex-tracted from the training data.?
Rules can contain at most two nonterminals.?
Rules can contain at most five terminals.?
Rules can span at most ten words.2A sample size of 100 is actually quite small for manyphrases, some of which occur tens or hundreds of thousandsof times.
It is perhaps surprising that such a small sample sizeworks as well as the full data.
However, recent work by Och(2005) and Federico and Bertoldi (2006) has shown that thestatistics used by phrase-based systems are not very precise.3In the canonical representation of the grammar, source-sidecoindexes are always in sorted order, making them unambigu-ous.977?
Nonterminals must span at least two words.?
Adjacent nonterminals are disallowed in thesource side of a rule.Expressed more economically, we say that our goalis to search for source phrases in the form u, uXv,or uXvXw, where 1 ?
|uvw| ?
5, and |v| > 0 inthe final case.
Note that the model also allows rulesin the form Xu, uX , XuX , XuXv, and uXvX .However, these rules are lexically identical to otherrules, and thus will match the same locations in thesource text.4 The Collocation ProblemOn-the-fly lookup using suffix arrays involves anadded complication when the rules are in form uXvor uXvXw.
Binary search enables fast lookupof contiguous substrings.
However, it cannot beused for discontiguous substrings.
Consider the ruleaXbXc.
If we search for this rule in the followinglogical suffix array fragment, we will find the bold-faced matches....a c a c b a d c a d ...a c a d b a a d b d ...a d d b a a d a b c ...a d d b d a a b b a ...a d d b d d c a a a ......Even though these suffixes are in lexicographicalorder, matching suffixes are interspersed with non-matching suffixes.
We will need another algorithmto find the source rules containing at least oneX sur-rounded by nonempty sequences of terminal sym-bols.4.1 Baseline ApproachIn the pattern-matching literature, words spanned bythe nonterminal symbols of Chiang?s grammar arecalled don?t cares and a nonterminal symbol in aquery pattern that matches a sequence of don?t caresis called a variable length gap.
The search prob-lem for patterns containing these gaps is a variant ofapproximate pattern matching, which has receivedsubstantial attention (Navarro, 2001).
The best algo-rithm for pattern matching with variable-length gapsin a suffix array is a recent algorithm by Rahmanet al (2006).
It works on a pattern w1Xw2X...wIconsisting of I contiguous substrings w1, w2, ...wI ,each separated by a gap.
The algorithm is straight-forward.
After identifying all ni occurrences ofeach wi in O(|wi| + log |T |) time, collocations thatmeet the gap constraints are computed using an ef-ficient data structure called a stratified tree (vanEmde Boas et al, 1977).
4 Although we refer thereader to the source text for a full description ofthis data structure, its salient characteristic is thatit implements priority queue operations insert andnext-element in O(log log |T |) time.
Therefore, thetotal running time for an algorithm to find all con-tiguous subpatterns and compute their collocationsis O(?Ii=1 [|wi|+ log|T |+ ni log log |T |]).We can improve on the algorithm of Rahman etal.
(2006) using a variation on the idea of hashing.We exploit the fact that our large text is actually acollection of relatively short sentences, and that col-located patterns must occur in the same sentence inorder to be considered a rule.
Therefore, we canuse the sentence id of each subpattern occurrenceas a kind of hash key.
We create a hash table whosesize is exactly the number of sentences in our train-ing corpus.
Each location of the partially matchedpattern w1X...Xwi is inserted into the hash bucketwith the matching sentence id.
To find collocatedpatterns wi+1, we probe the hash table with eachof the ni+1 locations for that subpattern.
When amatch is found, we compare the element with all el-ements in the bucket to see if it is within the windowimposed by the phrase length constraints.
Theoreti-cally, the worst case for this algorithm occurs whenall elements of both sets resolve to the same hashbucket, and we must compare all elements of oneset with all elements of the other set.
This leads to aworst case complexity of O(?Ii=1 [|wi|+ log|T |] +?Ii=1 ni).
However, for real language data the per-formance for sets of any significant size will beO(?Ii=1 [|wi|+ log|T |+ ni]), since most patternswill occur once in any given sentence.4.2 AnalysisIt is instructive to compare this with the complex-ity for contiguous phrases.
In that case, total lookuptime is O(|w| + log|T |) for a contiguous pattern w.4Often known in the literature as a van Emde Boas tree orvan Emde Boas priority queue.978The crucial difference between the contiguous anddiscontiguous case is the added term?Ii=1 ni.
Foreven moderately frequent subpatterns this term dom-inates complexity.To make matters concrete, consider the trainingcorpus used in our experiments (?6), which contains27M source words.
The three most frequent uni-grams occur 1.48M, 1.16M and 688K times ?
thefirst two occur on average more than once per sen-tence.
In the worst case, looking up a contiguousphrase containing any number and combination ofthese unigrams requires no more than 25 compari-son operations.
In contrast, the worst case scenariofor a pattern with a single gap, bookended on eitherside by the most frequent word, requires over twomillion operations using our baseline algorithm andover thirteen million using the algorithm of Rahmanet al (2006).
A single frequent word in an inputsentence is enough to cause noticeable slowdowns,since it can appear in up to 530 hierarchical rules.To analyze the cost empirically, we ran our base-line algorithm on the first 50 sentences of the NISTChinese-English 2003 test set and measured theCPU time taken to compute collocations.
We foundthat, on average, it took 2241.25 seconds (?37 min-utes) per sentence just to compute all of the neededcollocations.
By comparison, decoding time persentence is roughly 10 seconds with moderately ag-gressive pruning, using the Python implementationof Chiang (2007).5 Solving the Collocation ProblemClearly, looking up patterns in this way is not prac-tical.
To analyze the problem, we measured theamount of CPU time per computation.
Cumulativelookup time was dominated by a very small fractionof the computations (Fig.
1).
As expected, furtheranalysis showed that these expensive computationsall involved one or more very frequent subpatterns.In the worst cases a single collocation took severalseconds to compute.
However, there is a silver lin-ing.
Patterns follow a Zipf distribution, so the num-ber of pattern types that cause the problem is actu-ally quite small.
The vast majority of patterns arerare.
Therefore, our solution focuses on computa-tions where one or more of the component patternsis frequent.
Assume that we are computing a collo-Computations (ranked by time)CumulativeTime(s)300K150KFigure 1: Ranked computations vs. cumulative time.A small fraction of all computations account formost of the computational time.cation of pattern w1X...Xwi and pattern wi+1, andwe know all locations of each.
There are three cases.?
If both patterns are frequent, we resort to aprecomputed intersection (?5.1).
We were notaware of any algorithms to substantially im-prove the efficiency of this computation when itis requested on the fly, but precomputation canbe done in a single pass over the text at decoderstartup.?
If one pattern is frequent and the other is rare,we use an algorithm whose complexity is de-pendent mainly on the frequency of the rarepattern (?5.2).
It can also be used for pairsof rare patterns when one pattern is much rarerthan the other.?
If both patterns are rare, no special algorithmsare needed.
Any linear algorithm will suffice.However, for reasons described in ?5.3, ourother collocation algorithms depend on sortedsets, so we use a merge algorithm.Finally, in order to cut down on the number of un-necessary computations, we use an efficient methodto enumerate the phrases to lookup (?5.4).
Thismethod also forms the basis of various cachingstrategies for additional speedups.
We analyze thememory use of our algorithms in ?5.5.5.1 PrecomputationPrecomputation of the most expensive collocationscan be done in a single pass over the text.
As in-put, our algorithm requires the identities of the k979most frequent contiguous patterns.
5 It then iteratesover the corpus.
Whenever a pattern from the list isseen, we push a tuple consisting of its identity andcurrent location onto a queue.
Whenever the oldestitem on the queue falls outside the maximum phraselength window with respect to the current position,we compute that item?s collocation with all succeed-ing patterns (subject to pattern length constraints)and pop it from the queue.
We repeat this step forevery item that falls outside the window.
At the endof each sentence, we compute collocations for anyremaining items in the queue and then empty it.Our precomputation includes the most frequentn-gram subpatterns.
Most of these are unigrams,but in our experiments we found 5-grams amongthe 1000 most frequent patterns.
We precomputethe locations of source phrase uXv for any pair uand v that both appear on this list.
There is alsoa small number of patterns uXv that are very fre-quent.
We cannot easily obtain a list of these in ad-vance, but we observe that they always consist of apair u and v of patterns from near the top of the fre-quency list.
Therefore we also precompute the loca-tions uXvXw of patterns in which both u and v areamong these super-frequent patterns (all unigrams),treating this as the collocation of the frequent patternuXv and frequent pattern w. We also compute theanalagous case for u and vXw.5.2 Fast IntersectionFor collocations of frequent and rare patterns, weuse a fast set intersection method for sorted setscalled double binary search (Baeza-Yates, 2004).
6It is based on the intuition that if one set in a pairof sorted sets is much smaller than the other, thenwe can compute their intersection efficiently by per-forming a binary search in the larger data set D foreach element of the smaller query set Q.Double binary search takes this idea a step further.It performs a binary search in D for the median ele-ment of Q.
Whether or not the element is found, the5These can be identified using a single traversal over alongest common prefix (LCP) array, an auxiliary data struc-ture of the suffix array, described by Manber and Myers (1993).Since we don?t need the LCP array at runtime, we chose to dothis computation once offline.6Minor modifications are required since we are computingcollocation rather than intersection.
Due to space constraints,details and proof of correctness are available in Lopez (2007a).search divides both sets into two pairs of smaller setsthat can be processed recursively.
Detailed analysisand empirical results on an information retrieval taskare reported in Baeza-Yates (2004) and Baeza-Yatesand Salinger (2005).
If |Q| log |D| < |D| then theperformance is guaranteed to be sublinear.
In prac-tice it is often sublinear even if |Q| log |D| is some-what larger than |D|.
In our implementation we sim-ply check for the condition ?|Q| log |D| < |D| todecide whether we should use double binary searchor the merge algorithm.
This check is applied in therecursive cases as well as for the initial inputs.
Thevariable ?
can be adjusted for performance.
We de-termined experimentally that a good value for thisparameter is 0.3.5.3 Obtaining Sorted SetsDouble binary search requires that its input sets bein sorted order.
However, the suffix array returnsmatchings in lexicographical order, not numeric or-der.
The algorithm of Rahman et al (2006) dealswith this problem by inserting the unordered itemsinto a stratified tree.
This requires O(n log log |T |)time for n items.
If we used the same strategy, ouralgorithm would no longer be sublinear.An alternative is to precompute all n-gram occur-rences in order and store them in an inverted index.This can be done in one pass over the data.
7 Thisapproach requires a separate inverted index for eachn, up to the maximum n used by the model.
Thememory cost is one length-|T | array per index.In order to avoid the full n|T | cost in memory,our implementation uses a mixed strategy.
We keepa precomputed inverted index only for unigrams.For bigrams and larger n-grams, we generate the in-dex on the fly using stratified trees.
This results ina superlinear algorithm for intersection.
However,we can exploit the fact that we must compute col-locations multiple times for each input n-gram bycaching the sorted set after we create it (The cachingstrategy is described in ?5.4).
Subsequent computa-tions involving this n-gram can then be done in lin-ear or sublinear time.
Therefore, the cost of buildingthe inverted index on the fly is amortized over a largenumber of computations.7We combine this step with the other precomputations thatrequire a pass over the data, thereby removing a redundantO(|T |) term from the startup cost.9805.4 Efficient EnumerationA major difference between contiguous phrase-based models and hierarchical phrase-based modelsis the number of rules that potentially apply to aninput sentence.
To make this concrete, on our data,with an average of 29 words per sentence, there wereon average 133 contiguous phrases of length 5 orless that applied.
By comparison, there were on av-erage 7557 hierarchical phrases containing up to 5words.
These patterns are obviously highly overlap-ping and we employ an algorithm to exploit this fact.We first describe a baseline algorithm used for con-tiguous phrases (?5.4.1).
We then introduce someimprovements (?5.4.2) and describe a data structureused by the algorithm (?5.4.3).
Finally, we dis-cuss some special cases for discontiguous phrases(?5.4.4).5.4.1 The Zhang-Vogel AlgorithmZhang and Vogel (2005) present a clever algo-rithm for contiguous phrase searches in a suffix ar-ray.
It exploits the fact that for eachm-length sourcephrase that we want to look up, we will also want tolook up its (m?
1)-length prefix.
They observe thatthe region of the suffix array containing all suffixesprefixed by ua is a subset of the region containingthe suffixes prefixed by u.
Therefore, if we enumer-ate the phrases of our sentence in such a way thatwe always search for u before searching for ua, wecan restrict the binary search for ua to the range con-taining the suffixes prefixed by u.
If the search foru fails, we do not need to search for ua at all.
Theyshow that this approach leads to some time savingsfor phrase search, although the gains are relativelymodest since the search for contiguous phrases is notvery expensive to begin with.
However, the potentialsavings in the discontiguous case are much greater.5.4.2 Improvements and ExtensionsWe can improve on the Zhang-Vogel algorithm.An m-length contiguous phrase aub depends notonly on the existence of its prefix au, but also onthe existence of its suffix ub.
In the contiguous case,we cannot use this information to restrict the startingrange of the binary search, but we can check for theexistence of ub to decide whether we even need tosearch for aub at all.
This can help us avoid searchesthat are guaranteed to be fruitless.Now consider the discontiguous case.
As in theanalogous contiguous case, a phrase a?b will onlyexist in the text if its maximal prefix a?
and maxi-mal suffix ?b both exist in the corpus and overlap atspecific positions.
8 Searching for a?b is potentiallyvery expensive, so we put all available informationto work.
Before searching, we require that both a?and ?b exist.
Additionally, we compute the loca-tion of a?b using the locations of both maximal sub-phrases.
To see why the latter optimization is useful,consider a phrase abXcd.
In our baseline algorithm,we would search for ab and cd, and then perform acomputation to see whether these subphrases werecollocated within an elastic window.
However, if weinstead use abXc and bXcd as the basis of the com-putation, we gain two advantages.
First, the numberelements of each set is likely to be smaller then inthe former case.
Second, the computation becomessimpler, because we now only need to check to seewhether the patterns exactly overlap with a startingoffset of one, rather than checking within a windowof locations.We can improve efficiency even further if we con-sider cases where the same substring occurs morethan once within the same sentence, or even in mul-tiple sentences.
If the computation required to lookup a phrase is expensive, we would like to performthe lookup only once.
This requires some mecha-nism for caching.
Depending on the situation, wemight want to cache only certain subsets of phrases,based on their frequency or difficulty to compute.We would also like the flexibility to combine on-the-fly lookups with a partially precomputed phrasetable, as in the online/offline mixture of Zhang andVogel (2005).We need a data structure that provides this flex-ibility, in addition to providing fast access to boththe maximal prefix and maximal suffix of any phrasethat we might consider.5.4.3 Prefix Trees and Suffix LinksOur search optimizations are easily captured in aprefix tree data structure augmented with suffix links.Formally, a prefix tree is an unminimized determin-istic finite-state automaton that recognizes all of thepatterns in some set.
Each node in the tree repre-8Except when ?
= X , in which case a and b must be collo-cated within a window defined by the phrase length constraints.981abbccXX(1)(2)(3)d(4)dabbccXX(1)(2)(3)d(4)dabbccXX(1)(2)(3)d(4)dabbccXX(1)(2)(3)d(4)dXeacdCase 1Case 2Figure 2: Illustration of prefix tree construction showing a partial prefix tree, including suffix links.
Supposewe are interested in pattern abXcd, represented by node (1).
Its prefix is represented by node (2), and node(2)?s suffix is represented by node (3).
Therefore, node (1)?s suffix is represented by the node pointed to bythe d-edge from node (3), which is node (4).
There are two cases.
In case 1, node (4) is inactive, so wecan mark node (1) inactive and stop.
In case 2, node (4) is active, so we compute the collocation of abXcand bXcd with information stored at nodes (2) and (4), using either a precomputed intersection, doublebinary search, or merge, depending on the size of the sets.
If the result is empty, we mark the node inactive.Otherwise, we store the results at node (1) and add its successor patterns to the frontier for the next iteration.This includes all patterns containing exactly one more terminal symbol than the current pattern.sents the prefix of a unique pattern from the set thatis specified by the concatenation of the edge labelsalong the path from the root to that node.
A suffixlink is a pointer from a node representing path a?
tothe node representing path ?.
We will use this datastructure to record the set of patterns that we havesearched for and to cache information for those thatwere found successfully.Our algorithm generates the tree breadth-searchalong a frontier.
In the mth iteration we only searchfor patterns containingm terminal symbols.
Regard-less of whether we find a particular pattern, we cre-ate a node for it in the tree.
If the pattern was foundin the corpus, its node is marked active.
Otherwise,it is marked inactive.
For found patterns, we storeeither the endpoints of the suffix array range con-taining the phrase (if it is contiguous), or the list oflocations at which the phrase is found (if it is dis-contiguous).
We can also store the extracted rules.
9Whenever a pattern is successfully found, we add allpatterns with m + 1 terminals that are prefixed by it9Conveniently, the implementation of Chiang (2007) uses aprefix tree grammar encoding, as described in Klein and Man-ning (2001).
Our implementation decorates this tree with addi-tional information required by our algorithms.to the frontier for processing in the next iteration.To search for a pattern, we use location infor-mation from its parent node, which represents itsmaximal prefix.
Assuming that the node representsphrase ?b, we find the node representing its max-imal suffix by following the b-edge from the nodepointed to by its parent node?s suffix link.
If the nodepointed to by this suffix link is inactive, we can markthe node inactive without running a search.
When anode is marked inactive, we discontinue search forphrases that are prefixed by the path it represents.The algorithm is illustrated in Figure 2.5.4.4 Special Cases for Phrases with GapsA few subtleties arise in the extraction of hierar-chical patterns.
Gaps are allowed to occur at the be-ginning or end of a phrase.
For instance, we mayhave a source phrase Xu or uX or even XuX .
Al-though each of these phrases requires its own path inthe prefix tree, they are lexically identical to phraseu.
An analogous situation occurs with the patternsXuXv, uXvX , and uXv.
There are two cases thatwe are concerned with.The first case consists of all patterns prefixed withX .
The paths to nodes representing these patterns982will all contain the X-edge originating at the rootnode.
All of these paths form the shadow sub-tree.
Path construction in this subtree proceeds dif-ferently.
Because they are lexically identical to theirsuffixes, they are automatically extended if their suf-fix paths are active, and they inherit location infor-mation of their suffixes.The second case consists of all patterns suffixedwith X .
Whenever we successfully find a new pat-tern ?, we automatically extend it with an X edge,provided that ?X is allowed by the model con-straints.
The node pointed to by this edge inheritsits location information from its parent node (repre-senting the maximal prefix ?
).Note that both special cases occur for patterns inthe form XuX .5.5 Memory RequirementsAs shown in Callison-Burch et al (2005), we mustkeep an array for the source text F , its suffix array,the target text E, and alignment A in memory.
As-suming that A and E are roughly the size of F , thecost is 4|T |.
If we assume that all data use vocabu-laries that can be represented using 32-bit integers,then our 27M word corpus can easily be representedin around 500MB of memory.
Adding the invertedindex for unigrams increases this by 20%.
The mainadditional cost in memory comes from the storageof the precomputed collocations.
This is dependentboth on the corpus size and the number of colloca-tions that we choose to precompute.
Using detailedtiming data from our experiments we were able tosimulate the memory-speed tradeoff (Fig.
3).
If weinclude a trigram model trained on our bitext and theChinese Gigaword corpus, the overall storage costsfor our system are approximately 2GB.6 ExperimentsAll of our experiments were performed on Chinese-English in the news domain.
We used a large train-ing set consisting of over 1 million sentences fromvarious newswire corpora.
This corpus is roughlythe same as the one used for large-scale experimentsby Chiang et al (2005).
To generate alignments,we used GIZA++ (Och and Ney, 2003).
We sym-metrized bidirectional alignments using the grow-diag-final heuristic (Koehn et al, 2003).00010000Number of frequent subpatternsInsert text here41 sec/sent41 seconds405 sec/sent0 MB725MBFigure 3: Effect of precomputation on memory useand processing time.
Here we show only the mem-ory requirements of the precomputed collocations.We used the first 50 sentences of the NIST 2003test set to compute timing results.
All of our algo-rithms were implemented in Python 2.4.
10 Timingresults are reported for machines with 8GB of mem-ory and 4 3GHz Xeon processors running Red Hatlinux 2.6.9.
In order to understand the contributionsof various improvements, we also ran the systemwith with various ablations.
In the default setting,the prefix tree is constructed for each sentence toguide phrase lookup, and then discarded.
To showthe effect of caching we also ran the algorithm with-out discarding the prefix tree between sentences, re-sulting in full inter-sentence caching.
The results areshown in Table 1.
11It is clear from the results that each of the op-timizations is needed to sufficiently reduce lookuptime to practical levels.
Although this is still rela-tively slow, it is much closer to the decoding time of10 seconds per sentence than the baseline.10Python is an interpreted language and our implementationsdo not use any optimization features.
It is therefore reasonableto think that a more efficient reimplementation would result inacross-the-board speedups.11The results shown here do not include the startup time re-quired to load the data structures into memory.
In our Pythonimplementation this takes several minutes, which in principleshould be amortized over the cost for each sentence.
However,just as Zens and Ney (2007) do for phrase tables, we could com-pile our data structures into binary memory-mapped files, whichcan be read into memory in a matter of seconds.
We are cur-rently investigating this option in a C reimplementation.983Algorithms Secs/Sent CollocationsBaseline 2241.25 325548Prefix Tree 1578.77 69994Prefix Tree + precomputation 696.35 69994Prefix Tree + double binary 405.02 69994Prefix Tree + precomputation + double binary 40.77 69994Prefix Tree with full caching + precomputation + double binary 30.70 67712Table 1: Timing results and number of collocations computed for various combinations of algorithms.
Theruns using precomputation use the 1000 most frequent patterns.7 Conclusions and Future WorkOur work solves a seemingly intractable problemand opens up a number of intriguing potential ap-plications.
Both Callison-Burch et al (2005) andZhang and Vogel (2005) use suffix arrays to relaxthe length constraints on phrase-based models.
Ourwork enables this in hierarchical phrase-based mod-els.
However, we are interested in additional appli-cations.Recent work in discriminative learning for manynatural language tasks, such as part-of-speech tag-ging and information extraction, has shown that fea-ture engineering plays a critical role in these ap-proaches.
However, in machine translation most fea-tures can still be traced back to the IBM Models of15 years ago (Lopez, 2007b).
Recently, Lopez andResnik (2006) showed that most of the features usedin standard phrase-based models do not help verymuch.
Our algorithms enable us to look up phrasepairs in context, which will allow us to compute in-teresting contextual features that can be used in dis-criminative learning algorithms to improve transla-tion accuracy.
Essentially, we can use the trainingdata itself as an indirect representation of whateverfeatures we might want to compute.
This is not pos-sible with table-based architectures.Most of the data structures and algorithms dis-cussed in this paper are widely used in bioinformat-ics, including suffix arrays, prefix trees, and suf-fix links (Gusfield, 1997).
As discussed in ?4.1,our problem is a variant of the approximate patternmatching problem.
A major application of approx-imate pattern matching in bioinformatics is queryprocessing in protein databases for purposes of se-quencing, phylogeny, and motif identification.Current MT models, including hierarchical mod-els, translate by breaking the input sentence intosmall pieces and translating them largely indepen-dently.
Using approximate pattern matching algo-rithms, we imagine that machine translation couldbe treated very much like search in a proteindatabase.
In this scenario, the goal is to selecttraining sentences that match the input sentence asclosely as possible, under some evaluation functionthat accounts for both matching and mismatchedsequences, as well as possibly other data features.Once we have found the closest sentences we cantranslate the matched portions in their entirety, re-placing mismatches with appropriate word, phrase,or hierarchical phrase translations as needed.
Thismodel would bring statistical machine translationcloser to convergence with so-called example-basedtranslation, following current trends (Marcu, 2001;Och, 2002).
We intend to explore these ideas in fu-ture work.AcknowledgementsI would like to thank Philip Resnik for encour-agement, thoughtful discussions and wise counsel;David Chiang for providing the source code for histranslation system; and Nitin Madnani, SmarandaMuresan and the anonymous reviewers for veryhelpful comments on earlier drafts of this paper.Any errors are my own.
This research was supportedin part by ONR MURI Contract FCPO.810548265and the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-2-001.
Any opinions, findings, conclusions orrecommendations expressed in this paper are thoseof the author and do not necessarily reflect the viewof DARPA.984ReferencesMohamed Ibrahim Abouelhoda, Stefan Kurtz, and EnnoOhlebusch.
2004.
Replacing suffix trees with en-hanced suffix arrays.
Journal of Discrete Algorithms,2(1):53?86, Mar.Ricardo Baeza-Yates and Alejandro Salinger.
2005.
Ex-perimental analysis of a fast intersection algorithm forsorted sequences.
In M. Consens and G. Navarro, ed-itors, Proc.
of SPIRE, number 3772 in LNCS, pages13?24, Berlin.
Springer-Verlag.Ricardo Baeza-Yates.
2004.
A fast intersection algo-rithm for sorted sequences.
In Proc.
of CombinatorialPattern Matching, number 3109 in LNCS, pages 400?408, Berlin.
Springer-Verlag.Chris Callison-Burch, Colin Bannard, and Josh Shroeder.2005.
Scaling phrase-based statistical machine trans-lation to larger corpora and longer phrases.
In Proc.
ofACL, pages 255?262, Jun.David Chiang, Adam Lopez, Nitin Madnani, ChristofMonz, Philip Resnik, and Michael Subotin.
2005.
TheHiero machine translation system: Extensions, evalua-tion, and analysis.
In Proc.
of HLT-EMLP, pages 779?786, Oct.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).
In press.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In Proc.
of NAACL Workshop onStatistical Machine Translation, pages 94?101, Jun.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.Dan Klein and Christopher D. Manning.
2001.
Parsingwith treebank grammars: Empirical bounds, theoreti-cal models, and the structure of the Penn Treebank.
InProc.
of ACL-EACL, pages 330?337, Jul.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL, pages 127?133, May.Adam Lopez and Philip Resnik.
2006.
Word-basedalignment, phrase-based translation: What?s the link?In Proc.
of AMTA, pages 90?99, Aug.Adam Lopez.
2007a.
Hierarchical phrase-based trans-lation with suffix arrays.
Technical Report 2007-26,University of Maryland Institute for Advanced Com-puter Studies, May.Adam Lopez.
2007b.
A survey of statistical machinetranslation.
Technical Report 2006-47, University ofMaryland Institute for Advanced Computer Studies,Apr.Udi Manber and Gene Myers.
1993.
Suffix arrays: Anew method for on-line string searches.
SIAM Journalof Computing, 22(5):935?948.Daniel Marcu.
2001.
Towards a unified approach tomemory- and statistical-based machine translation.
InProc.
of ACL-EACL, pages 378?385, Jul.Gonzalo Navarro.
2001.
A guided tour to approximatestring matching.
ACM Computing Surveys, 33(1):31?88, Mar.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, Mar.Franz Josef Och and Hermann Ney.
2004.
The alignmenttemplate approach to machine translation.
Computa-tional Linguistics, 30(4):417?449, Jun.Franz Josef Och.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen, Oct.Franz Josef Och.
2005.
Statistical machine translation:The fabulous present and future.
In Proc.
of ACLWorkshop on Building and Using Parallel Texts, Jun.Invited talk.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proc.
of ACL, pages 271?279, Jun.Mohammad Sohel Rahman, Costas S. Iliopoulos, InbokLee, Manal Mohamed, and William F. Smyth.
2006.Finding patterns with variable length gaps or don?tcares.
In Proc.
of COCOON, Aug.Michel Simard, Nicola Cancedda, Bruno Cavestro, MarcDymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-mada, Philippe Langlais, and Arne Mauser.
2005.Translating with non-contiguous phrases.
In Proc.
ofHLT-EMNLP, pages 755?762, Oct.Peter van Emde Boas, R. Kaas, and E. Zijlstra.
1977.
De-sign and implementation of an efficient priority queue.Mathematical Systems Theory, 10(2):99?127.Richard Zens and Hermann Ney.
2007.
Efficient phrase-table representation for machine translation with appli-cations to online MT and speech translation.
In Proc.of HLT-NAACL.
To appear.Ying Zhang and Stephan Vogel.
2005.
An efficientphrase-to-phrase alignment model for arbitrarily longphrase and large corpora.
In Proc.
of EAMT, May.985
