Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1134?1144,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAre Two Heads Better than One?
Crowdsourced Translation via aTwo-Step Collaboration of Non-Professional Translators and EditorsRui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-BurchComputer and Information Science Department,University of Pennsylvania, Philadelphia, PA 19104, U.S.A.{ruiyan,gmingkun,epavlick}@seas.upenn.edu, ccb@cis.upenn.eduAbstractCrowdsourcing is a viable mechanism forcreating training data for machine trans-lation.
It provides a low cost, fast turn-around way of processing large volumesof data.
However, when compared to pro-fessional translation, naive collection oftranslations from non-professionals yieldslow-quality results.
Careful quality con-trol is necessary for crowdsourcing towork well.
In this paper, we examinethe challenges of a two-step collaborationprocess with translation and post-editingby non-professionals.
We develop graph-based ranking models that automaticallyselect the best output from multiple redun-dant versions of translations and edits, andimproves translation quality closer to pro-fessionals.1 IntroductionStatistical machine translation (SMT) systems aretrained using bilingual sentence-aligned parallelcorpora.
Theoretically, SMT can be applied toany language pair, but in practice it produces thestate-of-art results only for language pairs withample training data, like English-Arabic, English-Chinese, French-English, etc.
SMT gets stuckin a severe bottleneck for many minority or ?lowresource?
languages with insufficient data.
Thisdrastically limits which languages SMT can besuccessfully applied to.
Because of this, collect-ing parallel corpora for minor languages has be-come an interesting research challenge.
There arevarious options for creating training data for newlanguage pairs.
Past approaches have examinedharvesting translated documents from the web(Resnik and Smith, 2003; Uszkoreit et al, 2010;Smith et al, 2013), or discovering parallel frag-ments from comparable corpora (Munteanu andMarcu, 2005; Abdul-Rauf and Schwenk, 2009;Smith et al, 2010).
Until relatively recently, lit-tle consideration has been given to creating par-allel data from scratch.
This is because the costof hiring professional translators is prohibitivelyhigh.
For instance, Germann (2001) hoped to hireprofessional translators to create a modest sized100,000 word Tamil-English parallel corpus, butwere stymied by the costs and the difficulty offinding good translators for a short-term commit-ment.Recently, crowdsourcing has opened the possi-bility of translating large amounts of text at lowcost using non-professional translators.
Facebooklocalized its web site into different languages us-ing volunteers (TechCrunch, 2008).
DuoLingoturns translation into an educational game, andtranslates web content using its language learners(von Ahn, 2013).Rather than relying on volunteers or gamifica-tion, NLP research into crowdsourcing transla-tion has focused on hiring workers on the Ama-zon Mechanical Turk (MTurk) platform (Callison-Burch, 2009).
This setup presents unique chal-lenges, since it typically involves non-professionaltranslators whose language skills are varied, andsince it sometimes involves participants who tryto cheat to get the small financial reward (Zaidanand Callison-Burch, 2011).
A natural approachfor trying to shore up the skills of weak bilingualsis to pair them with a native speaker of the tar-get language to edit their translations.
We reviewrelevant research from NLP and human-computerinteraction (HCI) on collaborative translation pro-cesses in Section 2.
To sort good translations frombad, researchers often solicit multiple, redundanttranslations and then build models to try to predictwhich translations are the best, or which transla-tors tend to produce the highest quality transla-tions.The contributions of this paper are:1134?
An analysis of the difficulties posed by a two-step collaboration between editors and trans-lators in Mechanical Turk-style crowdsourc-ing environments.
Editors vary in quality,and poor editing can be difficult to detect.?
A new graph-based algorithm for selectingthe best translation among multiple transla-tions of the same input.
This method takesinto account the collaborative relationshipbetween the translators and the editors.2 Related workIn the HCI community, several researchers haveproposed protocols for collaborative translationefforts (Morita and Ishida, 2009b; Morita andIshida, 2009a; Hu, 2009; Hu et al, 2010).
Thesehave focused on an iterative collaboration betweenmonolingual speakers of the two languages, facil-itated with a machine translation system.
Thesestudies are similar to ours in that they rely on na-tive speakers?
understanding of the target languageto correct the disfluencies in poor translations.
Inour setup the poor translations are produced bybilingual individuals who are weak in the targetlanguage, and in their experiments the translationsare the output of a machine translation system.1Another significant difference is that the HCIstudies assume cooperative participants.
For in-stance, Hu et al (2010) recruited volunteers fromthe International Children?s Digital Library (Hour-cade et al, 2003) who were all well intentionedand participated out a sense of altruism and tobuild a good reputation among the other volunteertranslators at childrenslibrary.org.
Oursetup uses anonymous crowd workers hired onMechanical Turk, whose motivation to participateis financial.
Bernstein et al (2010) characterizedthe problems with hiring editors via MTurk for aword processing application.
Workers were eitherlazy (meaning they made only minimal edits) oroverly zealous (meaning they made many unnec-essary edits).
Bernstein et al (2010) addressedthis problem with a three step find-fix-verify pro-cess.
In the first step, workers click on one wordor phrase that needed to be corrected.
In the nextstep, a separate group of workers proposed correc-1A variety of HCI and NLP studies have confirmed theefficacy of monolingual or bilingual individuals post-editingof machine translation output (Callison-Burch, 2005; Koehn,2010; Green et al, 2013).
Past NLP work has also examinedautomatic post-editing(Knight and Chander, 1994).tions to problematic regions that had been identi-fied by multiple workers in the first pass.
In thefinal step, other workers would validate whetherthe proposed corrections were good.Most NLP research into crowdsourcing has fo-cused on Mechanical Turk, following pioneeringwork by Snow et al (2008) who showed that theplatform was a viable way of collecting data for awide variety of NLP tasks at low cost and in largevolumes.
They further showed that non-expert an-notations are similar to expert annotations whenmany non-expert labelings for the same inputare aggregated, through simple voting or throughweighting votes based on how closely non-expertsmatched experts on a small amount of calibra-tion data.
MTurk has subsequently been widelyadopted by the NLP community and used for anextensive range of speech and language applica-tions (Callison-Burch and Dredze, 2010).Although hiring professional translators to cre-ate bilingual training data for machine translationsystems has been deemed infeasible, MechanicalTurk has provided a low cost way of creating largevolumes of translations (Callison-Burch, 2009;Ambati and Vogel, 2010).
For instance, Zbib etal.
(2012; Zbib et al (2013) translated 1.5 mil-lion words of Levine Arabic and Egyptian Arabic,and showed that a statistical translation systemtrained on the dialect data outperformed a systemtrained on 100 times more MSA data.
Post et al(2012) used MTurk to create parallel corpora forsix Indian languages for less than $0.01 per word.MTurk workers translated more than half a millionwords worth of Malayalam in less than a week.Several researchers have examined the use of ac-tive learning to further reduce the cost of transla-tion (Ambati et al, 2010; Ambati, 2012; Blood-good and Callison-Burch, 2010).
Crowdsourcingallowed real studies to be conducted whereas mostpast active learning were simulated.
Pavlick et al(2014) conducted a large-scale demographic studyof the languages spoken by workers on MTurk bytranslating 10,000 words in each of 100 languages.Chen and Dolan (2012) examined the steps neces-sary to build a persistent multilingual workforceon MTurk.This paper is most closely related to previouswork by Zaidan and Callison-Burch (2011), whoshowed that non-professional translators could ap-proach the level of professional translators.
Theysolicited multiple redundant translations from dif-1135Urdu translator:According to the territory?s people the pamphlets fromthe Taaliban had been read in the announcements in allthe mosques of the Northern Wazeerastan.English post-editor:According to locals, the pamphlet released by the Talibanwas read out on the loudspeakers of all the mosques inNorth Waziristan.LDC professional:According to the local people, the Taliban?s pamphletwas read over the loudspeakers of all mosques in NorthWaziristan.Table 1: Different versions of translations.ferent Turkers for a collection of Urdu sentencesthat had been previously professionally translatedby the Linguistics Data Consortium.
They built amodel to try to predict on a sentence-by-sentenceand Turker-by-Turker which was the best transla-tion or translator.
They also hired US-based Turk-ers to edit the translations, since the translatorswere largely based in Pakistan and exhibited er-rors that are characteristic of speakers of Englishas a language.
Zaidan and Callison-Burch (2011)observed only modest improvements when incor-porating these edited translation into their model.We attempt to analyze why this is, and we pro-posed a new model to try to better leverage theirdata.3 Crowdsourcing TranslationSetup We conduct our experiments using thedata collected by Zaidan and Callison-Burch(2011).
This data set consists 1,792 Urdu sen-tences from a variety of news and online sources,each paired with English translations provided bynon-professional translators on Mechanical Turk.Each Urdu sentence was translated redundantlyby 3 distinct translators, and each translation wasedited by 3 separate (native English-speaking) ed-itors to correct for grammatical and stylistic er-rors.
In total, this gives us 12 non-professionalEnglish candidate sentences (3 unedited, 9 edited)per original Urdu sentence.
52 different Turkerstook part in the translation task, each translating138 sentences on average.
In the editing task, 320Turkers participated, averaging 56 sentences each.For comparison, the data also includes 4 differ-ent reference translations for each source sentence,produced by professional translators.Table 1 gives an example of an unedited trans-lation, an edited translation, and a professionaltranslation for the same sentence.
The transla-tions provided by translators on MTurk are gen-erally done conscientiously, preserving the mean-ing of the source sentence, but typically con-tain simple mistakes like misspellings, typos, andawkward word choice.
English-speaking editors,despite having no knowledge of the source lan-guage, are able to fix these errors.
In this work,we show that the collaboration design of twoheads?
non-professional Urdu translators and non-professional English editors?
yields better trans-lated output than would either one working in iso-lation, and can better approximate the quality ofprofessional translators.Analysis We know from inspection that trans-lations seem to improve with editing (Table 1).Given the data from MTurk, we explore whetherthis is the case in general: Do all translations im-prove with editing?
To what extent does the in-dividual translator and the individual editor effectthe quality of the final sentence?Figure 1: Relationship between editor aggressive-ness and effectiveness.
Each point represents aneditor/translation pair.
Aggressiveness (x-axis) ismeasured as the TER between the pre-edit andpost-edit version of the translation, and effective-ness (y-axis) is measured as the average amountby which the editing reduces the translation?sTERgold.
While many editors make only a fewchanges, those who make many changes can bringthe translation substantially closer to professionalquality.We use translation edit rate (TER) as a mea-sure of translation similarity.
TER represents theamount of change necessary to transform one sen-tence into another, so a low TER means the two11360.020.050.07?
TER goldEditor ?
TERgold 0.03  0.500.000.010.02?
TER goldEditor ?
TERgold 0.01  0.03-0.03-0.010.01?
TER goldEditor ?
TERgold -0.01  0.01-0.08-0.04-0.00?
TER goldEditor ?
TERgold -0.03  -0.010.3  0.9 0.2  0.3 0.2  0.2 0.1  0.2 0.0  0.1Translation TERgold-0.30-0.15-0.01?
TER goldEditor ?
TERgold -0.64  -0.03Figure 2: Effect of editing on translations of vary-ing quality.
Rows reflect bins of editors, with theworse editors (those whose changes result in in-creased TERgold) on the top and the most effectiveeditors (those whose changes result in the largestreduction in TERgold) on the bottom.
Bars re-flect bins of translations, with the highest TERgoldtranslations on the left, and the lowest on theright.
We can see from the consistently negative?
TERgoldin the bottom row that good editors areable to improve both good and bad translations.sentences are very similar.
To capture the quality(?professionalness?)
of a translation, we take theaverage TER of the translation against each of ourgold translations.
That is, we define TERgoldoftranslation t asTERgold=144?i=1TER(goldi, t) (1)where a lower TERgoldis indicative of a higherquality (more professional-sounding) translation.We first look at editors along two dimensions:their aggressiveness and their effectiveness.
Someeditors may be very aggressive (they make manychanges to the original translation) but still be in-effective (they fail to bring the quality of the trans-lation closer to that of a professional).
We measureaggressiveness by looking at the TER betweenthe pre- and post-edited versions of each editor?stranslations; higher TER implies more aggressiveediting.
To measure effectiveness, we look at thechange in TERgoldthat results from the editing;negative ?TERgoldmeans the editor effectivelyimproved the quality of the translation, while pos-itive ?TERgoldmeans the editing actually broughtthe translation further from our gold standard.Figure 1 shows the relationship between thesetwo qualities for individual editor/translationpairs.
We see that while most translations re-quire only a few edits, there are a large numberof translations which improve substantially afterheavy editing.
This trend conforms to our intu-ition that editing is most useful when the transla-tion has much room for improvement, and opensthe question of whether good editors can offer im-provements to translations of all qualities.To address this question, we split our transla-tions into 5 bins, based on their TERgold.
We alsosplit our editors into 5 bins, based on their effec-tiveness (i.e.
the average amount by which theirediting reduces TERgold).
Figure 2 shows the de-gree to which editors at each level are able to im-prove the translations from each bin.
We see thatgood editors are able to make improvements totranslations of all qualities, but that good editinghas the greatest impact on lower quality transla-tions.
This result suggests that finding good ed-itor/translator pairs, rather than good editors andgood translators in isolation, should produce thebest translations overall.
Figure 3 gives an exam-ple of how an initially medium-quality translation,when combined with good editing, produces a bet-ter result than the higher-quality translation pairedwith mediocre editing.4 Problem FormulationThe problem definition of the crowdsourcingtranslation task is straightforward: given a set ofcandidate translations for a source sentence, wewant to choose the best output translation.This output translation is the result of the com-bined translation and editing stages.
Therefore,our method operates over a heterogeneous net-work that includes translators and post-editors aswell as the translated sentences that they pro-duce.
We frame the problem as follows.
We formtwo graphs: the first graph (GT) represents Turk-ers (translator/editor pairs) as nodes; the secondgraph (GC) represents candidate translated and1137Figure 3: Three alternative translations (left) and the edited versions of each (right).
Each edit on theright was produced by a different editor.
Order reflects the TERgoldof each translation, with the lowestTERgoldon the top.
Some translators receive low TERgoldscores due to superficial errors, which can beeasily improved through editing.
In the above example, the middle-ranked translation (green) becomesthe best translation after being revised by a good editor.post-edited sentences (henceforth ?candidates?)
asnodes.
These two graphs, GTand GCare com-bined as subgraphs of a third graph (GTC).
Edgesin GTCconnect author pairs (nodes in GT) to thecandidate that they produced (nodes in GC).
To-gether, GT, GC, and GTCdefine a co-rankingproblem (Yan et al, 2012a; Yan et al, 2011b; Yanet al, 2012b) with linkage establishment (Yan etal., 2011a; Yan et al, 2012c), which we define for-mally as follows.Let G denote the heterogeneous graph withnodes V and edges E. Let G = (V ,E) =(VT, VC, ET, EC, ETC).
G is divided into threesubgraphs, GT, GC, and GTC.
GC= (VC, EC) isa weighted undirected graph representing the can-didates and their lexical relationships to one an-other.
Let VCdenote a collection of translatedand edited candidates, and ECthe lexical simi-larity between the candidates (see Section 4.3 fordetails).
GT= (VT, ET) is a weighted undirectedgraph representing collaborations between Turk-ers.
VTis the set of translator/editor pairs.
EdgesETconnect translator/editor pairs in VTwhichshare a translator and/or editor.
Each collabora-tion (i.e.
each node in VT) produces a candidate(i.e.
a node in VC).
GTC= (VTC, ETC) is anunweighted bipartite graph that ties GTand GCtogether and represents ?authorship?.
The graphG consists of nodes VTC= VT?
VCand edgesETCconnecting each candidate with its authoringtranslator/post-editor pair.
The three sub-networks(GT, GC, and GTC) are illustrated in Figure 4.4.1 Inter-Graph RankingThe framework includes three random walks, oneon GT, one on GCand one on GTC.
A randomwalk on a graph is a Markov chain, its states be-ing the vertices of the graph.
It can be describedby a stochastic square matrix, where the dimen-sion is the number of vertices in the graph, and theentries describe the transition probabilities fromone vertex to the next.
The mutual reinforcementframework couples the two random walks on GTand GCthat rank candidates and Turkers in iso-lation.
The ranking method allows us to obtaina global ranking by taking into account the intra-/inter-component dependencies.
In the followingsections, we describe how we obtain the rankingson GTand GC, and then move on to discuss howthe two are coupled.Our algorithm aims to capture the following in-tuitions.
A candidate is important if 1) it is similarto many of the other proposed candidates and 2)it is authored by better qualified translators and/orpost-editors.
Analogously, a translator/editor pairis believed to be better qualified if 1) the editoris collaborating with a good translator and viceversa and 2) the pair has authored important candi-dates.
This ranking schema is actually a reinforcedprocess across the heterogeneous graphs.
We usetwo vectors c = [pi(c)]|c|?1and t = [pi(t)]|t|?1todenote the saliency scores pi(.)
of candidates andTurker pairs.
The above-mentioned intuitions canbe formulated as follows:?
Homogeneity.
We use adjacency matrix1138Figure 4: 2-step collaborative crowdsourcing translation model based on graph ranking framework in-cluding three sub-networks.
The undirected links between users denotes translation-editing collabora-tion.
The undirected links between candidate translations indicate lexical similarity between candidates.A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer,some linkage is omitted).
A dashed circle indicates the group of candidate translations for a single sourcesentence to translate.
[M ]|c|?|c|to describe the homogeneous affinitybetween candidates and [N ]|t|?|t|to describe theaffinity between Turkers.c ?MTc, t ?
NTt (2)where c = |VC| is the number of vertices in thecandidate graph and t = |VT| is the number of ver-tices in the Turker graph.
The adjacency matrix[M ] denotes the transition probabilities betweencandidates, and analogously matrix [N ] denotesthe affinity between Turker collaboration pairs.?
Heterogeneity.
We use an adjacency matrix[?W ]|c|?|t|and [?W ]|t|?|c|to describe the authorshipbetween the output candidate and the producerTurker pair from both of the candidate-to-Turkerand Turker-to-candidate perspectives.c ?
?WTt, t ?
?WTc (3)All affinity matrices will be defined in the nextsection.
By fusing the above equations, we canhave the following iterative calculation in matrixforms.
For numerical computation of the saliencyscores, the initial scores of all sentences and Turk-ers are set to 1 and the following two steps arealternated until convergence to select the best can-didate.Step 1: compute the saliency scores of candi-dates, and then normalize using `-1 norm.c(n)= (1?
?
)MTc(n?1)+ ?
?W t(n?1)c(n)= c(n)/||c(n)||1(4)Step 2: compute the saliency scores of Turkerpairs, and then normalize using `-1 norm.t(n)= (1?
?
)NTt(n?1)+ ?
?W c(n?1)t(n)= t(n)/||t(n)||1(5)where ?
specifies the relative contributions to thesaliency score trade-off between the homogeneousaffinity and the heterogeneous affinity.
In orderto guarantee the convergence of the iterative form,we must force the transition matrix to be stochasticand irreducible.
To this end, we must make thec and t column stochastic (Langville and Meyer,2004).
c and t are therefore normalized after eachiteration of Equation (4) and (5).4.2 Intra-Graph RankingThe standard PageRank algorithm starts from anarbitrary node and randomly selects to either fol-low a random out-going edge (considering theweighted transition matrix) or to jump to a randomnode (treating all nodes with equal probability).1139In a simple random walk, it is assumed that allnodes in the transitional matrix are equi-probablebefore the walk starts.
Then c and t are calculatedas:c = ?MTc + (1?
?
)1|VC|(6)andt = ?NTt + (1?
?
)1|VT|(7)where 1 is a vector with all elements equaling to 1and the size is correspondent to the size of VCorVT.
?
is the damping factor usually set to 0.85, asin the PageRank algorithm.4.3 Affinity Matrix EstablishmentWe introduce the affinity matrix calculation, in-cluding homogeneous affinity (i.e., M,N ) andheterogeneous affinity (i.e.,?W,?W ).As discussed, we model the collection of can-didates as a weighted undirected graph, GC, inwhich nodes in the graph represent candidate sen-tences and edges represent lexical relatedness.
Wedefine an edge?s weight to be the cosine similaritybetween the candidates represented by the nodesthat it connects.
The adjacency matrix M describessuch a graph, with each entry corresponding to theweight of an edge.F(ci, cj) =ci?
cj||ci||||cj||Mij=F(ci, cj)?kF(ci, ck)(8)where F(.)
is the cosine similarity and c is a termvector corresponding to a candidate.
We treat acandidate as a short document and weight eachterm with tf.idf (Manning et al, 2008), where tfis the term frequency and idf is the inverse docu-ment frequency.The Turker graph, GT, is an undirected graphwhose edges represent ?collaboration.?
Formally,let tiand tjbe two translator/editor pairs; we saythat pair ti?collaborates with?
pair tj(and there-fore, there is an edge between tiand tj) if tiandtjshare either a translator or an editor (or shareboth a translator and an editor).
Let the functionI(ti, tj) denote the number of ?collaborations?
(#col) between tiand tj.I(ti, tj) ={#col (eij?
ET)0 otherwise, (9)Then the adjacency matrix N is then defined asNij=I(ti, tj)?kI(ti, tk)(10)In the bipartite candidate-Turker graph GTC,the entry ETC(i, j) is an indicator function denot-ing whether the candidate ciis generated by tj:A(ci, tj) ={1 (eij?
ETC)0 otherwise(11)Through ETCwe define the weight matrices?Wijand?Wij, containing the conditional probabil-ities of transitions from cito tjand vice versa:?Wij=A(ci, tj)?kA(ci, tk),?Wij=A(ci, tj)?kA(ck, tj)(12)5 EvaluationWe are interested in testing our random walkmethod, which incorporates information fromboth the candidate translations and from the Turk-ers.
We want to test two versions of our pro-posed collaborative co-ranking method: 1) basedon the unedited translations only and 2) based onthe edited sentences after translator/editor collab-orations.Metric Since we have four professional transla-tion sets, we can calculate the Bilingual Evalu-ation Understudy (BLEU) score (Papineni et al,2002) for one professional translator (P1) usingthe other three (P2,3,4) as a reference set.
Werepeat the process four times, scoring each pro-fessional translator against the others, to calculatethe expected range of professional quality transla-tion.
In the following sections, we evaluate each ofour methods by calculating BLEU scores againstthe same four sets of three reference translations.Therefore, each number reported in our experi-mental results is an average of four numbers, cor-responding to the four possible ways of choosing 3of the 4 reference sets.
This allows us to comparethe BLEU score achieved by our methods againstthe BLEU scores achievable by professional trans-lators.Baselines As a naive baseline, we choose onecandidate translation at random for each inputUrdu sentence.
To establish an upper bound forour methods, and to determine if there exist high-quality Turker translations at all, we compute four1140Reference (Avg.)
42.51Oracle (Seg-Trans) 44.93Oracle (Seg-Trans+Edit) 48.44Oracle (Turker-Trans) 38.66Oracle (Turker-Trans+Edit) 39.16Random 30.52Lowest TER 35.78Graph Ranking (Trans) 38.88Graph Ranking (Trans+Edit) 41.43Table 2: Overall BLEU performance for allmethods (with and without post-editing).
Thehighlighted result indicates the best performance,which is based on both candidate sentences andTurker information.oracle scores.
The first oracle operates at the seg-ment level on the sentences produced by transla-tors only: for each source segment, we choosefrom the translations the one that scores highest(in terms of BLEU) against the reference sen-tences.
The second oracle is applied similarly,but chooses from the candidates produced by thecollaboration of translator/post-editor pairs.
Thethird oracle operates at the worker level: for eachsource segment, we choose from the translationsthe one provided by the worker whose transla-tions (over all sentences) score the highest onaverage.
The fourth oracle also operates at theworker level, but selects from sentences producedby translator/post-editor collaborations.
These or-acle methods represent ideal solutions under ourscenario.
We also examine two voting-inspiredmethods.
The first method selects the translationwith the minimum average TER (Snover et al,2006) against the other translations; intuitively,this would represent the ?consensus?
translation.The second method selects the translation gen-erated by the Turker who, on average, providestranslations with the minimum average TER.Results A summary of our results in given in Ta-ble 2.
As expected, random selection yields badperformance, with a BLEU score of 30.52.
Theoracles indicate that there is usually an acceptabletranslation from the Turkers for any given sen-tence.
Since the oracles select from a small groupof only 4 translations per source segment, they arenot overly optimistic, and rather reflect the true po-tential of the collected translations.
On average,the reference translations give a score of 42.38.
Toput this in perspective, the output of a state-of-the-Figure 5: Effect of candidate-Turker coupling (?
)on BLEU score.art machine translation system (the syntax-basedvariant of Joshua) achieves a score of 26.91, whichis reported in (Zaidan and Callison-Burch, 2011).The approach which selects the translations withthe minimum average TER (Snover et al, 2006)against the other three translations (the ?consen-sus?
translation) achieves BLEU scores of 35.78.Using the raw translations without post-editing,our graph-based ranking method achieves a BLEUscore of 38.89, compared to Zaidan and Callison-Burch (2011)?
s reported score of 28.13, whichthey achieved using a linear feature-based classi-fication.
Their linear classifier achieved a reportedscore of 39.062when combining information fromboth translators and editors.
In contrast, our pro-posed graph-based ranking framework achieves ascore of 41.43 when using the same information.This boost in BLEU score confirms our intuitionthat the hidden collaboration networks betweencandidate translations and transltor/editor pairs areindeed useful.Parameter Tuning There are two parameters inour experimental setups: ?
controls the probabilityof starting a new random walk and ?
controls thecoupling between the candidate and Turker sub-graphs.
We set the damping factor ?
to 0.85, fol-lowing the standard PageRank paradigm.
In orderto determine a value for ?, we used the averageBLEU, computed against the professional refer-2Note that the data we used in our experiments are slightlydifferent, by discarding nearly 100 NULL sentences in theraw data.
We do not re-implement this baseline but report theresults from the paper directly.
According to our experiments,most of the results generated by baselines and oracles are veryclose to the previously reported values.1141Plain ranking 38.89w/o collaboration 38.88Shared translator 41.38Shared post-editor 41.29Shared Turker 41.43Table 3: Variations of all component settings.ence translations, as a tuning metric.
We experi-mented with values of ?
ranging from 0 to 1, witha step size of 0.05 (Figure 5).
Small ?
values placelittle emphasis on the candidate/Turker coupling,whereas larger values rely more heavily on the co-ranking.
Overall, we observed better performancewith values within the range of 0.05-0.15.
Thissuggests that both sources of information?
the can-didate itself and its authors?
are important for thecrowdsourcing translation task.
In all of our re-ported results, we used the ?
= 0.1.Analysis We examine the relative contributionof each component of our approach on the overallperformance.
We first examine the centroid-basedranking on the candidate sub-graph (GC) aloneto see the effect of voting among translated sen-tences; we denote this strategy as plain ranking.Then we incorporate the standard random walk onthe Turker graph (GT) to include the structural in-formation but without yet including any collabo-ration information; that is, we incorporate infor-mation from GTand GCwithout including edgeslinking the two together.
The co-ranking paradigmis exactly the same as the framework described inSection 3.2, but with simplified structures.Finally, we examine the two-step collaborationbased candidate-Turker graph using several varia-tions on edge establishment.
As before, the nodesare the translator/post-editor working pairs.
Weinvestigate three settings in which 1) edges con-nect two nodes when they share only a transla-tor, 2) edges connect two nodes when they shareonly a post-editor, and 3) edges connect two nodeswhen they share either a translator or a post-editor.These results are summarized in Table 3.Interestingly, we observe that when modelingthe linkage between the collaboration pairs, con-necting Turker pairs which share either a transla-tor or the post-editor achieves better performancethan connecting pairs that share only translators orconnecting pairs which share only editors.
Thisresult supports the intuition that a denser collabo-ration matrix will help propagate saliency to goodtranslators/post-editors and hence provides betterpredictions for candidate quality.6 ConclusionWe have proposed an algorithm for using a two-step collaboration between non-professional trans-lators and post-editors to obtain professional-quality translations.
Our method, based on aco-ranking model, selects the best crowdsourcedtranslation from a set of candidates, and is capableof selecting translations which near professionalquality.Crowdsourcing can play a pivotal role in fu-ture efforts to create parallel translation datasets.In addition to its benefits of cost and scalabil-ity, crowdsourcing provides access to languagesthat currently fall outside the scope of statisticalmachine translation research.
In future work oncrowdsourced translation, further benefits in qual-ity improvement and cost reduction could stemfrom 1) building ground truth data sets based onhigh-quality Turkers?
translations and 2) identify-ing when sufficient data has been collected for agiven input, to avoid soliciting unnecessary redun-dant translations.AcknowledgementsThis material is based on research sponsored bya DARPA Computer Science Study Panel phase 3award entitled ?Crowdsourcing Translation?
(con-tract D12PC00368).
The views and conclusionscontained in this publication are those of the au-thors and should not be interpreted as represent-ing official policies or endorsements by DARPAor the U.S. Government.
This research was sup-ported by the Johns Hopkins University HumanLanguage Technology Center of Excellence andthrough gifts from Microsoft, Google and Face-book.ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
On theuse of comparable corpora to improve SMT perfor-mance.
In Proceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL 2009), pages16?23, March.Vamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation sys-tems?
In Workshop on Creating Speech and Lan-guage Data with MTurk.1142Vamshi Ambati, Stephan Vogel, and Jaime G Car-bonell.
2010.
Active learning and crowd-sourcingfor machine translation.
In LREC, volume 11, pages2169?2174.
Citeseer.Vamshi Ambati.
2012.
Active Learning and Crowd-sourcing for Machine Translation in Low ResourceScenarios.
Ph.D. thesis, Language Technologies In-stitute, School of Computer Science, Carnegie Mel-lon University, Pittsburgh, PA.Michael S. Bernstein, Greg Little, Robert C. Miller,Bjrn Hartmann, Mark S. Ackerman, David R.Karger, David Crowell, and Katrina Panovich.2010.
Soylent: a word processor with a crowd in-side.
In Proceedings of the ACM Symposium onUser Interface Software and Technology (UIST).Michael Bloodgood and Chris Callison-Burch.
2010.Large-scale cost-focused active learning for statisti-cal machine translation.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics.Chris Callison-Burch and Mark Dredze.
2010.
Cre-ating speech and language data with Amazon?s Me-chanical Turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 1?12,June.Chris Callison-Burch.
2005.
Linear B system de-scription for the 2005 NIST MT evaluation exercise.In Proceedings of Machine Translation EvaluationWorkshop.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using amazon?s me-chanical turk.
In Proceedings of EMNLP.David L. Chen and William B. Dolan.
2012.
Build-ing a persistent workforce on mechanical turk formultilingual data collection.
In Proceedings of theHuman Computer Interaction International Confer-ence.Ulrich Germann.
2001.
Building a statistical machinetranslation system from scratch: How much bangfor the buck can we expect?
In Proceedings ofthe Workshop on Data-driven Methods in MachineTranslation - Volume 14, DMMT ?01, pages 1?8.Spence Green, Jeffrey Heer, and Christopher D. Man-ning.
2013.
The efficacy of human post-editing forlanguage translation.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, CHI ?13, pages 439?448.Juan Pablo Hourcade, Benjamin B Bederson, AllisonDruin, Anne Rose, Allison Farber, and YoshifumiTakayama.
2003.
The international children?s digi-tal library: viewing digital books online.
Interactingwith Computers, 15(2):151?167.Chang Hu, Benjamin B. Bederson, and Philip Resnik.2010.
Translation by iterative collaboration be-tween monolingual users.
In Proceedings ofACM SIGKDD Workshop on Human Computation(HCOMP).Chang Hu, Benjamin B. Bederson, Philip Resnik, andYakov Kronrod.
2011.
Monotrans2: A new humancomputation system to support monolingual trans-lation.
In Proceedings of the SIGCHI Conferenceon Human Factors in Computing Systems, CHI ?11,pages 1133?1136.Chang Hu.
2009.
Collaborative translation by mono-lingual users.
In CHI ?09 Extended Abstracts on Hu-man Factors in Computing Systems, CHI EA ?09,pages 3105?3108.Martin Kay.
1998.
The proper place of men and ma-chines in language translation.
Machine Transla-tion, 12(1/2):3?23, January.Kevin Knight and Ishwar Chander.
1994.
Automatedpostediting of documents.
In In Proceedings ofAAAI.Philipp Koehn.
2010.
Enabling monolingual transla-tors: Post-editing vs. options.
In HLT-NAACL?10,pages 537?545, June.Amy N Langville and Carl D Meyer.
2004.
Deeperinside pagerank.
Internet Mathematics, 1(3):335?380.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation withsyntax, semirings, discriminative training and othergoodies.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 133?137, July.Annie Louis and Ani Nenkova.
2013.
What makeswriting great?
first experiments on article qualityprediction in the science journalism domain.
Trans-actions of Association for Computational Linguis-tics.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Daisuke Morita and Toru Ishida.
2009a.
Collaborativetranslation by monolinguals with machine transla-tors.
In Proceedings of the 14th International Con-ference on Intelligent User Interfaces, IUI ?09, pages361?366.Daisuke Morita and Toru Ishida.
2009b.
Designingprotocols for collaborative translation.
In Interna-tional Conference on Principles of Practice in Multi-Agent Systems (PRIMA-09), pages 17?32.
Springer.1143Dragos Stefan Munteanu and Daniel Marcu.
2005.Improving machine translation performance by ex-ploiting non-parallel corpora.
Comput.
Linguist.,31(4):477?504, December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318.Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,and Chris Callison-Burch.
2014.
The language de-mographics of Amazon Mechanical Turk.
Transac-tions of the Association for Computational Linguis-tics (TACL), 2(Feb):79?92.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing parallel corpora for six Indianlanguages via crowdsourcing.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion.Philip Resnik and Noah A. Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380, September.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compa-rable corpora using document level alignment.
InHLT-NAACL?10, pages 403?411.Jason Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch, andAdam Lopez.
2013.
Dirt cheap web-scale paral-lel text from the Common Crawl.
In Proceedings ofthe 2013 Conference of the Association for Compu-tational Linguistics (ACL 2013), July.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine transla-tion in the Americas, pages 223?231.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for nat-ural language tasks.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 254?263.TechCrunch.
2008.
Facebook taps users to createtranslated versions of site, January.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large scale parallel docu-ment mining for machine translation.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 1101?1109.Luis von Ahn.
2013.
Duolingo: Learn a language forfree while helping to translate the web.
In Proceed-ings of the 2013 International Conference on Intel-ligent User Interfaces, IUI ?13, pages 1?2.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li, and Yan Zhang.
2011a.
Timeline gen-eration through evolutionary trans-temporal summa-rization.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 433?443.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011b.
Evolutionarytimeline summarization: A balanced optimizationframework via iterative substitution.
In Proceed-ings of the 34th International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR ?11, pages 745?754.Rui Yan, Mirella Lapata, and Xiaoming Li.
2012a.Tweet recommendation with graph co-ranking.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers- Volume 1, ACL ?12, pages 516?525.Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne XinZhao, Pu-Jen Cheng, and Xiaoming Li.
2012b.Visualizing timelines: Evolutionary summarizationvia iterative reinforcement between text and imagestreams.
In Proceedings of the 21st ACM Interna-tional Conference on Information and KnowledgeManagement, CIKM ?12, pages 275?284.Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xi-aoming Li.
2012c.
Hierarchical graph summariza-tion: leveraging hybrid information through visibleand invisible linkage.
In PAKDD?12, pages 97?108.Springer.Omar F. Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1220?1229.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012.
Machine translation of Arabic di-alects.
In The 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics.Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,Richard Schwartz, and John Makhoul.
2013.Systematic comparison of professional and crowd-sourced reference translations for machine transla-tion.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Atlanta, Georgia.Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He,and Xiaoming Li.
2013.
Timeline generation withsocial attention.
In Proceedings of the 36th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?13,pages 1061?1064.1144
