Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Statistical Machine Translation Model Based on a SyntheticSynchronous GrammarHongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo WangSchool of Computer Science and TechnologyHarbin Institute of Technology{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cnAbstractRecently, various synchronous grammarsare proposed for syntax-based machinetranslation, e.g.
synchronous context-freegrammar and synchronous tree (sequence)substitution grammar, either purely for-mal or linguistically motivated.
Aim-ing at combining the strengths of differ-ent grammars, we describes a syntheticsynchronous grammar (SSG), which ten-tatively in this paper, integrates a syn-chronous context-free grammar (SCFG)and a synchronous tree sequence substitu-tion grammar (STSSG) for statistical ma-chine translation.
The experimental re-sults on NIST MT05 Chinese-to-Englishtest set show that the SSG based transla-tion system achieves significant improve-ment over three baseline systems.1 IntroductionThe use of various synchronous grammar basedformalisms has been a trend for statistical ma-chine translation (SMT) (Wu, 1997; Eisner, 2003;Galley et al, 2006; Chiang, 2007; Zhang et al,2008).
The grammar formalism determines the in-trinsic capacities and computational efficiency ofthe SMT systems.To evaluate the capacity of a grammar formal-ism, two factors, i.e.
generative power and expres-sive power are usually considered (Su and Chang,1990).
The generative power refers to the abil-ity to generate the strings of the language, andthe expressive power to the ability to describe thesame language with fewer or no extra ambigui-ties.
For the current synchronous grammars basedSMT, to some extent, the generalization ability ofthe grammar rules (the usability of the rules for thenew sentences) can be considered as a kind of thegenerative power of the grammar and the disam-biguition ability to the rule candidates can be con-sidered as an embodiment of expressive power.However, the generalization ability and the dis-ambiguition ability often contradict each other inpractice such that various grammar formalismsin SMT are actually different trade-off be-tween them.
For instance, in our investiga-tions for SMT (Section 3.1), the Formally SCFGbased hierarchical phrase-based model (here-inafter FSCFG) (Chiang, 2007) has a better gen-eralization capability than a Linguistically moti-vated STSSG based model (hereinafter LSTSSG)(Zhang et al, 2008), with 5% rules of the formermatched by NIST05 test set while only 3.5% rulesof the latter matched by the same test set.
How-ever, from expressiveness point of view, the for-mer usually results in more ambiguities than thelatter.To combine the strengths of different syn-chronous grammars, this paper proposes a statisti-cal machine translation model based on a syntheticsynchronous grammar (SSG) which syncretizesFSCFG and LSTSSG.
Moreover, it is noteworthythat, from the combination point of view, our pro-posed scheme can be considered as a novel systemcombination method which goes beyond the ex-isting post-decoding style combination of N -besthypotheses from different systems.2 The Translation Model Based on theSynthetic Synchronous Grammar2.1 The Synthetic Synchronous GrammarFormally, the proposed Synthetic SynchronousGrammar (SSG) is a tupleG = ?
?s,?t, Ns, Nt, X,P?where ?s(?t) is the alphabet set of source (target)terminals, namely the vocabulary; Ns(Nt) is thealphabet set of source (target) non-terminals, such125?
?
??
?Figure 1: A syntax tree pair example.
Dotted linesstands for the word alignments.as the POS tags and the syntax labels; X repre-sents the special nonterminal label in FSCFG; andP is the grammar rule set which is the core part ofa grammar.
Every rule r in P is as:r = ?
?, ?,ANT, AT, ??
?where ?
?
[{X}, Ns,?s]+is a sequence of one ormore source words in ?sand nonterminals sym-bols in [{X}, Ns];?
?
[{X}, Nt,?t]+is a se-quence of one or more target words in ?tand non-terminals symbols in [{X}, Nt]; ATis a many-to-many corresponding set which includes the align-ments between the terminal leaf nodes from sourceand target side, and ANTis a one-to-one corre-sponding set which includes the synchronizing re-lations between the non-terminal leaf nodes fromsource and target side; ??
contains feature valuesassociated with each rule.Through this formalization, we can see thatFSCFG rules and LSTSSG rules are both in-cluded.
However, we should point out that therules with mixture of X non-terminals and syn-tactic non-terminals are not included in our cur-rent implementation despite that they are legalunder the proposed formalism.
The rule extrac-tion in current implementation can be consideredas a combination of the ones in (Chiang, 2007)and (Zhang et al, 2008).
Given the sentence pairin Figure 1, some SSG rules can be extracted asillustrated in Figure 2.2.2 The SSG-based Translation ModelThe translation in our SSG-based translationmodel can be treated as a SSG derivation.
Aderivation consists of a sequence of grammar ruleapplications.
To model the derivations as a latentvariable, we define the conditional probability dis-tribution over the target translation e and the cor-Input: A source parse tree T (fJ1)Output: A target translation e?for u := 0 to J ?
1 dofor v := 1 to J ?
u doforeach rule r = ?
?, ?,ANT, AT, ???
spanning[v, v + u] doif ANTof r is empty thenAdd r into H[v, v + u];endelseSubstitute the non-terminal leaf node pair(Nsrc, Ntgt) with the hypotheses in thehypotheses stack corresponding with Nsrc?sspan iteratively.endendendendOutput the 1-best hypothesis in H[1, J] as the final translation.Figure 3: The pseudocode for the decoding.responding derivation d of a given source sentencef as(1) p?
(d, e|f) =exp?k?kHk(d, e, f)??
(f)where Hkis a feature function ,?kis the corre-sponding feature weight and ??
(f) is a normal-ization factor for each derivation of f. The mainchallenge of SSG-based model is how to distin-guish and weight the different kinds of derivations.
For a simple illustration, using the rules listed inFigure 2, three derivations can be produced for thesentence pair in Figure 1 by the proposed model:d1= (R4, R1, R2)d2= (R6, R7, R8)d3= (R4, R7, R2)All of them are SSG derivations while d1is also aFSCFG derivation, d2is also a LSTSSG deriva-tion.
Ideally, the model is supposed to be ableto weight them differently and to prefer the betterderivation, which deserves intensive study.
Somesophisticated features can be designed for this is-sue.
For example, some features related withstructure richness and grammar consistency1of aderivation should be designed to distinguish thederivations involved various heterogeneous ruleapplications.
For the page limit and the fair com-parison, we only adopt the conventional featuresas in (Zhang et al, 2008) in our current implemen-tation.1This relates with reviewers?
questions: ?can a rule ex-pecting an NN accept an X??
and ?.
.
.
the interaction betweenthe two typed of rules .
.
.
?.
In our study in progress, wewould design some features to distinguish the derivation stepswhich fulfill the expectation or not, to measure how muchheterogeneous rules are applied in a derivation and so on.126R61?BAVV[2]NN[1]1VB[2] NP[1]?PNto meTO PRPPP1R7pentheDT NNNP?
?NN1R4 Give 1?
1 X[1] X[2]X[2]?
X[1] R5 X[1]X[1] ?
2 the pen 1 to 2me1?
?R1 penthe 1??
1 R3 theGive 2 pen 1?
2??
1R2 to me 1?
1R8?VVGiveVB11Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure1.
R1-R3are bilingual phrase rules, R4-R5are FSCFG rules and R6-R8are LSTSSG rules.2.3 DecodingFor efficiency, our model approximately search forthe single ?best?
derivation using beam search as(2) (?e,?d) = argmaxe,d{?k?khk(d, e, f)}.The major challenge for such a SSG-based de-coder is how to apply the heterogeneous rules in aderivation.
For example, (Chiang, 2007) adopts aCKY style span-based decoding while (Liu et al,2006) applies a linguistically syntax node basedbottom-up decoding, which are difficult to inte-grate.
Fortunately, our current SSG syncretizesFSCFG and LSTSSG.
And the conventional de-codings of both FSCFG and LSTSSG are span-based expansion.
Thus, it would be a natural wayfor our SSG-based decoder to conduct a span-based beam search.
The search procedure is givenby the pseudocode in Figure 3.
A hypothesesstack H[i, j] (similar to the ?chart cell?
in CKYparsing) is arranged for each span [i, j] for stor-ing the translation hypotheses.
The hypothesesstacks are ordered such that every span is trans-lated after its possible antecedents: smaller spansbefore larger spans.
For translating each span[i, j], the decoder traverses each usable rule r =?
?, ?,ANT, AT, ???.
If there is no nonterminalleaf node in r, the target side ?
will be added intoH[i, j] as the candidate hypothesis.
Otherwise, thenonterminal leaf nodes in r should be substitutediteratively by the corresponding hypotheses untilall nonterminal leaf nodes are processed.
The keyfeature of our decoder is that the derivations arebased on synthetic grammar, so that one derivationmay consist of applications of heterogeneous rules(Please see d3in Section 2.2 as a simple demon-stration).3 Experiments and DiscussionsOur system, named HITREE, is implemented instandard C++ and STL.
In this section we reportExtracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)BP 11,137 4,613(41.4%) 323(0.5%)LSTSSG 45,580 28,497(62.5%) 984(3.5%)FSCFG 59,339 25,520(43.0%) 1,266(5.0%)HITREE 93,782 49,404(52.7%) 1,927(3.9%)Table 1: The statistics of the counts of the rules indifferent phases.
?k?
means one thousand.on experiments with Chinese-to-English transla-tion base on it.
We used FBIS Chinese-to-Englishparallel corpora (7.2M+9.2M words) as the train-ing data.
We also used SRI Language Model-ing Toolkit to train a 4-gram language model onthe Xinhua portion of the English Gigaword cor-pus(181M words).
NIST MT2002 test set is usedas the development set.
The NIST MT2005 testset is used as the test set.
The evaluation met-ric is case-sensitive BLEU4.
For significant test,we used Zhang?s implementation (Zhang et al,2004)(confidence level of 95%).
For comparisons,we used the following three baseline systems:LSTSSG An in-house implementation of linguis-tically motivated STSSG based model similarto (Zhang et al, 2008).FSCFG An in-house implementation of purelyformally SCFG based model similar to (Chiang,2007).MBR We use an in-house combination systemwhich is an implementation of a classic sentencelevel combination method based on the MinimumBayes Risk (MBR) decoding (Kumar and Byrne,2004).3.1 Statistics of Rule Numbers in DifferentPhasesTable 1 summarizes the statistics of the rules fordifferent models in three phases: after extrac-tion (Extracted), after scoring(Scored), and af-ter filtering (Filtered) (filtered by NIST05 testset just, similar to the filtering step in phrase-based SMT system).
In Extracted phase, FSCFG127ID System BLEU4 #of used rules(k)1 LSTSSG 0.2659?0.0043 9842 FSCFG 0.2613?0.0045 1,2663 HITREE 0.2730?0.0045 1,9274 MBR(1,2) 0.2685?0.0044 ?Table 2: The Comparison of LSTSSG, FSCFG,HITREE and the MBR.has obvious more rules than LSTSSG.
However,in Scored phase, this situation reverses.
Inter-estingly, the situation reverses again in Filteredphase.
The reasons for these phenomenons arethat FSCFG abstract rules involves high-degreegeneralization.
Each FSCFG abstract rule aver-agely have several duplicates2in the extracted ruleset.
Then, the duplicates will be discarded dur-ing scoring.
However, due to the high-degree gen-eralization , the FSCFG abstract rules are morelikely to be matched by the test sentences.
Con-trastively, LSTSSG rules have more diversifiedstructures and thus weaker generalization capabil-ity than FSCFG rules.
From the ratios of two tran-sition states, Table 1 indicates that HITREE canbe considered as compromise of FSCFG betweenLSTSSG.3.2 Overall PerformancesThe performance comparison results are presentedin Table 2.
The experimental results show thatthe SSG-based model (HITREE) achieves signifi-cant improvements over the models based on thetwo isolated grammars: FSCFG and LSTSSG(both p < 0.001).
From combination point ofview, the newly proposed model can be consid-ered as a novel method going beyond the con-ventional post-decoding style combination meth-ods.
The baseline Minimum Bayes Risk com-bination of LSTSSG based model and FSCFGbased model (MBR(1, 2)) obtains significant im-provements over both candidate models (both p <0.001).
Meanwhile, the experimental results showthat the proposed model outperforms MBR(1, 2)significantly (p < 0.001).
These preliminary re-sults indicate that the proposed SSG-based modelis rather promising and it may serve as an alterna-tive, if not superior, to current combination meth-ods.4 ConclusionsTo combine the strengths of different gram-mars, this paper proposes a statistical machine2Rules with identical source side and target side are du-plicated.translation model based on a synthetic syn-chronous grammar (SSG) which syncretizes apurely formal synchronous context-free gram-mar (FSCFG) and a linguistically motivated syn-chronous tree sequence substitution grammar(LSTSSG).
Experimental results show that SSG-based model achieves significant improvementsover the FSCFG-based model and LSTSSG-basedmodel.In the future work, we would like to verifythe effectiveness of the proposed model on vari-ous datasets and to design more sophisticated fea-tures.
Furthermore, the integrations of more dif-ferent kinds of synchronous grammars for statisti-cal machine translation will be investigated.AcknowledgmentsThis work is supported by the Key Program ofNational Natural Science Foundation of China(60736014), and the Key Project of the NationalHigh Technology Research and Development Pro-gram of China (2006AA010108).ReferencesDavid Chiang.
2007.
Hierarchical phrase-based trans-lation.
In computational linguistics, 33(2).Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceedingsof ACL 2003.Galley, M. and Graehl, J. and Knight, K. and Marcu,D.
and DeNeefe, S. and Wang, W. and Thayer, I.2006.
Scalable inference and training of context-rich syntactic translation models In Proceedings ofACL-COLING.S.
Kumar and W. Byrne.
2004.
Minimum Bayes-riskdecoding for statistical machine translation.
In HLT-04.Yang Liu, Qun Liu, Shouxun Lin.
2006.
Tree-to-stringalignment template for statistical machine transla-tion.
In Proceedings of ACL-COLING.Keh-Yin Su and Jing-Shin Chang.
1990.
Some keyIssues in Designing Machine Translation Systems.Machine Translation, 5(4):265-300.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377-403.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much im-provement do we need to have a better system?
InProceedings of LREC 2004, pages 2051-2054.Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,Chew Lim Tan and Sheng Li.
2008.
A tree sequencealignment-based tree-to-tree translation model.
InProceedings of ACL-HLT.128
