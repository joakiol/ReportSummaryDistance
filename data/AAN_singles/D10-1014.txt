Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138?147,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsSoft Syntactic Constraints for Hierarchical Phrase-based TranslationUsing Latent Syntactic DistributionsZhongqiang HuangInstitute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742zqhuang@umiacs.umd.eduMartin C?mejrek and Bowen ZhouIBM T. J. Watson Research CenterYorktown Heights, NY 10598{martin.cmejrek,zhou}@us.ibm.comAbstractIn this paper, we present a novel approachto enhance hierarchical phrase-based machinetranslation systems with linguistically moti-vated syntactic features.
Rather than directlyusing treebank categories as in previous stud-ies, we learn a set of linguistically-guided la-tent syntactic categories automatically from asource-side parsed, word-aligned parallel cor-pus, based on the hierarchical structure amongphrase pairs as well as the syntactic structureof the source side.
In our model, each X non-terminal in a SCFG rule is decorated with areal-valued feature vector computed based onits distribution of latent syntactic categories.These feature vectors are utilized at decod-ing time to measure the similarity between thesyntactic analysis of the source side and thesyntax of the SCFG rules that are applied toderive translations.
Our approach maintainsthe advantages of hierarchical phrase-basedtranslation systems while at the same time nat-urally incorporates soft syntactic constraints.1 IntroductionIn recent years, syntax-based translation mod-els (Chiang, 2007; Galley et al, 2004; Liu etal., 2006) have shown promising progress in im-proving translation quality, thanks to the incorpora-tion of phrasal translation adopted from the widelyused phrase-based models (Och and Ney, 2004) tohandle local fluency and the engagement of syn-chronous context-free grammars (SCFG) to handlenon-local phrase reordering.
Approaches to syntax-based translation models can be largely categorizedinto two classes based on their dependency on anno-tated corpus (Chiang, 2007).
Linguistically syntax-based models (e.g., (Yamada and Knight, 2001; Gal-ley et al, 2004; Liu et al, 2006)) utilize structuresdefined over linguistic theory and annotations (e.g.,Penn Treebank) and guide the derivation of SCFGrules with explicit parsing on at least one side ofthe parallel corpus.
Formally syntax-based mod-els (e.g., (Wu, 1997; Chiang, 2007)) extract syn-chronous grammars from parallel corpora based onthe hierarchical structure of natural language pairswithout any explicit linguistic knowledge or anno-tations.
In this work, we focus on the hierarchi-cal phrase-based models of Chiang (2007), whichis formally syntax-based, and always refer the termSCFG, from now on, to the grammars of this modelclass.On the one hand, hierarchical phrase-based mod-els do not suffer from errors in syntactic constraintsthat are unavoidable in linguistically syntax-basedmodels.
Despite the complete lack of linguisticguidance, the performance of hierarchical phrase-based models is competitive when compared to lin-guistically syntax-based models.
As shown in (Miand Huang, 2008), hierarchical phrase-based modelssignificantly outperform tree-to-string models (Liuet al, 2006; Huang et al, 2006), even when at-tempts are made to alleviate parsing errors usingeither forest-based decoding (Mi et al, 2008) orforest-based rule extraction (Mi and Huang, 2008).On the other hand, when properly used, syntac-tic constraints can provide invaluable benefits to im-prove translation quality.
The tree-to-string mod-els of Mi and Huang (2008) can actually signif-138icantly outperform hierarchical phrase-based mod-els when using forest-based rule extraction togetherwith forest-based decoding.
Chiang (2010) also ob-tained significant improvement over his hierarchi-cal baseline by using syntactic parse trees on bothsource and target sides to induce fuzzy (not exact)tree-to-tree rules and by also allowing syntacticallymismatched substitutions.In this paper, we augment rules in hierarchicalphrase-based translation systems with novel syntac-tic features.
Unlike previous studies (e.g., (Zoll-mann and Venugopal, 2006)) that directly use ex-plicit treebank categories such as NP, NP/PP (NPmissing PP from the right) to annotate phrase pairs,we induce a set of latent categories to capture thesyntactic dependencies inherent in the hierarchicalstructure of phrase pairs, and derive a real-valuedfeature vector for each X nonterminal of a SCFGrule based on the distribution of the latent cate-gories.
Moreover, we convert the equality test oftwo sequences of syntactic categories, which are ei-ther identical or different, into the computation ofa similarity score between their corresponding fea-ture vectors.
In our model, two symbolically dif-ferent sequences of syntactic categories could havea high similarity score in the feature vector repre-sentation if they are syntactically similar, and a lowscore otherwise.
In decoding, these feature vectorsare utilized to measure the similarity between thesyntactic analysis of the source side and the syntaxof the SCFG rules that are applied to derive trans-lations.
Our approach maintains the advantages ofhierarchical phrase-based translation systems whileat the same time naturally incorporates soft syntacticconstraints.
To the best of our knowledge, this is thefirst work that applies real-valued syntactic featurevectors to machine translation.The rest of the paper is organized as follows.Section 2 briefly reviews hierarchical phrase-basedtranslation models.
Section 3 presents an overviewof our approach, followed by Section 4 describingthe hierarchical structure of aligned phrase pairs andSection 5 describing how to induce latent syntacticcategories.
Experimental results are reported in Sec-tion 6, followed by discussions in Section 7.
Sec-tion 8 concludes this paper.2 Hierarchical Phrase-Based TranslationAn SCFG is a synchronous rewriting system gener-ating source and target side string pairs simultane-ously based on a context-free grammar.
Each syn-chronous production (i.e., rule) rewrites a nonter-minal into a pair of strings, ?
and ?, where ?
(or?)
contains terminal and nonterminal symbols fromthe source (or target) language and there is a one-to-one correspondence between the nonterminal sym-bols on both sides.
In particular, the hierarchicalmodel (Chiang, 2007) studied in this paper exploreshierarchical structures of natural language and uti-lize only a unified nonterminal symbol X in thegrammar,X ?
?
?, ?,?
?where ?
is the one-to-one correspondence betweenX?s in ?
and ?, and it can be indicated by un-derscripted co-indexes.
Two example English-to-Chinese translation rules are represented as follows:X ?
?give the pen to me,?????
(1)X ?
?giveX1 to me, X1???
(2)The SCFG rules of hierarchical phrase-basedmodels are extracted automatically from corpora ofword-aligned parallel sentence pairs (Brown et al,1993; Och and Ney, 2000).
An aligned sentence pairis a tuple (E,F,A), where E = e1 ?
?
?
en can be in-terpreted as an English sentence of length n, F =f1 ?
?
?
fm its translation of length m in a foreign lan-guage, andA a set of links between words of the twosentences.
Figure 1 (a) shows an example of alignedEnglish-to-Chinese sentence pair.
Widely adoptedin phrase-based models (Och and Ney, 2004), a pairof consecutive sequences of words from E and F isa phrase pair if all words are aligned only within thesequences and not to any word outside.
We call a se-quence of words a phrase if it corresponds to eitherside of a phrase pair, and a non-phrase otherwise.Note that the boundary words of a phrase pair maynot be aligned to any other word.
We call the phrasepairs with all boundary words aligned tight phrasepairs (Zhang et al, 2008).
A tight phrase pair is theminimal phrase pair among all that share the sameset of alignment links.
Figure 1 (b) highlights thetight phrase pairs in the example sentence pair.13965432112 3 4 5(a) (b)Figure 1: An example of word-aligned sentence pair (a)with tight phrase pairs marked in a matrix representation(b).The extraction of SCFG rules proceeds as fol-lows.
In the first step, all phrase pairs below a max-imum length are extracted as phrasal rules.
In thesecond step, abstract rules are extracted from tightphrase pairs that contain other tight phrase pairs byreplacing the sub phrase pairs with co-indexed X-nonterminals.
Chiang (2007) also introduced severalrequirements (e.g., there are at most two nontermi-nals at the right hand side of a rule) to safeguardthe quality of the abstract rules as well as keepingdecoding efficient.
In our example above, rule (2)can be extracted from rule (1) with the following subphrase pair:X ?
?the pen,??
?The use of a unified X nonterminal makes hier-archical phrase-based models flexible at capturingnon-local reordering of phrases.
However, such flex-ibility also comes at the cost that it is not able todifferentiate between different syntactic usages ofphrases.
Suppose rule X ?
?I am readingX1, ?
?
?
?is extracted from a phrase pair with I am reading abook on the source side whereX1 is abstracted fromthe noun phrase pair .
If this rule is used to translateI am reading the brochure of a book fair, it wouldbe better to apply it over the entire string than oversub-strings such as I ... the brochure of.
This is be-cause the nonterminal X1 in the rule was abstractedfrom a noun phrase on the source side of the trainingdata and would thus be better (more informative) tobe applied to phrases of the same type.
Hierarchi-cal phrase-based models are not able to distinguishsyntactic differences like this.Zollmann and Venugopal (2006) attempted to ad-dress this problem by annotating phrase pairs withtreebank categories based on automatic parse trees.They introduced an extended set of categories (e.g.,NP+V for she went and DT\NP for great wall, announ phrase with a missing determiner on the left)to annotate phrase pairs that do not align with syn-tactic constituents.
Their hard syntactic constraintrequires that the nonterminals should match exactlyto rewrite with a rule, which could rule out poten-tially correct derivations due to errors in the syn-tactic parses as well as to data sparsity.
For exam-ple, NP cannot be instantiated with phrase pairs oftype DT+NN, in spite of their syntactic similarity.Venugopal et al (2009) addressed this problem bydirectly introducing soft syntactic preferences intoSCFG rules using preference grammars, but theyhad to face the computational challenges of largepreference vectors.
Chiang (2010) also avoided hardconstraints and took a soft alternative that directlymodels the cost of mismatched rule substitutions.This, however, would require a large number of pa-rameters to be tuned on a generally small-sized held-out set, and it could thus suffer from over-tuning.3 Approach OverviewIn this work, we take a different approach to intro-duce linguistic syntax to hierarchical phrase-basedtranslation systems and impose soft syntactic con-straints between derivation rules and the syntacticparse of the sentence to be translated.
For eachphrase pair extracted from a sentence pair of asource-side parsed parallel corpus, we abstract itssyntax by the sequence of highest root categories,which we call a tag sequence, that exactly1 domi-nates the syntactic tree fragments of the source-sidephrase.
Figure 3 (b) shows the source-side parse treeof a sentence pair.
The tag sequence for ?the pen?is simply ?NP?
because it is a noun phrase, whilephrase ?give the pen?
is dominated by a verb fol-lowed by a noun phrase, and thus its tag sequence is?VBP NP?.Let TS = {ts1, ?
?
?
, tsm} be the set of all tag se-quences extracted from a parallel corpus.
The syntaxof each X nonterminal2 in a SCFG rule can be then1In case of a non-tight phrase pair, we only abstract andcompare the syntax of the largest tight part.2There are three X nonterminals (one on the left and two onthe right) for binary abstract rules, two for unary abstract rules,and one for phrasal rules.140Tag Sequence ProbabilityNP 0.40DT NN 0.35DT NN NN 0.25Table 1: The distribution of tag sequences forX1 inX ?
?I am reading X1, ?
?
?
?.characterized by the distribution of tag sequences~PX(TS) = (pX(ts1), ?
?
?
, pX(tsm)), based on thephrase pairs it is abstracted from.
Table 1 showsan example distribution of tag sequences for X1 inX ?
?I am reading X1, ?
?
?
?.Instead of directly using tag sequences, as wediscussed their disadvantages above, we representeach of them by a real-valued feature vector.
Sup-pose we have a collection of n latent syntactic cate-gories C = {c1, ?
?
?
, cn}.
For each tag sequence ts,we compute its distribution of latent syntactic cate-gories ~Pts(C) = (pts(c1), ?
?
?
, pts(cn)).
For exam-ple, ~P?NP VP?
(C) = {0.5, 0.2, 0.3} means that the la-tent syntactic categories c1, c2, and c3 are distributedas p(c1) = 0.5, p(c2) = 0.2, and p(c3) = 0.3 for tagsequence ?NP VP?.
We further convert the distribu-tion to a normalized feature vector ~F (ts) to repre-sent tag sequence ts:~F (ts) = (f1(ts), ?
?
?
, fn(ts))=(pts(c1), ?
?
?
, pts(cn))?
(pts(c1), ?
?
?
, pts(cn))?The advantage of using real-valued feature vec-tors is that the degree of similarity between two tagsequences ts and ts?
in the space of the latent syn-tactic categories C can be simply computed as a dot-product3 of their feature vectors:~F (ts) ?
~F (ts?)
=?1?i?nfi(ts)fi(ts?
)which computes a syntactic similarity score in therange of 0 (totally syntactically different) to 1 (com-pletely syntactically identical).Similarly, we can represent the syntax of each Xnonterminal in a rule with a feature vector ~F (X),computed as the sum of the feature vectors of tag3Other measures such as KL-divergence in the probabilityspace are also feasible.sequences weighted by the distribution of tag se-quences of the nonterminal X:~F (X) =?ts?TSpX(ts)~F (ts)Now we can impose soft syntactic constraints us-ing these feature vectors when a SCFG rule is usedto translate a parsed source sentence.
Given that aXnonterminal in the rule is applied to a span with tagsequence4 ts as determined by a syntactic parser, wecan compute the following syntax similarity feature:SynSim(X, ts) = ?
log(~F (ts) ?
~F (X))Except that it is computed on the fly, this featurecan be used in the same way as the regular featuresin hierarchical translation systems to determine thebest translation, and its feature weight can be tunedin the same way together with the other features ona held-out data set.In our approach, the set of latent syntactic cate-gories is automatically induced from a source-sideparsed, word-aligned parallel corpus based on thehierarchical structure among phrase pairs along withthe syntactic parse of the source side.
In what fol-lows, we will explain the two critical aspects ofour approach, i.e., how to identify the hierarchi-cal structures among all phrase pairs in a sentencepair, and how to induce the latent syntactic cate-gories from the hierarchy to syntactically explain thephrase pairs.4 Alignment-based HierarchyThe aforementioned abstract rule extraction algo-rithm of Chiang (2007) is based on the property thata tight phrase pair can contain other tight phrasepairs.
Given two non-disjoint tight phrase pairs thatshare at least one common alignment link, there areonly two relationships: either one completely in-cludes another or they do not include one anotherbut have a non-empty overlap, which we call a non-trivial overlap.
In the second case, the intersection,differences, and union of the two phrase pairs are4A normalized uniform feature vector is used for tag se-quences (of parsed test sentences) that are not seen on the train-ing corpus.141Figure 2: A decomposition tree of tight phrase pairs withall tight phrase pairs listed on the right.
As highlighted,the two non-maximal phrase pairs are generated by con-secutive sibling nodes.also tight phrase pairs (see Figure 1 (b) for exam-ple), and the two phrase pairs, as well as their inter-section and differences, are all sub phrase pairs oftheir union.Zhang et al (2008) exploited this property to con-struct a hierarchical decomposition tree (Bui-Xuanet al, 2005) of phrase pairs from a sentence pair toextract all phrase pairs in linear time.
In this pa-per, we focus on learning the syntactic dependenciesalong the hierarchy of phrase pairs.
Our hierarchyconstruction follows Heber and Stoye (2001).Let P be the set of tight phrase pairs extractedfrom a sentence pair.
We call a sequentially-orderedlist5 L = (p1, ?
?
?
, pk) of unique phrase pairs pi ?
Pa chain if every two successive phrase pairs in Lhave a non-trivial overlap.
A chain is maximal ifit can not be extended to its left or right with otherphrase pairs.
Note that any sub-sequence of phrasepairs in a chain generates a tight phrase pair.
In par-ticular, chain L generates a tight phrase pair ?
(L)that corresponds exactly to the union of the align-ment links in p ?
L. We call the phrase pairsgenerated by maximal chains maximal phrase pairsand call the other phrase pairs non-maximal.
Non-maximal phrase pairs always overlap non-triviallywith some other phrase pairs while maximal phrasepairs do not, and it can be shown that any non-maximal phrase pair can be generated by a sequenceof maximal phrase pairs.
Note that the largest tightphrase pair that includes all alignment links in A isalso a maximal phrase pair.5The phrase pairs can be sequentially ordered first by theboundary positions of the source-side phrase and then by theboundary positions of the target-side phrase.givethe pen to me .X B B B X XXXXPPVBPDT NN TO PRP .NPVPSgivethe pen to me .
(a) (b)XXB B B X XXXXXXXB B B X XXXXXVBPXXB B B X XXXXXDTNN TO PRP.NP PPCRVPSI(!)O(!
)XXB B B X XXXXXVBP DTNN TO PRP.SCRNP PPCRO(!)I(!
)(c) (d)Figure 3: (a) decomposition tree for the English side ofthe example sentence pair with all phrases underlined, (b)automatic parse tree of the English side, (c) two examplebinarized decomposition trees with syntactic emissionsin depicted in (d), where the two dotted curves give anexample I(?)
and O(?)
that separate the forest into twoparts.Lemma 1 Given two different maximal phrasepairs p1 and p2, exactly one of the following alter-natives is true: p1 and p2 are disjoint, p1 is a subphrase pair of p2, or p2 is a sub phrase pair of p1.A direct outcome of Lemma 1 is that there is anunique decomposition tree T = (N,E) covering allof the tight phrase pairs of a sentence pair, where Nis the set of maximal phrase pairs and E is the set ofedges that connect between pairs of maximal phrasepairs if one is a sub phrase pair of another.
All of thetight phrase pairs of a sentence pair can be extracteddirectly from the nodes of the decomposition tree(these phrase pairs are maximal), or generated by se-quences of consecutive sibling nodes6 (these phrasepairs are non-maximal).
Figure 2 shows the decom-position tree as well as all of the tight phrase pairsthat can be extracted from the example sentence pairin Figure 1.We focus on the source side of the decompositiontree, and expand it to include all of the non-phrase6Unaligned words may be added.142single words within the scope of the decompositiontree as frontiers and attach each as a child of the low-est node that contains the word.
We then abstract thetrees nodes with two symbol, X for phrases, and Bfor non-phrases, and call the result the decomposi-tion tree of the source side phrases.
Figure 3 (a) de-picts such tree for the English side of our examplesentence pair.
We further recursively binarize7 thedecomposition tree into a binarized decompositionforest such that all phrases are directly representedas nodes in the forest.
Figure 3 (c) shows two of themany binarized decomposition trees in the forest.The binarized decomposition forest compactlyencodes the hierarchical structure among phrasesand non-phrases.
However, the coarse abstractionof phrases with X and non-phrases with B provideslittle information on the constraints of the hierarchy.In order to bring in syntactic constraints, we anno-tate the nodes in the decomposition forest with syn-tactic observations based on the automatic syntacticparse tree of the source side.
If a node aligns witha constituent in the parse tree, we add the syntacticcategory (e.g., NP) of the constituent as an emittedobservation of the node, otherwise, it crosses con-stituent boundaries and we add a designated crossingcategory CR as its observation.
We call the resultingforest a syntactic decomposition forest.
Figure 3 (d)shows two syntactic decomposition trees of the for-est based on the parse tree in Figure 3 (b).
We willnext describe how to learn finer-grained X and Bcategories based on the hierarchical syntactic con-straints.5 Inducing Latent Syntactic CategoriesIf we designate a unique symbol S as the new rootof the syntactic decomposition forests introducedin the previous section, it can be shown that theseforests can be generated by a probabilistic context-free grammar G = (V,?, S,R, ?
), where?
V = {S,X,B} is the set of nonterminals,?
?
is the set of terminals comprising treebankcategories plus the CR tag (the crossing cate-gory),7The intermediate binarization nodes are also labeled as ei-ther X or B based on whether they exactly cover a phrase ornot.?
S ?
V is the unique start symbol,?
R is the union of the set of production ruleseach rewriting a nonterminal to a sequence ofnonterminals and the set of emission rules eachgenerating a terminal from a nonterminal,?
and ?
assigns a probability score to each ruler ?
R.Such a grammar can be derived from the set ofsyntactic decomposition forests extracted from asource-side parsed parallel corpus, with rule prob-ability scores estimated as the relative frequenciesof the production and emission rules.The X and B nonterminals in the grammar arecoarse representations of phrase and non-phrasesand do not carry any syntactic information at all.In order to introduce syntax to these nonterminals,we incrementally split8 them into a set of latentcategories {X1, ?
?
?
, Xn} for X and another set{B1, ?
?
?
, Bn} for B, and then learn a set of ruleprobabilities9 ?
on the latent categories so that thelikelihood of the training forests are maximized.
Themotivation is to let the latent categories learn differ-ent preferences of (emitted) syntactic categories aswell as structural dependencies along the hierarchyso that they can carry syntactic information.
We callthem latent syntactic categories.
The learned Xi?srepresent syntactically-induced finer-grained cate-gories of phrases and are used as the set of latentsyntactic categories C described in Section 3.
In re-lated research, Matsuzaki et al (2005) and Petrov etal.
(2006) introduced latent variables to learn finer-grained distinctions of treebank categories for pars-ing, and Huang et al (2009) used a similar approachto learn finer-grained part-of-speech tags for tag-ging.
Our method is in spirit similar to these ap-proaches.Optimization of grammar parameters to maximizethe likelihood of training forests can be achieved8We incrementally split each nonterminal to 2, 4, 8, and fi-nally 16 categories, with each splitting followed by several EMiterations to tune model parameters.
We consider 16 an appro-priate number for latent categories, not too small to differentiatebetween different syntactic usages and not too large for the extracomputational and storage costs.9Each binary production rule is now associated with a 3-dimensional matrix of probabilities, and each emission rule as-sociated with a 1-dimensional array of probabilities.143by a variant of Expectation-Maximization (EM) al-gorithm.
Recall that our decomposition forests arefully binarized (except the root).
In the hypergraphrepresentation (Huang and Chiang, 2005), the hy-peredges of our forests all have the same format10?
(V,W ), U?, meaning that node U expands to nodesV and W with production rule U ?
VW .
Givena forest F with root node R, we denote e(U) theemitted syntactic category at node U and LR(U) (orPL(W ), or PR(V ))11 the set of node pairs (V,W )(or (U, V ), or (U,W )) such that ?
(V,W ), U?
is a hy-peredge of the forest.
Now consider node U , whichis either S, X , or B, in the forest.
Let Ux be thelatent syntactic category12 of node U .
We defineI(Ux) the part of the forest (includes e(U) but notUx) inside U , and O(Ux) the other part of the forest(includes Ux but not e(U)) outside U , as illustratedin Figure 3 (d).
The inside-outside probabilities aredefined as:PIN(Ux) = P (I(Ux)|Ux)POUT(Ux) = P (O(Ux)|S)which can be computed recursively as:PIN(Ux) =?
(V,W )?LR(U)?y,z?
(Ux ?
e(U))??
(Ux ?
VyWz)?PIN(Vy)PIN(Wz)POUT(Ux) =?
(V,W )?PL(U)?y,z?
(Vy ?
e(V ))??
(Vy ?WzUx)?POUT(Vy)PIN(Wz)+?
(V,W )?PR(U)?y,z?
(Vy ?
e(V ))??
(Vy ?
UxWz)?POUT(Vy)PIN(Wz)In the E-step, the posterior probability of the oc-currence of production rule13 Ux ?
VyWz is com-puted as:P (Ux ?
VyWz|F ) =?
(Ux ?
e(U))??
(Ux ?
VyWz)?POUT(Ux)PIN(Vy)PIN(Ww)PIN(R)10The hyperedge corresponding to the root node has a differ-ent format because it is unary, but it can be handled similarly.When clear from context, we use the same variable to presentboth a node and its label.11LR stands for the left and right children, PL for the parentand left children, and PR for the parent and right children.12We never split the start symbol S, and denote S0 = S.13The emission rules can be handled similarly.In the M-step, the expected counts of rule Ux ?VyWz for all latent categories Vy and Wz are accu-mulated together and then normalized to obtain anupdate of the probability estimation:?
(Ux ?
VyWz) =#(Ux ?
VyWz)?
(V ?,W ?
)?y,z#(Ux ?
VyWz)Recall that each node U labeled asX in a forest isassociated with a phrase whose syntax is abstractedby a tag sequence.
Once a grammar is learned, foreach such node with a corresponding tag sequencets in forest F , we compute the posterior probabilitythat the latent category of node U being Xi as:P (Xi|ts) =POUT(Ui)PIN(Ui)PIN(R)This contributes P (Xi|ts) evidence that tag se-quence ts belongs to a Xi category.
When allof the evidences are computed and accumulated in#(Xi, ts), they can then be normalized to obtain theprobability that the latent category of ts is Xi:pts(Xi) =#(Xi, ts)?i #(Xi, ts)As described in Section 3, the distributions of latentcategories are used to compute the syntactic featurevectors for the SCFG rules.6 ExperimentsWe conduct experiments on two tasks, English-to-German and English-to-Chinese, both aimed forspeech-to-speech translation.
The training data forthe English-to-German task is a filtered subset of theEuroparl corpus (Koehn, 2005), containing ?300kparallel bitext with ?4.5M tokens on each side.
Thedev and test sets both contain 1k sentences with onereference for each.
The training data for the English-to-Chinese task is collected from transcription andhuman translation of conversations in travel domain.It consists of ?500k parallel bitext with ?3M to-kens14 on each side.
Both dev and test sets contain?1.3k sentences, each with two references.
Both14The Chinese sentences are automatically segmented intowords.
However, BLEU scores are computed at character levelfor tuning and evaluation.144corpora are also preprocessed with punctuation re-moved and words down-cased to make them suitablefor speech translation.The baseline system is our implementation of thehierarchical phrase-based model of Chiang (2007),and it includes basic features such as rule andlexicalized rule translation probabilities, languagemodel scores, rule counts, etc.
We use 4-gram lan-guage models in both tasks, and conduct minimum-error-rate training (Och, 2003) to optimize featureweights on the dev set.
Our baseline hierarchicalmodel has 8.3M and 9.7M rules for the English-to-German and English-to-Chinese tasks, respectively.The English side of the parallel data isparsed by our implementation of the Berkeleyparser (Huang and Harper, 2009) trained on thecombination of Broadcast News treebank fromOntonotes (Weischedel et al, 2008) and a speechi-fied version of the WSJ treebank (Marcus et al,1999) to achieve higher parsing accuracy (Huang etal., 2010).
Our approach introduces a new syntacticfeature and its feature weight is tuned in the sameway together with the features in the baseline model.In this study, we induce 16 latent categories for bothX and B nonterminals.Our approach identifies ?180k unique tag se-quences for the English side of phrase pairs in bothtasks.
As shown by the examples in Table 2, the syn-tactic feature vector representation is able to identifysimilar and dissimilar tag sequences.
For instance,it determines that the sequence of ?DT JJ NN?
issyntactically very similar to ?DT ADJP NN?
whilevery dissimilar to ?NN CD VP?.
Notice that our la-tent categories are learned automatically to maxi-mize the likelihood of the training forests extractedbased on alignment and are not explicitly instructedto discriminate between syntactically different tagsequences.
Our approach is not guaranteed to al-ways assign similar feature vectors to syntacticallysimilar tag sequences.
However, as the experimentalresults show below, the latent categories are able tocapture some similarities among tag sequences thatare beneficial for translation.Table 3 and 4 report the experimental resultson the English-to-German and English-to-Chinesetasks, respectively.
The addition of the syntax fea-ture achieves a statistically significant improvement(p ?
0.01) of 0.6 in BLEU on the test set of theBaseline +Syntax ?Dev 16.26 17.06 0.80Test 16.41 17.01 0.60Table 3: BLEU scores of the English-to-German task(one reference).Baseline +Syntax ?Dev 46.47 47.39 0.92Test 45.45 45.86 0.41Table 4: BLEU scores of the English-to-Chinese task(two references).English-to-German task.
This improvement is sub-stantial given that only one reference is used for eachtest sentence.
On the English-to-Chinese task, thesyntax feature achieves a smaller improvement of0.41 BLEU on the test set.
One potential explanationfor the smaller improvement is that the sentences onthe English-to-Chinese task are much shorter, withan average of only 6 words per sentence, comparedto 15 words in the English-to-German task.
Thehypothesis space of translating a longer sentence ismuch larger than that of a shorter sentence.
There-fore, there is more potential gain from using syn-tax features to rule out unlikely derivations of longersentences, while phrasal rules might be adequate forshorter sentences, leaving less room for syntax tohelp as in the case of the English-to-Chinese task.7 DiscussionsThe incorporation of the syntactic feature into thehierarchical phrase-based translation system alsobrings in additional memory load and computationalcost.
In the worst case, our approach requires stor-ing one feature vector for each tag sequence and onefeature vector for each nonterminal of a SCFG rule,with the latter taking the majority of the extra mem-ory storage.
We observed that about 90% of theX nonterminals in the rules only have one tag se-quence, and thus the required memory space can besignificantly reduced by only storing a pointer to thefeature vector of the tag sequence for these nonter-minals.
Our approach also requires computing onedot-product of two feature vectors for each nonter-minal when a SCFG rule is applied to a source span.145Very similar Not so similar Very dissimilar~F (ts) ?
~F (ts?)
> 0.9 0.4 ?
~F (ts) ?
~F (ts?)
?
0.6 ~F (ts) ?
~F (ts?)
< 0.1DT JJ NNDT NN DT JJ JJ NML NN PP NP NNDT JJ JJ NN DT JJ CC INTJ VB NN CD VPDT ADJP NN DT NN NN JJ RB NP IN CDVPVB VP PP JJ NN JJ NN TO VPVB RB VB PP VB NN NN VB JJ WHNP DT NNVB DT DT NN VB RB IN JJ IN INTJ NPADJPJJ ADJP JJ JJ CC ADJP IN NP JJPDT JJ ADJP VB JJ JJ AUX RB ADJPRB JJ ADVP WHNP JJ ADJP VPTable 2: Examples of similar and dissimilar tag sequences.This cost can be reduced, however, by caching thedot-products of the tag sequences that are frequentlyaccessed.There are other successful investigations toimpose soft syntactic constraints to hierarchicalphrase-based models by either introducing syntax-based rule features such as the prior derivationmodel of Zhou et al (2008) or by imposing con-straints on translation spans at decoding time, e.g.,(Marton and Resnik, 2008; Xiong et al, 2009;Xiong et al, 2010).
These approaches are all or-thogonal to ours and it is expected that they can becombined with our approach to achieve greater im-provement.This work is an initial effort to investigate latentsyntactic categories to enhance hierarchical phrase-based translation models, and there are many direc-tions to continue this line of research.
First, whilethe current approach imposes soft syntactic con-straints between the parse structure of the sourcesentence and the SCFG rules used to derive thetranslation, the real-valued syntactic feature vectorscan also be used to impose soft constraints betweenSCFG rules when rule rewrite occurs.
In this case,target side parse trees could also be used alone or to-gether with the source side parse trees to induce thelatent syntactic categories.
Second, instead of usingsingle parse trees during both training and decod-ing, our approach is likely to benefit from exploringparse forests as in (Mi and Huang, 2008).
Third,in addition to the treebank categories obtained bysyntactic parsing, lexical cues directly available insentence pairs could also to explored to guide thelearning of latent categories.
Last but not the least,it would be interesting to investigate discriminativetraining approaches to learn latent categories that di-rectly optimize on translation quality.8 ConclusionWe have presented a novel approach to enhancehierarchical phrase-based machine translation sys-tems with real-valued linguistically motivated fea-ture vectors.
Our approach maintains the advan-tages of hierarchical phrase-based translation sys-tems while at the same time naturally incorpo-rates soft syntactic constraints.
Experimental resultsshowed that this approach improves the baseline hi-erarchical phrase-based translation models on bothEnglish-to-German and English-to-Chinese tasks.We will continue this line of research and exploitbetter ways to learn syntax and apply syntactic con-straints to machine translation.AcknowledgementsThis work was done when the first author was visit-ing IBM T. J. Watson Research Center as a researchintern.
We would like to thank Mary Harper forlots of insightful discussions and suggestions and theanonymous reviewers for the helpful comments.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-146ics of statistical machine translation: parameter esti-mation.
Computational Linguistics.Binh Minh Bui-Xuan, Michel Habib, and ChristophePaul.
2005.
Revisiting T. Uno and M. Yagiura?s al-gorithm.
In ISAAC.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In ACL.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2004.
What?s in a translation rule.
InHLT/NAACL.Steffen Heber and Jens Stoye.
2001.
Finding all commonintervals of k permutations.
In CPM.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In International Workshop on Parsing Tech-nology.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In EMNLP.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A syntax-directed translator with extended domain oflocality.
In CHSLP.Zhongqiang Huang, Vladimir Eidelman, and MaryHarper.
2009.
Improving a simple bigram hmm part-of-speech tagger by latent annotation and self-training.In NAACL.Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010.Self-training with products of latent variable.
InEMNLP.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In ACL.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor, 1999.
Treebank-3.Linguistic Data Consortium, Philadelphia.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In ACL.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In EMNLP.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In ACL.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In ACL.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In ACL.Ashish Venugopal, Andreas Zollmann, Noah A. Smith,and Stephan Vogel.
2009.
Preference grammars: soft-ening syntactic constraints to improve statistical ma-chine translation.
In NAACL.Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,Martha Palmer, Nianwen Xue, Mitchell Marcus, AnnTaylor, Craig Greenberg, Eduard Hovy, Robert Belvin,and Ann Houston, 2008.
OntoNotes Release 2.0.
Lin-guistic Data Consortium, Philadelphia.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009.A syntax-driven bracketing model for phrase-basedtranslation.
In ACL-IJCNLP.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Learn-ing translation boundaries for phrase-based decoding.In NAACL-HLT.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In ACL.Hao Zhang, Daniel Gildea, and David Chiang.
2008.
Ex-tracting synchronous grammar rules from word-levelalignments in linear time.
In COLING.Bowen Zhou, Bing Xiang, Xiaodan Zhu, and YuqingGao.
2008.
Prior derivation models for formallysyntax-based translation using linguistically syntacticparsing and tree kernels.
In SSST.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InStatMT.147
