Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616?625,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsStatistical Machine Translation with a Factorized GrammarLibin Shen and Bing Zhang and Spyros Matsoukas andJinxi Xu and Ralph WeischedelRaytheon BBN TechnologiesCambridge, MA 02138, USA{lshen,bzhang,smatsouk,jxu,weisched}@bbn.comAbstractIn modern machine translation practice, a sta-tistical phrasal or hierarchical translation sys-tem usually relies on a huge set of trans-lation rules extracted from bi-lingual train-ing data.
This approach not only results inspace and efficiency issues, but also suffersfrom the sparse data problem.
In this paper,we propose to use factorized grammars, anidea widely accepted in the field of linguis-tic grammar construction, to generalize trans-lation rules, so as to solve these two prob-lems.
We designed a method to take advantageof the XTAG English Grammar to facilitatethe extraction of factorized rules.
We experi-mented on various setups of low-resource lan-guage translation, and showed consistent sig-nificant improvement in BLEU over state-of-the-art string-to-dependency baseline systemswith 200K words of bi-lingual training data.1 IntroductionA statistical phrasal (Koehn et al, 2003; Och andNey, 2004) or hierarchical (Chiang, 2005; Marcuet al, 2006) machine translation system usually re-lies on a very large set of translation rules extractedfrom bi-lingual training data with heuristic methodson word alignment results.
According to our ownexperience, we obtain about 200GB of rules fromtraining data of about 50M words on each side.
Thisimmediately becomes an engineering challenge onspace and search efficiency.A common practice to circumvent this problemis to filter the rules based on development sets in thestep of rule extraction or before the decoding phrase,instead of building a real distributed system.
How-ever, this strategy only works for research systems,for which the segments for translation are alwaysfixed.However, do we really need such a large rule setto represent information from the training data ofmuch smaller size?
Linguists in the grammar con-struction field already showed us a perfect solutionto a similar problem.
The answer is to use a fac-torized grammar.
Linguists decompose lexicalizedlinguistic structures into two parts, (unlexicalized)templates and lexical items.
Templates are furtherorganized into families.
Each family is associatedwith a set of lexical items which can be used to lex-icalize all the templates in this family.
For example,the XTAG English Grammar (XTAG-Group, 2001),a hand-crafted grammar based on the Tree Adjoin-ing Grammar (TAG) (Joshi and Schabes, 1997) for-malism, is a grammar of this kind, which employsfactorization with LTAG e-tree templates and lexicalitems.Factorized grammars not only relieve the burdenon space and search, but also alleviate the sparsedata problem, especially for low-resource languagetranslation with few training data.
With a factoredmodel, we do not need to observe exact ?template?
lexical item?
occurrences in training.
New rulescan be generated from template families and lexicalitems either offline or on the fly, explicitly or im-plicitly.
In fact, the factorization approach has beensuccessfully applied on the morphological level inprevious study on MT (Koehn and Hoang, 2007).
Inthis work, we will go further to investigate factoriza-tion of rule structures by exploiting the rich XTAGEnglish Grammar.We evaluate the effect of using factorized trans-lation grammars on various setups of low-resourcelanguage translation, since low-resource MT suffersgreatly on poor generalization capability of trans-616lation rules.
With the help of high-level linguis-tic knowledge for generalization, factorized gram-mars provide consistent significant improvementin BLEU (Papineni et al, 2001) over string-to-dependency baseline systems with 200K words ofbi-lingual training data.This work also closes the gap between compacthand-crafted translation rules and large-scale unor-ganized automatic rules.
This may lead to a more ef-fective and efficient statistical translation model thatcould better leverage generic linguistic knowledgein MT.In the rest of this paper, we will first provide ashort description of our baseline system in Section 2.Then, we will introduce factorized translation gram-mars in Section 3.
We will illustrate the use of theXTAG English Grammar to facilitate the extractionof factorized rules in Section 4.
Implementation de-tails are provided in Section 5.
Experimental resultsare reported in Section 6.2 A Baseline String-to-Tree ModelAs the baseline of our new algorithm, we use astring-to-dependency system as described in (Shenet al, 2008).
There are several reasons why we takethis model as our baseline.
First, it uses syntactictree structures on the target side, which makes it easyto exploit linguistic information.
Second, depen-dency structures are relatively easier to implement,as compared to phrase structure grammars.
Third,a string-to-dependency system provides state-of-the-art performance on translation accuracy, so that im-provement over such a system will be more convinc-ing.Here, we provide a brief description of the base-line string-to-dependency system, for the sake ofcompleteness.
Readers can refer to (Shen et al,2008; Shen et al, 2009) for related information.In the baseline string-to-dependency model, eachtranslation rule is composed of two parts, source andtarget.
The source sides is a string rewriting rule,and the target side is a tree rewriting rule.
Bothsides can contain non-terminals, and source and tar-get non-terminals are one-to-one aligned.
Thus, inthe decoding phase, non-terminal replacement forboth sides are synchronized.Decoding is solved with a generic chart parsingalgorithm.
The source side of a translation rule isused to detect when this rule can be applied.
The tar-get side of the rule provides a hypothesis tree struc-ture for the matched span.
Mono-lingual parsing canbe viewed as a special case of this generic algorithm,for which the source string is a projection of the tar-get tree structure.Figure 1 shows three examples of string-to-dependency translation rules.
For the sake of con-venience, we use English for both source and target.Upper-cased words represent source, while lower-cased words represent target.
X is used for non-terminals for both sides, and non-terminal alignmentis represented with subscripts.In Figure 1, the top boxes mean the source side,and the bottom boxes mean the target side.
As forthe third rule, FUN Q stands for a function word inthe source language that represents a question.3 Translation with a Factorized GrammarWe continue with the example rules in Figure 1.Suppose, we have ?...
HATE ... FUN Q?
in a giventest segment.
There is no rule having both HATEand FUN Q on its source side.
Therefore, we haveto translate these two source words separately.
Forexample, we may use the second rule in Figure 1.Thus, HATE will be translated into hates, which iswrong.Intuitively, we would like to have translation rulethat tell us how to translate X1 HATE X2 FUN Qas in Figure 2.
It is not available directly from thetraining data.
However, if we obtain the three rulesin Figure 1, we are able to predict this missing rule.Furthermore, if we know like and hate are in thesame syntactic/semantic class in the source or targetlanguage, we will be very confident on the validityof this hypothesis rule.Now, we propose a factorized grammar to solvethis generalization problem.
In addition, translationrules represented with the new formalism will bemore compact.3.1 Factorized RulesWe decompose a translation rule into two parts,a pair of lexical items and an unlexicalized tem-plate.
It is similar to the solution in the XTAG En-glish Grammar (XTAG-Group, 2001), while here we617X1  LIKE  X2likesX1 X2X1  HATE  X2hatesX1 X2X1  LIKE X2  FUN_Qlikedoes X1 X2Figure 1: Three examples of string-to-dependency translation rules.X1  V  X2VBZX1 X2X1  V  X2VBZX1 X2X1  V  X2  FUN_QVBdoes X1 X2Figure 3: Templates for rules in Figure 1.X1  HATE  X2  FUN_Qhatedoes X1 X2Figure 2: An example of a missing rule.work on two languages at the same time.For each rule, we first detect a pair of aligned headwords.
Then, we extract the stems of this word pairas lexical items, and replace them with their POStags in the rule.
Thus, the original rule becomes anunlexicalized rule template.As for the three example rules in Figure 1, we willextract lexical items (LIKE, like), (HATE, hate) and(LIKE, like) respectively.
We obtain the same lexicalitems from the first and the third rules.The resultant templates are shown in Figure 3.Here, V represents a verb on the source side, VBstands for a verb in the base form, and VBZ meansa verb in the third person singular present form asin the Penn Treebank representation (Marcus et al,1994).In the XTAG English Grammar, tree templates fortransitive verbs are grouped into a family.
All transi-tive verbs are associated with this family.
Here, weassume that the rule templates representing struc-tural variations of the same word class can also beorganized into a template family.
For example, asshown in Figure 4, templates and lexical items areassociated with families.
It should be noted thata template or a lexical item can be associated withmore than one family.Another level of indirection like this providesmore generalization capability.
As for the missing618X1  V  X2VBZX1 X2Family Transitive_3X1  V  X2  FUN_QVBdoes X1 X2X1  V  FUN_PastVBDX1Family Intransitive_2( LIKE, like ) ( HATE, hate ) ( OPEN, open ) ( HAPPEN, happen )Figure 4: Templates and lexical items are associated with families.rule in Figure 2, we can now generate it by replac-ing the POS tags in the second template of Figure4 with lexical items (HATE, hate) with their correctinflections.
Both the template and the lexical itemshere are associated with the family Transitive 3..3.2 Statistical ModelsAnother level of indirection also leads to a desirableback-off model.
We decompose a rule R into to twoparts, its template PR and its lexical items LR.
As-suming they are independent, then we can computePr(R) asPr(R) = Pr(PR)Pr(LR), orPr(R) =?F Pr(PR|F )Pr(LR|F )Pr(F ), (1)if they are conditionally independent for each fam-ily F .
In this way, we can have a good estimate forrules that do not appear in the training data.
Thesecond generative model will also be useful for un-supervised learning of families and related probabil-ities.In this paper, we approximate families by usingtarget (English) side linguistic knowledge as whatwe will explain in Section 4, so this changes the def-inition of the task.
In short, we will be given a list offamilies.
We will also be given an association tableB(L,F ) for lexical items L and families F , suchthat B(L,F ) = true if and only L is associatedwith F , but we do not know the distributions.Let S be the source side of a rule or a rule tem-plate, T the target side of a rule of a rule template.We define Prb, the back-off conditional model oftemplates, as follows.Prb(PS |PT , L) =?F :B(L,F ) #(PS , PT , F )?F :B(L,F ) #(PT , F ), (2)where # stands for the count of events.Let P and L be the template and lexical items ofR respectively.
Let Prt be the MLE model obtainedfrom the training data.
The smoothed probability isthen defined as follows.Pr(RS |RT ) = (1 ?
?
)Prt(RS |RT )+?Prb(PS |PT , L), (3)where ?
is a parameter.
We fix it to 0.1 in later ex-periments.
Conditional probability Pr(RT |RS) isdefined in a similar way.3.3 DiscussionThe factorized models discussed in the previous sec-tion can greatly alleviate the sparse data problem,especially for low-resource translation tasks.
How-ever, when the training data is small, it is not easy to619learn families.
Therefore, to use unsupervised learn-ing with a model like (1) somehow reduces a hardtranslation problem to another one of the same diffi-culty, when the training data is small.However, in many cases, we do have extra infor-mation that we can take advantage of.
For example,if the target language has rich resources, althoughthe source language is a low-density one, we can ex-ploit the linguistic knowledge on the target side, andcarry it over to bi-lingual structures of the translationmodel.
The setup of X-to-English translation tasksis just like this.
This will be the topic of the nextsection.
We leave unsupervised learning of factor-ized translation grammars for future research.4 Using A Mono-Lingual GrammarIn this section, we will focus on X-to-English trans-lation, and explain how to use English resources tobuild a factorized translation grammar.
Although weuse English as an example, this approach can be ap-plied to any language pairs that have certain linguis-tic resources on one side.As shown in Figure 4, intuitively, the familiesare intersection of the word families of the two lan-guages involved, which means that they are refine-ment of the English word families.
For example,a sub-set of the English transitive families may betranslated in the same way, so they share the sameset of templates.
This is why we named the two fam-ilies Transitive 3 and Intransitive 2 in Figure 4.Therefore, we approximate bi-lingual familieswith English families first.
In future, we can usethem as the initial values for unsupervised learning.In order to learn English families, we need to takeaway the source side information in Figure 4, andwe end up with a template?family?word graph asshown in Figure 5.
We can learn this model on largemono-lingual data if necessary.What is very interesting is that there already existsa hand-crafted solution for this model.
This is theXTAG English Grammar (XTAG-Group, 2001).The XTAG English Grammar is a large-scale En-glish grammar based on the TAG formalism ex-tended with lexicalization and unification-based fea-ture structures.
It consists of morphological, syn-tactic, and tree databases.
The syntactic databasecontains the information that we have representedin Figure 5 and many other useful linguistic annota-tions, e.g.
features.The XTAG English grammar contains 1,004 tem-plates, organized in 53 families, and 221 individualtemplates.
About 30,000 lexical items are associ-ated with these families and individual templates 1.In addition, it also has the richest English morpho-logical lexicon with 317,000 inflected items derivedfrom 90,000 stems.
We use this resource to predictPOS tags and inflections of lexical items.In our applications, we select all the verb fami-lies plus one each for nouns, adjectives and adverbs.We use the families of the English word as the fam-ilies of bi-lingual lexical items.
Therefore, we havea list of about 20 families and an association tableas described in Section 3.2.
Of course, one can useother linguistic resources if similar family informa-tion is provided, e.g.
VerbNet (Kipper et al, 2006)or WordNet (Fellbaum, 1998).5 ImplementationNowadays, machine translation systems becomemore and more complicated.
It takes time to writea decoder from scratch and hook it with variousmodules, so it is not the best solution for researchpurpose.
A common practice is to reduce a newtranslation model to an old one, so that we can usean existing system, and see the effect of the newmodel quickly.
For example, the tree-based modelproposed in (Carreras and Collins, 2009) used aphrasal decoder for sub-clause translation, and re-cently, DeNeefe and Knight (2009) reduced a TAG-based translation model to a CFG-based model byapplying all possible adjunction operations offlineand stored the results as rules, which were then usedby an existing syntax-based decoder.Here, we use a similar method.
Instead of build-ing a new decoder that uses factorized grammars,we reduce factorized rules to baseline string-to-dependency rules by performing combination oftemplates and lexical items in an offline mode.
Thisis similar to the rule generation method in (DeNeefeand Knight, 2009).
The procedure is as follows.In the rule extraction phase, we first extract all thestring-to-dependency rules with the baseline system.1More information about XTAG is available online athttp://www.cis.upenn.edu/?xtag .620VBZX1 X2Family TransitiveVBdoes X1 X2VBDX1Family Intransitivelike hate open happenFigure 5: Templates, families, and words in the XTAG English Grammar.For each extracted rule, we try to split it into various?template?lexical item?
pairs by choosing differentaligned words for delexicalization, which turns rulesin Figure 1 into lexical items and templates in Fig-ure 3.
Events of templates and lexical items arecounted according to the family of the target En-glish word.
If an English word is associated withmore than one family, the count is distributed uni-formly among these families.
In this way, we collectsufficient statistics for the back-off model in (2).For each family, we keep the top 200 most fre-quent templates.
Then, we apply them to all thelexical items in this families, and save the gener-ated rules.
We merge the new rules with the originalone.
The conditional probabilities for the rules in thecombined set is smoothed according to (2) and (3).Obviously, using only the 200 most frequent tem-plates for each family is just a rough approxima-tion.
An exact implementation of a new decoder forfactorized grammars can make better use of all thetemplates.
However, the experiments will show thateven an approximation like this can already providesignificant improvement on small training data sets,i.e.
with no more than 2M words.Since we implement template application in an of-fline mode, we can use exactly the same decodingand optimization algorithms as the baseline.
The de-coder is a generic chart parsing algorithm that gen-erates target dependency trees from source string in-put.
The optimizer is an L-BFGS algorithm thatmaximizes expected BLEU scores on n-best hy-potheses (Devlin, 2009).6 Experiments on Low-Resource SetupsWe tested the performance of using factorized gram-mars on low-resource MT setups.
As what we notedabove, the sparse data problem is a major issue whenthere is not enough training data.
This is one of thecases that a factorized grammar would help.We did not tested on real low-resource languages.Instead, we mimic the low-resource setup with twoof the most frequently used language pairs, Arabic-to-English and Chinese-to-English, on newswireand web genres.
Experiments on these setups willbe reported in Section 6.1.
Working on a languagewhich actually has more resources allows us to studythe effect of training data size.
This will be reportedin Section 6.2.
In Section 6.3, we will show exam-ples of templates learned from the Arabic-to-Englishtraining data.6.1 Languages and GenresThe Arabic-to-English training data contains about200K (target) words randomly selected from anLDC corpus, LDC2006G05 A2E set, plus anArabic-English dictionary with about 89K items.We build our development sets from GALE P4 sets.There are one tune set and two test sets for the MTsystems 2.
TEST-1 has about 5000 segments andTEST-2 has about 3000 segments.2One of the two test sets will later be used to tune an MTcombination system.621MODELTUNE TEST-1 TEST-2BLEU %BL MET BLEU %BL MET BLEU %BL METArabic-to-English newswirebaseline 21.07 12.41 43.77 19.96 11.42 42.79 21.09 11.03 43.74factorized 21.70 13.17 44.85 20.52 11.70 43.83 21.36 11.77 44.72Arabic-to-English webbaseline 10.26 5.02 32.78 9.40 4.87 31.26 14.11 7.34 35.93factorized 10.67 5.34 33.83 9.74 5.20 32.52 14.66 7.69 37.11Chinese-to-English newswirebaseline 13.17 8.04 44.70 19.62 9.32 48.60 14.53 6.82 45.34factorized 13.91 8.09 45.03 20.48 9.70 48.61 15.16 7.37 45.31Chinese-to-English webbaseline 11.52 5.96 42.18 11.44 6.07 41.90 9.83 4.66 39.71factorized 11.98 6.31 42.84 11.72 5.88 42.55 10.25 5.34 40.34Table 1: Experimental results on Arabic-to-English / Chinese-to-English newswire and web data.
%BL stands forBLEU scores for documents whose BLEU scores are in the bottom 75% to 90% range of all documents.
MET standsfor METEOR scores.The Chinese-to-English training data containsabout 200K (target) words randomly selected fromLDC2006G05 C2E set, plus a Chinese-English dic-tionary (LDC2002L27) with about 68K items.
Thedevelopment data setup is similar to that of Arabic-to-English experiments.Chinese-to-English translation is from a morphol-ogy poor language to a morphology rich language,while Arabic-to-English translation is in the oppo-site direction.
It will be interesting to see if factor-ized grammars help on both cases.
Furthermore, wealso test on two genres, newswire and web, for bothlanguages.Table 1 lists the experimental results of all the fourconditions.
The tuning metric is expected BLEU.We are also interested in the BLEU scores for doc-uments whose BLEU scores are in the bottom 75%to 90% range of all documents.
We mark it as %BLin the table.
This metric represents how a systemperformances on difficult documents.
It is importantto certain percentile evaluations.
We also measureMETEOR (Banerjee and Lavie, 2005) scores for allsystems.The system using factorized grammars showsBLEU improvement in all conditions.
We measurethe significance of BLEU improvement with pairedbootstrap resampling as described by (Koehn, 2004).All the BLEU improvements are over 95% confi-dence level.
The new system also improves %BLand METEOR in most of the cases.6.2 Training Data SizeThe experiments to be presented in this sectionare designed to measure the effect of training datasize.
We select Arabic web for this set of experi-ments.
Since the original Arabic-to-English train-ing data LDC2006G05 is a small one, we switch toLDC2006E25, which has about 3.5M target wordsin total.
We randomly select 125K, 250K, 500K, 1Mand 2M sub-sets from the whole data set.
A largerone always includes a smaller one.
We still tune onexpected BLEU, and test on BLEU, %BL and ME-TEOR.The average BLEU improvement on test sets isabout 0.6 on the 125K set, but it gradually dimin-ishes.
For better observation, we draw the curves ofBLEU improvement along with significance test re-sults for each training set.
As shown in Figure 6 and7, more improvement is observed with fewer train-ing data.
This fits well with fact that the baseline MTmodel suffers more on the sparse data problem withsmaller training data.
The reason why the improve-ment diminishes on the full data set could be that therough approximation with 200 most frequent tem-plates cannot fully take advantage of this paradigm,which will be discussed in the next section.622MODEL SIZETUNE TEST-1 TEST-2BLEU %BL MET BLEU %BL MET BLEU %BL METArabic-to-English webbaseline125K8.54 2.96 28.87 7.41 2.82 26.95 11.29 5.06 31.37factorized 8.99 3.44 30.40 7.92 3.57 28.63 12.04 6.06 32.87baseline250K10.18 4.70 32.21 8.94 4.35 30.31 13.71 6.93 35.14factorized 10.57 4.96 33.22 9.34 4.78 31.51 14.02 7.28 36.25baseline500K12.18 5.84 35.59 10.82 5.77 33.62 16.48 8.30 38.73factorized 12.40 6.01 36.15 11.14 5.96 34.38 16.76 8.53 39.27baseline1M13.95 7.17 38.49 12.48 7.12 36.56 18.86 10.00 42.18factorized 14.14 7.41 38.99 12.66 7.34 37.14 19.11 10.29 42.56baseline2M15.74 8.38 41.15 14.18 8.17 39.26 20.96 11.95 45.18factorized 15.92 8.81 41.51 14.34 8.25 39.68 21.42 12.05 45.51baseline3.5M16.95 9.76 43.03 15.47 9.08 41.28 22.83 13.24 47.05factorized 17.07 9.99 43.18 15.49 8.77 41.41 22.72 13.10 47.23Table 2: Experimental results on Arabic web.
%BL stands for BLEU scores for documents whose BLEU scores arein the bottom 75% to 90% range of all documents.
MET stands for METEOR scores.-0.4-0.200.20.40.60.81100000  1e+06BLEUimprovementdata size in logscaleTEST-1Figure 6: BLEU Improvement with 95% confidencerange by using factorized grammars on TEST-1.6.3 Example TemplatesFigure 8 lists seven Arabic-to-English templatesrandomly selected from the transitive verb family.TMPL 151 is an interesting one.
It helps to alleviatethe pronoun dropping problem in Arabic.
However,we notice that most of the templates in the 200 listsare rather simple.
More sophisticated solutions areneeded to go deep into the list to find out better tem-plates in future.It will be interesting to find an automatic orsemi-automatic way to discover source counterpartsof target treelets in the XTAG English Grammar.-0.4-0.200.20.40.60.81100000  1e+06BLEUimprovementdata size in logscaleTEST-2Figure 7: BLEU Improvement with 95% confidencerange by using factorized grammars on TEST-2.Generic rules like this will be very close to hand-craft translate rules that people have accumulated forrule-based MT systems.7 Conclusions and Future WorkIn this paper, we proposed a novel statistical ma-chine translation model using a factorized structure-based translation grammar.
This model not only al-leviates the sparse data problem but only relieves theburden on space and search, both of which are im-minent issues for the popular phrasal and/or hierar-chical MT systems.623VVBTMPL_1X1  VVBDX1TMPL_121TMPL_31V  X1forVBGX1TMPL_151TMPL_61V  X1VBNbyX1TMPL_181TMPL_91X1  VVBDX1theV  X1VBDhe X1X1  V  X2VBZX1 X2Figure 8: Randomly selected Arabic-to-English templates from the transitive verb family.We took low-resource language translation, espe-cially X-to-English translation tasks, for case study.We designed a method to exploit family informa-tion in the XTAG English Grammar to facilitate theextraction of factorized rules.
We tested the newmodel on low-resource translation, and the use offactorized models showed significant improvementin BLEU on systems with 200K words of bi-lingualtraining data of various language pairs and genres.The factorized translation grammar proposed hereshows an interesting way of using richer syntacticresources, with high potential for future research.In future, we will explore various learning meth-ods for better estimation of families, templates andlexical items.
The target linguistic knowledge thatwe used in this paper will provide a nice startingpoint for unsupervised learning algorithms.We will also try to further exploit the factorizedrepresentation with discriminative learning.
Fea-tures defined on templates and families will havegood generalization capability.AcknowledgmentsThis work was supported by DARPA/IPTO ContractHR0011-06-C-0022 under the GALE program3.
Wethank Aravind Joshi, Scott Miller, Richard Schwartzand anonymous reviewers for valuable comments.3Distribution Statement ?A?
(Approved for Public Release,Distribution Unlimited).
The views, opinions, and/or find-ings contained in this article/presentation are those of the au-thor/presenter and should not be interpreted as representing theofficial views or policies, either expressed or implied, of the De-fense Advanced Research Projects Agency or the Department ofDefense.624ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments.
In Proceedings of the43th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 101?104, Ann Arbor,MI.Xavier Carreras and Michael Collins.
2009.
Non-projective parsing for statistical machine translation.In Proceedings of the 2009 Conference of EmpiricalMethods in Natural Language Processing, pages 200?209, Singapore.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 263?270, Ann Ar-bor, MI.Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proceedings ofthe 2009 Conference of Empirical Methods in NaturalLanguage Processing, pages 727?736, Singapore.Jacob Devlin.
2009.
Lexical features for statistical ma-chine translation.
Master?s thesis, Univ.
of Maryland.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
The MIT Press.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages, vol-ume 3, pages 69?124.
Springer-Verlag.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2006.
Extensive classifications of en-glish verbs.
In Proceedings of the 12th EURALEX In-ternational Congress.P.
Koehn and H. Hoang.
2007.
Factored translation mod-els.
In Proceedings of the 2007 Conference of Empiri-cal Methods in Natural Language Processing.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase based translation.
In Proceedingsof the 2003 Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 48?54, Edmonton,Canada.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe 2004 Conference of Empirical Methods in Natu-ral Language Processing, pages 388?395, Barcelona,Spain.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the 2006 Conference of EmpiricalMethods in Natural Language Processing, pages 44?52, Sydney, Australia.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4).Kishore Papineni, Salim Roukos, and Todd Ward.
2001.Bleu: a method for automatic evaluation of machinetranslation.
IBM Research Report, RC22176.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics (ACL).Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective Use of Lin-guistic and Contextual Information for Statistical Ma-chine Translation.
In Proceedings of the 2009 Confer-ence of Empirical Methods in Natural Language Pro-cessing, pages 72?80, Singapore.XTAG-Group.
2001.
A lexicalized tree adjoining gram-mar for english.
Technical Report 01-03, IRCS, Univ.of Pennsylvania.625
