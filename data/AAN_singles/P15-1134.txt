Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1385?1394,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsParse Imputation for Dependency AnnotationsJason Mielens1Liang Sun21Department of Linguistics2Department of Mechanical EngineeringThe University of Texas at Austin The University of Texas at Austin{jmielens,jbaldrid}@utexas.edu sally722@utexas.eduJason Baldridge1AbstractSyntactic annotation is a hard task, but itcan be made easier by allowing annotatorsflexibility to leave aspects of a sentenceunderspecified.
Unfortunately, partial an-notations are not typically directly usablefor training parsers.
We describe a methodfor imputing missing dependencies fromsentences that have been partially anno-tated using the Graph Fragment Language,such that a standard dependency parsercan then be trained on all annotations.
Weshow that this strategy improves perfor-mance over not using partial annotationsfor English, Chinese, Portuguese and Kin-yarwanda, and that performance competi-tive with state-of-the-art unsupervised andweakly-supervised parsers can be reachedwith just a few hours of annotation.1 IntroductionLinguistically annotated data is produced formany purposes in many contexts.
It typicallyrequires considerable effort, particularly for lan-guage documentation efforts in which tooling,data, and expertise in the language are scarce.The challenge presented by this scarcity is com-pounded when doing deeper analysis, such as syn-tactic structure, which typically requires greaterexpertise and existing tooling.
In such scenar-ios, unsupervised approaches are a tempting strat-egy.
While the performance of unsuperviseddependency parsing has improved greatly sinceKlein and Manning?s (2004) Dependency Modelwith Valence (DMV), state-of-the-art unsuper-vised parsers still perform well below supervisedapproaches (Martins et al, 2010; Spitkovsky et al,2012; Blunsom and Cohn, 2010).
Additionally,they typically require large amounts of raw data.While this is not a problem for some languages,many of the world?s languages do not have a clean,digitized corpus available.1For instance, the ap-proach of Naseem et al (2010) is unsupervised inthe sense that it requires no dependency annota-tions, but it still makes use of the raw version ofthe full Penn Treebank.
The approach of Mare-cek et al (2013) requires extra unlabeled texts toestimate parameters.Another strategy is to exploit small amounts ofsupervision or knowledge.
Naseem et al (2010)use a set of universal dependency rules and obtainsubstantial gains over unsupervised methods inmany languages.
Spitkovsky et al (2010b; 2011)use web mark-up and punctuation as additional an-notations.
Alternatively, one could try to obtainactual dependency annotations cheaply.
We usethe Graph Fragment Language (GFL), which wascreated with the goal of making annotations eas-ier for experts and possible for novices (Schneideret al, 2013; Mordowanec et al, 2014).
GFL sup-ports partial annotations, so annotators can omitobvious dependencies or skip difficult construc-tions.
The ability to focus on portions of a sen-tence frees the annotator to target constituents anddependencies that maximize information that willbe most useful for machine-learned parsers.
Forexample, Hwa (1999) found higher-level sentenceconstituents to be more informative for learningparsers than lower-level ones.To support this style of annotation while gettingthe benefit from partial annotations, we develop atwo-stage parser learning strategy.
The first stagecompletes the partial GFL annotations by adaptinga Gibbs tree sampler (Johnson et al, 2007; Sun etal., 2014).
The GFL annotations constrain the treesampling space by using both dependencies andthe constituent boundaries they express.
The sys-tem performs missing dependency arc imputationusing Gibbs sampling ?
we refer to this approach1In fact, standardized writing systems have yet to beadopted for some languages.1385as the Gibbs Parse Completer2(GPC).
The sec-ond stage uses the full dependencies output by theGPC to train Turbo Parser (Martins et al, 2010),and evaluation is done with this trained model onunseen sentences.
In simulation experiments forEnglish, Chinese and Portuguese, we show thatthe method gracefully degrades when applied totraining corpora with increasing percentages of thegold training dependencies removed.
We also doactual GFL annotations for those languages plusKinyarwanda, and show that using the GPC to fillin the missing dependencies after two hours ofannotation enables Turbo Parser to obtain 2-6%better absolute performance than when it has tothrow incomplete annotations out.
Furthermore,the gains are even greater with less annotation timeand it never hurts to use the GPC?so an annota-tion project can pursue a partial annotation strat-egy without undermining the utility of the workfor parser training.This strategy has the further benefit of needingonly a small number of sentences?in our case,under 100 sentences annotated in a 2-4 hour win-dow.
Furthermore, it relies on no outside tools orcorpora other than a part-of-speech tagger; a re-source that can be built with two hours of annota-tion time (Garrette and Baldridge, 2013).2 DataData sources We use four languages from threelanguage families in an effort to both verify thecross-linguistic applicability of our approach, ac-counting for variations in linguistic properties, aswell as to attempt to realistically simulate a real-world, low-resource environment.
Our data comesfrom English (ENG), Chinese (CHI), Portuguese(POR), and Kinyarwanda (KIN).For ENG we use the Penn Treebank (Marcuset al, 1993), converted into dependencies by thestandard process.
Section 23 was used as a testset, and a random sample of sentences from sec-tions 02-21 were selected for annotation with GFLas described below and subsequently used as theminimal training set.
For CHI we use the Chi-nese Treebank (CTB5) (Xue et al, 2005), alsoconverted to dependencies.
The testing set con-sisted of files 1-40/900-931, and the sentences pre-sented for GFL annotation were randomly sam-pled from files 81-899.
The POR data is from2The software, instructions, and data are available athttp://www.github.com/jmielens/gpc-acl-2015Figure 1: GFL example for Mr. Conlon was ex-ecutive vice president and director of the equitydivision.the CoNLL-X Shared Task on Multilingual De-pendency Parsing and is derived from the Bosqueportion of the Floresta sint?a(c)tica corpus (Afonsoet al, 2002), using the standard provided splits fortraining and testing.
The KIN data is a corpus con-sisting of transcripts of testimonies by survivorsof the Rwandan genocide, provided by the KigaliGenocide Memorial Center ?
this data is describedby Garrette and Baldridge (2013).GFL annotation We use a small number of sen-tences annotated using the Graph Fragment Lan-guage (GFL), a simple ASCII markup languagefor dependency grammar (Schneider et al, 2013).Unlike traditional syntactic annotation strategiesrequiring trained annotators and great effort, rapidGFL annotations can be collected from annotatorswho have minimal training.
Kong et al (2014)demonstrate the feasibility of training a depen-dency parser based on a GFL-annotated corpus ofEnglish tweets.An example of GFL is shown in Figure 1: (a) isthe GFL markup itself and (b) is a graphical repre-sentation of the dependencies it encodes.
Figure 1specifies several dependencies: of is a dependentof director, executive vice president and directorare conjuncts and and is the coordinator.
However,the complete internal structure of the phrase theequity division remains unspecified, other than di-vision being marked as the head (via an asterisk).3Finally, Mr. Conlon in square brackets indicates itis a multiword expression.3The graphical representation shows both of these as FEnodes, for fudge expression, indicating they are grouped to-gether but otherwise underspecified.1386CFG Rule EVG distribution DescriptionS ?
YHP (root = H) The head of the sentence is HYH?
LHRH- Split-head representationLH?
HLP (STOP |dir = L, head = H, val = 0) H has no left childrenLH?
L1HP (CONT |dir = L, head = H, val = 0) H has at least one left childL?H?
HLP (STOP |dir = L, head = H, val = 1) H has no more left childrenL?H?
L1HP (CONT |dir = L, head = H, val = 1) H has other left childrenL1H?
YAL?HP (ArgA|dir = L, head = H, val = 1) A is a left child of HTable 1: The CFG-DMV grammar schema from Klein and Manning (2004).
Note that in these rulesH and A are parts-of-speech.
For brevity, we omit the portion of the grammar that handles the right-hand arguments since they are symmetric to the left.
Valency (val) can take the value 1 (we have madeattachments in the direction (dir) d) or 0 (not).CHI ENG KIN PORSentences Annotated 24 34 69 63Tokens Annotated 820 798 988 1067Fully Specified Sentences 4 15 31 20Table 2: Two Hour GFL Annotation StatisticsKong et al (2014) stipulate that the GFL an-notations in their corpus must be fully-specified.They are thus unable to take advantage of such un-derspecified sentences, and we address that limita-tion in this paper.
From the GFL annotations wecan extract and deduce dependency arcs and con-straints (see Section 3.2 for full details) in order toguide the Gibbs sampling process.Time-bounded annotation As described inSection 1, a primary goal of this work was to con-sider the time in which a useful number of depen-dency tree annotations might be collected, suchas might be required during the initial phase of alanguage documentation project or corpus build.To this end our annotators were operating under astrict two hour time limit.
We also collected twofurther hours for English.The annotators were instructed to annotate asmany sentences as possible in the two hours, andthat they should liberally use underspecification,especially for particularly difficult sequences in agiven sentence.
This was done to facilitate theavailability of partial annotations for experimenta-tion.
All of the annotators had some previous ex-perience providing GFL annotations, so no train-ing period was needed.
Annotation was done in30-minute blocks, to provide short breaks for theannotators and so that learning curves could begenerated.
Each language was annotated by asingle annotator.
The ENG and CHI annotatorswere native speakers of their annotation language,while the POR and KIN annotators were non-nativethough proficient speakers.The annotators achieved rates of 400-500 to-kens/hr, whereas we find rates of 150-200 to-kens/hr more typical when annotators are asked tofully specify.
Requiring full specification also in-troduces more errors in cases of annotator uncer-tainty.Table 2 shows the size of the GFL corpora thatwere created.
Typically, over 50% of the sentenceswere not fully specified?the partial annotationsprovided in these are useless to Turbo Parser un-less the missing dependencies are imputed.3 Gibbs Parse Completer (GPC)3.1 Gibbs sampler for CFG-DMV ModelCFG-DMV model The GPC is based on theDMV model, a generative model for the unsu-pervised learning of dependency structures (Kleinand Manning, 2004).
We denote the input cor-pus as ?
= (?1, ?
?
?
,?N), where each ?sis asentence consisting of words and in a sentence ?,word ?ihas an corresponding part-of-speech tag?i.
We denote the set of all words as V?and theset of all parts-of-speech as V?.
We use the part-of-speech sequence as our terminal strings, result-ing in an unlexicalized grammar.
Dependenciescan be formulated as split head bilexical contextfree grammars (CFGs) (Eisner and Satta, 1999)and these bilexical CFGs require that each termi-nal ?iin sentence ?
is represented in a split formby two terminals, with labels marking the left andright heads (?i,L, ?i,R).
Henceforth, we denotew = w0,nas our terminals in the split-form ofsentence ?
(e.g., the terminals for the dog walksareDTLDTRNNLNNRVLVR).
Table 1 showsthe grammar rules for the DMV model, from Kleinand Manning (2004).1387Require: A is parent node of binary rule; wi,kis a validspan of terminals and i+ 1 < kfunction TREESAMPLER(A, i, k)for i < j < k and pair of child nodes ofA:B,C doP (j, B,C) =?wA?BCc(i,j)c(j,k)?pB,i,j?pC,j,kpA,i,kend forSample j?, B?, C?
from multinomial distri-bution for (j, B,C) with probabilities calculated abovereturn j?, B?, C?end functionAlgorithm 1: Sampling split position and rule toexpand parent node.Gibbs sampler The split-head representationencodes dependencies as a CFG.
This enables theuse of a Gibbs sampler algorithm for estimatingPCFGs (Johnson et al, 2007; Sun et al, 2014),and it is straightforward to incorporate constraintsfrom partial annotations into this sampler.
To dothis, we modified the tree-sampling step to incor-porate constraints derived from GFL annotationsand thereby impute the missing dependencies.Given a string w = (w1, ?
?
?wn), we definea span of w as wi,k= (wi+1, ?
?
?
, wk), so thatw = w0,n.
As introduced in Pereira and Schabes(1992), a bracketing B of w is a finite set of spanson w satisfying the requirement that no two spansin a bracketing may overlap unless one span con-tains the other.
For each sentence w = w0,nwedefine the auxiliary function for each span wi,j,0 ?
i < j ?
n:c(i, j) ={1 if span wi,jis valid for B;0 otherwise.Here one span is valid for B if it doesn?t crossany brackets.
Section 3.2 describes how to de-rive bracketing information from GFL annotationsand how to determine if a span wi,jis valid or not.Note that for parsing a corpus without any annota-tions and constraints, c(i, j) = 1 for any span, andthe algorithm is equivalent to the Gibbs sampler inSun et al (2014).There are two parts to the tree-sampling.
Thefirst constructs an inside table as in the Inside-Outside algorithm for PCFGs and the second se-lects the tree by recursively sampling productionsfrom top to bottom.
Consider a sentence w, withsub-spans wi,k= (wi+1, ?
?
?
, wk).
Given ?w(modified rule probabilities ?
given constraints ofsentence w, see Section 3.2), we construct the in-side table with entries pA,i,kfor each nonterminaland each span wi,k: 0 ?
i < k ?
n. We introduceRequire: Arcs is the set of all directed arcs extracted fromannotation for sentence wfunction RULEPROB-SENT(w, ?, Arcs)?w= ?for each directed arc wi< wjdoif i < j thenfor nonterminal A 6= L?jdo?wA?
?= 0 if ?
contains Y?iend forelsefor nonterminal A 6= R?jdo?wA?
?= 0 if ?
contains Y?iend forend ifend forreturn ?wend functionAlgorithm 2: Modifying Rule Probabilities for wto ensure parse tree contains all directed arcs.c(i, j) into the calculation of inside probabilities:pA,i,k= c(i, k)??A?BC?R?i<j<k?wA?BC?
pB,i,j?
pC,j,k(1)Here, pA,i,k= PGA(wi,k| ?w) is the probabilitythat terminals i through k were produced by thenon-terminal A, A ?
BC ?
R are possible rulesto expand A.
The inside table is computed recur-sively using Equation 1.The resulting inside probabilities are then usedto generate trees from the distribution of all validtrees of the sentence.
The tree is generatedfrom top to bottom recursively with the functionTreeSampler defined in Algorithm 1, which in-troduces c(i, j) into the sampling function fromSun et al (2014).3.2 Constraints derived from GFLWe exploit one dependency constraint and twoconstituency constraints from partial GFL anno-tations.Dependency rule Directed arcs are indicatedwith angle brackets pointing from the dependentto its head, e.g.
black > cat.
Once we have a di-rected arc annotation, say ?i> ?j, if i < j, whichmeans word j has a left child, we must have ruleL1?j?
Y?iL?
?jin our parse tree (similarly if i > j,we have R1?j?
R?
?jY?iin our parse tree), where?i, ?jare parts-of-speech for ?iand ?j.
We en-force this by modifying the rule probabilities forsample sentence w to ensure that any sampled treecontains all specified arcs.1388Figure 2: Generating brackets for known headFigure 3: Generating half bracketsBrackets GFL allows annotators to group wordswith parenthesis, which provides an explicit indi-cator of constituent brackets.
Even when the inter-nal structure is left underspecified (e.g.
(the eq-uity division*) in Figure 1 (a), the head is usu-ally marked with *, and we can use this to infersub-constituents.
Given such a set of parenthesesand the words inside them, we generate bracketsover the split-head representations of their parts-of-speech, based on possible positions of the head.Figure 2 shows how to generate brackets for threesituations: the head is the leftmost word, right-most word, or is in a medial position.
For exam-ple, the first annotation indicates that under is thehead of under the agreement, and the rest of wordsare right descendants of under.
This leads to thebracketing shown over the split-heads.Half brackets We can also derive one-sided halfbrackets from dependency arcs by assuming thatdependencies are projective.
For example, in Fig-ure 3, the annotation a > dog specifies that doghas a left child a, so we know that there is a rightbracket before the right-head of dog.
Thus, we candetect invalid spans using the half brackets; if aspan starts after a and ends after dog, this span isinvalid because it would result in crossing brack-ets.
This half bracketing is a unique advantageprovided by the split-head representation.
The de-tails of this algorithm are shown in Algorithm 3.Require: Arcs is the set of all directed arcs extracted forsentence, wa,bis a span to detectfunction DETECTINVALIDSPAN(a, b, Arcs)for each directed arc ?i< ?jdoif i < j thenif a < 2i?
1 < b < 2j thenc(a, b) = 0end ifelseif 2j ?
2 < a < 2i?
1 < b thenc(i, j) = 0end ifend ifend forreturn c(a, b)end functionAlgorithm 3: Detect whether one span is invalidgiven all directed arcs.Figure 4: Process of generating brackets and de-tecting invalid spans.We use both half bracket and full bracket infor-mation, B, to determine whether a span is valid.We set c(i, j) = 0 for all spans over w detectedby Algorithm 3 and violating B.
Then, in the sam-pling scheme, we?ll only sample parse trees thatsatisfy these underlying constraints.Figure 4 shows the resulting blocked out spansin the chart based on both types of brackets forthe given partial annotation, which is Step 1 of theprocess.
The black dog is a constituent with dogmarked as its head, so we generate a full bracketover the terminal string in Step 2.
Also, barks hasa right child loudly; this generates a half bracketbefore VR.
In Step 3, the chart in Figure 4 repre-sents all spans over terminal symbols.
The cells inblack are invalid spans based on the full bracket,and the hatched cells are invalid spans based onthe half bracket.4 ResultsExperiments There are two points of variationto consider in empirical evaluations of our ap-1389Figure 5: English oracle and degradation resultsproach.
The first is the effectiveness of the GPCin imputing missing dependencies and the secondis the effectiveness of the GFL annotations them-selves.
Of particular note with respect to the latteris the reasonable likelihood of divergence betweenthe annotator and the corpus used for evaluation?for example, how coordination is handled andwhether subordinate verbs are dependents or headsof auxiliary verbs.
To this end, we perform simu-lation experiments that remove increasing portionsof gold dependencies from a training corpus to un-derstand imputation performance and annotationexperiments to evaluate the entire pipeline in a re-alistically constrained annotation effort.In that regard, one thing to consider are the part-of-speech tags used by the unlexicalized GPC.These do not come for free, so rather than askannotators to provide them, the raw sentences tobe annotated were tagged automatically.
For En-glish and Kinyarwanda, we used taggers trainedwith resources built in under two hours (Garretteand Baldridge, 2013), so these results are actuallyconstrained to the GFL annotation time plus twohours.
Such taggers were not available for Chineseor Portuguese, so the Stanford tagger (Toutanovaet al, 2003) was used instead.After imputing missing dependencies, the GPCoutputs fully sentences that are used to train Tur-boParser (Martins et al, 2010).
In all cases,we compare to a right-branching baseline (RB).Although comparing to a random baseline ismore typical of imputation experiments, a right-branching baseline provides a stronger initial com-parison.
For the GFL annotation experiments, weuse two additional baselines.
The first is simplyto use the sentences with full annotations and dropany incomplete ones (GFL-DROP).
The second isLanguage ENG CHI PORRB 25.0 11.6 27.0GFL-GPC-25 58.7 33.5 60.2GFL-GPC-50 75.0 46.1 71.4GFL-GPC-75 77.8 50.1 73.7Full 81.6 56.2 78.1Table 3: Results with simulated partial annota-tions, GFL-GPC-X indicates X percent of depen-dencies were retained.to make any partial annotations usable by assum-ing a right-branching completion (GFL-RBC).Simulated partial annotations Figure 5 showsthe learning curve with respect to number of an-notated tokens when retaining 100%, 75%, 50%and 25% of gold-standard training dependenciesand using the GPC to impute the removed ones.With both 75% and 50% retained, performancedegrades gracefully.
It is substantially lower for25%, but the curve is steeper than the others, indi-cating it is on track to catch up.
Nonetheless, onerecommendation from these results is that it prob-ably makes sense to start with a small number offully annotated sentences and then start mixing inpartially annotated ones.Table 3 shows the attachment scores obtainedfor English, Chinese, and Portuguese with varyingproportions of dependencies removed for the GPCto impute.4English and Portuguese hold up wellwith 75% and 50% retained, while Chinese dropsmore precipitously, and 25% leads to substantialreductions in performance for all.Note that these simulations indicate that, givenan equivalent number of total annotated arcs, usingthe GPC is more beneficial than requiring annota-tors to fully specify annotations.
Imputing fiftypercent of the dependency arcs from sentencescontaining 1000 tokens is typically more effectiveby a few points than using the full gold-standardarcs from sentences containing 500 tokens.
Ac-tually, this simulation is too generous to completeannotations in that it leaves out consideration ofthe time and effort required to obtain those 100%full gold-standard arcs: it is often a small part of asentence that consumes the most effort when fullannotation is required.
Additionally, these simula-tion experiments randomly removed dependencieswhile humans tend to annotate higher-level con-4These are based on the same sentences used in the nextsection?s GFL annotation experiments for each language.1390Eval Length < 10 < 20 allGFL-DROP (4hr) 54.5 55.0 52.6GFL-GPC (4hr) 60.1 61.8 55.1Blunsom and Cohn, 2010 67.7 ?
55.7Naseem et al, 2010 71.9 50.4 ?Table 4: English results compared to previous un-supervised and weakly-supervised methods.stituents and leave internal structure (e.g.
of nounphrases) underspecified.
Given Hwa?s (1999) find-ings, we expect non-random partial annotations tobetter serve as a basis for imputation.GFL annotations We conducted three sets ofexperiments with GFL annotations, evaluating onsentences of all lengths, less than 10 words, andless than 20 words.
This was done to determinethe types of sentences that our method works beston and to compare to previous work that evaluateson sentences of different lengths.Table 4 shows how our results on ENG com-pare to others.
Blunsom and Cohn (2010) rep-resent state-of-the-art unsupervised results for alllengths, while Naseem et al (2010) was chosen asa previous weakly-supervised approach.
GFL-GPCachieves similar results on the ?all lengths?
crite-rion as Blunsom and Cohn and substantially out-performs Naseem et al on sentences less than 20words.
Our poor performance on short sentencesis slightly surprising, and may result from an un-even length distribution in the sentences selectedfor annotation?we have only 3 training sentencesless than 10 words?as discussed by Spitkovsky etal.
(2010a).
To correct this problem, both long andshort sentences should be included to construct amore representative sample for annotation.We did not expect GFL-RBC to perform so sim-ilarly to RB.
It is possible that the relatively largenumber of under-specified sentences led to theright-branching quality of GFL-RBC dominating,rather than the more informative GFL annotations.The results of the ENG annotation session canbe seen in Figure 6a.
GFL-GPC is quite strong evenat thirty minutes, with only seven sentences anno-tated.
GFL-DROP picks up substantially at the end;this may be in part explained by the fact that thelast block contained many short sentences, whichprovide greater marginal benefit to GFL-DROP thanto GFL-GPC.The learning curves for the other languages canbe seen in Figures 6b-6d, with a summary avail-able in Table 5.
Like ENG, CHI and POR bothLanguage KIN CHI PORRB 52.6 11.6 27.0GFL-DROP (2hr) 64.4 36.7 59.8GFL-GPC (2hr) 64.5 38.8 65.0Table 5: Non-English results summaryshow clear wins for the GPC strategy.
Of particu-lar note is that the CHI annotations contained manyfewer fully-completed sentences (4) than the ENGannotations (15).
This somewhat addresses thequestion raised by the 25% retention simulationexperiments?the GPC method improves resultsover dropping partial annotations.
The POR resultsshow a consistent strong win for GPC throughout.The KIN results in Figure 6c exhibit a patternunlike the other languages; specifically, the KINdata has a very high right-branching baseline (RBin figures) and responds nearly identically for allof the more informed methods.
Upon investi-gation, this appears to be an artifact of the dataused in KIN evaluation plus domain adaptation is-sues.
The gold data consists of transcribed naturalspeech, whereas the training data consists of sen-tences extracted from the Kinyarwanda Wikipedia.All of the learning curves display a large ini-tial jump after the first round of annotations.
Thisis encouraging for approaches that use annotatedsentences: just a small number of examples pro-vide tremendous benefit, regardless of the strategyemployed.Error analysis The primary errors seen on ananalysis of the GPC-completed sentences variessomewhat between languages.
The ENG data con-tains many short sentences, consisting often of afew words and a punctuation mark.
Part of theGFL convention is that the annotator is free to an-notate punctuation as part of the sentence or in-stead view it as extra-linguistic and drop the punc-tuation from the annotation.
Often the punctuationin the ENG data went unannotated, with the resultbeing that the final parse model is not particularlygood at handling these types of sentences whenencountered in the test set.Specific constructions like coordination andpossession also suffer a similar issue in that an-notators (and corpora) varied slightly on how theywere handled.
Thus, some languages like CHIcontained many of a particular type of error dueto mismatches in the conventions of the annota-tor and corpus.
Issues like this could have beenavoided by a longer training period prior to anno-1391(a) English(b) Chinese(c) Kinyarwanda(d) PortugueseFigure 6: GPC results by annotation time for eval sentences of all lengths.tation, although were this a real annotation project,there would be no existing corpus to compare to atfirst.
This brings up a more basic question of eval-uation - one of usability versus representationalnorm matching.
It is likely that the GFL anno-tations (and thus the models trained on them) di-verge from the gold standard in what amount toannotation conventions rather than substantive lin-guistic divergences.
To evaluate more fully orfairly, we would need test sets produced by thesame set of annotators or an external, task-basedevaluation that uses the dependencies as in input.5 ConclusionsWe have described a modeling strategy that takesadvantage of a Gibbs sampling algorithm for CFGparsing plus constraints obtained from partial an-notations to build dependency parsers.
This strat-egy?s performance improves on that of a parserbuilt only on the available complete annotations.In doing so, our approach supports annotation ef-forts that use GFL to obtain guidance from non-expert human annotators and allow any annotatorto put in less effort than they would to do completeannotations.We find that a remarkably small amount ofsupervised data can rival existing unsupervisedmethods.
While unsupervised methods have beenconsidered an attractive option for low-resourceparsing, they typically rely on large quantities ofclean, raw sentences.
Our method uses less thanone hundred sentences, so in a truly low-resourcescenario, it has the potential to require much lesstotal effort.
For instance, a single native speakercould easily both generate and annotate the sen-tences required for our method in a few hours,while the many thousands of raw sentences neededfor state-of-the-art unsupervised methods could1392take much longer to assemble if there is no ex-isting corpus.
This also means our method wouldbe useful for getting in-domain training data fordomain adaptation for parsers.Finally, our method has the ability to encodeboth universal grammar and test-language gram-mar as a prior.
This would be done by replacingthe uniform prior used in this paper with a priorfavoring those grammar rules during the updating-rule-probabilities phase of the GPC, and would es-sentially have the effect of weighting those gram-mar rules.AcknowledgmentsThis work was supported in part by the U. S. ArmyResearch Laboratory and the U. S. ArmyResearch Office under contract/grant numberW911NF-10-1-0533ReferencesS.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.?Floresta sint?a(c)tica?
: a Treebank for Portuguese.In Proceedings of the 3rd International Conferenceon Language Resources and Evaluation (LREC),pages 1698?1703.
LREC.Phil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1204?1213.
Association for Com-putational Linguistics.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
In Proceedings of the 37th an-nual meeting of the Association for ComputationalLinguistics on Computational Linguistics, pages457?464.
Association for Computational Linguis-tics.Dan Garrette and Jason Baldridge.
2013.
Learning apart-of-speech tagger from two hours of annotation.In HLT-NAACL, pages 138?147.
Citeseer.Rebecca Hwa.
1999.
Supervised grammar inductionusing training data with limited constituent infor-mation.
In Proceedings of the 37th annual meet-ing of the Association for Computational Linguisticson Computational Linguistics, pages 73?79.
Associ-ation for Computational Linguistics.Mark Johnson, Thomas L Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 139?146.Dan Klein and Christopher D Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, page 478.
Association for Com-putational Linguistics.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A Smith.
2014.
A dependency parser fortweets.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,Doha, Qatar, to appear.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational linguistics, 19(2):313?330.David Marecek and Milan Straka.
2013.
Stop-probability estimates computed on a large corpusimprove unsupervised dependency parsing.
In ACL(1), pages 281?290.Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-dro MQ Aguiar, and M?ario AT Figueiredo.
2010.Turbo parsers: Dependency parsing by approxi-mate variational inference.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 34?44.
Association forComputational Linguistics.Michael T. Mordowanec, Nathan Schneider, ChrisDyer, and Noah A Smith.
2014.
Simplified depen-dency annotations with GFL-Web.
ACL 2014, page121.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1234?1244.
Asso-ciation for Computational Linguistics.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th annual meetingon Association for Computational Linguistics, pages128?135.
Association for Computational Linguis-tics.Nathan Schneider, Brendan OConnor, Naomi Saphra,David Bamman, Manaal Faruqui, Noah A Smith,Chris Dyer, and Jason Baldridge.
2013.
Aframework for (under) specifying dependency syn-tax without overloading annotators.
LAW VII & ID,page 51.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2010a.
From baby steps to leapfrog: Howless is more in unsupervised dependency parsing.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages751?759.
Association for Computational Linguis-tics.1393Valentin I Spitkovsky, Daniel Jurafsky, and Hiyan Al-shawi.
2010b.
Profiting from mark-up: Hyper-textannotations for guided parsing.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 1278?1287.
Associa-tion for Computational Linguistics.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2011.
Punctuation: Making a point in un-supervised dependency parsing.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning, pages 19?28.
Association forComputational Linguistics.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2012.
Three dependency-and-boundarymodels for grammar induction.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 688?698.
Asso-ciation for Computational Linguistics.Liang Sun, Jason Mielens, and Jason Baldridge.
2014.Parsing low-resource languages using Gibbs sam-pling for PCFGs with latent annotations.
In Pro-ceedings of the 2014 Conference on Empirical Meth-ods in Natural Language Processing.
Associationfor Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MartaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(02):207?238.1394
