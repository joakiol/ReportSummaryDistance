Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 146?154,Sydney, July 2006. c?2006 Association for Computational LinguisticsIdentification of Event Mentions and their Semantic ClassSteven BethardDepartment of Computer ScienceUniversity of Colorado at Boulder430 UCB, Boulder, CO 80309, USAsteven.bethard@colorado.eduJames H. MartinDepartment of Computer ScienceUniversity of Colorado at Boulder430 UCB, Boulder, CO 80309, USAjames.martin@colorado.eduAbstractComplex tasks like question answeringneed to be able to identify events in textand the relations among those events.
Weshow that this event identification taskand a related task, identifying the seman-tic class of these events, can both be for-mulated as classification problems in aword-chunking paradigm.
We introduce avariety of linguistically motivated fea-tures for this task and then train a systemthat is able to identify events with a pre-cision of 82% and a recall of 71%.
Wethen show a variety of analyses of thismodel, and their implications for theevent identification task.1 IntroductionResearch in question answering, machine transla-tion and other fields has shown that being able torecognize the important entities in a text is oftena critical component of these systems.
Such en-tity information gives the machine access to adeeper level of semantics than words alone canprovide, and thus offers advantages for thesecomplex tasks.
Of course, texts are composed ofmuch more than just sets of entities, and archi-tectures that rely solely on word and entity-basedtechniques are likely to have difficulty with tasksthat depend more heavily on event and temporalrelations.
Consider a question answering systemthat receives the following questions:?
Is Anwar al-Sadat still the president ofEgypt??
How did the linking of the Argentineanpeso to the US dollar in 1991 contribute toeconomic crisis of Argentina in 2003?Processing such questions requires not onlyknowing what the important people, places andother entities are, but also what kind of eventsthey are involved in, the roles they play in thoseevents, and the relations among those events.Thus, we suggest that identifying such events ina text should play an important role in systemsthat attempt to address questions like these.Of course, to identify events in texts, we mustdefine what exactly it is we mean by ?event?.
Inthis work, we adopt a traditional linguistic defini-tion of an event that divides words into two as-pectual types: states and events.
States describesituations that are static or unchanging for theirduration, while events describe situations thatinvolve some internal structure.
For example,predicates like know and love would be statesbecause if we know (or love) someone for a pe-riod of time, we know (or love) that person ateach point during the period.
Predicates like runor deliver a sermon would be events becausethey are built of smaller dissimilar components:run includes raising and lowering of legs anddeliver a sermon includes the various tonguemovements required to produce words.To better explain how we approach the task ofidentifying such events, we first discuss somepast work on related tasks.
Then we briefly dis-cuss the characteristics of the TimeBank, a cor-pus containing event-annotated data.
Next wepresent our formulation of event identification asa classification task and introduce the linguisticfeatures that serve as input to the algorithm.
Fi-nally, we show the results of STEP (our ?Systemfor Textual Event Parsing?)
which applies thesetechniques to the TimeBank data.2 Related EffortsSuch aspectual distinctions have been alive andwell in the linguistic literature since at least thelate 60s (Vendler, 1967).
However, the use of the146term event in natural language processing workhas often diverged quite considerably from thislinguistic notion.
In the Topic Detection andTracking (TDT) task, events were sets of docu-ments that described ?some unique thing thathappens at some point in time?
(Allan et.
al.,1998).
In the Message Understanding Confer-ence (MUC), events were groups of phrases thatformed a template relating participants, timesand places to each other (Marsh and Per-zanowski, 1997).
In the work of Filatova andHatzivassiloglou (2003), events consisted of averb and two named-entities occurring togetherfrequently across several documents on a topic.Several recent efforts have stayed close to thelinguistic definition of events.
One such exampleis the work of Siegel and McKeown (2000)which showed that machine learning modelscould be trained to identify some of the tradi-tional linguistic aspectual distinctions.
Theymanually annotated the verbs in a small set oftexts as either state or event, and then used a va-riety of linguistically motivated features to trainmachine learning models that were able to makethe event/state distinction with 93.9% accuracy.Another closely related effort was the Evitasystem, developed by Saur?
et.
al.
(2005).
Thiswork considered a corpus of events calledTimeBank, whose annotation scheme was moti-vated largely by the linguistic definitions ofevents.
Saur?
et.
al.
showed that a linguisticallymotivated and mainly rule-based algorithm couldperform well on this task.Our work draws from both the Siegel andMcKeown and Saur?
et.
al.
works.
We considerthe same TimeBank corpus as Saur?
et.
al., butapply a statistical machine learning approachakin to that of Siegel and McKeown.
We demon-strate that combining machine learning tech-niques with linguistically motivated features canproduce models from the TimeBank data that arecapable of making a variety of subtle aspectualdistinctions.3 Events in the TimeBankTimeBank (Pustejovsky, et.
al.
2003b) consistsof just under 200 documents containing 70,000words; it is drawn from news texts from a varietyof different domains, including newswire andtranscribed broadcast news.
These documents areannotated using the TimeML annotation scheme(Pustejovsky, et.
al.
2003a), which aims to iden-tify not just times and dates, but events and thetemporal relations between these events.Of interest here are the EVENT annotations,of which TimeBank 1.1 has annotated 8312.TimeBank annotates a word or phrase as anEVENT if it describes a situation that can ?hap-pen?
or ?occur?, or if it describes a ?state?
or?circumstance?
that ?participate[s] in an opposi-tion structure in a given text?
(Pustejovsky, et.
al.2003b).
Note that the TimeBank events are notrestricted to verbs; nouns and adjectives denoteevents as well.The TimeBank definition of event differs in afew ways from the traditional linguistic defini-tion of event.
TimeBank EVENTs include notonly the normal linguistic events, but also somelinguistic states, depending on the contexts inwhich they occur.
For example1, in the sentenceNone of the people on board the airbus survivedthe crash the phrase on board would be consid-ered to describe an EVENT because that statechanges in the time span covered by the text.
Notall linguistic states become TimeBank EVENTsin this manner, however.
For example, the statedescribed by New York is on the east coast holdstrue for a time span much longer than the typicalnewswire document and would therefore not belabeled as an EVENT.In addition to identifying which words in theTimeBank are EVENTs, the TimeBank also pro-vides a semantic class label for each EVENT.The possible labels include OCCURRENCE,PERCEPTION, REPORTING, ASPECTUAL,STATE, I_STATE, I_ACTION, and MODAL,and are described in more detail in (Pustejovsky,et.
al.
2003a).We consider two tasks on this data:(1) Identifying which words and phrases areEVENTs, and(2) Identifying their semantic classes.The next section describes how we turn thesetasks into machine learning problems.4 Event Identification as ClassificationWe view event identification as a classificationtask using a word-chunking paradigm similar tothat used by Carreras et.
al.
(2002).
For eachword in a document, we assign a label indicatingwhether the word is inside or outside of an event.We use the standard B-I-O formulation of theword-chunking task that augments each classlabel with an indicator of whether the given word1These examples are derived from (Pustejovsky, et.
al.2003b)147is (B)eginning, (I)nside or (O)utside of a chunk(Ramshaw & Marcus, 1995).
So, for example,under this scheme, sentence (1) would have itswords labeled as in Table 1.
(1) The company?s sales force[EVENT(I_ACTION) applauded] the[EVENT(OCCURRENCE) shake up]The two columns of labels in Table 1 show howthe class labels differ depending on our task.
Ifwe?re interested only in the simple event identi-fication task, it?s sufficient to know that ap-plauded and shake both begin events (and sohave the label B), up is inside an event (and sohas the label I), and all other words are outsideevents (and so have the label O).
These labels areshown in the column labeled Event Label.
If inaddition to identifying events, we also want toidentify their semantic classes, then we need toknow that applauded begins an intentional actionevent (B_I_ACTION), shake begins an occur-rence event (B_OCCURRENCE), up is inside anoccurrence event (I_OCCURRENCE), and allother words are outside of events (O).
These la-bels are shown in the column labeled Event Se-mantic Class Label.
Note that while the eightsemantic class labels in the TimeBank could po-tentially introduce as many as 8 ?
2 + 1 = 17chunk labels, not all types of events appear asmulti-word phrases, so we see only 13 of theselabels in our data.5 Classifier FeaturesHaving cast the problem as a chunking task, ournext step is to select and represent a useful set offeatures.
In our case, since each classificationinstance is a word, our features need to providethe information that we deem important for rec-ognizing whether a word is part of an event ornot.
We consider a number of such features,grouped into feature classes for the purposes ofdiscussion.5.1 Text featureThis feature is just the textual string for the word.5.2 Affix featuresThese features attempt to isolate the potentiallyimportant subsequences of characters in theword.
These are intended to identify affixes thathave a preference for different types of events.Affixes: These features identify the first threeand four characters of the word, and the last threeand four characters of the word.Nominalization suffix: This feature indicateswhich of the suffixes typically associated withnominalizations ?
ing(s), ion(s), ment(s), andnce(s) ?
the word ends with.
This overlaps withthe Suffixes feature, but allows the classifier tomore easily treat nominalizations specially.5.3 Morphological featuresThese features identify the various morphologi-cal variants of a word, so that, for example, thewords resist, resisted and resistance can all beidentified as the same basic event type.Morphological stem: This feature gives the baseform of the word, so for example, the stem ofassisted is assist and the stem of investigations isinvestigation.
Stems are identified with a lookuptable from the University of Pennsylvania ofaround 300,000 words.Root verb: This feature gives the verb fromwhich the word is derived.
For example, assis-tance is derived from assist and investigation isderived from investigate.
Root verbs are identi-fied with an in-house lookup table of around5000 nominalizations.5.4 Word class featuresThese features attempt to group the words intodifferent types of classes.
The intention here isto identify correlations between classes of wordsand classes of events, e.g.
that events are morelikely to be expressed as verbs or in verb phrasesthan they are as nouns.Part-of-speech: This feature contains the word?spart-of-speech based on the Penn Treebank tagset.
Part-of-speech tags are assigned by the MX-POST maximum-entropy based part-of-speechtagger (Ratnaparkhi, 1996).Word Event Label Event SemanticClass LabelThe O Ocompany O O?s O Osales O Oforce O Oapplauded B B_I_ACTIONthe O Oshake B B_OCCURRENCEup I I_OCCURRENCE.
O OTable 1: Event chunks for sentence (1)148Syntactic-chunk label: The value of this featureis a B-I-O style label indicating what kind ofsyntactic chunk the word is contained in, e.g.noun phrase, verb phrase, or prepositionalphrase.
These are assigned using a word-chunking SVM-based system trained on theCoNLL-2000 data2 (which uses the lowest nodesof the Penn TreeBank syntactic trees to breaksentences into base phrases).Word cluster: This feature indicates which verbor noun cluster the word is a member of.
Theclusters were derived from the co-occurrencestatistics of verbs and their direct objects, in thesame manner as Pradhan et.
al.
(2004).
This pro-duced 128 clusters (half verbs, half nouns) cover-ing around 100,000 words.5.5 Governing featuresThese features attempt to include some simpledependency information from the surroundingwords, using the dependency parses produced byMinipar3.
These features aim to identify eventsthat are expressed as phrases or that requireknowledge of the surrounding phrase to be iden-tified.Governing light verb: This feature indicateswhich, if any, of the light verbs be, have, get,give, make, put, and take governs the word.
Thisis intended to capture adjectival predicates suchas may be ready, and nominal predicates such asmake an offer, where ready and offer should beidentified as events.Determiner type: This feature indicates the typeof determiner a noun phrase has.
If the nounphrase has an explicit determiner, e.g.
a, the orsome, the value of this feature is the determineritself.
We use the determiners themselves as fea-ture values here because they form a small,closed class of words.
For open-class determiner-like modifiers, we instead group them intoclasses.
For noun phrases that are explicitlyquantified, like a million dollars, the value isCARDINAL, while for noun phrases modifiedby other possessive noun phrases, like Bush'sreal objectives, the value is GENITIVE.
Fornoun phrases without a determiner-like modifier,the value is PROPER_NOUN, BARE_PLURALor BARE_SINGULAR, depending on the nountype.2http://cnts.uia.ac.be/conll2000/3http://www.cs.ualberta.ca/~lindek/minipar.htmSubject determiner type: This feature indicatesfor a verb the determiner type (as above) of itssubject.
This is intended to distinguish genericsentences like Cats have fur from non-genericslike The cat has fur.5.6 Temporal featuresThese features try to identify temporal relationsbetween words.
Since the duration of a situationis at the core of the TimeBank definition ofevents, features that can get at such informationare particularly relevant.Time chunk label: The value of this feature is aB-I-O label indicating whether or not this word iscontained in a temporal annotation.
The temporalannotations are produced by a word-chunkingSVM-based system trained on the temporal ex-pressions (TIMEX2 annotations) in the TERN2004 data4.
In addition to identifying expres-sions like Monday and this year, the TERN dataidentifies event-containing expressions like thetime she arrived at her doctor's office.Governing temporal: This feature indicateswhich kind of temporal preposition governs theword.
Since the TimeBank is particularly inter-ested in which events start or end within the timespan of the document, we consider prepositionslikely to indicate such a change of state, includ-ing after, before, during, following, since, till,until and while.Modifying temporal: This feature indicateswhich kind of temporal expression modifies theword.
Temporal expressions are recognized asabove, and the type of modification is either thepreposition that joins the temporal annotation tothe word, or ADVERBIAL for any non-preposition modification.
This is intended to cap-ture that modifying temporal expressions oftenindicate event times, e.g.
He ran the race in anhour.5.7 Negation featureThis feature indicates which negative particle,e.g.
not, never, etc., modifies the word.
The ideais based Siegel and McKeown?s (2000) findingswhich suggested that in some corpora states oc-cur more freely with negation than events do.5.8 WordNet hypernym featuresThese features indicate to which of the WordNetnoun and verb sub-hierarchies the word belongs.4http://timex2.mitre.org/tern.html149Rather than include all of the thousands of dif-ferent sub-hierarchies in WordNet, we first se-lected the most useful candidates by looking atthe overlap with WordNet and our training data.For each hierarchy in WordNet, we considered aclassifier that labeled all words in that hierarchyas events, and all words outside of that hierarchyas non-events5.
We then evaluated these classifi-ers on our training data, and selected the ten withthe highest F-measures.
This resulted in selectingthe following synsets:?
noun: state?
noun: psychological feature?
noun: event?
verb: think, cogitate, cerebrate?
verb: move, displace?
noun: group, grouping?
verb: act, move?
noun: act, human action, human activity?
noun: abstraction?
noun: entityThe values of the features were then whether ornot the word fell into the hierarchy defined byeach one of these roots.
Note that since there areno WordNet senses labeled in our data, we ac-cept a word as falling into one of the above hier-archies if any of its senses fall into that hierar-chy.6 Classifier ParametersThe features described in the previous sectiongive us a way to provide the learning algorithmwith the necessary information to make a classi-fication decision.
The next step is to convert ourtraining data into sets of features, and feed theseclassification instances to the learning algorithm.For the learning task, we use the TinySVM6 sup-port vector machine (SVM) implementation inconjunction with YamCha7 (Kudo & Matsumoto,2001), a suite for general-purpose chunking.YamCha has a number of parameters that de-fine how it learns.
The first of these is the win-dow width of the ?sliding window?
that it uses.5We also considered the reverse classifiers, which classi-fied all words in the hierarchy as non-events and all wordsoutside the hierarchy as events.6http://chasen.org/~taku/software/TinySVM/7http://chasen.org/~taku/software/yamcha/A sliding window is a way of including some ofthe context when the classification decision ismade for a word.
This is done by including thefeatures of preceding and following words inaddition to the features of the word to be classi-fied.
To illustrate this, we consider our earlierexample, now augmented with some additionalfeatures in Table 2.To classify up in this scenario, we now looknot only at its features, but at the features ofsome of the neighboring words.
For example, ifour window width was 1, the feature values wewould use for classification would be those in theoutlined box, that is, the features of shake, upand the sentence final period.
Note that we donot include the classification labels for either upor the period since neither of these classificationsis available at the time we try to classify up.
Us-ing such a sliding window allows YamCha toinclude important information, like that up ispreceded by shake and that shake was identifiedas beginning an event.In addition to the window width parameter,YamCha also requires values for the followingthree parameters: the penalty for misclassifica-tion (C), the kernel?s polynomial degree, and themethod for applying binary classifiers to ourmulti-class problem, either pair-wise or one-vs-rest.
In our experiments, we chose a one-vs-restmulti-class scheme to keep training time down,and then tried different variations of all the otherparameters to explore a variety of models.7 Baseline ModelsTo be able to meaningfully evaluate the modelswe train, we needed to establish a reasonablebaseline.
Because the majority class baselinewould simply label every word as a non-event,we introduce two baseline models that should bemore reasonable: Memorize and Sim-Evita.Word POS Stem LabelThe DT the Ocompany NN company O?s POS ?s Osales NNS sale Oforce NN force Oapplauded VBD applaud BThe DT the Oshake NN shake Bup RP up.
.
.Table 2: A window of word features150The Memorize baseline is essentially a lookuptable ?
it memorizes the training data.
This sys-tem assigns to each word the label with which itoccurred most frequently in the training data, orthe label O (not an event) if the word never oc-curred in the training data.The Sim-Evita model is our attempt to simu-late the Evita system (Saur?
et.
al.
2005).
As partof its algorithm, Evita includes a check that de-termines whether or not a word occurs as anevent in TimeBank.
It performs this check evenwhen evaluated on TimeBank, and thus thoughEvita reports 74% precision and 87% recall,these numbers are artificially inflated because thesystem was trained and tested on the same cor-pus.
Thus we cannot directly compare our resultsto theirs.
Instead, we simulate Evita by taking theinformation that it encodes as rules, and encod-ing this instead as features which we provide to aYamCha-based system.Saur?
et.
al.
(2005) provides a description ofEvita?s rules, which, according to the text, arebased on information from lexical stems, part ofspeech tags, syntactic chunks, weak stativepredicates, copular verbs, complements of copu-lar predicates, verbs with bare plural subjects andWordNet ancestors.
We decided that the follow-ing features most fully covered the same infor-mation:?
Text?
Morphological stem?
Part-of-speech?
Syntactic-chunk label?
Governing light verb?
Subject determiner type?
WordNet hypernymsWe also decided that since Evita does not con-sider a word-window around the word to be clas-sified, we should set our window size parameterto zero.Because our approximation of Evita uses afeature-based statistical machine learning algo-rithm instead of the rule-based Evita algorithm, itcannot predict how well Evita would perform ifit had not used the same data for training andtesting.
However, it can give us an approxima-tion of how well a model can perform using in-formation similar to that of Evita.8 ResultsHaving decided on our feature space, our learn-ing model, and the baselines to which we willcompare, we now describe the results of ourmodels on the TimeBank.
We selected a strati-fied sample of 90% of the TimeBank data for atraining set, and reserved the remaining 10% fortesting8.We consider three evaluation measures: preci-sion, recall and F-measure.
Precision is definedas the number of B and I labels our system iden-tifies correctly, divided by the total number of Band I labels our system predicted.
Recall is de-fined as the number of B and I labels our systemidentifies correctly, divided by the total numberof B and I labels in the TimeBank data.
F-measure is defined as the geometric mean of pre-cision and recall9.To determine the best parameter settings forthe models, we performed cross-validations onour training data, leaving the testing data un-touched.
We divided the training data randomlyinto five equally-sized sections.
Then, for eachset of parameters to be evaluated, we determineda cross-validation F-measure by averaging the F-measures of five runs, each tested on one of thetraining data sections and trained on the remain-ing training data sections.
We selected the pa-rameters of the model that had the best cross-validation F-measure on the training data as theparameters for the rest of our experiments.
Forthe simple event identification model this se-lected a window width of 2, polynomial degreeof 3 and C value of 0.1, and for the event andclass identification model this selected a windowwidth of 1, polynomial degree of 1 and C value0.1.
For the Sim-Evita simple event identificationmodel this selected a degree of 2 and C value of0.01, and for the Sim-Evita event and class iden-tification model, this selected a degree of 1 and Cvalue of 1.0.Having selected the appropriate parameters forour learning algorithm, we then trained our SVMmodels on the training data.
Table 3 presents theresults of these models on the test data.
Ourmodel (named STEP above for ?System for Tex-8The testing documents were:APW19980219.0476, APW19980418.0210,NYT19980206.0466, PRI19980303.2000.2550,ea980120.1830.0071, and the wsj_XXXX_orig documentsnumbered 0122, 0157, 0172, 0313, 0348, 0541, 0584, 0667,0736, 0791, 0907, 0991 and 1033.9RPRPF+?
?=2151tual Event Parsing?)
outperforms both baselineson both tasks.
For simple event identification, themain win over both baselines is an increased re-call.
Our model achieves a recall of 70.6%, about5% better than our simulation of Evita, andnearly 15% better than the Memorize baseline.For event and class identification, the win isagain in recall, though to a lesser degree.
Oursystem achieves a recall of 51.2%, about 5% bet-ter than Sim-Evita, and 10% better than Memo-rize.
On this task, we also achieve a precision of66.7%, about 10% better than the precision ofSim-Evita.
This indicates that the model trainedwith no context window and using the Evita-likefeature set was at a distinct disadvantage over themodel which had access to all of the features.Table 4 and Table 5 show the results of oursystems on various sub-tasks, with the ?%?
col-umn indicating what percent of the events in thetest data each subtask contained.
Table 4 showsthat in both tasks, we do dramatically better onverbs than on nouns, especially as far as recall isconcerned.
This is relatively unsurprising ?
notonly is there more data for verbs (59% of eventwords are verbs, while only 28% are nouns), butour models generally do better on words theyhave seen before, and there are many more nounswe have not seen than there are verbs.Table 5 shows how well we did individuallyon each type of label.
For simple event identifi-cation (the top two rows) we can see that we dosubstantially better on B labels than on I labels,as we would expect since 92% of event wordsare labeled B.
The label-wise performance forthe event and class identification (the bottomseven rows) is more interesting.
Our best per-formance is actually on Reporting event words,even though the data is mainly Occurrence eventwords.
One reason for this is that instances of theword said make up about 60% of Reportingevent words in the TimeBank.
The word said isrelatively easy to get right because it comes withby far the most training data10, and because it isalmost always an event: 98% of the time in theTimeBank, and 100% of the time in our test data.To determine how much each of the featuresets contributed to our models we also performeda pair of ablation studies.
In each ablation study,we trained a series of models on successivelyfewer feature sets, removing the least importantfeature set each time.
The least important featureset was determined by finding out which featureset?s removal caused the smallest drop in F-measure.
The result of this process was a list ofour feature sets, ordered by importance.
Theselists are given for both tasks in Table 6, alongwith the precision, recall and F-measures of thevarious corresponding models.
Each row inTable 6 corresponds to a model trained on thefeature sets named in that row and all the rowsbelow it.
Thus, on the top row, no feature setshave been removed, and on the bottom row onlyone feature set remains.10The word ?said?
has over 600 instances in TimeBank.The word with the next most instances has just over 200Event Identification Event and Class IdentificationModel Precision Recall F Precision Recall FMemorize 0.806 0.557 0.658 0.640 0.413 0.502Sim-Evita 0.812 0.659 0.727 0.571 0.459 0.509STEP 0.820 0.706 0.759 0.667 0.512 0.579Table 3: Overall results for both tasksEvent Identification Event and Class Identification% Precision Recall F % Precision Recall FVerbs 59 0.864 0.903 0.883 59 0.714 0.701 0.707Nouns 28 0.729 0.432 0.543 28 0.473 0.261 0.337Table 4: Results by word class for both tasks% Precision Recall FB 92 0.827 0.737 0.779I 8 0.679 0.339 0.452B Occurrence 44 0.633 0.727 0.677B State 14 0.519 0.136 0.215B Reporting 11 0.909 0.779 0.839B Istate 10 0.737 0.378 0.500B Iaction 10 0.480 0.174 0.255I State 7 0.818 0.173 0.286B Aspectual 3 0.684 0.684 0.684Table 5: Results by label152So, for example, in the simple event identifica-tion task, we see that the Governing, Negation,Affix and WordNet features are hurting the clas-sifier somewhat ?
a model trained without thesefeatures performs at an F-measure of 0.772, morethan 1% better than a model including these fea-tures.
In contrast, we can see that for the eventand semantic class identification task, the Word-Net and Affix features are actually among themost important, with only the Word class fea-tures accompanying them in the top three.
Theseablation results suggest that word class, textual,morphological and temporal information is mostuseful for simple event identification, and affix,WordNet and negation information is only reallyneeded when the semantic class of an event mustalso be identified.The last thing we investigated was the effectof additional training data.
To do so, we trainedthe model on increasing fractions of the trainingdata, and measured the classification accuracy onthe testing data of each of the models thustrained.
The resulting graph is shown in Figure 1.The Majority line indicates the classifier accu-racy when the classifier always guesses majorityclass, that is, (O)utside of an event.
We can seefrom the two learning curves that even with onlythe small amount of data available in theTimeBank, our models are already reaching thelevel part of the learning curve at somewherearound 20% of the data.
This suggests that,though additional data may help somewhat in thedata sparseness problem, substantial further pro-gress on this task will require new, more descrip-tive features.9 ConclusionsIn this paper, we showed that statistical machinelearning techniques can be successfully appliedto the problem of identifying fine-grained eventsin a text.
We formulated this task as a statisticalclassification task using a word-chunking para-digm, where words are labeled as beginning, in-side or outside of an event.
We introduced a va-riety of relevant linguistically-motivated fea-tures, and showed that models trained in this waycould perform quite well on the task, with a pre-cision of 82% and a recall of 71%.
This methodextended to the task of identifying the semanticclass of an event with a precision of 67% and arecall of 51%.
Our analysis of these models indi-cates that while the simple event identificationtask can be approached with mostly simple textand word-class based features, identifying thesemantic class of an event requires features thatencode more of the semantic context of thewords.
Finally, our training curves suggest thatfuture research in this area should focus primar-ily on identifying more discriminative features.Event Identification  Event and Class IdentificationFeature set Precision Recall F  Feature set Precision Recall FGoverning 0.820 0.706 0.759  Governing 0.667 0.512 0.579Negation 0.824 0.713 0.765  Temporal 0.675 0.513 0.583Affix 0.826 0.715 0.766  Negation 0.672 0.510 0.580WordNet 0.818 0.723 0.768  Morphological 0.670 0.509 0.579Temporal 0.820 0.729 0.772  Text 0.671 0.505 0.576Morphological 0.816 0.727 0.769  WordNet 0.679 0.497 0.574Text 0.816 0.697 0.752  Word class 0.682 0.474 0.559Word class 0.719 0.677 0.697  Affix 0.720 0.421 0.531Table 6: Ablations for both tasks.
For each task, the least important feature sets appear at the top of thetable, and most important feature sets appear at the bottom.
For each row, the precision, recall and F-measure indicate the scores of a model trained with only the feature sets named in that row and therows below it.0.850.890.930.970 0.2 0.4 0.6 0.8 1Fraction of Training DataAccuracyonTestDataEvent Event+Semantic MajorityFigure 1: Learning Curves15310 AcknowledgmentsThis work was partially supported by a DHS fel-lowship to the first author and by ARDA underAQUAINT project MDA908-02-C-0008.
Com-puter time was provided by NSF ARI Grant#CDA-9601817, NSF MRI Grant #CNS-0420873, NASA AIST grant #NAG2-1646, DOESciDAC grant #DE-FG02-04ER63870, NSFsponsorship of the National Center for Atmos-pheric Research, and a grant from the IBMShared University Research (SUR) program.Any opinions, findings, or recommendations arethose of the authors and do not necessarily reflectthe views of the sponsors.
Particular thanks go toWayne Ward and Martha Palmer for many help-ful discussions and comments.ReferencesJames Allan, Jaime Carbonell, George Dodding-ton,Jonathan Yamron and Yiming Yang.
1998.
TopicDetection and Tracking Pilot Study: Final Report.In: Proceedings of DARPA Broadcast News Tran-scription and Understanding Workshop.Xavier Carreras, Llu?s M?rquez and Llu?s Padr?.Named Entity Extraction using AdaBoost.
2002.
InProceedings of CoNNL-2002.Elena Filatova and Vasileios Hatzivassiloglou.
Do-main-Independent Detection, Extraction, and La-beling of Atomic Events.
2003.
In the Proceedingsof Recent Advances in Natural Language Process-ing Conference, September 2003.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofNAACL 2001.Elaine Marsh and Dennis Perzanowski.
1997.
MUC-7evaluation of IE technology: Over-view of results.In Proceedings of the Seventh MUC.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin and Daniel Jurafsky.
2004.
Shal-low Semantic Parsing using Support Vector Ma-chines.
In Proceedings of HLT/NAACL 2004.James Pustejovsky, Jos?
Casta?o, Robert Ingria,Roser Saur?, Robert Gaizauskas, Andrea Setzer andGraham Katz.
TimeML: 2003a.
Robust Specifica-tion of Event and Temporal Expressions in Text.
InProceedings of the Fifth International Workshop onComputational Semantics (IWCS-5)James Pustejovsky, Patrick Hanks, Roser Saur?, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro and Marcia Lazo.
2003b.
The TIMEBANKCorpus.
In Proceedings of Corpus Linguistics2003, 647-656.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text Chunking using Transformation-Based Learn-ing.
In Proceedings of the ACL Third Workshopon Very Large Corpora.
82-94.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of EMNLP 1996.Roser Saur?, Robert Knippen, Marc Verhagen andJames Pustejovsky 2005.
Evita: A RobustEvent Recognizer For QA Systems.
In Proceedingsof HLT-EMNLP 2005.Eric V. Siegel and Kathleen R. McKeown.
LearningMethods to Combine Linguistic Indicators: Im-proving Aspectual Classification and RevealingLinguistic Insights.
Computational Linguistics,26(4):595 627.Zeno Vendler.
1967.
Verbs and times.
In Linguisticsand Philosophy.
Cornell University Press, Ithaca,New154
