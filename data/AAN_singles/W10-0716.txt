Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 99?107,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing the Amazon Mechanical Turk to Transcribe andAnnotate Meeting Speech for Extractive SummarizationMatthew Marge   Satanjeev Banerjee   Alexander I. RudnickySchool of Computer Science, Carnegie Mellon UniversityPittsburgh, PA 15213, USA{mrmarge,banerjee,air}@cs.cmu.eduAbstractDue to its complexity, meeting speech pro-vides a challenge for both transcription andannotation.
While Amazon?s Mechanical Turk(MTurk) has been shown to produce good re-sults for some types of speech, its suitabilityfor transcription and annotation of spontane-ous speech has not been established.
We findthat MTurk can be used to produce high-quality transcription and describe two tech-niques for doing so (voting and corrective).We also show that using a similar approach,high quality annotations useful for summari-zation systems can also be produced.
In bothcases, accuracy is comparable to that obtainedusing trained personnel.1 IntroductionRecently, Amazon?s Mechanical Turk (MTurk) hasbeen shown to produce useful transcriptions ofspeech data; Gruenstein et al (2009) have success-fully used MTurk to correct the transcription out-put from a speech recognizer, while Novotney andCallison-Burch (2010) used MTurk for transcrib-ing a corpus of conversational speech.
These stu-dies suggest that transcription, formerly consideredto be an exacting task requiring at least some train-ing, could be carried out by casual workers.
How-ever, only fairly simple transcription tasks werestudied.We propose to assess the suitability of MTurkfor processing more challenging material, specifi-cally recordings of meeting speech.
Spontaneousspeech can be difficult to transcribe because it maycontain false starts, disfluencies, mispronunciationsand other defects.
Similarly for annotation, meet-ing content may be difficult to follow and conven-tions difficult to apply consistently.Our first goal is to ascertain whether MTurktranscribers can accurately transcribe spontaneousspeech, containing speech errors and of variableutterance length.Our second goal is to use MTurk for creatingannotations suitable for extractive summarizationresearch, specifically labeling each utterance aseither ?in-summary?
or ?not in-summary?.
Amongother challenges, this task cannot be decomposedinto small independent sub-tasks?for example,annotators cannot be asked to annotate a singleutterance independent of other utterances.
To ourknowledge, MTurk has not been previously ex-plored for the purpose of summarization annota-tion.2 Meeting Speech Transcription TaskWe recently explored the use of MTurk for tran-scription of short-duration clean speech (Marge etal., 2010) and found that combining independenttranscripts using ROVER yields very close agree-ment with a gold standard (2.14%, comparable toexpert agreement).
But simply collecting indepen-dent transcriptions seemed inefficient: the ?easy?parts of each utterance are all transcribed the same.In the current study our goal is determine whethera smaller number of initial transcriptions can beused to identify easy- and difficult-to-transcriberegions, so that the attention of subsequent tran-scribers can be focused on the more difficult re-gions.2.1 ProcedureIn this corrective strategy for transcription, wehave two turkers to independently produce tran-scripts.
A word-level minimum edit distance me-tric is then used to align the two transcripts andlocate disagreements.
These regions are replacedwith underscores, and new turkers are asked totranscribe those regions.Utterances were balanced for transcription dif-ficulty (measured by the native English back-99ground of the speaker and utterance length).
Forthe first pass transcription task, four sets of jobswere posted for turkers to perform, with each pay-ing $0.01, $0.02, $0.04, or $0.07 per approvedtranscription.
Payment was linearly scaled with thelength of the utterance to be transcribed at a rate of$0.01 per 10 seconds of speech, with an additionalpayment of $0.01 for providing feedback.
In eachjob set, there were 12 utterances to be transcribed(yielding a total of 24 jobs available given twotranscribers per utterance).
Turkers were free totranscribe as many utterances as they could acrossall payment amounts.After acquiring two transcriptions, we alignedthem, identified points of disagreement and re-posted the transcripts and the audio as part of anext round of job sets.
Payment amounts were keptthe same based on utterance length.
In this secondpass of transcriptions, three turkers were recruitedto correct and amend each transcription.
Thus, atotal of five workers worked on every transcriptionafter both iterations of the corrective task.
In ourexperiment 23 turkers performed the first phase ofthe task, and 28 turkers the corrective task (4workers did both passes).2.2 First and Second Pass InstructionsFirst-pass instructions asked turkers to listen toutterances with an embedded audio player pro-vided with the HIT.
Turkers were instructed totranscribe every word heard in the audio and tofollow guidelines for marking speaker mispronun-ciations and false starts.
Filled pauses (?uh?, ?um?,etc.)
were not to be transcribed in the first pass.Turkers could replay the audio as many times asnecessary.In the second pass, turkers were instructed tofocus on the portions of the transcript marked withunderscores, but also to correct any other wordsthey thought were incorrect.
The instructions alsoasked turkers to identify three types of filler words:?uh?, ?um?, and ?lg?
(laughter).
We selected thisset since they were the most frequent in the goldstandard transcripts.
Again, turkers could replaythe audio.2.3 Speech CorpusThe data were sampled from a previously-collectedcorpus of natural meetings (Banerjee and Rud-nicky, 2007).
The material used in this papercomes from four speakers, two native Englishspeakers and two non-Native English speakers (allmale).
We selected 48 audio clips; 12 from each ofthe four speakers.
Within each speaker's set ofclips, we further divided the material into fourlength categories: ~5, ~10, ~30 and ~60 sec.
Thespeech material is conversational in nature; thegold standard transcriptions of this data includedapproximately 15 mispronunciations and 125 falsestarts.
Table 1 presents word count informationrelated to the utterances in each length category.UtteranceLengthWord Count(mean)StandardDeviationUtteranceCount5 sec 14  5.58 1210 sec 24.5  7.26 1230 sec 84  22.09 1260 sec 146.6  53.17 12Table 1.
Utterance characteristics.3 Meeting Transcription AnalysisEvaluation of first and second pass corrections wasdone by calculating word error rate (WER) with agold standard, obtained using the transcriptionprocess described in (Bennett and Rudnicky,2002).
Before doing so, we normalized the candi-date MTurk transcriptions as follows: spell-checking (with included domain-specific technicalterms), and removal of punctuation (periods, com-mas, etc.).
Apostrophes were retained.Table 2.
WER across transcription iterations.3.1 First-Pass Transcription ResultsResults from aligning our first-pass transcriptionswith a gold standard are shown in the second col-umn of Table 2.
Overall error rate was 23.8%,which reveals the inadequacy of individual turkertranscriptions, if no further processing is done.
(Remember that first-pass transcribers were askedto leave out fillers even though the gold standardcontained them, thus increasing WER).UtteranceLengthFirst-PassWERSecond-PassWERROVER-3WER5 sec.
31.5% 19.8% 15.3%10 sec.
26.7% 20.3% 13.8%30 sec.
20.8% 16.9% 15.0%60 sec.
24.3% 17.1% 15.4%Aggregate 23.8% 17.5% 15.1%100In this first pass, speech from non-native speak-ers was transcribed more poorly (25.4% WER)than speech from native English speakers (21.7%WER).
In their comments sections, 17% of turkersnoted the difficulty in transcribing non-nativespeakers, while 13% found native English speechdifficult.
More than 80% of turkers thought theamount of work ?about right?
for the payment re-ceived.3.2 Second-Pass Transcription ResultsThe corrective process greatly improved agreementwith our expert transcriptions.
Aggregate WERwas reduced from 23.8% to 17.5% (27% relativereduction) when turkers corrected initial transcriptswith highlighted disagreements (third column ofTable 2).
In fact, transcriptions after correctionswere significantly more accurate than initial tran-scriptions (F(1, 238) = 13.4, p < 0.05).
With re-spect to duration, the WER of the 5-second utter-ances had the greatest improvement, a relative re-duction of WER by 37%.
Transcription alignmentwith the gold standard experienced a 39% im-provement to 13.3% for native English speech, anda 19% improvement to 20.6% for non-native Eng-lish speech (columns 2 and 3 of Table 3).We found that 30% of turkers indicated that thesecond-pass correction task was difficult, as com-pared with 15% for the first-pass transcription task.Work amount was perceived to be about right(85% of the votes) in this phase, similar to the first.3.3 Combining Corrected TranscriptionsIn order to improve the transcriptions further, wecombined the three second-pass transcriptions ofeach utterance using ROVER?s word-level votingscheme (Fiscus, 1997).
The WER of the resultingtranscripts are presented in the fourth column ofTable 2.
Aggregate WER was further reduced by14% relative to 15.1%.
This result is close to typ-ical disagreement rates of 6-12% reported in theliterature (Roy and Roy, 2009).
The best im-provements using ROVER were found with thetranscriptions of the shorter utterances: WERfrom the second-pass of 5-second utterances tran-scriptions was reduced by 23% to 15.3%.
The 10-second utterance transcriptions experienced thebest improvement, 32%, to a WER of 13.8%.Although segmenting audio into shorter seg-ments may yield fast turnaround times, we foundthat utterance length is not a significant factor indetermining alignment between combined, cor-rected transcriptions and gold-standard transcrip-tions (F(3, 44) = 0.16, p = 0.92).
We speculate thatlonger utterances show good accuracy due to theincreased context available to transcribers.Table 3.
WER across transcription iterations based onspeaker background.3.4 Error AnalysisOut of 3,281 words (48 merged transcriptions of48 utterances), 496 were errors.
Among the errorswere 37 insertions, 315 deletions, and 144 substitu-tions.
Thus the most common error was to miss aword.Further analysis revealed that two common cas-es of errors occurred: the misplacement or exclu-sion of filler words (even though the second phaseexplicitly instructed turkers to insert filler words)and failure to transcribe words considered to be outof the range of the transcriber?s vocabulary, suchas technical terms and foreign names.
Filler wordsaccounted for 112 errors (23%).
Removing fillersfrom both the combined transcripts and the goldstandard improved WER by 14% relative to13.0%.
Further, WER for native English speechtranscriptions was reduced to 8.9%.
This differencewas however not statistically significant (F(1,94) =1.64, p = 0.2).Turkers had difficulty transcribing uncommonwords, technical terms, names, acronyms, etc.
(e.g., ?Speechalyzer?, ?CTM?, ?PQs?).
Investiga-tion showed that at least 41 errors (8%) could beattributed to this out-of-vocabulary problem.
It isunclear if there is any way to completely eradicatesuch errors, short of asking the original speakers.3.5 Comparison to One-Pass ApproachAlthough the corrective model provides significantgain from individual transcriptions, this approachis logistically more complex.
We compared it toour one-pass approach, in which five turkers inde-pendently transcribe all utterances (Marge et al,2010).
Five new transcribers per utterance wererecruited for this task (yielding 240 transcriptions).SpeakerBackgroundFirst-PassWERSecond-Pass WERROVER-3WERNative  21.7% 13.3% 10.8%Non-native 25.4% 20.6% 18.4%101Individual error rate was 24.0%, comparable to theoverall error rate for the first step of the correctiveapproach (Table 2).After combining all five transcriptions withROVER, we found similar gains to the correctiveapproach: an overall improvement to 15.2% errorrate.
Thus both approaches can effectively producehigh-quality transcriptions.
We speculate that ifhigher accuracy is required, the corrective processcould be extended to iteratively re-focus effort onthe regions of greatest disagreement.3.6 LatencyAlthough payment scaled with the duration of ut-terances, we observed a consistent disparity in tur-naround time.
All HITs were posted at the sametime in both iterations (Thursday afternoon, EST).Turkers were able to transcribe 48 utterances twicein about a day in the first pass for the shorter utter-ances (5- and 10-second utterances), while it tooknearly a week to transcribe the 30- and 60-secondutterances.
Turkers were likely discouraged by thelong duration of the transcriptions compoundedwith the nature of the speech.
To increase turna-round time on lengthy utterances, we speculate thatit may be necessary to scale payment non-linearlywith length (or another measure of perceived ef-fort).3.7 ConclusionSpontaneous speech, even in long segments, canindeed be transcribed on MTurk with a level ofaccuracy that approaches expert agreement ratesfor spontaneous speech.
However, we expect seg-mentation of audio materials into smaller segmentswould yield fast turnaround time, and may keepcosts low.
In addition, we find that ROVER worksmore effectively on shorter segments becauselengths of candidate transcriptions are less likely tohave large disparities.
Thus, multiple transcriptionsper utterance can be utilized best when theirlengths are shorter.4 Annotating for Summarization4.1 MotivationTranscribing audio data into text is the first steptowards making information contained in audioeasily accessible to humans.
A next step is to con-dense the information in the raw transcription, andproduce a short summary that includes the mostimportant information.
Good summaries can pro-vide readers with a general sense of the meeting, orhelp them to drill down into the raw transcript (orthe audio itself) for additional information.4.2 Annotation ChallengesUnfortunately, summary creation is a difficult taskbecause ?importance?
is inherently subjective andvaries from consumer to consumer.
For example,the manager of a project, browsing a summary of ameeting, might be interested in all agenda items,whereas a project participant may be interested inonly those parts of the meeting that pertain to hisportion of the project.Despite this subjectivity, the usefulness of asummary is clear, and audio summarization is anactive area of research.
Within this field, two kindsof human annotations are generally created?annotators are either asked to write a short sum-mary of the audio, or they are asked to label eachtranscribed utterance as either ?in summary?
or?out of summary?.
The latter annotation is particu-larly useful for training and evaluating extractivesummarization systems?systems that create sum-maries by selecting a subset of the utterances.Due to the subjectivity involved, we find verylow inter-annotator agreement for this labelingtask.
Liu and Liu (2008) reported Kappa agreementscores of between 0.11 and 0.35 across 6 annota-tors, Penn and Zhu (2008) reported 0.38 on tele-phone conversation and 0.37 on lecture speech,using 3 annotators, and Galley (2006) reported0.32 on meeting data.
Such low levels of agree-ment imply that the resulting training data is likelyto contain a great deal of ?noise?
?utterances la-beled ?in summary?
or ?out of summary?, when infact they are not good examples of those classes.Disagreements arise due to the fact that utter-ance importance is a spectrum.
While some utter-ances are clearly important or unimportant, thereare many utterances that lie between these ex-tremes.
In order to label utterances as either ?in-summary?
or not, annotators must choose an arbi-trary threshold at which to make this decision.Simply asking annotators to provide a continuous?importance value?
between 0 and 1 is also likelyto be infeasible as the exact value for a given utter-ance is difficult to ascertain.1024.3 3-Class FormulationOne way to alleviate this problem is to redefine thetask as a 3-class labeling problem.
Annotators canbe asked to label utterances as either ?important?,?unimportant?
or ?in-between?.
Although this for-mulation creates two decision boundaries, insteadof the single one in the 2-class formulation, theexpectation is that a large number of utteranceswith middling importance will simply be assignedto the ?in between?
class, thus reducing the amountof noise in the data.
Indeed we have shown (Baner-jee and Rudnicky, 2009) that in-house annotatorsachieve high inter-annotator agreement when pro-vided with the 3-class formulation.Another way to alleviate the problem of lowagreement is to obtain annotations from many an-notators, and identify the utterances that a majorityof the annotators appear to agree on; such utter-ances may be considered as good examples of theirclass.
Using multiple annotators is typically notfeasible due to cost.
In this paper we investigateusing MTurk to create 3-class-based summariza-tion annotations from multiple annotators permeeting, and to combine and filter these annota-tions to create high quality labels.5 Using Mechanical Turk for Annotations5.1 Challenges of Using Mechanical TurkUnlike some other tasks that require little or nocontext in order to perform the annotation, summa-rization annotation requires a great deal of context.It is unlikely that an annotator can determine theimportance of an utterance without being aware ofneighboring utterances.
Moreover, the appropriatelength of context for a given utterance is likely tovary.
Presenting all contiguous utterances that dis-cuss the same topic might be appropriate, butwould require manual segmentation of the meetinginto topics.
In this paper we experiment with show-ing all utterances of a meeting.
This is a challengehowever, because MTurk is typically applied toquick low-cost tasks that need little context.
It isunclear whether turkers would be willing to per-form such a time-consuming task, even for higherpayment.Another challenge for turkers is being able tounderstand the discussion well enough to performthe annotation.
We experiment here with meetingsthat include significant technical content.
While in-house annotators can be trained over time to under-stand the material well enough to perform the task,it is impractical to provide turkers with such train-ing.
We investigate the degree to which turkers canprovide summarization annotation with minimaltraining.5.2 Data UsedWe selected 5 recorded meetings for our study.These meetings were not scripted?and wouldhave taken place even if they weren?t being rec-orded.
They were project meetings containing dis-cussions about software deliverables, problems,resolution plans, etc.
The contents included tech-nical jargon and concepts that non-experts are un-likely to grasp by reading the meeting transcriptalone.The 5 meetings had 2 to 4 participants each(mean: 3.5).
For all meetings, the speech from eachparticipant was recorded separately using head-mounted close-talking microphones.
We manuallysplit these audio streams into utterances?ensuringthat utterances did not have more than a 0.5 secondpause in them, and then transcribed them using anestablished process (Bennett and Rudnicky, 2002).The meetings varied widely in length from 15 mi-nutes and 282 utterances to 40 minutes and 948utterances (means: 30 minutes, 610 utterances).There were 3,052 utterances across the 5 meetings,each containing an mean of 7 words.
The utter-ances in the meetings were annotated using the 3-class formulation by two in-house annotators.Their inter-annotator agreement is presented alongwith the rest of the evaluation results in Section 6.0102030405060Important Neutral Unimportant%of UtterancesIn-house MturkFigure 1.
Label distribution of in-house and MTurkannotators.1035.3 HIT Design and InstructionsWe instructed turkers to imagine that someone else(not them) was going to eventually write a reportabout the meeting, and it was their task to identifythose utterances that should be included in the re-port.
We asked annotators to label utterances as?important?
if they should be included in the reportand ?unimportant?
otherwise.
In addition, utter-ances that they thought were of medium impor-tance and that may or may not need to be includedin the report were to be labeled as ?neutral?.
Weprovided examples of utterances in each of theseclasses.
For the ?important?
class, for instance, weincluded ?talking about a problem?
and ?discuss-ing future plan of action?
as examples.
For the ?un-important?
class, we included ?off topic joking?,and for the ?neutral?
class ?minute details of analgorithm?
was an example.In addition to these instructions and examples,we gave turkers a general guideline to the effectthat in these meetings typically 1/4th of the utter-ances are ?important?, 1/4th ?neutral?
and the rest?unimportant?.
As we discuss in section 6, it isunclear whether most turkers followed this guide-line.Following these instructions, examples and tips,we provided the text of the utterances in the formof an HTML table.
Each row contained a singleutterance, prefixed with the name of the speaker.The row also contained three radio buttons for thethree classes into which the annotator was asked toclassify the utterance.
Although we did not ensurethat annotators annotated every utterance beforesubmitting their work, we observed that for 95% ofthe utterances every annotator did provide a judg-ment; we ignore the remaining 5% of the utter-ances in our evaluation below.5.4 Number of Turkers and PaymentFor each meeting, we used 5 turkers and paid eachone the same.
That is, we did not vary the paymentamount as an experimental variable.
We calculatedthe amount to pay for a meeting based on in thelength of that meeting.
Specifically, we multipliedthe number of utterances by 0.13 US cents to arriveat the payment.
This resulted in payments rangingfrom 35 cents to $1.25 per meeting (mean 79cents).
The effective hourly rate (based on howmuch time turkers took to actually finish each job)was $0.87.6 Annotation Results6.1 Label DistributionWe first examine the average distribution of labelsacross the 3 classes.
Figure 1 shows the distribu-tions (expressed as percentages of the number ofutterances) for in-house and MTurk annotators,averaged across the 5 meetings.
Observe that thedistribution for the in-house annotators is far moreskewed away from a uniform 33% assignment,whereas the label distribution of turkers is lessskewed.
The likely reason for this difference is that0.00.10.20.30.40.50.6In house vIn houseTurker vTurkerIn house vTurkerKappaAgreementFigure 3.
Agreement with in-house annotators whenturker annotations are merged through voting.0.00.20.40.60.81.00.000.050.100.150.200.250.300.350.40 Fraction of data with agreement criteriaKappaAgreementAgreement Fraction of dataFigure 2.
Average kappa agreement between in-houseannotators, turkers, and in-house annotators and turkers.104turkers have a poorer understanding of the meet-ings, and are more likely than in-house annotatorsto make arbitrary judgments about utterances.
Thispoor understanding perhaps also explains the largedifference in the percentage of utterances labeledas important?for many utterances that are difficultto understand, turkers probably play it safe bymarking it important.The error bars represent the standard deviations ofthese averages, and capture the difference in labeldistribution from meeting to meeting.
While differentmeetings are likely to inherently have different ratiosof the 3 classes, observe that the standard deviationsfor the in-house annotators are much lower than thosefor the turkers.
For example, the percentage of utter-ances labeled ?important?
by in-house annotatorsvaries from 9% to 22% across the 5 meetings, whe-reas it varies from 30% to 57% for turkers, a muchwider range.
These differences in standard deviationpersist for each meeting as well?that is, for any giv-en meeting, the label distribution of the turkers variesmuch more between each other than the distributionof the in-house annotators.6.2 Inter-Annotator AgreementFigure 2 shows the kappa values for pairs of anno-tators, averaged across the 5 meetings, while theerror bars represent the standard deviations.
Thekappa between the two in-house annotators.
(0.4)is well within the range of values reported in thesummarization literature (see section 4).
The kappavalues range from 0.24 to 0.50 across the 5 meet-ings.
The inter-annotator agreement between pairsof turkers, averaged across the 10 possible pairsper meeting (5 choose 2), and across the 5 meet-ings show that turkers tend to agree less betweeneach other than in-house annotators, although thiskappa (0.28) is still within the range of typicalagreement (this kappa has lower variance becausethe sample size is larger).
The kappa between in-house annotators and turkers1 (0.19) is on the low-er end of the scale but remains within the range ofagreement reported in the literature, suggestingthat Mechanical Turk may be a useful tool forsummarization.1For each meeting, we measure agreement between everypossible pair of annotators such that one of the annotators wasan in-house annotator, and the other a turker.
Here we presentthe average agreement across all such pairs, and across all themeetings.6.3 Agreement after VotingWe consider merging the annotations from mul-tiple turkers using a simple voting scheme as fol-lows.
For each utterance, if 3, 4 or 5 annotatorslabeled the utterance with the same class, we la-beled the utterance with that class.
For utterancesin which 2 annotators voted for one class, 2 foranother and 1 for the third, we randomly pickedfrom one of the classes in which 2 annotators votedthe same way.
We then computed agreement be-tween this ?voted turker?
and each of the two in-house annotators, and averaged across the 5 meet-ings.
Figure 3 shows these agreement values.
Theleft-most point on the ?Kappa Agreement?
curveshows the average agreement obtained using indi-vidual turkers (0.19) while the second point showsthe agreement with the ?voted turker?
(0.22).
Thisis only a marginal improvement, implying thatsimply voting and using all the data does not im-prove much over the average agreement of indi-vidual annotators.The agreement does improve when we consideronly those utterances that a clear majority of anno-tators agreed on.
The 3rd, 4th and 5th points on the?Agreement?
curve plot the average agreementwhen considering only those utterances that at least3, 4 and 5 turkers agreed on.
The ?Fraction of da-ta?
curve plots the fraction of the meeting utter-ances that fit these agreement criteria.
Forutterances that at least 3 turkers agreed on, thekappa agreement value with in-house annotators is0.25, and this represents 84% of the data.
For about50% of the data 4 of 5 turkers agreed, and theseutterances had a kappa of 0.32.
Finally utterancesfor which annotators were unanimous had a kappaof 0.37, but represented only 22% of the data.
It isparticularly encouraging to note that although theamount of data reduces as we focus on utterancesthat more and more turkers agree on, the utterancesso labeled are not dominated by any one class.
Forexample, among utterances that 4 or more turkersagree on, 48% belong to the important class, 48%to unimportant class, and the remaining 4% to theneutral class.
These results show that with voting,it is possible to select a subset of utterances thathave higher agreement rates, implying that they areannotated with higher confidence.
For future workwe will investigate whether a summarization sys-tem trained on only the highly agreed-upon dataoutperforms one trained on all the annotation data.1057 ConclusionsIn this study, we found that MTurk can be used tocreate accurate transcriptions of spontaneous meet-ing speech when using a two-stage correctiveprocess.
Our best technique yielded a disagreementrate of 15.1%, which is competitive with reporteddisagreement in the literature of 6-12%.
We foundthat both fillers and out-of-vocabulary wordsproved troublesome.
We also observed that thelength of the utterance being transcribed wasn?t asignificant factor in determining WER, but that thenative language of the speaker was indeed a signif-icant factor.We also experimented with using MTurk for thepurpose of labeling utterances for extractive sum-marization research.
We showed that despite thelack of training, turkers produce labels with betterthan random agreement with in-house annotators.Further, when combined using voting, and with thelow-agreement utterances filtered out, we can iden-tify a set of utterances that agree significantly bet-ter with in-house annotations.In summary, MTurk appears to be a viable re-source for producing transcription and annotationof meeting speech.
Producing high-quality outputs,however, may require the use of techniques such asensemble voting and iterative correction or refine-ment that leverage performance of the same taskby multiple workers.ReferencesS.
Banerjee and A. I. Rudnicky.
2007.
Segmentingmeetings into agenda items by extracting implicitsupervision from human note-taking.
InProceedings of IUI.S.
Banerjee and A. I. Rudnicky.
2009.
Detecting thenoteworthiness of utterances in human meetings.
InProceedings of SIGDial.C.
Bennett and A. I. Rudnicky.
2002.
The CarnegieMellon Communicator corpus.
In Proceedings ofICSLP.J.
G. Fiscus.
1997.
A post-processing system to yieldword error rates: Recognizer Output Voting ErrorReduction (ROVER).
In Proceedings of ASRUWorkshop.M.
Galley.
(2006).
A skip-chain conditional ran-dom field for ranking meeting utterances by importance.
In Proceedings of EMNLP.A.
Gruenstein, I. McGraw, and A. Sutherland.
2009.
Aself-transcribing speech corpus: collectingcontinuous speech with an online educational game.In Proceedings of SLaTE Workshop.F.
Liu and Y. Liu.
2008.
Correlation betweenROUGE and human evaluation of extractivemeeting summaries.
In Proceedings of ACL-HLT.M.
Marge, S. Banerjee, and A. I. Rudnicky.
2010.Using the Amazon Mechanical Turk fortranscription of spoken language.
In Proceedingsof ICASSP.S.
Novotney and C. Callison-Burch.
2010.
Cheap, fastand good enough: Automatic speech recognitionwith non-expert transcription.
In Proceedings ofNAACL.G.
Penn and X. Zhu.
2008.
A critical reassessment ofevaluation baselines for speech  summarization.
InProceedings of ACL-HLT.B.
Roy and D. Roy.
2009.
Fast transcription of un-structured audio recordings.
In Proceedings of Interspeech.106AppendixTranscription task HIT type 1:Transcription task HIT type 2:Annotation task HIT:107
