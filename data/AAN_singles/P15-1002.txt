Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 11?19,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAddressing the Rare Word Problem inNeural Machine TranslationMinh-Thang Luong?
?Stanfordlmthang@stanford.eduIlya Sutskever?GoogleQuoc V. Le?Google{ilyasu,qvl,vinyals}@google.comOriol VinyalsGoogleWojciech Zaremba?New York Universitywoj.zaremba@gmail.comAbstractNeural Machine Translation (NMT) is anew approach to machine translation thathas shown promising results that are com-parable to traditional approaches.
A sig-nificant weakness in conventional NMTsystems is their inability to correctly trans-late very rare words: end-to-end NMTstend to have relatively small vocabularieswith a single unk symbol that representsevery possible out-of-vocabulary (OOV)word.
In this paper, we propose and im-plement an effective technique to addressthis problem.
We train an NMT systemon data that is augmented by the outputof a word alignment algorithm, allowingthe NMT system to emit, for each OOVword in the target sentence, the position ofits corresponding word in the source sen-tence.
This information is later utilized ina post-processing step that translates everyOOV word using a dictionary.
Our exper-iments on the WMT?14 English to Frenchtranslation task show that this method pro-vides a substantial improvement of up to2.8 BLEU points over an equivalent NMTsystem that does not use this technique.With 37.5 BLEU points, our NMT sys-tem is the first to surpass the best resultachieved on a WMT?14 contest task.1 IntroductionNeural Machine Translation (NMT) is a novel ap-proach to MT that has achieved promising results(Kalchbrenner and Blunsom, 2013; Sutskever etal., 2014; Cho et al, 2014; Bahdanau et al, 2015;Jean et al, 2015).
An NMT system is a conceptu-ally simple large neural network that reads the en-?Work done while the authors were in Google.
?
indicatesequal contribution.tire source sentence and produces an output trans-lation one word at a time.
NMT systems are ap-pealing because they use minimal domain knowl-edge which makes them well-suited to any prob-lem that can be formulated as mapping an inputsequence to an output sequence (Sutskever et al,2014).
In addition, the natural ability of neuralnetworks to generalize implies that NMT systemswill also generalize to novel word phrases and sen-tences that do not occur in the training set.
In addi-tion, NMT systems potentially remove the need tostore explicit phrase tables and language modelswhich are used in conventional systems.
Finally,the decoder of an NMT system is easy to imple-ment, unlike the highly intricate decoders used byphrase-based systems (Koehn et al, 2003).Despite these advantages, conventional NMTsystems are incapable of translating rare words be-cause they have a fixed modest-sized vocabulary1which forces them to use the unk symbol to repre-sent the large number of out-of-vocabulary (OOV)words, as illustrated in Figure 1.
Unsurpris-ingly, both Sutskever et al (2014) and Bahdanauet al (2015) have observed that sentences withmany rare words tend to be translated much morepoorly than sentences containing mainly frequentwords.
Standard phrase-based systems (Koehn etal., 2007; Chiang, 2007; Cer et al, 2010; Dyer etal., 2010), on the other hand, do not suffer from therare word problem to the same extent because theycan support a much larger vocabulary, and becausetheir use of explicit alignments and phrase tablesallows them to memorize the translations of evenextremely rare words.Motivated by the strengths of standard phrase-1Due to the computationally intensive nature of the soft-max, NMT systems often limit their vocabularies to be thetop 30K-80K most frequent words in each language.
How-ever, Jean et al (2015) has very recently proposed an efficientapproximation to the softmax that allows for training NTMswith very large vocabularies.
As discussed in Section 2, thistechnique is complementary to ours.11en: The ecotax portico in Pont-de-Buis , .
.
.
[truncated] .
.
.
, was taken down on Thursday morningfr: Le portique?ecotaxe de Pont-de-Buis , .
.
.
[truncated] .
.
.
, a e?te?
d?emont?e jeudi matinnn: Le unk de unk a` unk , .
.
.
[truncated] .
.
.
, a e?te?
pris le jeudi matin????????????????????
?Figure 1: Example of the rare word problem ?
An English source sentence (en), a human translation toFrench (fr), and a translation produced by one of our neural network systems (nn) before handling OOVwords.
We highlight words that are unknown to our model.
The token unk indicates an OOV word.
Wealso show a few important alignments between the pair of sentences.based system, we propose and implement a novelapproach to address the rare word problem ofNMTs.
Our approach annotates the training cor-pus with explicit alignment information that en-ables the NMT system to emit, for each OOVword, a ?pointer?
to its corresponding word in thesource sentence.
This information is later utilizedin a post-processing step that translates the OOVwords using a dictionary or with the identity trans-lation, if no translation is found.Our experiments confirm that this approach iseffective.
On the English to French WMT?14translation task, this approach provides an im-provement of up to 2.8 (if the vocabulary is rel-atively small) BLEU points over an equivalentNMT system that does not use this technique.Moreover, our system is the first NMT that out-performs the winner of a WMT?14 task.2 Neural Machine TranslationA neural machine translation system is any neuralnetwork that maps a source sentence, s1, .
.
.
, sn,to a target sentence, t1, .
.
.
, tm, where all sen-tences are assumed to terminate with a special?end-of-sentence?
token <eos>.
More con-cretely, an NMT system uses a neural network toparameterize the conditional distributionsp(tj|t<j, s?n) (1)for 1 ?
j ?
m. By doing so, it becomes pos-sible to compute and therefore maximize the logprobability of the target sentence given the sourcesentencelog p(t|s) =m?j=1log p (tj|t<j, s?n) (2)There are many ways to parameterize these con-ditional distributions.
For example, Kalchbrennerand Blunsom (2013) used a combination of a con-volutional neural network and a recurrent neuralnetwork, Sutskever et al (2014) used a deep LongShort-Term Memory (LSTM) model, Cho et al(2014) used an architecture similar to the LSTM,and Bahdanau et al (2015) used a more elabo-rate neural network architecture that uses an atten-tional mechanism over the input sequence, similarto Graves (2013) and Graves et al (2014).In this work, we use the model of Sutskever etal.
(2014), which uses a deep LSTM to encode theinput sequence and a separate deep LSTM to out-put the translation.
The encoder reads the sourcesentence, one word at a time, and produces a largevector that represents the entire source sentence.The decoder is initialized with this vector and gen-erates a translation, one word at a time, until itemits the end-of-sentence symbol <eos>.None the early work in neural machine transla-tion systems has addressed the rare word problem,but the recent work of Jean et al (2015) has tack-led it with an efficient approximation to the soft-max to accommodate for a very large vocabulary(500K words).
However, even with a large vocab-ulary, the problem with rare words, e.g., names,numbers, etc., still persists, and Jean et al (2015)found that using techniques similar to ours arebeneficial and complementary to their approach.3 Rare Word ModelsDespite the relatively large amount of work doneon pure neural machine translation systems, therehas been no work addressing the OOV problem inNMT systems, with the notable exception of Jeanet al (2015)?s work mentioned earlier.We propose to address the rare word problemby training the NMT system to track the originsof the unknown words in the target sentences.
Ifwe knew the source word responsible for each un-12en: The unk1portico in unk2.
.
.fr: Le unk?unk1de unk2.
.
.Figure 2: Copyable Model ?
an annotated exam-ple with two types of unknown tokens: ?copyable?unknand null unk?.known target word, we could introduce a post-processing step that would replace each unk inthe system?s output with a translation of its sourceword, using either a dictionary or the identitytranslation.
For example, in Figure 1, if themodel knows that the second unknown token inthe NMT (line nn) originates from the source wordecotax, it can perform a word dictionary lookupto replace that unknown token by?ecotaxe.
Sim-ilarly, an identity translation of the source wordPont-de-Buis can be applied to the third un-known token.We present three annotation strategies that caneasily be applied to any NMT system (Kalchbren-ner and Blunsom, 2013; Sutskever et al, 2014;Cho et al, 2014).
We treat the NMT system asa black box and train it on a corpus annotated byone of the models below.
First, the alignments areproduced with an unsupervised aligner.
Next, weuse the alignment links to construct a word dictio-nary that will be used for the word translations inthe post-processing step.2If a word does not ap-pear in our dictionary, then we apply the identitytranslation.The first few words of the sentence pair in Fig-ure 1 (lines en and fr) illustrate our models.3.1 Copyable ModelIn this approach, we introduce multiple tokensto represent the various unknown words in thesource and in the target language, as opposed tousing only one unk token.
We annotate the OOVwords in the source sentence with unk1, unk2,unk3, in that order, while assigning repeating un-known words identical tokens.
The annotationof the unknown words in the target language isslightly more elaborate: (a) each unknown targetword that is aligned to an unknown source wordis assigned the same unknown token (hence, the2When a source word has multiple translations, we usethe translation with the highest probability.
These translationprobabilities are estimated from the unsupervised alignmentlinks.
When constructing the dictionary from these alignmentlinks, we add a word pair to the dictionary only if its align-ment count exceeds 100.en: The unk portico in unk .
.
.fr: Le p0unk p?1unk p1de p?unk p?1.
.
.Figure 3: Positional All Model ?
an example ofthe PosAll model.
Each word is followed by therelative positional tokens pdor the null token p?.?copy?
model) and (b) an unknown target wordthat has no alignment or that is aligned with aknown word uses the special null token unk?.
SeeFigure 2 for an example.
This annotation enablesus to translate every non-null unknown token.3.2 Positional All Model (PosAll)The copyable model is limited by its inability totranslate unknown target words that are alignedto known words in the source sentence, such asthe pair of words, ?portico?
and ?portique?, in ourrunning example.
The former word is known onthe source sentence; whereas latter is not, so itis labelled with unk?.
This happens often sincethe source vocabularies of our models tend to bemuch larger than the target vocabulary since alarge source vocabulary is cheap.
This limita-tion motivated us to develop an annotation modelthat includes the complete alignments between thesource and the target sentences, which is straight-forward to obtain since the complete alignmentsare available at training time.Specifically, we return to using only a singleuniversal unk token.
However, on the targetside, we insert a positional token pdafter ev-ery word.
Here, d indicates a relative position(d = ?7, .
.
.
,?1, 0, 1, .
.
.
, 7) to denote that a tar-get word at position j is aligned to a source wordat position i = j ?
d. Aligned words that are toofar apart are considered unaligned, and unalignedwords rae annotated with a null token pn.
Our an-notation is illustrated in Figure 3.3.3 Positional Unknown Model (PosUnk)The main weakness of the PosAll model is thatit doubles the length of the target sentence.
Thismakes learning more difficult and slows the speedof parameter updates by a factor of two.
How-ever, given that our post-processing step is con-cerned only with the alignments of the unknownwords, so it is more sensible to only annotate theunknown words.
This motivates our positional un-known model which uses unkposdtokens (for din ?7, .
.
.
, 7 or ?)
to simultaneously denote (a)13the fact that a word is unknown and (b) its rela-tive position d with respect to its aligned sourceword.
Like the PosAll model, we use the symbolunkpos?for unknown target words that do nothave an alignment.
We use the universal unk forall unknown tokens in the source language.
SeeFigure 4 for an annotated example.en: The unk portico in unk .
.
.fr: Le unkpos1unkpos?1de unkpos1.
.
.Figure 4: Positional UnknownModel ?
an exam-ple of the PosUnk model: only aligned unknownwords are annotated with the unkposdtokens.It is possible that despite its slower speed, thePosAll model will learn better alignments becauseit is trained on many more examples of words andtheir alignments.
However, we show that this isnot the case (see ?5.2).4 ExperimentsWe evaluate the effectiveness of our OOV mod-els on the WMT?14 English-to-French translationtask.
Translation quality is measured with theBLEU metric (Papineni et al, 2002) on the new-stest2014 test set (which has 3003 sentences).4.1 Training DataTo be comparable with the results reported by pre-vious work on neural machine translation systems(Sutskever et al, 2014; Cho et al, 2014; Bahdanauet al, 2015), we train our models on the sametraining data of 12M parallel sentences (348MFrench and 304M English words), obtained from(Schwenk, 2014).
The 12M subset was selectedfrom the full WMT?14 parallel corpora using themethod proposed in Axelrod et al (2011).Due to the computationally intensive nature ofthe naive softmax, we limit the French vocabulary(the target language) to the either the 40K or the80K most frequent French words.
On the sourceside, we can afford a much larger vocabulary, sowe use the 200K most frequent English words.The model treats all other words as unknowns.3We annotate our training data using the threeschemes described in the previous section.
Thealignment is computed with the Berkeley aligner(Liang et al, 2006) using its default settings.
We3When the French vocabulary has 40K words, there areon average 1.33 unknown words per sentence on the targetside of the test set.discard sentence pairs in which the source or thetarget sentence exceed 100 tokens.4.2 Training DetailsOur training procedure and hyperparameterchoices are similar to those used by Sutskever etal.
(2014).
In more details, we train multi-layerdeep LSTMs, each of which has 1000 cells, with1000 dimensional embeddings.
Like Sutskever etal.
(2014), we reverse the words in the source sen-tences which has been shown to improve LSTMmemory utilization and results in better transla-tions of long sentences.
Our hyperparameters canbe summarized as follows: (a) the parameters areinitialized uniformly in [-0.08, 0.08] for 4-layermodels and [-0.06, 0.06] for 6-layer models, (b)SGD has a fixed learning rate of 0.7, (c) we trainfor 8 epochs (after 5 epochs, we begin to halvethe learning rate every 0.5 epoch), (d) the sizeof the mini-batch is 128, and (e) we rescale thenormalized gradient to ensure that its norm doesnot exceed 5 (Pascanu et al, 2012).We also follow the GPU parallelization schemeproposed in (Sutskever et al, 2014), allowing usto reach a training speed of 5.4K words per sec-ond to train a depth-6 model with 200K sourceand 80K target vocabularies ; whereas Sutskeveret al (2014) achieved 6.3K words per second fora depth-4 models with 80K source and target vo-cabularies.
Training takes about 10-14 days on an8-GPU machine.4.3 A note on BLEU scoresWe report BLEU scores based on both: (a) detok-enized translations, i.e., WMT?14 style, to be com-parable with results reported on the WMT web-site4and (b) tokenized translations, so as to beconsistent with previous work (Cho et al, 2014;Bahdanau et al, 2015; Schwenk, 2014; Sutskeveret al, 2014; Jean et al, 2015).5The existing WMT?14 state-of-the-art system(Durrani et al, 2014) achieves a detokenizedBLEU score of 35.8 on the newstest2014 test setfor English to French language pair (see Table 2).In terms of the tokenized BLEU, its performanceis 37.0 points (see Table 1).4http://matrix.statmt.org/matrix5The tokenizer.perl and multi-bleu.plscripts are used to tokenize and score translations.14System Vocab Corpus BLEUState of the art in WMT?14 (Durrani et al, 2014) All 36M 37.0Standard MT + neural componentsSchwenk (2014) ?
neural language model All 12M 33.3Cho et al (2014)?
phrase table neural features All 12M 34.5Sutskever et al (2014) ?
5 LSTMs, reranking 1000-best lists All 12M 36.5Existing end-to-end NMT systemsBahdanau et al (2015) ?
single gated RNN with search 30K 12M 28.5Sutskever et al (2014) ?
5 LSTMs 80K 12M 34.8Jean et al (2015) ?
8 gated RNNs with search + UNK replacement 500K 12M 37.2Our end-to-end NMT systemsSingle LSTM with 4 layers 40K 12M 29.5Single LSTM with 4 layers + PosUnk 40K 12M 31.8 (+2.3)Single LSTM with 6 layers 40K 12M 30.4Single LSTM with 6 layers + PosUnk 40K 12M 32.7 (+2.3)Ensemble of 8 LSTMs 40K 12M 34.1Ensemble of 8 LSTMs + PosUnk 40K 12M 36.9 (+2.8)Single LSTM with 6 layers 80K 36M 31.5Single LSTM with 6 layers + PosUnk 80K 36M 33.1 (+1.6)Ensemble of 8 LSTMs 80K 36M 35.6Ensemble of 8 LSTMs + PosUnk 80K 36M 37.5 (+1.9)Table 1: Tokenized BLEU on newstest2014 ?
Translation results of various systems which differ interms of: (a) the architecture, (b) the size of the vocabulary used, and (c) the training corpus, eitherusing the full WMT?14 corpus of 36M sentence pairs or a subset of it with 12M pairs.
We highlightthe performance of our best system in bolded text and state the improvements obtained by our techniqueof handling rare words (namely, the PosUnk model).
Notice that, for a given vocabulary size, the moreaccurate systems achieve a greater improvement from the post-processing step.
This is the case becausethe more accurate models are able to pin-point the origin of an unknown word with greater accuracy,making the post-processing more useful.System BLEUExisting SOTA (Durrani et al, 2014) 35.8Ensemble of 8 LSTMs + PosUnk 36.6Table 2: Detokenized BLEU on newstest2014 ?translation results of the existing state-of-the-artsystem and our best system.4.4 Main ResultsWe compare our systems to others, including thecurrent state-of-the-art MT system (Durrani etal., 2014), recent end-to-end neural systems, aswell as phrase-based baselines with neural com-ponents.The results shown in Table 1 demonstrate thatour unknown word translation technique (in par-ticular, the PosUnk model) significantly improvesthe translation quality for both the individual (non-ensemble) LSTM models and the ensemble mod-els.6For 40K-word vocabularies, the performancegains are in the range of 2.3-2.8 BLEU points.With larger vocabularies (80K), the performancegains are diminished, but our technique can stillprovide a nontrivial gains of 1.6-1.9 BLEU points.It is interesting to observe that our approach ismore useful for ensemble models as compared tothe individual ones.
This is because the useful-ness of the PosUnk model directly depends on theability of the NMT to correctly locate, for a givenOOV target word, its corresponding word in thesource sentence.
An ensemble of large modelsidentifies these source words with greater accu-racy.
This is why for the same vocabulary size,better models obtain a greater performance gain6For the 40K-vocabulary ensemble, we combine 5 mod-els with 4 layers and 3 models with 6 layers.
For the 80K-vocabulary ensemble, we combine 3 models with 4 layers and5 models with 6 layers.
Two of the depth-6 models are reg-ularized with dropout, similar to Zaremba et al (2015) withthe dropout probability set to 0.2.15our post-processing step.
e Except for the very re-cent work of Jean et al (2015) that employs a sim-ilar unknown treatment strategy7as ours, our bestresult of 37.5 BLEU outperforms all other NMTsystems by a arge margin, and more importanly,our system has established a new record on theWMT?14 English to French translation.5 AnalysisWe analyze and quantify the improvement ob-tained by our rare word translation approach andprovide a detailed comparison of the differentrare word techniques proposed in Section 3.
Wealso examine the effect of depth on the LSTMarchitectures and demonstrate a strong correla-tion between perplexities and BLEU scores.
Wealso highlight a few translation examples whereour models succeed in correctly translating OOVwords, and present several failures.5.1 Rare Word AnalysisTo analyze the effect of rare words on translationquality, we follow Sutskever et al (Sutskever et al,2014) and sort sentences in newstest2014 by theaverage inverse frequency of their words.
We splitthe test sentences into groups where the sentenceswithin each group have a comparable number ofrare words and evaluate each group independently.We evaluate our systems before and after translat-ing the OOV words and compare with the stan-dard MT systems ?
we use the best system fromthe WMT?14 contest (Durrani et al, 2014), andneural MT systems ?
we use the ensemble systemsdescribed in (Sutskever et al, 2014) and Section 4.Rare word translation is challenging for neuralmachine translation systems as shown in Figure 5.Specifically, the translation quality of our modelbefore applying the postprocessing step is shownby the green curve, and the current best NMT sys-tem (Sutskever et al, 2014) is the purple curve.While (Sutskever et al, 2014) produces bettertranslations for sentences with frequent words (theleft part of the graph), they are worse than best7Their unknown replacement method and ours both trackthe locations of target unknown words and use a word dictio-nary to post-process the translation.
However, the mechanismused to achieve the ?tracking?
behavior is different.
Jean et al(2015)?s uses the attentional mechanism to track the originsof all target words, not just the unknown ones.
In contrast,we only focus on tracking unknown words using unsuper-vised alignments.
Our method can be easily applied to anysequence-to-sequence models since we treat any model as ablackbox and manipulate only at the input and output levels.0 500 1000 1500 2000 2500 30002830323436384042SentsBLEUSOTA Durrani et al (37.0)Sutskever et al (34.8)Ours (35.6)Ours + PosUnk (37.5)Figure 5: Rare word translation ?
On the x-axis,we order newstest2014 sentences by their aver-age frequency rank and divide the sentences intogroups of sentences with a comparable prevalenceof rare words.
We compute the BLEU score ofeach group independently.system (red curve) on sentences with many rarewords (the right side of the graph).
When applyingour unknown word translation technique (purplecurve), we significantly improve the translationquality of our NMT: for the last group of 500 sen-tences which have the greatest proportion of OOVwords in the test set, we increase the BLEU scoreof our system by 4.8 BLEU points.
Overall, ourrare word translation model interpolates betweenthe SOTA system and the system of Sutskever etal.
(2014), which allows us to outperform the win-ning entry of WMT?14 on sentences that consistpredominantly of frequent words and approach itsperformance on sentences with many OOV words.5.2 Rare Word ModelsWe examine the effect of the different rare wordmodels presented in Section 3, namely: (a) Copy-able ?
which aligns the unknown words on boththe input and the target side by learning to copy in-dices, (b) the Positional All (PosAll) ?
which pre-dicts the aligned source positions for every targetword, and (c) the Positional Unknown (PosUnk)?
which predicts the aligned source positions foronly the unknown target words.8It is also interest-8In this section and in section 5.3, all models are trainedon the unreversed sentences, and we use the following hyper-parameters: we initialize the parameters uniformly in [-0.1,0.1], the learning rate is 1, the maximal gradient norm is 1,with a source vocabulary of 90k words, and a target vocab-ulary of 40k (see Section 4.2 for more details).
While theseLSTMs do not achieve the best possible performance, it isstill useful to analyze them.16NoAlign (5.31) Copyable (5.38) PosAll (5.30, 1.37) PosUnk (5.32)20222426283032BLEU+0.8+1.0+2.4+2.2Figure 6: Rare word models ?
translation perfor-mance of 6-layer LSTMs: a model that uses noalignment (NoAlign) and the other rare word mod-els (Copyable, PosAll, PosUnk).
For each model,we show results before (left) and after (right) therare word translation as well as the perplexity (inparentheses).
For PosAll, we report the perplexi-ties of predicting the words and the positions.ing to measure the improvement obtained when noalignment information is used during training.
Assuch, we include a baseline model with no align-ment knowledge (NoAlign) in which we simply as-sume that the ithunknown word on the target sen-tence is aligned to the ithunknown word in thesource sentence.From the results in Figure 6, a simple mono-tone alignment assumption for the NoAlign modelyields a modest gain of 0.8 BLEU points.
If wetrain the model to predict the alignment, then theCopyable model offers a slightly better gain of 1.0BLEU.
Note, however, that English and Frenchhave similar word order structure, so it wouldbe interesting to experiment with other languagepairs, such as English and Chinese, in which theword order is not as monotonic.
These harder lan-guage pairs potentially imply a smaller gain for theNoAlign model and a larger gain for the Copyablemodel.
We leave it for future work.The positional models (PosAll and PosUnk) im-prove translation performance by more than 2BLEU points.
This proves that the limitation of thecopyable model, which forces it to align each un-known output word with an unknown input word,is considerable.
In contrast, the positional mod-els can align the unknown target words with anysource word, and as a result, post-processing has amuch stronger effect.
The PosUnk model achievesbetter translation results than the PosAll modelwhich suggests that it is easier to train the LSTMDepth 3 (6.01) Depth 4 (5.71) Depth 6 (5.46)20222426283032BLEU+1.9+2.0+2.2Figure 7: Effect of depths ?
BLEU scoresachieved by PosUnk models of various depths (3,4, and 6) before and after the rare word transla-tion.
Notice that the PosUnk model is more usefulon more accurate models.on shorter sequences.5.3 Other EffectsDeep LSTM architecture ?
We compare PosUnkmodels trained with different number of layers (3,4, and 6).
We observe that the gain obtained bythe PosUnk model increases in tandem with theoverall accuracy of the model, which is consistentwith the idea that larger models can point to the ap-propriate source word more accurately.
Addition-ally, we observe that on average, each extra LSTMlayer provides roughly 1.0 BLEU point improve-ment as demonstrated in Figure 7.5.6 5.8 6 6.2 6.4 6.6 6.82323.52424.52525.52626.5PerplexityBLEUFigure 8: Perplexity vs. BLEU ?
we show thecorrelation by evaluating an LSTM model with 4layers at various stages of training.Perplexity and BLEU ?
Lastly, we find it inter-esting to observe a strong correlation between theperplexity (our training objective) and the transla-tion quality as measured by BLEU.
Figure 8 showsthe performance of a 4-layer LSTM, in which wecompute both perplexity and BLEU scores at dif-ferent points during training.
We find that on aver-age, a reduction of 0.5 perplexity gives us roughly1.0 BLEU point improvement.17Sentencessrc An additional 2600 operations including orthopedic and cataract surgery willhelp clear a backlog .trans En outre , unkpos1ope?rations supple?mentaires , dont la chirurgie unkpos5et la unkpos6, permettront de re?sorber l?
arrie?re?
.+unk En outre , 2600 ope?rations supple?mentaires , dont la chirurgie orthop?ediqueset la cataracte , permettront de re?sorber l?
arrie?re?
.tgt 2600 ope?rations supple?mentaires , notamment dans le domaine de la chirurgieorthope?dique et de la cataracte , aideront a` rattraper le retard .src This trader , Richard Usher , left RBS in 2010 and is understand to have begiven leave from his current position as European head of forex spot trading atJPMorgan .trans Ce unkpos0, Richard unkpos0, a quitte?
unkpos1en 2010 et a compris qu?il est autorise?
a` quitter son poste actuel en tant que leader europe?en du marche?des points de vente au unkpos5.+unk Ce n?egociateur , Richard Usher , a quitte?
RBS en 2010 et a compris qu?
il estautorise?
a` quitter son poste actuel en tant que leader europe?en du marche?
despoints de vente au JPMorgan .tgt Ce trader , Richard Usher , a quitte?
RBS en 2010 et aurait e?te?
mis suspendude son poste de responsable europe?en du trading au comptant pour les deviseschez JPMorgansrc But concerns have grown after Mr Mazanga was quoted as saying Renamo wasabandoning the 1992 peace accord .trans Mais les inquie?tudes se sont accrues apre`s que M. unkpos3a de?clare?
que launkpos3unkpos3l?
accord de paix de 1992 .+unk Mais les inquie?tudes se sont accrues apre`s que M. Mazanga a de?clare?
que laRenamo?etait l?
accord de paix de 1992 .tgt Mais l?
inquie?tude a grandi apre`s que M. Mazanga a de?clare?
que la Renamoabandonnait l?
accord de paix de 1992 .Table 3: Sample translations ?
the table shows the source (src) and the translations of our best modelbefore (trans) and after (+unk) unknown word translations.
We also show the human translations (tgt)and italicize words that are involved in the unknown word translation process.5.4 Sample TranslationsWe present three sample translations of our bestsystem (with 37.5 BLEU) in Table 3.
In ourfirst example, the model translates all the un-known words correctly: 2600, orthop?ediques, andcataracte.
It is interesting to observe that themodel can accurately predict an alignment of dis-tances of 5 and 6 words.
The second examplehighlights the fact that our model can translatelong sentences reasonably well and that it was ableto correctly translate the unknown word for JP-Morgan at the very far end of the source sentence.Lastly, our examples also reveal several penaltiesincurred by our model: (a) incorrect entries in theword dictionary, as with n?egociateur vs. trader inthe second example, and (b) incorrect alignmentprediction, such as when unkpos3is incorrectlyaligned with the source word was and not withabandoning, which resulted in an incorrect trans-lation in the third sentence.6 ConclusionWe have shown that a simple alignment-basedtechnique can mitigate and even overcome oneof the main weaknesses of current NMT systems,which is their inability to translate words that arenot in their vocabulary.
A key advantage of ourtechnique is the fact that it is applicable to anyNMT system and not only to the deep LSTMmodel of Sutskever et al (2014).
A technique likeours is likely necessary if an NMT system is toachieve state-of-the-art performance on machinetranslation.We have demonstrated empirically that on the18WMT?14 English-French translation task, ourtechnique yields a consistent and substantial im-provement of up to 2.8 BLEU points over variousNMT systems of different architectures.
Most im-portantly, with 37.5 BLEU points, we have estab-lished the first NMT system that outperformed thebest MT system on a WMT?14 contest dataset.AcknowledgmentsWe thank members of the Google Brain teamfor thoughtful discussions and insights.
The firstauthor especially thanks Chris Manning and theStanford NLP group for helpful comments on theearly drafts of the paper.
Lastly, we thank the an-nonymous reviewers for their valuable feedback.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In EMNLP.D.
Bahdanau, K. Cho, and Y. Bengio.
2015.
Neuralmachine translation by jointly learning to align andtranslate.
In ICLR.D.
Cer, M. Galley, D. Jurafsky, and C. D. Manning.2010.
Phrasal: A statistical machine translationtoolkit for exploring new model features.
In ACL,Demonstration Session.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representationsusing rnn encoder-decoder for statistical machinetranslation.
In EMNLP.Nadir Durrani, Barry Haddow, Philipp Koehn, andKenneth Heafield.
2014.
Edinburgh?s phrase-basedmachine translation systems for WMT-14.
In WMT.Chris Dyer, Jonathan Weese, Hendra Setiawan, AdamLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-itkevitch, Phil Blunsom, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In ACL, Demonstration Session.A.
Graves, G. Wayne, and I. Danihelka.
2014.
Neuralturing machines.
arXiv preprint arXiv:1410.5401.A.
Graves.
2013.
Generating sequences withrecurrent neural networks.
In Arxiv preprintarXiv:1308.0850.Se?bastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2015.
On using very large tar-get vocabulary for neural machine translation.
InACL.N.
Kalchbrenner and P. Blunsom.
2013.
Recurrentcontinuous translation models.
In EMNLP.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,Demonstration Session.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In NAACL.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In ACL.R.
Pascanu, T. Mikolov, and Y. Bengio.
2012.
Onthe difficulty of training recurrent neural networks.arXiv preprint arXiv:1211.5063.H.
Schwenk.
2014.
University le mans.http://www-lium.univ-lemans.fr/?schwenk/cslm_joint_paper/.
[Online;accessed 03-September-2014].I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Sequenceto sequence learning with neural networks.
In NIPS.Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.2015.
Recurrent neural network regularization.
InICLR.19
