Proceedings of NAACL HLT 2009: Short Papers, pages 5?8,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsRevisiting Optimal Decoding for Machine Translation IBM Model 4Sebastian Riedel??
James Clarke?
?Department of Computer Science, University of Tokyo, Japan?Database Center for Life Science, Research Organization of Information and System, Japan?Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801?sebastian.riedel@gmail.com ?clarkeje@gmail.comAbstractThis paper revisits optimal decoding for statis-tical machine translation using IBM Model 4.We show that exact/optimal inference usingInteger Linear Programming is more practicalthan previously suggested when used in con-junction with the Cutting-Plane Algorithm.
Inour experiments we see that exact inferencecan provide a gain of up to one BLEU pointfor sentences of length up to 30 tokens.1 IntroductionStatistical machine translation (MT) systems typ-ically contain three essential components: (1) amodel, specifying how the process of translation oc-curs; (2) learning regime, dictating the estimation ofmodel?s parameters; (3) decoding algorithm whichprovides the most likely translation of an input sen-tence given a model and its parameters.The search space in statistical machine transla-tion is vast which can make it computationally pro-hibitively to perform exact/optimal decoding (alsoknown as search and MAP inference) especiallysince dynamic programming methods (such as theViterbi algorithm) are typically not applicable.
Thusgreedy or heuristic beam-based methods have beenprominent (Koehn et al, 2007) due to their effi-ciency.
However, the efficiency of such methodshave two drawbacks: (1) they are approximate andgive no bounds as to how far their solution isaway from the true optimum; (2) it can be difficultto incorporate additional generic global constraintsinto the search.
The first point may be especiallyproblematic from a research perspective as withoutbounds on the solutions it is difficult to determinewhether the model or the search algorithm requiresimprovement for better translations.Similar problems exist more widely throughoutnatural language processing where greedy basedmethods and heuristic beam search have been usedin lieu of exact methods.
However, recently there hasbeen an increasing interest in using Integer LinearProgramming (ILP) as a means to find MAP solu-tions.
ILP overcomes the two drawbacks mentionedabove as it is guaranteed to be exact, and has theability to easily enforce global constraints throughadditional linear constraints.
However, efficiency isusually sacrificed for these benefits.Integer Linear Programming has previously beenused to perform exact decoding for MT using IBMModel 4 and a bigram language model.
Germannet al (2004) view the translation process akin to thetravelling salesman problem; however, from their re-ported results it is clear that using ILP naively for de-coding does not scale up beyond short sentences (ofeight tokens).
This is due to the exponential num-ber of constraints required to represent the decod-ing problem as an ILP program.
However, work independency parsing (Riedel and Clarke, 2006) hasdemonstrated that it is possible to use ILP to performefficient inference for very large programs whenused in an incremental manner.
This raises the ques-tion as to whether incremental (or Cutting-Plane)ILP can also be used to decode IBM Model 4 onreal world sentences.In this work we show that it is possible.
Decod-ing IBM Model 4 (in combination with a bigramlanguage model) using Cutting-Plane ILP scales tomuch longer sentences.
This affords us the oppor-tunity to finally analyse the performance of IBMModel 4 and the performance of its state-of-the-5art ReWrite decoder.
We show that using exact in-ference provides an increase of up to one BLEUpoint on two language pairs (French-English andGerman-English) in comparison to decoding usingthe ReWrite decoder.
Thus the ReWrite decoder per-forms respectably but can be improved slightly, al-beit at the cost of efficiency.Although the community has generally movedaway from word-based models, we believe that dis-playing optimal decoding in IBM Model 4 lays thefoundations of future work.
It is the first step in pro-viding a method for researchers to gain greater in-sight into their translation models by mapping thedecoding problem of other models into an ILP rep-resentation.
ILP decoding will also allow the incor-poration of global linguistic constraints in a mannersimilar to work in other areas of natural languageprocessing.The remainder of this paper is organised as fol-lows: Sections 2 and 3 briefly recap IBM Model 4and its ILP formulation.
Section 4 reviews theCutting-Plane Algorithm.
Section 5 outlines our ex-periments and we end the paper with conclusionsand a discussion of open questions for the commu-nity.2 IBM Model 4In this paper we focus on the translation model de-fined by IBM Model 4 (Brown et al, 1993).
Transla-tion using IBM Model 4 is performed by treating thetranslation process a noisy-channel model where theprobability of the English sentence given a Frenchsentence is, P (e|f) = P (f |e) ?
P (e), where P (e) isa language model of English.
IBM Model 4 definesP (f |e) and models the translation process as a gen-erative process of how a sequence of target words(in our case French or German) is generated from asequence of source words (English).The generative story is as follows.
Imagine wehave an English sentence, e = e1, .
.
.
, el and alongwith a NULL word (eo) and French sentence, f =f1, .
.
.
, fm.
First a fertility is drawn for each Englishword (including the NULL symbol).
Then, for eachei we then independently draws a number of Frenchwords equal to ei?s fertility.
Finally we process theEnglish source tokens in sequence to determine thepositions of their generated French target words.
Werefer the reader to Brown et al (1993) for full details.3 Integer Linear ProgrammingFormulationGiven a trained IBM Model 4 and a French sentencef we need to find the English sentence e and align-ment a with maximal p (a, e|f) w p (e) ?
p (a, f |e).1Germann et al (2004) present an ILP formula-tion of this problem.
In this section we will give avery high-level description of the formulation.2 Forbrevity we refer the reader to the original work formore details.In the formulation of Germann et al (2004) anEnglish translation is represented as the journey ofa travelling salesman that visits one English token(hotel) per French token (city).
Here the English to-ken serves as the translation of the French one.
Aset of binary variables denote whether or not cer-tain English token pairs are directly connected inthis journey.
A set of constraints guarantee that foreach French token exactly one English token is vis-ited.
The formulation also contains an exponentialnumber of constraints which forbid the possible cy-cles the variables can represent.
It is this set of con-straints that renders MT decoding with ILP difficult.4 Cutting Plane AlgorithmThe ILP program above has an exponential numberof (cycle) constraints.
Hence, simply passing the ILPto an off-the-shelf ILP solver is not practical for allbut the smallest sentences.
For this reason Germannet al (2004) only consider sentences of up to eighttokens.
However, recent work (Riedel and Clarke,2006) has shown that even exponentially large de-coding problems may be solved efficiently using ILPsolvers if a Cutting-Plane Algorithm (Dantzig et al,1954) is used.3A Cutting-Plane Algorithm starts with a subset ofthe complete set of constraints.
In our case this sub-set contains all but the (exponentially many) cycleconstraints.
The corresponding ILP is solved by a1Note that in theory we should be maximizing p (e|f).
How-ever, this requires summation over all possible alignments andhence the problem is usually simplified as described here.2Note that our actual formulation differs slightly from theoriginal work because we use a first order modelling languagethat imposed certain restrictions on the type of constraints al-lowed.3It is worth mentioning that Cutting Plane Algorithms havebeen successfully applied for solving very large instances of theTravelling Salesman Problem, a problem essentially equivalentto the decoding in IBM Model 4.6standard ILP solver, and the solution is inspectedfor cycles.
If it contains no cycles, we have foundthe true optimum: the solution with highest scorethat does not violate any constraints.
If the solutiondoes contain cycles, the corresponding constraintsare added to the ILP which is in turn solved again.This process is continued until no more cycles canbe found.5 EvaluationIn this section we describe our experimental setupand results.5.1 Experimental setupOur experimental setup is designed to answer sev-eral questions: (1) Is exact inference in IBM Model 4possible for sentences of moderate length?
(2) Howfast is exact inference using Cutting-Plane ILP?
(3) How well does the ReWrite Decoder4 performin terms of finding the optimal solution?
(4) Doesoptimal decoding produce better translations?In order to answer these questions we obtaina trained IBM Model 4 for French-English andGerman-English on Europarl v3 using GIZA++.
Abigram language model with Witten-Bell smooth-ing was estimated from the corpus using the CMU-Cambridge Language Modeling Toolkit.For exact decoding we use the two models to gen-erate ILP programs for sentences of length up to(and including) 30 tokens for French and 25 tokensfor German.5 We filter translation candidates follow-ing Germann et al (2004) by using only the top tentranslations for each word6 and a list of zero fertil-ity words.7 This resulted in 1101 French and 1062German sentences for testing purposes.
The ILP pro-grams were then solved using the method describedin Section 3.
This was repeated using the ReWriteDecoder using the same models.5.2 ResultsThe Cutting-Plane ILP decoder (which we will referto as ILP decoder) produced output for 986 Frenchsentences and 954 German sentences.
From this wecan conclude that it is possible to solve 90% of our4Available at http://www.isi.edu/licensed-sw/rewrite-decoder/5These limits were imposed to ensure the Python script gen-erating the ILP programs did not run out of memory.6Based on t(e|f).7Extracted using the rules in the filter scriptrewrite.mkZeroFert.perlsentences exactly using ILP.
For the remaining 115and 108 sentences we did not produce a solution dueto: (1) the solver not completing within 30 minutes,or (2) the solver running out of memory.8Table 1 shows a comparison of the results, bro-ken down by input sentence length, obtained on the986 French and 954 German sentences using the ILPand ReWrite decoders.
First we turn our attention tothe solve times obtained using ILP (for the sentencesfor which the solution was found within 30 min-utes).
The table shows that the average solve timeis under one minute per sentence.
As we increasethe sentence length we see the solve time increases,however, we never see an order of magnitude in-crease between brackets as witnessed by Germannet al (2004) thus optimal decoding is more practi-cal than previously suggested.
The average numberof Cutting-Plane iterations required was 4.0 and 5.6iterations for French and German respectively withlonger sentences requiring more on average.We next examine the performance of the two de-coders.
Following Germann et al (2004) we definethe ReWrite decoder as finding the optimal solutionif the English sentence is the same as that producedby the ILP decoder.
Table 1 shows that the ReWritedecoder finds the optimal solution 40.1% of the timefor French and 29.1% for German.
We also see theReWrite decoder is less likely to find the optimal so-lution of longer sentences.
We now look at the modelscores more closely.
The average log model errorper token shows that the ReWrite decoder?s error isproportional to sentence length and on average theReWrite decoder is 2.2% away from the optimal so-lution in log space and 60.6% in probability space9for French, and 4.7% and 60.9% for German.Performing exact decoding increases the BLEUscore by 0.97 points on the French-English data setand 0.61 points on the German-English data set withsimilar performance increases observed for all sen-tence lengths.6 Discussion and ConclusionsIn this paper we have demonstrated that optimal de-coding of IBM Model 4 is more practical than previ-ously suggested.
Our results and analysis show that8All experiments were run on 3.0GHz Intel Core 2 Duo with4GB RAM using a single core.9These high error rates are an artefact of the extremely smallprobabilities involved.7Len # Solve Stats BLEU%Eq Err Time ReW ILP Diff1?5 21 85.7 15.0 0.7 56.5 56.2 -0.326?10 121 64.5 7.8 1.4 26.1 28.0 1.9011?15 118 47.9 5.9 2.7 22.9 23.7 0.8516?20 238 37.4 6.3 13.9 20.4 20.8 0.4121?25 266 30.5 6.6 70.1 20.9 22.5 1.6226?30 152 25.7 5.3 162.6 20.9 22.3 1.381?30 986 40.1 6.5 48.1 21.7 22.6 0.97(a) French-EnglishLen # Solve Stats BLEU%Eq Err Time ReW ILP Diff1?5 31 83.9 27.4 0.8 40.7 41.1 0.446?10 175 51.4 19.7 1.7 19.2 20.9 1.7211?15 242 30.6 17.4 5.5 16.0 16.7 0.7216?20 257 19.1 14.4 23.9 15.8 15.9 0.1621?25 249 15.7 14.0 173.4 15.3 15.9 0.611?25 954 29.1 16.4 53.5 16.1 16.7 0.61(b) German-EnglishTable 1: Results on the two corpora.
Len: range of sentence lengths; #: number of sentences in this range; %Eq: percentage oftimes ILP decoder returned same English sentence; Err: average difference between decoder scores per token (?10?2) in log space;Time: the average solve time per sentence of ILP decoder in seconds; BLEU ReW, BLEU ILP, BLEU Diff: the BLEU scores of theoutput and difference between BLEU scores.exact decoding has a practical purpose.
It has al-lowed us to investigate and validate the performanceof the ReWrite decoder through comparison of theoutputs and model scores from the two decoders.Exact inference also provides an improvement intranslation quality as measured by BLEU score.During the course of this research we have en-countered numerous challenges that were not appar-ent at the start.
These challenges raise some interest-ing research questions and practical issues one mustconsider when embarking on exact inference usingILP.
The first issue is that the generation of the ILPprograms can take a long time.
This leads us to won-der if there may be a way to provide tighter integra-tion of program generation and solving.
Such an in-tegration would avoid the need to query the modelsin advance for all possible model components thesolver may require.Related to this issue is how to tackle the incor-poration of higher order language models.
Currentlywe use our bigram language model in a brute-forcemanner: in order to generate the ILP we evaluatethe probability of all possible bigrams of Englishcandidate tokens in advance.
It seems clear thatwith higher order models this process will becomeprohibitively expensive.
Moreover, even if the ILPcould be generated efficiently, they will obviously belarger and harder to solve than our current ILPs.
Onepossible solution may be the use of so-called de-layed column generation strategies which incremen-tally add parts of the objective function (and hencethe language model), but only when required by theILP solver.1010Note that delayed column generation is dual to performingcutting planes.The use of ILP in other NLP tasks has provideda principled and declarative manner to incorporateglobal linguistic constraints on the system output.This work lays the foundations for incorporatingsimilar global constraints for translation.
We are cur-rently investigating linguistic constraints for IBMModel 4 and other word-based models in general.
Afurther extension is to reformulate higher-level MTmodels (phrase- and syntax-based) within the ILPframework.
These representations could be more de-sirable from a linguistic constraint perspective as theformulation of constraints may be more intuitive.AcknowledgementsWe would like to thank Ulrich Germann and DanielMarcu for their help with the ISI ReWrite Decoder.ReferencesBrown, Peter F., Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematics of sta-tistical machine translation: parameter estimation.
Compu-tational Linguistics 19(2):263?311.Dantzig, George B., Ray Fulkerson, and Selmer M. Johnson.1954.
Solution of a large-scale traveling salesman problem.Operations Research 2:393?410.Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,and Kenji Yamada.
2004.
Fast and optimal decoding for ma-chine translation.
Artificial Intelligence 154(1-2):127?143.Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-drej Bojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machine transla-tion.
In ACL 2009 Demos.
Prague, Czech Republic, pages177?180.Riedel, Sebastian and James Clarke.
2006.
Incremental integerlinear programming for non-projective dependency parsing.In EMNLP 2006. pages 129?137.8
