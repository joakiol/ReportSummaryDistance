Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 387?392,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsWord Embeddings with Limited MemoryShaoshi Ling1and Yangqiu Song2and Dan Roth11Department of Computer Science, University of Illinois at Urbana-Champaign2Department of Computer Science and Engineering, HKUST1{sling3,danr}@illinois.edu,2yqsong@gmail.comAbstractThis paper studies the effect of limited pre-cision data representation and computa-tion on word embeddings.
We present asystematic evaluation of word embeddingswith limited memory and discuss method-s that directly train the limited precisionrepresentation with limited memory.
Ourresults show that it is possible to use andtrain an 8-bit fixed-point value for wordembedding without loss of performancein word/phrase similarity and dependencyparsing tasks.1 IntroductionThere is an accumulation of evidence that theuse of dense distributional lexical representations,known as word embeddings, often supports bet-ter performance on a range of NLP tasks (Ben-gio et al, 2003; Turian et al, 2010; Collobert etal., 2011; Mikolov et al, 2013a; Mikolov et al,2013b; Levy et al, 2015).
Consequently, wordembeddings have been commonly used in the lastfew years for lexical similarity tasks and as fea-tures in multiple, syntactic and semantic, NLP ap-plications.However, keeping embedding vectors for hun-dreds of thousands of words for repeated use couldtake its toll both on storing the word vectors ondisk and, even more so, on loading them intomemory.
For example, for 1 million words, load-ing 200 dimensional vectors takes up to 1.6 GBmemory on a 64-bit system.
Considering applica-tions that make use of billions of tokens and mul-tiple languages, size issues impose significant lim-itations on the practical use of word embeddings.This paper presents the question of whether it ispossible to significantly reduce the memory need-s for the use and training of word embeddings.Specifically, we ask ?what is the impact of repre-senting each dimension of a dense representationwith significantly fewer bits than the standard 64bits??
Moreover, we investigate the possibility ofdirectly training dense embedding vectors usingsignificantly fewer bits than typically used.The results we present are quite surprising.
Weshow that it is possible to reduce the memory con-sumption by an order of magnitude both whenword embeddings are being used and in training.In the first case, as we show, simply truncatingthe resulting representations after training and us-ing a smaller number of bits (as low as 4 bitsper dimension) results in comparable performanceto the use of 64 bits.
Moreover, we provide t-wo ways to train existing algorithms (Mikolovet al, 2013a; Mikolov et al, 2013b) when thememory is limited during training and show that,here, too, an order of magnitude saving in mem-ory is possible without degrading performance.We conduct comprehensive experiments on ex-isting word and phrase similarity and relatednessdatasets as well as on dependency parsing, to e-valuate these results.
Our experiments show that,in all cases and without loss in performance, 8bits can be used when the current standard is 64and, in some cases, only 4 bits per dimensionare sufficient, reducing the amount of space re-quired by a factor of 16.
The truncated wordembeddings are available from the papers webpage at https://cogcomp.cs.illinois.edu/page/publication_view/790.2 Related WorkIf we consider traditional cluster encoded wordrepresentation, e.g., Brown clusters (Brown et al,1992), it only uses a small number of bits to trackthe path on a hierarchical tree of word clustersto represent each word.
In fact, word embedding387generalized the idea of discrete clustering repre-sentation to continuous vector representation inlanguage models, with the goal of improving thecontinuous word analogy prediction and general-ization ability (Bengio et al, 2003; Mikolov et al,2013a; Mikolov et al, 2013b).
However, it hasbeen proven that Brown clusters as discrete fea-tures are even better than continuous word em-bedding as features for named entity recognitiontasks (Ratinov and Roth, 2009).
Guo et al (Guoet al, 2014) further tried to binarize embeddingsusing a threshold tuned for each dimension, andessentially used less than two bits to represen-t each dimension.
They have shown that bina-rization can be comparable to or even better thanthe original word embeddings when used as fea-tures for named entity recognition tasks.
More-over, Faruqui et al (Faruqui et al, 2015) showedthat imposing sparsity constraints over the em-bedding vectors can further improve the represen-tation interpretability and performance on sever-al word similarity and text classification bench-mark datasets.
These works indicate that, for sometasks, we do not need all the information encodedin ?standard?
word embeddings.
Nonetheless, it isclear that binarization loses a lot of information,and this calls for a systematic comparison of howmany bits are needed to maintain the expressivityneeded from word embeddings for different tasks.3 Value TruncationIn this section, we introduce approaches for wordembedding when the memory is limited.
We trun-cate any value x in the word embedding into an nbit representation.3.1 Post-processing RoundingWhen the word embedding vectors are given, themost intuitive and simple way is to round the num-bers to their n-bit precision.
Then we can use thetruncated values as features for any tasks that wordembedding can be used for.
For example, if wewant to round x to be in the range of [?r, r], asimple function can be applied as follows.Rd(x, n) ={bxc if bxc ?
x ?
bxc+2bxc+  if bxc+2< x ?
bxc+ (1)where  = 21?nr.
For example, if we want to use8 bits to represent any value in the vectors, then weonly have 256 numbers ranging from -128 to 127for each value.
In practice, we first scale all thevalues and then round them to the 256 numbers.3.2 Training with Limited MemoryWhen the memory for training word embeddingis also limited, we need to modify the trainingalgorithms by introducing new data structures toreduce the bits used to encode the values.
Inpractice, we found that in the stochastic gradien-t descent (SGD) iteration in word2vec algorithm-s (Mikolov et al, 2013a; Mikolov et al, 2013b),the updating vector?s values are often very smallnumbers (e.g., < 10?5).
In this case, if we direct-ly apply the rounding method to certain precisions(e.g., 8 bits), the update of word vectors will al-ways be zero.
For example, the 8-bit precision is2?7= 0.0078, so 10?5is not significant enoughto update the vector with 8-bit values.
Therefore,we consider the following two ways to improvethis.Stochastic Rounding.
We first consider us-ing stochastic rounding (Gupta et al, 2015) totrain word embedding.
Stochastic rounding intro-duces some randomness into the rounding mech-anism, which has been proven to be helpful whenthere are many parameters in the learning system,such as deep learning systems (Gupta et al, 2015).Here we also introduce this approach to updateword embedding vectors in SGD.
The probabilityof rounding x to bxc is proportional to the prox-imity of x to bxc:Rs(x, n) ={bxc w.p.
1?x?bxcbxc+  w.p.x?bxc.
(2)In this case, even though the update values are notsignificant enough to update the word embeddingvectors, we randomly choose some of the valuesbeing updated proportional to the value of howclose the update value is to the rounding precision.Auxiliary Update Vectors.
In addition to themethod of directly applying rounding to the val-ues, we also provide a method using auxiliaryupdate vectors to trade precision for more space.Suppose we know the range of update value in S-GD as [?r?, r?
], and we use additional m bits tostore all the values less than the limited numeri-cal precision .
Here r?can be easily estimatedby running SGD for several examples.
Then thereal precision is ?= 21?mr?.
For example, ifr?= 10?4and m = 8, then the numerical pre-cision is 7.8 ?10?7which can capture much higherprecision than the SGD update values have.
When388(a) CBOW model with 25 dimensions.
(b) Skipgram model with 25 dimensions.
(c) CBOW model with 200 dimensions.
(d) Skipgram model with 200 dimensions.Figure 1: Comparing performance on multiple similarity tasks, with different values of truncation.The y-axis represents the Spearman?s rank correlation coefficient for word similarity datasets, and thecosine value for paraphrase (bigram) datasets (see Sec.
4.2).the cumulated values in the auxiliary update vec-tors are greater than the original numerical preci-sion , e.g.,  = 2?7for 8 bits, we update the o-riginal vector and clear the value in the auxiliaryvector.
In this case, we can have final n-bit valuesin word embedding vectors as good as the methodpresented in Section 3.1.4 Experiments on Word/PhraseSimilarityIn this section, we describe a comprehensive studyon tasks that have been used for evaluating wordembeddings.
We train the word embedding algo-rithms, word2vec (Mikolov et al, 2013a; Mikolovet al, 2013b), based on the Oct. 2013 Wikipedi-a dump.1We first compare levels of truncationof word2vec embeddings, and then evaluate the s-tochastic rounding and the auxiliary vectors basedmethods for training word2vec vectors.4.1 DatasetsWe use multiple test datasets as follows.Word Similarity.
Word similarity datasetshave been widely used to evaluate word embed-ding results.
We use the datasets summarizedby Faruqui and Dyer (Faruqui and Dyer, 2014):wordsim-353, wordsim-sim, wordsim-rel, MC-30,RG-65, MTurk-287, MTurk-771, MEN 3000, YP-130, Rare-Word, Verb-143, and SimLex-999.2Wecompute the similarities between pairs of words1https://dumps.wikimedia.org/2http://www.wordvectors.org/389Table 1: The detailed average results for word similarity and paraphrases of Fig.
1.AverageCBOW SkipgramOriginal Binary 4-bits 6-bits 8-bits Original Binary 4-bits 6-bits 8-bitswordsim (25) 0.5331 0.4534 0.5223 0.5235 0.5242 0.4894 0.4128 0.4333 0.4877 0.4906wordsim (200) 0.5818 0.5598 0.4542 0.5805 0.5825 0.5642 0.5588 0.4681 0.5621 0.5637bigram (25) 0.3023 0.2553 0.3164 0.3160 0.3153 0.3110 0.2146 0.2498 0.3050 0.3082bigram (200) 0.3864 0.3614 0.2954 0.3802 0.3858 0.3565 0.3562 0.2868 0.3529 0.3548and check the Spearman?s rank correlation coeffi-cient (Myers and Well., 1995) between the com-puter and the human labeled ranks.Paraphrases (bigrams).
We use the paraphrase(bigram) datasets used in (Wieting et al, 2015),ppdb all, bigrams vn, bigrams nn, and bigram-s jnn, to test whether the truncation affects phraselevel embedding.
Our phrase level embedding isbased on the average of the words inside eachphrase.
Note that it is also easy to incorporateour truncation methods into existing phrase em-bedding algorithms.
We follow (Wieting et al,2015) in using cosine similarity to evaluate thecorrelation between the computed similarity andannotated similarity between paraphrases.4.2 Analysis of Bits NeededWe ran both CBOW and skipgram with negativesampling (Mikolov et al, 2013a; Mikolov et al,2013b) on the Wikipedia dump data, and set thewindow size of context to be five.
Then we per-formed value truncation with 4 bits, 6 bits, and 8bits.
The results are shown in Fig.
1, and the num-bers of the averaged results are shown in Table 1.We also used the binarization algorithm (Guo etal., 2014) to truncate each dimension to three val-ues; these experiments are is denoted using thesuffix ?binary?
in the figure.
For both CBOW andskipgram models, we train the vectors with 25 and200 dimensions respectively.The representations used in our experimentswere trained using the whole Wikipedia dump.
Afirst observation is that, in general, CBOW per-forms better than the skipgram model.
When us-ing the truncation method, the memory requiredto store the embedding is significantly reduced,while the performance on the test datasets remainsalmost the same until we truncate down to 4 bit-s.
When comparing CBOW and skipgram models,we again see that the drop in performance with 4-bit values for the skipgram model is greater thanthe one for the CBOW model.
For the CBOWmodel, the drop in performance with 4-bit valuesis greater when using 200 dimensions than it iswhen using 25 dimensions.
However, when usingskipgram, this drop is slightly greater when using25 dimensions than 200.We also evaluated the binarization ap-proach (Guo et al, 2014).
This model usesthree values, represented using two bits.
Weobserve that, when the dimension is 25, the bina-rization is worse than truncation.
One possibleexplanation has to do merely with the size ofthe space; while 325is much larger than the sizeof the word space, it does not provide enoughredundancy to exploit similarity as needed in thetasks.
Consequently, the binarization approachresults in worse performance.
However, whenthe dimension is 200, this approach works muchbetter, and outperforms the 4-bit truncation.
Inparticular, binarization works better for skipgramthan for CBOW with 200 dimensions.
Onepossible explanation is that the binarizationalgorithm computes, for each dimension of theword vectors, the positive and negative means ofthe values and uses it to split the original valuesin that dimension, thus behaving like a model thatclusters the values in each dimension.
The successof the binarization then indicates that skipgramembeddings might be more discriminative thanCBOW embeddings.4.3 Comparing Training MethodsWe compare the training methods for the CBOWmodel in Table 2.
For stochastic rounding, we s-cale the probability of rounding up to make surethat small gradient values will still update the val-ues.
Both stochastic rounding and truncation withauxiliary update vectors (shown in Sec.
3.2) re-quire 16 bits for each value in the training phase.Truncation with auxiliary update vectors finallyproduces 8-bit-value based vectors while stochas-tic rounding produces 16-bit-value based vectors.Even though our auxiliary update algorithm usessmaller memory/disk to store vectors, its perfor-mance is still better than that of stochastic round-ing.
This is simply because the update values inSGD are too small to allow the stochastic round-390Table 2: Comparing the training CBOWmodels: We set the average value of the originalword2vec embeddings to be 1, and the values inthe table are relative to the original embeddingsbaselines.
?avg.
(w.)?
represents the averagevalues of all word similarity datasets.
?avg.
(b.
)?represents the average values of all bigram phrasesimilarity datasets.
?Stoch.
(16 b.)?
representsthe method using stochastic rounding appliedto 16-bit precision.
?Trunc.
(8 b.)?
representsthe method using truncation with 8-bit auxiliaryupdate vectors applied to 8-bit precision.Stoch.
(16 b.)
Trunc.
(8 b.
)25 avg.
(w.) 0.990 0.997dim avg.
(b.)
0.966 0.992200 avg.
(w.) 0.994 1.001dim avg.
(b.)
0.991 0.999ing method to converge.
Auxiliary update vectorsachieve very similar results to the original vectors,and, in fact, result in almost the same vectors asproduced by the original truncation method.5 Experiments on Dependency ParsingWe also incorporate word embedding results intoa downstream task, dependency parsing, to eval-uate whether the truncated embedding results arestill good features compared to the original fea-tures.
We follow the setup of (Guo et al, 2015)in a monolingual setting3.
We train the parserwith 5,000 iterations using different truncation set-tings for word2vec embedding.
The data used totrain and evaluate the parser is the English datain the CoNLL-X shared task (Buchholz and Mar-si, 2006).
We follow (Guo et al, 2015) in usingthe labeled attachment score (LAS) to evaluate thedifferent parsing results.
Here we only show theword embedding results for 200 dimensions, sinceempirically we found 25-dimension results werenot as stable as 200 dimensions.The results shown in Table 3 for dependencyparsing are consistent with word similarity andparaphrasing.
First, we see that binarization forCBOW and skipgram is again better than the trun-cation approach.
Second, for truncation results,more bits leads to better results.
With 8-bits, wecan again obtain results similar to those obtained3https://github.com/jiangfeng1124/acl15-clnndepTable 3: Evaluation results for dependencyparsing (in LAS).Bits CBOW SkipgramOriginal 88.58% 88.15%Binary 89.25% 88.41%4-bits 87.56% 86.46%6-bits 88.62% 87.98%8-bits 88.63% 88.16%from the original word2vec embedding.6 ConclusionWe systematically evaluated how small can therepresentation size of dense word embedding bebefore it starts to impact the performance of NLPtasks that use them.
We considered both the finalsize of the size we provide it while learning it.
Ourstudy considers both the CBOW and the skipgrammodels at 25 and 200 dimensions and showed that8 bits per dimension (and sometimes even less) aresufficient to represent each value and maintain per-formance on a range of lexical tasks.
We also pro-vided two ways to train the embeddings with re-duced memory use.
The natural future step is toextend these experiments and study the impact ofthe representation size on more advanced tasks.AcknowledgmentThe authors thank Shyam Upadhyay for his helpwith the dependency parser embeddings results,and Eric Horn for his help with this write-up.
Thiswork was supported by DARPA under agreemen-t numbers HR0011-15-2-0025 and FA8750-13-2-0008.
The U.S. Government is authorized to re-produce and distribute reprints for Governmentalpurposes notwithstanding any copyright notationthereon.
The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of any of the organizations that supportedthe work.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-searc, 3:1137?1155.Peter F. Brown, Vincent J. Della Pietra, Peter V.de Souza, Jennifer C. Lai, and Robert L. Mercer.3911992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Sabine Buchholz and Erwin Marsi.
2006.
Conll-xshared task on multilingual dependency parsing.
InCoNLL, pages 149?164.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In EACL, pages 462?471.Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, ChrisDyer, and Noah A. Smith.
2015.
Sparse overcom-plete word vector representations.
In ACL, pages1491?1500.Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-u.
2014.
Revisiting embedding features for simplesemi-supervised learning.
In EMNLP, pages 110?120.Jiang Guo, Wanxiang Che, David Yarowsky, HaifengWang, and Ting Liu.
2015.
Cross-lingual depen-dency parsing based on distributed representations.In ACL, pages 1234?1244.Suyog Gupta, Ankur Agrawal, Kailash Gopalakrish-nan, and Pritish Narayanan.
2015.
Deep learningwith limited numerical precision.
In ICML, pages1737?1746.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
TACL, 3:211?225.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013a.
Distributed representa-tions of words and phrases and their compositional-ity.
In NIPS, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In NAACL-HLT, pages 746?751.Jerome L. Myers and Arnold D. Well.
1995.
ResearchDesign & Statistical Analysisn.
Routledge.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of CoNLL-09, pages 147?155.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In ACL, pages384?394.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2015.
From paraphrase database tocompositional paraphrase model and back.
TACL,3:345?358.392
