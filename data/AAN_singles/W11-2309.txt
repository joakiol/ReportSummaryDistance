Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 84?93,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsSource Language Categorization for improving a Speech into Sign Lan-guage Translation SystemV.
L?pez-Lude?a, R. San-Segundo, S. Lutfi, J.M.
Lucas-Cuesta, J.D.
Echevarry, B.Mart?nez-Gonz?lezGrupo de Tecnolog?a del HablaUniversidad Polit?cnica de Madrid{veronicalopez|lapiz|syaheerah|juanmak|jdec|beatrizmartinez}@die.upm.esAbstractThis paper describes a categorization mod-ule for improving the performance of aSpanish into Spanish Sign Language (LSE)translation system.
This categorizationmodule replaces Spanish words with asso-ciated tags.
When implementing this mod-ule, several alternatives for dealing withnon-relevant words have been studied.
Non-relevant words are Spanish words not rele-vant in the translation process.
The catego-rization module has been incorporated intoa phrase-based system and a Statistical Fi-nite State Transducer (SFST).
The evalua-tion results reveal that the BLEU hasincreased from 69.11% to 78.79% for thephrase-based system and from 69.84% to75.59% for the SFST.Keywords: Source language categoriza-tion, Speech into Sign Language transla-tion.
Lengua de Signos Espa?ola (LSE).1 IntroductionIn the world, there are around 70 million peoplewith hearing deficiencies (information from WorldFederation of the Deaf http://www.wfdeaf.org/).Deafness brings about significant communicationproblems: most deaf people are unable to use writ-ten languages, having serious problems when ex-pressing themselves in these languages orunderstanding written texts.
They have problemswith verb tenses, concordances of gender andnumber, etc., and they have difficulties when creat-ing a mental image of abstract concepts.
This factcan cause deaf people to have problems when ac-cessing information, education, job, social relation-ship, culture, etc.
According to information fromINE (Statistic Spanish Institute), in Spain, there are1,064,000 deaf people.
47% of deaf population donot have basic studies or are illiterate, and onlybetween 1% and 3% have finished their studies (asopposed to 21% of Spanish hearing people).
An-other example are the figures from the NationalDeaf Children?s Society (NDCS), Cymru, reveal-ing for the first time a shocking attainment gapbetween deaf and hearing pupils in Wales.
In 2008,deaf pupils were 30% less likely than hearing pu-pils to gain five A*-C grades at General Certificateof Secondary Education (GCSE) level, while atkey stage 3 only 42% of deaf pupils achieved thecore subject indicators, compared to 71% of theirhearing counterparts.
Another example is a studycarried out in Ireland in 2006; of 330 respondents?38% said they did not feel confident to read anewspaper and more than half were not fully con-fident in writing a letter or filling out a  form?
(Conroy, 2006).Deaf people use a sign language (their mothertongue) for communicating and there are notenough sign-language interpreters and communica-tion systems.
In Spain, there is the Spanish SignLanguage (Lengua de Signos Espa?ola LSE) thatis the official sign language.
In the USA, there are650,000 Deaf people (who use a sign language).Although there are more people with hearing defi-ciencies, there are only 7,000 sign-language inter-preters, i.e.
a ratio of 93 deaf people to 1interpreter.
In Finland we find the best ratio, 6 to 1,and in Slovakia the worst with 3,000 users to 1interpreter (Wheatley and Pabsch, 2010).
In Spainthis ratio is 221 to 1.
This information shows theneed to develop automatic translation systems withnew technologies for helping hearing and Deafpeople to communicate between themselves.84SpeechRecognitionLanguageTranslationWordSequenceTranslationInformationAcousticModelsLanguageModelSign AnimationSignDescriptionsSignSequenceNaturalSpeechCategorizationTaggedSequenceTagsFigure 1.
Spanish into LSE translation system.It is necessary to make a difference between?deaf?
and ?Deaf?
: the first one refers to non-hearing people, and the second one refers to hear-ing and non-hearing people who use a sign lan-guage to communicate between them, being part ofthe ?Deaf community?.
Each country has a differ-ent sign language, but there may even be differentsign languages in different regions.This paper describes a categorization modulefor improving the performance of a Speech intoSign Language Translation System.
This systemhelps Deaf people to communicate with govern-ment employees in a restricted domain: the re-newal of Identity Documents and Driver?s License(San-Segundo et al, 2008).
This system has beendesigned to translate the government employee?sexplanations into LSE when government employ-ees provide these face-to-face services.
The systemis made up of a speech recognizer (for decodingthe spoken utterance into a word sequence), a natu-ral language translator (a phrase-based system forconverting a word sequence into a sequence ofsigns belonging to the sign language), and a 3Davatar animation module (for playing back thesigns) (Figure 1).
This paper proposes to include afourth module named ?categorization?
between thespeech recognition and language translation mod-ules (Figure 1).
This categorization module re-places Spanish words with associated tags as willbe shown further.For the natural language translation module,two different statistical strategies have been ana-lyzed: a phrase-based system (Moses) and a Statis-tical Finite State Transducer (SFST).
The proposedcategorization module has been incorporated intoand evaluated with both translation strategies.This paper is organized as follows: section 2describes the state of the art.
Section 3 describesthe parallel corpus used in these experiments.
Themain characteristics of the LSE are presented insection 4.
Section 5 details the two main transla-tion strategies considered.
The categorizationmodule is described in section 6.
Section 7 in-cludes the main experiments and the obtained re-sults, and finally, sections 8 and 9 include the mainconclusions and the future work.2 State of the artIn recent years, several groups have developedprototypes for translating Spoken language intoSign Language: example-based (Morrissey, 2008),rule-based (Marshall and S?f?r, 2005; San-Segundo et al 2008), full sentence (Cox et al,2002) or statistical approaches (Stein et al, 2006;Morrissey et al, 2007; Vendrame et al, 2010)approaches.Given the sparseness of data for researchingin Sign Languages, in the last five years, severalprojects have started to generate more resources: inAmerican Sign Language (Dreuw et al., 2008),British Sign Language (Schembri, 2008), GreekSign Language (Efthimiou and Fotinea, 2008), inIrish Sign Language (Morrissey et al, 2010), NGS(German Sign Language) (Hanke et al, 2010), andItalian Sign Language (Geraci et al, 2010).
ForLSE, the biggest database was generated two yearsago in a Plan Avanza project(www.traduccionvozlse.es) (San-Segundo et al,2010) and it is has been used in this work.
Not onlythe data but also new practice (Forster et al, 2010)and new uses of traditional annotation tools (Cras-born et al, 2010) have been developed.The work presented in this paper describesexperiments with a relevant database Despite thesmall amount of data available for research into85sign languages, the system presented in this paperdemonstrates a very good performance comparedto similar systems previously developed.
The pre-sented results are also the best results for translat-ing Spanish into LSE using the biggest databasethat includes these languages.In Europe, the two main research projects in-volving sign languages are DICTA-SIGN (Hankeet al, 2010; Efthimiou et al, 2010) and SIGN-SPEAK (Dreuw et al, 2010a and 2010b), bothfinanced by The European Commission within theSeventh Frame Program.
DICTA-SIGN(http://www.dictasign.eu/) aims to develop thetechnologies necessary to make Web 2.0 interac-tions in sign language possible: users sign to awebcam using a dictation style.
The computer rec-ognizes the signed phrases, converts them into aninternal representation of sign language, and then ithas an animated avatar that signs them back to theusers.
In SIGN-SPEAK(http://www.signspeak.eu/), the overall goal is todevelop a new vision-based technology for recog-nizing and translating continuous sign languageinto text.3 Parallel corpusThis section describes the first Spanish-LSE paral-lel corpus developed for language processing intwo specific domains: the renewal of the IdentityDocument (ID) and Driver?s License (DL).
Thiscorpus has been obtained with the collaboration ofLocal Government Offices where these servicesare provided.
Over several weeks, the most fre-quent explanations (from the government employ-ees) and the most frequent questions (from theuser) were taken down.
In this period, more than5,000 sentences were noted and analyzed.Not all the sentences refer to ID or DL re-newal (Government Offices provide more ser-vices), so sentences had to be selected manually.This was possible because every sentence wastagged with the information about the service be-ing provided when it was collected.
Finally, 1360sentences were collected: 1,023 pronounced bygovernment employees and 337 by users.
Thesesentences have been translated into LSE, both intext (sequence of glosses) and in video (containingreplayed sentences by native LSE signers), andcompiled in an excel file.
Videos are not used inthis study but they were collected for generating acomplete parallel corpus.This corpus was increased to 4,080 by incor-porating different variants for Spanish sentences(maintaining the LSE translation) (San-Segundo etal.
2010).
Table 1 summarizes the main features ofthis database.Spanish LSESentence pairs 4,080Different sentences 3,342 1,289Words/signs per sentence 7.7 5.7Running words 31,501 23,256Vocabulary 1,232 636Table 1.
Main statistics of the corpusFor the experiments presented in this paper,this database has been divided randomly into threesets: training (75%), development (12.5%) and test(12.5%).
The training set was used for tuning thespeech recognizer (vocabulary and language mod-el) and training the translation models.
The devel-opment set was used for tuning the translationsystems and finally, the test set was used for evalu-ating the categorization module.4 Spanish Sign Language (LSE)Spanish Sign Language (LSE), just like other signlanguages, has a visual-gestural channel, but it alsohas grammatical characteristics similar to spokenlanguages.
Sign languages have complex gram-mars and professional linguists have found all ofthe necessarily linguistic characteristics for classi-fying sign languages as ?true languages?.
In lin-guistic terms, sign languages are as complex asspoken languages, despite the common misconcep-tion that they are a ?simplification?
of spoken lan-guages.
For example, The United Kingdom andUSA share the same language.
However, BritishSign Language is completely different from Amer-ican Sign Language.
W. Stokoe (Stokoe, 1960)supports the idea that sign languages have fourdimensions (three space dimensions plus time),and spoken languages have only one dimension,time, so it cannot say that sign languages are asimplification of any other language.One important difference between spokenlanguages and sign languages is sequentially.
Pho-nemes in spoken languages are produced in a se-quence.
On the other hand, sign languages have a86large non-sequential component, because fingers,hands and face movements can be involved in asign simultaneously, even two hands moving indifferent directions.
These features give a com-plexity to sign languages that traditional spokenlanguages do not have.
This fact makes it verydifficult to write sign languages.
Traditionally,signs have been written using words (in capitalletters) in Spanish (or English in the case of BSL,British Sign Language) with a similar meaning tothe sign meaning.
They are called glosses (i.e.?CAR?
for the sign ?car?
).In the last 20 years, several alternatives, basedon specific characteristics of the signs, have ap-peared in the international community: HamNoSys(Prillwitz et al 1989), SEA (Sistema de EscrituraAlfab?tica) (Herrero, A., 2004) and SignWriting(http://www.signwriting.org/).
HamNoSys andSignWriting require defining a specific picture fontto be used by computers.
SignWriting includesface features in the notation system but HamNoSysand SEA do not include them.
All of these alterna-tives are flexible enough for dealing with differentsign languages including LSE.
However, in thiswork, glosses have been considered for writingsigns because it is the most familiar and extendedalternative according to the Spanish Deaf Associa-tion.
These glosses include non-speech indicators(i.e.
PAY or PAY?
if the sign is localized at theend of an interrogative sentence) and finger spell-ing indicators (i.e.
DL-PETER that must be repre-sented letter by letter P-E-T-E-R).LSE has some characteristics that differ fromSpanish.
One important difference is the order ofarguments in sentences: LSE has a SOV (subject-object-verb) order in contrast to SVO (subject-verb-object) Spanish order.
An example that illus-trates this behaviour is shown below:Spanish: Juan ha comprado las entradas (Juan hasbought the tickets)LSE: JUAN ENTRADAS COMPRAR (JUANTICKETS TO-BUY)There are other typological differences that are notrelated to predication order:?
Gender is not usually specified in LSE, in con-trast to Spanish.?
In LSE, there can be concordances betweenverbs and subject, receiver or object and evensubject and receiver, but in Spanish there can beonly concordance between verb and subject:?
Spanish: Te explica (he explains to you)?
LSE: EXPLICAR-?l-a-ti (EXPLAIN-HIM-TO-YOU)?
The use of classifiers is common in LSE, butthey are not in Spanish.?
Spanish: debe acercarse a la c?mara (youmust approach the camera)?
LSE: FOTO CLD_GRANDE_NOCLL_ACERCARSE DEBER (PHOTOCLD_BIG_NO CLL_APPROACH MUST)?
Articles are used in Spanish, but not in LSE.?
Plural can be descriptive in LSE, but not inSpanish.?
In Spanish, there is a copula in non-verbalpredications (the verb ?to be?, ser and estar inSpanish), but there is not in LSE.?
There are Spanish impersonal sentences, but notin LSE.?
LSE is more lexically flexible than Spanish,and it is perfect for generating periphrasisthrough its descriptive nature and because ofthis, LSE has fewer nouns than Spanish.
(i.e.mud is translated into SAND+WATER)?
To finish, LSE has less glosses per sentence(5.7 in our database) than Spanish (7.7 in ourdatabase).?
LSE has smaller vocabulary variability.
LSEhas a vocabulary of around 10,000 signs whileSpanish has several millions of different words.Good examples are the different verb conjuga-tions.5 Statistical translation strategiesIn this paper, two different statistical strategieshave been considered: a phrase-based system and aStatistical Finite State Transducer.
The proposedautomatic categorization has been evaluated withboth translation strategies.
This section describesthe architectures used for the experiments.5.1 Phrase-based translation systemThe Phrase-based translation system is based onthe software released at the 2009 NAACL Work-shop on Statistical Machine Translation(http://www.statmt.org/wmt09/) (Figure 2).87WordAlignmentGIZA++Phrase extractionand scoringPhrase-modelParallelcorporaN-gram trainSRI-LMTarget lang.corporaTranslationMOSESSource lang.sentenceTranslationoutputTranslationModelTarget lang.ModelTarget Language: Sign LanguageSource Language: SpanishFigure 2.
Phrase-based translation architecture.In this study, a phrase consists of a subse-quence of words (in a sentence) that intends tohave a meaning.
Every sentence is split in severalphrases automatically so this segmentation canhave errors.
But, the main target, when training aphrase-based model, is to split the sentence in sev-eral phrases and to find their corresponding trans-lations in the target language.The phrase model has been trained startingfrom a word alignment computed using GIZA++(Och and Ney, 2003).
GIZA++ is a statistical ma-chine translation toolkit that is used to train IBMModels 1-5 and an HMM word alignment model.In this step, the alignments between words andsigns in both directions (Spanish-LSE and LSE-Spanish) are calculated.
The ?alignment?
parame-ter has been fixed to ?target-source?
as the bestoption (based on experiments over the develop-ment set): only this target-source alignment wasconsidered (LSE-Spanish).
In this configuration,alignment is guided by signs: this means that inevery sentence pair alignment, each word can bealigned to one or several signs (but not the oppo-site), and also, it is possible that some words werenot aligned to any sign.
When combining thealignment points from all sentences pairs in thetraining set, it is possible to have all possiblealignments: several words aligned to several signs.After the word alignment, the system per-forms a phrase extraction process (Koehn et al2003) where all phrase pairs that are consistentwith the word alignment (target-source alignmentin our case) are collected.
In the phrase extraction,the maximum phrase length has been fixed at 7consecutive words, based on development experi-ments over the development set (see previous sec-tion).Finally, the last step is phrase scoring.
In thisstep, the translation probabilities are computed forall phrase pairs.
Both translation probabilities arecalculated: forward and backward.For the translation process, the Moses decoderhas been used (Koehn, 2010).
This program is abeam search decoder for phrase-based statisticalmachine translation models.
In order to obtain a 3-gram language model, the SRI language modelingtoolkit has been used (Stolcke, 2002).5.2 Phrase-based translation systemThe translation based on SFST is carried outas set out in Figure 3.WordAlignmentGIZA++Finite StateTransducerGIATIParallelcorporaTranslationsearch over theFSTTranslationModelSource lang.sentenceTranslationoutputFigure 3.
Diagram of the FST-based translationmodule.The translation model consists of an SFSTmade up of aggregations: subsequences of alignedsource and target words.
The SFST is inferredfrom the word alignment (obtained with GIZA++)using the GIATI (Grammatical Inference andAlignments for Transducer Inference) algorithm(Casacuberta and Vidal, 2004).
The SFST prob-abilities are also trained from aligned corpora.
Thesoftware used in this paper has been downloadedfromhttp://prhlt.iti.es/content.php?page=software.php.6 Categorization moduleAs it was presented in Figure 1, the categorizationmodule proposed in this paper analyzes the sourcelanguage sentence (sentence in Spanish) and re-places Spanish words with their associated tags.This module uses a list of 1014 Spanish words (thevocabulary in this restricted domain) and the corre-sponding tags.
For every word, only one syntactic-semantic tag is associated.
In the case of homo-nyms, the most frequent meaning has been consid-ered for defining the syntactic-semantic tag.
Figure4 shows an extract of the word-tag list.
This list iscomposed of Spanish words and their correspond-ing tags, including the English translation in paren-thesis.88Figure 4.
Extract of the word-tag list.The categorization module executes a simpleprocedure: for all words in a Spanish sentence, thecategorization module looks for this word in thelist and replaces it with the associated tag.
It isimportant to comment two main aspects.
The firstone is that there is a tag named ?non-relevant?associated to those words that are not useful fortranslating the sentence.
The second one is that ifthe Spanish word is not in the list (it is an Out OfVocabulary word: OOV), this word is not replacedwith any tag: this word is kept as it is.In order to train the statistical translationmodules when using the categorization module, itis necessary to retrain the translation models con-sidering the tagged source language, not the origi-nal word sentences, and using the training set.
Thisway, the translation models learn the relationshipsbetween tags and signs.The main issue for implementing the catego-rization module is to generate the list of the Span-ish words with the associated tags.
In this work,the categorization module considers the categoriesused in the rule-based translation system previ-ously developed for this application domain (San-Segundo et al, 2008).
These categories were gen-erated manually during one week, approximately.In this case, the natural language translation mod-ule was implemented using a rule-based techniqueconsidering a bottom-up strategy.
The translationprocess is carried out in two steps.
In the first one,every word is mapped into one syntactic-pragmatictag.
After that, the translation module applies dif-ferent rules that convert the tagged words intosigns by means of grouping concepts or signs anddefining new signs.
These rules can define shortand large scope relationships between the conceptsor signs.When implementing the categorization mod-ule, several strategies for dealing with the ?non-relevant?
words have been proposed:?
In the first alternative, all the words are replacedby their tags with the exception of those wordsthat they do not appear in the list (OOV words).As, it was commented before, they are kept asthey are.
In the word-tag list, there is a ?non-relevant?
tag mapped to words that are not rele-vant for the translation process (named ?basura?(non-relevant)).
This alternative will be referredin the experiments like ?Base categorization?.For example:o Source sentence: debes pagar  las tasas en lacaja (you must pay the taxes in the cash desk)o Categorizated source sentence: DEBERPAGAR basura DINERO basura basuraDINERO-CAJA (MUST PAY non-relevantMONEY non-relevant non-relevant CASH-DESK)o Target sentence: VENTANILLAESPEC?FICO CAJA TU PAGAR (WINDOWSPECIFIC CASH-DESK YOU PAY)?
The second proposed alternative was not to tagany word in the source language but removingnon-relevant words from the source lexicon (as-sociated to the ?non-relevant?
tag).
This alterna-tive will be referred in the experiments like?Non-relevant word deletion?.
For example:o Source sentence: debes pagar las tasas en lacaja (you must pay the taxes in the cash desk)o Categorizated source sentence: debes pagartasas cajao Target sentence: VENTANILLAESPEC?FICO CAJA TU PAGAR (WINDOWSPECIFIC CASH-DESK YOU PAY)?
Finally, the third alternative proposes to replacewords with tags (with the exception of OOVs)and to remove ?non-relevant?
tags.
This alterna-tive will be referred in the experiments like?Categorization and non-relevant word dele-tion?.
For example:o Source sentence: debes pagar las tasas en lacaja (you must pay the taxes in the cash desk)o Categorizated source sentence: de-bes|DEBER pagar|PAGAR tasas|DINEROcaja|DINERO-CAJAo Target sentence: VENTANILLAESPEC?FICO CAJA TU PAGAR(WINDOW SPECIFIC CASH-DESK YOUPAY)In the next section, all the alternatives will beevaluated and discussed.word TAG (word and tag in English)?cerrado CERRAR-YA (closed CLOSE-ALREADY )cerramos CERRAR (we close CLOSE )cerrar CERRAR (to close CLOSE)cobradas COBRAR-YA (charged CHARGE-ALREADY)cobro COBRAR (I charge CHARGE)coge COGER (you get GET)cogido COGER-YA (gotten GET-ALREADY)coja COGER (you get GET)?897 Experiments and discussionFor the experiments, the corpus (described in sec-tion 3) was divided randomly into three sets: train-ing (75%), development (12.5%) and test (12.5%).Results are compared with a baseline.
This base-line consists of training models with originalsource and target corpus without any type of fac-torization, i.e, sentences contain words and signsfrom the original database.
For example: this sen-tence ?debes pagar las tasas en la caja?
(you mustpay the taxes in the cash desk) is translated into?VENTANILLA ESPEC?FICO CAJA TUPAGAR?
(WINDOW SPECIFIC CASH-DESKYOU PAY).For evaluating the performance of the transla-tion systems, the BLEU (BiLingual EvaluationUnderstudy) metric (Papineni et al, 2002) hasbeen used.
BLEU is one of the most well-knownmetric for evaluating automatic translation systemsbecause this metric presents a good correlationwith human evaluations.
This metric has been alsoadopted to evaluate speech into sign languagetranslation systems (Stein et al, 2006; Morrissey etal., 2007; Vendrame et al, 2010, San-Segundo etal.
2008).
In order to analyze the significance ofthe differences between several systems, for everyBLEU result, the confidence interval (at 95%) isalso presented.
This interval is calculated using thefollowing formula:)1()100(96,1nBLEUBLEU ?=?
?n is the number of signs used in evaluation, in thiscase n=2,906.
An improvement between two sys-tems is statistically significant when there is nooverlap between the confidence intervals of bothsystems.Related to the speech recognizer, it is impor-tant to comment that the Word Error Rate (WER)obtained in these experiments has been 4.7%.Table 2 compares the baseline system and thesystem with the categorization module for translat-ing the references (Reference) and the speech rec-ognizer outputs (ASR output) using the phrase-based translation system.Phrase-based translation Sys-tem BLEU ?
?Reference 73.66 1.60BaselineASR output 69.11 1.68Reference 81.91 1.40 Base categoriza-tion ASR output 74.55 1.58Reference 80.02 1.45 Non-relevantwords deletion ASR output 73.89 1.60Reference 84.37 1.32 Categorizationand non-relevantword deletion ASR output 78.79 1.49Table 2.
Evaluation results for the phrase-basedtranslation system.Table 3 compares the baseline system and thesystem with the categorization module for translat-ing the references (Reference) and the speech rec-ognizer outputs (ASR output) using the SFST-based translation system.SFST BLEU ?
?Reference 71.17 1.65BaselineASR output 69.84 1.67Reference 71.86 1.63 Base categoriza-tion ASR output 68.73 1.69Reference 76.71 1.54 Non-relevantwords deletion ASR output 72.77 1.62Reference 81.48 1.41 Categorizationand non-relevantword deletion ASR output 75.59 1.56Table 3.
Evaluation results for the SFST-basedtranslation system.Comparing the three alternatives for dealingwith the non-relevant words, it is shown that add-ing tags to the words and removing ?non-relevant?words are complementary actions that allow reach-ing the best results.In order to better understand the main causesof this improvement, an error analysis has beencarried out, establishing a relationship betweenthese errors and the main differences betweenSpanish and LSE.90The most important type of error (35% of thecases) is related to the fact that in Spanish there aremore words than signs in LSE (7.7 for Spanish and5.7 for LSE in this corpus).
This circumstanceprovokes different types of errors: generation ofmany phrases in the same output, producing a highnumber of insertions.
When dealing with long sen-tences there is the risk that the translation modelcannot deal properly with the big distortion.
Thisproduces important changes in order and some-times the sentence is truncated producing severaldeletions.The second most important source of errors(25% of the cases) is related to the fact that whentranslating Spanish into LSE, there is a relevantnumber of words in the testing set that they do notappear in the training set due to the higher variabil-ity presented in Spanish.
These words are namedOut Of Vocabulary words.
For example, in Span-ish there are many verb conjugations that are trans-lated into the same sign sequence.
So, when a newconjugation appears in the evaluation set, it is anOOV that provokes a translation error.Other important source of errors correspondsto ordering errors provoked by the different orderin predication: LSE has a SOV (Subject-Object-Verb) while Spanish SVO (Subject-Verb-Object).In this case, the frequency is close to 20%Finally, there are others causes of errors likethe wrong generation of the different classifiersneeded in LSE and not presented in Spanish (11%)and the existence of some deletions when translat-ing very specific names, even when they are in thetraining set.
Some of these names (i.e.
?mud?
istranslated into SAND + WATER) need some pe-riphrasis in LSE that not always are properly gen-erated.Based on this error analysis, the main causesof the translation errors are related to the differentvariability in the vocabulary for Spanish and LSE(much higher in Spanish), the different number forwords or signs in the sentences (higher in Spanish)and the different predication order.The categorization module allows reducingthe variability in the source language (for example,several verb conjugations are tagged with the sametag) and also the number of tokens composing theinput sentence (when removing non-relevantwords).
Also, reducing the source language vari-ability and the number of tokens provoke an im-portant reduction on the number of source-targetalignments the system has to train.
When having asmall corpus, as it is the case of many sign lan-guages, this reduction of alignment points permitsto obtain better training models with less data,improving the results.
These aspects allow increas-ing the system performance.
Presumably, if therewere a very large corpus of Spanish-to-Spanish-Sign-Language available, the system could learnbetter translation models and the improvementreached with this categorization module would belower.The evaluation results reveal that the BLEUhas increased from 69.11% to 78.79% for thephrase-based system and from 69.84% to 75.59%for the SFST.8 ConclusionsThis paper describes a categorization module forimproving a Spanish into Spanish Sign LanguageTranslation System.
This module allows incorpo-rating syntactic-semantic information during thetranslation process reducing the source languagevariability and the number of words composing theinput sentence.
These two aspects reduce the trans-lation error rate considering two statistical transla-tion systems: phrase-based and SFST-basedtranslation systems.
This system is used to translategovernment employee?s explanations into LSEwhen providing a personal service for renewing theIdentity Document and Driver?s License.9 Future workThe main issue for implementing the categoriza-tion module is to generate the list of the Spanishwords with the associated tags.
Generating this listmanually is a subjective, slow and difficult task.Because of this, in the near future, authors willwork on the possibility to define a procedure forcalculating this list automatically.AcknowledgmentsThe authors would like to thank the eSIGN consor-tium for permitting the use of the eSIGN Editorand the 3D avatar.
The authors would also like tothank discussions and suggestions from the col-leagues at GTH-UPM.
This work has been sup-ported by Plan Avanza Exp N?
: TSI-020100-2010-489), INAPRA (MEC ref: DPI2010-21247-C02-9102), and SD-TEAM (MEC ref: TIN2008-06856-C05-03) projects and FEDER program.ReferencesCasacuberta F., E. Vidal.
2004.
?Machine Translationwith Inferred Stochastic Finite-State Transducers?.Computational Linguistics, Vol.
30, No.
2, pp.
205-225, 2004.Conroy, P. 2006.
Signing in and Signing Out: The Edu-cation and Employment Experiences of Deaf Adultsin Ireland.
Research Report, Irish Deaf Society, Dub-lin.
2006.Cox, S.J., Lincoln M., Tryggvason J., Nakisa M., WellsM., Mand Tutt, and Abbott, S., 2002 ?TESSA, a sys-tem to aid communication with deaf people?.
InASSETS 2002, Edinburgh, Scotland, pp 205-212,2002.Crasborn O., Sloetjes H. 2010.
?Using ELAN for anno-tating sign language corpora in a team setting?.
In 4thWorkshop on the Representation and Processing ofSign Languages: Corpora and Sign Language Tech-nologies (CSLT 2010), Valletta, Malta, 2010. pp 61-65Dreuw P., Neidle C., Athitsos V., Sclaroff S., and NeyH.
2008.
?Benchmark Databases for Video-BasedAutomatic Sign Language Recognition?.
In Interna-tional Conference on Language Resources and Eval-uation (LREC), Marrakech, Morocco, May 2008. pp1115-1121.Dreuw P., Ney H., Martinez G., Crasborn O., Piater J.,Miguel Moya J., and Wheatley M., 2010 ?The Sign-Speak Project - Bridging the Gap Between Signersand Speakers?.
In 4th Workshop on the Representa-tion and Processing of Sign Languages: Corpora andSign Language Technologies (CSLT 2010), Valletta,Malta, 2010a.
pp 73-80.Dreuw P., Forster J., Gweth Y., Stein D., Ney H., Mar-tinez G., Verges Llahi J., Crasborn O., Ormel E., DuW., Hoyoux T., Piater J., Moya Lazaro JM, andWheatley M. 2010 ?SignSpeak - Understanding,Recognition, and Translation of Sign Languages?.
In4th Workshop on the Representation and Processingof Sign Languages: Corpora and Sign LanguageTechnologies (CSLT 2010), Valletta, Malta, May2010b.
pp 65-73Efthimiou E., and Fotinea, E., 2008 ?GSLC: Creationand ?nnotation of a Greek Sign Language Corpus forHCI?
LREC 2008. pp 1-10.Efthimiou E., Fotinea S., Hanke T., Glauert J., BowdenR., Braffort A., Collet C., Maragos P., Goudenove F.2010.
?DICTA-SIGN: Sign Language Recognition,Generation and Modelling with application in DeafCommunication?.
In 4th Workshop on the Represen-tation and Processing of Sign Languages: Corporaand Sign Language Technologies (CSLT 2010), Val-letta, Malta, May 2010. pp 80-84.Forster J., Stein D., Ormel E., Crasborn O., Ney H.2010.
?Best Practice for Sign Language Data Collec-tions Regarding the Needs of Data-Driven Recogni-tion and Translation?.
In 4th Workshop on theRepresentation and Processing of Sign Languages:Corpora and Sign Language Technologies (CSLT2010), Valletta, Malta, May 2010. pp 92-98.Geraci C., Bayley R., Branchini C., Cardinaletti A.,Cecchetto C., Donati C., Giudice S., Mereghetti E.,Poletti F., Santoro M., Zucchi S. 2010.
?Building acorpus for Italian Sign Language.
Methodological is-sues and some preliminary results?.
In 4th Workshopon the Representation and Processing of Sign Lan-guages: Corpora and Sign Language Technologies(CSLT 2010), Valletta, Malta, May 2010. pp 98-102.Hanke T., K?nig L., Wagner S., Matthes S. 2010.
?DGSCorpus & Dicta-Sign: The Hamburg Studio Setup?.In 4th Workshop on the Representation and Process-ing of Sign Languages: Corpora and Sign LanguageTechnologies (CSLT 2010), Valletta, Malta, May2010.
pp 106-110.Herrero, A., 2004 ?Escritura alfab?tica de la Lengua deSignos Espa?ola?
Universidad de Alicante.
Serviciode Publicaciones.Koehn P., F.J. Och D. Marcu.
2003.
?Statistical Phrase-based translation?.
Human Language TechnologyConference 2003 (HLT-NAACL 2003), Edmonton,Canada, 2003. pp.
127-133.Koehn, Philipp.
2010.
?Statistical Machine Transla-tion?.
phD.
Cambridge University Press.Marshall, I., S?f?r, E. (2005) ?Grammar Developmentfor Sign Language Avatar-Based Synthesis?, In Pro-ceedings HCII 2005, 11th International Conferenceon Human Computer Interaction (CD-ROM), LasVegas, USA, July 2005. pp 1-10.Morrissey S., Way A., Stein D., Bungeroth J., and NeyH., 2007 ?Towards a Hybrid Data-Driven MT Sys-tem for Sign Languages.
Machine Translation Sum-mit (MT Summit)?, Copenhagen, Denmark, 2007. pp329-335.Morrissey, S. 2008.
?Data-Driven Machine Translationfor Sign Languages?.
Thesis.
Dublin City University,Dublin, Ireland.Morrissey S., Somers H., Smith R., Gilchrist S., Danda-pat S., 2010 ?Building Sign Language Corpora forUse in Machine Translation?.
In 4th Workshop on92the Representation and Processing of Sign Lan-guages: Corpora and Sign Language Technologies(CSLT 2010), Valletta, Malta, May 2010. pp 172-178.Och J., Ney.
H., 2003.
?A systematic comparison ofvarious alignment models?.
Computational Linguis-tics, Vol.
29, No.
1, 2003. pp.
19-51.Papineni K., S. Roukos, T. Ward, W.J.
Zhu.
2002?BLEU: a method for automatic evaluation of ma-chine translation?.
40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL),Philadelphia, PA. 2002. pp.
311-318.Prillwitz, S., R. Leven, H. Zienert, T. Hanke, J.Henning, et-al.
1989.
?Hamburg Notation System forSign Languages ?
An introductory Guide?.
Interna-tional Studies on Sign Language and the Communi-cation of the Deaf, Volume 5.
Institute of GermanSign Language and Communication of the Deaf,University of Hamburg, 1989.San-Segundo R., Barra R., C?rdoba R., D?Haro L.F.,Fern?ndez F., Ferreiros J., Lucas J.M., Mac?as-Guarasa J., Montero J.M., Pardo J.M, 2008.
?Speechto Sign Language translation system for Spanish?.Speech Communication, Vol 50.
2008. pp.
1009-1020.San-Segundo, R., Pardo, J.M., Ferreiros, F., Sama, V.,Barra-Chicote, R., Lucas, JM., S?nchez, D., Garc?a.A., ?Spoken Spanish Generation from Sign Lan-guage?
Interacting with Computers, Vol.
22, No 2,2010. pp.
123-139.Schembri.
A., 2008 ?British Sign Language CorpusProject: Open Access Archives and the Observer?sParadox?.
Deafness Cognition and Language Re-search Centre, University College London.
LREC2008.
pp 1-5.Stein, D., Bungeroth, J. and Ney, H.: 2006 ?Morpho-Syntax Based Statistical Methods for Sign LanguageTranslation?.
11th Annual conference of the Euro-pean Association for Machine Translation, Oslo,Norway, June 2006. pp 223-231.Stolcke A., 2002.
?SRILM ?
An Extensible LanguageModelling Toolkit?.
Proc.
Intl.
Conf.
on SpokenLanguage Processing, vol.
2, Denver USA, 2002. pp.901-904,Stokoe W., Sign Language structure: an outline of thevisual communication systems of the American deaf,Studies in Linguistics, Buffalo University Paper 8,1960.Vendrame M., Tiotto G., 2010.
ATLAS Project: Fore-cast in Italian Sign Language and Annotation of Cor-pora.
In 4th Workshop on the Representation andProcessing of Sign Languages: Corpora and SignLanguage Technologies (CSLT 2010), Valletta, Mal-ta, May 2010. pp 239-243.Wheatley, M., Annika Pabsch, 2010.
?Sign Language inEurope?.
In 4th Workshop on the Representation andProcessing of Sign Languages: Corpora and SignLanguage Technologies.
LREC.
Malta 2010. pp 251-255.93
