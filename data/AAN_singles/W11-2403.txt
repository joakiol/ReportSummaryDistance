Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 20?29,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsClassification-based Contextual PreferencesShachar Mirkin, Ido Dagan, Lili KotlermanBar-Ilan UniversityRamat Gan, Israel{mirkins,dagan,davidol}@cs.biu.ac.ilIdan SzpektorYahoo!
ResearchHaifa, Israelidan@yahoo-inc.comAbstractThis paper addresses context matching in tex-tual inference.
We formulate the task underthe Contextual Preferences framework whichbroadly captures contextual aspects of infer-ence.
We propose a generic classification-based scheme under this framework which co-herently attends to context matching in infer-ence and may be employed in any inference-based task.
As a test bed for our scheme we usethe Name-based Text Categorization (TC) task.We define an integration of Contextual Prefer-ences into the TC setting and present a concreteself-supervised model which instantiates thegeneric scheme and is applied to address con-text matching in the TC task.
Experiments onstandard TC datasets show that our approachoutperforms the state of the art in context mod-eling for Name-based TC.1 IntroductionTextual inference is prevalent in text understandingapplications.
For example, in Question Answering(QA) the expected answer should be inferred fromretrieved passages, and in Information Extraction (IE)the meaning of the target event is inferred from itsmention in the text.Lexical inferences make a substantial part of theinference process.
In such cases, a target term isinferred from text expressions based on either one oftwo types of lexical matches: (i) a direct match ofthe target term in the text.
For instance, the IE eventinjure may be detected by finding the word injure inthe text; (ii) an indirect match, through a term thatimplies the meaning of the target term, e.g.
inferringinjure from hurt.In either case, due to word ambiguity, it is nec-essary to validate that the context of the match con-forms with the intended meaning of the target termbefore carrying out an inference operation based onthis match.
For example, ?You hurt my feelings?
con-stitutes an invalid context for the injure event as hurtin this text does not refer to a physical injury.
Simi-larly, inferring the protest-related event demonstratebased on demo is deemed invalid although demo im-plies the meaning of the word demonstrate in othercontexts, e.g., concerning software demonstration.Although seemingly equivalent, a closer look re-veals that the above two examples correspond to twodistinct contextual mismatch situations.
While thematch of hurt is invalid for injure in the particulargiven context, an inference based on demo is invalidfor the protest demonstrate event in any context.Thus, several types of context matching are in-volved in textual inference.
While most prior workaddressed only specific context matching scenarios,Szpektor et al (2008) presented a broader view,proposing a generic framework for context match-ing in inference, termed Contextual Preferences (CP).CP specifies the types of context matching that needto be considered in inference, allowing a model ofchoice to be applied for validating each type of match.Szpektor et al applied CP to an IE task using differ-ent models to validate each type of context match.In this work we adopt CP as our context matchingframework and propose a novel classification-basedscheme which provides unified modeling for CP.
Werepresent typical contexts of the textual objects thatparticipate in inference using classifiers; at inferencetime, each match is assessed by the respective classi-fiers which determine its contextual validity.As a test bed we applied our scheme to the task20of Name-based Text Categorization.
This is an unsu-pervised setting of TC where the only input given isthe category name, and in which context validationis of high importance.
We instantiate the schemewith a novel self-supervised model and apply it tothe TC task.
We suggest a method for integrating anyCP-based context matching model into TC and use itto combine the context matching scores generated byour model.
Results on two standard TC datasets showthat our approach outperforms the state of the art con-text model for this task and suggest applying thisscheme to additional inference-based applications.2 Background2.1 Context matching in inferenceWord ambiguity has been traditionally addressedthrough Word Sense Disambiguation (WSD) (Nav-igli, 2009).
The WSD task requires selecting themeaning of a target term from amongst a predefinedset of senses, based on sense-inventories such asWordNet (Fellbaum, 1998).An alternative approach eliminates the reliance onsuch inventories.
Instead of explicit sense identifi-cation, a direct sense-match between terms is pur-sued (Dagan et al, 2006).
Lexical substitution (Mc-Carthy and Navigli, 2009) is probably the most com-monly known task that follows this approach.
Con-text matching is a generalization of lexical substitu-tion, which seeks a match between terms in context,not necessarily for the purpose of substitution.
For in-stance, the word played in ?U2 played their first-everconcert in Russia?
contextually matches music, al-though music cannot substitute played in this context.The context matching task, therefore, is to determine(by quantifying or giving a binary decision) the va-lidity of a match between two terms in context.In Section 1 we informally presented two cases ofcontextual mismatches.
A comprehensive view ofcontext matching types is provided by the ContextualPreferences framework (Szpektor et al, 2008).
CPis phrased in terms of the Textual Entailment (TE)paradigm (Dagan et al, 2009).
In TE, a text t entailsa textual hypothesis h if the meaning of h can beinferred from t. Formulating the IE example fromSection 1 within TE, h may be the name of the targetevent, injure, and t is a text segment from which hcan be inferred.
A direct match occurs when a termin h is identical to a term in t. An inference basedon an indirect match is viewed as the application ofa lexical entailment rule, r, such as ?hurt?
injure?,where the entailing left-hand side (LHS) of the rule(hurt) is matched in the text, while the entailed right-hand side (RHS), injure, is matched in the hypothesis.Hence, three inference objects take part in infer-ence operations: t, h and r. Most prior work ad-dressed only specific contextual matches betweenthese objects.
For example, Harabagiu et al (2003)matched the contexts of t and h for QA (answer andquestion, respectively); Barak et al (2009) matchedt and h (document and category) in TC, while otherworks, including those applying lexical substitution,typically validated the context match between t and r(Kauchak and Barzilay, 2006; Dagan et al, 2006;Pantel et al, 2007; Connor and Roth, 2007).In comparison, in the CP framework, all possiblecontextual matches among t, h and r are considered:t?h, t?
r and r?h.
The three context matches aredepicted in Figure 1 (left).
In CP, the representationof each inference object is enriched with contextualinformation which is used to characterize its validcontexts.
Such information may be the words of theevent description in IE, corpus instances based onwhich a rule was learned, or an annotation of relevantWordNet senses in Name-based TC.
For example,a category name hockey may be assigned with thesense number corresponding to ice hockey, but notto field hockey, in order to designate information thatlimits the valid contexts of the category to the formeramong the two meanings of the name.Before an inference operation is performed, thecontext representations of each pair among the partic-ipating objects should be matched by a context modelin order to assess the contextual validity of the opera-tion.
Along with the context representation and thespecific context matching models, the way contextmodel decisions are combined needs to be specifiedin a concrete implementation of the CP framework.2.2 Context matching modelsSeveral approaches were taken in prior work tomodel context matching, mostly within the scopeof learning selectional preferences of templatizedlexical-syntactic rules (e.g.
?Xsubj????
hitobj???
Y ???Xsubj????
attackobj???
Y ?
).21Pantel et al (2007) and Szpektor et al (2008) rep-resented the context of such rules as the intersectionof preferences of the rule?s LHS and RHS, namely theobserved argument instantiations or their semanticclasses.
A rule is deemed applicable to a given text ifthe argument instantiations in the text are similar tothe selectional preferences of the rule.
To overcomesparseness, other works represented context in latentspace.
Pennacchiotti et al (2007) and Szpektor et al(2008) measured the similarity between the LatentSemantic Analysis (LSA) (Deerwester et al, 1990)representations of matched contexts.
Dinu and La-pata (2010) used Latent Dirichlet Allocation (LDA)(Blei et al, 2003) to model templates?
latent senses,determining rule applicability based on the similaritybetween the two sides of the rule when instantiatedby the context, while Ritter et al (2010) used LDAto model argument classes, considering a rule validfor a given argument instantiation if its instantiatedtemplates are drawn from the same hidden topic.A different approach is provided by classification-based models which learn classifiers for inferenceobjects.
A classifier is trained based on positive andnegative examples which represent valid or invalidcontexts of the object; from those, features charac-terizing the context are extracted, e.g.
words in awindow around the target term or syntactic links withit.
Given a new context, the classifier assesses its va-lidity with respect to the learned classification model.Classifiers in prior work were applied to determinerule applicability in a given context (t ?
r).
Train-ing a classifier for word paraphrasing, Kauchak andBarzilay (2006) used occurrences of the rule?s RHS aspositive context examples, and randomly picked neg-ative examples.
A similar approach was applied byDagan et al (2006), which used a single-class SVMto avoid selecting negative examples.
In both works,a resulting classifier represents a word with all itssenses intermixed.
Clearly, this poses no problem formonosemous words, but is biased towards the morecommon senses of polysemous words.
Indeed, Daganet al (2006) report a negative correlation between thedegree of polysemy of a word and the performance ofits classifier.
Connor and Roth (2007) used per-ruleclassifiers to produce a noisy training set for learninga global classifier for verb substitution.In this work we follow the classification-based ap-proach which seems appealing for several reasons.trtC h(r)hC r(t)C h(t)rhFigure 1: Left: An illustration of the CP relationships as in(Szpektor et al, 2008), with arrows indicting the contextmatching direction; Right: The application of classifiersto the tested contexts under our scheme.First, it allows seamlessly integrating various typesof information via classifiers?
features; unlike someof the above models, it is not inherently dependent onthe type of rules that are utilized and easily accom-modates to both lexical and lexical-syntactic rulesthrough the choice of features.
In addition, it doesnot rely on a predefined similarity measure and pro-vides flexibility in terms of model?s parameters.
Fi-nally, this approach captures the notion of direction-ality which is fundamental in textual inference, andis therefore better suited to applied inference thanpreviously proposed symmetric context models.In comparison to prior classification-based models,our approach addresses all three context matchesspecified by CP, rather than only the rule-text match.It is not limited to substitutable terms or even toterms with the same part of speech.
In addition, weavoid learning a classifier for all senses combined,but rather learn it for the specific intended meaning.2.3 Name-based Text CategorizationName-based TC (Gliozzo et al, 2009) is an unsu-pervised setting of Text Categorization in which theonly input provided is the category name, e.g.
trade,?mergers and acquisitions?
or guns.
When categorynames are ambiguous, e.g.
space, categories are notwell defined; thus, auxiliary information is expectedto accompany the name for disambiguation, such asa list of relevant senses or a category description.Typically, unsupervised TC consists of two steps.First, an unsupervised method is applied to an unla-beled corpus, automatically labeling some of the doc-uments to categories.
Then, the labeled documentsfrom the first step are used to train a supervised TCclassifier which is used to label any document in thetest set (Gliozzo et al, 2009; Downey and Etzioni,2009; Barak et al, 2009).22In this work we focus on the above unsupervisedstep.
Gliozzo et al (2009) addressed this task by rep-resenting both documents and categories by LSA vec-tors which implicitly capture contextual similaritiesbetween terms.
Each document was then assignedto the most similar category based on cosine simi-larity between the LSA vectors.
Barak et al (2009)required an occurrence of a term entailing the cat-egory name (or the category name itself) in orderto regard the category as a candidate for the docu-ment.
To assess the contextual validity of the match,they used LSA document-category similarity as in(Gliozzo et al, 2009).
For example, to classify a doc-ument into the category medicine, at least one lexicalentailment rule, e.g.
?drug?
medicine?, should bematched in the document.
Then, the validity of drugfor medicine in the matched document is assessed bythe LSA context model.
In this work we adopt Baraket al?s requirement for a match for the category inthe document, but address context matching in anentirely different way.Name-based TC provides a convenient setting forevaluating context matching approaches for two mainreasons.
First, all types of context matchings are real-ized in this application (see Section 3); second, as thehypothesis consists of a single term or a few terms,the TC gold standard annotation corresponds quitedirectly to the context matching task for lexical infer-ences; in other applications where longer hypothesesare involved, context matching performance may bemasked by other factors.3 Contextual Matches in TCWithin Name-based TC, the Textual Entailment ter-minology is mapped as follows: h is a term denotingthe category name (e.g.
merger or acquisition); t isa matched term in the document to be categorizedfrom which h may be inferred; and a match refersto an occurrence in the document of either h (directmatch) or the LHS of an entailment rule r whose RHSis a category name (indirect match).1Under the CP view, a context model needs to ad-dress the following three context matching caseswithin a TC setting.t?h: Assessing the validity of a match in the docu-ment with respect to the category?s intended meaning.1Note that t and h both refer here to individual terms.For example, the occurrence of the category namespace (in the sense of outer space) in ?the server ranout of disk space?
does not indicate a space-relatedtext, and should be dismissed by the context model.t?
r: This case refers to a rule match in the docu-ment.
A context model should ensure that the mean-ing of a match is compatible with that of the rule.For example, ?alien?
space?
is a valid rule for thespace category.
Yet, it should not be applied to ?TheUS welcomes a large number of aliens every year?,since alien in this sentence has a different meaningthan the intended meaning of the rule.r ?
h: The match between the intended meaningsof the category name and the RHS of the rule.
Forinstance, the rule ?room?
space?
is not suitable atall for the (outer) space category.4 A Classification-based Scheme for CPSzpektor et al (2008) introduced a vector-spacemodel to implement CP, in which the text t, the ruler and the hypothesis h share the same contextualrepresentation.
However, in CP, r, h and t have non-symmetric roles: the context of t should be tested asvalid for r and h and not vice versa, and the contextof r should be validated for h and not the other wayaround.
This stems from the need to consider direc-tionality in context matching.
For instance, a textabout football typically constitutes a valid contextfor the more general sports context, but not viceversa.
Indeed, directionality may be captured invector-space models by using a directional similaritymeasure (Kotlerman et al, 2010), but only symmetricmeasures were used in context matching work so far.Based on this distinction between the inferenceobjects?
roles, we present a novel scheme that usestwo types of classifiers to represent context:Ch: A classifier that identifies valid contexts for h. Ittests contexts of t (for t?
h matching) or r (forr ?
h matching), assigning them scores Ch(t)and Ch(r), respectively.Cr: A classifier that identifies valid contexts for ap-plying the rule r. It tests the context of t, assign-ing it a score Cr(t).Figure 1 (right) shows the classifiers scores whichare assigned to each of the matching types.23Hence, h always acts as the classifying object, t isalways the classified object, while r acts as both.
Con-text matching is quantified by the degree by whichthe classified object represents a valid context for theclassifying object in a given inference scenario.In comparison to the CP implementation in (Szpek-tor et al, 2008), our approach uses a unified modelwhich captures directionality in context matching.To instantiate the scheme, one needs to define theway training examples are obtained and processed.This may be done within supervised classification,where labeled examples are provided, or ?
as we doin this work ?
using self-supervised classifiers whichobtain training examples automatically.
We presentsuch an instantiation in Section 5, where a classifier istrained for each category and each rule.
When morecomplex hypotheses are involved, Ch classifiers canbe trained separately for each relevant part of thehypothesis, using the rest for disambiguation.A combination of the three model scores providesa final context matching score.
In Section 6 we sug-gest a way to combine the actual classification scoresas part of the integration of CP into TC, but othercombinations are plausible.
In particular, binary clas-sifications (valid vs. invalid) may be used as filters.That is, the context is classified as valid only if allrelevant models classify it as such.5 A Self-supervised Context ModelWe now turn to demonstrate how our classification-based scheme may be implemented.
The model be-low is exemplified on Name-based TC, but may beapplicable to other tasks, with few changes.5.1 Training-set generationOur implementation is self-supervised as we wantto integrate it within the unsupervised TC setting.That is, the classifiers automatically obtain trainingexamples for the classifying object (a category or arule) without relying on labeled documents.We obtain examples by querying the TC trainingcorpus with automatically-generated search queries.The difficulty lies in correctly constructing queriesthat will retrieve documents representing either validor invalid contexts for the classifying object.
To thisend, we retrieve examples through a gradual processin which the most accurate (least ambiguous) queryis used first and less accurate queries follow, until thedesignated number of examples is acquired.5.1.1 Obtaining positive examplesTo acquire positive training examples, we con-struct queries which are comprised of two mainclauses.
The first contains the seeds, terms whichcharacterize the classifying object.
Primarily, theseare the category name or the LHS of the rule.
The sec-ond consists of context words which are used whenthe seeds are polysemous, and are intended to assistdisambiguation.
When context words are used, atleast one seed and at least one context word mustbe matched to retrieve a document.
For example,given the highly ambiguous category name space,we first construct the query using only the monose-mous term outer space; if the number of retrieveddocuments does not meet the requirement, a secondquery may be constructed: (?outer space?
OR space)AND (infinite OR science OR .
.
.
).To generate a rule classifier Cr, we retrieve posi-tive examples as follows.
If the LHS term is monose-mous according to WordNet2, we first query usingthis term alone (e.g.
decrypt), and add its monose-mous synonyms and hyponyms if more examples arerequired (e.g.
decrypt OR decode).
If the LHS ispolysemous, we carry out Procedure 1.
Intuitively,this procedure tries to minimize ambiguity by usingmonosemous terms as much as possible; when poly-semous terms must be used, it tries to ensure there aremonosemous terms to disambiguate them.
Note thatentailment directionality is maintained throughoutthe process, as seeds are only expanded with morespecific (entailing) terms, while context words areonly expanded with more general (entailed) terms.Procedure 1 : Retrieval of Cr positive examplesApply sequentially until sufficient examples are obtained:1: Set the LHS as seed and the RHS?s monosemous syn-onyms, hypernyms and derivations as context words.2: Add monosemous synonyms and hyponyms of theLHS to the seeds.3: As in 2, but use polysemous terms as well.4: Add polysemous context words.Positive examples for category classifiers (Ch) areobtained through a similar procedure as for rule clas-2Terms not in WordNet are assumed monosemous.24sifiers.
If the category is part of a hierarchy, we alsouse the name of the parent category (e.g.
sport forrec.sport.hockey) as a context word.5.1.2 Obtaining negative examplesNegative examples are even more challenging toacquire.
In prior work negative examples were se-lected randomly (Kauchak and Barzilay, 2006; Con-nor and Roth, 2007).
We follow this method, butalso attempt to identify negative examples that aresemantically similar to the positive ones in order toimprove the discriminative power of the classifier(Smith and Eisner, 2005).
We do that by applyinga similar procedure which uses cohyponyms of theseeds, e.g.
baseball for hockey or islam for christian-ity.
Cohyponymy is a non-entailing relation; hence,by using it we expect to obtain semantically-related,yet invalid contexts.
If not enough negative exam-ples are retrieved using cohyponyms, we select theremaining required examples randomly.As the distribution of positive and negative ex-amples in the data is unknown, we set the ratio ofnegative to positive examples as a parameter of themodel, as in (Bergsma et al, 2008).5.1.3 Insufficient examplesWhen the number of training examples for a ruleor a category is below a certain minimum, the re-sulting classifier is expected to be of poor quality.This usually happens for positive examples in any ofthe following two cases: (i) the seed is rare in thetraining set; (ii) the desired sense of the seed is rarelyfound in the training set, and unwanted senses werefiltered by our retrieval query.
For instance, nazarenedoes not occur at all in the training set, and the classi-fier corresponding to the rule ?nazarene?
christian?cannot be generated.
On the other hand, cone doesappear in the corpus but not in the astrophysical sensethe rule ?cone?
space?
refers to.
In such cases werefrain from generating the classifier and use insteada default score of 0 for each classified object.
Theidea is that rare terms will also occur infrequently inthe test set, while cases where the term is found inthe corpus, but in a different sense than the desiredone, will be blocked.5.1.4 Feature extractionWe extract global and local lexical features that arestandard in WSD work.
Global features include allthe terms in the document or in the sentence in whicha match was found.
Local features are extractedaround matches of seeds which comprised the querythat retrieved the document.
These features includethe terms in a window around the match, and thenoun, verb, adjective and adverb nearest to the matchin either direction.
For randomly sampled negativeexamples, where no matched query terms exist, werandomly select terms in the document as ?matches?for local feature extraction.
If more than one match ofthe same term is found in a document, we assume one-sense-per-discourse (Gale et al, 1992) and jointlyextract features for all matches of the term.5.2 Applying the classifiersDuring inference, for each direct match in a docu-ment, the corresponding Ch is applied.
For an indi-rect match, the respective Cr is also applied.In addition, Ch is applied to the matched rules.Unlike t, a rule is not represented by a single text.Therefore, to test a rule?s match with the category,we randomly sample from the training set documentscontaining the rule?s LHS.
We apply Ch to each sam-pled example and compute the ratio of positive classi-fications.
The result is a score indicating the domain-specific probability of the rule to be applicable tothe category, and may be interpreted as an in-domainprior.
For instance, the rule ?check ?
hockey?
isassigned a score of 0.05, since the sense of checkas a hockey defense technique is rare in the corpus.On the other hand, non ambiguous rules, e.g.
?war-ship ?
ship?
are assigned a high probability (1.0),and so are rules whose LHS is ambiguous but its dom-inant sense in the training corpus is the same one therule refers to, e.g.
?margin?
earnings?
(0.85).We do not assign negative classifier scores to in-valid matches but rather set them to zero instead.
Thereason is that an invalid context only indicates thatthe term cannot be used for entailing the categoryname, but not that the document itself is irrelevant.6 CP for Text CategorizationCP may be employed in any inference-based task,but the integration with each task is somewhat dif-ferent and needs to be specified.
Below we presenta methodology for integrating CP into Name-basedText Categorization.25As in (Barak et al, 2009) (Barak09 below), werepresent documents and categories by term-vectorsin the following way: a document vector containsthe document terms; a category vector contains twosets of terms: C, the terms denoting the categoryname, and E , their entailing terms.
For example, oilis added to the vector of the category crude by therule ?oil?
crude?
(i.e.
crude ?
C and oil ?
E).Barak09 assigned equal values of 1 to all vectorentries.
We suggest integrating a CP-based contextmodel into TC by re-weighting the terms in the vec-tors, prior to determining the final document-categorycategorization score through vector similarity.
Givena category c, with term vector C, and a document dwith term vector D, the model re-weights vector en-tries of matching terms (i.e., terms in C ?D), basedon the validity of the context match.
Valid matchesshould be assigned with higher scores than invalidones, leading to higher overall vector similarity fordocuments with valid matches for the given category.Non-matching terms are ignored as their weights arecanceled out in the subsequent vector product.Specifically, the model assigns a new weightwD(u) to a matching term u in the document vec-tor D based on the model?s assessment of: (a) t?
h,the context match between the (match in the) doc-ument and the category; and (if an indirect match)(b) t ?
r, the context match between the documentand the rule ?u?
ci?, where ci ?
C. The model alsosets a new weight wC(v) to a term v in the categoryvector C based on the context match for r ?
h, be-tween the rule ?v ?
cj?
(cj ?
C) and the category.For instance, using our context matching scheme inTC, wD(u) is set to Ch(u) orCh(u)+Cr(u)2 for directand indirect matches, respectively; wC(v) is left as 1if v ?
C and set to Ch(v) when v ?
E .Barak09 assigned a single global context score toa document-category pair using the LSA representa-tions of their vectors.
In our approach, however, weconsider the actual matches from the three differentviews, hence the re-weighting of the vector entriesusing three model scores.7 Experimental Setting7.1 Datasets and knowledge resourcesFollowing (Gliozzo et al, 2009) and (Barak et al,2009), we evaluated our method on two standard TCdatasets: Reuters-10 and 20-Newsgroups.The Reuters-10 (R10, for short) is a sub-corpusof the Reuters-21578 collection3, constructed fromthe ten most frequent categories in the Reuters tax-onomy.
We used the Apte split of the Reuters-21578collection, often used in TC tasks.
The top 10 cate-gories include about 9,000 documents, split into train-ing (70%) and test (30%) sets.
The 20-Newsgroups(20NG) corpus is a collection of newsgroup postingsgathered from twenty different categories from theUsenet Newsgroups hierarchy4.
We used the ?by-date?
version of the corpus, which contains approxi-mately 20,000 documents partitioned (nearly) evenlyacross the categories and divided in advance to train-ing (60%) and test (40%) sets.As in (Gliozzo et al, 2009; Barak et al, 2009), weadjusted non-standard category names (e.g.
forsalewas renamed to sale) and manually specified for eachcategory its relevant WordNet senses.
The sense tag-ging properly defines the categories, and is expectedto accompany such hypotheses.
Other types of in-formation may be used for this purpose, e.g.
wordsfrom category descriptions, if such exist.We applied standard preprocessing (sentence split-ting, tokenization, lemmatization and part of speechtagging) to all documents in the datasets.
All terms,including those denoting category names and rules,are represented by their lemma and part of speech.As sources for lexical entailment rules we usedWordNet 3.0 (synonyms, hyponyms, derivationsand meronyms) and a Wikipedia-derived rule-base(Shnarch et al, 2009).
Unlike Barak09 we did notlimit the rules extracted from WordNet to the mostfrequent senses and used all rule types from theWikipedia-based resource.7.2 Self-supervised model tuningTuning of the self-supervised context model?s pa-rameters (number of training examples, negative topositive ratio, feature set and the way negative exam-ples are obtained) was performed over developmentsets sampled from the training sets.
Based on this tun-ing, some parameters varied between the datasets andbetween classifier types (Ch vs. Cr).
For example,3http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html4http://people.csail.mit.edu/jrennie/20Newsgroups/26selection of negative examples based on cohyponymswas found useful for Cr classifiers in R10, while ran-dom examples were used in the rest of the cases.We used SVMperf (Joachims, 2006) with a linearkernel and binary feature weighting.For querying the corpus we used the Lucene searchengine5 in its default setting.
Up to 150 positiveexamples were retrieved for each classifier, with 5examples set as the required minimum.
This resultedin generating 100% of the hypothesis classifiers forboth datasets and 95% and 70% of the rule classifiersfor R10 and 20NG, respectively.We computed Ch(r) scores based on up to 20 sam-pled instances.
If less than 2 examples were found inthe training set, we assigned an ?unknown?
contextmatch probability of 0.5, since a rare LHS occurrencedoes not indicate anything about its meaning in thecorpus.
Such cases constituted 2% (R10) and 11%(20NG) of the utilized rules.7.3 Baseline modelsTo provide a more meaningful comparison with priorwork, we focus on the first unsupervised step in thetypical Name-based TC flow, without the subsequentsupervised training.
Our goal is to improve the accu-racy of this first step, and we therefore compare ourcontext model?s performance to two unsupervisedmethods used by Barak09.The first baseline, denoted Barakno-cxt, is the co-sine similarity score between the document and cate-gory vectors where all terms are equally weighted toa score of 1.6 This baseline shows the performancewhen no context model is employed.The second baseline, denoted Barakfull, is a repli-cation of the state of the art context model for Name-based TC.
In this method, LSA vectors are con-structed for a document by averaging the LSA vectorsof its individual terms, and for a category by averag-ing the LSA vectors of the terms denoting its name.The categorization score of a document-category pairis set to be the product between the cosine similarityscore of the LSA vectors and the score given by theabove Barakno-cxt method.
We note that LSA-basedcontext models performed best also in (Gliozzo et al,2009) and (Szpektor et al, 2008).5http://lucene.apache.org6Other attempted weighting schemes, such as tf-idf, did notyield better performance.ModelReuters-10Accuracy P R F1Barakno-cxt 73.2 63.6 77.0 69.7Barakfull 76.3 68.0 79.2 73.2Class.-based 79.3 71.8 83.6 77.2Model20-NewsgroupsAccuracy P R F1Barakno-cxt 63.7 44.5 74.6 55.8Barakfull 69.4 50.1 82.8 62.4Class.-based 73.4 54.7 76.4 63.7Table 1: Evaluation results.All models were constructed based on the TC train-ing sets, using no external corpora.
The vocabularyconsists of terms that appear more than once in thetraining set.
The terms we consider include nouns,verbs, adjectives and adverbs, as well as nominalmulti-word expressions.8 Results and AnalysisGiven a document, all categories for which a lexicalmatch was found in the document are considered,and the document is classified to the highest scoringcategory.
If all categories are assigned non-positivescores, the document is not assigned to any of them.Based on this requirement that a document con-tains at least one match for the category, 4862document-category pairs were considered for clas-sification in R10 and 9955 pairs in 20NG.
We eval-uated our context model, as well as the baselines,based on the accuracy of these classifications, i.e.the percentage of correct decisions among the candi-date document-category pairs.
We also measured themodels?
performance in terms of micro-averaged pre-cision (P ), relative recall (R) and F1.
Like Barak09,recall is computed relative to the potential recall ofthe rule-set which provides the entailing terms.Table 1 presents the evaluation results.
As inBarak09, the LSA-based model outperforms the firstbaseline, supporting its usefulness as a context model.In both datasets our model outperformed the base-lines in terms of accuracy.
This result is statisticallysignificant with p < 0.01 according to McNemar?stest (McNemar, 1947).
Recall is lower for our modelin 20NG but F1 scores are higher for both datasets.These results indicate that the classification-basedcontext model provides a favorable alternative to the27RemovedReuters-10 20-NewsgroupsAccuracy F1 Accuracy F1- 79.3 77.2 73.4 63.7Ch(t) 76.2 72.3 71.9 61.0Cr(t) 80.5 77.6 74.3 64.5Ch(r) 78.4 75.7 73.1 63.4Table 2: Ablation tests results.state of the art LSA-based method.Table 2 presents ablation tests of our model.
Ineach test we measured the classification performancewhen one of the three classification scores is ignored.Clearly, Ch(t) is the most beneficial component, andin general the category classifiers help improvingoverall performance.
The limited performance of Crmay be related to higher ambiguity in rules relative tocategory names, resulting in noisier training data.
Inaddition, the small size of the training set limits thenumber of training examples for rule classifiers.
Thisproblem affects Cr more than Ch since, by nature,the corpus includes more occurrences of categorynames.
Still, Cr contributes to improved recall (thisfact is not visible in Table 2).The coverage of the utilized rule-set determinesthe maximal (absolute) recall that can be achievedby any model.
With the rule-set we used in this ex-periment, the recall upper bound was 59.1% for R10and 40.6% for 20NG.
However, rule coverage af-fects precision as well: In many cases documents areassigned to incorrect categories because the correctcategory is not even a candidate as no entailing termwas matched for it in the document.
For instance,a document with the sentence ?For sale or trade!!
!BMW R60US.
.
.
?
was classified by our method tothe category forsale, while its gold-standard categoryis motorcycles.
Yet, none of the rules in our rule-settriggered motorcycles as a candidate category for thisdocument.
Ideally, a context model would rule outall incorrect candidate categories; in practice even asingle low score for one of the competing categoriesresults in a false positive error in such cases (in addi-tion to the recall loss).
To reduce these problems weintend to employ additional knowledge resources infuture work.Our algorithm for retrieving training examplesturned out to be not sufficiently accurate, particularlyfor negative examples.
This is a challenging task thatrequires further research.
Although useful for someclassifier types, the use of cohyponyms may retrievepotentially positive examples as negative ones, sinceterms that are considered cohyponyms in WordNetare often perceived as near synonyms in commonusage, e.g.
buyout and purchase in the context ofacquisitions.
Likewise, using WordNet senses to de-termine ambiguity is also inaccurate.
Rare or toofine-grained senses, common in WordNet, cause aterm to be considered ambiguous, which in turn trig-gers the use of less accurate retrieval methods.
Forexample, auction has a bridge-related WordNet sensewhich is irrelevant for our dataset, but made the termbe considered ambiguous.
This calls for develop-ment of other methods for determining word ambigu-ity, which consider the actual usage of terms in thedomain rather than relying solely on WordNet.9 ConclusionsIn this paper we presented a generic classification-based scheme for comprehensively addressing con-text matching in textual inference scenarios.
Wepresented a concrete implementation of the proposedscheme for Name-based TC, and showed how CPdecisions can be integrated within the TC setting.Utilizing classifiers for context matching offersseveral advantages.
They naturally incorporate di-rectionality and allow integrating various types ofinformation, including ones not used in this worksuch as syntactic features.
Our results indeed supportthis approach.
Still, further research is required re-garding issues raised by the use of multiple classifiers,scalability in particular.Hypotheses in TC are available in advance.
Whilealso the case in other applications, it constitutes apractical challenge when hypotheses are given ?on-line?, like Information Retrieval queries, since classi-fiers will have to be generated on the fly.
We intendto address this issue in future work.Lastly, we plan to apply the generic classification-based approach to address context matching in otherinference-based applications.AcknowledgmentsThis work was partially supported by the Israel Sci-ence Foundation grant 1112/08 and the NEGEVproject (www.negev-initiative.org).28ReferencesLibby Barak, Ido Dagan, and Eyal Shnarch.
2009.
TextCategorization from Category Name via Lexical Refer-ence.
In HLT-NAACL (Short Papers).Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative Learning of Selectional Preference fromUnlabeled Text.
In In Proceedings of EMNLP.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3:993?1022, March.Michael Connor and Dan Roth.
2007.
Context SensitiveParaphrasing with a Global Unsupervised Classifier.
InProceedings of ECML.Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-morshtein, and Carlo Strapparava.
2006.
Direct WordSense Matching for Lexical Substitution.
In Proceed-ings of COLING-ACL.Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.2009.
Recognizing Textual Entailment: Rational, Eval-uation and Approaches.
Natural Language Engineer-ing, pages 15(4):1?17.Scott Deerwester, Scott Deerwester, Susan T. Dumais,George W. Furnas, Thomas K. Landauer, and RichardHarshman.
1990.
Indexing by Latent Semantic Anal-ysis.
Journal of the American Society for InformationScience, 41:391?407.Georgiana Dinu and Mirella Lapata.
2010.
Topic Modelsfor Meaning Similarity in Context.
In Proceedings ofColing 2010: Posters.Doug Downey and Oren Etzioni.
2009.
Look Ma, NoHands: Analyzing the Monotonic Feature Abstractionfor Text Classification.
In Proceedings of NIPS.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, and Com-munication).
The MIT Press.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One Sense per Discourse.
In Proceed-ings of the workshop on Speech and Natural Language.Alfio Gliozzo, Carlo Strapparava, and Ido Dagan.
2009.Improving Text Categorization Bootstrapping via Unsu-pervised Learning.
ACM Trans.
Speech Lang.
Process.,6:1:1?1:24, October.Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.Pas?ca.
2003.
Open-domain Textual Question Answer-ing Techniques.
Natural Language Engineering, 9:231?267, September.Thorsten Joachims.
2006.
Training Linear SVMs inLinear Time.
In Proceedings of the ACM Conferenceon Knowledge Discovery and Data Mining (KDD).David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings of HLT-NAACL.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional DistributionalSimilarity for Lexical Inference.
Natural LanguageEngineering, 16(4):359?389.Diana McCarthy and Roberto Navigli.
2009.
The EnglishLexical Substitution Task.
Language Resources andEvaluation, 43(2):139?159.Quinn McNemar.
1947.
Note on the Sampling Errorof the Difference between Correlated Proportions orPercentages.
Psychometrika, 12(2):153?157, June.Roberto Navigli.
2009.
Word Sense Disambiguation: ASurvey.
ACM Computing Surveys, 41(2):1?69.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timo-thy Chklovski, and Eduard Hovy.
2007.
ISP: LearningInferential Selectional Preferences.
In Proceedings ofNAACL-HLT.Marco Pennacchiotti, Roberto Basili, Diego De Cao, andPaolo Marocco.
2007.
Learning Selectional Prefer-ences for Entailment or Paraphrasing Rules.
In Pro-ceedings of RANLP.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A La-tent Dirichlet Allocation Method for Selectional Prefer-ences.
In Proceedings of ACL.Eyal Shnarch, Libby Barak, and Ido Dagan.
2009.
Ex-tracting Lexical Reference Rules from Wikipedia.
InProceedings of IJCNLP-ACL.Noah A. Smith and Jason Eisner.
2005.
ContrastiveEstimation: Training Log-linear Models on UnlabeledData.
In Proceedings of ACL.Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-berger.
2008.
Contextual Preferences.
In Proceedingsof ACL-08: HLT.29
