Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 984?993,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA CALL System for Learning Preposition UsageJohn LeeDepartment ofLinguistics and TranslationCity University of Hong Kongjsylee@cityu.edu.hkDonald SturgeonFairbank Centerfor Chinese StudiesHarvard Universitydjs@dsturgeon.netMengqi LuoDepartment ofLinguistics and TranslationCity University of Hong Kongmengqluo@cityu.edu.hkAbstractFill-in-the-blank items are commonly fea-tured in computer-assisted language learn-ing (CALL) systems.
An item displays asentence with a blank, and often proposesa number of choices for filling it.
Thesechoices should include one correct answerand several plausible distractors.
We de-scribe a system that, given an English cor-pus, automatically generates distractors toproduce items for preposition usage.We report a comprehensive evaluation onthis system, involving both experts andlearners.
First, we analyze the diffi-culty levels of machine-generated carriersentences and distractors, comparing sev-eral methods that exploit learner error andlearner revision patterns.
We show thatthe quality of machine-generated items ap-proaches that of human-crafted ones.
Fur-ther, we investigate the extent to whichmismatched L1 between the user and thelearner corpora affects the quality of dis-tractors.
Finally, we measure the system?simpact on the user?s language proficiencyin both the short and the long term.1 IntroductionFill-in-the-blank items, also known as gap-fill orcloze items, are a common form of exercise incomputer-assisted language learning (CALL) ap-plications.
Table 1 shows an example item de-signed for teaching English preposition usage.
Itcontains a sentence, ?The objective is to kick theball into the opponent?s goal?, with the preposi-tion ?into?
blanked out; this sentence serves as thestem (or carrier sentence).
It is followed by fourchoices for the blank, one of which is the key (i.e.,the correct answer), and the other three are dis-tractors.
These choices enable the CALL applica-tion to provide immediate and objective feedbackto the learner.A high-quality item must meet multiple re-quirements.
It should have a stem that is fluentand matches the reading ability of the learner; ablank that is appropriate for the intended peda-gogical goal; exactly one correct answer amongthe choices offered; and finally, a number of dis-tractors that seem plausible to the learner, and yetwould each yield an incorrect sentence.
Relyingon language teachers to author these items is timeconsuming.
Automatic generation of these itemswould not only expedite item authoring, but alsopotentially provide personalized items to suit theneeds of individual learners.
This paper addressestwo research topics:?
How do machine-generated items comparewith human-crafted items in terms of theirquality??
Do these items help improve the users?
lan-guage proficiency?For the first question, we focus on automaticgeneration of preposition distractors, comparingthree different methods for distractor generation.One is based on word co-occurrence in standardThe objective is to kick the ball theopponent?s goal.
(A) in(B) into(C) to(D) withTable 1: An automatically generated fill-in-the-blank item, where ?into?
is the key, and the otherthree choices are distractors.984corpora; a second leverages error annotations inlearner corpora; the third, a novel method, exploitslearners?
revision behavior.
Further, we investi-gate the effect of tailoring distractors to the user?snative language (L1).
For the second question,we measure users?
performance in the short andin the long term, through an experiment involvingten subjects, in multiple sessions tailored to theirproficiency and areas of weakness.Although a previous study has shown thatlearner error statistics can produce competitiveitems for prepositions on a narrow domain (Leeand Seneff, 2007), a number of research questionsstill await further investigation.
Through bothexpert and learner evaluation, we will comparethe quality of carrier sentences and the plausibil-ity of automatically generated distractors againsthuman-crafted ones.
Further, we will measure theeffect of mismatched L1 between the user and thelearner corpora, and the short- and long-term im-pact on the user?s preposition proficiency.
To thebest of our knowledge, this paper offers the mostdetailed evaluation to-date covering all these as-pects.The rest of the paper is organized as follows.Section 2 reviews previous work.
Section 3 out-lines the algorithms for generating the fill-in-the-blank items.
Section 4 gives details about the ex-perimental setup and evaluation procedures.
Sec-tion 5 analyzes the results.
Section 6 concludes thepaper.2 Previous Work2.1 Distractor generationMost research effort on automatic generation offill-in-the-blank items has focused on vocabularylearning.
In these items, the key is typically froman open-class part-of-speech (POS), e.g., nouns,verbs, or adjectives.To ensure that the distractor results in an incor-rect sentence, the distractor must rarely, or never,collocate with other words in the carrier sen-tence (Liu et al, 2005).
To ensure the plausibilityof the distractor, most approaches require it to besemantically close to the key, as determined by athesaurus (Sumita et al, 2005; Smith et al, 2010),an ontology (Karamanis et al, 2006), rules hand-crafted by experts (Chen et al, 2006), or context-sensitive inference rules (Zesch and Melamud,2014); or to have similar word frequency (Shei,2001; Brown et al, 2005).
Sakaguchi et al (2013)applied machine learning methods to select verbdistractors, and showed that they resulted in itemsthat can better predict the user?s English profi-ciency level.Less attention has been paid to items for closed-class POS, such as articles, conjunctions andprepositions, which learners also often find dif-ficult (Dahlmeier et al, 2013).
For these POS,the standard algorithms based on semantic relat-edness for open-class POS are not applicable.
Leeand Seneff (2007) reported the only previous studyon using learner corpora to generate items for aclosed-class POS.
They harvested the most fre-quent preposition errors in a corpus of Japaneselearners of English (Izumi et al, 2003), but per-formed an empirical evaluation with native Chi-nese speakers on a narrow domain.We expand on this study in several dimensions.First, carrier sentences, selected from the generaldomain rather than a specific one, will be analyzedin terms of their difficulty level.
Second, distrac-tor quality will be evaluated not only by learnersbut also by experts, who give scores based on theirplausibility; in contrast to most previous studies,their quality will be compared with the humangold standard.
Thirdly, the effect of mismatchedL1 will also be measured.2.2 Learner error correctionThere has been much recent research on auto-matic correction of grammatical errors.
Correc-tion of preposition usage errors, in particular, hasreceived much attention.
Our task can be viewedas the inverse of error correction ?
ensuring thatthe distractor yields an incorrect sentence ?
withthe additional requirement on the plausibility ofthe distractor.Most approaches in automatic grammar correc-tion can be classified as one of three types, ac-cording to the kind of statistics on which the sys-tem is trained.
Some systems are trained on ex-amples of correct usage (Tetreault and Chodorow,2008; Felice and Pulman, 2009).
Others aretrained on examples of pairs of correct and incor-rect usage, either retrieved from error-annotatedlearner corpora (Han et al, 2010; Dahlmeier et al,2013) or simulated (Lee and Seneff, 2008; Fos-ter and Andersen, 2009).
More recently, a sys-tem has been trained on revision statistics fromWikipedia (Cahill et al, 2013).
We build onall three paradigms, using standard English cor-985... kick the ball into the opponent?s goalVP head prep objprep pobjFigure 1: Parse tree for the carrier sentence in Ta-ble 1.
Distractors are generated on the basis of theprepositional object (?obj?)
and the NP/VP headto which the prepositional phrase is attached (Sec-tion 3).pora (Section 3.1), error-annotated learner corpora(Section 3.2) and learner revision corpora (Sec-tion 3.3) as resources to predict the most plausibledistractors.3 Item generationThe system assumes as input a set of English sen-tences, which are to serve as candidates for carriersentences.
In each candidate sentence, the systemscans for prepositions, and extracts two featuresfrom the linguistic context of each preposition:?
The prepositional object.
In Figure 1, forexample, the word ?goal?
is the prepositionalobject of the key, ?into?.?
The head of the noun phrase or verb phrase(NP/VP head) to which the prepositionalphrase (PP) is attached.
In Figure 1, the PP?into the opponent?s goal?
is attached to theVP head ?kick?.The system passes these two features to thefollowing methods to generate distractors.1Ifall three methods are able to return a distractor,the preposition qualifies to serve as the key.
Ifmore than one key is found, the system randomlychooses one of them.In the rest of this paper, we will sometimes ab-breviate these three methods as the ?Co-occur?
(Section 3.1), ?Error?
(Section 3.2), and ?Revi-sion?
(Section 3.3) methods, respectively.3.1 Co-occurrence methodProposed by Lee and Seneff (2007), this methodrequires co-occurrence statistics from a large cor-pus of well-formed English sentences.1We do not consider errors where a preposition should beinserted or deleted.Co-occurrence method (?Co-occur?)...
kicked the chair with ...... kicked the can with ...... with the goal of ...Learner error method (?Error?)...
kicked it <error>in</error> the goal.... kick the ball <error>in</error> theother team?s goal.Learner revision method (?Revision?)...
kick the ball to into his own goal.... kick the ball to towards his own goal.Table 2: The Co-occurrence Method (Section 3.1)generates ?with?
as the distractor for the carriersentence in Figure 1; the Learner Error Method(Section 3.2) generates ?in?
; the Learner RevisionMethod (Section 3.3) generates ?to?.This method first retrieves all prepositions thatco-occur with both the prepositional object and theNP/VP head in the carrier sentence.
These prepo-sitions are removed from consideration as distrac-tors, since they would likely yield a correct sen-tence.
The remaining candidates are those that co-occur with either the prepositional object or theNP/VP head, but not both.
The more frequentlythe candidate co-occurs with either of these words,the more plausible it is expected to appear to alearner.
Thus, the candidate with the highest co-occurrence frequency is chosen as the distractor.As shown in Table 2, this method generates thedistractor ?with?
for the carrier sentence in Fig-ure 1, since many instances of ?kick ... with?
and?with ... goal?
are attested.3.2 Learner error methodThis method requires examples of English sen-tences from an error-annotated learner corpus.The corpus must mark wrong preposition usage,but does not need to provide corrections for theerrors.This method first retrieves all PPs that have thegiven prepositional object and are attached to thegiven NP/VP head.
It then computes the frequencyof prepositions that head these PPs and are markedas wrong.
The one that is most frequently markedas wrong is chosen as the distractor.
As shown inTable 2, this method generates the distractor ?in?for the carrier sentence in Figure 1, since it is oftenmarked as an error.9863.3 Learner revision methodIt is expensive and time consuming to annotatelearner errors.
As an alternative, we exploit therevision behavior of learners in their English writ-ing.
This method requires draft versions of textswritten by learners.
In order to compute statis-tics on how often a preposition in an earlier draft(?draft n?)
is replaced with another one in the laterdraft (?draft n + 1?
), the sentences in successivedrafts must be sentence- and word-aligned.This method scans for PPs that have the givenprepositional object and are attached to the givenNP/VP head.
For all learner sentences in draft nthat contain these PPs, it consults the sentences indraft n+1 to which they are aligned; it retains onlythose sentences whose prepositional object and theNP/VP head remain unchanged, but whose prepo-sition has been replaced by another one.
Amongthese sentences, the method selects the prepositionthat is most frequently edited between two drafts.Our assumption is that frequent editing implies adegree of uncertainty on the part of the learner asto which of these prepositions is in fact correct,thus suggesting that they may be effective distrac-tors.
As shown in Table 2, this method generatesthe distractor ?to?
for the carrier sentence in Fig-ure 1, since it is most often edited in the given lin-guistic context.
This study is the first to exploit acorpus of learner revision history for item genera-tion.24 Experimental setupIn this section, we first describe our datasets (Sec-tion 4.1) and the procedure for item generation(Section 4.2).
We then give details on the expertevaluation (Section 4.3) and the learner evaluation(Section 4.4).4.1 DataCarrier sentences.
We used sentences in theEnglish portion of the Wikicorpus (Reese et al,2010) as carrier sentences.
To avoid selectingstems with overly difficult vocabulary, we rankedthe sentences in terms of their most difficult word.We measured the difficulty level of a word firstlywith the graded English vocabulary lists com-piled by the Hong Kong Education Bureau (EDB,2012); and secondly, for words not occurring in2A similar approach, using revision statistics inWikipedia, has been used for the purpose of correcting prepo-sition errors (Cahill et al, 2013).any of these lists, with frequency counts derivedfrom the Google Web Trillion Word Corpus.3Inorder to retrieve the prepositional object and theNP/VP head (cf.
Section 3), we parsed the Wiki-corpus, as well as the corpora mentioned below,with the Stanford parser (Manning et al, 2014).Co-occurrence method (?Co-occur?).
Thestatistics for the Co-occurrence method were alsobased on the English portion of Wikicorpus.Learner Revision method (?Revision?).
Weused an 8-million-word corpus of essay draftswritten by Chinese learners of English (Lee et al,2015).
This corpus contains over 4,000 essays,with an average of 2.7 drafts per essay.
The sen-tences and words between successive drafts havebeen automatically aligned.Learner Error method (?Error?).
In additionto the corpus of essay drafts mentioned above,we used two other error-annotated learner corpora.The NUS Corpus of Learner English (NUCLE)contains one million words of academic writingby students at the National University of Singa-pore (Dahlmeier et al, 2013).
The EF-CambridgeOpen Language Database (EFCAMDAT) containsover 70 million words from 1.2 million assign-ments written by learners from a variety of lin-guistic background (Geertzen et al, 2013).
A sub-set of the database has been error-annotated.
Wemade use of the writings in this subset that wereproduced by students from China and Russia.Human items (?Textbook?).
To provide a com-parison with human-authored items, we used thepractise tests for preposition usage offered in anEnglish exercise book designed for intermediateand advanced learners (Watcyn-Jones and Allsop,2000).
From the 50 tests in a variety of for-mats, we harvested 56 multiple-choice items, allof which had one key and three distractors.4.2 Item generation procedureWe gathered three sets of 400 carrier sentences, foruse in three evaluation sessions (see Section 4.4).Each sentence in Set 1 has one counterpart in Set2 and one counterpart in Set 3 that have the samekey, NP/VP head and prepositional object.
We willrefer to the items created from these counterpartcarrier sentences as ?similar?
items.
We will usethese ?similar?
items to measure the learning im-pact on the subjects.Each item has one key and distractors generated3http://norvig.com/ngrams/987by each of the three methods.
For about half of theitems, the three methods complemented one an-other to offer three distinct distractors.
In the otherhalf, two of the methods yielded the same dis-tractor, resulting in only two distractors for thoseitems.
In Set 1, for control purposes, 56 of theitems were replaced with the human items.4.3 Expert evaluation procedureTwo professional English teachers (henceforth, the?experts?)
examined each of the 400 items in Set1.
They annotated each item, and each choice inthe item, as follows.For each item, the experts labeled its diffi-culty level in terms of the preposition usage be-ing tested in the carrier sentence.
They did notknow whether the item was human-authored ormachine-generated.
Based on their experiencein teaching English to native speakers of Chi-nese, they labeled each item as suitable for thosein ?Grades 1-3?, ?Grades 4-6?, ?Grades 7-9?,?Grades 10-12?, or ?>Grade 12?.
We mappedthese five categories to integers ?
2, 5, 8, 11 and13, respectively ?
for the purpose of calculatingdifficulty scores.For each choice in the item, the experts judgedwhether it is correct or incorrect.
They did notknow whether each choice was the key or a dis-tractor.
They may judge one, multiple, or noneof the choices as correct.
For an incorrect choice,they further assessed its plausibility as a distractor,again from their experience in teaching English tonative speakers of Chinese.
They may label it as?Plausible?, ?Somewhat plausible?, or ?Obviouslywrong?.4.4 Learner evaluation procedureTen university students (henceforth, the ?learn-ers?)
took part in the evaluation.
They were allnative Chinese speakers who did not major in En-glish.
The evaluation consisted of three one-hoursessions held on different days.
At each session,the learner attempted 80 items on a browser-basedapplication (Figure 2).
The items were distributedin these sessions as follows.Session 1.
The 400 items in Set 1 were dividedinto 5 groups of 80 items, with 11 to 12 humanitems in each group.
The items in each group hadcomparable difficulty levels as determined by theexperts, with average scores ranging from 7.9 to8.1.
Each group was independently attempted bytwo learners.
The system recorded the items toFigure 2: Interface for the learner evaluation.
Onthe left, the learner selects a choice by tapping onit; on the right, the learner receives feedback.which the learner gave wrong answers; these willbe referred to as the ?wrong items?.
Among theitems to which the learner gave correct answers,the system randomly set aside 10 items; these willbe referred to as ?control items?.Session 2.
To measure the short-term impact,Session 2 was held on the day following Session 1.Each learner attempted 80 items, drawn from Set2.
These items were personalized according to the?wrong items?
of the individual learner.
For exam-ple, if a learner had 15 ?wrong items?
from Ses-sion 1, he or she then received 15 similar items4from Set 2.
In addition, he or she also receivedten items that were similar to the ?control items?from Session 1.
The remaining items were drawnrandomly from Set 2.
As in Session 1, the systemnoted the ?wrong items?
and set aside ten ?controlitems?.Session 3.
To test the long-term effect of theseexercises, Session 3 was held two weeks after Ses-sion 2.
Each learner attempted another 80 items,drawn from Set 3.
These 80 items were chosen inthe same manner as in Session 2.5 ResultsWe first report inter-annotator agreement betweenthe two experts on the difficulty levels of the car-rier sentences and the distractors (Section 5.1).
Wethen compare the difficulty levels of the human-and machine-generated items (Section 5.2).
Next,we analyze the reliability and difficulty5of the4See definition of ?similar?
in Section 4.2.5Another metric, ?validity?, measures the ability of thedistractor to discriminate between students of different profi-ciency levels.
This metric is relevant for items intended for988Figure 3: The difficulty level of the items in Set 1,as annotated by the experts.automatically generated distractors (Sections 5.3and 5.4), and the role of the native language (Sec-tion 5.5).
Finally, we measure the impact on thelearners?
preposition proficiency (Section 5.6).5.1 Inter-annotator agreementFor estimating the difficulty level of the prepo-sition usage in the carrier sentences, the expertsreached ?substantial?
agreement with kappa at0.765 (Landis and Koch, 1977).
In decidingwhether a choice is correct or incorrect, the expertsreached ?almost perfect?
agreement with kappaat 0.977.
On the plausibility of the distractors,they reached ?moderate?
agreement with kappa at0.537.
The main confusion was between the cate-gories ?Obviously wrong?
and ?Somewhat plausi-ble?.On the whole, expert judgment tended to cor-relate with actual behavior of the learners.
Fordistractors considered ?Plausible?
by both experts,63.6% were selected by the learners.
In contrast,for those considered ?Obviously wrong?
by bothexperts, only 11.8% attracted any learner.5.2 Carrier sentence difficultyFigure 3 shows the distribution of difficulty levelscores for the preposition usage in carrier sen-tences.
Most items were rated as ?Grades 7-9?,with ?Grades 4-6?
being the second largest group.A common concern over machine-generateditems is whether the machine can create or selectthe kind of carrier sentences that illustrate chal-lenging or advanced preposition usage, comparedto those crafted by humans.
In our system, thepreposition errors and revisions in the learner cor-pora ?
as captured by the NP/VP head and theassessment purposes (Brown et al, 2005; Sakaguchi et al,2013) rather than self-learning.prepositional object ?
effectively served as thefilter for selecting carrier sentences.
Some of theseerrors and revisions may well be careless or triv-ial mistakes, and may not necessarily lead to theselection of appropriate carrier sentences.To answer this question, we compared the diffi-culty levels of preposition usage in the machine-generated and human-crafted items.
The aver-age difficulty score for the human items was 8.7,meaning they were suitable for those in Grade 8.The average for the machine-generated items werelower, at 7.2.
This result suggests that our systemcan select carrier sentences that illustrate challeng-ing preposition usage, at a level that is only about1.5 grade points below those designed by humans.5.3 Distractor reliabilityA second common concern over machine-generated items is whether their distractors mightyield correct sentences.
When taken out of con-text, a carrier sentence often admits multiple pos-sible answers (Tetreault and Chodorow, 2008; Leeet al, 2009).
In this section, we compare the per-formance of the automatic distractor generationmethods against humans.A distractor is called ?reliable?
if it yields anincorrect sentence.
The Learner Revision methodgenerated the most reliable distractors6; on aver-age, 97.4% of the distractors were judged incor-rect by both experts (Table 3).
The Co-occurrencemethod ranked second at 96.1%, slightly betterthan those from the Learner Error method.
Manydistractors from the Learner Error method indeedled to incorrect sentences in their original con-texts, but became acceptable when their carriersentences were read in isolation.
Items with un-reliable distractors were excluded from the learnerevaluation.Surprisingly, both the Learner Revision and Co-occurrence methods outperformed the humans.Distractors in some of the human items did in-deed yield sentences that were technically correct,and were therefore deemed ?unreliable?
by the ex-perts.
In many cases, however, these distractorswere accompanied with keys that provided morenatural choices.
These items, therefore, remainedvalid.6The difference with the Co-occurrence method is not sta-tistically significant, in part due to the small sample size.989Method Reliable distractorCo-occur 96.1%Error 95.6%Revision 97.4%Textbook 95.8%Table 3: Distractors judged reliable by both ex-perts.5.4 Distractor difficultyIn the context of language learning, an item canbe considered more useful if one of its distractorselicits a wrong choice from the learner, who wouldthen receive corrective feedback.
In this section,we compare the ?difficulty?
of the distractor gen-erated by the various methods, in terms of theirability to attract the learners.Expert evaluation.
The two methods based onlearner statistics produced the highest-quality dis-tractors (Table 4).
The Learner Error method hadthe highest rate of plausible distractors (51.2%)and the lowest rate of obviously wrong ones(22.0%).
In terms of the number of distractorsconsidered ?Plausible?, this method significantlyoutperformed the Learner Revision method.7According to Table 4, all three automatic meth-ods outperformed the humans in terms of the num-ber of distractors rated ?Plausible?.
This compari-son, however, is not entirely fair, since the humanitems always supplied three distractors, whereasabout half of the machine-generated items sup-plied only two, when two of the methods returnedthe same distractor.An alternate metric is to compute the averagenumber of distractors rated ?Plausible?
per item.On average, the human items had 0.91 plausibledistractors; in comparison, the machine-generateditems had 1.27.
This result suggests that automaticgeneration of preposition distractors can performat the human level.Learner evaluation.
The most direct way toevaluate the difficulty of a distractor is to mea-sure how often a learner chose it.
The contrastis less clear cut in this evaluation.
Overall, thelearners correctly answered 76.2% of the machine-generated items, and 75.5% of the human items,suggesting that the human distractors were morechallenging.
One must also take into account,however, the fact that the carrier sentences are7p < 0.05 by McNemar?s test, for both expert annotators.Method Plausible Some- Obvious-what lyplausible wrongCo-occur 34.6% 31.5% 33.9%Error 51.2% 26.8% 22.0%Revision 45.4% 28.5% 26.1%Textbook 31.4% 34.2% 34.5%Table 4: Plausibility judgment of distractors by ex-perts.more difficult in the human items than in themachine-generated ones.
Broadly speaking, themachine-generated distractors were almost as suc-cessful as those authored by humans.Consistent with the experts?
opinion (Table 4),the Learner Error method was most successfulamong the three automatic methods (Table 5).
Thelearner selection rate of its distractors was 13.5%,which was significantly higher8than its closestcompetitor, the Learner Revision method, at 9.5%.The Co-occurrence method ranked last, at 9.2%.
Itis unfortunately difficult to directly compare theserates with that of the human distractors, whichthey were offered in different carrier sentences.5.5 Impact of L1We now turn our attention to the relation betweenthe native language (L1) of the user, and that ofthe learner corpora used for training the system.Specifically, we wish to measure the gain, if any,in matching the L1 of the user with the L1 of thelearner corpora.
To this end, for the Learner Er-ror method, we generated distractors from the EF-Cambridge corpus with two sets of statistics: oneharvested from the portion of the corpus with writ-ings by Chinese students, the others from the por-tion by Russian students.Expert evaluation.
Table 6 contrasts the ex-perts?
plausibility judgment on distractors gener-ated from these two sets.
Chinese distractors were8p < 0.05 by McNemar?s test.Method Learner selection rateCo-occur 9.2%Error 13.5%Revision 9.5%Table 5: Percentage of distractors selected bylearners.990Method Plausible Some- Obvious-what lyplausible wrongChinese 57.7% 24.0% 18.3%Russian 55.3% 22.0% 22.7%Table 6: Plausibility judgment of distractors gen-erated from the Chinese and Russian portions ofthe EF-Cambridge corpus, by experts.slightly more likely to be rated ?plausible?
thanthe Russian ones, and less likely to be rated ?ob-viously wrong?.9The gap between the two sets ofdistractors was smaller than may be expected.Learner evaluation.
The difference was some-what more pronounced in terms of the learners?behavior.
The learners selected Chinese distrac-tors, which matched their L1, 29.9% of the timeover the three sessions.
In contrast, they fell forthe Russian distractors, which did not match theirL1, only 25.1% of the time.
This result confirmsthe intuition that matching L1 improves the plau-sibility of the distractors, but the difference wasnonetheless relatively small.
This result suggetsthat it might be worth paying the price for mis-matched L1s, in return for a much larger pool oflearner statistics.5.6 Impact on learnersIn this section, we consider the impact of these ex-ercises on the learners.
The performance of thelearners was rather stable across all sessions; theiraverage scores in the three sessions were 73.0%,73.6% and 69.9%, respectively.
It is difficult, how-ever, to judge from these scores whether the learn-ers benefited from the exercises, since the compo-sition of the items differed for each session.Instead, we measured how often the learners re-tain the system feedback.
More specifically, if thelearner chose a distractor and received feedback(cf.
Figure 2), how likely would he or she suc-ceed in choosing the key in a ?similar?10item in asubsequent session.We compared the learners?
responses betweenSessions 1 and 2 to measure the short-term impact,and between Sessions 2 and 3 to measure the long-term impact.
In Session 2, when the learners at-9Data sparseness prevented us from generating both Chi-nese and Russian distractors for the same carrier sentencesfor evaluation.
These statistics are therefore not controlledwith regard to the difficulty level of the sentences.10See definition of ?similar?
in Section 4.2.Difficulty level Retention rateBelow 6 74.0%6-8 71.3%9-11 60.0%12 or above 25%Table 7: Retention rate for items at different levelsof difficulty.tempted items that were ?similar?
to their ?wrongitems?
from Session 1, they succeeded in choos-ing the key in 72.4% of the cases.11We refer tothis figure as the ?retention rate?, in this case overthe one-day period between the two sessions.
Theretention rate deteriorated over a longer term.
InSession 3, when the learners attempted items thatwere ?similar?
to their ?wrong items?
from Ses-sion 2, which took place two weeks before, theysucceeded only in 61.5% of the cases.12Further, we analyzed whether the difficultylevel of the items affected their retention rate.Statistics in Table 7 show that the rate variedwidely according to the difficulty level of the?wrong items?.
Difficult items, at Grade 12 orbeyond, proved hardest to learn, with a retentionrate of only 25%.
At the other end of the spec-trum, those below Grade 6 were retained 74% ofthe time.
This points to the need for the system toreinforce difficult items more frequently.6 ConclusionsWe have presented a computer-assisted languagelearning (CALL) system that automatically cre-ates fill-in-the-blank items for prepositions.
Wefound that the preposition usage tested in au-tomatically selected carrier sentences were onlyslightly less challenging than those crafted by hu-mans.
We compared the performance of threemethods for distractor generation, including anovel method that exploits learner revision statis-tics.
The method based on learner error statisticsyielded the most plausible distractors, followed bythe one based on learner revision statistics.
Theitems produced jointly by these automatic meth-ods, in both expert and learner evaluations, ri-valled the quality of human-authored items.
Fur-ther, we evaluated the extent to which mismatched11As a control, the retention rate for correctly answereditems in Session 1 was 80% in Session 2.12As a control, the retention rate for correctly answereditems in Session 2 was 69.0% in Session 3.991native language (L1) affects distractor plausibility.Finally, in a study on the short- and long-term im-pact on the learners, we showed that difficult itemshad lower retention rate.
In future work, we planto conduct larger-scale evaluations to further vali-date these results, and to apply these methods onother common learner errors.AcknowledgmentsWe thank NetDragon Websoft Holding Limitedfor their assistance with system evaluation, and thereviewers for their very helpful comments.
Thiswork was partially supported by an Applied Re-search Grant (Project no.
9667115) from CityUniversity of Hong Kong.ReferencesJonathan C. Brown, Gwen A. Frishkoff, and MaxineEskenazi.
2005.
Automatic Question Generationfor Vocabulary Assessment.
In Proc.
HLT-EMNLP.Aoife Cahill, Nitin Madnani, Joel Tetreault, and Di-ane Napolitano.
2013.
Robust Systems for Preposi-tion Error Correction using Wikipedia Revisions.
InProc.
NAACL-HLT.Chia-Yin Chen, Hsien-Chin Liou, and Jason S. Chang.2006.
FAST: An Automatic Generation System forGrammar Tests.
In Proc.
COLING/ACL InteractivePresentation Sessions.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a Large Annotated Corpus ofLearner English: The NUS Corpus of Learner En-glish.
In Proc.
8th Workshop on Innovative Use ofNLP for Building Educational Applications.EDB.
2012.
Enhancing English VocabularyLearning and Teaching at Secondary Level.http://www.edb.gov.hk/vocab learning sec.Rachele De Felice and Stephen Pulman.
2009.
Au-tomatic Detection of Preposition Errors in LearnerWriting.
CALICO Journal, 26(3):512?528.Jennifer Foster and ?istein E. Andersen.
2009.
Gen-ERRate: Generating Errors for Use in GrammaticalError Detection.
In Proc.
4th Workshop on Innova-tive Use of NLP for Building Educational Applica-tions.Jeroen Geertzen, Theodora Alexopoulou, and AnnaKorhonen.
2013.
Automatic Linguistic Annotationof Large Scale L2 Databases: The EF-CambridgeOpen Language Database (EFCAMDAT).
In Proc.31st Second Language Research Forum (SLRF).Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-Young Ha.
2010.
Using Error-annotated ESL Datato Develop an ESL Error Correction System.
InProc.
LREC.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi, and Hitoshi Isahara.
2003.
Auto-matic Error Detection in the Japanese Learners?
En-glish Spoken Data.
In Proc.
ACL.Nikiforos Karamanis, Le An Ha, and Ruslan Mitkov.2006.
Generating Multiple-Choice Test Items fromMedical Text: A Pilot Study.
In Proc.
4th Interna-tional Natural Language Generation Conference.J.
Richard Landis and Gary G. Koch.
1977.
TheMeasurement of Observer Agreement for Categor-ical Data.
Biometrics, 33:159?174.John Lee and Stephanie Seneff.
2007.
Automatic gen-eration of cloze items for prepositions.
In Proc.
In-terspeech.John Lee and Stephanie Seneff.
2008.
Correcting Mis-use of Verb Forms.
In Proc.
ACL.John Lee, Joel Tetreault, and Martin Chodorow.
2009.Human Evaluation of Article and Noun Number Us-age: Influences of Context and Construction Vari-ability.
In Proc.
Linguistic Annotation Workshop.John Lee, Chak Yan Yeung, Amir Zeldes, MarcReznicek, Anke L?udeling, and Jonathan Webster.2015.
CityU Corpus of Essay Drafts of EnglishLanguage Learners: a Corpus of Textual Revisionin Second Language Writing.
Language Resourcesand Evaluation, 49(3):659?683.Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao,and Shang-Ming Huang.
2005.
Applications ofLexical Information for Algorithmically ComposingMultiple-Choice Cloze Items.
In Proc.
2nd Work-shop on Building Educational Applications UsingNLP, pages 1?8.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP Natural Lan-guage Processing Toolkit.
In Proc.
ACL SystemDemonstrations, pages 55?60.Samuel Reese, Gemma Boleda, Montse Cuadros, Llu?
?sPadr?o, and German Rigau.
2010.
Wikicorpus: AWord-Sense Disambiguated Multilingual WikipediaCorpus.
In Proc.
LREC.Keisuke Sakaguchi, Yuki Arase, and Mamoru Ko-machi.
2013.
Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners.In Proc.
ACL.Chi-Chiang Shei.
2001.
FollowYou!
: An AutomaticLanguage Lesson Generation System.
ComputerAssisted Language Learning, 14(2):129?144.Simon Smith, P. V. S. Avinesh, and Adam Kilgar-riff.
2010.
Gap-fill Tests for Language Learners:Corpus-Driven Item Generation.
In Proc.
8th Inter-national Conference on Natural Language Process-ing (ICON).992Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-mamoto.
2005.
Measuring Non-native Speak-ers Proficiency of English by Using a Test withAutomatically-Generated Fill-in-the-Blank Ques-tions.
In Proc.
2nd Workshop on Building Educa-tional Applications using NLP.Joel Tetreault and Martin Chodorow.
2008.
The Upsand Downs of Preposition Error Detection in ESLWriting.
In Proc.
COLING.Peter Watcyn-Jones and Jake Allsop.
2000.
Test YourPrepositions.
Penguin Books Ltd.Torsten Zesch and Oren Melamud.
2014.
Auto-matic Generation of Challenging Distractors UsingContext-Sensitive Inference Rules.
In Proc.
Work-shop on Innovative Use of NLP for Building Educa-tional Applications (BEA).993
