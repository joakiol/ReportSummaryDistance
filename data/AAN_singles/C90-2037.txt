Using Test Suites in Evaluation of Machine Translation Systems.Margaret King and Kirsten FalkedalISSCO and ETIUniversity of Geneva54 rte des AcaciasCH- 1227 Genevae-mail: king@divsun.unige.chfalkedal@divsun.unige.ch1.
Background.As awareness of the increasing need fortranslations grows, readiness to considercomputerized aids to translation grows withit.
Recent years have seen increased fundingfor research in machine aids to translation,both in the public and the private sector,and potential customers are much inevidence in conferences devoted to work inthe area.
Activity in the area in its turnstimulate,; an interest in evaluationtechniques: sponsors would like to know iftheir money has been well spent, systemdevelopers would like to know how well theyfare compared to their rivals, and potentialcustomers need to be able to estimate thewisdom of their proposed investment.indeed, interest in evaluation extends beyondtranslation aids to natural languageprocessing as a whole, as a consequence ofattempts to facilitate storage and retrieval oflarge anaounts of information.
Concretemanifestations of this interest include aworkshop on evaluation in Philadelphia inlate 1988, and, in the particular field ofmachine translation, the publication of twobooks, the first \[4\] dedicated to the topic,the second \[7\] containing much discussion ofit.This paper is concerned with only one sub-topic within the large general area.
It isbased on work (carried out under mandatefor the Suissetra Association) aimed atdefining an evaluation strategy for atranslation service interested in acquiring aready-made translation system from acommercial firm, with no possibility ofexamining the internal workings of thesystem other than, perhaps, being able toglean clues from the content of dictionaryentries.
The type of test discussed here isonly one of several proposed, all of whichassume an evaluation set-up permitting aninitial up-dating of the dictionary to coverthe vocabulary of the test corpora, plus atleast one system up-date with subsequent re-execution of the tests.
It is hoped that theparticular type of test - the construction anduse of test suites - will be of interest o thenatural language processing community as awhole.2.
The evaluation strategy.Although space restrictions prevent anydetailed account of the whole proposal for anevaluation strategy, a brief discussion ofsome basic assumptions will help to set theconcerns of this paper in perspective.
Themost fundamental assumption is that no twopotential purchasers of a machine translationsystem are ever alike in their needs or intheir constraints.
Thus an essentialpreliminary to any evaluation is analysis ofwhat is actually needed and what theconstraints are.
A less obvious consequence isthat just as there can be no one "best"translation system, which is the most suitablefor all purposes, so there can be no fixedgeneral evaluation methodology, which willunequivocally determine which is the "best"system.
However, it is possible to distinguishdifferent factors which ~ be relevant tomaking an evaluation judgement, and tosuggest ways of collecting data on the basisof which a system's satisfactoriness withrespect to each of those factors may bejudged.
In any particular case, the evaluationcan then take account of the relativeimportance of the individual factors, eitherin deciding what data should be collected, orwhen making a final judgement.i 211In this paper, we concentrate on one methodof collecting data relevant to coverage of thesource language as evidenced by theacceptability of the translations produced forspecific test cases, to the treatment ofspecific translational problems, and, to someextent, to the ease with which the systemcan be modified or extended.
The fullproposal further considers factors likeoperational behaviour, need for and ease ofdictionary up-dating, post-editing andthrough-put time, user reactions etc., all ofthem factors whose interaction determinesjudgements on speed and efficiency.Once a particular factor has been retained asimportant in the particular context ofevaluation, our proposals require theconstruction of test materials to be used inobtaining the relevant data.
This assumes theavailability of someone with at least aknowledge of the languages concerned, oflinguistics and preferably some experience ofmachine translation.
Actual practice, asdemonstrated by the reports of evaluationsalready carried out, reveals the vulnerabilityof this assumption.
Nonetheless, we havechosen to retain it, on the grounds thatcommon sense must eventually prevail; it is acritical assumption for the rest of this paper.For the sake of brevity, we shall refer to thisperson as "the evaluator" in what follows.Also for the sake of brevity, we shall neglectthe special questions raised by systemsrelying on pre-editing or controlled input,and by inter-active systems.3.
Some Standard Evaluation Measures andtheir Weaknesses.Three kinds of metrics have prevailed in pastevaluations of machine translation systems.First, a very common technique has been toobtain ratings on some scale for aspects ofquality such as intelligibility, fidelity orclarity.
A second common approach has beento count the number of errors, typically bysimply counting the number of correctionsmade by the post-editor(s).
Thirdly, theperception that some errors are moreimportant than others has led to attempts toclassify errors according to pre-establishedclassification schemes.Each of these metrics has its own set ofinherent weaknesses, which we shall not gointo here.
(A detailed discussion can befound in \[5\] and \[l\]).
Beyond these inherentweaknesses, however, they all suffer fromthe same two major deficiences: the resultingdata, whatever their reliability, do notprovide the kind of information necessaryfor an assessment of the actual acceptabilityof the translation quality to the users, bethey readers or post-editors of the rawmachine output; moreover, these metrics donot provide the slightest hint about the easewith which the system can be extended ormodified, factors which have acquiredincreasing importance along with growinginsights into the slow but permanentlydeveloping nature of machine translationsystems.4.
The problem with trying to be moreinformative.A natural reaction to the above is to proposesetting up a series of systematically organisedtest inputs to test at least the syntacticcoverage of the system, along the lines of thetest suite set up at Hewlett Packard forsystematic testing of English languageinterfaces to data bases \[2\].The major problem with setting up testsuites of this kind is that of interactionbetween different linguistic phenomena.
Thestandard solution is to reduce the linguisticcomplexity of all items other than the itemof direct interest to an absolute minimum.Thus a sentence like "John gave Mary abook" would be used to test correct treatmentof ditransitive verbs, rather than, say, asentence with complex noun phrases or witha modal included in the verbal group.Even when a test suite is designed to testonly coverage of syntactic structures in asingle language, its construction is a lengthyand delicate task, requiring much linguisticinsight; adding even rudimentary testing ofsemantic phenomena would seriously increasethe complexity of the task.
When the systemto be tested is a translation system yet morecomplications arise, even if testing is limitedto testing syntactic coverage.The most serious of these occurs alreadyduring the construction of the test suite,where the fact that the "proof" of a correcttreatment of a test input is its intended - orat least an acceptable - translation into thetarget language imposes non-negligeableconstraints on the sub-structures, andespecially the vocabulary used: the test212 2input,,; should not only be adequate fortesting the source language, but alsotranslationally unproblematic, in the sensethat they do not introduce difficultiesirrelevant to the problem tested.
As aconcrete example, imagine that a test input isdesigned to test the treatment of verbs whichtake an infinitive verbal complement.
If theEngli,;h test input is "The chairman decidesto come", the corresponding French sentenceis equally simple, "Le pr6sident d6cide devenir", but if the English is "The chairmanexpects to come", the French equivalent of"expect" imposes a completely differentstructure ("Le pr6sident s'attend a ce qu'ilvienne (lui-m~me)"), and, if tile outputtranslation is incorrect, it may be difficult todetermine whether the reason is a generallywrong treatment of tile class of Englishverbs., or whether it is something specific to"expect".Thus, test inputs designed primarily toillumJinate the system's treatment of specificsource language phenomena should avoid theintroduction of "noise" triggered by aninjudicious choice of lexical material.However, since translational difficultiesbetween any given pair of languages doexist, a test suite for testing a machinetranslation system will have to include asubstantial component of eontrastively basedtest inputs, an area which has received farless investigation than mono-lingual syntax,and which is specific for each language pair.Thus, although a translation test suite for agiven pair of languages would be of greatutility to the community at large, itsconstruction and debugging is a long termproject, which no individual responsible fordefining test material in a limited time canhope to undertake.
Building test suites isperhaps one of the areas, like text-encodingand data collection, best undertaken ascollaborative work.Furthermore, given our basic assumption thatno two evaluations are alike as to the needsand constraints which determine whether asystem will or will not prove acceptable, thefact that general test suites do not, by theirnature, reflect the particular characteristicsof the application actually envisaged, maylimit their usefulness.Below, we try to suggest a way ofovercoming some of these difficulties, andalso of exploiting what seems to us the realvalue of using test suites, that is, theirability to serve as indicators of a system'spotential for improvement.S.
A Productive Compromise.Despite all tile difficulties, there is no wayround the fact that data on a system'scoverage of the source language, on itsability to produce acceptable translations andon its improvability are crucial input to anyjudgement of its suitability in a particularenvironment.
Somewhat paradoxically, wetherefore suggest hat an appropriate way outof the dilemma is to construct not one testsuite, but two, each in turn divided into atest suite concerned with syntactic coverageof the source language and one concernedwith specific translational problems.The first of these is based on (computeraided) study of a substantial bi-lingualcorpus of the kinds of texts the system willactually be expected to tackle.
It will be usedto collect data relevant to the system's abilityto fulfill the needs currently determined.The part of this test suite concerned withsource language coverage will includestructures which are expected to beunproblematic as well as structures known tobe likely to cause problems.
Particularattention should be paid to structures whichare specific to the corpus, for example,orders in French manuals expressed by theinfinitive, and to any deviations fromstandard grammar, for example, "Bonnesconnaissances d'Allemand" as a completesentence in a job advertisement.
If possible,the relative importance of any givenstructure on the evidence of its frequency inthe corpus will be indicated.
For reasonswhich will become clear later, the test suiteshould comprise at least two test inputs foreach structure selected, organised in twoparallel lists, referred to as the A list and theB list below.Every test input, in all of the test suites,should be accompanied by an acceptabletranslation.
This does not, of course, implythat the translation constitutes the onlyacceptable output from the system, but theexistence of a pre-defined set of translationswill help in maintaining a consistentjudgement of acceptability during the datacollection exercise.213The second section of the test suite concernstranslational problems.
The first goal is toinclude inputs based on mismatches betweenthe two languages.
Such mismatches mayrange from differences in the lexicons of thetwo languages, different behaviour ofotherwise corresponding lexical items (classicexamples are "like" vs."plaire"), to muchwider ranging differences such as a lack ofcorrespondence in the use of tenses or in theuse of articles.
Secondly, inputs will beincluded to test the system's treatment oflexical and structural ambiguity.
Once again,though, the choice of what test inputs toinclude will be informed by study of thecorpus of actual texts and their translations:no attempt should be made to think up orimport from the literature tricky examples.Here, too, A and B lists of examples shouldbe produced for all phenomena other thanthose dealing with idiosyncratic behaviour ofindividual lexical items.Where the aim of the first test suite is tocollect data on the system's ability (presentand future) to fulfill the needs currentlydetermined, the aim of the second is,broadly speaking, to collect data on whatmore or what else the system is capable of,to give some idea of to what degree thesystem could be used to treat satisfactorilytexts other than those seen as its immediatedomain of application.
This idea will,though, necessarily be more impressionisticthan founded on exhaustive vidence.The test suite will again be divided into asection aimed at looking at source languagecoverage and a section examining translationproblems.
Obvious sources of inspirationinclude those few test suites alreadyelaborated and the literature, includingprescriptive grammars and translators' aids.As before, A and B lists of examples houldbe produced.6.
Using the test suites.As mentioned previously, the test suites areonly one type of test in a full data collectionstrategy which space constraints prevent usfrom even outlining here, beyond indicatingthat, as a function of other parts of thestrategy, the dictionaries will already havebeen up-dated to cover the vocabularyneeded for all of the tests below.First, the test suite based on actual texts issubmitted to the system, and the resultingoutputs classified by the evaluator asacceptable or unacceptable, with no attemptmade at more refined classification.
That is,no attempt is made either to count mistakesor to classify them individually.The results will give a first indication ofhow well the system can deal with the textsfor which it is intended.
However, it istotally unrealistic to imagine that any ready-made system will produce only acceptableresults without further modification.
Giventhe assumption that the dictionaries containall the necessary vocabulary, the root causeof unacceptable outputs must lie ininadequacies in the dictionary coding orelsewhere in the system.
As a general rule(though not a universal truth) dictionaryinformation is quicker and easier to modifythan other parts of the system.
Thus, data onwhich of the unacceptable outputs is theresult of dictionary inadequacies, which not,is likely to be of considerable interest.The system manufacturer is therefore giventhe A-list inputs and their results, togetherwith the evaluator's classification of theiracceptability and asked to sort theunacceptable outputs according to the abovecriterion.
He is then given an agreed lapse oftime in which he may attempt to remedyerrors said to be due to dictionary coding.The whole test suite is then re-submitted, inorder to check improvement/deteriorationrelative to the first run.
The role of the B-lists (not used by the manufacturer forcorrections) becomes evident here: the B-listexamples serve as a control corpus, sincechanges in their treatment will be indicativeof the carry-over effects of the correctionsbased on the A-list.
For example, they willindicate whether changes have only a localeffect or whether the effects extend to wholeclasses of lexical items.
This procedure maybe repeated if time allows and if theevaluator considers it useful.The use of the non-corpus based test suite isessentially similar.
The only variation is that,in an attempt to develop intuitions about thecapacities of the system, the evaluator willattempt an independent estimate of which,amongst the unacceptable outputs in the A-list, are the consequence of inadequatedictionary coding, checking his estimateagainst that of the system manufacturer.214 4Even after modification of the dictionarycodin\[g, it is probable that some inputs in thetwo test suites will still give rise tounacceptable outputs.
The next step is to usethese remaining "intractable" inputs toacquire data on the system's extensibility.
Ifa technical description of the system'sprinciples is available, an experiencedevaluator will probably be able to makeinformed guesses about the system'sextensibility.
In many cases though, themanufacturer may be unwilling to give thenecessary information, and, in any case,some empirical evidence is required toconfirm or disconfirm the evaluator'sguesses.We therefore suggest taking the remainingintractable cases in the A-lists of both testsuites, and asking the manufacturer toclassify them according to his estimate ofhow long it would take to modify the systemto deal with each case, e.g.- less than a week- less than a month- more than a month- means of correction not known.As before, the manufacturer should be askedto demonstrate the reliability of his estimateby actually arranging for the modificationsto be carried out for some agreed number ofthe problem inputs, the actual choice ofinputs being left to the evaluator.
Both testsuites (B-lists included) are then re-executedand re-analyzed for improvements anddeteriorations.
Once again, the B-lists serveas a control corpus indicating carry-overeffects.7.
Conclusion.In summary, the test suites are used toproduce lists of inputs the system can andcan not deal with, and evidence on thedistribution of problem cases across fiveclasses, the first due to dictionary coding,the remaining four based on how long itwould take to improve or extend the systemto deal succesfully with the problem.
The useof a corpus based test suite adapts the testingto the actual foreseen needs, whilst the useof a non text specific test suite providesinformation on what more or what else thesystem might do.
Systematic testing of sourcelanguage structures is separated fromcontrastively based testing of translationalproblems, and the use of A and B lists givesan indication of how changes intended toimprove the treatment of specific problemsmay produce unwanted side effects orintroduce larger benefits.The only qualitative (and thereforesubjectivity prone) measure involved is theestimate of the acceptability of thetranslations produced, and even here, somedefence against bias is provided by ensuringthat each test input is accompanied by a pre-specified acceptable translation.
However, asargued earlier, straightforward lists of what asystem can or cannot do will not necessarilyprove sufficiently informative when forminga judgement about a system's overallacceptability.The main virtue of the test suite approach isnot, then, its relative objectivity, butflexibility in accounting for both thecompetence and the user-relatedperformance of the system under test.Furthermore, it furnishes a basis forjudgements on the potential for and costeffectiveness of systematic improvement ofthe system.
When several systems are to beevaluated for the same application, thisprocedure can be used to provide reliablecomparisons.
Finally, the raw data obtainedlend themselves readily to a variety ofadditional quantifications which decisionmakers may find informative, as well as tomore refined linguistic analysis of potentialvalue to the system developers.Acknowledgement.The authors would like to record theirgratitude to Ulrich Heid and JonathanSlocum, who read draft reports, offeredmuch constructive criticism, and providedgenerous opportunities for discussion.Se lected  B ib l iography .\[1\] Falkedal, K. Evaluation Methods forMachine Translation Systems: An HistoricalSurvey and A Critical Account.
ISSCO:Interim Report to Suissetra.
Forthcoming.\[2\] Flickinger, D. et al HP-NL Test Suite,(as of June 30, 1987).\[3\] King, Margaret.
A Practical Guide to theEvaluation o/ Machine Translation Systems.ISSCO.
Intermediate Report to Suissetra,Feb., 1989.
Revised version in preparation.5 215\[4\] Lehrberger, J. and Bourbeau, L. MachineTranslation: Linguistic Characteristics o /MTSystems and General Methodology ofEvaluation.
John Benjamin.
1988.\[5\] Slocum, J. and Bennet, W. An Evaluationo/ METAL: the LRC Machine TranslationSystem.
Second Conference of the EuropeanChapter of the ACL, 1985.\[6\] Van Slype, G. Critical Study o/ Methodsfor Evaluating the Quality o/ MachineTranslation Systems.
Bureau Marcel VanDijk, Bruxelles and CCE, 1979.\[7\] Vasconcellos, M.
(ed.)
Technology asTranslation Strategy.
SUNY, 1988.\[8\] Wilks, Y. and LATSEC Inc. ComparativeTranslation Quality Analysis, Final Report,Latsec Inc., 1979.216 6
