Using Syntactic Information to Extract Relevant Terms for Multi-DocumentSummarizationEnrique Amigo?
Julio Gonzalo V?
?ctor Peinado Anselmo Pen?as Felisa VerdejoDepartamento de Lenguajes y Sistemas Informa?ticosUniversidad Nacional de Educacio?n a Distanciac/ Juan del Rosal, 16 - 28040 Madrid - Spainhttp://nlp.uned.esAbstractThe identification of the key concepts in a set ofdocuments is a useful source of information forseveral information access applications.
We areinterested in its application to multi-documentsummarization, both for the automatic genera-tion of summaries and for interactive summa-rization systems.In this paper, we study whether the syntactic po-sition of terms in the texts can be used to predictwhich terms are good candidates as key con-cepts.
Our experiments show that a) distanceto the verb is highly correlated with the proba-bility of a term being part of a key concept; b)subject modifiers are the best syntactic locationsto find relevant terms; and c) in the task of auto-matically finding key terms, the combination ofstatistical term weights with shallow syntacticinformation gives better results than statisticalmeasures alone.1 IntroductionThe fundamental question addressed in this articleis: can syntactic information be used to find thekey concepts of a set of documents?
We will pro-vide empirical answers to this question in a multi-document summarization environment.The identification of key terms out of a set of doc-uments is a common problem in information accessapplications and, in particular, in text summariza-tion: a fragment containing one or more key con-cepts can be a good candidate to be part of a sum-mary.In single-document summarization, key terms areusually obtained from the document title or head-ing (Edmundson, 1969; Preston, 1994; Kupiec etal., 1995).
In multi-document summarization, how-ever, some processing is needed to identify key con-cepts (Lin and Hovy, 2002; Kraaij et al, 2002;Schlesinger et al, 2002).
Most approaches arebased on statistical criteria.Criteria to elaborate a manual summary depend,by and large, on the user interpretation of both theinformation need and the content of documents.This is why this task has also been attempted froman interactive perspective (Boguraev et al, 1998;Buyukkokten et al, 1999; Neff and Cooper, 1999;Jones et al, 2002; Leuski et al, 2003).
A standardfeature of such interactive summarization assistantsis that they offer a list of relevant terms (automati-cally extracted from the documents) which the usermay select to decide or refine the focus of the sum-mary.Our hypothesis is that the key concepts of a doc-ument set will tend to appear in certain syntacticfunctions along the sentences and clauses of thetexts.
To confirm this hypothesis, we have useda test bed with manually produced summaries tostudy:?
which are the most likely syntactic functionsfor the key concepts manually identified in thedocument sets.?
whether this information can be used to auto-matically extract the relevant terms from a setof documents, as compared to standard statis-tical term weights.Our reference corpus is a set of 72 lists of keyconcepts, manually elaborated by 9 subjects on8 different topics, with 100 documents per topic.It was built to study Information Synthesis tasks(Amigo et al, 2004) and it is, to the best ofour knowledge, the multi-document summarizationtestbed with a largest number of documents pertopic.
This feature enables us to obtain reliablestatistics on term occurrences and prominent syn-tactic functions.The paper is organized as follows: in Section 2we review the main approaches to the evaluationof automatically extracted key concepts for summa-rization.
In Section 3 we describe the creation of thereference corpus.
In Section 4 we study the correla-tion between key concepts and syntactic function intexts, and in Section 5 we discuss the experimentalresults of syntactic function as a predictor to extractkey concepts.
Finally, in Section 6 we draw someconclusions.2 Evaluation of automatically extractedkey conceptsIt is necessary, in the context of an interactive sum-marization system, to measure the quality of theterms suggested by the system, i.e., to what extentthey are related to the key topics of the documentset.
(Lin and Hovy, 1997) compared different strate-gies to generate lists of relevant terms for summa-rization using Topic Signatures.
The evaluation wasextrinsic, comparing the quality of the summariesgenerated by a system using different term lists asinput.
The results, however, cannot be directly ex-trapolated to interactive summarization systems, be-cause the evaluation does not consider how informa-tive terms are for a user.From an interactive point of view, the evaluationof term extraction approaches can be done, at least,in two ways:?
Evaluating the summaries produced in the in-teractive summarization process.
This optionis difficult to implement (how do we evaluatea human produced summary?
What is the ref-erence gold standard?)
and, in any case, it istoo costly: every alternative approach wouldrequire at least a few additional subjects per-forming the summarization task.?
Comparing automatically generated term listswith manually generated lists of key concepts.For instance, (Jones et al, 2002) describes aprocess of supervised learning of key conceptsfrom a training corpus of manually generatedlists of phrases associated to a single docu-ment.We will, therefore, use the second approach,evaluating the quality of automatically generatedterm lists by comparing them to lists of key con-cepts which are generated by human subjects after amulti-document summarization process.3 Test bed: the ISCORPUSWe have created a reference test bed, the ISCOR-PUS1 (Amigo et al, 2004) which contains 72 man-ually generated reports summarizing the relevant in-formation for a given topic contained in a large doc-ument set.For the creation of the corpus, nine subjects per-formed a complex multi-document summarization1Available at http://nlp.uned.es/ISCORPUS.task for eight different topics and one hundred rele-vant documents per topic.
After creating each topic-oriented summary, subjects were asked to make alist of relevant concepts for the topic, in two cate-gories: relevant entities (people, organizations, etc.
)and relevant factors (such as ?ethnic conflicts?
asthe origin of a civil war) which play a key role inthe topic being summarized.These are the relevant details of the ISCORPUStest bed:3.1 Document collection and topic setWe have used the Spanish CLEF 2001-2003 newscollection testbed (Peters et al, 2002), and selectedthe eight topics with the largest number of docu-ments manually judged as relevant from the CLEFassessment pools.
All the selected CLEF topicshave more than one hundred documents judged asrelevant by the CLEF assessors; for homogeneity,we have restricted the task to the first 100 docu-ments for each topic (using a chronological order).This set of eight CLEF topics was found to havetwo differentiated subsets: in six topics, it is neces-sary to study how a situation evolves in time: theimportance of every event related to the topic canonly be established in relation with the others.
Theinvasion of Haiti by UN and USA troops is an ex-ample of such kind of topics.
We refer to them as?Topic Tracking?
(TT) topics, because they are suit-able for such a task.
The other two questions, how-ever, resemble ?Information Extraction?
(IE) tasks:essentially, the user has to detect and describe in-stances of a generic event (for instance, cases ofhunger strikes and campaigns against racism in Eu-rope in this case); hence we will refer to them as IEsummaries.3.2 Generation of manual summariesNine subjects between 25 and 35 years-old were re-cruited for the manual generation of summaries.
Allsubjects were given an in-place detailed descriptionof the task, in order to minimize divergent interpre-tations.
They were told they had to generate sum-maries with a maximum of information about ev-ery topic within a 50 sentence space limit, using amaximum of 30 minutes per topic.
The 50 sentencelimit can be temporarily exceeded and, once the 30minutes have expired, the user can still remove sen-tences from the summary until the sentence limit isreached back.3.3 Manual identification of key conceptsAfter summarizing every topic, the following ques-tionnaire was filled in by users:?
Who are the main people involved in the topic??
What are the main organizations participating in the topic??
What are the key factors in the topic?Users provided free-text answers to these ques-tions, with their freshly generated summary at hand.We did not provide any suggestions or constraintsat this point, except that a maximum of eight slotswere available per question (i.e., a maximum of8X3 = 24 key concepts per topic, per user).This is, for instance, the answer of one user fora topic about the invasion of Haiti by UN and USAtroops:People OrganizationsJean Bertrand Aristide ONU (UN)Clinton EEUU (USA)Raoul Cedras OEA (OAS)Philippe BiambiMichel Josep FrancoisFactorsmilitares golpistas (coup attempting soldiers)golpe militar (coup attempt)restaurar la democracia (reinstatement of democracy)Finally, a single list of key concepts is generatedfor each topic, joining all the answers given by thenine subjects.
These lists of key concepts constitutethe gold standard for all the experiments describedbelow.3.4 Shallow parsing of documentsDocuments are processed with a robust shallowparser based in finite automata.
The parser splitssentences in chunks and assigns a label to everychunk.
The set of labels is:?
[N]: noun phrases, which correspond tonames or adjectives preceded by a determiner,punctuation sign, or beginning of a sentence.?
[V]: verb forms.?
[Mod]: adverbial and prepositional phrases,made up of noun phrases introduced by an ad-verb or preposition.
Note that this is the mech-anism to express NP modifiers in Spanish (ascompared to English, where noun compound-ing is equally frequent).?
[Sub]: words introducing new subordinateclauses within a sentence (que, cuando, mien-tras, etc.).?
[P]: Punctuation marks.This is an example output of the chunker:Previamente [Mod] ,[P]el presidente Bill Clinton [N] hab?
?a di-cho [V] que [Sub] tenemos [V] la obligacion [N] de cambiar lapol?
?tica estadounidense [Mod] que [Sub] no ha funcionado [V] enHait??
[Mod].
[P]Although the precision of the parser is limited,the results are good enough for the statistical mea-sures used in our experiments.4 Distribution of key concepts in syntacticstructuresWe have extracted empirical data to answer thesequestions:?
Is the probability of finding a key concept cor-related with the distance to the verb in a sen-tence or clause??
Is the probability of finding a key concept in anoun phrase correlated with the syntactic func-tion of the phrase (subject, object, etc.)??
Within a noun phrase, where is it more likelyto find key concepts: in the noun phrase head,or in the modifiers?We have used certain properties of Spanish syn-tax (such as being an SVO language) to decidewhich noun phrases play a subject function, whichare the head and modifiers of a noun phrase, etc.
Forinstance, NP modifiers usually appear after the NPhead in Spanish, and the specification of a conceptis usually made from left to right.4.1 Distribution of key concepts with verbdistanceFigure 1 shows, for every topic, the probability offinding a word from the manual list of key con-cepts in fixed distances from the verb of a sen-tence.
Stop words are not considered for computingword distance.
The broader line represents the aver-age across topics, and the horizontal dashed line isthe average probability across all positions, i.e., theprobability that a word chosen at random belongs tothe list of key concepts.The plot shows some clear tendencies in the data:the probability gets higher when we get close to theverb, falls abruptly after the verb, and then growssteadily again.
For TT topics, the probability offinding relevant concepts immediately before theverb is 56% larger than the average (0.39 before theverb, versus 0.25 in any position).
This is true notonly as an average, but also for all individual TTtopics.
This can be an extremely valuable result: itshows a direct correlation between the position of aterm in a sentence and the importance of the termin the topic.
Of course, this direct distance to theverb should be adapted for languages with differentsyntactic properties, and should be validated for dif-ferent domains.The behavior of TT and IE topics is substantiallydifferent.
IE topics have smaller probabilities over-all, because there are less key concepts common toall documents.
For instance, if the topic is ?cases ofhunger strikes?, there is little in common betweenFigure 1: Probability of finding key concepts at fixed distances from verball cases of hunger strikes found in the collection;each case has its own relevant people and organiza-tions, for instance.
Users try to make abstraction ofindividual cases to write key concepts, and then thenumber of key concepts is smaller.
The tendencyto have larger probabilities just before the verb andsmaller probabilities just after the verb, however,can also be observed for IE topics.Figure 2: Probability of finding key concepts in sub-ject NPs versus other NPs4.2 Key Concepts and Noun Phrase SyntacticFunctionWe wanted also to confirm that it is more likely tofind a key concept in a subject noun phrase thanin general NPs.
For this, we have split compoundsentences in chunks, separating subordinate clauses([Sub] type chunks).
Then we have extracted se-quences with the pattern [N][Mod]*.
We assumethat the sentence subject is a sequence [N][Mod]*occurring immediately before the verb.
For in-stance:El presidente [N] en funciones [Mod] deHait??
[Mod] ha afirmado [V] que [Sub]...The rest of [N] and [Mod] chunks are consid-ered as part of the sentence verb phrase.
In a ma-jority of cases, these assumptions lead to a correctidentification of the sentence subject.
We do notcapture, however, subjects of subordinate sentencesor subjects appearing after the verb.Figure 2 shows how the probability of finding akey concept is always larger in sentence subjects.This result supports the assumption in (Boguraevet al, 1998), where noun phrases receive a higherweight, as representative terms, if they are syntacticsubjects.4.3 Distribution of key concepts within nounphrasesFigure 3: Probability of finding key concepts in NPhead versus NP modifiersFor this analysis, we assume that, in[N][Mod]* sequences identified as subjects,[N] is the head and [Mod]* are the modifiers.Figure 3 shows that the probability of finding akey concept in the NP modifiers is always higherthan in the head (except for topic TT3, where it isequal).
This is not intuitive a priori; an examinationof the data reveals that the most characteristic con-cepts for a topic tend to be in the complements: forinstance, in ?the president of Haiti?, ?Haiti?
carriesmore domain information than ?president?.
Thisseems to be the most common case in our newscollection.
Of course, it cannot be guaranteed thatthese results will hold in other domains.5 Automatic Selection of Key TermsWe have shown that there is indeed a correlation be-tween syntactic information and the possibility offinding a key concept.
Now, we want to explorewhether this syntactic information can effectivelybe used for the automatic extraction of key concepts.The problem of extracting key concepts for sum-marization involves two related issues: a) Whatkinds of terms should be considered as candidates?and b) What is the optimal weighting criteria forthem?There are several possible answers to the firstquestion.
Previous work includes using nounphrases (Boguraev et al, 1998; Jones et al, 2002),words (Buyukkokten et al, 1999), n-grams (Leuskiet al, 2003; Lin and Hovy, 1997) or propernouns, multi-word terms and abbreviations (Neffand Cooper, 1999).Here we will focus, however, in finding appro-priate weighting schemes on the set of candidateterms.
The most common approach in interactivesingle-document summarization is using tf.idf mea-sures (Jones et al, 2002; Buyukkokten et al, 1999;Neff and Cooper, 1999), which favour terms whichare frequent in a document and infrequent acrossthe collection.
In the iNeast system (Leuski et al,2003), the identification of relevant terms is ori-ented towards multi-document summarization, andthey use a likelihood ratio (Dunning, 1993) whichfavours terms which are representative of the set ofdocuments as opposed to the full collection.Other sources of information that have been usedas complementary measures consider, for instance,the number of references of a concept (Boguraevet al, 1998), its localization (Jones et al, 2002)or the distribution of the term along the document(Buyukkokten et al, 1999; Boguraev et al, 1998).5.1 Experimental setupA technical difficulty is that the key concepts in-troduced by the users are intellectual elaborations,which result in complex expressions which mighteven not be present (literally) in the documents.Hence, we will concentrate on extracting lists ofterms, checking whether these terms are part ofsome key concept.
We will assume that, once keyterms are found, it is possible to generate full nomi-nal expressions using, for instance, phrase browsingstrategies (Pen?as et al, 2002).We will then compare different weighting criteriato select key terms, using two evaluation measures:a recall measure saying how well manually selectedkey concepts are covered by the automatically gen-erated term list; and a noise measure counting thenumber of terms which do not belong to any keyconcept.
An optimal list will reach maximum recallwith a minimum of noise.
Formally:R =|Cl||C|Noise = |Ln|where C is the set of key concepts manually se-lected by users; L is a (ranked) list of terms gen-erated by some weighting schema; Ln is the subsetof terms in L which do not belong to any key con-cept; and Cl is the subset of key concepts which arerepresented by at least one term in the ranked list L.Here is a (fictitious) example of how R andNoise are computed:C = {Haiti, reinstatement of democracy, UN and USA troops}L = {Haiti, soldiers, UN, USA, attempt}?Cl = {Haiti, UN and USA troops} R = 2/3Ln = {soldiers,attempt} Noise = 2We will compare the following weighting strate-gies:TF The frequency of a word in the set of documentsis taken as a baseline measure.Likelihood ratio This is taken from (Leuski et al,2003) and used as a reference measure.
Wehave implemented the procedure described in(Rayson and Garside, 2000) using unigramsonly.OKAPImod We have also considered a measurederived from Okapi and used in (Robertson etal., 1992).
We have adapted the measure toconsider the set of 100 documents as one singledocument.TFSYNTAX Using our first experimental result,TFSYNTAX computes the weight of a termas the number of times it appears preceding averb.Figure 4: Comparison of weighting schemes to ex-tract relevant terms5.2 ResultsFigure 4 draws Recall/Noise curves for all weight-ing criteria.
They all give similar results except ourTFSYNTAX measure, which performs better thanthe others for TT topics.
Note that the TFSYN-TAX measure only considers 10% of the vocabu-lary, which are the words immediately precedingverbs in the texts.In order to check whether this result is consistentacross topics (and not only the effect on an average)we have compared recall for term lists of size 50 forindividual topics.
We have selected 50 as a numberwhich is large enough to reach a good coverage andpermit additional filtering in an interactive summa-rization process, such as the iNeast terminologicalclustering described in (Leuski et al, 2003).Figure 5 shows these results by topic.
TFSYN-TAX performs consistently better for all topics ex-cept one of the IE topics, where the maximum like-lihood measure is slightly better.Apart from the fact that TFSYNTAX performsbetter than all other methods, it is worth noticingthat sophisticated weighting mechanisms, such asOkapi and the likelihood ratio, do not behave bet-ter than a simple frequency count (TF).6 ConclusionsThe automatic extraction of relevant concepts fora set of related documents is a part of many mod-els of automatic or interactive summarization.
Inthis paper, we have analyzed the distribution of rel-evant concepts across different syntactic functions,and we have measured the usefulness of detectingkey terms to extract relevant concepts.Our results suggest that the distribution of keyconcepts in sentences is not uniform, having a max-imum in positions immediately preceding the sen-tence main verb, in noun phrases acting as subjectsand, more specifically, in the complements (ratherthan the head) of noun phrases acting as subjects.This evidence has been collected using a Spanishnews collection, and should be corroborated outsidethe news domain and also adapted to be used for nonSVO languages.We have also obtained empirical evidence thatstatistical weights to select key terms can be im-proved if we restrict candidate words to those whichprecede the verb in some sentence.
The combi-nation of statistical measures and syntactic criteriaovercomes pure statistical weights, at least for TTtopics, where there is certain consistency in the keyconcepts across documents.AcknowledgmentsThis research has been partially supported by a re-search grant of the Spanish Government (projectHermes) and a research grant from UNED.
We areindebted to J. Cigarra?n who calculated the Okapiweights used in this work.ReferencesE.
Amigo, J. Gonzalo, V. Peinado, A.
Pen?as, andF.
Verdejo.
2004.
Information synthesis: an em-pirical study.
In Proceedings of the 42th AnnualMeeting of the ACL, Barcelona, July.B.
Boguraev, C. Kennedy, R. Bellamy, S. Brawer,Y.
Wong, and J. Swartz.
1998.
Dynamic Presen-tation of Document Content for Rapid On-lineSkimming.
In Proceedings of the AAAI SpringFigure 5: Comparison of weighting schemes by topic1998 Symposium on Intelligent Text Summariza-tion, Stanford, CA.O.
Buyukkokten, H.
Garc?
?a-Molina, andA.
Paepcke.
1999.
Seeing the Whole inParts: Text Summarization for Web Browsingon Handheld Devices.
In Proceedings of 10thInternational WWW Conference.T.
Dunning.
1993.
Accurate Methods for the Statis-tics of Surprise and Coincidence.
ComputationalLinguistics, 19(1):61?74.H.
P. Edmundson.
1969.
New Methods in Auto-matic Extracting.
Journal of the Association forComputing Machinery, 16(2):264?285.S.
Jones, S. Lundy, and G. W. Paynter.
2002.
In-teractive Document Summarization Using Auto-matically Extracted Keyphrases.
In Proceedingsof the 35th Hawaii International Conference onSystem Sciences, Big Island, Hawaii.W.
Kraaij, M. Spitters, and A. Hulth.
2002.Headline Extraction based on a Combination ofUni- and Multi-Document Summarization Tech-niques.
In Proceedings of the DUC 2002 Work-shop on Multi-Document Summarization Evalua-tion, Philadelphia, PA, July.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A train-able document summarizer.
In Proceedings of SI-GIR?95.A.
Leuski, C. Y. Lin, and S. Stubblebine.
2003.iNEATS: Interactive Multidocument Summariza-tion.
In Proceedings of the 4lst Annual Meetingof the ACL (ACL 2003), Sapporo, Japan.C.-Y.
Lin and E.H. Hovy.
1997.
Identifying Top-ics by Position.
In Proceedings of the 5th Con-ference on Applied Natural Language Processing(ANLP), Washington, DC.C.
Lin and E. Hovy.
2002.
NeATS in DUC2002.
In Proceedings of the DUC 2002 Work-shop on Multi-Document Summarization Evalu-ation, Philadelphia, PA, July.M.
S. Neff and J. W. Cooper.
1999.
ASHRAM: Ac-tive Summarization and Markup.
In Proceedingsof HICSS-32: Understanding Digital Documents.A.
Pen?as, F. Verdejo, and J. Gonzalo.
2002.
Ter-minology Retrieval: Towards a Synergy be-tween Thesaurus and Free Text Searching.
In IB-ERAMIA 2002, pages 684?693, Sevilla, Spain.C.
Peters, M. Braschler, J. Gonzalo, and M. Kluck,editors.
2002.
Evaluation of Cross-LanguageInformation Retrieval Systems, volume 2406 ofLecture Notes in Computer Science.
Springer-Verlag, Berlin-Heidelberg-New York.S.
Preston, K.and Williams.
1994.
Managing theInformation Overload.
Physics in Business, June.P.
Rayson and R. Garside.
2000.
Comparing Cor-pora Using Frequency Profiling.
In Proceedingsof the workshop on Comparing Corpora, pages1?6, Honk Kong.S.
E. Robertson, S. Walker, M. Hancock-Beaulieu,A.
Gull, and M. Lau.
1992.
Okapi at TREC.
InText REtrieval Conference, pages 21?30.J.
D. Schlesinger, M. E. Okurowski, J. M. Conroy,D.
P. O?Leary, A. Taylor, J. Hobbs, and H. Wil-son.
2002.
Understanding Machine Performancein the Context of Human Performance for Multi-Document Summarization.
In Proceedings of theDUC 2002 Workshop on Multi-Document Sum-marization Evaluation, Philadelphia, PA, July.
