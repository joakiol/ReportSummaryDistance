Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 6?13, New York City, June 2006. c?2006 Association for Computational LinguisticsPorting Statistical Parsers with Data-Defined KernelsIvan TitovUniversity of Geneva24, rue Ge?ne?ral DufourCH-1211 Gene`ve 4, Switzerlandivan.titov@cui.unige.chJames HendersonUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, United Kingdomjames.henderson@ed.ac.ukAbstractPrevious results have shown disappointingperformance when porting a parser trainedon one domain to another domain whereonly a small amount of data is available.We propose the use of data-defined ker-nels as a way to exploit statistics from asource domain while still specializing aparser to a target domain.
A probabilisticmodel trained on the source domain (andpossibly also the target domain) is used todefine a kernel, which is then used in alarge margin classifier trained only on thetarget domain.
With a SVM classifier anda neural network probabilistic model, thismethod achieves improved performanceover the probabilistic model alone.1 IntroductionIn recent years, significant progress has been madein the area of natural language parsing.
This re-search has focused mostly on the development ofstatistical parsers trained on large annotated corpora,in particular the Penn Treebank WSJ corpus (Marcuset al, 1993).
The best statistical parsers have showngood results on this benchmark, but these statisticalparsers demonstrate far worse results when they areapplied to data from a different domain (Roark andBacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999).This is an important problem because we cannot ex-pect to have large annotated corpora available formost domains.
While identifying this problem, pre-vious work has not proposed parsing methods whichare specifically designed for porting parsers.
Insteadthey propose methods for training a standard parserwith a large amount of out-of-domain data and asmall amount of in-domain data.In this paper, we propose using data-defined ker-nels and large margin methods to specifically ad-dress porting a parser to a new domain.
Data-definedkernels are used to construct a new parser which ex-ploits information from a parser trained on a largeout-of-domain corpus.
Large margin methods areused to train this parser to optimize performance ona small in-domain corpus.Large margin methods have demonstrated sub-stantial success in applications to many machinelearning problems, because they optimize a mea-sure which is directly related to the expected test-ing performance.
They achieve especially good per-formance compared to other classifiers when onlya small amount of training data is available.
Mostof the large margin methods need the definition of akernel.
Work on kernels for natural language parsinghas been mostly focused on the definition of kernelsover parse trees (e.g.
(Collins and Duffy, 2002)),which are chosen on the basis of domain knowledge.In (Henderson and Titov, 2005) it was proposed toapply a class of kernels derived from probabilisticmodels to the natural language parsing problem.In (Henderson and Titov, 2005), the kernel is con-structed using the parameters of a trained proba-bilistic model.
This type of kernel is called a data-defined kernel, because the kernel incorporates in-formation from the data used to train the probabilis-tic model.
We propose to exploit this property totransfer information from a large corpus to a statis-6tical parser for a different domain.
Specifically, wepropose to train a statistical parser on data includingthe large corpus, and to derive the kernel from thistrained model.
Then this derived kernel is used in alarge margin classifier trained on the small amountof training data available for the target domain.In our experiments, we consider two differentscenarios for porting parsers.
The first scenario isthe pure porting case, which we call ?transferring?.Here we only require a probabilistic model trainedon the large corpus.
This model is then reparameter-ized so as to extend the vocabulary to better suit thetarget domain.
The kernel is derived from this repa-rameterized model.
The second scenario is a mixtureof parser training and porting, which we call ?focus-ing?.
Here we train a probabilistic model on boththe large corpus and the target corpus.
The kernelis derived from this trained model.
In both scenar-ios, the kernel is used in a SVM classifier (Tsochan-taridis et al, 2004) trained on a small amount of datafrom the target domain.
This classifier is trained torerank the candidate parses selected by the associ-ated probabilistic model.
We use the Penn TreebankWall Street Journal corpus as the large corpus andindividual sections of the Brown corpus as the tar-get corpora (Marcus et al, 1993).
The probabilis-tic model is a neural network statistical parser (Hen-derson, 2003), and the data-defined kernel is a TOPreranking kernel (Henderson and Titov, 2005).With both scenarios, the resulting parser demon-strates improved accuracy on the target domain overthe probabilistic model alone.
In additional experi-ments, we evaluate the hypothesis that the primaryissue for porting parsers between domains is differ-ences in the distributions of words in structures, andnot in the distributions of the structures themselves.We partition the parameters of the probability modelinto those which define the distributions of wordsand those that only involve structural decisions, andderive separate kernels for these two subsets of pa-rameters.
The former model achieves virtually iden-tical accuracy to the full model, but the later modeldoes worse, confirming the hypothesis.2 Data-Defined Kernels for ParsingPrevious work has shown how data-defined kernelscan be applied to the parsing task (Henderson andTitov, 2005).
Given the trained parameters of a prob-abilistic model of parsing, the method defines a ker-nel over sentence-tree pairs, which is then used torerank a list of candidate parses.In this paper, we focus on the TOP reranking ker-nel defined in (Henderson and Titov, 2005), whichare closely related to Fisher kernels.
The rerank-ing task is defined as selecting a parse tree from thelist of candidate trees (y1, .
.
.
, ys) suggested by aprobabilistic model P (x, y|??
), where ??
is a vector ofmodel parameters learned during training the prob-abilistic model.
The motivation for the TOP rerank-ing kernel is given in (Henderson and Titov, 2005),but for completeness we note that the its feature ex-tractor is given by:???
(x, yk) =(v(x, yk, ??
), ?v(x,yk,??)?
?1 , .
.
.
,?v(x,yk,??)?
?l ),(1)where v(x, yk, ??)
= log P (x, yk|??)
?log ?t6=k P (x, yt|??).
The first feature reflectsthe score given to (x, yk) by the probabilisticmodel (relative to the other candidates for x), andthe remaining features reflect how changing theparameters of the probabilistic model would changethis score for (x, yk).The parameters ??
used in this feature extractor donot have to be exactly the same as the parameterstrained in the probabilistic model.
In general, wecan first reparameterize the probabilistic model, pro-ducing a new model which defines exactly the sameprobability distribution as the old model, but with adifferent set of adjustable parameters.
For example,we may want to freeze the values of some parame-ters (thereby removing them from ??
), or split someparameters into multiple cases (thereby duplicatingtheir values in ??).
This flexibility allows the featuresused in the kernel method to be different from thoseused in training the probabilistic model.
This can beuseful for computational reasons, or when the kernelmethod is not solving exactly the same problem asthe probabilistic model was trained for.3 Porting with Data-Defined KernelsIn this paper, we consider porting a parser trained ona large amount of annotated data to a different do-main where only a small amount of annotated datais available.
We validate our method in two different7scenarios, transferring and focusing.
Also we verifythe hypothesis that addressing differences betweenthe vocabularies of domains is more important thanaddressing differences between their syntactic struc-tures.3.1 Transferring to a Different DomainIn the transferring scenario, we are given just a prob-abilistic model which has been trained on a largecorpus from a source domain.
The large corpus isnot available during porting, and the small corpusfor the target domain is not available during trainingof the probabilistic model.
This is the case of pureparser porting, because it only requires the sourcedomain parser, not the source domain corpus.
Be-sides this theoretical significance, this scenario hasthe advantage that we only need to train a singleprobabilistic parser, thereby saving on training timeand removing the need for access to the large cor-pus once this training is done.
Then any number ofparsers for new domains can be trained, using onlythe small amount of annotated data available for thenew domain.Our proposed porting method first constructs adata-defined kernel using the parameters of thetrained probabilistic model.
A large margin clas-sifier with this kernel is then trained to rerank thetop candidate parses produced by the probabilisticmodel.
Only the small target corpus is used duringtraining of this classifier.
The resulting parser con-sists of the original parser plus a very computation-ally cheap procedure to rerank its best parses.Whereas training of standard large margin meth-ods, like SVMs, isn?t feasible on a large corpus, itis quite tractable to train them on a small target cor-pus.1 Also, the choice of the large margin classifieris motivated by their good generalization propertieson small datasets, on which accurate probabilisticmodels are usually difficult to learn.We hypothesize that differences in vocabularyacross domains is one of the main difficulties withparser portability.
To address this problem, we pro-pose constructing the kernel from a probabilisticmodel which has been reparameterized to better suit1In (Shen and Joshi, 2003) it was proposed to use an en-semble of SVMs trained the Wall Street Journal corpus, but webelieve that the generalization performance of the resulting clas-sifier is compromised in this approach.the target domain vocabulary.
As in other lexicalizedstatistical parsers, the probabilistic model we usetreats words which are not frequent enough in thetraining set as ?unknown?
words (Henderson, 2003).Thus there are no parameters in this model whichare specifically for these words.
When we considera different target domain, a substantial proportionof the words in the target domain are treated as un-known words, which makes the parser only weaklylexicalized for this domain.To address this problem, we reparameterize theprobability model so as to add specific parametersfor the words which have high enough frequencyin the target domain training set but are treated asunknown words by the original probabilistic model.These new parameters all have the same values astheir associated unknown words, so the probabilitydistribution specified by the model does not change.However, when a kernel is defined with this repa-rameterized model, the kernel?s feature extractor in-cludes features specific to these words, so the train-ing of a large margin classifier can exploit differ-ences between these words in the target domain.
Ex-panding the vocabulary in this way is also justifiedfor computational reasons; the speed of the proba-bilistic model we use is greatly effected by vocabu-lary size, but the large-margin method is not.3.2 Focusing on a SubdomainIn the focusing scenario, we are given the large cor-pus from the source domain.
We may also be givena parsing model, but as with other approaches to thisproblem we simply throw this parsing model awayand train a new one on the combination of the sourceand target domain data.
Previous work (Roark andBacchiani, 2003) has shown that better accuracy canbe achieved by finding the optimal re-weighting be-tween these two datasets, but this issue is orthogonalto our method, so we only consider equal weighting.After this training phase, we still want to optimizethe parser for only the target domain.Once we have the trained parsing model, our pro-posed porting method proceeds the same way in thisscenario as in transferring.
However, because theoriginal training set aleady includes the vocabularyfrom the target domain, the reparameterization ap-proach defined in the preceding section is not nec-essary so we do not perform it.
This reparameter-8ization could be applied here, thereby allowing usto use a statistical parser with a smaller vocabulary,which can be more computationally efficient bothduring training and testing.
However, we would ex-pect better accuracy of the combined system if thesame large vocabulary is used both by the proba-bilistic parser and the kernel method.3.3 Vocabulary versus StructureIt is commonly believed that differences in vo-cabulary distributions between domains effects theported parser performance more significantly thanthe differences in syntactic structure distributions.We would like to test this hypothesis in our frame-work.
The probabilistic model (Henderson, 2003)allows us to distinguish between those parametersresponsible for the distributions of individual vocab-ulary items, and those parameters responsible for thedistributions of structural decisions, as described inmore details in section 4.2.
We train two additionalmodels, one which uses a kernel defined in terms ofonly vocabulary parameters, and one which uses akernel defined in terms of only structure parameters.By comparing the performance of these models andthe model with the combined kernel, we can drawconclusion on the relative importance of vocabularyand syntactic structures for parser portability.4 An Application to a Neural NetworkStatistical ParserData-defined kernels can be applied to any kindof parameterized probabilistic model, but they areparticularly interesting for latent variable models.Without latent variables (e.g.
for PCFG models), thefeatures of the data-defined kernel (except for thefirst feature) are a function of the counts used to esti-mate the model.
For a PCFG, each such feature is afunction of one rule?s counts, where the counts fromdifferent candidates are weighted using the probabil-ity estimates from the model.
With latent variables,the meaning of the variable (not just its value) islearned from the data, and the associated features ofthe data-defined kernel capture this induced mean-ing.
There has been much recent work on latentvariable models (e.g.
(Matsuzaki et al, 2005; Kooand Collins, 2005)).
We choose to use an earlierneural network based probabilistic model of pars-ing (Henderson, 2003), whose hidden units can beviewed as approximations to latent variables.
Thisparsing model is also a good candidate for our exper-iments because it achieves state-of-the-art results onthe standard Wall Street Journal (WSJ) parsing prob-lem (Henderson, 2003), and data-defined kernels de-rived from this parsing model have recently beenused with the Voted Perceptron algorithm on theWSJ parsing task, achieving a significant improve-ment in accuracy over the neural network parseralone (Henderson and Titov, 2005).4.1 The Probabilistic Model of ParsingThe probabilistic model of parsing in (Henderson,2003) has two levels of parameterization.
The firstlevel of parameterization is in terms of a history-based generative probability model.
These param-eters are estimated using a neural network, theweights of which form the second level of param-eterization.
This approach allows the probabilitymodel to have an infinite number of parameters; theneural network only estimates the bounded numberof parameters which are relevant to a given partialparse.
We define our kernels in terms of the secondlevel of parameterization (the network weights).A history-based model of parsing first defines aone-to-one mapping from parse trees to sequencesof parser decisions, d1,..., dm (i.e.
derivations).
Hen-derson (2003) uses a form of left-corner parsingstrategy, and the decisions include generating thewords of the sentence (i.e.
it is generative).
Theprobability of a sequence P (d1,..., dm) is then de-composed into the multiplication of the probabilitiesof each parser decision conditioned on its history ofprevious decisions ?iP (di|d1,..., di?1).4.2 Deriving the KernelThe complete set of neural network weights isn?tused to define the kernel, but instead reparameteriza-tion is applied to define a third level of parameteriza-tion which only includes the network?s output layerweights.
As suggested in (Henderson and Titov,2005) use of the complete set of weights doesn?tlead to any improvement of the resulting rerankerand makes the reranker training more computation-ally expensive.Furthermore, to assess the contribution of vocab-ulary and syntactic structure differences (see sec-9tion 3.3), we divide the set of the parameters into vo-cabulary parameters and structural parameters.
Weconsider the parameters used in the estimation of theprobability of the next word given the history repre-sentation as vocabulary parameters, and the param-eters used in the estimation of structural decisionprobabilities as structural parameters.
We define thekernel with structural features as using only struc-tural parameters, and the kernel with vocabulary fea-tures as using only vocabulary parameters.5 Experimental ResultsWe used the Penn Treebank WSJ corpus and theBrown corpus to evaluate our approach.
We usedthe standard division of the WSJ corpus into train-ing, validation, and testing sets.
In the Brown corpuswe ran separate experiments for sections F (informa-tive prose: popular lore), K (imaginative prose: gen-eral fiction), N (imaginative prose: adventure andwestern fiction), and P (imaginative prose: romanceand love story).
These sections were selected be-cause they are sufficiently large, and because theyappeared to be maximally different from each otherand from WSJ text.
In each Brown corpus section,we selected every third sentence for testing.
Fromthe remaining sentences, we used 1 sentence out of20 for the validation set, and the remainder for train-ing.
The resulting datasets sizes are presented in ta-ble 1.For the large margin classifier, we used the SVM-Struct (Tsochantaridis et al, 2004) implementationof SVM, which rescales the margin with F1 mea-sure of bracketed constituents (see (Tsochantaridiset al, 2004) for details).
Linear slack penalty wasemployed.25.1 Experiments on Transferring acrossDomainsTo evaluate the pure porting scenario (transferring),described in section 3.1, we trained the SSN pars-ing model on the WSJ corpus.
For each tag, there isan unknown-word vocabulary item which is used forall those words not sufficiently frequent with that tagto be included individually in the vocabulary.
In the2Training of the SVM takes about 3 hours on a standarddesktop PC.
Running the SVM is very fast, once the probabilis-tic model has finished computing the probabilities needed toselect the candidate parses.testing training validationWSJ 2,416 39,832 1,346(54,268) (910,196) (31,507)Brown F 1,054 2,005 105(23,722) (44,928) (2,300)Brown K 1,293 2,459 129(21,215) (39,823) (1,971)Brown N 1,471 2,797 137(22,142) (42,071) (2,025)Brown P 1,314 2,503 125(21,763) (41,112) (1,943)Table 1: Number of sentences (words) for eachdataset.vocabulary of the parser, we included the unknown-word items and the words which occurred in thetraining set at least 20 times.
This led to the vo-cabulary of 4,215 tag-word pairs.We derived the kernel from the trained model foreach target section (F, K, N, P) using reparameteriza-tion discussed in section 3.1: we included in the vo-cabulary all the words which occurred at least twicein the training set of the corresponding section.
Thisapproach led to a smaller vocabulary than that of theinitial parser but specifically tied to the target do-main (3,613, 2,789, 2,820 and 2,553 tag-word pairsfor sections F, K, N and P respectively).
There is nosense in including the words from the WSJ which donot appear in the Brown section training set becausethe classifier won?t be able to learn the correspond-ing components of its decision vector.
The resultsfor the original probabilistic model (SSN-WSJ) andfor the kernel method (TOP-Transfer) on the testingset of each section are presented in table 2.3To evaluate the relative contribution of our portingtechnique versus the use of the TOP kernel alone,we also used this TOP kernel to train an SVM on theWSJ corpus.
We trained the SVM on data from thedevelopment set and section 0, so that the size of thisdataset (3,267 sentences) was about the same as foreach Brown section.4 This gave us a ?TOP-WSJ?3All our results are computed with the evalb program fol-lowing the standard criteria in (Collins, 1999).4We think that using an equivalently sized dataset providesa fair test of the contribution of the TOP kernel alone.
It wouldalso not be computationally tractable to train an SVM on the fullWSJ dataset without using different training techniques, whichwould then compromise the comparison.10model, which we tested on each of the four Brownsections.
In each case, the TOP-WSJ model didworse than the original SSN-WSJ model, as shownin table 2.
This makes it clear that we are getting noimprovement from simply using a TOP kernel aloneor simply using more data, and all our improvementis from the proposed porting method.5.2 Experiments on Focusing on a SubdomainTo perform the experiments on the approach sug-gested in section 3.2 (focusing), we trained the SSNparser on the WSJ training set joined with the train-ing set of the corresponding section.
We includedin the vocabulary only words which appeared in thejoint training set at least 20 times.
Resulting vocab-ularies comprised 4,386, 4,365, 4,367 and 4,348 forsections F, K, N and P, respectively.5 Experimentswere done in the same way as for the parser transfer-ring approach, but reparameterization was not per-formed.
Standard measures of accuracy for the orig-inal probabilistic model (SSN-WSJ+Br) and the ker-nel method (TOP-Focus) are also shown in table 2.For the sake of comparison, we also trained theSSN parser on only training data from one of theBrown corpus sections (section P), producing a?SSN-Brown?
model.
This model achieved an F1measure of only 81.0% for the P section testingset, which is worse than all the other models andis 3% lower than our best results on this testing set(TOP-Focus).
This result underlines the need to portparsers from domains in which there are large anno-tated datasets.5.3 Experiments Comparing Vocabulary toStructureWe conducted the same set of experiments with thekernel with vocabulary features (TOP-Voc-Transferand TOP-Voc-Focus) and with the kernel with thestructural features (TOP-Str-Transfer and TOP-Str-Focus).
Average results for classifiers with thesekernels, as well as for the original kernel and thebaseline, are presented in table 3.5We would expect some improvement if we used a smallerthreshold on the target domain, but preliminary results suggestthat this improvement would be small.section LR LP F?=1TOP-WSJ F 83.9 84.9 84.4SSN-WSJ F 84.4 85.2 84.8TOP-Transfer F 84.5 85.6 85.0SSN-WSJ+Br F 84.2 85.2 84.7TOP-Focus F 84.6 86.0 85.3TOP-WSJ K 81.8 82.3 82.1SSN-WSJ K 82.2 82.6 82.4TOP-Transfer K 82.4 83.5 83.0SSN-WSJ+Br K 83.1 84.2 83.6TOP-Focus K 83.6 85.0 84.3TOP-WSJ N 83.3 84.5 83.9SSN-WSJ N 83.5 84.6 84.1TOP-Transfer N 84.3 85.7 85.0SSN-WSJ+Br N 85.0 86.5 85.7TOP-Focus N 85.0 86.7 85.8TOP-WSJ P 81.3 82.1 81.7SSN-WSJ P 82.3 83.0 82.6TOP-Transfer P 82.7 83.8 83.2SSN-WSJ+Br P 83.1 84.3 83.7TOP-Focus P 83.3 84.8 84.0Table 2: Percentage labeled constituent recall (LR),precision (LP), and a combination of both (F?=1) onthe individual test sets.5.4 Discussion of ResultsFor the experiments which directly test the useful-ness of our proposed porting technique (SSN-WSJversus TOP-Transfer), our technique demonstratedimprovement for each of the Brown sections (ta-ble 2), and this improvement was significant forthree out of four of the sections (K, N, and P).6 Thisdemonstrates that data-defined kernels are an effec-tive way to port parsers to a new domain.For the experiments which combine training anew probability model with our porting technique(SSN-WSJ+Br versus TOP-Focus), our techniquestill demonstrated improvement over training alone.There was improvement for each of the Brown sec-tions, and this improvement was significant for two6We measured significance in F1 measure at the 5% levelwith the randomized significance test of (Yeh, 2000).
We thinkthat the reason the improvement on section F was only signif-icant at the 10% level was that the baseline model (SSN-WSJ)was particularly lucky, as indicated by the fact that it did evenbetter than the model trained on the combination of datasets(SSN-WSJ+Br).11LR LP F?=1SSN-WSJ 83.1 83.8 83.5TOP-Transfer 83.5 84.7 84.1TOP-Voc-Transfer 83.5 84.7 84.1TOP-Str-Transfer 83.1 84.3 83.7SSN-WSJ+Br 83.8 85.0 84.4TOP-Focus 84.1 85.6 84.9TOP-Voc-Focus 84.1 85.6 84.8TOP-Str-Focus 83.9 85.4 84.7Table 3: Average accuracy of the models on chaptersF, K, N and P of the Brown corpus.out of four of the sections (F and K).
This demon-strates that, even when the probability model is wellsuited to the target domain, there is still room forimprovement from using data-defined kernels to op-timize the parser specifically to the target domainwithout losing information about the source domain.One potential criticism of these conclusions is thatthe improvement could be the result of rerankingwith the TOP kernel, and have nothing to do withporting.
The lack of an improvement in the TOP-WSJ results discussed in section 5.1 clearly showsthat this cannot be the explanation.
The oppositecriticism is that the improvement could be the resultof optimizing to the target domain alone.
The poorperformance of the SSN-Brown model discussed insection 5.2 makes it clear that this also cannot bethe explanation.
Therefore reranking with data de-fined kernels must be both effective at preservinginformation about the source domain and effectiveat specializing to the target domain.The experiments which test the hypothesis thatdifferences in vocabulary distributions are more im-portant than difference in syntactic structure distri-butions confirm this belief.
Results for the classi-fier which uses the kernel with only vocabulary fea-tures are better than those for structural features ineach of the four sections with both the Transfer andFocus scenarios.
In addition, comparing the resultsof TOP-Transfer with TOP-Voc-Transfer and TOP-Focus with TOP-Voc-Focus, we can see that addingstructural features in TOP-Focus and TOP-Transferleads to virtually no improvement.
This suggest thatdifferences in vocabulary distributions are the onlyissue we need to address, although this result couldpossibly also be an indication that our method didnot sufficiently exploit structural differences.In this paper we concentrate on the situationwhere a parser is needed for a restricted target do-main, for which only a small amount of data is avail-able.
We believe that this is the task which is ofgreatest practical interest.
For this reason we do notrun experiments on the task considered in (Gildea,2001) and (Roark and Bacchiani, 2003), where theyare porting from the restricted domain of the WSJcorpus to the more varied domain of the Brown cor-pus as a whole.
However, to help emphasize thesuccess of our proposed porting method, it is rele-vant to show that even our baseline models are per-forming better than this previous work on parserportability.
We trained and tested the SSN parser intheir ?de-focusing?
scenario using the same datasetsas (Roark and Bacchiani, 2003).
When trainedonly on the WSJ data (analogously to the SSN-WSJ baseline for TOP-Transfer) it achieves resultsof 82.9%/83.4% LR/LP and 83.2% F1, and whentrained on data from both domains (analogouslyto the SSN-WSJ+Br baselines for TOP-Focus) itachieves results of 86.3%/87.6% LR/LP and 87.0%F1.
These results represent a 2.2% and 1.3% in-crease in F1 over the best previous results, respec-tively (see the discussion of (Roark and Bacchiani,2003) below).6 Related WorkMost research in the field of parsing has focused onthe Wall Street Journal corpus.
Several researchershave addressed the portability of these WSJ parsersto other domains, but mostly without addressing theissue of how a parser can be designed specificallyfor porting to another domain.
Unfortunately, no di-rect empirical comparison is possible between ourresults and results with other parsers, because thereis no standard portability benchmark to date where asmall amount of data from a target domain is used.
(Ratnaparkhi, 1999) performed portability exper-iments with a Maximum Entropy parser and demon-strated that the parser trained on WSJ achieves farworse results on the Brown corpus sections.
Addinga small amount of data from the target domain im-proves the results, but accuracy is still much lowerthan the results on the WSJ.
They reported resultswhen their parser was trained on the WSJ training12set plus a portion of 2,000 sentences from a Browncorpus section.
They achieved 80.9%/80.3% re-call/precision for section K, and 80.6%/81.3% forsection N.7 Our analogous method (TOP-Focus)achieved much better accuracy (3.7% and 4.9% bet-ter F1, respectively).In addition to portability experiments with theparsing model of (Collins, 1997), (Gildea, 2001)provided a comprehensive analysis of parser porta-bility.
On the basis of this analysis, a tech-nique for parameter pruning was proposed leadingto a significant reduction in the model size with-out a large decrease of accuracy.
Gildea (2001)only reports results on sentences of 40 or lesswords on all the Brown corpus sections combined,for which he reports 80.3%/81.0% recall/precisionwhen training only on data from the WSJ corpus,and 83.9%/84.8% when training on data from theWSJ corpus and all sections of the Brown corpus.
(Roark and Bacchiani, 2003) performed experi-ments on supervised and unsupervised PCFG adap-tation to the target domain.
They propose to usethe statistics from a source domain to define pri-ors over weights.
However, in their experimentsthey used only trivial sub-cases of this approach,namely, count merging and model interpolation.They achieved very good improvement over theirbaseline and over (Gildea, 2001), but the absoluteaccuracies were still relatively low (as discussedabove).
They report results with combined Browndata (on sentences of 100 words or less), achieving81.3%/80.9% when training only on the WSJ cor-pus and 85.4%/85.9% with their best method usingthe data from both domains.7 ConclusionsThis paper proposes a novel technique for improv-ing parser portability, applying parse reranking withdata-defined kernels.
First a probabilistic model ofparsing is trained on all the available data, includinga large set of data from the source domain.
Thismodel is used to define a kernel over parse trees.Then this kernel is used in a large margin classifier7The sizes of Brown sections reported in (Ratnaparkhi,1999) do not match the sizes of sections distributed in the PennTreebank 3.0 package, so we couldn?t replicate their split.
Wesuspect that a preliminary version of the corpus was used fortheir experiments.trained on a small set of data only from the target do-main.
This classifier is used to rerank the top parsesproduced by the probabilistic model on the target do-main.
Experiments with a neural network statisticalparser demonstrate that this approach leads to im-proved parser accuracy on the target domain, with-out any significant increase in computational cost.ReferencesMichael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures and the voted perceptron.
In Proc.
ACL 2002 , pages263?270, Philadelphia, PA.Michael Collins.
1997.
Three generative, lexicalized modelsfor statistical parsing.
In Proc.
ACL/EACL 1997 , pages 16?23, Somerset, New Jersey.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Daniel Gildea.
2001.
Corpus variation and parser performance.In Proc.
EMNLP 2001 , Pittsburgh, PA.James Henderson and Ivan Titov.
2005.
Data-defined kernelsfor parse reranking derived from probabilistic models.
InProc.
ACL 2005 , Ann Arbor, MI.James Henderson.
2003.
Inducing history representations forbroad coverage statistical parsing.
In Proc.
NAACL/HLT2003 , pages 103?110, Edmonton, Canada.Terry Koo and Michael Collins.
2005.
Hidden-variable modelsfor discriminative reranking.
In Proc.
EMNLP 2005 , Van-couver, B.C., Canada.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguistics,19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005.Probabilistic CFG with latent annotations.
In Proc.
ACL2005 , Ann Arbor, MI.Adwait Ratnaparkhi.
1999.
Learning to parse natural languagewith maximum entropy models.
Machine Learning, 34:151?175.Brian Roark and Michiel Bacchiani.
2003.
Supervised andunsuperised PCFG adaptation to novel domains.
In Proc.HLT/ACL 2003 , Edmionton, Canada.Libin Shen and Aravind K. Joshi.
2003.
An SVM based votingalgorithm with application to parse reranking.
In Proc.
7thConf.
on Computational Natural Language Learning, pages9?16, Edmonton, Canada.Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,and Yasemin Altun.
2004.
Support vector machine learningfor interdependent and structured output spaces.
In Proc.21st Int.
Conf.
on Machine Learning, pages 823?830, Banff,Alberta, Canada.Alexander Yeh.
2000.
More accurate tests for the statisticalsignificance of the result differences.
In Proc.
17th Int.
Conf.on Computational Linguistics, pages 947?953, Saarbruken,Germany.13
