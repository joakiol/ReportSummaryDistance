Proceedings of NAACL HLT 2007, pages 105?112,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Random Text Model for the Generation ofStatistical Language InvariantsChris BiemannNLP Dept., University of LeipzigJohannisgasse 2604103 Leipzig, Germanybiem@informatik.uni-leipzig.deAbstractA novel random text generation model isintroduced.
Unlike in previous randomtext models, that mainly aim at producinga Zipfian distribution of word frequencies,our model also takes the properties ofneighboring co-occurrence into accountand introduces the notion of sentences inrandom text.
After pointing out the defi-ciencies of related models, we provide ageneration process that takes neither theZipfian distribution on word frequenciesnor the small-world structure of theneighboring co-occurrence graph as aconstraint.
Nevertheless, these distribu-tions emerge in the process.
The distribu-tions obtained with the random generationmodel are compared to a sample of natu-ral language data, showing high agree-ment also on word length and sentencelength.
This work proposes a plausiblemodel for the emergence of large-scalecharacteristics of language without as-suming a grammar or semantics.1 IntroductionG.
K. Zipf (1949) discovered that if all words in asample of natural language are arranged in de-creasing order of frequency, then the relation be-tween a word?s frequency and its rank in the listfollows a power-law.
Since then, a significantamount of research in the area of quantitative lin-guistics has been devoted to the question how thisproperty emerges and what kind of processes gen-erate such Zipfian distributions.The relation between the frequency of a word atrank r and its rank is given by f(r) ?
r-z, where z isthe exponent of the power-law that corresponds tothe slope of the curve in a log plot (cf.
figure 2).The exponent z was assumed to be exactly 1 byZipf; in natural language data, also slightly differ-ing exponents in the range of about 0.7 to 1.2 areobserved (cf.
Zanette and Montemurro 2002).
B.Mandelbrot (1953) provided a formula with acloser approximation of the frequency distributionsin language data, noticing that Zipf?s law holdsonly for the medium range of ranks, whereas thecurve is flatter for very frequent words and steeperfor high ranks.
He also provided a word generationmodel that produces random words of arbitraryaverage length in the following way: With a prob-ability w, a word separator is generated at eachstep, with probability (1-w)/N, a letter from an al-phabet of size N is generated, each letter havingthe same probability.
This is sometimes called the?monkey at the typewriter?
(Miller, 1957).
Thefrequency distribution follows a power-law forlong streams of words, yet the equiprobability ofletters causes the plot to show a step-wise ratherthan a smooth behavior, as examined by Ferrer iCancho and Sol?
(2002), cf.
figure 2.
In the samestudy, a smooth rank distribution could be obtainedby setting the letter probabilities according to letterfrequencies in a natural language text.
But thequestion of how these letter probabilities emergeremains unanswered.Another random text model was given bySimon (1955), which does not take an alphabet ofsingle letters into consideration.
Instead, at eachtime step, a previously unseen new word is addedto the stream with a probability a, whereas withprobability (1-a), the next word is chosen amongstthe words at previous positions.
As words withhigher frequency in the already generated stream105have a higher probability of being added again, thisimposes a strong competition among differentwords, resulting in a frequency distribution thatfollows a power-law with exponent z=(1-a).
Thiswas taken up by Zanette and Montemurro (2002),who slightly modify Simon?s model.
They intro-duce sublinear vocabulary growth by additionallymaking the new word probability dependent on thetime step.
Furthermore, they introduce a thresholdon the maximal probability a previously seen wordcan be assigned to for generation, being able tomodify the exponent z as well as to model the flat-ter curve for high frequency words.
In (Ha et al,2002), Zipf?s law is extended to words andphrases, showing its validity for syllable-classbased languages when conducting the extension.Neither the Mandelbrot nor the Simon genera-tion model take the sequence of words into ac-count.
Simon treats the previously generatedstream as a bag of words, and Mandelbrot does notconsider the previous stream at all.
This is cer-tainly an over-simplification, as natural languageexhibits structural properties within sentences andtexts that are not grasped by bags of words.The work by Kanter and Kessler (1995) is, toour knowledge, the only study to date that takes theword order into account when generating randomtext.
They show that a 2-parameter Markov processgives rise to a stationary distribution that exhibitsthe word frequency distribution and the letter fre-quency distribution characteristics of natural lan-guage.
However, the Markov process is initializedsuch that any state has exactly two successorstates, which means that after each word, only twoother following words are possible.
This certainlydoes not reflect natural language properties, wherein fact successor frequencies of words follow apower-law and more successors can be observedfor more frequent words.
But even when allowinga more realistic number of successor states, thetransition probabilities of a Markov model need tobe initialized a priori in a sensible way.
Further,the fixed number of states does not allow for infi-nite vocabulary.In the next section we provide a model thatdoes not suffer from all these limitations.2 The random text generation modelWhen constructing a random text generationmodel, we proceed according to the followingguidelines (cf.
Kumar et al 1999 for web graphgeneration):?
simplicity: a generation model should reachits goal using the simplest mechanisms pos-sible but results should still comply to char-acteristics of real language?
plausibility: Without claiming that ourmodel is an exhaustive description of whatmakes human brains generate and evolvelanguage, there should be at least a possibil-ity that similar mechanisms could operate inhuman brains.
For a discussion on the sensi-tivity of people to bigram statistics, see e.g.
(Thompson and Newport, 2007).?
emergence: Rather than constraining themodel with the characteristics we would liketo see in the generated stream, these featuresshould emerge in the process.Our model is basically composed of two partsthat will be described separately: A word generatorthat produces random words composed of lettersand a sentence generator that composes randomsentences of words.
Both parts use an internalgraph structure, where traces of previously gener-ated words and sentences are memorized.
Themodel is inspired by small-world network genera-tion processes, cf.
(Watts and Strogatz 1998,Barab?si and Albert 1999, Kumar et al 1999,Steyvers and Tenenbaum 2005).
A key notion isthe strategy of following beaten tracks: Letters,words and sequences of words that have been gen-erated before are more likely to be generated againin the future - a strategy that is only fulfilled forwords in Simon?s model.But before laying out the generators in detail,we introduce ways of testing agreement of our ran-dom text model with natural language text.2.1 Testing properties of word streamsAll previous approaches aimed at reproducing aZipfian distribution on word frequency, which is acriterion that we certainly have to fulfill.
But thereare more characteristics that should be obeyed tomake a random text more similar to natural lan-guage than previous models:?
Lexical spectrum: The smoothness or step-wise shape of the rank-frequency distribu-tion affects the lexical spectrum, which isthe probability distribution on word fre-106quency.
In natural language texts, this distri-bution follows a power-law with an expo-nent close to 2 (cf.
Ferrer i Cancho and Sol?,2002).?
Distribution of word length: According to(Sigurd et al, 2004), the distribution of wordfrequencies by length follows a variant ofthe gamma distribution?
Distribution of sentence length: The randomtext?s sentence length distribution should re-semble natural language.
In (Sigurd et al,2004), the same variant of the gamma distri-bution as for word length is fit to sentencelength.?
Significant neighbor-based co-occurrence:As discussed in (Dunning 1993), it is possi-ble to measure the amount of surprise to seetwo neighboring words in a corpus at a cer-tain frequency under the assumption of in-dependence.
At random generation withoutword order awareness, the number of suchpairs that are significantly co-occurring inneighboring positions should be very low.We aim at reproducing the number of sig-nificant pairs in natural language as well asthe graph structure of the neighbor-based co-occurrence graph.The last characteristic refers to the distributionof words in sequence.
Important is the notion ofsignificance, which serves as a means to distin-guish random sequences from motivated ones.
Weuse the log-likelihood ratio for determining signifi-cance as in (Dunning, 1993), but other measuresare possible as well.
Note that the model of Kanterand Kessler (1995) produces a maximal degree of2 in the neighbor-based co-occurrence graph.As written language is rather an artifact of themost recent millennia then a realistic sample ofeveryday language, we use the beginning of thespoken language section of the British NationalCorpus (BNC) to test our model against.
For sim-plicity, all letters are capitalized and special char-acters are removed, such that merely the 26 lettersof the English alphabet are contained in the sam-ple.
Being aware that a letter transcription is initself an artifact of written language, we chose thisas a good-enough approximation, although operat-ing on phonemes instead of letters would be pref-erable.
The sample contains 1 million words in125,395 sentences with an average length of 7.975words, which are composed of 3.502 letters in av-erage.2.2 Basic notions of graph theoryAs we use graphs for the representation of memoryin both parts of the model, some basic notions ofgraph theory are introduced.
A graph G(V,E)consists of a set of vertices V and a set ofweighted, directed edges between two verticesE?V?V?R with R real numbers.
The first vertexof an edge is called startpoint, the second vertex iscalled endpoint.
A function weight: V?V?Rreturns the weight of edges.
The indegree(outdegree) of a vertex v is defined as the numberof edges with v as startpoint (endpoint).
Thedegree of a vertex is equal to its indegree andoutdegree if the graph is undirected, i.e.
(u,v,w)?Eimplies (v,u,w)?E.
The neighborhood neigh(v) ofa vertex v is defined as the set of vertices s?Swhere (v,s,weight(v,s))?E.The clustering coefficient is the probability thattwo neighbors X and Y of a given vertex Z arethemselves neighbors, which is measured forundirected graphs (Watts and Strogatz, 1998).
Theamount of existing edges amongst the vertices inthe neighborhood of a vertex v is divided by thenumber of possible edges.
The average over allvertices is defined as the clustering coefficient C.The small-world property holds if the averageshortest path length between pairs of vertices iscomparable to a random graph (Erd?s and R?nyi,1959), but its clustering coefficient is much higher.A graph is called scale-free (cf.
Barab?si andAlbert, 1999), if the degree distribution of verticesfollows a power-law.2.3 Word GeneratorThe word generator emits sequences of letters,which are generated randomly in the followingway: The word generator starts with a graph of allN letters it is allowed to choose from.
Initially, allvertices are connected to themselves with weight 1.When generating a word, the generator chooses aletter x according to its probability P(x), which iscomputed as the normalized weight sum ofoutgoing edges:107?
?=VvvweightsumxweightsumxP )()()(.),()()(?
?=yneighuuyweightyweightsumAfter the generation of the first letter, the wordgenerator proceeds with the next position.
At everyposition, the word ends with a probability w?
(0,1)or generates a next letter according to the letterproduction probability as given above.
For everyletter bigram, the weight of the directed edgebetween the preceding and current letter in theletter graph is increased by one.
This results inself-reinforcement of letter probabilities: the moreoften a letter is generated, the higher its weightsum will be in subsequent steps, leading to anincreased generation probability.
Figure 1 showshow a word generator with three letters A,B,Cchanges its weights during the generation of thewords AA, BCB and ABC.Figure 1: Letter graph of the word generator.
Left:initial state.
Right.
: State after generating AA,BCB and ABC.
The numbers next to edges areedge weights.
The probability for the letters for thenext step are P(A)=0.4, P(B)=0.4 and P(C)=0.2.The word end probability w directly influencesthe average word length, which is given by1+(1/w).
For random number generation, we usethe Mersenne Twister (Masumoto and Nishimura,1998).The word generator itself does produce asmooth Zipfian distribution on word frequenciesand a lexical spectrum following a power-law.Figure 2 shows frequency distribution and lexicalspectrum of 1 million words as generated by theword generator with w=0.2 on 26 letters incomparison to a Mandelbrot generator with thesame parameters.
The reader might note that asimilar behaviour could be reached by just settingthe probability of generating a letter according toits relative frequency in previously generatedwords.
The graph seems an unnecessarycomplication for that reason.
But retaining theletter graph with directed edges gives rise to modelthe sequence of letters for a more plausiblemorphological production in future extensions ofthis model, probably in a similar way than in thesentence generator as described in the followingsection.As depicted in figure 2, the word generatorfulfills the requirements on Zipf?s law and thelexical spectrum, yielding a Zipfian exponent ofaround 1 and a power-law exponent of 2 for a largeregime in the lexical spectrum, both matching thevalues as observed previously in natural languagein e.g.
(Zipf, 1949) and (Ferrer i Cancho and Sol?,2002).
In contrast to this, the Mandelbrot modelshows to have a step-wise rank-frequencydistribution and a distorted lexical spectrum.Hence, the word generator itself is already animprovement over previous models as it producesa smooth Zipfian distribution and a lexicalspectrum following a power-law.
But to comply tothe other requirements as given in section 2.1, theprocess has to be extended by a sentence generator.1101001000100001  10  100  1000  10000frequencyrankrank-frequencyword generator w=0.2power law z=1Mandelbrot model1e-0061e-0050.00010.0010.010.111  10  100  1000P(frequency)frequencylexical spectrumword generator w=0.2power law z=2Mandelbrot modelFigure 2: rank-frequency distribution and lexicalspectrum for the word generator in comparison tothe Mandelbrot modelinitial state state after 3 words1082.4 Sentence GeneratorThe sentence generator model retains another di-rected graph, which memorizes words and theirsequences.
Here, vertices correspond to words andedge weights correspond to the number of timestwo words were generated in a sequence.
The wordgraph is initialized with a begin-of-sentence (BOS)and an end-of-sentence (EOS) symbol, with anedge of weight 1 from BOS to EOS.
When gener-ating a sentence, a random walk on the directededges starts at the BOS vertex.
With a new wordprobability (1-s), an existing edge is followed fromthe current vertex to the next vertex according toits weight: the probability of choosing endpoint Xfrom the endpoints of all outgoing edges from thecurrent vertex C is given by?
?==)(),(),()(CneighNNCweightXCweightXwordP .Otherwise, with probability s?
(0,1), a newword is generated by the word generator model,and a next word is chosen from the word graph inproportion to its weighted indegree: the probabilityof choosing an existing vertex E as successor of anewly generated word N is given by.),()(,)()()(???
?===VvVvXvweightXindgwvindgwEindgwEwordPFor each sequence of two words generated, theweight of the directed edge between them is in-creased by 1.
Figure 3 shows the word graph forgenerating in sequence: (empty sentence), AA, AABC, AA, (empty sentence), AA CA BC AA, AACA CA BC.During the generation process, the word graphgrows and contains the full vocabulary used so farfor generating in every time step.
It is guaranteedthat a random walk starting from BOS will finallyreach the EOS vertex.
It can be expected that sen-tence length will slowly increase during the courseof generation as the word graph grows and the ran-dom walk has more possibilities before finally ar-riving at the EOS vertex.
The sentence length isinfluenced by both parameters of the model: theword end probability w in the word generator andthe new word probability s in the sentence genera-tor.
By feeding the word transitions back into thegenerating model, a reinforcement of previouslygenerated sequences is reached.
Figure 4 illustratesthe sentence length growth for various parametersettings of w and s.Figure 3: the word graph of the sentence generatormodel.
Note that in the last step, the second CAwas generated as a new word from the word gen-erator.
The generation of empty sentences happensfrequently.
These are omitted in the output.11010010000  100000  1e+006avg.sentencelengthtext intervalsentence length growthw=0.4 s=0.08w=0.4 s=0.1w=0.17 s=0.22w=0.3 s=0.09x^(0.25);Figure 4: sentence length growth, plotted in aver-age sentence length per intervals of 10,000 sen-tences.
The straight line in the log plot indicates apolynomial growth.It should be noted that the sentence generatorproduces a very diverse sequence of sentenceswhich does not deteriorate in repeating the samesentence all over again in later stages.
Both wordand sentence generator can be viewed as weightedfinite automata (cf.
Allauzen et al, 2003) with self-training.109After having defined our random text genera-tion model, the next section is devoted to testing itaccording to the criteria given in section 2.1.3 Experimental resultsTo measure agreement with our BNC sample, wegenerated random text with the sentence generatorusing w=0.4 and N=26 to match the English aver-age word length and setting s to 0.08 for reaching acomparable sentence length.
The first 50,000 sen-tences were skipped to reach a relatively stablesentence length throughout the sample.
To makethe samples comparable, we used 1 million wordstotaling 125,345 sentences with an average sen-tence length of 7.977.3.1 Word frequencyThe comparison between English and the sentencegenerator w.r.t the rank-frequency distribution isdepicted in figure 5.Both curves follow a power-law with z close to1.5, in both cases the curve is flatter for high fre-quency words as observed by Mandelbrot (1953).This effect could not be observed to this extent forthe word generator alone (cf.
figure 2).1101001000100001  10  100  1000  10000frequencyrankrank-frequencysentence generatorEnglishpower law z=1.5Figure 5: rank-frequency plot for English and thesentence generator3.2 Word lengthWhile the word length in letters is the same in bothsamples, the sentence generator produced morewords of length 1, more words of length>10 andless words of medium length.
The deviation in sin-gle letter words can be attributed to the writingsystem being a transcription of phonemes and fewphonemes being expressed with only one letter.However, the slight quantitative differences do notoppose the similar distribution of word lengths inboth samples, which is reflected in a curve of simi-lar shape in figure 6 and fits well the gamma dis-tribution variant of (Sigurd et al, 2004).1101001000100001000001  10frequencylength in lettersword lengthsentence generatorEnglishgamma distributionFigure 6: Comparison of word length distributions.The dotted line is the function as introduced in(Sigurd et al, 2004) and given by f(x) ?x1.5?0.45x.3.3 Sentence lengthThe comparison of sentence length distributionshows again a high capability of the sentence gen-erator to model the distribution of the Englishsample.
As can be seen in figure 7, the sentencegenerator produces less sentences of length>25 butdoes not show much differences otherwise.
In theEnglish sample, there are surprisingly many two-word sentences.1101001000100001  10  100numberof sentenceslength in wordssentence lengthsentence generatorEnglishFigure 7: Comparison of sentence length distribu-tion.3.4 Neighbor-based co-occurrenceIn this section, the structure of the significantneighbor-based co-occurrence graphs is examined.110The significant neighbor-based co-occurrencegraph contains all words as vertices that have atleast one co-occurrence to another word exceedinga certain significance threshold.
The edges are un-directed and weighted by significance.
Ferrer iCancho and Sol?
(2001) showed that the neighbor-based co-occurrence graph of the BNC is scale-free and the small-world property holds.For comparing the sentence generator sample tothe English sample, we compute log-likelihoodstatistics (Dunning, 1993) on neighboring wordsthat at least co-occur twice.
The significancethreshold was set to 3.84, corresponding to 5%error probability when rejecting the hypothesis ofmutual independence.
For both graphs, we give thenumber of vertices, the average shortest pathlength, the average degree, the clustering coeffi-cient and the degree distribution in figure 8.
Fur-ther, the characteristics of a comparable randomgraph as defined by (Erd?s and R?nyi, 1959) areshown.0.0010.010.11101001000100001  10  100  1000nrof verticesdegree intervaldegree distributionsentence generatorEnglishword generatorpower law z=2Englishsamplesentencegen.wordgen.randomgraph# of ver.
7154 15258 3498 10000avg.
sht.path2.933 3.147 3.601 4.964avg.deg.9.445 6.307 3.069 7cl.coeff.
0.2724 0.1497 0.0719 6.89E-4z 1.966 2.036 2.007 -Figure 8: Characteristics of the neighbor-based co-occurrence graphs of English and the generatedsample.From the comparison with the random graph itis clear that both neighbor-based graphs exhibit thesmall-world property as their clustering coefficientis much higher than in the random graph while theaverage shortest path lengths are comparable.
Inquantity, the graph obtained from the generatedsample has about twice as many vertices but itsclustering coefficient is about half as high as in theEnglish sample.
This complies to the steeper rank-frequency distribution of the English sample (seefig.
5), which is, however, much steeper than theaverage exponent found in natural language.
Thedegree distributions clearly match with a power-law exponent of 2, which does not confirm the tworegimes of different slopes as in (Ferrer i Canchoand Sol?
2001).
The word generator data producedan number of significant co-occurrences that lies inthe range of what can be expected from the 5%error of the statistical test.
The degree distributionplot appears shifted downwards about one decade,clearly not matching the distribution of words insequence of natural language.Considering the analysis of the significantneighbor-based co-occurrence graph, the claim issupported that the sentence generator model repro-duces the characteristics of word sequences innatural language on the basis of bigrams.4 ConclusionIn this work we introduced a random text genera-tion model that fits well with natural language withrespect to frequency distribution, word length, sen-tence length and neighboring co-occurrence.
Themodel was not constrained by any a priori distribu-tion ?
the characteristics emerged from a 2-levelprocess involving one parameter for the word gen-erator and one parameter for the sentence genera-tor.
This is, to our knowledge, the first random textgenerator that models sentence boundaries beyondinserting a special blank character at random:rather, sentences are modeled as a path betweensentence beginning and sentence end which im-poses restrictions on the words possible at sentencebeginnings and endings.
Considering its simplicity,we have therefore proposed a plausible model forthe emergence of large-scale characteristics of lan-guage without assuming a grammar or semantics.After all, our model produces gibberish ?
but gib-berish that is well distributed.The studies of Miller (1957) rendered Zipf?slaw un-interesting for linguistics, as it is a mereartifact of language rather than playing an impor-111tant role in its production, as it emerges when put-ting a monkey in front of a typewriter.
Our modeldoes not only explain Zipf?s law, but many othercharacteristics of language, which are obtainedwith a monkey that follows beaten tracks.
Theseadditional characteristics can be thought of as arti-facts as well, but we strongly believe that the studyof random text models can provide insights in theprocess that lead to the origin and the evolution ofhuman languages.For further work, an obvious step is to improvethe word generator so that it produces morphologi-cally more plausible sequences of letters and tointertwine both generators for the emergence ofword categories.
Furthermore, it is desirable toembed the random generator in models of commu-nication where speakers parameterize languagegeneration of hearers and to examine, which struc-tures are evolutionary stable (see J?ger, 2003).This would shed light on the interactions betweendifferent levels of human communication.AcknowledgementsThe author would like to thank Colin Bannard,Reinhard Rapp and the anonymous reviewers foruseful comments.ReferencesC.
Allauzen, M. Mohri, and B. Roark.
2003.
General-ized algorithms for constructing language models.
InProceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pp.
40?47A.-L. Barab?si and R. Albert.
1999.
Emergence of scal-ing in random networks.
Science, 286:509-512T.
Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, 19(1), pp.
61-74P.
Erd?s and A. R?nyi.
1959.
On Random Graphs I.Publicationes Mathematicae (Debrecen)R. Ferrer i Cancho and R. V. Sol?.
2001.
The small-world of human language.
Proceedings of the RoyalSociety of London B 268 pp.
2261-2266R.
Ferrer i Cancho and R. V. Sol?.
2002.
Zipf?s law andrandom texts.
Advances in Complex Systems, Vol.5No.
1 pp.
1-6L.
Q. Ha, E. Sicilia-Garcia, J. Ming and F.J. Smith.2002.
Extension of Zipf's law to words and phrases.Proceedings of 19th International Conference onComputational Linguistics (COLING-2002), pp.
315-320.G.
J?ger.
2003.
Evolutionary Game Theory and Linguis-tic Typology: A Case Study.
Proceedings of the 14thAmsterdam Colloquium, ILLC, University of Am-sterdam, 2003.I.
Kanter and D. A. Kessler.
1995.
Markov Processes:Linguistics and Zipf?s law.
Physical review letters,74:22S.
R. Kumar, P. Raghavan, S. Rajagopalan and A. Tom-kins.
1999.
Extracting Large-Scale Knowledge Basesfrom the Web.
The VLDB Journal, pp.
639-650B.
B. Mandelbrot.
1953.
An information theory of thestatistical structure of language.
In Proceedings ofthe Symposium on Applications of CommunicationsTheory, LondonM.
Matsumoto and T. Nishimura.
1998.
MersenneTwister: A 623-dimensionally equidistributed uni-form pseudorandom number generator.
ACM Trans.on Modeling and Computer Simulation, Vol.
8, No.1, pp.3-30G.
A. Miller.
1957.
Some effects of intermittent silence,.American Journal of Psychology, 70, pp.
311-314H.
A. Simon.
1955.
On a class of skew distributionfunctions.
Biometrika, 42, pp.
425-440B.
Sigurd, M. Eeg-Olofsson and J. van de Weijer.
2004.word length, sentence length and frequency ?
Zipfrevisited.
Studia Linguistica, 58(1), pp.
37-52M.
Steyvers and J.
B. Tenenbaum.
2005.
The large-scale structure of semantic networks: statisticalanalyses and a model of semantic growth.
CognitiveScience, 29(1)S. P. Thompson and E. L. Newport.
2007.
Statisticallearning of syntax: The role of transitional probabil-ity.
Language Learning and Development,  3, pp.
1-42.D.
J. Watts and S. H. Strogatz.
1998.
Collective dynam-ics of small-world networks.
Nature, 393 pp.
440-442D.
H. Zanette and M. A. Montemurro.
2002.
Dynamicsof text generation with realistic Zipf distribution.arXiv:cond-mat/0212496G.
K. Zipf.
1949.
Human Behavior and the Principle ofleast Effort.
Cambridge, MA: Addison Wesley112
