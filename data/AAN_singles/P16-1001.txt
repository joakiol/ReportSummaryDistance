Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1?11,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNoise reduction and targeted exploration in imitation learning forAbstract Meaning Representation parsingJames Goodman?Andreas Vlachos?Jason Naradowsky?
?Computer Science Department, University College Londonjames@janigo.co.uk, jason.narad@gmail.com?Department of Computer Science, University of Sheffielda.vlachos@sheffield.ac.ukAbstractSemantic parsers map natural languagestatements into meaning representations,and must abstract over syntactic phenom-ena, resolve anaphora, and identify wordsenses to eliminate ambiguous interpre-tations.
Abstract meaning representation(AMR) is a recent example of one suchsemantic formalism which, similar to a de-pendency parse, utilizes a graph to repre-sent relationships between concepts (Ba-narescu et al, 2013).
As with dependencyparsing, transition-based approaches are acommon approach to this problem.
How-ever, when trained in the traditional man-ner these systems are susceptible to the ac-cumulation of errors when they find un-desirable states during greedy decoding.Imitation learning algorithms have beenshown to help these systems recover fromsuch errors.
To effectively use these meth-ods for AMR parsing we find it highlybeneficial to introduce two novel exten-sions: noise reduction and targeted explo-ration.
The former mitigates the noise inthe feature representation, a result of thecomplexity of the task.
The latter targetsthe exploration steps of imitation learningtowards areas which are likely to providethe most information in the context of alarge action-space.
We achieve state-of-the art results, and improve upon standardtransition-based parsing by 4.7 F1points.1 IntroductionMeaning representation languages and systemshave been devised for specific domains, such asATIS for air-travel bookings (Dahl et al, 1994)and database queries (Zelle and Mooney, 1996;bolsteragainstdefensescenterwillattacksTheNATOcyber?sprepdobjnsubjauxpobjnndetpossessivepossbolster-01defend-01centermilitaryattack-01namecyberNATOARG1ARG0ARG1prep-againstnamemodop1Figure 1: Dependency (left) and AMR graph (right) for: ?Thecenter will bolster NATO?s defenses against cyber-attacks.
?Liang et al, 2013).
Such machine-interpretablerepresentations enable many applications relyingon natural language understanding.
The ambi-tion of Abstract Meaning Representation (AMR)is that it is domain-independent and useful in a va-riety of applications (Banarescu et al, 2013).The first AMR parser by Flanigan et al (2014)used graph-based inference to find a highest-scoring maximum spanning connected acyclicgraph.
Later work by Wang et al (2015b) was in-spired by the similarity between the dependencyparse of a sentence and its semantic AMR graph(Figure 1).
Wang et al (2015b) start from the de-pendency parse and learn a transition-based parserthat converts it incrementally into an AMR graphusing greedy decoding.
An advantage of this ap-proach is that the initial stage of dependency pars-ing is well-studied and trained using larger corporathan that for which AMR annotations exist.Greedy decoding, where the parser builds theparse while maintaining only the best hypothesisat each step, has a well-documented disadvantage:error propagation (McDonald and Nivre, 2007).When the parser encounters states during parsingthat are unlike those found during training, it ismore likely to make mistakes, leading to stateswhich are increasingly more foreign and causingerrors to accumulate.1One way to ameliorate this problem is toemploy imitation learning algorithms for struc-tured prediction.
Algorithms such as SEARN(Daum?e III et al, 2009), DAGGER (Ross et al,2011), and LOLS (Chang et al, 2015) addressthe problem of error propagation by iteratively ad-justing the training data to increasingly expose themodel to training instances it is likely to encounterduring test.
Such algorithms have been shown toimprove performance in a variety of tasks includ-ing information extraction(Vlachos and Craven,2011), dependency parsing (Goldberg and Nivre,2013), and feature selection (He et al, 2013).
Inthis work we build on the transition-based pars-ing approach of Wang et al (2015b) and explorethe applicability of different imitation algorithmsto AMR parsing, which has a more complex out-put space than those considered previously.The complexity of AMR parsing affectstransition-based methods that rely on features torepresent structure, since these often cannot cap-ture the information necessary to predict the cor-rect transition according to the gold standard.
Inother words, the features defined are not suffi-cient to ?explain?
why different actions shouldpreferred by the model.
Such instances becomenoise during training, resulting in lower accuracy.To address this issue, we show that the ?-boundKhardon and Wachman (2007), which drops con-sistently misclassified training instances, providesa simple and effective way of reducing noise andraising performance in perceptron-style classifica-tion training, and does so reliably across a range ofparameter settings.
This noise reduction is essen-tial for imitation learning to gain traction in thistask, and we gain 1.8 points of F1-Score using theDAGGER imitation learning algorithm.DAGGER relies on an externally specified ex-pert (oracle) to define the correct action in eachstate; this defines a simple 0-1 loss function foreach action.
Other imitation learning algorithms(such as LOLS, SEARN) and the variant ofDAGGER proposed by Vlachos and Clark (2014)(henceforth V-DAGGER) can leverage a task levelloss function that does not decompose over the ac-tions taken to construct the AMR graph.
Howeverthese require extra computations to roll-out to anend-state AMR graph for each possible action nottaken.
The large action-space of our transition sys-tem makes these algorithms computationally in-feasible, and roll-outs to an end-state for many ofthe possible actions will provide little additionalinformation.
Hence we modify the algorithms totarget this exploration to actions where the clas-sifier being trained is uncertain of the correct re-sponse, or disagrees with the expert.
This providesa further gain of 2.7 F1points.This paper extends imitation learning to struc-tured prediction tasks more complex than previ-ously attempted.
In the process, we review andcompare recently proposed algorithms and showhow their components can be recombined and ad-justed to construct a variant appropriate to the taskin hand.
Hence we invest some effort reviewingthese algorithms and their common elements.Overall, we obtain a final F-Score of 0.70 on thenewswire corpus of LDC2013E117 (Knight et al,2014).
This is identical to the score obtained byWang et al (2015a), the highest so far published.Our gain of 4.5 F1points from imitation learningover standard transition-based parsing is orthogo-nal to that of Wang et al (2015a) from additionaltrained analysers, including co-reference and se-mantic role labellers, incorporated in the featureset.
We further test on five other corpora of AMRgraphs, including weblog domains, and show aconsistent improvement in all cases with the ap-plication of imitation learning using DAGGER andthe targeted V-DAGGER we propose here.2 Transition-based AMR parsingAMR parsing is an example of the wider family ofstructured prediction problems, in which we seeka mapping from an input x ?
X to a structuredoutput y ?
Y .
Here x is the dependency tree,and y the AMR graph; both are graphs and we no-tationally replace x with s1and y with sT, withs1...T?
S. siare the intermediate graph configu-rations (states) that the system transitions through.A transition-based parser starts with an input s1,and selects an action a1?
A, using a classifier.
aiconverts siinto si+1, i.e.
si+1= ai(si).
We termthe set of states and actions ?s1, a1, .
.
.
aT?1, sT?a trajectory of length T .
The classifier p?i is trainedto predict aifrom si, with p?i(s) = arg maxa?Awa??
(s), assuming a linear classifier and a featurefunction ?
(s).We require an expert, pi?, that can indicate whatactions should be taken on each sito reach thetarget (gold) end state.
In problems like POS-tagging these are directly inferable from gold, asthe number of actions (T ) equals the number of2Action Name Param.
Pre-conditions Outcome of actionNextEdge lr?
non-empty Set label of edge (?0, ?0) to lr.
Pop ?0.NextNode lc?
empty Set concept of node ?0to lc.
Pop ?0, and initialise ?.Swap ?
non-empty Make ?0parent of ?0(reverse edge) and its sub-graph.
Pop ?0andinsert ?0as ?1.ReplaceHead ?
non-empty Pop ?0and delete it from the graph.
Parents of ?0become parents of?0.
Other children of ?0become children of ?0.
Insert ?0at the headof ?
and re-initialise ?.Reattach ?
?
non-empty Pop ?0and delete edge (?0, ?0).
Attach ?0as a child of ?.
If ?
hasalready been popped from ?
then re-insert it as ?1.DeleteNode ?
empty; leaf ?0Pop ?0and delete it from the graph.Insert lcInsert a new node ?
with AMR concept lcas the parent of ?0, and insert?
into ?.InsertBelow Insert a new node ?
with AMR concept lcas a child of ?0.Table 1: Action Space for the transition-based graph parsing algorithmAlgorithm 1: Greedy transition-based parsingData: policy pi, start state s1Result: terminal state sT1 scurrent?
s1;2 while scurrentnot terminal do3 anext?
pi(scurrent)scurrent?
anext(scurrent)4 sT?
scurrenttokens with a 1:1 correspondence between them.In dependency parsing and AMR parsing this isnot straightforward and dedicated transition sys-tems are devised.Given a labeled training dataset D, algorithm 1is first used to generate a trajectory for each of theinputs (d ?
D) with pi = pi?, the expert from whichwe wish to generalise.
The data produced fromall expert trajectories (i.e.
?si,d, ai,d?
for all i ?1 .
.
.
T and all d ?
1 .
.
.
D), are used to train theclassifier p?i, the learned classifier, using standardsupervised learning techniques.
Algorithm 1 is re-used to apply p?i to unseen data.
Our transitionsystem (defining A, S), and feature sets are basedon Wang et al (2015b), and are not the main focusof this paper.
We introduce the key concepts here,with more details in the supplemental material.We initialise the state with the stack of the nodesin the dependency tree, root node at the bottom.This stack is termed ?.
A second stack, ?
is ini-tialised with all children of the top node in ?.
Thestate at any time is described by ?, ?, and the cur-rent graph (which starts as the dependency treewith one node per token).
At any stage before ter-mination some of the nodes will be labelled withwords from the sentence, and others with AMRconcepts.
Each action manipulates the top nodesin each stack, ?0and ?0.
We reach a terminalstate when ?
is empty.
The objective function tomaximise is the Smatch score (Cai and Knight,2013), which calculates an F1-Score between thepredicted and gold-target AMR graphs.Table 1 summarises the actions inA.
NextNodeand NextEdge form the core action set, labellingnodes and edges respectively without changing thegraph structure.
Swap, Reattach and ReplaceHeadchange graph structure, keeping it a tree.
We per-mit a Reattach action to use parameter ?
equal toany node within six edges from ?0, excluding anythat would disconnect the graph or create a cycle.The Insert/InsertBelow actions insert a newnode as a parent/child of ?0.
These actions arenot used in Wang et al (2015b), but Insert is verysimilar to the Infer action of Wang et al (2015a).We do not use the Reentrance action of Wang etal.
(2015b), as we found it not to add any benefit.This means that the output AMR is always a tree.Our transition system has two characteristicswhich provide a particular challenge: given a sen-tence, the trajectory length T is theoretically un-bounded; and |A| can be of the order 103to 104.Commonly used transition-based systems have afixed trajectory length T , which often arises nat-urally from the nature of the problem.
In PoS-tagging each token requires a single action, andin syntactic parsing the total size of the graph islimited to the number of tokens in the input.
Thelack of a bound in T here is due to Insert actionsthat can grow the the graph, potentially ad infini-tum, and actions like Reattach, which can movea sub-graph repeatedly back-and-forth.
The ac-tion space size is due to the size of the AMR vo-cabulary, which for relations (edge-labels) is re-stricted to about 100 possible values, but for con-cepts (node-labels) is almost as broad as an En-3Algorithm 2: Generic Imitation LearningData: data D, expert pi?, Loss function F (s)Result: learned classifier C, trained policy p?i1 Initialise C0; for n = 1 to N do2 Initialise En= ?
;3 piRollin= RollInPolicy(pi?, C0...n?1, n);4 piRollout=RollOutPolicy(pi?, C0...n?1, n);5 for d ?
D do6 Predict trajectory s?1:Twith piRollin;7 for s?t?
s?1:Tdo8 foreachajt?
Explore(s?t, pi?, piRollin) do9 ?jt= ?
(d, ajt, s?1:t);10 Predict s?
?t+1:Twith piRollout;11 Ljt= F (s?
?T);12 foreach j do13 ActionCostjt= Ljt?minkLkt14 Add (?t, ActionCostt) to En;15 p?in, Cn= Train(C1...n?1, E1.
.
.
En);glish dictionary.
The large action space and un-bounded T also make beam search difficult to ap-ply since it relies on a fixed length T with com-mensurability of actions at the same index on dif-ferent search trajectories.3 Imitation Learning for StructuredPredictionImitation learning originated in robotics, traininga robot to follow the actions of a human expert(Schaal, 1999; Silver et al, 2008).
The robotmoves from state to state via actions, generatinga trajectory in the same manner as the transition-based parser of Algorithm 1.In the imitation learning literature, the learningof a policy p?i from just the expert generated trajec-tories is termed ?exact imitation?.As discussed, itis prone to error propagation, which arises becausethe implicit assumption of i.i.d.
inputs (si) duringtraining does not hold.
The states in any trajec-tory are dependent on previous states, and on thepolicy used.
A number of imitation learning algo-rithms have been proposed to mitigate error prop-agation, and share a common structure shown inAlgorithm 2.
Table 2 highlights some key differ-ences between them.The general algorithm firstly applies a policypiRollIn(usually the expert, pi?, to start) to thedata instances to generate a set of ?RollIn?
tra-jectories in line 6 (we adopt the terminology of?RollIn?
and ?RollOut?
trajectories from Chang etal.
(2015)).
Secondly a number of ?what if?
sce-narios are considered, in which a different actionajtis taken from a given stinstead of the ac-tual atin the RollIn trajectory (line 8).
Each ofthese exploratory actions generates a RollOut tra-jectory (line 10) to a terminal state, for which aloss (L) is calculated using a loss function, F (sjT),defined on the terminal states.
For a number ofdifferent exploratory actions taken from a state ston a RollIn trajectory, the action cost (or relativeloss) of each is calculated (line 13).
Finally thegenerated ?st, ajt, ActionCostjt?
data are used totrain a classifier, using any cost-sensitive classifi-cation (CSC) method (line 15).
New piRollInandpiRollOutare generated, and the process repeatedover a number of iterations.
In general the startingexpert policy is progressively removed in each it-eration, so that the training data moves closer andcloser to the distribution encountered by just thetrained classifier.
This is required to reduce errorpropagation.
For a general imitation learning al-gorithm we need to specify:?
the policy to generate the RollIn trajectory(the RollInPolicy)?
the policy to generate RollOut trajectories,including rules for interpolation of learnedand expert policies (the RollOutPolicy)?
which one-step deviations to explore with aRollOut (the Explore function)?
how RollOut data are used in the classi-fication learning algorithm to generate p?ii.
(within the Train function)Exact Imitation can be considered a single iter-ation of this algorithm, with piRollInequal to theexpert policy, and a 0-1 binary loss for F (0 lossfor pi?
(st), the expert action, and a loss of 1 forany other action); all one-step deviations from theexpert trajectory are considered without explicitRollOut to a terminal state.In SEARN (Daum?e III et al, 2009), one of thefirst imitation learning algorithms in this frame-work, the piRollInand piRollOutpolicies are identi-cal within each iteration, and are a stochastic blendof the expert and all classifiers trained in previousiterations.
The Explore function considers everypossible one-step deviation from the RollIn trajec-tories, with a full RollOut to a terminal state.
The4Algorithm p?i RollIn RollOut Explore TrainExact Imitation Deterministic Expert only None.
0/1 expert loss All 1-step E1onlySEARN Stochastic Mixture Mixture, step-level stochastic All 1-step EnonlyLOLS Deterministic Learned only Mixture, trajectory-level stoch.
All 1-step E1.
.
.
EnSCB-LOLS Deterministic Learned only Mixture, trajectory-level stoch.
Random E1.
.
.
EnSMILE Stochastic Mixture None.
0/1 expert loss All 1-step E1.
.
.
EnDAGGER Deterministic Mixture None.
0/1 expert loss All 1-step E1.
.
.
EnV-DAGGER Deterministic Mixture Mixture, step-level stochastic All 1-step E1.
.
.
EnAGGREVATE Deterministic Learned only Expert only Random E1.
.
.
EnTable 2: Comparison of selected aspects of Imitation Learning algorithms.Train function uses only the training data fromthe most recent iteration (En) to train Cn.LOLS extends this work to provide a deter-ministic learned policy (Chang et al, 2015), withp?in= Cn.
At each iteration p?inis trained on allpreviously gathered data E1...n; piRollInuses thelatest classifier p?in?1, and each RollOut uses thesame policy for all actions in the trajectory; eitherpi?with probability ?, or p?in?1otherwise.
BothLOLS and SEARN use an exhaustive search ofalternative actions as anExplore function.
Changet al (2015) consider Structured Contextual Ban-dits (SCB) as a partial information case, the SCBmodification of LOLS permits only one cost func-tion call per RollIn (received from the external en-vironment), so exhaustive RollOut exploration ateach step is not possible.
SCB-LOLS Explorepicks a single step t ?
{1 .
.
.
T} at random atwhich to make a random single-step deviation.Another strand of work uses only the expert pol-icy when calculating the action cost.
Ross andBagnell (2010) introduce SMILE, and later DAG-GER (Ross et al, 2011).
These do not RollOut assuch, but as in exact imitation consider all one-stepdeviations from the RollIn trajectory and obtain a0/1 action cost for each by asking the expert whatit would do in that state.
At the nth iteration thetraining trajectories are generated from an inter-polation of pi?and p?in?1, with the latter progres-sively increasing in importance; pi?is used withprobability (1-?
)n?1for some decay rate ?.
p?inistrained using all E1...n. Ross et al (2011) discussand reject calculating an action cost by complet-ing a RollOut from each one-step deviation to aterminal state.
Three reasons given are:1.
Lack of real-world applicability, for examplein robotic control.2.
Lack of knowledge of the final loss function,if we just have the expert?s actions.3.
Time spent calculating RollOuts and callingthe expert.Ross and Bagnell (2014) do incorporate RollOutsto calculate an action cost in their AGGREVATEalgorithm.
These RollOuts use the expert policyonly, and allow a cost-sensitive classifier to betrained that can learn that some mistakes are moreserious than others.
As with DAGGER, the trainedpolicy cannot become better than the expert.V-DAGGER is the variant proposed by Vlachosand Clark (2014) in a semantic parsing task.
It isthe same as DAGGER, but with RollOuts using thesame policy as RollIn.
For both V-DAGGER andSEARN, the stochasticity of the RollOut meansthat a number of independent samples are takenfor each one-step deviation to reduce the varianceof the action cost, and noise in the training data.This noise reduction comes at the expense of thetime needed to compute additional RollOuts.4 Adapting imitation learning to AMRAlgorithms with full RollOuts have particularvalue in the absence of an optimal (or near-optimal) expert able to pick the best action fromany state.
If we have a suitable loss function, thenthe benefit of RollOuts may become worth thecomputation expended on them.
For AMR pars-ing we have both a loss function in Smatch, andthe ability to generate arbitrary RollOuts.We therefore use a heuristic expert.
This re-duces the computational cost at the expense of notalways predicting the best action.
An expert needsan alignment between gold AMR nodes and to-kens in the parse-tree or sentence to determine theactions to convert to one from the other.
Thesealignments are not provided in the gold AMR, andour expert uses the AMR node to token alignmentsof JAMR (Flanigan et al, 2014).
These align-ments are not trained, but generated using regexand string matching rules.
However, trajectoriesare in the range 50-200 actions for most trainingsentences, which combined with the size of |A|makes an exhaustive search of all one-step devia-tions expensive.
Compare this to unlabeled shift-5reduce parsers with 4 actions, or POS tagging with|A| ?
30.4.1 Targeted explorationTo reduce this cost we note that exploring Roll-Outs for all possible alternative actions can be un-informative when the learned and expert policiesagree on an action and none of the other actionsscore highly with the learned policy.
Extendingthis insight we modify the Explore function inAlgorithm 2 to only consider the expert action,plus all actions scored by the current learned pol-icy that are within a threshold ?
of the score forthe best rated action.
In the first iteration, whenthere is no current learned policy, we pick a num-ber of actions (usually 10) at random for explo-ration.
Both SCB-LOLS and AGGREVATE usepartial exploration, but select the step t ?
1 .
.
.
T ,and the action atat random.
Here we optimisecomputational resources by directing the search toareas for which the trained policy is least sure ofthe optimal action, or disagrees with the expert.Using imitation learning to address error prop-agation of transition-based parsing provides the-oretical benefit from ensuring the distribution ofst, atin the training data is consistent with the dis-tribution on unseen test data.
Using RollOuts thatmix expert and learned policies additionally per-mits the learned policy to exceed the performanceof a poor expert.
Incorporating targeted explo-ration strategies in the Explore function makesthis computationally feasible.4.2 Noise ReductionDifferent samples for a RollOut trajectory usingV-DAGGER or SEARN can give very differentterminal states sT(the final AMR graph) fromthe same starting stand atdue to the step-levelstochasticity.
The resultant high variance in the re-ward signal hinders effective learning.
Daum?e IIIet al (2009) have a similar problem, and note thatan approximate cost function outperforms singleMonte Carlo sampling, ?likely due to the noise in-duced following a single sample?.To control noise we use the ?-bound discussedby Khardon and Wachman (2007).
This excludesa training example (i.e.
an individual tuple si, ai)from future training once it has been misclassified?
times in training.
We find that this simple ideaavoids the need for multiple RollOut samples.An attraction of LOLS is that it randomly se-lects either expert or learned policy for each Roll-Out, and then applies this consistently to the wholetrajectory.
Using LOLS should reduce noise with-out increasing the sample size.
Unfortunately theunbounded T of our transition system leads toproblems if we drop the expert from the RollIn orRollOut policy mix too quickly, with many trajec-tories never terminating.
Ultimately p?i learns tostop doing this, but even with targeted explorationtraining time is prohibitive and our LOLS exper-iments failed to provide results.
We find that V-DAGGER with an ?-bound works as a good com-promise, keeping the expert involved in RollIn,and speeding up learning overall.Another approach we try is a form of focusedcosting (Vlachos and Craven, 2011).
Instead ofusing the learned policy for ?% of steps in theRollOut, we use it for the first b steps, and thenrevert to the expert.
This has several potential ad-vantages: the heuristic expert is faster than scoringall possible actions; it focuses the impact of the ex-ploratory step on immediate actions/effects so thatmistakes p?i makes on a distant part of the graphdo not affect the action cost; it reduces noise forthe same reason.
We increase b in each iterationso that the expert is asymptotically removed fromRollOuts, a function otherwise supported by thedecay parameter, ?.4.3 Transition System adaptationsApplying imitation learning to a transition systemwith unbounded T can and does cause problemsin early iterations, with RollIn or RollOut trajec-tories failing to complete while the learned pol-icy, p?i, is still relatively poor.
To ensure every tra-jectory completes we add action constraints to thesystem.
These avoid the most pathological scenar-ios, such as disallowing a Reattach of a previouslyReattached sub-graph.
These constraints are onlyneeded in the first few iterations until p?i learns, viathe action costs, to avoid these scenarios.
They arelisted in the Supplemental Material.
As a final fail-safe we insert a hard-stop on any trajectory onceT > 300.To address the size of |A|, we only consider asubset of AMR concepts when labelling a node.Wang et al (2015b) use all concepts that occurin the training data in the same sentence as thelemma of the node, leading to hundreds or thou-sands of possible actions from some states.
Weuse the smaller set of concepts that were assignedby the expert to the lemma of the current node any-6Exact Imitation Imitation LearningExperiment No ?
?=1 ?-Gain No ?
?=1 IL Gain (?)
IL Gain (No ?)
Total GainAROW, C=10 65.5 66.8 1.3 65.5 67.4 0.6 0.0 1.9AROW, C=100 66.4 66.6 0.2 66.4 67.7 1.1 0.0 1.3AROW, C=1000 66.4 67.0 0.6 66.5 68.2 1.2 0.1 1.8PA, C=100 66.7 66.5 -0.2 67.2 68.7 2.2 0.5 2.0Perceptron 65.5 65.3 -0.2 66.6 68.6 3.3 1.1 3.1Table 3: DAGGER with ?-bound.
All figures are F-Scores on the validation set.
5 iterations of classifier training take placeafter each DAgger iteration.
A decay rate (?)
for pi?of 0.3 was used.where in the training data.
We obtain these assign-ments from an initial application of the expert tothe full training data.We add actions to use the actual word or lemmaof the current node to increase generalisation, plusan action to append ?-01?
to ?verbify?
an unseenword.
This is similar to the work of Werling et al(2015) in word to AMR concept mapping, and isuseful since 38% of the test AMR concepts do notexist in the training data (Flanigan et al, 2014).Full details of the heuristics of the expertpolicy, features used and pre-processing are inSupplemental Material.
All code is availableat https://github.com/hopshackle/dagger-AMR.4.4 Na?
?ve Smatch as Loss FunctionSmatch (Cai and Knight, 2013) uses heuristics tocontrol the combinatorial explosion of possiblemappings between the input and output graphs,but is still too computationally expensive to becalculated for every RollOut during training.
Weretain Smatch for reporting all final results, butuse ?Na?
?ve Smatch?
as an approximation duringtraining.
This skips the combinatorial mapping ofnodes between predicted and target AMR graphs.Instead, for each graph we compile a list of:?
Node labels, e.g.
name?
Node-Edge-Node label concatenations, e.g.leave-01:ARG0:room?
Node-Edge label concatenations, e.g.leave-01:ARG0, ARG0:roomThe loss is the number of entries that appear inonly one of the lists.
We do not convert to anF1score, as retaining the absolute number of mis-takes is proportional to the size of the graph.The flexibility of the transition system meansmultiple different actions from a given state sican lead, via different RollOut trajectories, to thesame target sT.
This can result in many actionshaving the best action cost, reducing the signalin the training data and giving poor learning.
Toencourage short trajectories we break these tieswith a penalty of T/5 to Na?
?ve Smatch.
Multipleroutes of the same length still exist, and are pre-ferred equally.
Note that the ordering of the stackof dependency tree nodes in the transition systemmeans we start at leaf nodes and move up the tree.This prevents sub-components of the output AMRgraph being produced in an arbitrary order.5 ExperimentsThe main dataset used is the newswire (proxy) sec-tion of LDC2014T12 (Knight et al, 2014).
Thedata from years 1995-2006 form the training data,with 2007 as the validation set and 2008 as thetest set.
The data split is the same as that used byFlanigan et al (2014) and Wang et al (2015b).1We first assess the impact of noise reductionusing the alpha bound, and report these experi-ments without Rollouts (i.e.
using DAGGER) toisolate the effect of noise reduction.
Table 3 sum-marises results using exact imitation and DAGGERwith the ?-bound set to discard a training instanceafter one misclassification.
This is the most ex-treme setting, and the one that gave best results.We try AROW (Crammer et al, 2013), Passive-Aggressive (PA) (Crammer et al, 2006), and per-ceptron (Collins, 2002) classifiers, with averagingin all cases.
We see a benefit from the ?-bound forexact imitation only with AROW, which is morenoise-sensitive than PA or the simple perceptron.With DAGGER there is a benefit for all classifiers.In all cases the ?-bound and DAGGER are syn-ergistic; without the ?-bound imitation learningworks less well, if at all.
?=1 was the optimal set-ting, with lesser benefit observed for larger values.We now turn our attention to targeted explo-ration and focused costing, for which we use V-DAGGER as explained in section 4.
For all V-1Formally Flanigan et al (2014; Wang et al (2015b) usethe pre-release version of this dataset (LDC2013E117).
Wer-ling et al (2015) conducted comparative tests on the two ver-sions, and found only a very minor changes of 0.1 to 0.2points of F-score when using the final release.7Authors Algorithmic Approach R P FFlanigan et al (2014) Concept identification with semi-markov model followed byoptimisation of constrained graph that contains all of these.0.52 0.66 0.58Werling et al (2015) As Flanigan et al (2014), with enhanced concept identification 0.59 0.66 0.62Wang et al (2015b) Single stage using transition-based parsing algorithm 0.62 0.64 0.63Pust et al (2015) Single stage System-Based Machine Translation - - 0.66Peng et al (2015) Hyperedge replacement grammar 0.57 0.59 0.58Artzi et al (2015) Combinatory Categorial Grammar induction 0.66 0.67 0.66Wang et al (2015a) Extensions to action space and features in Wang et al (2015b) 0.69 0.71 0.70This work Imitation Learning with transition-based parsing 0.68 0.73 0.70Table 4: Comparison of previous work on the AMR task.
R, P and F are Recall, Precision and F-Score.DAGGER experiments we use AROW with regu-larisation parameter C=1000, and ?=0.3.Figure 2 shows results by iteration of reducingthe number of RollOuts explored.
Only the expertaction, plus actions that score close to the best-scoring action (defined by the threshold) are usedfor RollOuts.
Using the action cost informationfrom RollOuts does surpass simple DAGGER, andunsurprisingly more exploration is better.Figure 3 shows the same data, but by total com-putational time spent2.
This adjusts the picture, assmall amounts of exploration give a faster bene-fit, albeit not always reaching the same peak per-formance.
As a baseline, three iterations of V-DAGGER without targeted exploration (threshold=?)
takes 9600 minutes on the same hardware togive an F-Score of 0.652 on the validation set.Figure 4 shows the improvement using focusedcosting.
The ?n/m?
setting sets b, the number ofinitial actions taken by p?i in a RollOut to n, andthen increases this by m at each iteration.
We gainan increase of 2.9 points from 0.682 to 0.711.
Inall the settings tried, focused costing improves theresults, and requires progressive removal of the ex-pert to achieve the best score.We use the classifier from the Focused Costing5/5 run to achieve an F-Score on the held-out testset of 0.70, equal to the best published result so far(Wang et al, 2015a).
Our gain of 4.7 points fromimitation learning over standard transition-basedparsing is orthogonal to that of Wang et al (2015a)using exact imitation with additional trained anal-ysers; they experience a gain of 2 points fromusing a Charniak parser (Charniak and Johnson,2005) trained on the full OntoNotes corpus insteadof the Stanford parser used here and in Wang et al(2015b), and a further gain of 2 points from a se-mantic role labeller.
Table 4 lists previous AMRwork on the same dataset.2experiments were run on 8-core Google Cloud n1-highmem-8 machines.Validation F-Score Test F-ScoreDataset EI D V-D V-D Rao et alproxy 0.670 0.686 0.704 0.70 0.61dfa 0.495 0.532 0.546 0.50 0.44bolt 0.456 0.468 0.524 0.52 0.46xinhua 0.598 0.623 0.683 0.62 0.52lpp 0.540 0.546 0.564 0.55 0.52Table 5: Comparison of Exact Imitation (EI), DAGGER (D),V-DAGGER (V-D) on all components of the LDC2014T12corpus.Using DAGGER with this system we obtainedan F-Score of 0.60 in the Semeval 2016 task onAMR parsing, one standard deviation above themean of all entries.
(Goodman et al, 2016)Finally we test on all components of theLDC2014T12 corpus as shown in Table 5, whichinclude both newswire and weblog data, as well asthe freely available AMRs for The Little Prince,(lpp)3.
For each we use exact imitation, DAG-GER, and V-DAGGER on the train/validation/splitsspecified in the corpus.
In all cases, imitationlearning without RollOuts (DAGGER) improveson exact imitation, and incorporating RollOuts (V-DAGGER) provides an additional benefit.
Rao etal.
(2015) use SEARN on the same datasets, butwith a very different transition system.
We showtheir results for comparison.Our expert achieves a Smatch F-Score of 0.94on the training data.
This explains why DAG-GER, which assumes a good expert, is effective.Introducing RollOuts provides additional theoret-ical benefits from a non-decomposable loss func-tion that can take into account longer-term impactsof an action.
This provides much more informa-tion than the 0/1 binary action cost in DAGGER,and we can use Na?
?ve Smatch as an approximationto our actual objective function during training.This informational benefit comes at the cost of in-creased noise and computational expense, whichwe control with targeted exploration and focused3http://amr.isi.edu/download.html8Figure 2: Targeted exploration with V-DAGGER by iteration.Figure 3: Targeted exploration with V-DAGGER by time.Figure 4: Focused costing with V-DAGGER.
All runs use threshold of 0.10.costing.
We gain 2.7 points in F-Score, at the costof 80-100x more computation.
In problems with aless good expert, the gain from exploration couldbe much greater.
Similarly, if designing an expertfor a task is time-consuming, then it may be a bet-ter investment to rely on exploration with a poorexpert to achieve the same result.6 Related WorkOther strategies have been used to mitigate the er-ror propagation problem in transition-based pars-ing.
A common approach is to use beam searchthrough state-space for each action choice to finda better approximation of the long-term score ofthe action, e.g.
Zhang and Clark (2008).
Goldbergand Elhadad (2010) remove the determinism ofthe sequence of actions to create easy-first parsers,which postpone uncertain, error-prone decisionsuntil more information is available.
This contrastswith working inflexibly left-to-right along a sen-tence, or bottom-to-top up a tree.Goldberg and Nivre (2012) introduce dynamicexperts that are complete in that they will respondfrom any state, not just those on the perfect trajec-tory assuming no earlier mistakes; any expert usedwith an imitation learning algorithm needs to becomplete in this sense.
Their algorithm takes ex-ploratory steps off the expert trajectory to augmentthe training data collected in a fashion very similarto DAGGER.Honnibal et al (2013) use a non-monotonicparser that allows actions that are inconsistent withprevious actions.
When such an action is takenit amends the results of previous actions to en-sure post-hoc consistency.
Our parser is non-monotonic, and we have the same problem en-countered by Honnibal et al (2013) with manydifferent actions from a state siable to reach thetarget sT, following different ?paths up the moun-tain?.
This leads to poor learning.
To resolvethis with fixed T they break ties with a monotonicparser, so that actions that do not require later cor-rection are scored higher in the training data.
Inour variable T environment, adding a penalty tothe size of T is sufficient (section 4.4).Vlachos and Clark (2014) use V-DAGGER togive a benefit of 4.8 points of F-Score in adomain-specific semantic parsing problem similarto AMR.
Their expert is sub-optimal, with no in-formation on alignment between words in the in-put sentence, and nodes in the target graph.
Theparser learns to link words in the input to one ofthe 35 node types, with the ?expert?
policy align-ing completely at random.
This is infeasible withAMR parsing due to the much larger vocabulary.7 ConclusionsImitation learning provides a total benefit of 4.5points with our AMR transition-based parser overexact imitation.
This is a more complex task thanmany previous applications of imitation learning,and we found that noise reduction was an essen-tial pre-requisite.
Using a simple 0/1 binary actioncost using a heuristic expert provided a benefit of1.8, with the remaining 2.7 points coming fromRollOuts with targeted exploration, focused cost-ing and a non-decomposable loss function that wasa better approximation to our objective.We have considered imitation learning algo-rithms as a toolbox that can be tailored to fit thecharacteristics of the task.
An unbounded T meantthat the LOLS RollIn was not ideal, but this couldbe modified to slow the loss of influence of theexpert policy.
We anticipate the approaches thatwe have found useful in the case of AMR to re-duce the impact of noise, efficiently support largeaction spaces with targeted exploration, and copewith unbounded trajectories in the transition sys-tem will be of relevance to other structured pre-diction tasks.9AcknowledgmentsAndreas Vlachos is supported by the EPSRC grantDiligent (EP/M005429/1) and Jason Naradowskyby a Google Focused Research award.
We wouldalso like to thank our anonymous reviewers formany comments that helped improve this paper.ReferencesYoav Artzi, Kenton Lee, and Luke Zettlemoyer.
2015.Broad-coverage ccg semantic parsing with amr.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages1699?1710, Lisbon, Portugal, September.
Associa-tion for Computational Linguistics.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proceedings of the 7th Linguis-tic Annotation Workshop and Interoperability withDiscourse, pages 178?186, Sofia, Bulgaria, August.Association for Computational Linguistics.Shu Cai and Kevin Knight.
2013.
Smatch: an evalua-tion metric for semantic feature structures.
In ACL(2), pages 748?752.Kai-wei Chang, Akshay Krishnamurthy, Alekh Agar-wal, Hal Daum?e III, and John Langford.
2015.Learning to search better than your teacher.
In Pro-ceedings of the 32nd International Conference onMachine Learning (ICML-15), pages 2058?2066.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
The Journal of Ma-chine Learning Research, 7:551?585.Koby Crammer, Alex Kulesza, and Mark Dredze.2013.
Adaptive regularization of weight vectors.Mach Learn, 91:155?187.Deborah A Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the atistask: The atis-3 corpus.
In Proceedings of the work-shop on Human Language Technology, pages 43?48.Association for Computational Linguistics.Hal Daum?e III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
Machinelearning, 75(3):297?325.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,Chris Dyer, and Noah A Smith.
2014.
A discrim-inative graph-based parser for the abstract meaningrepresentation.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics, pages 1426?1436.
Association for Compu-tational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750.
Association for Computa-tional Linguistics.Yoav Goldberg and Joakim Nivre.
2012.
A dynamicoracle for arc-eager dependency parsing.
In COL-ING, pages 959?976.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the association for ComputationalLinguistics, 1:403?414.James Goodman, Andreas Vlachos, and Jason Narad-owsky.
2016.
Ucl+sheffield at semeval-2016 task8: Imitation learning for amr parsing with an alpha-bound.
In Proceedings of the 10th InternationalWorkshop on Semantic Evaluation.He He, Hal Daum?e III, and Jason Eisner.
2013.
Dy-namic feature selection for dependency parsing.
InEmpirical Methods in Natural Language Process-ing.Matthew Honnibal, Yoav Goldberg, and Mark John-son.
2013.
A non-monotonic arc-eager transitionsystem for dependency parsing.
In Proceedings ofthe Seventeenth Conference on Computational Nat-ural Language Learning, pages 163?172.
Citeseer.Roni Khardon and Gabriel Wachman.
2007.
Noisetolerant variants of the perceptron algorithm.
Thejournal of machine learning research, 8:227?248.Kevin Knight, Laura Baranescu, Claire Bonial,Madalina Georgescu, Kira Griffitt, Ulf Hermjakob,Daniel Marcu, Martha Palmer, and Nathan Schnei-der.
2014.
Abstract meaning representation (amr)annotation release 1.0.
Linguistic Data ConsortiumCatalog.
LDC2014T12.Percy Liang, Michael I Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.10Ryan T McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency parsingmodels.
In EMNLP-CoNLL, pages 122?131.Xiaochang Peng, Linfeng Song, and Daniel Gildea.2015.
A synchronous hyperedge replacement gram-mar based approach for amr parsing.
CoNLL 2015,page 32.Michael Pust, Ulf Hermjakob, Kevin Knight, DanielMarcu, and Jonathan May.
2015.
Using syntax-based machine translation to parse english intoabstract meaning representation.
arXiv preprintarXiv:1504.06665.Sudha Rao, Yogarshi Vyas, Hal Daume III, and PhilipResnik.
2015.
Parser for abstract meaning repre-sentation using learning to search.
arXiv preprintarXiv:1510.07586.St?ephane Ross and Drew Bagnell.
2010.
Efficientreductions for imitation learning.
In 13th Inter-national Conference on Artificial Intelligence andStatistics, pages 661?668.Stephane Ross and J Andrew Bagnell.
2014.
Rein-forcement and imitation learning via interactive no-regret learning.
arXiv preprint arXiv:1406.5979.St?ephane Ross, Geoffrey J Gordon, and J Andrew Bag-nell.
2011.
A reduction of imitation learning andstructured prediction to no-regret online learning.
In14th International Conference on Artificial Intelli-gence and Statistics, volume 15, pages 627?635.Stefan Schaal.
1999.
Is imitation learning the routeto humanoid robots?
Trends in cognitive sciences,3(6):233?242.David Silver, James Bagnell, and Anthony Stentz.2008.
High performance outdoor navigation fromoverhead data using imitation learning.
Robotics:Science and Systems IV, Zurich, Switzerland.Andreas Vlachos and Stephen Clark.
2014.
A new cor-pus and imitation learning framework for context-dependent semantic parsing.
Transactions of the As-sociation for Computational Linguistics, 2:547?559.Andreas Vlachos and Mark Craven.
2011.
Search-based structured prediction applied to biomedicalevent extraction.
In Proceedings of the FifteenthConference on Computational Natural LanguageLearning, pages 49?57.
Association for Computa-tional Linguistics.Chuan Wang, Nianwen Xue, and Sameer Pradhan.2015a.
Boosting transition-based amr parsing withrefined actions and auxiliary analyzers.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 857?862,Beijing, China, July.
Association for ComputationalLinguistics.Chuan Wang, Nianwen Xue, and Sameer Pradhan.2015b.
A transition-based algorithm for amr pars-ing.
North American Association for ComputationalLinguistics, Denver, Colorado.Keenon Werling, Gabor Angeli, and Christopher D.Manning.
2015.
Robust subgraph generation im-proves abstract meaning representation parsing.
InProceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages982?991, Beijing, China, July.
Association for Com-putational Linguistics.John M Zelle and Raymond J Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the National Con-ference on Artificial Intelligence, pages 1050?1055.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.11
