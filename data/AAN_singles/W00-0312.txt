Building a Robust Dialogue System with Limited Data *Sharon  J .
Go ldwater ,  E l i zabeth  Owen Brat t ,  Jean  Mark  Gawron ,  and  John  Dowdingt, .
SR I  In ternat iona l333 Ravenswood AvenueMen lo  Park ,  CA 94025{goldwater, owen, gawron, dowding} @ai.sri.coraAbst ractWe describe robustness techniques used in the Com-mandTalk system at: the recognition level, the pars-ing level, and th~ dia16gue level, and how these wereinfluenced by the lack of domain data.
We usedinterviews with subject matter experts (SME's) todevelop a single grammar for recognition, under-standing, and generation, thus eliminating the needfor a robust parser.
We broadened the coverage ofthe recognition grammar by allowing word insertionsand deletions, and we implemented clarification andcorrection subdialogues to increase robustness at thedialogue level.
We discuss the applicability of thesetechniques to other domains.1 I n t roduct ionThree types of robustness must be considered whendesigning a dialogue system.
First, there is robust-ness at the recognition level.
When plentiful datais available, a robust n-gram language model can beproduced, but when data is limited, producing a ro-bust language model for recognition can be prob-lematic.
Second, there is robustness at the levelof the parser.
Robust parsing is often achieved bycombining a full parser with a partial parser andfragment-combining rules, but even then some utter-ances may be correctly recognized, only to be parsedincorrectly or not at all.
Finally, there is robustnessat the dialogue level.
Utterances may be uninter-pretable within the context of the dialogue due toerrors on the part of either the system or the user,and the dialogue manager should be able to handlesuch problems gracefully.Our CommandTalk dialogue system was designedfor a highly specialized omain with little availabledata, so finding ways to build a robust system with* This research was supported by the Defense Advanced Re-search Projects Agency under Contract N66001-94-C-6046with the Space and Naval Warfare Systems Center.
The viewsand conclusions contained in this document are those of theauthors and should not be interpreted asnecessarily repre-senting the official policies, either express or implied, of theDefense Advanced Research Projects Agency of the U.S.
Gov-ernment.?
Currently affiliated with GO.cornlimited data was a major concern.
In this paper,we discuss our methods and their applicability toother domains.
Section 2 gives a brief overview ofthe CommandTalk system.
In Section 3, we discussthe approach we took to building recognition, under-standing, and generaffon models for CommandTalk,and how it relates to the first two types of robustnessmentioned.
Section 4 discusses additional robust-ness techniques at the recognizer level, and Section 5describes dialogue-level robustness techniques.
Sec-tion 6 discusses the applicability of our methods toother domains.2 CommandTa lkCommandTalk is a spoken-language interface to theModSAF (Modular Semi-Automated Forces) battle-field simulator, developed with the goal of allow-ing military commanders to interact with simulatedforces in a manner as similar as possible to the waythey would command actual forces.
CommandTalkallows the use of ordinary English commands andmouse gestures to?
Create forces and control measures (points andlines)?
Assign missions to forces?
Modify missions during execution?
Control ModSAF system functions, such as themap display?
Get information about the state of the simula-tionCommandTalk consists of a number of indepen-dent, cooperating agents interacting through SRI'sOpen Agent Architecture (OAA) (Martin et al,1998).
OAA uses a facilitator agent that plans andcoordinates interactions among agents during dis-tributed computation.
An introduction to the basicCommandTalk agents can be found in Moore et al(1997).
CommandTalk's dialogue component is de-scribed in detail in Stent et al (1999), and its useof linguistic and situational context is described inDowding et al (1999).613 The  One-Grammar  ApproachIn a domain with limited data, the inability to col-lect a sufficient corpus for training a statistical lan-guage model can be a significant problem.
ForCommandTalk, we did not create a statistical lan-guage model.
Instead, with information gatheredfrom interviews of subject matter experts (SME's),we developed a handwritten grammar using Gemini(Dowding et al, 1993), a unification-based gram-mar formalism.
We used this unification grammarfor both natural language understanding and gener-ation, and, using a grammar compiler we developed,compiled it into a context-free form suitable for thespeech recognizer as well.The effe~s_ of this single-grammar pproach onthe robustness of the CommandTalk system weretwofold.
On the negative side, we presumably endedup with a recognition language model with less cov-erage than a statistical model would have had.
Ourattempts to deal with this are discussed in the nextsection.
On the positive side, we eliminated theusual discrepancy incoverage between the recognizerand the natural language parser.
This was advanta-geous, since no fragment-combining or other parsingrobustness techniques were needed.Our approach ad other advantages a well.
Anychanges we made to the understanding grammarwere automatically reflected in the recognition andgeneration grammars, making additions and modifi-cations efficient.
Also, anecdotal evidence suggeststhat the language used by the system often influ-ences the language used by speakers, o maintainingconsistency between the input and output of the sys-tem is desirable.4 Ut terance-Leve l  RobustnessIt is difficult o write a grammar that is constrainedenough to be useful without excluding some rea-sonable user utterances.
To alleviate this prob-lem, we modified the speech recognition grammarand natural language parser to allow certain "close-to-grammar" utterances.
Utterances with insertedwords, such as Center on Checkpoint 1 now or zoomway out (where Center on Checkpoint 1 and zoomout are grammatical) were permitted by allowingthe recognizer to skip unknown words.
We also al-lowed utterances with deleted words, as long as thosewords did not contribute to the semantics of the ut-terance as determined by the Gemini semantic rulesconstraining logical forms.
For example, a user couldsay, Set speed, 40 kph rather than Set speed to 40 kph.The idea behind these modifications was to allow ut-terances with a slightly broader ange of wordingsthan those in the grammar, but with essentially thesame meanings:We began by testing the effects of these modi-fications on in-grammar utterances, to ensure thatTime, CPURTSRRAWERSERNon-Robust Robust0.664 : 1.052.56% 1.70%1.68% 2.94%10.00% ~ 12.07%Table 1: In-Grammar Recognition Resultsthey did not significantly decr egse recognition per-formance.
We used a small test corpus of approxi-mately 800 utterances read by SRI employees.
Wecollected four measures of performance:?
Recognition time, measured, in multiples ofCPU real time (CPURT).
A recognition timeof lxCPURT means that on,our CPU (a SunUltra2), recognition took exactly as~ long as theduration of the utterance.
:?
Sentence reject rate (SRR).'
The percentage ofsentences that the recognizer rejects.?
Adjusted word error rate (A:WER).
The per-centage of words in non:rejected sentences thatare misrecognized.?
Sentence rror rate (SER).
The percentage ofsentences in which some sort of error occurred,either a complete rejection or misrecognizedword.Several parameters affected the results, most no-tably the numerical penalties assigned for insertingor deleting words, and the pruning threshold of therecognizer.
Raising the pruning threshold causedboth reject and error rates to go down, but slowedrecognition.
Lowering the penalties caused rejectionrates to go down, but word and Sentence rror ratesto go up, since some sentences which had been re-jected were now recognized partially correctly, andsome sentences which had been recognized correctlynow included some errors.
Lowering the penaltiesalso led to slower recognition.Table 1 shows recognition results for the non-robust and robust versions 0f the recognition gram-mar on in-grammar utterances: Th e pruning thresh-old is the same for both versions and the insertionand deletion penalties are set to intermediate val-ues.
Recognition times for the robust grammar areabout 60% slower than those of the control gram-mar, but still at acceptable l vels.
Reject and errorrates are fairly close for the two grammars.
Overall,adding robustness to the recognition grammar didnot severely penalize in-grammar recognition per-formance.We had very little out-of-grammar data for Com-mandTalk, and finding subjects in this highly spe-cialized domain would have been difficult and ex-pensive.
To test our robustness techniques on out-62of-grammar utterances, we decided to port themto another domain with easily accessible users anddata; namely, the ATIS air travel domain.
We wrotea small grammar covering part of the ATIS dataand ,compiled it into a recognition grammar usingthe same techniques as in CommandTalk.
Unfortu-nately, we were unable to carry out any experiments,because the recognition grammar we derived yieldedrecognition times that were so slow as to be imprac-tical.
We discuss these results further in Section 6.5 Diaiogue-Level RobustnessTo be considered robust at the dialogue level, a sys-tem must be able to deal with situations where anutterance is recognized and parsed, but cannot be in-terpreted withi~4he current system state or dialoguecontext.
In addition~it must be easy for the user tocorrect faulty interpretations on the part of the sys-tem.
Contextual interpretation problems may occurfor a variety of reasons, including misrecognitions,incorrect reference resolution, and confusion or in-completeness on the part of the user.The CommandTalk dialogue manager maintainsa Stack to ~keep 'track of the current discourse con-text and uses small finite-state machines to representdifferent~ types of subdialogues.
Below we illustratesome types of  subdialogues and other techniqueswhich provide robustness at the dialogue level.
Notethat for each utterance, we write what the systemrecognizes, not what the user actually says.5.1 Correction SubdlaloguesSx?
1:U 1 :Create a CEV at 76 53S 2 ?.U 3 Put Objective Golf here <click>S 4 ?
I will locate Objective Golf at FQ?
658 583U 5 I said Objective AlphaS 6 ?
I will locate Objective Alpha at FQ658 853Allowing the user to correct full or partial utterancescan remedy interpretation problems caused by mis-recognitions, incorrect reference resolution, or usererror.In Example 1, the system responds to the user'sfirst utterance by producing a rising tone, illustratedby the ?
symbol, to indicate successful interpreta-tion and execution of the command, in this case cre-ation of a CEV, a type of vehicle.
(Unsuccessfulinterpretation is indicated by a falling tone, illus-trated by the ?
symbol.)
In utterances 3 through6, a misrecognition causes the system to performthe wrong behavior.
The user initiates a correctionsubdialogue, and the system goes on to correctly re-interpret he full utterance.5.2 Impl ic i t  Conf i rmat ionEx.
2:U 7 CEV, advance to Checkpoint 1S 8 ?
100All will proceed to Checkpoint1Use of implicit confirmation in combination withcorrection subdialogues makes it easy to correctfaulty interpretations as soon as possible by alert-ing the user to possible sources of error.In utterances 7 and 8, the system must resolvethe user's reference, "CEV", to a particular unit.It therefore choes the user's command using theCEV's unique call sign.
This makes explicit the sys-tem's interpretation of the user's utterance, givingthe user a chance to correct he system if necessary.Note that utterance 4 also contains an implicitconfirmation, since the system has resolved theuser's gesture to a set of coordinates.5.3 C lar i f i ca t ion  Subd ia loguesEx.
3:U 9S 10U 11Create a CEV at 72 69?CEV, conduct crater breach facing-southS 12 ?
There are two CEV's.
Do you mean100All or 100A12?U 13 A12S 14 ?
Which point should 100A12 conductcrater breach oriented south at?U 15 Checkpoint 2S 16 ?
There is no Checkpoint 2.
Whichpoint should 100A12 conduct craterbreach oriented south at?U 17 I meant Checkpoint 1S 18 ?
100A12 will conduct crater breachoriented south at Checkpoint 1Clarification subdialogues are generally initiated bythe system as a result of errors or incomplete com-mands on the part of the user.Example 3 illustrates three different ypes of prob-lems that can be corrected by system questions.First, the user's reference to "CEV" in utterance11 is ambiguous, so the system asks a question todetermine which CEV the user is referring to.
Next,the system asks the user to supply a missing pieceof information that is required to carry out the com-mand.
Finally, when the user makes an error byreferring to a point that doesn't exist, the systemprompts for a correction.6 Discussion and ConclusionsCommandTalk is an example of a successful and ro-bust dialogue system in a domain with limited ac-63cess to both data and subjects.
The pre-dialogueversion of CommandTalk was used in the STOW(Synthetic Theater of War) '97 ACTD (AdvancedConcept Technology Demonstration) exercise, an in-tensive 48-hour continuous military simulation byall four U.S. military services, and received highpraise.
The dialogue portion of the system has in-creased CommandTalk's usefulness and robustness.Nevertheless, everal questions remain, not the leastof which is whether the robustness techniques usedfor CommandTalk can be successfully transferred toother domains.We have no doubt that our methods for adding ro-bustness at the dialogue level can and should be im-plemented in other domains, but this is not as clearfor our parsing a-nd recognition robustness methods.The one-grammar approach is key to our elimi-nating the necessity for robust parsing, renders alarge corpus for generating a recognition model un-necessary, and has other advantages as well.
Yetour experience in the ATIS domain suggests thatfurther research into this approach is needed.
OurATIS grammar is based on a grammar of generalEnglish and has a very different structure from thatof CommandTalk's semantic grammar, but we wereunable to isolate the factor or factors responsible forits poor recognition performance.
Recent research(Rayner et al, 2000) suggests that it may be pos-sible to compile a useful recognition model from ageneral English unification grammar if the gram-mar is constructed carefully and a few compromisesare made.
We also believe that using an appropri-ate grammar approximation algorithm to reduce thecomplexity of the recognition model may prove fruit-ful.
This would reintroduce some discrepancy be-tween the recognition and understanding languagemodels, but maintain the other advantages of theone-grammar pproach.In either case, the effectiveness of our recognitionrobustness techniques remains an open question.
Weknow they have no significant negative impact on in-grammar ecognition, but whether they are helpfulin recognizing and~ more importantly, interpretingout-of-grammar utterances is unknown.
We havebeen unable to evaluate them so far in the Com-mandTalk or any other domain, although we hopeto do so in the future.Another possible solution to the problem ofproducing a workable robust recognition grammarwould return to a statistical approach rather thanusing word insertions and deletions.
Stolcke andSegal (1994) describe a method for combining acontext-free grammar with an n-gram model gen-erated from a small corpus of a few hundred utter-ances to create a more accurate n-gram model.
Thismethod would provide a robust recognition modelbased on the context-free grammar compiled from64our unification grammar.
We would'still have towrite only one grammar for the system, it would stillinfluence the recognition model, and we could stillbe sure that the system would never say anything itcouldn't recognize.
This approach Would require us-ing robust parsing methods, but might be the bestsolution for other domains if compiling a practicalrecognition grammar proves too difficult.Despite the success of the CommandTalk system,it is clear that more investigation is called for todetermine how best to develop dialogue systems indomains with limited data.
Researchers must de-termine which types of unification grammars can becompiled into practical recognition grammars usingexisting technology, whether grammar approxima-tions or other techniques can produce good resultsfor a broader range of grammars, whether allow-ing word insertions and deletions is an effective ro-bustness technique, orwhether we should use othermethods altogether.Re ferencesJ.
Dowding, J. Gawron, D. Appelt, L. Cherny,R.
Moore, and D. Moran.
1993.
Gemini: A Natu-ral Language System for Spoken Language Under-standing.
In Proceedings of the Thirty-First An-nual Meeting of the ACL, Columbus, OH.
Associ-ation for Computational Linguistics.J.
Dowding, E. Owen Bratt, and S. Goldwater.1999.
Interpreting Language in Context in Com-mandTalk.
In Communicative Agents: The Useof Natural Language in Embodied Systems, pages63-67.D.
Martin, A. Cheyer, and D. Moran.
1998.
Build-ing Distributed Software Systems with the OpenAgent Architecture.
In Proceedings of the ThirdInternational Conference on the Practical Appli-cation of Intelligent Agents and Multi-Agent Tech-nology, Blackpool, Lancashire, UK.
The PracticalApplication Company Ltd.R.
Moore, J. Dowding, H. Bratt, J. Gawron,Y.
Gorfu, and A. Cheyer.
1997.
CommandTalk:A Spoken-Language Interface for Battlefield Sim-ulations.
In Proceedings of the Fifth Conferenceon Applied Natural Language Processing, pages1-7, Washington, DC.
Association for Computa-tional Linguistics.M.
Rayner, B.
A. Hockey, F. James, E. Owen Bratt,S.
Goldwater, and J. M. Gawron.
2000.
Compil-ing Language Models from a Linquistically Moti-vated Unification Grammar.
Submitted to COL-ING '00.A.
Stent, J. Dowding, J. Gawron, E. Owen Bratt,and R. Moore.
1999.
The CommandTalk SpokenDialogu.e System.
In Proceedings of the 37th An-nual Meeting of the A CL.
Association of Compu-tational Linguistics.A.
Stolcke and J. Segal.
1994.
Precise N-GramProbabilities from Stochastic Context-free Gram-mar.
: In Proceedings of the 32nd Annual Meetingoff :the ~Association for Computational Linguistics,pages 74~-79,65
