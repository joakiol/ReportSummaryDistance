Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?94,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUsing Domain-specific and Collaborative Resources for Term TranslationMihael Arcan, Paul BuitelaarUnit for Natural Language ProcessingDigital Enterprise Research InstituteGalway, Irelandfirstname.lastname@deri.orgChristian FedermannLanguage Technology LabGerman Research Center for AISaarbru?cken, Germanycfedermann@dfki.deAbstractIn this article we investigate the translationof terms from English into German and viceversa in the isolation of an ontology vocab-ulary.
For this study we built new domain-specific resources from the translation searchengine Linguee and from the online encyclo-pedia Wikipedia.
We learned that a domain-specific resource produces better results thana bigger, but more general one.
The first find-ing of our research is that the vocabulary andthe structure of the parallel corpus are impor-tant.
By integrating the multilingual knowl-edge base Wikipedia, we further improved thetranslation wrt.
the domain-specific resources,whereby some translation evaluation metricsoutperformed the results of Google Translate.This finding leads us to the conclusion thata hybrid translation system, a combination ofbilingual terminological resources and statis-tical machine translation can help to improvetranslation of domain-specific terms.1 IntroductionOur research on translation of ontology vocabulariesis motivated by the challenge of translating domain-specific terms with restricted or no additional textualcontext that in other cases can be used for transla-tion improvement.
For our experiment we startedby translating financial terms with baseline systemstrained on the EuroParl (Koehn, 2005) corpus andthe JRC-Acquis (Steinberger et al, 2006) corpus.Although both resources contain a large amount ofparallel data, the translations were not satisfying.
Toimprove the translations of the financial ontologyvocabulary we built a new parallel resource, whichwas generated using Linguee1, an online translationquery service.
With this data, we could train a smallsystem, which produced better translations than thebaseline model using only general resources.Since the manual development of terminologicalresources is a time intensive and expensive task, weused Wikipedia as a background knowledge baseand examined articles, tagged with domain-specificcategories.
With this extracted domain-specific datawe built a specialised English-German lexicon tostore translations of domain-specific terms.
Theseterms were then used in a pre-processing method inthe decoding approach.
This approach incorporatesthe work by Aggarwal et al (2011), which suggestsa sub-term analysis.
We split the financial termsinto n-grams and search for financial sub-terms inWikipedia.The remainder of the paper is organised like this.In Section 2 we describe related work while in Sec-tion 3 the ontology data, the training data that weused in training the language model, and the trans-lation decoder are discussed.
Section 4 presents thenew resources which were used for improving theterm translation.
In Section 5 we discuss the resultsof exploiting the different resources.
We concludewith a summary and give an outlook on future workin Section 6.2 Related WorkKerremans (2010) presents the issue of terminologi-cal variation in the context of specialised translationon a parallel corpus of biodiversity texts.
He showsthat a term often cannot be aligned to any term in1See www.linguee.com86the target language.
As a result, he proposes thatspecialised translation dictionaries should store dif-ferent translation possibilities or term variants.Weller et al (2011) describe methods for termi-nology extraction and bilingual term alignment fromcomparable corpora.
In their compound translationtask, they are using a dictionary to avoid out-of-domain translation.Zesch et al (2008) address issues in accessingthe largest collaborative resources: Wikipedia andWiktionary.
They describe several modules andAPIs for converting a Wikipedia XML Dump into amore suitable format.
Instead of parsing the largeWikipedia XML Dump, they suggest to store theDump into a database, which significantly increasesthe performance in retrieval time of queries.Wikipedia has not only a dense link structure be-tween articles, it has also inter-language links be-tween articles in different languages, which was themain reason to use this invaluable collaborative re-source.
Erdmann et al (2008) regarded the titles ofWikipedia articles as terminology.
They assumedthat two articles connected by an Interlanguage linkare likely to have the same content and thus anequivalent title.Vivaldi and Rodriguez (2010) proposed a method-ology for term extraction in the biomedical domainwith the help of Wikipedia.
As a starting point, theymanually select a set of seed words for a domain,which is used to find corresponding nodes in this re-source.
For cleaning their collected data, they usethresholds to avoid storing undesirable categories.Mu?ller and Gurevych (2008) use Wikipedia andWiktionary as knowledge bases to integrate seman-tic knowledge into Information retrieval.
Theirmodels, text semantic relatedness (for Wikipedia)and word semantic relatedness (for Wiktionary),are compared to a statistical model implemented inLucene.
In their approach to Bilingual Retrieval,they use the cross-language links in Wikipedia,which improved the retrieval performance in theirexperiment, especially when the machine translationsystem generated incorrect translations.3 ExperimentsOur experiment started with an analysis of the termsin the ontology to be translated, which was storedin RDF2 data model.
These terms were used toautomatically extract any corresponding WikipediaCategories, which helped us to define more exactlythe domain(s) of the ontology to be translated.
Thecollected Categories were further used to build adomain-specific lexicon to be used for improvingterm translation.
At the same time a new parallelcorpus was built, which was also generated with thehelp of the ontology terms.
This new data was thenused to pre-process the input data for the decoderand to build a specialised training model whichyielded to a translation improvement.In this section, several types of data will bepresented and furthermore the translation decoder,which has to access this data to build the trainingmodels.
Section 3.1 gives an overview of the datathat was used in translation.
In Sections 3.2 and3.3 we describe the data that is used to train thetranslation and language model.
We used differ-ent parallel corpora, JRC-Acquis, EuroParl and adomain-specific corpus built from Linguee.
In Sec-tion 3.4, we discuss a domain-specific lexicon, ex-tracted from Wikipedia.
In the last Section 3.5 wedescribe the phrase-based machine translation de-coder Moses that we used for our experiments.3.1 xEBR DatasetFor the translation dataset a financial ontology de-veloped by the XBRL European Business Registers3(xEBR) Working Group was used.
This financialontology is a framework for describing financial ac-counting and profile information of business entitiesacross Europe, see also Declerck et al (2010).
Theontology holds 263 concepts and is partially trans-lated into German, Dutch, Spanish, French and Ital-ian.
The terms in each language are aligned viathe SKOS4 Exact Match mechanism to the xEBRcore taxonomy.
In this partially translated taxon-omy, we identified 63 English financial terms andtheir German equivalents, which were used as refer-ence translations in evaluating the different experi-ment steps.The xEBR financial terms are not really termsfrom a linguistic point of view, but they are usedin financial or accounting reports as unique finan-2RDF: Resource Description Framework3XBRL: eXtensible Business Reporting Language4SKOS: Simple Knowledge Organization System87Length Count Examples11 1 Taxes Remuneration And Social SecurityPayable After More Than One Year10 2 Amounts Owed To Credit Institutions AfterMore Than One Year, Variation In Stocks OfFinished Goods And Work In Progress.
.
.2 57 Net Turnover, Liquid Assets, .
.
.1 10 Assets, Capital, Equity, .
.
.Table 1: Examples of xEBR termscial expressions or tags to organize and retrieve au-tomatically reported information.
Therefore it is im-portant to translate these financial terms exactly.Table 1 illustrates the structure of xEBR terms.It is obvious that they are not comparable to gen-eral language, but instead are more like headlines innewspapers, which are often short, very informativeand written in a telegraphic style.
xEBR terms areoften only noun phrases without determiners.
Thelength of the financial terms varies, e.g.
the longestfinancial term considered for translation has a lengthof 11 tokens, while others may consist of 1 or 2.3.2 General Resources: EuroParl andJRC-AcquisAs a baseline, the largest available parallel corporawere used: EuroParl and the JRC-Acquis parallelcorpus.
The EuroParl parallel corpus holds the pro-ceedings of the European Parliament in 11 Europeanlanguages.
The JRC-Acquis corpus is available inalmost all EU official languages (except Irish) and isa collection of legislative texts written between 1950and today.Although research work proved, that a trainingmodel built by using a general resource cannot beused to translate domain-specific terms (Wu et al,2008), we decided to train a baseline model on theseresources to illustrate any improvement steps from ageneral resource to specialised domain resources.3.3 Domain Resource: LingueeLinguee is a combination of a dictionary and asearch engine, which indexes around 100 Millionbilingual texts on words and expressions.
Lingueesearch results show example sentences that depicthow the searched expression has been translated incontext.In contrast to translation engines like GoogleTranslate and Bing Translator, which give you themost probable translation of a source text, every en-try in the Linguee database has been translated byhumans.
The bilingual dataset was gathered fromthe web, particularly from multilingual websitesof companies, organisations or universities.
Othersources include EU documents and patent specifica-tions.The language pairs available for query-ing are English?German, English?Spanish,English?French and English?Portuguese.Since Linguee includes EU documents, they alsouse parallel sentences from EuroParl and JRC-Acquis.
We investigated the proportion of sentencesreturned by Linguee which are contained in Eu-roParl or JRC-Acquis.
The outcome is that the num-ber of sentences is very low, where 131 sentences(0.54%) are gathered from JRC-Acquis corpus and466 (1.92%) from EuroParl.3.4 Collaborative Resource: WikipediaWikipedia is a multilingual, freely available ency-clopedia that was built by a collaborative effort ofvoluntary contributors.
All combined Wikipediashold approximately 20 million articles or more than8 billion words in more than 280 languages.
Withthese facts it is the largest collection of freely avail-able knowledge5.With the heavily interlinked information base,Wikipedia forms a rich lexical and semantic re-source.
Besides a large amount of articles, italso holds a hierarchy of Categories that WikipediaArticles are tagged with.
It includes knowledgeabout named entities, domain-specific terms andword senses.
Furthermore, the redirect system ofWikipedia articles can be used as a dictionary forsynonyms, spelling variations and abbreviations.3.5 Translation System: MosesFor generating translations from English into Ger-man and vice versa, the statistical translation toolkitMoses (Koehn et al, 2007) was used to build thetraining model and for decoding.
For this approach,a phrase-based approach was taken instead of a treebased model.
Further, we aimed at improving thetranslations only on the surface level, and thereforeno part-of-speech information was taken into ac-count.
Word and phrase alignments were built with5http://en.wikipedia.org/wiki/Wikipedia:Size_comparison88the GIZA++ toolkit (Och and Ney, 2003), wherebythe 5-gram language model was built by SRILM(Stolcke, 2002).4 Domain-specific Resource GenerationIn this section, two different types of data and theapproach of building them will be presented.
Sec-tion 4.1 gives an overview of generating a paral-lel resource from Linguee, which was used in gen-erating a new domain-specific training model.
InSection 4.2 a detailed description is given how weextracted terms from Wikipedia for generating adomain-specific lexicon.4.1 Domain-specific parallel corpus generationTo build a new training model that is specialised onour xEBR ontology, we used the Linguee search en-gine.
This resource can be queried on single wordsand on word expressions with or without quotationmarks.
We stored the HTML output of the Lingueequeries on our financial terms and parsed these filesto extract plain parallel text.
From this, we built a fi-nancial parallel corpus with 13,289 translation pairs,including single words, multi-word expressions andsentences.
The English part of the parallel resourcecontained 410,649 tokens, the German part 347,246.4.2 Domain-specific lexicon generationTo improve translation based on the domain-specificparallel corpus, we built a cross-lingual terminolog-ical lexicon extracted from Wikipedia.
From theWikipedia Articles we used different informationunits, i.e.
the Title of a Wikipedia Article, the Cat-egory (or Categories) of the Title and the internalInterwikiInterlanguage links of the Title.
The concept ofInterwiki links can be used to make links to otherWikipedia Articles in the same language or to an-other Wikipedia language i.e.
Interlanguage links.In our first approach, we used Wikipedia to de-termine the domain (or several domains) of the on-tology.
This approach (a) is to understand as theidentification of the domain through the vocabularyof the ontology.
For this approach, the financialterms, which were extracted from the ontology, wereused to query the Wikipedia knowledge base6.
The6For the Wikipedia Query we used the Wikipedia XMLCollected Wikipedia CategoriesFrequency Name8 Generally Accepted Accounting Principles4 Debt4 Accounting terminology.
.
.1 Political science terms1 Physical punishmentsTable 2: Collected Wikipedia Categories based on the ex-tracted financial termsWikipedia Article was considered for further exami-nation, if its Title is equivalent to our financial terms.In this first step, 7 terms of our ontology were iden-tified in the Wikipedia knowledge base.
With thisstep, we collected the Categories of these Titles,which was the main goal of this approach.
In a sec-ond round, we split all financial terms into all pos-sible n-grams and repeated the query again to findadditional Categories based on the split n-grams.
Ta-ble 2 shows the collected Categories of the first ap-proach and how often they appeared in respect to theextracted financial terms.After storing all Categories, only such Categorieswere considered, which frequency had a value morethan the calculated arithmetic mean of all frequen-cies (> 3.15).
For the calculation of the arithmeticmean only Categories were considered, which hada frequency more than 1, since 2,262 of 3,615 col-lected Categories (62.6%) had a frequency equals 1.With this threshold we avoided extraction of a vo-cabulary that is not related to the ontology.
Withoutthis threshold, out-of-domain Categories would bestored, which would extend the lexicon with vocab-ulary that would not benefit the ontology translation,e.g.
Physical punishments, which was access by thefinancial term Stocks.In the next step, we further extended the list ofCategories collected previously by use of full andsplit terms.
This was done by storing new Categoriesbased on the Wikipedia Interwiki links of each Arti-cle which was tagged with a Category from Table 2.For example, we collected all Categories wherewiththe Article Balance sheet7 is tagged and the Cate-gories of the 106 Interwiki links of the Article Bal-ance sheet.
The frequencies of these Categorieswere summed up for all Interwiki links.
Finally adump; enwiki-20120104-pages-articles.xml7Financial statements, Accounting terminology89Final Category ListFrequency Name95 Economics terminology62 Generally Accepted Accounting Principles61 Macroeconomics55 Accounting terminology47 Finance44 Economic theories.
.
.Table 3: Most frequent Categories based on the xEBRterms and their Interwiki linksnew Category was added to the final Category list, ifthe new Category frequency exceeds the arithmeticmean threshold (> 18.40).The final Category list contained 33 financialWikipedia Categories (Table 3), which was in thenext step used for financial term extraction.With the final list of Categories, we started aninvestigation of all Wikipedia articles tagged withthese financial Categories.
Each Wikipedia Titlewas considered as a useful domain-specific termand was stored in our lexicon if a German title inthe Wikipedia knowledge base also existed.
Asan example, we examined the Category Account-ing terminology and stored the English WikipediaTitle Balance sheet with the German equivalentWikipedia Title Bilanz.At the end of the lexicon generation we examined5228 Wikipedia Articles, which were tagged withone or more financial Categories.
From this set ofArticles we were able to generate a terminologicallexicon with 3228 English-German entities.5 EvaluationTables 4 to 5 illustrate the final results for our exper-iments on translating xEBR ontology terms, usingthe NIST (Doddington, 2002), BLEU (Papineni etal., 2002), and Meteor (Lavie and Agarwal, 2005)algorithms.
To further study any translation im-provements of our experiment, we also used GoogleTranslate8 in translating 63 financial xEBR terms(cf.
Section 3.1) from English into German and fromGerman into English.5.1 Interpretation of Evaluation MetricsIn our experiments translation models built froma general resource performed worst.
These re-8Translations were generated on February 2012.Scoring MetricSource # correct BLEU NIST MeteorGoogle Translate 18 0.264 4.382 0.369JRC-Acquis 12 0.167 3.598 0.323EuroParl 4 0.113 2.630 0.326Linguee 25 0.347 4.567 0.408Lexical substitution 4 0.006 0.223 0.233Linguee+Wiki 25 0.324 4.744 0.432Table 4: Evaluation scores for German term translationsScoring MetricSource # correct BLEU NIST MeteorGoogle Translate 21 0.452 4.830 0.641JRC-Acquis 9 0.127 2.458 0.480EuroParl 5 0.021 1.307 0.412Linguee 15 0.364 3.938 0.631Lexical substitution 4 0.006 0.243 0.260Linguee+Wiki 22 0.348 3.993 0.644Table 5: Evaluation scores for English term translationssults show that building resources from general lan-guage does not improve the translation of terms.The Linguee financial corpus, which is built from13,289 sentences and holds 304K English and Ger-man 250K words, however demonstrates the ben-efit of domain-specific resources.
Its size is lessthan two percent of that of the JRC-Acquis cor-pus (1,131,922 sentences, 21M English words, 19MGerman words), but evaluation scores are more thandouble than those for JRC-Acquis.
This is clear evi-dence that such a resource benefits the translation ofterms in a specific domain.The models produced by the Linguee search en-gine are generating better translations than thoseproduced by general resources.
This approach out-performs Google Translate translations from Ger-man into English for all used evaluation metrics.The table further shows results for our approachin using extracted Wikipedia terms as an example-based approach.
For this we used the terms extractedfrom Wikipedia and exchanged English terms withGerman translations and vice versa.
The evaluationmetrics are very low in this case; only for CorrectTranslation we generate four positive findings.Finally, the table gives results for our approachin using a combination of domain-specific paral-lel financial corpus with the lexicon extracted fromWikipedia.
The domain-specific lexicon contains3228 English-German translations, which were ex-tracted from 18 different financial Categories.
This90combination of highly specialised resources givesthe best results in our experiment.
Translating fi-nancial terms into German, we get more CorrectTranslations as well as the Meteor metric showsbetter results compared to Google Translate.
Fortranslations into English, all used evaluation metricsshow better results than those of Google Translate.As a final observation, we learned that translationsmade by domain-specific resources are on the samequality level, either if we translate from Englishinto German or vice versa.
In comparison, we seethat Google Translate has a larger discrepancy whentranslating into German or English respectively.
Ourresearch showed that translations from English intoGerman built by specialised resources were slightlybetter, which goes along with Google Translate thatalso produces better translations into German.5.2 Manual Evaluation of Translation QualityIn addition to the automatic evaluation with BLEU,NIST, and Meteor scores, we have also undertakena manual evaluation campaign to assess the transla-tion quality of the different systems.
In this section,we will a) describe the annotation setup and taskpresented to the human annotators, b) report on thetranslation quality achieved by the different systems,and c) present inter-annotator agreement scores thatallow to judge the reliability of the human rankings.5.2.1 Annotation SetupIn order to manually assess the translation qualityof the different systems under investigation, we de-signed a simple classification scheme consisting ofthree distinct classes:1.
Acceptable (A): terms classified as acceptableare either fully identical to the reference termor semantically equivalent;2.
Can easily be fixed (C): terms in this classrequire some minor correction (such as fixingof typos, removal of punctuation, etc.)
but arenearly acceptable.
The general semantics ofthe reference term are correctly conveyed tothe reader.3.
None of both (N): the translation of the termdoes not match the intended semantics or it isplain wrong.
Items in this class are consideredsevere errors which cannot easily be fixed andhence should be avoided wherever possible.ClassesSystem A C NLinguee+Wiki 58% 27% 15%Google Translate 55% 31% 14%Linguee 51% 37% 12%JRC-Acquis 32% 28% 40%EuroParl 5% 25% 70%Table 6: Results from the manual evaluation into GermanClassesSystem A C NLinguee+Wiki 56% 32% 12%Linguee 56% 31% 13%Google Translate 39% 40% 21%JRC-Acquis 39% 31% 30%EuroParl 15% 30% 55%Table 7: Results from the manual evaluation into English5.2.2 Annotation DataWe setup ten evaluation tasks, five for transla-tions into English, five for translations into German.Each of these sets was comprised of 63 term transla-tions and the corresponding reference.
Every set wasgiven to at least three human annotators who thenclassified the observed translation output accordingto the classification scheme described above.
Thehuman annotators included both domain experts andlay users without knowledge of the terms domain.In total, we collected 2,520 classification itemsfrom six annotators.
Tables 6, 7 show the resultsfrom the manual evaluation for term translations intoGerman and English, respectively.
We report thedistribution of classes per evaluation task which aredisplayed in best-to-worst order.In order to better be able to interpret these rank-ings, we computed the inter-annotator agreement be-tween human annotators.
We report scores gener-ated with the following agreement metrics:?
S (Bennet et al, 1954);?
pi (averaged across annotators) (Scott, 1955);?
?
(Fleiss and others, 1971);?
?
(Krippendorff, 1980).Tables 8, 9 present the aforementioned metricsscores for German and English term translations.Overall, we achieve an average ?
score of 0.463,which can be interpreted as moderate agreement fol-lowing (Landis and Koch, 1977).
Notably, we alsoreach substantial agreement for one of the anno-tation tasks with a ?
score of 0.657.
Given the91Agreement MetricSystem S pi ?
?Linguee+Wiki 0.599 0.528 0.533 0.530Google Translate 0.698 0.655 0.657 0.657Linguee 0.484 0.416 0.437 0.419JRC-Acquis 0.412 0.406 0.413 0.408EuroParl 0.515 0.270 0.269 0.273Table 8: Annotator agreement scores for GermanAgreement MetricSystem S pi ?
?Linguee+Wiki 0.532 0.452 0.457 0.454Linguee 0.599 0.537 0.540 0.539Google Translate 0.480 0.460 0.465 0.463JRC-Acquis 0.363 0.359 0.366 0.360EuroParl 0.552 0.493 0.499 0.495Table 9: Annotator agreement scores for Englishobserved inter-annotator agreement, we expect thereported ranking results to be meaningful.
OurLinguee+Wiki system performs best for both trans-lation directions while out-of-domain systems suchas JRC-Acquis and EuroParl perform badly.5.3 Manual error analysisTable 10 provides a manual analysis of the providedtranslations from Google Translate and the com-bined Linguee and Wikipedia Lexicon approach.Example Ex.
1 shows the results for [Other intan-gible] fixed assets.
Since both translating systemstranslate it the same, namely Vermo?genswerte, theycould be considered as term variants.A similar example is [Receivables and other] as-sets in Ex.
4.
Google Translate translates thesegment asset into Vermo?gensgegensta?nde, wherebythe domain-specific approach translates it intoVermo?genswerte.
These examples prove the re-search by Kerremans (2010) that one term does notnecessarily have only one translation on the targetside.
As term variants can further be consideredAufwendungen and Kosten, which were translatedfrom Costs [of old age pensions] (Ex.
5).In contrast, the German term in [sonstige be-triebliche] Aufwendungen (Ex.
8) is according to thexEBR translated into [Other operating] expenses,which was translated correctly by both systems.A deeper terminological analysis has to be donein the translation of the English term [Cost of] oldage pensions (Ex.
5).
In general it can be translatedinto Altersversorgung (provided by Google Trans-late and xEBR) or Altersrente (generated by thedomain-specific model).
Doing a compound anal-ysis, the translation of [Alters]versorgung is supplyor maintenance.
On the other side, the translation of[Alters]rente is pension, which has a stronger con-nection to the financial term in this domain.Ex.
6 shows an improvement of domain spe-cific translation model in comparison to a generalresource.
Both general resources translated Securi-ties as Sicherheiten, which is correct but not in the fi-nancial domain.
The domain-specific trained modeltranslates the ambiguous term correctly, namelyWertpapiere.
Google Translate generates the sameterm as on the source site, Securities.
Further, theterm Equity (Ex.
7) is translated by Google Translateas Gerechtigkeit, the domain-specific model trans-lates it as Eigenkapital, which is the correct trans-lation.
Finally, Ex.
2 and Ex.
3 open the issue ofaccurateness of the references for translation evalu-ation.
The translations of these terms are correct ifwe consider the source language.
On the other hand,if we compare them with the proposed references,they are not the same.
In Ex.
2 they are truncatedor extended in Ex.
3, which opens up problems intranslation evaluation.5.4 DiscussionOur approach shows the differences between im-proving translations with different resources.
It wasshown to be necessary to use additional languageresources, i.e.
specialised parallel corpora and ifavailable, specialised lexica with appropriate trans-lations.
Nevertheless, to move further in this direc-tion, translation of specific terms, more research isrequired in several areas that we identified in our ex-periment.
One is the quality of the translation model.Because the translation model can only translateterms that are in the training model, it is necessaryto use a domain-specific resource.
Although we gotbetter results with a smaller resource (if we translateinto English), comparing those results with GoogleTranslate, we learned that more effort has to be donein the direction of extending the size and quality ofdomain-specific resources.Apart from that, with the aid of Wikipedia, whichcan be easily adapted for other language pairs, wefurther improved the translations into English to a92Term Translations# Source Reference Google Domain-specific1 Other intangible sonstige immaterielle Sonstige immaterielle Sonstige immateriellefixed assets Vermo?gensgegensta?nde Vermo?genswerte Vermo?genswerte2 Long-term Finanzanlagen Langfristige finanzielle Langfristige finanziellefinancial assets Vermo?genswerte Vermo?genswerte3 Financial result Finanz- und Finanzergebnis FinanzergebnisBeteiligungsergebnis4 Receivables and Forderungen und sonstige Forderungen und sonstige Forderungen und sonstigeother assets Vermo?gensgegensta?nde Vermo?gensgegensta?nde Vermo?genswerte5 Cost of old age Aufwendungen fu?r Aufwendungen fu?r Kosten der Altersrentenpensions Altersversorgung Altersversorgung6 Securities Wertpapiere Securities Wertpapiere7 Equity Eigenkapital Gerechtigkeit Eigenkapital8 sonstige betriebliche Other operating expenses other operating expenses other operating expensesAufwendungen (TC)Table 10: Translations provided by Google Translate and by the domain-specific resourcepoint where we outperform translations providedby Google Translate.
Nevertheless, our experimentshowed that the translations into German were bet-ter in regard of Google translate only for the Meteorevaluation system, for BLEU and NIST we did notachieve significant improvements.
Also here morework has to be done in domain adaptation in a moresophisticated way to avoid building out-of-domainvocabulary.6 ConclusionThe approach of building new resources showed alarge impact on the translation quality.
Therefore,generating specialised resources for different do-mains will be the focus of our future work.
Onthe one hand, building appropriate training modelsis important, but our experiment also highlightedthe importance of additional collaborative resources,like Wikipedia, Wiktionary, and DBpedia.
Besidesextracting Wikipedia Articles with their multilin-gual equivalents, as shown in Section 4.2, Wikipediaholds much more information in the articles itself.Therefore exploiting non-parallel resources, shownby Fis?er et al (2011), would clearly help the trans-lation system to improve performance.
Future workneeds to better include the redirect system, whichwould allow a better understanding of synonymyand spelling variety of terms.Focusing on translating ontologies, we will tryto better exploit the structure of the ontology itself.Therefore, more work has to be done in the combi-nation of linguistic and semantic information (struc-ture of an ontology) as demonstrated by Aggarwal etal.
(2011), which showed first experiments in com-bining semantic, terminological and linguistic infor-mation.
They suggest that a deeper semantic analy-sis of terms, i.e.
understanding the relations betweenterms and analysing sub-terms needs to be consid-ered.
Another source of useful information may befound in using existing translations for improvingthe translation of other related terms in the ontology.AcknowledgmentsThis work has been funded under the SeventhFramework Programme for Research and Techno-logical Development of the European Commissionthrough the T4ME contract (grant agreement no.
:249119) and in part by the European Union underGrant No.
248458 for the Monnet project as well asby the Science Foundation Ireland under Grant No.SFI/08/CE/I1380 (Lion-2).
The authors would liketo thank Susan-Marie Thomas, Tobias Wunner, Ni-tish Aggarwal and Derek De Brandt for their helpwith the manual evaluation.
We are grateful to theanonymous reviewers for their valuable feedback.ReferencesNitish Aggarwal, Tobias Wunner, Mihael Arcan, PaulBuitelaar, and Sea?n O?Riain.
2011.
A similarity mea-sure based on semantic, terminological and linguistic93information.
In The Sixth International Workshop onOntology Matching collocated with the 10th Interna-tional Semantic Web Conference (ISWC?11).E.
M. Bennet, R. Alpert, and A. C. Goldstein.
1954.Communications through limited response question-ing.
Public Opinion Quarterly, 18:303?308.Thierry Declerck, Hans-Ulrich Krieger, Susan M.Thomas, Paul Buitelaar, Sean O?Riain, Tobias Wun-ner, Gilles Maguet, John McCrae, Dennis Spohr, andElena Montiel-Ponsoda.
2010.
Ontology-based mul-tilingual access to financial reports for sharing busi-ness knowledge across europe.
In Internal FinancialControl Assessment Applying Multilingual OntologyFramework.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, HLT ?02, pages 138?145.M.
Erdmann, K. Nakayama, T. Hara, and S. Nishio.2008.
An approach for extracting bilingual terminol-ogy from wikipedia.
Lecture Notes in Computer Sci-ence, (4947):380?392.
Springer.Darja Fis?er, S?pela Vintar, Nikola Ljubes?ic?, and Senja Pol-lak.
2011.
Building and using comparable corpora fordomain-specific bilingual lexicon extraction.
In Pro-ceedings of the 4th Workshop on Building and UsingComparable Corpora: Comparable Corpora and theWeb, BUCC ?11, pages 19?26.J.L.
Fleiss et al 1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.Koen Kerremans.
2010.
A comparative study of termino-logical variation in specialised translation.
In Recon-ceptualizing LSP Online proceedings of the XVII Eu-ropean LSP Symposium 2009, pages 1?14.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the ACL, ACL?07, pages 177?180.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Summit,pages 79?86.
AAMT.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Methodology.
Sage Publications, Inc.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159?174.Alon Lavie and Abhaya Agarwal.
2005.
Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments.
In Proceedings of theEMNLP 2011 Workshop on Statistical Machine Trans-lation, pages 65?72.Christof Mu?ller and Iryna Gurevych.
2008.
Usingwikipedia and wiktionary in domain-specific informa-tion retrieval.
In Working Notes for the CLEF 2008Workshop.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318.W.
A. Scott.
1955.
Reliability of Content Analysis: TheCase of Nominal Scale Coding.
Public Opinion Quar-terly, 19:321?325.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, and DnielVarga.
2006.
The jrc-acquis: A multilingual alignedparallel corpus with 20+ languages.
In Proceedingsof the 5th International Conference on Language Re-sources and Evaluation (LREC?2006).Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings International Con-ference on Spoken Language Processing, pages 257?286.Jorge Vivaldi and Horacio Rodriguez.
2010.
Usingwikipedia for term extraction in the biomedical do-main: first experiences.
Procesamiento del LenguajeNatural, 45:251?254.Marion Weller, Anita Gojun, Ulrich Heid, Be?atriceDaille, and Rima Harastani.
2011.
Simple methodsfor dealing with term variation and term alignment.In Proceedings of the 9th International Conference onTerminology and Artificial Intelligence, pages 87?93.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProceedings of the 22nd International Conference onComputational Linguistics - Volume 1, COLING ?08,pages 993?1000.Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.2008.
Extracting lexical semantic knowledge fromwikipedia and wiktionary.
In Proceedings of the SixthInternational Conference on Language Resources andEvaluation (LREC?08).94
