Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97?106,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsImproving Agreement and Disagreement Identificationin Online Discussions with A Socially-Tuned Sentiment LexiconLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractWe study the problem of agreement anddisagreement detection in online discus-sions.
An isotonic Conditional RandomFields (isotonic CRF) based sequentialmodel is proposed to make predictionson sentence- or segment-level.
We auto-matically construct a socially-tuned lex-icon that is bootstrapped from existinggeneral-purpose sentiment lexicons to fur-ther improve the performance.
We eval-uate our agreement and disagreement tag-ging model on two disparate online discus-sion corpora ?
Wikipedia Talk pages andonline debates.
Our model is shown tooutperform the state-of-the-art approachesin both datasets.
For example, the iso-tonic CRF model achieves F1 scores of0.74 and 0.67 for agreement and disagree-ment detection, when a linear chain CRFobtains 0.58 and 0.56 for the discussionson Wikipedia Talk pages.1 IntroductionWe are in an era where people can easily voice andexchange their opinions on the internet throughforums or social media.
Mining public opinionand the social interactions from online discus-sions is an important task, which has a wide rangeof applications.
For example, by analyzing theusers?
attitude in forum posts on social and po-litical problems, it is able to identify ideologicalstance (Somasundaran and Wiebe, 2009) and userrelations (Qiu et al., 2013), and thus further dis-cover subgroups (Hassan et al., 2012; Abu-Jbaraet al., 2012) with similar ideological viewpoint.Meanwhile, catching the sentiment in the conver-sation can help detect online disputes, reveal popu-lar or controversial topics, and potentially disclosethe public opinion formation process.In this work, we study the problem of agreementand disagreement identification in online discus-sions.
Sentence-level agreement and disagreementdetection for this domain is challenging in its ownright due to the dynamic nature of online conversa-tions, and the less formal, and usually very emo-tional language used.
As an example, consider asnippet of discussion from Wikipedia Talk pagefor article ?Iraq War?
where editors argue on thecorrectness of the information in the opening para-graph (Figure 1).
?So what??
should presumablybe tagged as a negative sentence as should the sen-tence ?If you?re going to troll, do us all a favorand stick to the guidelines.?.
We hypothesize thatthese, and other, examples will be difficult for thetagger unless the context surrounding each sen-tence is considered and in the absence of a sen-timent lexicon tuned for conversational text (Dinget al., 2008; Choi and Cardie, 2009).As a result, we investigate isotonic Condi-tional Random Fields (isotonic CRF) (Mao andLebanon, 2007) for the sentiment tagging tasksince they preserve the advantages of the popu-lar CRF sequential tagging models (Lafferty etal., 2001) while providing an efficient mechanismto encode domain knowledge ?
in our case, asentiment lexicon ?
through isotonic constraintson the model parameters.
In particular, we boot-strap the construction of a sentiment lexicon fromWikipedia talk pages using the lexical items in ex-isting general-purpose sentiment lexicons as seedsand in conjunction with an existing label propaga-tion algorithm (Zhu and Ghahramani, 2002).1To summarize, our chief contributions include:(1) We propose an agreement and disagree-ment identification model based on isotonic Con-ditional Random Fields (Mao and Lebanon, 2007)to identify users?
attitude in online discussion.Our predictions that are made on the sentence-1Our online discussion lexicon (Section 4) will be madepublicly available.97Zer0faults: So questions comments feedback welcome.Other views etc.
I just hope we can remove the assertationsthat WMD?s were in fact the sole reason for the US invasion,considering that HJ Res 114 covers many many reasons.>Mr.
Tibbs: So basically what you want to do is remove allmention of the cassus belli of the Iraq War and try to createthe false impression that this military action was as inevitableas the sunrise.
[NN ]No.
Just because things didn?t turn out theway the Bush administration wanted doesn?t give you licenseto rewrite history.
[NN ]...>>MONGO: Regardless, the article is an antiwar propa-ganda tool.
[NN ]...>>>Mr.
Tibbs: So what?
[NN ]That wasn?t the cassusbelli and trying to give that impression After the Fact isUntrue.
[NN ]Hell, the reason it wasn?t the cassus belli is be-cause there are dictators in Africa that make Saddam look likea pussycat...>>Haizum: Start using the proper format or it?s over for yourcomments.
[N ]If you?re going to troll, do us all a favor andstick to the guidelines.
[N ]...Tmorton166: Hi, I wonder if, as an outsider to this debate Ican put my word in here.
I considered mediating this discus-sion however I?d prefer just to comment and leave it at that :).I agree mostly with what Zer0faults is saying[PP ].
...>Mr.
Tibbs: Here?s the problem with that.
[NN ]It?s not aboutpublicity or press coverage.
It?s about the fact that the Iraqdisarmament crisis set off the 2003 Invasion of Iraq.
... Andtheres a huge problem with rewriting the intro as if the Iraqdisarmament crisis never happened.
[NN ]>>Tmorton166: ... To suggest in the opening paragraph thatthe ONLY reason for the war was WMD?s is wrong - becauseit simply isn?t.
[NN ]However I agree that the emphasis needsto be on the armaments crisis because it was the reason soldto the public and the major one used to justify the invasion butit needs to acknowledge that there was at least 12 reasons forthe war as well.
[PP ]...Figure 1: Example discussion from wikipedia talk pagefor article ?Iraq War?, where editors discuss about the cor-rectness of the information in the opening paragraph.
Weonly show some sentences that are relevant for demonstra-tion.
Other sentences are omitted by ellipsis.
Names of ed-itors are in bold.
?>?
is an indicator for the reply structure,where turns starting with > are response for most previousturn that with one less >.
We use ?NN?, ?N?, and ?PP?
to in-dicate ?strongly disagree?, ?disagree?, and ?strongly agree?.Sentences in blue are examples whose sentiment is hard todetect by an existing lexicon.or segment-level, are able to discover fine-grainedsentiment flow within each turn, which can be fur-ther applied in other applications, such as disputedetection or argumentation structure analysis.
Weemploy two existing online discussion data sets:the Authority and Alignment in Wikipedia Dis-cussions (AAWD) corpus of Bender et al.
(2011)(Wikipedia talk pages) and the Internet Argu-ment Corpus (IAC) of Walker et al.
(2012a).
Ex-perimental results show that our model signifi-cantly outperforms state-of-the-art methods on theAAWD data (our F1 scores are 0.74 and 0.67 foragreement and disagreement, vs. 0.58 and 0.56 forthe linear chain CRF approach) and IAC data (ourF1 scores are 0.61 and 0.78 for agreement and dis-agreement, vs. 0.28 and 0.73 for SVM).
(2) Furthermore, we construct a new senti-ment lexicon for online discussion.
We showthat the learned lexicon significantly improves per-formance over systems that use existing general-purpose lexicons (i.e.
MPQA lexicon (Wilson etal., 2005), General Inquirer (Stone et al., 1966),and SentiWordNet (Esuli and Sebastiani, 2006)).Our lexicon is constructed from a very large-scalediscussion corpus based on Wikipedia talk page,where previous work (Somasundaran and Wiebe,2010) for constructing online discussion lexiconrelies on human annotations derived from limitednumber of conversations.In the remainder of the paper, we describe firstthe related work (Section 2).
Then we intro-duce the sentence-level agreement and disagree-ment identification model (Section 3) as well asthe label propagation algorithm for lexicon con-struction (Section 4).
After explain the experimen-tal setup, we display the results and provide furtheranalysis in Section 6.2 Related WorkSentiment analysis has been utilized as a key en-abling technique in a number of conversation-based applications.
Previous work mainly stud-ies the attitudes in spoken meetings (Galley et al.,2004; Hahn et al., 2006) or broadcast conversa-tions (Wang et al., 2011) using Conditional Ran-dom Fields (CRF) (Lafferty et al., 2001).
Galleyet al.
(2004) employ Conditional Markov modelsto detect if discussants reach at an agreement inspoken meetings.
Each state in their model is anindividual turn and prediction is made on the turn-level.
In the same spirit, Wang et al.
(2011) alsopropose a sequential model based on CRF for de-tecting agreements and disagreements in broadcastconversations, where they primarily show the ef-ficiency of prosodic features.
While we also ex-ploit a sequential model extended from CRFs, ourpredictions are made for each sentence or segmentrather than at the turn-level.
Moreover, we experi-ment with online discussion datasets that exhibita more realistic distribution of disagreement vs.agreement, where much more disagreement is ob-served due to its function and the relation betweenthe participants.
This renders the detection prob-lem more challenging.Only recently, agreement and disagreement de-tection is studied for online discussion, especially98for online debate.
Abbott et al.
(2011) investi-gate different types of features based on depen-dency relations as well as manually-labeled fea-tures, such as if the participants are nice, nasty,or sarcastic, and respect or insult the target par-ticipants.
Automatically inducing those featuresfrom human annotation are challenging itself, soit would be difficult to reproduce their work onnew datasets.
We use only automatically gener-ated features.
Using the same dataset, Misra andWalker (2013) study the effectiveness of topic-independent features, e.g.
discourse cues indicat-ing agreement or negative opinion.
Those cues,which serve a similar purpose as a sentiment lex-icon, are also constructed manually.
In our work,we create an online discussion lexicon automat-ically and construct sentiment features based onthe lexicon.
Also targeting online debate, Yin etal.
(2012) train a logistic regression classifier withfeatures aggregating posts from the same partici-pant to predict the sentiment for each individualpost.
This approach works only when the speakerhas enough posts on each topic, which is not ap-plicable to newcomers.
Hassan et al.
(2010) focuson predicting the attitude of participants towardseach other.
They relate the sentiment words tothe second person pronoun, which produces strongbaselines.
We also adopt their baselines in ourwork.
Although there are available datasets with(dis)agreement annotated on Wikipedia talk pages,we are not aware of any published work that uti-lizes these annotations.
Dialogue act recognitionon talk pages (Ferschke et al., 2012) might be themost related.While detecting agreement and disagreement inconversations is useful on its own, it is also a keycomponent for related tasks, such as stance pre-diction (Thomas et al., 2006; Somasundaran andWiebe, 2009; Walker et al., 2012b) and subgroupdetection (Hassan et al., 2012; Abu-Jbara et al.,2012).
For instance, Thomas et al.
(2006) train anagreement detection classifier with Support Vec-tor Machines on congressional floor-debate tran-scripts to determine whether the speeches repre-sent support of or opposition to the proposed leg-islation.
Somasundaran and Wiebe (2009) designvarious sentiment constraints for inclusion in aninteger linear programming framework for stanceclassification.
For subgroup detection, Abu-Jbaraet al.
(2012) uses the polarity of the expressions inthe discussions and partition discussants into sub-groups based on the intuition that people in thesame group should mostly agree with each other.Though those work highly relies on the compo-nent of agreement and disagreement detection, theevaluation is always performed on the ultimate ap-plication only.3 The ModelWe first give a brief overview on isotonic Con-ditional Random Fields (isotonic CRF) (Mao andLebanon, 2007), which is used as the backboneapproach for our sentence- or segment-level agree-ment and disagreement detection model.
We deferthe explanation of online discussion lexicon con-struction in Section 4.3.1 Problem DescriptionConsider a discussion comprised of sequentialturns uttered by the participants; each turn con-sists of a sequence of text units, where each unitcan be a sentence or a segment of several sen-tences.
Our model takes as input the text unitsx = {x1, ?
?
?
, xn} in the same turn, and outputsa sequence of sentiment labels y = {y1, ?
?
?
, yn},where yi?
O,O = {NN,N,O,P,PP}.
The la-bels in O represent strongly disagree (NN), dis-agree (N), neutral (O), agree (P), strongly agree(PP), respectively.
In addition, elements in thepartially ordered set O possess an ordinal relation?.
Here, we differentiate agreement and disagree-ment with different intensity, because the outputof our classifier can be used for other applications,such as dispute detection, where ?strongly dis-agree?
(e.g.
NN) plays an important role.
Mean-while, fine-grained sentiment labels potentiallyprovide richer context information for the sequen-tial model employed for this task.3.2 Isotonic Conditional Random FieldsConditional Random Fields (CRF) have been suc-cessfully applied in numerous sequential labelingtasks (Lafferty et al., 2001).
Given a sequenceof utterances or segments x = {x1, ?
?
?
, xn}, ac-cording to linear-chain CRF, the probability of thelabels y for x is given by:p(y|x) =1Z(x)exp(?i??,????,??f??,??
(yi?1, yi)+?i??,w???,w?g??,w?
(yi, xi))(1)99f??,??
(yi?1, yi) and g??,w?
(yi, xi) are featurefunctions.
Given that yi?1, yi, xitake values of?, ?, w, the functions are indexed by pairs ?
?, ?
?and ??,w?.
???,?
?, ??
?,w?are the parameters.CRF, as defined above, is not appropriate for or-dinal data like sentiment, because it ignores theordinal relation among sentiment labels.
IsotonicConditional Random Fields (isotonic CRF) areproposed by Mao and Lebanon (2007) to enforce aset of monotonicity constraints on the parametersthat are consistent with the ordinal structure anddomain knowledge (in our case, a sentiment lexi-con automatically constructed from online discus-sions).Given a lexiconM = Mp?Mn, whereMpand Mnare two sets of features (usually words)identified as strongly associated with positive sen-timent and negative sentiment.
The constraints areencoded as below.
For each feature w ?
Mp, iso-tonic CRF enforces ?
?
???
???,w??
???
?,w?.Intuitively, the parameters ??
?,w?are intimatelytied to the model probabilities.
When a featuresuch as ?totally agree?
is observed in the trainingdata, the feature parameter for ?
?PP,totally agree?islikely to increase.
Similar constraints are also de-fined onMn.
In this work, we boostrap the con-struction of an online discussion sentiment lexiconused asM in the isotonic CRF (see Section 4).The parameters can be found by maximizing thelikelihood subject to the monotonicity constraints.We adopt the re-parameterization from Mao andLebanon (2007) for a simpler optimization prob-lem, and refer the readers to Mao and Lebanon(2007) for more details.23.3 FeaturesThe features used in sentiment prediction are listedin Table 1.
Features with numerical values are firstnormalized by standardization, then binned into 5categories.Syntactic/Semantic Features.
Dependency re-lations have been shown to be effective for varioussentiment prediction tasks (Joshi and Penstein-Ros?e, 2009; Somasundaran and Wiebe, 2009;Hassan et al., 2010; Abu-Jbara et al., 2012).
Wehave two versions of dependency relation as fea-tures, one being the original form, another gen-2The full implementation is based on MALLET (McCal-lum, 2002).
We thank Yi Mao for sharing the implementationof the core learning algorithm.Lexical Features- unigram/bigram- num of words all uppercased- num of wordsDiscourse Features- initial uni-/bi-/trigram- repeated punctuations- hedging (Farkas et al., 2010)- number of negatorsSyntactic/Semantic Features- unigram with POS tag- dependency relationConversation Features- quote overlap with target- TFIDF similarity with target (remove quote first)Sentiment Features- connective + sentiment words- sentiment dependency relation- sentiment wordsTable 1: Features used in sentiment prediction.eralizing a word to its POS tag in turn.
For in-stance, ?nsubj(wrong, you)?
is generlized as the?nsubj(ADJ, you)?
and ?nsubj(wrong, PRP)?.
Weuse Stanford parser (de Marneffe et al., 2006) toobtain parse trees and dependency relations.Discourse Features.
Previous work (Hirschbergand Litman, 1993; Abbott et al., 2011) suggeststhat discourse markers, such as what?, actually,may have their use for expressing opinions.
Weextract the initial unigram, bigram, and trigram ofeach utterance as discourse features (Hirschbergand Litman, 1993).
Hedge words are collectedfrom the CoNLL-2012 shared task (Farkas et al.,2010).Conversation Features.
Conversation featuresencode some useful information regarding thesimilarity between the current utterance(s) and thesentences uttered by the target participant.
TFIDFsimilarity is computed.
We also check if the cur-rent utterance(s) quotes target sentences and com-pute its length.Sentiment Features.
We gather connectivesfrom Penn Discourse TreeBank (Rashmi Prasadand Webber, 2008) and combine them with anysentiment word that precedes or follows it asnew features.
Sentiment dependency relations arethe subset of dependency relations with sentimentwords.
We replace those words with their polarityequivalents.
For example, relation ?nsubj(wrong,you)?
becomes ?nsubj(SentiWordneg, you)?.100POSITIVEplease elaborate, nod, await response, from experiences, anti-war, profits, promises of, is undisputed,royalty, sunlight, conclusively, badges, prophecies, in vivo, tesla, pioneer, published material, from god,plea for, lend itself, geek, intuition, morning, anti SentiWordneg, connected closely, Rel(undertake,to), intelligibility, Rel(articles, detailed), of noting, for brevity, Rel(believer, am), endorsements, testable,source carefullyNEGATIVE: (, TOT, ?!
!, in contrast, ought to, whatever, Rel(nothing, you), anyway, Rel(crap, your), by facts, pur-porting, disproven, Rel(judgement, our), Rel(demonstrating, you), opt for, subdue to, disinformation,tornado, heroin, Rel(newbies, the), Rel (intentional, is), pretext, watergate, folly, perjury, Rel(lock, ar-ticle), contrast with, poke to, censoring information, partisanship, insurrection, bigot, Rel(informative,less), clowns, Rel(feeling, mixed), never-endingTable 2: Example terms and relations from our online discussion lexicon.
We choose for display termsthat do not contain any seed word.4 Online Discussion Sentiment LexiconConstructionSo far as we know, there is no lexicon availablefor online discussions.
Thus, we create from alarge-scale corpus via label propagation.
The la-bel propagation algorithm, proposed by Zhu andGhahramani (2002), is a semi-supervised learningmethod.
In general, it takes as input a set of seedsamples (e.g.
sentiment words in our case), andthe similarity between pairwise samples, then it-eratively assigns values to the unlabeled samples(see Algorithm 1).
The construction of graph G isdiscussed in Section 4.1.
Sample sentiment wordsin the new lexicon are listed in Table 2.Input : G = (V,E), wij?
[0, 1], positiveseed words P , negative seed wordsN , number of iterations TOutput: {yi}|V |?1i=0yi= 1.0, ?vi?
Pyi= ?1.0, ?vi?
Nyi= 0.0, ?vi/?
P ?Nfor t = 1 ?
?
?T doyi=?(vi,vj)?Ewij?yj?
(vi,vj)?Ewij, ?vi?
Vyi= 1.0, ?vi?
Pyi= ?1.0, ?vi?
NendAlgorithm 1: The label propagation algo-rithm (Zhu and Ghahramani, 2002) used forconstructing online discussion lexicon.4.1 Graph ConstructionNode Set V .
Traditional lexicons, like GeneralInquirer (Stone et al., 1966), usually consist of po-larized unigrams.
As we mentioned in Section 1,unigrams lack the capability of capturing the sen-timent conveyed in online discussions.
Instead,bigrams, dependency relations, and even punctu-ation can serve as supplement to the unigrams.Therefore, we consider four types of text units asnodes in the graph: unigrams, bigrams, depen-dency relations, sentiment dependency relations.Sentiment dependency relations are described inSection 3.3.
We replace all relation names with ageneral label.
Text units that appear in at least 10discussions are retained as nodes to reduce noise.Edge Set E. As Velikovich et al.
(2010) andFeng et al.
(2013) notice, a dense graph with alarge number of nodes is susceptible to propagat-ing noise, and will not scale well.
We thus adoptthe algorithm in Feng et al.
(2013) to constructa sparsely connected graph.
For each text unit t,we first compute its representation vector ~a usingPairwise Mutual Information scores with respectto the top 50 co-occuring text units.
We define?co-occur?
as text units appearing in the same sen-tence.
An edge is created between two text unitst0and t1only if they ever co-occur.
The similar-ity between t0and t1is calculated as the Cosinesimilarity between ~a0and ~a1.Seed Words.
The seed sentiment are collectedfrom three existing lexicons: MPQA lexicon, Gen-eral Inquirer, and SentiWordNet.
Each word inSentiWordNet is associated with a positive scoreand a negative score; words with a polarity score101larger than 0.7 are retained.
We remove wordswith conflicting sentiments.4.2 DataThe graph is constructed based on Wikipedia talkpages.
We download the 2013-03-04 Wikipediadata dump, which contains 4,412,582 talk pages.Since we are interested in conversational lan-guages, we filter out talk pages with fewer than5 participants.
This results in a dataset of 20,884talk pages, from which the graph is constructed.5 Experimental Setup5.1 DatasetsWikipedia Talk pages.
The first dataset we useis Authority and Alignment in Wikipedia Dis-cussions (AAWD) corpus (Bender et al., 2011).AAWD consists of 221 English Wikipedia discus-sions with agreement and disagreement annota-tions.3The annotation of AAWD is made at utterance-or turn-level, where a turn is defined as continu-ous body of text uttered by the same participant.Annotators either label each utterance as agree-ment, disagreement or neutral, and select the cor-responding spans of text, or label the full turn.Each turn is annotated by two or three people.
Toinduce an utterance-level label for instances thathave only a turn-level label, we assume they havethe same label as the turn.To train our sentiment model, we further trans-form agreement and disagreement labels (i.e.
3-way) into the 5-way labels.
For utterances thatare annotated as agreement and have the textspan specified by at least two annotators, they aretreated as ?strongly agree?
(PP).
If an utterance isonly selected as agreement by one annotator or itgets the label by turn-level annotation, it is ?agree?(P).
?Strongly disagree?
(NN) and ?disagree?
(N)are collected in the same way from disagreementlabel.
All others are neutral (O).
In total, we have16,501 utterances.
1,930 and 1,102 utterances arelabeled as ?NN?
and ?N?.
532 and 99 of them are?PP?
and ?P?.
All other 12,648 are neutral sam-ples.43Bender et al.
(2011) originally use positive alignmentand negative alignment to indicate two types of social moves.They define those alignment moves as ?agreeing or disagree-ing?
with the target.
We thus use agreement and disagreementinstead of positive and negative alignment in this work.4345 samples with both positive and negative labels aretreated as neutral.Online Debate.
The second dataset is the Inter-net Argument Corpus (IAC) (Walker et al., 2012a)collected from an online debate forum.
Each dis-cussion in IAC consists of multiple posts, wherewe treat each post as a turn.
Most posts (72.3%)contain quoted content from the posts they targetat or other resources.
A post can have more thanone quote, which naturally break the post into mul-tiple segments.
1,806 discussions are annotatedwith agreement and disagreement on the segment-level from -5 to 5, with -5 as strongly disagree and5 as strongly agree.
We first compute the averagescore for each segment among different annotatorsand transform the score into sentiment label in thefollowing way.
We treat [?5,?3] as NN (1595segments), (?3,?1] as N (4548 segments), [1, 3)as P (911 samples), [3, 5] as PP (199), all others asO (290 segments).In the test phase, utterances or segments pre-dicted with NN or N are treated as disagreement;the ones predicted as PP or P are agreement; O isneutral.5.2 ComparisonWe compare with two baselines.
(1) Baseline (Po-larity) is based on counting the sentiment wordsfrom our lexicon.
An utterance or segment ispredicted as agreement if it contains more posi-tive words than negative words, or disagreementif more negative words are observed.
Other-wise, it is neutral.
(2) Baseline (Distance) is ex-tended from (Hassan et al., 2010).
Each sentimentword is associated with the closest second per-son pronoun, and a surface distance can be com-puted between them.
A classifier based on Sup-port Vector Machines (Joachims, 1999) (SVM) istrained with the features of sentiment words, min-imum/maximum/average of the distances.We also compare with two state-of-the-artmethods that are widely used in sentiment predic-tion for conversations.
The first one is an RBFkernel SVM based approach, which has been usedfor sentiment prediction (Hassan et al., 2010), and(dis)agreement detection (Yin et al., 2012) in on-line debates.
The second is linear chain CRF,which has been utilized for (dis)agreement iden-tification in broadcast conversations (Wang et al.,2011).102Strict F1 Soft F1Agree Disagree Neutral Agree Disagree NeutralBaseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84+ downsampling 21.60 36.32 72.11 31.86 49.58 74.92CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10+ downsampling 24.26 31.28 77.12 47.30 46.24 80.18isotonic CRF 24.32 21.95 86.26 68.18 62.53 88.87+ downsampling 29.62 34.17 80.97 55.38 53.00 84.56+ new lexicon 46.01 51.49 87.40 74.47 67.02 90.56+ new lexicon + downsampling 47.90 49.61 81.60 64.97 58.97 84.04Table 3: Strict and soft F1 scores for agreement and disagreement detection on Wikipedia talk pages(AAWD).
All the numbers are multiplied by 100.
In each column, bold entries (if any) are statisticallysignificantly higher than all the rest, and the italic entry has the highest absolute value.
Our model basedon the isotonic CRF with the new lexicon produces significantly better results than all the other systemsfor agreement and disagreement detection.
Downsampling, however, is not always helpful.6 ResultsIn this section, we first show the experimental re-sults on sentence- and segment-level agreementand disagreement detection in two types of onlinediscussions ?
Wikipedia Talk pages and online de-bates.
Then we provide more detailed analysis forthe features used in our model.
Furthermore, wediscuss several types of errors made in the model.6.1 Wikipedia Talk PagesWe evaluate the systems by standard F1 score oneach of the three categories: agreement, disagree-ment, and neutral.
For AAWD, we compute twoversions of F1 scores.
Strict F1 is computedagainst the true labels.
For soft F1, if a sentenceis never labeled by any annotator on the sentence-level and adopts its agreement/disagreement labelfrom the turn-level annotation, then it is treated asa true positive when predicted as neutral.Table 3 demonstrates our main results on theWikipedia Talk pages (AAWD dataset).
With-out downsampling, our isotonic CRF based sys-tems with the new lexicon significantly outper-form the compared approaches for agreement anddisagreement detection according to the paired-t test (p < 0.05).
We also perform downsam-pling by removing the turns only containing neu-tral utterances.
However, it does not always helpwith performance.
We suspect that, with less neu-tral samples in the training data, the classifier isless likely to make neutral predictions, which thusdecreases true positive predictions.
For strict F-scores on agreement/disagreement, downsamplingAgree Disagree NeuBaseline (Polarity) 3.33 5.96 65.61Baseline (Distance) 1.65 5.07 85.41SVM (3-way) 25.62 69.10 31.47+ new lexicon features 28.35 72.58 34.53CRF (3-way) 29.46 74.81 31.93CRF (5-way) 24.54 69.31 39.60+ new lexicon features 28.85 71.81 39.14isotonic CRF 53.40 76.77 44.10+ new lexicon 61.49 77.80 51.43Table 4: F1 scores for agreement and disagree-ment detection on online debate (IAC).
All thenumbers are multiplied by 100.
In each column,bold entries (if any) are statistically significantlyhigher than all the rest, and the italic entry has thehighest absolute value except baselines.
We havetwo main observations: 1) Both of our modelsbased on isotonic CRF significantly outperformother systems for agreement and disagreement de-tection.
2) By adding the new lexicon, either asfeatures or constraints in isotonic CRF, all systemsachieve better F1 scores.has mixed effect, but mostly we get slightly betterperformance.6.2 Online DebatesSimilarly, F1 scores for agreement, disagreementand neutral for online debates (IAC dataset) aredisplayed in Table 4.
Both of our systems basedon isotonic CRF achieve significantly better F1scores than the comparison.
Especially, our sys-tem with the new lexicon produces the best results.For SVM and linear-chain CRF based systems, wealso add new sentiment features constructed fromthe new lexicon as described in Section 3.3.
We103can see that those sentiment features also boost theperformance for both of the compared approaches.6.3 Feature EvaluationMoreover, we evaluate the effectiveness of fea-tures by adding one type of features each time.The results are listed in Table 5.
As it can be seen,the performance gets improved incrementally withevery new set of features.We also utilize ?2-test to highlight some ofthe salient features on the two datasets.
We cansee from Table 6 that, for online debates (IAC),some features are highly topic related, such as ?themale?
or ?the scientist?.
This observation concurswith the conclusion in Misra and Walker (2013)that features with topic information are indicativefor agreement and disagreement detection.AAWD Agree Disagree NeuLex 40.77 52.90 79.65Lex + Syn 68.18 63.91 88.87Lex + Syn + Disc 70.93 63.69 89.32Lex + Syn + Disc + Con 71.27 63.72 89.60Lex + Syn + Disc + Con + Sent 74.47 67.02 90.56IAC Agree Disagree NeuLex 56.65 75.35 45.72Lex + Syn 54.16 75.13 46.12Lex + Syn + Disc 54.27 76.41 47.60Lex + Syn + Disc + Con 55.31 77.25 48.87Lex + Syn + Disc + Con + Sent 61.49 77.80 51.43Table 5: Results on Wikipedia talk page(AAWD) (with soft F1 score) and online de-bate (IAC) with different feature sets (i.e Lexical,Syntacitc/Semantic, Discourse, Conversation, andSentiment features) by using isotonic CRF.
Thenumbers in bold are statistically significantlyhigher than the numbers above it (paired-t test,p < 0.05).6.4 Error AnalysisAfter a closer look at the data, we found two ma-jor types of errors.
Firstly, people express dis-agreement not only by using opinionated words,but also by providing contradictory example.
Thisneeds a deeper understanding of the semantic in-formation embedded in the text.
Techniques liketextual entailment can be used in the further work.Secondly, a sequence of sentences with sarcasm ishard to detect.
For instance, ?Bravo, my friends!Bravo!
Goebbles would be proud of your abilitiesto whitewash information.?
We observe terms like?Bravo?, ?friends?, and ?be proud of?
that are in-dicators for positive sentiment; however, they areAAWDPOSITIVE: agree, nsubj (agree, I), nsubj (right,you), Rel (Sentimentpos, I), thanks, amod (idea,good), nsubj(glad, I), good point, concur, happywith, advmod (good, pretty), suggestionHedgeNEGATIVE: you, your, nsubj (negative, you),numberOfNegator, don?t, nsubj (disagree, I),actuallySentInitial, please stopSentInitial, what?SentInitial, shouldHedgeIACPOSITIVE: amod (conclusion, logical), Rel (agree,on), Rel (have, justified), Rel (work, out), onemightSentInitial, to confirmHedge, womenNEGATIVE: their kind, the male, the female, thescientist, according to, is stated, poss (understand-ing, my), hellSentInitial, whateverSentInitialTable 6: Relevant features by ?2test on AAWDand IAC.in sarcastic tone.
We believe a model that is ableto detect sarcasm would further improve the per-formance.7 ConclusionWe present an agreement and disagreement detec-tion model based on isotonic CRFs that outputslabels at the sentence- or segment-level.
We boot-strap the construction of a sentiment lexicon foronline discussions, encoding it in the form of do-main knowledge for the isotonic CRF learner.
Oursentiment-tagging model is shown to outperformthe state-of-the-art approaches on both WikipediaTalk pages and online debates.Acknowledgments We heartily thank the CornellNLP Group and the reviewers for helpful com-ments.
This work was supported in part by NSFgrants IIS-0968450 and IIS-1314778, and DARPADEFT Grant FA8750-13-2-0015.
The views andconclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of NSF, DARPA orthe U.S. Government.ReferencesRob Abbott, Marilyn Walker, Pranav Anand, Jean E.Fox Tree, Robeson Bowmani, and Joseph King.
2011.How can you say such things?!?
: Recognizing disagree-ment in informal political argument.
In Proceedings ofthe Workshop on Languages in Social Media, LSM ?11,pages 2?11, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and104Dragomir Radev.
2012.
Subgroup detection in ideo-logical discussions.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 399?409,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, MarkZachry, Brian Hutchinson, Alex Marin, Bin Zhang, andMari Ostendorf.
2011.
Annotating social acts: Author-ity claims and alignment moves in wikipedia talk pages.In Proceedings of the Workshop on Languages in SocialMedia, LSM ?11, pages 48?57, Stroudsburg, PA, USA.Association for Computational Linguistics.Yejin Choi and Claire Cardie.
2009.
Adapting a polarity lex-icon using integer linear programming for domain-specificsentiment classification.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural Language Pro-cessing: Volume 2 - Volume 2, EMNLP ?09, pages 590?598, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Marie-Catherine de Marneffe, Bill MacCartney, and Christo-pher D. Manning.
2006.
Generating typed dependencyparses from phrase structure trees.
In LREC.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
A holisticlexicon-based approach to opinion mining.
In Proceed-ings of the 2008 International Conference on Web Searchand Data Mining, WSDM ?08, pages 231?240, New York,NY, USA.
ACM.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiwordnet:A publicly available lexical resource for opinion mining.In In Proceedings of the 5th Conference on Language Re-sources and Evaluation (LREC06, pages 417?422.Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anosCsirik, and Gy?orgy Szarvas.
2010.
The conll-2010 sharedtask: Learning to detect hedges and their scope in nat-ural language text.
In Proceedings of the FourteenthConference on Computational Natural Language Learn-ing ?
Shared Task, CoNLL ?10: Shared Task, pages 1?12, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Song Feng, Jun Seok Kang, Polina Kuznetsova, and YejinChoi.
2013.
Connotation lexicon: A dash of sentimentbeneath the surface meaning.
In ACL, pages 1774?1784.The Association for Computer Linguistics.Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.2012.
Behind the article: Recognizing dialog acts inwikipedia talk pages.
In Proceedings of the 13th Con-ference of the European Chapter of the Association forComputational Linguistics, EACL ?12, pages 777?786,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Michel Galley, Kathleen McKeown, Julia Hirschberg, andElizabeth Shriberg.
2004.
Identifying agreement and dis-agreement in conversational speech: use of Bayesian net-works to model pragmatic dependencies.
In ACL ?04:Proceedings of the 42nd Annual Meeting on Associationfor Computational Linguistics, pages 669+, Morristown,NJ, USA.
Association for Computational Linguistics.Sangyun Hahn, Richard Ladner, and Mari Ostendorf.
2006.Agreement/disagreement classification: Exploiting unla-beled data using contrast classifiers.
In Proceedings of theHuman Language Technology Conference of the NAACL,Companion Volume: Short Papers, pages 53?56, NewYork City, USA, June.
Association for Computational Lin-guistics.Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.2010.
What?s with the attitude?
: Identifying sentenceswith attitude in online discussions.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?10, pages 1245?1255,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.2012.
Detecting subgroups in online discussions by mod-eling positive and negative relations among participants.In Proceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing and Com-putational Natural Language Learning, EMNLP-CoNLL?12, pages 59?70, Stroudsburg, PA, USA.
Association forComputational Linguistics.Julia Hirschberg and Diane Litman.
1993.
Empirical studieson the disambiguation of cue phrases.
Comput.
Linguist.,19(3):501?530, September.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making Large-scale Support Vector MachineLearning Practical, pages 169?184.
MIT Press, Cam-bridge, MA, USA.Mahesh Joshi and Carolyn Penstein-Ros?e.
2009.
Generaliz-ing dependency features for opinion mining.
In Proceed-ings of the ACL-IJCNLP 2009 Conference Short Papers,ACLShort ?09, pages 313?316, Stroudsburg, PA, USA.Association for Computational Linguistics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InProceedings of the Eighteenth International Conferenceon Machine Learning, ICML ?01, pages 282?289, SanFrancisco, CA, USA.
Morgan Kaufmann Publishers Inc.Yi Mao and Guy Lebanon.
2007.
Isotonic conditional ran-dom fields and local sentiment flow.
In Advances in Neu-ral Information Processing Systems.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Amita Misra and Marilyn Walker.
2013.
Topic independentidentification of agreement and disagreement in social me-dia dialogue.
In Proceedings of the SIGDIAL 2013 Con-ference, pages 41?50, Metz, France, August.
Associationfor Computational Linguistics.Minghui Qiu, Liu Yang, and Jing Jiang.
2013.
Mining userrelations from online discussions using sentiment analy-sis and probabilistic matrix factorization.
In Proceedingsof the 2013 Conference of the North American Chapterof the Association for Computational Linguistics: HumanLanguage Technologies, pages 401?410, Atlanta, Georgia,June.
Association for Computational Linguistics.Alan Lee Eleni Miltsakaki Livio Robaldo Aravind JoshiRashmi Prasad, Nikhil Dinesh and Bonnie Webber.2008.
The penn discourse treebank 2.0.
In BenteMaegaard Joseph Mariani Jan Odijk Stelios PiperidisDaniel Tapias Nicoletta Calzolari (Conference Chair),105Khalid Choukri, editor, Proceedings of the Sixth Interna-tional Conference on Language Resources and Evaluation(LREC?08), Marrakech, Morocco, may.
European Lan-guage Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.Swapna Somasundaran and Janyce Wiebe.
2009.
Recogniz-ing stances in online debates.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 1 - Volume 1, ACL ?09,pages 226?234, Stroudsburg, PA, USA.
Association forComputational Linguistics.Swapna Somasundaran and Janyce Wiebe.
2010.
Recogniz-ing stances in ideological on-line debates.
In Proceedingsof the NAACL HLT 2010 Workshop on Computational Ap-proaches to Analysis and Generation of Emotion in Text,CAAGET ?10, pages 116?124, Stroudsburg, PA, USA.Association for Computational Linguistics.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith, andDaniel M. Ogilvie.
1966.
The General Inquirer: A Com-puter Approach to Content Analysis.
MIT Press, Cam-bridge, MA.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In Proceedings of the2006 Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?06, pages 327?335, Strouds-burg, PA, USA.
Association for Computational Linguis-tics.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan,and Ryan McDonald.
2010.
The viability of web-derivedpolarity lexicons.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics, HLT?10, pages 777?785, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott,and Joseph King.
2012a.
A corpus for research on de-liberation and debate.
In Proceedings of the Eight Inter-national Conference on Language Resources and Evalu-ation (LREC?12), Istanbul, Turkey, may.
European Lan-guage Resources Association (ELRA).Marilyn A. Walker, Pranav Anand, Rob Abbott, and RickyGrant.
2012b.
Stance classification using dialogic prop-erties of persuasion.
In HLT-NAACL, pages 592?596.
TheAssociation for Computational Linguistics.Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey,and Geoffrey Raymond.
2011.
Detection of agreementand disagreement in broadcast conversations.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Technolo-gies: Short Papers - Volume 2, HLT ?11, pages 374?378, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In Proceedings of the Conference on HumanLanguage Technology and Empirical Methods in NaturalLanguage Processing, HLT ?05, pages 347?354, Strouds-burg, PA, USA.
Association for Computational Linguis-tics.Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.
2012.Unifying local and global agreement and disagreementclassification in online debates.
In Proceedings of the3rd Workshop in Computational Approaches to Subjec-tivity and Sentiment Analysis, WASSA ?12, pages 61?69, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learning fromlabeled and unlabeled data with label propagation.
InTechnical Report CMU-CALD-02-107.106
