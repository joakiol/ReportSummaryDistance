Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 476?485,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLeveraging Synthetic Discourse Data via Multi-task Learning for ImplicitDiscourse Relation RecognitionMan Lan and Yu XuDepartment of Computer Science and TechnologyEast China Normal UniversityShanghai, P.R.Chinamlan@cs.ecnu.edu.cn51101201049@ecnu.cnZheng-Yu NiuBaidu Inc.Beijing, P.R.Chinaniuzhengyu@baidu.comAbstractTo overcome the shortage of labeled datafor implicit discourse relation recogni-tion, previous works attempted to auto-matically generate training data by remov-ing explicit discourse connectives fromsentences and then built models on thesesynthetic implicit examples.
However, aprevious study (Sporleder and Lascarides,2008) showed that models trained on thesesynthetic data do not generalize very wellto natural (i.e.
genuine) implicit discoursedata.
In this work we revisit this issue andpresent a multi-task learning based systemwhich can effectively use synthetic datafor implicit discourse relation recognition.Results on PDTB data show that under themulti-task learning framework our modelswith the use of the prediction of explicitdiscourse connectives as auxiliary learn-ing tasks, can achieve an averaged F1 im-provement of 5.86% over baseline models.1 IntroductionThe task of implicit discourse relation recognitionis to identify the type of discourse relation (a.k.a.rhetorical relation) hold between two spans oftext, where there is no discourse connective (a.k.a.discourse marker, e.g., but, and) in context to ex-plicitly mark their discourse relation (e.g., Con-trast or Explanation).
It can be of great benefitto many downstream NLP applications, such asquestion answering (QA) (Verberne et al, 2007),information extraction (IE) (Cimiano et al, 2005),and machine translation (MT), etc.
This task isquite challenging due to two reasons.
First, with-out discourse connective in text, the task is quitedifficult in itself.
Second, implicit discourse rela-tion is quite frequent in text.
For example, almosthalf the sentences in the British National Corpusheld implicit discourse relations (Sporleder andLascarides, 2008).
Therefore, the task of implicitdiscourse relation recognition is the key to im-proving end-to-end discourse parser performance.To overcome the shortage of manually anno-tated training data, (Marcu and Echihabi, 2002)proposed a pattern-based approach to automat-ically generate training data from raw corpora.This line of research was followed by (Sporlederand Lascarides, 2008) and (Blair-Goldensohn,2007).
In these works, sentences containing cer-tain words or phrases (e.g.
but, although) wereselected out from raw corpora using a pattern-based approach and then these words or phraseswere removed from these sentences.
Thus theresulting sentences were used as synthetic train-ing examples for implicit discourse relation recog-nition.
Since there is ambiguity of a word orphrase serving for discourse connective (i.e., theambiguity between discourse and non-discourseusage or the ambiguity between two or more dis-course relations if the word or phrase is used as adiscourse connective), the synthetic implicit datawould contain a lot of noises.
Later, with the re-lease of manually annotated corpus, such as PennDiscourse Treebank 2.0 (PDTB) (Prasad et al,2008), recent studies performed implicit discourserelation recognition on natural (i.e., genuine) im-plicit discourse data (Pitler et al, 2009) (Lin et al,2009) (Wang et al, 2010) with the use of linguis-tically informed features and machine learning al-gorithms.
(Sporleder and Lascarides, 2008) conducted astudy of the pattern-based approach presented by(Marcu and Echihabi, 2002) and showed that themodel built on synthetical implicit data has notgeneralize well on natural implicit data.
Theyfound some evidence that this behavior is largelyindependent of the classifiers used and seems tolie in the data itself (e.g., marked and unmarkedexamples may be too dissimilar linguistically and476removing unambiguous markers in the automaticlabelling process may lead to a meaning shift inthe examples).
We state that in some cases it istrue while in other cases it may not always be so.A simple example is given here:(E1) a.
We can?t win.b.
[but] We must keep trying.We may find that in this example whether the in-sertion or the removal of connective but wouldnot lead to a redundant or missing information be-tween the above two sentences.
That is, discourseconnectives can be inserted between or removedfrom two sentences without changing the seman-tic relations between them in some cases.
An-other similar observation is in the annotation pro-cedure of PDTB.
To label implicit discourse re-lation, annotators inserted connective which canbest express the relation between sentences with-out any redundancy1.
We see that there shouldbe some linguistical similarities between explicitand implicit discourse examples.
Therefore, thefirst question arises: can we exploit this kind oflinguistic similarity between explicit and implicitdiscourse examples to improve implicit discourserelation recognition?In this paper, we propose a multi-task learningbased method to improve the performance of im-plicit discourse relation recognition (as main task)with the help of relevant auxiliary tasks.
Specif-ically, the main task is to recognize the implicitdiscourse relations based on genuine implicit dis-course data and the auxiliary task is to recognizethe implicit discourse relations based on syntheticimplicit discourse data.
According to the princi-ple of multi-task learning, the learning model canbe optimized by the shared part of the main taskand the auxiliary tasks without bring unnecessarynoise.
That means, the model can learn from syn-thetic implicit data while it would not bring unnec-essary noise from synthetic implicit data.Although (Sporleder and Lascarides, 2008) didnot mention, we speculate that another possiblereason for the reported worse performance mayresult from noises in synthetic implicit discoursedata.
These synthetic data can be generated fromtwo sources: (1) raw corpora with the use ofpattern-based approach in (Marcu and Echihabi,1According to the PDTB Annotation Manual (PDTB-Group, 2008), if the insertion of connective leads to ?redun-dancy?, the relation is annotated as Alternative lexicalizations(AltLex), not implicit.2002) and (Sporleder and Lascarides, 2008), and(2) manually annotated explicit data with the re-moval of explicit discourse connectives.
Obvi-ously, the data generated from the second sourceis cleaner and more reliable than that from thefirst source.
Therefore, the second question to ad-dress in this work is: whether synthetic implicitdiscourse data generated from explicit discoursedata source (i.e., the second source) can lead toa better performance than that from raw corpora(i.e., the first source)?
To answer this question,we will make a comparison of synthetic discoursedata generated from two corpora, i.e., the BILLIPcorpus and the explicit discourse data annotated inPDTB.The rest of this paper is organized as follows.Section 2 reviews related work on implicit dis-course relation classification and multi-task learn-ing.
Section 3 presents our proposed multi-tasklearning method for implicit discourse relationclassification.
Section 4 provides the implemen-tation technique details of the proposed multi-taskmethod.
Section 5 presents experiments and dis-cusses results.
Section 6 concludes this work.2 Related Work2.1 Implicit discourse relation classification2.1.1 Unsupervised approachesDue to the lack of benchmark data for implicitdiscourse relation analysis, earlier work used un-labeled data to generate synthetic implicit dis-course data.
For example, (Marcu and Echi-habi, 2002) proposed an unsupervised methodto recognize four discourse relations, i.e., Con-trast, Explanation-evidence, Condition and Elab-oration.
They first used unambiguous pattern toextract explicit discourse examples from raw cor-pus.
Then they generated synthetic implicit dis-course data by removing explicit discourse con-nectives from sentences extracted.
In their work,they collected word pairs from synthetic data setas features and used machine learning method toclassify implicit discourse relation.
Based on thiswork, several researchers have extended the workto improve the performance of relation classifica-tion.
For example, (Saito et al, 2006) showed thatthe use of phrasal patterns as additional featurescan help a word-pair based system for discourserelation prediction on a Japanese corpus.
Further-more, (Blair-Goldensohn, 2007) improved previ-ous work with the use of parameter optimization,477topic segmentation and syntactic parsing.
How-ever, (Sporleder and Lascarides, 2008) showedthat the training model built on a synthetic dataset, like the work of (Marcu and Echihabi, 2002),may not be a good strategy since the linguistic dis-similarity between explicit and implicit data mayhurt the performance of a model on natural datawhen being trained on synthetic data.2.1.2 Supervised approachesThis line of research work approaches this relationprediction problem by recasting it as a classifica-tion problem.
(Soricut and Marcu, 2003) parsedthe discourse structures of sentences on RST Bankdata set (Carlson et al, 2001) which is annotatedbased on Rhetorical Structure Theory (Mann andThompson, 1988).
(Wellner et al, 2006) pre-sented a study of discourse relation disambigua-tion on GraphBank (Wolf et al, 2005).
Recently,(Pitler et al, 2009) (Lin et al, 2009) and (Wanget al, 2010) conducted discourse relation study onPDTB (Prasad et al, 2008) which has been widelyused in this field.2.1.3 Semi-supervised approachesResearch work in this category exploited both la-beled and unlabeled data for discourse relationprediction.
(Hernault et al, 2010) presented asemi-supervised method based on the analysis ofco-occurring features in labeled and unlabeleddata.
Very recently, (Hernault et al, 2011) in-troduced a semi-supervised work using structurelearning method for discourse relation classifica-tion, which is quite relevant to our work.
However,they performed discourse relation classification onboth explicit and implicit data.
And their work isdifferent from our work in many aspects, such as,feature sets, auxiliary task, auxiliary data, class la-bels, learning framework, and so on.
Furthermore,there is no explicit conclusion or evidence in theirwork to address the two questions raised in Sec-tion 1.Unlike their previous work, our previous work(Zhou et al, 2010) presented a method to predictthe missing connective based on a language modeltrained on an unannotated corpus.
The predictedconnective was then used as a feature to classifythe implicit relation.2.2 Multi-task learningMulti-task learning is a kind of machine learningmethod, which learns a main task together withother related auxiliary tasks at the same time, us-ing a shared representation.
This often leads toa better model for the main task, because it al-lows the learner to use the commonality amongthe tasks.
Many multi-task learning methods havebeen proposed in recent years, (Ando and Zhang,2005a), (Argyriou et al, 2008), (Jebara, 2004),(Bonilla et al, 2008), (Evgeniou and Pontil, 2004),(Baxter, 2000), (Caruana, 1997), (Thrun, 1996).One group uses task relations as regularizationterms in the objective function to be optimized.For example, in (Evgeniou and Pontil, 2004) theregularization terms make the parameters of mod-els closer for similar tasks.
Another group is pro-posed to find the common structure from data andthen utilize the learned structure for multi-tasklearning (Argyriou et al, 2008) (Ando and Zhang,2005b).3 Multi-task Learning for DiscourseRelation Prediction3.1 MotivationThe idea of using multi-task learning for implicitdiscourse relation classification is motivated bythe observations that we have made on implicitdiscourse relation.On one hand, since building a hand-annotatedimplicit discourse relation corpus is costly andtime consuming, most previous work attempted touse synthetic implicit discourse examples as train-ing data.
However, (Sporleder and Lascarides,2008) found that the model trained on syntheticimplicit data has not performed as well as expectedin natural implicit data.
They stated that the reasonis linguistic dissimilarity between explicit and im-plicit discourse data.
This indicates that straightlyusing synthetic implicit data as training data maynot be helpful.On the other hand, as shown in Section 1, weobserve that in some cases explicit discourse rela-tion and implicit discourse relation can express thesame meaning with or without a discourse connec-tive.
This indicates that in certain degree they mustbe similar to each other.
If it is true, the syntheticimplicit relations are expected to be helpful for im-plicit discourse relation classification.
Therefore,what we have to do is to find a way to train a modelwhich has the capabilities to learn from their sim-ilarity and to ignore their dissimilarity as well.To solve it, we propose a multi-task learn-ing method for implicit discourse relation classi-478fication, where the classification model seeks theshared part through jointly learning main task andmultiple auxiliary tasks.
As a result, the model canbe optimized by the similar shared part withoutbringing noise in the dissimilar part.
Specifically,in this work, we use alternating structure optimiza-tion (ASO) (Ando and Zhang, 2005a) to constructthe multi-task learning framework.
ASO has beenshown to be useful in a semi-supervised learningconfiguration for several NLP applications, suchas, text chunking (Ando and Zhang, 2005b) andtext classification (Ando and Zhang, 2005a).3.2 Multi-task learning and ASOGenerally, multi-task learning(MTL) considers mprediction problems indexed by ?
?
{1, ...,m},each with n?
samples (X?i , Y ?i ) for i ?
{1, ...n?
}(Xi are input feature vectors and Yi are corre-sponding classification labels) and assumes thatthere exists a common predictive structure sharedby these m problems.
Generally, the joint linearmodel for MTL is to predict problem ?
in the fol-lowing form:f?
(?, X) = wT?
X + vT?
?X,?
?T = I, (1)where I is the identity matrix,w?
and v?
are weightvectors specific to each problem ?, and ?
is thestructure matrix shared by all the m predictors.The main goal of MTL is to learn a common goodfeature map ?X for all the m problems.
SeveralMTL methods have been presented to learn ?Xfor all the m problems.
In this work, we adopt theASO method.Specifically, the ASO method adopted singu-lar value decomposition (SVD) to obtain ?
andm predictors that minimize the empirical risksummed over all the m problems.
Thus, the prob-lem of optimization becomes the minimization ofthe joint empirical risk written as:m?
?=1( n??i=1L(f?
(?, X?i ), Yi)n?+ ?||W?||2)(2)where loss function L(.)
quantifies the differencebetween the prediction f(Xi) and the true out-put Yi for each predictor, and ?
is a regulariza-tion parameter for square regularization to controlthe model complexity.
To minimize the empiricalrisk, ASO repeats the following alternating opti-mization procedure until a convergence criterionis met:1) Fix (?, V?
), and find m predictors f?
thatminimize the above joint empirical risk.2) Fix m predictors f?, and find (?, V?)
thatminimizes the above joint empirical risk.3.3 Auxiliary tasksThere are two main principles to create auxiliarytasks.
First, the auxiliary tasks should be auto-matically labeled in order to reduce the cost ofmanual labeling.
Second, since the MTL modellearns from the shared part of main task and aux-iliary tasks, the auxiliary tasks should be quite rel-evant/similar to the main task.
It is generally be-lieved that the more the auxiliary tasks are relevantto the main task, the more the main task can ben-efit from the auxiliary tasks.
Following these twoprinciples, we create the auxiliary tasks by gener-ating automatically labeled data as follows.Previous work (Marcu and Echihabi, 2002) and(Sporleder and Lascarides, 2008) adopted prede-fined pattern-based approach to generate syntheticlabeled data, where each predefined pattern hasone discourse relation label.
In contrast, we adoptan automatic approach to generate synthetic la-beled data, where each discourse connective be-tween two texts serves as their relation label.
Thereason lies in the very strong connection betweendiscourse connectives and discourse relations.
Forexample, the connective but always indicates acontrast relation between two texts.
And (Pitler etal., 2008) proved that using only connective itself,the accuracy of explicit discourse relation classifi-cation is over 93%.To build the mapping between discourse con-nective and discourse relation, for each connec-tive, we count the times it appears in each relationand regard the relation in which it appears mostfrequently as its most relevant relation.
Based onthis mapping between connective and relation, weextract the synthetic labeled data containing theconnective as training data for auxiliary tasks.For example, and appears 3, 000 times in PDTBas a discourse connective.
Among them, it is man-ually annotated as an Expansion relation for 2, 938times.
So we regard the Expansion relation as itsmost relevant relation and generate a mapping pat-tern like: ?and ?
Expansion?.
Then we extractall sentences which contain discourse ?and?
andremove this connective ?and?
from sentences togenerate synthetic implicit data.
The resulting sen-tences are used in auxiliary task and automatically479marked as Expansion relation.4 Implementation Details of Multi-taskLearning Method4.1 Data sets for main and auxiliary tasksTo examine whether there is a difference in syn-thetic implicit data generated from unannotatedand annotated corpus, we use two corpora.
Oneis a hand-annotated explicit discourse corpus, i.e.,the explicit discourse relations in PDTB, denotedas exp.
Another is an unannotated corpus, i.e.,BLLIP (David McClosky and Johnson., 2008).4.1.1 Penn Discourse TreebankPDTB (Prasad et al, 2008) is the largest hand-annotated corpus of discourse relation so far.
Itcontains 2, 312 Wall Street Journal (WSJ) articles.The sense label of discourse relations is hierarchi-cally with three levels, i.e., class, type and sub-type.
The top level contains four major seman-tic classes: Comparison (denoted as Comp.
), Con-tingency (Cont.
), Expansion (Exp.)
and Temporal(Temp.).
For each class, a set of types is used torefine relation sense.
The set of subtypes is to fur-ther specify the semantic contribution of each ar-gument.
In this paper, we focus on the top level(class) and the second level (type) relations be-cause the subtype relations are too fine-grainedand only appear in some relations.Both explicit and implicit discourse relationsare labeled in PDTB.
In our experiment, the im-plicit discourse relations are used in the main taskand for evaluation.
While the explicit discourserelations are used in the auxiliary task.
A detaileddescription of the data sources for different tasksis given below.Data set for main task Following previouswork in (Pitler et al, 2009) and (Zhou et al, 2010),the implicit relations in sections 2-20 are used astraining data for the main task (denoted as imp)and the implicit relations in sections 21-22 arefor evaluation.
Table 1 shows the distribution ofimplicit relations.
There are too few training in-stances for six second level relations (indicated by* in Table 1), so we removed these six relations inour experiments.Data set for auxiliary task All explicit in-stances in sections 00-24 in PDTB, i.e., 18, 459instances, are used for auxiliary task (denoted asexp).
Following the method described in Section3.3, we build the mapping patterns between con-Top level Second level train testTemp 736 83Synchrony 203 28Asynchronous 532 55Cont 3333 279Cause 3270 272Pragmatic Cause* 64 7Condition* 1 0Pragmatic condition* 1 0Comp 1939 152Contrast 1607 134Pragmatic contrast* 4 0Concession 183 17Pragmatic concession* 1 0Exp 6316 567Conjunction 2872 208Instantiation 1063 119Restatement 2405 213Alternative 147 9Exception* 0 0List 338 12Table 1: Distribution of implicit discourse rela-tions in the top and second level of PDTBnectives and relations in PDTB and generate syn-thetic labeled data by removing the connectives.According to the most relevant relation sense ofconnective removed, the resulting instances aregrouped into different data sets.4.1.2 BLLIPBLLIP North American News Text (Complete) isused as unlabeled data source to generate syn-thetic labeled data.
In comparison with the syn-thetic labeled data generated from the explicit re-lations in PDTB, the synthetic labeled data fromBLLIP contains more noise.
This is because theformer data is manually annotated whether a wordserves as discourse connective or not, while thelatter does not manually disambiguate two typesof ambiguity, i.e., whether a word serves as dis-course connective or not, and the type of discourserelation if it is a discourse connective.
Finally, weextract 26, 412 instances from BLLIP (denoted asBLLIP) and use them for auxiliary task.4.2 Feature representationFor both main task and auxiliary tasks, we adoptthe following three feature types.
These featuresare chosen due to their superior performance inprevious work (Pitler et al, 2009) and our previ-ous work (Zhou et al, 2010).Verbs: Following (Pitler et al, 2009), we ex-tract the pairs of verbs from both text spans.
Thenumber of verb pairs which have the same highest480Levin verb class levels (Levin, 1993) is countedas a feature.
Besides, the average length of verbphrases in each argument is included as a feature.In addition, the part of speech tags of the mainverbs (e.g., base form, past tense, 3rd person sin-gular present, etc.)
in each argument, i.e., MD,VB, VBD, VBG, VBN, VBP, VBZ, are recordedas features, where we simply use the first verb ineach argument as the main verb.Polarity: This feature records the number ofpositive, negated positive, negative and neutralwords in both arguments and their cross productas well.
For negated positives, we first locate thenegated words in text span and then define theclosely behind positive word as negated positive.The polarity of each word in arguments is de-rived from Multi-perspective Question AnsweringOpinion Corpus (MPQA) (Wilson et al, 2009).Modality: We examine six modal words (i.e.,can, may, must, need, shall, will) including theirvarious tenses or abbreviation forms in both argu-ments.
This feature records the presence or ab-sence of modal words in both arguments and theircross product.4.3 Classifiers used multi-task learningWe extract the above linguistically informed fea-tures from two synthetic implicit data sets (i.e.,BLLIP and exp) to learn the auxiliary classifier andfrom the natural implicit data set (i.e., imp) to learnthe main classifier.
Under the ASO-based multi-task learning framework, the model of main tasklearns from the shared part of main task and aux-iliary tasks.
Specifically, we adopt multiple binaryclassification to build model for main task.
Thatis, for each discourse relation, we build a binaryclassifier.5 Experiments and Results5.1 ExperimentsAlthough previous work has been done on PDTB(Pitler et al, 2009) and (Lin et al, 2009), we can-not make a direct comparison with them becausevarious experimental conditions, such as, differ-ent classification strategies (multi-class classifica-tion, multiple binary classification), different datapreparation (feature extraction and selection), dif-ferent benchmark data collections (different sec-tions for training and test, different levels of dis-course relations), different classifiers with variousparameters (MaxEnt, Na?
?ve Bayes, SVM, etc) andeven different evaluation methods (F1, accuracy)have been adopted by different researchers.Therefore, to address the two questions raised inSection 1 and to make the comparison reliable andreasonable, we performed experiments on the topand second level of PDTB using single task learn-ing and multi-task learning, respectively.
The sys-tems using single task learning serve as baselinesystems.
Under the single task learning, variouscombinations of exp and BLLIP data are incorpo-rated with imp data for the implicit discourse rela-tion classification task.We hypothesize that synthetical implicit datawould contribute to the main task, i.e., the implicitdiscourse relation classification.
Specifically, thenatural implicit data (i.e., imp) are used to createmain task and the synthetical implicit data (exp orBLLIP) are used to create auxiliary tasks for thepurpose of optimizing the objective functions ofmain task.
If the hypothesis is correct, the perfor-mance of main task would be improved by auxil-iary tasks created from synthetical implicit data.Thus in the experiments of multi-task learning,only natural implicit examples (i.e., imp) data areused for main task training while different combi-nations of synthetical implicit examples (exp andBLLIP) are used for auxiliary task training.We adopt precision, recall and their combina-tion F1 for performance evaluation.
We also per-form one-tailed t-test to validate if there is signif-icant difference between two methods in terms ofF1 performance analysis.5.2 ResultsTable 2 summarizes the experimental results undersingle and multi-task learning on the top level offour PDTB relations with respect to different com-binations of synthetic implicit data.
For each rela-tion, the first three rows indicate the results of us-ing different single training data under single tasklearning and the last three rows indicate the resultsusing different combinations of training data un-der single task and multi-task learning.
The bestF1 for every relation is shown in bold font.
Fromthis table, we can find that on four relations, ourmulti-task learning systems achieved the best per-formance using the combination of exp and BLLIPsynthetic data.Table 3 summarizes the best single task and thebest multi-task learning results on the second levelof PDTB.
For four relations, i.e., Synchrony, Con-481Single-task Multi-taskLevel 1 class Data P R F1 Data Data P R F1(main) (aux)Comp.
imp 21.43 37.50 27.27 - - - - -BLLIP 12.68 53.29 20.48 - - - - -exp 15.25 50.66 23.44 - - - - -imp + exp 16.94 40.13 23.83 imp exp 22.94 49.34 30.90imp + BLLIP 13.56 44.08 20.74 imp BLLIP 20.47 63.16 30.92imp + exp + BLLIP 14.54 38.16 21.05 imp exp + BLLIP 23.47 48.03 31.53Cont.
imp 37.65 43.73 40.46 - - - - -BLLIP 33.72 31.18 32.40 - - - - -exp 35.24 26.52 30.27 - - - - -imp + exp 39.00 13.98 20.58 imp exp 39.94 45.52 42.55imp + BLLIP 37.30 24.73 29.74 imp BLLIP 37.80 63.80 47.47imp + exp + BLLIP 39.37 31.18 34.80 imp exp + BLLIP 35.90 70.25 47.52Exp.
imp 56.59 66.67 61.21 - - - - -BLLIP 53.29 40.04 45.72 - - - - -exp 57.97 58.38 58.17 - - - - -imp + exp 57.32 65.61 61.18 imp exp 59.14 67.90 63.22imp + BLLIP 56.28 65.61 60.59 imp BLLIP 53.80 99.82 69.92imp + exp + BLLIP 55.81 65.26 60.16 imp exp + BLLIP 53.90 99.82 70.01Temp.
imp 16.46 63.86 26.17 - - - - -BLLIP 17.31 43.37 24.74 - - - - -exp 15.46 36.14 21.66 - - - - -imp + exp 15.35 39.76 22.15 imp exp 18.60 63.86 28.80imp + BLLIP 14.74 33.73 20.51 imp BLLIP 18.12 67.47 28.57imp + exp + BLLIP 15.94 39.76 22.76 imp exp + BLLIP 19.08 65.06 29.51Table 2: Performance of precision, recall and F1 for 4 Level 1 relation classes.
?-?
indicates N.A.Single-task Multi-taskLevel 2 type Data P R F1 Data Data P R F1(main) (aux)Asynchronous imp 11.36 74.55 19.71 imp exp + BLLIP 23.08 21.82 22.43Synchrony imp - - - imp exp + BLLIP - - -Cause imp 36.38 64.34 46.48 imp exp + BLLIP 36.01 67.65 47.00Contrast imp 20.07 42.54 27.27 imp exp + BLLIP 20.70 52.99 29.77Concession imp - - - imp exp + BLLIP - - -Conjunction imp 26.35 63.46 37.24 imp exp + BLLIP 26.29 73.56 38.73Instantiation imp 22.78 53.78 32.00 imp exp + BLLIP 22.55 57.98 32.47Restatement imp 23.11 67.61 34.45 imp exp + BLLIP 26.93 53.99 35.94Alternative imp - - - imp exp + BLLIP - - -List imp - - - imp exp + BLLIP - - -Table 3: Performance of precision, recall and F1 for 10 Level 2 relation types.
?-?
indicates 0.00.cession, Alternative and List, the classifier labelsno instances due to the small percentages for thesefour types.Table 4 summarizes the one-tailed t-test resultson the top level of PDTB between the best singletask learning system (i.e., imp) and three multi-task learning systems (imp:exp+BLLIP indicatesthat imp is used for main task and the combi-nation of exp and BLLIP are for auxiliary task).The systems with insignificant performance differ-ences are grouped into one set and ?>?
and ?>>?denote better than at significance level 0.01 and0.001 respectively.5.3 DiscussionFrom Table 2 to Table 4, several findings can befound as follows.We can see that the multi-task learning sys-tems perform consistently better than the singletask learning systems for the prediction of implicitdiscourse relations.
Our best multi-task learningsystem achieves an averaged F1 improvement of5.86% over the best single task learning system onthe top level of PDTB relations.
Specifically, for482Class One-tailed t-test resultsComp.
(imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)Cont.
(imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)Exp.
(imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)Temp.
(imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)Table 4: Statistical significance tests results.the relations Comp., Cont., Exp., Temp., our bestmulti-task learning system achieve 4.26%, 7.06%,8.8% and 3.34% F1 improvements over the bestsingle task learning system.
It indicates that usingsynthetic implicit data as auxiliary task greatly im-proves the performance of the main task.
This isconfirmed by the following t-tests in Table 4.In contrast to the performance of multi-tasklearning, the performance of the best single tasklearning system has been achieved on natural im-plicit discourse data alone.
This finding is con-sistent with (Sporleder and Lascarides, 2008).
Itindicates that under single task learning, directlyadding synthetic implicit data to increase the num-ber of training data cannot be helpful to implicitdiscourse relation classification.
The possible rea-sons result from (1) the different nature of implicitand explicit discourse data in linguistics and (2)the noise brought from synthetic implicit data.Based on the above analysis, we state that it isthe way of utilizing synthetic implicit data that isimportant for implicit discourse relation classifica-tion.Although all three multi-task learning systemsoutperformed single task learning systems, wefind that the two synthetic implicit data sets havenot been shown a universally consistent perfor-mance on four top level PDTB relations.
On onehand, for the relations Comp.
and Temp., the per-formance of the two synthetic implicit data setsalone and their combination are comparable toeach other and there is no significant differencebetween them.
On the other hand, for the rela-tions Cont.
and Exp., the performance of exp datais inferior to that of BLLIP and their combination.This is contrary to our original expectation that expdata which has been manually annotated for dis-course connective disambiguation should outper-form BLLIP which contains a lot of noise.
Thisfinding indicates that under the multi-task learn-ing, it may not be worthy of using manually anno-tated corpus to generate auxiliary data.
It is quitepromising since it can provide benefits to reducingthe cost of human efforts on corpus annotation.5.4 Ambiguity AnalysisAlthough our experiments show that synthetic im-plicit data can help implicit discourse relation clas-sification under multi-task learning framework,the overall performance is still quite low (44.64%in F1).
Therefore, we analyze the types of ambi-guity in relations and connectives in order to mo-tivate possible future work.5.4.1 Ambiguity of implicit relationWithout explicit discourse connective, the implicitdiscourse relation instance can be understood intwo or more different ways.
Given the exampleE2 in PDTB, the PDTB annotators explain it asContingency or Expansion relation and manuallyinsert corresponding implicit connective for onething or because to express its relation.
(E2) Arg1:Now the stage is set for the battle toplay outArg2:The anti-programmers are gettingsome helpful thunder from CongressConnective1:becauseSense1:Contingency.Cause.ReasonConnective2:for one thingSense2:Expansion.Instantiation(wsj 0118)Thus the ambiguity of implicit discourse rela-tions makes this task difficult in itself.5.4.2 Ambiguity of discourse connectivesAs we mentioned before, even given an explicitdiscourse connective in text, its discourse rela-tion still can be explained in two or more differ-ent ways.
And for different connectives, the am-biguity of relation senses is quite different.
Thatis, the most frequent sense is not always the onlysense that a connective expresses.
In example E3,?since?
is explained by annotators to express Tem-poral or Contingency relation.
(E3) Arg1:MiniScribe has been on the rocksArg2:since it disclosed early this year thatits earnings reports for 1988 weren?t accu-rate.483Sense1:Temporal.Asynchronous.SuccessionSense2:Contingency.Cause.Reason(wsj 0003)In PDTB, ?since?
appears 184 times in explicitdiscourse relations.
It expresses Temporal relationfor 80 times, Contingency relation for 94 timesand both Temporal and Contingency for 10 time(like example E3).
Therefore, although we use itsmost frequent sense, i.e., Contingency, to automat-ically extract sentences and label them, almost lessthan half of them actually express Temporal rela-tion.
Thus the ambiguity of discourse connectivesis another source which has brought noise to datawhen we generate synthetical implicit discourserelation.6 ConclusionsIn this paper, we present a multi-task learningmethod to improve implicit discourse relationclassification by leveraging synthetic implicit dis-course data.
Results on PDTB show that underthe framework of multi-task learning, using syn-thetic discourse data as auxiliary task significantlyimproves the performance of main task.
Our bestmulti-task learning system achieves an averagedF1 improvement of 5.86% over the best single tasklearning system on the top level of PDTB rela-tions.
Specifically, for the relations Comp., Cont.,Exp., Temp., our best multi-task learning systemachieves 4.26%, 7.06%, 8.8%, and 3.34% F1 im-provements over a state of the art baseline system.This indicates that it is the way of utilizing syn-thetic discourse examples that is important for im-plicit discourse relation classification.AcknowledgementsThis research is supported by grants from Na-tional Natural Science Foundation of China(No.60903093), Shanghai Pujiang Talent Program(No.09PJ1404500), Doctoral Fund of Ministry ofEducation of China (No.
20090076120029) andShanghai Knowledge Service Platform Project(No.
ZF1213).ReferencesR.K.
Ando and T. Zhang.
2005a.
A framework forlearning predictive structures from multiple tasksand unlabeled data.
The Journal of Machine Learn-ing Research, 6:1817?1853.R.K.
Ando and T. Zhang.
2005b.
A high-performancesemi-supervised learning method for text chunking.pages 1?9.
Association for Computational Linguis-tics.
Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics.A.
Argyriou, C.A.
Micchelli, M. Pontil, and Y. Ying.2008.
A spectral regularization framework formulti-task structure learning.
Advances in NeuralInformation Processing Systems, 20:2532.J.
Baxter.
2000.
A model of inductive bias learning.
J.Artif.
Intell.
Res.
(JAIR), 12:149?198.S.J.
Blair-Goldensohn.
2007.
Long-answer questionanswering and rhetorical-semantic relations.
Ph.D.thesis.E.
Bonilla, K.M.
Chai, and C. Williams.
2008.
Multi-task gaussian process prediction.
Advances in Neu-ral Information Processing Systems, 20(October).L.
Carlson, D. Marcu, and M.E.
Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of rhetorical structure theory.
pages 1?10.
As-sociation for Computational Linguistics.
Proceed-ings of the Second SIGdial Workshop on Discourseand Dialogue-Volume 16.R.
Caruana.
1997.
Multitask learning.
MachineLearning, 28(1):41?75.P.
Cimiano, U. Reyle, and J. Saric.
2005.
Ontology-driven discourse analysis for information extraction.Data and Knowledge Engineering, 55(1):59?83.Eugene Charniak David McClosky and Mark Johnson.2008.
Bllip north american news text, complete.T.
Evgeniou and M. Pontil.
2004.
Regularized multi?task learning.
pages 109?117.
ACM.
Proceedingsof the tenth ACM SIGKDD international conferenceon Knowledge discovery and data mining.H.
Hernault, D. Bollegala, and M. Ishizuka.
2010.
Asemi-supervised approach to improve classificationof infrequent discourse relations using feature vectorextension.
pages 399?409.
Association for Compu-tational Linguistics.
Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing.H.
Hernault, D. Bollegala, and M. Ishizuka.
2011.Semi-supervised discourse relation classificationwith structural learning.
In Proceedings of the 12thinternational conference on Computational linguis-tics and intelligent text processing - Volume PartI, CICLing?11, pages 340?352, Berlin, Heidelberg.Springer-Verlag.T.
Jebara.
2004.
Multi-task feature and kernel se-lection for svms.
page 55.
ACM.
Proceedings ofthe twenty-first international conference on Machinelearning.B.
Levin.
1993.
English verb classes and alternations:A preliminary investigation, volume 348.
Universityof Chicago press Chicago, IL:.484Z.
Lin, M.Y.
Kan, and H.T.
Ng.
2009.
Recogniz-ing implicit discourse relations in the penn discoursetreebank.
pages 343?351.
Association for Compu-tational Linguistics.
Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1-Volume 1.W.C.
Mann and S.A. Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text-Interdisciplinary Journal for theStudy of Discourse, 8(3):243?281.D.
Marcu and A. Echihabi.
2002.
An unsupervisedapproach to recognizing discourse relations.
pages368?375.
Association for Computational Linguis-tics.
Proceedings of the 40th Annual Meeting onAssociation for Computational Linguistics.PDTB-Group.
2008.
The penn discourse treebank 2.0annotation manual.
Technical report, Institute forResearch in Cognitive Science, University of Penn-sylvania.E.
Pitler, M. Raghupathy, H. Mehta, A. Nenkova,A.
Lee, and A. Joshi.
2008.
Easily identifiable dis-course relations.
Citeseer.
Proceedings of the 22ndInternational Conference on Computational Linguis-tics (COLING 2008), Manchester, UK, August.E.
Pitler, A. Louis, and A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations intext.
pages 683?691.
Association for ComputationalLinguistics.
Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2-Volume 2.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InIn Proceedings of LREC.M.
Saito, K. Yamamoto, and S. Sekine.
2006.
Us-ing phrasal patterns to identify discourse relations.pages 133?136.
Association for Computational Lin-guistics.
Proceedings of the Human Language Tech-nology Conference of the NAACL, Companion Vol-ume: Short Papers on XX.R.
Soricut and D. Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
pages 149?156.
Association for ComputationalLinguistics.
Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology-Volume 1.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: An assessment.
Natural Language Engineer-ing, 14(03):369?416.S.
Thrun.
1996.
Is learning the n-th thing any easierthan learning the first?
Advances in Neural Infor-mation Processing Systems, pages 640?646.S.
Verberne, L. Boves, N. Oostdijk, and P.A.
Coppen.2007.
Evaluating discourse-based answer extractionfor why-question answering.
pages 735?736.
ACM.Proceedings of the 30th annual international ACMSIGIR conference on Research and development ininformation retrieval.W.T.
Wang, J. Su, and C.L.
Tan.
2010.
Kernel baseddiscourse relation recognition with temporal order-ing information.
pages 710?719.
Association forComputational Linguistics.
Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics.B.
Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,and R. Sauri.
2006.
Classification of discourse co-herence relations: An exploratory study using multi-ple knowledge sources.
pages 117?125.
Associationfor Computational Linguistics.
Proceedings of the7th SIGdial Workshop on Discourse and Dialogue.T.
Wilson, J. Wiebe, and P. Hoffmann.
2009.
Rec-ognizing contextual polarity: An exploration of fea-tures for phrase-level sentiment analysis.
Computa-tional Linguistics, 35(3):399?433.F.
Wolf, E. Gibson, A. Fisher, and M. Knight.
2005.The discourse graphbank: A database of texts an-notated with coherence relations.
Linguistic DataConsortium.Z.M.
Zhou, Y. Xu, Z.Y.
Niu, M. Lan, J. Su, and C.L.Tan.
2010.
Predicting discourse connectives for im-plicit discourse relation recognition.
pages 1507?1514.
Association for Computational Linguistics.Proceedings of the 23rd International Conference onComputational Linguistics: Posters.485
