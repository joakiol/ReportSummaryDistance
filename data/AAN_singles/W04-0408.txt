Multiword expressions as dependency subgraphsRalph DebusmannProgramming Systems LabSaarland UniversityPostfach 15 11 5066041 Saarbru?cken, Germanyrade@ps.uni-sb.deAbstractWe propose to model multiword expres-sions as dependency subgraphs, and re-alize this idea in the grammar formal-ism of Extensible Dependency Gram-mar (XDG).
We extend XDG to lexi-calize dependency subgraphs, and showhow to compile them into simple lexicalentries, amenable to parsing and gener-ation with the existing XDG constraintsolver.1 IntroductionIn recent years, dependency grammar (DG)(Tesnie`re, 1959; Sgall et al, 1986; Mel?c?uk, 1988)has received resurgent interest.
Core conceptssuch as grammatical functions, valency and thehead-dependent asymmetry have now found theirway into most grammar formalisms, includingphrase structure-based ones such as HPSG, LFGand TAG.
This renewed interest in DG has alsogiven rise to new grammar formalisms based di-rectly on DG (Nasr, 1995; Heinecke et al, 1998;Bro?ker, 1999; Gerdes and Kahane, 2001; Kruijff,2001; Joshi and Rambow, 2003).A controversy among DG grammarians cir-cles around the question of assuming a 1:1-correspondence between words and nodes in thedependency graph.
This assumption simplifiesthe formalization of DGs substantially, and is of-ten required for parsing.
But as soon as se-mantics comes in, the picture changes.
Clearly,the 1:1-correspondence between words and nodesdoes not hold anymore for multiword expressions(MWEs), where one semantic unit, representedby a node in a semantically oriented dependencygraph, corresponds not to one, but to more thanone word.Most DGs interested in semantics have thusweakened the 1:1-assumption, starting withTesnie`re?s work.
Tesnie`re proposed the con-cept of nuclei to group together sets of nodes.FGD, on the other hand, allows for the dele-tion of solely syntactically motivated nodes in thetectogrammatical structure.
Similarly, in MTT,nodes present on the syntactic structures can bedeleted on the semantic structure.
This can happene.g.
through paraphrasing rules implemented bylexical functions (Mel?c?uk, 1996).
Unfortunately,these attempts to break the 1:1-correspondencehave not yet been formalized in a declarative way.Extensible Dependency Grammar (XDG) is anew grammar formalism based on TopologicalDependency Grammar (TDG) (Duchier and De-busmann, 2001).
From TDG, it inherits declara-tive word order constraints, the ability to distin-guish multiple dimensions of linguistic descrip-tion, and an axiomatization as a constraint sat-isfaction problem (Duchier, 2003) solvable usingconstraint programming (Apt, 2003).
One of thebenefits of this axiomatization is that the linear or-der of the words can be left underspecified, withthe effect that the constraint solver can be appliedfor both parsing and generation.XDG solving is efficient at least for the smaller-scale example grammars tested so far, but thesegood results hinge substantially on the assump-Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp.
56-63tion of a 1:1-correspondence between words andnodes.
As XDG has been created to cover not onlysyntax but also semantics, we have no choice butto weaken the 1:1-correspondence.In this paper, we outline a way to break out ofthe 1:1-straightjacket, without sacrificing the po-tential for efficient parsing and generation.
We in-troduce a new layer of lexical organization calledgroups above the basic XDG lexicon, allowing usto describe MWEs as tuples of dependency sub-graphs.
Groups can be compiled into simple lexi-cal entries, which can then be used by the alreadyexisting XDG solver for parsing and generation.With groups, we can omit nodes present in the syn-tactic dimensions in the semantic dimensions, andthus get away from the 1:1-correspondence.Groups are motivated by the continuity hypothe-sis of (Kay and Fillmore, 1999), assuming the con-tinuity of the lexicon and the construction.
Theycan also be regarded as a declarative formalizationof Mel?c?uk?s paraphrasing rules, or as a realizationof the extended domain of locality of TAG (Joshi,1987) in terms of dependency grammar, as alsoproposed in (Nasr, 1996) and (Joshi and Rambow,2003).The structure of this paper is as follows.
We in-troduce XDG in section 2.
In section 3, we intro-duce groups, the new layer of lexical organizationrequired for the modeling of MWEs, and in sec-tion 4, we show how to compile groups into sim-ple lexical entries amenable for parsing and gener-ation.
Section 5 concludes the paper and outlinesfuture work.2 XDGIn this section, we explain the intuitions behindXDG, before proceeding with its formalization,and a description of the XDG solver for parsingand generation.2.1 XDG intuitionsExtensible Dependency Grammar (XDG) is a newgrammar formalism generalizing Topological De-pendency Grammar (TDG) (Duchier and Debus-mann, 2001).
XDG characterizes linguistic struc-ture along arbitrary many dimensions of descrip-tion.
All dimensions correspond to labeled graphs,sharing the same set of nodes but having differentedges.The well-formedness conditions for XDG anal-yses are determined by principles.
Principles caneither be one-dimensional, applying to a singledimension only, or multi-dimensional, constrain-ing the relation of several dimensions.
Basic one-dimensional principles are treeness and valency.Multi-dimensional principles include climbing (asin (Duchier and Debusmann, 2001); one dimen-sion must be a flattening of another) and linking(e.g.
to specify how semantic arguments must berealized syntactically).The lexicon plays a central role in XDG.
Foreach node, it provides a set of possible lexical en-tries (feature structures) serving as the parametersfor the principles.
Because a lexical entry con-strains all dimensions simultaneously, it can alsohelp to synchronize the various dimensions, e.g.with respect to valency.
For instance, a lexical en-try could synchronize syntactic and semantic di-mensions by requiring a subject in the syntax, andan agent in the semantics.As an example, we show in (1) an analysisfor the sentence He dates her, along two dimen-sions of description, immediate dominance (ID)and predicate-argument structure (PA).
We displaythe ID part of the analysis on the left, and the PApart on the right:1.He dates herobjsubj.He dates herarg1arg2(1)The set of edge labels on the ID dimension in-cludes subj for subject and obj for object.
Onthe PA dimension, we have arg1 and arg2 stand-ing for the argument slots of semantic predicates.2The ID part of the analysis states that He is the sub-ject, and her the object of dates.
The PA part statesthat He is the first argument and her the second ar-gument of dates.1For lack of space, we omit the dimension of linear prece-dence (LP) from the presentation in this paper.
For this di-mension, we use the same theory as for TDG (Duchier andDebusmann, 2001).2We could also use some set of thematic roles for the PAedge labels, but since the assumption of thematic roles is verycontroversial, we decided to choose more neutral labels.The principles of the underlying grammar re-quire that the ID part of each analysis is a tree,and the PA part a directed acyclic graph (dag).34In addition, we employ the valency principle onboth dimensions, specifying the licensed incom-ing and outgoing edges of each node.
The onlyemployed multi-dimensional principle is the link-ing principle, specifying how semantic argumentsare realized syntactically.Figure 1 shows the lexicon of the underlyinggrammar.
Each lexical entry corresponds to botha word and a semantic literal.
inID and outIDparametrize the valency principle on the ID di-mension.
inID specifies the licensed incoming,and outID the licensed outgoing edges.
E.g.He licenses zero or one incoming edges labeledsubj, and no outgoing edges.
inPA and outPAparametrize the valency principle on the PA dimen-sion.
E.g.
dates licenses no incoming edges, andrequires precisely one outgoing edge labeled arg1and one labeled arg2.
link parametrizes the multi-dimensional linking principle.
E.g.
dates syntac-tically realizes its first argument by a subject, thesecond argument by an object.Observe that all the principles are satisfied in(1), and hence the analysis is well-formed.
Alsonotice that we can use the same grammar and lex-icon for both parsing (from words) and generation(from semantic literals).2.2 XDG formalizationFormally, an XDG grammar is built up of dimen-sions, a lexicon and principle, and characterizes aset of well-formed analyses.A dimension is a tuple D = (Lab, Fea, Val, Pri)of a set Lab of edge labels, a set Fea of fea-tures, a set Val of feature values, and a set of one-dimensional principles Pri.
A lexicon for the di-mension D is a set Lex ?
Fea ?
Val of total fea-ture assignments called lexical entries.
An analy-sis on dimension D is a triple (V,E, F ) of a set Vof nodes, a set E ?
V ?
V ?
Lab of directed la-beled edges, and an assignment F : V ?
(Fea ?Val) of lexical entries to nodes.
V and E form a3In the following, we will call the ID part of an analysisID tree, and the PA part PA dag.4The PA structure is a dag and not a tree because wewant it to reflect the re-entrancy e.g.
in control constructions,where the same subject is shared by more than one node.graph.
We write AnaD for the set of all possibleanalyses on dimension D. The principles charac-terize subsets of AnaD.
We assume that the ele-ments of Pri are finite representations of such sub-sets.An XDG grammar ((Labi, Feai, Vali, Prii)ni=1,Pri, Lex) consists of n dimensions, multi-dimensional principles Pri, and a lexicon Lex.An XDG analysis (V,Ei, Fi)ni=1 is an element ofAna = Ana1 ?
?
?
?
?
Anan where all dimensionsshare the same set of nodes V .
We call a dimen-sion of a grammar grammar dimension.Multi-dimensional principles specify subsets ofAna, i.e.
of tuples of analyses for the individual di-mensions.
The lexicon Lex ?
Lex1 ?
?
?
?
?
Lexnconstrains all dimensions at once, thereby syn-chronizing them.
An XDG analysis is licensed byLex iff (F1(v), .
.
.
, Fn(v)) ?
Lex for every nodev ?
V .In order to compute analyses for a given input,we employ a set of input constraints (Inp), whichagain specify a subset of Ana.
XDG solving thenamounts to finding elements of Ana that are li-censed by Lex, and consistent with Inp and Pri.The input constraints e.g.
determine whether XDGsolving is to be used for parsing or generation.
Forparsing, they specify a sequence of words, and forgeneration, a multiset of semantic literals.2.3 XDG solverXDG solving has a natural reading as a constraintsatisfaction problem (CSP) on finite sets of inte-gers, where well-formed analyses correspond tothe solutions of the CSP (Duchier, 2003).
Wehave implemented an XDG solver (Debusmann,2003) using the Mozart-Oz programming system(Mozart Consortium, 2004).XDG solving operates on all dimensions con-currently.
This means that the solver can infer in-formation about one dimension from informationon another, if there is either a multi-dimensionalprinciple linking the two dimensions, or by thesynchronization induced by the lexical entries.
Forinstance, not only can syntactic information trig-ger inferences in syntax, but also vice versa.Because XDG allows us to write grammarswith completely free word order, XDG solving isan NP-complete problem (Koller and Striegnitz,word literal inID outID inPA outPA linkHe he?
{subj?}
{} {arg1?, arg2?}
{} {}dates date?
{} {subj!, obj!}
{} {arg1!, arg2!}
{arg1 7?
{subj}, arg2 7?
{obj}}her she?
{obj?}
{} {arg1?, arg2?}
{} {}Figure 1: Lexicon for the sentence He dates her.2002).
This means that the worst-case complex-ity of the solver is exponential.
The average-casecomplexity of many smaller-scale grammars thatwe have experimented with seems polynomial, butit remains to be seen whether we can scale this upto large-scale grammars.3 GroupsIn this section, we consider MWE paraphrases andpropose to model them as tuples of dependencysubgraphs called groups.
We start with an exam-ple: Consider (3), a paraphrase of (2):He dates her.
(2)He takes her out.
(3)We display the XDG analysis of (3) in (4).Again, the ID tree is on the left, and the PA dagon the right:5.He takes her outobj partsubj.He takes her outarg1arg2(4)This example demonstrates that we cannot sim-ply treat MWEs as contiguous word strings andinclude those in the lexicon, since the MWE takesout is interrupted by the object her in (3).
Instead,we choose to implement the continuity hypothe-sis of (Kay and Fillmore, 1999) in terms of DG,and model MWEs as dependency subgraphs.
Werealize this idea by a new layer of lexical orga-nization called groups.
A group is tuple of de-pendency subgraphs covering one or more nodes,where each component of the tuple corresponds toa grammar dimension.
We call a group compo-nent group dimension.
We display the group cor-responding to dates in (5), and the one correspond-5In the ID tree, part stands for a particle.ing to takes out in (6)..datesobjsubj.datesarg1 arg2(5).takes outobj partsubj.takes outarg1 arg2(6)Groups can leave out nodes present in the IDdimension on the PA dimension.
E.g.
in (6), thenode corresponding to the particle out is presenton the ID dimension, but left out on the PA dimen-sion.
In this way, groups help us to weaken the1:1-correspondence between words and nodes.We can make use of groups also to handle morecomplicated MWE paraphrases.
Consider (8) be-low, a support verb construction paraphrasing (7):He argues with her.
(7)He has an argument with her.
(8)In (8), has is only a support verb; the semantichead of the construction is the noun argument.
Wedisplay the ID tree and PA dag of (7) in (9).
The IDtree of (8) is in (10), and the PA dag in (11):6.He argues with herpobjsubjpcomp.He argues with herarg1 arg2(9)6In the ID trees, pobj stands for a prepositional object,pcomp for the complement of a preposition, pmod for aprepositional modifier, and det for a determiner..He has an argument with herobjsubjdetpmodpcomp(10).He has an argument with herarg1 arg2(11)In (9), the node corresponding to with is deletedin the PA dag.
In (11), the support verb construc-tion leads to the deletion of three nodes (corre-sponding to resp.
has, an and with).
These dele-tions are reflected in the corresponding groups.The group corresponding to argues with is dis-played in (12), and the group corresponding to hasan argument with in (13) (ID) and (14) (PA):.argues withpobjsubjpcomp.argues witharg1 arg2 (12).has an argument withobjsubjdetpmodpcomp(13).has an argument witharg1 arg2(14)Groups can capture difficult constructions suchas the support verb construction above in an el-egant and transparent way, without having to re-sort to complicated machinery.
The key aspectof groups is their multi-dimensionality, describingtuples of dependency subgraphs spanning over ashared set of nodes.
This sharing enables groupsto express interdependencies between the differ-ent dimensions.
For instance in (13) and (14), thenoun argument, the object of the support verb hasin the ID dimension, is the semantic head in the PAdimension.4 CompilationIn this section, we show how to compile groupsinto simple lexical entries.
The benefit of this isthat we can retain XDG in its entirety, i.e.
we canretain its formalization and its axiomatization asa constraint satisfaction problem.
This means wecan also retain the implementation of the XDGsolver, and continue to use it for parsing and gen-eration.4.1 Node deletionThe 1:1-correspondence between words and nodesis a key assumption of XDG.
It requires that oneach dimension, each word must correspond toprecisely one node in the dependency graph.
Thegroups shown in the previous section clearly vio-late this assumption as they allow nodes present onthe ID dimension to be omitted on the PA dimen-sion.The first step of the compilation aims to accom-modate this deletion of nodes.
To this end, weassume for each analysis an additional root node,e.g.
corresponding to the full stop at the end of thesentence.
Each root in an XDG dependency graphbecomes a daughter of this additional root node,the edge labeled root.
The trick for deleting nodesis now the following: Each deleted node also be-comes a daughter of the additional root node, butwith a special node label indicating that this nodeis regarded to be deleted (del).As an example, here is the PA dag for example(3) including the additional root node:.He takes her out .arg1arg2delroot(15)4.2 Dependency subgraphsThe second step of the compilation is to compilethe dependency subgraphs into lexical entries forindividual words.
To this end, we make use of thevalency principle.
For each edge from v to v ?
la-beled l within a group, we require that v has anoutgoing edge labeled l, and that v?
licenses an in-coming edge labeled l. I.e.
we include l!
in the outspecification of the lexical entry for v, and l?
inthe in specification of the lexical entry for v ?.4.3 Group coherenceThe final step of the compilation is about ensur-ing group coherence, i.e.
to ensure that the innernodes of a group dimension (neither the root northe leaves) stay within this group in the depen-dency analysis.
In other words, group coherencemake sure that the inner nodes of a group dimen-sion cannot become daughters of nodes of a dif-ferent group.
In our support verb example, groupcoherence ensures that e.g.
that the determiner ancannot become the determiner of a noun of an-other group.
We realize this idea through a newXDG principle called group coherence principle.This principle must hold on all dimensions of thegrammar.Given a set of group identifiers Group , the prin-ciple assumes two new features: group : Group ,and outgroups : Lab ?
2Group .
For each node v,group(v) denotes the group identifier of the groupof v. For each edge within a group from v tov?
labeled l, i.e.
for each edge to an inner node,outgroups(v)(l) denotes the singleton set contain-ing the group of both v and v?.
For each edgefrom v labeled l which goes outside of a group,outgroups(v)(l) = Group , i.e.
the edge can endin any other group.
We formalize the group coher-ence principle as follows, where v l?
v?
denotesan edge from v to v?
labeled l:v l?
v?
?
group(v?)
?
outgroups(v)(l)(16)4.4 ExamplesFor illustration, we display the lexical entries oftwo compiled groups: The group argues with(with identifier g1), and the group has an argu-ment with (with identifier g2), resp.
in Figure 2 andFigure 3.
We omit the specification of outgroupsfor the PA dimension for lack of space, and since itis not relevant for the example: In all groups, thereare no edges which stay within a group in the PAdimension.4.5 Parsing and generationWe can use the same group lexicon for parsing andgeneration, but we have to slightly adapt the com-pilation for the generation case.For parsing, we can use the XDG parser as be-fore, without any changes.For generation, we assume a set Sem of seman-tic literals, multisets of which must be verbalized.To do this, we assume a function groups : Sem ?2Group , mapping each semantic literal to a set ofgroups which verbalize it.Now before using the XDG solver for genera-tion, we have to make sure for each semantic lit-eral s to be verbalized that the groups which canpotentially verbalize it have the same number ofnodes.
For this, we calculate the maximum num-ber n of syntactic nodes for the groups assignedto s, and fill up the groups having less syntacticnodes with deleted nodes.
Then for XDG solving,we introduce precisely n nodes for literal s. Usinggroups for generation thus comes at the expense ofhaving to introduce additional nodes.7As an example, consider we want to verbal-ize the semantic literal argue.
groups(argue) ={g1, g2}, i.e.
either groups g1 or g2 can verbalizeit.
The maximum number n of syntactic nodes forthe two groups is 4 for g2 (has an argument with).Hence, we have to fill up the groups having lesssyntactic nodes with deleted nodes.
In this case,we have to add two deleted nodes to the group g1(argue with) to get to four nodes.
The resultinglexical entries encoding g1 are displayed in Fig-ure 4.
The lexical entries for g2 stay the same asin Figure 3.After this step is done, we can use the existingXDG solver to generate from the semantic literalsprecisely the two possible verbalizations (7) and(8), as intended.7We should emphasize that n is not at all unrestricted.For each semantic literal s to be verbalized, we have to intro-duce only as many nodes as are contained in the largest groupwhich can verbalize s.word literal group outgroupsID inID outID inPA outPA linkargues argue?
g1 {pobj 7?
{g1}} {root?}
{subj!, pobj!}
{root?}
{arg1, arg2} { arg1 7?
{subj}arg2 7?
{pcomp}}with argue?
g1 {} {pobj?}
{pcomp!}
{del?}
{} {}Figure 2: Lexical entries encoding the group for argues withword literal group outgroupsID inID outID inPA outPA linkhas argue?
g2 {obj 7?
{g2}} {root?}
{subj!, obj!}
{del?}
{} {}an argue?
g2 {} {det?}
{} {del?}
{} {}argument argue?
g2 { det 7?
{g2}pmod 7?
{g2}} {obj?}
{det!, pmod!}
{root?}
{arg1!, arg2!}
{arg1 7?
{subj}arg2 7?
{pcomp}}with argue?
g2 {} {pmod?}
{pcomp!}
{del?}
{} {}Figure 3: Lexical entries encoding the group for has an argument with5 ConclusionWe extended the XDG grammar formalism witha means to break out of the straightjacket of the1:1-correspondence between words and nodes.
Tothis end, we proposed the new notion of groups,allowing to enrich the XDG lexicon with tuplesof dependency subgraphs.
We illustrated how totackle complicated MWEs such as support verbconstructions with this new idea, and how to com-pile groups into simple lexical entries.We see two main benefits of our approach.
Thefirst is that we can retain the XDG formalization,and also its axiomatization as a constraint satisfac-tion problem, in its entirety.
Thus, we can simplycontinue to use the existing XDG solver for pars-ing and generation.
The second benefit is that wecan use the same group lexicon for both parsingand generation, the only difference being that forgeneration, we have to slightly adapt the compila-tion into lexical entries.There are many issues which have remained un-touched in this paper.
For one, we did not talkabout our treatment of word order in this paper forlack of space.
Word order is among the best re-searched issues in the context of XDG.
For a thor-ough discussion about word order in XDG, we re-fer to (Duchier and Debusmann, 2001) and (De-busmann, 2001).Another issue is that of the relation betweengroups and the meta-grammatical functionalityof the XDG lexicon, offering lexical inheritance,templates and also disjunction in the sense ofcrossings (Candito, 1996) to lexically state lin-guistic generalizations.
How well groups can beintegrated with this meta-grammatical functional-ity is an open question.XDG research has so far mainly been focusedon parsing, only to a very small extent on genera-tion, and to no extent at all on Machine Translation(MT).
There is still a lot to do on both of the lat-ter topics, and even for parsing, we are only at thebeginning.
Although with our smaller-scale exam-ple grammars, parsing and generation takes poly-nomial time, we have yet to find out how we canscale this up to large-scale grammars.
We havestarted importing and inducing large-scale gram-mars from existing resources, but can so far onlyspeculate about if and how we can parse them ef-ficiently.
In a related line of research, we are alsoworking on the incorporation of statistical infor-mation (Dienes et al, 2003) to help us to guide thesearch for solutions.
This could improve perfor-mance because we would only have to search fora few good analyses instead of enumerating hun-dreds of them.AcknowledgementsThe idea for groups and this paper was triggeredby three events in fall 2003.
The first was a work-shop where Peter Dienes, Stefan Thater and meworked out how to do generation with XDG.
Thesecond was a presentation on the same workshopby Aravind Joshi and Owen Rambow of their en-coding of DG in TAG, and the third was a talkby Charles Fillmore titled Multiword Expressions:An Extremist Approach.
I?d like to thank all ofthem.
And I?d like to thank all the others involvedwith XDG, in alphabetical order: Ondrej Bojar,Denys Duchier, Alexander Koller, Geert-Jan Krui-word literal group outgroupsID inID outID inPA outPA linkargues argue?
g1 {pobj 7?
{g1}} {root?}
{subj!, pobj!}
{root?}
{arg1, arg2} { arg1 7?
{subj}arg2 7?
{pcomp}}with argue?
g1 {} {pobj?}
{pcomp!}
{del?}
{} {} argue?
g1 {} {del?}
{} {del?}
{} {} argue?
g1 {} {del?}
{} {del?}
{} {}Figure 4: Lexical entries for generation, encoding the group for argues withjff, Vladislav Kubon, Marco Kuhlmann, JoachimNiehren, Martin Platek and Gert Smolka for theirsupport and many helpful discussions.ReferencesKrzysztof R. Apt.
2003.
Principles of Constraint Pro-gramming.
Cambridge University Press.Norbert Bro?ker.
1999.
Eine Dependenzgrammatikzur Kopplung heterogener Wissensquellen.
Lin-guistische Arbeiten 405.
Max Niemeyer Verlag,Tu?bingen/GER.Marie-H e`le?ne Candito.
1996.
A principle-based hier-archical representation of LTAG.
In Proceedings ofCOLING 1996, Kopenhagen/DEN.Ralph Debusmann.
2001.
A declarative grammar for-malism for dependency grammar.
Master?s thesis,University of Saarland.Ralph Debusmann.
2003.
A parser system for extensi-ble dependency grammar.
In Denys Duchier, editor,Prospects and Advances in the Syntax/Semantics In-terface, pages 103?106.
LORIA, Nancy/FRA.Peter Dienes, Alexander Koller, and Marco Kuhlmann.2003.
Statistical A* Dependency Parsing.
InProspects and Advances in the Syntax/Semantics In-terface, Nancy/FRA.Denys Duchier and Ralph Debusmann.
2001.
Topo-logical dependency trees: A constraint-based ac-count of linear precedence.
In Proceedings of ACL2001, Toulouse/FRA.Denys Duchier.
2003.
Configuration of labeled treesunder lexicalized constraints and principles.
Re-search on Language and Computation, 1(3?4):307?336.Kim Gerdes and Sylvain Kahane.
2001.
Word orderin german: A formal dependency grammar using atopological hierarchy.
In Proceedings of ACL 2001,Toulouse/FRA.Johannes Heinecke, Ju?rgen Kunze, Wolfgang Menzel,and Ingo Schro?der.
1998.
Eliminative parsing withgraded constraints.
In Proceedings of COLING/ACL1998, pages 526?530, Montreal/CAN.Aravind Joshi and Owen Rambow.
2003.
A formal-ism for dependency grammar based on tree adjoin-ing grammar.
In Proceedings of MTT 2003, pages207?216, Paris/FRA.Aravind K. Joshi.
1987.
An introduction to tree-adjoining grammars.
In Alexis Manaster-Ramer, ed-itor, Mathematics of Language, pages 87?115.
JohnBenjamins, Amsterdam/NL.Paul Kay and Charles J. Fillmore.
1999.
Grammati-cal constructions and linguistic generalizations: thewhat?s x doing y?
construction.
Language, 75:1?33.Alexander Koller and Kristina Striegnitz.
2002.
Gen-eration as dependency parsing.
In Proceedings ofACL 2002, Philadelphia/USA.Geert-Jan M. Kruijff.
2001.
A Categorial-Modal Ar-chitecture of Informativity.
Ph.D. thesis, CharlesUniversity, Prague/CZ.Igor Mel?c?uk.
1988.
Dependency Syntax: Theoryand Practice.
State Univ.
Press of New York, Al-bany/USA.Igor Mel?c?uk.
1996.
Lexical functions: a tool for thedescription of lexical relations in a lexicon.
In LeoWanner, editor, Lexical Functions in Lexicographyand Natural Language Processing.
John Benjamins.Mozart Consortium.
2004.
The Mozart-Oz website.http://www.mozart-oz.org/.Alexis Nasr.
1995.
A formalism and a parser for lexi-calised dependency grammars.
In 4th InternationalWorkshop on Parsing Technologies, pages 186?195,Prague/CZ.Alexis Nasr.
1996.
Un syste`me de reformulation au-tomatique de phrases fonde?
sur la The?orie Sens-Texte: application aux langues contro?le?es.
Ph.D.thesis, Universite?
Paris 7.Petr Sgall, Eva Hajicova, and Jarmila Panevova.
1986.The Meaning of the Sentence in its Semantic andPragmatic Aspects.
D. Reidel, Dordrecht/NL.Lucien Tesni e`re.
1959.
El?ements de Syntaxe Struc-turale.
Klincksiek, Paris/FRA.
