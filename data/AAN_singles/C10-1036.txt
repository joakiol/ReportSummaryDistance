Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313?321,Beijing, August 2010Mixture Model-based Minimum Bayes Risk Decoding using MultipleMachine Translation SystemsNan Duan1School of Computer Science and TechnologyTianjin Universityv-naduan@microsoft.comMu Li, Dongdong Zhang, Ming ZhouMicrosoft Research Asiamuli@microsoft.comdozhang@microsoft.commingzhou@microsoft.comAbstractWe present Mixture Model-based Mini-mum Bayes Risk (MMMBR) decoding,an approach that makes use of multipleSMT systems to improve translation ac-curacy.
Unlike existing MBR decodingmethods defined on the basis of singleSMT systems, an MMMBR decoder re-ranks translation outputs in the combinedsearch space of multiple systems usingthe MBR decision rule and a mixture dis-tribution of component SMT models fortranslation hypotheses.
MMMBR decod-ing is a general method that is indepen-dent of specific SMT models and can beapplied to various commonly used searchspaces.
Experimental results on the NISTChinese-to-English MT evaluation tasksshow that our approach brings significantimprovements to single system-basedMBR decoding and outperforms a state-of-the-art system combination method.
11 IntroductionMinimum Bayes Risk (MBR) decoding is be-coming more and more popular in recent Statis-tical Machine Translation (SMT) research.
Thisapproach requires a second-pass decoding pro-cedure to re-rank translation hypotheses by riskscores computed based on model?s distribution.Kumar and Byrne (2004) first introducedMBR decoding to SMT field and developed it onthe N-best list translations.
Their work hasshown that MBR decoding performs better thanMaximum a Posteriori (MAP) decoding for dif-ferent evaluation criteria.
After that, many dedi-1 This work has been done while the author was visitingMicrosoft Research Asia.cated efforts have been made to improve the per-formances of SMT systems by utilizing MBR-inspired methods.
Tromble et al (2008) pro-posed a linear approximation to BLEU score(log-BLEU) as a new loss function in MBR de-coding and extended it from N-best lists to lat-tices, and Kumar et al (2009) presented moreefficient algorithms for MBR decoding on bothlattices and hypergraphs to alleviate the highcomputational cost problem in Tromble et al?swork.
DeNero et al (2009) proposed a fast con-sensus decoding algorithm for MBR for bothlinear and non-linear similarity measures.All work mentioned above share a commonsetting: an MBR decoder is built based on oneand only one MAP decoder.
On the other hand,recent research has shown that substantial im-provements can be achieved by utilizing consen-sus statistics over multiple SMT systems (Rostiet al, 2007; Li et al, 2009a; Li et al, 2009b;Liu et al, 2009).
It could be desirable to adaptMBR decoding to multiple SMT systems as well.In this paper, we present Mixture Model-based Minimum Bayes Risk (MMMBR) decoding,an approach that makes use of multiple SMTsystems to improve translation performance.
Inthis work, we can take advantage of a largersearch space for hypothesis selection, and em-ploy an improved probability distribution overtranslation hypotheses based on mixture model-ing, which linearly combines distributions ofmultiple component systems for Bayes riskcomputation.
The key contribution of this paperis the usage of mixture modeling in MBR, whichallows multiple SMT models to be involved inand makes the computation of n-gram consensusstatistics to be more accurate.
Evaluation resultshave shown that our approach not only bringssignificant improvements to single system-basedMBR decoding but also outperforms a state-of-the-art word-level system combination method.313The rest of the paper is organized as follows:In Section 2, we first review traditional MBRdecoding method and summarize various searchspaces that can be utilized by an MBR decoder.Then, we describe how a mixture model can beused to combine distributions of multiple SMTsystems for Bayes risk computation.
Lastly, wepresent detailed MMMBR decoding model onmultiple systems and make comparison withsingle system-based MBR decoding methods.Section 3 describes how to optimize differenttypes of parameters.
Experimental results will beshown in Section 4.
Section 5 discusses somerelated work and Section 6 concludes the paper.2 Mixture Model-based MBR Decoding2.1 Minimum Bayes Risk DecodingGiven a source sentence  , MBR decoding aimsto find the translation with the least expectedloss under a probability distribution.
The objec-tive of an MBR decoder can be written as:(1)where   denotes a search space for hypothesisselection;    denotes an evidence space forBayes risk computation;      denotes a functionthat measures the loss between    and  ;      isthe underlying distribution based on  .Some of existing work on MBR decoding fo-cused on exploring larger spaces for bothand   , e.g.
from N-best lists to lattices orhypergraphs (Tromble et al, 2008; Kumar et al,2009).
Various loss functions have also beeninvestigated by using different evaluation crite-ria for similarity computation, e.g.
Word ErrorRate, Position-independent Word Error Rate,BLEU and log-BLEU (Kumar and Byrne, 2004;Tromble et al, 2008).
But less attention hasbeen paid to distribution     .
Currently, manySMT systems based on different paradigms canyield similar performances but are good at mod-eling different inputs in the translation task(Koehn et al, 2004a; Och et al, 2004; Chiang,2007; Mi et al, 2008; Huang, 2008).
We expectto integrate the advantages of different SMTmodels into MBR decoding for further im-provements.
In particular, we make in-depth in-vestigation into MBR decoding concentrating onthe translation distribution      by leveraging amixture model based on multiple SMT systems.2.2 Summary of Translation Search SpacesThere are three major forms of search spacesthat can be obtained from an MAP decoder as abyproduct, depending on the design of the de-coder: N-best lists, lattices and hypergraphs.An N-best list contains the   most probabletranslation hypotheses produced by a decoder.
Itonly presents a very small portion of the entiresearch space of an SMT model.A hypergraph is a weighted acyclic graphwhich compactly encodes an exponential num-ber of translation hypotheses.
It allows us torepresent both phrase-based and syntax-basedsystems in a unified framework.
Formally, ahypergraph  is a pair      , where   is aset of hypernodes and   is a set of hyperedges.Each hypernode     corresponds to transla-tion hypotheses with identical decoding states,which usually include the span       of thewords being translated, the grammar symbolfor that span and the left and right boundarywords of hypotheses for computing languagemodel (LM) scores.
Each hyperedge     cor-responds to a translation rule and connects ahead node      and a set of tail nodes     .
Thenumber of tail nodes        is called the arity ofthe hyperedge   and the arity of a hypergraph isthe maximum arity of its hyperedges.
If the arityof a hyperedge   is zero,      is then called asource node.
Each hypergraph has a unique rootnode and each path in a hypergraph induces atranslation hypothesis.
A lattice (Ueffing et al,2002) can be viewed as a special hypergraph, inwhich the maximum arity is one.2.3 Mixture Model for SMTWe first describe how to construct a general dis-tribution for translation hypotheses over multipleSMT systems using mixture modeling for usagein MBR decoding.Mixture modeling is a technique that has beenapplied to many statistical tasks successfully.For the SMT task in particular, given   SMTsystems with their corresponding model distribu-tions, a mixture model is defined as a probabilitydistribution over the combined search space ofall component systems and computed as aweighted sum of component model distributions:314(2)In Equation 2,            are system weightswhich hold following constraints:and,            is theth distri-bution estimated on the search space   basedon the log-linear formulation:where         is the score function of thethsystem for translation  ,          is a scalingfactor that determines the flatness of the distri-bution    sharp (    ) or smooth (    ).Due to the inherent differences in SMT mod-els, translation hypotheses have different distri-butions in different systems.
A mixture modelcan effectively combine multiple distributionswith tunable system weights.
The distribution ofa single model used in traditional MBR can beseen as a special mixture model, where   is one.2.4 Mixture Model for SMTLet              denote   machine translationsystems,   denotes the search space producedby system    in MAP decoding procedure.
AnMMMBR decoder aims to seek a translationfrom the combined search space       thatmaximizes the expected gain score based on amixture model         .
We write the objec-tive function of MMMBR decoding as:(3)For the gain function     , we follow Trom-ble et al (2008) to use log-BLEU, which isscored by the hypothesis length and a linearfunction of n-gram matches as:In this definition,   is a reference translation,is the length of hypothesis   ,   is an n-gram presented in   ,is the number oftimes that  occurs in   , and       is an indi-cator function which equals to 1 when   occursin   and 0 otherwise.
are modelparameters, where   is the maximum order ofthe n-grams involved.For the mixture model     , we replace it byEquation 2 and rewrite the total gain score forhypothesis    in Equation 3:(4)In Equation 4, the total gain score on the com-bined search space   can be further decom-posed into each local search space    with aspecified distribution           .
This is a niceproperty and it allows us to compute the totalgain score as a weighted sum of local gainscores on different search spaces.
We expand thelocal gain score for    computed on search spacewith            using log-BLEU as:We make two approximations for the situationswhen    : the first isand the second isIn fact, due to the differences in ge-nerative capabilities of SMT models, trainingdata selection and various pruning techniquesused, search spaces of different systems are al-ways not identical in practice.
For the conveni-ence of formal analysis, we treat allas ideal distributions with assumptions that allsystems work in similar settings, and translationcandidates are shared by all systems.The method for computing n-gram posteriorprobability          in Equation 5 depends ondifferent types of search space  :?
When   is an N-best list, it can be com-puted immediately by enumerating all trans-lation candidates in the N-best list:315?
When   is a hypergraph (or a lattice) thatencodes exponential number of hypotheses,it is often impractical to compute this proba-bility directly.
In this paper, we use the al-gorithm presented in Kumar et al (2009)which is described in Algorithm 12:counts the edge   with n-gramthat has the highest edge posterior proba-bility relative to predecessors in the entiregraph  , and          is the edge posteriorprobability that can be efficiently computedwith standard inside and outside probabili-ties      and      as:where     is the weight of hyperedge   in,      is the normalization factor thatequals to the inside probability of the rootnode in  .Algorithm 1: Compute n-gram posterior proba-bilities on hypergraph   (Kumar et al, 2009)1: sort hypernodes topologically2: compute inside/outside probabilities      andfor each hypernode3: compute edge posterior probability          foreach hyperedge4: for each hyperedge      do5:       merge n-grams on      and keep the highestprobability when n-grams are duplicated6:      apply the rule of edge   to n-grams on      andpropagate     gram prefixes/suffixes to7:          for each n-gram   introduced by   do8:      if                      then9:10:           else11:12:   end if13:  end for14: end for15: return n-gram posterior probability set2 We omit the similar algorithm for lattices because of theirhomogenous structures comparing to hypergraphs as wediscussed in Section 2.2.Thus, the total gain score for hypothesis    oncan be further expanded as:where                   is a mixture n-gram posterior probability.
The most importantfact derived from Equation 6 is that, the mixtureof different distributions can be simplified to theweighted sum of n-gram posterior probabilitieson different search spaces.We now derive the decision rule of MMMBRdecoding based on Equation 6 below:(7)We also notice that MAP decoding and MBRdecoding are two different ways of estimatingthe probability        and each of them hasadvantages and disadvantages.
It is desirable tointerpolate them together when choosing the fi-nal translation outputs.
So we include each sys-tem?s MAP decoding cost as an additional fea-ture further and modify Equation 7 to:?
(8)whereis the model cost as-signed by the MAP decoder    for hypothesis.Because the costs of MAP decoding on differentSMT models are not directly comparable, weutilize the MERT algorithm to assign an appro-priate weight    for each component system.Compared to single system-based MBR de-coding, which obeys the decision rule below:316MMMBR decoding has a similar objective func-tion (Equation 8).
The key difference is that, inMMMBR decoding, n-gram posterior probabili-ty      is computed as              based onan ensemble of search spaces; meanwhile, insingle system-based MBR decoding, this quanti-ty is computed locally on single search space  .The procedure of MMMBR decoding on mul-tiple SMT systems is described in Algorithm 2.Algorithm 2: MMMBR decoding on multipleSMT systems1: for each component system    do2:     run MAP decoding and generate the correspond-ing search space3:  compute the n-gram posterior probability setfor   based on Algorithm 14: end for5 compute the mixture n-gram posterior  probabilityfor each  :6: for each unique n-gram   appeared in     do7:      for each search space   do89:         end for10: end for11: for each hyperedge   in     do12:     assign      to the edge   for all   contained in13: end for14: return the best path according to Equation 83 A Two-Pass Parameter OptimizationIn Equation 8, there are two types of parameters:parameters introduced by the gain functionand the model cost        , and system weightsintroduced by the mixture model     .
BecauseEquation 8 is not a linear function when all pa-rameters are taken into account, MERT algo-rithm (Och, 2003) cannot be directly applied tooptimize them at the same time.
Our solution isto employ a two-pass training strategy, in whichwe optimize parameters for MBR first and thensystem weights for the mixture model.3.1 Parameter Optimization for MBRThe inputs of an MMMBR decoder can be acombination of translation search spaces witharbitrary structures.
For the sake of a general andconvenience solution for optimization, we utilizethe simplest N-best lists with proper sizes asapproximations to arbitrary search spaces tooptimize MBR parameters using MERT in thefirst-pass training.
System weights can be setempirically based on different performances, orequally without any bias.
Note that although wetune MBR parameters on N-best lists, n-gramposterior probabilities used for Bayes riskcomputation could still be estimated onhypergraphs for non N-best-based search spaces.3.2 Parameter Optimization for MixtureModelAfter MBR parameters optimized, we begin totune system weights for the mixture model in thesecond-pass training.
We rewrite Equation 8 as:?For each   , the aggregated score surroundedwith braces can be seen as its feature value.
Eq-uation 9 now turns to be a linear function for allweights and can be optimized by the MERT.4 Experiments4.1 Data and MetricWe conduct experiments on the NIST Chinese-to-English machine translation tasks.
We use thenewswire portion of the NIST 2006 test set(MT06-nw) as the development set for parameteroptimization, and report results on the NIST2008 test set (MT08).
Translation performancesare measured in terms of case-insensitive BLEUscores.
Statistical significance is computed usingthe bootstrap re-sampling method proposed byKoehn (2004b).
Table 1 gives data statistics.Data Set #Sentence #WordMT06-nw (dev) 616 17,316MT08 (test) 1,357 31,600Table 1.
Statistics on dev and test data setsAll bilingual corpora available for the NIST2008 constrained track of Chinese-to-Englishmachine translation task are used as training data,which contain 5.1M sentence pairs, 128M Chi-nese words and 147M English words after pre-processing.
Word alignments are performed byGIZA++ with an intersect-diag-grow refinement.317A 5-gram language model is trained on theEnglish side of all bilingual data plus the Xinhuaportion of LDC English Gigaword Version 3.0.4.2 System DescriptionWe use two baseline systems.
The first one(SYS1) is a hierarchical phrase-based system(Chiang, 2007) based on Synchronous ContextFree Grammar (SCFG), and the second one(SYS2) is a phrasal system (Xiong et al, 2006)based on Bracketing Transduction Grammar(Wu, 1997) with a lexicalized reordering com-ponent based on maximum entropy model.Phrasal rules shared by both systems are ex-tracted on all bilingual data, while hierarchicalrules for SYS1 only are extracted on a selecteddata set, including LDC2003E07, LDC2003E14,LDC2005T06, LDC2005T10, LDC2005E83,LDC2006E26, LDC2006E34, LDC2006E85 andLDC2006E92, which contain about 498,000 sen-tence pairs.
Translation hypergraphs are generat-ed by each baseline system during the MAP de-coding phase, and 1000-best lists used forMERT algorithm are extracted from hyper-graphs by the k-best parsing algorithm (Huangand Chiang, 2005).
We tune scaling factor tooptimize the performance of HyperGraph-basedMBR decoding (HGMBR) on MT06-nw foreach system (0.5 for SYS1 and 0.01 for SYS2).4.3 MMMBR Results on Multiple SystemsWe first present the overall results of MMMBRdecoding on two baseline systems.To compare with single system-based MBRmethods, we re-implement N-best MBR, whichperforms MBR decoding on 1000-best lists withthe fast consensus decoding algorithm (DeNeroet al, 2009), and HGMBR, which performsMBR decoding on a hypergraph (Kumar et al,2009).
Both methods use log-BLEU as the lossfunction.
We also compare our method withIHMM Word-Comb, a state-of-the-art word-levelsystem combination approach based on incre-mental HMM alignment proposed by Li et al(2009b).
We report results of MMMBR decod-ing on both N-best lists (N-best MMMBR) andhypergraphs (Hypergraph MMMBR) of twobaseline systems.
As MBR decoding can be usedfor any SMT system, we also evaluate MBR-IHMM Word-Comb, which uses N-best listsgenerated by HGMBR on each baseline systems.The default beam size is set to 50 for MAP de-coding and hypergraph generation.
The settingof N-best candidates used for (MBR-) IHMMWord-Comb is the same as the one used in Li etal.
(2009b).
The maximum order of n-grams in-volved in MBR model is set to 4.
Table 2 showsthe evaluation results.MT06-nw MT08SYS1 SYS2 SYS1 SYS2MAP 38.1 37.1 28.5 28.0N-best MBR 38.3 37.4 29.0 28.1HGMBR 38.3 37.5 29.1 28.3IHMMWord-Comb39.1 29.3MBR-IHMMWord-Comb39.3 29.7N-bestMMMBR39.0* 29.4*HypergraphMMMBR39.4*+ 29.9*+Table 2.
MMMBR decoding on multiple sys-tems (*: significantly better than HGMBR with; +: significantly better than IHMMWord-Comb with       )From Table 2 we can see that, compared toMAP decoding, N-best MBR and HGMBR onlyimprove the performance in a relative smallrange (+0.1~+0.6 BLEU), while MMMBR de-coding on multiple systems can yield significantimprovements on both dev set (+0.9 BLEU onN-best MMMBR and +1.3 BLEU on Hyper-graph MMMBR) and test set (+0.9 BLEU on N-best MMMBR and +1.4 BLEU on HypergraphMMMBR); compared to IHMM Word-Comb,N-best MMMBR can achieve comparable resultson both dev and test sets, while HypergraphsMMMBR can achieve even better results (+0.3BLEU on dev and +0.6 BLEU on test); com-pared to MBR-IHMM Word-Comb, HypergraphMMMBR can also obtain comparable resultswith tiny improvements (+0.1 BLEU on dev and+0.2 BLEU on test).
However, MBR-IHMMWord-Comb has ability to generate new hypo-theses, while Hypergraph MMMBR only choos-es translations from original search spaces.We next evaluate performances of MMMBRdecoding on hypergraphs generated by differentbeam size settings, and compare them to (MBR-)318IHMM Word-Comb with the same candidatesize and HGMBR with the same beam size.
Welist the results of MAP decoding for comparison.The comparative results on MT08 are shown inFigure 1, where X-axis is the size used for allmethods each time, Y-axis is the BLEU score,MAP-  and HGMBR-  stand for MAP decodingand HGMBR decoding for the  th system.Figure 1.
MMMBR vs. (MBR-) IHMM Word-Comb and HGMBR with different sizesFrom Figure 1 we can see that, MMMBR de-coding performs consistently better than both(MBR-) IHMM Word-Comb and HGMBR onall sizes.
The gains achieved are around +0.5BLEU compared to IHMM Word-Comb, +0.2BLEU compared to MBR-IHMM Word-Comb,and +0.8 BLEU compared to HGMBR.
Com-pared to MAP decoding, the best result (30.1) isobtained when the size is 100, and the largestimprovement (+1.4 BLEU) is obtained when thesize is 50.
However, we did not observe signifi-cant improvement when the size is larger than 50.We then setup an experiment to verify that themixture model based on multiple distributions ismore effective than any individual distributionsfor Bayes risk computation in MBR decoding.We use Mix-HGMBR to denote MBR decodingperformed on single hypergraph of each systemin the meantime using a mixture model upondistributions of two systems for Bayes risk com-putation.
We compare it with HGMBR andHypergraph MMMBR and list results in Table 3.MT08SYS1 SYS2HGMBR 29.1 28.3Mix-HGMBR 29.4 28.9Hypergraph MMMBR 29.9Table 3.
Performance of MBR decoding on dif-ferent settings of search spaces and distributionsIt can be seen that based on the same searchspace, the performance of Mix-HGMBR is sig-nificantly better than that of HGMBR (+0.3/+0.6BLEU on dev/test).
Yet the performance is stillnot as good as Hypergraph, which indicates thefact that the mixture model and the combinationof search spaces are both helpful to MBR decod-ing, and the best choice is to use them together.We also empirically investigate the impacts ofdifferent system weight settings upon the per-formances of Hypergraph MMMBR on dev setin Figure 2, where X-axis is the weight    forSYS1, Y-axis is the BLEU score.
The weightfor SYS2 equals to      as only two systemsinvolved.
The best evaluation result on dev set isachieved when the weight pair is set to 0.7/0.3for SYS1/SYS2, which is also very close to theone trained automatically by the training strategypresented in Section 3.2.
Although this trainingstrategy can be processed repeatedly, the per-formance is stable after the 1st round finished.Figure 2.
Impacts of different system weights inthe mixture model4.4 MMMBR Results on Identical Systemswith Different Translation ModelsInspired by Macherey and Och (2007), we ar-range a similar experiment to test MMMBR de-coding for each baseline system on an ensembleof sub-systems built by the following two steps.Firstly, we iteratively apply the followingprocedure 3 times: at the  th time, we randomlysample 80% sentence pairs from the total bilin-gual data to train a translation model and use itto build a new system based on the same decod-er, which is denoted as sub-system- .
Table 4shows the evaluation results of all sub-systemson MT08, where MAP decoding (the formerones) and corresponding HGMBR (the latterones) are grouped together by a slash.
We set albeam sizes to 20 for a time-saving purpose.27.528.028.529.029.530.030.510 20 50 100 150MAP-1MAP-2HGMBR-1HGMBR-2IHMMMBR-IHMMMMMBR38.538.738.939.139.339.50.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9MMMBR319MT08SYS1 SYS2Baseline 28.4/29.0 27.6/27.8sub-system-1 28.1/28.5 26.8/27.3sub-system-2 28.3/28.4 27.0/27.1sub-system-3 27.7/28.0 27.3/27.6Table 4.
Performance of sub-systemsSecondly, starting from each baseline system,we gradually add one more sub-system eachtime and perform Hypergraph MMMBR onhypergraphs generated by current involved sys-tems.
Table 5 shows the evaluation results.MT08SYS1 SYS2MAP 28.4 27.6HGMBR 29.0 27.8Hypergraph MMMBR+ sub-system-1 29.1 27.9+ sub-system-2 29.1 28.1+ sub-system-3 29.3 28.3Table 5.
Performance of Hypergraph MMMBRon multiple sub-systemsWe can see from Table 5 that, compared tothe results of MAP decoding, MMMBR decod-ing can achieve significant improvements whenmore than one sub-system are involved; however,compared to the results of HGMBR on baselinesystems, there are few changes of performancewhen the number of sub-systems increases.
Onepotential reason is that the translation hypothesesbetween multiple sub-systems under the sameSMT model hold high degree of correlation,which is discussed in Macherey and Och (2007).We also evaluate MBR-IHMM Word-Combon N-best lists generated by each baseline sys-tem with its corresponding three sub-systems.Evaluation results are shown in Table 6, whereHypergraph MMMBR still outperforms MBR-IHMM Word-Comb on both baseline systems.MT08SYS1 SYS2MBR-IHMM Word-Comb 29.1 28.0Hypergraph MMMBR 29.3 28.3Table 6.
Hypergraph MMMBR vs. MBR-IHMMWord-Comb with multiple sub-systems5 Related WorkEmploying consensus between multiple systemsto improve machine translation quality has maderapid progress in recent years.
System combina-tion methods based on confusion networks (Ros-ti et al, 2007; Li et al, 2009b) have shownstate-of-the-art performances in MT benchmarks.Different from them, MMMBR decoding me-thod does not generate new translations.
It main-tains the essential of MBR methods to seektranslations from existing search spaces.
Hypo-thesis selection method (Hildebrand and Vogel,2008) resembles more our method in making useof n-gram statistics.
Yet their work does not be-long to the MBR framework and treats all sys-tems equally.
Li et al (2009a) presents a co-decoding method, in which n-gram agreementand disagreement statistics between translationsof multiple decoders are employed to re-rankboth full and partial hypotheses during decoding.Liu et al (2009) proposes a joint-decoding me-thod to combine multiple SMT models into onedecoder and integrate translation hypergraphsgenerated by different models.
Both of the lasttwo methods work in a white-box way and needto implement a more complicated decoder tointegrate multiple SMT models to work together;meanwhile our method can be conveniently usedas a second-pass decoding procedure, withoutconsidering any system implementation details.6 Conclusions and Future WorkIn this paper, we have presented a novelMMMBR decoding approach that makes use ofa mixture distribution of multiple SMT systemsto improve translation accuracy.
Compared tosingle system-based MBR decoding methods,our method can achieve significant improve-ments on both dev and test sets.
What is more,MMMBR decoding approach also outperforms astate-of-the-art system combination method.
Wehave empirically verified that the success of ourmethod comes from both the mixture modelingof translation hypotheses and the combinedsearch space for translation selection.In the future, we will include more SMT sys-tems with more complicated models into ourMMMBR decoder and employ more generalMERT algorithms on hypergraphs and lattices(Kumar et al, 2009) for parameter optimization.320ReferencesChiang David.
2007.
Hierarchical Phrase BasedTranslation.
Computational Linguistics, 33(2):201-228.DeNero John, David Chiang, and Kevin Knight.
2009.Fast Consensus Decoding over TranslationForests.
In Proc.
of 47th Meeting of the Associa-tion for Computational Linguistics, pages 567-575.Hildebrand Almut Silja and Stephan Vogel.
2008.Combination of Machine Translation Systemsvia Hypothesis Selection from Combined N-best lists.
In Proc.
of the Association for MachineTranslation in the Americas, pages 254-261.Huang Liang and David Chiang.
2005.
Better k-bestParsing.
In Proc.
of 7th International Conferenceon Parsing Technologies, pages 53-64.Huang Liang.
2008.
Forest Reranking: Discrimin-ative Parsing with Non-Local Features.
InProc.
of 46th Meeting of the Association for Com-putational Linguistics, pages 586-594.Koehn Philipp.
2004a.
Phrase-based Model forSMT.
Computational Linguistics, 28(1): 114-133.Koehn Philipp.
2004b.
Statistical SignificanceTests for Machine Translation Evaluation.
InProc.
of Empirical Methods on Natural LanguageProcessing, pages 388-395.Kumar Shankar and William Byrne.
2004.
MinimumBayes-Risk Decoding for Statistical MachineTranslation.
In Proc.
of the North AmericanChapter of the Association for Computational Lin-guistics, pages 169-176.Kumar Shankar, Wolfgang Macherey, Chris Dyer,and Franz Och.
2009.
Efficient Minimum ErrorRate Training and Minimum Bayes-Risk De-coding for Translation Hypergraphs and Lat-tices.
In Proc.
of 47th Meeting of the Associationfor Computational Linguistics, pages 163-171.Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009a.
Collaborative Decoding:Partial Hypothesis Re-Ranking Using Trans-lation Consensus between Decoders.
In Proc.of 47th Meeting of the Association for Computa-tional Linguistics, pages 585-592.Liu Yang, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint Decoding with Multiple TranslationModels.
In Proc.
of 47th Meeting of the Associa-tion for Computational Linguistics, pages 576-584.Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi.2009b.
Incremental HMM Alignment for MTsystem Combination.
In Proc.
of 47th Meeting ofthe Association for Computational Linguistics,pages 949-957.Mi Haitao, Liang Huang, and Qun Liu.
2008.
Forest-Based Translation.
In Proc.
of 46th Meeting ofthe Association for Computational Linguistics,pages 192-199.Macherey Wolfgang and Franz Och.
2007.
An Em-pirical Study on Computing Consensus Trans-lations from multiple Machine TranslationSystems.
In Proc.
of Empirical Methods on Natu-ral Language Processing, pages 986-995.Och Franz.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proc.
of41th Meeting of the Association for ComputationalLinguistics, pages 160-167.Och Franz and Hermann Ney.
2004.
The Alignmenttemplate approach to Statistical MachineTranslation.
Computational Linguistics, 30(4):417-449.Rosti Antti-Veikko, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved Word-Level SystemCombination for Machine Translation.
In Proc.of 45th Meeting of the Association for Computa-tional Linguistics, pages 312-319.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Trans-lation.
In Proc.
of Empirical Methods on NaturalLanguage Processing, pages 620-629.Ueffing Nicola, Franz Och, and Hermann Ney.
2002.Generation of Word Graphs in Statistical Ma-chine Translation.
In Proc.
of Empirical Me-thods on Natural Language Processing, pages156-163.Wu Dekai.
1997.
Stochastic Inversion Transduc-tion Grammars and Bilingual Parsing of Pa-rallel Corpora.
Computational Linguistics,23(3): 377-404.Xiong Deyi, Qun Liu, and Shouxun Lin.
2006.
Max-imum Entropy based Phrase ReorderingModel for Statistical Machine Translation.
InProc.
of 44th Meeting of the Association for Com-putational Linguistics, pages 521-528.321
