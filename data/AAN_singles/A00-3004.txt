A Weighted Robust Parsing Approach to Semantic Annotat ionHatem Ghorbe l  and  V incenzo  Pa l lo t taL ITH-MEDIA  groupSwiss Federa l  Ins t i tu te  of  Techno logyIN Ecub lens ,  1015 Lausanne,  Swi tzer land{ ghorbel, pallotta} @di.
epfl.
chAbst ractThis paper proposes a grammar-based approach tosemantic annotation which combines the notions ofrobust parsing and fuzzy grammars.
We present anoverview of a preliminary research aimed to gener-alize some results from a recent project on interac-tion through speech with information systems wheretechniques based on the above notions have beensuccessfully applied.
The goal of the article is togive a development environment to linguists?1 In t roduct ionIn this article we are mainly interested in seman-tic annotation (Sperberg-McQueen and Burnard,1994).
We are considering the Information Extrac-tion (I.E.)
problem as a semantic annotation prob-lem: extracting information is finding the relevantterms that contribute to describe an appropriate se-mantic structure of the text.
Some of the most ira-.portant works in I.E.
have been dealing with domaindependent documents like (Moll et al, 1998; Hobbset al, 1996).
Both systems employ complex analysisschemas.
Assigning semantic field tags is in generala difficult task.
This is due at least to the crucialneed of the domain knowledge and also of the lin-guistic knowledge.
Our approach considers that forsome specific domains a semantic annotation can beachieved by a light parsing of the text which is basedon the user of certain cue-words as a heuristic for de-scribing its semantic structure.1.1 h case s tudy  in query  generat ionThe availability of a large collection of annotatedtelephone calls for querying the Swiss phone-bookdatabase (i.e the Swiss French PolyPhone corpus) al-lowed us to experiment our recent findings in robusttext analysis obtained in the context of the SwissNational Fund research project ROTA (Robust TextAnalysis), and in the recent Swisscom funded projectISIS (Interaction through Speech with InformationSystems) 1 (Chappelier et al, 1999).
This databasecontains 4293 simulated recordings related to the1The final report of ISIS project is available atht tp: / / l i thwww.epf l .ch/ -pa l lot ta / is is .html"111" Swisscom service calls.
For instance a querycall like:Bonjour j'aimerais un num4ro det~l~phone & Saignelegier c'estMottaz m o deux ta z Monique rue duprintemps num~ro quatrewould produce the following query frame filling forthe Swiss Phone-book database:Nom de famille / Firme: MOTTAZPr~nom / Autres informations: MONIQUERue, num~ro: rue du PRINTEMPS, 4NPA, localitY: SAIGNELEGIER.The goal of semantic annotation is to provide a treestructure which can be superposed over the flat sen-tence.
This structure can be supported by a PRO-LOG compound term consisting of "tag" functorsand list arguments.
Moreover this same structurecan also be supported by the SGML structural rep-resentation.
The translation between the two modelsis an intuitive task and an example of such transla-tion is provided by the two following correspondingrepresentations for a possible query call schema.PROLOG:S(\[ .... announce(\[...\] ),query ( \[ .
.
.
.name ( \[ .
.
.
.f am_name ( C.. ?
\] ) .
.
.
.
.first_name(\[...\]),...\]) .....address  ( \[ .
.
.
.s t reet ( \ [ .
.
.
\ ] )  .
.
.
.
.city(\[...\]),.
.
.
\ ] ) ,.
.
.
\ ] ).
.
.
\ ] ) .SGML:<announce></~ounce><query><name>19?
.
?<fam_name><first_name></name><address><street><city></address>?
..</query>.
.
.
< / ram_name>?
.
.
< / f i r s t _name>... </street>.
.
.
</city>1.1.1 Processing phasesThe processing of the corpus data is performed atvarious linguistic levels by modules organized intoa pipeline?
Each module assumes as input the out-put of the preceding module?
The main goal of thisarchitecture is to understand how far it is possibleto go without using any kind of feedback and in-teractions among different linguistic modules?
Ata first stage, morphologic and syntactic processing 2is applied to the output from the speech recognizermodule which usually produces a huge word-graphhypothesis.
Thus the forest of syntactic trees pro-duced by this phase have been used to achieve twogoals:1."
The n-best analyses are used to disambiguatespeech recognizer hypotheses2.
They served as supplementary input for the ro-bust semantic analysis that we performed, thathad as goal the production of query frames forthe information system?Although robustness can be considered as being ap-plied at either a syntactic or semantic level, we be-lieve it is generally at the semantic level that it ismost effective.
This robust analysis needs a modelof the domain in which the system operates, anda way of linking this model to the lexicon used bythe other components.
The degree of detail requiredof the domain model used by the robust analyzerSOur partner institution ISSCO (Institute Dalle Molle,University of Geneva) performed this analysis phase usingtools that were developed in the European Linguistics En-gineering project MULTEXT.
For syntactic analysis, ISSCOdeveloped a Feature Unification Grammar based on a smallsample of the Polyphone data.
This grammar was taken byanother of our partners (the Laboratory for Artificial Intel-ligence of the Swiss Federal Institute of Technology, Lau-sanne) and converted into a probabilistic ontext-free gram-mar, which was then applied to a sample of 500 entries fromthe Polyphone data.depends upon the ultimate task that must be per-formed - -  in our case, furnishing a query to an infor-mation system.
The results of the processing phaseof the previous example is represented below as anSGML annotation:<announce>Bonjour j'aimerais un</announce><query> num~ro de t~l@phone ~ Saignelegier<name><ram_name> c~est Mottaz m o deux ta z</f am_name><first_name> Monique </first_name></name><address><street> rue  du  printemps </street><number> num~ro quatre </number><city> </city></address></query>2 Methodo logyIn this section we propose the use of a "light-parser"for doing sentence-level semantic annotation?
Themain idea comes from the observation that annota-tion does not always need to rely on the deep struc-ture of the sentence (e.g.
at morpho-syntactic level).It is sometimes ufficient to find some cue-wordswhich allow us to locate the logical sub-structuresof the sentence?
If the domain is simple enough, thistask can be easily mechanized.
A similar approach,using finite state parsing technology, has been pro-posed by Grefenstette in (Grefenstette, 1996) wherethe main applications are slanted to the extractionof syntactic information?2.1 Robust  Def in i te  C lause GrammarsLHIP (Left-corner Head-driven Island Parser) (Bal-lim and Russell, 1994; Lieske and Ballim, 1998) is asystem which performs robust analysis of its input,using a grammar defined in an extended form of thePROLOG Definite Clause Grammars (DCGs).
Thechief modifications to the standard PROLOG 'gram-mar rule' format are of two types: one or more right-hand side (RHS) items may be marked as 'heads'(e.g.
using a leading '*'), and one or more RHS itemsmay be marked as 'ignorable' (e.g.
using a leading'-').
LHIP employs a different control strategy fromthat used by PROLOG DCGs, in order to allow it tocope with ungrammatical or unforeseen i put?
Thebehavior of LHIP can best be understood in termsof the complementary notions of span and cover?A grammar ule is said to produce an island whichspans input terminals ti to ti+,~ if the island startsat the i ~h terminal, and the i + n th terminal is theterminal immediately to the right of the last termi-nal of the island?
A rule is said to cover m itemsif m terminals are consumed in the span of the rule.20Thus m < n. If m = n then the rule has completelycovered the span.
As implied here, rules need notcover all of the input in order to succeed.2.1.1 Weighted  LH IP  ru lesThe main goal of introducing weights into LHIPrules is to induce a partial order over the generatedhypotheses.
The following schema illustrate how tobuild a simple weighted rule in a compositional fash-ion where the resulting weight is computed fromthe sub-constituents u ing the minimum operator.Weights are real numbers in the interval \[0, 1\].cat (cat (Hyp) ,Weight )  "~>sub_cat l (H l ,Wl ) ,?
.
.
~sub_catn(Hn,Wn),{app_list(\[Hl .
.
.
.
.
Hn\],Hyp),min_list(\[Wl ..... Wn\] ,Weight)}.This strategy is not the only possible since the LHIPformalism allows a greater flexibility.
Without en-tering into formal details we can observe that if westrictly follow the above schema and we impose acover threshold of 1 we are dealing with fuzzy DCGgrammars (Lee and Zadeh, 1969; Asveld, 1996).
Weactually extend this class of grammars with a no-tion of fuzzy-robustness where weights are used tocompute confidence factors for the membership of is-lands to categories 3.
The order of constituents mayplay an important role in assigning weights for dif-ferent rules having the same number and type ofconstituents.
Each LHIP rule returns a weight to-gether with a term'which will contribute to buildthe resulting structure.
The confidence factor for apre-terminal rule has been assigned statically on thebasis of the rule designer's domain knowledge.2.2 The  methodo logy  at workIn our case study we try to integrate the above prin-ciples in order to effectively compute annotation hy-potheses for the query generation task.
This canbe done by building a lattice of annotation hypothe-ses and possibly selecting the best one.
This latticeis generated by means of a LHIP weighted gram-mar which is used to extract and assemble whatwe called semantic constituents.
At the end of thisprocess we presumably obtain suitable annotationsfrom which we will able to extract the content ofthe query (e.g.
name, address, city, etc.).
The rulesare designed taking into consideration the followingkind of knowledge:Domain  Knowledge is exploited to provide quan-titative support (or confidence factor) to ourrules.3Development of this notion is currently under investiga-tion and not yet formalized.Linguist ic  Knowledge (as for instance previousPOS tagging or syntactic analysis) is used fordetermining constraints in order to prune thehypotheses space.Lexical  knowledge:  As pointed out in (Basiliand M.T., 1997), lexical knowledge plays an im-portant role in Information Extraction since it cancontribute in guiding the analysis process at variouslinguistic level.
In our case we are concerned withlexical knowledge when we need to specify lexicalLHIP rules which represent the building blocks ofour parsing system.
Semantic markers are domain-dependent word patterns and must be defined fora given corpus.
They identify cue-words servingboth as separators among logical subparts of thesame sentence and as introducers of semantic con-stituents.
In our specific ase they allow us to searchfor the content of the query only in interesting partsof the sentence.
One of the most important sep-arators is the announcement-query separator.
TheLHIP clauses defining this separator can be one ormore words covering rule like for instance:ann_query_separator ( IX\] ,0.7) #I.
0 "'>~terminal (X) ,{X=' t~l~pbone ' }.ann_query_separator(\[X,Y\], i) #I.0 "~>~terminal (X),?terminal (Y),{\[X = 'num4ro*,Y = 'de'\]}.As an example of semantic onstituents introducerswe propose here the follo~;ing rule:street_intro(\[T,Prep\] ,I) #I.0 "'>* street_type(T),preposition (Prep).which make use of some word knowledge about streettypes coming from an external thesaurus like:street_type(X) "'>?terminal (X),{thesaurus (street, W),member (X, N) }.It should be noted that we mix weighted and non-weighted rules, simply because non-weighted rulesare rules with the highest weight 1.2.2.1 Generat ion  of  hypothesesThe generation of annotation hypotheses is per-formed by: composing weighted rules, assemblingconstituents and filtering possible hypotheses.
Inthis case the grammar should provide a means toprovide an empty constituent when all possible hy-pothesis rules have failed.
The highest level con-stituent is represented by the whole sentence struc-ture which simply specifies the possible orders ofconstituents relative to annotation hypotheses.21s(s(\[Ann,Query\]), W) - '>ann(Ann),query(Query,W2).ann(ann(Ann)) "-> closK(word(Ann)).query(query(Q) ,W) "'>* ann_query_separator (qSep, W1),target (Target, W2),address (Addr,W3){app_list ( \[Qsep, \[Target\],\[Addr\] \ ,  Q),rain_list ( \[Wl,W2, W3\] ,W) }.In the ann rule we have made use of the Kleeneclosure operator closK which allow LHIP to sim-ply formulate regular expressions.
In the query rulewe have specified a possible order of constituents in-terleaved by semantic markers (e.g.
separators andintroducers).
In this case we did not provide any lin-guistic constraint (e.g.
preferring names belongingto the minimal common syntactic sub-tree or thosehaving the longest sequence of proper names belong-ing to the same sub-tree).3 Conc lus ions  and  fu ture  worksIn this paper we summarized a proposal for a frame-work for designing grammar-based automated an-notation applications.
Starting with a case studyand following an approach which combines the no-tions of fuzziness and robustness in sentence parsing,we showed how to build practical domain-dependentrules which can be applied whenever it is possible tosuperimpose a sentence-level semantic structure toa text without relying on a previous deep syntacti-cal analysis.
Even if the query generation problemmay not seem a critical application one should bearin mind that the sentence processing must be doneon-line.As we have previously seen, the cue-words used assemantic markers are domain-dependent.
Even theirrelevance disposal and their weight within the rulesdepends on their linguistic usage.
Therefore, a com-plete automatic annotation system based on the ap-proach proposed in this article seems to be adequateto give precise results.
However, a semi-automaticsystem could satisfy our needs.
This system shouldbe based on the following techniques to achieve ahigh level of performance:1.
For each annotation, the system offers a listof propositions based on standard grammarsas well as on external knowledge (ontologies,knowledge bases ...)2.
According to the grammar initially proposed,the user may change the annotation accord-ing to his needs.
These modifications are heldwithin the system to change the grammar ulesas well as their weights.
This makes the systeminteractive and enhanced by a learning phase.3.
We could imagine that rule design process canbe partially automated and we intend to pursuesome research on developing methods for bothassisted rule design and corpus based rule in-duction.Re ferencesPeter.
R.J. Asveld.
1996.
Towards robustness inparsing - fuzzifying context-free language recog-nition.
In J. Dassow, G. Rozemberg, and A. Sa-lomaa, editors, Developments in Language TheoryH-  At the Crossroad of Mathematics, ComputerScience and Biology, pages 443-453.
World Scien-tific, Singapore.A.
Ballim and G. Russell.
1994.
LHIP: ExtendedDCGs for Configurable Robust Parsing.
In Pro-ceedings of the 15th International Conference onComputational Linguistics, pages 501 - 507, Ky-oto, Japan.
ACL.R.
Basili and Pazienza M.T.
1997.
Lexical ac-quisitiion and information extraction.
In PazienzaM.T., editor, Information Extraction - A multi-diciplinary approach to an ermerging informationtechnology, volume 1299 of LNAI, pages 44-72.Springer Verlag.J-C. Chappelier, M. Rajman, P. Bouillon, S. Arm-strong, V. Pallotta, and A Ballim.
1999.
Isisproject: final report.
Technical report, ComputerScience Department - Swiss Federal Institute ofTechnology, September..G. Grefenstette.
1996.
Light parsing as finite-statefiltering.
In Kornai A., editor, Proceedings ofthe ECAI 96 Workshop on Extended Finite StateModels of Language, pages 20-25.J.
Hobbs, D. Appelt, J.
Bear, D. Israel,M.
Kameyama, M. Stickel, and M. Tyson.
1996.Fastus: a cascaded finite-state transducer for ex-tracting information from natural-language t xt.In E. Roche and Y. Schabes, editors, Finite StateDevices for Natural Language Processing.
MITPress, Cambridge MA.E.T.
Lee and L.A. Zadeh.
1969.
Note on fuzzy lan-guages.
Information Science, 1:421-434.C.
Lieske and A. Ballim.
1998.
Rethinking nat-ural language processing with prolog.
In Pro-ceedings of Practical Applications of Prolog andPractical Applications of Constraint Technology(PAPPACTS98), London,UK.
Practical Applica-tion Company.D.
Moll, J. Berri, and M. Hess.
1998.
A real worldimplementation of answer extraction.
In Proe.
ofthe 9th International Conference and Workshopon Database and Expert Systems.
Workshop onNatural Language and Information Systems, vol-ume NLIS'98, pages 143-148, Vienna.22C.M.
Sperberg-McQueen a d L. Burnard, editors.1994.
Guidelines for Electronic Text Encoding andInterchange, Text Encoding Initiative.
Chicagoand Oxford.23
