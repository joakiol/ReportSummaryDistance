AbstractAKT is a major research projectapplying a variety of technologies toknowledge management.
Knowledgeis a dynamic, ubiquitous resource,which is to be found equally in anexpert's head, under terabytes of data,or explicitly stated in manuals.
AKTwill extend knowledge managementtechnologies to exploit the potentialof the semantic web, covering the useof knowledge over its entire lifecycle,from acquisition to maintenance anddeletion.
In this paper we discuss howHLT will be used in AKT and howthe use of HLT will affect differentareas of KM, such as knowledgeacquisition, retrieval and publishing.1 IntroductionAs globalisation reduces the competitiveadvantage existing between companies, the roleof proprietary information and its appropriatemanagement becomes all-important.
Acompany?s value depends more and more on?intangible assets?1 which exist in the minds ofemployees, in databases, in files and in amultitude of documents.
It is the goal ofknowledge management (KM) technologies tomake computer systems which provide access tothis intangible knowledge present in a companyor organisation.
The system must make itpossible to share, store and retrieve thecollective expertise of all the people in anorganization.
At present, many companies spend1 A term coined by Karl-Erik Sveibyconsiderable resources on knowledgemanagement; estimates range between 7 and10% of revenues (Davenport 1998).In developing a knowledge managementsystem, the knowledge must first be captured oracquired in some form which is usable by acomputer.
The knowledge acquisitionbottleneck, so well-known in AI, is just asimportant in knowledge management.
Theacquisition of knowledge does not become lessdifficult in a business environment and oftenrequires a sea-change in company culture inorder to persuade users to accommodate to thetechnology adopted, precisely becauseknowledge acquisition is so difficult.Once knowledge has been acquired, it must bemanaged, i.e.
modelled, updated and published.Modelling means representing information in away that is both manageable and easy tointegrate with the rest of the company?sknowledge.
Updating is necessary becauseknowledge is dynamic.
Part of its importancefor a company or individual lies in the fact thatknowledge is ever changing and keeping upwith the change is a crucial dimension inknowledge management.
Publishing is theprocess that allows sharing the knowledgeacross the company.
These needs havecrystallised in efforts to develop the so-calledSemantic Web.
It is envisaged that in the future,the content currently available on the Web (bothInternets and Intranets) as raw data will beautomatically annotated with machine-readablesemantic information.
In such a case, we willno longer speak of information retrieval butrather of Knowledge Retrieval because insteadof obtaining thousands of potentially relevant orirrelevant documents, only the dozen or sodocuments that are truly needed by the user willbe presented to them.Using HLT for Acquiring, Retrieving and Publishing Knowledge in AKT:Position PaperK.
Bontcheva, C. Brewster, F. Ciravegna, H. Cunningham,L.
Guthrie, R. Gaizauskas, Y. WilksDepartment of Computer Science, the University of Sheffield,Regent Court, 211 Portobello Street, S1 4DP Sheffield, UKEmail: N.Surname@dcs.shef.ac.ukIn this paper we present the way HumanLanguage Technology (HLT) is used to addressseveral facets of the KM problem:  acquiring,retrieving, and publishing knowledge.
The workpresented in this paper is supported by the AKTproject (Advanced Knowledge Technologies), amultimillion pound six year research projectfunded by the EPSRC in the UK.
AKT, startedin 2000, involves the University ofSouthampton, the Open University, theUniversity of Edinburgh, the University ofAberdeen, and the University of Sheffieldtogether with a large number of major UKcompanies.
Its objectives are to developtechnologies to cope with the six mainchallenges of knowledge management:?
acquisition ?
reuse?
modelling ?
publication?
retrieval/extraction ?
maintenanceThese challenges will be addressed by theUniversity of Sheffield in the context of AKTby the application of a variety of humanlanguage technologies.
Here, we consider onlythe contribution of HLT to the acquisition ofknowledge, its retrieval and extraction, itspublication, and finally the role of appropriateHLT infrastructure to the completion of thesegoals.2 Knowledge AcquisitionKnowledge acquisition (KA) is concerned withthe process of turning data into coherentknowledge for a computer program.
The needfor effective KA methods increases as thequantity of data available electronicallyincreases year by year, and the importance itplays in our society is more and morerecognised.
The challenge, we believe, lies indesigning effective techniques for acquiring thevast amounts of (largely) tacit knowledge.
KA isa complex process, which traditionally isextremely time consuming.Existing KA methodologies are varied butalmost always require a great deal of manualinput.
One methodology, often used in ExpertSystems, involves the time-consuming processof structured interviews (?protocols?
), which arethen analysed by knowledge engineers in orderto codify and model the knowledge of an expertin a particular domain.
Even if a complex expertsystem is not required, all forms of KA are verylabour intensive.
Yahoo currently employs over100 people to keep its category hierarchy up todate (Dom 1999).
Some methodologies havestarted to appear to automate this process,although still limited to some steps in the KAprocess.
They depend on replacing theintrospection of knowledge engineers or theextended elicitations of the protocol methods(Ericsson and Simon 1984) by using HumanLanguage Technologies, more specificallyInformation Extraction, Natural LanguageProcessing and Information Retrieval.Although knowledge acquisition producesdata (knowledge) for use by a computerprogram, the form and content of thatknowledge is often debated in the researchcommunity.
Ontologies have emerged as one ofthe most popular means of modelling theknowledge of a domain.
The meaning of thisword varies somewhat in the literature, butminimally it is a hierarchical taxonomy ofcategories, concepts or words.
Ontologies canact as an index to the memory of an organisationand facilitate semantic searches and the retrievalof knowledge from the corporate memory as itis embodied in documents and other archives.Repeated research has shown their usefulness,especially for specific domains (J?rvelin andKek?l?inen 2000).
The process of ontologyconstruction is illustrated in the rest of thissection.2.1 Taxonomy constructionWe propose to introduce automation in the stageof taxonomy construction mainly in order toeliminate or reduce the need for extensiveelicitation of data.
In the literature approachesto construction of taxonomies of concepts havebeen proposed (Brown et al 1992, McMahonand Smith 1996, Sanderson and Croft 1999).Such approaches either use a large collection ofdocuments as their sole data source, or they canattempt to use existing concepts to extend thetaxonomy (Agirre et al2000, Scott 1998).
Weintend to develop a semi-automatic method that,starting from a seed ontology sketched by theuser, produces the final ontology via a cycle ofrefinements by eliciting knowledge from acollection of texts.
In this approach the role ofthe user should only be that of proposing aninitial ontology and validating/changing thedifferent versions proposed by the system.We intend to integrate a methodology forautomatic hierarchy definition (such as(Sanderson and Croft 1999)) with a method forthe identification of terms related to a conceptin a hierarchy (such as (Scott 1998)).
Theadvantage of this integration is that, asknowledge is continually changing, we canreconstruct an appropriate domain specificontology very rapidly.
This does not precludeincorporating an existing ontology and using thetools to extend and update it on the basis ofappropriate texts.
Finally an ontology defined inthis way has the particular advantage that itovercomes the well-known ?Tennis problem?associated with many predefined ontologiessuch as WordNet, i.e where terms closelyrelated in a given domain are structurally verydistant such as ball and court, for example.In addition we intend to employ classicInformation Extraction techniques (describedbelow) such as named entity recognition(Humphreys et al 1998) in order to pre-processthe text, as the identification of complex termssuch as proper names, dates, numbers, etc,allows to reduce data sparseness in learning(Ciravegna 2000).We plan to introduce many cycles of ontologylearning and validation.
At each stage thedefined ontology can be: i) validated/correctedby a user/expert; ii) used to retrieve a larger setof appropriate documents to be used for furtherrefinement (J?rvelin and Kek?l?inen 2000); iii)passed on to the next development stage.2.2 Learning Other RelationsThis stage proceeds to build on the skeletalontology in order to specify, as much aspossible without human intervention, relationsamong concepts in the ontology, other thanISAs.
In order to flesh the concept relations, weneed to identify relations such as synonymy,meronymy, antonymy and other relations.
Weplan to integrate a variety of methods existing inthe literature, e.g.
by using recurrences in verbsubcategorisation as a symptom of generalrelations (Basili et al 1998), by using Morin?suser-guided approach to identify the correctlexico/syntactic environment (Morin 1999), andby using methods such as (Hays 1997) to locatespecific cases of synonymy.3 Knowledge ExtractionAssuming that the shape of knowledge has beenacquired and adequately modelled, it will haveto be stored in a repository from which it isretrieved as and when needed.
On the one handthere is the problem of retrieving instances inorder to populate the resulting knowledge base.On the other hand, considering that repositoriescould become very substantial in size, there isthe necessity to navigate the repository in orderto extract the knowledge when needed.
In thissection we focus on the problem of knowledgebase population, as it is in our opinion the mostchallenging from the HLT point of view.3.1 Knowledge Base  PopulationInstance identification for Knowledge Basepopulation can be performed by HLT-baseddocument analysis.
With the term documents,we mean a wide variety of types of texts such asplain texts, web pages, knowledge elicitationinterview transcriptions (protocols), etc.
For thesake of this paper we limit our analysis tolanguage related tasks only, ignoring theproblem of multi-media information.
As a firststep instance identification requires theidentification of relevant documents containingcitation of the interesting information(document classification).
Then it requires theability to identify and extract information fromdocuments (Information Extraction from text).3.2 Document ClassificationText classification for IE purposes has beenexplored both in the MUC conferences as wellas in some commercially oriented projects(Ciravegna et al 2000).
In concrete termsclassification is used in order to identify thescenario to apply to a specific set of texts, whileIE will identify (i.e.
index) the instances in thetexts.
In most cases of application documentclassification is quite straightforward, beinglimited to the Boolean classification of adocument between relevant/irrelevant (singlescenario application as in the MUCconferences).
In cases in which knowledge maybe distributed along a number of differentdetailed scenarios, full document classificationis then needed.
In such cases, two maincharacteristics are relevant for the classificationapproach: flexibility and refinability (Ciravegnaet al 1999).
Flexibility is needed with respectto both the number of the categories and thegranularity of the classification to be copedwith.
Three main types of classification can beidentified: coarse-grained, fine-grained, andcontent-based.
Coarse-grained classification isperformed among a relatively small number ofclasses (e.g., some dozens) that are sharplydifferent (e.g., sport vs finance).
This can beobtained reliably and efficiently by theapplication of statistical classifiers.
Fine-grained classification is performed over ausually larger number of classes that can bevery similar (e.g., discriminating between newsabout private bond issues and news about publicbond issues).
This type of classificationgenerally requires some more knowledge-oriented approaches such as pattern-basedclassification.
Sometimes categories are sosimilar that classification needs to be content-based, i.e.
it can be performed only byextracting the news content (e.g., finding newsarticles issued by English financial institutionsreferring to amounts in excess of 100,000 Euro).In this case some forms of shallow adaptiveInformation Extraction can be used (see nextsection).
Refinability concerns the possibilityof performing classification in a sequence ofsteps, each one providing a more preciseclassification (from coarse-grained to content-based).
In the current technological situationcoarse-grained classification can be performedquickly, while the systems available for morefine-grained classification are much slower andless general purpose.
When the amount oftextual material is large an incrementalapproach, based on some level of coarse-grainedclassification further refined by successiveanalysis, proves to be very effective.
A refinableclassification is generally performed over ahierarchy of classes.
A refinement may revisethe categories assigned to specific texts withmore specialised classes from the hierarchy.More complex techniques are invoked onlywhen needed and, in any case, within an alreadydetected context (Ciravegna et al 1999).We plan to produce a number of solutions fortext classification, adaptable to differentscenarios and situations, following the criteriamentioned above.3.3 Information ExtractionInformation extraction from text (IE) is theprocess of mapping of texts into fixed formatoutput (templates) representing the keyinformation (Gaizauskas 1997).
In using IE forKM, templates represent an intermediate formatfor mapping the information in the texts intoontology instances.
Templates can be semi-automatically derived from the ontology.
Weplan to use IE for a number of passes: on theone hand, we plan to populate a knowledge basewith instances as mentioned above.
On the otherhand, IE can be used to monitor relevantchanges in the information, providing afundamental contribution to the problem ofknowledge updating.
We have a long experiencein IE from texts, Sheffield having activelyparticipated in the MUC conferences and in theTIPSTER project, activities that historicallyhave made a fundamental contribution tomaking IE as we now know it.
The newchallenge we are currently addressing isadaptivity.
Adaptivity is a major goal forInformation Extraction, especially in the case ofits application to knowledge management, asKM is a process that has to be distributedthroughout companies.
The real value of IE willbecome apparent when it can be adapted to newapplications and scenarios directly by the finaluser without the intervention of IE experts.
Thegoal for research in adaptive IE is to createsystems adaptable to new applications/domainsby using only an analyst?s knowledge, i.e.knowledge about the domain/scenario.There are two directions of research inadaptive IE, both involving the use of MachineLearning.
On the one hand machine learning isused to automate as much as possible the tasksan IE expert would perform in applicationdevelopment (Cardie 1997) (Yangarber et al2000).
The goal here is to reduce the portingtime to a new application (and hence the cost).This area of research comes mainly from theMUC community.
Currently, the technologymakes use mainly of NLP-intensivetechnologies and the type of texts addressed aremainly journal articles.On the other hand, there is an attempt to makeIE systems adaptable to newdomains/applications by using only an analyst?sknowledge, i.e.
knowledge about thedomain/scenario only (Kushmerick et al 1997),(Califf 1998), (Muslea et al 1998), (Freitag andMcCallum 1999), (Soderland 1999), (Freitagand Kushmerick 2000), (Ciravegna 2001a).Most research has so far focused on Web-related texts (e.g.
web pages, email, etc.
)Successful commercial products have beencreated and there is an increasing interest on IEin the Web-related market.
Current adaptivetechnologies make no use of natural languageprocessing in the web context, as extra linguisticstructures (e.g.
HTML tags, documentformatting, and ungrammatical stereotypicallanguage) are the elements used to identifyinformation.
Linguistically intensive approachesare difficult or unnecessary in such cases.
Whenthese non-linguistic approaches are used ontexts with a reduced (or no) structure, they tendto be ineffective.There is a technological gap between adaptiveIE on free texts and adaptive IE on web-relatedtexts.
For the purposes of KM, such a gap has tobe bridged so to create a set of technologies ableto cover the whole range of potentialapplications for different kinds of texts, as thetype of texts to be analysed for KM may varydramatically from case to case.
We plan tobridge this gap via the use of lazy naturallanguage processing.
We intend to use anapproach where the system starts with a rangeof potential methodologies (from shallow tolinguistically intensive) and learns from atraining corpus which is the most effectiveapproach for the particular case underconsideration.
A number of factors caninfluence the choice: from the type of texts to beanalysed to the type of information the user isable to provide in adapting the system.
In thefirst case the system will have to identify whattype of task is under consideration and select thecorrect level of analysis  (e.g.
language basedfor free texts).
Formally in this case the level oflanguage analysis is one of the parameters thelearner will have to learn.
Concerning the typeof tagging the user is able to provide: differentusers are able to provide different levels ofinformation in training the system: IE-trainedusers are able to provide sophisticated tagging,maybe inclusive of syntactic, semantic orpragmatic information.
Na?ve users on the otherhand are only able to provide some basicinformation (e.g.
to spot the relevantinformation in the texts and highlight it indifferent colours).
We plan to develop a systemable to cope with a wide of variety of situationsby starting from the (LP)2 algorithm andenhancing its learning capabilities on free texts(Ciravegna 2001) and developing a powerfulhuman-computer interface for system adaptation(Ciravegna and Petrelli 2001).4 Knowledge PublishingKnowledge is only effective if it is delivered inthe right form, at the right place, to the rightperson at the right time.
Knowledge publishingis the process that allows getting knowledge tothe people who need it in a form that they canuse.
As a matter of fact, different users need tosee knowledge presented and visualised in quitedifferent ways.
The dynamic construction ofappropriate perspectives is a challenge which, inAKT, we will address from the perspective ofgenerating automatically such presentationsfrom the ontologies acquired by the KA and KEmethods, discussed in the previous sections.Natural Language Generation (NLG) systemsautomatically produce language output (rangingfrom a single sentence to an entire document)from computer-accessible data, usually encodedin a knowledge or data base (Reiter 2000).
NLGtechniques have already been used successfullyin a number of application domains, the mostrelevant of which is automatic production oftechnical documentation (Reiter et al 1995),(Paris et al 1996).
In the context of KM andknowledge publishing in particular, NLG isneeded for knowledge diffusion anddocumenting ontologies.
The first task isconcerned with personalised presentation ofknowledge, in the form needed by each specificuser and tailored to the correct language typeand the correct level of details.
The latter is avery important issue, because as discussedearlier, knowledge is dynamic and needs to beupdated frequently.
Consequently, theaccompanying documentation which is vital forthe understanding and successful use of theacquired knowledge, needs to be updated insync.
The use of NLG simplifies the ontologymaintenance and update tasks, so that theknowledge engineer can concentrate on   theknowledge itself, because the documentation isautomatically updated as the ontology changes.The NLG-based knowledge publishing toolswill also utilise the ontology instances extractedfrom documents using the IE approachesdiscussed in Section 3.3.
The dynamicallygenerated documentation will not only includethese instances, as soon as they get extracted,but it will also provide examples of theiroccurrence in the documents, thus facilitatingusers?
understanding and use of the ontology.Our approach to knowledge publishing is basedon an existing framework for generation of user-adapted hypertext explanations (Bontcheva2001), (Bontcheva and Wilks 2001).
Theframework incorporates a powerful agentmodelling module, which is used to tailor theexplanations to the user?s knowledge, task, andpreferences.
We are now also extending thepersonalisation techniques to account for userinterests.
The main challenge for NLG will beto develop robust and efficient techniques forknowledge publishing which can operate onlarge-scale knowledge resources and support thepersonalised presentation of diverseinformation, such as speech, video, text,graphics (see (Maybury 2001)).The other challenge in using NLG forknowledge publishing is to develop tools andtechniques that will enable knowledgeengineers, instead of linguists, to create andcustomise the linguistic resources (e.g., domainlexicon) at the same time as they create and editthe ontology.
In order to allow such inter-operability with the KA tools, we will integratethe NLG tools in the GATE infrastructure,discussed next.5 HLT InfrastructureThe range and complexity of the task ofknowledge management make imperative theneed for standardisation.
While there has beenmuch talk about the re-use of knowledgecomponents such ontologies, much less hasbeen undertaken to standardise theinfrastructure for tools and their development.The types of data structures typically involvedare large and complex, and without good toolsto manage and allow succinct viewing of thedata we will continue to work below ourpotential.
The University of Sheffield haspioneered in the Gate and Gate 2 projects thedevelopment of an architecture for textengineering (Cunningham et al 1997),(Cunningham et al 2000).
Given the modulararchitecture and component structure of Gate, itis natural to build on this basis to extend thecapabilities of Gate so as to provide the mostsuitable possible environment for tooldevelopment, implementation and evaluation inAKT.
The system will provide a singleinteraction and deployment point for the roll-outof HLT in Knowledge Management.
We expectGate2 to act as the skeleton for a large range ofknowledge management activities within AKTand plan to extend its capabilities within the lifeof the AKT project by integrating with suitableontological and lexical databases in order topermit the use of  the Gate system with largebodies of heterogeneous data6 Conclusion and Future WorkWe have presented how we plan to use HLT forhelping KM in AKT.
We believe that HLT canmake a substantial contribution to the followingissues in  KM:?
Cost reduction: KM is an expensive task,especially in the acquisition phase.
HLT canaid in automating both the acquisition of thestructure of the ontology to be learnt and inpopulating such ontology with instances.
Itwill also provide support for automaticknowledge documentation.?
Time reduction: KM is a slow task: HLTcan help in making it more efficient byreducing the need for the human effort;?
Subjectivity reduction: this is a mainproblem in knowledge identification andselection.
Subjective knowledge is difficultto integrate with the rest of the company?sknowledge and its use is somehow difficult.KM constitutes a challenge for HLT as itprovides a number of fields of application andin particular it challenges the integration of a setof techniques for a common goal.AcknowledgementThis work is supported under the AdvancedKnowledge Technologies (AKT)Interdisciplinary Research Collaboration (IRC),which is sponsored by the UK Engineering andPhysical Sciences Research Council under grantnumber GR/N15764/01.
The AKT IRCcomprises the Universities of Aberdeen,Edinburgh, Sheffield, Southampton and theOpen University.ReferencesAgirre, E. O. Ansa, E. Hovy, and  D.Mart?nez 2000.
Enriching very large ontologiesusing the WWW, Proceedings of the ECAI 2000workshop ?Ontology Learning?.Basili, R., R. Catizone, M. Stevenson, P.Velardi, M. Vindigni, and Y. Wilks.
1998.
?AnEmpirical Approach to Lexical Tuning?.Proceedings of the Adapting Lexical andCorpus Resources to Sublanguages andApplications Workshop, held jointly with 1stLREC Granada, Spain.Bontcheva, K. 2001.
Generating adaptivehypertext explainations with a nested agentmodel.
Ph.
D. Thesis, University of Sheffield.Bontcheva, K. and Wilks, Y.
2001.
Dealingwith Dependencies between Content Planningand Surface Realisation in a PipelineGeneration Architecture.
Proceedings of the 17thInternational Joint Conference on ArtificialIntelligence (IJCAI2001), Seattle.Brown, P.F., Peter F., V. J. Della Pietra, P.V.
DeSouza, J. C. Lai, and R. L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18, 467-479.Califf, M. E. 1998.
Relational LearningTechniques for Natural Language InformationExtraction.
Ph.D. thesis, Univ.
Texas, Austin,www/cs/utexas.edu/users/mecaliffC.
Cardie, `Empirical methods ininformation extraction', AI Journal,18(4), 65-79, (1997).F.
Ciravegna, A. Lavelli, N. Mana, J.Matiasek, L. Gilardoni, S. Mazza, M. Ferraro,W.
J.
Black F. Rinaldi, and D. Mowatt.FACILE: Classifying Texts Integrating PatternMatching and Information Extraction.
InProceedings of the 16th International JointConference On Artificial Intelligence(IJCAI99), Stockholm, Sweden, 1999.F.
Ciravegna, A. Lavelli,, L. Gilardoni, S.Mazza, W. J.
Black, M. Ferraro, N. Mana, J.Matiasek, F. Rinaldi.
Flexible TextClassification for FinancialApplications: The FACILE System.
InProceedings of Prestigious Applications sub-conference (PAIS2000) sub-conference of the14th European Conference On ArtificialIntelligence (ECAI2000), Berlin, Germany,August, 2000.Ciravegna, F. 2001.
Adaptive InformationExtraction from Text by Rule Induction andGeneralisation.
Proceedings of the 17thInternational Joint Conference on ArtificialIntelligence (IJCAI2001), Seattle.Ciravegna, F. and D. Petrelli.
2001.
UserInvolvement in customizing AdaptiveInformation Extraction from Texts: PositionPaper.
Proceedings of the IJCAI01 Workshop onAdaptive Text Extraction and Mining, Seattle.Cunningham, H., K. Humphreys, R.Gaizauskas and Y. Wilks.
1997.
SoftwareInfrastructure for Natural Language Processing.Proceedings of the Fifth Conference on AppliedNatural Language Processing (ANLP-97).Cunningham H., K. Bontcheva, V. Tablanand Y. Wilks.
2000.
Software Infrastructure forLanguage Resources: a Taxonomy of PreviousWork and a Requirements Analysis.Proceedings of the Second Conference onLanguage Resources Evaluation, Athens.Dom, B.
1999.
Automatically finding thebest pages on the World Wide Web (CLEVER).Search Engines and Beyond: Developingefficient knowledge management systems.Boston, MA.Ericsson, K. A. and H. A. Simon.
1984.Protocol Analysis: verbal reports as data.
MITPress, Cambridge, Mass.Freitag, D. and A. McCallum.
1999Information Extraction with HMMs andShrinkage.
AAAI-99 Workshop on MachineLearning for Information Extraction, Orlando,FL.
(www.isi.edu/~muslea/RISE/ML4IE/)Freitag, D. and N. Kushmerick.
2000.Boosted wrapper induction.
F. Ciravegna, R.Basili, R. Gaizauskas, ECAI2000 Workshop onMachine Learning for Information Extraction,Berlin, 2000, (www.dcs.shef.ac.uk/~fabio/ecai-workshop.html)Hays, P. R. 1997.
Collocational Similarity:Emergent Patterns in Lexical Environments,PhD.
Thesis.
School of English, University ofBirminghamHumphreys, K., R. Gaizauskas, S. Azzam, C.Huyck, B. Mitchell, H. Cunningham and  Y.Wilks.
1998.
Description of the University ofSheffield LaSIE-II System as used for MUC-7.Proceedings of the 7th Message UnderstandingConference.J?rvelin, K.  and J. Kek?l?inen.
2000.
IRevaluation methods for retrieving highlyrelevant documents.
Proceedings of the 23rdAnnual International ACM SIGIR Conferenceon Research and Development in InformationRetrieval, , Athens.Kushmerick, N., D. Weld, and R.Doorenbos.
1997.
Wrapper induction forinformation extraction.
Proceedings of 15thInternational Conference on ArtificialIntelligence, IJCAI-97.Manchester, P. 1999.
Survey ?
KnowledgeManagement.
Financial Times, 28.04.99.Maybury, M.. 2001.
Human LanguageTechnologies for Knowledge Management:Challenges and Opportunities.
Workshop onHuman Language Technology and KnowledgeManagement.
Toulouse, France.McMahon, J. G. and  F. J. Smith.
1996Improving Statistical Language ModelsPerformance with Automatically  GeneratedWord Hierarchies.
Computational Linguistics,22(2), 217-247, ACL/MIT.Morin, E. 1999.
Using Lexico-Syntacticpatterns to Extract Semantic Relations betweenTerms from Technical Corpus, TKE 99,Innsbruck, Austria.Muslea, I., S. Minton, and C. Knoblock.1998.
Wrapper induction for semi-structured,web-based information sources.
Proceedings ofthe Conference on Autonomous Learning andDiscovery CONALD-98.Paris, C. , K. Vander Linden.
1996.DRAFTER: An interactive support tool forwriting multilingual instructions, IEEEComputer, Special Issue on Interactive NLP.Reiter, E. 1995.
NLG vs. Templates.Proceedings of the 5th European workshop onnatural language generation, (ENLG-95),Leiden.Reiter, E. , C. Mellish and J. Levine.
1995Automatic generation of technicaldocumentation.
Journal of Applied ArtificialIntelligence,  9(3) 259-287, 1995Sanderson, M.  and B. Croft.
1999.
Derivingconcept hierarchies from text.
Proceedings ofthe 22nd ACM SIGIR Conference, 206-213.Scott, M. 1998.
Focusing on the Text and ItsKey Words.
TALC 98 Proceedings, Oxford,Humanities Computing Unit, Oxford University.Soderland, S. 1999.
Learning informationextraction rules for semi-structured and freetext.
Machine Learning, (1), 1-44.Yangarber, R., R. Grishman, P. Tapanainenand S. Huttunen.
2000.
Automatic Acquisitionof Domain Knowledge for InformationExtraction.
Proceedings of COLING 2000: The18th International Conference onComputational Linguistics, Saarbr?cken.
