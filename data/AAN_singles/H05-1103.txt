Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 819?826, Vancouver, October 2005. c?2005 Association for Computational LinguisticsAutomatic Question Generation for Vocabulary AssessmentJonathan C. Brown Gwen A. Frishkoff Maxine EskenaziLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213  USALearning Research &Development CenterUniversity of PittsburghPittsburgh, PA 15260  USALanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213  USAjonbrown@cs.cmu.edu gwenf@pitt.edu max@cs.cmu.eduAbstractIn the REAP system, users are automati-cally provided with texts to read targeted totheir individual reading levels.
To find ap-propriate texts, the user?s vocabularyknowledge must be assessed.
We describean approach to automatically generatingquestions for vocabulary assessment.
Tra-ditionally, these assessments have beenhand-written.
Using data from WordNet,we generate 6 types of vocabulary ques-tions.
They can have several forms, includ-ing wordbank and multiple-choice.
Wepresent experimental results that suggestthat these automatically-generated ques-tions give a measure of vocabulary skillthat correlates well with subject perform-ance on independently developed human-written questions.
In addition, strong corre-lations with standardized vocabulary testspoint to the validity of our approach toautomatic assessment of word knowledge.1 IntroductionThe REAP system automatically provides userswith individualized authentic texts to read.
Thesetexts, usually retrieved from the Web, are chosento satisfy several criteria.
First, they are selected tomatch the reading level of the student (Collins-Thompson and Callan, 2004).
They must also havevocabulary terms known to the student.
To meetthis goal, it is necessary to construct an accuratemodel of the student?s vocabulary knowledge(Brown and Eskenazi, 2004).
Using this model, thesystem can locate documents that include a givenpercentage (e.g., 95%) of words that are known tothe student.
The remaining percentage (e.g.
5%)consists of new words that the student needs tolearn.
This percentage is controlled so that there isnot so much stretch in the document that the stu-dent cannot focus their attention on understandingthe new words and the meaning of the text.
Afterreading the text, the student?s understanding ofnew words is assessed.
The student?s responses areused to update the student model, to support re-trieval of furture documents that take into accountthe changes in student word knowledge.In this paper, we describe our work on automaticgeneration of vocabulary assessment questions.
Wealso report results from a study that was designedto assess the validity of the generated questions.
Inaddition to the importance of these assessments inthe REAP system, tests of word knowledge arecentral to research on reading and language and areof practical importance for student placement andin enabling teachers to track improvements in wordknowledge throughout the school year.
Becausetests such as these are traditionally hand-written,development is time-consuming and often relies onmethods that are informal and subjective.
The re-search described here addresses these issuesthrough development of automated, explicit meth-ods for generation of vocabulary tests.
In addition,these tools are designed to capture the graded andcomplex nature of word knowledge, allowing formore fine-grained assessment of word learning.2 Measuring Vocabulary KnowledgeWord knowledge is not all-or-none.
Rather, thereare different aspects, such as knowledge of thespoken form, the written form, grammatical behav-819ior, collocation behavior, word frequency, stylisticregister constraints, conceptual meaning, and theassociations a word has with other related words(Nation, 1990).
In this paper, we focus on knowl-edge of conceptual word meaning.
Because wordmeaning itself is complex, our focus is not simplyon all-or-none estimates of vocabulary knowledge,but also on graded and incomplete knowledge ofmeanings that readers possess for different wordsand at different stages of acquisition.Several models have been proposed to accountfor these multiple levels of word knowledge.
Forexample, Dale posited four stages of knowledge ofword meaning (Dale and O?Rourke, 1965).
Instage 1, the subject has never seen the word.
Instage 2, she has seen the word but is unable to ver-balize its meaning.
In stage 3, the subject recog-nizes the word in a given context and has partialword knowledge.
In stage 4, the subject has fullword knowledge, and can explain the word mean-ing so that its usage is clear in multiple contexts.Stahl (1986) proposed a similar model of wordknowledge, the levels of which overlap with Dale?slast two stages.
According to this model, the firstlevel is characterized by association processing, orthe passive association of the new word meaningwith other, familiar concepts.
The second level,comprehension processing, involves active com-prehension of the word in a particular context.
Thethird level, generation processing, requires usageof a word in a novel context reflecting a deep (andmultidimensional) understanding of its meaning.Taking Stahl?s framework as a working model,we constructed multiple types of vocabulary ques-tions designed to assess different ?stages?
or ?lev-els?
of word knowledge.3 Question GenerationIn this section, we describe the process used togenerate vocabulary questions.
After introducingthe WordNet resource we discuss the six questiontypes and the forms in which they appear.
The useof distractors is covered in section 3.3.3.1 WordNetWordNet is a lexical resource in which Englishnouns, verbs, adjectives, and adverbs are groupedinto synonym sets.
A word may appear in a num-ber of these synonym sets, or synsets, each corre-sponding to a single lexical concept and a singlesense of the word (Fellbaum ed., 1998).
The word?bat?
has ten distinct senses and thus appears in tensynsets in WordNet.
Five of these senses corre-spond to noun senses, and the other five corre-spond to verb senses.
The synset for the verb senseof the word which refers to batting one?s eyelashescontains the words ?bat?
and ?flutter?, while thesynset for the noun sense of the word which refersto the flying mammal contains the words ?bat?
and?chiropteran?.
Each sense or synset is accompa-nied by a definition and, often, example sentencesor phrases.
A synset can also be linked to othersynsets with various relations, including synonym,antonym, hypernym, hyponym, and other syntacticand semantic relations (Fellbaum ed., 1998).
For aparticular word sense, we programmatically accessWordNet to find definitions, example phrases, etc.3.2 Question TypesGiven Stahl?s three levels of word mastery and theinformation available in WordNet, we generated 6types of questions: definition, synonym, antonym,hypernym, hyponym, and cloze questions.In order to retrieve data from WordNet, we mustchoose the correct sense of the word.
The systemcan work with input of varying specificity.
Themost specific case is when we have all the data: theword itself and a number indicating the sense ofthe word with respect to WordNet?s synsets.
Whenthe target words are known beforehand and theword list is short enough, the intended sense can behand-annotated.
More often, however, the input iscomprised of just the target word and its part ofspeech (POS).
It is much easier to annotate POSthan it is to annotate the sense.
In addition, POStagging can be done automatically in many cases.In the REAP system, where the user has just read aspecific text, the words of the document were al-ready automatically POS annotated.
When there isonly one sense of the word per part of speech, wecan simply select the correct sense of the word inWordNet.
Otherwise, we select the most frequentlyused sense of the word with the correct POS, usingWordNet?s frequency data.
If we have only theword, we select the most frequent sense, ignoringpart of speech.
Future work will use word sensedisambiguation techniques to automatically deter-mine the correct word sense given a document thatincludes the target word, as in REAP (Brown andEskenazi, 2004).820Once the system has determined the word sense,it can retrieve data from WordNet for each of the 6question types.
The definition question requires adefinition of the word, available in WordNet?sgloss for the chosen sense.
The system chooses thefirst definition which does not include the targetword.
This question should provide evidence forthe first of Stahl?s three levels, association process-ing, although this was not explicitly evaluated.The synonym question has the testee match thetarget word to a synonym.
The system can extractthis synonym from WordNet using two methods.One method is to select words that belong to thesame synset as the target word and are thus syno-nyms.
In addition, the synonym relation in Word-Net may connect this synset to another synset, andall the words in the latter are acceptable synonyms.The system prefers words in the synset to those insynonym synsets.
It also restricts synonyms to sin-gle words and to words which are not morphologi-cal variants of the target word.
When more thanone word satisfies all criteria, the most frequentlyused synonym is chosen, since this should makethe question easier.
This question could be consid-ered either association processing or comprehen-sion processing.
If the testee has seen this synonym(e.g.
as a hint), this question type would requireassociation processing as a word is simply beingassociated with another already-presented word.Otherwise, this may require comprehension proc-essing ?
understanding beyond memorization.The antonym question requires matching a wordwith an antonymous word.
WordNet provides twokinds of relations that can be used to procure anto-nyms: direct and indirect antonyms.
Direct anto-nyms are antonyms of the target word, whereasindirect antonyms are direct antonyms of a syno-nym of the target.
The words ?fast?
and ?slow?
aredirect antonyms of one another.
The word ?quick?does not have a direct antonym, but it does have anindirect antonym, ?slow?, via ?fast?, its synonym.When more than one antonym is available, themost frequently used is chosen.
Unless the testeehas already seen the antonym, this type of questionis normally considered to provide evidence forStahl?s second level, comprehension processing.The hypernym and hyponym questions are simi-lar in structure.
Hypernym is the generic term usedto describe a whole class of specific instances.
Theword ?organism?
is a hypernym of ?person?.
Hy-ponyms are members of a class.
The words?adult?, ?expert?
and ?worker?
are hyponyms of?person?.
For the questions the testee matches thetarget word to either a hypernym or hyponym.
Formore than one possibility, the most frequently usedterm is chosen.
Unless the testee has previouslyseen the hypernym or hyponym, these questionsare normally regarded as providing evidence forStahl?s second level.Cloze is the final question type.
It requires theuse of the target word in a specific context, either acomplete sentence or a phrase.
The example sen-tence or phrase is retrieved from the gloss for aspecific word sense in WordNet.
There is oftenmore than one example phrase.
The system preferslonger phrases, a feature designed to increase theprobability of retrieving complete sentences.
Pas-sages using the target word are preferred, althoughexamples for any of the words in the synset areappropriate.
The present word is replaced by ablank in the cloze question phrase.
Some considera cloze question to be more difficult than any ofthe other question types, but it is still expected toprovide evidence for Stahl?s second level.Although our question types provide evidencefor the highest level of schemes such as Dale?sfour stages, they do not provide evidence forStahl?s highest level, generation processing, wherethe testee must, for instance, write a sentence usingthe word in a personalized context.
We expectquestions that provide evidence of this level to re-quire free-form or near-free-form responses, whichwe do not yet alow.
We expect the six questiontypes to be of increasing difficulty, with definitionor synonym being the easiest and cloze the hardest.3.3 Question FormsEach of the 6 types of questions can be generatedin several forms, the primary ones being wordbankand multiple-choice.
In wordbank, the testee sees alist of answer choices, followed by a set of ques-tions or statements (see Figure 1).
For the defini-tion version, each of the items below the wordbankis a definition.
The testee must select the wordwhich best corresponds to the definition.
For thesynonym and antonym questions, the testee selectsthe word which is the most similar or the most op-posite in meaning to the synonym or antonym.
Forthe hypernym and hyponym question types, thetestee is asked to complete phrases such as ?___ isa kind of person?
(with target ?adult?)
or ?person821is a kind of ___?
(with target ?organism?).
In thecloze question, the testee fills in the blank with theappropriate word.
There is traditionally one ques-tion for each target word in the wordbank.
Thesequestions require no information beyond the targetwords and their definitions, synonyms, hypernyms,etc.Wordbank:verbose   infallible   obdurate   opaqueChoose the word from the wordbank that best completes eachphrase below:1.
___ windows of the jail2.
the Catholic Church considers the Pope ___3.
___ and ineffective instructional methods4.
the child's misery would move even the most ___ heartFig.
1.
Example Wordbank QuestionThe second generated form is multiple-choice,with one question per target word.
The testee seesthe main question, the stem, followed by severalanswer choices, of which only one is correct (seeFigure 2).
Depending on the question type, the tar-get word may appear in either the stem or the an-swer choices.
For the definition question type, thestem holds the definition of the target word andone of the answer choices is the target word.
Forthe word ?verbose?, the stem would be ?using orcontaining too many words?
and the choices ?an-cillary?, ?churlish?, ?verbose?, and ?convivial?.The cloze question is of a similar form, with thestem containing the example sentence or phrasewith a blank where the target word should be used.For ?verbose?, we have the stem ?___ and ineffec-tive instructional methods?
and choices ?verbose?,?incipient?, ?invidious?, and ?titular?.
For thesynonym, antonym, hypernym, and hyponym ques-tions, the target word appears in the stem instead ofthe answer choices.
The synonym question for theword ?verbose?
would have the stem ?Select theword that is most similar in meaning to the wordverbose?
with choices ?inflammable?, ?piping?,matrilineal?, and ?long-winded?.
The antonymquestion would have the stem ?Select the word thatis most opposite in meaning to the word verbose?and the choices ?discernable?, ?concise?, ?unbro-ken?, and ?soused?.
Figure 2 shows a formattedexample of an automatically generated multiple-choice cloze question for the word ?obdurate?.Choose the word that best completes the phrase below:the child's misery would move even the most ___ heartA) torpidB) invidiousC) stolidD) obdurateFig.
2.
Example Multiple-Choice Cloze QuestionTwo issues to consider when creating multiple-choice format questions are the wording or appear-ance of the questions and the criteria for selectionof distractors.
We followed the guidelines for goodmultiple-choice questions described by researcherssuch as Graesser and Wisher (2001).
In accordwith these guidelines, our questions had 4 choices,although the number of choices is a variable sup-plied to the question generation software.
We alsoconsidered the most appropriate wording for thesequestions, leading us to choose stems such as ?Se-lect the word that is most similar in meaning to theword plausible?
for the synonym question ratherthan ?Choose the word that means the same as theword plausible.?
The latter would be problematicwhen the correct answer is a near-synonym ratherthan a word with precisely the same meaning.Concerning distractor choice, the question gen-eration system chooses distractors of the same partof speech and similar frequency to the correct an-swer, as recommended by Coniam (1997).
For thesynonym, antonym, hypernym, and hyponym ques-tions, the correct answer is the highest frequencyword of all the words chosen from WordNet thatsatisfy all the criteria.
Thus, the distractors are ofthe same POS and similar frequency to the syno-nym, antonym, or whatever word is the correctanswer, as opposed to the target word.
The systemchooses distractors from Kilgarriff?s (1995) wordfrequency database, based on the British NationalCorpus (BNC) (Burnage, 1991).
The systemchooses 20 words from this database that are of thesame POS and are equal or similar in frequency tothe correct answer, and randomly chooses the dis-tractors from these words.
Since the distractorsmay be different for each run of the question gen-eration software, slightly different versions of thesame basic question may appear.
The words of theBNC and the word frequency database have beenPOS tagged using the CLAWS tagger (Leech,1994).
This tagger uses detailed POS tags, ena-bling us to choose distractors that are, for instance,822verbs in the past tense, when the correct answer issuch as verb, instead of selecting verbs of un-known tense.
In the definition and cloze questions,the correct answer is the target word itself, so dis-tractors are chosen based on this word.
The systemalso restricts distractors to be in the list of targetwords so that the testee cannot simply choose theword that appears in the stems of other questions.An alternate multiple-choice question format isused when the testee has just read a document us-ing the target word, as in the REAP system (Brownand Eskenazi, 2004).
In this case, the system alsoattempts to finds words which may be semanticallyrelated to the correct answer, as in (Nagy, 1985).This is done by choosing distractors that satisfy thestandard criteria and were present in the document.This should increase the chance that the distractorsare semantically related and eliminate the chancethat a testee will simply select as the correct an-swer the word that appeared in the document theyjust read, without understanding the word meaning.4 Question AssessmentThe validity of the automatically generated vo-cabulary questions was examined in reference tohuman-generated questions for 75 low-frequencyEnglish words.
We compared student performance(accuracy and response time) on the computer andhuman-generated questions.
We focused on theautomatically generated multiple-choice questions,with distractors based on frequency and POS.
Wedid not examine using more complicated strategiesfor picking distractors or assume there was an as-sociated text.
Four of the six computer-generatedquestion types were assessed: the definition, syno-nym, antonym, and cloze questions.
Hypernym andhyponym questions were excluded, since we wereunable to generate a large number of these ques-tions for adjectives, which constitute a large por-tion of the word list.
Subject scores on thecomputer and human-generated assessments werecompared with scores on standardized measures ofreading and vocabulary skill, as described below.4.1 Question CoveragePotential experimental stimuli comprised 156 low-frequency and rare English words that have beenused in previous studies of vocabulary skill in na-tive English-speaking adults.
We first examinedthe percentage of words for which we could gener-ate various question types.
We were unable to gen-erate any questions for 16 of these words, or ~9%of the list, since they were not in WordNet.
Table 1shows the percentage of words for which each ofthe four question types was generated.
All fourquestions were able to be generated for only 75(about half) of the words.
Therefore, the experi-mental word list included only these 75 items.Given the rarity of the words, we predicted that thepercentage of words for which we could generatequestions would be lower than average.
However,we expected that the percentage of words forwhich we could generate synonym and antonymquestions to be higher than average, due to theheavy focus on adjectives in this list.Question type Percentage of QuestionsGeneratedDefinition Question 91%Synonym Question 80%Antonym Question 60%Cloze Question 60%Table 1.
Question Coverage for the 156-Word List4.2 Experiment DesignBehavioral measures of vocabulary knowledgewere acquired for the 75 target words using thefour computer-generated question types describedabove, as well as five human-generated questiontypes.
The human-generated questions were devel-oped by a group of three learning researchers,without knowledge of the computer-generatedquestion types.
Researchers were asked merely todevelop a set of question types that could be usedto assess different levels, or different aspects, ofword knowledge.
Examples of each question type(including distractors) were hand-written for eachof the 75 words.Two of the five human-generated assessments,the synonym and cloze questions, were similar inform to the corresponding computer-generatedquestion types in that they had the same type ofstem and answer.
The other three human-generatedquestions included an inference task, a sentencecompletion task, and a question based on the Os-good semantic differential (Osgood, 1970).
In theinference task, participants were asked to select acontext where the target word could be meaning-fully applied.
For example, the correct response to823the question ?Which of the following is most likelyto be lenitive??
was ?a glass of iced tea,?
and dis-tractors were ?a shot of tequila,?
?a bowl of rice,?and ?a cup of chowder.?
In the sentence comple-tion task, the participant was presented with a sen-tence fragment containing the target word and wasasked to choose the most probable completion.
Forexample, the stem could be ?The music was solenitive?,?
with the correct answer ?
?it wastempting to lie back and go to sleep,?
and with dis-tractors such as ?
?it took some concentration toappreciate the complexity.?
The fifth question typewas based on the Osgood semantic differential, afactor-analytic model of word-level semantic di-mensions (Osgood, 1970).
Numerous studies usingthe Osgood paradigm have shown that variabilityin the semantic ?structure?
of word meanings canlargely be accounted for in terms of three dimen-sions, valence (good?bad), potency (strong?weak),and activity (active?passive).
In our version of theOsgood task, subjects were asked to classify aword such as ?lenitive?
along one of these dimen-sions (e.g., more good or more bad).In addition to the human-generated questions,we administered a battery of standardized tests,including the Nelson-Denny Reading Test, the Ra-ven?s Matrices Test, and the Lexical KnowledgeBattery.
The Nelson-Denny Reading Test is a stan-dardized test of vocabulary and reading compre-hension (Brown, 1981).
The Raven?s Matrices Testis a test of non-verbal reasoning (Raven, 1960).The Lexical Knowledge Battery has multiple sub-sections that test orthographic and phonologicalskills (Perfetti and Hart, 2001).Twenty-one native-English speaking adults par-ticipated in two experiment sessions.
Session 1lasted for about one hour and included the batteryof vocabulary and reading-related assessments de-scribed above.
Session 2 lasted between two andthree hours and comprised 10 tasks, including thefive human and four computer-generated ques-tions.
The experiment began with a confidence-rating task, in which participants indicated with akey press how well they knew the meaning of eachtarget word (on a 1?5 scale).
This task was notspeeded.
For the remaining tasks, subjects wereasked to respond ?as quickly as possible withoutmaking errors.?
Test items for a given questiontype were answered together.
The order of thetasks (question types) and the order of the 75 itemswithin each task were randomized across subjects.4.3 Experiment ResultsWe report on four aspects of this study: participantperformance on questions, correlations betweenquestion types, correlations with confidence rat-ings, and correlations with external assessments.Mean accuracy scores for each question typevaried from .5286 to .6452.
Performance on indi-vidual words and across subjects (averaging acrosswords) varied widely.
The easiest question types(those with the highest average accuracy), were thecomputer-generated definition task and the human-generated semantic differential task, both havingmean accuracy scores of .6452.
The hardest wasthe computer-generated cloze task, with a meanscore of .5286.
The accuracy on computer-generated synonym and antonym questions fallbetween these two limits, with slightly greater ac-curacy on the synonym type.
This implies a gen-eral ordering of difficulty from definition to cloze,as expected.
The accuracies on the other human-generated questions also fall into this range.We also computed correlations between the dif-ferent question types.
Mean accuracies were highlyand statistically significantly correlated across thenine question types (r>.7, p<.01 for all correla-tions).
The correlation between participant accu-racy on the computer-generated synonym and thehuman-generated synonym questions was particu-larly high (r=.906), as was the correlation betweenthe human and computer cloze questions (r= .860).The pattern of correlations for the response-time(RT) data was more complicated and is discussedelsewhere (Frishkoff et al In Prep).
Importantly,RTs for the human versus computer versions ofboth the synonym and cloze questions werestrongly correlated (r>.7, p<.01), just as for theaccuracy results.
The accuracy correlations implythat the computer-generated questions are giving ameasure of vocabulary skill for specific words thatcorrelates well with that of the human-generatedquestions.An item analysis (test item discrimination) wasalso performed.
For each word, scores on a particu-lar question type were compared with the compos-ite test score for that word.
This analysis revealedrelatively low correlations (.12 < r < .25) betweenthe individual question types and the test as awhole (without that question type).
Since the ques-tion types were designed to test different aspects ofvocabulary knowledge, this result is encouraging.824In addition, the average total-score correlationsfor the four computer-generated questions (r=.18)and for the five human-generated questions (r=.19)were not significantly different.
This is positive,since it suggests that the human and computer-generated vocabulary test are accounting for simi-lar patterns of variance across the different ques-tion types.The average correlation between accuracy onthe question types and confidence ratings for a par-ticular word was .265.
This correlation was unex-pectedly low.
This may be because participantsthought they knew these words, but were confusedby their rarity, or because confidence simply doesnot correlate well with accuracy.
Further work isneeded to determine whether confidence ratingscan be accurate predictors of vocabulary knowl-edge.Finally, we examined correlations between par-ticipant performance on the nine question typesand the external assessments.
The correlations be-tween the accuracy on each of the nine questiontypes and the Nelson-Denny vocabulary subtestwere fairly high (.61 < r < .85, p=.01 for all com-parisons).
Thus, both the computer and human-generated questions show good correspondencewith an external assessment of vocabulary skill.Correlations between the accuracy on the questiontypes and the Nelson-Denny reading comprehen-sion test were mixed, showing a higher correlationwith vocabulary than reading comprehension.
Cor-relations between the accuracy on the nine ques-tion types and the Raven?s Matrices test ofnonverbal reasoning were positive, but low and notstatistically significant.
This provides strong evi-dence that the computer-generated vocabularyquestions tap vocabulary knowledge specifically,rather than intelligence in general.5 Related WorkCloze tests are one area of related work.
They wereoriginally intended to measure text readability(Taylor, 1953) since native speakers should be ableto reproduce certain removed words in a readabletext.
Other researchers have used it to assess read-ing comprehension (Ruddell, 1964), with studentsfilling in the blanks, given a high quality text.
Themain issue in automating the creation of cloze testsis determining which words to remove from thetext.
Coniam (1997) examined a several options fordetermining the words to remove and producedrelatively good-quality cloze tests by removingwords with the same POS or similar frequency.Wolfe (1976) automatically generated readingcomprehension questions.
This involved varioustechniques for rewriting sentences into questions,testing syntactic understanding of individual sen-tences.
Of the 50 questions Wolfe was able to gen-erate for a single text, 34 were found to besatisfactory.
More recently, Kunichika (2003) car-ried out work in automatically generating readingcomprehension questions that included both syn-tactic and semantic questions, and was able to gen-erate several different types of questions, includingasking about the content of a sentence, using dic-tionaries of synonyms and antonyms to generatequestions such as ?Is Jane busy??
from sentenceslike ?Jane is free.
?, and testing semantic under-standing across sentence boundaries.
Approx.
93%of the generated questions were found to be satis-factory.Aist (2001) automatically generated factoids toassist students reading.
The factoids gave a syno-nym, an antonym, or a hypernym for the word,which were automatically extracted from Word-Net.
He also automated the creation of a singletype of vocabulary question, with the target wordin the stem and the correct answer a synonym, hy-pernym, or sibling from WordNet.
It is unclearwhat type of vocabulary knowledge this questionwould tap, given the different possible answers.6 ConclusionsExtending our experiments to the question typesthat we have not yet assessed is an important nextstep.
In addition, we want to assess questions indi-vidually, evaluating their use of distractors.
Fi-nally, we need to assess questions generated onword lists with different characteristics.There are also a number of ongoing extensionsto this project.
One is the creation of new questiontypes to test other aspects of word knowledge.
An-other is using other resources such as text collec-tions to enable us to generate more questions perword, especially for the cloze questions.
In addi-tion, we are looking at ways to predict wordknowledge using confidence ratings and morpho-logical and semantic cohorts in situations wherewe cannot perform a standard assessment or cannottest all the vocabulary words we would like to.825In this paper, we have described our work inautomatically generating questions for vocabularyassessment.
We have described the six types ofcomputer-generated questions and the forms inwhich they appear.
Finally, we have presented evi-dence that the computer-generated questions give ameasure of vocabulary skill for individual wordsthat correlates well with human-written questionsand standardized assessments of vocabulary skill.AcknowledgementsThe authors would like to thank Jamie Callan andKevyn Collins-Thompson for their help in this re-search.
The authors would also like to thank EveLanden, Erika Taylor, and Charles Perfetti for theirassistance with experimental stimuli and data col-lection.
This project is supported U.S. Departmentof Education, Institute of Education Sciences,Award #R305G030123, and the APA/IES Postdoc-toral Education Research Training fellowshipawarded by the Department of Education, Instituteof Education Sciences, Grant #R305U030004.
Anyopinions, findings and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofthe U.S. Department of Education.ReferencesGregory Aist.
2001.
Towards automatic glossarization:automatically constructing and administering vo-cabulary assistance factoids and multiple-choice as-sessment, International Journal of AI in Ed., 2001.James Brown, J. M. Bennett, and Gerald Hanna.
1981.The Nelson-Denny Reading Test.
Chicago: The Riv-erside Publishing Company.Jonathan Brown and Maxine Eskenazi.
2004.
Retrievalof Authentic Documents for Reader-Specific LexicalPractice.
In Proceedings of InSTIL/ICALL Sympo-sium 2004.
Venice, Italy, 2004.Gavin Burnage.
1991.
Text Corpora and the British Na-tional Corpus.
Computers & Texts 2, Nov, 1991.Kevyn Collins-Thompson and Jamie Callan.
2004.
Alanguage modeling approach to predicting readingdifficulty.
In Proceedings of the HLT/NAACL 2004Conference.
Boston, 2004.David Coniam.
1997.
A preliminary inquiry into usingcorpus word frequency data in the automatic genera-tion of English language cloze tests.
CALICO Jour-nal, Volume 14, No.
2.Edgar Dale and Joseph O?Rourke.
1986.
Vocabularybuilding.
Columbus, Ohio: Zaner-Bloser.Christiane Fellbaum, Ed.
1998.
WordNet.
An electroniclexical database.
Ed.
by Christiane Fellbaum, prefaceby George Miller.
Cambridge, MA: MIT Press; 1998.Arthur C. Graesser, R. A. Wisher.
2001.
Question Gen-eration as a Learning Multiplier in DistributedLearning Environments.
Army research inst for thebehavioral and social sciences Alexandria VA. Re-port number A654993, 2001.Adam Kilgarriff.
1995.http://www.itri.brighton.ac.uk/~Adam.Kilgarriff/bnc-readme.htmlH.
Kunichika, T. Katayama, T. Hirashima, and A. Ta-keuchi.
2003.
Automated question generation meth-ods for intelligent English learning systems and itsevaluation.
Proceedings of ICCE2004, Hong Kong.G.
Leech, R. Garside, and M. Bryant.
1994.
CLAWS4:The tagging of the British National Corpus.
In Proc.of 15th International Conference on ComputationalLinguistics, Kyoto, Japan, 622-628, 1994.W.E.
Nagy, P.A.
Herman, and R.C.
Anderson.
1985.Learning words from context.
Reading ResearchQuarterly, 20, 233-253.Paul Nation.
1990.
Teaching and learning vocabulary.Rowley, MA: Newbury House.Charles E. Osgood, P. H. Tannenbaum, and G. J. Suci.1957.
The Measurement of Meaning.
Urbana: Uni-versity of Illinois Press.Charles A. Perfetti, and Lesley Hart.
2001.
The lexicalquality hypothesis.
In L. Verhoeven, C. Elbro & P.Reitsma (Eds.
), Precursors of Functional Literacy(Vol.
11, pp.
67?86).
Amsterdam: John Benjamins.J.C.
Raven.
1960.
Progressive matrices, standard.
SanAntonio, TX: Psychological Corporation.R.
B. Ruddell.
1964.
A study of the cloze comprehen-sion technique in relation to structurally controlledreading material.
Improvement of Reading ThroughClassroom Practice, 9, 298-303.Steven A. Stahl.
1986.
Three principals of effective vo-cabulary instruction.
Journal of Reading, 29.W.L.
Taylor.
1953.
Cloze procedure: a new tool formeasuring readability.
Journalism Quarterly, 30.John H. Wolfe.
1976.
Automatic question generationfrom text - an aid to independent study.
ACMSIGCUE Bulletin, 2(1), 104-112.826
