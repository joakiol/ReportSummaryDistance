Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134?137,Sydney, July 2006. c?2006 Association for Computational LinguisticsOn Closed Task of Chinese Word Segmentation: An Improved CRFModel Coupled with Character Clustering andAutomatically Generated Template MatchingRichard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,Hong-Jie Dai, and Wen-Lian HsuIntelligent Agent Systems LabInstitute of Information Science, Academia SinicaNo.
128, Sec.
2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C.
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.twAbstractThis paper addresses two major prob-lems in closed task of Chinese wordsegmentation (CWS): tagging sentencesinterspersed with non-Chinese words,and long named entity (NE) identifica-tion.
To resolve the former, we apply K-means clustering to identify non-Chinesecharacters, and then adopt a two-taggerarchitecture: one for Chinese text and theother for non-Chinese text.
For the latterproblem, we apply postprocessing to ourCWS output using automatically gener-ated templates.
The experiment resultsshow that, when non-Chinese charactersare sparse in the training corpus, ourtwo-tagger method significantly im-proves the segmentation of sentencescontaining non-Chinese words.
Identifi-cation of long NEs and long words isalso enhanced by template-based post-processing.
In the closed task ofSIGHAN 2006 CWS, our systemachieved F-scores of 0.957, 0.972, and0.955 on the CKIP, CTU, and MSR cor-pora respectively.1 IntroductionUnlike Western languages, Chinese does nothave explicit word delimiters.
Therefore, wordsegmentation (CWS) is essential for Chinesetext processing or indexing.
There are two mainproblems in the closed CWS task.
The first is toidentify and segment non-Chinese word se-quences in Chinese documents, especially in aclosed task (described later).
A good CWS sys-tem should be able to handle Chinese texts pep-pered with non-Chinese words or phrases.
Sincenon-Chinese language morphologies are quitedifferent from that of Chinese, our approachmust depend on how many non-Chinese wordsappear, whether they are connected with eachother, and whether they are interleaved withChinese words.
If we can distinguish non-Chinese characters automatically and apply dif-ferent strategies, the segmentation performancecan be improved.
The second problem in closedCWS is to correctly identify longer NEs.
MostML-based CWS systems use a five-charactercontext window to determine the current charac-ter?s tag.
In the majority of cases, given the con-straints of computational resources, this com-promise is acceptable.
However, limited by thewindow size, these systems often handle longwords poorly.In this paper, our goal is to construct a generalCWS system that can deal with the above prob-lems.
We adopt CRF as our ML model.2 Chinese Word Segmentation System2.1 Conditional Random FieldsConditional random fields (CRFs) are undirectedgraphical models trained to maximize a condi-tional probability (Lafferty et al, 2001).
A lin-ear-chain CRF with parameters ?={?1, ?2, ?
}defines a conditional probability for a state se-quence y = y1 ?yT , given that an input sequencex = x1 ?xT  is???????
?= ??=?
?Tt kttkk tyyfZP11 ),,,(exp1)|( xxyx?
,(1)where Zx is the normalization factor that makesthe probability of all state sequences sum to one;fk(yt-1, yt, x, t) is often a binary-valued featurefunction and ?k is its weight.
The feature134functions can measure any aspect of a statetransition, yt-1?yt, and the entire observationsequence, x, centered at the current time step, t.For example, one feature function might havethe value 1 when yt-1 is the state B, yt is the stateI, and tx  is the character ??
?.2.2 Character ClusteringIn many cases, Chinese sentences may be inter-spersed with non-Chinese words.
In a closedtask, there is no way of knowing how many lan-guages there are in a given text.
Our solution isto apply a clustering algorithm to find homoge-neous characters belonging to the same characterclusters.
One general rule we adopted is that alanguage?s characters tend to appear together intokens.
In addition, character clusters exhibitcertain distinct properties.
The first property isthat the order of characters in some pairs can beinterchanged.
This is referred to as exchange-ability.
The second property is that some charac-ters, such as lowercase characters, can appear inany position of a word; while others, such asuppercase characters, cannot.
This is referred toas location independence.
According to the gen-eral rule, we can calculate the pairing frequencyof characters in tokens by checking all tokens inthe corpus.
Assuming the alphabet is ?, we firstneed to represent each character as a |?|-dimensional vector.
For each character ci, we usevj to represent its j-dimension value, which iscalculated as follows:rjiijj ffv )],)[min(1( ??
?+= ?
(2),where fij denotes the frequency with which ci andcj appear in the same word when ci?s positionprecedes that of cj.
We take the minimum valueof fij and fji because even when ci and cj have ahigh co-occurrence frequency, if either fij or fji islow, then one order does not occur often, so vj?svalue will be low.
We use two parameters tonormalize vj within the range 0 to 1; ?
is used toenlarge the gap between non-zero and zero fre-quencies, and ?
is used to weaken the influenceof very high frequencies.Next, we apply the K-means algorithm togenerate candidate cluster sets composed of Kclusters (Hartigan et al, 1979).
Different K?s,?
?s, and ?
?s are used to generate possible charac-ter cluster sets.
Our K-means algorithm uses thecosine distance.After obtaining the K clusters, we need to se-lect the N1 best character clusters among them.Assuming the angle between the cluster centroidvector and (1, 1, ... , 1) is ?, the cluster with thelargest cosine ?
will be removed.
This is becausecharacters whose co-occurrence frequencies arenearly all zero will be transformed into vectorsvery close to (?, ?, ... , ?
); thus, their centroidswill also be very close to (?, ?, ... , ?
), leading tounreasonable clustering results.After removing these two types of clusters,for each character c in a cluster M, we calculatethe inverse relative distance (IRDist) of c using(3):??????????=?
),cos(),(coslog)IRDist(mcmccii   ,            (3)where mi stands for the centroid of cluster Mi,and m stands for the centroid of M.We then calculate the average inverse dis-tance for each cluster M. The N1 best clusters areselected from the original K clusters.The above K-means clustering and charactercluster selection steps are executed iterativelyfor each cluster set generated from K-meansclustering with different K?s, ?
?s, and ?
?s.After selecting the N1 best clusters for eachcluster set, we pool and rank them according totheir inner ratios.
Each cluster?s inner ratio iscalculated by the following formula:???
?= ?jijiccjiMccjiccccM,,),occurence(co),occurence(co)inner(,   (4)where co-occurrence(ci, cj) denotes the fre-quency with which characters  ci and cj co-occurin the same word.To ensure that we select a balanced mix ofclusters, for each character in an incoming clus-ter M, we use Algorithm 1 to check if the fre-quency of each character in C?M is greaterthan a threshold ?.Algorithm 1 Balanced Cluster SelectionInput: A set of character clusters P={M1 ,  .
.
.
, MK}Number of selections N2,Output: A set of clusters Q={ '1M  ,  .
.
.
,'2NM }.1: C={}2: sort the clusters in P by their inner ratios;3: while |C|<=N2 do4:     pick the cluster M that has highest inner ratio;5:     for each character c in M do6:          if the frequency of c in C?M is over thresh-old ?7:                 P?P?M;8:                 continue;9 :        else13510:               C?C?M;11:               P?P?M;12:        end;13:   end;14: endThe above algorithm yields the best N1 clus-ters in terms of exchangeability.
Next, we exe-cute the above procedures again to select thebest  N2 clusters based on their location inde-pendence and exchangeability.
However, foreach character ci, we use vj to denote the value ofits j-th dimension.
We calculate vj as follows:rjijiijijj ffffv )]',,',)[min(1(' ??
?+= ,      (5)where ijf  stands for the frequency with which ciand cj appear in the same word when ci is thefirst character; and f?ij stands for the frequencywith which ci and cj co-occur in the same wordwhen ci precedes cj  but not in the first position.We choose the minimum value from ijf , f?ij, jif ,and f?ji  because if ci and cj both appear in thefirst position of a word and their order is ex-changeable, the four frequency values, includingthe minimum value, will all be large enough.Type Cluster Inner (K, ?, ?
),.0123456789 0.94 (10, 0.60, 0.16)EX-/ABCDEFGHIKLMNOPRSTUVWabcdefghiklmnoprstuvwxy0.93 (10, 0.70, 0.16)?
?ABCDEFGHIKLMNOPRSTUVWabcdefghiklmnoprstvwxy0.84 (10, 0.50, 0.25)EL??????????
0.76 (10, 0.50, 0.26)Table 1.
Clustering Results of the CTU corpusOur next goal is to create the best hybrid ofthe above two cluster sets.
The set selected forexchangeability is referred to as the EX set,while the set selected for both exchangeabilityand location independence is referred to as theEL set.
We create a development set and use thebest first strategy to build the optimal cluster setfrom EX?EL.
The EX and EL for the CTUcorpus are shown in Table 1.2.3 Handling Non-Chinese WordsNon-Chinese characters suffer from a seriousdata sparseness problem, since their frequenciesare much lower than those of Chinese characters.In bigrams containing at least one non-Chinesecharacter (referred as non-Chinese bigrams), theproblem is more serious.
Take the phrase ???20??
(about 20 years old) for example.
?2?
isusually predicted as I, (i.e., ????
is connectedwith ?2?)
resulting in incorrect segmentation,because the frequency of ?2?
in the I class ismuch higher than that of ?2?
in the B class, eventhough the feature C-2C-1=????
has a highweight for assigning ?2?
to the B class.Traditional approaches to CWS only use onegeneral tagger (referred as the G tagger) forsegmentation.
In our system, we use two CWStaggers.
One is a general tagger, similar to thetraditional approaches; the other is a specializedtagger designed to deal with non-Chinese words.We refer to the composite tagger (the generaltagger plus the specialized tagger) as the GStagger.Here, we refer to all characters in the selectedclusters as non-Chinese characters.
In the devel-opment stage, the best-first feature selector de-termines which clusters will be used.
Then, weconvert each sentence in the training data andtest data into a normalized sentence.
Each non-Chinese character c is replaced by a cluster rep-resentative symbol ?M, where c is in the clusterM.
We refer to the string composed of all ?M asF.
If the length of F is more than that of W, itwill be shortened to W. The normalized sentenceis then placed in one file, and the non-Chinesecharacter sequence is placed in another.
Next,we use the normalized training and test file forthe general tagger, and the non-Chinese se-quence training and test file for the specializedtagger.
Finally, the results of these two taggersare combined.The advantage of this approach is that it re-solves the data sparseness problem in non-Chinese bigrams.
Consider the previous examplein which ?
stands for the numeral cluster.
Sincethere is a phrase ???
8??
in the training data,C-1C0= ??
8?
is still an unknown bigram usingthe G tagger.
By using the GS tagger, however,???
20??
and ???
8??
will be convertedas ???
????
and ???
??
?, respectively.Therefore, the bigram feature C-1C0=??
??
is nolonger unknown.
Also, since ?
in ??
??
istagged as B, (i.e., ???
and ???
are separated),???
and ???
will be separated in  ???
???
?.2.4 Generating and Applying TemplatesTemplate GenerationWe first extract all possible word candidatesfrom the training set.
Given a minimum wordlength L, we extract all words whose length isgreater than or equal to L, after which we alignall word pairs.
For each pair, if more than fifty136percent of the characters are identical, a templatewill be generated to match both words in the pair.Template FilteringWe have two criteria for filtering the extractedtemplates.
First, we test the matching accuracyof each template t on the development set.
Thisis calculated by the following formula:strings matched all of #separators no with strings matched of #)( =tA .In our system, templates whose accuracy islower than the threshold ?1 are discarded.
For theremaining templates, we apply two differentstrategies.
According to our observations of thedevelopment set, most templates whose accu-racy is less than ?2 are ineffective.
To refine suchtemplates, we employ the character class infor-mation generated by character clustering to im-pose a class limitation on certain template slots.This regulates the potential input and improvesthe precision.
Consider a template with one ormore wildcard slots.
If any string matched withthese wildcard slots contains characters in dif-ferent clusters, this template is also discarded.Template-Based Post-Processing (TBPP)After the generated templates have been filtered,they are used to match our CWS output andcheck if the matched tokens can be combinedinto complete words.
If a template?s accuracy isgreater than ?2, then all separators within thematched strings will be eliminated; otherwise,for a template t with accuracy between ?1 and ?2,we eliminate all separators in its matched stringif no substring matched with t?s wildcard slotscontains characters in different clusters.
Resul-tant words of less than three characters in lengthare discarded because CRF performs well withsuch words.3 Experiment3.1 DatasetWe use the three larger corpora in SIGHANBakeoff 2006: a Simplified Chinese corpus pro-vided by Microsoft Research Beijing, and twoTraditional Chinese corpora provided by Aca-demia Sinica in Taiwan and the City Universityof Hong Kong respectively.
Details of each cor-pus are listed in Table 2.Training Size Test SizeCorpusTypes Words Types WordsCKIP 141 K 5.45 M 19 K 122 KCity University (CTU) 69 K 1.46 M 9 K 41 KMicrosoft Research (MSR) 88 K 2.37 M 13 K 107 KTable 2.
Corpora Information3.2 ResultsTable 3 lists the best combination of n-gram fea-tures used in the G tagger.Uni-gram BigramC-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1Table 3.
Best Combination of N-gram FeaturesTable 4 compares the baseline G tagger and theenhanced GST tagger.
We observe that the GSTtagger outperforms the G tagger on all three cor-pora.Conf R P F ROOV RIVCKIP-g 0.958 0.949 0.954 0.690 0.969CKIP-gst 0.961 0.953 0.957 0.658 0.974CTU-g 0.966 0.967 0.966 0.786 0.973CTU-gst 0.973 0.972 0.972 0.787 0.981MSR-g 0.949 0.957 0.953 0.673 0.959MSR-gst 0.953 0.956 0.955 0.574 0.966Table 4 Performance Comparison of the G Tag-ger and the GST Tagger4 ConclusionThe contribution of this paper is two fold.
First,we successfully apply the K-means algorithm tocharacter clustering and develop several clusterset selection algorithms for our GS tagger.
Thissignificantly improves the handling of sentencescontaining non-Chinese words as well as theoverall performance.
Second, we develop a post-processing method that compensates for theweakness of ML-based CWS on longer words.ReferencesHartigan, J.
A., & Wong, M. A.
(1979).
A K-meansClustering Algorithm.
Applied Statistics, 28, 100-108.Lafferty, J., McCallum, A., & Pereira, F. (2001).Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data.
Pa-per presented at the ICML-01.137
