The Acquisition of Lexical SemanticKnowledge from Large CorporaJames PustejovskyComputer  Sc ience Depar tmentBrande is  Un ivers i tyWal tham,  MA 02254ABSTRACTMachine-readable dictionaries provide the raw material fromwhich to construct computationaily useful representations ofthe generic vocabulary contained within it.
Many sublan-guages, however, are poorly represented in on-line dictionar-ies, ff represented at all.
Vocabularies geared to specializeddomains are necessary for many applications, such as textcategorization and information retrieval.
In this paper I de-scribe research devoted to developing techniques for build-ing sublanguage l xicons via syntactic and statistical corpusanalysis coupled with analytic techniques based on the tenetsof a generative l xicon.1.
In t roduct ionMachine-readable dictionaries provide the raw materialfrom which to construct computationally useful repre-sentations of the generic vocabulary contained within it.Many sublanguages, however, are poorly represented inon-line dictionaries, if represented at all (cf.
Grishmanet al(1986)).
Yet vocabularies geared to specialized o-mains are necessary for many applications, uch as textcategorization and information retrieval.
In this paperI describe research devoted to developing techniques forbuilding sublanguage l xicons via syntactic and statis-tical corpus analysis coupled with analytic techniquesbased on the tenets of a generative theory of the lexicon(Pustejovsky 1991).Unlike with purely statistical collocational analyses, theframework of a lexical semantic theory allows the auto-matic construction of predictions about deeper seman-tic relationships among words appearing in collocationalsystems.
I illustrate the approach for the acquisition oflexical information for several exical classes, and howsuch techniques can fine tune the lexical structures ac-quired from an initial seeding of a machine-readabledictionary, i.e.
the machine-tractable v rsion of theLDOCE (Wilks et al(1991)).The aim of our research is to discover what kinds ofknowledge can be reliably acquired through the use ofthese methods, exploiting, as they do, general inguis-tic knowledge rather than domain knowledge.
In thisrespect, our program is similar to Zernik (1989) andZernik and Jacobs (1990), working on extracting verbsemantics from corpora using lexical categories.
Ourresearch, however, differs in two respects: first, weemploy a more expressive lexical semantics; secondly,our focus is on all major categories in the language,and not just verbs.
This is important since for full-text information retrieval, information about nominalsis paramount, as most queries tend to be expressed asconjunctions of nouns.
From a theoretical perspective,I believe that the contribution of the lexical semanticsof nominals to the overall structure of the lexicon hasbeen somewhat neglected, relative to that of verbs (cf.Pustejovsky and Anick (1988), Bogutaev and Puste-jovsky (1990)).
Therefore, where others present ambi-guity and metonymy as a potential obstacle to effectivecorpus analysis, we believe that the existence of moti-vated metonymic structures actually provides valuableclues for semantic analysis of nouns in a corpus.
Todemonstrate these points, I describe experiments per-formed within the DIDEROT Tipster Extraction project(of Brandeis University and New Mexico State Univer-sity), over a corpus of joint venture articles.2.
P ro jec t ing  Syntact i c  Behav ior  f romDeep Semant ic  TypesThe purpose of the research is to experiment with au-tomatic acquisition of semantic tags for words in a sub-language, tags which are well beyond that available fromthe seeding of MRDs.
The identification of semantic tagsfor a word associated with particular lexical forms (i.e.semantic ollocations) can be represented as that partof the lexical structure of a word called the projectiveconclusion space (Pustejovsky (1991))).For this work, we will need to define several semantic no-tions.
These include: type coercion, where a lexical itemrequires a specific type specification for its argument,and the argument is able to change type accordingly- - this  explains the behavior of logical metonymy andthe syntactic variation seen in complements o verbs andnominals; cospecification, a semantic tagging of what col-locationM patterns the lexical item may enter into; andcontextual opacity/transparency, which characterizes ofa243word just how it is used in particular contexts.
Formally,we will identify this property with specific cospecificationvalues for the lexical item (cf.
Pustejovsky (forthcom-ing)).Metonymy, in this view, can be seen as a case of the"licensed violation" of selectional restrictions.
For ex-ample, while the verb announce selects for a humansubject, sentences like The Dow Corporation announcedthird quarter losses are not only an acceptable para-phrase of the selectionally correct form Mr. Dow arr.
an-nounced third quarter losses for Dow Corp, but they arethe preferred form in the corpora being examined (i.e.the ACL-DCI WSJ and T IPSTER Corpora).
This is anexample of subject type coercion, where the semanticsfor Dow Corp. as a company must specify that thereis a human typically associated with such official pro-nouncements (Bergler (forthcoming)).2.1.
Coercive Envi ronments  in CorporaAnother example of type coercion is that seen in the com-plements of verbs such as begin, enjoy, finish, etc.
Thatis, in sentences such as "John began the book", the nor-mal complement expected is an action or event of somesort, most often expressed by a gerundive or infinitivalphrase: "John began reading the book", "John began toread the book".
In Pustejovsky (1991) it is argued thatin such cases, the verb need not have multiple subcatego-rizations, but only one deep semantic type, in this case,an event.
Thus, the verb 'coerces' its complement (e.g.
"the book") into an event related to that object.
Suchinformation can be represented by means of a represen-tational schema called qualia structure, which, amongother things, specifies the relations associated with ob-jects.In related work being carried out with Mats Rooth ofATT, we are exploring what the range of coercion typesis, and what environments hey may appear in, as dis-covered in corpora.
Some of our initial data suggest hatthe hypothesis of deep semantic selection may in fact becorrect, as well as indicating what the nature of the coer-cion rules may be.
Using techniques described in Churchand Hindle (1990), Church and Hanks (1990), and Hin-dle and Rooth (1991), below are some examples of themost frequent V-O pairs from the AP corpus.Counts for "objects" of begin/V:205 begin/V career /O176 begin/V day/O159 begin/V work/O140 begin/V talk/O120 begin/V campaign/O113 begin/V investigation/O106 begin/V process/O92 begin/V program/O8S begin/V operation/O86 begin/V negotiation/O66 begin/V strike/O64 begin/V production/O59 begin/V meeting/O89 begin/V term/O50 begin/V visit/O45 begin/V test/O39 begin/V construction/O31 begin/V debate/O29 begin/V trial/OCorpus studies confirm similar results for "weakly inten-sional contexts" (Pustejovsky (1991)) such as the com-plement of coercive verbs such as veto.
These are in-teresting because regardless of the noun type appearingas complement, it is embedded within a semantic in-terpretation of "the proposal to", thereby clothing thecomplement within an intensional context.
The exam-ples below with the verb veto indicate two things: first,that such coercions are regular and pervasive in corpora;secondly, that almost anything can be vetoed, but thatthe most frequently occurring objects are closest to thetype selected by the verb.303 veto/V bi11/O84 veto/V legislation/O58 veto/V measure/O36 veto/V resolution/O21 veto/V law/O14 veto/V item/O12 veto/V decision/O9 veto/V proposal/O9 veto/V plan/O7 veto/V package/O6 veto/V increase/O5 veto/V sanction/O6 veto/V penalty/O4 veto/V notice/O4 veto/V idea/O4 veto/V appropriation/O4 veto/V mission/O4 veto/V attempt/O3 veto/V search/O3 veto/V cut/O3 veto/V deal/OI veto/V expedition/OWhat these data "show is that the highest count comple-ment types match the type required by the verb; namely,that one vetoes a bill or proposal to do something, not244the thing itself.
These nouns can therefore be used withsome predictive certainty for inducing the semantic typein coercive environments such as "veto the expedition.
"This work is still preliminary, however, and requires fur-ther examination (Pustejovsky and Rooth (in prepara-tion)).3.
Imp l i ca t ions  fo r  Natura l  LanguageProcess ingThe framework proposed here is attractive for NLP, forat least two reasons.
First, it can be formalized, and thusmake the basis for a computational procedure for wordinterpretation i  context.
Second, it does not require thenotion of exhaustive enumeration ofall the different waysin which a word can behave, in particular in collocationswith other words.
Consequently, the framework can nat-urally cope with the 'creative' use of language; that is,the open-ended nature of word combinations and theirassociated meanings.The method of fine-grained characterization f lexical en-tries, as proposed here, effectively allows us to conflatedifferent word senses (in the traditional meaning of thisterm) into a single meta-entry, thereby offering greatpotential not only for systematically encoding regulari-ties of word behavior dependent on context, but also forgreatly reducing the size of the lexicon.
Following Puste-jovsky and Anick (1988), we call such meta-entries lexi-cal conceptuM paradigms (LCPs).
The theoretical claimhere is that such a characterization constrains what apossible word meaning can be, through the mechanismof logically well-formed semantic expressions.
The ex-pressive power of a KR formalism can then be viewed assimply a tool which gives substance to this claim.The notion of a meta-entry turns out to be very use-ful for capturing the systematic ambiguities which areso pervasive throughout language.
Among the alterna-tions captured by LCPs are the following (see Puste-jovsky (forthcoming) and Levin (1989)):1.
Count/Mass alternations; e.g.
sheep.2.
Container/Containee alternations; e.g.
bottle.3.
Figure/Ground Reversals; e.g.
door, window.4.
Product/Producer diathesis; e.g.
newspaper, IBM,Ford.For example, an apparently unambiguous noun such asnewspaper can appear in many semantically distinct con-texts.1.
The coffee cup is on top of the newspaper.2.
The article is in the newspaper.3.
The newspaper attacked the senator from Mas-sachusetts.4.
The newspaper is hoping to fire its editor nextmonth.This noun falls into a particular specialization of theProduct/Producer paradigm, where the noun can logi-cally denote either the organization or the product pro-duced by the organization.
This is another example oflogical polysemy and is represented in the lexical struc-ture for newspaper explicitly (Pustejovsky (1991)).Another class of logically polysemous nominals is a spe-cialization of the process/result nominals uch as merger,joint venture, consolidation, etc.
Examples of how thesenominals pattern syntactically in text are given below:1.
Trustcorp Inc. will become Society Bank 8J Trustwhen its merger is completed with Society Corp. ofCleveland, the bank said.2.
Shareholders must approve the merger at generalmeetings of the two companies in late November.3.
But Mr. Rey brought about a merger in the next fewyears between the country's major producers.4.
A pharmaceutical joint venture of Johnson ~4 John-son and Merck agreed in principle to buy the U.S.over-the-counter drug business of ICI Americas forover $450 million.5.
The four-year-old business is turning a smallprofit and the entrepreneurs are about to sign ajoint venture agreement with a Moscow cooperativeto export the yarn to the Soviet Union.Because of their semantic type, these nominals enter intoan LCP which generates a set of structural templatespredicted for that noun in the language.
For example,the LCP in this case is the union concept, and has thefollowing lexical structure associated with it:5.
Plant/Food alternations; e.g.
fig, apple.6.
Process/Result diathesis; e.g.
ezamination, combi-nation.7.
Place/People diathesis; e.g.
city, New York.LCP: type: union\[ Const: >2x:entity(x) \]\[ Form: exist(ly) \[entity(y)\] \]\[ Agent: type:event ?
join(x) \]\[ Telic: nil \]245This states that a union is an event which brings aboutone entity from two or more, and comes about by a join-ing ewmt.
The lexical structure for the nominal mergeris inherited from this paradigm.=erger(*x*)\[ Const: ({w}>2) \[compamy(w) or firm(w)\] \]\[ Form: ex is ts (y )  \[company(y)\] \]\[ Agent: event(*x*): join(*x*,{~}) \]\[ Telie: contextual\]It is interesting to note that all synonyms for this word(or, alternatively, viewed as clustered under this con-cept) will share in the same LCP behavior: e.g.
merging,unification, coalition, combination, consolidation, etc.With this LCP there are associated syntactic realizationpatterns for how the word and its arguments are real-ized in text.
Such a paradigm is a very generic, domainindependent set of schemas, which is a significant pointfor multi-domain and multi-task NLP applications.For the particular LCP of union, the syntactic schemasinclude the following:LCP schemas:\[where N=UNION; X=argl; Y=arg2\]N of X and YX's N with YY's N with XN between X and YN of Z (Z=X+Y)N between ZEXAMPLE:merger of x and yx's merger with yy's merger with xmerger between x and ymerger of the two companiesmerger between two companiesThere are several things to note here.
First, suchparadigmatic behavior is extremely regular for nounsin a language, and as a result, the members of suchparadigms can be found using knowledge acquisitiontechniques from large corpora (cf.
Anick and Puste-jovsky (1990) for one such algorithm).
Secondly, be-cause these are very common ominal patterns for nounssuch as merger, it is significant when the noun appearswithout all arguments explicitly expressed.
For example,in (5) below, presuppositions from the lexical structurecombine with discourse clues in the form of definite refer-ence in the noun phrase (the merger) to suggest that theother partner in the merger was mentioned previously inthe text.5.
Florida National said yesterday that it remains com-mitted to the merger.Similarly powerful inferences can be made from an in-definite nominal when introduced into the discourse asin (6).
Here, there is a strong presupposition that bothpartners in the merger are mentioned someplace in theimmediately ocal context, e.g.
as a coordinate subject,since the NP is a newly mentioned entity.6.
Orkem and Coates said last Wednesday that the twowere considering a merger, through Orkem's Britishsubsidiary, Orkem Coatings U.K. Lid.Thus, the lexical structures provide a rich set of schemasfor argument mapping and semantic inferencing, as wellas directed presuppositions for discontinuous semanticrelations.One final and important note about lexical structuresand paradigmatic behavior.
The seed informationfor these structures is largely derivable from machine-readable dictionaries.
For example, a dictionary defini-tion for merger (from the Longman Dictionary of Con-temporary English is "the joining of 2 or more companiesor firms" with subject code FINANCE.
This makes thetask of automatic onstruction of a robust lexicon forNLP applications a very realizable goal (cf.
Boguraev(1991) and Wilks et ai.
(1991)).4.
Induction of Semantic Relations fromSyntactic FormsFrom discussion in the previous section, it should beclear that such paradigmatic information would be help-ful if available.
In this section, we present preliminaryresults indicating the feasability of learning LCPs fromcorpora, both tagged and untagged.
Imagine being ableto take the V-O pairs such as those given in section 2.1,and then applying semantic tags to the verbs which areappropriate to the role they play for that object (i.e.
in-duction of the qualia roles for that noun).
This is in factthe type of experiment reported on in Anick and Puste-jovsky (1990).
Here we apply a similar technique to amuch larger corpus, in order to induce the agentive rolefor nouns.
That is, the semantic predicate associatedwith bringing about the object.In this example we look at the behavior of noun phrases246and the prepositional phrases that follow them.
In par-ticular, we look at the co-occurrence ofnominals with be-tween, with, and to.
Table 1 shows results of the conflat-ing verb/noun plus preposition patterns.
The percent-age shown indicates the ratio of the particular colloca-tion to the key word.
Mutual information (MI) statisticsfor the two words in collocation are also shown.
Whatthese results indicate is that induction of semantic typefrom conflating syntactic patterns is possible.
Based onthe semantic types for these prepositions, the syntacticevidence suggests that there is a symmetric relation be-tween the arguments in the following two patterns:a.
Z with y = Ax3Rz, y\[Rz(x, y) A Rz(y, x)\]b.
Z between x and y =3Rz, x, y\[Rz(x, y) ^  Rz(y, x)\]We then take these results and, for those nouns wherethe association ratios for N with and N between are sim-ilar, we pair them with the set of verbs governing these"NP PP" combinations in corpus, effectively partition-ing the original V-O set into \[+agentive\] predicates and\[-agentive\] predicates.
If our hypothesis i correct, weexpect hat verbs governing nominals collocated with awith-phrase will be mostly those predicates referring tothe agentive quale of the nominal.
This is because thewith-phrase is unsaturated as a predicate, and acts toidentify the agent of the verb as its argument.
This isconfirmed by our data, shown below.Verb-Object Pairs with Prep = to19 form/V venture/O3 announce/V venture/O3 enter/V venture/O2 discuss/V venture/O1 be/V venture/O1 abandon/V venture/OI begin/V venture/OI ?omplete/V venture/OI negotiate/V venture/O1 start/V venture/O1 expect/V venture/OConversely, verbs governing nominals collocating witha between-phrase will not refer to the agentive since thephrase is saturated already.
Indeed, the only verb occur-ring in this position with any frequency is the copula be,namely with the following counts: 12 be/Y venture/0.Thus, week semantic types can be induced on the basis ofsyntactic behavior.
In Pustejovsky et al(1991), we dis-cuss how this general technique compares to somewhatdifferent but related approaches described in Smadja(1991) and Zernik and Jacobs (1991).5.
Conc lus ionWe contend that using lexical semantic methods toguide lexical knowledge acquisition from corpora canyield structured thesaurus-like information in a formamenable for use within information retrieval applica-tions.
The work reported here illustrates the applica-bility of this approach for several important classes ofnominals.
Future work includes refining the discoveryprocedures to reduce misses and false alarms and ex-tending the coverage of the lexical semantics componentto allow the testing of such techniques on a greater rangeof terms.
Finally, we are applying the results of the anal-ysis within the context of data extraction for IR, to testtheir effectiveness as indexing and retrieval aids.
Muchof what we have outlined is still programmatic, but webelieve that the approach to extracting information fromcorpora making use of lexical semantic information is afruitful one and an area definitely worth exploring.AcknowledgementThis research was supported by DARPA contractMDAg04-91-C-9328.
I would like to thank Scott Wa-terman for his assistance in preparing the statistics usedhere.
I would also like to thank Scott Waterman, Fed-erica Busa, Peter Anick, and Sabine Bergler for usefuldiscussion.References1.
Anick, P. and J. Pustejovsky (1990) "An Appli-cation of Lexical Semantics to Knowledge Acquisi-tion from Corpora," Proceedings of the 13th Inter-national Conference of Computational Linguistics,August 20-25, 1990, Helsinki, Finland.2.
Bergler, S. (forthcoming) The Evidential Analysis ofReported Speech, Ph.D. Computer Science Depart-ment,Brandeis University.3.
Boguraev, B.
"Building a Lexicon: The Contribu-tion of Computers", in B. Boguraev (ed.
), Specialissue on computational lexicons, International Jour-nal of Lexicography, 4(3), 1991.4.
Boguraev, B. and J. Pustejovsky (1990) "LexicalAmbiguity and the Role of Knowledge Represen-tation in Lexicon Design," Proceedings of the 13thInternationM Conference of ComputationM Linguis-tics, August 20-25, 1990, Helsinki, Finland.5.
Church, K. and Hanks, P., (1990) "Word Associ-ation Norms, Mutual Information and Lexicogra-247phy".
Computational Linguistics Vol.
16(1).6.
Church, K. and D. IIindle (1990)" CollocationalConstraints and Corpus-Based Linguistics."
InWorking Notes of the AAAI Symposium: Text-Based Intelligent Systems, 1990.7.
Grishman, R., L. Hirschman, N. Nhan (1986) "Dis-covery Procedures for Sublanguage Selectional Pat-terns: Initial Experiments."
Computational Lin-guistics, Vol.
12, Number 3, pp.
205-215.8.
IIindle, D. and M. Rooth, "Structural Ambiguityand Lexical Relations", Proceedings of the ACL,1991.9.
Levin, B.
(1989) "The Lexical Organization of theEnglish Verb", ms. to appear University of ChicagoPress.10.
Pustejovsky, J.
(1991) "The Generative Lexicon,"Computational Linguistics, 17.4, 1991.11.
Pustejovsky, J.
(forthcoming) The Generative Lexi-con: A Theory of Computational Lezical Semantics,MIT Press, Cambridge, MA.12.
Pustejovsky, J. and P. Anick (1988) "The Seman-tic Interpretation of Nominals",Proceedings of the12th International Conference on ComputationalLinguistics, Budapest, IIungary.13.
Pustejovsky, J., S. Bergler, and P. Anick (1991)"Lexical Semantic Techniques for Corpus Analysis,"(submitted to) Computational Linguistics.14.
Pustejovsky, J. and M. Rooth (in preparation)"Type Coercive Environments in Corpora".15.
Smadja, F. (1991) "Macro-coding the lexicon withco-occurrence knowledge," in Zernik (ed) LexicalAcquisition: Using On-Line Resources to Build aLexicon, LEA, IIillsdale, NJ, 1991.16.
Wilks, Y., D. C. Fass, C.-M. Guo, J. E. McDonald,T.
Plate, and B. M. Slator (1991) "Providing Ma-chine Tractable Dictionary Tools," Machine Trans-lation 5, 1991.17.
Zernik, U.
(1989) "Lexicon Acquisition: Learn-ing from Corpus by Exploiting Lexical Categories.
"Proceedings of IJCAI 89.18.
Zernik, U. and P. Jacobs (1990) "Tagging for learn-ing: Collecting thematic relations from corpus.
"Proceedings of the 13th International Conference onComputational Linguistics, Helsinki, Finland.Word Word WordWord + to + with + between(%)/MI (%)/MI (%)/MIagreement .117 .159 .0281.512 3.423 3.954announcement .010 .003 0-.918 -.409 n/abarrier .215 0 .0302.117 n/a 4.046competition .019 .028 .021-.269 1.701 3.666confrontation .029 .283 .074.141 4.000 4.932contest .052 .052 .039.715 2.323 4.301contract .066 .060 .002.947 2.463 1.701deal .028 .193 .004.086 3.616 2.015dialogue 0 .326 .152n/a 4.140 5.644difference .017 .009 .348-.410 .638 6.474expansion .013 .007 0-.666 .381 n/aimpasse 0 .064 .096n/a 2.520 5.192interactions 0 0 .250n/a n/a 6.141market .013 .006 .000-.637 .240 -.500range .005 .002 .020-1.533 -.618 3.663relations .009 .217 .103-1.017 3.736 5.254settlement .013 .091 .012-.626 2.868 3.142talks .029 i .218 .030.138 3.740 4.043venture .032 .105 .035.226 3.008 4.185war .010 .041 .015-.937 2.079 3.372Table 1: Mutual information for noun/verb + preposi-tion patterns.248
