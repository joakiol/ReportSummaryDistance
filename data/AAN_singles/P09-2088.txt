Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349?352,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPImproved Smoothing for N-gram Language ModelsBased on Ordinary CountsRobert C. Moore Chris QuirkMicrosoft ResearchRedmond, WA 98052, USA{bobmoore,chrisq}@microsoft.comAbstractKneser-Ney (1995) smoothing and its vari-ants are generally recognized as havingthe best perplexity of any known methodfor estimating N-gram language models.Kneser-Ney smoothing, however, requiresnonstandard N-gram counts for the lower-order models used to smooth the highest-order model.
For some applications, thismakes Kneser-Ney smoothing inappropri-ate or inconvenient.
In this paper, we in-troduce a new smoothing method based onordinary counts that outperforms all of theprevious ordinary-count methods we havetested, with the new method eliminatingmost of the gap between Kneser-Ney andthose methods.1 IntroductionStatistical language models are potentially usefulfor any language technology task that producesnatural-language text as a final (or intermediate)output.
In particular, they are extensively used inspeech recognition and machine translation.
De-spite the criticism that they ignore the structure ofnatural language, simple N-gram models, whichestimate the probability of each word in a textstring based on the N?1 preceding words, remainthe most widely used type of model.The simplest possible N-gram model is themaximum likelihood estimate (MLE), which takesthe probability of a word wn, given the precedingcontext w1 .
.
.
wn?1, to be the ratio of the num-ber of occurrences in a training corpus of the N-gram w1 .
.
.
wn to the total number of occurrencesof any word in the same context:p(wn|w1 .
.
.
wn?1) =C(w1 .
.
.
wn)?w?
C(w1 .
.
.
wn?1w?
)One obvious problem with this method is that itassigns a probability of zero to any N-gram that isnot observed in the training corpus; hence, numer-ous smoothing methods have been invented thatreduce the probabilities assigned to some or all ob-served N-grams, to provide a non-zero probabilityfor N-grams not observed in the training corpus.The best methods for smoothing N-gram lan-guage models all use a hierarchy of lower-ordermodels to smooth the highest-order model.
Thus,if w1w2w3w4w5 was not observed in the train-ing corpus, p(w5|w1w2w3w4) is estimated basedon p(w5|w2w3w4), which is estimated based onp(w5|w3w4) if w2w3w4w5 was not observed, etc.In most smoothing methods, the lower-ordermodels, for all N > 1, are recursively estimatedin the same way as the highest-order model.
How-ever, the smoothing method of Kneser and Ney(1995) and its variants are the most effective meth-ods known (Chen and Goodman, 1998), and theyuse a different way of computing N-gram countsfor all the lower-order models used for smooth-ing.
For these lower-order models, the actual cor-pus counts C(w1 .
.
.
wn) are replaced byC ?
(w1 .
.
.
wn) =??
{w?|C(w?w1 .
.
.
wn) > 0}?
?In other words, the count used for a lower-orderN-gram is the number of distinct word types thatprecede it in the training corpus.The fact that the lower-order models are es-timated differently from the highest-order modelmakes the use of Kneser-Ney (KN) smooth-ing awkward in some situations.
For example,coarse-to-fine search using a sequence of lower-order to higher-order language models has beenshown to be an efficient way of constraining high-dimensional search spaces for speech recognition(Murveit et al, 1993) and machine translation(Petrov et al, 2008).
The lower-order models usedin KN smoothing, however, are very poor esti-mates of the probabilities for N-grams that havebeen observed in the training corpus, so they are349p(wn|w1 .
.
.
wn?1) =????????????w1...wn?1Cn(w1...wn)?Dn,Cn(w1...wn)?w?
Cn(w1...wn?1w?
)+ ?w1...wn?1p(wn|w2 .
.
.
wn?1) if Cn(w1 .
.
.
wn) > 0?w1...wn?1p(wn|w2 .
.
.
wn?1) if Cn(w1 .
.
.
wn) = 0Figure 1: General language model smoothing schemanot suitable for use in coarse-to-fine search.
Thus,two versions of every language model below thehighest-order model would be needed to use KNsmoothing in this case.Another case in which use of special KN countsis problematic is the method presented by Nguyenet al (2007) for building and applying languagemodels trained on very large corpora (up to 40 bil-lion words in their experiments).
The scalabilityof their approach depends on a ?backsorted trie?,but this data structure does not support efficientcomputation of the special KN counts.In this paper, we introduce a new smoothingmethod for language models based on ordinarycounts.
In our experiments, it outperformed allof the previous ordinary-count methods we tested,and it eliminated most of the gap between KNsmoothing and the other previous methods.2 Overview of Previous MethodsAll the language model smoothing methods wewill consider can be seen as instantiating the recur-sive schema presented in Figure 1, for all n suchthat N ?
n ?
2,1 where N is the greatest N-gramlength used in the model.In this schema, Cn denotes the counting methodused for N-grams of length n. For most smoothingmethods, Cn denotes actual training corpus countsfor all n. For KN smoothing and its variants, how-ever, Cn denotes actual corpus counts only whenn is the greatest N-gram length used in the model,and otherwise denotes the special KN C ?
counts.In this schema, each N-gram count is dis-counted according to a D parameter that depends,at most, on the N-gram length and the the N-gramcount itself.
The values of the ?, ?, and ?
parame-ters depend on the context w1 .
.
.
wn?1.
For eachcontext, the values of ?, ?, and ?
must be set toproduce a normalized conditional probability dis-tribution.
Additional constraints on the previous1For n = 2, we take the expression p(wn|w2 .
.
.
wn?1)to denote a unigram probability estimate p(w2).models we consider further reduce the degrees offreedom so that ultimately the values of these para-meters are completely fixed by the values selectedfor the D parameters.The previous smoothing methods we considercan be classified as either ?pure backoff?, or ?pureinterpolation?.
In pure backoff methods, all in-stances of ?
= 1 and all instances of ?
= 0.
Thepure backoff methods we consider are Katz back-off and backoff absolute discounting, due to Neyet al2 In Katz backoff, if C(w1 .
.
.
wn) is greaterthan a threshold (here set to 5, as recommendedby Katz) the corresponding D = 0; otherwise Dis set according to the Good-Turing method.3In backoff absolute discounting, the D parame-ters depends, at most, on n; there is either one dis-count per N-gram length, or a single discount usedfor all N-gram lengths.
The values of D can be seteither by empirical optimization on held-out data,or based on a theoretically optimal value derivedfrom a leaving-one-out analysis, which Ney et alshow to be approximated for each N-gram lengthby N1/(N1 + 2N2), where Nr is the number ofdistinct N-grams of that length occuring r times inthe training corpus.In pure interpolation methods, for each context,?
and ?
are constrained to be equal.
The modelswe consider that fall into this class are interpolatedabsolute discounting, interpolated KN, and modi-fied interpolated KN.
In these three methods, allinstances of ?
= 1.4 In interpolated absolute dis-counting, the instances of D are set as in backoffabsolute discounting.
The same is true for inter-2For all previous smoothing methods other than KN, werefer the reader only to the excellent comparative study ofsmoothing methods by Chen and Goodman (1998).
Refer-ences to the original sources may be found there.3Good-Turing discounting is usually expressed in termsof a discount ratio, but this can be reformulated as Dr =r ?
drr, where Dr is the subtractive discount for an N-gramoccuring r times, and dr is the corresponding discount ratio.4Jelinek-Mercer smoothing would also be a pure interpo-lation instance of our language model schema, in which allinstances of D = 0 and, for each context, ?+ ?
= 1.350polated KN, but the lower-order models are esti-mated using the special KN counts.In Chen and Goodman?s (1998) modified inter-polated KN, instead of one D parameter for eachN-gram length, there are three: D1 for N-gramswhose count is 1, D2 for N-grams whose count is2, and D3 for N-grams whose count is 3 or more.The values of these parameters may be set eitherby empirical optimization on held-out data, or bya theoretically-derived formula analogous to theNey et al formula for the one-discount case:Dr = r ?
(r + 1)YNr+1Nr,for 1 ?
r ?
3, where Y = N1/(N1 + 2N2), thediscount value derived by Ney et al3 The New MethodOur new smoothing method is motivated by theobservation that unsmoothed MLE language mod-els suffer from two somewhat independent sourcesof error in estimating probabilities for the N-gramsobserved in the training corpus.
The problem thathas received the most attention is the fact that, onthe whole, the MLE probabilities for the observedN-grams are overestimated, since they end up withall the probability mass that should be assigned tothe unobserved N-grams.
The discounting used inKatz backoff is based on the Good-Turing estimateof exactly this error.Another source of error in MLE models, how-ever, is quantization error, due to the fact that onlycertain estimated probability values are possiblefor a given context, depending on the number ofoccurrences of the context in the training corpus.No pure backoff model addresses this source oferror, since no matter how the discount parame-ters are set, the number of possible probability val-ues for a given context cannot be increased justby discounting observed counts, as long as all N-grams with the same count receive the same dis-count.
Interpolation models address quantizationerror by interpolation with lower-order estimates,which should have lower quantization error, due tohigher context counts.
As we have noted, most ex-isting interpolation models are constrained so thatthe discount parameters fully determine the inter-polation parameters.
Thus the discount parametershave to correct for both types of error.55Jelinek-Mercer smoothing is an exception to this gener-alization, but since it has only interpolation parameters andOur new model provides additional degrees offreedom so the ?
and ?
interpolation parameterscan be set independently of the discount parame-ters D, with the intention that the ?
and ?
para-meters correct for quantization error, and the Dparameters correct for overestimation error.
Thisis accomplished by relaxing the link between the?
and ?
parameters.
We require that for each con-text, ?
?
0, ?
?
0, and ?
+ ?
= 1, and thatfor every Dn,Cn(w1...wn) parameter, 0 ?
D ?Cn(w1 .
.
.
wn).
For each context, whatever valueswe choose for these parameters within these con-straints, we are guaranteed to have some probabil-ity mass between 0 and 1 left over to be distributedacross the unobserved N-grams by a unique valueof ?
that normalizes the conditional distribution.Previous smoothing methods suggest severalapproaches to setting the D parameters in our newmodel.
We try four such methods here:1.
The single theory-based discount for each N-gram length proposed by Ney et al,2.
A single discount used for all N-gramlengths, optimized on held-out data,3.
The three theory-based discounts for each N-gram length proposed by Chen and Good-man,4.
A novel set of three theory-based discountsfor each N-gram length, based on Good-Turing discounting.The fourth method is similar to the third, butfor the three D parameters per context, we use thediscounts for 1-counts, 2-counts, and 3-counts es-timated by the Good-Turing method.
This yieldsthe formulaDr = r ?
(r + 1)Nr+1Nr,which is identical to the Chen-Goodman formula,except that the Y factor is omitted.
Since Y is gen-erally between 0 and 1, the resulting discounts willbe smaller than with the Chen-Goodman formula.To set the ?
and ?
parameters, we assume thatthere is a single unknown probability distributionfor the amount of quantization error in every N-gram count.
If so, the total quantization error fora given context will tend to be proportional to theno discount parameters, it forces the interpolation parametersto do the same double duty that other models force the dis-count parameters to do.351number of distinct counts for that context, in otherwords, the number of distinct word types occur-ring in that context.
We then set ?
and ?
to replacethe proportion of the total probability mass for thecontext represented by the estimated quantizationerror with probability estimates derived from thelower-order models:?w1...wn?1 = ?
|{w?|Cn(w1...wn?1w?)>0}|?w?
Cn(w1...wn?1w?
)?w1...wn?1 = 1?
?w1...wn?1where ?
is the estimated mean of the quantizationerror introduced by each N-gram count.We use a single value of ?
for all contexts andall N-gram lengths.
As an a priori ?theory?-basedestimate, we assume that, since the distance be-tween possible N-gram counts, after discounting,is approximately 1.0, their mean quantization errorwould be approximately 0.5.
We also try setting ?by optimization on held-out data.4 Evaluation and ConclusionsWe trained and measured the perplexity of 4-gram language models using English data fromthe WMT-06 Europarl corpus (Koehn and Monz,2006).
We took 1,003,349 sentences (27,493,499words) for training, and 2000 sentences each fortesting and parameter optimization.We built models based on six previous ap-proaches: (1) Katz backoff, (2) interpolated ab-solute discounting with Ney et al formula dis-counts, backoff absolute discounting with (3) Neyet al formula discounts and with (4) one empir-ically optimized discount, (5) modified interpo-lated KN with Chen-Goodman formula discounts,and (6) interpolated KN with one empirically op-timized discount.
We built models based on fourways of computing the D parameters of our newmodel, with a fixed ?
= 0.5: (7) Ney et al formuladiscounts, (8) one empirically optimized discount,(9) Chen-Goodman formula discounts, and (10)Good-Turing formula discounts.
We also built amodel (11) based on one empirically optimizeddiscount D = 0.55 and an empircially optimizedvalue of ?
= 0.9.
Table 1 shows that each of thesevariants of our method had better perplexity thanevery previous ordinary-count method tested.Finally, we performed one more experiment, tosee if the best variant of our model (11) combinedwith KN counts would outperform either variantof interpolated KN.
It did not, yielding a perplex-ity of 53.9 after reoptimizing the two free parame-Method PP1 Katz backoff 59.82 interp-AD-fix 62.63 backoff-AD-fix 59.94 backoff-AD-opt 58.85 KN-mod-fix 52.86 KN-opt 53.07 new-AD-fix 56.38 new-AD-opt 55.69 new-CG-fix 57.410 new-GT-fix 56.111 new-AD-2-opt 54.9Table 1: 4-gram perplexity resultsters of the model with the KN counts.
However,the best variant of our model eliminated 65% ofthe difference in perplexity between the best pre-vious ordinary-count method tested and the bestvariant of KN smoothing tested, suggesting that itmay currently be the best approach when languagemodels based on ordinary counts are desired.ReferencesChen, Stanley F., and Joshua Goodman.
1998.An empirical study of smoothing techniques forlanguage modeling.
Technical Report TR-10-98, Harvard University.Kneser, Reinhard, and Hermann Ney.
1995.
Im-proved backing-off for m-gram language mod-eling.
In Proceedings of ICASSP-95, vol.
1,181?184.Koehn, Philipp, and Christof Monz.
2006.
Manualand automatic evaluation of machine translationbetween European languages.
In Proceedingsof WMT-06, 102?121.Murveit, Hy, John Butzberger, Vassilios Digalakis,and Mitch Weintraub.
1993.
Progressive searchalgorithms for large-vocabulary speech recogni-tion.
In Proceedings of HLT-93, 87?90.Nguyen, Patrick, Jianfeng Gao, and Milind Maha-jan. 2007.
MSRLM: a scalable language mod-eling toolkit.
Technical Report MSR-TR-2007-144.
Microsoft Research.Petrov, Slav, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation us-ing language projections.
In Proceedings ofACL-08.
108?116.352
