Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 618?626,Sydney, July 2006. c?2006 Association for Computational LinguisticsSolving the Problem of Cascading Errors: Approximate BayesianInference for Linguistic Annotation PipelinesJenny Rose Finkel, Christopher D. Manning and Andrew Y. NgComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel, manning, ang}@cs.stanford.eduAbstractThe end-to-end performance of naturallanguage processing systems for com-pound tasks, such as question answeringand textual entailment, is often hamperedby use of a greedy 1-best pipeline archi-tecture, which causes errors to propagateand compound at each stage.
We presenta novel architecture, which models thesepipelines as Bayesian networks, with eachlow level task corresponding to a variablein the network, and then we perform ap-proximate inference to find the best la-beling.
Our approach is extremely sim-ple to apply but gains the benefits of sam-pling the entire distribution over labels ateach stage in the pipeline.
We apply ourmethod to two tasks ?
semantic role la-beling and recognizing textual entailment?
and achieve useful performance gainsfrom the superior pipeline architecture.1 IntroductionAlmost any system for natural language under-standing must recover hidden linguistic structureat many different levels: parts of speech, syntac-tic dependencies, named entities, etc.
For exam-ple, modern semantic role labeling (SRL) systemsuse the parse of the sentence, and question answer-ing requires question type classification, parsing,named entity tagging, semantic role labeling, andoften other tasks, many of which are dependenton one another and must be pipelined together.Pipelined systems are ubiquitous in NLP: in ad-dition to the above examples, commonly parsersand named entity recognizers use part of speechtags and chunking information, and also word seg-mentation for languages such as Chinese.
Almostno NLP task is truly standalone.Most current systems for higher-level, aggre-gate NLP tasks employ a simple 1-best feed for-ward architecture: they greedily take the best out-put at each stage in the pipeline and pass it on tothe next stage.
This is the simplest architecture tobuild (particularly if reusing existing componentsystems), but errors are frequently made duringthis pipeline of annotations, and when a systemis given incorrectly labeled input it is much harderfor that system to do its task correctly.
For ex-ample, when doing semantic role labeling, if nosyntactic constituent of the parse actually corre-sponds to a given semantic role, then that seman-tic role will almost certainly be misidentified.
Itis therefore disappointing, but not surprising, thatF-measures on SRL drop more than 10% whenswitching from gold parses to automatic parses(for instance, from 91.2 to 80.0 for the joint modelof Toutanova (2005)).A common improvement on this architecture isto pass k-best lists between processing stages, forexample (Sutton and McCallum, 2005; Wellner etal., 2004).
Passing on a k-best list gives usefulimprovements (e.g., in Koomen et al (2005)), butefficiently enumerating k-best lists often requiresvery substantial cognitive and engineering effort,e.g., in (Huang and Chiang, 2005; Toutanova etal., 2005).At the other extreme, one can maintain theentire space of representations (and their proba-bilities) at each level, and use this full distribu-tion to calculate the full distribution at the nextlevel.
If restricting oneself to weighted finite statetransducers (WFSTs), a framework applicable to anumber of NLP applications (as outlined in Kart-tunen (2000)), a pipeline can be compressed down618into a single WFST, giving outputs equivalentto propagating the entire distribution through thepipeline.
In the worst case there is an exponentialspace cost, but in many relevant cases compositionis in practice quite practical.
Outside of WFSTs,maintaining entire probability distributions is usu-ally infeasible in NLP, because for most intermedi-ate tasks, such as parsing and named entity recog-nition, there is an exponential number of possiblelabelings.
Nevertheless, for some models, such asmost parsing models, these exponential labelingscan be compactly represented in a packed form,e.g., (Maxwell and Kaplan, 1995; Crouch, 2005),and subsequent stages can be reengineered to workover these packed representations, e.g., (Gemanand Johnson, 2002).
However, doing this normallyalso involves a very high cognitive and engineer-ing effort, and in practice this solution is infre-quently adopted.
Moreover, in some cases, a sub-sequent module is incompatible with the packedrepresentation of a previous module and an ex-ponential amount of work is nevertheless requiredwithin this architecture.Here we present an attractive middle groundin dealing with linguistic pipelines.
Rather thanonly using the 1 or k most likely labelings at eachstage, we would indeed like to take into accountall possible labelings and their probabilities, butwe would like to be able to do so without a lot ofthinking or engineering.
We propose that this canbe achieved by use of approximate inference.
Theform of approximate inference we use is very sim-ple: at each stage in the pipeline, we draw a sam-ple from the distribution of labels, conditioned onthe samples drawn at previous stages.
We repeatthis many times, and then use the samples fromthe last stage, which corresponds to the ultimate,higher-level task, to form a majority vote classifier.As the number of samples increases, this methodwill approximate the complete distribution.
Use ofthe method is normally a simple modification to anexisting piece of code, and the method is general.It can be applied not only to all pipelines, but tomulti-stage algorithms which are not pipelines aswell.We apply our method to two problems: seman-tic role labeling and recognizing textual entail-ment.
For semantic role labeling we use a twostage pipeline which parses the input sentence, andfor recognizing textual entailment we use a threestage pipeline which tags the sentence with namedentities and then parses it before passing it to theentailment decider.2 Approach2.1 OverviewIn order to do approximate inference, we modelthe entire pipeline as a Bayesian network.
Eachstage in the pipeline corresponds to a variable inthe network.
For example, the parser stage cor-responds to a variable whose possible values areall possible parses of the sentence.
The probabil-ities of the parses are conditioned on the parentvariables, which may just be the words of the sen-tence, or may be the part of speech tags output bya part of speech tagger.The simple linear structure of a typical linguis-tic annotation network permits exact inference thatis quadratic in the number of possible labels ateach stage, but unfortunately our annotation vari-ables have a very large domain.
Additionally,some networks may not even be linear; frequentlyone stage may require the output from multipleprevious stages, or multiple earlier stages may becompletely independent of one another.
For ex-ample, a typical QA system will do question typeclassification on the question, and from that ex-tract keywords which are passed to the informa-tion retreival part of the system.
Meanwhile, theretreived documents are parsed and tagged withnamed entities; the network rejoins those outputswith the question type classification to decide onthe correct answer.
We address these issues byusing approximate inference instead of exact in-ference.
The structure of the nodes in the networkpermits direct sampling based on a topological sortof the nodes.
Samples are drawn from the condi-tional distributions of each node, conditioned onthe samples drawn at earlier nodes in the topolog-ical sort.2.2 Probability of a Complete LabelingBefore we can discuss how to sample from theseBayes nets, we will formalize how to move froman annotation pipeline to a Bayes net.
Let A bethe set of n annotators A1, A2, ..., An (e.g., partof speech tagger, named entity recognizer, parser).These are the variables in the network.
For annota-tor ai, we denote the set of other annotators whoseinput is directly needed as Parents(Ai) ?
Aand a particular assignment to those variables isparents(Ai).
The possible values for a particu-619lar annotator Ai are ai (e.g., a particular parse treeor named entity tagging).
We can now formulatethe probability of a complete annotation (over allannotators) in the standard way for Bayes nets:PBN(a1, a2, ..., an) =N?i=1P (ai|parents(Ai))(1)2.3 Approximate Inference in BayesianNetworksThis factorization of the joint probability distri-bution facilitates inference.
However, exact in-ference is intractable because of the number ofpossible values for our variables.
Parsing, part ofspeech tagging, and named entity tagging (to namea few) all have a number of possible labels that isexponential in the length of the sentence, so weuse approximate inference.
We chose Monte Carloinference, in which samples drawn from the jointdistribution are used to approximate a marginaldistribution for a subset of variables in the dis-tribution.
First, the nodes are sorted in topologi-cal order.
Then, samples are drawn for each vari-able, conditioned on the samples which have al-ready been drawn.
Many samples are drawn, andare used to estimate the joint distribution.Importantly, for many language processingtasks our application only needs to provide themost likely value for a high-level linguistic an-notation (e.g., the guessed semantic roles, or an-swer to a question), and other annotations such asparse trees are only present to assist in performingthat task.
The probability of the final annotation isgiven by:PBN(an) =?a1,a2,...,an?1PBN(a1, a2, ..., an) (2)Because we are summing out all variables otherthan the final one, we effectively use only the sam-ples drawn from the final stage, ignoring the labelsof the variables, to estimate the marginal distribu-tion over that variable.
We then return the labelwhich had the highest number of samples.
Forexample, when trying to recognize textual entail-ment, we count how many times we sampled ?yes,it is entailed?
and how many times we sampled?no, it is not entailed?
and return the answer withmore samples.When the outcome you are trying to predict isbinary (as is the case with RTE) or n-ary for smalln, the number of samples needed to obtain a goodestimate of the posterior probability is very small.This is true even if the spaces being sampled fromduring intermediate stages are exponentially large(such as the space of all parse trees).
Ng andJordan (2001) show that under mild assumptions,with only N samples the relative classification er-ror will be at most O( 1N ) higher than the error ofthe Bayes optimal classifier (in our case, the clas-sifier which does exact inference).
Even if the out-come space is not small, the sampling techniquewe present can still be very useful, as we will seelater for the case of SRL.3 Generating SamplesThe method we have outlined requires the abilityto sample from the conditional distributions in thefactored distribution of (1): in our case, the prob-ability of a particular linguistic annotation, condi-tioned on other linguistic annotations.
Note thatthis differs from the usual annotation task: takingthe argmax.
But for most algorithms the change isa small and easy change.
We discuss how to ob-tain samples efficiently from a few different anno-tation models: probabilistic context free grammars(PCFGs), and conditional random fields (CRFs).3.1 Sampling ParsesBod (1995) discusses parsing with probabilistictree substitution grammars, which, unlike simplePCFGs, do not have a one-to-one mapping be-tween output parse trees and a derivation (a bag ofrules) that produced it, and hence the most-likelyderivation may not correspond to the most likelyparse tree.
He therefore presents a bottom-up ap-proach to sampling derivations from a derivationforest, which does correspond to a sample from thespace of parse trees.
Goodman (1998) presents atop-down version of this algorithm.
Although weuse a PCFG for parsing, it is the grammar of (Kleinand Manning, 2003), which uses extensive state-splitting, and so there is again a many-to-one cor-respondence between derivations and parses, andwe use an algorithm similar to Goodman?s in ourwork.PCFGs put probabilities on each rule, such asS ?
NP VP and NN ?
?dog?.
The probability ofa parse is the product of the probabilities of therules used to construct the parse tree.
A dynamicprograming algorithm, the inside algorithm, canbe used to find the probability of a sentence.
The620inside probability ?k(p, q) is the probability thatwords p through q, inclusive, were produced bythe non-terminal k. So the probability of the sen-tence The boy pet the dog.
is equal to the insideprobability ?S(1, 6), where the first word, w1 isThe and the sixth word, w6, is [period].
It is alsouseful for our purposes to view this quantity as thesum of the probabilities of all parses of the sen-tence which have S as the start symbol.
The prob-ability can be defined recursively (Manning andSchu?tze, 1999) as follows:?k(p, q) =????????
?P (Nk ?
wp) if p = q?r,sq?1?d=pP (Nk ?
N rN s)?r(p, d)?s(d + 1, q)otherwise(3)where Nk, N r and N s are non-terminal symbolsand wp is the word at position p. We have omit-ted the case of unary rules for simplicity since itrequires a closure operation.These probabilities can be efficiently computedusing a dynamic program.
or memoization of eachvalue as it is calculated.
Once we have computedall of the inside probabilities, they can be used togenerate parses from the distribution of all parsesof the sentence, using the algorithm in Figure 1.This algorithm is called after all of the insideprobabilities have been calculated and stored, andtake as parameters S, 1, and length(sentence).
Itworks by building the tree, starting from the root,and recursively generating children based on theposterior probabilities of applying each rule andeach possible position on which to split the sen-tences.
Intuitively, the algorithm is given a non-terminal symbol, such as S or NP, and a span ofwords, and has to decide (a) what rule to apply toexpand the non-terminal, and (b) where to split thespan of words, so that each non-terminal result-ing from applying the rule has an associated wordspan, and the process can repeat.
The inside prob-abilities are calculated just once, and we can thengenerate many samples very quickly; DrawSam-ples is linear in the number of words, and rules.3.2 Sampling Named Entity TaggingsTo do named entity recognition, we chose to usea conditional random field (CRF) model, based onLafferty et al (2001).
CRFs represent the state offunction DRAWSAMPLE(Nk, r, s)if r = stree.label = Nktree.child = word(r)return (tree)for each rule m ?
{m?
: head(m?)
= Nk}N i ?
lChild(m)Nj ?
rChild(m)for q ?
r to s?
1scores(m,q)?
P (m)?i(r, q)?j(q + 1, s)(m, q)?
SAMPLEFROM(scores)tree.label = head(m)tree.lChild = DRAWSAMPLE(lChild(m), r, q)tree.rChild = DRAWSAMPLE(rChild(m), q + 1, s)return (tree)Figure 1: Pseudo-code for sampling parse trees from a PCFG.This is a recursive algorithm which starts at the root of thetree and expands each node by sampling from the distribu-tion of possible rules and ways to split the span of words.
Itsarguments are a non-terminal and two integers correspondingto word indices, and it is initially called with arguments S, 1,and the length of the sentence.
There is a call to sampleFrom,which takes an (unnormalized) probability distribution, nor-malizes it, draws a sample and then returns the sample.the art in sequence modeling ?
they are discrimi-natively trained, and maximize the joint likelihoodof the entire label sequence in a manner whichallows for bi-directional flow of information.
Inorder to describe how samples are generated, wegeneralize CRFs in a way that is consistent withthe Markov random field literature.
We create alinear chain of cliques, each of which representsthe probabilistic relationship between an adjacentset of n states using a factor table containing |S|nvalues.
These factor tables on their own shouldnot be viewed as probabilities, unnormalized orotherwise.
They are, however, defined in terms ofexponential models conditioned on features of theobservation sequence, and must be instantiated foreach new observation sequence.
The probabilityof a state sequence is then defined by the sequenceof factor tables in the clique chain, given the ob-servation sequence:PCRF(s|o) =1Z(o)N?i=1Fi(si?n .
.
.
si) (4)where Fi(si?n .
.
.
si) is the element of the fac-tor table at position i corresponding to states si?nthrough si, and Z(o) is the partition functionwhich serves to normalize the distribution.1 To in-1To handle the start condition properly, imagine also thatwe define a set of distinguished start states s?
(n?1) .
.
.
s0.621fer the most likely state sequence in a CRF it iscustomary to use the Viterbi algorithm.We then apply a process called clique tree cal-ibration, which involves passing messages be-tween the cliques (see Cowell et al (2003) fora full treatment of this topic).
After this pro-cess has completed, the factor tables can beviewed as unnormalized probabilities, which canbe used to compute conditional probabilities,PCRF(si|si?n .
.
.
si?1, o).
Once these probabili-ties have been calculated, generating samples isvery simple.
First, we draw a sample for the labelat the first position,2 and then, for each subsequentposition, we draw a sample from the distributionfor that position, conditioned on the label sampledat the previous position.
This process results ina sample of a complete labeling of the sequence,drawn from the posterior distribution of completenamed entity taggings.Similarly to generating sample parses, the ex-pensive part is calculating the probabilities; oncewe have them we can generate new samples veryquickly.3.3 k-Best ListsAt first glance, k-best lists may seem like theyshould outperform sampling, because in effectthey are the k best samples.
However, there areseveral important reasons why one might prefersampling.
One reason is that the k best pathsthrough a word lattice, or the k best derivations inparse forest do not necessarily correspond to thek best sentences or parse trees.
In fact, there areno known sub-exponential algorithms for the bestoutputs in these models, when there are multipleways to derive the same output.3 This is not just atheoretical concern ?
the Stanford parser uses sucha grammar, and we found that when generating a50-best derivation list that on average these deriva-tions corresponded to about half as many uniqueparse trees.
Our approach circumvents this issueentirely, because the samples are generated fromthe actual output distribution.Intuition also suggests that sampling shouldgive more diversity at each stage, reducing thelikelihood of not even considering the correct out-put.
Using the Brown portion of the SRL testset (discussed in sections 4 and 6.1), and 50-samples/50-best, we found that on average the 50-2Conditioned on the distinguished start states.3Many thanks to an anonymous reviewer for pointing outthis argument.samples system considered approximately 25%more potential SRL labelings than the 50-best sys-tem.When pipelines have more than two stages, itis customary to do a beam search, with a beamsize of k. This means that at each stage in thepipeline, more and more of the probability massgets ?thrown away.?
Practically, this means thatas pipeline length increases, there will be in-creasingly less diversity of labels from the earlierstages.
In a degenerate 10-stage, k-best pipeline,where the last stage depends mainly on the firststage, it is probable that all but a few labelingsfrom the first stage will have been pruned away,leaving something much smaller than a k-bestsample, possibly even a 1-best sample, as input tothe final stage.
Using approximate inference to es-timate the marginal distribution over the last stagein the pipeline, such as our sampling approach, thepipeline length does not have this negative impactor affect the number of samples needed.
And un-like k-best beam searches, there is an entire re-search community, along with a large body of lit-erature, which studies how to do approximate in-ference in Bayesian networks and can provide per-formance bounds based on the method and thenumber of samples generated.One final issue with the k-best method ariseswhen instead of a linear chain pipeline, one is us-ing a general directed acyclic graph where a nodecan have multiple parents.
In this situation, doingthe k-best calculation actually becomes exponen-tial in the size of the largest in-degree of a node ?for a node with n parents, you must try all kn com-binations of the values for the parent nodes.
Withsampling this is not an issue; each sample can begenerated based on a topological sort of the graph.4 Semantic Role Labeling4.1 Task DescriptionGiven a sentence and a target verb (the predicate)the goal of semantic role labeling is to identify andlabel syntactic constituents of the parse tree withsemantic roles of the predicate.
Common rolesare agent, which is the thing performing the ac-tion, patient, which is the thing on which the ac-tion is being performed, and instrument, which isthe thing with which the action is being done.
Ad-ditionally, there are modifier arguments which canspecify the location, time, manner, etc.
The fol-lowing sentence provides an example of a predi-622cate and its arguments:[The luxury auto maker]agent [lastyear]temp [sold]pred [1,214 cars]patientin [the U.S]location.Semantic role labeling is a key component forsystems that do question answering, summariza-tion, and any other task which directly uses a se-mantic interpretation.4.2 System DescriptionWe modified the system described in Haghighiet al (2005) and Toutanova et al (2005) to testour method.
The system uses both local models,which score subtrees of the entire parse tree inde-pendently of the labels of other nodes not in thatsubtree, and joint models, which score the entirelabeling of a tree with semantic roles (for a partic-ular predicate).First, the task is separated into two stages, andlocal models are learned for each.
At the firststage, the identification stage, a classifier labelseach node in the tree as either ARG, meaning thatit is an argument (either core or modifier) to thepredicate, or NONE, meaning that it is not an argu-ment.
At the second stage, the classification stage,the classifier is given a set of arguments for a pred-icate and must label each with its semantic role.Next, a Viterbi-like dynamic algorithm is usedto generate a list of the k-best joint (identificationand classification) labelings according to the lo-cal models.
The algorithm enforces the constraintthat the roles should be non-overlapping.
Finally,a joint model is constructed which scores a com-pletely labeled tree, and it is used to re-rank the k-best list.
The separation into local and joint mod-els is necessary because there are an exponentialnumber of ways to label the entire tree, so usingthe joint model alone would be intractable.
Ide-ally, we would want to use approximate inferenceinstead of a k-best list here as well.
Particle fil-tering would be particularly well suited - particlescould be sampled from the local model and thenreweighted using the joint model.
Unfortunately,we did not have enough time modify the code of(Haghighi et al, 2005) accordingly, so the k-beststructure remained.To generate samples from the SRL system, wetake the scores given to the k-best list, normalizethem to sum to 1, and sample from them.
Oneconsequence of this, is that any labeling not on thek-best list has a probability of 0.5 Recognizing Textual Entailment5.1 Task DescriptionIn the task of recognizing textual entailment(RTE), also commonly referred to as robust textualinference, you are provided with two passages, atext and a hypothesis, and must decide whether thehypothesis can be inferred from the text.
The termrobust is used because the task is not meant to bedomain specific.
The term inference is used be-cause this is not meant to be logical entailment, butrather what an intelligent, informed human wouldinfer.
Many NLP applications would benefit fromthe ability to do robust textual entailment, includ-ing question answering, information retrieval andmulti-document summarization.
There have beentwo PASCAL workshops (Dagan et al, 2005) withshared tasks in the past two years devoted to RTE.We used the data from the 2006 workshop, whichcontains 800 text-hypothesis pairs in each of thetest and development sets4 (there is no trainingset).
Here is an example from the developmentset from the first RTE challenge:Text: Researchers at the Harvard School of Pub-lic Health say that people who drink coffeemay be doing a lot more than keeping them-selves awake ?
this kind of consumption ap-parently also can help reduce the risk of dis-eases.Hypothesis: Coffee drinking has health benefits.The positive and negative examples are bal-anced, so the baseline of guessing either all yesor all no would score 50%.
This is a hard task ?
atthe first challenge no system scored over 60%.5.2 System DescriptionMacCartney et al (2006) describe a system for do-ing robust textual inference.
They divide the taskinto three stages ?
linguistic analysis, graph align-ment, and entailment determination.
The first ofthese stages, linguistic analysis is itself a pipelineof parsing and named entity recognition.
They usethe syntactic parse to (deterministically) producea typed dependency graph for each sentence.
Thispipeline is the one we replace.
The second stage,graph alignment consists of trying to find goodalignments between the typed dependency graphs4The dataset and further information from bothchallenges can be downloaded from http://www.pascal-network.org/Challenges/RTE2/Datasets/623NER parser RTEFigure 2: The pipeline for recognizing textual entailment.for the text and hypothesis.
Each possible align-ment has a score, and the alignment with the bestscore is propagated forward.
The final stage, en-tailment determination, is where the decision isactually made.
Using the score from the align-ment, as well as other features, a logistic modelis created to predict entailment.
The parametersfor this model are learned from development data.5While it would be preferable to sample possiblealignments, their system for generating alignmentscores is not probabilistic, and it is unclear howone could convert between alignment scores andprobabilities in a meaningful way.Our modified linguistic analysis pipeline doesNER tagging and parsing (in their system, theparse is dependent on the NER tagging becausesome types of entities are pre-chunked beforeparsing) and treats the remaining two sections oftheir pipeline, the alignment and determinationstages, as one final stage.
Because the entailmentdetermination stage is based on a logistic model, aprobability of entailment is given and sampling isstraightforward.6 Experimental ResultsIn our experiments we compare the greedypipelined approach with our sampling pipeline ap-proach.6.1 Semantic Role LabelingFor the past two years CoNLL has had sharedtasks on SRL (Carreras and Ma`rquez (2004) andCarreras and Ma`rquez (2005)).
We used theCoNLL 2005 data and evaluation script.
Whenevaluating semantic role labeling results, it is com-mon to present numbers on both the core argu-ments (i.e., excluding the modifying arguments)and all arguments.
We follow this convention andpresent both sets of numbers.
We give precision,5They report their results on the first PASCAL dataset,and use only the development set from the first challenge forlearning weights.
When we test on the data from the secondchallenge, we use all data from the first challenge and thedevelopment data from the second challenge to learn theseweights.SRL Results ?
Penn Treebank PortionCore Args Precision Recall F-measureGreedy 79.31% 77.7% 78.50%K-Best 80.05% 78.45% 79.24%Sampling 80.13% 78.25% 79.18%All Args Precision Recall F-measureGreedy 78.49% 74.77% 76.58%K-Best 79.58% 74.90% 77.16%Sampling 79.81% 74.85% 77.31%SRL Results ?
Brown PortionCore Args Precision Recall F-measureGreedy 68.28% 67.72% 68.0%K-Best 69.25% 69.02% 69.13%Sampling 69.35% 68.93% 69.16%All Args Precision Recall F-measureGreedy 66.6% 60.45% 63.38%K-Best 68.82% 61.03% 64.69%Sampling 68.6% 61.11% 64.64%Table 1: Results for semantic role labeling task.
The samplednumbers are averaged over several runs, as discussed.recall and F-measure, which are based on the num-ber of arguments correctly identified.
For an argu-ment to be correct both the span and the classifica-tion must be correct; there is no partial credit.To generate sampled parses, we used the Stan-ford parser (Klein and Manning, 2003).
TheCoNLL data comes with parses from Charniak?sparser (Charniak, 2000), so we had to re-parsethe data and retrain the SRL system on these newparses, resulting in a lower baseline than previ-ously presented work.
We choose to use Stan-ford?s parser because of the ease with which wecould modify it to generate samples.
Unfortu-nately, its performance is slightly below that of theother parsers.The CoNLL data has two separate test sets; thefirst is section 23 of the Penn Treebank (PTB),and the second is ?fresh sentences?
taken from theBrown corpus.
For full results, please see Table 1.On the Penn Treebank portion we saw an absoluteF-score improvement of 0.7% on both core and allarguments.
On the Brown portion of the test set wesaw an improvement of 1.25% on core and 1.16%on all arguments.
In this context, a gain of over1% is quite large: for instance, the scores for thetop 4 systems on the Brown data at CoNLL 2005were within 1% of each other.
For both portions,we generated 50 samples, and did this 4 times, av-eraging the results.
We most likely saw better per-formance on the Brown portion than the PTB por-tion because the parser was trained on the PennTreebank training data, so the most likely parseswill be of higher quality for the PTB portion ofthe test data than for the Brown portion.
We also624RTE ResultsAccuracy Average PrecisionGreedy 59.13% 59.91%Sampling 60.88% 61.99%Table 2: Results for recognizing textual entailment.
The sam-pled numbers are averaged over several runs, as discussed.ran the pipeline using a 50-best list, and found thetwo results to be comparable.6.2 Textual EntailmentFor the second PASCAL RTE challenge, two dif-ferent types of performance measures were usedto evaluate labels and confidence of the labels forthe text-hypothesis pairs.
The first measure is ac-curacy ?
the percentage of correct judgments.
Thesecond measure is average precision.
Responsesare sorted based on entailment confidence and thenaverage precision is calculated by the followingequation:1Rn?i=1E(i)# correct up to pair ii (5)where n is the size of the test set, R is the numberof positive (entailed) examples, E(i) is an indi-cator function whose value is 1 if the ith pair isentailed, and the is are sorted based on the entail-ment confidence.
The intention of this measure isto evaluate how well calibrated a system is.
Sys-tems which are more confident in their correct an-swers and less confident in their incorrect answerswill perform better on this measure.Our results are presented in Table 2.
We gen-erated 25 samples for each run, and repeated theprocess 7 times, averaging over runs.
Accuracywas improved by 1.5% and average precision by2%.
It does not come as a surprise that the averageprecision improvement was larger than the accu-racy improvement, because our model explicitlyestimates its own degree of confidence by estimat-ing the posterior probability of the class label.7 Conclusions and Future WorkWe have presented a method for handling lan-guage processing pipelines in which later stagesof processing are conditioned on the results ofearlier stages.
Currently, common practice is totake the best labeling at each point in a linguisticanalysis pipeline, but this method ignores informa-tion about alternate labelings and their likelihoods.Our approach uses all of the information available,and has the added advantage of being extremelysimple to implement.
By modifying your subtasksto generate samples instead of the most likely la-beling, our method can be used with very little ad-ditional overhead.
And, as we have shown, suchmodifications are usually simple to make; further,with only a ?small?
(polynomial) number of sam-ples k, under mild assumptions the classificationerror obtained by the sampling approximation ap-proaches that of exact inference.
(Ng and Jordan,2001) In contrast, an algorithm that keeps trackonly of the k-best list enjoys no such theoreticalguarantee, and can require an exponentially largevalue for k to approach comparable error.
We alsonote that in practice, k-best lists are often morecomplicated to implement and more computation-ally expensive (e.g.
the complexity of generat-ing k sample parses or CRF outputs is substan-tially lower than that of generating the k best parsederivations or CRF outputs).The major contribution of this work is notspecific to semantic role labeling or recognizingtextual entailment.
We are proposing a generalmethod to deal with all multi-stage algorithms.
Itis common to build systems using many differentsoftware packages, often from other groups, and tostring together the 1-best outputs.
If, instead, allNLP researchers wrote packages which can gen-erate samples from the posterior, then the entireNLP community could use this method as easilyas they can use the greedy methods that are com-mon today, and which do not perform as well.One possible direction for improvement of thiswork would be to move from a Bayesian networkto an undirected Markov network.
This is desir-able because influence should be able to flow inboth directions in this pipeline.
For example, thesemantic role labeler should be able to tell theparser that it did not like a particular parse, andthis should influence the probability assigned tothat parse.
The main difficulty here lies in howto model this reversal of influence.
The problemof using parse trees to help decide good semanticrole labelings is well studied, but the problem ofusing semantic role labelings to influence parses isnot.
Furthermore, this requires building joint mod-els over adjacent nodes, which is usually a non-trivial task.
However, we feel that this approachwould improve performance even more on thesepipelined tasks and should be pursued.6258 AcknowledgementsWe would like to thank our anonymous review-ers for their comments and suggestions.
Wewould also like to thank Kristina Toutanova, AriaHaghighi and the Stanford RTE group for their as-sistance in understanding and using their code.This paper is based on work funded in part by aStanford School of Engineering fellowship and inpart by the Defense Advanced Research ProjectsAgency through IBM.
The content does not nec-essarily reflect the views of the U.S. Government,and no official endorsement should be inferred.ReferencesRens Bod.
1995.
The problem of computing the most proba-ble tree in data-oriented parsing and stochastic tree gram-mars.
In Proceedings of EACL 1995.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introduction tothe CoNLL-2004 shared task: Semantic role labeling.
InProceedings of CoNLL 2004.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL 2005.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 14th National Conferenceon Artificial Intelligence.Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, andDavid J. Spiegelhalter.
2003.
Probabilistic Networks andExpert Systems.
Springer.Richard Crouch.
2005.
Packed rewriting for mapping se-mantics to KR.
In Proceedings of the 6th InternationalWorkshop on Computational Semantics.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.The PASCAL recognizing textual entailment challenge.
InProceedings of the PASCAL Challenges Workshop on Rec-ognizing Textual Entailment.Stuart Geman and Mark Johnson.
2002.
Dynamic program-ming for parsing and estimation of stochastic unification-based grammars.
In Proceedings of ACL 2002.Joshua Goodman.
1998.
Parsing Inside-Out.
Ph.D. thesis,Harvard University.Aria Haghighi, Kristina Toutanova, and Christopher D. Man-ning.
2005.
A joint model for semantic role labeling.
InProceedings of CoNLL 2005.Liang Huang and David Chiang.
2005.
Better k-best pars-ing.
In Proceedings of the 9th International Workshop onParsing Technologies.Lauri Karttunen.
2000.
Applications of finite-state trans-ducers in natural-language processing.
In Proceesings ofthe Fifth International Conference on Implementation andApplication of Automata.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL 2003.Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen tauYih.
2005.
Generalized inference with multiple semanticrole labeling systems.
In Proceedings of CoNLL 2005,pages 181?184.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceed-ings of the Eighteenth International Conference on Ma-chine Learning, pages 282?289.Bill MacCartney, Trond Grenager, Marie de Marneffe, DanielCer, and Christopher D. Manning.
2006.
Learning to rec-ognize features of valid textual entailments.
In Proceed-ings of NAACL-HTL 2006.Christopher D. Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.
TheMIT Press, Cambridge, Massachusetts.John T. Maxwell, III and Ronald M. Kaplan.
1995.
A methodfor disjunctive constraint satisfaction.
In Mary Dalrymple,Ronald M. Kaplan, John T. Maxwell III, and Annie Zae-nen, editors, Formal Issues in Lexical-Functional Gram-mar, number 47 in CSLI Lecture Notes Series, chapter 14,pages 381?481.
CSLI Publications.Andrew Ng and Michael Jordan.
2001.
Convergence rates ofthe voting Gibbs classifier, with application to Bayesianfeature selection.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning.Charles Sutton and Andrew McCallum.
2005.
Joint pars-ing and semantic role labeling.
In Proceedings of CoNLL2005, pages 225?228.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
2005.
Joint learning improves semantic role label-ing.
In Proceedings of ACL 2005.Kristina Toutanova.
2005.
Effective statistical models for syn-tactic and semantic disambiguation.
Ph.D. thesis, Stan-ford University.Ben Wellner, Andrew McCallum, Fuchun Peng, and MichaelHay.
2004.
An integrated, conditional model of informa-tion extraction and coreference with application to citationmatching.
In Proceedings of the 20th Annual Conferenceon Uncertainty in Artificial Intelligence.626
