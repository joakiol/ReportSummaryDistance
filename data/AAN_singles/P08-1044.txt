Proceedings of ACL-08: HLT, pages 380?388,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsWhich words are hard to recognize?Prosodic, lexical, and disfluency factors that increase ASR error ratesSharon Goldwater, Dan Jurafsky and Christopher D. ManningDepartment of Linguistics and Computer ScienceStanford University{sgwater,jurafsky,manning}@stanford.eduAbstractMany factors are thought to increase thechances of misrecognizing a word in ASR,including low frequency, nearby disfluencies,short duration, and being at the start of a turn.However, few of these factors have been for-mally examined.
This paper analyzes a varietyof lexical, prosodic, and disfluency factors todetermine which are likely to increase ASR er-ror rates.
Findings include the following.
(1)For disfluencies, effects depend on the type ofdisfluency: errors increase by up to 15% (ab-solute) for words near fragments, but decreaseby up to 7.2% (absolute) for words near repeti-tions.
This decrease seems to be due to longerword duration.
(2) For prosodic features, thereare more errors for words with extreme valuesthan words with typical values.
(3) Althoughour results are based on output from a systemwith speaker adaptation, speaker differencesare a major factor influencing error rates, andthe effects of features such as frequency, pitch,and intensity may vary between speakers.1 IntroductionIn order to improve the performance of automaticspeech recognition (ASR) systems on conversationalspeech, it is important to understand the factorsthat cause problems in recognizing words.
Previouswork on recognition of spontaneous monologuesand dialogues has shown that infrequent words aremore likely to be misrecognized (Fosler-Lussier andMorgan, 1999; Shinozaki and Furui, 2001) and thatfast speech increases error rates (Siegler and Stern,1995; Fosler-Lussier and Morgan, 1999; Shinozakiand Furui, 2001).
Siegler and Stern (1995) andShinozaki and Furui (2001) also found higher er-ror rates in very slow speech.
Word length (inphones) has also been found to be a useful pre-dictor of higher error rates (Shinozaki and Furui,2001).
In Hirschberg et al?s (2004) analysis oftwo human-computer dialogue systems, misrecog-nized turns were found to have (on average) highermaximum pitch and energy than correctly recog-nized turns.
Results for speech rate were ambiguous:faster utterances had higher error rates in one corpus,but lower error rates in the other.
Finally, Adda-Decker and Lamel (2005) demonstrated that bothFrench and English ASR systems had more troublewith male speakers than female speakers, and foundseveral possible explanations, including higher ratesof disfluencies and more reduction.Many questions are left unanswered by these pre-vious studies.
In the word-level analyses of Fosler-Lussier and Morgan (1999) and Shinozaki and Fu-rui (2001), only substitution and deletion errors wereconsidered, so we do not know how including inser-tions might affect the results.
Moreover, these stud-ies primarily analyzed lexical, rather than prosodic,factors.
Hirschberg et al?s (2004) work suggests thatprosodic factors can impact error rates, but leavesopen the question of which factors are important atthe word level and how they influence recognitionof natural conversational speech.
Adda-Decker andLamel?s (2005) suggestion that higher rates of dis-fluency are a cause of worse recognition for malespeakers presupposes that disfluencies raise errorrates.
While this assumption seems natural, it hasyet to be carefully tested, and in particular we do not380know whether disfluent words are associated witherrors in adjacent words, or are simply more likely tobe misrecognized themselves.
Other factors that areoften thought to affect a word?s recognition, such asits status as a content or function word, and whetherit starts a turn, also remain unexamined.The present study is designed to address all ofthese questions by analyzing the effects of a widerange of lexical and prosodic factors on the accu-racy of an English ASR system for conversationaltelephone speech.
In the remainder of this paper, wefirst describe the data set used in our study and intro-duce a new measure of error, individual word errorrate (IWER), that allows us to include insertion er-rors in our analysis, along with deletions and substi-tutions.
Next, we present the features we collectedfor each word and the effects of those features indi-vidually on IWER.
Finally, we develop a joint sta-tistical model to examine the effects of each featurewhile controlling for possible correlations.2 DataFor our analysis, we used the output from theSRI/ICSI/UW RT-04 CTS system (Stolcke et al,2006) on the NIST RT-03 development set.
This sys-tem?s performance was state-of-the-art at the time ofthe 2004 evaluation.
The data set contains 36 tele-phone conversations (72 speakers, 38477 referencewords), half from the Fisher corpus and half fromthe Switchboard corpus.1The standard measure of error used in ASR isword error rate (WER), computed as 100(I + D +S)/R, where I,D and S are the number of inser-tions, deletions, and substitutions found by align-ing the ASR hypotheses with the reference tran-scriptions, and R is the number of reference words.Since we wish to know what features of a referenceword increase the probability of an error, we needa way to measure the errors attributable to individ-ual words ?
an individual word error rate (IWER).We assume that a substitution or deletion error canbe assigned to its corresponding reference word, butfor insertion errors, there may be two adjacent ref-erence words that could be responsible.
Our so-lution is to assign any insertion errors to each of1These conversations are not part of the standard Fisher andSwitchboard corpora used to train most ASR systems.Ins Del Sub Total % dataFull word 1.6 6.9 10.5 19.0 94.2Filled pause 0.6 ?
16.4 17.0 2.8Fragment 2.3 ?
17.3 19.6 2.0Backchannel 0.3 30.7 5.0 36.0 0.6Guess 1.6 ?
30.6 32.1 0.4Total 1.6 6.7 10.9 19.7 100Table 1: Individual word error rates for different wordtypes, and the proportion of words belonging to eachtype.
Deletions of filled pauses, fragments, and guessesare not counted as errors in the standard scoring method.the adjacent words.
We could then define IWER as100(ni + nd + ns)/R, where ni, nd, and ns are theinsertion, deletion, and substitution counts for indi-vidual words (with nd = D and ns = S).
In general,however, ni > I , so that the IWER for a given dataset would be larger than the WER.
To facilitate com-parisons with standard WER, we therefore discountinsertions by a factor ?, such that ?ni = I .
In thisstudy, ?
= .617.3 Analysis of individual features3.1 FeaturesThe reference transcriptions used in our analysisdistinguish between five different types of words:filled pauses (um, uh), fragments (wh-, redistr-),backchannels (uh-huh, mm-hm), guesses (where thetranscribers were unsure of the correct words), andfull words (everything else).
Error rates for eachof these types can be found in Table 1.
The re-mainder of our analysis considers only the 36159 in-vocabulary full words in the reference transcriptions(70 OOV full words are excluded).
We collected thefollowing features for these words:Speaker sex Male or female.Broad syntactic class Open class (e.g., nouns andverbs), closed class (e.g., prepositions and articles),or discourse marker (e.g., okay, well).
Classes wereidentified using a POS tagger (Ratnaparkhi, 1996)trained on the tagged Switchboard corpus.Log probability The unigram log probability ofeach word, as listed in the system?s language model.Word length The length of each word (in phones),determined using the most frequent pronunciation381BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFryeah i i i think you should um ask for the ref- recommendationFigure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, andfragments; first, middle, and last words in a repeated sequence.found for that word in the recognition lattices.Position near disfluency A collection of featuresindicating whether a word occurred before or after afilled pause, fragment, or repeated word; or whetherthe word itself was the first, last, or other word in asequence of repetitions.
Figure 1 illustrates.
Onlyidentical repeated words with no intervening wordsor filled pauses were considered repetitions.First word of turn Turn boundaries were assignedautomatically at the beginning of any utterance fol-lowing a pause of at least 100 ms during which theother speaker spoke.Speech rate The average speech rate (in phones persecond) was computed for each utterance using thepronunciation dictionary extracted from the latticesand the utterance boundary timestamps in the refer-ence transcriptions.In addition to the above features, we used Praat(Boersma and Weenink, 2007) to collect the follow-ing additional prosodic features on a subset of thedata obtained by excluding all contractions:2Pitch The minimum, maximum, mean, and rangeof pitch for each word.Intensity The minimum, maximum, mean, andrange of intensity for each word.Duration The duration of each word.31017 words (85.8% of the full-word data set) re-main in the no-contractions data set after removingwords for which pitch and/or intensity features couldnot be extracted.2Contractions were excluded before collecting prosodic fea-tures for the following reason.
In the reference transcriptionsand alignments used for scoring ASR systems, contractions aretreated as two separate words.
However, aside from speech rate,our prosodic features were collected using word-by-word times-tamps from a forced alignment that used a transcription wherecontractions are treated as single words.
Thus, the start and endtimes for a contraction in the forced alignment correspond totwo words in the alignments used for scoring, and it is not clearhow to assign prosodic features appropriately to those words.3.2 Results and discussionResults of our analysis of individual features can befound in Table 2 (for categorical features) and Figure2 (for numeric features).
Comparing the error ratesfor the full-word and the no-contractions data sets inTable 2 verifies that removing contractions does notcreate systematic changes in the patterns of errors,although it does lower error rates (and significancevalues) slightly overall.
(First and middle repetitionsare combined as non-final repetitions in the table,because only 52 words were middle repetitions, andtheir error rates were similar to initial repetitions.
)3.2.1 Disfluency featuresPerhaps the most interesting result in Table 2 isthat the effects of disfluencies are highly variable de-pending on the type of disfluency and the positionof a word relative to it.
Non-final repetitions andwords next to fragments have an IWER up to 15%(absolute) higher than the average word, while fi-nal repetitions and words following repetitions havean IWER up to 7.2% lower.
Words occurring be-fore repetitions or next to filled pauses do not havesignificantly different error rates than words not inthose positions.
Our results for repetitions supportShriberg?s (1995) hypothesis that the final word of arepeated sequence is in fact fluent.3.2.2 Other categorical featuresOur results support the common wisdom thatopen class words have lower error rates than otherwords (although the effect we find is small), and thatwords at the start of a turn have higher error rates.Also, like Adda-Decker and Lamel (2005), we findthat male speakers have higher error rates than fe-males, though in our data set the difference is morestriking (3.6% absolute, compared to their 2.0%).3.2.3 Word probability and word lengthTurning to Figure 2, we find (consistent with pre-vious results) that low-probability words have dra-matically higher error rates than high-probability382Filled Pau.
Fragment Repetition Syntactic Class SexBef Aft Bef Aft Bef Aft NonF Fin Clos Open Disc 1st M F All(a) IWER 17.6 16.9 33.8 21.6 16.7 13.8 26.0 11.6 19.7 18.0 19.6 21.2 20.6 17.0 18.8% wds 1.7 1.7 1.6 1.5 0.7 0.9 1.2 1.1 43.8 50.5 5.8 6.2 52.5 47.5 100(b) IWER 17.6 17.2 32.0 21.5 15.8 14.2 25.1 11.6 18.8 17.8 19.0 20.3 20.0 16.4 18.3% wds 1.9 1.8 1.6 1.5 0.8 0.8 1.4 1.1 43.9 49.6 6.6 6.4 52.2 47.8 100Table 2: IWER by feature and percentage of words exhibiting each feature for (a) the full-word data set and (b) the no-contractions data set.
Error rates that are significantly different for words with and without a given feature (computedusing 10,000 samples in a Monte Carlo permutation test) are in bold (p < .05) or bold italics (p < .005).
Featuresshown are whether a word occurs before or after a filled pause, fragment, or repetition; is a non-final or final repetition;is open class, closed class, or a discourse marker; is the first word of a turn; or is spoken by a male or female.
All isthe IWER for the entire data set.
(Overall IWER is slightly lower than in Table 1 due to the removal of OOV words.)words.
More surprising is that word length inphones does not seem to have a consistent effect onIWER.
Further analysis reveals a possible explana-tion: word length is correlated with duration, butanti-correlated to the same degree with log proba-bility (the Kendall ?
statistics are .50 and -.49).
Fig-ure 2 shows that words with longer duration havelower IWER.
Since words with more phones tend tohave longer duration, but lower frequency, there isno overall effect of length.3.2.4 Prosodic featuresFigure 2 shows that means of pitch and intensityhave relatively little effect except at extreme val-ues, where more errors occur.
In contrast, pitchand intensity range show clear linear trends, withgreater range of pitch or intensity leading to lowerIWER.3 As noted above, decreased duration is as-sociated with increased IWER, and (as in previouswork), we find that IWER increases dramaticallyfor fast speech.
We also see a tendency towardshigher IWER for very slow speech, consistent withShinozaki and Furui (2001) and Siegler and Stern(1995).
The effects of pitch minimum and maximumare not shown for reasons of space, but are similarto pitch mean.
Also not shown are intensity mini-mum (with more errors at higher values) and inten-sity maximum (with more errors at lower values).For most of our prosodic features, as well as logprobability, extreme values seem to be associated3Our decision to use the log transform of pitch range wasoriginally based on the distribution of pitch range values in thedata set.
Exploratory data analysis also indicated that using thetransformed values would likely lead to a better model fit (Sec-tion 4) than using the raw values.with worse recognition than average values.
We ex-plore this possibility further in Section 4.4 Analysis using a joint modelIn the previous section, we investigated the effectsof various individual features on ASR error rates.However, there are many correlations between thesefeatures ?
for example, words with longer durationare likely to have a larger range of pitch and inten-sity.
In this section, we build a single model with allof our features as potential predictors in order to de-termine the effects of each feature after controllingfor the others.
We use the no-contractions data set sothat we can include prosodic features in our model.Since only 1% of tokens have an IWER > 1, wesimplify modeling by predicting only whether eachtoken is responsible for an error or not.
That is, ourdependent variable is binary, taking on the value 1 ifIWER > 0 for a given token and 0 otherwise.4.1 ModelTo model data with a binary dependent variable, alogistic regression model is an appropriate choice.In logistic regression, we model the log odds as alinear combination of feature values x0 .
.
.
xn:log p1 ?
p = ?0x0 + ?1x1 + .
.
.
+ ?nxnwhere p is the probability that the outcome occurs(here, that a word is misrecognized) and ?0 .
.
.
?nare coefficients (feature weights) to be estimated.Standard logistic regression models assume that allcategorical features are fixed effects, meaning thatall possible values for these features are known inadvance, and each value may have an arbitrarily dif-ferent effect on the outcome.
However, features3832 4 6 8 1002040Word length (phones)IWER100 200 30002040Pitch mean (Hz)50 60 70 8002040Intensity mean (dB)0.0 0.2 0.4 0.6 0.8 1.002040Duration (sec)?5 ?4 ?3 ?202040Log probabilityIWER1 2 3 4 502040log(Pitch range) (Hz)IWER10 30 5002040Intensity range (dB)5 10 15 2002040Speech rate (phones/sec)Figure 2: Effects of numeric features on IWER of the SRI system for the no-contractions data set.
All feature valueswere binned, and the average IWER for each bin is plotted, with the area of the surrounding circle proportional to thenumber of points in the bin.
Dotted lines show the average IWER over the entire data set.such as speaker identity do not fit this pattern.
In-stead, we control for speaker differences by assum-ing that speaker identity is a random effect, mean-ing that the speakers observed in the data are a ran-dom sample from a larger population.
The base-line probability of error for each speaker is thereforeassumed to be a normally distributed random vari-able, with mean equal to the population mean, andvariance to be estimated by the model.
Stated dif-ferently, a random effect allows us to add a factorto the model for speaker identity, without allowingarbitrary variation in error rates between speakers.Models such as ours, with both fixed and randomeffects, are known as mixed-effects models, and arebecoming a standard method for analyzing linguis-tic data (Baayen, 2008).
We fit our models using thelme4 package (Bates, 2007) of R (R DevelopmentCore Team, 2007).To analyze the joint effects of all of our features,we initially built as large a model as possible, andused backwards elimination to remove features oneat a time whose presence did not contribute signifi-cantly (at p ?
.05) to model fit.
All of the featuresshown in Table 2 were converted to binary variablesand included as predictors in our initial model, alongwith a binary feature controlling for corpus (Fisheror Switchboard), and all numeric features in Figure2.
We did not include minimum and maximum val-ues for pitch and intensity because they are highlycorrelated with the mean values, making parameterestimation in the combined model difficult.
Prelimi-nary investigation indicated that using the mean val-ues would lead to the best overall fit to the data.In addition to these basic fixed effects, our ini-tial model included quadratic terms for all of the nu-meric features, as suggested by our analysis in Sec-tion 3, as well as random effects for speaker iden-tity and word identity.
All numeric features wererescaled to values between 0 and 1 so that coeffi-cients are comparable.4.2 Results and discussionFigure 3 shows the estimated coefficients and stan-dard errors for each of the fixed effect categoricalfeatures remaining in the reduced model (i.e., afterbackwards elimination).
Since all of the features arebinary, a coefficient of ?
indicates that the corre-sponding feature, when present, adds a weight of ?to the log odds (i.e., multiplies the odds of an errorby a factor of e?).
Thus, features with positive co-efficients increase the odds of an error, and featureswith negative coefficients decrease the odds of an er-ror.
The magnitude of the coefficient corresponds tothe size of the effect.Interpreting the coefficients for our numeric fea-tures is less intuitive, since most of these variableshave both linear and quadratic effects.
The contribu-tion to the log odds of a particular numeric feature384?1.5 ?1.0 ?0.5 0.0 0.5 1.0corpus=SWsex=Mstarts turnbefore FPafter FPbefore fragafter fragnon?final repopen classFigure 3: Estimates and standard errors of the coefficientsfor the categorical predictors in the reduced model.xi, with linear and quadratic coefficients a and b, isaxi + bx2i .
We plot these curves for each numericfeature in Figure 4.
Values on the x axes with posi-tive y values indicate increased odds of an error, andnegative y values indicate decreased odds of an er-ror.
The x axes in these plots reflect the rescaledvalues of each feature, so that 0 corresponds to theminimum value in the data set, and 1 to the maxi-mum value.4.2.1 DisfluenciesIn our analysis of individual features, we foundthat different types of disfluencies have different ef-fects: non-final repeated words and words near frag-ments have higher error rates, while final repetitionsand words following repetitions have lower errorrates.
After controlling for other factors, a differ-ent picture emerges.
There is no longer an effect forfinal repetitions or words after repetitions; all otherdisfluency features increase the odds of an error bya factor of 1.3 to 2.9.
These differences from Sec-tion 3 can be explained by noting that words nearfilled pauses and repetitions have longer durationsthan other words (Bell et al, 2003).
Longer durationlowers IWER, so controlling for duration reveals thenegative effect of the nearby disfluencies.
Our re-sults are also consistent with Shriberg?s (1995) find-ings on fluency in repeated words, since final rep-etitions have no significant effect in our combinedmodel, while non-final repetitions incur a penalty.4.2.2 Other categorical featuresWithout controlling for other lexical or prosodicfeatures, we found that a word is more likely tobe misrecognized at the beginning of a turn, andless likely to be misrecognized if it is an open classword.
According to our joint model, these effectsstill hold even after controlling for other features.Similarly, male speakers still have higher error ratesthan females.
This last result sheds some light onthe work of Adda-Decker and Lamel (2005), whosuggested several factors that could explain males?higher error rates.
In particular, they showed thatmales have higher rates of disfluency, produce wordswith slightly shorter durations, and use more alter-nate (?sloppy?)
pronunciations.
Our joint modelcontrols for the first two of these factors, suggestingthat the third factor or some other explanation mustaccount for the remaining differences between malesand females.
One possibility is that female speech ismore easily recognized because females tend to haveexpanded vowel spaces (Diehl et al, 1996), a factorthat is associated with greater intelligibility (Brad-low et al, 1996) and is characteristic of genres withlower ASR error rates (Nakamura et al, 2008).4.2.3 Prosodic featuresExamining the effects of pitch and intensity indi-vidually, we found that increased range for these fea-tures is associated with lower IWER, while higherpitch and extremes of intensity are associated withhigher IWER.
In the joint model, we see the sameeffect of pitch mean and an even stronger effect forintensity, with the predicted odds of an error dra-matically higher for extreme intensity values.
Mean-while, we no longer see a benefit for increased pitchrange and intensity; rather, we see small quadraticeffects for both features, i.e.
words with averageranges of pitch and intensity are recognized moreeasily than words with extreme values for these fea-tures.
As with disfluencies, we hypothesize that thelinear trends observed in Section 3 are primarily dueto effects of duration, since duration is moderatelycorrelated with both log pitch range (?
= .35) andintensity range (?
= .41).Our final two prosodic features, duration andspeech rate, showed strong linear and weakquadratic trends when analyzed individually.
Ac-cording to our model, both duration and speech rateare still important predictors of error after control-ling for other features.
However, as with the otherprosodic features, predictions of the joint model aredominated by quadratic trends, i.e., predicted errorrates are lower for average values of duration andspeech rate than for extreme values.Overall, the results from our joint analysis suggest3850.0 0.4 0.8?404Word lengthlogoddsy = ?0.8x0.0 0.4 0.8?404Pitch meanlogoddsy = 1x0.0 0.4 0.8?404Intensity meanlogoddsy = ?13.2x + 11.5x20.0 0.4 0.8?404Durationlogoddsy = ?12.6x + 14.6x20.0 0.4 0.8?404Log probabilitylogoddsy = ?0.6x + 4.1x20.0 0.4 0.8?404log(Pitch range)logoddsy = ?2.3x + 2.2x20.0 0.4 0.8?404Intensity rangelogoddsy = ?1x + 1.2x20.0 0.4 0.8?404Speech ratelogoddsy = ?3.9x + 4.4x2Figure 4: Predicted effect on the log odds of each numeric feature, including linear and (if applicable) quadratic terms.Model Neg.
log lik.
Diff.
dfFull 12932 0 32Reduced 12935 3 26No lexical 13203 271 16No prosodic 13387 455 20No speaker 13432 500 31No word 13267 335 31Baseline 14691 1759 1Table 3: Fit to the data of various models.
Degrees offreedom (df) for each model is the number of fixed ef-fects plus the number of random effects plus 1 (for theintercept).
Full model contains all predictors; Reducedcontains only predictors contributing significantly to fit;Baseline contains only intercept.
Other models are ob-tained by removing features from Full.
Diff is the differ-ence in log likelihood between each model and Full.that, after controlling for other factors, extreme val-ues for prosodic features are associated with worserecognition than typical values.4.2.4 Differences between lexical itemsAs discussed above, our model contains a randomeffect for word identity, to control for the possibil-ity that certain lexical items have higher error ratesthat are not explained by any of the other factorsin the model.
It is worth asking whether this ran-dom effect is really necessary.
To address this ques-tion, we compared the fit to the data of two models,each containing all of our fixed effects and a ran-dom effect for speaker identity.
One model also con-tained a random effect for word identity.
Results areshown in Table 3.
The model without a random ef-fect for word identity is significantly worse than thefull model; in fact, this single parameter is more im-portant than all of the lexical features combined.
Tosee which lexical items are causing the most diffi-culty, we examined the items with the highest esti-mated increases in error.
The top 20 items on thislist include yup, yep, yes, buy, then, than, and r., allof which are acoustically similar to each other or toother high-frequency words, as well as the words af-ter, since, now, and though, which occur in manysyntactic contexts, making them difficult to predictbased on the language model.4.2.5 Differences between speakersWe examined the importance of the random effectfor speaker identity in a similar fashion to the ef-fect for word identity.
As shown in Table 3, speakeridentity is a very important factor in determining theprobability of error.
That is, the lexical and prosodicvariables examined here are not sufficient to fullyexplain the differences in error rates between speak-ers.
In fact, the speaker effect is the single most im-portant factor in the model.Given that the differences in error rates betweenspeakers are so large (average IWER for differentspeakers ranges from 5% to 51%), we wonderedwhether our model is sufficient to capture the kindsof speaker variation that exist.
The model assumesthat each speaker has a different baseline error rate,but that the effects of each variable are the same foreach speaker.
Determining the extent to which thisassumption is justified is beyond the scope of thispaper, however we present some suggestive resultsin Figure 5.
This figure illustrates some of the dif-38640 60 800.00.20.4Intensity mean (dB)FittedP(err)100 250 4000.00.20.4Pitch mean (Hz)0.0 0.5 1.0 1.50.00.20.4Duration (sec)?6 ?5 ?4 ?3 ?20.00.20.4Neg.
log prob.0 5 10 200.00.20.4Sp.
rate (ph/sec)40 60 800.00.20.4Intensity mean (dB)FittedP(err)100 250 4000.00.20.4Pitch mean (Hz)0.0 0.5 1.0 1.50.00.20.4Duration (sec)?6 ?5 ?4 ?3 ?20.00.20.4Neg.
log prob.0 5 10 200.00.20.4Sp.
rate (ph/sec)Figure 5: Estimated effects of various features on the error rates of two different speakers (top and bottom).
Dashedlines illustrate the baseline probability of error for each speaker.
Solid lines were obtained by fitting a logistic regres-sion model to each speaker?s data, with the variable labeled on the x-axis as the only predictor.ferences between two speakers chosen fairly arbi-trarily from our data set.
Not only are the baselineerror rates different for the two speakers, but the ef-fects of various features appear to be very different,in one case even reversed.
The rest of our data setexhibits similar kinds of variability for many of thefeatures we examined.
These differences in ASR be-havior between speakers are particularly interestingconsidering that the system we investigated here al-ready incorporates speaker adaptation models.5 ConclusionIn this paper, we introduced the individual word er-ror rate (IWER) for measuring ASR performanceon individual words, including insertions as well asdeletions and substitutions.
Using IWER, we ana-lyzed the effects of various word-level lexical andprosodic features, both individually and in a jointmodel.
Our analysis revealed the following effects.
(1) Words at the start of a turn have slightly higherIWER than average, and open class (content) wordshave slightly lower IWER.
These effects persist evenafter controlling for other lexical and prosodic fac-tors.
(2) Disfluencies heavily impact error rates:IWER for non-final repetitions and words adjacentto fragments rises by up to 15% absolute, whileIWER for final repetitions and words following rep-etitions decreases by up to 7.2% absolute.
Control-ling for prosodic features eliminates the latter ben-efit, and reveals a negative effect of adjacent filledpauses, suggesting that the effects of these disfluen-cies are normally obscured by the greater duration ofnearby words.
(3) For most acoustic-prosodic fea-tures, words with extreme values have worse recog-nition than words with average values.
This effectbecomes much more pronounced after controllingfor other factors.
(4) After controlling for lexicaland prosodic characteristics, the lexical items withthe highest error rates are primarily homophones ornear-homophones (e.g., buy vs. by, then vs.
than).
(5) Speaker differences account for much of the vari-ance in error rates between words.
Moreover, the di-rection and strength of effects of different prosodicfeatures may vary between speakers.While we plan to extend our analysis to otherASR systems in order to determine the generalityof our findings, we have already gained importantinsights into a number of factors that increase ASRerror rates.
In addition, our results suggest a richarea for future research in further analyzing the vari-ability of both lexical and prosodic effects on ASRbehavior for different speakers.AcknowledgmentsThis work was supported by the Edinburgh-StanfordLINK and ONR MURI award N000140510388.
Wethank Andreas Stolcke for providing the ASR out-put, language model, and forced alignments usedhere, and Raghunandan Kumaran and Katrin Kirch-hoff for earlier datasets and additional help.387ReferencesM.
Adda-Decker and L. Lamel.
2005.
Do speech rec-ognizers prefer female speakers?
In Proceedings ofINTERSPEECH, pages 2205?2208.R.
H. Baayen.
2008.
Analyzing Linguistic Data.
APractical Introduction to Statistics.
CambridgeUniversity Press.
Prepublication version available athttp://www.mpi.nl/world/persons/private/baayen/pub-lications.html.Douglas Bates, 2007. lme4: Linear mixed-effects modelsusing S4 classes.
R package version 0.99875-8.A.
Bell, D. Jurafsky, E. Fosler-Lussier, C. Girand,M.
Gregory, and D. Gildea.
2003.
Effects of disflu-encies, predictability, and utterance position on wordform variation in English conversation.
Journal of theAcoustical Society of America, 113(2):1001?1024.P.
Boersma and D. Weenink.
2007.
Praat:doing phonetics by computer (version 4.5.16).http://www.praat.org/.A.
Bradlow, G. Torretta, and D. Pisoni.
1996.
Intelli-gibility of normal speech I: Global and fine-grainedacoustic-phonetic talker characteristics.
Speech Com-munication, 20:255?272.R.
Diehl, B. Lindblom, K. Hoemeke, and R. Fahey.
1996.On explaining certain male-female differences in thephonetic realization of vowel categories.
Journal ofPhonetics, 24:187?208.E.
Fosler-Lussier and N. Morgan.
1999.
Effects ofspeaking rate and word frequency on pronunciationsin conversational speech.
Speech Communication,29:137?
158.J.
Hirschberg, D. Litman, and M. Swerts.
2004.
Prosodicand other cues to speech recognition failures.
SpeechCommunication, 43:155?
175.M.
Nakamura, K. Iwano, and S. Furui.
2008.
Differ-ences between acoustic characteristics of spontaneousand read speech and their effects on speech recogni-tion performance.
Computer Speech and Language,22:171?
184.R Development Core Team, 2007.
R: A Language andEnvironment for Statistical Computing.
R Foundationfor Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.A.
Ratnaparkhi.
1996.
A Maximum Entropy model forpart-of-speech tagging.
In Proceedings of the FirstConference on Empirical Methods in Natural Lan-guage Processing, pages 133?142.T.
Shinozaki and S. Furui.
2001.
Error analysis using de-cision trees in spontaneous presentation speech recog-nition.
In Proceedings of ASRU 2001.E.
Shriberg.
1995.
Acoustic properties of disfluent rep-etitions.
In Proceedings of the International Congressof Phonetic Sciences, volume 4, pages 384?387.M.
Siegler and R. Stern.
1995.
On the effects of speechrate in large vocabulary speech recognition systems.In Proceedings of ICASSP.A.
Stolcke, B. Chen, H. Franco, V. R. R. Gadde, M. Gra-ciarena, M.-Y.
Hwang, K. Kirchhoff, A. Mandal,N.
Morgan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,A.
Venkataraman, D. Vergyri, W. Wang, J. Zheng, andQ.
Zhu.
2006.
Recent innovations in speech-to-texttranscription at SRI-ICSI-UW.
IEEE Transactions onAudio, Speech and Language Processing, 14(5):1729?1744.388
