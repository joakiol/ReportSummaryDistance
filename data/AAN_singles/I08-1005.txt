Semi-Supervised Learning for Relation ExtractionZHOU GuoDong    LI JunHui    QIAN LongHua    ZHU QiaomingJiangsu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and TechnologySoochow Univ., Suzhou, China 215006Email : {gdzhou, lijunhui, qianlonghua, qmzhu}@suda.edu.cnAbstractThis paper proposes a semi-supervised learn-ing method for relation extraction.
Given asmall amount of labeled data and a largeamount of unlabeled data, it first bootstraps amoderate number of weighted support vectorsvia SVM through a co-training procedure withrandom feature projection and then applies alabel propagation (LP) algorithm via the boot-strapped support vectors.
Evaluation on theACE RDC 2003 corpus shows that our methodoutperforms the normal LP algorithm via allthe available labeled data without SVM boot-strapping.
Moreover, our method can largelyreduce the computational burden.
This sug-gests that our proposed method can integratethe advantages of both SVM bootstrappingand label propagation.1 IntroductionRelation extraction is to detect and classify variouspredefined semantic relations between two entitiesfrom text and can be very useful in many NLP ap-plications such as question answering, e.g.
to an-swer the query ?Who is the president of the UnitedStates?
?, and information retrieval, e.g.
to expandthe query ?George W. Bush?
with ?the president ofthe United States?
via his relationship with ?theUnited States?.During the last decade, many methods havebeen proposed in relation extraction, such as su-pervised learning (Miller et al2000; Zelenko et al2003; Culota and Sorensen 2004; Zhao and Grish-man 2005; Zhang et al2006; Zhou et al2005,2006), semi-supervised learning (Brin 1998;Agichtein and Gravano 2000; Zhang 2004; Chen etal 2006), and unsupervised learning (Hasegawa etal 2004; Zhang et al2005).
Among these methods,supervised learning-based methods perform muchbetter than the other two alternatives.
However,their performance much depends on the availabilityof a large amount of manually labeled data and it isnormally difficult to adapt an existing system toother applications and domains.
On the other hand,unsupervised learning-based methods do not needthe definition of relation types and the availabilityof manually labeled data.
However, they fail toclassify exact relation types between two entitiesand their performance is normally very low.
Toachieve better portability and balance between hu-man efforts and performance, semi-supervisedlearning has drawn more and more attention re-cently in relation extraction and other NLP appli-cations.This paper proposes a semi-supervised learningmethod for relation extraction.
Given a smallamount of labeled data and a large amount of unla-beled data, our proposed method first bootstraps amoderate number of weighted support vectors fromall the available data via SVM using a co-trainingprocedure with random feature projection and thenapplies a label propagation (LP) algorithm to cap-ture the manifold structure in both the labeled andunlabeled data via the bootstrapped support vectors.Compared with previous methods, our method canintegrate the advantages of both SVM bootstrap-ping in learning critical instances for the labelingfunction and label propagation in capturing themanifold structure in both the labeled and unla-beled data to smooth the labeling function.The rest of this paper is as follows.
In Section 2,we review related semi-supervised learning workin relation extraction.
Then, the LP algorithm viabootstrapped support vectors is proposed in Sec-tion 3 while Section 4 shows the experimental re-sults.
Finally, we conclude our work in Section 5.2 Related WorkGenerally, supervised learning is preferable to un-supervised learning due to prior knowledge in the32annotated training data and better performance.However, the annotated data is usually expensiveto obtain.
Hence, there has been growing interest insemi-supervised learning, aiming at inducing clas-sifiers by leveraging a small amount of labeleddata and a large amount of unlabeled data.
Relatedwork in relation extraction using semi-supervisedlearning can be classified into two categories:bootstrapping-based (Brin 1998; Agichtein andGravano 2000; Zhang 2004) and label propaga-tion(LP)-based (Chen et al2006).Currently, bootstrapping-based methods domi-nate semi-supervised learning in relation extraction.Bootstrapping works by iteratively classifyingunlabeled instances and adding confidently classi-fied ones into labeled data using a model learnedfrom augmented labeled data in previous iteration.Brin (1998) proposed a bootstrapping-basedmethod on the top of a self-developed patternmatching-based classifier to exploit the dualitybetween patterns and relations.
Agichtein and Gra-vano (2000) shared much in common with Brin(1998).
They employed an existing pattern match-ing-based classifier (i.e.
SNoW) instead.
Zhang(2004) approached the much simpler relation clas-sification sub-task by bootstrapping on the top ofSVM.
Although bootstrapping-based methods haveachieved certain success, one problem is that theymay not be able to well capture the manifold struc-ture among unlabeled data.As an alternative to the bootstrapping-basedmethods, Chen et al(2006) employed a LP-basedmethod in relation extraction.
Compared withbootstrapping, the LP algorithm can effectivelycombine labeled data with unlabeled data in thelearning process by exploiting the manifold struc-ture (e.g.
the natural clustering structure) in boththe labeled and unlabeled data.
The rationale be-hind this algorithm is that the instances in high-density areas tend to carry the same labels.
The LPalgorithm has also been successfully applied inother NLP applications, such as word sense disam-biguation (Niu et al2005), text classification(Szummer and Jaakkola 2001; Blum and Chawla2001; Belkin and Niyogi 2002; Zhu and Ghahra-mani 2002; Zhu et al2003; Blum et al2004), andinformation retrieval (Yang et al2006).
However,one problem is its computational burden, espe-cially when a large amount of labeled and unla-beled data is taken into consideration.In order to take the advantages of both boot-strapping and label propagation, our proposedmethod propagates labels via bootstrapped supportvectors.
On the one hand, our method can wellcapture the manifold structure in both the labeledand unlabeled data.
On the other hand, our methodcan largely reduce the computational burden in thenormal LP algorithm via all the available data.3 Label Propagation via BootstrappedSupport VectorsThe idea behind our LP algorithm via bootstrappedsupport vectors is that, instead of propagating la-bels through all the available labeled data, ourmethod propagates labels through critical instancesin both the labeled and unlabeled data.
In this pa-per, we use SVM as the underlying classifier tobootstrap a moderate number of weighted supportvectors for this purpose.
This is based on an as-sumption that the manifold structure in both thelabeled and unlabeled data can be well preservedthrough the critical instances (i.e.
the weightedsupport vectors bootstrapped from all the availablelabeled and unlabeled data).
The reason why wechoose SVM is that it represents the state-of-the-art in machine learning research and there are goodimplementations of the algorithm available.
In par-ticular, SVMLight (Joachims 1998) is selected asour classifier.
For efficiency, we apply the one vs.others strategy, which builds K classifiers so as toseparate one class from all others.
Another reasonis that we can adopt the weighted support vectorsreturned by the bootstrapped SVMs as the criticalinstances, via which label propagation is done.3.1 Bootstrapping Support VectorsThis paper modifies the SVM bootstrapping algo-rithm BootProject(Zhang 2004) to bootstrap sup-port vectors.
Given a small amount of labeled dataand a large amount of unlabeled data, the modifiedBootProject algorithm bootstraps on the top ofSVM by iteratively classifying  unlabeled  in-stances  and moving   confidently  classified  onesinto  labeled data using a model learned from theaugmented labeled data in previous  iteration,  untilnot enough unlabeled instances can be classifiedconfidently.
Figure 1 shows the modified BootPro-ject algorithm for bootstrapping support vectors.33_________________________________________Assume:L :  the labeled data;U :  the unlabeled data;S :  the batch size (100 in our experiments);P :  the number of views(feature projections);r :   the number of classes (including all the rela-tion (sub)types and the non-relation)BEGINREPEATFOR i = 1 to P DOGenerate projected feature space iF  fromthe original feature space F ;Project both L  and U  onto iF , thus gener-ate iL  and iU ;Train SVM classifier ijSVM  on iL  for eachclass )1( rjr j K= ;Run ijSVM  on iU  for each class)1( rjr j K=END FORFind (at most) S instances in U  with thehighest agreement (with threshold 70% inour experiments) and the highest averageSVM-returned confidence value (withthreshold 1.0 in our experiments);Move them from U to L;UNTIL not enough unlabeled instances (lessthan 10 in our experiments) can be confidentlyclassified;Return all the (positive and negative) supportvectors  included in all the latest SVM classifi-ers ijSVM  with their collective weight (abso-lute alpha*y) information as the set ofbootstrapped support vectors to act as the la-beled data in the LP algorithm;Return U (those hard cases which can not beconfidently classified) to act as the unlabeleddata in the LP algorithm;END_________________________________________Figure 1: The algorithmfor bootstrapping support vectorsIn particular, this algorithm generates multipleoverlapping ?views?
by projecting from the origi-nal feature space.
In this paper, feature views withrandom feature projection, as proposed in Zhang(2004), are explored.
Section 4 will discuss thisissue in more details.
During the iterative trainingprocess, classifiers trained on the augmented la-beled data using the projected views are then askedto vote on the remaining unlabeled instances andthose with the highest probability of being cor-rectly labeled are chosen to augment the labeleddata.During the bootstrapping process, the supportvectors included in all the trained SVM classifiers(for all the relation (sub)types and the non-relation)are bootstrapped (i.e.
updated) at each iteration.When the bootstrapping process stops, all the(positive and negative) support vectors included inthe SVM classifiers are returned as bootstrappedsupport vectors with their collective weights (abso-lute a*y) to act as the labeled data in the LP algo-rithm and all the remaining unlabeled instances (i.e.those hard cases which can not be confidently clas-sified in the bootstrapping process) in the unla-beled data are returned to act as the unlabeled datain the LP algorithm.
Through SVM bootstrapping,our LP algorithm will only depend on the criticalinstances (i.e.
support vectors with their weightinformation bootstrapped from all the availablelabeled and unlabeled data) and those hard in-stances, instead of all the available labeled andunlabeled data.3.2 Label PropagationIn the LP algorithm (Zhu and Ghahramani 2002),the manifold structure in data is represented as aconnected graph.
Given the labeled data (the abovebootstrapped support vectors with their weights)and unlabeled data (the remaining hard instances inthe unlabeled data after bootstrapping, includingall the test instances for evaluation), the LP algo-rithm first represents labeled and unlabeled in-stances as vertices in a connected graph, thenpropagates the label information from any vertexto nearby vertex through weighted edges and fi-nally infers the labels of unlabeled instances until aglobal stable stage is achieved.
Figure 2 presentsthe label propagation algorithm on bootstrappedsupport vectors in details.34_________________________________________Assume:Y : the rn * labeling matrix, where ijy  repre-sents the probability of vertex )1( nixi K=with label )1( rjr j K=  (including the non-relation label);LY : the top l  rows of0Y .
LY corresponds to thel  labeled instances;UY : the bottom u  rows of0Y .
UY correspondsto the u  unlabeled instances;T : a nn *  matrix, with ijt  is the probabilityjumping from vertex ix to vertex jx ;BEGIN (the algorithm)Initialization:1) Set the iteration index 0=t ;2) Let 0Y  be the initial soft labels attached toeach vertex;3) Let 0LY  be consistent with the labeling inthe labeled (including all the relation(sub)types and the non-relation) data, where0ijy = the weight of the bootstrapped supportvector if ix  has label jr  (Please note thatjr  can be the non-relation label) and 0 oth-erwise;4) Initialize 0UY ;REPEATPropagate the labels of any vertex to nearbyvertices by tt YTY =+1 ;Clamp the labeled data, that is, replace 1+tLYwith 0LY ;UNTIL Y converges(e.g.
1+tLY  converges to0LY );Assign each unlabeled instance with a label: for)( nilxi ?p , find its label withjijymaxarg ;END (the algorithm)_________________________________________Figure 2: The LP algorithmHere, each vertex corresponds to an instance,and the edge between any two instances ix  and jxis weighted by ijw  to measure their similarity.
Inprinciple, larger edge weights allow labels to travelthrough easier.
Thus the closer the instances are,the more likely they have similar labels.
The algo-rithm first calculates the weight ijw  using a kernel,then transforms it to ?==?=nkkjijij wwijpt1/)( ,which measures the probability of propagating alabel from instance jx to instance ix , and finallynormalizes ijt row by row using ?==nkikijij ttt1/  tomaintain the class probability interpretation of thelabeling matrix Y .During the label propagation process, the labeldistribution of the labeled data is clamped in eachloop using the weights of the bootstrapped supportvectors and acts like forces to push out labelsthrough the unlabeled data.
With this push origi-nates from the labeled data, the label boundarieswill be pushed much faster along edges with largerweights and settle in gaps along those with lowerweights.
Ideally, we can expect that ijw  acrossdifferent classes should be as small as possible andijw  within the same class as big as possible.
In thisway, label propagation happens within the sameclass most likely.This algorithm has been shown to converge toa unique solution (Zhu and Ghahramani 2002),which can be obtained without iteration in theory,and the initialization of YU0 (the unlabeled data) isnot important since YU0 does not affect its estima-tion.
However, proper initialization of YU0 actuallyhelps the algorithm converge more rapidly in prac-tice.
In this paper, each row in YU0 is initialized tothe average similarity with the labeled instances.4 ExperimentationThis paper uses the ACE RDC 2003 corpus pro-vided by LDC for evaluation.
This corpus is gath-ered from various newspapers, newswires andbroadcasts.35MethodLP via bootstrapped(weighted) SVsLP via bootstrapped(un-weighted) SVsLP w/o SVMbootstrappingSVM(BootProject) SVMBootstrapping5% 46.5 (+1.4) 44.5 (+1.7) 43.1 (+1.0) 35.4 (-) 40.6 (+0.9)10% 48.6 (+1.7) 46.5 (+2.1) 45.2 (+1.5) 38.6 (-) 43.1 (+1.4)25% 51.7 (+1.9) 50.4 (+2.3) 49.6 (+1.8) 43.9 (-) 47.8 (+1.7)50% 53.6 (+1.8) 52.6 (+2.2) 52.1 (+1.7) 47.2 (-) 50.5 (+1.6)75% 55.2 (+1.3) 54.5 (+1.8) 54.2 (+1.2) 53.1 (-) 53.9 (+1.2)100% 56.2 (+1.0) 55.8 (+1.3) 55.6 (+0.8) 55.5 (-) 55.8 (+0.7)Table 1: Comparison of different methods using a state-of-the-art linear kernel on the ACE RDC 2003corpus (The numbers inside the parentheses indicate the increases in F-measure if we add the ACE RDC2004 corpus as the unlabeled data)4.1 Experimental SettingIn the ACE RDC 2003 corpus, the training dataconsists of 674 annotated text documents (~300kwords) and 9683 instances of relations.
Duringdevelopment, 155 of 674 documents in the trainingset are set aside for fine-tuning.
The test set is heldout only for final evaluation.
It consists of 97documents (~50k words) and 1386 instances ofrelations.
The ACE RDC 2003 task defines 5 rela-tion types and 24 subtypes between 5 entity types,i.e.
person, organization, location, facility and GPE.All the evaluations are measured on the 24 sub-types including relation identification and classifi-cation.In all our experiments, we iterate over all pairsof entity mentions occurring in the same sentenceto generate potential relation instances1.
For betterevaluation, we have adopted a state-of-the-art lin-ear kernel as similarity measurements.
In our linearkernel, we apply the same feature set as describedin a state-of-the-art feature-based system (Zhou etal 2005): word, entity type, mention level, overlap,base phrase chunking, dependency tree, parse treeand semantic information.
Given above variouslexical, syntactic and semantic features, multipleoverlapping feature views are generated in thebootstrapping process using random feature projec-tion (Zhang 2004).
For each feature projection inbootstrapping support vectors, a feature is ran-domly selected with probability p and therefore theeventually projected feature space has p*F features1  In this paper, we only measure the performance ofrelation extraction on ?true?
mentions with ?true?chaining of co-reference (i.e.
as annotated by the cor-pus annotators) in the ACE corpora.
We also explic-itly model the argument order of the two mentionsinvolved and only model explicit relations because ofpoor inter-annotator agreement in the annotation ofimplicit relations and their limited number.on average, where F is the size of the original fea-ture space.
In this paper, p and the number of dif-ferent views are fine-tuned to 0.5 and 10 2respectively using 5-fold cross validation on thetraining data of the ACE RDC 2003 corpus.4.2 Experimental ResultsTable 1 presents the F-measures 3  (the numbersoutside the parentheses) of our algorithm using thestate-of-the-art linear kernel on different sizes ofthe ACE RDC training data with all the remainingtraining data and the test data4  as the unlabeleddata on the ACE RDC 2003 corpus.
In this paper,we only report the performance (averaged over 5trials) with the percentages of 5%, 10%, 25%, 50%,75% and 100%5.
For example, our LP algorithmvia bootstrapped (weighted) support vectorsachieves the F-measure of 46.5 if using only 5% ofthe ACE RDC 2003 training data as the labeleddata and the remaining training data and the testdata in this corpus as the unlabeled data.
Table 12 This suggests that the modified BootProject algorithmin the bootstrapping phase outperforms the SelfBootalgorithm (with p=1.0 and m=1) which uses all thefeatures as the only view.
In the related NLP literature,co-training has also shown to typically outperformself-bootstrapping.3 Our experimentation also shows that most of perform-ance improvement with either bootstrapping or labelpropagation comes from gain in recall.
Due to spacelimitation, this paper only reports the overall F-measure.4  In our label propagation algorithm via bootstrappedsupport vectors, the test data is only included in thesecond phase (i.e.
the label propagation phase) and notused in the first phase (i.e.
bootstrapping support vec-tors).
This is to fairly compare different semi-supervised learning methods.5 We have tried less percentage than 5%.
However, ourexperiments show that using much less data will sufferfrom performance un-stability.
Therefore, we only re-port the performance with percentage not less than 5%.36also compares our method with SVM and theoriginal SVM bootstrapping algorithm BootPro-ject(i.e.
bootstrapping on the top of SVM with fea-ture projection, as proposed in Zhang (2004)).Finally, Table 1 compares our LP algorithm viabootstrapped (weighted by default) support vectorswith other possibilities, such as the scheme viabootstrapped (un-weighted, i.e.
the importance ofsupport vectors is not differentiated) support vec-tors and the scheme via all the available labeleddata (i.e.
without SVM bootstrapping).
Table 1shows that:1) Inclusion of unlabeled data using semi-supervised learning, including the SVM boot-strapping algorithm BootProject, the normalLP algorithm via all the available labeled andunlabeled data without SVM bootstrapping,and our LP algorithms via bootstrapped (eitherweighted or un-weighted) support vectors,consistently improves the performance, al-though semi-supervised learning has shown totypically decrease the performance when a lotof (enough) labeled data is available (Nigam2001).
This may be due to the insufficiency oflabeled data in the ACE RDC 2003 corpus.Actually, most of relation subtypes in the twocorpora much suffer from the data sparsenessproblem (Zhou et al2006).2) All the three LP algorithms outperform thestate-of-the-art SVM classifier and the SVMbootstrapping algorithm BootProject.
Espe-cially, when a small amount of labeled data isavailable, the performance improvements bythe LP algorithms are significant.
This indi-cates the usefulness of the manifold structurein both labeled and unlabeled data and thepowerfulness of the LP algorithm in modelingsuch information.3) Our LP algorithms via bootstrapped (eitherweighted or un-weighted) support vectors out-performs the normal LP algorithm via all theavailable labeled data w/o SVM bootstrapping.For example, our LP algorithm via boot-strapped (weighted) support vectors outper-forms the normal LP algorithm from 0.6 to 3.4in F-measure on the ACE RDC 2003 corpusrespectively when the labeled data ranges from100% to 5%.
This suggests that the manifoldstructure in both the labeled and unlabeled datacan be well preserved via bootstrapped supportvectors, especially when only a small amountof labeled data is available.
This implies thatweighted support vectors may represent themanifold structure (e.g.
the decision boundaryfrom where label propagation is done) betterthan the full set of data ?
an interesting resultworthy more quantitative and qualitative justi-fication in the future work.4) Our LP algorithms via bootstrapped (weighted)support vectors perform better than LP algo-rithms via bootstrapped (un-weighted) supportvectors by ~1.0 in F-measure on average.
Thissuggests that bootstrapped support vectors withtheir weights can better represent the manifoldstructure in all the available labeled and unla-beled data than bootstrapped support vectorswithout their weights.5) Comparison of SVM, SVM bootstrapping andlabel propagation with bootstrapped (weighted)support vectors shows that both bootstrappingand label propagation contribute much to theperformance improvement.Table 1 also shows the increases in F-measure(the numbers inside the parentheses) if we add allthe instances in the ACE RDC 20046 corpus intothe ACE RDC 2003 corpus in consideration asunlabeled data in all the four semi-supervisedlearning methods.
It shows that adding more unla-beled data can consistently improve the perform-ance.
For example, compared with using only 5%of the ACE RDC 2003 training data as the labeleddata and the remaining training data and the testdata in this corpus as the unlabeled data, includingthe ACE RDC 2004 corpus as the unlabeled dataincreases the F-measures of 1.4 and 1.0 in our LPalgorithm and the normal LP algorithm respec-tively.
Table 1 shows that the contribution growsfirst when the labeled data begins to increase andreaches a maximum of ~2.0 in F-measure at a cer-tain point.Finally, it is found in our experiments thatcritical and hard instances normally occupy only15~20% (~18% on average) of all the availablelabeled and unlabeled data.
This suggests that,through bootstrapped support vectors, our LP algo-6  Compared with the ACE RDC 2003 task, the ACERDC 2004 task defines two more entity types, i.e.weapon and vehicle, much more entity subtypes, anddifferent 7 relation types and 23 subtypes between 7entity types.
The ACE RDC 2004 corpus from LDCcontains 451 documents and 5702 relation instances.37rithm can largely reduce the computational burdensince it only depends on the critical instances (i.e.bootstrapped support vectors with their weights)and those hard instances.5 ConclusionThis paper proposes a new effective and efficientsemi-supervised learning method in relation ex-traction.
First, a moderate number of weightedsupport vectors are bootstrapped from all the avail-able labeled and unlabeled data via SVM through aco-training procedure with feature projection.
Here,a random feature projection technique is used togenerate multiple overlapping feature views inbootstrapping using a state-of-the-art linear kernel.Then, a LP algorithm is applied to propagate labelsvia the bootstrapped support vectors, which, to-gether with those hard unlabeled instances and thetest instances, are represented as vertices in a con-nected graph.
During the classification process, thelabel information is propagated from any vertex tonearby vertex through weighted edges and finallythe labels of unlabeled instances are inferred until aglobal stable stage is achieved.
In this way, themanifold structure in both the labeled and unla-beled data can be well captured by label propaga-tion via bootstrapped support vectors.
Evaluationon the ACE RDC 2004 corpus suggests that our LPalgorithm via bootstrapped support vectors cantake the advantages of both SVM bootstrappingand label propagation.For the future work, we will systematicallyevaluate our proposed method on more corporaand explore better metrics of measuring the simi-larity between two instances.AcknowledgementThis research is supported by Project 60673041under the National Natural Science Foundation ofChina and Project 2006AA01Z147 under the ?863?National High-Tech Research and Development ofChina.ReferencesACE.
(2000-2005).
Automatic Content Extraction.http://www.ldc.upenn.edu/Projects/ACE/Agichtein E. and Gravano L. (2000).
Snowball:Extracting relations from large plain-text collec-tions.
Proceedings of the 5th ACM InternationalConference on Digital Libraries(ACMDL?2000).Belkin, M. and Niyogi, P. (2002).
Using ManifoldStructure for Partially Labeled Classification.NIPS 15.Blum A. and Chawla S. (2001).
Learning from la-beled and unlabeled data using graph mincuts.ICML?2001.Blum A., Lafferty J., Rwebangira R and Reddy R.(2004).
Semi-supervised learning using random-ized mincuts.
ICML?2004.Brin S. (1998).
Extracting patterns and relationsfrom world wide web.
Proceedings of WebDBWorkshop at 6th International Conference onExtending Database Technology:172-183.Charniak E. (2001).
Immediate-head Parsing forLanguage Models.
ACL?2001: 129-137.
Tou-louse, FranceChen J.X., Ji D.H., Tan C.L.
and Niu Z.Y.
(2006).Relation extraction using label propagationbased semi-supervised learning.
COLING-ACL?2006: 129-136.
July 2006.
Sydney, Austra-lia.Culotta A. and Sorensen J.
(2004).
Dependencytree kernels for relation extraction.
ACL?2004.423-429.
21-26 July 2004.
Barcelona, Spain.Hasegawa T., Sekine S. and Grishman R. (2004).Discovering relations among named entitiesform large corpora.
ACL?2004.
Barcelona, Spain.Miller S., Fox H., Ramshaw L. and Weischedel R.(2000).
A novel use of statistical parsing to ex-tract information from text.
ANLP?2000.
226-233.
29 April  - 4 May 2000, Seattle, USAMoschitti A.
(2004).
A study on convolution ker-nels for shallow semantic parsing.ACL?2004:335-342.Nigam K.P.
(2001).
Using unlabeled data to im-prove text classification.
Technical ReportCMU-CS-01-126.Niu Z.Y., Ji D.H., and Tan C.L.
(2005).
WordSense Disambiguation Using Label PropagationBased Semi-supervised Learning.ACL?2005:395-402., Ann Arbor, Michigan,USA.Szummer, M., & Jaakkola, T. (2001).
Partially La-beled Classification with Markov RandomWalks.
NIPS 14.38Yang L.P., Ji D.H., Zhou G.D. and Nie Y.
(2006).Document Re-ranking using cluster validationand label propagation.
CIKM?2006.
5-11 Nov2006.
Arlington, Virginia, USA.Zelenko D., Aone C. and Richardella.
(2003).
Ker-nel methods for relation extraction.
Journal ofMachine Learning Research.
3(Feb):1083-1106.Zhang M., Su J., Wang D.M., Zhou G.D. and TanC.L.
(2005).
Discovering Relations from aLarge Raw Corpus Using Tree Similarity-basedClustering, IJCNLP?2005, Lecture Notes in Arti-ficial Intelligence (LNAI 3651).
378-389.Zhang M., Zhang J., Su J. and Zhou G.D. (2006).A Composite Kernel to Extract Relations be-tween Entities with both Flat and StructuredFeatures.
COLING-ACL-2006: 825-832.
Sydney,AustraliaZhang Z.
(2004).
Weakly supervised relation clas-sification for information extraction.CIKM?2004.
8-13 Nov 2004.
Washington D.C.USA.Zhao S.B.
and Grishman R. (2005).
Extracting re-lations with integrated information using kernelmethods.
ACL?2005: 419-426.
Univ of Michi-gan-Ann Arbor,  USA,  25-30 June 2005.Zhou G.D., Su J. Zhang J. and Zhang M. (2005).Exploring various knowledge in relation extrac-tion.
ACL?2005.
427-434.
25-30 June, Ann Ar-bor, Michgan, USA.Zhou G.D., Su J. and Zhang M. (2006).
Modelingcommonality among related classes in relationextraction, COLING-ACL?2006: 121-128.
Syd-ney, Australia.Zhu, X. and Ghahramani, Z.
(2002).
Learning fromLabeled and Unlabeled Data with LabelPropagation.
CMU CALD Technical Report.CMU-CALD-02-107.Zhu, X., Ghahramani, Z. and Lafferty, J.
(2003).Semi-Supervised Learning Using GaussianFields and Harmonic Functions.
ICML?2003.39
