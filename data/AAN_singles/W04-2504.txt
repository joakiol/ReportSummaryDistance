Discourse Structure for Context Question AnsweringJoyce Y. Chai Rong JinDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48864jchai@cse.msu.edu, rongjin@cse.msu.eduAbstractIn a real-world setting, questions are notasked in isolation, but rather in a cohesivemanner that involves a sequence of relatedquestions to meet user?s information needs.The capability to interpret and answerquestions based on context is important.
Inthis paper, we discuss the role of discoursemodeling in context question answering.
Inparticular, we motivate a semantic-richdiscourse representation and discuss theimpact of refined discourse structure onquestion answering.1 IntroductionIn a real-world setting, questions are not asked inisolation, but rather in a cohesive manner thatinvolves a sequence of related questions to meetuser?s information needs.
The capability to interpretand answer questions based on context is important.For example, Figure 1 shows an example of a seriesof context questions.
In this example, theinterpretation of Q2 and Q4 depends on theresolution of ?it?
and ?this?
from the contextrespectively.
Although neither Q3 nor Q6 requiresany anaphora resolution, the interpretation of Q3depends on Q2 while the interpretation of Q6depends solely on itself.
Furthermore, in Q5, thereare no explicit references.
Its interpretation dependson a preceding question (e.g.,Q4), however, in adifferent manner.This example indicates that interpreting each ofthese questions and extracting answers needs to besituated in a particular context as the QA sessionproceeds.
There are situations where a question is?complete?
enough and its interpretation does notdepend on the previous questions (Q6).
There arealso situations where the interpretation of a questiondepends on preceding questions no matter whether itrequires anaphora or ellipsis resolution.
Based onthese observations, a natural question to ask is whatmakes the use of discourse differently in differentsituations?
What is the role of discourse in contextquestion answering?To address these questions, a key issue, in ourmind, is that every question and its answer have adiscourse status with respect to an entire QA session.This discourse status includes two aspects.
The firstaspect relates to discourse roles of entities in aquestion and the corresponding answer.
Entities (suchas noun phrase, verb phrase, preposition phrase, etc)in a question carry distinctive roles that indicate whatis the topic or focus of a question in terms of theoverall information seeking discourse.
Topic relatesto the ?aboutness?
of a question and focus relates to aspecific perspective of the topic.
The second aspectof discourse status relates to discourse transitions thatindicate how discourse roles are changed from onequestion to another as the interaction proceeds andhow such changes reflect the progress of userinformation needs.
Both discourse roles anddiscourse transitions determine whether the context isuseful, and if so, how to use the context to interpret aquestion.This paper takes an initial attempt to investigatethe discourse status for context question answering.In particular, it motivates a semantic-rich discourserepresentation that captures both discourse roles of aquestion and discourse transitions between questions.Through examples, this paper further discusses thepotential impact of this refined discourse structure oncontext question answering.Q1: What is the name of the volcano that destroyedthe ancient city of Pompeii?Q2: When did it happen?Q3: how many people were killed?Q4: how tall was this volcano?Q5: Any pictures?Q6: Where is Mount Rainier?Figure 1: An example of context questions2 Semantic-rich Discourse ModelingFor processing single questions, an earlier studyshows that an impressive improvement can beachieved when more knowledge-intensive NLPtechniques are applied at both question and answerprocessing level (Harabagiu et al, 2000).
For contextquestions, a parallel question would be whether richcontextual knowledge will help interpret subsequentquestions and extracting answers.
To address thisquestion, we propose a semantic-rich discoursemodeling that captures both discourse roles ofquestions and discourse transitions betweenquestions, and investigate its usefulness in contextquestion answering.2.1 Discourse RolesIn context question answering, each question issituated in a context.
In addition to the semanticinformation carried by important syntactic entities(such as noun phrase, verb phrase, prepositionphrase, etc), each question also carries distinctivediscourse roles with respect to the whole questionanswering discourse.
Specifically, the discourse rolescan be categorized based on both the informationaland intentional perspectives of discourse (Hobbs,1996), as well as the presentation aspect of bothquestions and answers.The intentional perspective relates to the purposeof a question.
In a fully interactive questionanswering environment, instead of asking questions,a user may need to reply to a clarification questionprompted by the system or may need to simply askfor a confirmation.
Therefore, it is important tocapture the intention from the user (Grosz and Sidner1986).
The informational perspective relates to theinformation content of a question, in particular, thetopic and the focus based on the semantics of thecontent.
In addition to the intentional andinformational aspects, there is also a presentationalaspect of discourse that relates to both the inputmodality (i.e., questions) and the output modality(i.e., answers).
For example, a user may explicitly askfor images or pictures of a person or event.
Thepresentation aspect is particularly important tofacilitate multimodal multimedia question answering.Therefore, for a given question, three types ofdiscourse roles: Intent, Content, and Media can becaptured to reflect the intentional, informational, andpresentational perspectives of discourse respectively.These discourse roles can be further characterizedby a set of features.
For example, Intent can berepresented by Act and Motivator, where Act indicateswhether the user is requesting information from thesystem or replying to a system question.
Motivatorcorresponds to the information goal as to what typeof action is expected from the system, for example,whether information retrieval or confirmation (Chaiet al 2003).
We will not elaborate Intent here since ithas been widely modeled for most dialog systems.Content can be characterized as Target, Topic andFocus.
Target indicates the expected answer type suchas whether it should be a proposition (e.g., for whyand how questions), or a specific type of entity (e.g.,TIME and PLACE).Topic indicates the ?aboutness?
or the scoperelated to a question.
Focus indicates the currentfocus of attention given a particular topic.
Focusalways refers to a particular aspect of Topic.
Since theinformational perspective of discourse should capturethe semantics of what has been conveyed, Topic andFocus are linked with the semantic information of aquestion, for example, semantic roles as described in(Gildea and Jurafsky 2002).
Semantic roles concernwith the roles of constitutes in a question in terms ofits predicate-argument structure.
The discourse roleslink the semantic roles of individual questionstogether with respect to the discourse progressthrough Topic and Focus.For example, Topic can be of type Activity or Entity.Activity can be further categorized by ActType,Participant, and Peripheral.
ActType indicates the typeof the activity; Participant indicates entities that areparticipating in the activity with different semanticroles.
Peripheral captures auxiliary information suchContentTarget: $NAMETopic: ActivityActType: Destroy [Term: ?destroy?
]Participant1: EntitySemRole: AgentSemType: volcanoId: ?Term: ?the volcano?Participant2: EntitySemRole: ThemeSemType: cityId: ?Pompeii?Term: ?Pompeii?Focus: Topic.Activity-Participant1[Element: Name[Value: ?
; Term: ?name?
]]IntentAct: RequestMotivator: AnsRequestMediaFormat: TextGenre: DefaultFigure 2: Discourse roles for Q1.as the time, place, purpose, and reason for such anactivity.
Entity can be categorized by SemRole,SemType, Id, Element, and Constraint.
SemRoleindicates the semantic role of the entity in a particularactivity (if any).
SemType represents the semantictype of the entity.
Element indicates the specificfeatures associated with the entity.
Constraintspecifies the constraints need to be satisfied toidentify the entity, and Id specifies the particularidentifier of the entity that particularly corresponds topronouns, demonstratives, and definite noun phrases.Media indicates the desired information media,which can be further characterized as Format andGenre as shown in Figure 2.
Format indicates whetherit is an image, a table, or text, etc.
Genre specifies theanswer needs such as summary or list.
If it is a list,how many should be in the list as in the question?number ten largest cities in the world.
?Figure 2 shows the representation of discourseroles of Q1 using typed feature structures (Carpenter1992), where Intent indicates that the user isrequesting for the system to retrieve an answer.
Topicindicates the topic of Q1 is a Destroy Activity, whichhas two participants.
The first participant is somekind of unknown volcano that takes the role of Agentin the activity (i.e., the destroyer).
The secondparticipant is the city of Pompeii that takes the role ofTheme indicating the thing destroyed.
The Focus ofQ1 is about the name (i.e., Element) of the entity inthe first participant (i.e., Participant1) in the Topicrepresentation.The granularity of discourse roles can be varied.The finer the granularity, the better is the use ofcontext for inference (as discussed later).
However,the finer granularity also implies deeper semanticprocessing.
This semantic rich representation can beused to generate other representations such as queriesbased on weighted terms for information retrieval orlogical forms for deduction and inference (Waldingeret al, 2003).
Furthermore, this representation is ableto keep all QA sessions in a structured way to supportinference, summarization, and collaborative fusion asdescribed later.2.2 Discourse TransitionsTransitions from one question to another alsodetermine how context will be used in interpretingquestions and retrieving answers.
In this section, weuse query formation as an example to illustrate therole of different types of discourse transitions.Discourse transitions also correspond to theintentional, informational, and presentationalperspectives of discourse.
Intentional transitions areclosely related to Grosz and Sidner?s ?dominance?and ?satisfaction precedence?
relations, which aremore relevant to plan-based discourse (Grosz andSidner, 1986).
Here we focus on informationaltransitions and presentational transitions that aremore relevant to QA systems since they are targetedfor information exchange.Informational transitions are mainly centeredaround Topics of questions.
In context questionanswering, how questions are related to each otherdepends on how ?topics?
of those questions evolve.Currently, we categorize information transitions intothree types: Topic Extension, Topic Exploration, andTopic Shift.Topic ExtensionA question concerns a similar topic as that of aprevious question, but with different participants,peripheral, or constraints.
It has the followingsubcategories:Constraint RefinementA question is about a similar topic with additional orrevised constraints.
For example:Q7: What?s the crime rate in Maryland and Virginia?Q8: What is it ten years ago?For another example:Q9: What?s the crime rate in Maryland and Virginia?Q10: What was it in Alabama and Florida?In both examples, both questions share the topic of?crime rate?, but concerning different crime rateswith different constraints.
Interpreting the secondquestion requires not only identifying constraints, butalso the relations between constraints.
In the firstexample, the constraints from Q7 need to be used toform a query for Q8.
However, constraints from Q9should not be used for Q10.Participant ShiftA question is about a similar topic with differentparticipants.For example:Q11: In what country did the game of croquetoriginate?Q12: What about soccer?In this example, both questions are about theorigination of a certain sport.
The Content structurefor both questions are the same except for theParticipant role, which in Q11 is ?croquet?
and in Q12is ?soccer?.
Therefore, the query created for Q12would be {country, soccer, originate}, the keyword?croquet?
should not appear in the query list.Topic ExplorationTwo questions are concerning the same topic, butwith different focus (i.e., asking about differentaspects of the topic).
For example,Q13: What is the name of the volcano that destroyed theancient city of Pompeii?Q14: When did this happen?In this example, ?this?
in Q14 refers to the sameactivity topic in Q13, but focus on the TIMEperipheral information about the activity.In the following example,Q15: Where is Mount Rainier?Q16: How tall is it?Q15 asks about the location of Mount Rainier (whichis an entity topic) and Q16 asks about a differentaspect (i.e., the height) of the same entity topic.
Inboth examples, significant terms representing theTopic from the preceding question can be mergedwith the significant terms in the current question toform a query.Topic ShiftTwo consecutive questions could ask about twodifferent topics.
Different topic shifts indicatedifferent semantic relations between two questions.Activity Topic shifts to another Activity TopicIn the following example,Q17: What is the name of the volcano that destroyed theancient city of Pompeii?Q18: How many people were killed?The topic of both questions concerns about certainactivities.
This activity shift indicates that ?kill?activity is a consequence of ?destroy?
activity (i.e.,Q18 is a consequence of Q17).Other relations can also be entailed from such atransition such as ?effect-cause?
relation as in thefollowing example (Harabagiu et al 2001):Q19: Which museum in Florence was damaged by amajor bomb explosion in 1993?Q20: How much explosive was used?Activity Topic shifts to Entity TopicIn the example:Q21: What is the name of the volcano that destroyed theancient city of Pompeii?Q22: How tall is this volcano?The topic of Q21 is an activity of ?destroying?
andthe focus is the agent of the activity ?the volcano?.This focus becomes the topic of Q22.
This transitionindicates a further probing of a particular participantin an activity that can be independent of the activityitself.
Therefore, the terms in Q21 will not be helpfulin setting up the stage for processing Q22.
Q21should be used only to resolve reference to thedefinite noun phrase ?this volcano?.Related to the presentational perspective of a QAdiscourse, we currently only identify: Media Shift.This relation indicates that two questions are aboutthe same information content, but with differentpreference of media presentation.For example,Q25: how tall is Mount Vesuvius?Q26: Any pictures?Q26 is asking for the images of the Mount Vesuvius.This indicates that the backend should perform imageretrieval rather than text retrieval.In summary, given two consecutive questions (Qi,Qi+1), a certain transition exists from Qi to Qi+1.These transitions determine how the context, forexample, proceeding questions and answers can beused in interpreting the following question andidentifying the potential answers.
Here we only listseveral examples to show the importance of thesetransitions, which are by no means complete.
Weplan to identify a list of salient transitions forprocessing context questions as well as theirimplications (e.g., semantic relations) in interpretingcontext questions.2.3 Discourse ProcessingGiven the above discussion, the goal of discoursemodeling for context question answering is toautomatically identify the discourse roles of aquestion and discourse relations between questions asthe QA session proceeds.
This may be a difficult taskthat requires rich knowledge and deep semanticprocessing.
However, the recent advancement insemantic processing and discourse parsing hasprovided an excellent foundation for this task.The discourse roles are higher-level abstracts ofthe semantic roles as those provided in FrameNet(Baker et al, 1998) and Propbank (Kingsbury andPalmer 2002).
Recent corpus-based approaches toidentify semantic roles (Roth et al2002, Gildea andJurafsky 2002; Gildea and Palmer 2002; Surdeanu etal., 2003) have been successful in identifying domainindependent semantic relations with respect to thepredicate-argument structure.
Furthermore, recentwork also provides discourse annotated corpora withrhetorical relations (Carlson, et al, 2003) andtechniques for discourse paring for texts (Soricut andMarcu, 2003).
All these recent advances make thesemantic-rich discourse modeling possible.For example, a collection of context questions(and answers) can be annotated in terms of theirdiscourse roles and relations.
Specifically, thefollowing information can be either automaticallyidentified or manually annotated:?
Syntactic structures automatically identified froma parser (Collins, 1997);?
Semantic roles of entities in the question (Gildeaand Jurafsky 2002; Gildea and Palmer 2002;Surdeanu et al, 2003);?
Discourse roles either manually annotated oridentified by rules that map directly from semanticroles to discourse roles.?
Discourse transitions automatically determinedonce discourse roles are identified for eachquestion.?
Semantic relations between questions manuallyannotated.?
Answers provided by the system.Based on this information, important features can beidentified.
Different learning models such as decisiontrees or Bayesian classifier can be applied to learn theclassifier for discourse roles and relations.
Strategiescan be built to take into account of discourse rolesand relations from preceding questions and answersto process a subsequent question and extract answers.These models can then be applied to process newcontext questions.3 Refined Discourse Structure in ContextQuestion AnsweringBased on the above discussion, during the questionanswering process, a discourse structure can becreated to capture the discourse roles of eachActTypeActivityCity?VolcanoNameThemeAgentEntityDestroy?Pompeii?EntityIdIdParticipant1SemTypeSemRoleParticipant2 SemRoleSemTypeElementTopicFocus??
?ActTypeActivityCity?Mount Vesuvius?VolcanoNameThemeAgentEntityDestroy?Pompeii?EntityIdIdParticipant1SemTypeSemRoleParticipant2 SemRoleSemTypeElementTopic?Mount Vesuvius?Time?PeripheralValueFocus(a) Discourse repreentation after processing Q1                (b) Discourse representation after processing Q2ActTypeActivityCity?Mount Vesuvius?VolcanoNameThemeAgentEntityDestroy?Pompeii?EntityIdIdParticipant1SemTypeSemRoleParticipant2 SemRoleSemTypeElementTopic?Mount Vesuvius?Time79 ADPeripheral1ValueFocusConsequenceActivityEntityKillPatientPerson SizeOfSet ?Peripheral2TypeActType Participant1SemTypeSemRoleElementValueValueActTypeActivityCity?Mount Vesuvius?VolcanoNameThemeAgentEntityDestroy?Pompeii?EntityIdIdParticipant1SemTypeSemRoleParticipant2 SemRoleSemTypeElementTopic?Mount Vesuvius?Time79 ADPeripheralValueFocusHeightElementValue?ActivityEntityKillPatientPerson SizeOfSet ThousandsPeripheral2TypeActType Participant1SemTypeSemRoleElementValueConsequence(c) Discourse representation after processing Q3              (d) Discourse representation after processing Q4question and discourse relations between questions.Similar to information extraction for free texts, thisrefined discourse structure captures the salientinformation extracted from the question answeringprocess.
This discourse provides a structuredinformation space that indicates what type ofinformation has been exchanged and howinformation obtained at different stages is related.
Inother words, we can also consider this representationas the ?mental map?
of user information needs.
Thismental map will potentially provide a basis toimprove question interpretation and answerextraction through inference, summarization, andcollaborative question answering.3.1 Discourse RepresentationThe typed feature structures can be represented asDirected Acyclic Graph (DAG).
Thus the describeddiscourse structure can be represented as semanticnetworks using DAGs.
For example, Figure 3 showsthe discourse representation after processing the eachof the first four questions in Figure 1.
In this network,each node is either a specific value (i.e., leaf nodes)or a typed feature structure itself (i.e., internal node).Each directed link corresponds to a particular feature.Note that because of the space limit, not everythingrepresented in the feature structure in Figure 2 isshown here in the semantic network.
For example,the type of an activity (e.g., Destroy) by itself is afeature structure (in Figure 2) that further consists ofthe specific term used in the question.
This term isnot shown but is included in the semantic nets.As context question answering proceeds, thesemantic network(s) for discourse grows, withdifferent pointers of Topic and Focus.
For example,Figure 3(a) represents Q1, where Topic points anActivity feature structure and Focus points to the NameElement of the Participant1 in the Activity.
From Q1 toQ2, there is a transition of Topic Exploration whichindicates that Q2 is about the same topic, but with adifferent focus.
Therefore, in Figure 3(b), the Topicpoints to the same activity, but the Focus now pointsto the peripheral Time information of that activity.Next, Q3 is about a different topic involving activityKill.
However, since there is a consequence relationfrom Q2 to Q3, the activity asked in Q3 actuallyfulfills the Peripheral role of Consequence for theprevious activity as shown in Figure 3(c).
Finally, inQ4, there is a gap between Q3 and Q4, however,there is a transition of Probing from Q2 to Q4.
Nowthe Topic becomes the Participant in Q1 as shown inFigure 3(d).3.2 Potential ImpactsThe growth of the semantic networks represents theoverall information needs of a user and how suchinformation needs are related.
Since this is astructured representation, it can be queried and usedto facilitate context question answering, for example,in the following aspects:?
Query expansion and answer retrieval?
Inference and summary for question answering?
Collaborative question answeringTo process questions, most systems will first forma query of keywords to represent the current questionand to retrieve relevant passages that may contain thepotential answers.
In context question answering,since the interpretation of a question may depend onpreceding questions, some keywords from precedingquestions may need to be included in the query forthe current question.
The fine-grained discoursestructure will enhance answer retrieval through morecontrolled selection of terms from precedingquestions and answers.
For example, strategies can bedeveloped to select query terms depending on thediscourse relations.
Different discourse roles andtransitions may lead to different weighting schemesfor query expansion.Furthermore, the information captured in thediscourse structure can help make predication aboutwhat the user information need is and thereforeprovide more intelligent services to help user findanswers.
For example, semantic and discourserelations between different topics and focuses of aseries of questions can help a system infer and predictthe overall interest of a user.
Although answers toeach question may come from different sources,based on the structured discourse (e.g., in semanticnetwork), the system can aggregate information andgenerate summaries.Another potential impact of the refined structureddiscourse is to facilitate collaborative questionanswering.
Very often, various users may have asimilar interest about a set of topics.
The structureddiscourse built for one user can be used to helpanswers questions from another user.
A user mayhave a certain information goal in mind, but does notknow what types of questions to ask.
Therefore, auser?s question may be very general and vague, suchas ?what happened to Pompeii??
This question needsto be decomposed into a set of smaller questions.
Thediscourse structure that connects different aspects oftopics together can provide some insight on how suchdecomposition should be made.
Furthermore, thediscourse structure from a skilled user can enable thesystem to intelligently direct a novice user in hisinformation seeking process.4 DiscussionTREC 10 Question Answering Track initiated acontext-task that was designed to investigate thesystem capability to track context through a series ofquestions.
As described in (Voorhees 2001), therewere two unexpected results of this task.
First, theability to identify the correct answer to a question inthe later series had no correlation with the capabilityto identify correct answers to preceding questions.Second, since the first question in a series alreadyrestricted answers to a small set of documents, theperformance was determined by whether the systemcould answer a particular type of question, rather thanthe ability to track context.
Because of theseunexpected results, the context task has been stoppedin the following TREC evaluations (Voorhees 2002).The reasons that TREC 10 did not achieve theexpected results, in our opinion, lie in two aspects.The first aspect relates to the uniqueness of opendomain context question answering.
In open domainQA, first, there may be many occurrences of correctanswers in various part(s) of document(s).
Second,there may be multiple paths (e.g., differentcombination of key query terms) that can lead to oneoccurrence of the correct answer.
Therefore, thecorrect answer to a previous question may not becritical in finding answers to subsequent questions.This phenomenon may provide an opportunity to findanswers without explicitly modeling context (i.e., bykeeping track of the discourse objects from answers),but rather identifying and using relevant context.For example, in the LCC system (that achievedthe best result for the context task in TREC 10), thediscourse was not explicitly represented (Harabagiuet al2001).
Instead of resolving references usingdiscourse information, the LCC system first identifiesthe question that contains the potential referents anduses those questions and the current question toidentify the target paragraph.
Thus, questioninterpretation does not depend on the answers, butrather depends on the context that is dynamicallyidentified as a list of preceding questions.
Now thequestion is whether the system will achieve evenbetter results (e.g., correctly find answers to the restof the eight questions) with some contextrepresentation?Another more important question to be asked iswhether the design of the context task just happenedto provide an opportunity to achieve good resultswithout modeling the context.
As discussed in(Harabagiu et al, 2001), answers to 85% of contextquestions actually occurred within the sameparagraph as the answers to the previous questions.Therefore, just using preceding questions, the systemwas able to find the target paragraph and the finalanswer really depended on the capability to identifydifferent types of answers in that paragraph.
What ifa series of questions were designed differently so thatquestions are related but answers are scattered indifferent documents or paragraphs.
Will the shallowprocessing of discourse succeed in finding theanswers?Furthermore, the ultimate goal of QA systems isto be able to access information from differentsources (e.g., unstructured text or structureddatabase) and to provide intelligent dialog capability.One important question we need to address is whatkind of discourse representation will be sufficient tosupport these capabilities?
For example, to accessstructured databases, the answer to a previousquestion usually narrows down the search spaces inthe database for subsequent questions.
Thus, previousanswers usually determine where in the database ananswer can be found.
Therefore, it is important tokeep track of previous questions and answers in somekind of structure for later use.The second reason that TREC 10 did not achieveexpected results relates to evaluation methodology.
Incontext QA, good performance depends on twoimportant components: the capability of representingand using the relevant context (both explicitly orimplicitly) and the general capability of interpretingquestions and extracting answers.
The level ofsophistication in either component will influence thefinal performance.
Thus, by comparing the finalanswers to each context question, the evaluation ofthe context task in TREC 10 was not able to isolatethe effect of one component from another.
It is notfeasible to identify that when an answer is notidentified, whether it is because of poorrepresentation and use of the discourse information orit is because of the general limitations of thecapability to process certain types of questions.Therefore, to study the role of discourse in contextquestion answering, a more controlled evaluationmechanism is desired.
For example, one approach isto keep the general processing capability as aconstant and vary the representations of discourseand strategies to use the discourse so that theirdifferent impacts on the final answer extraction canbe learned.As a summary, the experience in the TREC 10context task is very valuable.
It does not discount theimportance of context modeling for contextquestions.
But rather, it motivates a more in-depthinvestigation of the role of discourse in contextquestion answering.5 ConclusionQuestions are not asked in isolation, but rather in acohesive manner that involves a sequence of relatedquestions to meet user?s information needs.
It isimportant to understand the role of discourse tosupport this cohesive question answering.By all means, a QA discourse can be representedas coarse as a list of keywords extracted fromprevious questions or as sophisticated as a fine-grained representation as described in this paper.There is a balance between how much we like torepresent the context and how far we can get there.Given recent advances in text-based domainindependent semantic processing and discourseparsing, as well as the availability of rich semanticknowledge sources, we believe it is the time to startfrom the other end of the spectrum to examine thepossibility and impact of semantic-rich discourserepresentation for open-domain question answering.ReferencesBaker, C., Fillmore, C., and Lowe, J.
1998.
The BerkeleyFrameNet project.
In Proceedings of COLING/ACL, pp.86-90, Montreal, CanadaCarpenter, R.1992.
The Logic of Typed Feature Structures.Cambridge University Press.Chai, J., Pan, S., and Zhou, M. 2003.
MIND: A Context-based Multimodal Interpretation Framework in Conver-sational Systems, Natural, Intelligent and Effective In-teraction in Multimodal Dialogue Systems,  Eds.
O.Bernsen ,  L. Dybkjaer and  J.  van Kuppevelt, KluwerAcademic Publishers.Carlson, L., Marcu, D., and Okurowski, M. 2003.
Buildinga discourse-tagged corpus in the framework of Rhetori-cal Structure Theory.
In Jan van Kuppevelt and RonnieSmith, editors, Current Directions in Discourse andDialogue, Kluwer Academic publishers.Collins, M. 1997.
Three Generative, Lexicalized Modelsfor Statistical Parsing.
In Proceedings of the 35th Annualmeeting of the Association for Computational Linguis-tics (ACL 1997): 16-23, Madrid, Spain.Fillmore, C. and Atkins, B.
1998.
FrameNet and lexico-graphic relevance, Proceedings of the First Interna-tional Conference on Language Resources andEvaluation, Granada, Spain.Grosz, B. J. and Sidner, C. 1986.
Attention, intention, andthe structure of discourse.
Computational Linguistics,12(3):175-204.
1986.Gildea, D. and Jurafsky, D. 2002.
Automatic Labeling ofSemantic Roles.
Computational Linguistics, 28(3):245-288.Gildea, D. and Palmer, M. 2002.
The Necessity of Parsingfor Predicate Argument Recgnition.
In Proceedings ofthe 40th Meeting of the Association for ComputationalLinguistics (ACL 2002): 239-246, Philadelphia, PA.Harabagiu, S., Pasca, M., and Maiorano, S. Experimentswith Open-domain Textual Question Answering.
InProceedings of the 18th International Conference onComputational Linguistics (COLING-2000), 2000Harabagiu, S., et al, Answering Complex, List and Con-text Questions with LCC?s Question-Answering Server.In the Proceedings of TREC 2001, 2001.Hobbs, J.
1996.
On the relations between the informationaland Intentional Perspectives on Discourse.
In BurningIssues in Discourse: An inter-Disciplinary Account(eds.
E. Hovy and D. Scott), volume 151 of NATO ASISeries, Series F: Computer and Systems Sciences, pp139-157.
Springer-Verlag, Berlin, GermanyKingsbury, P. and Palmer, M. 2002.
From Treebank toPropbank.
In Proceedings of the 3rd International Con-ference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.Roth, D., et al  2002.
Question Answering via EnhancedUnderstanding of Questions.
Proceedings ofTREC2002.Surdeanu, M., Harabagiu, S., Williams, J., and Aarseth, P.2003.
Using Predicate-Argument Structures for Infor-mation Extraction.
In Proceedings of the 41th Meeting ofthe Association for Computational Linguistics (ACL2003), Sapporo, Japan.Soricut, R. and Marcu, D. 2003.
Sentence Level DiscourseParsing using Syntactic and Lexical Information.
InProceedings of HLT-NAACL.
Edmonton, Canada.Waldinger, R., et al, 2003.
Deductive Question Answeringfrom Multiple Resources, New Directions in QuestionAnswering, AAAI, 2003Voorhees, E. 2001.
Overview of TREC 2001 QuestionAnswering Track.
Proceedings of TREC2001.Voorhees, E. 2002.
Overview of TREC 2002 QuestionAnswering Track.
Proceedings of TREC2002.
