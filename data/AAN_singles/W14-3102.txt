Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 14?21,Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational LinguisticsInteractive Learning of Spatial Knowledgefor Text to 3D Scene GenerationAngel X. Chang, Manolis Savva and Christopher D. ManningComputer Science Department, Stanford University{angelx,msavva,manning}@cs.stanford.eduAbstractWe present an interactive text to 3D scenegeneration system that learns the expectedspatial layout of objects from data.
A userprovides input natural language text fromwhich we extract explicit constraints onthe objects that should appear in the scene.Given these explicit constraints, the sys-tem then uses prior observations of spa-tial arrangements in a database of scenesto infer the most likely layout of the ob-jects in the scene.
Through further userinteraction, the system gradually adjustsand improves its estimates of where ob-jects should be placed.
We present exam-ple generated scenes and user interactionscenarios.1 IntroductionPeople possess the power of visual imaginationthat allows them to turn descriptions of scenes intoimagery.
The conceptual simplicity of generatingpictures from descriptions has spurred the desireto make systems capable of this task.
However, re-search into computational systems for creating im-agery from textual descriptions has seen only lim-ited success.Most current 3D scene design systems requirethe user to learn complex manipulation interfacesthrough which objects are constructed and pre-cisely positioned within scenes.
However, arrang-ing objects in scenes can much more easily beachieved using natural language.
For instance, itis much easier to say ?Put a cup on the table?,rather than having to search for a 3D model of acup, insert it into the scene, scale it to the correctsize, orient it, and position it on a table ensuringit maintains contact with the table.
By making3D scene design more accessible to novice userswe empower a broader demographic to create 3Dscenes for use cases such as interior design, virtualstoryboarding and personalized augmented reality.Unfortunately, several key technical challengesrestrict our ability to create text to 3D scene sys-tems.
Natural language is difficult to map to for-mal representations of spatial knowledge and con-straints.
Furthermore, language rarely mentionscommon sense facts about the world, that containcritically important spatial knowledge.
For exam-ple, people do not usually mention the presence ofthe ground or that most objects are supported by it.As a consequence, spatial knowledge is severelylacking in current computational systems.Pioneering work in mapping text to 3D scenerepresentations has taken two approaches to ad-dress these challenges.
First, by restricting the dis-course domain to a micro-world with simple geo-metric shapes, the SHRDLU system demonstratedparsing of natural language input for manipulatingthe scene, and learning of procedural knowledgethrough interaction (Winograd, 1972).
However,generalization to scenes with more complex ob-jects and spatial relations is very hard to attain.More recently, the WordsEye system has fo-cused on the general text to 3D scene generationtask (Coyne and Sproat, 2001), allowing a userto generate a 3D scene directly from a textual de-scription of the objects present, their properties andtheir spatial arrangement.
The authors of Words-Eye demonstrated the promise of text to scene gen-eration systems but also pointed out some funda-mental issues which restrict the success of theirsystem: a lot of spatial knowledge is requiredwhich is hard to obtain.
As a result, the user has touse unnatural language (e.g.
?the stool is 1 feet tothe south of the table?)
to express their intent.For a text to scene system to understand morenatural text, it must be able to infer implicit in-formation not explicitly stated in the text.
For in-stance, given the sentence ?there is an office witha red chair?, the system should be able to infer14that the office also has a desk in front of the chair.This sort of inference requires a source of priorspatial knowledge.
We propose learning this spa-tial knowledge from existing 3D scene data.
How-ever, since the number of available scenes is small,it is difficult to have broad coverage.
Therefore,we also rely on user interaction to augment andgrow the spatial knowledge.
Luckily, user inter-action is also natural for scene design since it is aninherently interactive process where user input isneeded for refinement.Our contributions address the fundamental chal-lenges of establishing and interactively expandinga spatial knowledge base.
We build on prior workin data-driven scene synthesis (Fisher et al., 2012)to automatically extract general spatial knowledgefrom data: knowledge of what objects occur inscenes, and their expected spatial relations.
Oursystem then uses this knowledge to generate scenesfrom natural text inferring implicit constraints.
Itthen leverages user interaction to allow refinementof the scene, and improve the spatial knowledgebase.
We demonstrate that user interaction is criti-cal in expanding and improving spatial knowledgelearned from data.2 BackgroundA key insight for enabling text to scene generationis that linguistic and non-linguistic spatial knowl-edge is critical for this task and can be learned di-rectly from data representing the physical worldand from interactions of people with such data.User feedback allows us to interactively updatespatial knowledge, an idea that we illustrate herein the domain of spatial relations.
Early work onthe PUT system (Clay andWilhelms, 1996) and theSHRDLU system (Winograd, 1972) gives a goodformalization of the interactive linguistic manipu-lation of objects in 3D scenes.
Recently, there hasbeen promising work on generating 2D clipart forsentences using probabilistic models with place-ment priors learned from data (Zitnick et al., 2013).2.1 Text to Scene SystemsPrior work on text to 3D scene generation has re-sulted in systems such as WordsEye (Coyne andSproat, 2001) and other similar approaches (Sev-ersky and Yin, 2006).
These systems are typi-cally not designed to be fully interactive and do notleverage user interaction to improve their results.Furthermore, they mostly rely on manual annota-tion of 3Dmodels and on hand crafted rules to maptext to object placement decisions, which makesthem hard to extend and generalize.
More re-cent work has used crowdsourcing platforms, suchas Amazon Mechanical Turk, to collect necessaryannotations (Coyne et al., 2012).
However, thisdata collection is treated as a separate pre-processand the user still has no influence on the system?sknowledge base.
We address one part of this is-sue: learning simple spatial knowledge from dataand interactively updating it through user feed-back.
We also infer unstated implicit constraintsthus allowing for more natural text input.2.2 Automatic Scene LayoutPrior work on scene layout has focused largely onroom interiors and determining good furniture lay-outs by optimizing energy functions that capturethe quality of a proposed layout.
These energyfunctions are encoded from interior design guide-lines (Merrell et al., 2011) or learned from inputscene data (Fisher et al., 2012).
Knowledge of ob-ject co-occurrences and spatial relations is repre-sented by simple models such as mixtures of Gaus-sians on pairwise object positions and orientations.Methods to learn scene structure have been demon-strated using various data sources including sim-ulation of human agents in 3D scenes (Jiang etal., 2012; Jiang and Saxena, 2013), and analysisof supporting contact points in scanned environ-ments (Rosman and Ramamoorthy, 2011).However, prior work has not explored methodsfor enabling users of scene generation algorithmsto interactively refine and improve an underlyingspatial knowledge model ?
a capability which iscritically important.
Our work focuses on demon-strating an interactive system which allows a userto manipulate and refine such spatial knowledge.Such a system is useful regardless of the algorithmused to get the input spatial knowledge.2.3 Interactive LearningIn many tasks, user interaction can provide feed-back to an automated system and guide it towardsa desired goal.
There is much prior work in variousdomains including interactive systems for refin-ing image search algorithms (Fogarty et al., 2008)and for manipulating social network group cre-ation (Amershi et al., 2012).
We focus on the do-main of text to 3D scene generation where despitethe success of data-driven methods there has beenlittle work on interactive learning systems.153 Approach OverviewWhat should an interactive text to scene systemlook like from the perspective of a user?
The usershould be able to provide a brief scene descriptionin natural language as input.
The system parsesthis text to a set of explicitly provided constraintson what objects should be present, and how theyare arranged.
This set of constraints should be au-tomatically expanded by using prior knowledge sothat ?common sense?
facts are reflected in the gen-eral scene ?
an example is the static support hier-archy for objects in the scene (i.e.
plate goes ontable, table goes on ground).
The system gener-ates a candidate scene and then the user is free tointeract with it by direct control or through textualcommands.
The system can then leverage user in-teraction to update its spatial knowledge and inte-grate newly learned constraints or relations.
Thefinal output is a 3D scene that can be viewed fromany position and rendered by a graphics engine.
Inthis paper we select an initial viewpoint such thatobjects are in the frame and view-based spatial re-lations are satisfied.How might we create such a system?
Spatialknowledge is critical for this task.
We need it tounderstand spatial language, to plausibly positionobjects within scenes and to allow users to manip-ulate them.
We learn spatial knowledge from ex-ample scene data to ensure that our approach canbe generalized to different scenarios.
We also learnfrom user interaction to refine and expand existingspatial knowledge.
In ?5 we describe the spatialknowledge used by our system.We define our problem as the task of taking textdescribing a scene as input, and generating a plau-sible 3D scene described by that text as output.More concretely, based on the input text, we se-lect objects from a dataset of 3D models (?4) andarrange them to generate output scenes.
See Fig-ure 1 for an illustration of the system architecture.We break the system down into several subtasks:Constraint Parsing (?6): Parse the input textualdescription of a concrete scene into a set of con-straints on the objects present and spatial relationsbetween them.
Automatically expand this set ofconstraints to account for implicit constraints notspecified in the text.SceneGeneration (?7): Using above constraintsand prior knowledge on the spatial arrangement ofobjects, construct a scene template.
Next, sampleObjects:PLATE, FORKON(FORK, TABLE)ON(PLATE, TABLE)ON(CAKE, PLATE)?There is a piece of cake on a table.
?Create SceneIdentify missing objects3D ModelsSpatial KBObjects:CAKE, TABLEON(CAKE, TABLE)Identify objects and relationshipsINTERACTIONCONSTRAINTPARSINGFigure 1: Diagram illustrating the architecture ofour system.the template and select a set of objects to be in-stantiated.
Finally, optimize the placement of theobjects to finalize the arrangement of the scene.Interaction and Learning (?8): Provide meansfor a user to interactively adjust the scene throughdirect manipulation and textual commands.
Useany such interaction to update the system?s spatialknowledge so it better captures the user?s intent.4 Object Knowledge from 3D ModelsTo generate scenes we need to have a collectionof 3D models for representing physical objects.We use a 3D model dataset collected from Google3D Warehouse by prior work in scene synthe-sis and containing about 12490 mostly indoor ob-jects (Fisher et al., 2012).
These models have textassociated with them in the form of names andtags.
In addition, we semi-automatically annotatedmodels with object category labels (roughly 270classes).
We used model tags to set these labels,and verified and augmented them manually.In addition, we automatically rescale models sothat they have physically plausible sizes and orientthem so that they have a consistent up and frontdirection (Savva et al., 2014).
Due to the num-ber of models in the database, not all models wererescaled and re-oriented.
We then indexed all mod-els in a database that we query at run-time for re-trieval based on category and tag labels.5 Spatial KnowledgeHere we describe how we learn spatial knowledgefrom existing scene data.
We base our approachon that of (Fisher et al., 2012) and use their dataset16of 133 small indoor scenes created with 1723 3DWarehouse models.
Relative object-to-object po-sition and orientation priors can also be learnedfrom the scene data but we have not yet incorpo-rated them in the results for this paper.5.1 Support HierarchyWe observe the static support relations of objectsin existing scenes to establish a prior over what ob-jects go on top of what other objects.
As an exam-ple, by observing plates and forks on tables mostof the time, we establish that tables are more likelyto support plates and forks than chairs.
We esti-mate the probability of a parent category Cpsup-porting a given child category Ccas a simple con-ditional probability based on normalized observa-tion counts.Psupport(Cp|Cc) =count(Ccon Cp)count(Cc)5.2 Supporting surfacesTo identify which surfaces on parent objects sup-port child objects, we first segment parent modelsinto planar surfaces using a simple region-growingalgorithm based on (Kalvin and Taylor, 1996).
Wecharacterize support surfaces by the direction oftheir normal vector limited to the six canonical di-rections: up, down, left, right, front, back.
We thenlearn a probability of supporting surface normaldirection Sngiven child object category Cc.
Forexample, posters are typically found on walls sotheir support normal vectors are in the horizontaldirections.
Any unobserved child categories areassumed to have Psurf(Sn= up|Cc) = 1 sincemost things rest on a horizontal surface (e.g.
floor).Psurf(Sn|Cc) =count(Ccon surface with Sn)count(Cc)5.3 Spatial RelationsFor spatial relations we use a set of predefined re-lations: left, right, above, below, front, back, ontop of, next to, near, inside, and outside.
Theseare measured using axis-aligned bounding boxesfrom the viewer?s perspective.
More concretely,the bounding boxes of the two objects involved ina spatial relation are compared to determine vol-ume overlap or closest distance (for proximity re-lations).
Table 1 gives a few examples of the defi-nitions of these spatial relations.Since these spatial relations are resolvedwith re-spect to the current view of the scene, they corre-spond to view-centric definitions of these spatialRelation P (relation)inside(A,B) V ol(A?B)V ol(A)outside(A,B) 1 - V ol(A?B)V ol(A)left(A,B) V ol(A?
left (B))V ol(A)right(A,B) V ol(A?
right (B))V ol(A)near(A,B) 1(dist(A,B) < tnear)Table 1: Definitions of spatial relation using objectbounding box computations.
Note that dist(A,B)is normalized with respect to the maximum extentof the bounding box of B.concepts.
An interesting line of future work wouldbe to explore when ego-centric and object-centricspatial reference models are more likely in a givenutterance, and resolve the spatial term accordingly.6 Constraint ParsingDuring constraint parsing we take the input textand identify the objects and the relations betweenthem.
For each object, we also identify proper-ties associated with it such as category label, ba-sic attributes such as color and material, and num-ber of occurrences in the scene.
Based on the ob-ject category and attributes, and other words inthe noun phrase mentioning the object, we iden-tify a set of associated keywords to be used laterfor querying the 3D model database.
Spatial re-lations between objects are extracted as predicatesof the form on(A,B) or left(A,B) where A and B arerecognized objects.As an example, given the input ?There is aroom with a desk and a red chair.
The chair isto the left of the desk.?
we extract the followingobjects and spatial relations:Objects:index category attributes keywords0 room room1 desk desk2 chair color:red chair, redRelations: left(chair, desk)The input text is processed using the StanfordCoreNLP pipeline1.
We use the Stanford corefer-ence system to determine when the same object isbeing referred to.
To identify objects, we look fornoun phrases and use the head word as the cate-gory, filtering with WordNet (Miller, 1995) to de-termine which objects are visualizable (under the1http://nlp.stanford.edu/software/corenlp.shtml17Dependency Pattern Example Texttag:VBN=verb >nsubjpass =nsubj >prep (=prep >pobj =pobj) The chair[nsubj] is made[verb] of[prep] wood[pobj]tag:VB=verb >dobj =dobj >prep (=prep >pobj =pobj) Put[verb] the cup[dobj] on[prep] the table[pobj]Table 2: Example dependency patterns for extracting spatial relations.Figure 2: Generated scene for ?There is a roomwith a desk and a lamp.
There is a chair to theright of the desk.?
The inferred scene hierarchy isoverlayed in the center.physical object synset, excluding locations).
Toidentify properties of the objects, we extract otheradjectives and nouns in the noun phrase.
We alsomatch dependency patterns such as ?X is made ofY?
to extract more attributes and keywords.
Fi-nally, we use dependency patterns to extract spa-tial relations between objects (see Table 2 for someexample patterns).We used a fairly simple deterministic approachto map text to the scene template and user actionson the scene.
An interesting avenue for future re-search is to automatically learn how to map textusing more advanced semantic parsing methods.7 Scene GenerationDuring scene generation we aim to find the mostlikely scene given the input utterance, and priorknowledge.
Once we have determined from theinput text what objects exist and their spatial re-Figure 3: Generated scene for ?There is a roomwith a poster bed and a poster.
?Figure 4: Generated scene for ?There is a roomwith a table and a sandwich.?
Note that the plate isnot explicitly stated, but is inferred by the system.lations in the scene, we select 3D models match-ing the objects and their associated properties.
Wesample the support hierarchy prior Psupportto ob-tain the support hierarchy for the scene.We then initialize the positions of objects withinthe scene by traversing the support hierarchy indepth-first order, positioning the largest availablechild node and recursing.
Child nodes are posi-tioned by selecting a supporting surface on a can-didate parent object through sampling ofPsurfandensuring no collisions exist with other objects.
Ifthere are any spatial constraints that are not satis-fied, we remove and randomly reposition the ob-jects violating the constraints, and iterate to im-prove the layout.
The resulting scene is renderedand presented to the user.Figure 2 shows a rendering of a generated scenealong with the support hierarchy and input text.Even though the spatial relation between lamp anddesk was not mentioned explicitly, we infer thatthe lamp is supported by the top surface of thedesk.
In Figure 3 we show another example ofa generated scene for the input ?There is a roomwith a poster bed and a poster?.
Note that the sys-tem differentiates between a ?poster?
and a ?posterbed?
?
it correctly selects and places the bed on thefloor, while the poster is placed on the wall.Figure 4 shows an example of inferring missingobjects.
Even though the plate was not explicitlymentioned in the input, we infer that the sandwichis more likely to be supported by a plate rather thandirectly placed on the table.
Without this infer-18Figure 5: Left: chair is selected using ?the chair tothe right of the table?
or ?the object to the right ofthe table?.
Chair is not selected for ?the cup to theright of the table?.
Right: Different view resultsin different chair being selected for the input ?thechair to the right of the table?.ence, the user would need to bemuchmore verbosewith text such as ?There is a room with a table, aplate and a sandwich.
The sandwich is on the plate,and the plate is on the table.
?8 Interactive SystemOnce a scene is generated, the user can view thescene and manipulate it using both simple actionphrases and mouse interaction.
The system sup-ports traditional 3D scene interaction mechanismssuch as navigating the viewpoint with mouse andkeyboard, selection and movement of object mod-els by clicking.
In addition, a user can give simpletextual commands to select and modify objects, orto refine the scene.
For example, a user can re-quest to ?remove the chair?
or ?put a pot on thetable?
which requires the system to resolve refer-ents to objects in the scene (see ?8.1).
The systemtracks user interactions throughout this process andcan adjust its spatial knowledge accordingly.
Inthe following sections, we give some examples ofhow the user can interact with the system and howthe system learns from this interaction.8.1 View centric spatial relationsDuring interaction, the user can refer to objectswith their categories and with spatial relations be-tween them.
Objects are disambiguated by bothcategory and view-centric spatial relations.
We usethe WordNet hierarchy to resolve hyponym or hy-pernym referents to objects in the scene.
In the leftscreenshot in Figure 5, the user can select a chairto the right of the table using the phrase ?chair tothe right of the table?
or ?object to the right of thetable?.
The user can then change their viewpointby rotating and moving around.
Since spatial rela-tions are resolved with respect to the current view-point, we see that a different chair is selected forFigure 6: Left: initial scene.
Right: after input?Put a lamp on the table?.the same phrase from the different viewpoint in theright screenshot.8.2 Scene Editing with TextBy using simple textual commands the user canedit the scene.
For example, given the initial sceneon the left in Figure 6, the user can then issue thecommand ?put a lamp on the table?
which resultsin the scene on the right.
The system currently al-lows for adding objects to new positions and re-moving existing objects.
Currently, repositioningof objects is performed only with direct control,but in the future we also plan to support reposi-tioning of objects by using textual commands.8.3 Learning Support HierarchyAfter a user requests that a lamp be placed on a ta-ble, the system updates its prior on the likelihoodof a lamp being supported by a table.
Based onprior observations the likelihood of lamps beingplaced on tables was very low (4%) since very fewlamps were observed on tables in the scene dataset.However, after the user interaction, we recomputethe prior including the scene that the user has cre-ated and the probability of lamp on table increasesto 12% (see Figure 7).8.4 Learning Object NamesOften, objects or parts may not have associated la-bels that the user would use to refer to the objects.In those cases, the system can inform the user thatit cannot resolve a given name, and the user canthen select the object or part of the object they werereferring to and annotate it with a label.
For in-stance, in Figure 8, the user annotated the differ-ent parts of the room as ?floor?, ?wall?, ?window?,and ?door?.
Before annotation, the system did notknow any labels for these parts of the room.
Afterannotation, the user can select these parts using theassociated names.
In addition, the system updatesits spatial knowledge base and can now predict thatthe probability of a poster being placed on a wall190% 25% 50% 75% 100%BeforeAfterNightstandNightstandRoomRoomTableTableDeskDeskFigure 7: Probability of supporting parent categories for lamps before and after the user explicitly requestsa lamp on a table.Figure 8: The user clicks and selects parts of the scene, annotating them as ?floor?, ?wall?, ?window?,?door?.
After annotation, the user can also refer to these parts with the associated names.
The systemspatial knowledge base is updated accordingly.is 40%, and that the probability of a table beingplaced on the floor is 23%.
Note that these prob-abilities are based on multiple observations of theannotated room.
Accumulating annotations suchas these and propagating labels to new models isan effective way to expand spatial knowledge.9 Future WorkWe described a preliminary interactive text to 3Dscene generation system that can learn from priordata and user interaction.
We hope to improvethe system by incorporating more feedback mech-anisms for the user, and the learning algorithm.If the user requests a particular object be se-lected but the system gets the referent wrong, theuser could then indicate the error and provide a cor-rection.
We can then use this feedback as a sourceof training data to improve the interpretation of textto the desired user action.
For example, if the userasks to ?select the red bowl?
and the system couldnot resolve ?red bowl?
to the correct object, theuser could intervene by clicking on the correct ref-erent object.
Simple interactions such as this areincredibly powerful for providing additional datafor learning.
Though we did not focus on this as-pect, a dialogue-based interaction pattern is naturalfor our system.
The user can converse with the sys-tem to iteratively refine the scene and the systemcan ask for clarifications at any point ?
when andhow the system should inquire for more informa-tion is interesting future research.To evaluate whether the generated scenes aresatisfactory, we can ask people to rate them againstinput text descriptions.
We can also study usageof the system in concrete tasks to see how oftenusers need to provide corrections and manuallymanipulate the scene.
A useful baseline to com-pare against would be a traditional scenemanipula-tion system.
By doing these studies at a large scale,for instance by making the interface available on20the web, we can crowdsource the accumulation ofuser interactions and gathering of spatial knowl-edge.
Simultaneously, running formal user stud-ies to better understand preference for text-basedversus direct interactions during different actionswould be very beneficial for more informed designof text-to-scene generation systems.10 ConclusionWe have demonstrated the usefulness of an inter-active text to 3D scene generation system.
Spatialknowledge is essential for text to 3D scene gener-ation.
While it is possible to learn spatial knowl-edge purely from data, it is hard to have completecoverage of all possible scenarios.
Interaction anduser feedback is a good way to improve coverageand to refine spatial knowledge.
In addition, in-teraction is a natural mode of user involvement inscene generation and creative tasks.Little prior work has addressed the need for in-teraction or the need for recovering implicit spatialconstraints.
We propose that the resolution of un-mentioned spatial constraints, and leveraging userinteraction to acquire spatial knowledge are criti-cal for enabling natural text to scene generation.User interaction is essential for text to scenegeneration since the process is fundamentallyunder-constrained.
Most natural textual descrip-tions of scenes will not mention many visual as-pects of a physical scene.
However, it is still pos-sible to automatically generate a plausible startingscene for refinement.Our work focused on showing that user interac-tion is both natural and useful for a text to scenegeneration system.
Furthermore, refining spatialknowledge through interaction is a promising wayof acquiring more implicit knowledge.
Finally,any practically useful text to scene generation willby necessity involve interaction with users whohave particular goals and tasks in mind.ReferencesSaleema Amershi, James Fogarty, and Daniel Weld.2012.
Regroup: interactive machine learning for on-demand group creation in social networks.
In Pro-ceedings of the SIGCHI Conference on Human Fac-tors in Computing Systems.Sharon Rose Clay and Jane Wilhelms.
1996.
Put:Language-based interactive manipulation of objects.Computer Graphics and Applications, IEEE.Bob Coyne and Richard Sproat.
2001.
WordsEye: anautomatic text-to-scene conversion system.
In Pro-ceedings of the 28th annual conference on Computergraphics and interactive techniques.BobCoyne, Alexander Klapheke,MasoudRouhizadeh,Richard Sproat, and Daniel Bauer.
2012.
Annota-tion tools and knowledge representation for a text-to-scene system.
Proceedings of COLING 2012: Tech-nical Papers.Matthew Fisher, Daniel Ritchie, Manolis Savva,Thomas Funkhouser, and Pat Hanrahan.
2012.Example-based synthesis of 3D object arrangements.ACM Transactions on Graphics (TOG).James Fogarty, Desney Tan, Ashish Kapoor, and SimonWinder.
2008.
CueFlik: interactive concept learn-ing in image search.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems.Yun Jiang and Ashutosh Saxena.
2013.
Infinite la-tent conditional random fields for modeling environ-ments through humans.Yun Jiang, Marcus Lim, and Ashutosh Saxena.
2012.Learning object arrangements in 3D scenes using hu-man context.
In Proceedings of the 29th Interna-tional Conference on Machine Learning (ICML-12).AlanDKalvin andRussell HTaylor.
1996.
Superfaces:Polygonal mesh simplification with bounded error.Computer Graphics and Applications, IEEE.Paul Merrell, Eric Schkufza, Zeyang Li, ManeeshAgrawala, and Vladlen Koltun.
2011.
Interactivefurniture layout using interior design guidelines.
InACM Transactions on Graphics (TOG).G.A.
Miller.
1995.
WordNet: a lexical database forenglish.
CACM.Benjamin Rosman and Subramanian Ramamoorthy.2011.
Learning spatial relationships between ob-jects.
The International Journal of Robotics Re-search.Manolis Savva, Angel X. Chang, Gilbert Bernstein,Christopher D. Manning, and Pat Hanrahan.
2014.On being the right scale: Sizing large collections of3D models.
Stanford University Technical ReportCSTR 2014-03.Lee M Seversky and Lijun Yin.
2006.
Real-time au-tomatic 3D scene generation from natural languagevoice and text descriptions.
In Proceedings of the14th annual ACM international conference on Mul-timedia.Terry Winograd.
1972.
Understanding natural lan-guage.
Cognitive psychology.C Lawrence Zitnick, Devi Parikh, and Lucy Vander-wende.
2013.
Learning the visual interpretationof sentences.
In IEEE Intenational Conference onComputer Vision (ICCV).21
