Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1305?1315,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsShallow Analysis Based Assessment of Syntactic Complexity forAutomated Speech ScoringSuma BhatBeckman Institute,University of Illinois,Urbana, ILspbhat2@illinois.eduHuichao XueDept.
of Computer ScienceUniversity of PittsburghPittsburgh, PAhux10@cs.pitt.eduSu-Youn YoonEducational Testing ServicePrinceton, NJsyoon@ets.orgAbstractDesigning measures that capture variousaspects of language ability is a centraltask in the design of systems for auto-matic scoring of spontaneous speech.
Inthis study, we address a key aspect of lan-guage proficiency assessment ?
syntacticcomplexity.
We propose a novel measureof syntactic complexity for spontaneousspeech that shows optimum empirical per-formance on real world data in multipleways.
First, it is both robust and reliable,producing automatic scores that agree wellwith human rating compared to the state-of-the-art.
Second, the measure makessense theoretically, both from algorithmicand native language acquisition points ofview.1 IntroductionAssessment of a speaker?s proficiency in a secondlanguage is the main task in the domain of au-tomatic evaluation of spontaneous speech (Zech-ner et al, 2009).
Prior studies in language ac-quisition and second language research have con-clusively shown that proficiency in a second lan-guage is characterized by several factors, some ofwhich are, fluency in language production, pro-nunciation accuracy, choice of vocabulary, gram-matical sophistication and accuracy.
The design ofautomated scoring systems for non-native speakerspeaking proficiency is guided by these studies inthe choice of pertinent objective measures of thesekey aspects of language proficiency.The focus of this study is the design and per-formance analysis of a measure of the syntacticcomplexity of non-native English responses foruse in automatic scoring systems.
The state-of-the art automated scoring system for spontaneousspeech (Zechner et al, 2009; Higgins et al, 2011)currently uses measures of fluency and pronuncia-tion (acoustic aspects) to produce scores that are inreasonable agreement with human-rated scores ofproficiency.
Despite its good performance, thereis a need to extend its coverage to higher order as-pects of language ability.
Fluency and pronunci-ation may, by themselves, already be good indi-cators of proficiency in non-native speakers, butfrom a construct validity perspective1, it is neces-sary that an automatic assessment model measurehigher-order aspects of language proficiency.
Syn-tactic complexity is one such aspect of proficiency.By ?syntactic complexity?, we mean a learner?sability to use a wide range of sophisticated gram-matical structures.This study is different from studies that fo-cus on capturing grammatical errors in non-nativespeakers (Foster and Skehan, 1996; Iwashita et al,2008).
Instead of focusing on grammatical errorsthat are found to be highly representative of lan-guage proficiency, our interest is in capturing therange of forms that surface in language productionand the degree of sophistication of such forms,collectively referred to as syntactic complexity in(Ortega, 2003).The choice and design of objective measures oflanguage proficiency is governed by two crucialconstraints:1.
Validity: a measure should show high dis-criminative ability between various levels oflanguage proficiency, and the scores pro-duced by the use of this measure should showhigh agreement with human-assigned scores.2.
Robustness: a measures should be derivedautomatically and should be robust to errorsin the measure generation process.A critical impediment to the robustness con-straint in the state-of-the-art is the multi-stage au-1Construct validity is the degree to which a test measureswhat it claims, or purports, to be measuring and an importantcriterion in the development and use of assessments or tests.1305tomated process, where errors in the speech recog-nition stage (the very first stage) affect subsequentstages.
Guided by studies in second language de-velopment, we design a measure of syntactic com-plexity that captures patterns indicative of profi-cient and non-proficient grammatical structures bya shallow-analysis of spoken language, as opposedto a deep syntactic analysis, and analyze the per-formance of the automatic scoring model with itsinclusion.
We compare and contrast the proposedmeasure with that found to be optimum in Yoonand Bhat (2012).Our primary contributions in this study are:?
We show that the measure of syntactic com-plexity derived from a shallow-analysis ofspoken utterances satisfies the design con-straint of high discriminative ability betweenproficiency levels.
In addition, including ourproposed measure of syntactic complexity inan automatic scoring model results in a statis-tically significant performance gain over thestate-of-the-art.?
The proposed measure, derived through acompletely automated process, satisfies therobustness criterion reasonably well.?
In the domain of native language acquisition,the presence or absence of a grammaticalstructure indicates grammatical development.We observe that the proposed approach ele-gantly and effectively captures this presence-based criterion of grammatical development,since the feature indicative of presence or ab-sence of a grammatical structure is optimalfrom an algorithmic point of view.2 Related WorkSpeaking in a non-native language requires diverseabilities, including fluency, pronunciation, into-nation, grammar, vocabulary, and discourse.
In-formed by studies in second language acquisitionand language testing that regard these factors askey determiners of spoken language proficiency,some researchers have focused on the objectivemeasurement of these aspects of spoken languagein the context of automatic assessment of languageability.
Notable are studies that have focused onassessment of fluency (Cucchiarini et al, 2000;Cucchiarini et al, 2002), pronunciation (Witt andYoung, 1997; Witt, 1999; Franco et al, 1997;Neumeyer et al, 2000), and intonation (Zechneret al, 2009).
The relative success of these studieshas yielded objective measures of acoustic aspectsof speaking ability, resulting in a shift in focusto more complex aspects of assessment of gram-mar (Bernstein et al, 2010; Chen and Yoon, 2011;Chen and Zechner, 2011), topic development (Xieet al, 2012), and coherence (Wang et al, 2013).In an effort to assess grammar and usage in asecond language learning environment, numerousstudies have focused on identifying relevant quan-titative measures.
These measures have been usedto estimate proficiency levels in English as a sec-ond language (ESL) writing with reasonable suc-cess.
Wolf-Quintero et al (1998), Ortega (2003),and Lu (2010) found that measures such as meanlength of T-unit2and dependent clauses per clause(henceforth termed as length-based measures) arewell correlated with holistic proficiency scoressuggesting that these quantitative measures can beused as objective indices of grammatical develop-ment.In the context of spoken ESL, these measureshave been studied as well but the results have beeninconclusive.
The measures could only broadlydiscriminate between students?
proficiency levels,rated on a scale with moderate to weak correla-tions, and strong data dependencies on the par-ticipant groups were observed (Halleck, 1995;Iwashita et al, 2008; Iwashita, 2010).With the recent interest in the area of auto-matic assessment of speech, there is a concur-rent need to assess the grammatical developmentof ESL students automatically.
Studies that ex-plored the applicability of length-based measuresin an automated scoring system (Chen and Zech-ner, 2011; Chen and Yoon, 2011) observed anotherimportant drawback of these measures in that set-ting.
Length-based measures do not meet the con-straints of the design, that, in order for measuresto be effectively incorporated in the automatedspeech scoring system, they must be generated ina fully automated manner, via a multi-stage au-tomated process that includes speech recognition,part of speech (POS) tagging, and parsing.A major bottleneck in the multi-stage processof an automated speech scoring system for secondlanguage is the stage of automated speech recog-nition (ASR).
Automatic recognition of non-nativespeakers?
spontaneous speech is a challenging taskas evidenced by the error rate of the state-of-the-2T-units are defined as ?the shortest grammatically allow-able sentences into which writing can be split.?
(Hunt, 1965)1306art speech recognizer.
For instance, Chen andZechner (2011) reported a 50.5% word error rate(WER) and Yoon and Bhat (2012) reported a 30%WER in the recognition of ESL students?
spokenresponses.
These high error rates at the recogni-tion stage negatively affect the subsequent stagesof the speech scoring system in general, and inparticular, during a deep syntactic analysis, whichoperates on a long sequence of words as its con-text.
As a result, measures of grammatical com-plexity that are closely tied to a correct syntac-tic analysis are rendered unreliable.
Not surpris-ingly, Chen and Zechner (2011) studied measuresof grammatical complexity via syntactic parsingand found that a Pearson?s correlation coefficientof 0.49 between syntactic complexity measures(derived from manual transcriptions) and profi-ciency scores, was drastically reduced to near non-existence when the measures were applied to ASRword hypotheses.
This suggests that measuresthat rely on deep syntactic analysis are unreliablein current ASR-based scoring systems for sponta-neous speech.In order to avoid the problems encounteredwith deep analysis-based measures, Yoon andBhat (2012) explored a shallow analysis-based ap-proach, based on the assumption that the level ofgrammar sophistication at each proficiency levelis reflected in the distribution of part-of-speech(POS) tag bigrams.
The idea of capturing dif-ferences in POS tag distributions for classificationhas been explored in several previous studies.
Inthe area of text-genre classification, POS tag dis-tributions have been found to capture genre differ-ences in text (Feldman et al, 2009; Marin et al,2009); in a language testing context, it has beenused in grammatical error detection and essayscoring (Chodorow and Leacock, 2000; Tetreaultand Chodorow, 2008).
We will see next what as-pects of syntactic complexity are captured by sucha shallow-analysis.3 Shallow-analysis approach tomeasuring syntactic complexityThe measures of syntactic complexity in this ap-proach are POS bigrams and are not obtained by adeep analysis (syntactic parsing) of the structure ofthe sentence.
Hence we will refer to this approachas ?shallow analysis?.
In a shallow-analysis ap-proach to measuring syntactic complexity, we relyon the distribution of POS bigrams at every profi-ciency level to be representative of the range andsophistication of grammatical constructions at thatlevel.
At the outset, POS-bigrams may seem toosimplistic to represent any aspect of true syntacticcomplexity.
We illustrate to the contrary, that theyare indeed able to capture certain grammatical er-rors and sophisticated constructions by means ofthe following instances.
Consider the two sentencefragments below taken from actual responses (thebigrams of interest and their associated POS tagsare bold-faced).1.
They can/MD to/TO survive .
.
.2.
They created the culture/NN that/WDTnow/RB is common in the US.We notice that Example 1 is not only less gram-matically sophisticated than Example 2 but alsohas a grammatical error.
The error stems from thefact that it has a modal verb followed by the word?to?.
On the other hand, Example 2 contains arelative clause composed of a noun introduced by?that?.
Notice how these grammatical expressions(one erroneous and the other sophisticated) can bedetected by the POS bigrams ?MD-TO?
and ?NN-WDT?, respectively.The idea that the level of syntactic complex-ity (in terms of its range and sophistication) canbe assessed based on the distribution of POS-tagsis informed by prior studies in second languageacquisition.
It has been shown that the usage ofcertain grammatical constructions (such as that ofthe embedded relative clause in the second sen-tence above) are indicators of specific milestonesin grammar development (Covington et al, 2006).In addition, studies such as Foster and Skehan(1996) have successfully explored the utility offrequency of grammatical errors as objective mea-sures of grammatical development.Based on this idea, Yoon and Bhat (2012) de-veloped a set of features of syntactic complex-ity based on POS sequences extracted from alarge corpus of ESL learners?
spoken responses,grouped by human-assigned scores of proficiencylevel.
Unlike previous studies, it did not relyon the occurrence of normative grammatical con-structions.
The main assumption was that eachscore level is characterized by different types ofprominent grammatical structures.
These repre-sentative constructions are gathered from a collec-tion of ESL learners?
spoken responses rated foroverall proficiency.
The syntactic complexity ofa test spoken response was estimated based on its1307similarity to the proficiency groups in the refer-ence corpus with respect to the score-specific con-structions.
A score was assigned to the responsebased on how similar it was to the high scoregroup.
In Section 4.1, we go over the approachin further detail.Our current work is inspired by the shallowanalysis-based approach of Yoon and Bhat (2012)and operates under the same assumptions of cap-turing the range and sophistication of grammati-cal constructions at each score level.
However,the approaches differ in the way in which a spo-ken response is assigned to a score group.
Wefirst analyze the limitations of the model studied in(Yoon and Bhat, 2012) and then describe how ourmodel can address those limitations.
The result isa new measure based on POS bigrams to assessESL learners?
mastery of syntactic complexity.4 Models for Measuring GrammaticalCompetenceWe mentioned that the measure proposed in thisstudy is derived from assumptions similar to thosestudied in (Yoon and Bhat, 2012).
Accordingly,we will summarize the previously studied model,outline its limitations, show how our proposedmeasure addresses those limitations and comparethe two measures for the task of automatic scoringof speech.4.1 Vector-Space Model based approachYoon and Bhat (2012) explored an approach in-spired by information retrieval.
They treat the con-catenated collection of responses from a particularscore-class as a ?super?
document.
Then, regard-ing POS bigrams as terms, they construct POS-based vector space models for each score-class(there are four score classes denoting levels of pro-ficiency as will be explained in Section 5.2), thusyielding four score-specific vector-space models(VSMs).
The terms of the VSM are weighted bythe term frequency-inverse document frequency(tf -idf ) weighting scheme (Salton et al, 1975).The intuition behind the approach is that responsesin the same proficiency level often share similargrammar and usage patterns.
The similarity be-tween a test response and a score-specific vector isthen calculated by a cosine similarity metric.
Al-though a total of 4 cosine similarity scores (oneper score group) were generated, only cos4fromamong the four similarity scores, and cosmax,were selected as features.?
cos4: the cosine similarity score between thetest response and the vector of POS bigramsfor the highest score class (level 4); and,?
cosmax: the score level of the VSM withwhich the given response shows maximumsimilarity.Of these, cos4was selected based on its empir-ical performance (it showed the strongest corre-lation with human-assigned scores of proficiencyamong the distance-based measures).
In addition,an intuitive justification for the choice is that thescore-4 vector is a grammatical ?norm?
represent-ing the average grammar usage distribution of themost proficient ESL students.
The measure of syn-tactic complexity of a response, cos4, is its simi-larity to the highest score class.The study found that the measures showed rea-sonable discriminative ability across proficiencylevels.
Despite its encouraging empirical perfor-mance, the VSM method of capturing grammati-cal sophistication has the following limitations.First, the VSM-based method is likely to over-estimate the contribution of the POS bigramswhen highly correlated bigrams occur as terms inthe VSM.
Consider the presence of a grammar pat-tern represented by more than one POS bigram.For example, both ?NN-WDT?
and ?WDT-RB?
inSentence 2 reflect the learner?s usage of a relativeclause.
However, we note that the two bigrams arecorrelated and including them both results in anover-estimation of their contribution.
The VSMset-up has no mechanism to handle correlated fea-tures.Second, the tf -idf weighting scheme for rela-tively rare POS bigrams does not adequately cap-ture their underlying distribution with respect toscore groups.
Grammatical expressions that occurfrequently in one score level but rarely in otherlevels can be assumed to be characteristic of aspecific score level.
Therefore, the more uneventhe distribution of a grammatical expression acrossscore classes, the more important that grammaticalexpression should be as an indicator of a particularscore class.
However, the simple idf scheme can-not capture this uneven distribution.
A pattern thatoccurs rarely but uniformly across different scoregroups can get the same weight as a pattern whichis unevenly distributed to one score group.
Mar-tineau and Finin (2009) observed this weakness ofthe tf -idf weighting in the domain of sentiment1308analysis.
When using tf -idf weighting to extractwords that were strongly associated with positivesentiment in a movie review corpus (they consid-ered each review as a document and a word as aterm), it was found that a substantial proportionof words with the highest tf -idf were rare words(e.g., proper nouns) which were not directly asso-ciated with the sentiment.We propose to address these important limita-tions of the VSM approach by the use of a methodthat accounts for each of the deficiencies.
This isdone by resorting to a maximum entropy modelbased approach, to which we turn next.4.2 Maximum Entropy-Based modelIn order to address the limitations discussed in 4.1,we propose a classification-based approach.
Tak-ing an approach different from previous studies,we formulate the task of assigning a score of syn-tactic complexity to a spoken response as a classi-fication problem: given a spoken response, assignthe response to a proficiency class.
A classifier istrained in an inductive fashion, using a large cor-pus of learner responses that is divided into pro-ficiency scores as the training data and then usedto test data that is similar to the training data.
Adistinguishing feature of the current study is thatthe measure is based on a comparison of charac-teristics of the test response to models trained onlarge amounts of data from each score point, as op-posed to measures that are simply characteristicsof the responses themselves (which is how mea-sures have been considered in prior studies).The inductive classifier we use here is themaximum-entropy model (MaxEnt) which hasbeen used to solve several statistical natural lan-guage processing problems with much success(Berger et al, 1996; Borthwick et al, 1998; Borth-wick, 1999; Pang et al, 2002; Klein et al, 2003;Rosenfeld, 2005).
The productive feature en-gineering aspects of incorporating features intothe discriminative MaxEnt classifier motivate themodel choice for the problem at hand.
In partic-ular, the ability of the MaxEnt model?s estimationroutine to handle overlapping (correlated) featuresmakes it directly applicable to address the first lim-itation of the VSM model.
The second limitation,related to the ineffective weighting of terms viathe the tf -idf scheme, seems to be addressed bythe fact that the MaxEnt model assigns a weightto each feature (in our case, POS bigrams) on aper-class basis (in our case, score group), by tak-ing every instance into consideration.
Therefore,a MaxEnt model has an advantage over the modeldescribed in 4.1 in that it uses four different weightschemes (one per score level) and each scheme isoptimized for each score level.
This is beneficialin situations where the features are not evenly im-portant across all score levels.5 Experimental SetupOur experiments seek answers to the followingquestions.1.
To what extent does a MaxEnt-score of syn-tactic complexity discriminate between levelsof proficiency?2.
What is the effect of including the proposedmeasure of syntactic complexity in the state-of-the-art automatic scoring model?3.
How robust is the measure to errors in the var-ious stages of automatic generation?5.1 TasksIn order to answer the motivating questions of thestudy, we set-up two tasks.
In the first task, wecompare the extent to which the VSM-based mea-sure and the MaxEnt-based measure (outlined in4.1 and 4.2 above) discriminate between levels ofsyntactic complexity.
Additionally, we comparethe performance of an automatic scoring model ofoverall proficiency that includes the measures ofsyntactic complexity from each of the two mod-els being compared and analyze the gains with re-spect to the state-of-the-art.
In the second task, westudy the measures?
robustness to errors incurredby ASR.5.2 DataIn this study, we used a collection of responsesfrom an international English language assess-ment.
The assessment consisted of questions towhich speakers were prompted to provide sponta-neous spoken responses lasting approximately 45-60 seconds per question.
Test takers read and/orlistened to stimulus materials and then respondedto questions based on the stimuli.
All questions so-licited spontaneous, unconstrained natural speech.A small portion of the available data with inad-equate audio quality and lack of student responsewas excluded from the study.
The remaining re-sponses were partitioned into two datasets: theASR set and the scoring model training/test (SM)1309set.
The ASR set, with 47,227 responses, wasused for ASR training and POS similarity modeltraining.
The SM set, with 2,950 responses, wasused for feature evaluation and automated scoringmodel evaluation.
There was no overlap in speak-ers between the ASR set and the SM set.Each response was rated for overall proficiencyby trained human scorers using a 4-point scoringscale, where 1 indicates low speaking proficiencyand 4 indicated high speaking proficiency.
Thedistribution of proficiency scores, along with otherdetails of the data sets, are presented in Table 1.As seen in Table 1, there is a strong bias towardsthe middle scores (score 2 and 3) with approxi-mately 84-85% of the responses belonging to thesetwo score levels.
Although the skewed distributionlimits the number of score-specific instances forthe highest and lowest scores available for modeltraining, we used the data without modifying thedistribution since it is representative of responsesin a large-scale language assessment scenario.Human raters?
extent of agreement in the sub-jective task of rating responses for language pro-ficiency constrains the extent to which we can ex-pect a machine?s score to agree with that of hu-mans.
An estimate of the extent to which humanraters agree on the subjective task of proficiencyassessment, is obtained by two raters scoring ap-proximately 5% of data (2,388 responses fromASR set and 140 responses from SM set).
Pear-son correlation r between the scores assigned bythe two raters was 0.62 in ASR set and 0.58 in SMset.
This level of agreement will guide the evalua-tion of the human-machine agreement on scores.5.3 Stages of Automatic GrammaticalCompetence AssessmentHere we outline the multiple stages involved in theautomatic syntactic complexity assessment.
Thefirst stage, ASR, yields an automatic transcription,which is followed by the POS tagging stage.
Sub-sequently, the feature extraction stage (a VSM ora MaxEnt model as the case may be) generates thesyntactic complexity feature which is then incor-porated in a multiple linear regression model togenerate a score.The steps for automatic assessment of overallproficiency follow an analogous process (either in-cluding the POS tagger or not), depending on theobjective measure being evaluated.
The variousobjective measures are then combined in the mul-tiple regression scoring model to generate an over-all score of proficiency.5.3.1 Automatic Speech RecognizerAn HMM recognizer was trained using ASR set(approximately 733 hours of non-native speechcollected from 7,872 speakers).
A gender inde-pendent triphone acoustic model and combinationof bigram, trigram, and four-gram language mod-els were used.
A word error rate (WER) of 31%on the SM dataset was observed.5.3.2 POS taggerPOS tags were generated using the POS taggerimplemented in the Open-NLP toolkit3.
It wastrained on the Switchboard (SWBD) corpus.
ThisPOS tagger was trained on about 528K word/tagpairs.
A combination of 36 tags from the PennTreebank tag set and 6 tags generated for spokenlanguages were used in the tagger.The tagger achieved a tagging accuracy of96.3% on a Switchboard evaluation set composedof 379K words, suggesting high accuracy of thetagger.
However, due to substantial amount ofspeech recognition errors in our data, the POSerror rate (resulting from the combined errors ofASR and automated POS tagger) is expected to behigher.5.3.3 VSM-based ModelWe used the ASR data set to train a POS-bigramVSM for the highest score class and generatedcos4and cosmax reported in Yoon and Bhat(2012), for the SM data set as outlined in Sec-tion 4.1.5.3.4 Maximum Entropy Model ClassifierThe input to the classifier is a set of POS bi-grams (1366 bigrams in all) obtained from thePOS-tagged output of the data.
We consideredbinary-valued features (whether a POS bigram oc-curred or not), occurrence frequency, and relativefrequency as input for the purpose of experimen-tation.
We used the maximum entropy classifierimplementation in the MaxEnt toolkit4.
The clas-sifier was trained using the LBFGS algorithm forparameter estimation and used equal-scale gaus-sian priors for smoothing.
The results that fol-low are based on MaxEnt classifier?s parametersettings initialized to zero.
Since a preliminary3http://opennlp.apache.org4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.1310Data set No.
of No.
of Score Score distributionresponses speakers Mean SD 1 2 3 4ASR 47,227 7,872 2.67 0.73 1,953 16,834 23,106 5,3344% 36% 49% 11%SM 2,950 500 2.61 0.74 166 1,103 1,385 2966% 37% 47% 10%Table 1: Data size and score distributionanalysis of the effect of varying the feature (bi-nary or frequency) revealed that the binary-valuedfeature was optimal (in terms of yielding the bestagreement between human and machine scores),we only report our results for this case.
The ASRdata set was used to train the MaxEnt classifier andthe features generated from the SM data set wereused for evaluation.One straightforward way of using the maximumentropy classifier?s prediction for our case is todirectly use its predicted score-level ?
1, 2, 3 or4.
However, this forces the classifier to make acoarse-grained choice and may over-penalize theclassifier?s scoring errors.
To illustrate this, con-sider a scenario where the classifier assigns tworesponses A and B to score level 2 (based on themaximum a posteriori condition).
Suppose that,for response A, the score class with the secondhighest probability corresponds to score level 1and that, for response B, it corresponds to scorelevel 3.
It is apparent that the classifier has anoverall tendency to assign a higher score to B, butlooking at its top preference alone (2 for both re-sponses), masks this tendency.We thus capture the classifier?s finer-grainedscoring tendency by calculating the expected valueof the classifier output.
For a given response, theMaxEnt classifier calculates the conditional prob-ability of a score-class given the response, in turnyielding conditional probabilities of each scoregroup given the observation ?
pifor score groupi ?
{1, 2, 3, 4}.
In our case, we consider the pre-dicted score of syntactic complexity to be the ex-pected value of the class label given the observa-tion as, mescore = 1?p1+2?p2+3?p3+4?p4.This permits us to better represent the score as-signed by the MaxEnt classifier as a relative pref-erence over score assignments.5.3.5 Automatic Scoring SystemWe consider a multiple regression automatic scor-ing model as studied in Zechner et al (2009; Chenand Zechner (2011; Higgins et al (2011).
In itsstate-of-the-art set-up, the following model usesthe features ?
HMM acoustic model score (globalnormalized), speaking rate, word types per sec-ond, average chunk length in words and languagemodel score (global normalized).
We use thesefeatures by themselves (Base), and also in con-junction with the VSM-based feature (cva4) andthe MaxEnt-based feature (mescore).5.4 Evaluation MetricWe evaluate the measures using the metrics cho-sen in previous studies (Zechner et al, 2009; Chenand Zechner, 2011; Yoon and Bhat, 2012).
Ameasure?s utility has been evaluated according toits ability to discriminate between levels of pro-ficiency assigned by human raters.
This is doneby considering the Pearson correlation coefficientbetween the feature and the human scores.
In anideal situation, we would have compared machinescore with scores of grammatical skill assigned byhuman raters.
In our case, however, with onlyaccess to the overall proficiency scores, we usescores of language proficiency as those of gram-matical skill.A criterion for evaluating the performance ofthe scoring model is the extent to which the au-tomatic scores of overall proficiency agree withthe human scores.
As in prior studies, here toothe level of agreement is evaluated by means ofthe weighted kappa measure as well as unroundedand rounded Pearson?s correlations between ma-chine and human scores (since the output of the re-gression model can either be rounded or regardedas is).
The feature that maximizes this degree ofagreement will be preferred.6 Experimental ResultsFirst, we compare the discriminative ability ofmeasures of syntactic complexity (VSM-modelbased measure with that of the MaxEnt-basedmeasure) across proficiency levels.
Table 2 sum-marizes our experimental results for this task.
We1311Features Manual Transcriptions ASRmescore 0.57 0.52cos40.48 0.43cosmax - 0.31Table 2: Pearson correlation coefficients between measures and holistic proficiency scores.
All valuesare significant at level 0.01.
Only the measures cos4and mescore were compared for robustness usingmanual and ASR transcriptions.notice that of the measures compared, mescoreshows the highest correlation with scores of syn-tactic complexity.
The correlation was approxi-mately 0.1 higher in absolute value than that ofcos4, which was the best performing feature in theVSM-based model and the difference is statisti-cally significant.Seeking to study the robustness of the mea-sures derived using a shallow analysis, we nextcompare the two measures studied here, with re-spect to the impact of speech recognition errors ontheir correlation with scores of syntactic complex-ity.
Towards this end, we compare mescore andcos4when POS bigrams are extracted from man-ual transcriptions (ideal ASR) and ASR transcrip-tions.In Table 2, noticing that the correlations de-crease going along a row, we can say that the er-rors in the ASR system caused both mescore andcos4to under-perform.
However, the performancedrop (around 0.05) resulting from a shallow anal-ysis is relatively small compared to the drop ob-served while employing a deep syntactic analysis.Chen and Zechner (2011) found that while usingmeasures of syntactic complexity obtained fromtranscriptions, errors in ASR transcripts causedover 0.40 drop in correlation from that found withmanual transcriptions5.
This comparison suggeststhat the current POS-based shallow analysis ap-proach is more robust to ASR errors compared toa syntactic analysis-based approach.The effect of the measure of syntactic complex-ity is best studied by including it in an automaticscoring model of overall proficiency.
We com-pare the performance gains over the state-of-the-art with the inclusion of additional features (VSM-based and MaxEnt-based, in turn).
Table 3 showsthe system performance with different grammarsophistication measures.
The results reported areaveraged over a 5-fold cross validation of the mul-tiple regression model, where 80% of the SM data5Due to differences in the dataset and ASR system, a di-rect comparison between the current study and the cited priorstudy was not possible.set is used to train the model and the evaluation isdone using 20% of the data in every fold.As seen in Table 3, using the proposed measure,mescore, leads to an improved agreement be-tween human and machine scores of proficiency.Comparing the unrounded correlation results inTable 3 we notice that the model Base+mescoreshows the highest correlation of predicted scoreswith human scores.
In addition, we test the sig-nificance of the difference between two depen-dent correlations using Steiger?s Z-test (via thepaired.r function in the R statistical package(Revelle, 2012)).
We note that the performancegain of Base+mescore over Base as well as overBase + cos4 is statistically significant at level =0.01.
The performance gain of Base+cos4 overBase, however, is not statistically significant atlevel = 0.01.
Thus, the inclusion of the MaxEnt-based measure of syntactic complexity results inimproved agreement between machine and hu-man scores compared to the state-of-the-art model(here, Base).7 DiscussionsWe now discuss some of the observations and re-sults of our study with respect to the followingitems.Improved performance: We sought to verifyempirically that the MaxEnt model really outper-forms the VSM in the case of correlated POSbigrams.
To see this, we separate the test setinto three subsets A,B,C.
Set A contains re-sponses where MaxEnt outperforms VSM; set Bcontains responses where VSM outperforms Max-Ent; set C contains responses where their predic-tions are comparable.
For each group of responsess ?
{A,B,C}, we calculate the percentage of re-sponses Pswhere two highly correlated POS bi-grams occur6.
We found that the percentages fol-low the order: PA= 12.93% > PC= 7.29% >6We consider two POS bigrams to be highly correlated,when the their pointwise-mutual information is higher than4.1312Evaluation method Base Base+cos4 Base+mescoreWeighted kappa 0.503 0.524 0.546Correlation (unrounded) 0.548 0.562 0.592Correlation (rounded) 0.482 0.492 0.519Table 3: Comparison of scoring model performances using features of syntactic complexity studied inthis paper along with those available in the state-of-the-art.
Here, Base is the scoring model without themeasures of syntactic complexity.
All correlations are significant at level 0.01.PB= 4.41%.
This suggests that when correlatedPOS bigrams occur, MaxEnt is more likely to pro-vide better score predictions than VSM does.Feature design: In the case of MaxEnt,the observation that binary-valued features (pres-ence/absence of POS bigrams) yield better perfor-mance than features indicative of the occurrencefrequency of the bigram has interesting implica-tions.
This was also observed in Pang et al (2002)where it was interpreted to mean that overall senti-ment is indicated by the presence/absence of key-words, as opposed to topic of a text, which is in-dicated by the repeated use of the same or simi-lar terms.
An analogous explanation is applicablehere.At first glance, the use of the presence/absenceof grammatical structures may raise concernsabout a potential loss of information (e.g.
the dis-tinction between an expression that is used onceand another that is used multiple times is lost).However, when considered in the context of lan-guage acquisition studies, this approach seems tobe justified.
Studies in native language acquisi-tion, have considered multiple grammatical devel-opmental indices that represent the grammaticallevels reached at various stages of language acqui-sition.
For instance, Covington et al (2006) pro-posed the revised D-level scale which was origi-nally studied by Rosenberg and Abbeduto (1987).The D-Level Scale categorizes grammatical de-velopment into 8 levels according to the pres-ence of a set of diverse grammatical expressionsvarying in difficulty (for example, level 0 con-sists of simple sentences, while level 5 consistsof sentences joined by a subordinating conjunc-tion).
Similarly, Scarborough (1990) proposedthe Index of Productive Syntax (IPSyn), accord-ing to which, the presence of particular grammati-cal structures, from a list of 60 structures (rangingfrom simple ones such as including only subjectsand verbs, to more complex constructions such asconjoined sentences) is evidence of language ac-quisition milestones.Despite the functional differences between theindices, there is a fundamental operational simi-larity - that they both use the presence or absenceof grammatical structures, rather than their oc-currence count, as evidence of acquisition of cer-tain grammatical levels.
The assumption that apresence-based view of grammatical level acquisi-tion is also applicable to second language assess-ment helps validate our observation that binary-valued features yield a better performance whencompared with frequency-valued features.Generalizability: The training and test setsused in this study had similar underlying distribu-tions ?
they both sought unconstrained responsesto a set of items with some minor differences initem type.
Looking ahead, an important questionis the extent to which our measure is sensitive to amismatch between training and test data.8 ConclusionsSeeking alternatives to measuring syntactic com-plexity of spoken responses via syntactic parsers,we study a shallow-analysis based approach foruse in automatic scoring.Empirically, we show that the proposed mea-sure, based on a maximum entropy classification,satisfied the constraints of the design of an objec-tive measure to a high degree.
In addition, the pro-posed measure was found to be relatively robust toASR errors.
The measure outperformed a relatedmeasure of syntactic complexity (also based onshallow-analysis of spoken response) previouslyfound to be well-suited for automatic scoring.
In-cluding the measure of syntactic complexity inan automatic scoring model resulted in statisti-cally significant performance gains over the state-of-the-art.
We also make an interesting observa-tion that the impressionistic evaluation of syntacticcomplexity is better approximated by the presenceor absence of grammar and usage patterns (andnot by their frequency of occurrence), an idea sup-ported by studies in native language acquisition.1313ReferencesAdam L Berger, Vincent J Della Pietra, and StephenA Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional linguistics, 22(1):39?71.Jared Bernstein, Jian Cheng, and Masanori Suzuki.2010.
Fluency and structural complexity as predic-tors of L2 oral proficiency.
In Proceedings of Inter-Speech, pages 1241?1244.Andrew Borthwick, John Sterling, Eugene Agichtein,and Ralph Grishman.
1998.
Exploiting diverseknowledge sources via maximum entropy in namedentity recognition.
In Proc.
of the Sixth Workshopon Very Large Corpora.Andrew Borthwick.
1999.
A maximum entropy ap-proach to named entity recognition.
Ph.D. thesis,New York University.Lei Chen and Su-Youn Yoon.
2011.
Detectingstructural events for assessing non-native speech.In Proceedings of the 6th Workshop on InnovativeUse of NLP for Building Educational Applications,IUNLPBEA ?11, pages 38?45, Stroudsburg, PA,USA.
Association for Computational Linguistics.Miao Chen and Klaus Zechner.
2011.
Computingand evaluating syntactic complexity features for au-tomated scoring of spontaneous non-native speech.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, pages 722?731.Martin Chodorow and Claudia Leacock.
2000.
An un-supervised method for detecting grammatical errors.In Proceedings of NAACL, pages 140?147.Michael A Covington, Congzhou He, Cati Brown, Lo-rina Naci, and John Brown.
2006.
How complexis that sentence?
a proposed revision of the rosen-berg and abbeduto d-level scale.
ReVision.
Wash-ington, DC http://www.
ai.
uga.
edu/caspr/2006-01-Covington.
pdf.
(Accessed May 10, 2010.
).Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learn-ers?
fluency by means of automatic speech recogni-tion technology.
The Journal of the Acoustical Soci-ety of America, 107(2):989?999.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2002.Quantitative assessment of second language learn-ers?
fluency: comparisons between read and sponta-neous speech.
The Journal of the Acoustical Societyof America, 111(6):2862?2873.Sergey Feldman, M.A.
Marin, Mari Ostendorf, andMaya R. Gupta.
2009.
Part-of-speech histogramsfor genre classification of text.
In Proceedings ofICASSP, pages 4781 ?4784.Pauline Foster and Peter Skehan.
1996.
The influenceof planning and task type on second language per-formance.
Studies in Second Language Acquisition,18:299?324.Horacio Franco, Leonardo Neumeyer, Yoon Kim, andOrith Ronen.
1997.
Automatic pronunciation scor-ing for language instruction.
In Proceedings ofICASSP, pages 1471?1474.Gene B Halleck.
1995.
Assessing oral proficiency: acomparison of holistic and objective measures.
TheModern Language Journal, 79(2):223?234.Derrick Higgins, Xiaoming Xi, Klaus Zechner, andDavid Williamson.
2011.
A three-stage approachto the automated scoring of spontaneous spoken re-sponses.
Computer Speech & Language, 25(2):282?306.Kellogg W Hunt.
1965.
Grammatical structures writ-ten at three grade levels.
ncte research report no.
3.Noriko Iwashita, Annie Brown, Tim McNamara, andSally O?Hagan.
2008.
Assessed levels of secondlanguage speaking proficiency: How distinct?
Ap-plied Linguistics, 29(1):24?49.Noriko Iwashita.
2010.
Features of oral proficiency intask performance by efl and jfl learners.
In Selectedproceedings of the Second Language Research Fo-rum, pages 32?47.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings of theseventh conference on Natural language learning atHLT-NAACL 2003-Volume 4, pages 180?183.
Asso-ciation for Computational Linguistics.Xiaofei Lu.
2010.
Automatic analysis of syntac-tic complexity in second language writing.
Inter-national Journal of Corpus Linguistics, 15(4):474?496.M.A Marin, Sergey Feldman, Mari Ostendorf, andMaya R. Gupta.
2009.
Filtering web text to matchtarget genres.
In Proceedings of ICASSP, pages3705?3708.Justin Martineau and Tim Finin.
2009.
Delta tfidf: Animproved feature space for sentiment analysis.
InICWSM.Leonardo Neumeyer, Horacio Franco, Vassilios Di-galakis, and Mitchel Weintraub.
2000.
Automaticscoring of pronunciation quality.
Speech Communi-cation, pages 88?93.Lourdes Ortega.
2003.
Syntactic complexity measuresand their relationship to L2 proficiency: A researchsynthesis of college?level L2 writing.
Applied Lin-guistics, 24(4):492?518.1314Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.William Revelle, 2012. psych: Procedures for Psycho-logical, Psychometric, and Personality Research.Northwestern University, Evanston, Illinois.
Rpackage version 1.2.1.Sheldon Rosenberg and Leonard Abbeduto.
1987.
In-dicators of linguistic competence in the peer groupconversational behavior of mildly retarded adults.Applied Psycholinguistics, 8:19?32.Ronald Rosenfeld.
2005.
Adaptive statistical languagemodeling: a maximum entropy approach.
Ph.D. the-sis, IBM.Gerard Salton, Anita Wong, and Chung-Shu Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613?620.Hollis S Scarborough.
1990.
Index of productive syn-tax.
Applied Psycholinguistics, 11(1):1?22.Joel R. Tetreault and Martin Chodorow.
2008.
Theups and downs of preposition error detection in ESLwriting.
In Proceedings of COLING, pages 865?872.Xinhao Wang, Keelan Evanini, and Klaus Zechner.2013.
Coherence modeling for the automated as-sessment of spontaneous spoken responses.
In Pro-ceedings of NAACL-HLT, pages 814?819.Silke Witt and Steve Young.
1997.
Performancemeasures for phone-level pronunciation teaching inCALL.
In Proceedings of STiLL, pages 99?102.Silke Witt.
1999.
Use of the speech recognition incomputer-assisted language learning.
Unpublisheddissertation, Cambridge University Engineering de-partment, Cambridge, U.K.Kate Wolf-Quintero, Shunji Inagaki, and Hae-YoungKim.
1998.
Second language development in writ-ing: Measures of fluency, accuracy, and complexity.Technical Report 17, Second Language Teachingand curriculum Center, The University of Hawai?i,Honolulu, HI.Shasha Xie, Keelan Evanini, and Klaus Zechner.
2012.Exploring content features for automated speechscoring.
In Proceedings of the NAACL-HLT, pages103?111.Su-Youn Yoon and Suma Bhat.
2012.
Assessment ofesl learners?
syntactic competence based on similar-ity measures.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 600?608.
Association for Compu-tational Linguistics.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51(10):883?895.1315
