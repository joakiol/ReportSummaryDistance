Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 248?258,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsMetaphor Detection with Cross-Lingual Model TransferYulia Tsvetkov Leonid Boytsov Anatole Gershman Eric Nyberg Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA{ytsvetko, srchvrs, anatoleg, ehn, cdyer}@cs.cmu.eduAbstractWe show that it is possible to reliably dis-criminate whether a syntactic constructionis meant literally or metaphorically usinglexical semantic features of the words thatparticipate in the construction.
Our modelis constructed using English resources,and we obtain state-of-the-art performancerelative to previous work in this language.Using a model transfer approach by piv-oting through a bilingual dictionary, weshow our model can identify metaphoricexpressions in other languages.
We pro-vide results on three new test sets in Span-ish, Farsi, and Russian.
The results sup-port the hypothesis that metaphors areconceptual, rather than lexical, in nature.1 IntroductionLakoff and Johnson (1980) characterize metaphoras reasoning about one thing in terms of another,i.e., a metaphor is a type of conceptual mapping,where words or phrases are applied to objects andactions in ways that do not permit a literal inter-pretation.
They argue that metaphors play a fun-damental communicative role in verbal and writ-ten interactions, claiming that much of our every-day language is delivered in metaphorical terms.There is empirical evidence supporting the claim:recent corpus studies have estimated that the pro-portion of words used metaphorically ranges from5% to 20% (Steen et al, 2010), and Thibodeau andBoroditsky (2011) provide evidence that a choiceof metaphors affects decision making.Given the prevalence and importance ofmetaphoric language, effective automatic detec-tion of metaphors would have a number of ben-efits, both practical and scientific.
Language pro-cessing applications that need to understand lan-guage or preserve meaning (information extrac-tion, machine translation, dialog systems, senti-ment analysis, and text analytics, etc.)
would haveaccess to a potentially useful high-level bit of in-formation about whether something is to be under-stood literally or not.
Second, scientific hypothe-ses about metaphoric language could be testedmore easily at a larger scale with automation.However, metaphor detection is a hard problem.On one hand, there is a subjective component: hu-mans may disagree whether a particular expres-sion is used metaphorically or not, as there is noclear-cut semantic distinction between figurativeand metaphorical language (Shutova, 2010).
Onthe other, metaphors can be domain- and context-dependent.1Previous work has focused on metaphor identi-fication in English, using both extensive manually-created linguistic resources (Mason, 2004; Gedi-gian et al, 2006; Krishnakumaran and Zhu, 2007;Turney et al, 2011; Broadwell et al, 2013) andcorpus-based approaches (Birke and Sarkar, 2007;Shutova et al, 2013; Neuman et al, 2013; Shutovaand Sun, 2013; Hovy et al, 2013).
We build onthis foundation and also extend metaphor detec-tion into other languages in which few resourcesmay exist.
Our work makes the following con-tributions: (1) we develop a new state-of-the-artEnglish metaphor detection system that uses con-ceptual semantic features, such as a degree of ab-stractness and semantic supersenses;2(2) we cre-ate new metaphor-annotated corpora for Russianand English;3(3) using a paradigm of model trans-fer (McDonald et al, 2011; T?ackstr?om et al, 2013;Kozhenikov and Titov, 2013), we provide sup-port for the hypothesis that metaphors are concep-1For example, drowning students could be used metaphor-ically to describe the situation where students are over-whelmed with work, but in the sentence a lifeguard saveddrowning students, this phrase is used literally.2https://github.com/ytsvetko/metaphor3http://www.cs.cmu.edu/?ytsvetko/metaphor/datasets.zip248tual (rather than lexical) in nature by showing thatour English-trained model can detect metaphors inSpanish, Farsi, and Russian.2 MethodologyOur task in this work is to define features that dis-tinguish between metaphoric and literal uses oftwo syntactic constructions: subject-verb-object(SVO) and adjective-noun (AN) tuples.4We giveexamples of a prototypical metaphoric usage ofeach type:?
SVO metaphors.
A sentence containing ametaphoric SVO relation is my car drinksgasoline.
According to Wilks (1978), thismetaphor represents a violation of selectionalpreferences for the verb drink, which is nor-mally associated with animate subjects (thecar is inanimate and, hence, cannot drink inthe literal sense of the verb).?
AN metaphors.
The phrase broken promiseis an AN metaphor, where attributes froma concrete domain (associated with the con-crete word broken) are transferred to a moreabstract domain, which is represented by therelatively abstract word promise.
That is, wemap an abstract concept promise to a concretedomain of physical things, where things canbe literally broken to pieces.Motivated by Lakoff?s (1980) argument thatmetaphors are systematic conceptual mappings,we will use coarse-grained conceptual, rather thanfine-grained lexical features, in our classifier.
Con-ceptual features pertain to concepts and ideas asopposed to individual words or phrases expressedin a particular language.
In this sense, as long astwo words in two different languages refer to thesame concepts, their conceptual features shouldbe the same.
Furthermore, we hypothesize thatour coarse semantic features give us a language-invariant representation suitable for metaphor de-tection.
To test this hypothesis, we use a cross-lingual model transfer approach: we use bilingualdictionaries to project words from other syntacticconstructions found in other languages into En-glish and then apply the English model on the de-rived conceptual representations.4Our decision to focus on SVO and AN metaphors is jus-tified by corpus studies that estimate that verb- and adjective-based metaphors account for a substantial proportion of allmetaphoric expressions, approximately 60% and 24%, re-spectively (Shutova and Teufel, 2010; Gandy et al, 2013).Each SVO (or AN) instance will be representedby a triple (duple) from which a feature vectorwill be extracted.5The vector will consist of theconcatenation of the conceptual features (whichwe discuss below) for all participating words, andconjunction features for word pairs.6For example,to generate the feature vector for the SVO triple(car, drink, gasoline), we compute all the featuresfor the individual words car, drink, gasoline andcombine them with the conjunction features forthe pairs car drink and drink gasoline.We define three main feature categories (1) ab-stractness and imageability, (2) supersenses, (3)unsupervised vector-space word representations;each category corresponds to a group of featureswith a common theme and representation.?
Abstractness and imageability.
Abstract-ness and imageability were shown to be use-ful in detection of metaphors (it is easier toinvoke mental pictures of concrete and im-ageable words) (Turney et al, 2011; Broad-well et al, 2013).
We expect that abstract-ness, used in conjunction features (e.g., afeature denoting that the subject is abstractand the verb is concrete), is especially use-ful: semantically, an abstract agent perform-ing a concrete action is a strong signal ofmetaphorical usage.Although often correlated with abstractness,imageability is not a redundant property.While most abstract things are hard to visu-alize, some call up images, e.g., vengeancecalls up an emotional image, torture calls upemotions and even visual images.
There areconcrete things that are hard to visualize too,for example, abbey is harder to visualize thanbanana (B. MacWhinney, personal commu-nication).?
Supersenses.
Supersenses7are coarse se-mantic categories originating in WordNet.For nouns and verbs there are 45 classes:26 for nouns and 15 for verbs, for example,5Looking at components of the syntactic constructions in-dependent of their context has its limitations, as discussedabove with the drowning students example; however, it sim-plifies the representation challenges considerably.6If word one is represented by features u ?
Rnand wordtwo by features v ?
Rmthen the conjunction feature vectoris the vectorization of the outer product uv>.7Supersenses are called ?lexicographer classes?
in Word-Net documentation (Fellbaum, 1998), http://wordnet.princeton.edu/man/lexnames.5WN.html249noun.body, noun.animal, verb.consumption,or verb.motion (Ciaramita and Altun, 2006).English adjectives do not, as yet, have a sim-ilar high-level semantic partitioning in Word-Net, thus we use a 13-class taxonomy of ad-jective supersenses constructed by Tsvetkovet al (2014) (discussed in ?3.2).Supersenses are particularly attractive fea-tures for metaphor detection: coarse sensetaxonomies can be viewed as semantic con-cepts, and since concept mapping is a pro-cess in which metaphors are born, weexpect different supersense co-occurrencesin metaphoric and literal combinations.In ?drinks gasoline?, for example, map-ping to supersenses would yield a pair<verb.consumption, noun.substance>, con-trasted with <verb.consumption, noun.food>for ?drinks juice?.
In addition, this coarsesemantic categorization is preserved in trans-lation (Schneider et al, 2013), which makessupersense features suitable for cross-lingualapproaches such as ours.?
Vector space word representations.
Vec-tor space word representations learned us-ing unsupervised algorithms are often effec-tive features in supervised learning methods(Turian et al, 2010).
In particular, many suchrepresentations are designed to capture lex-ical semantic properties and are quite effec-tive features in semantic processing, includ-ing named entity recognition (Turian et al,2009), word sense disambiguation (Huang etal., 2012), and lexical entailment (Baroni etal., 2012).
In a recent study, Mikolov etal.
(2013) reveal an interesting cross-lingualproperty of distributed word representations:there is a strong similarity between the vec-tor spaces across languages that can be eas-ily captured by linear mapping.
Thus, vectorspace models can also be seen as vectors of(latent) semantic concepts, that preserve their?meaning?
across languages.3 Model and Feature ExtractionIn this section we describe a classification model,and provide details on mono- and cross-lingualimplementation of features.3.1 Classification using Random ForestsTo make classification decisions, we use a randomforest classifier (Breiman, 2001), an ensemble ofdecision tree classifiers learned from many inde-pendent subsamples of the training data.
Givenan input, each tree classifier assigns a probabil-ity to each label; those probabilities are averagedto compute the probability distribution across theensemble.
Random forest ensembles are partic-ularly suitable for our resource-scarce scenario:rather than overfitting, they produce a limitingvalue of the generalization error as the numberof trees increases,8and no hyperparameter tuningis required.
In addition, decision-tree classifierslearn non-linear responses to inputs and often out-perform logistic regression (Perlich et al, 2003).9Our random forest classifier models the probabil-ity that the input syntactic relation is metaphorical.If this probability is above a threshold, the relationis classified as metaphoric, otherwise it is literal.We used the scikit-learn toolkit to train ourclassifiers (Pedregosa et al, 2011).3.2 Feature extractionAbstractness and imageability.
The MRC psy-cholinguistic database is a large dictionary listinglinguistic and psycholinguistic attributes obtainedexperimentally (Wilson, 1988).10It includes,among other data, 4,295 words rated by the de-grees of abstractness and 1,156 words rated by theimageability.
Similarly to Tsvetkov et al (2013),we use a logistic regression classifier to propagateabstractness and imageability scores from MRCratings to all words for which we have vector spacerepresentations.
More specifically, we calculatethe degree of abstractness and imageability of allEnglish items that have a vector space representa-tion, using vector elements as features.
We traintwo separate classifiers for abstractness and im-ageability on a seed set of words from the MRCdatabase.
Degrees of abstractness and imageabil-ity are posterior probabilities of classifier predic-tions.
We binarize these posteriors into abstract-concrete (or imageable-unimageable) boolean in-dicators using pre-defined thresholds.11Perfor-8See Theorem 1.2 in (Breiman, 2001) for details.9In our experiments, random forests model slightly out-performed logistic regression and SVM classifiers.10http://ota.oucs.ox.ac.uk/headers/1054.xml11Thresholds are equal to 0.8 for abstractness and to 0.9for imageability.
They were chosen empirically based on ac-250mance of these classifiers, tested on a sampledheld-out data, is 0.94 and 0.85 for the abstractnessand imageability classifiers, respectively.Supersenses.
In the case of SVO relations, weincorporate supersense features for nouns andverbs; noun and adjective supersenses are used inthe case of AN relations.Supersenses of nouns and verbs.
A lexical itemcan belong to several synsets, which are associ-ated with different supersenses.
Degrees of mem-bership in different supersenses are representedby feature vectors, where each element corre-sponds to one supersense.
For example, the wordhead (when used as a noun) participates in 33synsets, three of which are related to the super-sense noun.body.
The value of the feature corre-sponding to this supersense is 3/33 ?
0.09.Supersenses of adjectives.
WordNet lackscoarse-grained semantic categories for adjectives.To divide adjectives into groups, Tsvetkov et al(2014) use 13 top-level classes from the adaptedtaxonomy of Hundsnurscher and Splett (1982),which is incorporated in GermaNet (Hamp andFeldweg, 1997).
For example, the top-levelclasses in GermaNet include: adj.feeling (e.g.,willing, pleasant, cheerful); adj.substance (e.g.,dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-gantic).12For each adjective type in WordNet,they produce a vector with a classifier posteriorprobabilities corresponding to degrees of mem-bership of this word in one of the 13 semanticclasses,13similar to the feature vectors we buildfor nouns and verbs.
For example, for a wordcalm the top-2 categories (with the first and secondhighest degrees of membership) are adj.behaviorand adj.feeling.Vector space word representations.
We em-ploy 64-dimensional vector-space word represen-tations constructed by Faruqui and Dyer (2014).14Vector construction algorithm is a variation ontraditional latent semantic analysis (Deerwesteret al, 1990) that uses multilingual informationto produce representations in which synonymouswords have similar vectors.
The vectors werecuracy during cross-validation.12For the full taxonomy see http://www.sfs.uni-tuebingen.de/lsd/adjectives.shtml13http://www.cs.cmu.edu/?ytsvetko/adj-supersenses.tar.gz14http://www.cs.cmu.edu/?mfaruqui/soft.htmltrained on the news commentary corpus releasedby WMT-2011,15comprising 180,834 types.3.3 Cross-lingual feature projectionFor languages other than English, feature vectorsare projected to English features using translationdictionaries.
We used the Babylon dictionary,16which is a proprietary resource, but any bilingualdictionary can in principle be used.
For a non-English word in a source language, we first ob-tain all translations into English.
Then, we av-erage all feature vectors related to these transla-tions.
Consider an example related to projectionof WordNet supersenses.
A Russian word ?????
?is translated as head and brain.
Hence, we selectall the synsets of the nouns head and brain.
Thereare 38 such synsets (33 for head and 5 for brain).Four of these synsets are associated with the su-persense noun.body.
Therefore, the value of thefeature noun.body is 4/38 ?
0.11.4 DatasetsIn this section we describe a training and testingdataset as well a data collection procedure.4.1 English training setsTo train an SVO metaphor classifier, we employthe TroFi (Trope Finder) dataset.17TroFi includes3,737 manually annotated English sentences fromthe Wall Street Journal (Birke and Sarkar, 2007).Each sentence contains either literal or metaphori-cal use for one of 50 English verbs.
First, we use adependency parser (Martins et al, 2010) to extractsubject-verb-object (SVO) relations.
Then, we fil-ter extracted relations to eliminate parsing-relatederrors, and relations with verbs which are not inthe TroFi verb list.
After filtering, there are 953metaphorical and 656 literal SVO relations whichwe use as a training set.In the case of AN relations, we construct andmake publicly available a training set contain-ing 884 metaphorical AN pairs and 884 pairswith literal meaning.
It was collected by twoannotators using public resources (collections ofmetaphors on the web).
At least one additionalperson carefully examined and culled the col-lected metaphors, by removing duplicates, weakmetaphors, and metaphorical phrases (such as15http://www.statmt.org/wmt11/16http://www.babylon.com17http://www.cs.sfu.ca/?anoop/students/jbirke/251drowning students) whose interpretation dependson the context.4.2 Multilingual test setsWe collect and annotate metaphoric and literal testsentences in four languages.
Thus, we compileeight test datasets, four for SVO relations, andfour for AN relations.
Each dataset has an equalnumber of metaphors and non-metaphors, i.e., thedatasets are balanced.
English (EN) and Russian(RU) datasets have been compiled by our teamand are publicly available.
Spanish (ES) and Farsi(FA) datasets are published elsewhere (Levin et al,2014).
Table 1 lists test set sizes.SVO ANEN 222 200RU 240 200ES 220 120FA 44 320Table 1: Sizes of the eight test sets.
Each dataset isbalanced, i.e., it has an equal number of metaphorsand non-metaphors.
For example, English SVOdataset has 222 relations: 111 metaphoric and 111literal.We used the following procedure to compile theEN and RU test sets.
A moderator started with seedlists of 1000 most common verbs and adjectives.18Then she used the SketchEngine, which pro-vides searching capability for the TenTen Web cor-pus,19to extract sentences with words that fre-quently co-occurred with words from the seedlists.
From these sentences, she removed sen-tences that contained more than one metaphor, andsentences with non-SVO and non-AN metaphors.Remaining sentences were annotated by severalnative speakers (five for English and six for Rus-sian), who judged AN and SVO phrases in con-text.
The annotation instructions were general:?Please, mark in bold all words that, in your opin-ion, are used non-literally in the following sen-tences.
In many sentences, all the words may beused literally.?
The Fleiss?
Kappas for 5 Englishand 6 Russian annotators are: EN-AN = .76, RU-18Selection of 1000 most common verbs and adjectivesachieves much broader lexical and domain coverage thanwhat can be realistically obtained from continuous text.
Ourtest sentence domains are, therefore, diverse: economic, po-litical, sports, etc.19http://trac.sketchengine.co.uk/wiki/Corpora/enTenTenAN = .85, EN-SVO = .75, RU-SVO = .78.
For the fi-nal selection, we filtered out low-agreement (<.8)sentences.The test candidate sentences were selected bya person who did not participate in the selectionof the training samples.
No English annotators ofthe test set, and only one Russian annotator outof 6 participated in the selection of the trainingsamples.
Thus, we trust that annotator judgmentswere not biased towards the cases that the systemis trained to process.5 Experiments5.1 English experimentsOur task, as defined in Section 2, is to classifySVO and AN relations as either metaphoric or lit-eral.
We first conduct a 10-fold cross-validationexperiment on the training set defined in Section4.1.
We represent each candidate relation usingthe features described in Section 3.2, and evalu-ate performance of the three feature categories andtheir combinations.
This is done by computing anaccuracy in the 10-fold cross validation.
Experi-mental results are given in Table 2, where we alsoprovide the number of features in each feature set.SVO AN# FEAT ACC # FEAT ACCAbsImg 20 0.73?16 0.76?Supersense 67 0.77?116 0.79?AbsImg+Sup.
87 0.78?132 0.80?VSM 192 0.81 228 0.84?All 279 0.82 360 0.86Table 2: 10-fold cross validation results for threefeature categories and their combination, for clas-sifiers trained on English SVO and AN trainingsets.
# FEAT column shows a number of features.ACC column reports an accuracy score in the 10-fold cross validation.
Statistically significant dif-ferences (p < 0.01) from the all-feature combina-tion are marked with a star.These results show superior performance overprevious state-of-the-art results, confirming ourhypothesis that conceptual features are effectivein metaphor classification.
For the SVO task, thecross-validation accuracy is about 10% better thanthat of Tsvetkov et al (2013).
For the AN task,the cross validation accuracy is better by 8% thanthe result of Turney et al (2011) (two baseline252methods are described in Section 5.2).
We cansee that all types of features have good perfor-mance on their own (VSM is the strongest featuretype).
Noun supersense features alone allows us toachieve an accuracy of 75%, i.e., adjective super-sense features contribute 4% to adjective-noun su-persense feature combination.
Experiments withthe pairs of features yield better results than in-dividual features, implying that the feature cate-gories are not redundant.
Yet, combining all fea-tures leads to even higher accuracy during cross-validation.
In the case of the AN task, a differencebetween the All feature combination and any othercombination of features listed in Table 2 is statis-tically significant (p < 0.01 for both the sign andthe permutation test).Although the first experiment shows very highscores, the 10-fold cross-validation cannot fullyreflect the generality of the model, because allfolds are parts of the same corpus.
They are col-lected by the same human judges and belong to thesame domain.
Therefore, experiments on out-of-domain data are crucial.
We carry out such exper-iments using held-out SVO and AN EN test sets,described in Section 4.2 and Table 1.
In this ex-periment, we measure the f -score.
We classifySVO and AN relations using a classifier trained onthe All feature combination and balanced thresh-olds.
The values of the f -score are 0.76, both forSVO and AN tasks.
This out-of-domain experi-ment suggests that our classifier is portable acrossdomains and genres.However, (1) different application may havedifferent requirements for recall/precision, and (2)classification results may be skewed towards hav-ing high precision and low recall (or vice versa).
Itis possible to trade precision for recall by choos-ing a different threshold.
Thus, in addition togiving a single f -score value for balanced thresh-olds, we present a Receiver Operator Characteris-tic (ROC) curve, where we plot a fraction of truepositives against the fraction of false positives for100 threshold values in the range from zero to one.The area under the ROC curve (AUC) can be in-terpreted as the probability that a classifier will as-sign a higher score to a randomly chosen positiveexample than to a randomly chosen negative ex-ample.20For a randomly guessing classifier, theROC curve is a dashed diagonal line.
A bad classi-20Assuming that positive examples are labeled by ones,and negative examples are labeled by zeros.fier has an ROC curve that goes close to the dasheddiagonal or even below it.0.0 0.2 0.4 0.6 0.8 1.0False Positive Rate0.00.20.40.60.81.0TruePositiveRateSupersenses (area = 0.77)AbsImg (area = 0.73)VSM (area = 0.8)All (area = 0.79)(a) SVO0.0 0.2 0.4 0.6 0.8 1.0False Positive Rate0.00.20.40.60.81.0TruePositiveRateAbsImg (area = 0.9)Supersenses (area = 0.86)VSM (area = 0.89)All (area = 0.92)(b) ANFigure 1: ROC curves for classifiers trained usingdifferent feature sets (English SVO and AN testsets).According to ROC plots in Figure 1, all threefeature sets are effective, both for SVO and forAN tasks.
Abstractness and Imageability featureswork better for adjectives and nouns, which is inline with previous findings (Turney et al, 2011;Broadwell et al, 2013).
It can be also seen thatVSM features are very effective.
This is in linewith results of Hovy et al (2013), who found thatit is hard to improve over the classifier that usesonly VSM features.5.2 Comparison to baselinesIn this section, we compare our method to state-of-the-art methods of Tsvetkov et al (2013) and ofTurney et al (2011), who focused on classifyingSVO and AN relations, respectively.In the case of SVO relations, we use software253and datasets from Tsvetkov et al (2013).
Thesedatasets, denoted as an SVO-baseline, consist of98 English and 149 Russian sentences.
We trainSVO metaphor detection tools on SVO relationsextracted from TroFi sentences and evaluate themon the SVO-baseline dataset.
We also use the samethresholds for classifier posterior probabilities asTsvetkov et al (2013).
Our approach is differentfrom that of Tsvetkov et al (2013) in that it usesadditional features (vector space word representa-tions) and a different classification method (we userandom forests while Tsvetkov et al (2013) uselogistic regression).
According to Table 3, we ob-tain higher performance scores for both Russianand English.EN RUSVO-baseline 0.78 0.76This work 0.86 0.85Table 3: Comparing f -scores of our SVOmetaphor detection method to the baselines.In the case of AN relations, we use the dataset(denoted as an AN-baseline) created by Turneyet al (2011) (see Section 4.1 in the referred pa-per for details).
Turney et al (2011) manu-ally annotated 100 pairs where an adjective wasone of the following: dark, deep, hard, sweet,and worm.
The pairs were presented to fivehuman judges who rated each pair on a scalefrom 1 (very literal/denotative) to 4 (very non-literal/connotative).
Turney et al (2011) trainlogistic-regression employing only abstractnessratings as features.
Performance of the methodwas evaluated using the 10-fold cross-validationseparately for each judge.We replicate the above described evaluationprocedure of Turney et al (2011) using theirmodel and features.
In our classifier, we use theAll feature combination and the balanced thresh-old as described in Section 5.1.According to results in Table 4, almost all of thejudge-specific f -scores are slightly higher for oursystem, as well as the overall average f -score.In both baseline comparisons, we obtain perfor-mance at least as good as in previously publishedstudies.5.3 Cross-lingual experimentsIn the next experiment we corroborate the mainhypothesis of this paper: a model trained on En-AN-baseline This workJudge 1 0.73 0.75Judge 2 0.81 0.84Judge 3 0.84 0.88Judge 4 0.79 0.81Judge 5 0.78 0.77average 0.79 0.81Table 4: Comparing AN metaphor detectionmethod to the baselines: accuracy of the 10-fold cross validation on annotations of five humanjudges.glish data can be successfully applied to otherlanguages.
Namely, we use a trained Englishmodel discussed in Section 5.1 to classify literaland metaphoric SVO and AN relations in English,Spanish, Farsi and Russian test sets, listed in Sec-tion 4.2.
This time we used all available features.Experimental results for all four languages, aregiven in Figure 2.
The ROC curves for SVO andAN tasks are plotted in Figure 2a and Figure 2b,respectively.
Each curve corresponds to a test setdescribed in Table 1.
In addition, we perform anoracle experiment, to obtain actual f -score valuesfor best thresholds.
Detailed results are shown inTable 5.Consistent results with high f -scores are ob-tained across all four languages.
Note that higherscores are obtained for the Russian test set.
We hy-pothesize that this happens due to a higher-qualitytranslation dictionary (which allows a more accu-rate model transfer).
Relatively lower (yet rea-sonable) results for Farsi can be explained by asmaller size of the bilingual dictionary (thus, fewerfeature projections can be obtained).
Also notethat, in our experience, most of Farsi metaphorsare adjective-noun constructions.
This is why theAN FA dataset in Table 1 is significantly largerthan SVO FA.
In that, for the AN Farsi task weobserve high performance scores.Figure 2 and Table 5 confirm, that we ob-tain similar, robust results on four very differ-ent languages, using the same English classi-fiers.
We view this result as a strong evidence oflanguage-independent nature of our metaphor de-tection method.
In particular, this shows that pro-posed conceptual features can be used to detect se-lectional preferences violation across languages.To summarize the experimental section, ourmetaphor detection approach obtains state-of-the-2540.0 0.2 0.4 0.6 0.8 1.0False Positive Rate0.00.20.40.60.81.0TruePositiveRateEN (area = 0.79)ES (area = 0.71)FA (area = 0.69)RU (area = 0.89)(a) SVO0.0 0.2 0.4 0.6 0.8 1.0False Positive Rate0.00.20.40.60.81.0TruePositiveRateEN (area = 0.92)ES (area = 0.73)FA (area = 0.83)RU (area = 0.8)(b) ANFigure 2: Cross-lingual experiment: ROC curvesfor classifiers trained on the English data using acombination of all features, and applied to SVOand AN metaphoric and literal relations in four testlanguages: English, Russian, Spanish, and Farsi.art performance in English, is effective when ap-plied to out-of-domain English data, and workscross-lingually.5.4 ExamplesManual data analysis on adjective-noun pairs sup-ports an abstractness-concreteness hypothesis for-mulated by several independent research studies.For example, in English we classify as metaphoricdirty word and cloudy future.
Word pairs dirtydiaper and cloudy weather have same adjectives.Yet they are classified as literal.
Indeed, diaperis a more concrete term than word and weatheris more concrete than future.
Same pattern is ob-served in non-English datasets.
In Russian, ????-???
????????
?sick society?
and ??????
????
?empty sound?
are classified as metaphoric, whileSVO ANEN 0.79 0.85RU 0.84 0.77ES 0.76 0.72FA 0.75 0.74Table 5: Cross-lingual experiment: f -scores forclassifiers trained on the English data using a com-bination of all features, and applied, with optimalthresholds, to SVO and AN metaphoric and literalrelations in four test languages: English, Russian,Spanish, and Farsi.???????
???????
?sick grandmother?
and ??-????
?????
?empty cup?
are classified as literal.Spanish example of an adjective-noun metaphoris a well-known m?usculo econ?omico ?economicmuscle?.
We also observe that non-metaphoric ad-jective noun pairs tend to have more imageable ad-jectives, such as literal derecho humano ?humanright?.
In Spanish, human is more imageable thaneconomic.Verb-based examples that are correctly clas-sified by our model are: blunder escaped no-tice (metaphoric) and prisoner escaped jail (lit-eral).
We hypothesize that supersense features areinstrumental in the correct classification of theseexamples: <noun.person,verb.motion> is usuallyused literally, while <noun.act,verb.motion> isused metaphorically.6 Related WorkFor a historic overview and a survey ofcommon approaches to metaphor detection,we refer the reader to recent reviews byShutova et al (Shutova, 2010; Shutova et al,2013).
Here we focus only on recent approaches.Shutova et al (2010) proposed a bottom-upmethod: one starts from a set of seed metaphorsand seeks phrases where verbs and/or nouns be-long to the same cluster as verbs or nouns in seedexamples.Turney et al (2011) show how abstractnessscores could be used to detect metaphorical ANphrases.
Neuman et al (2013) describe a ConcreteCategory Overlap algorithm, where co-occurrencestatistics and Turney?s abstractness scores are usedto determine WordNet supersenses that corre-spond to literal usage of a given adjective or verb.For example, given an adjective, we can learn thatit modifies concrete nouns that usually have the255supersense noun.body.
If this adjective modifiesa noun with the supersense noun.feeling, we con-clude that a metaphor is found.Broadwell et al (2013) argue that metaphorsare highly imageable words that do not belongto a discussion topic.
To implement this idea,they extend MRC imageability scores to all dic-tionary words using links among WordNet super-senses (mostly hypernym and hyponym relations).Strzalkowski et al (2013) carry out experimentsin a specific (government-related) domain for fourlanguages: English, Spanish, Farsi, and Russian.Strzalkowski et al (2013) explain the algorithmonly for English and say that is the same for Span-ish, Farsi, and Russian.
Because they heavilyrely on WordNet and availability of imageabilityscores, their approach may not be applicable tolow-resource languages.Hovy et al (2013) applied tree kernels tometaphor detection.
Their method also employsWordNet supersenses, but it is not clear from thedescription whether WordNet is essential or canbe replaced with some other lexical resource.
Wecannot compare directly our model with this workbecause our classifier is restricted to detection ofonly SVO and AN metaphors.Tsvetkov et al (2013) propose a cross-lingualdetection method that uses only English lexical re-sources and a dependency parser.
Their study fo-cuses only on the verb-based metaphors.
Tsvetkovet al (2013) employ only English and Russiandata.
Current work builds on this study, and incor-porates new syntactic relations as metaphor candi-dates, adds several new feature sets and different,more reliable datasets for evaluating results.
Wedemonstrate results on two new languages, Span-ish and Farsi, to emphasize the generality of themethod.A words sense disambiguation (WSD) is a re-lated problem, where one identifies meanings ofpolysemous words.
The difference is that in theWSD task, we need to select an already existingsense, while for the metaphor detection, the goalis to identify cases of sense borrowing.
Studiesshowed that cross-lingual evidence allows one toachieve a state-of-the-art performance in the WSDtask, yet, most cross-lingual WSD methods em-ploy parallel corpora (Navigli, 2009).7 ConclusionThe key contribution of our work is that we showhow to identify metaphors across languages bybuilding a model in English and applying it?without adaptation?to other languages: Spanish,Farsi, and Russian.
This model uses language-independent (rather than lexical or language spe-cific) conceptual features.
Not only do we estab-lish benchmarks for Spanish, Farsi, and Russian,but we also achieve state-of-the-art performancein English.
In addition, we present a comparisonof relative contributions of several types of fea-tures.
We concentrate on metaphors in the con-text of two kinds of syntactic relations: subject-verb-object (SVO) relations and adjective-noun(AN) relations, which account for a majority of allmetaphorical phrases.Future work will expand the scope of metaphoridentification by including nominal metaphoric re-lations as well as explore techniques for incor-porating contextual features, which can play akey role in identifying certain kinds of metaphors.Second, cross-lingual model transfer can be im-proved with more careful cross-lingual featureprojection.AcknowledgmentsWe are extremely grateful to Shuly Wintner for athorough review that helped us improve this draft;we also thank people who helped in creating thedatasets and/or provided valuable feedback on thiswork: Ed Hovy, Vlad Niculae, Davida Fromm,Brian MacWhinney, Carlos Ram?
?rez, and othermembers of the CMU METAL team.
This workwas supported by the U.S. Army Research Labo-ratory and the U.S. Army Research Office undercontract/grant number W911NF-10-1-0533.ReferencesMarco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,and Chung-chieh Shan.
2012.
Entailment above theword level in distributional semantics.
In Proc.
ofEACL, pages 23?32.Julia Birke and Anoop Sarkar.
2007.
Active learningfor the identification of nonliteral language.
In Proc.of the Workshop on Computational Approaches toFigurative Language, FigLanguages ?07, pages 21?28.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.256George Aaron Broadwell, Umit Boz, Ignacio Cases,Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.2013.
Using imageability and topic chaining to lo-cate metaphors in linguistic corpora.
In Social Com-puting, Behavioral-Cultural Modeling and Predic-tion, pages 102?110.
Springer.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.
InProc.
of EMNLP, pages 594?602.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proc.
of EACL.
Association for Com-putational Linguistics.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Language, Speech andCommunication.
MIT Press.Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder,Newton Howard, Sergey Kanareykin, Moshe Kop-pel, Mark Last, Yair Neuman, and Shlomo Arga-mon.
2013.
Automatic identification of conceptualmetaphors with limited knowledge.
In Proc.
of theTwenty-Seventh AAAI Conference on Artificial Intel-ligence, pages 328?334.Matt Gedigian, John Bryant, Srini Narayanan, and Bra-nimir Ciric.
2006.
Catching metaphors.
In Pro-ceedings of the 3rd Workshop on Scalable NaturalLanguage Understanding, pages 41?48.Birgit Hamp and Helmut Feldweg.
1997.
Germanet-a lexical-semantic net for German.
In Proc.
ofACL workshop Automatic Information Extractionand Building of Lexical Semantic Resources for NLPApplications, pages 9?15.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
In Proc.
ofthe First Workshop on Metaphor in NLP, page 52.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proc.
of ACL, pages 873?882.Franz Hundsnurscher and Jochen Splett.
1982.
Se-mantik der Adjektive des Deutschen.
Number 3137.Westdeutscher Verlag.Mikhail Kozhenikov and Ivan Titov.
2013.
Cross-lingual transfer of semantic role labeling models.
InProc.
of ACL, pages 1190?1200.Saisuresh Krishnakumaran and Xiaojin Zhu.
2007.Hunting elusive metaphors using lexical resources.In Proc.
of the Workshop on Computational ap-proaches to Figurative Language, pages 13?20.George Lakoff and Mark Johnson.
1980.
Conceptualmetaphor in everyday language.
The Journal of Phi-losophy, pages 453?486.Lori Levin, Teruko Mitamura, Davida Fromm, BrianMacWhinney, Jaime Carbonell, Weston Feely,Robert Frederking, Anatole Gershman, and CarlosRamirez.
2014.
Resources for the detection of con-ventionalized metaphors in four languages.
In Proc.of LREC.Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-dro M. Q. Aguiar, and M?ario A. T. Figueiredo.
2010.Turbo parsers: dependency parsing by approximatevariational inference.
In Proc.
of ENMLP, pages 34?44.Zachary J Mason.
2004.
CorMet: a computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proc.
of EMNLP.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for Ma-chine Translation.
CoRR, abs/1309.4168.Roberto Navigli.
2009.
Word sense disambiguation:A survey.
ACM Comput.
Surv., 41(2):10:1?10:69,February.Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,Shlomo Argamon, Newton Howard, and OphirFrieder.
2013.
Metaphor identification in large textscorpora.
PloS one, 8(4):e62343.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Claudia Perlich, Foster Provost, and Jeffrey S. Si-monoff.
2003.
Tree induction vs. logistic regres-sion: a learning-curve analysis.
Journal of MachineLearning Research, 4:211?255.Nathan Schneider, Behrang Mohit, Chris Dyer, KemalOflazer, and Noah A Smith.
2013.
Supersense tag-ging for Arabic: the MT-in-the-middle attack.
InProc.
of NAACL-HLT, pages 661?667.Ekaterina Shutova and Lin Sun.
2013.
Unsupervisedmetaphor identification using hierarchical graph fac-torization clustering.
In Proc.
of NAACL-HLT,pages 978?988.257Ekaterina Shutova and Simone Teufel.
2010.Metaphor corpus annotated for source-target domainmappings.
In Proc.
of LREC, pages 3255?3261.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proc.
of COLING, pages 1002?1010.Ekaterina Shutova, Simone Teufel, and Anna Korho-nen.
2013.
Statistical metaphor processing.
Com-putational Linguistics, 39(2):301?353.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proc.
of ACL, pages 688?697.Gerard J Steen, Aletta G Dorst, J Berenike Her-rmann, Anna A Kaal, and Tina Krennmayr.2010.
Metaphor in usage.
Cognitive Linguistics,21(4):765?796.Tomek Strzalkowski, George Aaron Broadwell, SarahTaylor, Laurie Feldman, Boris Yamrom, SamiraShaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,et al 2013.
Robust extraction of metaphors fromnovel data.
In Proc.
of the First Workshop onMetaphor in NLP, page 67.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
TACL, 1:1?12.Paul H Thibodeau and Lera Boroditsky.
2011.Metaphors we think with: The role of metaphor inreasoning.
PLoS One, 6(2):e16782.Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-man.
2013.
Cross-lingual metaphor detection usingcommon semantic features.
In The 1st Workshop onMetaphor in NLP 2013, page 45.Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, ArchnaBhatia, Manaal Faruqui, and Chris Dyer.
2014.Augmenting English adjective senses with super-senses.
In Proc.
of LREC.Joseph Turian, Lev Ratinov, Yoshua Bengio, and DanRoth.
2009.
A preliminary evaluation of word rep-resentations for named-entity recognition.
In NIPSWorkshop on Grammar Induction, Representation ofLanguage and Language Learning, pages 1?8.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proc.
of ACL, pages384?394.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProc.
of EMNL, pages 680?690.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11(3):197?223.Michael Wilson.
1988.
MRC PsycholinguisticDatabase: Machine-usable dictionary, version 2.00.Behavior Research Methods, Instruments, & Com-puters, 20(1):6?10.258
