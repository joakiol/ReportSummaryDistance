Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 22?27,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomatic detection of plagiarized spoken responsesKeelan Evanini and Xinhao WangEducational Testing Service660 Rosedale Road, Princeton, NJ, USA{kevanini,xwang002}@ets.orgAbstractThis paper addresses the task of auto-matically detecting plagiarized responsesin the context of a test of spoken En-glish proficiency for non-native speakers.A corpus of spoken responses containingplagiarized content was collected from ahigh-stakes assessment of English profi-ciency for non-native speakers, and sev-eral text-to-text similarity metrics wereimplemented to compare these responsesto a set of materials that were identifiedas likely sources for the plagiarized con-tent.
Finally, a classifier was trained usingthese similarity metrics to predict whethera given spoken response is plagiarized ornot.
The classifier was evaluated on adata set containing the responses with pla-giarized content and non-plagiarized con-trol responses and achieved accuracies of92.0% using transcriptions and 87.1% us-ing ASR output (with a baseline accuracyof 50.0%).1 IntroductionThe automated detection of plagiarism has beenwidely studied in the domain of written studentessays, and several online services exist for thispurpose.1In addition, there has been a series ofshared tasks using common data sets of writtenlanguage to compare the performance of a vari-ety of approaches to plagiarism detection (Potthastet al., 2013).
In contrast, the automated detectionof plagiarized spoken responses has received littleattention from both the NLP and assessment com-munities, mostly due to the limited application of1For example, http://turnitin.com/en_us/features/originalitycheck, http://www.grammarly.com/plagiarism-checker/,and http://www.paperrater.com/plagiarism_checker.automated speech scoring for the types of spo-ken responses that could be affected by plagiarism.Due to a variety of factors, though, this is likely tochange in the near future, and the automated detec-tion of plagiarism in spoken language will becomean increasingly important application.First of all, English continues its spread as theglobal language of education and commerce, andthere is a need to assess the communicative com-pentance of high volumes of highly proficient non-native speakers.
In order to provide a valid evalua-tion of the complex linguistic skills that are nec-essary for these speakers, the assessment mustcontain test items that elicit spontaneous speech,such as the Independent and Integrated Speakingitems in the TOEFL iBT test (ETS, 2012), theRetell Lecture item in the Pearson Test of EnglishAcademic (Longman, 2010), and the oral inter-view in the IELTS Academic assessment (Cullenet al., 2014).
However, with the increased em-phasis on complex linguistic skills in assessmentsof non-native speech, there is an increased chancethat test takers will prepare canned answers usingtest preparation materials prior to the examination.Therefore, research should also be conducted ondetecting spoken plagiarized responses in order toprevent this type of cheating strategy.In addition, there will also likely be an increasein spoken language assessments for native speak-ers in the K-12 domain in the near future.
Curricu-lum developers and assessment designers are rec-ognizing that the assessment of spoken commu-nication skills is important for determining a stu-dent?s college readiness.
For example, the Com-mon Core State Standards include Speaking &Listening English Language Arts standards foreach grade that pertain to a student?s ability tocommunicate information and ideas using spokenlanguage.2In order to assess these standards, it2http://www.corestandards.org/ELA-Literacy/SL/22will be necessary to develop standardized assess-ments for the K-12 domain that contain items elic-iting spontaneous speech from the student, such aspresentations, group discussions, etc.
Again, withthe introduction of these types of tasks, there is arisk that a test taker?s spoken response will containprepared material drawn from an external source,and there will be a need to automatically detectthis type of plagiarism on a large scale, in order toprovide fair and valid assessments.In this paper, we present an initial study of au-tomated plagiarism detection on spoken responsescontaining spontaneous non-native speech.
A dataset of actual plagiarized responses was collected,and text-to-text similarity metrics were applied tothe task of classifying responses as plagiarized ornon-plagiarized.2 Previous WorkA wide variety of techniques have been employedin previous studies for the task of detecting plagia-rized written documents, including n-gram over-lap (Lyon et al., 2006), document fingerprinting(Brin et al., 1995), word frequency statistics (Shiv-akumar and Garcia-Molina, 1995), InformationRetrieval-based metrics (Hoad and Zobel, 2003),text summarization evaluation metrics (Chen etal., 2010), WordNet-based features (Nahnsen etal., 2005), and features based on shared syntacticpatterns (Uzuner et al., 2005).
This task is alsorelated to the widely studied task of paraphraserecognition, which benefits from similar types offeatures (Finch et al., 2005; Madnani et al., 2012).The current study adopts several of these featuresthat are designed to be robust to the presence ofword-level modifications between the source andthe plagiarized text; since this study focuses onspoken responses that are reproduced from mem-ory and subsequently processed by a speech recog-nizer, metrics that rely on exact matches are likelyto perform sub-optimally.
To our knowledge, noprevious work has been reported on automaticallydetecting similar spoken documents, although re-search in the field of Spoken Document Retrieval(Haputmann, 2006) is relevant.Due to the difficulties involved in collecting cor-pora of actual plagiarized material, nearly all pub-lished results of approaches to the task of plagia-rism detection have relied on either simulated pla-giarism (i.e., plagiarized texts generated by experi-mental human participants in a controlled environ-ment) or artificial plagiarism (i.e., plagiarized textsgenerated by algorithmically modifying a sourcetext) (Potthast et al., 2010).
These results, how-ever, may not reflect actual performance in a de-ployed setting, since the characteristics of the pla-giarized material may differ from actual plagia-rized responses.
To overcome this limitation, thecurrent study is based on a set of actual plagiarizedresponses drawn from a large-scale assessment.3 DataThe data used in this study was drawn from theTOEFLR?Internet-based test (TOEFLR?iBT), alarge-scale, high-stakes assessment of English fornon-native speakers, which assesses English com-munication skills for academic purposes.
TheSpeaking section of TOEFL iBT contains sixtasks, each of which requires the test taker to pro-vide an extended response containing spontaneousspeech.
Two of the tasks are referred to as In-dependent tasks; these tasks cover topics that arefamiliar to test takers and ask test takers to drawupon their own ideas, opinions, and experiences ina 45-second spoken response (ETS, 2012).
Sincethese two Independent tasks ask questions that arenot based on any stimulus materials that were pro-vided to the test taker (such as a reading passage,figure, etc.
), the test takers can provide responsesthat contain a wide variety of specific examples.In some cases, test takers may attempt to gamethe assessment by memorizing canned materialfrom an external source and adapting it to a ques-tion that is asked in one of the Independent tasks.This type of plagiarism can affect the validity ofa test taker?s speaking score; however, it is oftendifficult even for trained human raters to recog-nize plagiarized spoken responses, due to the largenumber and variety of external sources that areavailable from online test preparation sites.In order to better understand the strategies usedby test takers who incorporated material from ex-ternal sources into their spoken responses and todevelop a capability for automated plagiarism de-tection for speaking items, a data set of opera-tional spoken responses containing potentially pla-giarized material was collected.
This data set con-tains responses that were flagged by human ratersas potentially containing plagiarized material andthen subsequently reviewed by rater supervisors.In the review process, the responses were tran-scribed and compared to external source materi-23als obtained through manual internet searches; ifit was determined that the presence of plagiarizedmaterial made it impossible to provide a valid as-sessment of the test taker?s performance on thetask, the response was assigned a score of 0.
Thisstudy investigates a set of 719 responses that wereflagged as potentially plagiarized between Octo-ber 2010 and December 2011; in this set, 239 re-sponses were assigned a score of 0 due to the pres-ence of a significant amount of plagiarized con-tent from an identified source.
This set of 239 re-sponses is used in the experiments described be-low.During the process of reviewing potentially pla-giarized responses, the raters also collected a dataset of external sources that appeared to have beenused by test takers in their responses.
In somecases, the test taker?s spoken response was nearlyidentical to an identified source; in other cases,several sentences or phrases were clearly drawnfrom a particular source, although some modifi-cations were apparent.
Table 1 presents a samplesource that was identified for several of the 239 re-sponses in the data set.3Many of the plagiarizedresponses contained extended sequences of wordsthat directly match idiosyncratic features of thissource, such as the phrases ?how romantic it canever be?
and ?just relax yourself on the beach.
?In total, 49 different source materials were iden-tified for all of the potentially plagiarized re-sponses in the corpus.4In addition to the sourcematerials and the plagiarized responses, a set ofnon-plagiarized control responses was also ob-tained in order to conduct classification experi-ments between plagiarized and non-plagiarized re-sponses.
Since the plagiarized responses werecollected over the course of more than one year,they were drawn from many different TOEFL iBTtest forms; in total, the 239 plagiarized responsescomprise 103 distinct Independent test questions.Therefore, it was not practical to obtain controldata from all of the test items that were representedin the plagiarized set; rather, approximately 300responses were extracted from each of the four test3This source is available from several online test prepara-tion websites, for example http://www.mhdenglish.com/eoenglish_article_view_1195.html.4A total of 39 sources were identified for the set of 239responses in the Plagiarized set; however, all 49 identifiedsources were used in the experiments in order to make theexperimental design more similar to an operational set-up inwhich the exact set of source texts that will be represented ina given set of plagiarized responses is not known.Well, the place I enjoy the most is a smalltown located in France.
I like this small townbecause it has very charming ocean view.
Imean the sky there is so blue and the beachis always full of sunshine.
You know howromantic it can ever be, just relax yourselfon the beach, when the sun is setting down,when the ocean breeze is blowing and theseabirds are singing.
Of course I like thissmall French town also because there aremany great French restaurants.
They offerthe best seafood in the world like lobsters andtuna fishes.
The most important, I have beenbenefited a lot from this trip to France becauseI made friends with some gorgeous Frenchgirls.
One of them even gave me a little watchas a souvenir of our friendship.Table 1: Sample source passage used in plagia-rized responsesitems that were most frequently represented in theset of plagiarized responses.
Table 2 provides asummary of the three data sets used in the study,along with summary statistics about the length ofthe responses in each set.Data Set NNumber of WordsMean Std.
Dev.Sources 49 122.5 36.5Plagiarized 239 109.1 18.9Control 1196 84.9 24.1Table 2: Summary of the data setsAs Table 2 shows, the plagiarized responsesare on average a little longer than the control re-sponses.
This is likely due to the fact that the pla-giarized responses contain a large percentage ofmemorized material, which the test takers are ableto produce using a fast rate of speech, since theyhad likely rehearsed the content several times be-fore taking the assessment.4 MethodologyThe general approach taken in this study for deter-mining whether a spoken response is plagiarizedor not was to compare its content to the content ofeach of the source materials that had been iden-tified for the responses in this corpus.
Given atest response, a comparison was made with each24of the 49 reference sources using the following 9text-to-text similarity metrics: 1) Word Error Rate(WER), or edit distance between the response andthe source; 2) TER, similar to WER, but allowingshifts of words within the text at a low edit cost(Snover et al., 2006); 3) TER-Plus, an extension ofTER that includes matching based on paraphrases,stemming, and synonym substitution (Snover etal., 2008); 4) a WordNet similarity metric based onpresence in the same synset;55) a WordNet sim-ilarity metric based on the shortest path betweentwo words in the is-a taxonomy; 6) a WordNetsimilarity metric similar to (5) that also takes intoaccount the maximum depth of the taxonomy inwhich the words occur (Leacock and Chodorow,1998); 7) a WordNet similarity metric based on thedepth of the Least Common Subsumer of the twowords (Wu and Palmer, 1994); 8) Latent SemanticAnalysis, using a model trained on the British Na-tional Corpus (BNC, 2007); 9) BLEU (Papineni etal., 2002).
Most of these similarity metrics (withthe exception of WER and TER) are expected tobe robust to modifications between the source textand the plagiarized response, since they do not relyon exact string matches.Each similarity metric was used to compute 4different features comparing the test response toeach of the 49 source texts: 1) the document-levelsimilarity between the test response and the sourcetext; 2) the single maximum similarity value froma sentence-by-sentence comparison between thetest response and the source text; 3) the average ofthe similarity values for all sentence-by-sentencecomparisons between the test response and thesource text; 4) the average of the maximum simi-larity values for each sentence in the test response,where the maximum similarity of a sentence is ob-tained by comparing it with each sentence in thesource text.
The intuition behind using the fea-tures that compare sentence-to-sentence similarityas opposed to only the document-level similarityfeature is that test responses may contain a combi-nation of both passages that were memorized froma source text and novel content.
Depending on theamount of the response that was plagiarized, thesetypes of responses may also receive a score of 0;so, in order to also detect these responses as pla-5For the WordNet-based similarity metrics, the similarityscores for pairs of words were combined to obtain document-and sentence-level similarity scores by taking the averagemaximum pairwise similarity values, similar to the sentence-level similarity feature defined in (4) below.giarized, a sentence-by-sentence comparison ap-proach may be more effective.The experiments described below were con-ducted using both human transcriptions of the spo-ken responses as well as the output from an au-tomated speech recognition (ASR) system.
TheASR system was trained on approximately 800hours of TOEFL iBT responses; the system?sWER on the data used in this study was 0.411 forthe Plagiarized set and 0.362 for the Control set.Since the ASR output does not contain sentenceboundaries, these were obtained using a Maxi-mum Entropy sentence boundary detection systembased on lexical features (Chen and Yoon, 2011).Before calculating the similarity features, all of thetexts were preprocessed to normalize case, seg-ment the text into sentences, and remove disfluen-cies, including filled pauses (such as uh and um)and repeated words.
No stemming was performedon the words in the texts for this study.5 ResultsAs described in Section 4, 36 similarity featureswere calculated between each spoken responseand each of the 49 source texts.
In order to exam-ine the performance of these features in discrim-inating between plagiarized and non-plagiarizedresponses, classification experiments were con-ducted on balanced sets of Plagiarized and Con-trol responses, and the results were averaged using1000 random subsets of 239 responses from theControl set.6In addition, the following differentfeature sets were compared: All (all 36 features),Doc (the 9 document-level features), and Sent (the27 features based on sentence-level comparisons).The J48 decision tree model from the Weka toolkit(with the default parameter settings) was usedfor classification, and 10-fold cross-validation wasperformed using both transcriptions and ASR out-put.
Table 3 presents the results of these experi-ments, including the means (and standard devia-tions) of the accuracy and kappa (?)
values (for allexperiments, the baseline accuracy is 50%).6 Discussion and Future WorkAs Table 3 shows, the classifier achieved a higheraccuracy when using the 9 document-level simi-larity features compared to using the 27 sentence-6Experiments were also conducted using the full Controlset, and the results showed a similar relative performance ofthe feature sets.25Text Features Accuracy ?Trans.All 0.903 (0.01) 0.807 (0.02)Doc 0.920 (0.01) 0.839 (0.02)Sent 0.847 (0.01) 0.693 (0.03)ASRAll 0.852 (0.02) 0.703 (0.03)Doc 0.871 (0.01) 0.742 (0.03)Sent 0.735 (0.02) 0.470 (0.04)Table 3: Mean Accuracy and ?
values (and stan-dard deviations) for classification results using the239 responses in the Plagiarized set and 1000 ran-dom subsets of 239 responses from the Control setlevel similarity features.
In addition, the combinedset of 36 features resulted in a slightly lower per-formance than when only the 9 document-levelfeatures were used.
This suggests that the sentencelevel features are not as robust as the document-level features, probably due to the increased like-lihood of chance similarities between sentences inthe response and a source text.
Despite the factthat the plagiarized spoken responses in this dataset may contain some original content (in particu-lar, introductory material provided by the test takerin an attempt to make the plagiarized content seemmore relevant to the specific test question), it ap-pears that the document-level features are most ef-fective.
Table 3 also indicates that the performanceof the classifier decreases by approximately 5% -10% when ASR output is used.
This indicates thatthe similarity metrics are reasonably robust to thepresence of speech recognition errors in the text,and that the approach is viable in an operationalsetting in which transcriptions of the spoken re-sponses are not available.A more detailed error analysis indicates that theprecision of the classifier, with respect to the Pla-giarized class, is higher than the recall: on thetranscriptions, the average precision using the Docfeatures was 0.948 (s.d.= 0.01), whereas the av-erage recall was 0.888 (s.d.=0.01); for the ASRset, the average precision was 0.904 (s.d.=0.02),whereas the average recall was 0.831 (s.d.=0.02).This means that the rate of false positives pro-duced by this classifier is somewhat lower than therate of false negatives.
In an operational scenario,an automated plagiarized spoken response detec-tion system such as this one would likely be de-ployed in tandem with human raters to review theresults and provide a final decision about whethera given spoken response was plagiarized or not.
Inthat case, it may be desirable to tune the classi-fier parameters to increase the recall so that fewercases of plagiarism would go undetected, assum-ing that there are suffient human reviewers avail-able to process the increased number of false pos-itives that would result from this approach.
Im-proving the classifier?s recall is also important forpractical applications of this approach, since thedistribution of actual responses is heavily imbal-anced in favor of the non-plagiarized class.
Thecurrent set of experiments only used a relativelysmall Control set of 1196 responses for whichtranscriptions could be obtained in a cost effectivemanner in order to be able to compare the system?sperformance using transcriptions and ASR output.Since there was only a minor degradation in per-formance when ASR output was used, future ex-periments will be conducted using a much largerControl set in order to approximate the distributionof categories that would be observed in practice.One drawback of the method described in thisstudy is that it requires matching source texts inorder to detect a plagiarized spoken response.
Thismeans that plagiarized spoken responses based ona given source text will not be detected by thesystem until the appropriate source text has beenidentified, thus limiting the system?s recall.
Be-sides attempting to obtain additional source texts(either manually, as was done for this study, or byautomated means), this could also be addressedby comparing a test response to all previouslycollected spoken responses for a given popula-tion of test takers in order to flag pairs of sim-ilar responses.
While this method would likelyproduce a high number of false positives whenthe ASR output was used, due to chance simi-larities between two responses in a large pool oftest taker responses resulting from imperfect ASR,performance could be improved by consideringadditional information from the speech recognizerwhen computing the similarity metrics, such asthe N-best list.
Additional sources of informa-tion that could be used for detecting plagiarized re-sponses include stylistic patterns and prosodic fea-tures; for example, spoken responses that are re-produced from memory likely contain fewer filledpauses and have a faster rate of speech than non-plagiarized responses; these types of non-lexicalfeatures should also be investigated in future re-search into the detection of plagiarized spoken re-sponses.26AcknowledgmentsWe would like to thank Beata Beigman Klebanov,Dan Blanchard, Nitin Madnani, and three anony-mous BEA-9 reviewers for their helpful com-ments.ReferencesBNC.
2007.
The British National Corpus, version 3.Distributed by Oxford University Computing Ser-vices on behalf of the BNC Consortium, http://www.natcorp.ox.ac.uk/.Sergey Brin, James Davis, and Hector Garcia-Molina.1995.
Copy detection mechanisms for digital docu-ments.
In Proceedings of the ACM SIGMOD AnnualConference, pages 398?409.Lei Chen and Su-Youn Yoon.
2011.
Detectingstructural events for assessing non-native speech.In Proceedings of the 6th Workshop on InnovativeUse of NLP for Building Educational Applications,NAACL-HLT, pages 38?45, Portland, OR.
Associa-tion for Computational Linguistics.Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.2010.
Plagiarism detection using ROUGE andWordNet.
Journal of Computing, 2(3):34?44.Pauline Cullen, Amanda French, and Vanessa Jakeman.2014.
The Official Cambridge Guide to IELTS.Cambridge University Press.ETS.
2012.
The Official Guide to the TOEFLR?Test,Fourth Edition.
McGraw-Hill.Andrew Finch, Young-Sook Hwang, and EiichiroSumita.
2005.
Using machine translation evalua-tion techniques to determine sentence-level seman-tic equivalence.
In Proceedings of the Third Inter-national Workshop on Paraphrasing, pages 17?24.Alexander Haputmann.
2006.
Automatic spoken doc-ument retrieval.
In Ketih Brown, editor, Encylclope-dia of Language and Linguistics (Second Edition),pages 95?103.
Elsevier Science.Timothy C. Hoad and Justin Zobel.
2003.
Methodsfor identifying versioned and plagiarised documents.Journal of the American Society for Information Sci-ence and Technology, 54:203?215.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity forword sense identification.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database,pages 305?332.
MIT Press.Pearson Longman.
2010.
The Official Guide to Pear-son Test of English Academic.
Pearson EducationESL.Caroline Lyon, Ruth Barrett, and James Malcolm.2006.
Plagiarism is easy, but also easy to detect.Plagiary, 1:57?65.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 182?190, Montr?eal, Canada, June.
Association for Com-putational Linguistics.Thade Nahnsen,?Ozlem Uzuner, and Boris Katz.
2005.Lexical chains and sliding locality windows incontent-based text similarity detection.
CSAILTechnical Report, MIT-CSAIL-TR-2005-034.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics.Martin Potthast, Benno Stein, Alberto Barr?on-Cede?no,and Paolo Rosso.
2010.
An evaluation frame-work for plagiarism detection.
In Proceedings of the23rd International Conference on ComputationalLinguistics.Martin Potthast, Matthias Hagen, Tim Gollub, MartinTippmann, Johannes Kiesel, Paolo Rosso, EfstathiosStamatatos, and Benno Stein.
2013.
Overview ofthe 5th International Competition on Plagiarism De-tection.
In Pamela Forner, Roberto Navigli, andDan Tufis, editors, CLEF 2013 Evaluation Labs andWorkshop ?
Working Notes Papers.Narayanan Shivakumar and Hector Garcia-Molina.1995.
SCAM: A copy detection mechanism for digi-tal documents.
In Proceedings of the Second AnnualConference on the Theory and Practice of DigitalLibraries.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas, pages 223?231.Matt Snover, Nitin Madnani, Bonnie Dorr, and RichardSchwartz.
2008.
TERp: A system descrip-tion.
In Proceedings of the First NIST Metricsfor Machine Translation Challenge (MetricsMATR),Waikiki, Hawaii, October.
?Ozlem Uzuner, Boris Katz, and Thade Nahnsen.
2005.Using syntactic information to identify plagiarism.In Proceedings of the 2nd Workshop on Building Ed-ucational Applications using NLP.
Ann Arbor.Zhibiao Wu and Martha Palmer.
1994.
Verb semanticsand lexical selection.
In Proceedings of the 32ndAnnual Mmeeting of the Association for Computa-tional Linguistics (ACL).27
