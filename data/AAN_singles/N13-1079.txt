Proceedings of NAACL-HLT 2013, pages 680?684,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsUnsupervised Domain Tuning to Improve Word Sense DisambiguationJudita Preiss and Mark Stevensonj.preiss@sheffield.ac.uk and m.stevenson@dcs.shef.ac.ukDepartment of Computer Science, University of Sheffield211 Portobello, Sheffield, S1 4DP, UKAbstractThe topic of a document can prove to be use-ful information for Word Sense Disambigua-tion (WSD) since certain meanings tend to beassociated with particular topics.
This paperpresents an LDA-based approach for WSD,which is trained using any available WSD sys-tem to establish a sense per (Latent Dirich-let alcation based) topic.
The technique istested using three unsupervised and one su-pervised WSD algorithms within the SPORTand FINANCE domains giving a performanceincrease each time, suggesting that the tech-nique may be useful to improve the perfor-mance of any available WSD system.1 IntroductionAssigning each word its most frequent sense (MFS)is commonly used as a baseline in Word Sense Dis-ambiguation (WSD).
This baseline can be difficult tobeat, particularly for unsupervised systems which donot have access to the annotated training data used todetermine the MFS.
However, it has also been shownthat unsupervised methods can be used to identifythe most likely sense for each ambiguous word typeand this approach can be effective for disambigua-tion (McCarthy et al 2004).Knowledge of the domain of a document has beenshown to be useful information for WSD.
For ex-ample, Khapra et al(2010) improve the perfor-mance of a graph-based WSD system using a smallnumber of hand-tagged examples, but further ex-amples would be required for each new domain.Agirre et al(2009) automatically construct a the-saurus from texts in a domain which they use forWSD.
Unfortunately, performance drops when thethesaurus is combined with information from localcontext.
Stevenson et al(2011) showed that per-formance of an unsupervised WSD algorithm canbe improved by supplementing the context with do-main information.
Cai et al(2007) use LDA tocreate an additional feature for a supervised WSDalgorithm, by inferring topics for labeled trainingdata.
Boyd-Graber et al(2007) integrate a topicmodel with WordNet and use it to carry out dis-ambiguation and learn topics simultaneously.
Li etal.
(2010) use sense paraphrases to estimate prob-abilities of senses and carry out WSD.
Koeling etal.
(2005) showed that automatically acquiring thepredominant sense of a word from a corpus fromthe same domain increases performance (over usinga predominant sense acquired from a balanced cor-pus), but their work requires a separate thesaurus tobe built for each domain under investigation.
Nav-igli et al(2011) extracted relevant terms from textsin a domain and used them to initialize a randomwalk over the WordNet graph.Our approaches rely on a one sense per topichypothesis (Gale et al 1992), making use of top-ics induced using LDA ?
we present three noveltechniques for exploiting domain information thatare employable with any WSD algorithm (unsuper-vised or supervised).
Using any WSD algorithm, wecreate a sense per topic distribution for each LDAtopic, and the classification of a new document into atopic determines the sense distribution of the wordswithin.
Once a sense per topic distribution is ob-tained, no further WSD annotation of new texts isrequired.
Instead of fixing domains, our technique680allows these to be dynamically created (using LDA)and we using four existing publicly available WSDalgorithms (three unsupervised and one supervised)to show that our technique increases their perfor-mance with no changes to the original algorithm.Section 2 briefly introduces LDA, while Section 3describes our three techniques for adding domaininformation to a WSD algorithm.
The WSD algo-rithms employed in the evaluation of our techniquesare described in Section 4 with experiments and re-sults in Section 5.
Section 6 draws our conclusionsand presents avenues for future work.2 Latent Dirichlet alcationLDA (Blei et al 2003) is a widely used topic model,which views the underlying document distributionas having a Dirichlet prior.
We employ a pub-licly available implementation of LDA1 which hastwo main execution methods: parameter estimation(model building) and inference for new data (classi-fication of a new document).
Both invocation meth-ods produce ?
distributions (the topic-document dis-tributions, i.e., p(ti|d) for ti topics and d document),and ?
distributions (word-topic distributions, i.e.,p(wj |ti) for words wj).
The parameter estimationphase also creates a list of n words most likely to beassociated with each topic.3 Using LDA for WSDThe underlying idea of our approach lies in deriv-ing a document invariant sense distribution for eachtopic, p(w, s|t).
Once this word sense distributionis obtained, the underlying WSD algorithm is neverneeded again.
We make the assumption that whilethe WSD algorithm may not be able to select thecorrect sense within an individual text due to insuf-ficient domain information, the topic specific sensewill be selected with a greater frequency over alldocuments pertaining to a topic, and thus the prob-ability distributions over senses generated in thisfashion should be more accurate.Only the distribution p(w, s|t) is dependent on anunderlying WSD algorithm ?
once this distributionis obtained, it can be combined with the LDA de-rived ?
distribution, p(t|dnew), to compute the de-1http://jgibblda.sourceforge.net/.sired word sense distribution within the new docu-ment dnew:p(w, s|dnew) =?tp(w, s|t)p(t|dnew)Sections 3.1, 3.2 and 3.3 describe three differentmethods for deriving p(w, s|t), and we investigatethe performance changes with different WSD algo-rithms: two versions of Personalized PageRank, de-scribed in Section 4.1, a similarity based WSD sys-tem outlined in Section 4.2, and a supervised graphbased algorithm (Section 4.3).3.1 Sense-based topic model (SBTM)In its usual form, the ?
distribution generatedby LDA merely provides a word-topic distribution(p(w|t)).
However, we modify the approach to di-rectly output p(w, s|t), but we remain able to clas-sify (non WSD annotated) new text.
The topicmodel is built from documents annotated with wordsenses using the chosen WSD algorithm.2 The topicmodel created from this data is based on word-sensecombinations and thus ?
represents p(w, s|t).To classify new (non sense disambiguated) doc-uments, the model is transformed to a word (ratherthan word-sense) based for: i.e., the p(w, s|t) prob-abilities are summed over all senses of w to give re-sulting probabilities for the wordform.
A new docu-ment, dnew, classified using this system gives rise toa number of distributions, including the probabilityof a topic given a document distribution (p(t|dnew)).3.2 Linear equations (LinEq)If the topic model is created directly from word-forms, we can use the known probabilities p(s|w, d)(obtained from the WSD algorithm), and p(t|d)(from the LDA classifier) to yield an overdeterminedsystem of linear equations of the formp(s|w, d) =?tp(s|w, t)p(t|d)We use an existing implementation of linear leastsquares to find a solution (i.e.
p(s|w, t) for each t)2It is not crucial to word sense disambiguate all words in thetext ?
a word can be passed to LDA in either its word-sense, dis-ambiguated, form or in its raw form.
While we do not attemptthis in our work, it would be possible to build a model specifi-cally for noun senses of a word, by including noun senses of theword and leaving the raw form for any non-noun occurrences.681by minimizing the sum of squared differences be-tween the data values and their corresponding mod-eled values, i.e., minimizing:?d(p(s|w, d)?
?tp(s|w, t)p(t|d))23.3 Topic words (TopicWord)The techniques presented in Sections 3.1 and 3.2both require the WSD algorithm to annotate a rea-sonably high proportion of the data used to build thetopic model.
For systems which do not rely on wordorder, an alternative based on the most likely wordsper topic is possible: the LDA algorithm generates?, a word-topic distribution.
It is therefore possibleto extract the most likely words per topic.To acquire a sense-topic distribution for a topic t,each target word w is included in a bag of wordswhich includes the most likely words for t andthe unsupervised WSD algorithm is executed (w isadded to the list if t does not already contain it).This technique is not applicable to non bag-of-wordsWSD algorithms, as structure is absent.4 Word Sense DisambiguationOnly the topic model documents need to be auto-matically annotated with the chosen WSD system,after this, the WSD system is never applied again(an LDA classification determines the sense distri-bution) ?
this is particularly useful for supervisedsystem which frequently have a long execution time.We explore three different types of WSD system:two versions of a knowledge base based system(Section 4.1), an unsupervised system (Section 4.2)and a supervised system (Section 4.3).4.1 Personalized PageRank (ppr and w2w)We use the freely available3 Personalized PageRankalgorithm (Agirre and Soroa, 2009) with WordNet3.0.
In Section 5 we present results from two optionsof the Personalized PageRank algorithm: ppr, whichperforms one PageRank calculation for a whole con-tent, and w2w, which performs one PageRank cal-culation for every word in the context to be disam-biguated.3Available from http://ixa2.si.ehu.es/ukb/4.2 WordNet similarity (sim)We also evaluated another unsupervised approach,the Perl package WordNet::SenseRelate::AllWords(Pedersen and Kolhatkar, 2009), which finds sensesof each word in a text based on senses of the sur-rounding words.
The algorithm is invoked with Lesksimilarity (Banerjee and Pedersen, 2002).4.3 Vector space model (vsm)An existing vector space model (VSM) based state-of-the-art supervised WSD system with features de-rived from the text surrounding the ambiguous word(Stevenson et al 2008) is trained on Semcor (Milleret al 1993).45 Experiments5.1 DataThe approach is evaluated using a domain-specificWSD corpus (Koeling et al 2005) which includesarticles from the FINANCE and SPORTS domainstaken from the Reuters corpus (Rose et al 2002).This corpus contains 100 manually annotated in-stances (from each domain) for 41 words.5The word-sense LDA topic models are createdfrom 80,128 documents randomly selected from theReuters corpus (this corresponds to a tenth of the en-tire Reuters corpus).
LDA can abstract a model froma relatively small corpus and a tenth of the Reuterscorpus is much more manageable in terms of mem-ory and time requirements, particularly given theneed to word sense disambiguate (some part of) eachdocument in this dataset.64A version of Semcor automatically transformed toWordNet 3.0 available from http://www.cse.unt.edu/?rada/downloads.html#semcor was used in this work.5Unfortunately, the entire domain-specific sense disam-biguated corpus could not be used in the evaluation ofour system, as the released corpus does not link eachannotated sentence to its source document, and it isnot always possible to recover these; approximately 87%of the data could be used.
This dataset is availableat http://staffwww.dcs.shef.ac.uk/people/J.Preiss/downloads/source_texts.tgz6In this work, all 80,128 documents were word sense disam-biguated.
However, it would be possible to restrict this set to asmaller number, as long as a reliable distribution of word sensesper topic could be obtained.682ppr w2w sim vsmBaseline 36 41 23 27SBTM model 39 43 30 31LinEq 41 44 ?
33TopicWord 38 41 ?
?Table 1: Summary of results based on 150 topics5.2 ResultsTable 1 presents the performance results for the fourWSD algorithms based on 150 topics.
A range oftopic values was explored, and 150 topics yieldedhighest performance, though the variance betweenthe performance based on different topics (rangingfrom 50 to 250) was very small (0.4% difference tothe average performance with 250 topics, and 3%with 50).
The performance shown indicates the pre-cision (number correct / number attempted).
Recallis 100% in all cases.The similarity algorithm (sim) fails on certaindocuments and therefore the linear equations tech-nique could not be applied.
The topic word tech-nique (TopicWord) could not be evaluated using thesimilarity algorithm, due to the high sensitivity toword order within the test paragraph.
In addition,the topic words technique is not applicable to su-pervised systems, due to its reliance on structuredsentences.
The best results with this technique wereobtained with including all likely words with proba-bilities exceeding 0.001 and smoothing of 0.1 of thetopic document distribution.Using a Wilcoxon signed-rank test, the resultswere found to be significantly better over the orig-inal algorithms in every case (apart from Topic-Words).
Both the WordNet similarity (sim) andthe VSM approach (vsm) have a lower performancethan the two PPR based WSD algorithms (ppt andw2w).
For example, sim assigns the same (usuallyincorrect) sense to all occurrences of the word tie,while both PPR based algorithms detect an obviousdomain change.
The vsm approach suffers from alack of training data (only a small number of exam-ples of each word appear in Semcor), while sim doesnot get enough information from the context.As an interesting aside, the topic models based onword-sense combinations, as opposed to wordformsonly, are more informative with less overlap.
Exam-ining the word stake annotated with the w2w WSDalgorithm: only topic 1 contains stake among the top12 terms associated with a topic in the word-sensemodel, while 10 topics are found in the wordformtopic model.
Table 2 shows the top 12 terms associ-ated with topics containing the word stake.Topic Word-based model39 say, will, company, share, deal, euro-pean, buy, agreement, stake, new, hun-gary, oil63 say, share, united, market, offer, stock,union, percent, stake, will, point, new90 say, will, fund, price, london, sell,stake, indonesia, court, investment,share, buy91 say, market, bond, russia, press, party,stake, russian, country, indonesia, new,election97 say, million, bank, uk, percent, share,stake, world, will, year, central, british113 say, will, percent, week, billion, last,italy, plan, stake, year, budget, czech134 say, china, percent, hong, kong, offi-cial, stake, billion, report, buy, group,year142 say, percent, market, first, bank, rate,year, dealer, million, money, close,stake145 say, will, new, brazil, dollar, group,percent, stake, year, one, make, do147 say, yen, forecast, million, parent, mar-ket, share, will, profit, percent, stake,groupSense-based model1 stake*13286801-n, share*13285176-n, sell*02242464-v, buy*02207206-v,have*02204692-v, group*00031264-n, company*08058098-n,percent*13817526-n, hold*02203362-v, deal*01110274-n, shareholder,interest*13286801-nTable 2: The presence of stake within the word- andsense-based topic models6836 ConclusionWe present three unsupervised techniques based onacquiring LDA topics which can be used to improvethe performance of a number of WSD algorithms.All approaches make use of topic information ob-tained using LDA and do not require any modifi-cation of the underlying WSD system.
While thetechnique is dependent on the accuracy of the WSDalgorithm, it consistently outperforms the baselinesfor all four different algorithms.AcknowledgmentsThis research was supported by a Google ResearchAward.
Our thanks also go to the two anonymousreviewers whose comments have made this papermuch clearer.ReferencesAgirre, E., de Lacalle, O. L., and Soroa, A.
(2009).Knowledge-based WSD on specific domains: per-forming better than generic supervised WSD.
In Pro-ceedings of the 21st International Joint Conference onArtificial Intelligence, pages 1501?1506.Agirre, E. and Soroa, A.
(2009).
Personalizing pager-ank for word sense disambiguation.
In Proceedings ofEACL.Banerjee, S. and Pedersen, T. (2002).
An adapted lesk al-gorithm for word sense disambiguation using wordnet.In Proceedings of the Third International Conferenceon Intelligent Text Processing and Computational Lin-guistics, pages 135?145.Blei, D. M., Ng, A. Y., and Jordan, M. I.
(2003).
LatentDirichlet alcation.
Journal of Machine Learning Re-search, 3:993?1022.Boyd-Graber, J., Blei, D., and Zhu, X.
(2007).
A topicmodel for word sense disambiguation.
In Proceedingsof the EMNLP-CoNLL, pages 1024?1033.Cai, J. F., Lee, W. S., and Teh, Y. W. (2007).
Nus-ml:Improving word sense disambiguation using topic fea-tures.
In Proceedings of SEMEVAL.Gale, W. A., Church, K. W., and Yarowsky, D. (1992).One sense per discourse.
In Proceedings of the4th DARPA Speech and Natural Language Workshop,pages 233?237.Khapra, M., Kulkarni, A., Sohoney, S., and Bhat-tacharyya, P. (2010).
All words domain adapted WSD:Finding a middle ground between supervision andunsupervision.
In Proceedings of ACL 2010, pages1532?1541, Uppsala, Sweden.Koeling, R., Mccarthy, D., and Carroll, J.
(2005).
Do-main specific sense distributions and predominantsense acquisition.
In Proceedings of Joint HLT-EMNLP05, pages 419?426.Li, L., Roth, B., and Sporleder, C. (2010).
Topic modelsfor word sense disambiguation and token-based idiomdetection.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 1138?1147.McCarthy, D., Koeling, R., Weeds, J., and Carroll, J.(2004).
Finding predominant senses in untagged text.In Proceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages 280?287.Miller, G. A., Leacock, C., Tengi, R., and Bunker, R. T.(1993).
A semantic concordance.
In Proceedings ofthe ARPA Workshop on Human Language Technology,pages 303?308.Navigli, R., Faralli, S., Soroa, A., de Lacalle, O. L., andAgirre, E. (2011).
Two birds with one stone: learn-ing semantic models for text categorization and wordsense disambiguation.
In CIKM, pages 2317?2320.Pedersen, T. and Kolhatkar, V. (2009).
Word-net::senserelate::allwords - a broad coveragewordsense tagger that maximizes semantic relatedness(demonstration system).
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language TechnologiesConference, pages 17?20.Rose, T. G., Stevenson, M., and Whitehead, M. (2002).The Reuters corpus volume 1 - from yesterday?s newsto tomorrow?s language resources.
In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation, pages 827?832.Stevenson, M., Agirre, E., and Soroa, A.
(2012).
Exploit-ing domain information for word sense disambigua-tion of medical documents.
Journal of the AmericanMedical Informatics Association, 19(2):235?240.Stevenson, M., Guo, Y., Gaizauskas, R., and Martinez,D.
(2008).
Knowledge sources for word sense disam-biguation of biomedical text.
In Proceedings of theWorkshop on Current Trends in Biomedical NaturalLanguage Processing at ACL, pages 80?87.684
