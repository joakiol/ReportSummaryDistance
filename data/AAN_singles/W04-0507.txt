A Practical QA System in Restricted DomainsHoojung Chung, Young-In Song, Kyoung-Soo Han,Do-Sang Yoon, Joo-Young Lee, Hae-Chang RimDept.
of Comp.
Science and EngineeringKorea UniversitySeoul 136-701 Korea{hjchung,sprabbit,kshan,yds5004,jylee,rim}@nlp.korea.ac.krSoo-Hong KimDept.
of Comp.
Software EngineeringSangmyung UniversityChonan 330-720 Koreasoohkim@smuc.ac.krAbstractThis paper describes an on-going research for apractical question answering system for a homeagent robot.
Because the main concern of the QAsystem for the home robot is the precision, ratherthan coverage (No answer is better than wrong an-swers), our approach is try to achieve high accuracyin QA.
We restrict the question domains and extractanswers from the pre-selected, semi-structured doc-uments on the Internet.
A named entity tagger and adependency parser are used to analyze the questionaccurately.
User profiling and inference rules areused to infer hidden information that is required forfinding a precise answer.
Testing with a small set ofqueries on weather domain, the QA system showed90.9% of precision and 75.0% of recall.1 IntroductionDuring the last decade, automatic question-answering has become an interesting research fieldand resulted in a significant improvement in itsperformance, which has been largely driven bythe TREC (Text REtrieval Conference) QA Track(Voorhees, 2004).
The best of the systems in the QATrack is able to answer questions correctly 70% ofthe time (Light et al, 2003).
The 70% of accuracyis, of course, high enough to surprise the researchersof this field, but, on the other hand, the accuracy isnot enough to satisfy the normal users in the realworld, who expect more precise answers.The difficulty of constructing open-domainknowledge base is one reason for the difficulties ofopen-domain question answering.
Since questionanswering requires understanding of natural lan-guage text, the QA system requires much linguis-tic and common knowledge for answering correctly.The simplest approach to improve the accuracy of aquestion answering system might be restricting thedomain it covers.
By restricting the question do-main, the size of knowledge base to build becomessmaller.This paper describes our restricted domain ques-tion answering system for an agent robot in homeenvironment.
One of the roles of the home agentrobot is to be able to answer the practical ques-tions such as weather information, stock quote, TVbroadcasting schedule, traffic information etc.
via aspeech interface.
The agent should provide high-precision answers, otherwise the users will not trustthe entire functions of the home agent robot, whichincludes not only the ability of question answeringbut also the speech interface for controlling homeappliances.
That means no answer is preferred toa wrong answer and the primary concern in our re-search is improving the precision of the question an-swering system.In this paper, we present a question answeringsystem which is restricted to answer only to thequestions on weather forecasts 1, and provide someexperimental results of the restricted QA system.To achieve the high accuracy, the QA system pro-cesses the semi-structured text data on the Inter-net and store it in the form of relational database.The domain specific hand-coded ontology contain-ing weather terms and cities is manually built for thequestion analysis and the inference process.The remainder of the paper is organized as fol-lows.
Section 2 describes the overall architectureof the QA system.
Section 3 describes the prac-tical QA system.
Section 4 evaluates the systemand reports the limitation of the QA system.
Sec-tion 5 compares our system with other QA systems.Section 6 concludes with some directions for futurework.2 Overall ArchitectureThe overall framework of the QA system is pre-sented in Figure 1.
The QA system consists of twomajor parts; the IE (Information Extractor) engineand the QA engine.1We?ve developed the QA system for a TV broadcast sched-ule domain as well, which is more complex to process than theweather forecasts QA, but have not evaluated it yet.
So, in thispaper, we present the system for weather forecasts only.QAEngineDBMSIEEngineWeb BrowserInternet(WWW)Web InterfaceSpeech ProcessorQA SYSTEMFigure 1: Overall architecture of the Question Answering SystemThe IE engine consists of two parts; a webcrawler and a wrapper.
The web crawler down-loads the selected webpages from the website of theKorea Meteorological Administration (KMA) everyhour.
The website provides current weather con-ditions and 7 day-forecasts for dozens of Koreancities.
The wrapper, which is a set of extractionrules, is used to extract weather information fromthe webpages .
The extracted information is storedin the database.TheQA engine performs three-phase processing:First, it analyzes natural language questions andtranslates the questions into Structured Query Lan-guage (SQL) statements.
Second, the SQL queriesare directed to a DBMS to retrieve the answers inthe database.
Finally, the result from the DBMSis converted to natural language sentences for out-put.
Figure 2 depicts overall processes for the QAengine.
A DBMS (Currently, Oracle Database) isused for managing extracted data.
A speech proces-sor can be merged with the system when it is usedin the home agent robot, which provides speech in-terface.
A web interface is used for providing web-based QA service with the QA system.3 A Practical QA SystemThe question answering starts from extractingweather information from the web site.
The userrequest is analyzed with the question analyzer andthe appropriate query frame is selected, and then therequest is translated into the SQL expression.
TheSQL query is used to retrieve the correct answerfrom the database, which stores weather informa-tion from the webpages.
Finally, natural languageQuestionAnalyzerNamed EntityTaggerTemporal DataNormalizerQuery FrameClassifierKeywordsNatural LanguageQuestionSQLGeneratorQueryFrameUser ProfileSQLQueryInferenceRulesNL AnswerGeneratorQueryResultNatural LanguageAnswerQA EngineDBMSFigure 2: The QA Engineanswer is generated based on the every result ex-tracted from the DBMS.3.1 Information ExtractionThe weather information in the webpages is semi-structured.
Semi-structured resources generally donot employ unrestricted natural language text, butrather exhibit a fair degree of structure (Kushmer-ick, 1997).
Therefore, information can be accu-rately and easily extracted from the webpage, com-pared to IE from unstructured data.On the other hand, semi-structured resourcesare usually formatted for use by people, and con-tain irrelevant elements that must be ignored, suchas images, advertisements, and HTML formattingtags (Figure 3).
Thus information extraction fromthe semi-structured documents is not entirely triv-ial.
Currently, the QA system is using hand-codedwrappers.
However, we are developing an auto-matic process of constructing wrappers (wrapper in-duction) for semi-structured resources and that candetect the modification of the web page design andadapt the wrapper according to the modification, au-tomatically, like (Sigletos et al, 2003).Presently, the IE engine extracts following infor-mation :?
Current observation: weather summary, visi-bility, temperature, wind, relative humidity?
7 days-forecasts : weather summary, forecasttemperature (highest/lowest).3.2 Question AnalysisFirst, user?s request is analyzed with the query an-alyzer as represented in Figure 2.
The analyzer ex-tracts several keywords that describing the question,such as event word, date, time, and location, by us-ing a dependency parser, and the user question isrepresented only by these extracted keywords.The named entity tagger is used to identify tem-poral expressions, place names, and weather events.The tagger consults the domain-dependent ontologyfor recognizing weather events, and the domain-independent ontology for place names.
The ontol-ogy for the weather events consists of event con-cepts, which are similar to Synset in WORDNET(Fellbaum, 1998).
For example, rain and umbrellaare in same event concept in the domain ontologyfor weather events, because the questions about us-ing umbrella are usually asking about raining (e.g.Will I need to bring umbrella tomorrow?
and Will itbe raining tomorrow?
)The temporal data normalizer converts temporalexpressions such as today, this weekend and nowinto absolute values that can be used in querying tothe database.Seoul, March.
11., wide spread dust, (-/-)Seoul, March.
12., cloudy, (0/11)Seoul, March, 13., Sunny, (1/11)...Figure 3: Wrappers extracts weather informationfrom the semi-structured documentsIf the information on date, time, or location isnot expressed in the user?s request, the question an-alyzer infers the missing information.
The infer-ence rules, which are built based on our observationon various user questions, are domain-independent,because the omission of temporal or spatial infor-mation is common not only in weather informationquestion, but also in questions for other domains.The user profile is used for the inference inquery analysis.
We observed many people omit theplace name in the weather-domain question.
Unlikethe temporal information, it is impossible to guessthe current location without any user information.Thus, we store some user-related information in theuser profile.
Portfolio of stocks or favorite TV pro-grams can be stored in the user profile if the QA sys-tem processes queries on stock quote or TV sched-ule domain.Let?s take an example of the query analysis.
Thefollowing keywords are extracted from the question?Is it raining?
?EVENT : rainDATE : 03/12/04TIME : 02:20CITY : SeoulEven though the time, date, and city is not explic-itly mentioned in the question, the question analyzerinfers the information with the user profile and theinference rules.3.3 Query Frame DecisionRestricting the question domain and information re-source, we could restrict the scope of user request.That is, there is a finite number of expected ques-tion topics.
Each expected question topic is definedas a single query frame.
The following are queryframe examples.
They are used for processing thequery for the precipitation forecast for the next day,diurnal range of today, current wind direction, andcurrent temperature, respectively.
[PRECIPITATION_TOMORROW][DIURNALRANGE_TODAY][WINDDIRECTION_CURRENT][TEMPERATURE_CURRENT]Each frame has a rule for SQL generation.
PRE-CIPITATION TOMORROW has the followingSQL generation rule.
[PRECIPITATION_TOMORROW]SELECT date, amprecpr, pmprecpr FROMforecast tbl WHERE $date $citydate, amprecpr and pmprecpr are field names in thedatabase table forecast tbl, which mean date, theprecipitation probability of morning and afternoonof the day.
The rule generates the SQL statementthat means: retrieve the precipitation probability oftomorrow morning and afternoon from the DB tablewhich stores forecast information.Here is another example, which is the SQL gen-eration rule for [DIURNALRANGE TODAY].
[DIURNALRANGE_TODAY]SELECT city, max(temp)-main(temp) FROMcurrent tbl WHERE $date $city group by cityAnalyzing a question means selecting a queryframe in this system.
Thus, it is important to se-lect the appropriate query frame for the user request.The selection process is a great influence on the pre-cision of the system, while there is not much likeli-hood of errors in other processes, such as generatingSQL query from the selected query frame, retriev-ing DB records, and generating an answer.As represented in Figure 2, the extracted event,temporal and spatial keywords are used for selectingan appropriate query frame.
Currently, we are us-ing a hand-coded decision tree-like classifier for se-lecting an appropriate query frame for the extractedkeywords.
If a question isn?t proper for the handlingIs it raining?
?EVENT : rainDATE : 03/12/04TIME : 02:20CITY : Seoul?The frame [RAIN_CURRENT] is selected.
?SELECT time, city, weather FROM current tblWHERE time=?03/12/04/0200?, city=?Seoul?Figure 4: Interpreting the natural language questionto the SQL querySELECT time, city, weather FROM current tblWHERE time=?03/12/04/0200?, city=?Seoul?
?DBMS returns ?03/12/04/0200 Seoul Sunny?
?On 2:00 p.m., Seoul is sunny.Figure 5: Answer generation from the result ofquerydomain, the classifier rejects it.
Machine learnedclassifier is being developed in order to substitutefor the hand-coded classifier.3.4 SQL GenerationIf a query frame is selected for a question, an SQLquery statement is generated from the SQL pro-duction rule of the frame.
The query is sent tothe DBMS to acquire the records that match to thequery.
Figure 4 depicts the conversion from a natu-ral language question to its SQL expression.3.5 Answer GenerationBased on the result of the DBMS, a natural lan-guage answer is generated.
We use a rule basedanswer generation method.
Each query frame hasan answer generation pattern for the frame.
Forexample, DIURNALRANGE TODAY has thefollowing generation pattern.
[DIURNALRANGE_TODAY]The diurnal temperature range of $date($1) is $2?C$1 and $2 are the the first and second field value ofthe queried result.
$date() is the function that con-verts a normalized date expression to a natural lan-guage expression.
Figure 5 shows the answer gener-ated from the SQL query shown in Figure 4 (Moresample outputs from the QA System are presentedon the Appendix) .4 Evaluation and LimitationWe have evaluated our domain restricted QA sys-tem based on precision and recall, and investigatedthe limitation of the our approach to the restricted-domain QA system.For evaluation, we?ve collected 50 weather ques-tions from 10 graduate students.
Precision and re-call rates are 90.9 % and 75.0% respectively.The low recall rate is due to some questions re-lated to invalid date and topic.
The system providesweather forecasts for 7 days from the querying day.But some of queries are asking for a future weatheroutlook which is out of range ( e.g.
Will it be veryhot summer this year?
or Will it be snow on thisChristmas?).
Some questions asked the informationthat the database doesn?t contain, such as UVI (ul-traviolet index).The primary reason for the wrong answer is thefailure of invalid topic rejection.
It is due to theinsufficient of weather-domain ontology data.
Forexample, from the question What is the discom-fort index calculated from the today?s weather?,the keyword discomfort index was not extractedwhile weather was extracted, because the formerwas not in the ontology.
So the query frameWEATHER TODAY was misselected and the sys-tem generated the wrong answer Seoul will be sunnyon March 9th 2004.An error was caused by the flaw of our keyword-based query frame decision approach.
For the ques-tion Can the flight for Jeju Island take off today?,the extracted keywords areEVENT : flight take_offDATE : 03/12/04CITY : JejuIn order to know whether the flight can take offor not, the weather information of the departure cityinstead of the destination city (i.e.
Jeju) should bereturned, but our keyword based approach failed tomake an appropriate query.
To solve this problem,more sophisticated semantic representation, ratherthan the sequence of keywords, is required for thequestion.5 Related WorksIn this section, we compare our system with otherQA-related approaches and briefly describe the dis-tinctive characteristics of our system.
Open-domainQA systems in QA track mostly extract answersfrom unstructrued documents.
In the contrast, oursystem extracts answers from semi-structured webpages, which are pre-selected by us, because oursystem aims to achieve high precision with the sac-rifice of the coverage of questions.Natural language front ends for databases(Copestake and Jones, 1990) and our system handleuser questions similarly.
However, our system hasinformation extraction part that makes the databasebe updated regularly and automatically.
Moreover,our system returns natural language responses tousers.The START system (Katz, 1997) is a web-basedQA system.
It uses World Wide Web as knowledgeresource.
Unstructured natural language sentencesare indexed in the form of ternary expressions andstored in RDB.
The START system covers muchwider domain of questions than ours, however, itseems that the system returns more wrong answersthan ours, because we extract the answer only fromsemi-structured documents.The Jupiter system (Zue et al, 2000) is a con-versational system that provides weather informa-tion over the phone.
Based on the Galaxy architec-ture (Goddeau et al, 1994), Jupiter recognizes userquestion over the phone, parses the question withthe TINA language understanding system (Seneff,1992) and generates SQL and natural language an-swer with the GENESIS system (Baptist and Sen-eff, 2000).
The generated answer is synthesizedwith the ENVOICE system.
Even the Jupiter systemdeals with the same domain, ours can process a bitwider-range of weather topics.
Our QA system cancover the question which requires inferences suchas When is the best day for washing my car in thisweek?
Moreover, our system has an ability of infer-ring missing information from the user profile andthe inferring algorithm.6 ConclusionThis paper describes the practical QA system for re-stricted domains.
To be practically used, our sys-tem tries to achieve high precision at the sacrifice ofquestion coverage.To achieve high accuracy, we pre-designate semi-structured information resource webpages and ex-tracted domain-specific information from them.
Wealso prepare a domain-specific ontology and queryframes for the question analysis.
The user?s requestin natural language is converted into SQL expres-sion to generate an answer for the question.
Testingwith a small set of queries on weather domain, theQA system showed 90.9% of precision and 75.0%of recall.
By restricting the coverage of questions,our system could achieve relatively high precision.However, the figures are not enough for a real prac-tical system.QuestionAnalyzerfor Domain 2QuestionAnalyzerfor Domain 1QuestionAnalyzerfor Domain nDomainClassifierSQL Generatorfor Domain 1SQL Generatorfor Domain 2SQL Generatorfor Domain n.........Natural LanguageQuestionQuery FrameClassifier forDomain 1Query FrameClassifier forDomain 2Query FrameClassifier forDomain nQA Enginefor Domain 1QA Enginefor Domain 2QA Enginefor Domain nFigure 6: A domain classifier selects a proper re-stricted domain QA engineMuch work is left for our future work.
First,we are expanding the domain for the system.
Adomain classifier will be added to the QA sys-tem to process multiple-domain questions, as rep-resented in Figure 6.
We will separate domain de-pendent resources (query frames, ontology contain-ing domain-dependent information, and etc.)
anddomain independent resources (linguistic resources,and ontology for domain-independent information)to allow easier domain expansion.Second, the information extractor has to be up-graded.
Currently, the QA system is using hand-coded wrappers, and the wrappers cannot extractnecessary information robustly when the webpagesare modified.
We are developing an information ex-tractor that can recognize the modification of thewebpages and modify the wrappers automatically.The upgraded information extractor will improvethe robustness of our system.Finally, we will increase the size of ontology tocover more question types.
From the experimenta-tion, we realize that a larger ontology for weatherterms is necessary to classify a question correctly.It seems more query frames are necessary for moreproper answers to the users?
requests.ReferencesL.
Baptist and S. Seneff.
2000.
Genesis-II: A ver-stile system for language generation in conversa-tional system applications.
In Proceedings of In-ternational Conference on Spoken Language Pro-cessing, October.A.
Copestake and K. Sparck Jones.
1990.
Naturallanguage interfaces to databases.
The KnowledgeEngineering Review, 5(4):225?249.C.
Fellbaum.
1998.
WordNet: an Electronic Lexi-cal Database.
The MIT Press.D.
Goddeau, E. Brill, J.
Glass., C. Pao, M. Phillips,J.
Polifrini, S. Seneff, and V. Zue.
1994.
Galaxy:A human language interface to on-line travel in-formation.
In Proceedings of International Con-ference on Spoken Language Processing, pages707?710.B.
Katz.
1997.
Annotating the World Wide Webusing natural language.
In Proceedings of the 5thRIAO conference on Computer Assisted Informa-tion Searching on the Internet.N.
Kushmerick.
1997.
Wrapper Inductin for In-formation Extraction.
Ph.D. thesis, University ofWashington.M.
Light, A. Ittycheriah, A. Latto, and N. Mac-Cracken.
2003.
Reuse in question answering: Apreliminary study.
In New Directions in QuestionAnswering: Papers from the 2003 AAAI Sympo-sium, pages 78?86.S.
Seneff.
1992.
Tina: A natural language systemfor spoken language applications.
ComputationalLinguistics, 18(1):pp.
61?86.G.
Sigletos, G. Paliouras, C. D. Spyropoulos, andM.
Hatzopoulos.
2003.
Mining web sites us-ing wrapper induction, named entities and post-processing.
In 1st European Web Mining ForumWorkshop at ECML/PKDD-2003.Ellen .M.
Voorhees.
2004.
Overview of the TREC2003 question answering track.
In Proceedingsof the 12th Text Retrieval Conference.V.
Zue, S. Seneff, J.
Glass, J. Polifroni, C. Pao,T.
J. Hazen, and L. Hetherington.
2000.
Jupiter:A telephone-based conversational interface forweather information.
IEEE Transactions onSpeech and Audio Processing, pages 100?112.Appendix: Example of QAQ: Is it cold outside?A: The temperature of Seoul at 1 p.m. is 11?
C.Q: Will it be cold today?A: The highest temperature of Seoul on March 9th2004, will be 12?
C.Q: What?s the diurnal temperature range of today?A: The diurnal temperature range of Seoul is 1?C.Q: I am planing a picnic to Yongin on these week-ends, and which day would be better for thepicnic?A: Yongin will be sunny on March 3rd 2004.
Yon-gin will be partly cloudy on March 4th 2004.Q: What is the ultraviolet index?A: The system cannot answer to that topic.Q: Can I have car wash today?A: The precipitation probability of Seoul this after-noon is 10%.
