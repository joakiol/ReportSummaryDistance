ACL Lifetime Achievement AwardThe Right Tools: Reflections on Computationand LanguageWilliam A.
Woods?ITA Software, Inc.1.
IntroductionGood morning.
I want to thank the ACL for awarding me the 2010 Lifetime Achieve-ment Award.
I?m honored to be included in the ranks of my respected colleagues whohave received this award previously.
I want to talk to you this morning about theevolution of some ideas that I think are important, with a little bit of historical andbiographical context thrown in.
I hope you?ll find in what I say not only an appreciationfor some of the ideas and where they came from, but also a trajectory that continuesforward and suggests some solutions to problems not yet solved.1.1 Space: The First FrontierFigure 1 is a picture of a moon rock.
It has been argued that the modern era of com-puters, and specifically the creation of DARPA and the subsequent invention of theInternet, were all stimulated and driven by the race to conquer space.
Coincidentally,the beginnings of my own career in computing and computational linguistics are alsotied to space.On October 4, 1957, the Soviet Union astounded the world by launching earth?sfirst artificial satellite.
Soon after the Sputnik launch, the Smithsonian AstrophysicalObservatory contacted an astronomer, Joseph Brady, at the University of California?sLawrence Radiation Laboratory in Livermore, California.
They wanted to know if thelab would track Sputnik and predict what was going to happen to it.
Joe Brady hadthe right tools for the job.
He and his colleagues had been computing planetary orbitsin the solar system, and they could directly apply their programs to satellite orbits.Joe was able to successfully predict the exact day that Sputnik was last seen in thenight skies over Washington, DC (December 1, 1957), 58 days after it was launched.Joe Brady and his fellow astronomers were meticulous about accuracy.
(Incidentally,the Vanguard I satellite that the U.S. launched five months later is still up there and wasstill transmitting signals in 1964.
It was the first satellite to use solar cells, clearly theright tool for that job!)
Three years later, Joe Brady and his colleague Nevin Shermanwould initiate me into the small group of people who could program computers.When Sputnik went up, I was a freshman in high school and just learning algebra,which I thought was the greatest thing since sliced bread.
When I graduated from high?
ITA Software, Inc., 141 Portland Street, Cambridge, MA 02139, USA.
E-mail: wwoods@itasoftware.com.This article is an extended version of the talk given on receipt of the ACL?s Lifetime Achievement Awardat the Association?s annual meeting in Uppsala, Sweden, on July 13, 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 4Figure 1A photograph of lunar rock, sample 10046.school, I went to work for Joe Brady at the Livermore Laboratory as a programmingintern, before starting college at Ohio Wesleyan University.
I began my professionalcomputing career by proofreading 300 years of Mars observations.
I worked at theLivermore Labs for four summers and learned a lot of things from that job, not theleast of which were patience, diligence, and thoughtful programming.
I first learned toprogram in numerical machine language on an IBM 650, the machine that Donald Knuthcredits as the origin of linked lists.
Because the memory on the IBM 650 was a rotatingdrum, latency could be optimized by distributing instructions through memory andlinking them together with next-instruction pointers.
If you located your instructionscarefully, the next instruction would be rolling up to the read heads just as it was needed.Later I programmed the IBM 709 and the Univac LARC.
The LARC (which stoodfor Livermore Advanced Research Computer) was one of the first supercomputers, andwas designed specifically for the Livermore Laboratory to perform scientific computa-tions.
One of the programs I wrote for the LARC was a sine and cosine routine thatwas accurate to half a decimal digit in the 12th decimal place and was subsequentlyincorporated into the Labs?
Fortran library.Twelve years later, on July 20, 1969, the United States astounded the world byvisiting the moon and subsequently returning with 47.5 pounds of lunar rocks.
Thefollowing summer, Jeffrey Warner, from the NASA Manned Spacecraft Center LunarReceiving Laboratory, visited me at Harvard University, where I was a recently mintedAssistant Professor.
He wanted to know if I could build a question-answering systemthat would allow lunar scientists to directly access the data that he had collected.
Itturned out that I had the right tools for the job.
I had just built a system for answeringnatural English questions, using a methodology that could be applied to arbitrarydomains.
I had already applied that system to airline flight schedules and to questionsabout states, arcs, and paths in an ATN grammar.At the invitation of Danny Bobrow, then at Bolt Beranek and Newman, Inc. (BBN),I moved to BBN in 1970 and pursued this endeavor as my first project there.
(However,I continued to teach my courses at Harvard for several years thereafter and returned602Woods The Right Toolsto Harvard later as a Gordon McKay Professor of the Practice of Computer Science.
)With the help of Ron Kaplan, Bonnie Webber, and others at BBN, I was able to applymy system to the lunar rocks domain and demonstrate the Lunar Sciences Natural Lan-guage Information System, answering live questions from lunar scientists, at the SecondAnnual Lunar Science Conference in January 1971.
This demonstration, six months afterthe project began, proved the validity of my claim that this methodology could beapplied easily to new domains.
The basic elements, a general-purpose ATN grammarfor English and a procedural-semantics framework for semantic interpretation, werepowerful tools.
I kept track of the questions that were asked at the conference and theanswers that the system produced, and I analyzed the things it got wrong.
I catalogedthe successes and the failures (most of which were due to simple errors or missinglexical entries or rules), and presented the results at the AFIPS Fall Joint ComputerConference in 1973 (Woods 1973).So this talk is going to be about tools: a bit about tools in general, but mostly aboutthe tools of our trade.
First, however, I want to tell you a little bit about myself that mayhelp illustrate where I?m coming from and how I approach problems.1.2 BeginningsI consider myself fortunate to have grown up in West Virginia at a place and time whendogs could run free, and so could little boys (see Figure 2).
When I was in the secondgrade, I showed my first interest in linguistics when I told my teacher that the Englishpronoun system would work better if the pronoun you were confined to the singularand you all was used for the plural.
When I was about the same age, my father taughtFigure 2A photograph of me sitting on a rock in West Virginia.603Computational Linguistics Volume 36, Number 4me to use basic hand tools.
I have been building things ever since.
In high school, I builtmy own computer.
My algebra teacher had given me a handout that showed how amechanical relay could represent the binary numbers 0 and 1, and based on that anda book called The Basics of Digital Computers, I built a binary adder and complementerand entered it in a local science fair.
I made my own relays for that computer, usingmetal that I salvaged from used dog-food cans and wire that I unwound from cast-offsteering yokes from TV picture tubes.
Figure 3 is a picture of me with my computer atthe science fair.
I won first prize in the mathematics division and second place overallin the fair.
I have no doubt that this adventure was instrumental in my getting the job atthe Livermore Laboratory.In college, I majored in Math and Physics and fell in love with the machine shopin the Physics lab.
Some of my physics experiments involved building the equipment Ineeded to do the experiment.
I learned to use a drill press and a metal lathe.
I liked themetal lathe so much that when I graduated, I bought one of my own.
The thing aboutlathes is that they are more of a platform than a tool.
They hold a piece of materialand rotate it against a cutting tool that cuts away material until you?re left with theshape you want.
You can make almost any shape that is rotationally symmetric.
Ona wood lathe, you select various gouges and cutting tools and hold them against thework to make the cuts.
On a metal lathe, the tool is mounted on a carriage that is movedautomatically along the work, while an operator (or a gear box, or a computer program)makes precision adjustments.
For many jobs, you make a custom cutting tool by takinga bar of tool steel and grinding away metal to produce exactly the tool you need for thejob.
Figure 4 shows some of the cutting tools I made and one of the resulting products.You can see that I enjoyed the work.What I learned from the lathe is that you don?t have to confine yourself to anexisting set of tools.
You can make custom tools that are tailored to exactly the job youFigure 3A photograph of me in front of my science fair computer.604Woods The Right ToolsFigure 4A photograph of some lathe tools I made and a miniature canon I made with them.want to do.
This is a perspective that I?ve carried into my research, and one that I wantto share with you today.2.
Language and ComputationI want to start with the observation that language is fundamentally computational.Computational linguistics has a more intimate relationship with computers than manyother disciplines that use computers as a tool.
When a computational biologist simulatespopulation dynamics, no animals die.
When a meteorologist simulates the weather,nothing gets wet.
But computers really do parse sentences.
Natural language question-answering systems really do answer questions.
The actions of language are in thesame space as computational activities, or alternatively, these particular computationalactivities are in the same space as language and communication.2.1 The Tools of Our TradeSo when I talk about tools of our trade you might think first of the computer, but that?snot exactly what I have in mind.
I?m thinking more of abstract things like theories,algorithms, formalisms, and methodologies.
In my graduate training, I learned manysuch tools, including finite-state machines, context-free grammars, linear bounded au-tomata, Turing machines, circuit theory, lattice theory, first-order logic, and algorithmsfor parsing, search, optimization, and theorem-proving.
I was fortunate to take SheilaGreibach?s course in Formal Language Theory the year before she discovered abstractfamilies of languages.
When I took it, she was teaching constructive proofs for thingslike converting context-free grammars to Greibach normal form, constructing determin-istic minimal state finite-state machines, and intersecting context-free grammars withregular sets.
These algorithms have been staples of my subsequent research.
Tools of ourtrade also include principles, notations, architectures, and resources such as dictionariesand corpora.605Computational Linguistics Volume 36, Number 43.
FormalismsThe first tool I want to talk about is the ATN grammar formalism, but first somebackground.The year that Sputnik went up was the same year that Noam Chomsky pub-lished Syntactic Structures.
When I started graduate school at Harvard, TransformationalGrammar was all the rage, and I attended lectures and seminars by linguists such asBarbara Hall, George Lakoff, and Haj Ross.
Two things impressed me about GeorgeLakoff.
One of them was the meticulous examples that he would muster to supporthis theories.
The other was the fact that he would completely abandon a theory andlook for a new one as soon as he encountered a key example that his previous theorycouldn?t handle.
Initially, these linguists were trying to write specific sequences of trans-formational rules in an attempt to capture the regularities of English.
Even as I watched,however, this proved too difficult, and the effort shifted to proposing constraints onsuch rule sets, without attempting to specify complete sequences of rules.At the time, there were people trying to write parsers for transformational gram-mars, but the transformational grammar formalism didn?t lend itself to efficient parsingalgorithms.
Stanley Petrick at IBM worked for many years on a parsing system fortransformational grammars, and it was never very fast.In contrast, the Harvard Predictive Analyzer developed by Kuno, Oettinger, andGreibach for machine translation used a form of context-free grammar that becameknown as Greibach normal form.
With this formalism, they could parse real sentencesfrom real documents in reasonable times.
In Greibach normal form, the right-hand sideof every rule started with a terminal symbol, so the operation of the parser was tosimply take the next word from the input, pair it with the symbol on the top of thestack, and look for matching rules to replace the stack symbol with a sequence of newsymbols.
Despite this efficiency, this was an exponential parsing algorithm, unlike theCKY algorithm which was an n-cubed algorithm.The problem with these context-free grammars, however, was that expressing thedetailed knowledge of real English syntax in a context-free grammar formalism couldrequire thousands of rules and generate hundreds of parses for real sentences.
Thiswas because each time you wanted to account for a feature, such as number agreementbetween a subject and object, or constraints between a category of verb and the kindsof complements it could take, it would require copying a portion of the grammar toproduce specialized versions for each different case.
This generally involved doublingthe size of some portion of the grammar for each new feature.For example, consider the following expansions:(1) S ?
NP V (NP)(2) S ?
NP ViS ?
NP Vt NP(3) S ?
NPs VisS ?
NPs Vts NPS ?
NPp VipS ?
NPp Vtp NPIn order to capture the constraint between transitive verbs and direct objects, we haveto make two copies of the original rule.
In order to capture number agreement betweensingular and plural subjects and singular and plural verbs, we have to make two copies606Woods The Right Toolsof each of these.
After a number of such doublings, the size of the grammar can becomequite large and unmanageable.3.1 ATN GrammarsIn my doctoral thesis, which was devoted to semantic interpretation of natural lan-guage questions, I presented a transformational grammar for a subset of English asan existence proof that English sentences could be mapped into the kinds of syntacticstructures that I wanted for input to my semantic interpreter.
Later, when I wantedto implement a parser that would actually do it, neither context-free grammars nortransformational grammars were good tools for the job.
I wanted a tool that could doeverything that a transformational grammar could do, but do it efficiently.I developed ATN grammars to meet this need.
ATN grammars are represented bystate transition diagrams similar to finite-state transition diagrams, except that theyallow transitions to be labeled with phrase categories as well as individual word cat-egories, and each phrase category has a corresponding transition diagram that specifieshow to recognize (or generate) phrases of that kind.
In addition, the transitions can beaugmented with conditions and actions that can record information about constituentsparsed and can be tested to determine whether the transition is to be allowed or not.Final states in the diagram are represented by POP arcs that can also have conditionsand can specify the structure to be returned for the constituent recognized.
Transitionsare ordered, so that the grammar can express preferences about the relative order ofpossible parses.
Actions can include setting a weight that can also express preferencesamong alternative parsings.
These ATN grammars were able to do the kind of deep-structure analyses of English sentences that transformational grammars could do, butdo them efficiently.
I published my ATN paper in the Communications of the ACM in 1970(Woods 1970).I found that ATN diagrams were good tools for evolving a grammar as I uncoveredmore and more of the patterns and regularities of natural English.
Unlike context-freegrammars, I could set features for number agreement and test them later.
I could testwhether a verb could be transitive before permitting a transition to pick up a directobject.
I found that after my ATN grammar reached a certain maturity, the changesneeded to handle new phenomena correctly were generally small and focused.
WhenI encountered a sentence that should have parsed, but didn?t, or one that didn?t parsecorrectly, usually it was only necessary to tighten or relax a condition, or add a newtransition to pick up something and then join back into the main flow at a later state.This was in marked contrast to the context-free grammar formalism, where changesthat were conceptually small might involve copying large portions of the grammar andintroducing a host of name variations to express a new feature.
Figure 5 shows a portionof the ATN grammar for the LUNAR system (Woods, Kaplan, and Nash-Webber 1972).
Ifound that something like 50 states and 200 transitions could account for quite a diverserange of natural English.3.1.1 Making Sense of Shorthands.
ATNs developed from my observation that commonnotations that linguists used to write compact rules with optional, repeatable, and alter-native constituent sequences, like the following, were the same operations that definedregular expressions:(4) S ?
NP V (NP) PP?607Computational Linguistics Volume 36, Number 4Figure 5A portion of the ATN grammar for the LUNAR system.
(5) S ?
NP {Vi | Vt NP} PP?
(6) S ?
NP {AUX Vtu NP | AUX Viu | Vt NP | Vi} (PP)?Regular expressions, I knew, were equivalent to finite-state machines, which were typ-ically represented by finite-state transition diagrams.
So what if we replaced the right-hand sides of these rules with transition diagrams?
I realized that one could considersuch transition diagrams to be a form of pushdown-store automaton.
Thus, we couldgeneralize finite-state transition diagrams to pushdown-store automata by allowingnonterminal symbols on transitions.
This provided a natural parsing algorithm forgrammars that used these notations, and would provide a better semantics for the useof such notations than just thinking of them as a shorthand for a possibly infinite set ofordinary context-free grammar rules.This basic generalization of finite-state machines is now known as a recursive tran-sition network or RTN.
RTNs are essentially ATNs without the augmentations.
RTNsare functionally equivalent to context-free grammars, but they have a capability thatordinary context-free grammars do not.
With an RTN (or an ATN) it is possible to factortogether common parts of different context-free rules while maintaining the constituentstructure of the original.
In an RTN (and in ATNs), one can distinguish a finite-stateportion of the machinery and a constituent-structure portion of the machinery andkeep the two distinct.
One can apply factoring transformations to the finite-state partwithout changing the constituent structure.
Figure 6 illustrates a simple factoring of608Woods The Right Toolscommon parts of rules in the transformation of an ordinary context-free grammar intoan equivalent RTN.3.1.2 Augmenting the Transition Network.
ATNs add augmentations to the basic recursivetransition network of an RTN.
Associated with the states of the grammar are a set ofregisters that can be carried forward through an analysis of a sentence.
Conditions andactions on the transitions (or ?arcs?)
of the grammar can set and test these registers.Transitions are blocked if the conditions fail, and the information carried in the registerscan be used to build the structure returned by the transition network when it completesa phrase.
The completion of a phrase is indicated by a POP arc, which also can have acondition, and which specifies how to build the structure to be returned for the phrase.PUSH arcs indicate transitions that can be made if a phrase of the specified type isparsed.
This can be done by pushing the current configuration onto a stack and invokingthe transition diagram for a phrase.
When the phrase that was pushed for is completed,the result is returned as the value of the suspended transition.
Parsing then continuesfrom the configuration that is popped from the stack.
This is the most direct algorithmfor parsing with an ATN, but, as we shall see, it is not the only one.ATN actions can also pass parameters into and out of constituents, and a specialHOLD list can carry information for long-distance dependencies equivalent to the tracesof transformational grammar theory.
A special SYSCONJ meta operator was imple-mented to handle conjunctions, including reduced conjunction constructions, withoutrequiring special grammar rules for conjunction.
This machinery could handle construc-tions like:(7) The man drove his car through and completely demolished a plate glasswindowHere, common parts of an underlying conjunction are factored out and appear onlyonce in the surface structure, so that we appear to be conjoining the fragments drovehis car through and completely demolished.
The SYSCONJ machinery would automaticallyhandle this kind of construction and build an appropriate deep structure.
This kind ofFigure 6Factoring the common parts of rules.609Computational Linguistics Volume 36, Number 4conjunction would otherwise seem to require something like a combinatory categoricalgrammar.My ATN grammar parser could also use a special kind of POP arc called an SPOPthat would assign a movable modifier to a preferred scope by searching up the stackfor alternative locations for that modifier and then picking the scope that made themost semantic sense.
This feature, called ?selective modifier placement?
would parse asentence like:(8) Does American have a flight from an East coast city to Chicago?with to Chicago attached as a modifier to flight instead of the immediately preceding city,because there was a semantic rule that would allow flight to use to Chicago, althoughthere was no such rule for city.Unlike context-free grammars, ATNs lend themselves to representing regularitiesin so-called ?free word-order languages.?
Whenever the regularities in the language arenot well expressed by the order of constituents, these regularities can be moved out ofthe state transitions and into the registers.
Suppose, for example, you can have subject,verb, and object in any order, except for the sequence ?object, subject, verb?, which isnot permitted.
To model this, you can simply have three self-looping transitions on asingle state, with a condition on the subject arc that blocks it if the object is alreadyset.
All three transitions will be blocked if they have already been followed once.
If thelanguage requires at least a verb, but possibly no subject or object, then the POP arc willhave a condition that requires a verb.3.1.3 ATN Transformations.
As I said, RTNs and ATNs permit transformations that sharecommon parts of rules without changing the constituent structure.
This sharing canreduce the number of configurations that have to be enumerated and explored by aparsing algorithm.
If you are willing to change the constituent structure assigned bythe grammar and accept a weakly equivalent grammar (one that accepts the samestrings, but doesn?t necessarily assign them the same structures), then you can obtaineven more efficiency.
For example, you can replace any nonterminal symbol with thetransition network that defines it, as long as the symbol doesn?t itself occur in its owntransition diagram.
If you do this until you can?t do it anymore, you get a grammarwhose only non-terminals are recursive symbols.
This gets more of the grammar into thestate transition diagram, where it can be optimized by standard finite-state optimizationprocedures.Further, you can replace any left and right recursive symbols with jump transitionsand turn these recursions into iterations.
This is illustrated in Figure 7.
First you elimi-nate left recursion by removing the transition that pushes for S from state S and addinginstead a jump transition from each final state (in this case, state 5) to the state that theleft-recursive transition went to (in this case, state 3).
These two operations are markedwith (1) in the figure.
Then you eliminate right recursion by removing any transitionthat pushes for an S to reach a final state and replacing it with a jump transition thatgoes back to the start state.
These two operations are marked (2) in the figure.This gets even more of the grammar into the transition diagram where you canoptimize it.
In fact, one can argue that this gets as much as possible of the grammarinto finite-state machinery.
If you have done all of these transformations, then youhave a grammar whose only nonterminal symbols are self-embedding recursive sym-bols.
There is a theorem, called the Chomsky?Schu?tzenberger Theorem, that showsself-embedding recursion to be the essential element that distinguishes context-free610Woods The Right ToolsFigure 7Turning recursions into iterations.languages from finite-state languages.
Specifically, it shows that a language is properlycontext-free, as opposed to finite-state, only if every context-free grammar for it con-tains at least one self-embedding recursive nonterminal.So by these transformations, we can essentially move as much of the grammar aspossible into finite-state machinery, where it can be optimized with finite-state opti-mization algorithms.
Figure 8 illustrates the result of applying these transformationsfollowed by this optimization.3.1.4 Earley?s Algorithm.
The same year that my ATN paper was published in CACM, JayEarley?s algorithm was also published in CACM (Earley 1970).
When Earley?s algorithmcame out, I was very excited by it.
Originally designed for extensible programming lan-guages, Earley?s algorithm had the remarkable property that not only did it parse anycontext-free grammar in at most n-cubed time, but it automatically achieved the bestknown computation bounds on all of the subclasses of context-free grammar that wereknown to have time bounds less than n-cubed.
By indexing things carefully, and doingjust what was needed, Earley?s algorithm automatically achieved n-squared boundson linear grammars and unambiguous grammars and linear bounds on deterministicgrammars, without having to be told what kind of grammar it was being given.
Prior toEarley?s algorithm, special parsing algorithms were used to obtain those tighter bounds.Figure 8A highly optimized RTN equivalent to Figure 6.611Computational Linguistics Volume 36, Number 4Earley?s algorithm achieved its basic n-cubed result by getting rid of the stack usedby pushdown-store automata, and instead leaving the equivalent of the stack partiallyrepresented and threaded through a network of ?triples?
stored in columns correspond-ing to positions in the input string.
It achieved the tighter n-squared and linear boundsby indexing the triples in such a way that no unnecessary searching was required at anypoint.
Earley kept track of progress through a rule with what he called ?dotted rules.
?These were copies of the original context-free grammar rule with a dot somewherein the right-hand side to mark the position in the rule that the parser had reached sofar.
A dotted rule with the dot at the beginning of the right-hand side represented thebeginning point for processing a rule, and one at the end represented a rule that hadbeen completely matched.
These dots marked the progress of the algorithm through therule.
The algorithm operated on triples consisting of the number of the rule, the positionof the dot, and the column where the current constituent was begun.3.1.5 Earley?s Algorithm and ATN Grammars.
It turned out that Earley?s algorithm was anatural for ATN grammars.
All you had to do was use the states from the ATN in placeof the dotted rule (i.e., the rule number and the position of the dot).
Looked at this way,the dotted rules are simply states in an ATN.
Earley?s three operations of prediction,scanning, and completion are simply the PUSH, CAT, and POP arcs of the ATN.
Earley?salgorithm can be applied directly to RTNs and to certain restricted classes of ATNs.Generalizatons of Earley?s algorithm can be applied to more general ATNs.By transforming a context-free grammar into an ATN, you can combine the com-mon parts of different rules and achieve a reduction in the number of states in theparsing computation.
Further, you can apply the transformations described earlier andproduce a very compact, minimally branching, minimal state diagram, such as the onein Figure 8, and apply Earley?s algorithm to that.
Figure 9 illustrates a normal Earleyalgorithm parse of the context-free grammar in Figure 6, and Figure 10 illustrates theequivalent parse for the optimized equivalent RTN grammar of Figure 8.
The result ofthe optimizations is a nearly deterministic parse that needs to consider only 12 stateconfigurations compared to 50 state configurations for the original.Figure 9An Earley parse of the grammar in Figure 6.612Woods The Right ToolsFigure 10The Earley parse for the grammar in Figure 8.Adapting Earley?s algorithm to more general ATN grammars is more complicatedthan for simple RTNs, and beyond the scope of this talk, but I?d be happy to discuss itwith anyone who is interested.3.1.6 Theoretical Properties of ATNs.
Viewed as automata, ATNs have a number of in-teresting theoretical properties.
For example, without some kind of restriction on theconditions and actions on the arcs, ATN grammars would be Turing complete.
Inparticular, any single action on an arc could be Turing complete.
This has the advantageof assuring us that whatever linguistic phenomena we encounter can be captured in thisformalism, but it could also make the outcome of an ATN computation semi-decidable.Clearly, as a linguist, you?d like your grammar formalism to be at least decidable.To address this issue, I was able to prove that, with certain restrictions, parsing asentence with an ATN grammar would be not only decidable, but primitive recursive.The only necessary restrictions were that the conditions and actions on the arcs beprimitive recursive, and there must be no arbitrarily repeatable chains of singleton self-embeddings or jump transitions that could be followed in loops without consuming anyinput.
Because any reasonable grammar would automatically satisfy these restrictions,these restrictions are effectively no restriction at all.
So, parsing with any reasonableATN grammar will be at worst primitive recursive.As automata that are capable of assigning deep structures to natural English sen-tences with a primitive recursive algorithm, ATNs answered a theoretical questionabout the computational complexity of natural language parsing that was concerningsome transformational grammarians at the time.
Noam Chomsky wanted the transfor-mational grammar formalism to express exactly the computational power required bythe human language facility, which he assumed and wanted to be weaker than a Turingmachine.
But the transformational grammar formalism as he invented it was shown tobe Turing complete, and even when constraints were added to try to avoid this, JoyceFriedman was able to prove that the result was still Turing complete.So showing that the kinds of transformational accounts of English syntax thatthe transformational grammarians were developing could be expressed in an ATNformalism that was less powerful than a Turing machine and whose computationswere decidable provided an answer to this question about the necessary computationalpower of the human language facility.Of course you?d like your parsing algorithm to be much more efficient than anarbitrary primitive recursive computation, and in fact you?d like it to be better thanexponential in the length of the input string.
Applying Earley?s algorithm to ATNsshows that there are at least subclasses of ATN that are n-cubed, n-squared, and evenlinear.613Computational Linguistics Volume 36, Number 43.1.7 Applications.
ATNs have been applied to a variety of problems.
I originally devel-oped them in the context of a question-answering system for airline flight schedulesand then applied that framework to answering questions about states, edges, andpaths in ATN grammars.
The same grammar and interpretation framework could easilyhandle constructions from either domain, and in fact could handle the union of the twodomains.
You could ask this system things like the following:(9) Does American have a flight from Boston to Chicago?
(10) Is there a Jump arc from state S/ to S/NP?The second of these was actually asking about the very grammar used to parse both ofthem.Moon Rocks.
The first major test of the ATN grammar formalism was the LUNARsystem that my colleagues and I developed at BBN for the NASA Manned SpacecraftCenter to answer questions about the Apollo 11 moon rocks (Woods, Kaplan, and Nash-Webber 1972).
Bonnie Webber and Ron Kaplan were part of that team, among others.The LUNAR domain was interesting in that it involved fluent natural English with atechnical vocabulary.
The lunar scientists even had some linguistic constructions thatone doesn?t encounter in ordinary speech.
They thought of samples as partitioned into?phases?
and would ask for things like:(11) nickel concentrations in the olivine phase in sample S10046Here, the noun olivine is actually a parameter to the operator phase, occurring as aprenominal modifier.
Parameters to such operators are more commonly presented aspostnominal prepositional phrases.Other examples of questions LUNAR could handle are:(12) In how many samples has apatite been identified?
(13) Give me analyses of the olivine phase of each breccia.
(14) What is the average Be concentration in breccias?
(15) What are the Be concentrations in each breccia?
(16) What samples contain silicon and do not contain sodium?One of the interesting things about this domain was that many of the common Englishfunction words were also the names of chemical elements (As, At, Be, I, In, and He)as were the common names Al and Mo.
This put a burden on the ATN to deal withthe ambiguity of these terms.
(LUNAR was implemented at a time when computerinput was typically all upper case, so capitalization was not available as a clue.)
In thisdomain, these words could not be used as reliable syntactic anchors as they were inmany other grammars of the time.In addition to parsing sentences, the ATN parser in LUNAR could be run in a modein which it would keep track of every step of the parse and could display a browsabletree structure of the entire parse search space.
At any point in this tree, you couldreenter the parsing process and follow the computation step-by-step from that point.This facility was invaluable for debugging grammars and developing an ATN grammar614Woods The Right Toolswith large coverage.
Tools such as this, which can help a developer understand what asystem is doing, can be extremely important.Continuous Speech Understanding.
After the LUNAR project, BBN became involved inthe first DARPA speech understanding project (1971?1976).
In 1971, no one had anyidea how to do speech understanding.
Pierre Vicens and Raj Reddy had done a tinypilot using Terry Winograd?s blocks-world domain to show that high level constraintsfrom syntax and semantics could make up for ambiguity at the acoustic level.
To dothat, however, they restricted themselves to sentences containing the word block.
Thealgorithm knew that and searched for that word as an anchor.
The word block was agood anchor because it begins and ends with plosives, which show up as ?notches?
ina spectrogram and are thus easy to locate.
Clearly that algorithm wouldn?t generalize.I organized a workshop at Harvard that pulled together experts in acoustics, signalprocessing, speech recognition, linguistic phonology, computational linguistics, andartificial intelligence, and we shared what we knew.
The outcome of this conferenceand other discussions led to the ?Newell Report?
(Newell et al 1973), on the basis ofwhich DARPA decided to fund a program in Continuous Speech Understanding.
BBNwas one of the contractors, John Makhoul was our speech and signal processing expert,and I was the principal investigator and language expert.
Other participants were LynnBates, Geoff Brown, Chip Bruce, Craig Cook, Jack Klovstad, Bonnie Webber, RichardSchwartz, Jerry Wolf, and Victor Zue.
Dennis Klatt consulted on the project.
Ultimately,we produced a system called HWIM, for Hear What I Mean (Woods et al 1976).
HWIMunderstood sentences in the context of an interactive trip-planning and travel-budget-management system we called TRIPSYS.One of the things we discovered about speech was that people generally don?tpronounce all of the words in a sentence with equally clear articulation.
In unstressedparts of a sentence, words are typically pronounced with little effort and their acousticrecognition is less reliable.
In stressed portions of the utterance, words are more clearlypronounced and can be more reliably recognized.
This led to an algorithm I developed,called the shortfall density algorithm.
This algorithm could work outward from theseislands of reliability until they collided with each other and could be combined intolarger islands.
This posed an interesting challenge for the syntactic component of thesystem, which was required to judge whether an arbitrary sequence of words could beextended to a complete well-formed sentence and, if possible, to predict the classes ofwords that would be compatible with such extensions at each end of the island.I developed a parsing algorithm that could take an ATN grammar, index it by theconstituents on the arcs of the grammar, and for any word that was hypothesized in anutterance, find all of the arcs in the grammar that could use that word.
I could then test,for any two adjacent words, which of the arcs compatible with the first word could beconnected to one or more of the arcs of the second by some combination of pushes, pops,and jumps.
I could then combine sequences of words into islands and record a compact,factored record of all of the possible paths that the grammar could follow through thatsequence of words.
The arcs at the ends of these islands could be used to predict theclasses of words that the lexical recognizer should look for at those ends.
When a wordwas added to one end, the algorithm could propagate constraints through the island tothe other end to possibly narrow the predicted possibilities at the other end.For the internal structure of this ?island?
parse, I used a trick similar to Earley?sto leave the details of a chain of pushes unspecified until needed.
However, I did it forboth pushes and pops and for jumps as well.
A resulting path through the island had thestructure of a ?stile?
(a set of steps used to cross a fence by going up one side and down615Computational Linguistics Volume 36, Number 4the other).
At the top was a word or word sequence, which in general could be poppedto by something on the left and could push for something on the right.
The details ofwhether that pop or push was direct or indirect were left unspecified until more contextwas available to determine the answer.
Figure 11 gives an example of such a stile pathcovering a fragment.
The wavy lines in the figure represent the indefiniteness of thedetails of a pop or push transition.
Although I developed this algorithm in the contextof speech, I also considered it useful for processing ill-formed utterances.
If someonemakes an error in an utterance such as doubling or dropping a word or inserting thewrong word at some point, an island-driven parser can parse the rest of the utteranceinto partially overlapping islands and look for places where a small change could makea connection in or around the overlap area.
The shortfall density algorithm could evenbe used to rank the likelihoods of alternative repairs and find the most likely correction.One of the interesting experiments that we never got to complete with the HWIMsystem was to annotate the transition network grammar with information aboutprosodic signatures that should be present if certain syntactic transitions are followed.For example, the sentenceHave any people done chemical analyses on this rockwas misheardby one version of our system as Give any people done chemical analyses on this rock, whichastoundingly parsed and passed all of the semantic and pragmatic filters.
One parsewas equivalent to Give [me] any people-done chemical analyses on this rock.
However, if thatwas the correct parse, then there should have been a very salient prosodic signatureon the phrase people-done, and the main verb give should have been stressed.
Becauseneither was the case for the given utterance, this false interpretation would have beenrejected, and the correct interpretation would have been found.
We got as far as makingthe annotations in the grammar, but had not yet been able to test them when the projectended.Conversational Discourse Structure.
One of my students, Rachel Reichman, used an inter-esting kind of ATN to express models of discourse structure she called context spaces(Reichman 1981).
Reichman studied real conversations and found consistent, regularFigure 11A stile path covering a fragment.616Woods The Right Toolsstructure above the sentence level across a variety of kinds of discourse and varioustopics.
She was able to use the conditions and actions on ATN arcs to characterize howdiscourse moves depended on context and could track focus, topic, and role throughthe discourse.
She could even detect when a speaker switched sides in a debate.Unlike other discourse grammars, which tend to think of discourse structure interms of sequences of categories, Reichman could use the conditions on the ATNtransitions to capture discourse moves such as ?Speaker 2 utters a declarative sentencethat is truth-functionally inconsistent with the previous statement by speaker 1?.
Shecalled this transition a ?challenge.?
The content in these discourse ATNs was all in theconditions.
The category labels on the transitions played a very minor role, if any.One of Reichman?s contentions was that there is a structure of possible moves indiscourse that is orthogonal to the intentions of the speaker, and from which the speakerhas to choose, just as a chess player chooses possible moves from among the legal movesof chess.
In this way, her work complemented that of Perrault, Cohen, Allen, and Bruce,dealing with the beliefs, desires, and intentions of speakers and hearers.
It seemedto me that her work also provided a missing link for David McDonald?s low-leveltactics of sentence generation.
Reichman?s discourse ATNs could provide the trackingmechanism for the parameters that McDonald?s system required to decide things likewhether to passivize a sentence or not.
Reichman?s discourse ATNs could also track the?reference times?
required for Reichenbach?s account of perfect and progressive tenses.It had previously been a mystery to me where these parameters were expected to comefrom.Other Applications.
ATNs and related automata have been applied to a variety of otherproblems: ATN grammars, without a computer, have been used for field linguisticsto record the evolving understanding of grammars of native languages(Joseph Grimes, personal communication). ATNs were used by Joyce Friedman and David Warren to represent thegrammar of Montague?s Proper Treatment of Quantification in English(PTQ) and thus provide a parsing algorithm for Montague Grammar(Friedman and Warren 1978). ATNs were used for other kinds of perception.
For example, ATNs wereused to recognize patterns of chromosomes (Chou and Fu 1975).3.2 Generalized Transition NetworksA Generalized Transition Network (GTN) is a generalization of an ATN that can beused for generalized perception.
In GTNs, the transitions are augmented with actionsof sensory perception that can be directed to any point in a perceptual space, replacingthe ATN?s implicit parsing of sequences of constituents in left-to-right-order.
Statetransitions in GTNs represent moving to more complete states of knowledge as aresult of additional measurements, without any assumption that those measurementsare being applied to a provided sequence of inputs.
In a GTN, the results of previousmeasurements can determine where the next measurement is to be taken.
Students inmy classes at Harvard used GTNs to recognize basic strokes in images of cuneiform617Computational Linguistics Volume 36, Number 4tablets and gliders and other basic patterns in simulations of cellular automata runningConway?s game of Life.3.3 Cascaded ATN GrammarsA Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperating sequence of ATNtransducers, each feeding its output to the next stage.
One example of a cascade ofATNs arose in the speech understanding project.
The lexical analyzer in HWIM knewabout coarticulation effects between phonemes that could reduce the pronunciation ofa phrase like hand label by suppressing the d in the context between n and l. This wascaptured by an algorithm that could be viewed as an ATN transducer that nondeter-ministically transcribed a sequence of input phonemes into a sequence of words.
In thistransducer, the transmission of the words and the consumption of the input phonemescould be somewhat asynchronous.
The ATN transducer needed to consume the l of thenext word label, before it could confirm the appropriate context for the missing d in hand.So by the time it transmitted the word hand, it was already working on the input for thenext word label.
Registers in the ATN could remember the hypothesized current wordwhile the ATN entered a state that dealt with the coarticulation effects, which couldbe shared by all words ending in nd.
Then when the confirming l was consumed, theremembered word was transmitted.Another example of an ATN cascade would consist of an ATN transducer to recog-nize syntactic structure, followed by an ATN that performs a semantic interpretation.Subsequent stages could do pragmatic interpretation, discourse structure, and planrecognition.
Cascading provides benefits similar to sequential decomposition of finite-state machines: It reduces the overall combinatorics and produces a simpler model.
Byfactoring these stages apart, we can reduce the extent to which phenomena have tobe learned and recorded multiple times in different contexts.
For example, we don?thave to learn the basic structure of a noun phrase, including the English system ofquantifiers and determiners, repeatedly, for all of the different semantic classes of headnoun that we might want to distinguish.
If we?re running a machine learning algorithm,this means that we don?t have to encounter examples of all of these combinations inorder to learn the correct model.
Also, evidence for syntactic structure will be pooledover different semantic variations and vice versa.
Finally, what we learn for the syntacticstage can carry over to other domains and other applications.Separating syntactic and semantic processing into two stages of a cascade can alsooptimize the flow of information between the two.
For example, when parsing a nounphrase, the syntactic stage can parse the determiner and any adjectives up to the headnoun before transmitting anything to the next stage.
Then it can transmit the headnoun and any premodifiers.
This allows the semantic interpretation stage, which islargely head-driven, to start its processing with the most important information.
Thenthe syntactic stage can parse any subsequent PPs or other postnominal modifiers andtransmit them, followed finally by transmitting the determiner information.
After thesemantic stage has interpreted the meanings of the head noun and its modifiers, it canshare the portion of the semantic stage that analyzes determiners.4.
FactoringThis discussion of cascading illustrates a principle I call factoring: the sharing of com-mon parts of grammars and hypotheses.
Factoring is a key element of what makes ATNsefficient, both for grammar development and for syntactic processing.
I distinguish two618Woods The Right Toolskinds of factoring: conceptual factoring and hypothesis factoring.
Conceptual factoringis the sharing of common parts of grammars or other formal models to help a linguistor a grammar-learning algorithm to capture generalities.
Hypothesis factoring is thesharing of common parts of alternative hypotheses that arise in parsing, perception, orlearning.
Hypothesis factoring helps a parsing algorithm or learning algorithm operatemore efficiently by generating fewer cases that need to be considered independently.Earley?s trick to avoid enumerating different alternative stack contexts while processingthe transitions in a constituent is an example of factoring, as is my generalization of thistrick to handle middle-out island parsing for speech.
It?s also the purpose of the RTN op-timization algorithm I described earlier, and one of the principal benefits of cascading.Usually factoring transformations that merge the common parts of grammar rulesprovide both kinds of factoring benefits, although occasionally these two principlescould compete with each other.
In general, one should look for conceptual factoring is-sues at the level of how a linguist might interact with a formalism to understand modelsand theories.
Hypothesis factoring optimizations can then be applied behind the sceneby compilers that transform models into efficient structures for processing.
For example,if a linguist prefers to formulate grammars as sets of rules, these could be transformedto ATNs behind the scene and optimized to obtain efficient parsing algorithms.
I seemto recall that Martin Kay compiled some of his chart-parsing algorithms into somethingvery similar to ATNs before parsing.5.
Procedural SemanticsI want to turn now to the topic of Procedural Semantics.In a term paper that led to my thesis on ?Semantics for a Question-AnsweringSystem?
(Woods 1967, 1979), I proposed procedural semantics to resolve the dilemmaI had about how to get a computer to answer questions about a database.
Accordingto my dictionary, ?semantics?
was the relationship between signs and symbols and thethings they denote or mean.
So what, I wanted to know, is meaning?
For my application,semantics had to be more than just assigning features and calling them semantic, ordrawing diagrams and calling them semantic networks.
I wanted to know what wecould store in our presumably finite brains that could conceivably play the roles weattribute to meanings.
The best I could find in the philosophy literature was a quotefrom Carnap: ?To know the truth conditions of a sentence is to know what is assertedby it?in usual terms, its ?meaning?.?
The only thing I knew that could finitely representthe truth conditions of a sentence (which is an infinite set of assignments of truth valuesto propositions in all possible worlds) was some form of procedure: a Turing machine,a Post production system, or a computer program.The idea of procedural semantics is that the semantics of natural language sentencescan be characterized in a formalism whose meanings are defined by abstract proceduresthat a computer (or a person) can either execute or reason about.
In this theory: The meaning of a noun is a procedure for recognizing or generatinginstances. The meaning of a proposition is a procedure for determining if it?s true. The meaning of an action is the ability to do it or to tell if it has been done.This theory can be thought of either as an alternative to the standard Tarskian semanticsfor formal systems or as an extension of them.
The idea is that the computational619Computational Linguistics Volume 36, Number 4primitives consisting of represented symbols, the ordered pair, the assignment of valuesto variables, conditional branching and iteration, addition and subtraction, and thesubroutine call, together with sensorimotor operators that interact with a real world,constitute a stronger (and more well-understood) foundation on which to build a theoryof meaning than does set theory and the logical operations of universal and existentialquantification over an all-inclusive infinite universe.
These latter theories have theirown paradoxes and incompleteness issues, which we usually ignore, due to familiarity.For more about the limitations of classical logic alone as a basis for meaning, see Woods(1987).A procedural semantics foundation allows for a definition of the standard logicaloperators as well as extensions of them to deal with generalized quantifiers, as well asquestions and imperative operations.
Building on computational primitives, like theordered pair and conditional iteration, seemed to me to be at least as well understood,and more grounded, than building on the logical operations of universal and existentialquantification over an infinite universe and then having to make some kind of extensionto handle the meanings of questions and imperatives.
Moreover, because proceduralspecifications can be installed in a machine, where they can be physically executed,they can interact with sensors like keyboards and cameras and with output devices likeprinters and manipulators, so that this approach allows meanings that actually interactwith a physical world, something that no previous theory of meaning had been able toachieve.5.1 Semantics for a Question-Answering SystemIn the case of question answering, procedural semantics allows one to decouple theparsing and semantic interpretation of English sentences from the details of the storageand representational conventions of the information in the database.
The proceduralsemantics approach allows a computer to understand, in a single, uniform way, themeanings of conditions to be tested, questions to be answered, and actions to be carriedout.
Moreover, it permits a general-purpose system for language understanding to beused with different databases, and even combinations of databases that may have differ-ent representational conventions and different data structures, and it allows questionsto be answered by results that are computed from the data in the database without beingexplicitly stored.In ?Semantics for a Question-Answering System?
(Woods 1967), I formulated a the-ory of procedural semantics and a methodology for assigning semantic interpretationsto parsed sentences of English.
This amounted to compiling the English sentence into anequivalent abstract procedure, expressed in a meaning representation language (MRL)that was an extension of conventional predicate calculus notations.
This MRL extendedthe logical inventory with commands and actions in addition to propositions, and itintroduced generalized and typed quantifiers.
The range of quantification for a typedquantifier was specified by an abstract procedure for enumerating elements of the class,and the class could be parametrically specified.This can be illustrated by an example from the LUNAR system discussed earlier.The semantic interpretation of the questionWhat is the average concentration of Aluminumin each breccia?was as follows:(17) (FOR EVERY X5 / (SEQ TYPECS) : T ;(PRINTOUT (AVGCOMP X5 (QUOTE OVERALL) (QUOTE AL2O3))))620Woods The Right ToolsThis semantic interpretation can be read as follows:For every x5 in the class TYPECS such that the universally true condition T is true, printout the value computed by the averaging function AVGCOMP for the sample x5 for theoverall concentration of Al2O3, where TYPECS is the name used in the database for?Type-C rocks?
(that?s how breccias are encoded in the database) and Al2O3 is thechemical name for Aluminum Oxide (which is how Aluminum is stored in thedatabase).Note that answering this question involves the computation of an average that was notexplicitly stored in the database, and that the grounding for the semantics of terms suchas Aluminum and breccia is in a database whose structure and content and encodingconventions were previously and independently defined.LUNAR was influential not only as an example of a successful natural languagequestion-answering system and an exemplar of ATN parsing and procedural semantics,but as a stimulus for a lot of subsequent intellectual development.
For example, RonKaplan built on the things he learned from working with the LUNAR ATN grammarto develop his formalism of Lexical Functional Grammars, which in turn influenced thedevelopment of Head-driven Phrase Structure Grammars.
Danny Bobrow proposed theidea of spaghetti stacks (Bobrow and Wegbreit 1973) in response to my description ofa Lisp feature that would make it easier to implement ATN parsers.
Spaghetti stacksare now used to implement continuations in modern Lisp dialects such as Scheme.And many commercial systems have followed LUNAR as a model when developingquestion-answering systems for practical applications.5.2 Formal Meaning Representation LanguageLUNAR?s meaning representation language was an extension of the Predicate Calculuswith generalized quantifiers and imperative operators.
Some examples of the schematafor these quantifiers and operators are:(18) (FOR <quant> <vbl> / <class> : <condition> : <command>)(19) (FOR <quant> <vbl> / <class> : <condition> : <condition>)(20) (TEST <condition>)(21) (PRINTOUT <designator>)For example, Example (18) above can be read ?For <quant> <vbl> in the class <class>such that <condition> is true, do <command>.?
This is the schema for a quantified com-mand.
Example (19) can be read: ?For <quant> <vbl> in the class <class> such that thefirst <condition> is true, the second <condition> is also true.?
This is the schema for aquantified proposition.
The quantifiers <quant> in these schemata include not only thetraditional universal and existential quantifiers EVERY and SOME, but also nontraditionalquantifiers like THE and (MORETHAN <number>) and a generic quantifier GEN which corre-sponds to the use in English of undetermined noun phrases with a plural noun (e.g.,birds have wings).
The paradigm can accommodate numerical quantifiers as exotic asan even number of or a prime number of and all sorts of typically, often, rarely, and ?unless-contradicted?
probabilistic quantifiers as well.621Computational Linguistics Volume 36, Number 4The schema (TEST <condition>) is a command to test a condition and print out Yesor No according to the result, and (PRINTOUT <designator>) is a command to print out aname or description of the referent of the specified designator.The fact that LUNAR?s meaning representation language uses typed quantifiers andprovides for additional restrictions on the range of quantification permits a uniformtreatment of different quantifiers and different kinds of head nouns and modifiersin English noun phrases.
For example, Some tall men play basketball has the followinginterpretation:(22) (FOR SOME X / MAN : (TALL X) ; (PLAY X BASKETBALL))The sentence All long flights to Boston serve meals has the following interpretation:(23) (FOR EVERY X / (FLIGHT-TO BOSTON) : (LONG X) ; (SERVE-MEAL X))If we try mapping English directly to classical logic, we need different treatments fornoun phrases with universal versus existential quantifiers.
For example, for Some tallmen play basketball, the three predicates (MAN X), (TALL X), and (PLAY X BASKETBALL) areall conjoined under the quantifier (for some x, x is a man and x is tall and x playsbasketball), whereas for All tall men play basketball, the first two conditions would beconjoined as the antecedent of an implication whose consequent is the third condition(for all x, if x is a man and x is tall, then x plays basketball).
For nonstandard quantifiers,such as the or rarely, mapping to classical logic is different still.5.3 Reasoning with MeaningsThe procedural semantics framework allows procedural interpretations to be treated intwo ways.
In the simplest way, the semantic interpretation is simply executed as a pro-gram to compute an answer to a question.
However, in a more general case, the systemcan take the interpretation as an object to be reasoned about and possibly modified.
Forexample in the following query from my Airline flight schedules application:(24) (FOR EVERY X / FLIGHT :(AND (OPERATOR X AA) (CONNECT X BOSTON CHICAGO)) ;(PRINTOUT X))the system can reason that it could get the same result more efficiently by using a specialenumeration function that uses a database index to enumerate only the flights that gofrom Boston to Chicago to specify the class of the quantifier, resulting in:(25) (FOR EVERY X / (FLIGHT-FROM-TO BOSTON CHICAGO) :(OPERATOR X AA) ; (PRINTOUT X))This is an example of what I called ?smart quantifiers,?
quantifiers that apply reasoningto their range of quantification and any specified filters on that range to see if there aremore efficient ways to enumerate the same effective range.There are lots of other reasons why a system might want to reason about the inter-pretation of a request before acting on it: it may not be appropriate to take literally, orthere might be some additional helpful actions to take, or the request might be estimatedto be unduly expensive, or there may be a more efficient way to do it, or the user may622Woods The Right Toolsnot be authorized to do what is being requested, or the system may have reasons not todo the thing that is requested, and so forth.
So, we would like semantic representationsto be useful both for execution as a procedure and as objects of mechanical reasoning.5.4 Answering Questions about Paths in an ATNWhen I first implemented my semantic interpreter for the airline flight schedules do-main, I had no actual database of flight schedules (other than a paper copy of theOfficialAirline Guide).
This was before there existed a multiplicity of on-line databases.
What Idid have, though, was my implemented ATN parser with its ATN grammar, whichconsisted of a set of states with transitions to other states.
So to test out my theory,and to illustrate that I could apply the theory to a database whose structure had beenpreviously determined, I decided to make the ATN grammar itself the object of myrequests.
I wrote semantic interpretation rules to allow me to ask questions like Is therea jump arc from S/ to S/NP?
or Is there a non-looping path from S/NP to S/POP?.
These rulescoexisted with my rules for airline schedules and both shared the same grammar fornatural language input.
One could ask the system Is there a connection from S/NP to S/V?or Is there a connection from Boston to Chicago?
and the semantic rules would disambiguatewhich domain was intended.Being able to query paths was especially interesting, because they didn?t actuallyexist as objects in the grammar, but had to be constructively enumerated by the enumer-ation functions for the path class.
Moreover, because there were a potentially infinitenumber of such paths, the formulation of quantifier classes defined by generatorsturned out to be essential.
I used smart quantifiers to infer when I could use specializedgenerators for non-looping paths, paths rooted at a given start state, paths between twoend points, and so on.
I used a resolution theorem prover to prove that the conditionsfor one of my specialized generators were implied by the filters on the quantifier,and then used instantiated variables from this proof to provide the parameters forthe resulting generator.
Because I was only using the theorem prover to seek a moreefficient generator for the paths, if the theorem prover took longer to come up with ananswer than an estimate of how much would be saved by using the result, then it wouldabandon the effort and use the original quantifier.5.5 Procedural Semantics in LUNARWhen Jeff Warner approached me from NASA about answering questions about theApollo 11 moon rocks, I already had the machinery in place for parsing and answeringEnglish questions and a rule-based system for adding more domains.
All I had to do wasbuild a dictionary for the new vocabulary, write the appropriate semantic interpretationrules, oh, and by the way, implement a database system, import their data into it, writethe procedures to define the semantics, and do all this in 256 K words of memory on aPDP-10!
The PDP-10 in question was a DEC machine that BBN had modified to supportvirtual memory, multiple forks, and time-sharing.
I was programming in Lisp and mylanguage-processing program took up one 256-K fork all by itself, so Danny Bobrowenlisted Lisp wizard Alice Hartley to add the ability for Lisp to create and controlsubforks, and I implemented the database and database retrieval programs in a sepa-rate fork.At NASA, Jeff Warner had extracted all of the data from the conference proceed-ings of the First Annual Lunar Science Conference, normalized the units, and cross-referenced the data with the articles from which it came.
He could generate answers623Computational Linguistics Volume 36, Number 4to questions by having his Fortran programmer write programs to query this database.He wanted to know if he could get his Fortran programmer out of the loop.
He hadcollected a set of queries to illustrate the kinds of questions he wanted to be able toanswer, and a vocabulary of 3,000 words that included all of the chemical elements, alarge vocabulary of mineral names, and every word that appeared in the proceedingsof the First Annual Lunar Science Conference.
For each word, he had recorded itsmost common syntactic category in English.
I had to take this list and expand it withadditional categories for ambiguous words and add in syntactic and semantic featuresand semantic interpretation rules for the words that needed them.The procedural semantics approach worked fine, and I learned some interestingthings about semantics and quantifiers from this experience (Woods 1978).
One of theinteresting discoveries was how the ?average?
operator interacted with quantificationand treated the generic quantifier differently from a quantifier like each.
If the quantifieris each you get a separate average for each value, but if the quantifier is generic or all,the average is computed over all of the values.
LUNAR answered 78% of the queriesasked of it at the Second Annual Lunar Science Conference, and 90% of those queriesfell within its scope.
However, LUNAR was far from being a complete solution.
If youasked LUNAR, What is a breccia?, it would reply S10046.
S10046 was indeed a breccia,and LUNAR was programmed to give you what you asked for.
If you asked it Whatis S10046?, it would reply S10046, since that was a sample that was equal to S10046.LUNAR simply found referents of referring expressions and gave you their names.
Ithad no model of the purpose behind the user?s question or of different kinds of answersfor different purposes.6.
Knowledge and LanguageI want to shift now and talk about knowledge.It doesn?t take much thought to realize that background knowledge and contextualknowledge is essential to language interpretation.
For example, knowledge is necessaryto resolve the ambiguity (and get the joke) in Groucho Marx?s famous Time flies likean arrow (but fruit flies like a banana).
Here, both syntactic and semantic ambiguity areresolved by knowing that there are fruit flies, but not time flies, that arrows fly butbananas don?t, and that flies can be used metaphorically for moving swiftly.Consider a spoken utterance that could be segmented either as his wheat germand honey or his sweet German honey.
One would need to know something about thecontext of this utterance in order to venture a prediction as to which interpretation wasintended.
Here, we need knowledge even to know what words we are hearing.In the HWIM system, the sentence Show me Bill?s trips to Washington was misheardas Show me Bell?s trips to Washington in the context of a travel planning system that knewtravel plans for a group of people that included Bill Woods and Alan Bell.
There is a min-imal difference of one phoneme between these two sentences (one letter in the writtenorthography), and there is only one feature difference between these two vowels.
Theacoustic scores of the two hypotheses were virtually identical, and the correct choicehappened to come second.
However, the system could easily have resolved the choiceby using a semantic interpretation to check the trip database to learn that Bill Woodswas scheduled to go to Washington, while Alan Bell was not.I once called home on the telephone and asked my young son, who answered, Isyour mother there?
He said, Yes.
I said Can I speak to her?
He said, Yes.
Finally, I said, Tellher to come to the phone.
OK, he said.
We know that interpreting speech acts depends onbeliefs, desires, and intentions, but how do we manage and acquire all of the knowledge624Woods The Right Toolsit takes to correctly infer those beliefs and desires and intentions and do so efficiently atthe right time?6.1 Requirements for Knowledge RepresentationWe need a system that can organize and use large amounts of world knowledge andfacilitate the efficient associative access to that knowledge during the analysis of sen-tences.
My experiences in a variety of natural language applications have convincedme that understanding and using knowledge is the bottleneck in both speech andnatural language processing (Woods 2007).
A key problem is how to find the pieces ofknowledge relevant to a problem from among all of the knowledge in a large knowledgebase.We need a representation system that can satisfy two requirements:1.
It should be expressively adequate to represent all of the necessaryelements of natural language questions, commands, assertions, conditions,and designators.2.
It should be structured to support semantic interpretation, retrieval, andinference.6.2 Links and Logic, KL-One et alThe KL-One project at BBN (?1977?1983) attempted to develop a representation tomeet these conditions.
KL-One was a knowledge representation system developedas part of a research project on Knowledge Representation for Natural LanguageUnderstanding.
A number of people worked on this contract, including: me, MadelineBates, Rusty Bobrow, Ron Brachman, Bertram Bruce, Eugene Ciccarelli, Phil Cohen,Brad Goodman, Norton Greenfeld, Andrew Haas, Robert Ingria, David Israel, JackKlovstad, David McAllester, Ray Reiter, James Schmolze, Candace Sidner, Marc Vilain,Bonnie Webber, Martin Yonke, and Frank Zdybel.
The project began as an attempt todevelop a knowledge representation system suitable to represent and deliver all of theknowledge required for human-level reasoning (Sidner et al 1981).
In it, we sought tocombine the best features of two traditions:1. logical reasoning, which is rigorous and formal, but often counterintuitive,and which has algorithms that match expressions, substitute values forvariables, and invoke rules, and2.
associative networks, which are structured and intuitive, but typicallyinformal; however, they support efficient algorithms that follow pathsthrough links to draw conclusions.We wanted the associativity of link-based representations, in order to exploit efficientpath-following algorithms, but we also needed representations with a clean andwell-understood semantics.
A key element of our approach was based on RonBrachman?s thesis on ?Structured Inheritance Networks?
(Brachman 1977).
Ron wasone of my thesis students at Harvard, and his thesis arose from my challenge to figureout how to index material at the sentence level so that one could find where particularthings were said.
Structured inheritance networks not only related concepts to eachother by generality, but also aligned corresponding roles of those concepts.625Computational Linguistics Volume 36, Number 4One of the achievements of the KL-One project was the creation of a knowledgerepresentation system whose semantics were sufficiently well defined that an algorithmcould automatically place new concepts at the correct position in a conceptual taxon-omy.
I wrote the first algorithm to do this, which I called the MSS algorithm (for MostSpecific Subsumer).
This algorithm would automatically find the most specific conceptsin an existing taxonomy that subsumed a new concept (i.e., were more general than orequivalent to the new concept).
The new concept could then be added to the taxonomydirectly under those concepts.
An analogous algorithm, the MGS algorithm (for MostGeneral Subsumee), could find the most general concepts that were subsumed by thenew concept.
James Schmolze wrote subsequent ?classifiers?
for KL-One.KL-One began a wave of research in knowledge representation, inspired a vastnumber of complexity results, initiated the field now known as Description Logic, andspawned a family of related systems (Woods and Schmolze 1992).6.3 Understanding Subsumption and TaxonomyAlthough the original KL-One was focused on the structure of concepts, most of thesubsequent work it inspired adopted a declarative approach, based on first-order logic.In this work, subsumption was identified with logical implication and set inclusion.While most of this flurry of activity was going on, I was involved in a couple of startupcompanies and watching all this from the sidelines.
However, I felt that the declara-tive semantics approach had thrown out the baby with the bathwater by eliminatingall of the intuitions for how the structure of links can support efficient algorithms.I also felt that the extensional subsumption criterion was a mistake.
In 1990, duringan interim appointment at Harvard University, I started to revisit the original goalsof KL-ONE in light of where the field had gotten, under sponsorship from the KaporFamily Foundation.
I wanted a representational system that would be an efficient andprincipled methodology for organizing knowledge, and I came to focus on a differentcriterion for subsumption that I called ?intensional?
rather than ?extensional.?
Theresult was my 1991 paper on ?Understanding Subsumption and Taxonomy?
(Woods1991).The idea of intensional subsumption is that for one concept to subsume another,there must be a direct and recognizable relationship between the meanings of theconcepts.
It is not sufficient merely to have a set inclusion of their logical extensions.Because it takes a theorem to prove that context-free languages are the same set oflanguages as the languages accepted by a pushdown-store automaton, these two con-cepts must have different meanings, even though they have the same extension.
Bothmeanings are essentially procedural.
The first says that the language is accepted by acontext-free grammar.
The second says that the language is accepted by a pushdown-store automaton.
The proof involves showing that these two procedures produce thesame results.
If it takes this much reasoning to determine that one concept implies theother, then that?s not intensional subsumption.My definition of intensional subsumption was that each part of the more generalconcept subsumes some part of the more specific concept.
Thus [a man with a pet]subsumes [a man with a dog], because pet subsumes dog and a man is a man.
Bothof these subsume [a man with a dog and a cat].I was able to extend this notion of subsumption to include ?gap?
predicates, whichhave sufficient conditions and necessary conditions that are not equivalent, leaving anundefined gap in between.
So-called ?natural kinds,?
like [chair], which are supposedlynot definable, can often be modeled with such gap predicates.
For example, one can626Woods The Right Toolsspecify some necessary conditions for being a chair (you can sit on it, it was intended tobe sat on, it has a back), and some additional conditions that are collectively sufficientfor being a chair (four legs, one seat, one back), while leaving odd cases undefined(sitting on a log, in a crook of a tree).The idea of intensional subsumption proved to be both more expressive than theextensional semantics approach, and also computationally more tractable.
I was able toshow that under certain assumptions about the structure of a conceptual taxonomy, anew concept could be assimilated by an MSS algorithm in sublinear time (on the orderof the log of the size of the taxonomy).
The MGS algorithm was less well-behaved, butcould be expected to take this same sublinear time on the average.With this machinery in hand, I was able to return to KL-One?s original goals, seekinga knowledge representation structure to organize everything necessary for human-level reasoning.
My thesis is that we use a conceptual taxonomy, based on somethinglike intensional subsumption, to organize everything we know.
In it, we can recordwhat do do about different situations, efficiently find the most specific applicable rules,record rules for acquiring more information, record alternatives to consider, and recordpriorities and procedures for doing things.
An example of this is the famous Nixondiamond, shown in Figure 12.
The classic conundrum is based on the fact that Nixon isa Republican, and Republicans are usually Hawks, but Nixon is also a Quaker, andQuakers are usually Doves.
So which is Nixon?
Many AI researchers have felt thatsome inheritance principle should answer such questions.
My opinion is that the jobof the knowledge representation system is to identify the greatest-lower-bound concept[Republican Quaker] as the locus of the issue, classify it under both Republican andQuaker, classify Nixon under that, and propose that some other component figureout the political leanings of Republican Quakers (via a poll?).
Then for any futureperson, if they are classified under both Republican and Quaker, the MSS algorithm willplace them under the more specific concept [Republican Quaker] and they will inheritwhatever answer this poll recorded there.Figure 12The Nixon Diamond.627Computational Linguistics Volume 36, Number 46.4 A Practical Application of Conceptual TaxonomyIn 1991, I joined the new Sun Microsystems Laboratories in Burlington, Massachusetts,carrying with me my new theory of conceptual taxonomy.
My focus was to be onimproving search technology.
The rationale was that if we could improve the abilityto find specific information in text, it would have wide applicability.
The goal wasto understand phrases, handle paraphrase variations, find specific passages, and helppeople find specific information quickly.
In addition to these goals, I was also interestedin having a laboratory for gaining experience with subsumption technology on largepopulations of natural concepts.
The population in this case was all of the words andphrases extracted from unrestricted text.My Knowledge Technology Group at Sun Labs developed a search engine wecalled Nova that truly delivered on these goals.
It contained a universal lexicon andmorphology for general English that enabled me to apply Nova to any new sub-ject matter with no initial preparation.
It had a core lexicon of about 40,000 words,which we expanded by rule to a lexicon of approximately 150,000 word forms.
Theexpansion was based on a known word list and a set of approximately 1,200 mor-phological rules that could analyze an unknown word and produce a complete lexicalentry for future use.
These lexical entries included syntactic word categories, semanticfeatures, and preferences among word senses.
The lexicon knew semantic subsumptionfacts for approximately 20,000 words.
The indexer contained a scanning ATN grammarthat could extract basic phrases, which were then automatically classified into aconceptual taxonomy that was automatically created for each collection we indexed.In addition to finding specific passages, Nova allowed you to browse in the conceptualtaxonomy to get ideas about how to generalize a query and to understand whatparaphrases exist in the material that have already been covered by what you asked.One experiment showed a five times speedup in human search productivity usingNova, compared to conventional document retrieval technology.This search technology was incorporated into several Sun products and deployedinternally to search Sun?s e-mail archives.
Among the people who worked on thisproject that you might know are Phil Resnik, Paul Martin, and Peter Norvig.
SteveGreen and I, with help from Paul Martin, implemented the final Java version of thisproject.6.5 Generalized PerceptionSince working on Nova, I?ve been thinking about generalized perception at a levelthat subsumes natural language understanding, speech understanding, visual scenerecognition, and general situation awareness.
This is in some sense the opposite of textsearch.
It?s more like you have a huge taxonomy of queries and only one text.
When youare presented with a situation, you want all of the queries that it would satisfy to wakeup, and you?d like to be alerted to the most specific ones, which will in turn provide youwith information about what to do or expect in that situation.
The ?queries?
in this caseare the concepts in your taxonomy.
In addition, the taxonomy should serve as a kindof ?grammar?
that can analyze the elements of a situation and characterize how theyrelate.
Such a structure, I believe, is at the core of intelligent behavior, including naturallanguage use.Imagine such a conceptual taxonomy inserted as a stage in a CATN at the positionwhere semantic interpretation is to occur.
The taxonomy would find the most specificsubsumers of each partial interpretation as it accrues, and notify the earlier stage if the628Woods The Right Toolspieces don?t make sense.
This stage would turn phrases into concepts, relate them toother concepts, and provide associated information such as interesting specializationsand other elements to expect.
I?d like to find some good applications to explore theseideas in a context where the result could actually help people do things.7.
MethodologyBefore I close, I?d like to say a few words about a methodology I call Directed Research.This is how I approach problems, and I recommend it for your consideration.
The ideais to understand real problems that one would like to solve, and to do it with thestandards of the highest quality research.
This combines the best features of ?appliedresearch?
and ?basic research.?
I?ve always found it productive to look at the details ofreal problems.
Real problems often reveal issues that you wouldn?t think of otherwise.It?s important to look at the details.
Try to understand what would be necessary to solvethe whole problem.
At this point, don?t settle for approximations.
If you have a practicaljob to do, and it?s important to get it done quickly as well as possible, and you can onlydo that by partially solving the problem, then by all means do that.
That?s practicalengineering, and I do that with my Engineer?s hat on.
But that?s not going to advancethe science, and with my Scientist?s hat on, I?ll keep worrying the problem, trying todiscover what it takes to really do the job.It?s really useful here to have an arsenal of intellectual tools to try to fit to theproblem, but pay attention to the fit.
Don?t restrict yourself to existing tools, howevernice they are.
If the fit is not good, look for tools that can really do the job.
Modify oldones or invent new ones as necessary.
That?s my message about ?the right tools.
?I was asked how I decide to stop working on one problem and work on a new one.For me that is easy: Once I understand a technology well enough to know its strengthsand weaknesses and what it can and can?t do, I start working on the next problem.
Ialways have a queue of problems I want to work on, and I can usually find a matchbetween one of them and something that someone needs.8.
The Best Is Yet to ComeIn closing, I want to observe that there seems to have been an evolution in the thingsI?ve worked on that is moving closer and closer to the goal of truly effective person?computer communication.
We still haven?t begun to try some of the most interestingapplications of language and computation.
I?d like to make more progress on thefollowing goals: genuine interchange of knowledge between a person and a machine; cooperative problem-solving and decision making with a machine partner;and a machine that can understand what you need and respond appropriately.I believe that doing this will require integrating all of the tools I?ve just presented plussome machine learning and some extraction technology.
And I fully expect to have toinvent a few more tools as well.You can find links to some of my papers and follow my progress on my personalWeb page: http://parsecraft.com.Thank you.629Computational Linguistics Volume 36, Number 4ReferencesBobrow, D. G. and B. Wegbreit.
1973.
Amodel and stack implementation ofmultiple environments.
Communicationsof the ACM, 16(10):591?603.Brachman, R. A.
1977.
A Structural Paradigmfor Representing Knowledge.
Ph.D. thesis,Harvard University.Chou, S. M. and K. S. Fu.
1975.
Transitionnetworks for pattern recognition.
TechnicalReport TR-EE 75-39, School of ElectricalEngineering, Purdue University, Indiana.Earley, J.
1970.
An efficient context-freeparsing algorithm.
Communications ofthe ACM, 13(2):94?102.Friedman, J. and D. S. Warren.
1978.
Aparsing method for Montague Grammars.Linguistics and Philosophy, 2(3):347?372.Newell, A., J. Barnett, J. Forgie, C. Green,D.
Klatt, J. C. R. Licklider, M. Munson,R.
Reddy, and W. Woods.
1973.
SpeechUnderstanding Systems: Final Reportof a Study Group.
North-Holland,American Elsevier.Reichman, R. 1981.
Modeling informaldebates.
In IJCAI?81: Proceedings of the7th International Joint Conference onArtificial intelligence, pages 19?24,San Francisco, CA.Sidner, C. L., M. Bates, R. J. Bobrow,J.
Schmolze, R. J. Brachman, P. R. Cohen,D.
J. Israel, B. L. Webber, and W. A. Woods.1981.
Research in knowledgerepresentation for natural languageunderstanding: Annual report.
BBNTechnical Report 4785, Bolt Beranekand Newman, Inc., Cambridge, MA.Woods, W. A.
1967.
Semantics for aQuestion-Answering System.
Ph.D. thesis,Division of Engineering and AppliedPhysics, Harvard University.
Alsoavailable as Report NSF-19, ComputationLaboratory, Harvard University,September 1967.
Republished inGarland Publishing?s OutstandingDissertations in Computer Scienceseries, Garland Publishing, 1979.Woods, W. A.
1970.
Transition networkgrammars for natural languageanalysis.
Communications of the ACM,13(10):591?606.
Reprinted in Yoh-Han Paoand George W. Ernest (eds.
), Tutorial:Context-Directed Pattern Recognitionand Machine Intelligence Techniques forInformation Processing.
IEEE ComputerSociety Press, Silver Spring, MD, 1982.Also reprinted in Barbara Grosz, KarenSparck Jones, and Bonnie Webber (eds.
),Readings in Natural Language Processing,San Mateo, CA, Morgan Kaufmann,1986, pages 71?87.Woods, W. A.
1973.
Progress in naturallanguage understanding: an applicationto lunar geology.
In AFIPS ?73: Proceedingsof the June 4-8, 1973, National ComputerConference and Exposition, pages 441?450,New York.Woods, W. A.
1978.
Semantics andquantification in natural languagequestion answering.
In M. Yovits,editor, Advances in Computers.
AcademicPress.
Reprinted in Barbara Grosz, KarenSparck Jones, and Bonnie Webber (eds.
),Readings in Natural Language Processing,San Mateo, CA, Morgan Kaufmann,1986, pages 205?248.Woods, W. A.
1979.
Semantics for a QuestionAnswering System.
New York, GarlandPublishing.Woods, W. A.
1980.
Cascaded ATNgrammars.
American Journal ofComputational Linguistics, 6(1):1?12.Woods, W. A.
1987.
Don?t blame the tool.Computational Intelligence, 3(1):228?237.Woods, W. A.
1991.
Understandingsubsumption and taxonomy: Aframework for progress.
In John Sowa,editor, Principles of Semantic Networks:Explorations in the Representation ofKnowledge.
San Mateo, CA, MorganKaufmann, pages 45?94.Woods, W. A.
2007.
Meaning and links:A semantic odyssey.
AI Magazine,28(4):71?92.Woods, W. A., M. Bates, G. Brown, B. Bruce,C.
Cook, J. Klovstad, J. Makhoul,B.
Nash-Webber, R. Schwartz, J. Wolf,and V. Zue.
1976.
Speech understandingsystems: Final technical progress report,volumes i?v.
BBN Technical Report 3848,Bolt Beranek and Newman Inc.,Cambridge, MA.Woods, W. A., R. M. Kaplan, and B. L.Nash-Webber.
1972.
The lunar sciencesnatural language information system:Final report.
BBN Report No.
2378,Bolt Beranek and Newman Inc.,Cambridge, MA.
Available from NTISas N72-28984.Woods, W. A. and J. G. Schmolze.
1992.
TheKL-ONE family.
Computers & Mathematicswith Applications, 23(2-5):133?177.630
