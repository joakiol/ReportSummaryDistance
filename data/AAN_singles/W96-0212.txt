Figures of Merit  forBest-F i rst  Probabi l ist ic Chart ParsingSharon  A .
Caraba l lo  and  Eugene Charn iakBrown Un ivers i ty{ SO, ec}Ocs, brown, eduAbst rac tBest-first parsing methods for natural languagetry to parse efficiently by considering the mostlikely constituents first.
Some figure of merit isneeded by which to compare the likelihood of con-stituents, and the choice of this figure has a sub-stantial impact on the efficiency of the parser.While several parsers described in the literaturehave used such techniques, there is no publisheddata on their efficacy, much less attempts to judgetheir relative merits.
We propose and evaluateseveral figures of merit for best-first parsing.I n t roduct ionChart parsing is a commonly-used algorithm forparsing natural anguage texts.
The chart is a datastructure which contains all of the constituentswhich may occur in the sentence being parsed.At any point in the algorithm, there exist con-stituents which have been proposed but not ac-tually included in a parse.
These proposed con-stituents are stored in a data structure called thekeylist.
When a constituent is removed from thekeylist, the system considers how this constituentcan be used to extend its current structural hy-pothesis.
In general this can lead to the creation ofnew, more encompassing constituents which them-selves are then added to the keylist.
When we arefinished processing one constituent, a new one ischosen to be removed from the keylist, and so on.Traditionally, the keylist is represented as a stack,so that the last item added to the keylist is thenext one removed.Best-first chart parsing is a variation of chartparsing which attempts to find the most likelyparses first, by adding constituents to the chartin order of the likelihood that they will appear ina correct parse, rather than simply popping con-stituents off of a stack.
Some figure of merit isassigned to potential constituents, and the con-stituent maximizing this value is the next to beadded to the chart.127In best-first probabilistic hart parsing a prob-abilistic measure is used.
In this paper we con-sider probabilities primarily based on probabilisticcontext-free grammars, though in principle other,more complicated schemes could be used.Ideally, we would like to use as our figureof merit the conditional probability of that con-stituent, given the entire sentence, in order tochoose a constituent that not only appears likely inisolation, but maximizes the likelihood of the sen-tence as a whole; that is, we would like to pick theconstituent that maximizes the following quantity:i P(N~,klto,~)where to,n is the sequence of the n tags, or partsof speech, in the sentence (numbered to , .
.
.
,  tn- 1),and Nj, k is a nonterminal of type i covering termstj .
.
.tk_l.
However, we cannot calculate thisquantity, since in order to do so, we would needto completely parse the sentence.
In this paper,we examine the performance of several proposedfigures of merit that approximate it in one way oranother.In our experiments, we use only tag sequencesfor parsing.
More accurate probability estimatesshould be attainable using lexical information.Figures of MeritSt ra ightIt seems reasonable to base a figure of merit onthe inside probability fl of the constituent.
In-side probability is defined as the probability of thewords or tags in the constituent given that the con-stituent is dominated by a particular nonterminalsymbol.
This seems to be a reasonable basis forcomparing constituent probabilities, and has theadditional advantage that it is easy to computeduring chart parsing.The inside probability of the constituent N~, kis defined as/3(Nj, k) ~ p(tj,klN i)where N i represents the ith nonterminal sym-bol.in terms of our earlier discussion, our "ideal"figure of merit can be rewritten as:i p( Nj,k lto,,dp(Nj, , to,n)p(to, )p(Nij, k, to,j, t j, k, tk, n)p(to,**)p(to,j,Nj, k,tk,~)p(tj,klto,j, ' Nj,a, ta,n)p(to, )We apply the usual independence assumptionthat given a nonterminal, the tag sequence it gen-erates depends only on that nonterminal, givingp( to , j ,  i i N;, k, tk,n)p(tj,k INj,k)P( N;,k lto,,d p(to,n)p(to,j, i i Nj,k,tk,~)~(N;,k)p(to,.
)The first term in the numerator is just thedefinition of the outside probability a of the con-stituent.
Outside probability a of a constituentNj, k is defined as the probability of that con-stituent and the rest of the words in the sentence(or rest of the tags in the tag sequence, in ourcase).-(Nj,k) =- p(t0,j, Nj, ,We can therefore rewrite our ideal figure of meritasi i ?p(to, )In this equation, we can see that a(Nj,k) andp(to,~) represent he influence of the surroundingwords.
Thus using j3 alone assumes that a andP(tom) can be ignored.We will refer to this figure of merit asstraight ft.Normal i zed  /~One side effect from omitting the a and p(to,,~)terms in the m-only figure above is that insideprobability alone tends to prefer shorter con-stituents to longer ones, as the inside probabil-ity of a longer constituent involves the product of128more probabilities.
This can result in a "thrash-ing" effect, where the system parses short con-stituents, even very low probability ones, whileavoiding combining them into longer constituents.To avoid thrashing, typically some technique isused to normalize the inside probability for use asa figure of merit.
One approach is to take the ge-ometric mean of the inside probability, to obtaina "per-word" inside probability.
(In the "ideal"model, the p(to,~) term acts as a normalizing fac-tor.
)The per-word inside probability of the con-stituent Nj, k is calculated asWe will refer to this figure as normal ized/3.Normal i zed  aLf~In the previous ection, we showed that our idealfigure of merit can be written asi i .
(  N3,k )fl( Nj,k )p(N3, lt0, ) p(t0,.
)However, the a term, representing outsideprobability, cannot be calculated irectly during aparse, since we need the full parse of the sentenceto compute it.
In some of our figures of merit, weuse the quantity p(Nj,k, t0,j), which is closely re-lated to outside probability.
We call this quantitythe left outside probability, and denote it ai.The following recursive formula can be used tocompute aL.
Let g~,k be the set of all completededges, or rule expansions, in which the nontermi-nal Nj, k appears.
For each edge e in gj,k, we com-pute the the product of aL of the nonterminal p-pearing on the left-hand side (lhs) of the rule, theprobability of the rule itself, and /33 of each non-terminal N~s appearing to the left of Nj, a in therule.
Then aL(N),k) is the sum of these products:i  L(Nj,k)E lhs(e) ---- ~L(N~tart(e),end(e))p(rule(e)) H f~(Nvq, s )"eE$~, k N:.
,This formula can be infinitely recursive,depending on the properties of the grammar.A method for calculating aL more efficientlycan be derived from the calculations given in(3elinek and Lafferty, 1991).A simple extension to the normalized fl modelallows us to estimate the per-word probability ofall tags in the sentence through the end of theconstituent under consideration.
This allows us totake advantage of information already obtained ina left-right parse.
We calculate this quantity asfollows:k O~ i i L ( N;,k ) J3( N;,k )"We are again~ taking the geometric mean toavoid thrashing by compensating for the aj3 quan-tity's preference for shorter constituents, as ex-plained in the previous ection.We refer to this figure of merit as normal -ized O~Lfl.Tr ig ram est imateAn alternative way to rewrite the "ideal" figure ofmerit is as followS:P(Nj,ktto,n)__ P(Nj, k't?,~)p(to,,dp(to,j, tk,n)p(N~, klto,j t i __ , , k,n)p(tj,klN~,k,to,j, tk,n)p(to,j, tk,~)p(tj,k Ito,i, tk,~)Once again applying the usual independenceassumption that given a nonterminal, the tag se-quence it generates depends only on that nonter-minal, we can rewrite the figure of merit as follows:p(tj,k Ito,j, tk,.
)To derive an estimate of this quantity for prac-tical use as a figure of merit, we make some addi-tional independence assumptions.
We assume thatp(N),klto,j, tk,~) ~ p(N~,k), that is, that the prob-ability of a nonterminal is independent of the tagsbefore and after it in the sentence.
We also usea trigram model for the tags themselves, givingp(tj,klto,j, tk,n) ,~ p(tj,kltj_2,j).
Then we have:i i p(N )fl(N\],k)p(Nj, ktto,,~) .~.
p(tj,kltj_2,j)"We can calculate ~(Nj, k) as usual.
The p(N ~)term is estimated from our PCFG as the sum ofthe counts for all rules having N i as their left-hand side, divided by the sum of the counts forall rules.
The p(tj,kltj_2,j) term is just the proba-bility of the tag sequence t j .
.
.
tk- 1 according to atrigram model.
1 (Technically, this is not a trigrammodel but a tritag model, since we are consider-ing sequences of tags, not words.)
We refer to thismodel as the t r ig ram est imate .1Our results how that the p(N i) term can be omit-ted without much effect.129Pre f ix  est imateWe also derived an estimate of the ideal figure ofmerit which takes advantage of statistics on thefirst j - 1 tags of the sentence as well as tj,k.This estimate represents the probability of theconstituent in the context of the preceding tags.p(Nj, klto,n)P(Nj,k,to,~)p(to, )p(tk,~)p(N), k, to,j Itk,~)p(tj,k \]Nj, k, to,j, ta,n)p(tk,,,)p(to,k\]tk,=)p( Nj, k, to,j \] tk,~ )p( t j,k I Nj, k , to,a, t k,~ )p(to,kltk,,~)We again make the independence assumptionthat p(tj,kINj, k,to,j, tk,~) ~ fl(Nj, k).
Addition-ally, we assume that i P(N~,k,to,i) and p(to,k) areindependent of p(tk,n), givingi ip(N),klto,.)
p(to,k)The denominator, p(t0,k), is once again calcu-lated from a tritag model.
The p(N),k, t0,j) termis just O~L, defined above in the discussion of thenormalized O~Lfl model.
Thus this figure of meritcan be written asi i L ( N3,k ) Z( N;,k )p(to,k)We will refer to this as the pref ix  est imate .The ExperimentWe used as our grammar a probabilisticcontext-free grammar learned from the Browncorpus (see (Francis and K@era, 1982), Car-roll and Charniak (1992a) and (1992b), and(Charniak and Carroll, 1994)).
We parsed 500sentences of length 3 to 30 (including punctua-tion) from the Penn Treebank Wall Street Journalcorpus using a best-first parsing method and eachof the following estimates for p(Nj, klto,~) as thefigure of merit:1. straight2.
normalized \[33. normalized O~Lfl4.
trigram estimate5.
prefix estimateThe probability p(N i) in the trigram estimatewas determined from the same training data fromwhich our grammar was learned initially.
Ourtritag probabilities for the trigram and prefix es-timates were learned from this data as well, usingthe deleted interpolation method for smoothing.For each figure of merit, we compared the per-formance of best-first parsing using that figure ofmerit to exhaustive parsing.
By exhaustive pars-ing, we mean continuing to parse until there areno more constituents available to be added to thechart.
We parse exhaustively to determine the to-tal probability of a sentence, that is, the sum of theprobabilities of all parses found for that sentence.We then computed several quantities for best-first parsing with each figure of merit at the pointwhere the best-first parsing method has foundparses contributing at least 95% of the probabilitymass of the sentence.Resu l tsThe chart below presents the following measuresfor each figure of merit:1.
%E: The percentage of edges, or rule expan-sions, in the exhaustive parse that have beenused by the best-first parse to get 95% of theprobability mass.
Edge creation is generallyconsidered the best measure of CFG parser ef-fort.2.
%non-0 E: The percentage of nonzero-lengthedges used by the best-first parse to get 95%.Zero-length edges are required by our parser asa book-keeping measure, and as such are virtu-ally un-elimitable.
We anticipated that remov-ing them from consideration would highlight he"true" differences in the figures of merit.3.
%popped: The percentage of constituents in theexhaustive parse that were used by the best-firstparse to get 95% of the probability mass.Figure of Merit %Estraight/3 97.6normalized/3 34.7normahzed crL/3 39.7trigram estimate 25.2prefix estimate 21.8%non-0 E %popped97.5 93.831.6 61.536.4 57.321.7 44.317.4 38.3The statistics converged to their final valuesquickly.
The edge-count percentages were gener-ally within .01 of their final values after processingonly 200 sentences, o the results were quite stableby the end of our 500-sentence t st corpus.We gathered statistics for each sentence lengthfrom 3 to 30.
Sentence length was limited to amaximum of 30 because of the huge number ofedges that are generated in doing a full parse of130long sentences; using this grammar, sentences inthis length range have produced up to 130,000edges.
Figure 1 shows a graph of %non-0 E, thatis, the percent of nonzero-length edges needed toget 95% of the probability mass, for each sentencelength.We also measured the total CPU time (in sec-onds) needed to get 95% of the probability massfor each of the 500 sentences.
The results are pre-sented in the following chart:Figure of Merit CPU timestraight fl 3966normahzed/3 1631normahzed aL/3 68660trigram estimate 1547prefix estimate 26520Figure 2 shows the average CPU time to get95% of the probability mass for each estimate andeach sentence length.
Each estimate averaged be-low 1 second on sentences of fewer than 7 words.
(The y-axis has been restricted so that the normal-ized /3 and trigram estimates can be better com-pared.
)Prev ious  workThe literature shows many implementations ofbest-first parsing, but none of the previous workshares our goal of explicitly comparing figures ofmerit.Bobrow (1990) and Chitrao and Grishman(1990) introduced statistical agenda-based parsingtechniques.
Chitrao and Grishman implementeda best-first probabilistic parser and noted theparser's tendency to prefer shorter constituents.They proposed a heuristic solution of penalizingshorter constituents by a fixed amount per word.Miller and Fox (1994) compare the perfor-mance of parsers using three different types ofgrammars, and show that a probabilistic ontext-free grammar using inside probability (unnormal-ized) as a figure of merit outperforms both acontext-free grammar and a context-dependentgrammar.Kochman and Kupin (1991) propose a fig-ure of merit closely related to our prefix estimate.They do not actually incorporate this figure intoa best-first parser.Magerman and Marcus (1991) use the geomet-ric mean to compute a figure of merit that is in-dependent of constituent length.
Magerman andWeir (1992) use a similar model with a differentparsing algorithm.tm"olO0 -80604020I , .~ ,  ~,, . "
-  .
.
.
.iV?.
: ",% ",,'" ,'~ "" .
.
.
.
:'.
.
":,:' "~.
- "L  """ "'.
":,//  ">:~. "
.
.
.
/  '? '
-" :  "~" - "~ ' /  \ '~"x~ s t /C"\ L J  \? '
' ' ' ' ' 1 ' " ' " ' ' ' ' " I " ' " ' ' w ,I0  20Sentence LengthFigure 1: Nonzero-length edges for 95% of  Probabil ity MassI3O- -  stlraigl}t beta.
.
.
.
.
normal ized beta.
.
.
.
.
.
normal ized a lphaL beta.
.
.
.
t r ig ram est imatepref ix  est imate=///fIIIIIIIIII // 'B~# Ii l l ' l  ,11 I //" I I//: / Ii l  ,';"-;: I /; '5 ; I: I f l: I: I , , /</  : /  f - -f J p?--V10 15 20 25 30Sentence  LengthFigure 2: Average CPU Time for 95% of  Probabil ity Mass131- -  s t ra ight  beta.
.
.
.
normal i zed  beta.
.
.
.
.
.
normal ized a lphaL  betat r ig ram est imate- - - - - -  pref ix es t imateConc lus ionsFrom the edge count statistics, it is clear thatstraight ,3 is a poor figure of merit.
Figure 1 alsodemonstrates that its performance generally wors-ens as sentence length increases.The best performance in terms of edge countsof the figures we tested was the model which usedthe most information available from the sentence,the prefix model.
However, so far, the additionalrunning time needed for the computation of O' Lterms has exceeded the time saved by processingfewer edges, as is made clear in the CPU timestatistics, where these two models perform sub-stantially worse than even the straight j3 figure.While chart parsing and calculations of j3 canbe done in O(n 3) time, we have been unable tofind an algorithm to compute the o~ L terms fasterthan O(nS).
When a constituent is removed fromthe keylist, it only affects the j3 values of its an-cestors in the parse trees; however, ~L values arepropagated to all of the constituent's siblings tothe right and all of its descendants.
Recomput-ing the o~ L terms when a constituent is removedfrom the keylist can be done in O(n 3) time, andsince there are O(n 2) possible constituents, the to-tal time needed to compute the ol L terms in thismanner is O(n5).The best performer in running time was theparser using the trigram estimate as a figure ofmerit.
This figure has the additional advantagethat it can be easily incorporated into existingbest-first parsers using a figure of merit based oninside probability.
From the CPU time statistics,it can be seen that the running time begins to showa real improvement over the normalized j3 modelon sentences of length 25 or greater, and the trendsuggests that the improvement would be greaterfor longer sentences.It is also interesting to note that while themodels using figures of merit normalized by thegeometric mean performed similarly to the othermodels on shorter sentences, the superior perfor-mance of the other models becomes more pro-nounced as sentence length increases.
From Figure1, we can see that the models using the geometricmean appear to level off with respect o an exhaus-tive parse when used to parse sentences of lengthgreater than about 15.
The other two estimatesseem to continue improving with greater sentencelength.
In fact, the measurements presented herealmost certainly underestimate he true benefits ofthe better models.
We restricted sentence lengthto a maximum of 30 words, in order to keep thenumber of edges in the exhaustive parse to a prac-tical size; however, since the percentage of edgesneeded by the best-first parse decreases with in-creasing sentence length, we assume that the ira-132provement would be even more dramatic for sen-tences longer than 30 words.Re ferences\[1990\] Robert J. Bobrow.
1990.
Statistical agendaparsing.
In DARPA Speech and LanguageWorkshop, pages 222-224.\[1992a\] Glenn Carroll and Eugene Charniak.1992a.
Learning probabilistic dependency gram-mars from labeled text.
In Working Notes, FallSymposium Series, pages 25-32.
AAAI.\[1992b\] Glenn Carroll and Eugene Charniak.1992b.
Two experiments on learning proba-bilistic dependency grammars from corpora.
InWorkshop Notes, Statistically-Based NLP Tech-niques, pages 1-13.
AAAI.\[1994\] Eugene Charniak and Glenn Carroll.
1994.Context-sensitive statistics for improved gram-matical anguage models.
In Proceedings of theTwelfth National Conference on Artificial Intel-ligence, pages 728-733.\[1990\] Mahesh V. Chitrao and Ralph Grishrnan.1990.
Statistical parsing of messages.
InDARPA Speech and Language Workshop, pages263-266.\[1982\] W. Nelson Francis and Henry Ku~era.1982.
Frequency Analysis of English Usage:Lexicon and Grammar.
Houghton Mifflin.\[1991\] Frederick Jelinek and John D. Lafferty.1991.
Computation of the probability of initialsubstring generation by stochastic ontext-freegrammars.
Computational Linguistics, 17:315-323.\[1991\] Fred Kochman and Joseph Kupin.
1991.Calculating the probability of a partial parse ofa sentence.
In DARPA Speech and LanguageWorkshop, pages 237-240.\[1991\] David M. Magerman and Mitchell P. Mar-cus.
1991.
Parsing the voyager domain usingpearl.
In DARPA Speech and Language Work-shop, pages 231-236.\[1992\] David M. Magerman and Carl Weir.
1992.Efficiency, robustness and accuracy in pickychart parsing.
In Proceedings of the 30th ACLConference, pages 40-47.\[1994\] Scott Miller and Heidi Fox.
1994.
Au-tomatic grammar acquisition.
In Proceedingsof the Human Language Technology Workshop,pages 268-271.
