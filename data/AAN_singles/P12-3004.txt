Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19?24,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsNiuTrans: An Open Source Toolkit forPhrase-based and Syntax-based Machine TranslationTong Xiao?
?
, Jingbo Zhu?
?
, Hao Zhang?
and Qiang Li?
?Natural Language Processing Lab, Northeastern University?Key Laboratory of Medical Image Computing, Ministry of Education{xiaotong,zhujingbo}@mail.neu.edu.cn{zhanghao1216,liqiangneu}@gmail.comAbstractWe present a new open source toolkit forphrase-based and syntax-based machinetranslation.
The toolkit supports severalstate-of-the-art models developed instatistical machine translation, includingthe phrase-based model, the hierachicalphrase-based model, and various syntax-based models.
The key innovation providedby the toolkit is that the decoder can workwith various grammars and offers differentchoices of decoding algrithms, such asphrase-based decoding, decoding asparsing/tree-parsing and forest-baseddecoding.
Moreover, several useful utilitieswere distributed with the toolkit, includinga discriminative reordering model, a simpleand fast language model, and animplementation of minimum error ratetraining  for weight tuning.1 IntroductionWe present NiuTrans, a new open source machinetranslation toolkit, which was developed forconstructing high quality machine translationsystems.
The NiuTrans toolkit supports moststatistical machine translation (SMT) paradigmsdeveloped over the past decade, and allows fortraining and decoding with several state-of-the-artmodels, including: the phrase-based model (Koehnet al, 2003), the hierarchical phrase-based model(Chiang, 2007), and various syntax-based models(Galley et al, 2004; Liu et al, 2006).
In particular,a unified framework was adopted to decode withdifferent models and ease the implementation ofdecoding algorithms.
Moreover, some usefulutilities were distributed with the toolkit, such as: adiscriminative reordering model, a simple and fastlanguage model, and an implementation ofminimum error rate training that allows for variousevaluation metrics for tuning the system.
Inaddition, the toolkit provides easy-to-use APIs forthe development of new features.
The toolkit hasbeen used to build translation systems that haveplaced well at recent MT evaluations, such as theNTCIR-9 Chinese-to-English PatentMT task (Gotoet al, 2011).We implemented the toolkit in C++ language,with special consideration of extensibility andefficiency.
C++ enables us to develop efficienttranslation engines which have high running speedfor both training and decoding stages.
Thisproperty is especially important when the programsare used for large scale translation.
While thedevelopment of C++ program is slower than that ofthe similar programs written in other popularlanguages such as Java, the modern compliersgenerally result in C++ programs beingconsistently faster than the Java-based counterparts.The toolkit is available under the GNU generalpublic license 1 .
The website of NiuTrans ishttp://www.nlplab.com/NiuPlan/NiuTrans.html.2 MotivationAs in current approaches to statistical machinetranslation, NiuTrans is based on a log-linear1 http://www.gnu.org/licenses/gpl-2.0.html19model where a number of features are defined tomodel the translation process.
Actually NiuTrans isnot the first system of this kind.
To date, severalopen-source SMT systems (based on either phrase-based models or syntax-based models) have beendeveloped, such as Moses (Koehn et al, 2007),Joshua (Li et al, 2009), SAMT (Zollmann andVenugopal, 2006), Phrasal (Cer et al, 2010), cdec(Dyer et al, 2010), Jane (Vilar et al, 2010) andSilkRoad 2 , and offer good references for thedevelopment of the NiuTrans toolkit.
While ourtoolkit includes all necessary components asprovided within the above systems, we haveadditional goals for this project, as follows:z It fully supports most state-of-the-art SMTmodels.
Among these are: the phrase-basedmodel, the hierarchical phrase-based model,and the syntax-based models that explicitlyuse syntactic information on either (both)source and (or) target language side(s).z It offers a wide choice of decodingalgorithms.
For example, the toolkit hasseveral useful decoding options, including:standard phrase-based decoding, decodingas parsing, decoding as tree-parsing, andforest-based decoding.z It is easy-to-use and fast.
A new system canbe built using only a few commands.
Tocontrol the system, users only need tomodify a configuration file.
In addition tothe special attention to usability, therunning speed of the system is alsoimproved in several ways.
For example, weused several pruning and multithreadingtechniques to speed-up the system.3 ToolkitThe toolkit serves as an end-to-end platform fortraining and evaluating statistical machinetranslation models.
To build new translationsystems, all you need is a collection of word-aligned sentences 3 , and a set of additionalsentences with one or more reference translationsfor weight tuning and test.
Once the data isprepared, the MT system can be created using a2 http://www.nlp.org.cn/project/project.php?proj_id=143 To obtain word-to-word alignments, several easy-to-usetoolkits are available, such as GIZA++ and Berkeley Aligner.sequence of commands.
Given a number ofsentence-pairs and the word alignments betweenthem, the toolkit first extracts a phrase table andtwo reordering models for the phrase-based system,or a Synchronous Context-free/Tree-substitutionGrammar (SCFG/STSG) for the hierarchicalphrase-based and syntax-based systems.
Then, ann-gram language model is built on the target-language corpus.
Finally, the resulting models areincorporated into the decoder which canautomatically tune feature weights on thedevelopment set using minimum error rate training(Och, 2003) and translate new sentences with theoptimized weights.In the following, we will give a brief review ofthe above components and the main featuresprovided by the toolkit.3.1 Phrase Extraction and Reordering ModelWe use a standard way to implement the phraseextraction module for the phrase-based model.That is, we extract all phrase-pairs that areconsistent with word alignments.
Five features areassociated with each phrase-pair.
They are twophrase translation probabilities, two lexical weights,and a feature of phrase penalty.
We follow themethod proposed in (Koehn et al, 2003) toestimate the values of these features.Unlike previous systems that adopt only onereordering model, our toolkit supports twodifferent reordering models which are trainedindependently but jointly used during decoding.z The first of these is a discriminativereordering model.
This model is based onthe standard framework of maximumentropy.
Thus the reordering problem ismodeled as a classification problem, andthe reordering probability can be efficientlycomputed using a (log-)linear combinationof features.
In our implementation, we useall boundary words as features which aresimilar to those used in (Xiong et al, 2006).z The second model is the MSD reorderingmodel4 which has been successfully used inthe Moses system.
Unlike Moses, ourtoolkit supports both the word-based andphrase-based methods for estimating the4 Term MSD refers to the three orientations (reordering types),including Monotone (M), Swap (S), and Discontinuous (D).20probabilities of the three orientations(Galley and Manning, 2008).3.2 Translation Rule ExtractionFor the hierarchical phrase-based model, we followthe general framework of SCFG where a grammarrule has three parts ?
a source-side, a target-sideand alignments between source and target non-terminals.
To learn SCFG rules from word-alignedsentences, we choose the algorithm proposed in(Chiang, 2007) and estimate the associated featurevalues as in the phrase-based system.For the syntax-based models, all non-terminalsin translation rules are annotated with syntacticlabels.
We use the GHKM algorithm to extract(minimal) translation rules from bilingualsentences with parse trees on source-language sideand/or target-language side5 .
Also, two or moreminimal rules can be composed together to obtainlarger rules and involve more contextualinformation.
For unaligned words, we attach themto all nearby rules, instead of using the most likelyattachment as in (Galley et al, 2006).3.3 N-gram Language ModelingThe toolkit includes a simple but effective n-gramlanguage model (LM).
The LM builder is basicallya ?sorted?
trie structure (Pauls and Klein, 2011),where a map is developed to implement an array ofkey/value pairs, guaranteeing that the keys can beaccessed in sorted order.
To reduce the size ofresulting language model, low-frequency n-gramsare filtered out by some thresholds.
Moreover, ann-gram cache is implemented to speed up n-gramprobability requests for decoding.3.4 Weight TuningWe implement the weight tuning componentaccording to the minimum error rate training(MERT) method (Och, 2003).
As MERT suffersfrom local optimums, we added a small programinto the MERT system to let it jump out from thecoverage area.
When MERT converges to a (local)optimum, our program automatically conducts theMERT run again from a random starting point nearthe newly-obtained optimal point.
This procedure5 For tree-to-tree models, we use a natural extension of theGHKM algorithm which defines admissible nodes on tree-pairs and obtains tree-to-tree rules on all pairs of source andtarget tree-fragments.is repeated for several times until no better weights(i.e., weights with a higher BLEU score) are found.In this way, our program can introduce somerandomness into weight training.
Hence users donot need to repeat MERT for obtaining stable andoptimized weights using different starting points.3.5 DecodingChart-parsing is employed to decode sentences indevelopment and test sets.
Given a source sentence,the decoder generates 1-best or k-best translationsin a bottom-up fashion using a CKY-style parsingalgorithm.
The basic data structure used in thedecoder is a chart, where an array of cells isorganized in topological order.
Each cell maintainsa list of hypotheses (or items).
The decodingprocess starts with the minimal cells, and proceedsby repeatedly applying translation rules orcomposing items in adjunct cells to obtain newitems.
Once a new item is created, the associatedscores are computed (with an integrated n-gramlanguage model).
Then, the item is added into thelist of the corresponding cell.
This procedure stopswhen we reach the final state (i.e., the cellassociates with the entire source span).The decoder can work with all (hierarchical)phrase-based and syntax-based models.
Inparticular, our toolkit provides the followingdecoding modes.z Phrase-based decoding.
To fit the phrase-based model into the CKY paringframework, we restrict the phrase-baseddecoding with the ITG constraint (Wu,1996).
In this way, each pair of items inadjunct cells can be composed in eithermonotone order or inverted order.
Hencethe decoding can be trivially implementedby a three-loop structure as in standardCKY parsing.
This algorithm is actually thesame as that used in parsing withbracketing transduction grammars.z Decoding as parsing (or string-baseddecoding).
This mode is designed fordecoding with SCFGs/STSGs which areused in the hierarchical phrase-based andsyntax-based systems.
In the generalframework of synchronous grammars andtree transducers, decoding can be regardedas a parsing problem.
Therefore, the abovechart-based decoder is directly applicable to21the hierarchical phrase-based and syntax-based models.
For efficient integration of n-gram language model into decoding, rulescontaining more than two variables arebinarized into binary rules.
In addition tothe rules learned from bilingual data, gluerules are employed to glue the translationsof a sequence of chunks.z Decoding as tree-parsing (or tree-baseddecoding).
If the parse tree of sourcesentence is provided, decoding (for tree-to-string and tree-to-tree models) can also becast as a tree-parsing problem (Eisner,2003).
In tree-parsing, translation rules arefirst mapped onto the nodes of input parsetree.
This results in a translation tree/forest(or a hypergraph) where each edgerepresents a rule application.
Thendecoding can proceed on the hypergraph asusual.
That is, we visit in bottom-up ordereach node in the parse tree, and calculatethe model score for each edge rooting at thenode.
The final output is the 1-best/k-besttranslations maintained by the root node ofthe parse tree.
Since tree-parsing restrictsits search space to the derivations thatexactly match with the input parse tree, it ingeneral has a much higher decoding speedthan a normal parsing procedure.
But it inturn results in lower translation quality dueto more search errors.z Forest-based decoding.
Forest-baseddecoding (Mi et al, 2008) is a naturalextension of tree-based decoding.
Inprinciple, forest is a data structure that canencode exponential number of treesefficiently.
This structure has been provedto be helpful in reducing the effects causedby parser errors.
Since our internalrepresentation is already in a hypergraphstructure, it is easy to extend the decoder tohandle the input forest, with littlemodification of the code.4 Other FeaturesIn addition to the basic components describedabove, several additional features are introduced toease the use of the toolkit.4.1 MultithreadingThe decoder supports multithreading to make fulladvantage of the modern computers where morethan one CPUs (or cores) are provided.
In general,the decoding speed can be improved when multiplethreads are involved.
However, modern MTdecoders do not run faster when too many threadsare used (Cer et al, 2010).4.2 PruningTo make decoding computational feasible, beampruning is used to aggressively prune the searchspace.
In our implementation, we maintain a beamfor each cell.
Once all the items of the cell areproved, only the top-k best items according tomodel score are kept and the rest are discarded.Also, we re-implemented the cube pruning methoddescribed in (Chiang, 2007) to further speed-up thesystem.In addition, we develop another method thatprunes the search space using punctuations.
Theidea is to divide the input sentence into a sequenceof segments according to punctuations.
Then, eachsegment is translated individually.
The MT outputsare finally generated by composing the translationsof those segments.4.3 APIs for Feature EngineeringTo ease the implementation and test of newfeatures, the toolkit offers APIs for experimentingwith the features developed by users.
For example,users can develop new features that are associatedwith each phrase-pair.
The system canautomatically recognize them and incorporate theminto decoding.
Also, more complex features can beactivated during decoding.
When an item is createdduring decoding, new features can be introducedinto an internal object which returns feature valuesfor computing the model score.5 Experiments5.1 Experimental SetupWe evaluated our systems on NIST Chinese-English MT tasks.
Our training corpus consists of1.9M bilingual sentences.
We used GIZA++ andthe ?grow-diag-final-and?
heuristics to generateword alignment for the bilingual data.
The parsetrees on both the Chinese and English sides were22BLEU4[%] EntryDev  TestMoses: phrase  36.51  34.93Moses: hierarchical phrase  36.65  34.79phrase  36.99  35.29hierarchical phrase  37.41  35.35parsing  36.48  34.71tree-parsing  35.54  33.99t2sforest-based  36.14  34.25parsing  35.99  34.01tree-parsing  35.04  33.21t2tforest-based  35.56  33.45NiuTranss2t  parsing  37.63  35.65Table 1: BLEU scores of various systems.
t2s, t2t,and s2t represent the tree-to-string, tree-to-tree, andstring-to-tree systems, respectively.generated using the Berkeley Parser, which werethen binarized in a head-out fashion 6.
A 5-gramlanguage model was trained on the Xinhua portionof the Gigaword corpus in addition to the Englishpart of the LDC bilingual training data.
We usedthe NIST 2003 MT evaluation set as ourdevelopment set (919 sentences) and the NIST2005 MT evaluation set as our test set (1,082sentences).
The translation quality was evaluatedwith the case-insensitive IBM-version BLEU4.For the phrase-based system, phrases are of atmost 7 words on either source or target-side.
Forthe hierarchical phrase-based system, all SCFGrules have at most two variables.
For the syntax-based systems, minimal rules were extracted fromthe binarized trees on both (either) language-side(s).
Larger rules were then generated bycomposing two or three minimal rules.
By default,all these systems used a beam of size 30 fordecoding.5.2 Evaluation of TranslationsTable 1 shows the BLEU scores of different MTsystems built using our toolkit.
For comparison,the result of the Moses system is also reported.
Wesee, first of all, that our phrase-based andhierarchical phrase-based systems achievecompetitive performance, even outperforms theMoses system over 0.3 BLEU points in some cases.Also, the syntax-based systems obtain very6 The parse trees follow the nested bracketing format, asdefined in the Penn Treebank.
Also, the NiuTrans packageincludes a tool for tree binarization.BLEU4[%] EntryDev TestSpeed(sent/sec)Moses: phrase  36.69  34.99    0.11+ cube pruning   36.51  34.93    0.47NiuTrans: phrase  37.14  35.47    0.14+ cube pruning  36.98  35.39    0.60+ cube & punct pruning  36.99  35.29    3.71+ all pruning & 8 threads  36.99  35.29  21.89+ all pruning & 16 threads  36.99  35.29  22.36Table 2: Effects of pruning and multithreadingtechniques.promising results.
For example, the string-to-treesystem significantly outperforms the phrase-basedand hierarchical phrase-based counterparts.
Inaddition, Table 1 gives a test of different decodingmethods (for syntax-based systems).
We see thatthe parsing-based method achieves the best BLEUscore.
On the other hand, as expected, it runsslowest due to its large search space.
For example,it is 5-8 times slower than the tree-parsing-basedmethod in our experiments.
The forest-baseddecoding further improves the BLEU scores on topof tree-parsing.
In most cases, it obtains a +0.6BLEU improvement but is 2-3 times slower thanthe tree-parsing-based method.5.3 System Speed-upWe also study the effectiveness of pruning andmultithreading techniques.
Table 2 shows that allthe pruning methods implemented in the toolkit ishelpful in speeding up the (phrase-based) system,while does not result in significant decrease inBLEU score.
On top of a straightforward baseline(only beam pruning is used), cube pruning andpruning with punctuations give a speedimprovement of 25 times together7.
Moreover, thedecoding process can be further accelerated byusing multithreading technique.
However, morethan 8 threads do not help in our experiments.6 Conclusion and Future WorkWe have presented a new open-source toolkit forphrase-based and syntax-based machine translation.It is implemented in C++ and runs fast.
Moreover,it supports several state-of-the-art models rangingfrom phrase-based models to syntax-based models,7 The translation speed is tested on Intel Core Due 2 E8500processors running at 3.16 GHz.23and provides a wide choice of decoding methods.The experimental results on NIST MT tasks showthat the MT systems built with our toolkit achievestate-of-the-art translation performance.The next version of NiuTrans will supportARPA-format LMs, MIRA for weight tuning and abeam-stack decoder which removes the ITGconstraint for phrase decoding.
In addition, aHadoop-based MapReduce-parallelized version isunderway and will be released in near future.AcknowledgmentsThis research was supported in part by the NationalScience Foundation of China (61073140), theSpecialized Research Fund for the DoctoralProgram of Higher Education (20100042110031)and the Fundamental Research Funds for theCentral Universities in China.ReferencesDaniel Cer, Michel Galley, Daniel Jurafsky andChristopher D. Manning.
2010.
Phrasal: A Toolkitfor Statistical Machine Translation with Facilities forExtraction and Incorporation of Arbitrary ModelFeatures.
In Proc.
of HLT/NAACL 2010demonstration Session, pages 9-12.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201?228.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, Philip Resnik.
2010. cdec: ADecoder, Alignment, and Learning Framework forFinite-State and Context-Free Translation Models.
InProc.
of ACL 2010 System Demonstrations, pages 7-12.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003, pages 205-208.Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What's in a translation rule?
In Proc.
ofHLT-NAACL 2004, pages 273-280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang and IgnacioThayer.
2006.
Scalable inferences and training ofcontext-rich syntax translation models.
In Proc.
ofCOLING/ACL 2006, pages 961-968.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proc.
of EMNLP2008, pages 848-856.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita andBenjamin K. Tsou.
2011.
Overview of the PatentMachine Translation Task at the NTCIR-9 Workshop.In Proc.
of NTCIR-9 Workshop Meeting, pages 559-578.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
ofHLT/NAACL 2003, pages 127-133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.
InProc.
of ACL 2007, pages 177?180.Zhifei Li, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton,Jonathan Weese, and Omar Zaidan.
2009.
Joshua: AnOpen Source Toolkit for Parsing-Based MachineTranslation.
In Proc.
of the Workshop on StatisticalMachine Translation, pages 135?139.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
of ACL 2006, pages 609-616.Haitao Mi, Liang Huang and Qun Liu.
2008.
Forest-Based Translation.
In Proc.
of ACL 2008, pages 192-199.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL 2003,pages 160-167.Adam Pauls and Dan Klein.
2011.
Faster and SmallerN-Gram Language Models.
In Proc.
of ACL 2011,pages 258?267.David Vilar, Daniel Stein, Matthias Huck and HermannNey.
2010.
Jane: Open Source HierarchicalTranslation, Extended with Reordering and LexiconModels.
In Proc.
of the Joint 5th Workshop onStatistical Machine Translation and MetricsMATR,pages 262-270.Dekai Wu.
1996.
A polynomial-time algorithm forstatistical machine translation.
In Proc.
of ACL1996,pages 152?158.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.Maximum Entropy Based Phrase Reordering Modelfor Statistical Machine Translation.
In Proc.
of ACL2006, pages 521-528.Andreas Zollmann and Ashish Venugopal.
2006.
SyntaxAugmented Machine Translation via Chart Parsing.In Proc.
of HLT/NAACL 2006, pages 138-141.24
