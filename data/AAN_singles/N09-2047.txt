Proceedings of NAACL HLT 2009: Short Papers, pages 185?188,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMICA: A Probabilistic Dependency ParserBased on Tree Insertion GrammarsApplication NoteSrinivas Bangalore Pierre BoulllierAT&T Labs ?
Research INRIAFlorham Park, NJ, USA Rocquencourt, Francesrini@research.att.com Pierre.Boullier@inria.frAlexis Nasr Owen Rambow Beno?
?t SagotAix-Marseille Universite?
CCLS, Columbia Univserity INRIAMarseille, France New York, NY, USA Rocquencourt, Francealexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.frAbstractMICA is a dependency parser which returnsdeep dependency representations, is fast, hasstate-of-the-art performance, and is freelyavailable.1 OverviewThis application note presents a freely avail-able parser, MICA (Marseille-INRIA-Columbia-AT&T).1 MICA has several key characteristics thatmake it appealing to researchers in NLP who needan off-the-shelf parser.?
MICA returns a deep dependency parse, inwhich dependency is defined in terms of lex-ical predicate-argument structure, not in termsof surface-syntactic features such as subject-verbagreement.
Function words such as auxiliariesand determiners depend on their lexical head, andstrongly governed prepositions (such as to for give)are treated as co-heads rather than as syntactic headsin their own right.
For example, John is giving booksto Mary gets the following analysis (the arc label ison the terminal).givingJohnarc=0isarc=adjbooksarc=1toarc=co-headMaryarc=2The arc labels for the three arguments John,books, and Mary do not change when the sentenceis passivized or Mary undergoes dative shift.1We would like to thank Ryan Roth for contributing theMALT data.?
MICA is based on an explicit phrase-structuretree grammar extracted from the Penn Treebank.Therefore, MICA can associate dependency parseswith rich linguistic information such as voice, thepresence of empty subjects (PRO), wh-movement,and whether a verb heads a relative clause.
?MICA is fast (450 words per second plus 6 sec-onds initialization on a standard high-end machineon sentences with fewer than 200 words) and hasstate-of-the-art performance (87.6% unlabeled de-pendency accuracy, see Section 5).?
MICA consists of two processes: the supertag-ger, which associates tags representing rich syntac-tic information with the input word sequence, andthe actual parser, which derives the syntactic struc-ture from the n-best chosen supertags.
Only the su-pertagger uses lexical information, the parser onlysees the supertag hypotheses.?
MICA returns n-best parses for arbitrary n;parse trees are associated with probabilities.
Apacked forest can also be returned.?
MICA is freely available2, easy to install underLinux, and easy to use.
(Input is one sentence perline with no special tokenization required.
)There is an enormous amount of related work,and we can mention only the most salient, givenspace constraints.
Our parser is very similar to thework of (Shen and Joshi, 2005).
They do not em-ploy a supertagging step, and we do not restrict ourtrees to spinal projections.
Other parsers using su-pertagging include the LDA of Bangalore and Joshi(1999), the CCG-based parser of Clark and Curran(2004), and the constraint-based approach of Wang2http://www1.ccls.columbia.edu/?rambow/mica.html185and Harper (2004).
Widely used dependency parserswhich generate deep dependency representations in-clude Minipar (Lin, 1994), which uses a declarativegrammar, and the Stanford parser (Levy and Man-ning, 2004), which performs a conversion from astandard phrase-structure parse.
All of these systemsgenerate dependency structures which are slightlydifferent from MICA?s, so that direct comparisonis difficult.
For comparison purposes, we thereforeuse the MALT parser generator (Nivre et al, 2004),which allows us to train a dependency parser on ourown dependency structures.
MALT has been amongthe top performers in the CoNLL dependency pars-ing competitions.2 Supertags and SupertaggingSupertags are elementary trees of a lexicalizedtree grammar such as a Tree-Adjoining Gram-mar (TAG) (Joshi, 1987).
Unlike context-free gram-mar rules which are single level trees, supertags aremulti-level trees which encapsulate both predicate-argument structure of the anchor lexeme (by includ-ing nodes at which its arguments must substitute)and morpho-syntactic constraints such as subject-verb agreement within the supertag associated withthe anchor.
There are a number of supertags for eachlexeme to account for the different syntactic trans-formations (relative clause, wh-question, passiviza-tion etc.).
For example, the verb give will be associ-ated with at least these two trees, which we will calltdi and tdi-dat.
(There are also many other trees.
)tdi tdi-datSNP0 ?
VPV?
NP1 ?
PPPtoNP2 ?SNP0 ?
VPV?
NP2 ?NP1 ?Supertagging is the task of disambiguating amongthe set of supertags associated with each word ina sentence, given the context of the sentence.
Inorder to arrive at a complete parse, the only stepremaining after supertagging is establishing the at-tachments among the supertags.
Hence the result ofsupertagging is termed as an ?almost parse?
(Banga-lore and Joshi, 1999).The set of supertags is derived from the PennTreebank using the approach of Chen (2001).
Thisextraction procedure results in a supertag set of4,727 supertags and about one million words of su-pertag annotated corpus.
We use 950,028 annotatedwords for training (Sections 02-21) and 46,451 (Sec-tion 00) annotated words for testing in our exper-iments.
We estimate the probability of a tag se-quence directly as in discriminative classificationapproaches.
In such approaches, the context of theword being supertagged is encoded as features forthe classifier.
Given the large scale multiclass la-beling nature of the supertagging task, we train su-pertagging models as one-vs-rest binary classifica-tion problems.
Detailed supertagging experiment re-sults are reported in (Bangalore et al, 2005) whichwe summarize here.
We use the lexical, part-of-speech attributes from the left and right contextin a 6-word window and the lexical, orthographic(e.g.
capitalization, prefix, suffix, digit) and part-of-speech attributes of the word being supertagged.Crucially, this set does not use the supertags for thewords in the history.
Thus during decoding the su-pertag assignment is done locally and does not needa dynamic programming search.
We trained a Max-ent model with such features using the labeled dataset mentioned above and achieve an error rate of11.48% on the test set.3 Grammars and ModelsMICA grammars are extracted in a three steps pro-cess.
In a first step, a Tree Insertion Grammar (TIG)(Schabes and Waters, 1995) is extracted from thetreebank, along with a table of counts.
This is thegrammar that is used for supertagging, as describedin Section 2.
In a second step, the TIG and the counttable are used to build a PCFG.
During the last step,the PCFG is ?specialized?
in order to model morefinely some lexico-syntactic phenomena.
The sec-ond and third steps are discussed in this section.The extracted TIG is transformed into a PCFGwhich generates strings of supertags as follows.
Ini-tial elementary trees (which are substituted) yieldrules whose left hand side is the root category ofthe elementary tree.
Left (respectively right) aux-iliary trees (the trees for which the foot node is the186left (resp.
right) daughter of the root) give birth torules whose left-hand side is of the form Xl (resp.Xr), where X is the root category of the elementarytree.
The right hand side of each rule is built duringa top down traversal of the corresponding elemen-tary tree.
For every node of the tree visited, a newsymbol is added to the right hand side of rule, fromleft to right, as follows:?
The anchor of the elementary tree adds the su-pertag (i.e., the name of the tree), which is a terminalsymbol, to the context-free rule.?
A substitution node in the elementary tree addsits nonterminal symbol to the context-free rule.?
A interior node in the elementary tree at whichadjunction may occur adds to the context-free rulethe nonterminal symbol X ?r or X ?l , where X is thenode?s nonterminal symbol, and l (resp.
r) indicateswhether it is a left (resp.
right) adjunction.
Eachinterior node is visited twice, the first time from theleft, and then from the right.
A set of non-lexicalizedrules (i.e., rules that do not generate a terminal sym-bol) allow us to generate zero or more trees anchoredby Xl from the symbol X ?l .
No adjunction, the firstadjunction, and the second adjunction are modeledexplicitly in the grammar and the associated prob-abilistic model, while the third and all subsequentadjunctions are modeled together.This conversion method is basically the same asthat presented in (Schabes and Waters, 1995), ex-cept that our PCFG models multiple adjunctions atthe same node by positions (a concern Schabes andWaters (1995) do not share, of course).
Our PCFGconstruction differs from that of Hwa (2001) in thatshe does not allow multiple adjunction at one node(Schabes and Shieber, 1994) (which we do since weare interested in the derivation structure as a repre-sentation of linguistic dependency).
For more in-formation about the positional model of adjunctionand a discussion of an alternate model, the ?bigrammodel?, see (Nasr and Rambow, 2006).Tree tdi from Section 2 gives rise to the followingrule (where tdi and tCO are terminal symbols andthe rest are nonterminals): S ?
S?l NP VP?l V?l tdiV?r NP PP?l P?l tCO P?r NP PP?r VP?r S?rThe probabilities of the PCFG rules are estimatedusing maximum likelihood.
The probabilistic modelrefers only to supertag names, not to words.
In thebasic model, the probability of the adjunction or sub-stitution of an elementary tree (the daughter) in an-other elementary tree (the mother) only depends onthe nonterminal, and does not depend on the mothernor on the node on which the attachment is per-formed in the mother elementary tree.
It is wellknown that such a dependency is important for anadequate probabilistic modelling of syntax.
In orderto introduce such a dependency, we condition an at-tachment on the mother and on the node on whichthe attachment is performed, an operation that wecall mother specialization.
Mother specialization isperformed by adding to all nonterminals the name ofthe mother and the address of a node.
The special-ization of a grammar increase vastly the number ofsymbols and rules and provoke severe data sparse-ness problems, this is why only a subset of the sym-bols are specialized.4 ParserSYNTAX (Boullier and Deschamp, 1988) is a sys-tem used to generate lexical and syntactic analyzers(parsers) (both deterministic and non-deterministic)for all kind of context-free grammars (CFGs) aswell as some classes of contextual grammars.
Ithas been under development at INRIA for severaldecades.
SYNTAX handles most classes of determin-istic (unambiguous) grammars (LR, LALR, RLR)as well as general context-free grammars.
Thenon-deterministic features include, among others,an Earley-like parser generator used for natural lan-guage processing (Boullier, 2003).Like most SYNTAX Earley-like parsers, the archi-tecture of MICA?s PCFG-based parser is the follow-ing:?
The Earley-like parser proper computes a sharedparse forest that represents in a factorized (polyno-mial) way all possible parse trees according to theunderlying (non-probabilistic) CFG that representsthe TIG;?
Filtering and/or decoration modules are appliedon the shared parse forest; in MICA?s case, an n-best module is applied, followed by a dependencyextractor that relies on the TIG structure of the CFG.The Earley-like parser relies on Earley?s algo-rithm (Earley, 1970).
However, several optimiza-tions have been applied, including guiding tech-niques (Boullier, 2003), extensive static (offline)187computations over the grammar, and efficient datastructures.
Moreover, Earley?s algorithm has beenextended so as to handle input DAGs (and not onlysequences of forms).
A particular effort has beenmade to handle huge grammars (over 1 millionsymbol occurrences in the grammar), thanks to ad-vanced dynamic lexicalization techniques (Boullierand Sagot, 2007).
The resulting efficiency is satisfy-ing: with standard ambiguous NLP grammars, hugeshared parse forest (over 1010 trees) are often gener-ated in a few dozens of milliseconds.Within MICA, the first module that is applied ontop of the shared parse forest is SYNTAX?s n-bestmodule.
This module adapts and implements the al-gorithm of (Huang and Chiang, 2005) for efficientn-best trees extraction from a shared parse forest.
Inpractice, and within the current version of MICA,this module is usually used with n = 1, which iden-tifies the optimal tree w.r.t.
the probabilistic modelembedded in the original PCFG; other values canalso be used.
Once the n-best trees have been ex-tracted, the dependency extractor module transformseach of these trees into a dependency tree, by ex-ploiting the fact that the CFG used for parsing hasbeen built from a TIG.5 EvaluationWe compare MICA to the MALT parser.
Bothparsers are trained on sections 02-21 of our de-pendency version of the WSJ PennTreebank, andtested on Section 00, not counting true punctuation.?Predicted?
refers to tags (PTB-tagset POS and su-pertags) predicted by our taggers; ?Gold?
refers tothe gold POS and supertags.
We tested MALT usingonly POS tags (MALT-POS), and POS tags as wellas 1-best supertags (MALT-all).
We provide unla-beled (?Un?)
and labeled (?Lb?)
dependency accu-racy (%).
As we can see, the predicted supertags donot help MALT.
MALT is significantly slower thanMICA, running at about 30 words a second (MICA:450 words a second).MICA MALT-POS MALT-allPred Gold Pred Gold Pred GoldLb 85.8 97.3 86.9 87.4 86.8 96.9Un 87.6 97.6 88.9 89.3 88.5 97.2ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?266.Srinivas Bangalore, Patrick Haffner, and Gae?l Emami.2005.
Factoring global inference by enriching local rep-resentations.
Technical report, AT&T Labs ?
Reserach.Pierre Boullier and Philippe Deschamp.1988.
Le syste`me SYNTAXTM ?
manueld?utilisation et de mise en ?uvre sous UNIXTM.http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.Pierre Boullier and Beno?
?t Sagot.
2007.
Are very largegrammars computationnaly tractable?
In Proceedings ofIWPT?07, Prague, Czech Republic.Pierre Boullier.
2003.
Guided Earley parsing.
In Pro-ceedings of the 7th International Workshop on =20 Pars-ing Technologies, pages 43?54, Nancy, France.John Chen.
2001.
Towards Efficient Statistical ParsingUsing Lexicalized Grammatical Information.
Ph.D. the-sis, University of Delaware.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In ACL?04.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communication of the ACM, 13(2):94?102.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of IWPT?05, Vancouver, Canada.Rebecca Hwa.
2001.
Learning Probabilistic LexicalizedGrammars for Natural Language Processing.
Ph.D. the-sis, Harvard University.Aravind K. Joshi.
1987.
An introduction to Tree Ad-joining Grammars.
In A. Manaster-Ramer, editor, Math-ematics of Language.
John Benjamins, Amsterdam.Roger Levy and Christopher Manning.
2004.
Deep de-pendencies from context-free statistical parsers: Correct-ing the surface dependency approximation.
In ACL?04.Dekang Lin.
1994.
PRINCIPAR?an efficient, broad-coverage, principle-based parser.
In Coling?94.Alexis Nasr and Owen Rambow.
2006.
Parsing withlexicalized probabilistic recursive transition networks.
InFinite-State Methods and Natural Language Processing,Springer Verlag Lecture Notes in Commputer Science.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In CoNLL-2004.Yves Schabes and Stuart Shieber.
1994.
An alternativeconception of tree-adjoining derivation.
ComputationalLinguistics, 1(20):91?124.Yves Schabes and Richard C. Waters.
1995.
Tree Inser-tion Grammar.
Computational Linguistics, 21(4).Libin Shen and Aravind Joshi.
2005.
Incremental ltagparsing.
In HLT-EMNLP?05.Wen Wang and Mary P. Harper.
2004.
A statistical con-straint dependency grammar (CDG) parser.
In Proceed-ings of the ACL Workshop on Incremental Parsing.188
