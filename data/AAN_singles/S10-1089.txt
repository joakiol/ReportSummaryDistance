Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 396?401,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTreeMatch: A Fully Unsupervised WSD System Using DependencyKnowledge on a Specific DomainAndrew Tran          Chris Bowes         David Brown         Ping ChenUniversity of Houston-DowntownMax Choly           Wei DingUniversity of Massachusetts-BostonAbstractWord sense disambiguation (WSD) is one ofthe main challenges in ComputationalLinguistics.
TreeMatch is a WSD systemoriginally developed using data from SemEval2007 Task 7 (Coarse-grained English All-words Task) that has been adapted for use inSemEval 2010 Task 17 (All-words WordSense Disambiguation on a Specific Domain).The system is based on a fully unsupervisedmethod using dependency knowledge drawnfrom a domain specific knowledge base thatwas built for this task.
When evaluated on thetask, the system precision performs above theFirst Sense Baseline.1 IntroductionThere are many words within naturallanguages that can have multiple meanings orsenses depending on its usage.
These words arecalled homographs.
Word sense disambiguationis the process of determining which sense of ahomograph is correct in a given context.
MostWSD systems use supervised methods toidentify senses and tend to achieve the bestresults.
However, supervised systems rely onmanually annotated training corpora.Availability of manually tagged corpora islimited and generating these corpora is costlyand time consuming.
With our TreeMatchsystem, we use a fully unsupervised domain-independent method that only requires adictionary (WordNet, Fallbaum, 1998.)
andunannotated text as input (Chen et.al, 2009).WSD systems trained on general corpora tendto perform worse when disambiguating wordsfrom a document on a specific domain.
TheSemEval 2010 WSD-domain task (Agirre et.
al.,2010) addresses this issue by testing participantsystems on documents from the environmentdomain.
The environment domain specificcorpus for this taskwas built from documents contributed by theEuropean Centre for Nature Conservation(ECNC) and the World Wildlife Fund (WWF).We adapted our existing TreeMatch system fromrunning on a general context knowledge base toone targeted at the environment domain.This paper is organized as follows.
Section 2will detail the construction of the knowledgebase.
In Section 3 the WSD algorithm will beexplained.
The construction procedure and WSDalgorithm described in these two sections aresimilar to the procedure presented in ourNAACL 2009 paper (Chen et.al, 2009).
InSection 4 we present our experiments andresults, and Section 5 discusses related work onWSD.
Section 6 finishes the paper withconclusions.2 Context Knowledge Acquisition andRepresentationFigure 1 shows an overview of our contextknowledge acquisition process.
The collectedknowledge is saved in a local knowledge base.Here are some details about each step.Figure 1: Context Knowledge Acquisition andRepresentation Process2.1 Corpus Building Through Web SearchThe goal of this step is to collect as manyvalid sample sentences as possible that containinstances of the target word.
Preferably theseinstances are also diverse enough to contain allthe different glosses of a word.396The World Wide Web is a boundless sourceof textual information that can be utilized forcorpus building.
This huge dynamic textcollection represents a wide cross section ofwriting backgrounds that may not be representedin other corpora and may be able to betterrepresent common human knowledge.However, because the content on the internetis not necessarily checked for grammatical orfactual accuracy, concerns may arise about theuse of a corpus built from it.
The quality ofcontext knowledge will be affected by sentencesof poor linguistic and poor word usage but fromour experience these kind of errors are negligiblewhen weighted against the staggering volume ofvalid content also retrieved.To start the acquisition process, words that arecandidates for disambiguation are compiled andsaved in a text file as seeds for search queries.Each single word is submitted to a Web searchengine as a query.
Several search enginesprovide API?s for research communities toautomatically retrieve large number of Webpages.
In our experiments we used MSN Bing!API (Bing!, 2010) to retrieve up to 1,000 Webpages and PDF documents for each to-be-disambiguated word.
Collected Web pages arecleaned first, e.g., control characters and HTMLtags are removed.
Then sentences are segmentedsimply based on punctuation (e.g., ?, !, .).
PDFfiles undergo a similar cleaning process, exceptthat they are converted from PDF to HMTLbeforehand.
Sentences that contain the instancesof a specific word are extracted and saved into alocal repository.2.2 ParsingAfter the sentences have been cleaned andsegmented they are sent to the dependencyparser Minipar (Lin, 1998).
After parsing,sentences are converted to parsing trees andsaved into files.
The files contain the weights ofall connections between all words existingwithin the knowledge base.
Parsing tends to takethe most time in the entire WSD process.Depending on the initial size of the corpus,parsing can take weeks.
The long parsing timecan be attributed to Minipar?s execution throughsystem calls and also to the lack ofmultithreading used.
However, we only need toparse the corpus once to construct the knowledgebase.
Any further parsing is only done on theinput sentences from the words to-be-disambiguated, and the glosses of those words.2.3 Merging dependency relationsAfter parsing, dependency relations fromdifferent sentences are merged and saved in acontext knowledge base.
The merging process isstraightforward.
A dependency relation includesone head word/node and one dependentword/node.
Nodes from different dependencyrelations are merged into one as long as theyrepresent the same word.
An example is shownin Figure 2, which merges the following twosentences:?Computer programmers write software.?
?Many companies hire computerprogrammers.
?Figure 2: Merging two parsing trees.
The numberbeside each edge is the number of occurrences of thisdependency relation existing in the contextknowledge base.In a dependency relation ?word1 -> word2?,word1 is the head word, and word2 is thedependent word.
After merging dependencyrelations, we will obtain a weighted directedgraph with a word as a node, a dependencyrelation as an edge, and the number ofoccurrences of dependency relation as weight ofan edge.
This weight indicates the strength ofsemantic relevancy of head word and dependentword.
This graph will be used in the followingWSD process as our context knowledge base.
Asa fully automatic knowledge acquisition process,it is inevitable to include erroneous dependencyrelations in the knowledge base.
However, sincein a large text collection valid dependencyrelations tend to repeat far more times thaninvalid ones, these erroneous edges only haveminimal impact on the disambiguation quality asshown in our evaluation results.3973 WSD AlgorithmOur WSD approach is based on the followinginsight:If a word is semantically coherent with itscontext, then at least one sense of this word issemantically coherent with its context.Assuming that the documents given aresemantically coherent, if we replace a targetedto-be-disambiguated word with its glosses oneby one, eventually one of the glosses will havesemantic coherence within the context of itssentence.
From that idea we can show theoverview of our WSD procedure in Figure 3.
Fora given to-be-disambiguated word, its glossesfrom WordNet are parsed one by one along withthe original sentence of the target word.
Thesemantic coherency between the parse tree ofeach individual gloss and the parse tree of theoriginal sentence are compared one by one todetermine which sense is the most relevant.Figure 3: WSD ProcedureTo measure the semantic coherence we usethe following hypotheses (assume word1 is theto-be-disambiguated word):?
If in a sentence word1 is dependent on word2,and we denote the gloss of the correct senseof word1 as g1i, then g1i contains the mostsemantically coherent words that aredependent on word2;?
If in a sentence a set of words DEP1 aredependent on word1, and we denote the glossof the correct sense of word1 as g1i, then g1icontains the most semantically coherentwords that DEP1 are dependent on.These hypotheses are used for the functions inFigure 4.
The TreeMatching function uses whatwe call dependency matching to ascertain thecorrect sense of the to-be-disambiguated word.NodeMatching function is an extension fromLesk algorithm (Lesk, 1986).Input: Glosses from WordNet;S: the to-be-disambiguated sentence;G: the knowledge base generated in Section 2;1.
Input a sentence S, W = {w| w?s part of speechis noun, verb, adjective, or adverb, w ?
S};2.
Parse S with a dependency parser, generateparsing tree TS;3.
For each w ?W {4.
Input all w?s glosses from WordNet;5.
For each gloss wi {6.
Parse wi, get a parsing tree Twi;7.   scored = TreeMatching(TS, Twi);Scoren = NodeMatching(TS, Twi);}8.
If the highest scored and Scoren indicatethe sense, choose this sense;9.
Otherwise, choose the first sense.10.
}TreeMatching(TS, Twi)11.
For each node nSi ?TS {12.
Assign weight wSi =1?????
?, lSi is thelength between nSi and wi in TS;13.
}14.
For each node nwi ?
Twi {15.
Load its dependent words Dwi from G;16.
Assign weight wwi =1?????
?, lwi is thelevel number of nwi in Twi;17.
For each nSj {18.
If nSj ?
Dwi19.
calculate connection strength sjibetween nSj and nwi;20.   score = score + wSi ?
wwi ?
sji;21.
}}22.
Return score;NodeMatching (TS, Twi)23.
For each node nSi ?TS {24.
Assign weight wwi =1?????
?, lwi is thelevel number of nwi in Twi;25.
For each nSj {28.
If nSi == wwi29.
score = score + wSi ?
wwi}}Figure 4: WSD Algorithm4 ExperimentThe WSD-domain task for SemEval 2010focused on the environment domain.
To preparefor the tests, we constructed a new domainspecific knowledge base.398Table 1: Fine-Grained SemEval 2010 Task 17Disambiguation ScoresSince we knew the task?s domain specificcorpus would be derived from ECNC and WWFmaterials, we produced our query list from thesame source.
A web crawl starting from both theECNC and WWF main web pages wasperformed that retrieved 772 PDF documents.Any words that were in the PDFs and also hadmore than one gloss in WordNet were retainedfor Bing!
search queries to start the acquisitionprocess as described in section 2.
10779 uniquewords were obtained in this manner.Using the 10779 unique words for searchqueries, the web page and PDF retrieval steptook 35 days, collecting over 3 TB of raw htmland PDF files, and the cleaning and sentenceextraction step took 2 days, reducing it down to3 GB of relevant sentences, while running on 5machines.
Parsing took 26 days and mergingtook 6 days on 9 machines.
From the parse treeswe obtained 2202295 total nodes with anaverage of 87 connections and 13 dependents pernode.Each machine was a 2.66 GHz dual core PCwith 2 GB of memory with a total of 10machines used throughout the process.There were 3 test documents provided by thetask organizers with about 6000 total words and1398 to-be-disambiguated words.Disambiguation of the target words took 1.5hours for each complete run.
Each run used thesame WSD procedure with different parameters.The overall disambiguation results are shownin Table 1.
The precision of our best submissionedged out the First Sense Baseline (1sense)baseline by .001 and is ahead of the Randomselection baseline by .276.The recall of our submissions is lower thanthe precision because of our reliance on Miniparfor the part of speech and lemma information ofthe target words.
Sometimes Minipar would givean incorrect lemma which at times cannot befound in WordNet and thus our system wouldnot attempt to disambiguate the words.
Previoustasks provided the lemma and part of speech fortarget words so we were able to bypass that step.5 Related workGenerally WSD techniques can be divided intofour categories (Agirre, 2006),?
Dictionary and knowledge based methods.These methods use lexical knowledge bases(LKB) such as dictionaries and thesauri, andextract knowledge from word definitions(Lesk, 1986) and relations amongwords/senses.
Recently, several graph-basedWSD methods were proposed.
In theseapproaches, first a graph is built with sensesas nodes and relations among words/senses(e.g., synonymy, antonymy) as edges, andthe relations are usually acquired from aLKB (e.g., Wordnet).
Then a rankingalgorithm is conducted over the graph, andsenses ranked the highest are assigned to thecorresponding words.
Different relations andranking algorithms were experimented withthese methods, such as TexRank algorithm(Mihalcea, 2005), personalized PageRankalgorithm (Agirre, 2009), a two-stagesearching algorithm (Navigli, 2007),Structural Semantic Interconnectionsalgorithm (Navigli, 2005), centralityalgorithms (Sinha, 2009).?
Supervised methods.
A supervised methodincludes a training phase and a testing phase.In the training phase, a sense-annotatedtraining corpus is required, from whichsyntactic and semantic features are extractedto build a classifier using machine learningtechniques, such as Support Vector Machine(Novisch, 2007).
In the following testingphase, the classifier picks the best sense for aword based on its surrounding words(Mihalcea, 2002).
Currently supervisedmethods achieved the best disambiguationquality (about 80% in precision and recallfor coarse-grained WSD in the most recentWSD evaluation conference SemEval 2007(Novisch, 2007).
Nevertheless, since trainingcorpora are manually annotated andexpensive, supervised methods are oftenbrittle due to data scarcity, and it isimpractical to manually annotate hugenumber of words existing in a naturallanguage.?
Semi-supervised methods.
To overcome theknowledge acquisition bottleneck suffered insupervised methods, semi-supervisedmethods make use of a small annotatedcorpus as seed data in a bootstrappingprocess (Hearst, 1991) (Yarowsky, 1995).
ASystem Precision Recall1sense 0.505 0.505TreeMatch-1 0.506 0.493TreeMatch-2 0.504 0.491TreeMatch-3 0.492 0.479Random 0.23 0.23399word-aligned bilingual corpus can also serveas seed data (Zhong, 2009).?
Unsupervised methods.
These methodsacquire knowledge from unannotated rawtext, and induce senses using similaritymeasures (Lin, 1997).
Unsupervisedmethods overcome the problem ofknowledge acquisition bottleneck, but noneof existing methods can outperform the mostfrequent sense baseline, which makes themnot useful at all in practice.
The bestunsupervised systems only achieved about70% in precision and 50% in recall in theSemEval 2007 (Navigli, 2007).
One recentstudy utilized automatically acquireddependency knowledge and achieved 73% inprecision and recall (Chen, 2009), which isstill below the most-frequent-sense baseline(78.89% in precision and recall in theSemEval 2007 Task 07).Additionally there exist some ?meta-disambiguation?
methods that ensemble multipledisambiguation algorithms following the ideas ofbagging or boosting in supervised learning(Brody, 2006).6 ConclusionThis paper has described a WSD systemwhich has been adapted for use in a specificdomain for SemEval 2010 Task 17: All-WordsWord Sense Disambiguation on a SpecificDomain.
Our system has shown that domainadaptation can be handled by unsupervisedsystems without the brittleness of supervisedmethods by utilizing readily availableunannotated text from internet sources and stillachieve viable results.AcknowledgmentsThis work is partially funded by NationalScience Foundation grants CNS 0851984 andDHS #2009-ST-061-C10001.ReferencesE.
Agirre, Philip Edmonds, editors.
Word SenseDisambiguation: Algorithms and Applications,Springer.
2006.E.
Agirre, O. Lopez de Lacalle, C. Fellbaum, S.Hsieh, M. Tesconi, P. Vossen, and R. Segers.SemEval-2010 Task 17: All-words Word SenseDisambiguation on a Specific Domain.
InProceedings of the 5th International Workshop onSemantic Evaluations(SemEval-2010),Association for Computational Linguistics,Uppsala, Sweden.
2010.E.
Agirre, A. Soroa.
Personalizing pagerank for wordsense disambiguation.
In Proceedings of the 12thconference of the European chapter of theAssociation for Computational Linguistics(EACL-2009).Bing!
API, available at msdn.microsoft.comA.
Brody, R. Navigli, M. Lapata, Ensemble MethodsFor Unsupervised WSD, COLING-ACL, 2006P.
Chen, W. Ding, C. Bowes, D. Brown.
2009.
AFully Unsupervised Word Sense DisambiguationMethod and Its Evaluation on Coarse-grained All-words Task, NAACL 2009.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase, MIT press, 1998M.
Hearst.
Noun Homograph Disambiguation UsingLocal Context in Large Text Corpora.
Proc.
7thAnnual Conference of the Univ.
of WaterlooCenter for the New OED and Text Research,Oxford.
1991.M.
Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedingsof the 5th Annual international Conference onSystems Documentation (Toronto, Ontario,Canada).
V. DeBuys, Ed.
SIGDOC ?86.D.
Lin.
Using syntactic dependency as local contextto resolve word sense ambiguity.
In Proceedingsof the 35th Annual Meeting of the Association ForComputational Linguistics and Eighth Conferenceof the European Chapter of the Association ForComputational Linguistics.
1997.D.
Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the LREC Workshopon the Evaluation of Parsing Systems, pages 234?241, Granada, Spain.R.
Mihalcea.
Unsupervised Large-Vocabulary WordSense Disambiguation with Graph-basedAlgorithms for Sequence Data Labeling, inProceedings of the Joint Conference on HumanLanguage Technology Empirical Methods inNatural Language Processing (HLT/EMNLP),Vancouver, October, 2005.R.
Mihalcea.
Instance based learning with automaticfeature selection applied to word sensedisambiguation.
In Proceedings of the 19thInternational Conference on Computationallinguistics.
2002.R.
Navigli, Mirella Lapata.
Graph ConnectivityMeasures for Unsupervised Word SenseDisambiguation.
IJCAI 2007R.
Navigli, Paola Velardi.
Structural semanticinterconnections: a knowledge-based approach toword sense disambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence(PAMI), 27(7):1063-1074.
2005.A.
Novischi, Muirathnam Srikanth, and AndrewBennett.
Lcc-wsd: System description for English400coarse grained all words task at semeval 2007.Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages223--226, Prague, Czech Republic.
2007.R.
Sinha, Rada Mihalcea.
Unsupervised Graph-basedWord Sense Disambiguation, in ?Current Issues inLinguistic Theory: Recent Advances in NaturalLanguage Processing?, Editors Nicolas Nicolovand Ruslan Mitkov, John Benjamins, 2009.D.
Yarowsky.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting onAssociation For Computational Linguistics,Cambridge, Massachusetts, 1995.Z.
Zhong, Hwee Tou Ng.
Word SenseDisambiguation for All Words without HardLabor.
In Proceeding of the Twenty-firstInternational Joint Conference on ArtificialIntelligence.
2009.401
