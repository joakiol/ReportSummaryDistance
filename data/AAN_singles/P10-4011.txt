Proceedings of the ACL 2010 System Demonstrations, pages 60?65,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsSpeech-driven Access to the Deep Web on Mobile DevicesTaniya Mishra and Srinivas BangaloreAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932 USA.
{taniya,srini}@research.att.com.AbstractThe Deep Web is the collection of infor-mation repositories that are not indexedby search engines.
These repositories aretypically accessible through web formsand contain dynamically changing infor-mation.
In this paper, we present a sys-tem that allows users to access such richrepositories of information on mobile de-vices using spoken language.1 IntroductionThe World Wide Web (WWW) is the largestrepository of information known to mankind.
Itis generally agreed that the WWW continues tosignificantly enrich and transform our lives in un-precedent ways.
Be that as it may, the WWW thatwe encounter is limited by the information thatis accessible through search engines.
Search en-gines, however, do not index a large portion ofWWW that is variously termed as the Deep Web,Hidden Web, or Invisible Web.Deep Web is the information that is in propri-etory databases.
Information in such databases isusually more structured and changes at higher fre-quency than textual web pages.
It is conjecturedthat the Deep Web is 500 times the size of thesurface web.
Search engines are unable to indexthis information and hence, unable to retrieve itfor the user who may be searching for such infor-mation.
So, the only way for users to access thisinformation is to find the appropriate web-form,fill in the necessary search parameters, and use itto query the database that contains the informationthat is being searched for.
Examples of such webforms include, movie, train and bus times, and air-line/hotel/restaurant reservations.Contemporaneously, the devices to access infor-mation have moved out of the office and home en-vironment into the open world.
The ubiquity ofmobile devices has made information access anany time, any place activity.
However, informa-tion access using text input on mobile devices is te-dious and unnatural because of the limited screenspace and the small (or soft) keyboards.
In addi-tion, by the mobile nature of these devices, usersoften like to use them in hands-busy environments,ruling out the possibility of typing text.
Fillingweb-forms using the small screens and tiny key-boards of mobile devices is neither easy nor quick.In this paper, we present a system, Qme!, de-signed towards providing a spoken language inter-face to the Deep Web.
In its current form, Qme!provides a unifed interface onn iPhone (shown inFigure 1) that can be used by users to search forstatic and dynamic questions.
Static questions arequestions whose answers to these questions re-main the same irrespective of when and where thequestions are asked.
Examples of such questionsare What is the speed of light?, When is GeorgeWashington?s birthday?.
For static questions, thesystem retrieves the answers from an archive ofhuman generated answers to questions.
This en-sures higher accuracy for the answers retrieved (iffound in the archive) and also allows us to retrieverelated questions on the user?s topic of interest.Figure 1: Retrieval results for static and dynamicquestions using Qme!Dynamic questions are questions whose an-swers depend on when and where they are asked.Examples of such questions are What is the stockprice of General Motors?, Who won the game lastnight?, What is playing at the theaters near me?.60The answers to dynamic questions are often part ofthe DeepWeb.
Our system retrieves the answers tosuch dynamic questions by parsing the questionsto retrieve pertinent search keywords, which are inturn used to query information databases accessi-ble over the Internet using web forms.
However,the internal distinction between dynamic and staticquestions, and the subsequent differential treat-ment within the system is seamless to the user.
Theuser simply uses a single unified interface to ask aquestion and receive a collection of answers thatpotentially address her question directly.The layout of the paper is as follows.
In Sec-tion 2, we present the system architecture.
InSection 3, we present bootstrap techniques to dis-tinguish dynamic questions from static questions,and evaluate the efficacy of these techniques on atest corpus.
In Section 4, we show how our systemretrieves answers to dynamic questions.
In Sec-tion 5, we show how our system retrieves answersto static questions.
We conclude in Section 6.2 Speech-driven Question AnswerSystemSpeech-driven access to information has been apopular application deployed by many compa-nies on a variety of information resources (Mi-crosoft, 2009; Google, 2009; YellowPages, 2009;vlingo.com, 2009).
In this prototype demonstra-tion, we describe a speech-driven question-answerapplication.
The system architecture is shown inFigure 2.The user of this application provides a spokenlanguage query to a mobile device intending tofind an answer to the question.
The speech recog-nition module of the system recognizes the spo-ken query.
The result from the speech recognizercan be either a single-best string or a weightedword lattice.1 This textual output of recognition isthen used to classify the user query either as a dy-namic query or a static query.
If the user query isstatic, the result of the speech recognizer is used tosearch a large corpus of question-answer pairs toretrieve the relevant answers.
The retrieved resultsare ranked using tf.idf based metric discussed inSection 5.
If the user query is dynamic, the an-swers are retrieved by querying a web form fromthe appropriate web site (e.g www.fandango.comfor movie information).
In Figure 1, we illustratethe answers that Qme!returns for static and dy-1For this paper, the ASR used to recognize these utter-ances incorporates an acoustic model adapted to speech col-lected from mobile devices and a four-gram language modelthat is built from the corpus of questions.namic questions.Lattice1?bestQ&A corpusASRSpeechDynamicClassifyfrom WebRetrieveRankSearchRanked ResultsMatchFigure 2: The architecture of the speech-drivenquestion-answering system2.1 DemonstrationIn the demonstration, we plan to show the usersstatic and dynamic query handling on an iPhoneusing spoken language queries.
Users can use theiphone and speak their queries using an interfaceprovided by Qme!.
A Wi-Fi access spot will makethis demonstation more compelling.3 Dynamic and Static QuestionsAs mentioned in the introduction, dynamic ques-tions require accessing the hidden web through aweb form with the appropriate parameters.
An-swers to dynamic questions cannot be preindexedas can be done for static questions.
They dependon the time and geographical location of the ques-tion.
In dynamic questions, there may be no ex-plicit reference to time, unlike the questions in theTERQAS corpus (Radev and Sundheim., 2002)which explicitly refer to the temporal propertiesof the entities being questioned or the relative or-dering of past and future events.The time-dependency of a dynamic questionlies in the temporal nature of its answer.
For exam-ple, consider the question, What is the address ofthe theater White Christmas is playing at in NewYork?.
White Christmas is a seasonal play thatplays in New York every year for a few weeksin December and January, but not necessarily atthe same theater every year.
So, depending whenthis question is asked, the answer will be differ-ent.
If the question is asked in the summer, theanswer will be ?This play is not currently playinganywhere in NYC.?
If the question is asked dur-ing December, 2009, the answer might be differentthan the answer given in December 2010, becausethe theater at which White Christmas is playingdiffers from 2009 to 2010.There has been a growing interest in tempo-ral analysis for question-answering since the late1990?s.
Early work on temporal expressions iden-61tification using a tagger culminated in the devel-opment of TimeML (Pustejovsky et al, 2001),a markup language for annotating temporal ex-pressions and events in text.
Other examples in-clude, QA-by-Dossier with Constraints (Prager etal., 2004), a method of improving QA accuracy byasking auxiliary questions related to the originalquestion in order to temporally verify and restrictthe original answer.
(Moldovan et al, 2005) detectand represent temporally related events in naturallanguage using logical form representation.
(Sa-quete et al, 2009) use the temporal relations in aquestion to decompose it into simpler questions,the answers of which are recomposed to producethe answers to the original question.3.1 Question Classification: Dynamic andStatic QuestionsWe automatically classify questions as dynamicand static questions.
The answers to static ques-tions can be retrieved from the QA archive.
To an-swer dynamic questions, we query the database(s)associated with the topic of the question throughweb forms on the Internet.
We first use a topicclassifier to detect the topic of a question followedby a dynamic/static classifier trained on questionsrelated to a topic, as shown in Figure 3.
For thequestion what movies are playing around me?,we detect it is a movie related dynamic ques-tion and query a movie information web site (e.g.www.fandango.com) to retrieve the results basedon the user?s GPS information.Dynamic questions often contain temporal in-dexicals, i.e., expressions of the form today, now,this week, two summers ago, currently, recently,etc.
Our initial approach was to use such signalwords and phrases to automatically identify dy-namic questions.
The chosen signals were basedon annotations in TimeML.
We also included spa-tial indexicals, such as here and other clauses thatwere observed to be contained in dynamic ques-tions such as cost of, and how much is in the list ofsignal phrases.
These signals words and phraseswere encoded into a regular-expression-based rec-ognizer.This regular-expression based recognizer iden-tified 3.5% of our dataset ?
which consisted ofseveral million questions ?
as dynamic.
The typeof questions identified were What is playing inthe movie theaters tonight?, What is tomorrow?sweather forecast for LA?, Where can I go to getThai food near here?
However, random samplingsof the same dataset, annotated by four independenthuman labelers, indicated that on average 13.5%of the dataset is considered dynamic.
This showsthat the temporal and spatial indexicals encoded asa regular-expression based recognizer is unable toidentify a large percentage of the dynamic ques-tions.This approach leaves out dynamic questionsthat do not contain temporal or spatial indexicals.For example, What is playing at AMC Loew?s?, orWhat is the score of the Chargers and Dolphinesgame?.
For such examples, considering the tenseof the verb in question may help.
The last two ex-amples are both in the present continuous tense.But verb tense does not help for a question suchas Who got voted off Survivor?.
This question iscertainly dynamic.
The information that is mostlikely being sought by this question is what is thename of the person who got voted off the TV showSurvivor most recently, and not what is the nameof the person (or persons) who have gotten votedoff the Survivor at some point in the past.Knowing the broad topic (such as movies, cur-rent affairs, and music) of the question may bevery useful.
It is likely that there may be manydynamic questions about movies, sports, and fi-nance, while history and geography may have fewor none.
This idea is bolstered by the followinganalysis.
The questions in our dataset are anno-tated with a broad topic tag.
Binning the 3.5%of our dataset identified as dynamic questions bytheir broad topic produced a long-tailed distribu-tion.
Of the 104 broad topics, the top-5 topics con-tained over 50% of the dynamic questions.
Thesetop five topics were sports, TV and radio, events,movies, and finance.Considering the issues laid out in the previ-ous section, our classification approach is to chaintwo machine-learning-based classifiers: a topicclassifier chained to a dynamic/static classifier, asshown in Figure 3.
In this architecture, we buildone topic classifier, but several dynamic/staticclassifiers, each trained on data pertaining to onebroad topic.Figure 3: Chaining two classifiersWe used supervised learning to train the topic62classifier, since our entire dataset is annotated byhuman experts with topic labels.
In contrast, totrain a dynamic/static classifier, we experimentedwith the following three different techniques.Baseline: We treat questions as dynamic if theycontain temporal indexicals, e.g.
today, now, thisweek, two summers ago, currently, recently, whichwere based on the TimeML corpus.
We also in-cluded spatial indexicals such as here, and othersubstrings such as cost of and how much is.
Aquestion is considered static if it does not containany such words/phrases.Self-training with bagging: The general self-training with bagging algorithm (Banko and Brill,2001).
The benefit of self-training is that we canbuild a better classifier than that built from thesmall seed corpus by simply adding in the largeunlabeled corpus without requiring hand-labeling.Active-learning: This is another popular methodfor training classifiers when not much annotateddata is available.
The key idea in active learningis to annotate only those instances of the datasetthat are most difficult for the classifier to learn toclassify.
It is expected that training classifiers us-ing this method shows better performance than ifsamples were chosen randomly for the same hu-man annotation effort.We used the maximum entropy classifier inLLAMA (Haffner, 2006) for all of the above clas-sification tasks.
We have chosen the active learn-ing classifier due to its superior performance andintegrated it into the Qme!
system.
We pro-vide further details about the learning methods in(Mishra and Bangalore, 2010).3.2 Experiments and Results3.2.1 Topic ClassificationThe topic classifier was trained using a trainingset consisting of over one million questions down-loaded from the web which were manually labeledby human experts as part of answering the ques-tions.
The test set consisted of 15,000 randomlyselected questions.
Word trigrams of the questionare used as features for a MaxEnt classifier whichoutputs a score distribution on all of the 104 pos-sible topic labels.
The error rate results for modelsselecting the top topic and the top two topics ac-cording to the score distribution are shown in Ta-ble 1.
As can be seen these error rates are far lowerthan the baseline model of selecting the most fre-quent topic.Model Error RateBaseline 98.79%Top topic 23.9%Top-two topics 12.23%Table 1: Results of topic classification3.2.2 Dynamic/static ClassificationAs mentioned before, we experimented withthree different approaches to bootstrapping a dy-namic/static question classifier.
We evaluatedthese methods on a 250 question test set drawnfrom the broad topic of Movies.
The error ratesare summarized in Table 2.
We provide further de-tails of this experiment in (Mishra and Bangalore,2010).Training approach Lowest Error rateBaseline 27.70%?Supervised?
learning 22.09%Self-training 8.84%Active-learning 4.02%Table 2: Best Results of dynamic/static classifica-tion4 Retrieving answers to dynamicquestionsFollowing the classification step outlined in Sec-tion 3.1, we know whether a user query is static ordynamic, and the broad category of the question.If the question is dynamic, then our system per-forms a vertical search based on the broad topicof the question.
In our system, so far, we have in-corporated vertical searches on three broad topics:Movies, Mass Transit, and Yellow Pages.For each broad topic, we have identified a fewtrusted content aggregator websites.
For example,for dynamic questions related to Movies-relateddynamic user queries, www.fandango.com isa trusted content aggregator website.
Other suchtrusted content aggregator websites have beenidentified for Mass Transit related and for Yellow-pages related dynamic user queries.
We have alsoidentified the web-forms that can be used to searchthese aggregator sites and the search parametersthat these web-forms need for searching.
So, givena user query, whose broad category has been deter-mined and which has been classified as a dynamicquery by the system, the next step is to parse thequery to obtain pertinent search parameters.The search parameters are dependent on thebroad category of the question, the trusted con-tent aggregator website(s), the web-forms associ-ated with this category, and of course, the content63of the user query.
From the search parameters, asearch query to the associated web-form is issuedto search the related aggregator site.
For exam-ple, for a movie-related query, What time is Twi-light playing in Madison, New Jersey?, the per-tinent search parameters that are parsed out aremovie-name: Twilight, city: Madison, and state:New Jersey, which are used to build a search stringthat Fandango?s web-form can use to search theFandango site.
For a yellow-pages type of query,Where is the Saigon Kitchen in Austin, Texas?, thepertinent search parameters that are parsed out arebusiness-name: Saigon Kitchen, city: Austin, andstate: Texas, which are used to construct a searchstring to search the Yellowpages website.
Theseare just two examples of the kinds of dynamic userqueries that we encounter.
Within each broad cat-egory, there is a wide variety of the sub-types ofuser queries, and for each sub-type, we have toparse out different search parameters and use dif-ferent web-forms.
Details of this extraction arepresented in (Feng and Bangalore, 2009).It is quite likely that many of the dynamicqueries may not have all the pertinent search pa-rameters explicitly outlined.
For example, a masstransit query may be When is the next train toPrinceton?.
The bare minimum search parametersneeded to answer this query are a from-location,and a to-location.
However, the from-location isnot explicitly present in this query.
In this case,the from-location is inferred using the GPS sensorpresent on the iPhone (on which our system is builtto run).
Depending on the web-form that we arequerying, it is possible that we may be able to sim-ply use the latitude-longitude obtained from theGPS sensor as the value for the from-location pa-rameter.
At other times, we may have to performan intermediate latitude-longitude to city/state (orzip-code) conversion in order to obtain the appro-priate search parameter value.Other examples of dynamic queries in whichsearch parameters are not explicit in the query, andhence, have to be deduced by the system, includequeries such as Where is XMen playing?
and Howlong is Ace Hardware open?.
In each of theseexamples, the user has not specified a location.Based on our understanding of natural language,in such a scenario, our system is built to assumethat the user wants to find a movie theatre (or, isreferring to a hardware store) nearwhere he is cur-rently located.
So, the system obtains the user?slocation from the GPS sensor and uses it to searchfor a theatre (or locate the hardware store) withina five-mile radius of her location.In the last few paragraphs, we have discussedhow we search for answers to dynamic userqueries from the hidden web by using web-forms.However, the search results returned by these web-forms usually cannot be displayed as is in ourQme!
interface.
The reason is that the results areoften HTML pages that are designed to be dis-played on a desktop or a laptop screen, not a smallmobile phone screen.
Displaying the results asthey are returned from search would make read-ability difficult.
So, we parse the HTML-encodedresult pages to get just the answers to the userquery and reformat it, to fit the Qme!
interface,which is designed to be easily readable on theiPhone (as seen in Figure 1).25 Retrieving answers to static questionsAnswers to static user queries ?
questions whoseanswers do not change over time ?
are retrievedin a different way than answers to dynamic ques-tions.
A description of how our system retrievesthe answers to static questions is presented in thissection.0how:qa25/c1old:qa25/c2is:qa25/c3obama:qa25/c4old:qa150/c5how:qa12/c6obama:qa450/c7is:qa1450/c8Figure 4: An example of an FST representing thesearch index.5.1 Representing Search Index as an FSTTo obtain results for static user queries, wehave implemented our own search engine usingfinite-state transducers (FST), in contrast to usingLucene (Hatcher and Gospodnetic., 2004) as it isa more efficient representation of the search indexthat allows us to consider word lattices output byASR as input queries.The FST search index is built as follows.
Weindex each question-answer (QA) pair from ourrepository ((qi, ai), qai for short) using the words(wqi) in question qi.
This index is represented asa weighted finite-state transducer (SearchFST) asshown in Figure 4.
Here a word wqi (e.g old) is theinput symbol for a set of arcs whose output sym-bol is the index of the QA pairs where old appears2We are aware that we could use SOAP (Simple ObjectAccess Protocol) encoding to do the search, however not allaggregator sites use SOAP yet.64in the question.
The weight of the arc c(wqi ,qi) isone of the similarity based weights discussed inSection 4.1.
As can be seen from Figure 4, thewords how, old, is and obama contribute a score tothe question-answer pair qa25; while other pairs,qa150, qa12, qa450 are scored by only one ofthese words.5.2 Search Process using FSTsA user?s speech query, after speech recogni-tion, is represented as a finite state automaton(FSA, either 1-best or WCN), QueryFSA.
TheQueryFSA is then transformed into another FSA(NgramFSA) that represents the set of n-gramsof the QueryFSA.
In contrast to most text searchengines, where stop words are removed from thequery, we weight the query terms with their idf val-ues which results in a weighted NgramFSA.
TheNgramFSA is composed with the SearchFST andwe obtain all the arcs (wq, qawq , c(wq ,qawq )) wherewq is a query term, qawq is a QA index with thequery term and, c(wq ,qawq ) is the weight associ-ated with that pair.
Using this information, weaggregate the weight for a QA pair (qaq) acrossall query words and rank the retrieved QAs in thedescending order of this aggregated weight.
Weselect the top N QA pairs from this ranked list.The query composition, QA weight aggregationand selection of top N QA pairs are computedwith finite-state transducer operations as shownin Equations 1 and 23.
An evaluation of thissearch methodology on word lattices is presentedin (Mishra and Bangalore, 2010).D = pi2(NgramFSA ?
SearchFST ) (1)TopN = fsmbestpath(fsmdeterminize(D), N)(2)6 SummaryIn this demonstration paper, we have presentedQme!, a speech-driven question answering systemfor use on mobile devices.
The novelty of this sys-tem is that it provides users with a single unifiedinterface for searching both the visible and the hid-den web using the most natural input modality foruse on mobile phones ?
spoken language.7 AcknowledgmentsWe would like to thank Junlan Feng, MichaelJohnston and Mazin Gilbert for the help we re-ceived in putting this system together.
We would3We have dropped the need to convert the weights into thereal semiring for aggregation, to simplify the discussion.also like to thank ChaCha for providing us the dataincluded in this system.ReferencesM.
Banko and E. Brill.
2001.
Scaling to very verylarge corpora for natural language disambiguation.In Proceedings of the 39th annual meeting of the as-sociation for computational linguistics: ACL 2001,pages 26?33.J.
Feng and S. Bangalore.
2009.
Effects of word con-fusion networks on voice search.
In Proceedings ofEACL-2009, Athens, Greece.Google, 2009. http://www.google.com/mobile.P.
Haffner.
2006.
Scaling large margin classifiers forspoken language understanding.
Speech Communi-cation, 48(iv):239?261.E.
Hatcher and O. Gospodnetic.
2004.
Lucene in Ac-tion (In Action series).
Manning Publications Co.,Greenwich, CT, USA.Microsoft, 2009. http://www.live.com.T.
Mishra and S. Bangalore.
2010.
Qme!
: A speech-based question-answering system on mobile de-vices.
In Proceedings of NAACL-HLT.D.
Moldovan, C. Clark, and S. Harabagiu.
2005.
Tem-poral context representation and reasoning.
In Pro-ceedings of the 19th International Joint Conferenceon Artificial Intelligence, pages 1009?1104.J.
Prager, J. Chu-Carroll, and K. Czuba.
2004.
Ques-tion answering using constraint satisfaction: Qa-by-dossier-with-contraints.
In Proceedings of the 42ndannual meeting of the association for computationallinguistics: ACL 2004, pages 574?581.J.
Pustejovsky, R. Ingria, R.
Saur?
?, J. Casta no,J.
Littman, and R.
Gaizauskas., 2001.
The languageof time: A reader, chapter The specification languae?
TimeML.
Oxford University Press.D.
Radev and B. Sundheim.
2002.
Using timeml inquestion answering.
Technical report, Brandies Uni-versity.E.
Saquete, J. L. Vicedo, P.
Mart?
?nez-Barco, R. Mu noz,and H. Llorens.
2009.
Enhancing qa systems withcomplex temporal question processing capabilities.Journal of Artificial Intelligence Research, 35:775?811.vlingo.com, 2009.http://www.vlingomobile.com/downloads.html.YellowPages, 2009. http://www.speak4it.com.65
