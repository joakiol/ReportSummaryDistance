Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 122?132,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning New Semi-Supervised Deep Auto-encoder Featuresfor Statistical Machine TranslationShixiang Lu, Zhenbiao Chen, Bo XuInteractive Digital Media Technology Research Center (IDMTech)Institute of Automation, Chinese Academy of Sciences, Beijing, China{shixiang.lu,zhenbiao.chen,xubo}@ia.ac.cnAbstractIn this paper, instead of designing new fea-tures based on intuition, linguistic knowl-edge and domain, we learn some newand effective features using the deep auto-encoder (DAE) paradigm for phrase-basedtranslation model.
Using the unsupervisedpre-trained deep belief net (DBN) to ini-tialize DAE?s parameters and using the in-put original phrase features as a teacher forsemi-supervised fine-tuning, we learn newsemi-supervised DAE features, which aremore effective and stable than the unsuper-vised DBN features.
Moreover, to learnhigh dimensional feature representation,we introduce a natural horizontal compo-sition of more DAEs for large hidden lay-ers feature learning.
On two Chinese-English tasks, our semi-supervised DAEfeatures obtain statistically significant im-provements of 1.34/2.45 (IWSLT) and0.82/1.52 (NIST) BLEU points over theunsupervised DBN features and the base-line features, respectively.1 IntroductionRecently, many new features have been exploredfor SMT and significant performance have beenobtained in terms of translation quality, such assyntactic features, sparse features, and reorderingfeatures.
However, most of these features are man-ually designed on linguistic phenomena that arerelated to bilingual language pairs, thus they arevery difficult to devise and estimate.Instead of designing new features based on in-tuition, linguistic knowledge and domain, for thefirst time, Maskey and Zhou (2012) explored thepossibility of inducing new features in an unsuper-vised fashion using deep belief net (DBN) (Hintonet al, 2006) for hierarchical phrase-based trans-lation model.
Using the 4 original phrase fea-tures in the phrase table as the input features, theypre-trained the DBN by contrastive divergence(Hinton, 2002), and generated new unsupervisedDBN features using forward computation.
Thesenew features are appended as extra features to thephrase table for the translation decoder.However, the above approach has two majorshortcomings.
First, the input original featuresfor the DBN feature learning are too simple, thelimited 4 phrase features of each phrase pair,such as bidirectional phrase translation probabil-ity and bidirectional lexical weighting (Koehn etal., 2003), which are a bottleneck for learning ef-fective feature representation.
Second, it only usesthe unsupervised layer-wise pre-training of DBNbuilt with stacked sets of Restricted BoltzmannMachines (RBM) (Hinton, 2002), does not have atraining objective, so its performance relies on theempirical parameters.
Thus, this approach is un-stable and the improvement is limited.
In this pa-per, we strive to effectively address the above twoshortcomings, and systematically explore the pos-sibility of learning new features using deep (multi-layer) neural networks (DNN, which is usually re-ferred under the name Deep Learning) for SMT.To address the first shortcoming, we adapt andextend some simple but effective phrase featuresas the input features for new DNN feature learn-ing, and these features have been shown sig-nificant improvement for SMT, such as, phrasepair similarity (Zhao et al, 2004), phrase fre-quency, phrase length (Hopkins and May, 2011),and phrase generative probability (Foster et al,2010), which also show further improvement fornew phrase feature learning in our experiments.To address the second shortcoming, inspiredby the successful use of DAEs for handwrit-ten digits recognition (Hinton and Salakhutdinov,2006; Hinton et al, 2006), information retrieval(Salakhutdinov and Hinton, 2009; Mirowski et122al., 2010), and speech spectrograms (Deng et al,2010), we propose new feature learning usingsemi-supervised DAE for phrase-based translationmodel.
By using the input data as the teacher, the?semi-supervised?
fine-tuning process of DAE ad-dresses the problem of ?back-propagation withouta teacher?
(Rumelhart et al, 1986), which makesthe DAE learn more powerful and abstract features(Hinton and Salakhutdinov, 2006).
For our semi-supervised DAE feature learning task, we use theunsupervised pre-trained DBN to initialize DAE?sparameters and use the input original phrase fea-tures as the ?teacher?
for semi-supervised back-propagation.
Compared with the unsupervisedDBN features, our semi-supervised DAE featuresare more effective and stable.Moreover, to learn high dimensional featurerepresentation, we introduce a natural horizontalcomposition for DAEs (HCDAE) that can be usedto create large hidden layer representations simplyby horizontally combining two (or more) DAEs(Baldi, 2012), which shows further improvementcompared with single DAE in our experiments.It is encouraging that, non-parametric featureexpansion using gaussian mixture model (GMM)(Nguyen et al, 2007), which guarantees invari-ance to the specific embodiment of the originalfeatures, has been proved as a feasible feature gen-eration approach for SMT.
Deep models such asDNN have the potential to be much more represen-tationally efficient for feature learning than shal-low models like GMM.
Thus, instead of GMM,we use DNN (DBN, DAE and HCDAE) to learnnew non-parametric features, which has the sim-ilar evolution in speech recognition (Dahl et al,2012; Hinton et al, 2012).
DNN features arelearned from the non-linear combination of theinput original features, they strong capture high-order correlations between the activities of theoriginal features, and we believe this deep learn-ing paradigm induces the original features to fur-ther reach their potential for SMT.Finally, we conduct large-scale experimentson IWSLT and NIST Chinese-English translationtasks, respectively, and the results demonstratethat our solutions solve the two aforementionedshortcomings successfully.
Our semi-supervisedDAE features significantly outperform the unsu-pervised DBN features and the baseline features,and our introduced input phrase features signifi-cantly improve the performance of DAE featurelearning.The remainder of this paper is organized as fol-lows.
Section 2 briefly summarizes the recent re-lated work about the applications of DNN for SMTtasks.
Section 3 presents our introduced input fea-tures for DNN feature learning.
Section 4 de-scribes how to learn our semi-supervised DAE fea-tures for SMT.
Section 5 describes and discussesthe large-scale experimental results.
Finally, weend with conclusions in section 6.2 Related WorkRecently, there has been growing interest in use ofDNN for SMT tasks.
Le et al (2012) improvedtranslation quality of n-gram translation modelby using a bilingual neural LM, where transla-tion probabilities are estimated using a continu-ous representation of translation units in lieu ofstandard discrete representations.
Kalchbrennerand Blunsom (2013) introduced recurrent contin-uous translation models that comprise a class forpurely continuous sentence-level translation mod-els.
Auli et al (2013) presented a joint lan-guage and translation model based on a recur-rent neural network which predicts target wordsbased on an unbounded history of both sourceand target words.
Liu et al (2013) went be-yond the log-linear model for SMT and proposeda novel additive neural networks based translationmodel, which overcome some of the shortcom-ings suffered by the log-linear model: linearityand the lack of deep interpretation and represen-tation in features.
Li et al (2013) presented anITG reordering classifier based on recursive auto-encoders, and generated vector space representa-tions for variable-sized phrases, which enable pre-dicting orders to exploit syntactic and semanticinformation.
Lu et al (2014) adapted and ex-tended the max-margin based RNN (Socher et al,2011) into HPB translation with force decodingand converting tree, and proposed a RNN basedword topology model for HPB translation, whichsuccessfully capture the topological structure ofthe words on the source side in a syntactically andsemantically meaningful order.However, none of these above works have fo-cused on learning new features automatically withinput data, and while learning suitable features(representations) is the superiority of DNN sinceit has been proposed.
In this paper, we systemat-ically explore the possibility of learning new fea-123tures using DNN for SMT.3 Input Features for DNN FeatureLearningThe phrase-based translation model (Koehn et al,2003; Och and Ney, 2004) has demonstrated supe-rior performance and been widely used in currentSMT systems, and we employ our implementationon this translation model.
Next, we adapt and ex-tend some original phrase features as the input fea-tures for DAE feature learning.3.1 Baseline phrase featuresWe assume that source phrase f = f1, ?
?
?
, flfand target phrase e = e1, ?
?
?
, eleinclude lfandlewords, respectively.
Following (Maskey andZhou, 2012), we use the following 4 phrase fea-tures of each phrase pair (Koehn et al, 2003)in the phrase table as the first type of input fea-tures, bidirectional phrase translation probability(P (e|f) and P (f |e)), bidirectional lexical weight-ing (Lex(e|f) and Lex(f |e)),X1?
P (f |e), Lex(f |e), P (e|f), Lex(e|f)3.2 Phrase pair similarityZhao et al (2004) proposed a way of using termweight based models in a vector space as addi-tional evidences for phrase pair translation quality.This model employ phrase pair similarity to en-code the weights of content and non-content wordsin phrase translation pairs.
Following (Zhao et al,2004), we calculate bidirectional phrase pair simi-larity using cosine distance and BM25 distance as,Scosi(e, f) =?lej=1?lfi=1wejp(ej|fi)wfisqrt(?lej=1w2ej)sqrt(?lej=1weja2)Scosd(f, e) =?lfi=1?lej=1wfip(fi|ej)wejsqrt(?lfi=1w2fi)sqrt(?lfi=1wfia2)where, p(ej|fi) and p(fi|ej) represents bidirec-tional word translation probability.
wfiand wejare term weights for source and target words, wejaand wfiaare the transformed weights mapped fromall source/target words to the target/source dimen-sion at word ejand fi, respectively.Sbm25i(e, f) =lf?i=1idffi(k1+ 1)wfi(k3+ 1)wfia(K + wfi)(k3+ wfia)Sbm25d(f, e) =le?j=1idfej(k1+ 1)wej(k3+ 1)weja(K + wej)(k3+ weja)where, k1, b, k3are set to be 1, 1 and 1000, re-spectively.
K = k1((1?
b) + J/avg(l)), and J isthe phrase length (leor lf), avg(l) is the averagephrase length.
Thus, we have the second type ofinput featuresX2?
Scosi(f, e), Sbm25i(f, e), Scosd(e, f), Sbm25d(e, f)3.3 Phrase generative probabilityWe adapt and extend bidirectional phrase genera-tive probabilities as the input features, which havebeen used for domain adaptation (Foster et al,2010).
According to the background LMs, we esti-mate the bidirectional (source/target side) forwardand backward phrase generative probabilities asPf(f) = P (f1)P (f2|f1) ?
?
?P (flf|flf?n+1, ?
?
?
, flf?1)Pf(e) = P (e1)P (e2|e1) ?
?
?P (ele|ele?n+1, ?
?
?
, ele?1)Pb(f) = P (flf)P (flf?1|flf) ?
?
?P (f1|fn, ?
?
?
, f2)Pb(e) = P (ele)P (ele?1|ele) ?
?
?P (e1|en, ?
?
?
, e2)where, the bidirectional forward and backward1background 4-gram LMs are trained by the corre-sponding side of bilingual corpus2.
Then, we havethe third type of input featuresX3?
Pf(e), Pb(e), Pf(f), Pb(f)3.4 Phrase frequencyWe consider bidirectional phrase frequency as theinput features, and estimate them asP (f) =count(f)?|fi|=|f |count(fi)P (e) =count(e)?|ej|=|e|count(ej)where, the count(f)/count(e) are the total num-ber of phrase f/e appearing in the source/target sideof the bilingual corpus, and the denominator arethe total number of the phrases whose length areequal to |f |/|e|, respectively.
Then, we have theforth type of input featuresX4?
P (f), P (e)1Backward LM has been introduced by Xiong et al(2011), which successfully capture both the preceding andsucceeding contexts of the current word, and we estimate thebackward LM by inverting the order in each sentence in thetraining data from the original order to the reverse order.2This corpus is used to train the translation model in ourexperiments, and we will describe it in detail in section 5.1.1243.5 Phrase lengthPhrase length plays an important role in the trans-lation process (Koehn, 2010; Hopkins and May,2011).
We normalize bidirectional phrase lengthby the maximum phrase length, and introducethem as the last type of input featuresX5?
lne, lnfIn summary, except for the first type of phrasefeature X1which is used by (Maskey and Zhou,2012), we introduce another four types of effec-tive phrase features X2, X3, X4and X5.
Now, theinput original phrase features X includes 16 fea-tures in our experiments, as follows,X ?
X1, X2, X3, X4, X5We build the DAE network where the first layerwith visible nodes equaling to 16, and each visiblenode vicorresponds to the above original featuresX in each phrase pair.4 Semi-Supervised Deep Auto-encoderFeatures Learning for SMTEach translation rule in the phrase-based transla-tion model has a set number of features that arecombined in the log-linear model (Och and Ney,2002), and our semi-supervised DAE features canalso be combined in this model.
In this section,we design our DAE network with various networkstructures for new feature learning.4.1 Learning a Deep Belief NetInspired by (Maskey and Zhou, 2012), we firstlearn a deep generative model for feature learningusing DBN.
DBN is composed of multiple layersof latent variables with the first layer represent-ing the visible feature vectors, which is built withstacked sets of RBMs (Hinton, 2002).For a RBM, there is full connectivity betweenlayers, but no connections within either layer.
Theconnection weight W , hidden layer biases c andvisible layer biases b can be learned efficientlyusing the contrastive divergence (Hinton, 2002;Carreira-Perpinan and Hinton, 2005).
When givena hidden layer h, factorial conditional distributionof visible layer v can be estimated byP (v = 1|h) = ?
(b+ hTWT)where ?
denotes the logistic sigmoid.
Given v, theelement-wise conditional distribution of h isP (h = 1|v) = ?
(c+ vTW )Figure 1: Pre-training consists of learning a stackof RBMs, and these RBMs create an unsupervisedDBN.The two conditional distributions can be shownto correspond to the generative model,P (v, h) =1Zexp(?E(v, h))where,Z =?v,he?E(v,h)E(v, h) = ?bTv ?
cTh?
vTWhAfter learning the first RBM, we treat the acti-vation probabilities of its hidden units, when theyare being driven by data, as the data for traininga second RBM.
Similarly, a nthRBM is built onthe output of the n ?
1thone and so on until asufficiently deep architecture is created.
These nRBMs can then be composed to form a DBN inwhich it is easy to infer the states of the nthlayerof hidden units from the input in a single forwardpass (Hinton et al, 2006), as shown in Figure 1.This greedy, layer-by-layer pre-training can be re-peated several times to learn a deep, hierarchicalmodel (DBN) in which each layer of features cap-tures strong high-order correlations between theactivities of features in the layer below.To deal with real-valued input features X in ourtask, we use an RBM with Gaussian visible units(GRBM) (Dahl et al, 2012) with a variance of 1on each dimension.
Hence, P (v|h) and E(v, h) inthe first RBM of DBN need to be modified asP (v|h) = N (v; b+ hTWT, I)E(v, h) =12(v ?
b)T(v ?
b)?
cTh?
vTWhwhere I is the appropriate identity matrix.125Figure 2: After the unsupervised pre-training,the DBNs are ?unrolled?
to create a semi-supervised DAE, which is then fine-tuned usingback-propagation of error derivatives.To speed-up the pre-training, we subdivide theentire phrase pairs (with features X) in the phrasetable into small mini-batches, each containing 100cases, and update the weights after each mini-batch.
Each layer is greedily pre-trained for50 epochs through the entire phrase pairs.
Theweights are updated using a learning rate of 0.1,momentum of 0.9, and a weight decay of 0.0002?
weight ?
learning rate.
The weight matrix Ware initialized with small random values sampledfrom a zero-mean normal distribution with vari-ance 0.01.After the pre-training, for each phrase pair inthe phrase table, we generate the DBN features(Maskey and Zhou, 2012) by passing the originalphrase featuresX through the DBN using forwardcomputation.4.2 From DBN to Deep Auto-encoderTo learn a semi-supervised DAE, we first ?unroll?the above n layer DBN by using its weight ma-trices to create a deep, 2n-1 layer network whoselower layers use the matrices to ?encode?
the in-put and whose upper layers use the matrices inreverse order to ?decode?
the input (Hinton andSalakhutdinov, 2006; Salakhutdinov and Hinton,2009; Deng et al, 2010), as shown in Figure 2.The layer-wise learning of DBN as above must betreated as a pre-training stage that finds a goodregion of the parameter space, which is used toinitialize our DAE?s parameters.
Starting in thisregion, the DAE is then fine-tuned using averagesquared error (between the output and input) back-propagation to minimize reconstruction error, as tomake its output as equal as possible to its input.For the fine-tuning of DAE, we use the methodof conjugate gradients on larger mini-batches of1000 cases, with three line searches performedfor each mini-batch in each epoch.
To determinean adequate number of epochs and to avoid over-fitting, we fine-tune on a fraction phrase tableand test performance on the remaining validationphrase table, and then repeat fine-tuning on the en-tire phrase table for 100 epochs.We experiment with various values for the noisevariance and the threshold, as well as the learn-ing rate, momentum, and weight-decay parame-ters used in the pre-training, the batch size andepochs in the fine-tuning.
Our results are fairly ro-bust to variations in these parameters.
The preciseweights found by the pre-training do not matteras long as it finds a good region of the parameterspace from which to start the fine-tuning.The fine-tuning makes the feature representa-tion in the central layer of the DAE work muchbetter (Salakhutdinov and Hinton, 2009).
Afterthe fine-tuning, for each phrase pair in the phrasetable, we estimate our DAE features by passing theoriginal phrase features X through the ?encoder?part of the DAE using forward computation.To combine these learned features (DBN andDAE feature) into the log-linear model, we needto eliminate the impact of the non-linear learningmechanism.
Following (Maskey and Zhou, 2012),these learned features are normalized by the av-erage of each dimensional respective feature set.Then, we append these features for each phrasepair to the phrase table as extra features.4.3 Horizontal Composition of DeepAuto-encoders (HCDAE)Although DAE can learn more powerful and ab-stract feature representation, the learned featuresusually have smaller dimensionality comparedwith the dimensionality of the input features, suchas the successful use for handwritten digits recog-nition (Hinton and Salakhutdinov, 2006; Hintonet al, 2006), information retrieval (Salakhutdinovand Hinton, 2009; Mirowski et al, 2010), and126Figure 3: Horizontal composition of DAEs to ex-pand high-dimensional features learning.speech spectrograms (Deng et al, 2010).
More-over, although we have introduced another fourtypes of phrase features (X2, X3, X4and X5), theonly 16 features in X are a bottleneck for learninglarge hidden layers feature representation, becauseit has limited information, the performance of thehigh-dimensional DAE features which are directlylearned from single DAE is not very satisfactory.To learn high-dimensional feature representa-tion and to further improve the performance, weintroduce a natural horizontal composition forDAEs that can be used to create large hidden layerrepresentations simply by horizontally combiningtwo (or more) DAEs (Baldi, 2012), as shown inFigure 3.
Two single DAEs with architectures16/m1/16 and 16/m2/16 can be trained and thehidden layers can be combined to yield an ex-panded hidden feature representation of sizem1+m2, which can then be fed to the subsequent lay-ers of the overall architecture.
Thus, these newm1+m2-dimensional DAE features are added asextra features to the phrase table.Differences in m1- and m2-dimensional hiddenrepresentations could be introduced by many dif-ferent mechanisms (e.g., learning algorithms, ini-tializations, training samples, learning rates, ordistortion measures) (Baldi, 2012).
In our task,we introduce differences by using different initial-izations and different fractions of the phrase table.4-16-8-2 4-16-8-4 4-16-16-84-16-8-4-2 4-16-16-8-4 4-16-16-8-84-16-16-8-4-2 4-16-16-8-8-4 4-16-16-16-8-84-16-16-8-8-4-2 4-16-16-16-8-8-4 4-16-16-16-16-8-86-16-8-2 6-16-8-4 6-16-16-86-16-8-4-2 6-16-16-8-4 6-16-16-8-86-16-16-8-4-2 6-16-16-8-8-4 6-16-16-16-8-86-16-16-16-8-4-2 6-16-16-16-8-8-4 6-16-16-16-16-8-88-16-8-2 8-16-8-4 8-16-16-88-16-8-4-2 8-16-16-8-4 8-16-16-8-88-16-16-8-4-2 8-16-16-8-8-4 8-16-16-16-8-88-16-16-16-8-4-2 8-16-16-16-8-8-4 8-16-16-16-16-8-816-32-16-2 16-32-16-4 16-32-16-816-32-16-8-2 16-32-16-8-4 16-32-32-16-816-32-16-8-4-2 16-32-32-16-8-4 16-32-32-16-16-816-32-32-16-8-4-2 16-32-32-16-16-8-4 16-32-32-32-16-16-8Table 1: Details of the used network structure.For example, the architecture 16-32-16-2 (4 lay-ers?
network depth) corresponds to the DAE with16-dimensional input features (X) (input layer),32/16 hidden units (first/second hidden layer), and2-dimensional output features (new DAE features)(output layer).
During the fine-tuning, the DAE?snetwork structure becomes 16-32-16-2-16-32-16.Correspondingly, 4-16-8-2 and 6(8)-16-8-2 repre-sent the input features are X1and X1+Xi.5 Experiments and Results5.1 Experimental SetupWe now test our DAE features on the followingtwo Chinese-English translation tasks.IWSLT.
The bilingual corpus is the Chinese-English part of Basic Traveling Expression cor-pus (BTEC) and China-Japan-Korea (CJK) cor-pus (0.38M sentence pairs with 3.5/3.8M Chi-nese/English words).
The LM corpus is the En-glish side of the parallel data (BTEC, CJK andCWMT083) (1.34M sentences).
Our developmentset is IWSLT 2005 test set (506 sentences), and ourtest set is IWSLT 2007 test set (489 sentences).NIST.
The bilingual corpus is LDC4(3.4M sen-tence pairs with 64/70M Chinese/English words).The LM corpus is the English side of the paral-lel data as well as the English Gigaword corpus(LDC2007T07) (11.3M sentences).
Our develop-ment set is NIST 2005 MT evaluation set (1084sentences), and our test set is NIST 2006 MT eval-uation set (1664 sentences).We choose the Moses (Koehn et al, 2007)framework to implement our phrase-based ma-chine system.
The 4-gram LMs are estimatedby the SRILM toolkit with modified Kneser-Ney3the 4th China Workshop on Machine Translation4LDC2002E18, LDC2002T01, LDC2003E07,LDC2003E14, LDC2003T17, LDC2004T07, LDC2004T08,LDC2005T06, LDC2005T10, LDC2005T34, LDC2006T04,LDC2007T09127# FeaturesIWSLT NISTDev Test Dev Test1 Baseline 50.81 41.13 36.12 32.592X1+DBN X12f 51.92 42.07?36.33 33.11?3 +DAE X12f 52.49 43.22?
?36.92 33.44?
?4 +DBN X14f 51.45 41.78?36.45 33.12?5 +DAE X14f 52.45 43.06?
?36.88 33.47?
?6 +HCDAE X12+2f 53.69 43.23??
?37.06 33.68??
?7 +DBN X18f 51.74 41.85?36.61 33.24?8 +DAE X18f 52.33 42.98?
?36.81 33.36?
?9 +HCDAE X14+4f 52.52 43.26??
?37.01 33.63??
?10X+DBN X 2f 52.21 42.24?36.72 33.21?11 +DAE X 2f 52.86 43.45?
?37.39 33.83?
?12 +DBN X 4f 51.83 42.08?34.45 33.07?13 +DAE X 4f 52.81 43.47?
?37.48 33.92?
?14 +HCDAE X 2+2f 53.05 43.58??
?37.59 34.11??
?15 +DBN X 8f 51.93 42.01?36.74 33.29?16 +DAE X 8f 52.69 43.26?
?37.36 33.75?
?17 +HCDAE X 4+4f 52.93 43.49??
?37.53 34.02??
?18 +(X2+X3+X4+X5) 52.23 42.91?36.96 33.65?19 +(X2+X3+X4+X5)+DAE X 2f 53.55 44.17+??
?38.23 34.50+??
?20 +(X2+X3+X4+X5)+DAE X 4f 53.61 44.22+??
?38.28 34.47+??
?21 +(X2+X3+X4+X5)+HCDAE X 2+2f 53.75 44.28+???
?38.35 34.65+???
?22 +(X2+X3+X4+X5)+DAE X 8f 53.47 44.19+??
?38.26 34.46+??
?23 +(X2+X3+X4+X5)+HCDAE X 4+4f 53.62 44.29+???
?38.39 34.57+???
?Table 2: The translation results by adding new DNN features (DBN feature (Maskey and Zhou, 2012),our proposed DAE and HCDAE feature) as extra features to the phrase table on two tasks.
?DBN X1xf?,?DBN X xf?, ?DAE X1xf?
and ?DAE X xf?
represent that we use DBN and DAE, input featuresX1and X , to learn x-dimensional features, respectively.
?HCDAE X x+xf?
represents horizontallycombining two DAEs and each DAE has the same x-dimensional learned features.
All improvements ontwo test sets are statistically significant by the bootstrap resampling (Koehn, 2004).
*: significantly betterthan the baseline (p < 0.05), **: significantly better than ?DBN X1xf?
or ?DBN X xf?
(p < 0.01),***: significantly better than ?DAE X1xf?
or ?DAE X xf?
(p < 0.01), ****: significantly better than?HCDAE X x+xf?
(p < 0.01), +: significantly better than ?X2+X3+X4+X5?
(p < 0.01).discounting.
We perform pairwise ranking opti-mization (Hopkins and May, 2011) to tune featureweights.
The translation quality is evaluated bycase-insensitive IBM BLEU-4 metric.The baseline translation models are generatedby Moses with default parameter settings.
In thecontrast experiments, our DAE and HCDAE fea-tures are appended as extra features to the phrasetable.
The details of the used network structure inour experiments are shown in Table 1.5.2 ResultsTable 2 presents the main translation results.
Weuse DBN, DAE and HCDAE (with 6 layers?
net-work depth), input features X1and X , to learn 2-,4- and 8-dimensional features, respectively.
Fromthe results, we can get some clear trends:1.
Adding new DNN features as extra featuressignificantly improves translation accuracy (row2-17 vs. 1), with the highest increase of 2.45(IWSLT) and 1.52 (NIST) (row 14 vs. 1) BLEUpoints over the baseline features.2.
Compared with the unsupervised DBN fea-tures, our semi-supervised DAE features are moreeffective for translation decoder (row 3 vs. 2; row5 vs. 4; row 8 vs. 7; row 11 vs. 10; row 13 vs.12; row 16 vs. 15).
Specially, Table 3 shows thevariance distributions of the learned each dimen-sional DBN and DAE feature, our DAE featureshave bigger variance distributions which means128FeaturesIWSLT NIST?1?2?3?4?1?2?3?4DBN X14f 0.1678 0.2873 0.2037 0.1622 0.0691 0.1813 0.0828 0.1637DBN X 4f 0.2010 0.1590 0.2793 0.1692 0.1267 0.1146 0.2147 0.1051DAE X14f 0.5072 0.4486 0.1309 0.6012 0.2136 0.2168 0.2047 0.2526DAE X 4f 0.5215 0.4594 0.2371 0.6903 0.2421 0.2694 0.3034 0.2642Table 3: The variance distributions of each dimensional learned DBN feature and DAE feature on thetwo tasks.Figure 4: The compared results of feature learning with different network structures on two developmentsets.FeaturesIWSLT NISTDev Test Dev Test+DAE X14f 52.45 43.06 36.88 33.47+DAE X1+X24f 52.76 43.38?37.28 33.80?+DAE X1+X34f 52.61 43.27?37.13 33.66?+DAE X1+X44f 52.52 43.24?36.96 33.58?+DAE X1+X54f 52.49 43.13?36.96 33.56?+DAE X 4f 52.81 43.47?37.48 33.92?Table 4: The effectiveness of our introduced in-put features.
?DAE X1+Xi4f?
represents thatwe use DAE, input features X1+ Xi, to learn 4-dimensional features.
*: significantly better than?DAE X14f?
(p < 0.05).our DAE features have more discriminative power,and also their variance distributions are more sta-ble.3.
HCDAE outperforms single DAE for highdimensional feature learning (row 6 vs. 5; row 9vs.
8; row 14 vs. 13; row 17 vs. 16), and furtherimprove the performance of DAE feature learning,which can also somewhat address the bring short-coming of the limited input features.4.
Except for the phrase feature X1(Maskeyand Zhou, 2012), our introduced input featuresX significantly improve the DAE feature learn-ing (row 11 vs. 3; row 13 vs. 5; row 16 vs. 8).Specially, Table 4 shows the detailed effectivenessof our introduced input features for DAE featurelearning, and the results show that each type offeatures are very effective for DAE feature learn-ing.5.
Adding the original features (X2,X3,X4andX5) and DAE/HCDAE features together can fur-ther improve translation performance (row 19-23vs.
18), with the highest increase of 3.16 (IWSLT)and 2.06 (NIST) (row 21 vs. 1) BLEU points overthe baseline features.
DAE and HCDAE featuresare learned from the non-linear combination of theoriginal features, they strong capture high-ordercorrelations between the activities of the originalfeatures, as to be further interpreted to reach theirpotential for SMT.
We suspect these learned fea-129tures are complementary to the original features.5.3 AnalysisFigure 5: The compared results of using singleDAE and the HCDAE for feature learning on twodevelopment sets.Figure 4 shows our DAE features are not onlymore effective but also more stable than DBNfeatures with various network structures.
Also,adding more input features (X vs. X1) not onlysignificantly improves the performance of DAEfeature learning, but also slightly improves theperformance of DBN feature learning.Figure 5 shows there is little change in the per-formance of using single DAE to learn differentdimensional DAE features, but the 4-dimensionalfeatures work more better and more stable.
HC-DAE outperforms the single DAE and learns high-dimensional representation more effectively, espe-cially for the peak point in each condition.Figures 5 also shows the best network depth forDAE feature learning is 6 layers.
When the net-work depth of DBN is 7 layers, the network depthof corresponding DAE during the fine-tuning is 13layers.
Although we have pre-trained the corre-sponding DBN, this DAE network is so deep, thefine-tuning does not work very well and typicallyfinds poor local minima.
We suspect this leads tothe decreased performance.6 ConclusionsIn this paper, instead of designing new featuresbased on intuition, linguistic knowledge and do-main, we have learned new features using the DAEfor the phrase-based translation model.
Using theunsupervised pre-trained DBN to initialize DAE?sparameters and using the input original phrase fea-tures as the ?teacher?
for semi-supervised back-propagation, our semi-supervised DAE featuresare more effective and stable than the unsuper-vised DBN features (Maskey and Zhou, 2012).Moreover, to further improve the performance, weintroduce some simple but effective features asthe input features for feature learning.
Lastly, tolearn high dimensional feature representation, weintroduce a natural horizontal composition of twoDAEs for large hidden layers feature learning.On two Chinese-English translation tasks, theresults demonstrate that our solutions solve thetwo aforementioned shortcomings successfully.Firstly, our DAE features obtain statistically sig-nificant improvements of 1.34/2.45 (IWSLT) and0.82/1.52 (NIST) BLEU points over the DBN fea-tures and the baseline features, respectively.
Sec-ondly, compared with the baseline phrase featuresX1, our introduced input original phrase featuresX significantly improve the performance of notonly our DAE features but also the DBN features.The results also demonstrate that DNN (DAEand HCDAE) features are complementary to theoriginal features for SMT, and adding them to-gether obtain statistically significant improve-ments of 3.16 (IWSLT) and 2.06 (NIST) BLEUpoints over the baseline features.
Compared withthe original features, DNN (DAE and HCDAE)features are learned from the non-linear combi-nation of the original features, they strong cap-ture high-order correlations between the activitiesof the original features, and we believe this deeplearning paradigm induces the original features tofurther reach their potential for SMT.AcknowledgmentsThis work was supported by 863 program inChina (No.
2011AA01A207).
We would like tothank Xingyuan Peng, Lichun Fan and HongyanLi for their helpful discussions.
We also thankthe anonymous reviewers for their insightful com-ments.130ReferencesMichael Auli, Michel Galley, Chris Quirk and GeoffreyZweig.
2013.
Joint language and translation model-ing with recurrent neural networks.
In Proceedingsof EMNLP, pages 1044-1054.Pierre Baldi.
2012.
Autoencoders, unsupervised learn-ing, and deep architectures.
JMLR: workshop on un-supervised and transfer learning, 27:37-50.Miguel A. Carreira-Perpinan and Geoffrey E. Hinton.2005.
On contrastive divergence learning.
In Pro-ceedings of AI and Statistics.George Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-dependent pre-trained deep neuralnetworks for large vocabulary speech recognition.IEEE Transactions on Audio, Speech, and LanguageProcessing, 20(1):30-42.Li Deng, Mike Seltzer, Dong Yu, Alex Acero, Abdel-rahman Mohamed, and Geoffrey E. Hinton.
2010.Binary coding of speech spectrograms using a deepauto-encoder.
In Proceedings of INTERSPEECH,pages 1692-1695.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of EMNLP, pages 451-459.Geoffrey E. Hinton.
2002.
Training products of ex-perts by minimizing contrastive divergence.
NeuralComputation, 14(8):1771-1800.Geoffrey E. Hinton, Li Deng, Dong Yu, George Dahl,Abdel-rahman Mohamed, Navdeep Jaitly, AndrewSenior, Vincent Vanhoucke, Patrick Nguyen, TaraSainath, and Brian Kingsbury.
2012.
Deep neuralnetworks for acoustic modeling in speech tecogni-tion.
IEEE Signal Processing Magazine, 29(6):82-97.Geoffrey E. Hinton, Alex Krizhevsky, and Sida D.Wang.
2001.
Transforming auto-encoders.
In Pro-ceedings of ANN.Geoffrey E. Hinton and Ruslan R. Salakhutdinov.2006.
Reducing the dimensionality of data withneural networks.
Science, 313:504-507.Geoffrey E. Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A fast learning algorithm for deep beliefnets.
Neural Computation, 18:1527-1544.Mark Hopkins and Jonathan May 2011.
Tuning asranking.
In Proceedings of EMNLP, pages 1352-1362.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In Proceedingsof EMNLP, pages 1700-1709.Philipp Koehn.
2004.
Statistical significance testsfrom achine translation evaluation.
In Proceedingsof ACL, pages 388-395.Philipp Koehn.
2010.
Statistical machine translation.Cambridge University Press.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL, Demonstration Session, pages177-180.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof NAACL, pages 48-54.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of NAACL, pages39-48.Peng Li, Yang Liu, Maosong Sun.
2013.
Recursiveautoencoders for ITG-based translation.
In Proceed-ings of EMNLP, pages 567-577.Lemao Liu, Taro Watanabe, Eiichiro Sumita, andTiejun Zhao.
2013.
Additive neural networks forstatistical machine translation.
In Proceedings ofACL, pages 791-801.Shixiang Lu, Wei Wei, Xiaoyin Fu and Bo Xu.
2014.Recursive neural network based word topologymodel for hierarchical phrase-based speech transla-tion.
In Proceedings of ICASSP.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrase-based translation.In Proceedings of ACL, pages 1003-1011.Sameer Maskey and Bowen Zhou.
2012.
Unsuper-vised deep belief features for speech translation.
InProceedings of INTERSPEECH.Piotr Mirowski, MarcAurelio Ranzato, and Yann Le-Cun.
2010.
Dynamic auto-encoders for semanticindexing.
In Proceedings of NIPS-2010 Workshopon Deep Learning.Patrick Nguyen, Milind Mahajan, and Xiaodong He.2007.
Training non-parametric features for statis-tical machine translation.
In Proceedings of WMT,pages 72-79.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of ACL,pages 440-447.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proceedings of ACL, pages295-302.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(4):417-449.131David Rumelhart, Geoffrey E. Hinton, and RonaleWilliams.
1986.
Learning internal representationsby back-propagation errors.
Parallel DistributedProcessing, Vol 1: Foundations, MIT Press.Ruslan R. Salakhutdinov and Geoffrey E. Hinton.2009.
Semantic hashing.
International Journal ofApproximate Reasoning, 50(7):969-978.Richard Socher, Cliff C. Lin, Andrew Y. Ng, andChristopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neuralnetworks.
In Proceedings of ICML.Deyi Xiong, Min Zhang, and Haizhou Li.
2011.Enhancing language models in statistical machinetranslation with backward n-grams and mutual in-formation triggers.
In Proceedings of ACL, pages1288-1297.Bing Zhao, Stephan Vogel, and Alex Waibel.
2004.Phrase pair rescoring with term weightings for sta-tistical machine translation.
In Proceedings ofEMNLP.132
