Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 77?82,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnsupervised Discovery of Rhyme SchemesSravana ReddyDepartment of Computer ScienceThe University of ChicagoChicago, IL 60637sravana@cs.uchicago.eduKevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CA 90292knight@isi.eduAbstractThis paper describes an unsupervised,language-independent model for findingrhyme schemes in poetry, using no priorknowledge about rhyme or pronunciation.1 IntroductionRhyming stanzas of poetry are characterized byrhyme schemes, patterns that specify how the linesin the stanza rhyme with one another.
The questionwe raise in this paper is: can we infer the rhymescheme of a stanza given no information about pro-nunciations or rhyming relations among words?Background A rhyme scheme is represented as astring corresponding to the sequence of lines thatcomprise the stanza, in which rhyming lines are de-noted by the same letter.
For example, the limerick?srhyme scheme is aabba, indicating that the 1st, 2nd,and 5th lines rhyme, as do the the 3rd and 4th.Motivation Automatic rhyme scheme annotationwould benefit several research areas, including:?
Machine Translation of Poetry There has beena growing interest in translation under con-straints of rhyme and meter, which requirestraining on a large amount of annotated poetrydata in various languages.?
?Culturomics?
The field of digital humanitiesis growing, with a focus on statistics to trackcultural and literary trends (partially spurredby projects like the Google Books Ngrams1).1http://ngrams.googlelabs.com/Rhyming corpora could be extremely useful forlarge-scale statistical analyses of poetic texts.?
Historical Linguistics/Study of DialectsRhymes of a word in poetry of a given timeperiod or dialect region provide clues about itspronunciation in that time or dialect, a fact thatis often taken advantage of by linguists (Wyld,1923).
One could automate this task givenenough annotated data.An obvious approach to finding rhyme schemesis to use word pronunciations and a definition ofrhyme, in which case the problem is fairly easy.However, we favor an unsupervised solution that uti-lizes no external knowledge for several reasons.?
Pronunciation dictionaries are simply not avail-able for many languages.
When dictionariesare available, they do not include all possiblewords, or account for different dialects.?
The definition of rhyme varies across poetictraditions and languages, and may includeslant rhymes like gate/mat, ?sight rhymes?
likeword/sword, assonance/consonance like shore/alone, leaves/lance, etc.?
Pronunciations and spelling conventionschange over time.
Words that rhymed histori-cally may not anymore, like prove and love ?or proued and beloued.2 Related WorkThere have been a number of recent papers on theautomated annotation, analysis, or translation of po-77etry.
Greene et al (2010) use a finite state trans-ducer to infer the syllable-stress assignments in linesof poetry under metrical constraints.
Genzel et al(2010) incorporate constraints on meter and rhyme(where the stress and rhyming information is derivedfrom a pronunciation dictionary) into a machinetranslation system.
Jiang and Zhou (2008) develop asystem to generate the second line of a Chinese cou-plet given the first.
A few researchers have also ex-plored the problem of poetry generation under someconstraints (Manurung et al, 2000; Netzer et al,2009; Ramakrishnan et al, 2009).
There has alsobeen some work on computational approaches tocharacterizing rhymes (Byrd and Chodorow, 1985)and global properties of the rhyme network (Son-deregger, 2011) in English.
To the best of our knowl-edge, there has been no language-independent com-putational work on finding rhyme schemes.3 Finding Stanza Rhyme SchemesA collection of rhyming poetry inevitably containsrepetition of rhyming pairs.
For example, the wordtrees will often rhyme with breeze across differentstanzas, even those with different rhyme schemesand written by different authors.
This is partly dueto sparsity of rhymes ?
many words that have norhymes at all, and many others have only a handful,forcing poets to reuse rhyming pairs.In this section, we describe an unsupervised al-gorithm to infer rhyme schemes that harnesses thisrepetition, based on a model of stanza generation.3.1 Generative Model of a Stanza1.
Pick a rhyme scheme r of length n with proba-bility P (r).2.
For each i ?
[1, n], pick a word sequence,choosing the last2 word xi as follows:(a) If, according to r, the ith line does notrhyme with any previous line in the stanza, picka word xi from a vocabulary of line-end wordswith probability P (xi).
(b) If the ith line rhymes with some previousline(s) j according to r, choose a word xi that2A rhyme may span more than one word in a line ?
for ex-ample, laureate... / Tory at... / are ye at (Byron, 1824), but thisis uncommon.
An extension of our model could include a latentvariable that selects the entire rhyming portion of a line.rhymes with the last words of all such lineswith probability?j<i:ri=rjP (xi|xj).The probability of a stanza x of length n is givenby Eq.
1.
Ii,r is the indicator variable for whetherline i rhymes with at least one previous line under r.P (x) =?r?RP (r)P (x|r) =?r?RP (r)n?i=1(1?
Ii,r)P (xi) + Ii,r?j<i:ri=rjP (xi|xj) (1)3.2 LearningWe denote our data by X , a set of stanzas.
Eachstanza x is represented as a sequence of its line-endwords, xi, .
.
.
xlen(x).
We are also given a large setR of all possible rhyme schemes.3If each stanza in the data is generated indepen-dently (an assumption we relax in ?4), the log-likelihood of the data is?x?X logP (x).
We wouldlike to maximize this over all possible rhyme schemeassignments, under the latent variables ?, which rep-resents pairwise rhyme strength, and ?, the distribu-tion of rhyme schemes.
?v,w is defined for all wordsv and w as a non-negative real value indicating howstrongly the words v and w rhyme, and ?r is P (r).The expectation maximization (EM) learning al-gorithm for this formulation is described below.
Theintuition behind the algorithm is this: after one iter-ation, ?v,w = 0 for all v and w that never occur to-gether in a stanza.
If v and w co-occur in more thanone stanza, ?v,w has a high pseudo-count, reflectingthe fact that they are likely to be rhymes.Initialize: ?
and ?
uniformly (giving ?
the samepositive value for all word pairs).Expectation Step: Compute P (r|x) =P (x|r)?r/?q?R P (x|q)?q, whereP (x|r) =n?i=1(1?
Ii,r)P (xi) +Ii,r?j<i:ri=rj?xi,xj/?w?w,xi (2)3While the number of rhyme schemes of length n is tech-nically the number of partitions of an n- element set (the Bellnumber), only a subset of these are typically used.78P (xi) is simply the relative frequency of theword xi in the data.Maximization Step: Update ?
and ?
:?v,w =?r,x:v rhymes with wP (r|x) (3)?r =?x?XP (r|x)/?q?R,x?XP (q|x) (4)After Convergence: Label each stanza x with thebest rhyme scheme, argmaxr?R P (r|x).3.3 DataWe test the algorithm on rhyming poetry in En-glish and French.
The English data is an edited ver-sion of the public-domain portion of the corpus usedby Sonderegger (2011), and consists of just under12000 stanzas spanning a range of poets and datesfrom the 15th to 20th centuries.
The French datais from the ARTFL project (Morrissey, 2011), andcontains about 3000 stanzas.
All poems in the dataare manually annotated with rhyme schemes.The set R is taken to be all the rhyme schemesfrom the gold standard annotations of both corpora,numbering 462 schemes in total, with an average of6.5 schemes per stanza length.
There are 27.12 can-didate rhyme schemes on an average for each En-glish stanza, and 33.81 for each French stanza.3.4 ResultsWe measure the accuracy of the discovered rhymeschemes relative to the gold standard.
We also eval-uate for each word token xi, the set of words in{xi+1, xi+2, .
.
.}
that are found to rhyme with xi bymeasuring precision and recall.
This is to accountfor partial correctness ?
if abcb is found instead ofabab, for example, we would like to credit the algo-rithm for knowing that the 2nd and 4th lines rhyme.Table 1 shows the results of the algorithm for theentire corpus in each language, as well as for a fewsub-corpora from different time periods.3.5 Orthographic Similarity BiasSo far, we have relied on the repetition of rhymes,and have made no assumptions about word pronun-ciations.
Therefore, the algorithm?s performanceis strongly correlated4 with the predictability ofrhyming words.
For writing systems where thewritten form of a word approximates its pronunci-ation, we have some additional information aboutrhyming: for example, English words ending withsimilar characters are most probably rhymes.
Wedo not want to assume too much in the interest oflanguage-independence ?
following from our earlierpoint in ?1 about the nebulous definition of rhyme?
but it is safe to say that rhyming words involvesome orthographic similarity (though this does nothold for writing systems like Chinese).
We thereforeinitialize ?
at the start of EM with a simple similaritymeasure: (Eq.
5).
The addition of  = 0.001 ensuresthat words with no letters in common, like new andyou, are not eliminated as rhymes.
?v,w =# letters common to v & wmin(len(v), len(w))+  (5)This simple modification produces results thatoutperform the na?
?ve baselines for most of the databy a considerable margin, as detailed in Table 2.3.6 Using Pronunciation, Rhyming DefinitionHow does our algorithm compare to a standard sys-tem where rhyme schemes are determined by pre-defined rules of rhyming and dictionary pronunci-ations?
We use the accepted definition of rhymein English: two words rhyme if their final stressedvowels and all following phonemes are identical.For every pair of English words v, w, we let ?v,w =1 +  if the CELEX (Baayen et al, 1995) pronun-ciations of v and w rhyme, and ?v,w = 0 +  if not(with  = 0.001).
If either v or w is not presentin CELEX, we set ?v,w to a random value in [0, 1].We then find the best rhyme scheme for each stanza,using Eq.
2 with uniformly initialized ?.Figure 1 shows that the accuracy of this systemis generally much lower than that of our model forthe sub-corpora from before 1750.
Performance iscomparable for the 1750-1850 data, after which weget better accuracies using the rhyming definitionthan with our model.
This is clearly a reflection oflanguage change; older poetry differs more signifi-cantly in pronunciation and lexical usage from con-4For the five English sub-corpora,R2 = 0.946 for the nega-tive correlation of accuracy with entropy of rhyming word pairs.79Table 1: Rhyme scheme accuracy and F-Score (computed from average precision and recall over all lines) using our algorithmfor independent stanzas, with uniform initialization of ?.
Rows labeled ?All?
refer to training and evaluation on all the data in thelanguage.
Other rows refer to training and evaluating on a particular sub-corpus only.
Bold indicates that we outperform the na?
?vebaseline, where most common scheme of the appropriate length from the gold standard of the entire corpus is assigned to everystanza, and italics that we outperform the ?less na??ve?
baseline, where we assign the most common scheme of the appropriate lengthfrom the gold standard of the given sub-corpus.Sub-corpus Sub-corpus overview Accuracy (%) F-Score(time- # of Total # # of line- EM Na?
?ve Less na?
?ve EM Na?
?ve Lessperiod) stanzas of lines end words induction baseline baseline induction baseline na?
?veEnAll 11613 93030 13807 62.15 56.76 60.24 0.79 0.74 0.771450-1550 197 1250 782 17.77 53.30 97.46 0.41 0.73 0.981550-1650 3786 35485 7826 67.17 62.28 74.72 0.82 0.78 0.851650-1750 2198 20110 4447 87.58 58.42 82.98 0.94 0.68 0.911750-1850 2555 20598 5188 31.00 69.16 74.52 0.65 0.83 0.871850-1950 2877 15587 4382 50.92 37.43 49.70 0.81 0.55 0.68FrAll 2814 26543 10781 40.29 39.66 64.46 0.58 0.57 0.801450-1550 1478 14126 7122 28.21 58.66 77.67 0.59 0.83 0.891550-1650 1336 12417 5724 52.84 18.64 61.23 0.70 0.28 0.75temporary dictionaries, and therefore, benefits morefrom a model that assumes no pronunciation knowl-edge.
(While we may get better results on olderdata using dictionaries that are historically accurate,these are not easily available, and require a greatdeal of effort and linguistic knowledge to create.
)Initializing ?
as specified above and then runningEM produces some improvement compared to or-thographic similarity (Table 2).4 Accounting for Stanza DependenciesSo far, we have treated stanzas as being indepen-dent of each other.
In reality, stanzas in a poem areusually generated using the same or similar rhymeschemes.
Furthermore, some rhyme schemes spanmultiple stanzas ?
for example, the Italian form terzarima has the scheme aba bcb cdc... (the 1st and 3rdlines rhyme with the 2nd line of the previous stanza).4.1 Generative ModelWe model stanza generation within a poem as aMarkov process, where each stanza is conditionedon the previous one.
To generate a poem y consist-ing of m stanzas, for each k ?
[1,m], generate astanza xk of length nk as described below:1.
If k = 1, pick a rhyme scheme rk of length nkwith probability P (rk), and generate the stanzaas in the previous section.Figure 1: Comparison of EM with a definition-based system0?0.2?0.4?0.6?0.8?1?1.2?1.4?1.6?1450-1550 1550-1650 1650-1750 1750-1850 1850-1950RatioofrhymingrulestoEMperformanceAccuracyF-Score(a) Accuracy and F-Score ratios of the rhyming-definition-based system over that of our model with orthographic sim-ilarity.
The former is more accurate than EM for post-1850data (ratio > 1), but is outperformed by our model for olderpoetry (ratio< 1), largely due to pronunciation changes likethe Great Vowel Shift that alter rhyming relations.Found by EM Found by definitions1450-1550 left/craft, shone/done edify/lie, adieu/hue1550-1650 appeareth/weareth, obtain/vain, amend/speaking/breaking, depend, breed/heed,proue/moue, doe/two prefers/hers1650-1750 most/cost, presage/ see/family, blade/rage, join?d/mind shade, noted/quoted1750-1850 desponds/wounds, gore/shore, ice/vice,o?er/shore, it/basket head/tread, too/blew1850-1950 of/love, lover/ old/enfold, within/half-over, again/rain win, be/immortality(b) Some examples of rhymes in English found by EM but notthe definition-based system (due to divergence from the contem-porary dictionary or rhyming definition), and vice-versa (due toinadequate repetition).80Table 2: Performance of EM with ?
initialized by orthographic similarity (?3.5), pronunciation-based rhyming definitions (?3.6),and the HMM for stanza dependencies (?4).
Bold and italics indicate that we outperform the na?
?ve baselines shown in Table 1.Sub-corpus Accuracy (%) F-Score(time- HMM Rhyming Orthographic Uniform HMM Rhyming Ortho.
Uniformperiod) stanzas definition init.
initialization initialization stanzas defn.
init.
init.
init.EnAll 72.48 64.18 63.08 62.15 0.88 0.84 0.83 0.791450-1550 74.31 75.63 69.04 17.77 0.86 0.86 0.82 0.411550-1650 79.17 69.76 71.98 67.17 0.90 0.86 0.88 0.821650-1750 91.23 91.95 89.54 87.58 0.97 0.97 0.96 0.941750-1850 49.11 42.74 33.62 31.00 0.82 0.77 0.70 0.651850-1950 58.95 57.18 54.05 50.92 0.90 0.89 0.84 0.81FrAll 56.47 - 48.90 40.29 0.81 - 0.75 0.581450-1550 61.28 - 35.25 28.21 0.86 - 0.71 0.591550-1650 67.96 - 63.40 52.84 0.79 - 0.77 0.702.
If k > 1, pick a scheme rk of length nk withprobability P (rk|rk?1).
If no rhymes in rkare shared with the previous stanza?s rhymescheme, rk?1, generate the stanza as before.If rk shares rhymes with rk?1, generate thestanza as a continuation of xk?1.
For exam-ple, if xk?1 = [dreams, lay, streams], and rk?1and rk = aba and bcb, the stanza xk should begenerated so that xk1 and xk3 rhyme with lay.4.2 LearningThis model for a poem can be formalized as an au-toregressive HMM, an hidden Markov model whereeach observation is conditioned on the previous ob-servation as well as the latent state.
An observationat a time step k is the stanza xk, and the latent state atthat time step is the rhyme scheme rk.
This model isparametrized by ?
and ?, where ?r,q = P (r|q) for allschemes r and q. ?
is initialized with orthographicsimilarity.
The learning algorithm follows from EMfor HMMs and our earlier algorithm.Expectation Step: Estimate P (r|x) for eachstanza in the poem using the forward-backwardalgorithm.
The ?emission probability?
P (x|r)for the first stanza is same as in ?3, and forsubsequent stanzas xk, k > 1 is given by:P (xk|xk?1, rk) =nk?i=1(1?
Ii,rk)P (xki ) +Ii,rk?j<i:rki =rkjP (xki |xkj )?j:rki =rk?1jP (xki |xk?1j ) (6)Maximization Step: Update ?
and ?
analogouslyto HMM transition and emission probabilities.4.3 ResultsAs Table 2 shows, there is considerable improve-ment over models that assume independent stanzas.The most gains are found in French, which containsmany instances of ?linked?
stanzas like the terzarima, as well as English data containing long poemsmade of several stanzas with the same scheme.5 Future WorkSome possible extensions of our work include au-tomatically generating the set of possible rhymeschemes R, and incorporating partial supervisioninto our algorithm as well as better ways of usingand adapting pronunciation information when avail-able.
We would also like to test our method on arange of languages and texts.To return to the motivations, one could usethe discovered annotations for machine translationof poetry, or to computationally reconstruct pro-nunciations, which is useful for historical linguis-tics as well as other applications involving out-of-vocabulary words.AcknowledgmentsWe would like to thank Morgan Sonderegger forproviding most of the annotated English data in therhyming corpus and for helpful discussion, and theanonymous reviewers for their suggestions.81ReferencesR.
H. Baayen, R. Piepenbrock, and L. Gulikers.
1995.The CELEX Lexical Database (CD-ROM).
LinguisticData Consortium.Roy J. Byrd and Martin S. Chodorow.
1985.
Using anonline dictionary to find rhyming words and pronunci-ations for unknown words.
In Proceedings of ACL.Lord Byron.
1824.
Don Juan.Dmitriy Genzel, Jakob Uszkoreit, and Franz Och.
2010.?Poetic?
statistical machine translation: Rhyme andmeter.
In Proceedings of EMNLP.Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010.Automatic analysis of rhythmic poetry with applica-tions to generation and translation.
In Proceedings ofEMNLP.Long Jiang and Ming Zhou.
2008.
Generating Chinesecouplets using a statistical MT approach.
In Proceed-ings of COLING.Hisar Maruli Manurung, Graeme Ritchie, and HenryThompson.
2000.
Towards a computational model ofpoetry generation.
In Proceedings of AISB Symposiumon Creative and Cultural Aspects and Applications ofAI and Cognitive Science.Robert Morrissey.
2011.
ARTFL : American researchon the treasury of the French language.
http://artfl-project.uchicago.edu/content/artfl-frantext.Yael Netzer, David Gabay, Yoav Goldberg, and MichaelElhadad.
2009.
Gaiku : Generating Haiku with wordassociations norms.
In Proceedings of the NAACLworkshop on Computational Approaches to LinguisticCreativity.Ananth Ramakrishnan, Sankar Kuppan, andSobha Lalitha Devi.
2009.
Automatic genera-tion of Tamil lyrics for melodies.
In Proceedings ofthe NAACL workshop on Computational Approachesto Linguistic Creativity.Morgan Sonderegger.
2011.
Applications of graph the-ory to an English rhyming corpus.
Computer Speechand Language, 25:655?678.Henry Wyld.
1923.
Studies in English rhymes from Sur-rey to Pope.
J Murray, London.82
