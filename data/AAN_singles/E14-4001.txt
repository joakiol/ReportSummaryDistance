Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?5,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsEasy Web Search Results Clustering:When Baselines Can Reach State-of-the-Art AlgorithmsJose G. MorenoNormandie UniversityUNICAEN, GREYC CNRSF-14032 Caen, Francejose.moreno@unicaen.frGae?l DiasNormandie UniversityUNICAEN, GREYC CNRSF-14032 Caen, Francegael.dias@unicaen.frAbstractThis work discusses the evaluation ofbaseline algorithms for Web search re-sults clustering.
An analysis is performedover frequently used baseline algorithmsand standard datasets.
Our work showsthat competitive results can be obtained byeither fine tuning or performing cascadeclustering over well-known algorithms.
Inparticular, the latter strategy can lead toa scalable and real-world solution, whichevidences comparative results to recenttext-based state-of-the-art algorithms.1 IntroductionVisualizing Web search results remains an openproblem in Information Retrieval (IR).
For exam-ple, in order to deal with ambiguous or multi-faceted queries, many works present Web page re-sults using groups of correlated contents insteadof long flat lists of relevant documents.
Amongexisting techniques, Web Search Results Cluster-ing (SRC) is a commonly studied area, whichconsists in clustering ?on-the-fly?
Web page re-sults based on their Web snippets.
Therefore,many works have been recently presented includ-ing task adapted clustering (Moreno et al., 2013),meta clustering (Carpineto and Romano, 2010)and knowledge-based clustering (Scaiella et al.,2012).Evaluation is also a hot topic both in NaturalLanguage Processing (NLP) and IR.
Within thespecific case of SRC, different metrics have beenused such as F1-measure (F1), kSSL1 and Fb3-measure (Fb3) over different standard datasets:ODP-239 (Carpineto and Romano, 2010) andMoresque (Navigli and Crisafulli, 2010).
Unfor-tunately, comparative results are usually biased as1This metric is based on subjective label evaluation and assuch is out of the scope of this paper.baseline algorithms are run with default parame-ters whereas proposed methodologies are usuallytuned to increase performance over the studieddatasets.
Moreover, evaluation metrics tend to cor-relate with the number of produced clusters.In this paper, we focus on deep understand-ing of the evaluation task within the context ofSRC.
First, we provide the results of baseline algo-rithms with their best parameter settings.
Second,we show that a simple cascade strategy of base-line algorithms can lead to a scalable and real-world solution, which evidences comparative re-sults to recent text-based algorithms.
Finally, wedraw some conclusions about evaluation metricsand their bias to the number of output clusters.2 Related WorkSearch results clustering is an active research area.Two main streams have been proposed so far:text-based strategies such as (Hearst and Peder-sen, 1996; Zamir and Etzioni, 1998; Zeng et al.,2004; Osinski et al., 2004; Carpineto and Romano,2010; Carpineto et al., 2011; Moreno et al., 2013)and knowledge-based ones (Ferragina and Gulli,2008; Scaiella et al., 2012; Di Marco and Nav-igli, 2013).
Successful results have been obtainedby recent works compared to STC (Zamir and Et-zioni, 1998) and LINGO (Osinski et al., 2004)which provide publicly available implementations,and as a consequence, are often used as state-of-the-art baselines.
On the one hand, STC pro-poses a monothetic methodology which mergesbase clusters with high string overlap relying onsuffix trees.
On the other hand, LINGO is a poly-thetic solution which reduces a term-documentmatrix using single value decomposition and as-signs documents to each discovered latent topic.All solutions have been evaluated on differ-ent datasets and evaluation measures.
The well-known F1has been used as the standard evaluationmetric.
More recently, (Carpineto and Romano,1Moresque ODP-239F1Fb3F1Fb3Algo.
Stand.
k Tuned k Stand.
k Tuned k Stand.
k Tuned k Stand.
k Tuned kSTC 0.4550 12.7 0.6000 2.9 0.4602 12.7 0.4987 2.9 0.3238 12.4 0.3350 3.0 0.4027 12.4 0.4046 14.5LINGO 0.3258 26.7 0.6034 3.0 0.3989 26.7 0.5004 5.8 0.2029 27.7 0.3320 3.0 0.3461 27.7 0.4459 8.7BiKm 0.3165 9.7 0.5891 2.1 0.3145 9.7 0.4240 2.1 0.1995 12.1 0.3381 2.2 0.3074 12.1 0.3751 2.2Random - - 0.5043 2 - - 0.3548 2 - - 0.2980 2 - - 0.3212 2Table 1: Standard, Tuned and Random Results for Moresque and ODP-239 datasets.2010) evidenced more complete results with thegeneral definition of the F?-measure for ?
={1, 2, 5}, (Navigli and Crisafulli, 2010) introducedthe Rand Index metric and (Moreno et al., 2013)used Fb3 introduced by (Amigo?
et al., 2009) as amore adequate metric for clustering.Different standard datasets have been built suchas AMBIENT2 (Carpineto and Romano, 2009),ODP-2393 (Carpineto and Romano, 2010) andMoresque4 (Navigli and Crisafulli, 2010).
ODP-239, an improved version of AMBIENT, is basedon DMOZ5 where each query, over 239 ones, is aselected category in DMOZ and its associated sub-categories are considered as the respective clus-ter results.
The small text description included inDMOZ is considered as a Web snippet.
Moresqueis composed by 114 queries selected from a listof ambiguous Wikipedia entries.
For each query, aset of Web results have been collected from a com-mercial search engine and manually classified intothe disambiguation Wikipedia pages which formthe reference clusters.In Table 2, we report the results obtained sofar in the literature by text-based and knowledge-based strategies for the standard F1over ODP-239and Moresque datasets.F1ODP239 MoresqueTextSTC 0.324 0.455LINGO 0.273 0.326(Carpineto and Romano, 2010) 0.313 -(Moreno et al., 2013) 0.390 0.665Know.
(Scaiella et al., 2012) 0.413 -(Di Marco and Navigli, 2013) - 0.7204*Table 2: State-of-the-art Results for SRC.
(*) Theresult of (Di Marco and Navigli, 2013) is basedon a reduced version of AMBIENT + Moresque.3 Baseline SRC AlgorithmsNewly proposed algorithms are usually tuned to-wards their maximal performance.
However, theresults of baseline algorithms are usually run with2http://credo.fub.it/ambient/ [Last acc.
: Jan., 2014]3http://credo.fub.it/odp239/ [Last acc.
: Jan., 2014]4http://lcl.uniroma1.it/moresque/ [Last acc.
: Jan., 2014]5http://www.dmoz.org [Last acc.
: Jan., 2014]default parameters based on available implemen-tations.
As such, no conclusive remarks can bedrawn knowing that tuned versions might provideimproved results.In particular, available implementations6 ofSTC, LINGO and the Bisection K-means (BiKm)include a fixed stopping criterion.
However, itis well-known that tuning the number of outputclusters may greatly impact the clustering perfor-mance.
In order to provide fair results for base-line algorithms, we evaluated a k-dependent7 ver-sion for all baselines.
We ran all algorithms fork = 2..20 and chose the best result as the ?op-timal?
performance.
Table 1 sums up results forall the baselines in their different configurationsand shows that tuned versions outperform standard(available) ones both for F1and Fb3 over ODP-239 and Moresque.4 Cascade SRC AlgorithmsIn the previous section, our aim was to claim thattunable versions of existing baseline algorithmsmight evidence improved results when faced tothe ones reported in the literature.
And thesevalues should be taken as the ?real?
baseline re-sults within the context of controllable environ-ments.
However, exploring all the parameter spaceis not an applicable solution in a real-world situa-tion where the reference is unknown.
As such, astopping criterion must be defined to adapt to anydataset distribution.
This is the particular case forthe standard implementations of STC and LINGO.Previous results (Carpineto and Romano, 2010)showed that different SRC algorithms provide dif-ferent results and hopefully complementary ones.For instance, STC demonstrates high recall andlow precision, while LINGO inversely evidenceshigh precision for low recall.
Iteratively apply-ing baseline SRC algorithms may thus lead toimproved results by exploiting each algorithm?sstrengths.6http://carrot2.org [Last acc.
: Jan., 2014]7Carrot2 parameters maxClusters, desiredClusterCount-Base and clusterCount were used to set k value.2In a cascade strategy, we first cluster the ini-tial set of Web page snippets with any SRC al-gorithm.
Then, the input of the second SRC al-gorithm is the set of meta-documents built fromthe documents belonging to the same cluster8.
Fi-nally, each clustered meta-document is mapped tothe original documents generating the final clus-ters.
This process can iteratively be applied, al-though we only consider two-level cascade strate-gies in this paper.This strategy can be viewed as an easy, re-producible and parameter free baseline SRC im-plementation that should be compared to existingstate-of-the-art algorithms.
Table 3 shows the re-sults obtained with different combinations of SRCbaseline algorithms for the cascade strategy bothfor F1and Fb3 over ODP-239 and Moresque.
The?Stand.?
column corresponds to the performanceof the cascade strategy and k to the automaticallyobtained number of clusters.
Results show thatthe combination STC-STC achieves the best per-formance overall for the F1and STC-LINGO isthe best combination for the Fb3 in both datasets.In order to provide a more complete evaluation,we included in column ?Equiv.?
the performancethat could be obtained by the tunable version ofeach single baseline algorithm based on the samek.
Interestingly, the cascade strategy outperformsthe tunable version for any k for F1but fails tocompete (not by far) with Fb3 .
This issue will bediscussed in the next section.5 DiscussionIn Table 1, one can see that when using the tunedversion and evaluating with F1, the best perfor-mance for each baseline algorithm is obtained forthe same number of output clusters independentlyof the dataset (i.e.
around 3 for STC and LINGOand 2 for BiKm).
As such, a fast conclusion wouldbe that the tuned versions of STC, LINGO andBiKm are strong baselines as they show similarbehaviour over datasets.
Then, in a realistic situa-tion, k might be directly tuned to these values.However, when comparing the output numberof clusters based on the best F1value to the refer-ence number of clusters, a huge difference is ev-idenced.
Indeed, in Moresque, the ground-truthaverage number of clusters is 6.6 and exactly 10in ODP-239.
Interestingly, Fb3 shows more accu-rate values for the number of output clusters for8Fused using concatenation of strings.the best tuned baseline performances.
In particu-lar, the best Fb3 results are obtained for LINGOwith 5.8 clusters for Moresque and 8.7 clustersfor ODP-239 which most approximate the ground-truths.In order to better understand the behaviour ofeach evaluation metric (i.e.
F?and Fb3) over dif-ferent k values, we experienced a uniform randomclustering over Moresque and ODP-239.
In Fig-ure 1(c), we illustrate these results.
The importantissue is that F?is more sensitive to the numberof output clusters than Fb3 .
On the one hand, allF?measures provide best results for k = 2 anda random algorithm could reach F1=0.5043 forMoresque and F1=0.2980 for ODP-239 (see Ta-ble 1), thus outperforming almost all standard im-plementations of STC, LINGO and BiKm for bothdatasets.
On the other hand, Fb3 shows that moststandard baseline implementations outperform therandom algorithm.Moreover, in Figures 1(a) and 1(b), we illus-trate the different behaviours between F1and Fb3for k = 2..20 for both standard and tuned ver-sions of STC, LINGO and BiKm.
One may clearlysee that Fb3 is capable to discard the algorithm(BiKm) which performs worst in the standard ver-sion while this is not the case for F1.
And, forLINGO, the optimal performances over Moresqueand ODP-239 are near the ground-truth number ofclusters while this is not the case for F1which ev-idences a decreasing tendency when k increases.In section 4, we showed that competitive resultscould be achieved with a cascade strategy based onbaseline algorithms.
Although results outperformstandard and tunable baseline implementations forF1, it is wise to use Fb3 to better evaluate the SRCtask, based on our previous discussion.
In thiscase, the best values are obtained by STC-LINGOwith Fb3=0.4980 for Moresque and Fb3=0.4249for ODP-239, which highly approximate the val-ues reported in (Moreno et al., 2013): Fb3=0.490(Moresque) and Fb3=0.452 (ODP-239).
Addition-ally, when STC is performed first and LINGO laterthe cascade algorithm scale better due to LINGOand STC scaling properties9.6 ConclusionThis work presents a discussion about the use ofbaseline algorithms in SRC and evaluation met-9http://carrotsearch.com/lingo3g-comparison [Last acc.
:Jan., 2014]3Moresque ODP-239F1Fb3F1Fb3Level 1 Level 2 Stand.
Equiv.
k Stand.
Equiv.
k Stand.
Equiv.
k Stand.
Equiv.
kSTCSTC 0.6145 0.5594 3.1 0.4550 0.4913 3.1 0.3629 0.3304 3.2 0.3982 0.4023 3.2LINGO 0.5611 0.4932 7.3 0.4980 0.4716 7.3 0.3624 0.3258 6.9 0.4249 0.4010 6.9BiKm 0.5413 0.5160 4.5 0.4395 0.4776 4.5 0.3319 0.3276 4.3 0.3845 0.4020 4.3LINGOSTC 0.5696 0.5176 6.7 0.4602 0.4854 6.7 0.3457 0.3029 7.2 0.4229 0.4429 7.2LINGO 0.4629 0.4371 13.7 0.4447 0.4566 13.7 0.2789 0.2690 13.6 0.3931 0.4237 13.6BiKm 0.4038 0.4966 8.6 0.3801 0.4750 8.6 0.2608 0.2953 8.5 0.3510 0.4423 8.5BiKmSTC 0.5873 0.5891 2.7 0.4144 0.4069 2.7 0.3425 0.3381 2.7 0.3787 0.3677 2.7LINGO 0.4773 0.5186 5.4 0.3832 0.3869 5.4 0.2819 0.3191 6.3 0.3546 0.3644 6.3BiKm 0.4684 0.5764 3.5 0.3615 0.4114 3.5 0.2767 0.3322 4.3 0.3328 0.3693 4.3Table 3: Cascade Results for Moresque and ODP-239 datasets.
(a) F1for Moresque (Left) and ODP-239 (Right).0.30.350.40.450.50.550.60.652 4 6 8 10 12 14 16 18 20F1kBiKm(Tuned)STC(Tuned)LINGO(Tuned)BiKm(Stand.)STC(Stand.)LINGO(Stand.
)0.180.20.220.240.260.280.30.320.340.362 4 6 8 10 12 14 16 18 20F1kBiKm(Tuned)STC(Tuned)LINGO(Tuned)BiKm(Stand.)STC(Stand.)LINGO(Stand.
)(b) Fb3for Moresque (Left) and ODP-239 (Right).0.30.320.340.360.380.40.420.440.460.480.50.522 4 6 8 10 12 14 16 18 20FbcubedkBiKm(Tuned)STC(Tuned)LINGO(Tuned)BiKm(Stand.)STC(Stand.)LINGO(Stand.
)0.30.320.340.360.380.40.420.440.462 4 6 8 10 12 14 16 18 20FbcubedkBiKm(Tuned)STC(Tuned)LINGO(Tuned)BiKm(Stand.)STC(Stand.)LINGO(Stand.
)(c) Evaluation Metrics for Random Clustering for Moresque (Left) and ODP-239 (Right).0.050.10.150.20.250.30.350.40.450.50.552 4 6 8 10 12 14 16 18 20PerformancekF1F2F5Fbcubed0.050.10.150.20.250.30.350.40.450.52 4 6 8 10 12 14 16 18 20PerformancekF1F2F5FbcubedFigure 1: F1and Fb3 for Moresque and ODP-239 for Standard, Tuned and Random Clustering.rics.
Our experiments show that Fb3 seems moreadapted to evaluate SRC systems than the com-monly used F1over the standard datasets avail-able so far.
New baseline values which approxi-mate state-of-the-art algorithms in terms of clus-tering performance can also be obtained by aneasy, reproducible and parameter free implemen-tation (the cascade strategy) and could be consid-ered as the ?new?
baseline results for future works.4ReferencesE.
Amigo?, J. Gonzalo, J. Artiles, and F. Verdejo.
2009.A comparison of extrinsic clustering evaluation met-rics based on formal constraints.
Information Re-trieval, 12(4):461?486.C.
Carpineto and G. Romano.
2009.
Mobile infor-mation retrieval with search results clustering : Pro-totypes and evaluations.
Journal of the AmericanSociety for Information Science, 60:877?895.C.
Carpineto and G. Romano.
2010.
Optimal metasearch results clustering.
In 33rd International ACMSIGIR Conference on Research and Development inInformation Retrieval (SIGIR), pages 170?177.C.
Carpineto, M. D?Amico, and A. Bernardini.
2011.Full discrimination of subtopics in search resultswith keyphrase-based clustering.
Web Intelligenceand Agent Systems, 9(4):337?349.A.
Di Marco and R. Navigli.
2013.
Clustering anddiversifying web search results with graph-basedword sense induction.
Computational Linguistics,39(3):709?754.P.
Ferragina and A. Gulli.
2008.
A personalized searchengine based on web-snippet hierarchical clustering.Software: Practice and Experience, 38(2):189?225.M.A.
Hearst and J.O.
Pedersen.
1996.
Re-examiningthe cluster hypothesis: Scatter/gather on retrieval re-sults.
In 19th Annual International Conference onResearch and Development in Information Retrieval(SIGIR), pages 76?84.J.G.
Moreno, G. Dias, and G. Cleuziou.
2013.
Post-retrieval clustering using third-order similarity mea-sures.
In 51st Annual Meeting of the Association forComputational Linguistics (ACL), pages 153?158.R.
Navigli and G. Crisafulli.
2010.
Inducing wordsenses to improve web search result clustering.In Proceedings of the 2010 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 116?126.S.
Osinski, J. Stefanowski, and D. Weiss.
2004.
Lingo:Search results clustering algorithm based on singu-lar value decomposition.
In Intelligent InformationSystems Conference (IIPWM), pages 369?378.U.
Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.2012.
Topical clustering of search results.
In 5thACM International Conference on Web Search andData Mining (WSDM), pages 223?232.O.
Zamir and O. Etzioni.
1998.
Web document clus-tering: A feasibility demonstration.
In 21st AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval (SIGIR),pages 46?54.H.J.
Zeng, Q.C.
He, Z. Chen, W.Y.
Ma, and J. Ma.2004.
Learning to cluster web search results.
In27th Annual International Conference on Researchand Development in Information Retrieval (SIGIR),pages 210?217.5
