Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 33?40,New York, June 2006. c?2006 Association for Computational LinguisticsEffectively Using Syntax for Recognizing False EntailmentRion SnowComputer Science DepartmentStanford UniversityStanford, CA 94305rion@cs.stanford.eduLucy Vanderwende and Arul MenezesMicrosoft ResearchOne Microsoft WayRedmond, WA 98027{lucyv,arulm}@microsoft.comAbstractRecognizing textual entailment is a chal-lenging problem and a fundamental com-ponent of many applications in naturallanguage processing.
We present a novelframework for recognizing textual entail-ment that focuses on the use of syntacticheuristics to recognize false entailment.We give a thorough analysis of our sys-tem, which demonstrates state-of-the-artperformance on a widely-used test set.1 IntroductionRecognizing the semantic equivalence of two frag-ments of text is a fundamental component of manyapplications in natural language processing.
Recog-nizing textual entailment, as formulated in the recentPASCAL Challenge 1, is the problem of determiningwhether some text sentence T entails some hypothe-sis sentence H .The motivation for this formulation was to iso-late and evaluate the application-independent com-ponent of semantic inference shared across many ap-plication areas, reflected in the division of the PAS-CAL RTE dataset into seven distinct tasks: Informa-tion Extraction (IE), Comparable Documents (CD),Reading Comprehension (RC), Machine Translation(MT), Information Retrieval (IR), Question Answer-ing (QA), and Paraphrase Acquisition (PP).1http://www.pascal-network.org/Challenges/RTE.
The ex-amples given throughout this paper are from the first PASCALRTE dataset, described in Section 6.The RTE problem as presented in the PASCALRTE dataset is particularly attractive in that it is areasonably simple task for human annotators withhigh inter-annotator agreement (95.1% in one inde-pendent labeling (Bos and Markert, 2005)), but anextremely challenging task for automated systems.The highest accuracy systems on the RTE test setare still much closer in performance to a randombaseline accuracy of 50% than to the inter-annotatoragreement.
For example, two high-accuracy systemsare those described in (Tatu and Moldovan, 2005),achieving 60.4% accuracy with no task-specific in-formation, and (Bos and Markert, 2005), whichachieves 61.2% task-dependent accuracy, i.e.
whenable to use the specific task labels as input.Previous systems for RTE have attempted a widevariety of strategies.
Many previous approacheshave used a logical form representation of the textand hypothesis sentences, focusing on deriving aproof by which one can infer the hypothesis logicalform from the text logical form (Bayer et al, 2005;Bos and Markert, 2005; Raina et al, 2005; Tatu andMoldovan, 2005).
These papers often cite that a ma-jor obstacle to accurate theorem proving for the taskof textual entailment is the lack of world knowledge,which is frequently difficult and costly to obtain andencode.
Attempts have been made to remedy thisdeficit through various techniques, including model-building (Bos and Markert, 2005) and the additionof semantic axioms (Tatu and Moldovan, 2005).Our system diverges from previous approachesmost strongly by focusing upon false entailments;rather than assuming that a given entailment is falseuntil proven true, we make the opposite assump-33tion, and instead focus on applying knowledge-freeheuristics that can act locally on a subgraph of syn-tactic dependencies to determine with high confi-dence that the entailment is false.
Our approach isinspired by an analysis of the RTE dataset that sug-gested a syntax-based approach should be approxi-mately twice as effective at predicting false entail-ment as true entailment (Vanderwende and Dolan,2006).
The analysis implied that a great deal of syn-tactic information remained unexploited by existingsystems, but gave few explicit suggestions on howsyntactic information should be applied; this paperprovides a starting point for creating the heuristicscapable of obtaining the bound they suggest2.2 System DescriptionSimilar to most other syntax-based approaches torecognizing textual entailment, we begin by rep-resenting each text and hypothesis sentence pairin logical forms.
These logical forms are gener-ated using NLPWIN3, a robust system for naturallanguage parsing and generation (Heidorn, 2000).Our logical form representation may be consid-ered equivalently as a set of triples of the formRELATION(nodei, nodej), or as a graph of syntac-tic dependencies; we use both terminologies inter-changeably.
Our algorithm proceeds as follows:1.
Parse each sentence with the NLPWIN parser,resulting in syntactic dependency graphs for thetext and hypothesis sentences.2.
Attempt an alignment of each content node inthe dependency graph of the hypothesis sen-tence to some node in the graph of the text sen-tence, using a set of heuristics for alignment(described in Section 3).3.
Using the alignment, apply a set of syntacticheuristics for recognizing false entailment (de-scribed in Section 4); if any match, predict thatthe entailment is false.2(Vanderwende and Dolan, 2006) suggest that the truth orfalsehood of 48% of the entailment examples in the RTE test setcould be correctly identified via syntax and a thesaurus alone;thus by random guessing on the rest of the examples one mighthope for an accuracy level of 0.48 + 0.522 = 74%.3To aid in the replicability of our experiments, we havepublished the NLPWIN logical forms for all sentences fromthe development and test sets in the PASCAL RTE dataset athttp://research.microsoft.com/nlp/Projects/RTE.aspx.lemma: freepos: Verbfeatures: Past,Pass,T1,Propositionlemma: _Xpos: PronTsublemma: hostagepos: Nounfeatures: Plur,Humn,Count,Anim,Conc,Humn_srTobjlemma: sixpos: Adjfeatures: Quant,Plur,Num,Value 6Lopslemma: Iraqpos: Nounfeatures: Sing,PrprN,Pers3,CntryLocn_inFigure 1: Logical form produced by NLPWIN forthe sentence ?Six hostages in Iraq were freed.?4.
If no syntactic heuristic matches, back off toa lexical similarity model (described in section5.1), with an attempt to align detected para-phrases (described in section 5.2).In addition to the typical syntactic information pro-vided by a dependency parser, the NLPWIN parserprovides an extensive number of semantic featuresobtained from various linguistic resources, creatinga rich environment for feature engineering.
For ex-ample, Figure 1 (from Dev Ex.
#616) illustrates thedependency graph representation we use, demon-strating the stemming, part-of-speech tagging, syn-tactic relationship identification, and semantic fea-ture tagging capabilities of NLPWIN.We define a content node to be any node whoselemma is not on a small stoplist of common stopwords.
In addition to content vs. non-content nodes,among content nodes we distinguish between en-tities and nonentities: an entity node is any nodeclassified by the NLPWIN parser as being a propernoun, quantity, or time.Each of the features of our system were developedfrom inspection of sentence pairs from the RTE de-velopment data set, and used in the final system onlyif they improved the system?s accuracy on the de-velopment set (or improved F-score if accuracy wasunchanged); sentence pairs in the RTE test set wereleft uninspected and used for testing purposes only.3 Linguistic cues for node alignmentOur syntactic heuristics for recognizing false entail-ment rely heavily on the correct alignment of wordsand multiword units between the text and hypothesislogical forms.
In the notation below, we will con-sider h and t to be nodes in the hypothesis H and34Hypothesis: ?
?Hepburn, who won four Oscars...?
?Text: ?
?Hepburn, a four-time Academy Award winner...?
?HepburnNoun winVerbTsubHepburnNounStringmatchOscarNounTobjwinnerNounDerivationalform matchfourAdjLopsAcademy_AwardNounSynonymmatchfour-timeAdjValuematchAppostnAttribModFigure 2: Example of synonym, value, and deriva-tional form alignment heuristics, Dev Ex.
#767text T logical forms, respectively.
To accomplishthe task of node alignment we rely on the followingheuristics:3.1 WordNet synonym matchAs in (Herrera et al, 2005) and others, we aligna node h ?
H to any node t ?
T that has boththe same part of speech and belongs to the samesynset in WordNet.
Our alignment considers mul-tiword units, including compound nouns (e.g., wealign ?Oscar?
to ?Academy Award?
as in Figure 2),as well as verb-particle constructions such as ?setoff?
(aligned to ?trigger?
in Test Ex.
#1983).3.2 Numeric value matchThe NLPWIN parser assigns a normalized numericvalue feature to each piece of text inferred to cor-respond to a numeric value; this allows us to align?6th?
to ?sixth?
in Test Ex.
#1175. and to align ?adozen?
to ?twelve?
in Test Ex.
#1231.3.3 Acronym matchMany acronyms are recognized using the syn-onym match described above; nonetheless, manyacronyms are not yet in WordNet.
For these cases wehave a specialized acronym match heuristic whichaligns pairs of nodes with the following properties:if the lemma for some node h consists only of cap-italized letters (with possible interceding periods),and the letters correspond to the first characters ofsome multiword lemma for some t ?
T , then weconsider h and t to be aligned.
This heuristic allowsus to align ?UNDP?
to ?United Nations Develop-ment Programme?
in Dev Ex.
#357 and ?ANC?
to?African National Congress?
in Test Ex.
#1300.3.4 Derivational form matchWe would like to align words which have the sameroot form (or have a synonym with the same rootform) and which possess similar semantic meaning,but which may belong to different syntactic cate-gories.
We perform this by using a combination ofthe synonym and derivationally-related form infor-mation contained within WordNet.
Explicitly ourprocedure for constructing the set of derivationally-related forms for a node h is to take the union of allderivationally-related forms of all the synonyms ofh (including h itself), i.e.
:DERIV(h) = ?s?WN-SYN(h)WN-DERIV(s)In addition to the noun/verb derivationally-relatedforms, we detect adjective/adverb derivationally-related forms that differ only by the suffix ?ly?.Unlike the previous alignment heuristics, we donot expect that two nodes aligned via derivationally-related forms will play the same syntactic role intheir respective sentences.
Thus we consider twonodes aligned in this way to be soft-aligned, and wedo not attempt to apply our false entailment recog-nition heuristics to nodes aligned in this way.3.5 Country adjectival form / demonym matchAs a special case of derivational form match, wesoft-align matches from an explicit list of placenames, adjectival forms, and demonyms4; e.g.,?Sweden?
and ?Swedish?
in Test Ex.
#1576.3.6 Other heuristics for alignmentIn addition to these heuristics, we implemented a hy-ponym match heuristic similar to that discussed in(Herrera et al, 2005), and a heuristic based on thestring-edit distance of two lemmas; however, theseheuristics yielded a decrease in our system?s accu-racy on the development set and were thus left outof our final system.4 Recognizing false entailmentThe bulk of our system focuses on heuristics forrecognizing false entailment.
For purposes of no-tation, we define binary functions for the existence4List of adjectival forms and demonyms based on the list at:http://en.wikipedia.org/wiki/List of demonyms35Unaligned Entity: ENTITY(h) ?
?t.
?ALIGN(h, t) ?
False.Negation Mismatch: ALIGN(h, t) ?
NEG(t) 6= NEG(h) ?
False.Modal Mismatch: ALIGN(h, t) ?
MOD(t) ?
?MOD(h) ?
False.Antonym Match: ALIGN(h1, t1) ?
REL(h0, h1) ?
REL(t0, t1) ?
LEMMA(t0) ?
ANTONYMS(h0) ?
FalseArgument Movement: ALIGN(h1, t1) ?
ALIGN(h2, t2) ?
REL(h1, h2) ?
?REL(t1, t2) ?
REL ?
{SUBJ, OBJ, IND} ?
FalseSuperlative Mismatch: ?
(SUPR(h1) ?
(ALIGN(h1, t1) ?
ALIGN(h2, t2) ?
REL1(h2, h1) ?
REL1(t2, t1)??t3.
(REL2(t2, t3) ?
REL2 ?
{MOD,POSSR,LOCN} ?
REL2(h2, h3) ?
ALIGN(h3, t3))) ?
FalseConditional Mismatch: ALIGN(h1, t1) ?
ALIGN(h2, t2) ?
COND ?
PATH(t1, t2) ?
COND /?
PATH(h1, h2) ?
FalseTable 1: Summary of heuristics for recognizing false entailmentof each semantic node feature recognized by NLP-WIN; e.g., if h is negated, we state that NEG(h) =TRUE.
Similarly we assign binary functions forthe existence of each syntactic relation defined overpairs of nodes.
Finally, we define the functionALIGN(h, t) to be true if and only if the node h ?
Hhas been ?hard-aligned?
to the node t ?
T using oneof the heuristics in Section 3.
Other notation is de-fined in the text as it is used.
Table 1 summarizes allheuristics used in our final system to recognize falseentailment.4.1 Unaligned entityIf some node h has been recognized as an entity (i.e.,as a proper noun, quantity, or time) but has not beenaligned to any node t, we predict that the entailmentis false.
For example, we predict that Test Ex.
#1863is false because the entities ?Suwariya?, ?20 miles?,and ?35?
in H are unaligned.4.2 Negation mismatchIf any two nodes (h, t) are aligned, and one (andonly one) of them is negated, we predict that the en-tailment is false.
Negation is conveyed by the NEGfeature in NLPWIN.
This heuristic allows us to pre-dict false entailment in the example ?Pertussis is notvery contagious?
and ?...pertussis, is a highly conta-gious bacterial infection?
in Test Ex.
#1144.4.3 Modal auxiliary verb mismatchIf any two nodes (h, t) are aligned, and t is modifiedby a modal auxiliary verb (e.g, can, might, should,etc.)
but h is not similarly modified, we predict thatthe entailment is false.
Modification by a modal aux-iliary verb is conveyed by the MOD feature in NLP-WIN.
This heuristic allows us to predict false en-tailment between the text phrase ?would constitutea threat to democracy?, and the hypothesis phrase?constitutes a democratic threat?
in Test Ex.
#1203.4.4 Antonym matchIf two aligned noun nodes (h1, t1) are both subjectsor both objects of verb nodes (h0, t0) in their re-spective sentences, i.e., REL(h0, h1)?
REL(t0, t1)?REL ?
{SUBJ,OBJ}, then we check for a verbantonym match between (h0, t0).
We constructthe set of verb antonyms using WordNet; we con-sider the antonyms of h0 to be the union of theantonyms of the first three senses of LEMMA(h0),or of the nearest antonym-possessing hypernyms ifthose senses do not themselves have antonyms inWordNet.
Explicitly our procedure for constructingthe antonym set of a node h0 is as follows:1.
ANTONYMS(h0) = {}2.
For each of the first three listed senses s ofLEMMA(h0) in WordNet:(a) While |WN-ANTONYMS(s)| = 0i.
s ?
WN-HYPERNYM(s)(b) ANTONYMS(h0) ?
ANTONYMS(h0) ?WN-ANTONYMS(s)3. return ANTONYMS(h0)In addition to the verb antonyms in WordNet, wedetect the prepositional antonym pairs (before/after,to/from, and over/under).
This heuristic allows us topredict false entailment between ?Black holes canlose mass...?
and ?Black holes can regain some oftheir mass...?
in Test Ex.
#1445.4.5 Argument movementFor any two aligned verb nodes (h1, t1), we con-sider each noun child h2 of h1 possessing any of36Hypothesis TextkillVerbPrime MinisterRobert MalvalNounTobjAristideNounTsubkillVerbPrime MinisterRobert MalvalNounAristideNounTsubconferenceNounTobjcallVerbAttribconferenceNounTobjTsubPort-au-PrinceNounLocn_inFigure 3: Example of object movement signalingfalse entailmentthe subject, object, or indirect object relations toh1, i.e., there exists REL(h1, h2) such that REL ?
{SUBJ, OBJ, IND}.
If there is some node t2 such thatALIGN(h2, t2), but REL(t1, t2) 6= REL(h1, h2), thenwe predict that the entailment is false.As an example, consider Figure 3, representingsubgraphs from Dev Ex.
#1916:T : ...U.N. officials are also dismayed that Aristide killed a con-ference called by Prime Minister Robert Malval...H: Aristide kills Prime Minister Robert Malval.Here let (h1, t1) correspond to the aligned verbswith lemma kill, where the object of h1 has lemmaPrime Minister Robert Malval, and the object of t1has lemma conference.
Since h2 is aligned to somenode t2 in the text graph, but ?OBJ(t1, t2), the sen-tence pair is rejected as a false entailment.4.6 Superlative mismatchIf some adjective node h1 in the hypothesis is iden-tified as a superlative, check that all of the followingconditions are satisfied:1. h1 is aligned to some superlative t1 in the textsentence.2.
The noun phrase h2 modified by h1 is alignedto the noun phrase t2 modified by t1.3.
Any additional modifier t3 of the noun phraset2 is aligned to some modifier h3 of h2 in thehypothesis sentence (reverse subset match).If any of these conditions are not satisfied, we pre-dict that the entailment is false.
This heuristic allowsus to predict false entailment in (Dev Ex.
#908):T : Time Warner is the world?s largest media and Internet com-pany.H: Time Warner is the world?s largest company.Here ?largest media and Internet company?
in Tfails the reverse subset match (condition 3) to?largest company?
in H .4.7 Conditional mismatchFor any pair of aligned nodes (h1, t1), if there ex-ists a second pair of aligned nodes (h2, t2) suchthat the shortest path PATH(t1, t2) in the depen-dency graph T contains the conditional relation,then PATH(h1, h2) must also contain the conditionalrelation, or else we predict that the entailment isfalse.
For example, consider the following false en-tailment (Dev Ex.
#60):T : If a Mexican approaches the border, he?s assumed to be try-ing to illegally cross.H: Mexicans continue to illegally cross border.Here, ?Mexican?
and ?cross?
are aligned, and thepath between them in the text contains the condi-tional relation, but does not in the hypothesis; thusthe entailment is predicted to be false.4.8 Other heuristics for false entailmentIn addition to these heuristics, we additionally im-plemented an IS-A mismatch heuristic, which at-tempted to discover when an IS-A relation in the hy-pothesis sentence was not implied by a correspond-ing IS-A relation in the text; however, this heuristicyielded a loss in accuracy on the development setand was therefore not included in our final system.5 Lexical similarity and paraphrasedetection5.1 Lexical similarity using MindNetIn case none of the preceding heuristics for rejec-tion are applicable, we back off to a lexical sim-ilarity model similar to that described in (Glick-man et al, 2005).
For every content node h ?
H37not already aligned by one of the heuristics in Sec-tion 3, we obtain a similarity score MN(h, t) from asimilarity database that is constructed automaticallyfrom the data contained in MindNet5 as described in(Richardson, 1997).
Our similarity function is thus:sim(h, t) =????
?1 if ANY-ALIGN(h, t)MN(h, t) if MN(h, t) > minmin otherwiseWhere the minimum score min is a parametertuned for maximum accuracy on the developmentset; min = 0.00002 in our final system.
We thencompute the entailment score:score(H,T ) = 1|H|?h?Hmaxt?Tsim(h, t)This approach is identical to that used in (Glick-man et al, 2005), except that we use alignmentheuristics and MindNet similarity scores in placeof their web-based estimation of lexical entailmentprobabilities, and we take as our score the geomet-ric mean of the component entailment scores ratherthan the unnormalized product of probabilities.5.2 Measuring phrasal similarity using the webThe methods discussed so far for alignment are lim-ited to aligning pairs of single words or multiple-word units constituting single syntactic categories;these are insufficient for the problem of detectingmore complicated paraphrases.
For example, con-sider the following true entailment (Dev Ex.
#496):T : ...Muslims believe there is only one God.H: Muslims are monotheistic.Here we would like to align the hypothesis phrase?are monotheistic?
to the text phrase ?believe thereis only one God?
; unfortunately, single-node align-ment aligns only the nodes with lemma ?Muslim?.In this section we describe the approach used in oursystem to approximate phrasal similarity via distrib-utional information obtained using the MSN Searchsearch engine.We propose a metric for measuring phrasal simi-larity based on a phrasal version of the distributionalhypothesis: we propose that a phrase template Ph5http://research.microsoft.com/mnex(e.g.
?xh are monotheistic?)
has high semantic simi-larity to a template Pt (e.g.
?xt believe there is onlyone God?
), with possible ?slot-fillers?
xh and xt, re-spectively, if the overlap of the sets of observed slot-fillers Xh ?Xt for those phrase templates is high insome sufficiently large corpus (e.g., the Web).To measure phrasal similarity we issue the sur-face text form of each candidate phrase template asa query to a web-based search engine, and parse thereturned sentences in which the candidate phrase oc-curs to determine the appropriate slot-fillers.
For ex-ample, in the above example, we observe the set ofslot-fillers Xt = {Muslims, Christians, Jews, Saiv-ities, Sikhs, Caodaists, People}, and Xh ?
Xt ={Muslims, Christians, Jews, Sikhs, People}.Explicitly, given the text and hypothesis logicalforms, our algorithm proceeds as follows to computethe phrasal similarity between all phrase templatesin H and T :1.
For each pair of aligned single node and un-aligned leaf node (t1, tl) (or pair of alignednodes (t1, t2)) in the text T :(a) Use NLPWIN to generate a surface textstring S from the underlying logical formPATH(t1, t2).
(b) Create the surface string template phrasePt by removing from S the lemmas corre-sponding to t1 (and t2, if path is betweenaligned nodes).
(c) Perform a web search for the string Pt.
(d) Parse the resulting sentences containingPt and extract all non-pronoun slot fillersxt ?
Xt that satisfy the same syntacticroles as t1 in the original sentence.2.
Similarly, extract the slot fillers Xh for eachdiscovered phrase template Ph in H .3.
Calculate paraphrase similarity as a function ofthe overlap between the slot-filler sets Xt andXh, i.e: score(Ph, Pt) = |Xh?Xt||Xt| .We then incorporate paraphrase similarity within thelexical similarity model by allowing, for some un-aligned node h ?
Ph, where t ?
Pt:sim(h, t) = max(MN(h, t), score(Ph, Pt))38Our approach to paraphrase detection is most similarto the TE/ASE algorithm (Szpektor et al, 2004), andbears similarity to both DIRT (Lin and Pantel, 2001)and KnowItAll (Etzioni et al, 2004).
The chiefdifference in our algorithm is that we generate thesurface text search strings from the parsed logicalforms using the generation capabilities of NLPWIN(Aikawa et al, 2001), and we verify that the syn-tactic relations in each discovered web snippet areisomorphic to those in the original candidate para-phrase template.6 Results and DiscussionIn this section we present the final results of our sys-tem on the PASCAL RTE-1 test set, and examine ourfeatures in an ablation study.
The PASCAL RTE-1development and test sets consist of 567 and 800 ex-amples, respectively, with the test set split equallybetween true and false examples.6.1 Results and Performance Comparison onthe PASCAL RTE-1 Test SetTable 2 displays the accuracy and confidence-weighted score6 (CWS) of our final system on eachof the tasks for both the development and test sets.Our overall test set accuracy of 62.50% rep-resents a 2.1% absolute improvement over thetask-independent system described in (Tatu andMoldovan, 2005), and a 20.2% relative improve-ment in accuracy over their system with respect toan uninformed baseline accuracy of 50%.To compute confidence scores for our judgments,any entailment determined to be false by any heuris-tic was assigned maximum confidence; no attemptswere made to distinguish between entailments re-jected by different heuristics.
The confidence ofall other predictions was calculated as the ab-solute value in the difference between the outputscore(H,T ) of the lexical similarity model and thethreshold t = 0.1285 as tuned for highest accu-racy on our development set.
We would expect ahigher CWS to result from learning a more appro-priate confidence function; nonetheless our overall6As in (Dagan et al, 2005) we compute the confidence-weighted score (or ?average precision?)
over n examples{c1, c2, ..., cn} ranked in order of decreasing confidence ascws = 1n?ni=1(#correct-up-to-rank-i)iDev Set Test SetTask acc cws acc cwsCD 0.8061 0.8357 0.7867 0.8261RC 0.5534 0.5885 0.6429 0.6476IR 0.6857 0.6954 0.6000 0.6571MT 0.7037 0.7145 0.6000 0.6350IE 0.5857 0.6008 0.5917 0.6275QA 0.7111 0.7121 0.5308 0.5463PP 0.7683 0.7470 0.5200 0.5333All 0.6878 0.6888 0.6250 0.6534Table 2: Summary of accuracies and confidence-weighted scores, by taskAlignment Feature Dev TestSynonym Match 0.0106 0.0038Derivational Form 0.0053 0.0025Paraphrase 0.0053 0.0000Lexical Similarity 0.0053 0.0000Value Match 0.0017 0.0013Acronym Match 0.0017 0.0013Adjectival Form7 0.0000 0.0063False Entailment Feature Dev TestNegation Mismatch 0.0106 0.0025Argument Movement 0.0070 0.0250Conditional Mismatch 0.0053 0.0037Modal Mismatch 0.0035 0.0013Superlative Mismatch 0.0035 -0.0025Entity Mismatch 0.0018 0.0063Table 3: Feature ablation study; quantity is the ac-curacy loss obtained by removal of single featuretest set CWS of 0.6534 is higher than previously-reported task-independent systems (however, thetask-dependent system reported in (Raina et al,2005) achieves a CWS of 0.686).6.2 Feature analysisTable 3 displays the results of our feature ablationstudy, analyzing the individual effect of each feature.Of the seven heuristics used in our final systemfor node alignment (including lexical similarity andparaphrase detection), our ablation study showed7As discussed in Section 2, features with no effect on devel-opment set accuracy were included in the system if and only ifthey improved the system?s unweighted F-score.39that five were helpful in varying degrees on our testset, but that removal of either MindNet similarityscores or paraphrase detection resulted in no accu-racy loss on the test set.Of the six false entailment heuristics used in thefinal system, five resulted in an accuracy improve-ment on the test set (the most effective by far wasthe ?Argument Movement?, resulting in a net gainof 20 correctly-classified false examples); inclusionof the ?Superlative Mismatch?
feature resulted in asmall net loss of two examples.We note that our heuristics for false entailment,where applicable, were indeed significantly more ac-curate than our final system as a whole; on the set ofexamples predicted false by our heuristics we had71.3% accuracy on the training set (112 correct outof 157 predicted), and 72.9% accuracy on the test set(164 correct out of 225 predicted).7 ConclusionIn this paper we have presented and analyzed a sys-tem for recognizing textual entailment focused pri-marily on the recognition of false entailment, anddemonstrated higher performance than achieved byprevious approaches on the widely-used PASCALRTE test set.
Our system achieves state-of-the-art performance despite not exploiting a wide ar-ray of sources of knowledge used by other high-performance systems; we submit that the perfor-mance of our system demonstrates the unexploitedpotential in features designed specifically for therecognition of false entailment.AcknowledgmentsWe thank Chris Brockett, Michael Gamon, GaryKacmarick, and Chris Quirk for helpful discussion.Also, thanks to Robert Ragno for assistance withthe MSN Search API.
Rion Snow is supported byan NDSEG Fellowship sponsored by the DOD andAFOSR.ReferencesTakako Aikawa, Maite Melero, Lee Schwartz, and AndiWu.
2001.
Multilingual Sentence Generation.
InProc.
of 8th European Workshop on Natural LanguageGeneration.Samuel Bayer, John Burger, Lisa Ferro, John Henderson,and Alexander Yeh.
2005.
MITRE?s Submissions tothe EU Pascal RTE Challenge.
In Proc.
of the PASCALChallenges Workshop on RTE 2005.Johan Bos and Katja Markert.
2005.
Recognizing Tex-tual Entailment with Logical Inference.
In Proc.
HLT-EMNLP 2005.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL Recognising Textual EntailmentChallenge.
In Proceedings of the PASCAL ChallengesWorkshop on RTE 2005.Oren Etzioni, Michael Cafarella, Doug Downey, StanleyKok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-land, Daniel S. Weld, and Alexander Yates.
2004.Web-scale information extraction in KnowItAll.
InProc.
WWW 2004.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.Web Based Probabilistic Textual Entailment.
In Proc.of the PASCAL Challenges Workshop on RTE 2005.George E. Heidorn.
2000.
Intelligent Writing Assis-tance.
In R. Dale, H. Moisl, and H. Somers (eds.
),A Handbook of Natural Language Processing: Tech-niques and Applications for the Processing of Lan-guage as Text.
Marcel Dekker, New York.
181-207.Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.
2005.Textual Entailment Recognision Based on Depen-dency Analysis and WordNet.
In Proc.
of the PASCALChallenges Workshop on RTE 2005.Dekang Lin and Patrick Pantel.
2001.
DIRT - Discoveryof Inference Rules from Text.
In Proc.
KDD 2001.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via learning andabductive reasoning.
In Proc.
AAAI 2005.Stephen D. Richardson.
1997.
Determining Similarityand Inferring Relations in a Lexical Knowledge Base.Ph.D.
thesis, The City University of New York.Idan Szpektor, Hristo Tanev, Ido Dagan, and BonaventuraCoppola.
2004.
Scaling Web-based Acquisition ofEntailment Relations.
In Proc.
EMNLP 2004.Marta Tatu and Dan Moldovan.
2005.
A Semantic Ap-proach to Recognizing Textual Entailment.
In Proc.HLT-EMNLP 2005.Lucy Vanderwende and William B. Dolan.
2006.
WhatSyntax Can Contribute in the Entailment Task.
InMLCW 2005, LNAI 3944, pp.
205?216.
J. Quinonero-Candela et al (eds.).
Springer-Verlag.40
