Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 28?36,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsEvaluating Evaluation Metrics for Minimalist ParsingThomas GrafDepartment of LinguisticsStony Brook Universitymail@thomasgraf.netBradley MarcinekDepartment of LinguisticsStony Brook Universitybradley.marcinek@stonybrook.eduAbstractIn response to Kobele et al.
(2012), weevaluate four ways of linking the process-ing difficulty of sentences to the behav-ior of the top-down parser for Minimal-ist grammars developed in Stabler (2012).We investigate the predictions these fourmetrics make for a number of relativeclause constructions, and we conclude thatat this point, none of them capture the fullrange of attested patterns.1 IntroductionMinimalist grammars (MGs; (Stabler, 1997)) area mildly context-sensitive formalism inspired byMinimalist syntax (Chomsky, 1995), the domi-nant theory in generative syntax.
MGs allow us toevaluate syntactic proposals with respect to com-putational and cognitive criteria such as genera-tive capacity (Harkema, 2001; Michaelis, 2001) orthe memory structures they require (Kobele et al.,2007; Graf, 2012).A new kind of top-down parser for MGs has re-cently been presented by Stabler (2011b; 2012).Stabler?s parser is noteworthy because it usesderivation trees as a data structure in order toreduce MG parsing to a special case of parsingcontext-free grammars (CFGs).
This raises thequestion, though, whether derivation trees are apsychologically plausible data structure, and if so,to which extent the Stabler parser makes it possi-ble to test the psycholinguistic predictions of com-peting syntactic analyses.In order to address this question, a linking hy-pothesis is needed that connects the behavior ofthe parser to a processing difficulty metric.
Ko-bele et al.
(2012) ?
henceforth KGH ?
proposethat the difficulty of sentence s correlates withthe maximum number of parse steps the parserhas to keep a parse item in memory while pro-cessing s. This metric is called maximum tenure(Max).
Max is appealing because of its simplicityand sensitivity to differences in linguistic analysis,which makes it easy to determine the psycholin-guistic predictions of a specific syntactic analyses.In this paper, we show that Max does not makethe right predictions for I) relative clauses embed-ded in a sentential complement and II) subjectsgaps versus object gaps in relative clauses.
Wepresent a number of simple alternative measuresthat handle these phenomena correctly, but we alsoshow that these metrics fail in other cases (all re-sults are summarized in Tab.
1 on page 8).
We con-clude that the prospect of a simple direct link be-tween syntactic analysis and processing difficultyis tempting but not sufficiently developed at thispoint.The paper starts with a quick introduction toMGs (Sec.
2.1) and how they are parsed (Sec.
2.2).Section 3 then introduces three alternatives toMax.
Max is then shown to fare worse than thosethree with respect to well-known contrasts involv-ing relative clauses (Sec.
4).
Section 5 brieflylooks at three other constructions that pose prob-lems for the alternative metrics.2 Preliminaries2.1 Minimalist GrammarsMGs (Stabler, 1997; Stabler, 2011a) are a highlylexicalized formalism in which structures are builtvia the operations Merge and Move.
Intuitively,Merge enforces local dependencies via subcatego-rization, whereas Move establishes long-distancefiller-gap dependencies.Every lexical item comes with a non-empty listof unchecked features, and each feature has eitherpositive or negative polarity and is checked by ei-ther Merge or Move.
Suppose that I) s is a treewhose head has a positive Merge feature F+as itsfirst unchecked feature, and II) t is a tree whosehead has a matching negative Merge feature F?28as its first unchecked feature.
Then Merge checksF+and F?and combines s and t into the treel(s, t) or l(t, s), where l is a label projected by thehead of s and s is linearized to the left of t iff sconsists of exactly one node.
Move, on the otherhand, applies to a single tree s whose head h hasa positive Move feature f+as its first uncheckedfeature.
Suppose that t is a subtree of s whosehead has the matching negative Move feature f?as its first unchecked feature.
Then Move checksf+and f?and returns the tree l(t, s?
), where l isa label projected by h and s?is obtained by remov-ing t from s. Crucially, Move may apply to s iffthere is exactly one subtree like t. This restrictionis known as the Shortest Move Constraint (SMC).For example, the sentence John left involves (atleast) the following steps under a simplified Mini-malist analysis (Adger, 2003):Merge(John :: D?nom?, left :: D+V?
)= [VPleft :: V?John :: nom?]
(1)Merge(?
:: V+nom+T?, (1))= [TP?
:: nom+T?
[VPleftJohn :: nom?]
] (2)Move((2)) = [TPJohn [T??
:: T?
[VPleft ] ] ] (3)This derivation can be represented more succinctlyas the derivation tree in Fig 1, where all leaves arelabeled by lexical items while unary and binarybranching nodes are labeled Move and Merge, re-spectively.Even though MGs (with the SMC) are weaklyequivalent to MCFGs (Michaelis, 2001) and thusmildly context-sensitive in the sense of Joshi(1985), their derivation tree languages can be gen-erated by CFGs (modulo relabeling of interiornodes).
As we will see next, this makes it pos-sible to treat MG parsing as a special case of CFGparsing.2.2 Parsing Minimalist GrammarsThanks to the SMC, the mapping from deriva-tion trees to phrase structure trees is determin-istic.
Consequently, MG parsing reduces to as-signing context-free derivation trees to input sen-tences, rather than the more complex phrase struc-ture trees.
The major difference from CFGs isthat the linear order of nodes in an MG deriva-tion tree does not necessarily match the linear or-der of words in the input sentence ?
for instancebecause a moving phrase remains in its base posi-tion in the derivation tree.
But as long as one cantell for every MG operation how its output is lin-earized, these discrepancies in linear order can betaken care of in the inference rules of the parser.Stabler (2011b; 2012) shows how exactly this isdone for a parser that constructs derivation treesin a top-down fashion.
Intuitively, MG top-downparsing is CFG top-down parsing with a slightlydifferent algorithm for traversing/expanding thetree.Instead of presenting the parser?s full set of in-ference rules, we adopt KGH?s index notation toindicate how the parser constructs a given deriva-tion.
For instance, if a derivation contains the node5Merge38, this means that the parser makes a pre-diction at step 5 that Merge occurs at this posi-tion in the derivation and keeps this prediction inmemory until step 38, at which point the parserreplaces it by suitable predictions for the argu-ments for Merge, i.e.
the daughters of the Mergenode.
Similarly,22the :: N+D?28denotes thatthe parser conjectures this lexical item at step 22and finally gets to scan it in the input string at step28.In principle the parser could simply predict acomplete derivation and then scan the input stringto see if the two match.
In order to obtain an in-cremental parser, however, scanning steps have totake place as soon as possible.
The MG parser im-plements this as follows: predictions are put into apriority queue, and the prediction with the highestpriority is worked on first.
The priority of the pre-dictions corresponds to the linear order that holdsbetween the constituents that are obtained fromthem.
For example, if the parser replaces a predic-tion for a Merge node yielding l(s, t) by predic-tions psand ptthat eventually derive s and t, thenpshas higher priority than ptiff s is predicted toprecede t. Since Move only takes one argument s,replacing a Move prediction by the prediction ofs trivially involves no such priority management.However, if movement is to a position to the leftof s (as is standard for MGs), none of the lexicalitems contained within s can be scanned until theentire subtree moving out of s has been predictedand scanned.If a prediction does not have the highest prior-29TPT?VPtleft?JohnMoveMergeMergeJohn :: D?nom?left :: D+V??
:: V+nom+T?Figure 1: Minimalist phrase structure tree (left) and MG derivation tree (right) for John leftity, it remains in the queue for a few steps before itis expanded into other predictions or dischargedby scanning a word from the input string.
Thenumber of steps a prediction stays in the queueis called its tenure.
With KGH?s index notation,the tenure of each node is the difference betweenits indices.
Given a parse, its maximum tenureMax is the smallest n such that the parser storedno prediction in its queue for more than n steps.KGH demonstrate that Max can be used to gaugehow hard it is for humans to process certain struc-tures.
This amounts to equating processing dif-ficulty with memory retention requirements.
Butas we show in the remainder of this paper, Maxfaces problems with relative clause constructionsthat were not considered by KGH.3 Alternative Metrics3.1 Three New MetricsIn an attempt to home in on the shortcomings ofMax, we contrast it with a number of alternativemetrics.
Since the main advantage of Max is itssimplicity, which makes it possible to quickly de-termine the processing predictions of a given syn-tactic analysis, the metrics we consider are alsokept as simple as possible.MaxLex the maximum tenure of all leaves in thederivationBox the maximum number of nodes with tenurestrictly greater than 2BoxLex the maximum number of leaves withtenure strictly greater than 2MaxLex is simply the restriction of Max to leafnodes.
Box and BoxLex provide a measure of howmany items have to be stored in memory duringthe parse and hence incur some non-trivial amountof tenure.
The threshold is set to 2 rather than 1 toexclude lexical items that are right siblings of an-other lexical item.
In such a case, a single predic-tion is immediately followed by two consecutivescan steps, which could just as well be thoughtof as one scan step spanning two words.
Nodeswith tenure over 2 are highlighted by a box in ourderivation trees, hence the name for these two met-rics.All four measures are also divided into two sub-types depending on whether unpronounced leaves(e.g.
the empty T-head in Fig.
1) are taken into ac-count ?
this is inspired by the exclusion of un-pronounced material in the TAG-parser of Ram-bow and Joshi (1995).
When reporting the val-ues for the metrics, we thus give slashed values ofthe form m/n, where m is the value with unpro-nounced leaves and n the value without them.3.2 Methodological RemarksThe following sections investigate the predictionsof our difficulty metrics with respect to the em-bedding of sentential complements versus relativeclauses, subject gaps versus object gaps in relativeclauses, left embedding, and verb clusters.
In or-der for this comparison to be meaningful, we haveto make the same methodological assumptions asKGH.First, the difficulty metric only has to accountfor overall sentence difficulty, it does not neces-sarily correlate with difficulty at a specific word.More importantly, though, all reported processingdifficulties are assumed to be due to memory load.This is a very strong assumption.
A plethora ofalternative accounts are available in the literature.The contrast between subject gaps and object gapsalone has been explained by information-theoreticnotions such as surprisal (Hale, 2003; Levy, 2013),the active filler strategy (Frazier and D?Arcais,1989), or theta role assignment (Pritchett, 1992),to name but a few (see Lin (2006) and Wu (2009)for extensive surveys).Even those accounts that attribute processingdifficulty to memory requirements make ancillaryassumptions that are not reflected by the simplememory model entertained here.
Gibson?s Depen-dency Locality Theory (1998), for instance, cru-30cially relies on discourse reference as a means fordetermining how much of a memory burden is in-curred by each word.We take no stance as to whether these accountsare correct.
Our primary interest is the feasibilityof a memory-based evaluation metric for Stabler?stop-down parser.
Memory is more likely to playa role in the constructions we look at in the nexttwo sections than in, say, attachment ambiguitiesor local syntactic coherence effects (Tabor et al.,2004).
It may well turn out that memory is notinvolved at all, but for the purpose of comparingseveral memory-based metrics, they are the safeststarting point.4 Relative Clauses4.1 Empirical GeneralizationsTwo major properties of relative clauses are firmlyestablished in the literature (see Gibson (1998) andreferences therein).?
SC/RC < RC/SCA sentential complement containing a rela-tive clause is easier to process than a relativeclause containing a sentential complement.?
SubjRC < ObjRCA relative clause containing a subject gap iseasier to parse than a relative clause contain-ing an object gap.These generalizations were obtained via self-paced reading experiments and ERP studies withminimal pairs such as (1) and (2), respectively.
(1) a.
The fact [SCthat the employeei[RCwho the manager hired ti] stole officesupplies] worried the executive.b.
The executivei[RCwho the fact [SCthat the employee stole offices sup-plies] worried ti] hired the manager.
(2) a.
The reporteri[RCwho tiattacked thesenator] admitted the error.b.
The reporteri[RCwho the senator at-tacked ti] admitted the error.4.2 SC/RC and RC/SCWe first consider the contrast between relativeclauses embedded inside a sentential complement(SC/RC) and relative clauses containing a sen-tential complement (SC/RC).
Figures 2 and 3 onpages 5 and 6 show the augmented derivations for(1a) and (1b), respectively.
For the sake of read-ability, we omit all features in our derivation treesand instead use standard X?labels to indicate pro-jection and dashed branches for movement.Like KGH, we adopt a promotion analysis ofrelative clauses (Vergnaud, 1974; Kayne, 1994).That is to say, the head noun is selected by anempty determiner to form a DP, which starts outas an argument of the embedded verb and under-goes movement into the specifier of the relativeclause (which is treated as an NP).
The entire rel-ative clause is then selected by the determiner thatwould usually select the head noun under the tra-ditional, head-external analysis (Montague, 1970;Chomsky, 1977).1In both derivations the maximum tenure obtainsat two points in the matrix clause: I) the unpro-nounced T-head, and II) the Merge step that intro-duces the remainder of the VP.
The parser mustfirst build the entire subject before it can proceedscanning or expanding material to its right.
Con-sequently, the tenure of these nodes increases withthe size of the subject, and since both the SC/RCpattern and the RC/SC pattern necessarily involvelarge subjects, maximum tenure for both types ofsentences is predicted to be relatively high.
Theparser shows a slightly lower Max value for SC/RC than for RC/SC ?
32/32 versus 33/33.Although this shows that strictly speaking Maxis not incompatible with the generalization thatSC/RC is easier to process than RC/SC, the differ-ence is so small that even the presence of one moreword in the SC/RC sentence could tip the balancetowards RC/SC, which seems rather unlikely.The contrast emerges more clearly with theother measures.
MaxLex yields the values 32/9versus 33/17, so it fares better than Max onlyif one ignores unpronounced leaves.
This is ex-pected since one of the nodes incurring the highesttenure value is the unpronounced T-head.
The Boxvalues are 14/11 and 5/3, and those of BoxLexare 12/9 and 3/1.The box values fare better in this case becausethey are sensitive to the number of dependenciesthat cannot be discharged immediately.
The waythe MG parser traverses the tree, a sentential com-1The promotion analysis was chosen to maintain consis-tency with KGH.
But our observations hold for every anal-ysis that involves some movement dependency between thegap and the specifier of the relative clause.
This includes themore common head-external analyses mentioned above.310CP11TP33T?44VP55V?3737DP3939executive4139the4037worried385DP66NP88CP1010TP1212T?1313VP1414V?3333office supplies3533stole3414DP1515NP1717N?1818TP1919T?2020VP2121V?2323DP2424employee2624D2523hired3121DP2222manager2922the2820T3018who2715the1613T3210that118fact96the74T361C2Figure 2: Sentential complement with embedded relative clause; Max = 32/32, MaxLex = 32/9, Box= 9/6, BoxLex = 7/4plement in the subject position of a relative clausecannot be fully processed until the movement de-pendency within the relative clause has been takencare of.
So even though the sentential complementis explored first, all its predicted elements must bekept in memory.
A relative clause within the sub-ject of a sentential complement, on the other hand,poses less of a challenge because the movement ofits containing subject is so short that it only delaysthe processing of the T-head and V?.4.3 Subject Gaps and Object GapsA stronger argument against Max is furnished bythe preference for subject gaps over object gaps:maximum tenure is always the same for both con-structions.
Consider the derivations in Fig.
4 and 5on pages 7 and 8.
They have the same Max valuebecause the maximum tenure once again obtainsat the T-head of the matrix clause and the Mergenode that expands the matrix VP.
The tenure ofthese nodes is determined by the size of the sub-ject, which contains the relative clause.
But sincethe size of the subject is not affected by whether itis the subject or the object that is extracted fromthe relative clause, maximum tenure will nevervary between these two constructions.Once again the alternative metrics fare bet-ter than Max.
MaxLex evaluates to 19/7 and19/9.
As before the tenure on the T-head causesMaxLex to behave like Max unless unpronouncedwords are ignored.
If one does so, however, themaximum tenure value occurs on the relative pro-noun who instead.
Since who is the head of therelative clause, it is introduced early on duringthe structure building process, but it cannot bescanned until the parser reaches the element thatmoves into its specifier.
Objects are more deeplyembedded than subjects, and consequently it takesthe parser less time to reach the subject than theobject.
As a result, who has greater tenure if therelative clause contains an object gap instead of asubject gap.Box and BoxLex also predict the attested con-320CP11TP33T?44VP55V?3838DP4040manager4140the4138hired395DP66NP88N?99TP1010T?1111VP1212V?2222DP2323executive2523D2422worried3612DP1314NP1515CP1616TP1717T?1818VP1919VP2121office supplies3421stole3319DP2020employee3120the3018T3216that2915fact2814the2711T359who266the74T371C2Figure 3: Relative clause containing a sentential complement; Max = 33/33, MaxLex = 33/17, Box= 14/11, BoxLex = 12/9trast.
Box produces the values 5/3 and 7/5,whereas BoxLex returns 3/1 and 6/4.
Since sub-jects are introduced at a higher position than ob-jects, movement of the subject causes fewer nodesto be delayed in their processing ?
the VP has notbeen fully expanded yet, so the nodes contained byit do not need to be stored in memory because theparser hasn?t even predicted them at this point.5 Further Observations5.1 Verb Clusters in Dutch and GermanKGH show that Max correctly predicts the at-tested difficulty differences between German andDutch verb clusters (Bach et al., 1986).
Ger-man verb clusters instantiate nested dependen-cies of the form DP1DP2?
?
?
DPnVn?
?
?
V2V1.
Dutch verb clusters, on the other hand, showcrossing dependencies: DP1DP2?
?
?
DPnV1V2?
?
?
Vn.
Even though the latter not context-freeand hence computationally more complex thanthe former, they are actually easier to process.Since KGH?s account relies on the tenure of (pro-nounced) leaves, it also carries over to MaxLex.2Box and BoxLex, however, do not make thisprediction.
In both Dutch and German every Vihas to be kept in memory before it can be scanned,so that a sentence with n verbs will have n boxes.According to Box and BoxLex, there should be noprocessing difference between German and Dutch.This can be partially fixed by summing the tenureof all boxed nodes so that overall memory load isat least partially taken into account, yielding themeasures SumBox and SumBoxLex.
But eventhose still make the wrong prediction for n < 4,that is to say, they establish the desired differenceonly after a point where both cluster types are al-ready very hard to process.2Strictly speaking KGH build their argument on the tenureof T, which MaxLex must ignore for the constructions inves-tigated in this paper.
However, tenure can be measured at V1instead, in which case Dutch clusters with three or more verbshave lower MaxLex values than the corresponding Germanclusters.
Clusters consisting of only two verbs have the sameMaxLex value in both languages.
An anonymous reviewerpoints out that this is exactly the pattern found by Bach et al.
(1986).330CP11TP33T?44VP55V?2424DP2626error2826the2724admitted255DP66NP88N?99TP1010T?1111VP1212V?1818DP2020senator2220the2118attacked1912r1313reporter1513D1411T179who166the74T231C2Figure 4: Relative clause with subject gap; Max= 19/19, MaxLex= 19/7, Box= 5/3, BoxLex= 3/15.2 Left EmbeddingKGH note that if processing difficulty is deter-mined by Max, then left embedding constructionssuch as English possessor nesting should lead toa sharp increase in parsing difficulty similar tocenter-embedding, which is not the case (Resnik,1992).
(3) [[[Mike [?s uncle]] [?s cousin]] [?s room-mate]] went to the store.Box makes a similar prediction, whereas MaxLexand BoxLex do not (cf.
Tab.
1 on page 1).
Keepin mind that a left embedding construction c in-creases the tenure of the right sibling of c withevery level of embedding.
As long as c is nota lexical item, it will be ignored by MaxLexand BoxLex.
Therefore possessor-embedding ispredicted to be unproblematic, whereas a right-adjunction structure as in [VP[VP[VPleft ] quickly] yesterday ] should increase the processing load.While we are not aware of any studies on thistopic, such a split strikes us as highly unnatural.5.3 Head-Final Relative ClausesPreliminary work of ours suggests that almostnone of the metrics covered in this paper workfor languages where relative clauses precede theirhead nouns, such as Chinese, Japanese, and Ko-rean.
There is overwhelming evidence that theselanguages still show a preference for subject gapsover object gaps (Lin, 2006; Wu, 2009; Kwonet al., 2013).
The syntactic structure of relativeclauses in these languages is up to debate; but as-suming that they involve rightward movement ofthe head noun into a specifier of the relative clausefollowed by remnant leftward movement of the TPinto another specifier, most metrics derive a pref-erence for object gaps (see the last two rows inTab.
1).
Only Box shows a small advantage forsubject gaps.6 Discussion and Future WorkSeveral metrics were compared in this paper thatmeasure processing difficulty in terms of very dif-ferent parameters: I) how long an item stays inmemory (Max, MaxLex), II) how many itemsmust be stored in memory (Box, BoxLex), andIII) for what kind of material these criteria matter(?lexical, ?pronounced).A quick glance at Tab.
1 reveals that no clearwinner emerges.
Box and BoxLex fail to cap-ture the differences between Dutch and Germanverb clusters, whereas Max struggles with relativeclause constructions and left embedding.
MaxLexcaptures all these fact if only pronounced elementsare taken into account, but makes the dubiousprediction that right adjunction of a single wordshould be harder than left embedding or right ad-junction of an adjunct that consists of at least twowords.
In addition, MaxLex fails to derive a sub-ject gap preference for head-final relative clauses.340CP11TP33T?44VP55V?2424DP2626error2726the2724admitted255DP66NP88N?99TP1010T?1111VP1212V?1414DP1515reporter1715D1614attacked2212DP1313senator2013the1911T219who186the74T231C2Figure 5: Relative clause with object gap; Max = 19/19, MaxLex = 19/9, Box = 7/5, BoxLex = 6/4Phenomenon Max MaxLex Box BoxLex SumBox SumBoxLexSC/RC 32/32 32/9 9/6 7/4 142/81 91/30RC/SC 33/33 33/17 14/11 12/9 219/149 186/116subject gap RC 19/19 19/7 5/3 3/1 57/32 32/7object gap RC 19/19 19/9 7/5 6/4 78/49 59/301 possessor 7/7 7/2 2/1 1/0 14/7 7/02 possessors 11/11 11/2 3/2 1/0 27/16 11/03 possessors 15/15 15/2 4/3 1/0 46/31 15/01 right adjunct 7/7 7/3 3/2 2/1 17/10 10/32 right adjuncts 12/12 12/8 5/4 4/3 42/30 30/183 right adjuncts 15/15 15/12 7/6 6/5 58/43 43/28crossing < nesting yes yes no no partially partiallyhead-final subj RC 20/20 20/11 5/4 4/3 66/39 46/19head-final obj RC 20/20 20/10 6/4 3/1 63/38 35/10Table 1: Overview of evaluation metricsIt is very likely that a more complicated met-ric could account for all these facts.
But the ap-peal of Max and the alternatives investigated hereis their simplicity.
A simple metric is easier tostudy from a formal perspective.
In an ideal world,the metric would turn out to correlate with a basictree-geometric property of derivations so that theprocessing predictions of syntactic analyses canbe determined at a glance without simulations orlarge-scale corpus work.Two routes seems promising at this point.
Inorder to rule out that the problem isn?t with themetrics but rather the MG parser itself, the metricsshould be tested with other parsing models.
Thoseneed not even be based on MGs, since the metricsmeasure aspects of memory management, whichis an integral part of every parser.Alternatively, we may look into how the metricsare applied.
An anonymous reviewer points outthat Max derives the preference for subject gapsif derivations that tie for Max are then comparedwith respect to the second-highest tenure value,which is 7/7 for subject gaps and 10/9 for objectgaps.
While this still leaves us with cases like leftembedding where Max predicts a higher process-ing load than expected, it eliminates the problemof Max incorrectly equating two structures.AcknowledgmentsWe are extremely grateful to the three anonymousreviewers.
Their extensive comments not onlybrought about many improvements in the presen-tation and structure of the paper but also made usconsider the wider implications of the work re-ported here.35ReferencesDavid Adger.
2003.
Core Syntax: A Minimalist Ap-proach.
Oxford University Press, Oxford.Emmon Bach, Colin Brown, and William Marslen-Wilson.
1986.
Crossed and nested dependencies inGerman and Dutch: A psycholinguistic study.
Lan-guage and Cognitive Processes, 1:249?262.Noam Chomsky.
1977.
Essays on Form and Interpre-tation.
New York, North Holland.Noam Chomsky.
1995.
The Minimalist Program.
MITPress, Cambridge, Mass.Lyn Frazier and Flores D?Arcais.
1989.
Filler drivenparsing: A study of gap filling in Dutch.
Journal ofMemory and Language, 28:331?344.Edward Gibson.
1998.
Linguistic complexity: Local-ity of syntactic dependencies.
Cognition, 68:1?76.Thomas Graf.
2012.
Locality and the complexity ofminimalist derivation tree languages.
In Philippede Groot and Mark-Jan Nederhof, editors, FormalGrammar 2010/2011, volume 7395 of Lecture Notesin Computer Science, pages 208?227, Heidelberg.Springer.John T. Hale.
2003.
Grammar, Uncertainty and Sen-tence Processing.
Ph.D. thesis, John Hopkins Uni-versity.Henk Harkema.
2001.
A characterization of min-imalist languages.
In Philippe de Groote, GlynMorrill, and Christian Retor?e, editors, Logical As-pects of Computational Linguistics (LACL?01), vol-ume 2099 of Lecture Notes in Artificial Intelligence,pages 193?211.
Springer, Berlin.Aravind Joshi.
1985.
Tree-adjoining grammars: Howmuch context sensitivity is required to provide rea-sonable structural descriptions?
In David Dowty,Lauri Karttunen, and Arnold Zwicky, editors, Nat-ural Language Parsing, pages 206?250.
CambridgeUniversity Press, Cambridge.Richard S. Kayne.
1994.
The Antisymmetry of Syntax.MIT Press, Cambridge, Mass.Gregory M. Kobele, Christian Retor?e, and Sylvain Sal-vati.
2007.
An automata-theoretic approach to min-imalism.
In James Rogers and Stephan Kepser, edi-tors, Model Theoretic Syntax at 10, pages 71?80.Gregory M. Kobele, Sabrina Gerth, and John T. Hale.2012.
Memory resource allocation in top-downminimalist parsing.
In Proceedings of FormalGrammar 2012.Nayoung Kwon, Robert Kluender, Marta Kutas, andMaria Polinsky.
2013.
Subject/object processingasymmetries in korean relative clauses: Evidencefrom ERP data.
Language, 89:537?585.Roger Levy.
2013.
Memory and surprisal in humansentence comprehension.
In Roger P. G. van Gom-pel, editor, Sentence Processing, pages 78?114.
Psy-chology Press, Hove.Chien-Jer Charles Lin.
2006.
Grammar and Parsing:A Typological Investigation of Relative-Clause Pro-cessing.
Ph.D. thesis, University of Arizona.Jens Michaelis.
2001.
Transforming linear context-free rewriting systems into minimalist grammars.Lecture Notes in Artificial Intelligence, 2099:228?244.Richard Montague.
1970.
English as a formal lan-guage.
In Bruno Visentini and et al., editors, Lin-guaggi nella Societ e nella Tecnica, pages 189?224.Edizioni di Comunit, Milan.Bradley L. Pritchett.
1992.
Grammatical Competenceand Parsing Performance.
University of ChicagoPress, Chicago.Owen Rambow and Aravind Joshi.
1995.
A process-ing model for free word order languages.
TechnicalReport IRCS-95-13, University of Pennsylvania.Philip Resnik.
1992.
Left-corner parsing and psycho-logical plausibility.
In Proceedings of COLING-92,pages 191?197.Edward P. Stabler.
1997.
Derivational minimalism.
InChristian Retor?e, editor, Logical Aspects of Compu-tational Linguistics, volume 1328 of Lecture Notesin Computer Science, pages 68?95.
Springer, Berlin.Edward P. Stabler.
2011a.
Computational perspec-tives on minimalism.
In Cedric Boeckx, editor,Oxford Handbook of Linguistic Minimalism, pages617?643.
Oxford University Press, Oxford.Edward P. Stabler.
2011b.
Top-down recognizers forMCFGs and MGs.
In Proceedings of the 2011 Work-shop on Cognitive Modeling and ComputationalLinguistics.
to appear.Edward P. Stabler.
2012.
Bayesian, minimalist, incre-mental syntactic analysis.
Topics in Cognitive Sci-ence, 5:611?633.Whitney Tabor, Bruno Galantucci, and Daniel Richard-son.
2004.
Effects of merely local syntactic coher-ence on sentence processing.
Journal of Memoryand Language, 50:355?370.Jean-Roger Vergnaud.
1974.
French Relative Clauses.Ph.D.
thesis, MIT.Fuyun Wu.
2009.
Factors Affecting Relative ClauseProcessing in Mandarin.
Ph.D. thesis, University ofSouthern California.36
