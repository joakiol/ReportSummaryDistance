Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 593?601,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsNon-Parametric Bayesian Areal LinguisticsHal Daume?
IIISchool of ComputingUniversity of UtahSalt Lake City, UT 84112me@hal3.nameAbstractWe describe a statistical model over linguis-tic areas and phylogeny.
Our model recov-ers known areas and identifies a plausible hi-erarchy of areal features.
The use of areasimproves genetic reconstruction of languagesboth qualitatively and quantitatively accordingto a variety of metrics.
We model linguisticareas by a Pitman-Yor process and linguisticphylogeny by Kingman?s coalescent.1 IntroductionWhy are some languages more alike than others?This question is one of the most central issues in his-torical linguistics.
Typically, one of three answersis given (Aikhenvald and Dixon, 2001; Campbell,2006).
First, the languages may be related ?genet-ically.?
That is, they may have all derived from acommon ancestor language.
Second, the similaritiesmay be due to chance.
Some language propertiesare simply more common than others, which is of-ten attributed to be mostly due to linguistic univer-sals (Greenberg, 1963).
Third, the languages maybe related areally.
Languages that occupy the samegeographic area often exhibit similar characteristics,not due to genetic relatedness, but due to sharing.Regions (and the languages contained within them)that exhibit sharing are called linguistic areas andthe features that are shared are called areal features.Much is not understood or agreed upon in the fieldof areal linguistics.
Different linguists favor differ-ent defintions of what it means to be a linguistic area(are two languages sufficient to describe an area ordo you need three (Thomason, 2001; Katz, 1975)?
),what areal features are (is there a linear ordering of?borrowability?
(Katz, 1975; Curnow, 2001) or isthat too prescriptive?
), and what causes sharing totake place (does social status or number of speakersplay a role (Thomason, 2001)?
).In this paper, we attempt to provide a statisticalanswer to some of these questions.
In particular,we develop a Bayesian model of typology that al-lows for, but does not force, the existence of linguis-tic areas.
Our model also allows for, but does notforce, preference for some feature to be shared are-ally.
When applied to a large typological databaseof linguistic features (Haspelmath et al, 2005), wefind that it discovers linguistic areas that are welldocumented in the literature (see Campbell (2005)for an overview), and a small preference for cer-tain features to be shared areally.
This latter agrees,to a lesser degree, with some of the published hi-erarchies of borrowability (Curnow, 2001).
Finally,we show that reconstructing language family trees issignificantly aided by knowledge of areal features.We note that Warnow et al (2005) have indepen-dently proposed a model for phonological change inIndo-European (based on the Dyen dataset (Dyen etal., 1992)) that includes notions of borrowing.
Ourmodel is different in that we (a) base our model ontypological features rather than just lexical patternsand (b) we explicitly represent language areas, notjust one-time borrowing phenomena.2 BackgroundWe describe (in Section 3) a non-parametric, hier-archical Bayesian model for finding linguistic areasand areal features.
In this section, we provide nec-essary background?both linguistic and statistical?593for understanding our model.2.1 Areal LinguisticsAreal effects on linguistic typology have been stud-ied since, at least, the late 1920s by Trubetzkoy,though the idea of tracing family trees for languagesgoes back to the mid 1800s and the comparativestudy of historical linguistics dates back, perhaps toGiraldus Cambrenis in 1194 (Campbell, In press).A recent article provides a short introduction to boththe issues that surround areal linguistics, as well asan enumeration of many of the known language ar-eas (Campbell, 2005).
A fairly wide, modern treat-ment of the issues surrounding areal diffusion is alsogiven by essays in a recent book edited by Aikhen-vald and Dixon (2001).
The essays in this book pro-vide a good introduction to the issues in the field.Campbell (2006) provides a critical survey of theseand other hypotheses relating to areal linguistics.There are several issues which are basic to thestudy of areal linguistics (these are copied almostdirectly from Campbell (2006)).
Must a linguisticarea comprise more than two languages?
Must itcomprise more than one language family?
Is a sin-gle trait sufficient to define an area?
How ?nearby?must languages in an area be to one another?
Aresome feature more easily borrowed that others?Despite these formal definitional issues of whatconstitutes a language area and areal features, mosthistorical linguists seem to believe that areal effectsplay some role in the change of languages.2.1.1 Established Linguistic AreasBelow, we list some of the well-known linguisticareas; Campbell (2005) provides are more completelisting together with example areal features for theseareas.
For each area, we list associated languages:The Balkans: Albanian, Bulgarian, Greek, Mace-donian, Rumanian and Serbo-Croatian.
(Sometimes:Romani and Turkish)South Asian: Languages belonging to the Dravid-ian, Indo-Aryan, Munda, Tibeto-Burman families.Meso-America: Cuitlatec, Huave, Mayan, Mixe-Zoquean, Nahua, Otomanguean, Tarascan, Tequist-latecan, Totonacan and Xincan.North-west America: Alsea, Chimakuan, Coosan,Eyak, Haida, Kalapuyan, Lower Chinook, Salishan,Takelman, Tlingit, Tsimshian and Wakashan.The Baltic: Baltic languages, Baltic German, andFinnic languages (especially Estonian and Livo-nian).
(Sometimes many more are included, such as:Belorussian, Lavian, Lithuanian, Norwegian, OldPrussian, Polish, Romani, Russian, Ukranian.
)Ethiopia: Afar, Amharic, Anyuak, Awngi, Beja,Ge?ez, Gumuz, Janjero, Kefa, Sidamo, Somali, Ti-gre, Tigrinya and Wellamo.Needless to say, the exact definition and extent ofthe actual areas is up to significant debate.
More-over, claims have been made in favor of many lin-guistic areas not defined above.
For instance, Dixon(2001) presents arguments for several Australian lin-guistic areas and Matisoff (2001) defines a South-East Asian language area.
Finally, although ?folklore?
is in favor of identifying a linguistic area in-cluding English, French and certain Norse languages(Norwegian, Swedish, Low Dutch, High German,etc.
), there are counter-arguments to this position(Thomason, 2001) (see especially Case Study 9.8).2.1.2 Linguistic FeaturesIdentifying which linguistic features are most eas-ily shared ?areally?
is a long standing problem incontact linguistics.
Here we briefly review some ofthe major claims.
Much of this overview is adopedfrom the summary given by Curnow (2001).Haugen (1950) considers only borrowability asfar as the lexicon is concerned.
He provided evi-dence that nouns are the easiest, followed by verbs,adjectives, adverbs, prepositions, etc.
Ross (1988)corroborates Haugen?s analysis and deepens it tocover morphology, syntax and phonology.
He pro-poses the following hierarchy of borrowability (eas-iest items coming first): nouns > verbs > adjectives> syntax > non-bound function words > boundmorphemes > phonemes.
Coming from a ?con-straints?
perspective, Moravcsik (1978) suggeststhat: lexical items must be borrowed before lexi-cal properties; inflected words before bound mor-phemes; verbal items can never be borrowed; etc.Curnow (2001) argues that coming up with a rea-sonable hierarchy of borrowability is that ?we maynever be able to develop such constraints.?
Never-theless, he divides the space of borrowable featuresinto 15 categories and discusses the evidence sup-porting each of these categories, including: phonet-ics (rare), phonology (common), lexical (very com-mon), interjections and discourse markers (com-594mon), free grammatical forms (occasional), boundgrammatical forms (rare), position of morphology(rare), syntactic frames (rare), clause-internal syntax(common), between-clause syntax (occasional).2.2 Non-parametric Bayesian ModelsWe treat the problem of understanding areal linguis-tics as a statistical question, based on a database oftypological information.
Due to the issues raised inthe previous section, we do not want to commit tothe existence of a particular number of linguistic ar-eas, or particular sizes thereof.
(Indeed, we do noteven want to commit to the existence of any linguis-tic areas.)
However, we will need to ?unify?
thelanguages that fall into a linguistic area (if such athing exists) by means of some statistical param-eter.
Such problems have been studied under thename non-parametric models.
The idea behind non-parametric models is that one does not commit a pri-ori to a particularly number of parameters.
Instead,we allow the data to dictate how many parametersthere are.
In Bayesian modeling, non-parametricdistributions are typically used as priors; see Jor-dan (2005) or Ghahramani (2005) for overviews.
Inour model, we use two different non-parametric pri-ors: the Pitman-Yor process (for modeling linguisticareas) and Kingman?s coalescent (for modeling lin-guistic phylogeny), both described below.2.2.1 The Pitman-Yor ProcessOne particular example of a non-parametric prioris the Pitman-Yor process (Pitman and Yor, 1997),which can be seen as an extension to the better-known Dirichlet process (Ferguson, 1974).
ThePitman-Yor process can be understood as a particu-lar example of a Chinese Restaurant process (CRP)(Pitman, 2002).
The idea in all CRPs is that thereexists a restaurant with an infinite number of ta-bles.
Customers come into the restaurant and haveto choose a table at which to sit.The Pitman-Yor process is described by three pa-rameters: a base rate ?, a discount parameter d anda mean distribution G0.
These combine to describea process denoted by PY(?, d,G0).
The parameters?
and d must satisfy: 0 ?
d < 1 and ?
> ?d.
Inthe CRP analogy, the model works as follows.
Thefirst customer comes in and sits at any table.
AfterN customers have come in and seated themselves(at a total of K tables), the N th customer arrives.
Inthe Pitman-Yor process, the N th customer sits at anew table with probability proportional to ?
+ Kdand sits at a previously occupied table k with proba-bility proportional to #k ?
d, where #k is the num-ber of customers already seated at table k. Finally,with each table k we associate a parameter ?k, witheach ?k drawn independently from G0.
An impor-tant property of the Pitman-Yor process is that drawsfrom it are exchangable: perhaps counterintuitively,the distribution does not care about customer order.The Pitman-Yor process induces a power-law dis-tribution on the number of singleton tables (i.e., thenumber of tables that have only one customer).
Thiscan be seen by noticing two things.
In general,the number of singleton tables grows as O(?Nd).When d = 0, we obtain a Dirichlet process with thenumber of singleton tables growing as O(?
logN).2.2.2 Kingman?s CoalescentKingman?s coalescent is a standard model in pop-ulation genetics describing the common genealogy(ancestral tree) of a set of individuals (Kingman,1982b; Kingman, 1982a).
In its full form it is a dis-tribution over the genealogy of a countable set.Consider the genealogy of n individuals alive atthe present time t = 0.
We can trace their ances-try backwards in time to the distant past t = ?
?.Assume each individual has one parent (in genet-ics, haploid organisms), and therefore genealogiesof [n] = {1, .
.
.
, n} form a directed forest.
King-man?s n-coalescent is simply a distribution over ge-nealogies of n individuals.
To describe the Markovprocess in its entirety, it is sufficient to describethe jump process (i.e.
the embedded, discrete-time,Markov chain over partitions) and the distributionover coalescent times.
In the n-coalescent, everypair of lineages merges independently with rate 1,with parents chosen uniformly at random from theset of possible parents at the previous time step.The n-coalescent has some interesting statisticalproperties (Kingman, 1982b; Kingman, 1982a).
Themarginal distribution over tree topologies is uni-form and independent of the coalescent times.
Sec-ondly, it is infinitely exchangeable: given a geneal-ogy drawn from an n-coalescent, the genealogy ofany m contemporary individuals alive at time t?
0embedded within the genealogy is a draw from them-coalescent.
Thus, taking n?
?, there is a distri-595bution over genealogies of a countably infinite pop-ulation for which the marginal distribution of the ge-nealogy of any n individuals gives the n-coalescent.Kingman called this the coalescent.Teh et al (2007) recently described efficient in-ference algorithms for Kingman?s coalescent.
Theyapplied the coalescent to the problem of recoveringlinguistic phylogenies.
The application was largelysuccessful?at least in comparison to alternative al-gorithms that use the same data-.
Unfortunately,even in the results they present, one can see signif-icant areal effects.
For instance, in their Figure(3a),Romanian is very near Albanian and Bulgarian.
Thisis likely an areal effect: specifically, an effect due tothe Balkan langauge area.
We will revisit this issuein our own experiments.3 A Bayesian Model for Areal LinguisticsWe will consider a data set consisting of N lan-guages and F typological features.
We denote thevalue of feature f in language n as Xn,f .
For sim-plicity of exposition, we will assume two things: (1)there is no unobserved data and (2) all features arebinary.
In practice, for the data we use (described inSection 4), neither of these is true.
However, bothextensions are straightforward.When we construct our model, we attempt to beas neutral to the ?areal linguistics?
questions definedin Section 2.1 as possible.
We allow areas with onlytwo languages (though for brevity we do not presentthem in the results).
We allow areas with only onefamily (though, again, do not present them).
We aregenerous with our notion of locality, allowing a ra-dius of 1000 kilometers (though see Section 5.4 foran analysis of the effect of radius).1 And we allow,but do not enforce trait weights.
All of this is ac-complished through the construction of the modeland the choice of the model hyperparameters.At a high-level, our model works as follows.
Val-ues Xn,f appear for one of two reasons: they are ei-ther areally derived or genetically derived.
A latentvariable Zn,f determines this.
If it is derived areally,then the value Xn,f is drawn from a latent variable1An reader might worry about exchangeability: Our methodof making language centers and locations part of the Pitman-Yordistribution ensures this is not an issue.
An alternative wouldbe to use a location-sensitive process such as the kernel stick-breaking process (Dunson and Park, 2007), though we do notexplore that here.corresponding to the value preferences in the lan-gauge area to which language n belongs.
If it is de-rived genetically, thenXn,f is drawn from a variablecorresponding to value preferences for the geneticsubstrate to which language n belongs.
The set ofareas, and the area to which a language belongs aregiven by yet more latent variables.
It is this aspect ofthe model for which we use the Pitman-Yor process:languages are customers, areas are tables and areavalue preferences are the parameters of the tables.3.1 The formal modelWe assume that the value a feature takes for a par-ticular language (i.e., the value of Xn,f ) can be ex-plained either genetically or areally.2 We denote thisby a binary indicator variable Zn,f , where a value 1means ?areal?
and a value 0 means ?genetic.?
We as-sume that each Zn,f is drawn from a feature-specificbinomial parameter pif .
By having the parameterfeature-specific, we express the fact that some fea-tures may be more or less likely to be shared thanothers.
In other words, a high value of pif wouldmean that feature f is easily shared areally, while alow value would mean that feature f is hard to share.Each language n has a known latitude/longitude `n.We further assume that there are K linguistic ar-eas, whereK is treated non-parametrically by meansof the Pitman-Yor process.
Note that in our context,a linguistic area may contain only one language,which would technically not be allowed accordingto the linguistic definition.
When a language belongsto a singleton area, we interpret this to mean that itdoes not belong to any language area.Each language area k (including the singleton ar-eas) has a set of F associated parameters ?k,f , where?k,f is the probability that feature f is ?on?
in area k.It also has a ?central location?
given by a longitudeand latitude denoted ck.
We only allow languagesto belong to areas that fall within a given radius Rof them (distances computed according to geodesicdistance).
This accounts for the ?geographical?
con-straints on language areas.
We denote the area towhich language n belongs as an.We assume that each language belongs to a ?fam-ily tree.?
We denote the parent of language n in the2As mentioned in the introduction, (at least) one more optionis possible: chance.
We treat ?chance?
as noise and model it inthe data generation process, not as an alternative ?source.
?596Xn,f ?
{Bin(?pn,f ) if Zn,f = 0Bin(?an,f ) if Zn,f = 1 feature values are derived genetically or areallyZn,f ?
Bin(pif ) feature source is a biased coin, parameterized per feature`n ?
Ball(can , R) language position is uniform within a ball around area center, radius Rpif ?
Bet(1, 1) bias for a feature being genetic/areal is uniform(p, ?)
?
Coalescent(pi0,m0) language hierarchy and genetic traits are drawn from a Coalescent(a, ?
?, c?)
?
PY(?0, d0,Bet(1, 1)?
Uni) area features are drawn Beta and centers Uniformly across the globeFigure 1: Full hierarchical Areal model; see Section 3.1 for a complete description.family tree by pn.
We associate with each node i inthe family tree and each feature f a parameter ?i,f .As in the areal case, ?i,f is the probability that fea-ture f is on for languages that descend from node iin the family tree.
We model genetic trees by King-man?s coalescent with binomial mutation.Finally, we put non-informative priors on all thehyperparameters.
Written hierarchically, our modelhas the following shown in Figure 1.
There, by(p, ?)
?
Coalescent(pi0,m0), we mean that the treeand parameters are given by a coalescent.3.2 InferenceInference in our model is mostly by Gibbs sam-pling.
Most of the distributions used are conju-gate, so Gibbs sampling can be implemented effi-ciently.
The only exceptions are: (1) the coales-cent for which we use the GreedyRate1 algorithmdescribed by Teh et al (2007); (2) the area centers c,for which we using a Metropolis-Hastings step.
Ourproposal distribution is a Gaussian centered at theprevious center, with standard deviation of 5.
Ex-perimentally, this resulted in an acceptance rate ofabout 50%.In our implementation, we analytically integrateout pi and ?
and sample only over Z, the coalescenttree, and the area assignments.
In some of our ex-periments, we treat the family tree as given.
In thiscase, we also analytically integrate out the ?
param-eters and sample only over Z and area assignments.4 Typological DataThe database on which we perform our analysis isthe World Atlas of Language Structures (henceforth,WALS) (Haspelmath et al, 2005).
The databasecontains information about 2150 languages (sam-pled from across the world).
There are 139 typologi-cal features in this database.
The database is sparse:only 16% of the possible language/feature pairs areknown.
We use the version extracted and prepro-cessed by Daume?
III and Campbell (2007).In WALS, languages a grouped into 38 languagefamilies (including Indo-European, Afro-Asiatic,Austronesian, Niger-Congo, etc.).
Each of these lan-guage families is grouped into a number of languagegeni.
The Indo-European family includes ten geni,including: Germanic, Romance, Indic and Slavic.The Austronesian family includes seventeen geni,including: Borneo, Oceanic, Palauan and Sundic.Overall, there are 275 geni represented in WALS.We further preprocess the data as follows.
Forthe Indo-European subset (hence-forth, ?IE?
), we re-move all languages with ?
10 known features andthen remove all features that appear in at most 1/4of the languages.
This leads to 73 languages and87 features.
For the whole-world subset, we removelanguages with ?
25 known features and then fea-tures that appear in at most 1/10 of the languages.This leads to 349 languages and 129 features.5 Experiments5.1 Identifying Language AreasOur first experiment is aimed at discovering lan-guage areas.
We first focus on the IE family, andthen extend the analysis to all languages.
In bothcases, we use a known family tree (for the IE ex-periment, we use a tree given by the language genusstructure; for the whole-world experiment, we use atree given by the language family structure).
We runeach experiment with five random restarts and 2000iterations.
We select the MAP configuration fromthe combination of these runs.In the IE experiment, the model identified theareas shown in Figure 5.1.
The best area identi-fied by our model is the second one listed, whichclearly correlates highly with the Balkans.
Thereare two areas identified by our model (the first andlast) that include only Indic and Iranian languages.While we are not aware of previous studies of theseas linguistic areas, they are not implausible given597(Indic) Bhojpuri, Darai, Gujarati, Hindi, Kalami, Kashmiri,Kumauni, Nepali, Panjabi, Shekhawati, Sindhi (Iranian) Or-muri, Pashto(Albanian) Albanian (Greek) Greek (Modern) (Indic) Romani(Kalderash) (Romance) Romanian, Romansch (Scharans), Ro-mansch (Sursilvan), Sardinian (Slavic) Bulgarian, Macedonian,Serbian-Croatian, Slovak, Slovene, Sorbian(Baltic) Latvian, Lithuanian (Germanic) Danish, Swedish(Slavic) Polish, Russian(Celtic) Irish (Germanic) English, German, Norwegian (Ro-mance) French(Indic) Prasuni, Urdu (Iranian) Persian, TajikPlus 46 non-areal languagesFigure 2: IE areas identified.
Areas that consist of justone genus are not listed, nor are areas with two languages.
(Mayan) Huastec, Jakaltek, Mam, Tzutujil (Mixe-Zoque)Zoque (Copainala?)
(Oto-Manguean) Mixtec (Chalcatongo),Otom??
(Mezquital) (Uto-Aztecan) Nahualtl (Tetelcingo), Pipil(Baltic) Latvian, Lithuanian (Finnic) Estonian, Finnish(Slavic) Polish, Russian, Ukranian(Austro-Asiatic) Khasi (Dravidian) Telugu (IE) Bengali(Sino-Tibetan) Bawm, Garo, Newari (Kathmandu)Figure 3: A small subset of the world areas identified.the history of the region.
The fourth area identi-fied by our model corresponds roughly to the de-bated ?English?
area.
Our area includes the req-uisite French/English/German/Norwegian group, aswell as the somewhat surprising Irish.
However, inaddition to being intuitively plausible, it is not hardto find evidence in the literature for the contact re-lationship between English and Irish (Sommerfelt,1960).In the whole-world experiment, the model identi-fied too many linguistic areas to fit (39 in total thatcontained at least two languages, and contained atleast two language families).
In Figure 5.1, we de-pict the areas found by our model that best corre-spond to the areas described in Section 2.1.1.
Weacknowledge that this gives a warped sense of thequality of our model.
Nevertheless, our model isable to identify large parts of the the Meso-Americanarea, the Baltic area and the South Asian area.
(Italso finds the Balkans, but since these languagesare all IE, we do not consider it a linguistic area inthis evaluation.)
While our model does find areasthat match Meso-American and North-west Ameri-can areas, neither is represented in its entirety (ac-cording to the definition of these areas given in Sec-Model Rand F-Sc Edit NVIK-means 0.9149 0.0735 0.1856 0.5889Pitman-Yor 0.9637 0.1871 0.6364 0.7998Areal model 0.9825 0.2637 0.8295 0.9090Table 1: Area identification scores for two baseline algo-rithms (K-means and Pitman-Yor clustering) that do notuse hierarchical structure, and for the Areal model wehave presented.
Higher is better and all differences arestatistically significant at the 95% level.tion 2.1.1).Despite the difficulty humans have in assigninglinguistic areas, In Table 1, we explicitly comparethe quality of the areal clusters found on the IE sub-set.
We compare against the most inclusive areallists from Section 2.1.1 for IE: the Balkans and theBaltic.
When there is overlap (eg., Romani appearsin both lists), we assigned it to the Balkans.We compare our model with a flat Pitman-Yormodel that does not use the hierarchy.
We alsocompare to a baseline K-means algorithm.
For K-means, we ran with K ?
{5, 10, 15, .
.
.
, 80, 85}and chose the value of K for each metric that didbest (giving an unfair advantage).
Clustering per-formance is measured on the Indo-European taskaccording to the Rand Index, F-score, NormalizedEdit Score (Pantel, 2003) and Normalized Variationof Information (Meila, 2003).
In these results, wesee that the Pitman-Yor process model dominates theK-means model and the Areal model dominates thePitman-Yor model.5.2 Identifying Areal FeaturesOur second experiment is an analysis of the featuresthat tend to be shared areally (as opposed to genet-ically).
For this experiment, we make use of thewhole-world version of the data, again with knownlanguage family structure.
We initialize a Gibbssampler from the MAP configuration found in Sec-tion 5.1.
We run the sampler for 1000 iterations andtake samples every ten steps.From one particular sample, we can estimate aposterior distribution over each pif .
Due to con-jugacy, we obtain a posterior distribution of pif ?Bet(1 +?n Zn,f , 1 +?n[1?Zn,f ]).
The 1s comefrom the prior.
From this Beta distribution, we canask the question: what is the probability that a valueof pif drawn from this distribution will have value< 0.5?
If this value is high, then the feature is likely598p(gen) #f Feature Category.00 1 Tea.73 19 Phonology.73 9 Lexicon.74 4 Nominal Categories / Numerals.79 5 Simple Clauses / Predication.80 5 Verbal Categories / Tense and Aspect.87 8 Nominal Syntax.87 8 Simple Clauses / Simple Clauses.91 12 Nominal Categories / Articles and Pronouns.94 17 Word Order.99 10 Morphology.99 6 Simple Clauses / Valence and Voice.99 7 Complex Sentences.99 7 Nominal Categories / Gender and Number.99 5 Simple Clauses / Negation and Questions1.0 1 Other / Clicks1.0 2 Verbal Categories / Suppletion1.0 9 Verbal Categories / Modality1.0 4 Nominal Categories / CaseTable 2: Average probability of genetic for each featurecategory and the number of features in that category.to be a ?genetic feature?
; if it is low, then the featureis likely to be an ?areal feature.?
We average theseprobabilities across all 100 samples.The features that are most likely to be areal ac-cording to our model are summaries in Table 2.
Inthis table, we list the categories to which each fea-ture belongs, together with the number of features inthat category, and the average probability that a fea-ture in that category is genetically transmitted.
Ap-parently, the vast majority of features are not areal.We can treat the results presented in Table 2 as ahierarchy of borrowability.
In doing so, we see thatour hierarchy agrees to a large degree with the hier-archies summarized in Section 2.1.2.
Indeed, (asidefrom ?Tea?, which we will ignore) the two mosteasily shared categories according to our model arephonology and the lexicon; this is in total agreementwith the agreed state of affairs in linguistics.Lower in our list, we see that noun-related cat-egories tend to precede their verb-related counter-parts (nominal categories before verbal categores,nominal syntax before complex sentences).
Accord-ing to Curnow (2001), the most difficult features toborrow are phonetics (for which we have no data),bound grammatical forms (which appear low on ourlist), morphology (which is 99% genetic, accordingto our model) and syntactic frames (which wouldroughly correspond to ?complex sentences?, anotherIndo-EuropeanModel Accuracy Log ProbBaseline 0.635 (?0.007) ?0.583 (?0.008)Areal model 0.689 (?0.010) ?0.526 (?0.027)WorldModel Accuracy Log ProbBaseline 0.628 (?0.001) ?0.654 (?0.003)Areal model 0.635 (?0.002) ?0.565 (?0.011)Table 3: Prediction accuracies and log probabilities forIE (top) and the world (bottom).item which is 99% genetic in our model).5.3 Genetic ReconstructionIn this section, we investigate whether the use ofareal knowledge can improve the automatic recon-struction of language family trees.
We use King-man?s coalescent (see Section 2.2.2) as a probabilis-tic model of trees, endowed with a binomial muta-tion process on the language features.Our baseline model is to run the vanilla coalescenton the WALS data, effective reproducing the resultspresented by Teh et al (2007).
This method was al-ready shown to outperform competing hierarchicalclustering algorithms such as average-link agglom-erative clustering (see, eg., Duda and Hart (1973))and the Bayesian Hierarchical Clustering algorithm(Heller and Ghahramani, 2005).We run the same experiment both on the IE sub-set of data and on the whole-world subset.
We eval-uate the results qualitatively, by observing the treesfound (on the IE subset) and quantitatively (below).For the qualitative analysis, we show the subset ofIE that does not contain Indic languages or Iranianlanguages (just to keep the figures small).
The treederived from the original data is on the left in Fig-ure 4, below:The tree based on areal information is on the right inFigure 4, below.
As we can see, the use of areal in-formation qualitatively improves the structure of thetree.
Where the original tree had a number of errorswith respect to Romance and Germanic languages,these are sorted out in the areally-aware tree.
More-over, Greek now appears in a more appropriate partof the tree and English appears on a branch that isfurther out from the Norse languages.We perform two varieties of quantitative analysis.In the first, we attempt to predict unknown featurevalues.
In particular, we hide an addition 10% ofthe feature values in the WALS data and fit a model599[Germanic] Afrikaans[Celtic  ]Cornish[Albanian] Albanian[Celtic  ]Breton[Celtic  ]Welsh[Celtic  ]Gaelic (Scots)[Celtic  ]Irish[Armenian] Armenian(Eastern)[Armenian] Armenian(Western)[Slavic] Belorussian[Germanic] Faroese[Slavic] Sorbian[Slavic] Bulgarian[Slavic] Slovak[Slavic] Macedonian[Greek] Greek (Modern)[Slavic] Czech[Celtic  ]Irish(Donegal)[Baltic  ]Lithuanian[Slavic] Russian[Slavic] Ukrainian[Slavic] Serbian?Croatian[Slavic] Slovene[Baltic  ]Latvian[Slavic] Polish[Romance] Catalan[Romance] Italian[Romance] Spanish[Romance] French[Romance] Portuguese[Romance] Romansch(Sursilvan)[Romance] Sardinian[Germanic] Danish[Romance] Romansch(Scharans)[Germanic] Swedish[Germanic] Norwegian[Germanic] English[Germanic] Icelandic[Romance] Romanian[Germanic] Dutch[Germanic] Frisian[Germanic] German[Germanic] Afrikaans[Celtic  ] Cornish[Albanian] Albanian[Celtic  ] Breton[Celtic  ] Welsh[Celtic  ] Gaelic (Scots)[Celtic  ] Irish[Armenian] Armenian (Eastern)[Armenian] Armenian (Western)[Slavic  ] Belorussian[Germanic] Faroese[Slavic  ] Sorbian[Slavic  ] Bulgarian[Slavic  ] Slovak[Slavic  ] Macedonian[Greek   ] Greek (Modern)[Slavic  ] Czech[Celtic  ] Irish(Donegal)[Baltic  ] Lithuanian[Slavic  ] Russian[Slavic  ] Ukrainian[Slavic  ] Serbian?Croatian[Slavic  ] Slovene[Baltic  ] Latvian[Slavic  ] Polish[Romance ] Catalan[Romance ] Italian[Romance ] Spanish[Romance ] French[Romance ] Portuguese[Romance ] Romansch (Sursilvan)[Romance ] Sardinian[Germanic] Danish[Romance ] Romansch (Scharans)[Germanic] Swedish[Germanic] Norwegian[Germanic] English[Germanic] Icelandic[Romance ] Romanian[Germanic] Dutch[Germanic] Frisian[Germanic] German[Germanic] Afrikaans[Celtic  ]Cornish[Celtic  ]Welsh[Celtic  ]Breton[Celtic  ]Gaelic (Scots)[Romance] Romansch(Scharans)[Celtic  ]Irish[Armenian] Armenian(Eastern)[Armenian] Armenian(Western)[Slavic] Belorussian[Germanic] Faroese[Slavic] Czech[Celtic  ]Irish(Donegal)[Slavic] Russian[Slavic] Ukrainian[Slavic] Serbian?Croatian[Slavic] Slovene[Baltic  ]Lithuanian[Baltic  ]Latvian[Slavic] Polish[Slavic] Bulgarian[Slavic] Slovak[Slavic] Sorbian[Greek] Greek (Modern)[Slavic] Macedonian[Albanian] Albanian[Romance] French[Romance] Portuguese[Romance] Romansch(Sursilvan)[Romance] Sardinian[Romance] Catalan[Romance] Italian[Romance] Spanish[Romance] Romanian[Germanic] Danish[Germanic] Norwegian[Germanic] Swedish[Germanic] Dutch[Germanic] Frisian[Germanic] German[Germanic] Icelandic[Germanic] EnglishFigure 4: Genetic trees of IE languages.
(Left) with no areal knowledge; (Right) with areal model.Indo-European versus GenusModel Purity Subtree LOO AccBaseline 0.6078 0.5065 0.3218Areal model 0.6494 0.5455 0.2528World versus GenusModel Purity Subtree LOO AccBaseline 0.3599 0.2253 0.7747Areal model 0.4001 0.2450 0.7982World versus FamilyModel Purity Subtree LOO AccBaseline 0.4163 0.3280 0.4842Areal model 0.5143 0.3318 0.5198Table 4: Scores for IE as compared against genus (top);for world against genus (mid) and against family (low).to the remaining 90%.
We then use that model topredict the hidden 10%.
The baseline model is tomake predictions according to the family tree.
Theaugmented model is to make predictions accordingto the family tree for those features identified as ge-netic and according to the linguistic area for thosefeatures identified as areal.
For both settings, wecompute both the absolute accuracy as well as thelog probability of the hidden data under the model(the latter is less noisy).
We repeat this experiment10 times with a different random 10% hidden.
Theresults are shown in Table 3, below.
The differencesare not large, but are outside one standard deviation.For the second quantitative analysis, we usepresent purity scores (Heller and Ghahramani,2005), subtree scores (the number of interior nodeswith pure leaf labels, normalized) and leave-one-outlog accuracies (all scores are between 0 and 1, andhigher scores are better).
These scores are computedagainst both language family and language genus asthe ?classes.?
The results are in Table 4, below.
Aswe can see, the results are generally in favor of theAreal model (LOO Acc on IE versus Genus non-withstanding), depending on the evaluation metric.Radius Purity Subtree LOO Acc125 0.6237 0.4855 0.2013250 0.6457 0.5325 0.2299500 0.6483 0.5455 0.24131000 0.6494 0.5455 0.25282000 0.6464 0.4935 0.32184000 0.6342 0.4156 0.4138Table 5: Scores for IE vs genus at varying radii.5.4 Effect of RadiusFinally, we evaluate the effect of the radius hyper-parameter on performance.
Table 5 shows perfor-mance for models built with varying radii.
As canbe seen by purity and subtree scores, there is a?sweet spot?
around 500 to 1000 kilometers wherethe model seems optimal.
LOO (strangely) seemsto continue to improve as we allow areas to growarbitrarily large.
This is perhaps overfitting.
Never-theless, performance is robust for a range of radii.6 DiscussionWe presented a model that is able to recover well-known linguistic areas.
Using this areas, we haveshown improvement in the ability to recover phylo-genetic trees of languages.
It is important to notethat despite our successes, there is much at ourmodel does not account for: borrowing is known tobe assymetric; contact is temporal; borrowing mustobey univeral implications.
Despite the failure ofour model to account for these issues, however, itappears largely successful.
Moreover, like any ?datamining?
expedition, our model suggests new lin-guistic areas (particularly in the ?whole world?
ex-periments) that deserve consideration.AcknowledgmentsDeep thanks to Lyle Campbell, Yee Whye Teh andEric Xing for discussions; comments from the threeanonymous reviewers were very helpful.
This workwas partially supported by NSF grant IIS0712764.600ReferencesAlexandra Aikhenvald and R.M.W.
Dixon, editors.
2001.Areal diffusion and genetic inheritance: problems incomparative linguistics.
Oxford University Press.Lyle Campbell.
2005.
Areal linguistics.
In Keith Brown,editor, Encyclopedia of Language and Linguistics.
El-sevier, 2 edition.Lyle Campbell.
2006.
Areal linguistics: the problemto the answer.
In April McMahon, Nigel Vincent, andYaron Matras, editors, Language contact and areal lin-guistics.Lyle Campbell.
In press.
Why Sir William Jonesgot it all wrong, or Jones?
role in how to estab-lish language families.
In Joseba Lakarra, editor,Festschrift/Memorial volume for Larry Trask.Timothy Curnow.
2001.
What language features can be?borrowed??
In Aikhenvald and Dixon, editors, Arealdiffusion and genetic inheritance: problems in com-parative linguistics, pages 412?436.
Oxford Univer-sity Press.Hal Daume?
III and Lyle Campbell.
2007.
A Bayesianmodel for discovering typological implications.
InProceedings of the Conference of the Association forComputational Linguistics (ACL).R.M.W.
Dixon.
2001.
The Australian linguistic area.In Aikhenvald and Dixon, editors, Areal diffusion andgenetic inheritance: problems in comparative linguis-tics, pages 64?104.
Oxford University Press.R.
O. Duda and P. E. Hart.
1973.
Pattern ClassificationAnd Scene Analysis.
Wiley and Sons, New York.David Dunson and Ju-Hyun Park.
2007.
Kernel stickbreaking processes.
Biometrika, 95:307?323.Isidore Dyen, Joseph Kurskal, and Paul Black.
1992.
AnIndoeuropean classification: A lexicostatistical experi-ment.
Transactions of the American Philosophical So-ciety, 82(5).
American Philosophical Society.Thomas S. Ferguson.
1974.
Prior distributions on spacesof probability measures.
The Annals of Statistics,2(4):615?629, July.Zoubin Ghahramani.
2005.
Nonparametric Bayesianmethods.
Tutorial presented at UAI conference.Joseph Greenberg, editor.
1963.
Universals of Lan-guages.
MIT Press.Martin Haspelmath, Matthew Dryer, David Gil, andBernard Comrie, editors.
2005.
The World Atlas ofLanguage Structures.
Oxford University Press.E.
Haugen.
1950.
The analysis of linguistic borrowing.Language, 26:210?231.Katherine Heller and Zoubin Ghahramani.
2005.Bayesian hierarchical clustering.
In Proceedings ofthe International Conference on Machine Learning(ICML), volume 22.Michael I. Jordan.
2005.
Dirichlet processes, Chineserestaurant processes and all that.
Tutorial presented atNIPS conference.Harmut Katz.
1975.
Generative Phonologie und phonol-ogische Sprachbu?nde des Ostjakischen un Samojedis-chen.
Wilhelm Fink.J.
F. C. Kingman.
1982a.
The coalescent.
StochasticProcesses and their Applications, 13:235?248.J.
F. C. Kingman.
1982b.
On the genealogy of largepopulations.
Journal of Applied Probability, 19:27?43.
Essays in Statistical Science.James Matisoff.
2001.
Genetic versus contact relation-ship: prosodic diffusibility in South-East Asian lan-guages.
In Aikhenvald and Dixon, editors, Areal diffu-sion and genetic inheritance: problems in comparativelinguistics, pages 291?327.
Oxford University Press.Marina Meila.
2003.
Comparing clusterings.
In Pro-ceedings of the Conference on Computational Learn-ing Theory (COLT).E.
Moravcsik.
1978.
Language contact.
In J.H.
Green-berg, C. Ferguson, and E. Moravcsik, editors, Univer-sals of Human Language, volume 1; Method and The-ory, pages 3?123.
Stanford University Press.Patrick Pantel.
2003.
Clustering by Committee.
Ph.D.thesis, University of Alberta.J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
Annals of Probability, 25:855?900.Jim Pitman.
2002.
Combinatorial stochastic processes.Technical Report 621, University of California atBerkeley.
Lecture notes for St. Flour Summer School.M.D.
Ross.
1988.
Proto Oceanic and the Austronesianlanguages of western melanesia.
Canberra: PacificLinguitics, Australian National University.Alf Sommerfelt.
1960.
External versus internal factorsin the development of language.
Norsk Tidsskrift forSprogvidenskap, 19:296?315.Yee Whye Teh, Hal Daume?
III, and Daniel Roy.
2007.Bayesian agglomerative clustering with coalescents.In Advances in Neural Information Processing Sys-tems (NIPS).Sarah Thomason.
2001.
Language contact: an introduc-tion.
Edinburgh University Press.T.
Warnow, S.N.
Evans, D. Ringe, and L. Nakhleh.
2005.A stochastic model of language evolution that incor-porates homoplasy and borrowing.
In PhylogeneticMethods and the Prehistory of Language.
CambridgeUniversity Press.
Invited paper.601
