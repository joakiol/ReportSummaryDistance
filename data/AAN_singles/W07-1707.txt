Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 51?58,Prague, June 2007. c?2007 Association for Computational LinguisticsUnsupervised Methods of Topical Text Segmentation for PolishDominik FlejterUniversity of Economicsal.
Niepodleg?os?ci 10Poznan?, PolandD.Flejter@kie.ae.poznan.plKarol WielochUniversity of Economicsal.
Niepodleg?os?ci 10Poznan?, PolandK.Wieloch@kie.ae.poznan.plWitold AbramowiczUniversity of Economicsal.
Niepodleg?os?ci 10Poznan?, PolandW.Abramowicz@kie.ae.poznan.plAbstractThis paper describes a study on performanceof existing unsupervised algorithms of textdocuments topical segmentation when ap-plied to Polish plain text documents.
Forperformance measurement five existing top-ical segmentation algorithms were selected,three different Polish test collections werecreated and seven approaches to text pre-processing were implemented.
Based onquantitative results (Pk and WindowDiffmetrics) use of specific algorithm was rec-ommended and impact of pre-processingstrategies was assessed.
Thanks to use ofstandardized metrics and application of pre-viously described methodology for test col-lection development, comparative results forPolish and English were also obtained.1 IntroductionRapid development of Internet-based informationservices is marked by a proliferation of informationavailable on-line.
Even if the Web shifts towardsmultimedia content and structured documents, con-tents of the Web sources remains predominantly tex-tual and poorly structured; an important fraction ofthis flood of plain text documents consists in multi-topical documents.
This abundance of complex butplain text or just visually structured documents (asis in case of most HTML files) creates a strong needfor intelligent text processing including robust andefficient information extraction and retrieval.One way of increasing efficiency of typical textprocessing tasks consists in processing separatelytext segments instead of whole documents.
Whiledifferent text segmentation strategies can be ap-plied including splitting text into segments of equallength, using moving windows of constant size ordiscourse units (Reynar, 1998), division into topicalsegment is intuitively more justified.
This approachis applicable in several IR and NLP areas includingdocument indexing, automatic summarization andquestion answering (Choi, 2002).For Information Extraction domain, two main ap-plication of topical document segmentation are re-lated to documents pre-processing and supportingsome basic tasks widely used by IE tools.
Topi-cal segmentation applied to individual documents aswell as documents streams (e.g.
dialogues of radiobroadcast transcripts) is an initial step for further IEprocessing (Manning, 1998); when combined withsegments labelling, classification or clustering (Al-lan et al, 1998) it allows to pre-select ranges of textto be mined for mentions of events, entities and re-lations relevant to users needs and available IE re-sources.
This limits significantly size of text to beprocessed by IE methods (Hearst and Plaunt, 1993)and thus influences significantly overall IE perfor-mance.On the other hand, many basic tasks required byIE domain (both related to IE resources creationand document (pre-)processing) make use of sec-tions rather than whole documents.
In these tasksincluding language modelling (esp.
gathering ofco-occurrences statistics for trigger-based languagemodels), anaphora resolution, word sense disam-biguation and coreference detection, definition ofproper context is crucial.
To this extend entities51discovered by topical segmentation are more reli-able than document, paragraph or sentence contextsas they help avoid usage of unrelated parts of doc-uments as well as minimize sparse data problems(Choi, 2002).1.1 Problem StatementIn this study we adopted definition proposed in (Fle-jter, 2006) stating that ?linear topical segmentationof text consists in extracting coherent blocks of textsuch that: 1) any block focuses on exactly one topic2) any pair of consecutive blocks have different maintopics.?
We also accepted that two segments shouldbe defined as having different or same topics when-ever they are judged so by people.Depending on the used document collection anduser needs, text segmentation objective is to find top-ical segments in individual multi-topical text docu-ments or to discover boundaries between consecu-tive stories or news items (e.g.
in radio broadcasttranscriptions or text news streams).Approaches to the problem of text segmentationcan be divided according to a number of dimensions(Flejter, 2006) including cognitive approach (in op-timistic approach text structure intended by author iscognitively accessible, in pessimistic approach it isnot), hierarchical or linear segmentation (do we seg-ment text into some levels of embedded segments?
),completeness of segmentation (does the segmenta-tion need to cover the whole text?
), disjointness ofsegments (can two segments have any common textranges?
), fuzziness of segments boundaries (howfuzzy is the actual boundary location?
), global or lo-cal view of topics (is there any global, document-independent list of topics?
).In this study we investigate linear, complete, dis-joint segments with binary boundaries and localview of topics.
Selected algorithms focus on textsegmentation without considering labelling or clus-tering of discovered segments.1.2 Our ContributionsThe contributions of our research described in thispaper are twofold.
Firstly, we developed three Pol-ish test collections for text segmentation task (see:Section 3.1).
Secondly, we performed an extensivestudy of performance of most popular segmentationalgorithms (see: Section 3.2) and pre-processingstrategies (see: Section 3.3) when applied to Pol-ish documents; a total of 42 scenarios including dif-ferent algorithms, pre-processing strategies and testcollections were evaluated (see: Section 4).2 Approaches to Topical SegmentationAs in case of most NLP tasks, a number of differ-ent linguistic theories influenced topic segmentationresulting in a variety of approaches applied.
Thissection gives a short presentation of major theoriesunderlying topical segmentation and an overview ofmost popular segmentation algorithms with empha-sis on those evaluated in our experiment.2.1 Theoretical FoundationsOut of linguistic approaches cohesion theory of Hal-liday and Hassan had the strongest impact on top-ical text segmentation.
It analyzes several mecha-nisms of documents internal cohesion including ref-erences, substitution, ellipsis, conjunction (logicalrelations) and lexical cohesion (reuse of the samewords to address the same object or objects of thesame class as well as use of terms which are moregeneral or semantically related in systematic or non-systematic way).
Other relevant linguistic theoriesinclude Grosz and Sinder?s discourse segmentationtheory, Rhetorical Structure Theory and taxonomyof text structures proposed Skorochod?ko (Reynar,1998).Out of empirical statistical rules some authorsmake use of heuristics resulting form Heaps?
law fornew-words-based topical segment boundary detec-tion.
However, the most important theoretical foun-dations in quantitative methods are related to strongprobabilistic frameworks including Hidden MarkovModels (Mulbregt et al, 1998) and Maximal En-tropy Theory (Beeferman et al, 1997).2.2 Basic MethodsThe most simple but also the most frequently usedmethods of topical text segmentation do not re-quire training (thus they are domain-independent)nor make use of any complex linguistic resources orutilities.
Apart from methods based on new vocab-ulary analysis, this category of algorithms applieswidely the simplest form of lexical cohesion i.e.
re-iterations of the same word.52The first classical text segmentation algorithmof this type is TextTiling described in (Hearst andPlaunt, 1993).
Its analysis unit consists of pseudo-sentences corresponding to series of consecutivewords (typically 20 words).
After the whole textis divided into pseudo-sentences, a window of 12pseudo-sentences is slid over the text (with onepseudo-sentence step).
At any position the windowis decomposed into two six-pseudo-sentences blocksand their similarity is calculated by means of cosinemeasure.
Measurements for all consecutive windowpositions (understood as positions of centre of thewindow) form lexical cohesion curve local minimaof which correspond to segments boundaries.
Theoriginal algorithm was further enhanced in severalways including use of words similarity measurementbased on co-occurrences (Kaufmann, 1999).Another group of basic algorithms makes use oftechnique of DotPlotting, originally proposed byRaynar in (Reynar, 1994).
In this approach 2D chartis used for lexical cohesion analysis with both axescorresponding to positions (in words) in the text; onthe chart points are drawn at coordinates (x, y) and(y, x) iff words at positions x and y are equal.
In thissettings coherent text segments correspond visuallyto squares with high density of points.
DotPlottingimage is than segmented using one of two strate-gies: minimization of points density at the bound-aries (minimization of external incoherence) or max-imization of density of segments (maximization ofinternal coherence) (Reynar, 1998).
The originalDotPlotting algorithm requires to explicitly provideexpected number of segments as input.Improved version of DotPlotting algorithm calledC99 (Choi, 2000) uses DotPlotting chart for vi-sualization of similarity measurements at consecu-tive point of the text (thus resulting in point withdifferent levels of intensity) instead of words co-occurences.
Afterwards, mask-based ranking tech-nique is used for image enhancement.
For ac-tual segmentation, dynamic programming techniquesimilar to DotPlotting maximization algorithm isused; an optional automatic termination strategy isalso implemented thus allowing the algorithm to as-sess number of boundaries.
In further work of thesame and other authors several enhancements of C99algorithm were proposed.2.3 Methods Requiring External ResourcesStill not requiring training and domain independent,some methods make use of linguistic resources moresophisticated than stop-list.
Two classes of such so-lution described in existing work are solutions us-ing lexical chains (Morris and Hirst, 1991; Min-Yen Kan, 1998) (which require to use some the-saurus) and based on spreading activation (Kozima,1993) (which depend on weights-based semanticnetwork constructed from thesaurus).
In both casesthe effort put in algorithm enactment is quite high;however in principle no additional resources need tobe developed for new texts (even from different do-mains).2.4 Methods Requiring TrainingLast group of methods includes supervised meth-ods with generally strong mathematical foundations.They perform very well; however they require train-ing that possibly needs to be repeated when newdomain needs to be addressed.
The methods inthis group use probabilistic frameworks includingmaximal entropy (Beeferman et al, 1997), HiddenMarkov Models (Mulbregt et al, 1998) and Proba-bilistic Latent Semantic Analysis (Blei and Moreno,2001).3 Experimental SetupSegmentation algorithms performance was evalu-ated for 42 scenarios corresponding to different al-gorithms, pre-processing strategies and test collec-tions.
For quantitative analysis and comparabilitywith previous and future research results two stan-dard segmentation metrics were applied in all sce-narios.3.1 Test CollectionsFor performance measurement three test collectionscorresponding to different types of segmentationtasks were developed: artificial documents collec-tion (AC), stream collection (SC) and individualdocuments collection (DC).
AC and SC were con-structed based on 936 issues of EuroPAP (Euro-pean information service of Polish Press Agency)plain-text e-mail newsletter (EuroPAP, 2005) col-lected from November 2001 to May 2005.
Typ-ical issue of EuroPAP newsletter contained about5325 complete news articles and a number of short(containing at most several sentences) news items.DC was constructed based on articles retrieved formWikipedia corresponding to ten most populous Pol-ish cities (Wikipedia, 2007) covering typically sev-eral topics (e.g.
geography, culture, transportation).For AC creation we followed precisely themethod applied for English in (Choi, 2000).
Eachartificial document was created as a concatenation ofrandom number (n) of first sentences from ten newsarticles randomly selected from a total of 24927news items in EuroPAP corpus.
Four subcollectionswere created depending on allowed range of n aslisted in Table 1.
Any two selected articles were as-sumed to cover two different topics; thus referencesegmentation boundaries corresponded to points ofconcatenations.AC AC1 AC2 AC3 AC4n 3-11 3-5 6-8 9-11documents count 400 100 100 100Table 1: Artificial collection subcollectionsFor SC creation newsletter messages from Eu-roPAP were used as text streams (936 messages).The reference segmentation was created using origi-nal article boundaries present in EuroPAP mail mes-sages (almost 30000 segments were marked).For individual documents collection developmenttext content was extracted from Wikipedia docu-ments, all headings were removed and all list items(LI tags) with no terminal punctuation sign wereadded a dot.
Manual tagging by two authors ofthis paper was performed.
The instructions wereto put segment boundaries in the places of poten-tial section titles.
Obtained percent agreement of0.988 and ?
coefficient (Carletta, 1996) of 0.975suggest high convergence of both annotations.
Fur-ther, in places where the two annotators opinions dif-fered (one marked boundary and the other did not),negotiation-based approach (Flejter, 2006) was ap-plied in order to develop reference segmentation.3.2 Selected algorithmsIn our experiment we used Choi?s publicly availableimplementation of several text segmentation algo-rithms not requiring training (with several adapta-Sentences Tokensavg std avg stdAC 6.8 1.7 122.6 33.4SC 15.0 3.8 267.6 64.0DC 28.5 9.9 300.0 110.2Table 2: The average length of reference segmentstion concerning pre-processing stages).
Specificallywe used Choi?s implementation of TextTiling algo-rithm (TT ), C99 algorithm for both known (C99l)and unknown (C99) number of boundaries as wellas DotPlotting maximization (DP ) and minimiza-tion (DPmin) algorithms.Algorithms not requiring to provide number ofsegments as input (TT , C99) were evaluated on alltest collections; performance of the remaining algo-rithms (C99l, DP , DPmin) was measured only forAC.3.3 Pre-processing variantsWe decided to prepare seven variants of the test col-lections (see Table 3).
The motivation for the firstgroup (variants: P1, P2, P3, P4) was to be as closeto Choi?s methodology as possible.
That?s why weused simple pre-processing techniques like lemma-tization and stop-lists.
The remaining variants (P5,P6, P7) were chosen arbitrarily to check how addi-tional morphological information will influence theperformance of the main segmentation algorithms incase of Polish language.The pre-processing stage included two steps.
Ini-tially, documents were split into sentences and wordtokens (punctuation signs were removed) by meansof tokenizer and sentence boundary recognizer ofSProUT ?
a shallow text processing system tai-lored to processing Polish language (Piskorski etal., 2004).
Afterwards the generated token streamwas normalized; for this task SProUT?s interface fora dictionary based Polish morphological analyzer?
Morfeusz (Wolin?ski, 2007) was used.
This al-lowed us to use variety of morphological informa-tion (including STEM, POS, NUMBER, TENSE).The drawback of such an approach was that to-kens not present in Morfeusz?s dictionary were notstemmed (accounting for 12.8% of all tokens or31.7% of unique tokens; note that Morfeusz input54Id Variant DescriptionP1 I no changes (tokens remain inflected)P2 L lemmatized tokensP3 LSL L ?
words in the lemmatized stop-listP4 ISI I?without words in the inflected stop-listP5 L-VT L + verbs tagged with POS and TENSEP6 L-VT-N-AL-VT + nouns and adjectives tagged withPOSP7 L-VT-NN-ANL-VT-N-A + nouns and adjectives taggedwith NUMBERTable 3: pre-processing variantscontained all the tokens including numbers, hyper-links, dates, etc.).
Another problematic issue wasambiguity of morphological analysis (1.4 interpre-tation on average); we addressed this issue by us-ing the following order of preference: 1) verbs, 2)nouns, 3) adjectives, 4) other word classes.The stop-lists (inflected and lemmatized versionscontained 616 and 350 tokens respectively) wereprepared manually by analysing frequency lists ofpreviously used text corpus.3.4 Evaluation metricsA number of different measurement methods wereapplied to topical texts segmentation includingrecall-precision pair (Hearst and Plaunt, 1993; Pas-sonneau and Litman, 1997), edit distance (Ponteand Croft, 1997), P?
(Beeferman et al, 1997), Pk(Beeferman et al, 1999) and WindowDiff (Pevznerand Hearst, 2002).Pk is simplified version of probabilistic measureP?
based on assumption that any two consecutiveboundaries are at distance of k sentences (k beingparameter normally set to half of length of aver-age segment in reference segmentation).
After somesimplifications Pk is defined by the following for-mula (Flejter, 2006):Pk(r, h) = 1 ?1n?
kn?k?i=1(|?r(i, k) ?
?h(i, k)|)where ?X(i, k) equals to one if ith and (i + k)thsentences are in the same segment of segmentationX , otherwise it is equal to zero; X = r correspondsto reference segmentation and X = h correspondsto hypothetical (algorithm-generated) segmentation.In most publication instead of performance mea-surement using Pk, probabilistic error metric (P =1 ?
Pk(r, h)) is applied.
For easier comparisonwith previous results we calculated this measure fortested evaluation scenarios.Based on a profound analysis of Pk and proba-bilistic metric drawbacks more recently WindowD-iff error measure was proposed based on countingnumber of boundaries within window of size of ksentences sliding parallelly over both hypotheticaland reference segmentations.
WindowDiff can becalculated by the following formula:Wk(r, h) =1N ?
kN?k?i=1(|br(i, k) ?
bh(i, k)| > 0)with bX(i, k) corresponding to the number ofboundaries between positions i and i+k in segmen-tation X .Both probabilistic error and WindowDiff measurethe segmentation error; therefore, the lower is theirvalue, the better is segmentation result.4 Experimental ResultsCalculated values of P and WindowDiff (WD)measures were used to compare performance ofdifferent algorithms, collections and pre-processingstrategies.
If not stated otherwise, results displayedin this Section correspond to P1 variant (no pre-processing) of test collections.C99 TTP WD P WDAC 0.365 0.355 0.527 0.638AC1 0.360 0.350 0.539 0.639AC2 0.387 0.360 0.435 0.436AC3 0.370 0.360 0.549 0.650AC4 0.359 0.364 0.551 0.821SC 0.381 0.390 0.562 0.932DC 0.429 0.477 0.554 0.877Table 4: Comparison of methods not requiring num-ber of segments as inputComparative results of both algorithms not requir-ing to provide expected number of segments as in-put (i.e.
C99 and TT ) are listed in Table 4.
C9955performs much better than TextTiling on all test col-lections with extremely high WindowDiff value forTextTiling (especially for longer texts).
Both al-gorithms perform better on artificial than on actualdocuments; for C99 drop in performance betweenstream and cities documents is also visible.C99l DP DPminP WD P WD P WDAC 0.339 0.332 0.353 0.338 0.462 0.485AC1 0.342 0.336 0.368 0.353 0.460 0.483AC2 0.324 0.304 0.343 0.318 0.455 0.483AC3 0.342 0.337 0.347 0.332 0.464 0.487AC4 0.338 0.341 0.310 0.300 0.475 0.495Table 5: Comparison of methods requiring numberof segments as inputProbabilistic error and WindowDiff results for al-gorithms requiring expected number of segmentsas input (C99l, DP , DPmin) are listed in Ta-ble 5.
C99l performs slightly better than DotPlottingwith maximization strategy (DP ) and the perfor-mance of DotPlotting applying minimization strat-egy (DPmin) is visibly lower.
Results do not differsignificantly between AC subcollections suggestingthat length of document has minor impact on perfor-mance.As C99 was developed both in version requiringand not requiring to specify the expected number ofsegments, impact of this information on algorithmperformance was analyzed.
As expected C99l out-performs C99; in our experiments additional infor-mation on segments count lowered the error rates by4?16% (see: Table 6).As previous research on English text segmenta-tion (Choi, 2000) was led for the same artificial col-C99 C99l ?%P WD P WD P WDAC 0.365 0.355 0.339 0.332 7% 6%AC1 0.360 0.350 0.342 0.336 5% 4%AC2 0.387 0.360 0.324 0.304 16% 16%AC3 0.370 0.360 0.342 0.337 8% 6%AC4 0.359 0.364 0.338 0.341 6% 6%Table 6: Impact of segments count provided as input3-11 3-5 6-8 9-11C99(P3)PL 0.32 0.34 0.31 0.29EN 0.13 0.18 0.10 0.10C99l(P3)PL 0.30 0.27 0.29 0.28EN 0.12 0.12 0.09 0.09DP (P3)PL 0.32 0.28 0.26 0.24EN 0.22 0.21 0.18 0.16DPmin(P3)PL 0.46 0.46 0.46 0.47EN n/a 0.34 0.37 0.37TT (P3)PL 0.50 0.39 0.49 0.54EN 0.54 0.45 0.52 0.53Table 7: Polish versus English resultslection creation methodology (see section 3.1), algo-rithm implementations and probabilistic error met-ric, comparison of algorithms performance for Pol-ish and English was possible.
The results of suchcomparison are gathered in Table 7; for English re-sults were taken from Choi?s paper and for PolishP3 variant of our artificial collection correspondingto Choi?s pre-processing approach was used.
Com-parative analysis shows that all algorithms (exceptfor TT which is highly inefficient for both Polishand English) perform significantly worse for Polish.Our hypothesis is that it can be attributed both tolower performance of pre-processing tools for Pol-ish and usage of domain specific corpus as opposedto balanced Brown corpus used by Choi.AC + C99l SC + C99 DC + C99P WD P WD P WDP1 0.365 0.355 0.381 0.390 0.429 0.477P2 0.322 0.315 0.356 0.367 0.433 0.477P3 0.319 0.311 0.354 0.368 0.452 0.497P4 0.342 0.334 0.381 0.391 0.425 0.473P5 0.362 0.318 0.356 0.368 0.435 0.480P6 0.416 0.403 0.413 0.419 0.406 0.430P7 0.416 0.403 0.413 0.419 0.406 0.430?% 13% 12% 7% 6% 5% 10%Table 8: Impact of different pre-processing variantsFinal part of our analysis of quantitative resultsfocused on impact of different pre-processing strate-gies.
For each of three test collections we analyzedthe impact of pre-processing strategies in case of56best performing algorithm.As the results from Table 8 show, for artificial andstream collections the most promising strategy is touse P3 (LSL) variant of pre-processing which de-creased the error metric by up to 13% as comparedwith P1 (no pre-processing) variant.
Interestingly,the same approach applied to individual documentscollection actually increased values of error met-rics; in this case significant decrease of error rateswas possible by use of the most complex P6 and P7strategies.
Reasons for this difference may include:a) disproportion in number of unrecognized tokens(22.8% for DC vs. 12.75% for AC/SC), b) differentstructure of DC reference segments (higher numberof shorter sentences, see: Table 2), c) standard de-viation of segment length in DC much higher thanin AC/DC (35% of average length in case of DC vs.25% in case of both AC and SC).
We leave analysisof this factors?
impact for future work.Our analysis also shows that adding NUMBERtag for nouns and adjectives (P7) has no impact onalgorithms performance as compared with P6.5 Conclusions and Future WorkIn this paper we analyzed performance of severaltopical text segmentation algorithms for Polish withseveral pre-processing strategies on three differenttest collections.
Our research demonstrate that sim-ilarly to English C99 (and its variant with expectedsegments count input C99l) is the best performingsegmentation algorithm and we recommend that itbe applied to text segmentation for Polish.
Based onour research we also suggest that lemmatization andstop-list words removal (P3 variant) be used for fur-ther improvement of performance.
However, our re-search revealed that the performance of almost all al-gorithms (including C99) is significantly worse forPolish than for English and remains unsatisfactory.Therefore our further research direction will be tofocus on improvements at the pre-processing stagesof text segmentation (including enhancements intext division into sentences, lemmatization of propernames, and filtering of unrecognized tokens withlow document-based frequency) as well as on anal-ysis of performance of more recent algorithms bothrequiring and not requiring linguistic resources.
Wewould like also to evaluate text segmentation impacton performance of coreference resolution algorithmwe are currently developing.ReferencesJames Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
Topic de-tection and tracking pilot study.
final report.Doug Beeferman, Adam Berger, and John Lafferty.1997.
Text segmentation using exponential models.
InClaire Cardie and Ralph Weischedel, editors, Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 35?46.
Asso-ciation for Computational Linguistics, Somerset, NewJersey.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.
Ma-chine Learning, 34(1-3):177?210.David M. Blei and Pedro J. Moreno.
2001.
Topic seg-mentation with an aspect hidden Markov model.
InSIGIR ?01: Proceedings of the 24th annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 343?348, NewYork, NY, USA.
ACM Press.Jean Carletta.
1996.
Assessing agreement on classifica-tion task: the kappa statistic.
Computational Linguis-tic, 22:250?254.Freddy Y. Y. Choi.
2000.
Advances in domain indepen-dent linear text segmentation.
In Proceedings of thefirst conference on North American chapter of the As-sociation for Computational Linguistics, pages 26?33,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Freddy Y. Y. Choi.
2002.
Content-based Text Naviga-tion.
Ph.D. thesis, Department of Computer Science,University of Manchester.EuroPAP.
2005.
Serwis o Unii Europejskiej,http://euro.pap.com.pl/ (2001-2005).Dominik Flejter.
2006.
Automatic topical segmentationof documents in text collections (in polish).
Master?sthesis, Poznan University of Economics, June.Marti A. Hearst and Christian Plaunt.
1993.
Subtopicstructuring for full-length document access.
In SI-GIR ?93: Proceedings of the 16th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 59?68, New York, NY,USA.
ACM Press.Stefan Kaufmann.
1999.
Cohesion and collocation: Us-ing context vectors in text segmentation.
In Proceed-ings of the 37th annual meeting of the Association for57Computational Linguistics on Computational Linguis-tics, pages 591?595, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Hideki Kozima.
1993.
Text segmentation based on sim-ilarity between words.
In Proceedings of the 31st an-nual meeting on Association for Computational Lin-guistics, pages 286?288, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.C.
Manning.
1998.
Rethinking text segmentation mod-els: An information extraction case study.
Technicalreport, University of Sydney.Kathleen R. McKeown Min-Yen Kan, Judith L. Klavans.1998.
Linear segmentation and segment significance.In Proceedings of 6th International Workshop of VeryLarge Corpora (WVLC-6), pages 197?205.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indicatorof the structure of text.
Computational Linguistics,17(1):21?48.P.
Mulbregt, I. van, L. Gillick, S. Lowe, and J. Yamron.1998.
Text segmentation and topic tracking on broad-cast news via a hidden markov model approach.
InProceedings of the ICSLP?98, volume 6, pages 2519?2522.Rebecca J. Passonneau and Diane J. Litman.
1997.
Dis-course segmentation by human and automated means.Comput.
Linguist., 23(1):103?139.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Comput.
Linguist., 28(1):19?36.Jakub Piskorski, Peter Homola, Malgorzata Marciniak,Agnieszka Mykowiecka, Adam Przepio?rkowski, andMarcin Wolinski.
2004.
Information extraction forPolish using the SProUT platform.
In Intelligent In-formation Processing and Web Mining, Proceedingsof the International Intelligent Information Systems:IIPWM?04 Conference, Advances in Soft Computing,pages 227?236.
Springer.Jay M. Ponte and W. Bruce Croft.
1997.
Text segmen-tation by topic.
In ECDL ?97: Proceedings of theFirst European Conference on Research and AdvancedTechnology for Digital Libraries, pages 113?125, Lon-don, UK.
Springer-Verlag.Jeffrey C. Reynar.
1994.
An automatic method of findingtopic boundaries.
In Proceedings of the 32nd annualmeeting on Association for Computational Linguistics,pages 331?333, Morristown, NJ, USA.
Association forComputational Linguistics.Jeffrey C. Reynar.
1998.
Topic segmentation: algorithmsand applications.
A dissertation in Computer and In-formation Science.
Ph.D. thesis, University of Penn-sylvania.Wikipedia.
2007.
Miasta w Polsce wedlug liczbyludnosci, Accessed on 20.03.2007.Marcin Wolin?ski.
2007.
Analizator morfologiczny Mor-feusz SIAT.
On-line.
Accessed on: 30.01.2007.58
