WordNet-based Text Document ClusteringJulian SeddingDepartment of Computer ScienceUniversity of YorkHeslington, York YO10 5DD,United Kingdom,juliansedding@gmx.deDimitar KazakovAIG, Department of Computer ScienceUniversity of YorkHeslington, York YO10 5DD,United Kingdom,kazakov@cs.york.ac.ukAbstractText document clustering can greatly simplifybrowsing large collections of documents by re-organizing them into a smaller number of man-ageable clusters.
Algorithms to solve this taskexist; however, the algorithms are only as goodas the data they work on.
Problems include am-biguity and synonymy, the former allowing forerroneous groupings and the latter causing sim-ilarities between documents to go unnoticed.
Inthis research, na?
?ve, syntax-based disambigua-tion is attempted by assigning each word apart-of-speech tag and by enriching the ?bag-of-words?
data representation often used for docu-ment clustering with synonyms and hypernymsfrom WordNet.1 IntroductionText document clustering is the grouping of textdocuments into semantically related groups, oras Hayes puts it, ?they are grouped because theyare likely to be wanted together?
(Hayes, 1963).Initially, document clustering was developed toimprove precision and recall of information re-trieval systems.
More recently, however, drivenby the ever increasing amount of text docu-ments available in corporate document reposi-tories and on the Internet, the focus has shiftedtowards providing ways to efficiently browselarge collections of documents and to reorganisesearch results for display in a structured, oftenhierarchical manner.The clustering of Internet search results hasattracted particular attention.
Some recentstudies explored the feasibility of clustering ?inreal-time?
and the problem of adequately label-ing clusters.
Zamir and Etzioni (1999) havecreated a clustering interface for the meta-search engine ?HuskySearch?
and Zhang andDong (2001) present their work on a systemcalled SHOC.
The reader is also referred toVivisimo,1 a commercial clustering interfacebased on results from a number of search-engines.Ways to increase clustering speed are ex-plored in many research papers, and the recenttrend towards web-based clustering, requiringreal-time performance, does not seem to changethis.
However, van Rijsbergen points out, ?itseems to me a little early in the day to insist onefficiency even before we know much about thebehaviour of clustered files in terms of the ef-fectiveness of retrieval?
(van Rijsbergen, 1989).Indeed, it may be worth exploring which factorsinfluence the quality (or effectiveness) of docu-ment clustering.Clustering can be broken down into twostages.
The first one is to preprocess the doc-uments, i.e.
transforming the documents intoa suitable and useful data representation.
Thesecond stage is to analyse the prepared data anddivide it into clusters, i.e.
the clustering algo-rithm.Steinbach et al (2000) compare the suitabil-ity of a number of algorithms for text clusteringand conclude that bisecting k-means, a parti-tional algorithm, is the current state-of-the-art.Its processing time increases linearly with thenumber of documents and its quality is similarto that of hierarchical algorithms.Preprocessing the documents is probably atleast as important as the choice of an algorithm,since an algorithm can only be as good as thedata it works on.
While there are a number ofpreprocessing steps, that are almost standardnow, the effects of adding background knowl-edge are still not very extensively researched.This work explores if and how the two followingmethods can improve the effectiveness of clus-tering.1http://www.vivisimo.comPart-of-Speech Tagging.
Segond et al(1997) observe that part-of-speech tagging(PoS) solves semantic ambiguity to someextent (40% in one of their tests).
Basedon this observation, we study whetherna?
?ve word sense disambiguation by PoStagging can help to improve clusteringresults.WordNet.
Synonymy and hypernymy can re-veal hidden similarities, potentially leadingto better clusters.
WordNet,2 an ontologywhich models these two relations (amongmany others) (Miller et al, 1991), is usedto include synonyms and hypernyms in thedata representation and the effects on clus-tering quality are observed and analysed.The overall aim of the approach outlined aboveis to cluster documents by meaning, hence itis relevant to language understanding.
The ap-proach has some of the characteristics expectedfrom a robust language understanding system.Firstly, learning only relies on unannoted textdata, which is abundant and does not containthe individual bias of an annotator.
Secondly,the approach is based on general-purpose re-sources (Brill?s PoS Tagger, WordNet), and theperformance is studied under pessimistic (hencemore realistic) assumptions, e.g., that the tag-ger is trained on a standard dataset with poten-tially different properties from the documentsto be clustered.
Similarly, the approach studiesthe potential benefits of using all possible senses(and hypernyms) from WordNet, in an attemptto postpone (or avoid altogether) the need forWord Sense Disambiguation (WSD), and the re-lated pitfalls of a WSD tool which may be biasedtowards a specific domain or language style.The remainder of the document is structuredas follows.
Section 2 describes related workand the techniques used to preprocess the data,as well as cluster it and evaluate the resultsachieved.
Section 3 provides some backgroundon the selected corpus, the Reuters-21578 testcollection (Lewis, 1997b), and presents the sub-corpora that are extracted for use in the exper-iments.
Section 4 describes the experimentalframework, while Section 5 presents the resultsand their evaluation.
Finally, conclusions aredrawn and further work discussed in Section 6.2available at http://www.cogsci.princeton.edu/?wn2 BackgroundThis work is most closely related to the recentlypublished research of Hotho et al (2003b), andcan be seen as a logical continuation of their ex-periments.
While these authors have analysedthe benefits of using WordNet synonyms and upto five levels of hypernyms for document clus-tering (using the bisecting k-means algorithm),this work describes the impact of tagging thedocuments with PoS tags and/or adding all hy-pernyms to the information available for eachdocument.Here we use the vector space model, as de-scribed in the work of Salton et al (1975), inwhich a document is represented as a vector or?bag of words?, i.e., by the words it containsand their frequency, regardless of their order.A number of fairly standard techniques havebeen used to preprocess the data.
In addition,a combination of standard and custom softwaretools have been used to add PoS tags and Word-Net categories to the data set.
These will bedescribed briefly below to allow for the experi-ments to be repeated.The first preprocessing step is to PoS tag thecorpus.
The PoS tagger relies on the text struc-ture and morphological differences to determinethe appropriate part-of-speech.
For this reason,if it is required, PoS tagging is the first stepto be carried out.
After this, stopword removalis performed, followed by stemming.
This or-der is chosen to reduce the amount of wordsto be stemmed.
The stemmed words are thenlooked up in WordNet and their correspond-ing synonyms and hypernyms are added to thebag-of-words.
Once the document vectors arecompleted in this way, the frequency of eachword across the corpus can be counted and ev-ery word occurring less often than the pre speci-fied threshold is pruned.
Finally, after the prun-ing step, the term weights are converted to tf idfas described below.Stemming, stopword removal and pruning allaim to improve clustering quality by removingnoise, i.e.
meaningless data.
They all lead toa reduction in the number of dimensions in theterm-space.
Weighting is concerned with the es-timation of the importance of individual terms.All of these have been used extensively and areconsidered the baseline for comparison in thiswork.
However, the two techniques under in-vestigation both add data to the representa-tion.
PoS tagging adds syntactic informationand WordNet is used to add synonyms and hy-pernyms.
The rest of this section discusses pre-processing, clustering and evaluation in moredetail.PoS Tagging PoS tags are assigned to thecorpus using Brill?s PoS tagger.
As PoS taggingrequires the words to be in their original orderthis is done before any other modifications onthe corpora.Stopword Removal Stopwords, i.e.
wordsthought not to convey any meaning, are re-moved from the text.
The approach taken inthis work does not compile a static list of stop-words, as usually done.
Instead PoS informa-tion is exploited and all tokens that are notnouns, verbs or adjectives are removed.Stemming Words with the same meaning ap-pear in various morphological forms.
To cap-ture their similarity they are normalised intoa common root-form, the stem.
The morphol-ogy function provided with WordNet is used forstemming, because it only yields stems that arecontained in the WordNet dictionary.WordNet Categories WordNet, the lexicaldatabase developed by Miller et al, is used toinclude background information on each word.Depending on the experiment setup, words arereplaced with their synset IDs, which constitutetheir different possible senses, and also differentlevels of hypernyms, more general terms for thea word, are added.Pruning Words appearing with low fre-quency throughout the corpus are unlikely toappear in more than a handful of documentsand would therefore, even if they contributedany discriminating power, be likely to cause toofine grained distinctions for us to be useful, i.eclusters containing only one or two documents.Therefore all words (or synset IDs) that ap-pear less often than a pre-specified threshold arepruned.Weighting Weights are assigned to give anindication of the importance of a word.
Themost trivial weight is the word-frequency.
How-ever, more sophisticated methods can providebetter results.
Throughout this work, tf idf(term frequency x inverse document frequency)as described by Salton et al (1975), is used.One problem with term frequency is that thelengths of the documents are not taken into ac-count.
The straight forward solution to thisproblem is to divide the term frequency by thetotal number of terms in the document, thedocument length.
Effectively, this approach isequivalent to normalising each document vectorto length one and is called relative term fre-quency.However, for this research a more sophisti-cated measure is used: the product of term fre-quency and inverse document frequency tf idf.Salton et al define the inverse document fre-quency idf asidft= log2n ?
log2dft+ 1 (1)where dftis the number of documents in whichterm t appears and n the total number of doc-uments.
Consequently, the tf idf measure is cal-culated astf idft= tf ?
(log2n ?
log2dft+ 1) (2)simply the multiplication of tf and idf .
Thismeans that larger weights are assigned to termsthat appear relatively rarely throughout thecorpus, but very frequently in individual doc-uments.
Salton et al (1975) measure a 14%improvement in recall and precision for tf idf incomparison to the standard term frequency tf.Clustering is done with the bisecting k-means algorithm as it is described by Stein-bach et al (2000).
In their comparison ofdifferent algorithms they conclude that bisect-ing k-means is the current state of the art fordocument clustering.
Bisecting k-means com-bines the strengths of partitional and hierarchi-cal clustering methods by iteratively splittingthe biggest cluster using the basic k-means algo-rithm.
Basic k-means is a partitional clusteringalgorithm based on the vector space model.
Atthe heart of such algorithms is a similarity mea-sure.
We choose the cosine distance, which mea-sures the similarity of two documents by cal-culating the cosine of the angle between them.The cosine distance is defined as follows:s(di, dj) = cos((di, dj)) =di?dj|di| ?
|dj|(3)where |di| and |dj| are the lengths of vectorsdiand dj, respectively, and di?djis the dot-product of the two vectors.
When the lengths ofthe vectors are normalised, the cosine distanceis equivalent to the dot-product of the vectors,i.e.
d1?d2.Evaluation Three different evaluation mea-sures are used in this work, namely purity, en-tropy and overall similarity.
Purity and entropyare both based on precision,prec(C,L) := |C ?
L||C| , (4)where each cluster C from a clustering C of theset of documents D is compared with the man-ually assigned category labels L from the man-ual categorisation L, which requires a category-labeled corpus.
Precision is the probability of adocument in cluster C being labeled L.Purity is the percentage of correctly clustereddocuments and can be calculated as:purity(C, L) :=?C?C|C||D| ?
maxL?L prec(C,L) (5)yielding values in the range between 0 and 1.The intra-cluster entropy (ice) of a cluster C,as described by Steinbach et al (2000), consid-ers the dispersion of documents in a cluster, andis defined as:ice(C) :=?L?Lprec(C,L) ?
log(prec(C,L)) (6)Based on the intra-cluster entropy of all clus-ters, the average, weighted by the cluster size,is calculated.
This results in the following for-mula, which is based on the one used by Stein-bach et al (2000):entropy(C) :=?C?C|C||D| ?
ice(C) (7)Overall similarity is independent of pre-annotation.
Instead the intra-cluster similari-ties are calculated, giving an idea of the cohe-siveness of a cluster.
This is the average similar-ity between each pair of documents in a cluster,including the similarity of a document with it-self.
Steinbach et al (2000) show that this isequivalent to the squared length of the clustercentroid, i.e.
|c|2.
The overall similarity is thencalculated asoverall similarity(C) :=?C?C|C||D| ?
|c|2 (8)Similarity is expressed as a percentage, there-fore the possible values for overall similarityrange from 0 to 1.3 The CorpusHere we look at what kind of corpus is requiredto assess the quality of clusters, and presentour choice, the Reuters-21578 test collection.This is followed by a discussion of the ways sub-corpora can be extracted from the whole corpusin order to address some of the problems of theReuters corpus.A corpus useful for evaluating text documentclustering needs to be annotated with class orcategory labels.
This is not a straightforwardtask, as even human annotators sometimes dis-agree on which label to assign to a specific doc-ument.
Therefore, all results depend on thequality of annotation.
It is therefore unrealis-tic to aim at high rates of agreement with re-gard to the corpus, and any evaluation shouldrather focus on the relative comparison of theresults achieved by different experiment setupsand configurations.Due to the aforementioned difficulty of agree-ing on a categorisation and the lack of a defini-tion of ?correct?
classification, no standard cor-pora for evaluation of clustering techniques ex-ist.
Still, although not standardised, a numberof pre-categorised corpora are available.
Apartfrom various domain-specific corpora with classannotations, there is the Reuters-21578 test col-lection (Lewis, 1997b), which consists of 21578newswire articles from 1987.The Reuters corpus is chosen for use in theexperiments of this projects for four reasons.1.
Its domain is not specific, therefore it canbe understood by a non-expert.2.
WordNet, an ontology, which is not tailoredto a specific domain, would not be effectivefor domains with a very specific vocabulary.3.
It is freely available for download.4.
It has been used in comparable studies be-fore (Hotho et al, 2003b).On closer inspection of the corpus, there re-main some problems to solve.
First of all, onlyabout half of the documents are annotated withcategory-labels.
On the other hand some doc-uments are attributed to multiple categories,meaning that categories overlap.
Some confu-sion seems to have been caused in the researchcommunity by the fact that there is a TOPICSattribute in the SGML, the value of which is ei-ther set to YES or NO (or BYPASS).
However,this does not correspond to the values observedwithin the TOPICS tag; sometimes categoriescan be found, even if the TOPICS attribute isset to NO and sometimes there are no categoriesassigned, even if the attribute indicates YES.Lewis explains that this is not an error in thecorpus, but has to do with the evolution of thecorpus and is kept for historic reasons (Lewis,1997a).Therefore, to prepare a base-corpus, theTOPICS attribute is ignored and all documentsthat have precisely one category assigned tothem are selected.
Additionally, all documentswith an empty document body are also dis-carded.
This results in the corpus ?reut-base?containing 9446 documents.
The distributionof category sizes in the ?reut-base?
is shown inFigure 1.
It illustrates that there are a few cat-egories occurring extremely often, in fact thetwo biggest categories contain about two thirdsof all documents in the corpus.
This unbal-anced distribution would blur test results, be-cause even ?random clustering?
would poten-tially obtain purity values of 30% and more onlydue to the contribution of the two main cate-gories.05001000150020002500300035004000earntrade shipcoffee cpijobsalumnat-gas bopwpiretailstrategic-metalheatlumbersilverinstal-debttea cpuinventoriescoconutnaphtha ricecategory (ordered by frequency)numberofdocumentsFigure 1: Category Distribution for Corpus?reut-base?
(only selected categories are listed).Similar to Hotho et al (2003b), we get aroundthis problem by deriving new corpora from thebase corpus.
Their maximum category size isreduced to 20, 50 and 100 documents respec-tively.
Categories containing more documentsare not excluded, but instead they are reducedin size to comply with the defined maximum,i.e., all documents in excess of the maximumare removed.Creating derived corpora has the further ad-vantages of reducing the size of corpora and thuscomputational requirements for the test runs.Also, tests can be run on more and less homo-geneous corpora, homogeneous with regard tothe cluster size, that is, which can give an ideaof how the method performs under different con-ditions.
Especially for this purpose a fourth, ex-tremely homogeneous test corpus, ?reut-min15-max20?
is derived.
It is like the ?reut-max20?corpus, but all categories containing less than15 documents are entirely removed.
The ?reut-min15-max20?
is thus the most homogeneoustest corpus, with a standard deviation in clustersize of only 0.7 documents.A summary of the derived test corpora isshown in Table 1, including the number of doc-uments they contain, i.e.
their size, the averagecategory size and the standard deviation.
Fig-ure 2 shows the distribution of categories withinthe derived corpora graphically.Table 1: Corpus StatisticsCategory SizeName Size ?
stdevreut-min15-max20 713 20 0.7reut-max20 881 13 7.7reut-max50 1690 24 19.9reut-max100 2244 34 35.2reut-base 9446 143 553.2Note: The average category size is rounded to the nearestwhole number, the standard deviation to the first decimalplace.4 Clustering with PoS andBackground KnowledgeThe aim of this work is to explore the bene-fits of partial disambiguation of words by theirPoS and the inclusion of WordNet concepts.This has been tested on five different setups,as shown in Table 2.Table 2: Experiment ConfigurationsName Base PoS Syns HyperBaseline yesPoS Only yes yesSyns yes yes yesHyper 5 yes yes yes 5Hyper All yes yes yes allBase: stopword removal, stemming, pruning and tf idf weightingare performed; PoS tags are stripped.PoS: PoS tags are kept attached to the words.Syns: all senses of a word are included using synset offsets.Hyper: hypernyms to the specified depth are included.Empty fields indicate ?no?
or ?0?.Baseline The first configuration setting is usedto get a baseline for comparison.
All ba-sic preprocessing techniques are used, i.e.stopword removal, stemming, pruning and0510152025acqbopcoffeecottoncrude gasgoldinterestiron-steellivestockmoney-supplyorangereservesrubbersugartrade wpihousingcategory (ordered by frequency)numberofdocumentsreut-min15-max200510152025acqcocoacottonearngold ipilivestocknat-gasreservesshiptradestrategic-metalheatlumbersilverinstal-debttea cpuinventoriescoconutnaphtha ricecategory (ordered by frequency)numberofdocumentsreut-max200102030405060money-fx cpiinteresttradereservescrudealumnat-gas bopwpiretailstrategic-metalheatlumbersilverinstal-debtyencpuinventoriescoconutstgnaphthacategory (ordered by frequency)numberofdocumentsreut-max50020406080100120money-fxsugarinterestearn cpicopperalumnat-gas bopwpigasstrategic-metalheatincomefuelinstal-debtyen jetinventories wooll-cattlenaphthacategory (ordered by frequency)numberofdocumentsreut-max100Figure 2: Category Distributions for DerivedCorporaweighting.
PoS tags are removed from thetokens to get the equivalent of a raw textcorpus.PoS Only Identical to the baseline, but thePoS tags are not removed.Syns In addition to the previous configuration,all WordNet senses (synset IDs) of eachPoS tagged token are included.Hyper 5 Here five levels of hypernyms are in-cluded in addition to the synset IDs.Hyper All Same as above, but all hypernymsfor each word token are included.Each of the configurations is used to create16, 32 and 64 clusters from each of the fourtest-corpora.
Due to the random choice of ini-tial cluster centroids in the bisecting k-meansalgorithm, the means of three test-runs withthe same configuration is calculated for corpora?reut-max20?
and ?reut-max50?.
The existingproject time constraints allowed us to gain someadditional insight by doing one test-run for eachof ?reut-max100?
and ?reut-min15-max20?.
Thisresults in 120 experiments in total.All configurations use tf idf weighting andpruning.
The pruning thresholds vary.
For allexperiments using the ?reut-max20?
corpus allterms occurring less than 20 times are pruned.The experiments on corpora ?reut-max50?
and?reut-min15-max20?
are carried out with a prun-ing threshold of 50.
For the corpus ?reut-max100?, the pruning threshold is set to 50when configurations Baseline, PoS Only orSyns are used and to 200 otherwise.
This rel-atively high threshold is chosen, in order to re-duce memory requirements.
To ensure that thisinconsistency does not distort the conclusionsdrawn from the test data, the results of thesetests are considered with great care and are ex-plicitly referred to when used.Further details of this research are describedin an unpublished report (Sedding, 2004).5 Results and EvaluationThe results are presented in the format of onegraph per corpus, showing the entropy, purityand overall similarity values for each of the con-figurations shown in Table 2.On the X-axis, the different configuration set-tings are listed.
On the right-hand side, hyperefers to the hypernym depth, syn refers towhether synonyms were included or not, posrefers to the presence or absence of PoS tagsand clusters refers to the number of clusters cre-ated.
For improved readability, lines are drawn,splitting the graphs into three sections, one foreach number of clusters.
For experiments onthe corpora ?reut-max20?
and ?reut-max50?, thevalues in the graphs are the average of threetest runs, whereas for the corpora ?reut-min15-max20?
and ?reut-max100?, the values are thoseobtained from a single test run.The Y-axis indicates the numerical values foreach of the measures.
Note that the valuesfor purity and similarity are percentages, andthus limited to the range between 0 and 1.
Forthose two measures, higher values indicate bet-ter quality.
High entropy values, on the otherhand, indicate lower quality.
Entropy values arealways greater than 0 and for the particular ex-periments carried out, they never exceed 1.3.In analysing the test results, the main focus ison the data of corpora ?reut-max20?
and ?reut-max50?, shown in Figure 3 and Figure 4, re-spectively.
This data is more reliable, becauseit is the average of repeated test runs.
Figures6?7 show the test data obtained from cluster-ing the corpora ?reut-min15-max20?
and ?reut-max100?, respectively.The fact that the purity and similarity valuesare far from 100 percent is not unusual.
In manycases, not even human annotators agree on howto categorise a particular document (Hotho etal., 2003a).
More importantly, the number ofcategories are not adjusted to the number of la-bels present in a corpus, which makes completeagreement impossible.All three measures indicate that the qual-ity increases with the number of clusters.
Thegraph in Figure 5 illustrates this for the entropyin ?reut-max50?.
For any given configuration, itappears that the decrease in entropy is almostconstant when the number of clusters increases.This is easily explained by the average clustersizes, which decrease with an increasing numberof clusters; when clusters are smaller, the proba-bility of having a high percentage of documentswith the same label in a cluster increases.
This0.0000.2000.4000.6000.8001.0001.2000 0 0 5 all 0 0 0 5 all 0 0 0 5 all hypefalse false true true true false false true true true false false true true true synfalse true true true true false true true true true false true true true true pospurity entropy similarity16 32 64 clustersFigure 3: Test Results for ?reut-max20?becomes obvious when very small clusters arelooked at.
For instance, the minimum purityvalue for a cluster containing three documentsis 33 percent, for two documents it is 50 percent,and, in the extreme case of a single documentper cluster, purity is always 100 percent.The PoS Only experiment results in perfor-mance, which is very similar to the Baseline,and is sometimes a little better, sometimes a lit-tle worse.
This is expected, and the experimentis included to allow for a more accurate inter-pretation of the subsequent experiments usingsynonyms and hypernyms.A more interesting observation is that purityand entropy values indicate better clusters forBaseline than for any of the configurations us-ing background knowledge from WordNet (i.e.Syns, Hyper 5 and Hyper All).
One possi-ble conclusion is that adding background knowl-edge is not helpful at all.
However, the reasonsfor the relatively poor performance could alsobe due to the way the experiments are set up.Therefore, a possible explanation for these re-sults could be that the benefit of extra overlapbetween documents, which the added synonymsand hypernyms should provide, is outweighedby the additional noise they create.
WordNetdoes often provide five or more senses for aword, which means that for one correct sensea number of incorrect senses are added, even ifthe PoS tags eliminate some of them.The overall similarity measure gives a differ-ent indication.
Its values appear to increasefor the cases where background knowledge is in-cluded, especially when hypernyms are added.Overall similarity is the weighted average of theintra-cluster similarities of all clusters.
So theintra-cluster similarity actually increases with0.0000.2000.4000.6000.8001.0001.2000 0 0 5 all 0 0 0 5 all 0 0 0 5 all hypefalse false true true true false false true true true false false true true true synfalse true true true true false true true true true false true true true true pospurity entropy similarity16 32 64 clustersFigure 4: Test Results for ?reut-max50?0.5000.6000.7000.8000.9001.0001.1001.2001.3000 0 0 5 all hypefalse false true true true synfalse true true true true pos16 clusters 32 clusters 64 clustersFigure 5: Entropies for Different Cluster Sizesin ?reut-max50?0.0000.2000.4000.6000.8001.0001.2000 0 0 5 all 0 0 0 5 all 0 0 0 5 all hypefalse false true true true false false true true true false false true true true synfalse true true true true false true true true true false true true true true pospurity entropy similarity16 32 64 clustersFigure 6: Test Results for ?reut-min15-max20?added information.
As similarity increases withadditional overlap, the overall similarity mea-sure shows that additional overlap is achieved.The main problem with the approach ofadding all synonyms and all hypernyms intothe document vectors seems to be the addednoise.
The expectation that tf idf weightingwould take care of these quasi-random new con-cepts is not met, but the results also indicatepossible improvements to this approach.If word-by-word disambiguation would be0.0000.2000.4000.6000.8001.0001.2000 0 0 5 all 0 0 0 5 all 0 0 0 5 all hypefalse false true true true false false true true true false false true true true synfalse true true true true false true true true true false true true true true pospurity entropy similarity16 32 64 clustersFigure 7: Test Results for ?reut-max100?used, the correct sense of a word could be cho-sen and only the hypernyms for the correctsense of the word could be taken into account.This should drastically reduce noise.
The ben-efit of the added ?correct?
concepts would thenprobably improve cluster quality.
Hotho et al(2003a) experimented successfully with simpledisambiguation strategies, e.g., they used onlythe first sense provided by WordNet.As an alternative to word-by-word disam-biguation, a strategy to disambiguate based ondocument vectors could be devised; after addingall alternative senses of the terms, the least fre-quent ones could be removed.
This is similarto pruning but would be done on a documentby document basis, rather than globally on thewhole corpus.
The basis for this idea is that onlyconcepts that appear repeatedly in a documentcontribute (significantly) to the meaning of thedocument.
It is important that this is done be-fore hypernyms are added, especially when alllevels of hypernyms are added, because the mostgeneral terms are bound to appear more oftenthan the more specific ones.
This would leadto lots of very similar, but meaningless bags ofwords or bags of concepts.Comparing Syns, Hyper 5 and Hyper Allwith each other, in many cases Hyper 5 givesthe best results.
A possible explanation couldagain be the equilibrium between valuable in-formation and noise that are added to the vec-tor representations.
From these results it seemsthat there is a point where the amount of in-formation added reaches its maximum benefit;adding more knowledge afterwards results indecreased cluster quality again.
It should benoted that a fixed threshold for the levels of hy-pernyms used is unlikely to be optimal for allwords.
Instead, a more refined approach couldset this threshold as a function of the semanticdistance (Resnik and Yarowsky, 2000; Stetina,1997) between the word and its hypernyms.The maximised benefit is most evident inthe ?reut-max100?
corpus (Figure 7).
However,it needs to be kept in mind that for the lasttwo data points, Hyper 5 and Hyper All,the pruning threshold is 200.
Therefore, thecomparison with Syns needs to be done withcare.
This is not much of a problem, becausethe other graphs consistently show that theperformance for Syns is worse than for Hy-per 5.
The difference between Hyper 5 andHyper All in ?reut-max100?, can be directlycompared though, because the pruning thresh-old of 200 is used for both configurations.Surprisingly, there is a sharp drop in theoverall similarity from Hyper 5 to Hyper All,much more evident than in the other threecorpora.
One possible explanation could bethe different structure of the corpus.
It seemsmore probable, however, that the high prun-ing threshold is the cause again.
Assumingthat Hyper 5 seldom includes the most gen-eral concepts, whereas Hyper All always in-cludes them, their frequency in Hyper All be-comes so high that the frequencies of all theother terms are very low in comparison.
Thedocument vectors in case of Hyper All endup containing mostly meaningless concepts, be-cause most of the others are pruned.
This leadsto decreased cluster quality because the gen-eral concepts have little discriminating power.In the corresponding experiments on other cor-pora, more of the specific concepts are retained.Therefore, a better balance between generaland specific concepts is maintained, keeping thecluster quality higher than in the case of corpus?reut-max100?.PoS Only performs similar to Baseline, al-though usually a slight decrease in quality canbe observed.
Despite the assumption that thedisambiguation achieved by the PoS tags shouldimprove clustering results, this is clearly notthe case.
PoS tags only disambiguate the caseswhere different word classes are represented bythe same stem, e.g., the noun ?run?
and the verb?run?.
Clearly the meanings of these pairs arein most cases related.
Therefore, distinguishingbetween them reduces the weight of their com-mon concept by splitting it between two con-cepts.
In the worst case, they are pruned iftreated separately, instead of contributing sig-nificantly to the document vector as a joint con-cept.6 ConclusionsThe main finding of this work is that includ-ing synonyms and hypernyms, disambiguatedonly by PoS tags, is not successful in improv-ing clustering effectiveness.
This could be at-tributed to the noise introduced by all incor-rect senses that are retrieved from WordNet.
Itappears that disambiguation by PoS alone is in-sufficient to reveal the full potential of includingbackground knowledge.
One obviously imprac-tical alternative would be manual sense disam-biguation.
The automated approach of only us-ing the most common sense adopted by Hothoet al(2003b) seems more realistic yet beneficial.When comparing the use of different levels ofhypernyms, the results indicate that includingonly five levels is better than including all.
Apossible explanation of this is that the termsbecome too general when all hypernym levelsare included.Further research is needed to determinewhether this way of document clustering canbe improved by appropriately selecting a sub-set of the synonyms and hypernyms usedhere.
There is a number of corpus-based ap-proaches to word-sense disambiguation (Resnikand Yarowsky, 2000), which could be used forthis purpose.The other point of interest that could be fur-ther analysed is to find out why using five lev-els of hypernyms produces better results thanusing all levels of hypernyms.
It would be in-teresting to see whether this effect persists whenbetter disambiguation is used to determine ?cor-rect?
word senses.AcknowledgementsThe authors wish to thank the three anonymousreferees for their valuable comments.ReferencesR.M.
Hayes.
1963.
Mathematical models in in-formation retrieval.
In P.L.
Garvin, editor,Natural Language and the Computer, page287.
McGraw-Hill, New York.A.
Hotho, S. Staab, and G. Stumme.
2003a.Text clustering based on background knowl-edge.
Technical Report No.
425.A.
Hotho, S. Staab, and G. Stumme.
2003b.Wordnet improves text document clustering.In Proceedings of the Semantic Web Work-shop at SIGIR-2003, 26th Annual Interna-tional ACM SIGIR Conference.D.D.
Lewis.
1997a.
Readme file of Reuters-21578 text categorization test collection, dis-tribution 1.0.D.D.
Lewis.
1997b.
Reuters-21578 text catego-rization test collection, distribution 1.0.G.
Miller, R. Beckwith, C. Fellbaum, D. Gross,and K. Miller.
1991.
Five papers on wordnet.International Journal of Lexicography.P.
Resnik and D. Yarowsky.
2000.
Distin-guishing systems and distinguishing senses:New evaluation methods for word sense dis-ambiguation.
Natural Language Engineering,5(2):113?133.G.
Salton, A. Wong, and C.S.
Yang.
1975.
Avector space model for automatic indexing.Communications of the ACM, 18:613?620.J.
Sedding.
2004.
Wordnet-based text docu-ment clustering.
Bachelor?s Thesis.F.
Segond, A. Schiller, G. Grefenstette, andJ.P.
Chanod.
1997.
An experiment in seman-tic tagging using hidden Markov model tag-ging.
In Automatic Information Extractionand Building of Lexical Semantic Resourcesfor NLP Applications, pages 78?81.
Asso-ciation for Computational Linguistics, NewBrunswick, New Jersey.M.
Steinbach, G. Karypis, and V. Kumar.
2000.A comparison of document clustering tech-niques.
In KDD Workshop on Text Mining.Jiri Stetina.
1997.
Corpus Based Natural Lan-guage Ambiguity Resolution.
Ph.D. thesis,Kyoto University.C.J.
van Rijsbergen.
1989.
Information Re-trieval (Second Edition).
Buttersworth, Lon-don.O.
Zamir and O. Etzioni.
1999.
Grouper: Adynamic clustering interface to Web searchresults.
Computer Networks, Amsterdam,Netherlands, 31(11?16):1361?1374.D.
Zhang and Y. Dong.
2001.
Semantic, hierar-chical, online clustering of web search results.3rd International Workshop on Web Informa-tion and Data Management, Atlanta, Geor-gia.
