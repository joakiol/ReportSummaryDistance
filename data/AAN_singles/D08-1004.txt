Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31?40,Honolulu, October 2008. c?2008 Association for Computational LinguisticsModeling Annotators:A Generative Approach to Learning from Annotator Rationales?Omar F. Zaidan and Jason EisnerDept.
of Computer Science, Johns Hopkins UniversityBaltimore, MD 21218, USA{ozaidan,jason}@cs.jhu.eduAbstractA human annotator can provide hints to a machine learnerby highlighting contextual ?rationales?
for each of hisor her annotations (Zaidan et al, 2007).
How can oneexploit this side information to better learn the desiredparameters ??
We present a generative model of howa given annotator, knowing the true ?, stochasticallychooses rationales.
Thus, observing the rationales helpsus infer the true ?.
We collect substring rationales fora sentiment classification task (Pang and Lee, 2004) anduse them to obtain significant accuracy improvements foreach annotator.
Our new generative approach exploits therationales more effectively than our previous ?maskingSVM?
approach.
It is also more principled, and could beadapted to help learn other kinds of probabilistic classi-fiers for quite different tasks.1 BackgroundMany recent papers aim to reduce the amount of an-notated data needed to train the parameters of a sta-tistical model.
Well-known paradigms include ac-tive learning, semi-supervised learning, and eitherdomain adaptation or cross-lingual transfer from ex-isting annotated data.A rather different paradigm is to change the ac-tual task that is given to annotators, giving them agreater hand in shaping the learned classifier.
Af-ter all, human annotators themselves are more thanjust black-box classifiers to be run on training data.They possess some introspective knowledge abouttheir own classification procedure.
The hope is tomine this knowledge rapidly via appropriate ques-tions and use it to help train a machine classifier.How to do this, however, is still being explored.1.1 Hand-crafted rulesAn obvious option is to have the annotators directlyexpress their knowledge by hand-crafting rules.
This?This work was supported by National Science Foundationgrant No.
0347822 and the JHU WSE/APL Partnership Fund.Special thanks to Christine Piatko for many useful discussions.approach remains ?data-driven?
if the annotators re-peatedly refine their system against a corpus of la-beled or unlabeled examples.
This achieves highperformance in some domains, such as NP chunk-ing (Brill and Ngai, 1999), but requires more analyt-ical skill from the annotators.
One empirical study(Ngai and Yarowsky, 2000) found that it also re-quired more annotation time than active learning.1.2 Feature selection by humansMore recent work has focused on statistical classi-fiers.
Training such classifiers faces the ?credit as-signment problem.?
Given a training example xwithmany features, which features are responsible for itsannotated class y?
It may take many training exam-ples to distinguish useful vs. irrelevant features.1To reduce the number of training examplesneeded, one can ask annotators to examine or pro-pose some candidate features.
This is possible evenfor the very large feature sets that are typically usedin NLP.
In document classification, Raghavan et al(2006) show that feature selection by an oracle couldbe helpful, and that humans are both rapid and rea-sonably good at distinguishing highly useful n-gramfeatures from randomly chosen ones, even whenviewing these n-grams out of context.Druck et al (2008) show annotators some featuresf from a fixed feature set, and ask them to choose aclass label y such that p(y | f) is as high as possible.Haghighi and Klein (2006) do the reverse: for eachclass label y, they ask the annotators to propose afew ?prototypical?
features f such that p(y | f) is ashigh as possible.1.3 Feature selection in contextThe above methods consider features out of context.An annotator might have an easier time examining1Most NLP systems use thousands or millions of features,because it is helpful to include lexical features over a large vo-cabulary, often conjoined with lexical or non-lexical context.31features in context to recognize whether they appearrelevant.
This is particularly true for features thatare only modestly or only sometimes helpful, whichmay be abundant in NLP tasks.Thus, Raghavan et al (2006) propose an activelearning method in which, while classifying a train-ing document, the annotator also identifies some fea-tures of that document as particularly relevant.
E.g.,the annotator might highlight particular unigrams ashe or she reads the document.
In their proposal, afeature that is highlighted in any document is as-sumed to be globally more relevant.
Its dimensionin feature space is scaled by a factor of 10 so thatthis feature has more influence on distances or innerproducts, and hence on the learned classifier.1.4 Concerns about marking featuresDespite the success of the above work, we haveseveral concerns about asking annotators to identifyglobally relevant features.First, a feature in isolation really does not have awell-defined worth.
A feature may be useful only inconjunction with other features,2 or be useful onlyto the extent that other correlated features are notselected to do the same work.Second, it is not clear how an annotator wouldeasily view and highlight features in context, ex-cept for the simplest feature sets.
In the phraseApple shares up 3%, there may be several fea-tures that fire on the substring Apple?respondingto the string Apple, its case-invariant form apple,its lemma apple- (which would also respond to ap-ples), its context-dependent sense Apple2, its partof speech noun, etc.
How does the annotator indi-cate which of these features are relevant?Third, annotating features is only appropriatewhen the feature set can be easily understood by ahuman.
This is not always the case.
It would be hardfor annotators to read, write, or evaluate a descrip-tion of a complex syntactic configuration in NLP ora convolution filter in machine vision.Fourth, traditional annotation efforts usually try toremain agnostic about the machine learning methods2For example, a linear classifier can learn that most trainingexamples satisfyA?
B by setting ?A = ?5 and ?A?B = +5,but this solution requires selecting bothA andA?B as features.More simply, a polynomial kernel can consider the conjunctionA ?B only if both A and B are selected as features.and features to be used.
The project?s cost is justi-fied by saying that the annotations will be reused bymany researchers (perhaps in a ?shared task?
), whoare free to compete on how they tackle the learningproblem.
Unfortunately, feature annotation commitsto a particular feature set at annotation time.
Subse-quent research cannot easily adjust the definition ofthe features, or obtain annotation of new features.2 Annotating RationalesTo solve these problems, we propose that annotatorsshould not select features but rather mark relevantportions of the example.
In earlier work (Zaidan etal., 2007), we called these markings ?rationales.
?For example, when classifying a movie review aspositive or negative, the annotator would also high-light phrases that supported that judgment.
Figure 1shows two such rationales.A multi-annotator timing study (Zaidan et al,2007) found that highlighting rationale phraseswhile reading movie reviews only doubled annota-tion time, although annotators marked 5?11 ratio-nale substrings in addition to the simple binary class.The benefit justified the extra time.
Furthermore,much of the benefit could have been obtained by giv-ing rationales for only a fraction of the reviews.In the visual domain, when classifying an im-age as containing a zoo, the annotator might circlesome animals or cages and the sign reading ?Zoo.
?The Peekaboom game (von Ahn et al, 2006) was infact built to elicit such approximate yet relevant re-gions of images.
Further scenarios were discussed in(Zaidan et al, 2007): rationale annotation for namedentities, linguistic relations, or handwritten digits.Annotating rationales does not require the anno-tator to think about the feature space, nor even toknow anything about it.
Arguably this makes an-notation easier and more flexible.
It also preservesthe reusability of the annotated data.
Anyone is freeto reuse our collected rationales (section 4) to aidin learning a classifier with richer features, or a dif-ferent kind of classifier altogether, using either ourprocedures or novel procedures.3 Modeling Rationale AnnotationsAs rationales are more indirect than explicit features,they present a trickier machine learning problem.32We wish to learn the parameters ?
of some classi-fier.
How can the annotator?s rationales help us todo this without many training examples?
We willhave to exploit a presumed relationship between therationales and the optimal value of ?
(i.e., the valuethat we would learn on an infinite training set).This paper exploits an explicit, parametric modelof that relationship.
The model?s parameters ?
areintended to capture what that annotator is doingwhen he or she marks rationales.
Most importantly,they capture how he or she is influenced by the true?.
Given this, our learning method will prefer valuesof ?
that would adequately explain the rationales (aswell as the training classifications).3.1 A generative approachFor concreteness, we will assume that the task isdocument classification.
Our training data consistsof n triples {(x1, y1, r1), ..., (xn, yn, rn)}), where xiis a document, yi is its annotated class, and ri is itsrationale markup.
At test time we will have to pre-dict yn+1 from xn+1, without any rn+1.We propose to jointly choose parameter vectors ?and ?
to maximize the following regularized condi-tional likelihood:3n?i=1p(yi, ri | xi, ?, ?)
?
pprior(?, ?)
(1)def=n?i=1p?
(yi | xi) ?
p?
(ri | xi, yi, ?)
?
pprior(?, ?
)Here we are trying to model all the annotations, bothyi and ri.
The first factor predicts yi using an ordi-nary probabilistic classifier p?, while the novel sec-ond factor predicts ri using a model p?
of how an-notators generate the rationale annotations.The crucial point is that the second factor dependson ?
(since ri is supposed to reflect the relation be-tween xi and yi that is modeled by ?).
As a result,the learner has an incentive to modify ?
in a waythat increases the second factor, even if this some-what decreases the first factor on training data.43It would be preferable to integrate out ?
(and even ?
), butmore difficult.4Interestingly, even examples where the annotation yi iswrong or unhelpful can provide useful information about ?
viathe pair (yi, ri).
Two annotators marking the same movie re-view might disagree on whether it is overall a positive or nega-After training, one should simply use the first fac-tor p?
(y | x) to classify test documents x.
The sec-ond factor is irrelevant for test documents, since theyhave not been annotated with rationales r.The second factor may likewise be omitted for anytraining documents i that have not been annotatedwith rationales, as there is no ri to predict in thosecases.
In the extreme case where no documents areannotated with rationales, equation (1) reduces tothe standard training procedure.3.2 Noisy channel design of rationale modelsLike ordinary class annotations, rationale annota-tions present us with a ?credit assignment problem,?albeit a smaller one that is limited to features that fire?in the vicinity?
of the rationale r. Some of these?-features were likely responsible for the classifica-tion y and hence triggered the rationale.
Other such?-features were just innocent bystanders.Thus, the interesting part of our model is p?
(r |x, y, ?
), which models the rationale annotation pro-cess.
The rationales r reflect ?, but in noisy ways.Taking this noisy channel idea seriously, p?
(r |x, y, ?)
should consider two questions when assess-ing whether r is a plausible set of rationales given?.
First, it needs a ?language model?
of rationales:does r consist of rationales that are well-formed apriori, i.e., before ?
is considered?
Second, it needsa ?channel model?
: does r faithfully signal the fea-tures of ?
that strongly support classifying x as y?If a feature contributes heavily to the classificationof document x as class y, then the channel modelshould tell us which parts of document x tend to behighlighted as a result.The channel model must know about the partic-ular kinds of features that are extracted by f andscored by ?.
Suppose the feature not .
.
.
gripping,5with weight ?h, is predictive of the annotated class y.This raises the probabilities of the annotator?s high-lighting each of various words, or combinations ofwords, in a phrase like not the most gripping ban-quet on film.
The channel model parameters in ?tive review?but the second factor still allows learning positivefeatures from the first annotator?s positive rationales, and nega-tive features from the second annotator?s negative rationales.5Our current experiments use only unigram features, tomatch past work, but we use this example to outline how ourapproach generalizes to complex linguistic (or visual) features.33should specify how much each of these probabilitiesis raised, based on the magnitude of ?h ?
R, theclass y, and the fact that the feature is an instanceof the template <Neg> .
.
.<Adjective>.
(Thus, ?has no parameters specific to the word gripping; itis a low-dimensional vector that only describes theannotator?s general style in translating ?
into r.)The language model, however, is independent ofthe feature set ?.
It models what rationales tend tolook like in the input domain?e.g., documents orimages.
In the document case, ?
should describe:How frequent and how long are typical rationales?Do their edges tend to align with punctuation or ma-jor syntactic boundaries in x?
Are they rarer in themiddle of a document, or in certain documents?6Thanks to the language model, we do not need toposit high ?
features to explain every word in a ratio-nale.
The language model can ?explain away?
somewords as having been highlighted only because thisannotator prefers not to end a rationale in mid-phrase, or prefers to sweep up close-together fea-tures with a single long rationale rather than manyshort ones.
Similarly, the language model can helpexplain why some words, though important, mightnot have been included in any rationale of r.If there are multiple annotators, one can learn dif-ferent ?
parameters for each annotator, reflectingtheir different annotation styles.7 We found this tobe useful (section 8.2).We remark that our generative modeling approach(equation (1)) would also apply if r were not ratio-nale markup, but some other kind of so-called ?sideinformation,?
such as the feature annotations dis-cussed in section 1.
For example, Raghavan et al(2006) assume that if feature h is relevant?a bi-6Our current experiments do not model this last point.
How-ever, we imagine that if the document only has a few ?-featuresthat support the classification, the annotator will probably markmost of them, whereas if such features are abundant, the anno-tator may lazily mark only a few of the strongest ones.
A simpleapproach would equip ?
with a different ?bias?
or ?threshold?parameter ?x for each rationale training document x, to mod-ulate the a priori probability of marking a rationale in x. Byfitting this bias parameter, we deduce how lazy the annotatorwas (for whatever reason) on document x.
If desired, a prioron ?x could consider whether x has many strong ?-features,whether the annotator has recently had a coffee break, etc.7Given insufficient rationale data to recover some annota-tor?s ?well, one could smooth using data from other annotators.But in our situation, ?
had relatively few parameters to learn.nary distinction?iff it was selected in at least onedocument.
But it might be more informative to ob-serve that h was selected in 3 of the 10 documentswhere it appeared, and to predict this via a modelp?
(3 of 10 | ?h), where ?
describes (e.g.)
how to de-rive a binomial parameter nonlinearly from ?h.
Thisapproach would not how often h was marked and in-fer how relevant is feature h (i.e., infer ?h).
In thiscase, p?
is a simple channel that transforms relevantfeatures into direct indicators of the feature.
Ourside information merely requires a more complextransformation?from relevant features into well-formed rationales, modulated by documents.4 Experimental Data: Movie ReviewsIn Zaidan et al (2007), we introduced the ?MovieReview Polarity Dataset Enriched with AnnotatorRationales.
?8 It is based on the dataset of Pang andLee (2004),9 which consists of 1000 positive and1000 negative movie reviews, tokenized and dividedinto 10 folds (F0?F9).
All our experiments use F9as their final blind test set.The enriched dataset adds rationale annotationsproduced by an annotator A0, who annotated foldsF0?F8 of the movie review set with rationales (in theform of textual substrings) that supported the gold-standard classifications.
We will use A0?s data todetermine the improvement of our method over a(log-linear) baseline model without rationales.
Wealso use A0 to compare against the ?masking SVM?method and SVM baseline of Zaidan et al (2007).Since ?
can be tuned to a particular annotator, wewould also like to know how well this works withdata from annotators other than A0.
We randomlyselected 100 reviews (50 positive and 50 negative)and collected both class and rationale annotationdata from each of six new annotators A3?A8,10 fol-lowing the same procedures as (Zaidan et al, 2007).We report results using only data from A3?A5, sincewe used the data from A6?A8 as development datain the early stages of our work.We use this new rationale-enriched dataset8 to de-termine if our method works well across annotators.We will only be able to carry out that comparison8Available at http://cs.jhu.edu/?ozaidan/rationales.9Polarity dataset version 2.0.10We avoid annotator names A1?A2, which were alreadyused in (Zaidan et al, 2007).34Figure 1: Rationales as sequence an-notation: the annotator highlightedtwo textual segments as rationales fora positive class.
Highlighted words in~x are tagged I in ~r, and other wordsare tagged O.
The figure also showssome ?-features.
For instance, gO(,)-Iis a count of O-I transitions that occurwith a comma as the left word.
Noticealso that grel is the sum of the under-lined values.at small training set sizes, due to limited data fromA3?A8.
The larger A0 dataset will still allow us toevaluate our method on a range of training set sizes.5 Detailed Models5.1 Modeling class annotations with p?We define the basic classifier p?
in equation (1) to bea standard conditional log-linear model:p?
(y | x)def=exp(~?
?
~f(x, y))Z?
(x)def=u(x, y)Z?
(x)(2)where ~f(?)
extracts a feature vector from a classifieddocument, ~?
are the corresponding weights of thosefeatures, and Z?
(x)def=?y u(x, y) is a normalizer.We use the same set of binary features as in pre-vious work on this dataset (Pang et al, 2002; Pangand Lee, 2004; Zaidan et al, 2007).
Specifically, letV = {v1, ..., v17744} be the set of word types withcount ?
4 in the full 2000-document corpus.
Definefh(x, y) to be y if vh appears at least once in x, and0 otherwise.
Thus ?
?
R17744, and positive weightsin ?
favor class label y = +1 and equally discouragey = ?1, while negative weights do the opposite.This standard unigram feature set is linguisticallyimpoverished, but serves as a good starting point forstudying rationales.
Future work should considermore complex features and how they are signaled byrationales, as discussed in section 3.2.5.2 Modeling rationale annotations with p?The rationales collected in this task are textual seg-ments of a document to be classified.
The docu-ment itself is a word token sequence ~x = x1, ..., xM .We encode its rationales as a corresponding tag se-quence ~r = r1, ..., rM , as illustrated in Figure 1.Here rm ?
{I, O} according to whether the tokenxm is in a rationale (i.e., xm was at least partly high-lighted) or outside all rationales.
x1 and xM arespecial boundary symbols, tagged with O.We predict the full tag sequence ~r at once usinga conditional random field (Lafferty et al, 2001).
ACRF is just another conditional log-linear model:p?
(r |x, y, ~?)def=exp(~?
?
~g(r, x, y, ~?))Z?
(x, y, ~?
)def=u(r, x, y, ~?)Z?
(x, y, ~?
)where ~g(?)
extracts a feature vector, ~?
are thecorresponding weights of those features, andZ?
(x, y, ~?
)def=?r u(r, x, y,~?)
is a normalizer.As usual for linear-chain CRFs, ~g(?)
extracts twokinds of features: first-order ?emission?
features thatrelate rm to (xm, y, ?
), and second-order ?transi-tion?
features that relate rm to rm?1 (although someof these also look at x).These two kinds of features respectively capturethe ?channel model?
and ?language model?
of sec-tion 3.2.
The former says rm is I because xm isassociated with a relevant ?-feature.
The latter saysrm is I simply because it is next to another I.5.3 Emission ?-features (?channel model?
)Recall that our ?-features (at present) correspond tounigrams.
Given (~x, y, ~?
), let us say that a unigramw ?
~x is relevant, irrelevant, or anti-relevant ify ?
?w is respectively 0, ?
0, or 0.
That is, wis relevant if its presence in x strongly supports theannotated class y, and anti-relevant if its presencestrongly supports the opposite class ?y.35Figure 2: Thefunction family Bsin equation (3),shown for s ?
{10, 2,?2,?10}.We would like to learn the extent ?rel to whichannotators try to include relevant unigrams in theirrationales, and the (usually lesser) extent ?antirel towhich they try to exclude anti-relevant unigrams.This will help us infer ~?
from the rationales.The details are as follows.
?rel and ?antirel are theweights of two emission features extracted by ~g:grel(~x, y, ~r, ~?
)def=M?m=1I(rm = I) ?B10(y ?
?xm)gantirel(~x, y, ~r, ~?
)def=M?m=1I(rm = I) ?B?10(y ?
?xm)Here I(?)
denotes the indicator function, returning1 or 0 according to whether its argument is true orfalse.
Relevance and negated anti-relevance are re-spectively measured by the differentiable nonlinearfunctions B10 and B?10, which are defined byBs(a) = (log(1 + exp(a ?
s))?
log(2))/s (3)and graphed in Figure 2.
Sample values of B10 andgrel are shown in Figure 1.How does this work?
The grel feature is a sumover all unigrams in the document ~x.
It does not firestrongly on the irrelevant or anti-relevant unigrams,since B10 is close to zero there.11 But it fires posi-tively on relevant unigrams w if they are tagged withI, and the strength of such firing increases approxi-mately linearly with ?w.
Since the weight ?rel > 0 inpractice, this means that raising a relevant unigram?s?w (if y = +1) will proportionately raise its log-odds of being tagged with I. Symmetrically, since?antirel > 0 in practice, lowering an anti-relevant un-igram?s ?w (if y = +1) will proportionately lower11B10 sets the threshold for relevance to be about 0.
Onecould also include versions of the grel feature that set a higherthreshold, using B10(y ?
?xm ?
threshold).its log-odds of being tagged with I, though not nec-essarily at the same rate as for relevant unigrams.12Should ?
also include traditional CRF emis-sion features, which would recognize that particularwords like great tend to be tagged as I?
No!
Suchfeatures would undoubtedly do a better job predict-ing the rationales and hence increasing equation (1).However, crucially, our true goal is not to predictthe rationales but to recover the classifier parame-ters ?.
Thus, if great tends to be highlighted, thenthe model should not be permitted to explain thisdirectly by increasing some feature ?great, but onlyindirectly by increasing ?great.
We therefore permitour rationale prediction model to consider only thetwo emission features grel and gantirel, which see thewords in ~x only through their ?-values.5.4 Transition ?-features (?language model?
)Annotators highlight more than just the relevant un-igrams.
(After all, they aren?t told that our current?-features are unigrams.)
They tend to mark fullphrases, though perhaps taking care to exclude anti-relevant portions.
?models these phrases?
shape, viaweights for several ?language model?
features.Most important are the 4 traditional CRF tag tran-sition features gO-O, gO-I, gI-I, gI-O.
For example,gO-I counts the number of O-to-I transitions in ~r(see Figure 1).
Other things equal, an annotator withhigh ?O-I is predicted to have many rationales per1000 words.
And if ?I-I is high, rationales are pre-dicted to be long phrases (including more irrelevantunigrams around or between the relevant ones).We also learn more refined versions of these fea-tures, which consider how the transition probabil-ities are influenced by the punctuation and syntaxof the document ~x (independent of ~?).
These re-fined features are more specific and hence moresparsely trained.
Their weights reflect deviationsfrom the simpler, ?backed-off?
transition featuressuch as gO-I.
(Again, see Figure 1 for examples.
)Conditioning on left word.
A feature of the formgt1(v)-t2 is specified by a pair of tag types t1, t2 ?
{I,O} and a vocabulary word type v. It counts the12If the two rates are equal (?rel = ?antirel), we get a simplermodel in which the log-odds change exactly linearly with ?w foreach w, regardless of w?s relevance/irrelevance/anti-relevance.This follows from the fact thatBs(a)+B?s(a) simplifies to a.36number of times an t1?t2 transition occurs in ~r con-ditioned on v appearing as the first of the two wordtokens where the transition occurs.
Our experimentsinclude gt1(v)-t2 features that tie I-O and O-I tran-sitions to the 4 most frequent punctuation marks v(comma, period, ?, !
).Conditioning on right word.
A feature gt1-t2(v)is similar, but v must appear as the second of thetwo word tokens where the transition occurs.
Againhere, we use gt1-t2(v) features that tie I-O and O-Itransitions to the four punctuation marks mentionedabove.
We also include five features that tie O-Itransitions to the words no, not, so, very, and quite,since in our development data, those words weremore likely than others to start rationales.13Conditioning on syntactic boundary.
We parsedeach rationale-annotated training document (noparsing is needed at test time).14 We then markedeach word bigram x1-x2 with three nonterminals:NEnd is the nonterminal of the largest constituentthat contains x1 and not x2, NStart is the nontermi-nal of the largest constituent that contains x2 andnot x1, and NCross is the nonterminal of the smallestconstituent that contains both x1 and x2.For a nonterminalN and pair of tag types (t1, t2),we define three features, gt1-t2/E=N , gt1-t2/S=N ,and gt1-t2/C=N , which count the number of timesa t1-t2 transition occurs in ~r with N matching theNEnd, NStart, or NCross nonterminal, respectively.Our experiments include these features for 11 com-mon nonterminal types N (DOC, TOP, S, SBAR,FRAG, PRN, NP, VP, PP, ADJP, QP).6 Training: Joint Optimization of ?
and ?To train our model, we use L-BFGS to locally max-imize the log of the objective function (1):1513These are the function words with count ?
40 in a randomsample of 100 documents, and which were associated with theO-I tag transition at more than twice the average rate.
We donot use any other lexical ?-features that reference ~x, for fear thatthey would enable the learner to explain the rationales withoutchanging ?
as desired (see the end of section 5.3).14We parse each sentence with the Collins parser (Collins,1999).
Then the document has one big parse tree, whose root isDOC, with each sentence being a child of DOC.15One might expect this function to be convex because p?
andp?
are both log-linear models with no hidden variables.
How-ever, log p?
(ri | xi, yi, ?)
is not necessarily convex in ?.n?i=1log p?
(yi | xi)?12?2???
?2+C(n?i=1log p?
(ri | xi, yi, ?))?12?2???
?2 (4)This defines pprior from (1) to be a standard diago-nal Gaussian prior, with variances ?2?
and ?2?
for thetwo sets of parameters.
We optimize ?2?
in our ex-periments.
As for ?2?, different values did not affectthe results, since we have a large number of {I,O}rationale tags to train relatively few ?
weights; sowe simply use ?2?
= 1 in all of our experiments.Note the new C factor in equation (4).
Our ini-tial experiments showed that optimizing equation (4)without C led to an increase in the likelihood of therationale data at the expense of classification accu-racy, which degraded noticeably.
This is becausethe second sum in (4) has a much larger magnitudethan the first: in a set of 100 documents, it predictsaround 74,000 binary {I,O} tags, versus the onehundred binary class labels.
While we are willingto reduce the log-likelihood of the training classifi-cations (the first sum) to a certain extent, focusingtoo much on modeling rationales (the second sum)is clearly not our ultimate goal, and so we optimizeC on development data to achieve some balance be-tween the two terms of equation (4).
Typical valuesof C range from 1300 to150 .16We perform alternating optimization on ?
and ?:1.
Initialize ?
to maximize equation (4) but withC = 0 (i.e.
based only on class data).2.
Fix ?, and find ?
that maximizes equation (4).3.
Fix ?, and find ?
that maximizes equation (4).4.
Repeat 2 and 3 until convergence.The L-BFGS method requires calculating the gra-dient of the objective function (4).
The partialderivatives with respect to components of ?
and ?involve calculating expectations of the feature func-tions, which can be computed in linear time (withrespect to the size of the training set) using theforward-backward algorithm for CRFs.
The par-tial derivatives also involve the derivative of (3),to determine how changing ?
will affect the firingstrength of the emission features grel and gantirel.16C also balances our confidence in the classifications yagainst our confidence in the rationales r; either may be noisy.377 Experimental ProceduresWe report on two sets of experiments.
In the firstset, we use the annotation data that A3?A5 providedfor the small set of 100 documents (as well as thedata from A0 on those same 100 documents).
Inthe second set, we used A0?s abundant annotationdata to evaluate our method with training set sizes upto 1600 documents, and compare it with three othermethods: log-linear baseline, SVM baseline, and theSVM masking method of (Zaidan et al, 2007).7.1 Learning curvesThe learning curves reported in section 8.1 are gen-erated exactly as in (Zaidan et al, 2007).
Each curveshows classification accuracy at training set sizesT = 1, 2, ..., 9 folds (i.e.
200, 400, ..., 1600 trainingdocuments).
For a given size T , the reported accu-racy is an average of 9 experiments with differentsubsets of the entire training set, each of size T :198?i=0acc(F9 | Fi+1 ?
.
.
.
?
Fi+T ) (5)where Fj denotes the fold numbered j mod 9, andacc(F9 | Y ) means classification accuracy on theheld-out test set F9 after training on set Y .We use an appropriate paired permutation test, de-tailed in (Zaidan et al, 2007), to test differences in(5).
We call a difference significant at p < 0.05.7.2 Comparison to ?masking SVM?
methodWe compare our method to the ?masking SVM?method of (Zaidan et al, 2007).
Briefly, that methodused rationales to construct several so-called con-trast examples from every training example.
A con-trast example is obtained by ?masking out?
one ofthe rationales highlighted to support the training ex-ample?s class.
A good classifier should have moretrouble on this modified example.
Hence, Zaidan etal.
(2007) required the learned SVM to classify eachcontrast example with a smaller margin than the cor-responding original example (and did not require itto be classified correctly).The masking SVM learner relies on a simple geo-metric principle; is trivial to implement on top of anexisting SVM learner; and works well.
However, webelieve that the generative method we present here ismore interesting and should apply more broadly.Figure 3: Classification accuracy curves for the 4 meth-ods: the two baseline learners that only utilize class data,and the two learners that also utilize rationale annota-tions.
The SVM curves are from (Zaidan et al, 2007).First, the masking method is specific to improvingan SVM learner, whereas our method can be used toimprove any classifier by adding a rationale-basedregularizer (the second half of equation (4)) to itsobjective function during training.More important, there are tasks where it is unclearhow to generate contrast examples.
For the moviereview task, it was natural to mask out a rationaleby pretending its words never occurred in the doc-ument.
After all, most word types do not appear inmost documents, so it is natural to consider the non-presence of a word as a ?default?
state to which wecan revert.
But in an image classification task, howshould one modify the image?s features to ignoresome spatial region marked as a rationale?
There isusually no natural ?default?
value to which we couldset the pixels.
Our method, on the other hand, elim-inates contrast examples altogether.8 Experimental Results and Analysis8.1 The added benefit of rationalesFig.
3 shows learning curves for four methods.
Alog-linear model shows large and significant im-provements, at all training sizes, when we incor-porate rationales into its training via equation (4).Moreover, the resulting classifier consistently out-performs17 prior work, the masking SVM, whichstarts with a slightly better baseline classifier (anSVM) but incorporates the rationales more crudely.17Differences are not significant at sizes 200, 1000, and 1600.38size A0 A3 A4 A5SVM baseline 100 72.0 72.0 72.0 70.0SVM+contrasts 100 75.0 73.0 74.0 72.0Log-linear baseline 100 71.0 73.0 71.0 70.0Log-linear+rats 100 76.0 76.0 77.0 74.0SVM baseline 20 63.4 62.2 60.4 62.6SVM+contrasts 20 65.4 63.4 62.4 64.8Log-linear baseline 20 63.0 62.2 60.2 62.4Log-linear+rats 20 65.8 63.6 63.4 64.8Table 1: Accuracy rates using each annotator?s data.
In agiven column, a value in italics is not significantly differ-ent from the highest value in that column, which is bold-faced.
The size=20 results average over 5 experiments.To confirm that we could successfully model an-notators other than A0, we performed the samecomparison for annotators A3?A5; each had pro-vided class and rationale annotations on a small 100-document training set.
We trained a separate ?
foreach annotator.
Table 1 shows improvements overbaseline, usually significant, at 2 training set sizes.8.2 AnalysisExamining the learned weights ~?
gives insight intoannotator behavior.
High weights include I-O andO-I transitions conditioned on punctuation, e.g.,?I(.
)-O = 3.55,18 as well as rationales ending at theend of a major phrase, e.g., ?I-O/E=VP = 1.88.The large emission feature weights, e.g., ?rel =14.68 and ?antirel = 15.30, tie rationales closely to?
values, as hoped.
For example, in Figure 1, theword w = succeeds, with ?w = 0.13, drives upp(I)/p(O) by a factor of 7 (in a positive document)relative to a word with ?w = 0.In fact, feature ablation experiments showed thatalmost all the classification benefit from rationalescan be obtained by using only these 2 emission?-features and the 4 unconditioned transition ?-features.
Our full ?
(115 features) merely improvesour ability to predict the rationales (whose likeli-hood does increase significantly with more features).We also checked that annotators?
styles differenough that it helps to tune ?
to the ?target?
annota-torAwho gave the rationales.
Table 3 shows that a ?model trained onA?s own rationales does best at pre-dicting new rationales fromA.
Table 2 shows that as18When trained on folds F4?F8 with A0?s rationales.
?A0 ?A3 ?A4 ?A5 Baseline?A0 76.0 73.0 74.0 73.0 71.0?A3 73.0 76.0 74.0 73.0 73.0?A4 75.0 73.0 77.0 74.0 71.0?A5 74.0 71.0 72.0 74.0 70.0Table 2: Accuracy rate for an annotator?s ?
(rows) ob-tained when using some other annotator?s ?
(columns).Notice that the diagonal entries and the baseline columnare taken from rows of Table 1 (size=100).Trivial?A0 ?A3 ?A4 ?A5 model?L(rA0) 0.073 0.086 0.077 0.088 0.135?L(rA3) 0.084 0.068 0.071 0.068 0.130?L(rA4) 0.088 0.084 0.075 0.085 0.153?L(rA5) 0.058 0.044 0.047 0.044 0.111Table 3: Cross-entropy per tag of rationale annotations~r for each annotator (rows), when predicted from thatannotator?s ~x and ~?
via a possibly different annotator?s?
(columns).
For comparison, the trivial model is a bi-gram model of ~r, which is trained on the target annotatorbut ignores ~x and ~?.
5-fold cross-validation on the 100-document set was used to prevent testing on training data.a result, classification performance on the test set isusually best if it wasA?s own ?
that was used to helplearn ?
from A?s rationales.
In both cases, however,a different annotator?s ?
is better than nothing.9 ConclusionsWe have demonstrated a effective method for elic-iting extra knowledge from naive annotators, inthe form of lightweight ?rationales?
for their an-notations.
By explicitly modeling the annotator?srationale-marking process, we are able to infer a bet-ter model of the original annotations.We showed that our method performs signifi-cantly better than two strong baseline classifiers,and also outperforms our previous discriminativemethod for exploiting rationales (Zaidan et al,2007).
We also saw that it worked across four anno-tators who have different rationale-marking styles.In future, we are interested in new domains thatcan adaptively solicit rationales for some or alltraining examples.
Our new method, being essen-tially Bayesian inference, is potentially extensible tomany other situations?other tasks, classifier archi-tectures, and more complex features.39ReferencesEric Brill and Grace Ngai.
1999.
Man [and woman] vs.machine: A case study in base noun phrase learning.In Proceedings of the 37th ACL Conference.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.G.
Druck, G. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In Proceedings of ACM Special InterestGroup on Information Retrieval, (SIGIR).A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,Main Conference, pages 320?327, New York City,USA, June.
Association for Computational Linguis-tics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on MachineLearning.Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: Cost-efficient resource usage for basenoun phrase chunking.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 117?125, Hong Kong.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
of ACL, pages 271?278.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learningtechniques.
In Proc.
of EMNLP, pages 79?86.Hema Raghavan and James Allan.
2007.
An interactivealgorithm for asking and incorporating feature feed-back into support vector machines.
In Proceedings ofSIGIR.Hema Raghavan, Omid Madani, and Rosie Jones.
2006.Active learning on both features and instances.
Jour-nal of Machine Learning Research, 7:1655?1686,Aug.Luis von Ahn, Ruoran Liu, and Manuel Blum.
2006.Peekaboom: A game for locating objects.
In CHI?06: Proceedings of the SIGCHI Conference on Hu-man Factors in Computing Systems, pages 55?64.Omar Zaidan, Jason Eisner, and Christine Piatko.
2007.Using ?annotator rationales?
to improve machinelearning for text categorization.
In NAACL HLT 2007;Proceedings of the Main Conference, pages 260?267,April.40
