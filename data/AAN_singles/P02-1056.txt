An Integrated Architecture for Shallow and Deep ProcessingBerthold Crysmann, Anette Frank, Bernd Kiefer, Stefan Mu?ller,Gu?nter Neumann, Jakub Piskorski, Ulrich Scha?fer, Melanie Siegel, Hans Uszkoreit,Feiyu Xu, Markus Becker and Hans-Ulrich KriegerDFKI GmbHStuhlsatzenhausweg 3Saarbru?cken, Germanywhiteboard@dfki.deAbstractWe present an architecture for the integra-tion of shallow and deep NLP componentswhich is aimed at flexible combinationof different language technologies for arange of practical current and future appli-cations.
In particular, we describe the inte-gration of a high-level HPSG parsing sys-tem with different high-performance shal-low components, ranging from named en-tity recognition to chunk parsing and shal-low clause recognition.
The NLP com-ponents enrich a representation of natu-ral language text with layers of new XMLmeta-information using a single shareddata structure, called the text chart.
We de-scribe details of the integration methods,and show how information extraction andlanguage checking applications for real-world German text benefit from a deepgrammatical analysis.1 IntroductionOver the last ten years or so, the trend in application-oriented natural language processing (e.g., in thearea of term, information, and answer extraction)has been to argue that for many purposes, shallownatural language processing (SNLP) of texts canprovide sufficient information for highly accurateand useful tasks to be carried out.
Since the emer-gence of shallow techniques and the proof of theirutility, the focus has been to exploit these technolo-gies to the maximum, often ignoring certain com-plex issues, e.g.
those which are typically well han-dled by deep NLP systems.
Up to now, deep naturallanguage processing (DNLP) has not played a sig-nificant role in the area of industrial NLP applica-tions, since this technology often suffers from insuf-ficient robustness and throughput, when confrontedwith large quantities of unrestricted text.Current information extractions (IE) systemstherefore do not attempt an exhaustive DNLP analy-sis of all aspects of a text, but rather try to analyse or?understand?
only those text passages that containrelevant information, thereby warranting speed androbustness wrt.
unrestricted NL text.
What exactlycounts as relevant is explicitly defined by meansof highly detailed domain-specific lexical entriesand/or rules, which perform the required mappingsfrom NL utterances to corresponding domain knowl-edge.
However, this ?fine-tuning?
wrt.
a particularapplication appears to be the major obstacle whenadapting a given shallow IE system to another do-main or when dealing with the extraction of com-plex ?scenario-based?
relational structures.
In fact,(Appelt and Israel, 1997) have shown that the cur-rent IE technology seems to have an upper perfor-mance level of less than 60% in such cases.
It seemsreasonable to assume that if a more accurate analy-sis of structural linguistic relationships could be pro-vided (e.g., grammatical functions, referential rela-tionships), this barrier might be overcome.
Actually,the growing market needs in the wide area of intel-ligent information management systems seem to re-quest such a break-through.In this paper we will argue that the quality of cur-Computational Linguistics (ACL), Philadelphia, July 2002, pp.
441-448.Proceedings of the 40th Annual Meeting of the Association forrent SNLP-based applications can be improved byintegrating DNLP on demand in a focussed manner,and we will present a system that combines the fine-grained anaysis provided by HPSG parsing with ahigh-performance SNLP system into a generic andflexible NLP architecture.1.1 Integration ScenariosOwing to the fact that deep and shallow technologiesare complementary in nature, integration is a non-trivial task: while SNLP shows its strength in theareas of efficiency and robustness, these aspects areproblematic for DNLP systems.
On the other hand,DNLP can deliver highly precise and fine-grainedlinguistic analyses.
The challenge for integration isto combine these two paradigms according to theirvirtues.Probably the most straightforward way to inte-grate the two is an architecture in which shallow anddeep components run in parallel, using the results ofDNLP, whenever available.
While this kind of ap-proach is certainly feasible for a real-time applica-tion such as Verbmobil, it is not ideal for processinglarge quantities of text: due to the difference in pro-cessing speed, shallow and deep NLP soon run outof sync.
To compensate, one can imagine two possi-ble remedies: either to optimize for precision, or forspeed.
The drawback of the former strategy is thatthe overall speed will equal the speed of the slow-est component, whereas in case of the latter, DNLPwill almost always time out, such that overall preci-sion will hardly be distinguishable from a shallow-only system.
What is thus called for is an integrated,flexible architecture where components can play attheir strengths.
Partial analyses from SNLP can beused to identify relevant candidates for the focusseduse of DNLP, based on task or domain-specific crite-ria.
Furthermore, such an integrated approach opensup the possibility to address the issue of robustnessby using shallow analyses (e.g., term recognition)to increase the coverage of the deep parser, therebyavoiding a duplication of efforts.
Likewise, integra-tion at the phrasal level can be used to guide thedeep parser towards the most likely syntactic anal-ysis, leading, as it is hoped, to a considerable speed-up.shallowNLPcomponentsNLPdeepcomponents internal repr.layermultichartannot.XMLexternal repr.generic OOPcomponentinterfaceWHAMapplicationspecificationinput andresultFigure 1: The WHITEBOARD architecture.2 ArchitectureThe WHITEBOARD architecture defines a platformthat integrates the different NLP components by en-riching an input document through XML annota-tions.
XML is used as a uniform way of represent-ing and keeping all results of the various processingcomponents and to support a transparent softwareinfrastructure for LT-based applications.
It is knownthat interesting linguistic information ?especiallywhen considering DNLP?
cannot efficiently berepresented within the basic XML markup frame-work (?typed parentheses structure?
), e.g., linguisticphenomena like coreferences, ambiguous readings,and discontinuous constituents.
The WHITEBOARDarchitecture employs a distributed multi-level repre-sentation of different annotations.
Instead of trans-lating all complex structures into one XML docu-ment, they are stored in different annotation layers(possibly non-XML, e.g.
feature structures).
Hyper-links and ?span?
information together support effi-cient access between layers.
Linguistic informationof common interest (e.g.
constituent structure ex-tracted from HPSG feature structures) is available inXML format with hyperlinks to full feature struc-ture representations externally stored in correspond-ing data files.Fig.
1 gives an overview of the architecture ofthe WHITEBOARD Annotation Machine (WHAM).Applications feed the WHAM with input texts anda specification describing the components and con-figuration options requested.
The core WHAM en-gine has an XML markup storage (external ?offline?representation), and an internal ?online?
multi-levelannotation chart (index-sequential access).
Follow-ing the trichotomy of NLP data representation mod-els in (Cunningham et al, 1997), the XML markupcontains additive information, while the multi-levelchart contains positional and abstraction-based in-formation, e.g., feature structures representing NLPentities in a uniform, linguistically motivated form.Applications and the integrated components ac-cess the WHAM results through an object-orientedprogramming (OOP) interface which is designedas general as possible in order to abstract fromcomponent-specific details (but preserving shallowand deep paradigms).
The interfaces of the actu-ally integrated components form subclasses of thegeneric interface.
New components can be inte-grated by implementing this interface and specifyingDTDs and/or transformation rules for the chart.The OOP interface consists of iterators that walkthrough the different annotation levels (e.g., tokenspans, sentences), reference and seek operators thatallow to switch to corresponding annotations on adifferent level (e.g., give all tokens of the currentsentence, or move to next named entity startingfrom a given token position), and accessor meth-ods that return the linguistic information containedin the chart.
Similarily, general methods supportnavigating the type system and feature structures ofthe DNLP components.
The resulting output of theWHAM can be accessed via the OOP interface or asXML markup.The WHAM interface operations are not onlyused to implement NLP component-based applica-tions, but also for the integration of deep and shallowprocessing components itself.2.1 Components2.1.1 Shallow NL componentShallow analysis is performed by SPPC, a rule-based system which consists of a cascade ofweighted finite?state components responsible forperforming subsequent steps of the linguistic anal-ysis, including: fine-grained tokenization, lexico-morphological analysis, part-of-speech filtering,named entity (NE) recognition, sentence bound-ary detection, chunk and subclause recognition,see (Piskorski and Neumann, 2000; Neumann andPiskorski, 2002) for details.
SPPC is capable of pro-cessing vast amounts of textual data robustly and ef-ficiently (ca.
30,000 words per second in standardPC environment).
We will briefly describe the SPPCcomponents which are currently integrated with thedeep components.Each token identified by a tokenizer as a poten-tial word form is morphologically analyzed.
Foreach token, its lexical information (list of valid read-ings including stem, part-of-speech and inflectioninformation) is computed using a fullform lexiconof about 700,000 entries that has been compiled outfrom a stem lexicon of about 120,000 lemmas.
Af-ter morphological processing, POS disambiguationrules are applied which compute a preferred read-ing for each token, while the deep components canback off to all readings.
NE recognition is based onsimple pattern matching techniques.
Proper names(organizations, persons, locations), temporal expres-sions and quantities can be recognized with an av-erage precision of almost 96% and recall of 85%.Furthermore, a NE?specific reference resolution isperformed through the use of a dynamic lexiconwhich stores abbreviated variants of previously rec-ognized named entities.
Finally, the system splitsthe text into sentences by applying only few, buthighly accurate contextual rules for filtering implau-sible punctuation signs.
These rules benefit directlyfrom NE recognition which already performs re-stricted punctuation disambiguation.2.1.2 Deep NL componentThe HPSG Grammar is based on a large?scalegrammar for German (Mu?ller, 1999), which wasfurther developed in the VERBMOBIL project fortranslation of spoken language (Mu?ller and Kasper,2000).
After VERBMOBIL the grammar was adaptedto the requirements of the LKB/PET system (Copes-take, 1999), and to written text, i.e., extended withconstructions like free relative clauses that were ir-relevant in the VERBMOBIL scenario.The grammar consists of a rich hierarchy of5,069 lexical and phrasal types.
The core grammarcontains 23 rule schemata, 7 special verb move-ment rules, and 17 domain specific rules.
All ruleschemata are unary or binary branching.
The lexiconcontains 38,549 stem entries, from which more than70% were semi-automatically acquired from the an-notated NEGRA corpus (Brants et al, 1999).The grammar parses full sentences, but also otherkinds of maximal projections.
In cases where no fullanalysis of the input can be provided, analyses offragments are handed over to subsequent modules.Such fragments consist of maximal projections orsingle words.The HPSG analysis system currently integratedin the WHITEBOARD system is PET (Callmeier,2000).
Initially, PET was built to experimentwith different techniques and strategies to processunification-based grammars.
The resulting sys-tem provides efficient implementations of the bestknown techniques for unification and parsing.As an experimental system, the original designlacked open interfaces for flexible integration withexternal components.
For instance, in the beginningof the WHITEBOARD project the system only ac-cepted fullform lexica and string input.
In collabora-tion with Ulrich Callmeier the system was extended.Instead of single word input, input items can nowbe complex, overlapping and ambiguous, i.e.
essen-tially word graphs.
We added dynamic creation ofatomic type symbols, e.g., to be able to add arbitrarysymbols to feature structures.
With these enhance-ments, it is possible to build flexible interfaces toexternal components like morphology, tokenization,named entity recognition, etc.3 IntegrationMorphology and POS The coupling between themorphology delivered by SPPC and the input neededfor the German HPSG was easily established.
Themorphological classes of German are mapped ontoHPSG types which expand to small feature struc-tures representing the morphological information ina compact way.
A mapping to the output of SPPCwas automatically created by identifying the corre-sponding output classes.Currently, POS tagging is used in two ways.
First,lexicon entries that are marked as preferred by theshallow component are assigned higher priority thanthe rest.
Thus, the probability of finding the cor-rect reading early should increase without excludingany reading.
Second, if for an input item no entry isfound in the HPSG lexicon, we automatically createa default entry, based on the part?of?speech of thepreferred reading.
This increases robustness, whileavoiding increase in ambiguity.Named Entity Recognition Writing HPSG gram-mars for the whole range of NE expressions etc.
isa tedious and not very promising task.
They typi-cally vary across text sorts and domains, and wouldrequire modularized subgrammars that can be easilyexchanged without interfering with the general core.This can only be realized by using a type interfacewhere a class of named entities is encoded by a gen-eral HPSG type which expands to a feature structureused in parsing.
We exploit such a type interface forcoupling shallow and deep processing.
The classesof named entities delivered by shallow processingare mapped to HPSG types.
However, some fine-tuning is required whenever deep and shallow pro-cessing differ in the amount of input material theyassign to a named entity.An alternative strategy is used for complex syn-tactic phrases containing NEs, e.g., PPs describ-ing time spans etc.
It is based on ideas fromExplanation?based Learning (EBL, see (Tadepalliand Natarajan, 1996)) for natural language analy-sis, where analysis trees are retrieved on the basisof the surface string.
In our case, the part-of-speechsequence of NEs recognised by shallow analysis isused to retrieve pre-built feature structures.
Thesestructures are produced by extracting NEs from acorpus and processing them directly by the deepcomponent.
If a correct analysis is delivered, thelexical parts of the analysis, which are specific forthe input item, are deleted.
We obtain a sceletalanalysis which is underspecified with respect to theconcrete input items.
The part-of-speech sequenceof the original input forms the access key for thisstructure.
In the application phase, the underspeci-fied feature structure is retrieved and the empty slotsfor the input items are filled on the basis of the con-crete input.The advantage of this approach lies in the moreelaborate semantics of the resulting feature struc-tures for DNLP, while avoiding the necessity ofadding each and every single name to the HPSG lex-icon.
Instead, good coverage and high precision canbe achieved using prototypical entries.Lexical Semantics When first applying the origi-nal VERBMOBIL HPSG grammar to business newsarticles, the result was that 78.49% of the miss-ing lexical items were nouns (ignoring NEs).
Inthe integrated system, unknown nouns and NEs canbe recognized by SPPC, which determines morpho-syntactic information.
It is essential for the deep sys-tem to associate nouns with their semantic sorts bothfor semantics construction, and for providing se-mantically based selectional restrictions to help con-straining the search space during deep parsing.
Ger-maNet (Hamp and Feldweg, 1997) is a large lexicaldatabase, where words are associated with POS in-formation and semantic sorts, which are organized ina fine-grained hierarchy.
The HPSG lexicon, on theother hand, is comparatively small and has a morecoarse-grained semantic classification.To provide the missing sort information when re-covering unknown noun entries via SPPC, a map-ping from the GermaNet semantic classification tothe HPSG semantic classification (Siegel et al,2001) is applied which has been automatically ac-quired.
The training material for this learning pro-cess are those words that are both annotated with se-mantic sorts in the HPSG lexicon and with synsetsof GermaNet.
The learning algorithm computes amapping relevance measure for associating seman-tic concepts in GermaNet with semantic sorts in theHPSG lexicon.
For evaluation, we examined a cor-pus of 4664 nouns extracted from business newsthat were not contained in the HPSG lexicon.
2312of these were known in GermaNet, where they areassigned 2811 senses.
With the learned mapping,the GermaNet senses were automatically mapped toHPSG semantic sorts.
The evaluation of the map-ping accuracy yields promising results: In 76.52%of the cases the computed sort with the highest rel-evance probability was correct.
In the remaining20.70% of the cases, the correct sort was among thefirst three sorts.3.1 Integration on Phrasal LevelIn the previous paragraphs we described strategiesfor integration of shallow and deep processing wherethe focus is on improving DNLP in the domain oflexical and sub-phrasal coverage.We can conceive of more advanced strategies forthe integration of shallow and deep analysis at thelength cover- complete LP LR 0CB   2CBage match  40 100 80.4 93.4 92.9 92.1 98.9 40 99.8 78.6 92.4 92.2 90.7 98.5Training: 16,000 NEGRA sentencesTesting: 1,058 NEGRA sentencesFigure 2: Stochastic topological parsing: resultslevel of phrasal syntax by guiding the deep syntac-tic parser towards a partial pre-partitioning of com-plex sentences provided by shallow analysis sys-tems.
This strategy can reduce the search space, andenhance parsing efficiency of DNLP.Stochastic Topological Parsing The traditionalsyntactic model of topological fields divides basicclauses into distinct fields: so-called pre-, middle-and post-fields, delimited by verbal or senten-tial markers.
This topological model of Germanclause structure is underspecified or partial as tonon-sentential constituent boundaries, but providesa linguistically well-motivated, and theory-neutralmacrostructure for complex sentences.
Due to itslinguistic underpinning the topological model pro-vides a pre-partitioning of complex sentences that is(i) highly compatible with deep syntactic structuresand (ii) maximally effective to increase parsing ef-ficiency.
At the same time (iii) partiality regardingthe constituency of non-sentential material ensuresthe important aspects of robustness, coverage, andprocessing efficiency.In (Becker and Frank, 2002) we present a corpus-driven stochastic topological parser for German,based on a topological restructuring of the NEGRAcorpus (Brants et al, 1999).
For topological tree-bank conversion we build on methods and resultsin (Frank, 2001).
The stochastic topological parserfollows the probabilistic model of non-lexicalisedPCFGs (Charniak, 1996).
Due to abstraction fromconstituency decisions at the sub-sentential level,and the essentially POS-driven nature of topologi-cal structure, this rather simple probabilistic modelyields surprisingly high figures of accuracy and cov-erage (see Fig.2 and (Becker and Frank, 2002) formore detail), while context-free parsing guaranteesefficient processing.The next step is to elaborate a (partial) map-ping of shallow topological and deep syntactic struc-tures that is maximally effective for preference-gui-Topological Structure:CL-V2VF-TOPIC LK-FIN MF RK-tNN VVFIN ADV NN PREP NN VVFIN[ 	 [ 	 Peter] [  i?t] [ gerne Wu?rstchen mit Kartoffelsalat] [ ff -]]Peter eats happily sausages with potato saladDeep Syntactic Structure:[ fi [ fl Peter] [ffi[  i?t] [ 	 gerne [  [ fl Wu?rstchen [ fi mit [ fl Kartoffelsalat]]] [  ff -]]]]]Mapping:CL-V2 !
CP, VF-TOPIC !
XP, LK-FIN !
V, " LK-FIN MF RK-t #!
C?, " MF RK-t #fi!
VP, RK-t !
V-tFigure 3: Matching topological and deep syntactic structuresded deep syntactic analysis, and thus, efficiency im-provements in deep syntactic processing.
Such amapping is illustrated for a verb-second clause inFig.3, where matching constituents of topologicaland deep-syntactic phrase structure are indicated bycircled nodes.
With this mapping defined for all sen-tence types, we can proceed to the technical aspectsof integration into the WHITEBOARD architectureand XML text chart, as well as preference-drivenHPSG analysis in the PET system.4 ExperimentsAn evaluation has been started using the NEGRAcorpus, which contains about 20,000 newspaper sen-tences.
The main objectives are to evaluate the syn-tactic coverage of the German HPSG on newspapertext and the benefits of integrating deep and shallowanalysis.
The sentences of the corpus were used intheir original form without stripping, e.g.
parenthe-sized insertions.We extended the HPSG lexicon semi-automatically from about 10,000 to 35,000stems, which roughly corresponds to 350,000 fullforms.
Then, we checked the lexical coverageof the deep system on the whole corpus, whichresulted in 28.6% of the sentences being fullylexically analyzed.
The corresponding experimentwith the integrated system yielded an improvedlexical coverage of 71.4%, due to the techniquesdescribed in section 3.
This increase is not achievedby manual extension, but only through synergybetween the deep and shallow components.To test the syntactic coverage, we processed thesubset of the corpus that was fully covered lexically(5878 sentences) with deep analysis only.
The re-sults are shown in table 4 in the second column.
Inorder to evaluate the integrated system we processed20,568 sentences from the corpus without further ex-tension of the HPSG lexicon (see table 4, third col-umn).Deep Integrated# sentences 20,568avg.
sentence length 16.83avg.
lexical ambiguity 2.38 1.98avg.
# analyses 16.19 18.53analysed sentences 2,569 4,546lexical coverage 28.6% 71.4%overall coverage 12.5% 22.1%Figure 4: Evaluation of German HPSGAbout 10% of the sentences that were success-fully parsed by deep analysis only could not beparsed by the integrated system, and the number ofanalyses per sentence dropped from 16.2% to 8.6%,which indicates a problem in the morphology inter-face of the integrated system.
We expect better over-all results once this problem is removed.5 ApplicationsSince typed feature structures (TFS) in Whiteboardserve as both a representation and an interchangeformat, we developed a Java package (JTFS) thatimplements the data structures, together with thenecessary operations.
These include a lazy-copyingunifier, a subsumption and equivalence test, deepcopying, iterators, etc.
JTFS supports a dynamicconstruction of typed feature structures, which is im-portant for information extraction.5.1 Information ExtractionInformation extraction in Whiteboard benefits bothfrom the integration of the shallow and deep analy-sis results and from their processing methods.
Wechose management succession as our applicationdomain.
Two sets of template filling rules aredefined: pattern-based and unification-based rules.The pattern-based rules work directly on the outputdelivered by the shallow analysis, for example,(1) Nachfolger von 1 $fl%'&(*)*+ +-,./%10324person out 1 5 .This rule matches expressions like Nachfolgervon Helmut Kohl (successor of) which contains twostring tokens Nachfolger and von followed by a per-son name, and fills the slot of person outwith therecognized person name Helmut Kohl.
The pattern-based grammar yields good results by recognitionof local relationships as in (1).
The unification-based rules are applied to the deep analysis re-sults.
Given the fine-grained syntactic and seman-tic analysis of the HPSG grammar and its robust-ness (through SNLP integration), we decided to usethe semantic representation (MRS, see (Copestakeet al, 2001)) as additional input for IE.
The reasonis that MRSs express precise relationships betweenthe chunks, in particular, in constructions involving(combinations of) free word order, long distance de-pendencies, control and raising, or passive, whichare very difficult, if not impossible, to recognize fora pattern-based grammar.
E.g., the short sentence(2) illustrates a combination of free word order, con-trol, and passive.
The subject of the passive verbwurde gebeten is located in the middle field and isat the same time the subject of the infinitive verbzu u?bernehmen.
A deep (HPSG) analysis can recog-nize the dependencies quite easily, whereas a patternbased grammar cannot determine, e.g., for whichverb Peter Miscke or Dietmar Hopp is the subject.
(2) Peter Miscke following was Dietmar Hoppasked, the development sector to take over.PeterEntwicklungsabteilungMisckezuzufolgeu?bernehmen.wurde Dietmar Hoppgebeten, die?
According to Peter Miscke, Dietmar Hoppwas asked to take over the developmentsector.
?We employ typed feature structures (TFS) as ourmodelling language for the definition of scenariotemplate types and template element types.
There-fore, the template filling results from shallow anddeep analysis can be uniformly encoded in TFS.
As aside effect, we can easily adapt JTFS unification forthe template merging task, by interperting the par-tially filled templates from deep and shallow anal-ysis as constraints.
E.g., to extract the relevant in-formation from the above sentence, the followingunification-based rule can be applied:677778PERSON IN DIVISION 9MRS68PRED ?u?bernehmen?AGENT THEME 9:;:=<<<<;5.2 Language checkingAnother area where DNLP can support existingshallow-only tools is grammar and controlled lan-guage checking.
Due to the scarce distribution oftrue errors (Becker et al, to appear), there is a higha priori probability for false alarms.
As the num-ber of false alarms decides on user-acceptance, pre-cision is of utmost importance and cannot easilybe traded for recall.
Current controlled languagechecking systems for German, such as MULTILINT(http://www.iai.uni-sb.de/en/multien.html) or FLAG(http://flag.dfki.de), build exclusively on SNLP:while checking of local errors (e.g.
NP-internalagreement, prepositional case) can be performedquite reliably by such a system, error types involv-ing non-local dependencies, or access to grammati-cal functions are much harder to detect.
The use ofDNLP in this area is confronted with several system-atic problems: first, formal grammars are not alwaysavailable, e.g., in the case of controlled languages;second, erroneous sentences lie outside the languagedefined by the competence grammar, and third, dueto the sparse distribution of errors, a DNLP systemwill spend most of the time parsing perfectly well-formed sentences.
Using an integrated approach, ashallow checker can be used to cheaply identify ini-tial error candidates, while false alarms can be elim-inated based on the richer annotations provided bythe deep parser.6 DiscussionIn this paper we reported on an implemented sys-tem called WHITEBOARD which integrates differ-ent shallow components with a HPSG?based deepsystem.
The integration is realized through themetaphor of textual annotation.
To best of ourknowledge, this is the first implemented systemwhich integrates high-performance shallow process-ing with an advanced deep HPSG?based analysissystem.
There exists only very little other work thatconsiders integration of shallow and deep NLP usingan XML?based architecture, most notably (Groverand Lascarides, 2001).
However, their integrationefforts are largly limited to the level of POS tag in-formation.AcknowledgementsThis work was supported by a research grant fromthe German Federal Ministry of Education, Science,Research and Technology (BMBF) to the DFKIproject WHITEBOARD, FKZ: 01 IW 002.
Specialthanks to Ulrich Callmeier for his technical supportconcerning the integration of PET.ReferencesD.
Appelt and D. Israel.
1997.
Building information ex-traction systems.
Tutorial during the 5th ANLP, Wash-ington.M.
Becker and A. Frank.
2002.
A Stochastic TopologicalParser of German.
In Proceedings of COLING 2002,Teipei, Taiwan.M.
Becker, A. Bredenkamp, B. Crysmann, and J. Klein.to appear.
Annotation of error types for german news-group corpus.
In Anne Abeille?, editor, Treebanks:Building and Using Syntactically Annotated Corpora.Kluwer, Dordrecht.T.
Brants, W. Skut, and H. Uszkoreit.
1999.
SyntacticAnnotation of a German newspaper corpus.
In Pro-ceedings of the ATALA Treebank Workshop, pages 69?76, Paris, France.U.
Callmeier.
2000.
PET ?
A platform for experimenta-tion with efficient HPSG processing techniques.
Natu-ral Language Engineering, 6 (1) (Special Issue on Ef-ficient Processing with HPSG):99 ?
108.E.
Charniak.
1996.
Tree-bank Grammars.
In AAAI-96.Proceedings of the 13th AAAI, pages 1031?1036.
MITPress.A.
Copestake, A. Lascarides, and D. Flickinger.
2001.An algebra for semantic construction in constraint-based grammars.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2001), Toulouse, France.A.
Copestake.
1999.
The (new) LKB system.ftp://www-csli.stanford.edu/> aac/newdoc.pdf.H.
Cunningham, K. Humphreys, R. Gaizauskas, andY.
Wilks.
1997.
Software Infrastructure for Natu-ral Language Processing.
In Proceedings of the FifthANLP, March.A.
Frank.
2001.
Treebank Conversion.
Convertingthe NEGRA Corpus to an LTAG Grammar.
In Pro-ceedings of the EUROLAN Workshop on Multi-layerCorpus-based Analysis, pages 29?43, Iasi, Romania.C.
Grover and A. Lascarides.
2001.
XML-based datapreparation for robust deep parsing.
In Proceedings ofthe 39th ACL, pages 252?259, Toulouse, France.B.
Hamp and H. Feldweg.
1997.
Germanet - a lexical-semantic net for german.
In Proceedings of ACL work-shop Automatic Information Extraction and Buildingof Lexical Semantic Resources for NLP Applications,Madrid.S.
Mu?ller and W. Kasper.
2000.
HPSG analysis ofGerman.
In W. Wahlster, editor, Verbmobil: Founda-tions of Speech-to-Speech Translation, Artificial Intel-ligence, pages 238?253.
Springer-Verlag, Berlin Hei-delberg New York.S.
Mu?ller.
1999.
Deutsche Syntax deklarativ.
Head-Driven Phrase Structure Grammar fu?r das Deutsche.Max Niemeyer Verlag, Tu?bingen.G.
Neumann and J. Piskorski.
2002.
A shallow text pro-cessing core engine.
Computational Intelligence, toappear.J.
Piskorski and G. Neumann.
2000.
An intelligent textextraction and navigation system.
In Proceedings ofthe RIAO-2000.
Paris, April.M.
Siegel, F. Xu, and G. Neumann.
2001.
Customiz-ing germanet for the use in deep linguistic processing.In Proceedings of the NAACL 2001 Workshop Word-Net and Other Lexical Resources: Applications, Ex-tensions and Customizations, Pittsburgh,USA, July.P.
Tadepalli and B. Natarajan.
1996.
A formal frame-work for speedup learning from problems and solu-tions.
Journal of AI Research, 4:445 ?
475.
