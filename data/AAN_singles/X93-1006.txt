DOCUMENT DETECTIONDATA PREPARATIONDonna HarmanNational Institute of Standards and TechnologyGaithersburg, MD.
208991.
THE ENGLISH TEST COLLECTION1.1 IntroductionCritical to the success of TIPSTER was the creation of thetest collection.
Like most traditional retrieval collections,there are three distinct parts to this collection--the docu-ments, the queries or topics, and the relevance judgmentsor "right answers".
It was important to match all threeparts of the collection to the TIPSTER application.The document collection needed to reflect the corpusimagined to be seen by analysts.
This meant hat a verylarge collection was needed to test the scaling of the algo-rithms, including documents from many different domainsto test the domain independence of the algorithms.
Addi-tionally the documents selected needed to mirror the dif-ferent ypes of documents used in the TIPSTER applica-tion.
Specifically they had to have a varied length, a var-ied writing style, a varied level of editing and a varied vo-cabulary.
As a final requirement, the documents had tocover different fimeframes to show the effects of docu-ment date on the routing task.The topics for the TIPSTER collection were also designedto model some of the needs of analysts.
It was assumedthat the typical user of these retrieval systems was a dedi-cated searcher, not a novice searcher, and that the modelfor the application was one needing both the monitoringof data streams for information on specific topics (rout-ing), and the ability to do adhoc searches on archived atafor new topics.
It was also assumed that the users needthe ability to do both high precision and high recallsearches, and are willing to look at many documents andrepeatedly modify queries in order to get high recall.
Thetopics therefore were created to be very specific, but in-cluded both broad and narrow searching needs.
Many ofthe topics were created to test performance on specifictypes of searches.The relevance assessments were made by retired analystswho were asked to view the task as if they were address-ing a real information eed.
The narrative section of thetopic (described in more detail later) contained a cleardefinition of what made a document relevant, and the as-sessors used this section as the definition of the informa-tion need.
Each topic was judged by a single assessor sothat all documents screened would reflect he same user'sinterpretation f topic.1.2 The DocumentsThe documents came from many different sources.
Thesesources were selected not only because of their suitabilityto the TIPSTER task, but also because of their availability.The data was provided by the University of Pennsylvania,initially as part of the ACL/DCI text initiative, but later aspart of the Linguistic Data Consortium effort.
A success-ful pattern for data source selection was established forthe first disk and was followed for disks 2 and 3.
First,two sets of documents were obtained that contained arti-cles from all domains.
The first set of articles was from anewspaper (the Wall Street Journal in disks 1 and 2, theSan Jose Mercury News in disk 3), and the second set ofarticles was from a newswire (AP in all three disks).
Inaddition to covering all domains, these two sets provide astrong contrast in their format, style, and accuracy of edit-ing, and were both readily available.
The third set of doc-uments was selected to cover more deeply a particular do-main.
Partially because of availability, the particular setused was a subset of the Computer Select disks, fromZiff-Davis publishing.
These documents cover the widedomain of computers and computer technology, but in-clude many different sources of actual documents.
Thiscreates a set of documents in a single (broad) domain, buthaving a range of formatting and writing styles.
The finalset of documents (or final two sets in the first disk) wereselected less for content han for length of articles.
Sincethe documents in the first three sets were of mediumlength, and of fairly uniform length, the final set of docu-ments was picked to be especially long, and of non-uniform length.
Documents from the Federal Registerwere used for the first two disks, and some U.S. Patentswere used on disk 3.
Additionally the first disk containedsome very short abstracts from the Department of Energy.17Because most of this material is copyrighted, all users ofthis data were required to sign a detailed agreement in or-der to protect the copyrighted source material.The following shows the actual contents of each disk.?
Disk 1?
WSJ -- Wall Street Journal (1986, 1987,1988, 1989)?
AP -- AP Newswire (1989)?
ZIFF -- Information from Computer Selectdisks (Ziff-Davis Publishing)?
FR -- Federal Register (1989)?
DOE -- Short abstracts from the Departmentof Energy?
Disk 2?
WSJ -- Wall Street Journal (1990, 1991,1992)?
AP -- AP Newswire (1988)?
ZIFF -- Information from Computer Selectdisks (Ziff-Davis Publishing)?
FR -- Federal Register (1988)?
Disk 3?
SJMN -- San Jose Mercury News (1991)?
AP -- AP Newswire (1990)?
ZIFF -- Information from Computer Selectdisks (Ziff-Davis Publishing)?
PAT -- U.S.
Patents (1993)All documents were originally received at the Universityof Pennsylvania in various pnnt-tape formats.
These for-mats were converted to an SGML-Iike structure and sentto NIST.
At NIST the documents were assigned uniquedocument identifiers and the formats were more standard-ized.
The following example shows an abbreviated ver-sion of a typical document.<DOC><DOCNO> WSJ880406-O090 <IDOCNO><HL> AT&T Unveils Services to Upgrade Phone Net-works Under Global Plan </HL><AUTHOR> Janet Guyon (WSJ Staff) </AUTHOR><DATELINE> NEW YORK </DATELINE><TEXT>American Telephone & Telegraph Co. introduced thefirst of a new generation of phone services with/,roadimplications for computer and communications equip-ment markets.18AT&T said it is the first national ong-distance carri-er to announce prices for specific services under aworld-wide standardization plan to upgrade phone net-works.
By announcing commercial services under theplan, which the industry calls the Integrated ServicesDigital Network, AT&T will influence volving commu-nications tandards to its advantage, consultants aid,just as International Business Machines Corp. has cre-ated de facto computer standards favoring its products.</TEXT></DOC>Al l  documents  have beginning and end markers, and aunique DOCNO id field.
Additionally other fields takenfrom the initial data appear, but these vary widely acrossthe different sources.
The documents have differingamounts of errors, which were not checked or corrected.Not only would this have been an impossible task, but theerrors in the data provide a better simulation of the TIP-STER task.
Errors in missing document separators or baddocument numbers were screened out, although a fewwere missed and later reported as errors.Table 1 shows some basic document collection statistics.Note that although the collection sizes are roughly equiv-alent in megabytes, there is a range of document lengthsfrom very short documents (DOE) to very long (FR).
Al-so the range of document lengths within a collectionvaries.
For example, the documents from AP are similarin length (the median and the average length are veryclose), but the WSJ and ZIFF documents have a widerrange of lengths.
The documents from the Federal Regis-ter (FR) have a very wide range of lengths.The distribution of terms in these subsets how interestingvariations.
Table 2 shows some term distribution statisticsfound using a small stopword list of 25 terms and nostemming.
For example the AP has more unique termsthan the others, probably reflecting both more propernames and more spelling errors.
The DOE collection,while very small, is highly technical and covers many do-mains, resulting in many specific technical terms.
Thetypical distribution of terms in the collections in generalcorresponds to Zipf's law \[1\] in that about half the totalnumber of unique terms only appear once.
This is leastapplicable to the newspaper data, possibly because the vo-cabulary used is more controlled than in the other collec-tions.
The newspaper data also has the highest number ofoccurrences of terms for those terms appearing more thanonce.Subset of collection WSJ AP ZIFFSJMNSize of collection(megabytes)(disk 1) 295 266 251(disk 2) 255 248 188(disk 3) 315 248 358Number of records(disk 1) 98,736 84,930 75,180(disk 2) 74,520 79,923 56,920(disk 3) 90,257 78,325 161,021Median number ofterms per record(disk 1) 182 353 181(disk 2) 218 346 167(disk 3) 279 358 119Average number ofterms per record(disk 1) 329 375 412(disk 2) 377 370 394(disk 3) 337 I 379 263FRPAT25821125126,20720,1086,7113133152896101710733~3DOE190226~878289Table 1: Document StatisticsSubset of collection WSJ AP ZIFF FR DOESJMN PATTotal number ofunique terms(disk I)(disk 2)(disk 3)Occurring once(disk 1)(disk 2)(disk 3)Occurring more > 1(disk 1)(disk 2)(disk 3)Average number ofoccurrences > 1(disk 1)(disk 2)(disk 3)156,298153,725179,49064,65664,84473,06491,64288,881106,426199178168197,608186,500190,27889,62783,01986,976107~81103,481103,302174169169173,501147,40521232985,99272,053106,85787,50975,352105,872165139205126,258116,586156~7758,67754,82390,12867,58161,76365,9491069163186,22595,78290,443159Table 2: Dictionary Statistics191.3 The  TopicsTraditional information retrieval test collections have typi-cally included sentence-length queries.
These queries areusually automatically transformed into a machine versionfor searching, with minimal changes.
In designing theTIPSTER task, there was a conscious decision made toprovide "user need" statements rather than the more tradi-tional queries.
Two major issues were involved in this de-cision.
First there was a desire to allow a wide range ofquery construction methods by keeping the topic (theneed statement) distinct from the query (the actual textsubmitted to the system).
The second issue was the abili-ty to increase the amount of information available abouteach topic, in particular to include with each topic a clearstatement of what criteria make a document relevant.The topics were designed to mimic a real user's need, andwere written by people who are actual users of a retrievalsystem.
Topics 1-25 and 51-80 were written by a groupof different users.
Topics 26-50 were mostly written by asingle user and cover the general domain of computers.Topics 81-150 were also written by a single user, butcover many domains.Although the subject domain of the topics was diverse,some consideration was given to the documents to besearched.
The initial ideas for topics were either generat-ed spontaneously, or by seeing interesting topic areaswhile doing other searches.
These initial ideas were thenused in trial searches against a sample of the documentset, and those topics that had roughly 25 to 100 hits in thatsample were used as a final topic.
This created a range ofbroader and narrower topics.
After a topic idea was final-ized, each topic was developed into a standardized format.The following is one of the topics used in TIPSTER andshows the formatting.<top><head> Tipster Topic Description<hUm> Number: 066<dora> Domain: Science and Technology<title> Topic: Natural Language Processing<con> Concept(s):1. natural anguage processing2.
translation, language, dictionary, font3.
software applications<fac> Factor(s):<nat> Nationality: U.S.</fac><def> Definition(s):</top>Each topic is formatted in the same standard manner to al-low easier automatic onstruction of queries.
Besides abeginning and an end marker, each topic has a number, ashort rifle, and a one-sentence d scription.
Then there arethree major types of sections.The first type of section is the narrative section.
This sec-tion is meant o be a full description of the informationneed, in terms of what separates a relevant document froma non-relevant document.
The narrative sections wereconstructed by looking at relevant documents in the trialsample arid determining what kinds of information wereneeded to provide focus for the topic.
These sections areprimarily meant as instructions to the assessors, but couldbe used in building the queries either manually or auto-matically.
The narratives often contain augmentations ofthe description, such as examples, or resU'ictions to focusthe topic.
The following three narratives illustrate these.Example of augmentation<num> Number: 082A relevant document will discuss a product, e.g., drug,microorganism, vaccine, animal, plant, agriculturalproduct, developed by genetic engineering techniques;identify an application, such as to clean up the environ-ment or human gene therapy for a specific problem; or,present human attitudes toward genetic engineering.Example of positive restrictions<desc> Description:Document will identify a type of natural anguage pro-cessing technology which is being developed or market-ed in the U.S.<narr> Narrative: A relevant document will identify acompany or institution developing or marketing a natu-ral language processing technology, identify the tech-nology, and identify one or more features of the compa-ny's product.<num> Number: 121A relevant document will provide obituary informationon a prominent U.S. person who died of an identifiedtype of cancer.
In addition to the individual's name andcancer, the report must provide sufficient biographicalinformation for a determination of why the life andcontributions of the individual were worthy of somecomment upon death.
In other words, a one or two lineobituary is NOT sufficient.20Example of negative restrictions<num> Number: 067A relevant document will report he location of the dis-turbance, the identity of the group causing the distur-bance, the nature of the disturbance, the identity of thegroup suppressing the disturbance and the politicalgoals of the protesters.
It should NOT be about eco-nomically-motivated civil disturbances and NOT beabout a civil disturbance directed against a secondcountry.Many narratives are a mix of augmentation and restric-tion.
Whereas the narratives did provide the type of cleardirection needed by the human assessors, they also pro-vide a challenge for query consu'uction by machines.The second type of section is the concepts ection.
Thissection is meant o reflect he "world-knowledge" broughtto the task by the users, and is the type of information thatcould be elicited by prompts from a good interface.
Theconcepts ections were constructed by locating "useful"information in some of the relevant documents in the trialsample.
This information was then grouped into concep-tually related ideas, although these relationships varywidely across topics.
To show some examples of con-cepts, the concept lists for the three narratives previouslyshown are given.<num> Number: 067<con> Concept(s):1. protest, unrest, demonstration, march, riot,clash, uprising, rally, boycott, sit-in2.
students, agitators, dissidents3.
police, riot police, troops, army, NationalGuard, government forces4.
NOT economically-motivated<num> Number: 082<con> Concept(s):1. genetic engineering, molecular manipulation2.
biotechnology3.
genetically engineered product: plant, animal,drug, microorganism, vaccine, agriculturalproduct4.
cure a disease, clean up the environment, in-crease agricultural productivity<num> Number: 121<con> Concept(s):211. cancer2.
death, obituaryIt should be noted that the number of concepts given, theorganization of the concepts, and the "usefulness" of theconcepts vary widely across the topics.
The concepts ingeneral provide excellent keywords for retrieval systems,although this also varies widely across the topics.The third type of section is the optional factors sectionand optional definition section.
These sections only ap-pear when necessary; the factors ection is an attempt tocodify some of the text in the narrative for easier use byautomatic query construction algorithms, and the defini-tion section has one or two of the definitions critical to ahuman understanding of the topic.
Two particular factorswere used in the TIPSTER topics: a time factor (current,before a given date, etc.)
and a geographic factor (eitherinvolving only certain countries or excluding certaincountries).
The definitions ection was minimally used inthe TIPSTER topics, but did provide some critical defini-tions for some of the more unusual terminology.1.4 The Relevance JudgmentsThe relevance judgments are of critical importance to atest collection.
For each topic it is necessary to compile alist of relevant documents; hopefully as comprehensive alist as possible.
For the TIPSTER task, three possiblemethods for finding the relevant documents could havebeen used.
In the first method, full relevance judgmentscould have been made on over a million documents foreach topic, resulting in over 100 million judgments.
Thiswas clearly impossible.
As a second approach, a randomsample of the documents could have been taken, with rel-evance judgments done on that sample only.
The problemwith this approach is that a random sample that is largeenough to find on the order of 200 relevant documents pertopic is a very large random sample, and is likely to resultin insufficient relevance judgments.
The third method, theone used in building the TIPSTER collection, was tomake relevance judgments on the sample of documentsselected by the various participating systems (includingthe systems in TREC).
This method is known as the pool-ing method, and has been used successfully in creatingother collections.
It was the recommended method in1975 proposal to the British Library to build a very largetest collection \[2\].To consU'uct the pool, the following was done.1.
Divide each set of results into results for a giventopic2.
For each topic within a set of results, select hetop X ranked ocuments for input to the pool3.
For each topic, merge results from all systems4.
For each topic, sort results based on documentnumbers5.
For each topic, remove duplicate documentsThe aim in a pooling method is to have a broad enoughsample of search systems so that most relevant documentsfor a given topic are found.
Since it is well known thatdifferent systems retrieve different sets of relevant docu-ments \[3\], it is critical to have as many systems as possi-ble contributing to this pool.
For that reason, the TIP-STER systems were judged against he pool of relevantdocuments constructed from both the TREC and TIP-STER evaluations.
For the 12-month evaluation, noTREC results were available and therefore only a partialTIPSTER evaluation was possible.
However the relevantdocuments found at the 12-month evaluation were pooledwith the TREC-1 results at the first TREC conference.At each evaluation period, the merged list of results wasthen shown to the human assessors.
For the TIPSTER12-month evaluation, the top 200 documents for each runwere judged, but the overwhelming number of uniquedocuments for TREC-1 meant that only the top 100 docu-ments for each run were judged.
This still resulted in anaverage of 1462.24 documents judged for each topic,ranging from a high of 2893 for topic 74 to a low of 611for topic 46.
Each topic was judged by a single assessorto insure the best consistency of judgment and varyingnumbers of documents were judged relevant to the topics.The two histograms on the next page show the number ofdocuments judged relevant for each of the topics.
Thetopics are sorted by the number of relevant documents obetter show their range and median.
The first histogramshows the number of relevant documents for topics51-100, first used as adhoc topics against disks 1 and 2,and then used as routing topics against disk 3.
The sec-ond histogram shows the number of relevant documentsfor topics 101-150 used as adhoc topics against disks 1and 2.Several interesting facts can be seen by looking at the firsthistogram.
The median number of relevant documentsper topic for disks I and 2 is 277, with 22 topics having300 or more relevant documents, and 11 topics havingmore than 500 relevant documents.
When these same top-ics were used as routing topics against different data (disk3), only about half the number of relevant documentswere found.
This is reasonable given that there is onlyabout half the amount of data.
Some topics have far fewerthan half the number of relevant documents; in generalthese are the topics that are particularly time dependent.The second histogram shows the number of relevant docu-ments for topics 101-150 against disks 1 and 2.
The top-ics are narrower than topics 51-100 as the result of an ef-fort to create fewer topics with more than 300 relevantdocuments.
This effort was triggered by the concern thattopics with more than 300 relevant documents are likelyto have incomplete relevance assessments.
Only 11 topicshave more than 300 relevant documents, with only 2 top-ics having more than 500 relevant documents.
The medi-an number of relevant documents i 201 for this set oftopics, down from 277 for topics 51-100.1.5 Some Preliminary AnalysisMuch analysis needs to be done on this test collection.Questions about he effec/.s of the long and varied docu-ments, the complex topics, the pooling of system results,and the relevance judgments will be asked (and answered)as the test collection is further used and investigated.
Thefollowing section discusses ome preliminary answers tothese questions, particularly questions relating to howwell the collection has served the TIPSTER evaluationtask.The documentsThe first question involves the effect of the varioussources and lengths of the documents.
Table 3 shows thedistribution of retrieved, judged, and relevant documentsacross the collections.
The first 5 rows show the docu-ments from disks 1 and 2 using topics 101-150.
The last4 rows show the documents from disk 3 using topics51-100.
The second column shows the total number ofdocuments from each document source that was retrievedby rank 100 by any of the systems in TIPSTER andTREC-2.
The third column shows how many of thesedocuments were unique and therefore were judged.
Thefinal column shows how many of these documents wereactually relevant.DatabaseAPRetrieved96135Judged16530(17%)Relevant4823(29%)DOE 1554.4 4018 (26%) 678 (17%)FR 24848 11818 (48%) 410 (3%)WSJ 125921 23706 (19%) 4556 (19%)ZIFF 19886 6770 (34%) 1183 (17%)AP3 187037 49304 (26%) 5848 (12%)ZIFF3 57580 25577 (44%) 2668 (10%)PAT 12070 4656 (39%) 146 (3%)37695 (23%) SJMN 161102 2322 (6%)Table 3: Distribution of top 100 documents acrossdatabases22lOOOTop ics  51 through 1009008007006005004003002001000 -- 1tt   jtjllllltilllllllltttTop icsD isk  3 W D isk  1 and  2lOOOTop ics  101 through 150 Aga ins t  D isks  1 + 2800600,,,,,,,,,,,,,,,,llllllllllllllllllllllllllll 0Top ics23The analysis of the adhoc topics 101-150 (first 5 rows)shows that by far the largest number of relevant docu-menU; come from the document sources covering all do-mains (AP -- 4823; WSJ -- 4556).
Of the five documentsources on disks 1 and 2, these two had the overwhelm-ingly highest number of relevant documents, the lowestpercentage of unique documents, and the highest percent-age of relevant documents found in the judged set.
Thevery long FR documents had few relevant documents, buta high number of retrieved ocuments and a high percent-age of unique documents.
This demonstrates the difficul-ty most retrieval systems had in screening out long docu-ments and shows the almost random nature of retrievalfrom this set of "noise" documents.
The much shorterDOE documents caused no such problems, with a largernumber of relevant documents being found, but with a farfewer number of documents being retrieved.
The lowerpercentage of unique documents for the DOE documentsas opposed to the FR documents indicates that hese shortdocuments are being effectively handled by the systems.The single-domain ZIFF document source also appears tobe as effectively retrieved as the all-domain sources.The analysis of the routing topics 51-100 (last 4 rows)shows much the same pattern.
The long PAT documentscause the same high retrieval and low accuracy as the FRdocuments did for the adhoc.
Additionally, the number ofunique documents is a higher percentage for all datasources in the routing task, This is due to the broad rangeof query terms used in routing, where the query terms areusually taken from both the topic and the training docu-ments.
The broad range of terms Used across the systemscaused more unique documents obe retrieved.
However,a smaller percentage of these documents are relevant,likely because of time differences in the test data.Many of the same trends can be seen if this distribution isbroken down by topics.
Tables 4 and 5 show the distribu-tion of the relevant documents across databases on a pertopic basis.
Table 4 shows the first set of topics, used forrouting in TREC-1.
The first 25 topics are mostly basedon information i  the Wall Street Journal, with many inthe financial domain.
Almost all of the second 25 topics(26-50) are in the single domain of computers.
As wouldbe expected, the first 25 topics have mostly WSJ docu-ments as relevant, whereas the second 25 topics havemostly ZIFF documents as relevant.
Very few of thesetopics had any DOE or FR relevant documents.
Ad-dit;ionally the first 25 topics have many AP documents asrelevant, as many of the financial topics are discussed inthe newswire.
The second 25 topics have few AP relevantdocuments in general, but a reasonably large number ofWSJ documents ince computers are often discussed inthe Wall Street Journal.24Topic AP DOE FR WSJ ZIFF1 111 0 12 234 13 i2 215 0 0 273 163 74 3 6 386 1254 41 0 0 136 0 I5 40 0 47 145 53 i6 129 0 0 240 1 \]7 158 1 1 174 38 29 2 0 212 79 53 0 3 85 010 148 0 5 315 111 231 5 0 116 2012 272 24 13 152 313 71 23 8 148 614 53 0 26 234 015 58 0 0 41 016 43 1 2 144 017 144 5 209 139 018 69 0 0 143 019 61 0 42 185 1320 .63 0 5 370 15321 23 4 0 51 1722 515 0 18 191 323 142 1 2 101 024 145 6 0 364 325 39 32 0 23 026 14 0 0 82 43027 6 11 0 39 30628 19 0 0 110 35229 6 0 1 95 6830 0 0 0 57 42531 1 0 0 24 26432 3 0 0 20 19133 13 0 0 77 72534 0 1 1 19 45235 1 0 0 17 39736 6 0 2 4 29037 7 0 1 10 54838 216 0 0 74 039 0 2 1 13 78040 148 0 129 199 141 8 1 2 23 15942 78 5 3 161 86643 43 11 33 64 6344 57 0 0 130 15445 52 1 10 44 36446 38 0 0 14 6247 36 0 0 225 18948 19 0 0 103 13949 25 3 0 74 13450 5 0 1 5 27Table 4: Number of relevant documents by databaseTopics 51-100 shown in table 5 were used as adhoc topicsin TREC-1 and TIPSTER-12 month, and as routing topicsin the later evaluations.
Topics 101-150 were only used asadhoc topics.
Both these sets of  topics cover many do-mains, and have equally large numbers of relevant docu-ments from both the AP newswke and the Wall StreetJournal.
They have few relevant from either DOE or FR,and those relevant are concentrated in about 11 topics outof the 100.
The relevant documents from ZIFF are alsoconcentrated in general, with slightly more topics havingrelevant ZIFF documents than FR or DOE documents.Note that the count of the relevant AP and ZIFF docu-ments shown for topics 51-100 include documents fromall three disks.
However the WSJ documents from disks 1and 2 reflect the use of the topics for the adhoc runs,whereas the SJIVlN documents are used only for routing.This may explain why many of the topics have high num-bers of relevant documents from the WSJ set, with fewfrom the SJMN set.
The time difference of these setscould affect some topics, or this difference could becaused by a slight difference in the domains covered bythese newspapers.The topicsThe topics are both very long and very complex, and havemany more relevant documents han other test collections.A major question therefore ishow these characteristics af-fect retrieval performance.
Table 6 shows a preliminaryanalysis of system performance with respect to topicbroadness and the level of restrictions and the use of fac-tors in the topics.
The difficulty or "hardness" measureshown in column two is the average relative recall for thattopic across all systems in TREC and TIPSTER that usedthe full document collections.
The relative recall is de-fined as the recall at R relevant documents (where R is thetotal number of relevant documents for a topic) if R is lessthan some threshold (100 in this case), OR the precisionat that threshold if R is greater than the threshold.
Thismeasure shows how effectively a system is performing atthe early stage of retrieval, and is to a large extent inde-pendent of the total number of relevant documents foundfor a topic.
The average of this measure over many sys-tems is a good indication of the difficulty of a given topic.Column 3 of table 6 shows the total number of relevantdocuments for a given topic.
A strong relationship can beseen between the difficulty of a topic and the number ofrelevant documents found for topics 101-150 in that thenarrower topics are also the harder topics.
Column 4shows the type of restrictions used in the narrative sectionof a topic.
The "R" stands for some type of resuictionwhereas the "N" means that a "NOT" was explicitly usedin the topic narrative (e.g."
A document isNOT relevant ifit merely mentions the illegal spread of a computer virus...").
There is a possible weak relation between the re-strictions and the performance, but this trend seems to bemore related to the broadness of a topic, i.e.
narrow top-iscs are more likely to have restrictions in the narrative.The final column contains a code for any factors used inthe factors ection of the topic--"G" stands for geographicfactors and "T" stands for time factors.
There is no clearrelationship between the use of factors and the difficultyof a topic.Table 7 shows the same statistics but for topics 51-100 asused in TREC-1 and the earlier TIPSTER evaluations asadhoc topics.
Again there is no relationship between theuse of factors and topic difficulty.
The relationship be-tween the use of restrictions and topic difficulty is not ob-vious for this set of topics, and the relationship betweenrestrictions and topic broadness has also disappeared.
Fi-nally the relationship between topic broadness and topicdifficulty that is clearly seen for topics 101-150 is veryweak for topics 51-100.What is the explanation for these observations?
First, thelack of correlation between the use of factors and retrievalperformance, and the weak correlation between the use ofrestrictions and retrieval performance r flects the idiosyn-cratic use of language.
The performance of retrieval sys-tems on a given topic depends on the ease with which atopic maps to the document set.
If a topic contains muchthe same terms as do its relevant documents, then thematch is easier than for a topic which has more generalterminology or one that requires inferences to make themap to many of the relevant documents.
For example, thedescription for topic 74 reads "Document will cite an in-stance in which the U.S. government propounds two con-flicting or opposing policies".
Clearly this topic will bedifficult, regardless of the presence of any type of restric-tions or factors.
In contrast, he topic description for topic87 is "Document will report on current criminal actionsagainst officers of a failed U.S. financial institution".
Thistopic not only has more specific terminology, but morereadily lends itself to term expansion techniques.The relationship of topic difficulty and topic broadness imore tentative.
Whereas there is a strong relationship fortopics 101-150, the relationship is weak to non-existentfor topics 51-100.
Three different explanations eed fur-ther exploration.
First, there may be an overall differencebetween the topic sets that remains to be characterized.Second, there may be a relationship between the maturityof retrieval systems and the correlation of topic hardnessand broadness.
The overall performance l vel used tomeasure topic hardness for topics 101-150 was much im-proved from that used for topics 51-100.
And third, it25Topic AP DOE FR/PAT SJMN WSJ ZIFF Topic AP DOE FR WSJ ZIFF51 36 0 1/0 6 104 2 101 27 17 2 7 652 631 2 0/0 107 ' 251 13 102 19 33 3 7 253 389 0 0/0 33 296 7 103 55 0 14 25 054 157 0 0/0 9 63 66 104 51 0 3 25 055 518 0 0/0 41 564 8 105 33 0 0 26 056 549 0 1/0 131 591 10 106 47 0 27 146 357 II0 0 0/0 22 177 471 107 30 0 3 65 058 144 0 0/0 30 60 1 108 94 0 11 189 3659 926 0 0/0 175 72 0 109 8 1 15 219 56060 24 0 0/0 2 45 8 110 387 0 0 150 161 104 0 0/0 29 139 2 111 113 56 3 124 262 590 0 0/0 70 70 0 112 12 11 7 334 563 6 10 0/0 1 6 264 113 32 6 0 72 12664 464 1 0/0 119 88 1 114 123 4 1 20 365 0 68 0/4 0 20 514 115 79 0 21 85 266 2 33 0/0 4 3 245 116 17 0 6 28 067 758 0 0/0 91 92 0 117 29 0 3 106 14868 111 96 3/2 10 49 0 118 198 1 4 87 069 16 3 2/0 1 31 0 119 239 3 11 84 470 60 0 0/0 15 15 0 120 40 0 5 48 271 561 0 0/0 63 72 0 121 48 0 0 2 572 85 0 0/0 54 74 0 122 20 5 7 86 273 371 0 0/0 93 77 2 123 70 156 103 106 874 504 9 30/0 143 273 3 124 58 31 4 77 675 37 64 25/12 26 86 493 125 95 0 15 76 076 175 2 23/0 52 191 17 126 174 11 4 57 077 180 0 4/0 24 19 0 127 165 1 4 68 078 200 2 1/0 12 28 2 128 56 7 3 296 4379 452 5 0/0 19 98 2 129 167 0 0 49 680 292 1 0/0 24 205 0 130 228 0 3 64 181 55 0 0/0 2 9 0 131 6 0 0 22 082 217 84 72/35 71 317 10 132 137 0 0 64 283 450 137 119/0 39 125 9 133 13 10 0 29 2884 76 175 68/22 29 130 4 134 8 140 7 23 1085 1144 2 0/0 184 268 12 135 75 183 11 156 486 27 0 5/0 33 190 1 136 10 0 0 121 9287 162 0 0/0 46 136 0 137 54 0 0 111 288 99 0 0/0 0 99 0 138 36 0 0 20 089 71 1 1/0 4 115 0 139 47 0 0 10 090 98 26 6/0 4 206 1 140 25 0 0 6 091 12 0 0/0 2 35 0 141 22 0 0 16 092 67 0 0/0 6 41 2 142 336 2 54 338 393 198 0 1/0 46 20 2 143 271 0 45 135 194 119 0 0/0 92 67 347 144 42 0 0 7 095 92 0 63/4 95 52 341 145 103 0 0 64 096 40 490 13/60 63 73 284 146 320 0 0 68 097 40 10 6/6 38 90 488 147 118 0 2 209 1698 38 3 1/1 31 186 1157 148 220 0 1 77 099 392 0 0/0 80 115 0 149 30 0 1 98 49100 107 3 64/0 51 95 207 150 236 0 7 254 5Table 5: Number of relevant documents by database26Topic121120141139140101114149124131122102138113144105107147116125106117104127118119108112115136145137129128143123134133126132130111109135142146150110110148"Hardness" No.
relevant Restrictions Factors0.0471 55 R GT0.0893 83 N0.1552 36 N G0.1759 55  N0.1765 25 N0.1925 57 N G0.2032 138 N0.2153 135 N0.2176 1730.2269 28 N0.2341 114 R0.2362 64 N G0.2472 520.2568 206 N0.2749 49 N0.2849 54 N G0.2911 98 G0.3165 315 N G0.3271 49 N0.3285 169 N0.3479 201 N , G0.3538 275 N GT0.3612 75 G0.3647 223 N G0.3665 273 R0.3688 326 R0.3718 294 N G0.4044 2910.4344 165 N0.4538 206 N0.4538 169 G0.4544 158 N G0.4771 207 N0.5050 381 N T0.5291 412 G0.5585 4350.5924 188 N0.6162 80 N0.6297 240 N0.6359 201 N G0.6574 2860.6991 2860.7065 7420.7609 400 N0.7809 6600.7953 3580.7956 458 G0.8200 496 R0.8200 94 R0.8935 250Table 6: Characterization f topics by "hardness", broadness, and levels of restrictions277Topic "Hardness" No.
relevant Restrictions Factors Routing "Hardness" No.
Routing Relevant499 N G 0.11400.1407 1710.1811 2060.1833 1660.1893 1830.2280 6660.2370 2660.2447 8960.2493 2940.2615 6020.2807 6930.2915 620.2940 3750.2967 1750.3051 880.3099 520.3160 2630.3194 400.3433 3650.3773 5340.3813 1190.4007 3740.4020 2080.4100 1950.4260 1880.4320 1390.4367 3860.44.40 1620.4481 550.4620 3100.4736 2880.4820 1970.4947 5790.5220 1590.5307 8100.5333 3520.5487 5350.5487 1710.5507 2980.5653 600.5877 3160.5947 1380.5980 2320.6067 3960.6480 2140.6544 3800,8133 6330.8393 8780.8547 5710.9347 461RRRRRRRRRGR TRR GTN TR GR GRRR GTRRRRNRR GRR TRRR GN TR0 GR GTR GTNRR 0N T0.AA A.
A , 3230.2245 940.2500 670.3878 320.3255 3550.2688 7220.1746 750.1600 6700.2763 1630.5625 2030.4394 3100.1109 40.3049 2820.6777 170.2917 270.4622 10.4019 3590.1608 90.3069 3720.2209 365.0.3919 910.7147 1430.7134 740.4747 760.4016 1510.2288 850.5103 2140.3743 830.3438 340.5884 3000.6000 2910.2931 860.5263 5740.6584 760.5438 3200.4725 3190.8580 4540.4816 1240.3647 4260.7328 180.8984 2040.7716 110.6261 3410.3203 1010.7016 400.8134 3000.6441 2350.7872 3950.9366 1540.7581 319Table 7: Characterization f topics by hardness, broadness, and levels of restrictions, both as adhoc topicsand as rouiing topics28may be that the hard topics have less complete relevancejudgments than the easy ones (however this does not ex-plain the difference between the topic sets).
All three ofthese hypotheses are currently under investigation.The last section of table 7 shows the performance of top-ics 51-100 as routing topics.
Column 6 shows the relativerecall for each topic for routing against new data (disk 3).There is a weak correlation between topic difficulty as anadhoc topic, and its difficulty as a routing topic.
In partic-ular, topics that are easy as adhoc topics are still generallyeasy as routing topics.
However some topics that werehard adhoc topics became asy routing topics, usually be-cause of the term expansion available using the trainingdocuments (the relevant documents from the adhoc task).Some hard topics remained hard, and some topics becamemore difficult because of time differentials between thetraining data and the test data.
There is a minimal correla-tion between the number of relevant documents as adhoctopics (i.e.
the number of documents available for train-ing) and the performance of the topic as a routing topic.The TIPSTER topics are not only broad and complex, butalso very long.
Other test collections have much shortertopics of one or two sentences in length, whereas the TIP-STER topics are generally about a page in length.
Longertopics provide more information, but also a large numberof non-discriminating terms.
One section of the TIP-STER topics, the description section, could be viewed as aone-sentence summary of the topics, and table 8 showsthe difference in performance for one of the TIPSTERsystems using just the description section vs. the "full"topic (results from the TIPSTER 24-month INQUERYsystem, INQ009 using the title, description and conceptsand INQ013 using the description only).
Use of the de-scription only cuts retrieval effectiveness by about onehalf, both on average across the entire recall range and atthe high performance end (precision at 100).
The recallperformance is particular affected, as would be expected.This drop in performance is due to fewer topic terms be-ing available for matching.
Interestingly, however, theshorter topics do retrieve some relevant documents not re-trieved by the longer ones, although this is clearly a rank-ing effect since the terms from the description are a subsetof the full topic terms.It should be noted, however, that this performance differ-ence is not due solely to topic length.
The concepts ec-tion of the topic consistently provides very valuable termsfor retrieval, and minimal additional improvement comesfrom adding the longer narrative section.
This may becaused by a temporary lag in research in how to properlyhandle this narrative section.
"Full" topic Description onlyAverage precision 0.3465 0.1420Precision at 100 0.4934 0.2730Total relevantRetrieved 8688 4953Unique relevant(total) 3794 59Unique relevant Iiat 100 1651 549Table 8: Performance differences using the full topics vsthe description onlyThe relevance judgmentsTwo particular issues are being investigated with respectto the relevance judgments.
The first is the effectivenessof the merging of results across the TIPSTER and TRECsystems to form the pool of documents sent to the rele-vance assessors.
One of the measures of this effectivenessis the overlap of retrieved ocuments.
If there is a heavyoverlap in that all systems retrieve the same documentsfor a given topic, then it is likely that relevant documentswill be missed.Table 9 shows the overlap statistics for TREC-1, with atotal of 28 systems including the TIPSTER systems.UniqueDocumentsPer TopicAdhoc, 45 runs18 groupsUniqueDocumentsPer TopicRouting, 26 runs17 groupsTop 200Pos.
Act.9000 2398.4Top 100Pos.
Act.4500 1278.865200 1932.42 2600 1066.86Table 9: Overlap of submitted results (TIPSTER12-month +TREC-1)The first overlap statistics are for the adhoc topics and thesecond statistics are for the routing topics.
For example,out of a maximum possible 9000 unique documents (45runs times 200 documents), about 27% of the documentswere actually unique for the adhoc topics.
There are sig-nificantly more unique documents for routing.
The over-lap is much less than was expected, which means that thesystems were finding different documents as likely rele-vant documents for a topic.
Whereas this might be ex-29pected from widely differing systems, these overlaps wereoften between two runs for a given system, or betweentwo systems run on the same basic retrieval engine.
Onereason for the lack of overlap is the very large number ofdocuments hat contain many of the same keywords as therelevant documents.
More importantly, however, manysystems used different sets of keywords in the constructedqueries, resulting in the often unique sets of retrieved oc-uments.
This is particularly true for the muting topics,where often unique terms were added ffrom the trainingdocuments.
The fact that the muting topics are runagainst only half the number of documents as the adhoctopics, but still have many more unique retrieved ocu-ments per topic suggests trongly that the wide variationin query terms is the cause of the small overlap.
Thesame lack of overlap still holds at the 100 document mark,indicating that the systems retrieve different documentseven at the beginning of the ranked lists.This lack of overlap improves the coverage of the relevantset, and verifies the use of the pooling methodology toproduce the sample.
It should be noted, however, that be-cause complete judgments were not made, the recall mea-sures must be considered to be relative rather than abso-lute.
Whereas each system is fairly evaluated against hispool, future systems (with widely differing methods)might find additional relevant documents that would beconsidered non-relevant by default.UniqueDocumentsPer TopicAdhoc, 52 runs24 groupsUniqueDocumentsPer TopicRouting, 52 runs25 groupsTREC-2Pos.
AcL5200 1106.05200 1465.6TIPSTER 24-mo.Pos.
Act.1900 144.73600 288.8Table 10: Overlap of submitted results (TIPSTER18-month + TREC-2) plus TIPSTER 24-monthTable 10 shows the overlap of the submitted results fromthe later TIPSTER evaluations and the TREC-2 evalua-tion.
In this case only the top I00 documents are shownas these were the only ones judged due to limited time.Note that there is a significantly higher overlap in theTREC-2 results (including the TIPSTER 18-month runs).Only about 21% of the retrieved ocuments for the adhc~ctopics were actually unique, with slightly more uniquedocuments for the routing topics.
This is likely due tobetter performance atthis later date (fewer errors with Icssspread of retrieved ocuments).
In particular, the fewerunique documents for the routing topics is probably dueto the better training data in addition to better systems.The many additional runs made for the TIPSTER24-month evaluation on the same data as for TREC-2 re-sulted in few new documents.
For example, out of a pos-sible 3600 unique documents retrieved for routing (pertopic), only about 8% had not already been seen inTREC-2.
This implies that, at least for the current sys-tems in TIPSTER and TREC, some asymptote is beingreached for finding new documents.This hypothesis will be investigated in the comingmonths.
A series of experiments i  being planned to in-vestigate the coverage of the relevant documents, i.e.
howmany relevant documents have been missed because theydid not appear in the pool for judgment.
Two types of ex-periments are planned involving additional relevancejudgments to be made for some topics.
The topics will beselected to cover a range of narrow and broad topics, withan emphasis on the mid-range topics which constitute themajority of the TIPSTER topics (mid-range being around200 relevant documents).
First, systems that seem likelyto retrieve unique relevant documents beyond the current100 mark will be selected and a new pool of possible rele-vant documents will be created for the subset of topics be-ing investigated.
This new pool will be submitted for as-sessment to the same assessor used for the earlier judg-ments.
Additionally topics will be submitted to haman in-termediaries to conduct extensive searches.
The resultsfrom these searches will also be assessed by the officialrelevance assessors.
These two sets of experiments willprovide some indication of the completeness of the pool.An issue related to the completeness problem is the issueof relevance bias.
Whereas it can be assumed that thejudgments are not absolutely complete, it is hoped that thejudgments are not biased toward one type of system asopposed to another, e.g.
a system using a particular typeof weighting scheme.
This is a possibility given that theTIPSTER contractors contributed many more documentsfor assessment than the TREC participants (and thereforethe assessments may be more biased toward the TIPSTERsystems), or alternatively many of the TREC systemswere based on a single weighting scheme, and this couldcause a bias.
The existence of this bias will also be inves-tigated.In addition to the completeness or bias of the poolingmethod, it is necessary to investigate he quality of the rel-evance assessments themselves.
Two different consisten-cy issues must be addressed-- the consistency of a singlejudge and the consistency between judges on a single top-ic.
The first issue will be addressed by submitting random30Subset ot ~ collection WSJ AP ZIFF FR CRANSize of collection(megabytes) 295 266 251 258 1.5Number of records 98,736 84,930 75,180 26,207 1400Median number ofterms per record 182 353 181 313 79Average number ofterms per record 329 375 412 1017 88Number of uniqueterms 156,298 197,608 147,405 116,586 8226Table 11: Comparison of the TIPSTER collection to other collectionssamples of judged documents othe same judge for newassessments and the second issue will be addressed bysubmitting the same random samples to a different judgefor assessments.1.6 Comparison to Other CollectionsThe TIPSTER collection is many magnitudes of scalelarger than existing collections in several ways.
Firstthere are many more documents.
Table 11 shows a com-parison of the documents on disk 1 of the TIPSTER col-lection (about one-third of the full collection) to the Cran-field collection, ne of the oldest collections.
Even thelargest publically-available collection, the NPL collection,only has 11,429 documents.
Second, the documents arelonger, with most documents being full text as opposed tothe abstracts found in other collections.The topics are also longer and more complex.
This issuewas discussed earlier in the analysis ection, and thereforewill not be repeated here.
However the long and complextopics are the main research challenge of the TIPSTERcollection; the difference in document collection size anddocument length generally present system engineeringchallenges and cause less difficulty after the initial systemscaling-up.There are also many more relevant documents per topic.Older collections tend to have an average of ten or fewerrelevant documents per topic, whereas the average num-ber of relevant documents per topic for the TIPSTER col-lection is over 200.ject.
Topic development was also delayed ue to difficul-ty in discovering methods of constructing topics.
Seventopics were delivered in time for the 24-month evaluation,and results from the two TIPSTER contractors working inJapanese were assessed for relevance.
The developmentof a full collection in Japanese is a continuing effort.3.
REFERENCES\[1\] Belkin N.J. and Croft W.B.
Retrieval Techniques.
InWilliams, M.
(Ed.
), Annual Review of lnformation Scienceand Technology (pp.
109-145).
New York, NY: ElsevierScience Publishers, 1987.\[2\] Sparck Jones K. and Van Rijsbergen C. (1975).
Reporton the Need for and Provision of an "Ideal" InformationRetrieval Test Collection, British Library Research andDevelopment Report 5266, Computer Laboratory, Uni-versity of Cambridge.\[3\] Katzer J., McGill M.J., Tessier J.A., Frakes W., andDasGupta P. (1982).
A Study of the Overlap among Doc-ument Representations.
Information Technology: Re-search and Development, 1 (2), 261-274.2.
THE JAPANESE TEST  COLLECT IONThe design of the Japanese test collection was similar tothat of the English collection, with similar formats used inboth the documents and topics.
However because of thedifficulty in obtaining large amounts of Japanese data, theJapanese part of the project was slower to start.
The doc-uments, 206 megabytes (151,650 records), of the Nikkeinewspaper were distributed in the second year of the pro-31
