Proceedings of the SIGDIAL 2014 Conference, pages 310?317,Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational LinguisticsExtrinsic Evaluation of Dialog State Tracking and Predictive Metricsfor Dialog Policy OptimizationSungjin LeeLanguage Technologies Institute,Carnegie Mellon University,Pittsburgh, Pennsylvania, USAsungjin.lee@cs.cmu.eduAbstractDuring the recent Dialog State TrackingChallenge (DSTC), a fundamental questionwas raised: ?Would better performance indialog state tracking translate to betterperformance of the optimized policy byreinforcement learning??
Also, during thechallenge system evaluation, another non-trivial question arose: ?Which evaluationmetric and schedule would best predictimprovement in overall dialog performance?
?This paper aims to answer these questions byapplying an off-policy reinforcement learningmethod to the output of each challenge system.The results give a positive answer to the firstquestion.
Thus the effort to separately improvethe performance of dialog state tracking ascarried out in the DSTC may be justified.
Theanswer to the second question also drawsseveral insightful conclusions on thecharacteristics of different evaluation metricsand schedules.1 IntroductionStatistical approaches to spoken dialogmanagement have proven very effective ingracefully dealing with noisy input due toAutomatic Speech Recognition (ASR) andSpoken Language Understanding (SLU) error(Lee, 2013; Williams et al., 2013).
Most recentadvances in statistical dialog modeling have beenbased on the Partially Observable MarkovDecision Processes (POMDP) framework whichprovides a principled way for sequential actionplanning under uncertainty (Young et al., 2013).In this approach, the task of dialog managementis generally decomposed into two subtasks, i.e.,dialog state tracking and dialog policy learning.The aim of dialog state tracking is to accuratelyestimate the true dialog state from noisyobservations by incorporating patterns betweenturns and external knowledge as a dialog unfolds(Fig.
1).
The dialog policy learning process thenstrives to select an optimal system action giventhe estimated dialog state.Many dialog state tracking algorithms havebeen developed.
Few studies, however, havereported the strengths and weaknesses of eachmethod.
Thus the Dialog State TrackingChallenge (DSTC) was organized to advancestate-of-the-art technologies for dialog statetracking by allowing for reliable comparisonsbetween different approaches using the samedatasets (Williams et al., 2013).
Thanks to theDSTC, we now have a better understanding ofeffective models, features and training methodswe can use to create a dialog state tracker that isnot only of superior performance but also veryrobust to realistic mismatches betweendevelopment and deployment environments (Leeand Eskenazi, 2013).Despite the fruitful results, it was largelylimited to intrinsic evaluation, thus leaving animportant question unanswered: ?Would theimproved performance in dialog state trackingcarry over to dialog policy optimization?
?Furthermore, there was no consensus on whatand when to measure, resulting in a large set ofmetrics being evaluated with three differentschedules.
With this variety of metrics, it is notclear what the evaluation result means.
Thus it isimportant to answer the question: ?Which metricbest serves as a predictor to the improvement indialog policy optimization?
since this is theultimate goal, in terms of end-to-end dialogperformance.
The aim of this paper is to answerthese two questions via corpus-basedexperiments.
Similar to the rationale behind theDSTC, the corpus-based design allows us to310compare different trackers on the same data.
Weapplied a sample efficient off-policyreinforcement learning (RL) method to theoutputs of each tracker so that we may examinethe relationship between the performance ofdialog state tracking and that of the optimizedpolicy as well as which metric shows the highestcorrelation with the performance of theoptimized policy.This paper is structured as follows.
Section 2briefly describes the DSTC and the metricsadopted in the challenge.
Section 3 elaborates onthe extrinsic evaluation method based on off-policy RL.
Section 4 presents the extrinsicevaluation results and discusses its implicationon metrics for dialog state tracking evaluation.Finally, Section 5 concludes with a briefsummary and suggestions for future research.2 DSTC Task and Evaluation MetricsThis section briefly describes the task for theDSTC and evaluation metrics.
For more details,please refer to the DSTC manual1.1 http://research.microsoft.com/apps/pubs/?id=1690242.1 Task DescriptionDSTC data is taken from several differentspoken dialog systems which all provided busschedule information for Pittsburgh,Pennsylvania, USA (Raux et al., 2005) as part ofthe Spoken Dialog Challenge (Black et al., 2011).There are 9 slots which are evaluated: route,from.desc, from.neighborhood, from.monument,to.desc, to.neighborhood, to.monument, date, andtime.
Since both marginal and jointrepresentations of dialog states are important fordeciding dialog actions, the challenge takes bothinto consideration.
Each joint representation is anassignment of values to all slots.
Thus there are9 marginal outputs and 1 joint output in total,which are all evaluated separately.The dialog tracker receives the SLU N-besthypotheses for each user turn, each with aconfidence score.
In general, there are a largenumber of values for each slot, and the coverageof N-best hypotheses is good, thus the challengeconfines its determination of whether a goal hasbeen reached to slots and values that have beenobserved in an SLU output.
By exploiting thisaspect, the task of a dialog state tracker is togenerate a set of observed slot and value pairs,with a score between 0 and 1.
The sum of allFigure 1: An example of dialog state tracking for the Route slot.
At each turn the system asks for user?sgoal or attempts to confirm one of hypotheses.
The user?s utterance is recognized to output an N-bestlist.
The SLU module generates semantic inputs to the dialog manager by parsing the N-besthypotheses.
Each SLU hypothesis receives a confidence score.
From the current turn?s SLUhypotheses and all previous ones thus far, the dialog state tracker computes a probability distributionover a set of dialog state hypotheses.
Note that the number of hypotheses in a dialog state can bedifferent from the number of SLU hypotheses, e.g., at turn t+1, 3 and 5 respectively.311scores is restricted to sum to 1.0.
Thus 1.0 ?
totalscore is defined as the score of a special valueNone that indicates the user?s goal has not yetappeared on any SLU output.2.2 Evaluation MetricsTo evaluate tracker output, the correctness ofeach hypothesis is labeled at each turn.
Thenhypothesis scores and labels over the entiredialogs are collected to compute 11 metrics:?
Accuracy measures the ratio of states underevaluation where the top hypothesis iscorrect.?
ROC.V1 computes the following quantity:( )( )where   is the total number of tophypotheses over the entire data and   ( )denotes the number of correctly accepted tophypotheses with the threshold being set to  .Similarly FA denotes false-accepts and FRfalse-rejects.
From these quantities, severalmetrics are derived.
ROC.V1.EERcomputes FA.V1(s) where FA.V1(s) =FR.V1(s).
The metrics ROC.V1.CA05,ROC.V1.CA10, and ROC.V1.CA20compute CA.V1(s) when FA.V1(s) = 0.05,0.10, and 0.20 respectively.
These metricsmeasure the quality of score via plottingaccuracy with respect to false-accepts so thatthey may reflect not only accuracy but alsodiscrimination.?
ROC.V2 computes the conventional ROCquantity:( )( )( )ROC.V2.CA05, ROC.V2.CA10, andROC.V2.CA20 do the same as the V1versions.
These metrics measure thediscrimination of the score for the tophypothesis independently of accuracy.Note that Accuracy and ROC curves do not takeinto consideration non-top hypotheses while thefollowing measures do.?
L2 calculates the Euclidean distancebetween the vector consisting of the scoresof all hypotheses and a zero vector with 1 inthe position of the correct one.
Thismeasures the quality of tracker?s outputscore as probability.?
AvgP indicates the averaged score of thecorrect hypothesis.
Note that this measuresthe quality of the score of the correcthypothesis, ignoring the scores assigned toincorrect hypotheses.?
MRR denotes the mean reciprocal rank ofthe correct hypothesis.
This measures thequality of rank instead of score.As far as evaluation schedule is concerned, thereare three schedules for determining which turnsto include in each evaluation.?
Schedule 1: Include all turns.
This scheduleallows us to account for changes in conceptsthat are not in focus.
But this makes across-concept comparison invalid since differentconcepts appear at different times in a dialog.?
Schedule 2: Include a turn for a givenconcept only if that concept either appears onthe SLU N-Best list in that turn, or if thesystem?s action references that concept inthat turn.
Unlike schedule 1, this schedulemakes comparisons across concepts valid butcannot account for changes in conceptswhich are not in focus.?
Schedule 3: Include only the turn before thesystem starts over from the beginning, andthe last turn of the dialog.
This schedule doesnot consider what happens during a dialog.3 Extrinsic Evaluation Using Off-PolicyReinforcement LearningIn this section, we present a corpus-basedmethod for extrinsic evaluation of dialog statetracking.
Thanks to the corpus-based designwhere outputs of various trackers with differentcharacteristics are involved, it is possible toexamine how the differences between trackersaffect the performance of learned policies.
Theperformance of a learned policy is measured bythe expected return at the initial state of a dialogwhich is one of the common performancemeasures for episodic tasks.3.1 Off-Policy RL on Fixed DataTo learn an optimal policy from fixed data, weapplied a state-of-the-art kernelized off-policyRL method.
Off-policy RL methods allows foroptimization of a policy by observing how otherpolicies behave.
The policy used to control the312system?s behavior is called Behavior policy.
Asfar as a specific algorithm is concerned, we haveadopted Least-Squares Temporal Difference(LSTD) (Bradtke and Barto, 1996) for policyevaluation and Least-Squares Policy Iteration(LSPI) (Lagoudakis and Parr, 2003) for policylearning.
LSTD and LSPI have been well knownto be sample efficient, thus easily lendingthemselves to the application of RL to fixed data(Pietquin et al., 2011).
LSPI is an instance ofApproximate Policy Iteration where anapproximated action-state value function (a.k.a Qfunction) is established for a current policy andan improved policy is formed by taking greedyactions with respect to the estimated Q function.The process of policy evaluation andimprovement iterates until convergence.
Forvalue function approximation, in this work, weadopted the following linear approximationarchitecture:?
(   )(   )where   is the set of parameters,  (   )  anactivation vector of basis functions,   a state andan action.
Given a policy    and a set of statetransitions  (             )      , where    is thereward that the system would get from theenvironment by executing action    at state   ,the approximated state-action value function  ?is estimated by LSTD.
The most important partof LSTD lies in the computation of the gradientof temporal difference:(   )    (    (  ))In LSPI,  (  ) takes the form of greedy policy:(  )?
()It is however critical to take into considerationthe inherent problem of insufficient explorationin fixed data to avoid overfitting (Henderson etal., 2008).
Thus we confined the set of availableactions at a given state to the ones that have anoccurrence probability greater than somethreshold  :(  )(  |  )?
()The conditional probability  (  |  )  can beeasily estimated by any conventionalclassification methods which provide posteriorprobability.
In this study, we set   to 0.1.3.2 State Representation and Basis FunctionIn order to make the process of policyoptimization tractable, the belief state isnormally mapped to an abstract space by onlytaking crucial information for dialog actionselection, e.g., the beliefs of the top and secondhypotheses for a concept.
Similarly, the actionspace is also mapped into a smaller space byonly taking the predicate of an action.
In thiswork, the simplified state includes the followingelements:?
The scores of the top hypothesis for eachconcept with None excluded?
The scores of the second hypothesis for eachconcept with None excluded?
The scores assigned to None for eachconcept?
Binary indicators for a concept if there arehypotheses except None?
The values of the top hypothesis for eachconcept?
A binary indicator if the user affirms whenthe system asks a yes-no question for nextbusIt has been shown that the rapid learning speedof recent approaches is partly attributed to theuse of kernels as basis functions (Gasic et al.,2010; Lee and Eskenazi, 2012; Pietquin et al.,2011).
Thus to make the best of the limitedamount of data, we adopted a kernelizedapproach.
Similar to previous studies, we used aproduct of kernel functions:(    )    ()?
()where   (   )  is responsible for a vector ofcontinuous elements of a state and   (   )  foreach discrete element.
For the continuouselements, we adopted Gaussian kernels:()       (??
)where   governs the value at center,   controlsthe width of the kernel and   represents thevector of continuous elements of a state.
In theexperiments,  and   were set to 4 and 3,313respectively.
For a discrete element, we adopteddelta kernel:()     ()where   ()  returns one if     , zerootherwise and    represents an element of a state.As the number of data points increases,kernelized approaches commonly encountersevere computational problems.
To address thisissue, it is necessary to limit the active kernelfunctions being used for value functionapproximation.
This sparsification process has tofind out the sufficient number of kernels whichkeeps a good balance between computationaltractability and approximation quality.
Weadopted a simple sparsification method whichwas commonly used in previous studies (Engel etal., 2004).
The key intuition behind of thesparsification method is that there is a mapping( )  to a Hilbert space in which the kernelfunction  (    )  is represented as the innerproduct of  ( )  and  (  )  by the Mercer?stheorem.
Thus the kernel-based representation ofQ function can be restated as a plain linearequation in the Hilbert space:?
( )  ?
()  ?
( ) ?
()?where   denotes the pair of state and action.
Theterm ?
()  plays the role of the weightvector in the Hilbert space.
Since this term takesthe form of linear combination, we can safelyremove any linearly dependent  ()  withoutchanging the weighted sum by tuning  .
It isknown that the linear dependence of  ( ) fromthe rest can be tested based on kernel functionsas follows:(     )      (  )(1)where          (       )  (       )and   is a sparsification threshold.
Whenequation 1 is satisfied,    can be safely removedfrom the set of basis functions.
Thus the sparsitycan be controlled by changing  .
It can be shownthat equation 1 is minimized when(  ) , whereis the Gram matrixexcluding   .
In the experiments,   was set to 3.3.3 Reward FunctionThe reward function is defined following acommon approach to form-filling, task-orientedsystems:?
Every correct concept filled is rewarded 100?
Every incorrect concept filled is assigned-200?
Every empty concept is assigned -300 if thesystem terminated the session, -50 otherwise.?
At every turn, -20 is assignedThe reward structure is carefully designed suchthat the RL algorithm cannot find a way tomaximize the expected return without achievingthe user goal.4 Experimental SetupIn order to see the relationship between theperformance of dialog state tracking and that ofthe optimized policy, we applied the off-policyRL method presented in Section 3 to the outputsof each tracker for all four DSTC test datasets2.The summary statistics of the datasets arepresented in Table 1.
In addition, to quantify theimpact of dialog state tracking on an end-to-enddialog, the performance of policies optimized byRL was compared with Behavior policies andanother set of learned policies using supervisedlearning (SL).
Note that Behavior policies weredeveloped by experts in spoken dialog research.The use of a learned policy using supervised2 We took the entry from each team that achieved thehighest ranks of that team in the largest number ofevaluation metrics: entry2 for team3 and team6,entry3 for team8, entry4 for team9, and entry1 for therest of the teams.
We were not, however, able toprocess the tracker output of team2 due to its largesize.
This does not negatively impact the generalresults of this paper.# Dialogs # TurnsTraining Test Training TestDS1 274 312 2594 2168DS2 321 339 3394 2579DS3 277 286 2221 1988DS4 141 165 1060 979Table 1: The DSTC test datasets (DS1-4)were evenly divided into two groups ofdatasets for off-policy RL training and test.
Tosimplify the analysis, the dialogs that includestartover and canthelp were excluded.314learning (Hurtado et al., 2005) is also one of thecommon methods of spoken dialog systemdevelopment.
We exploited the SVM methodwith the same kernel functions as defined inSection 3.2 except that the action element is notincluded.
The posterior probability of the SVMmodel was also used for handling the insufficientexploration problem (in Section 3.1).5 Results and DiscussionThe comparative results between RL, SL andBehavior policies are plotted in Fig.
2.
Despitethe relatively superior performance of SLpolicies over Behavior policies, the performanceimprovement is neither large nor constant.
Thisconfirms that Behavior policies are very strongbaselines which were designed by expertresearchers.
RL policies, however, consistentlyoutperformed Behavior as well as SL policies,with a large performance gap.
This resultindicates that the policies learned by theproposed off-policy RL method are a lot closer tooptimal ones than the hand-crafted policiescreated by human experts.
Given that many statefeatures are derived from the belief state, thelarge improvement in performance implies thatthe estimated belief state is indeed a goodsummary representation of a state, maintainingthe Markov property between states.
The Markovproperty is a crucial property for RL methods toapproach to the optimal policy.
On the otherhand, most of the dialog state trackers surpassedthe baseline tracker (team0) in the performanceof RL policies.
This result assures that the betterthe performance in dialog state tracking, thebetter a policy we can learn in the policyoptimization stage.
Given these two results, wecan strongly assert that dialog state trackingplays a key role in enhancing end-to-end dialogperformance.Another interesting result worth noticing isthat the performance of RL policies does notexactly align with the accuracy measured at theend of a dialog (Schedule 3) which would havebeen the best metric if the task were a one-timeclassification (Fig.
2).
This misalignmenttherefore supports the speculation that accuracy-schedule3 might not be the most appropriatemetric for predicting the effect of dialog statetracking on end-to-end dialog performance.
Inorder to better understand What To Measure andWhen To Measure to predict end-to-end dialogperformance, a correlation analysis was carriedout between the performance of RL policies andthat of the dialog state tracking measured bydifferent metrics and schedules.
The correlationsare listed in descending order in Fig.
3.
Thisresult reveals several interesting insights fordifferent metrics.First, metrics which are intended to measurethe quality of a tracker?s score (e.g., L2 andAvgP) are more correlated than other metrics.This tendency can be understood as aconsequence of the sequential decision-makingnature of a dialog task.
A dialog system canalways initiate an additional turn, unless the userFigure 2: The left vertical axis is associated with the performance plots of RL, SL and Behaviorpolicies for each team.
The right vertical axis measures the accuracies of each team?s tracker at the endof a dialog (schedule 3).315terminates the session, to refine its belief statewhen there is no dominant hypothesis.
Thusaccurate estimation of the beliefs of all observedhypotheses is essential.
This is why theevaluation of only the top hypothesis does notprovide sufficient information.Second, schedule1 and schedule3 showed astronger correlation than schedule2.
In factschedule2 was more preferred in previous studiessince it allows for a valid comparison of differentconcepts (Williams, 2013; Williams et al., 2013).This result can be explained by the fact that thebest system action is selected by considering allof the concepts together.
For example, when thesystem moves the conversation focus from oneconcept to another, the beliefs of the conceptsthat are not in focus are as important as theconcept in focus.
Thus evaluating all concepts atthe same time is more suitable for predicting theperformance of a sequential decision-makingtask involving multiple concepts in its state.Finally, metrics for evaluating discriminationquality (measured by ROC.V2) have littlecorrelation with end-to-end dialog performance.In order to understand this relatively unexpectedresult, we need to give deep thought to how thescores of a hypothesis are distributed during thesession.
For example, the score of a truehypothesis usually starts from a small value dueto the uncertainty of ASR output and gets biggerevery time positive evidence is observed.
Thescore of a false hypothesis usually stays small ormedium.
This leads to a situation where both trueand false hypotheses are pretty much mixed inthe zone of small and medium scores withoutsignificant discrimination.
It is, however, veryimportant for a metric to reveal a differencebetween true and false hypotheses before theirscores fully arrive at sufficient certainty sincemost additional turns are planned for hypotheseswith a small or medium score.
Thus generalmetrics evaluating discrimination alone arehardly appropriate for a tracking problem wherethe score develops gradually.
Furthermore, thechoice of threshold (i.e.
FA = 0.05, 0.10, 0.20)was made to consider relatively unimportantregions where the true hypothesis is likely tohave a higher score, meaning that no furtherturns need to be planned.6 ConclusionIn this paper, we have presented a corpus-basedstudy that attempts to answer two fundamentalquestions which, so far,  have not beenrigorously addressed: ?Would betterperformance in dialog state tracking translate tobetter performance of the optimized policy byRL??
and ?Which evaluation metric andschedule would best predict improvement inoverall dialog performance??
The resultsupports a positive answer to the first question.Thus the effort to separately improve theperformance of dialog state tracking as carriedout in the recent held DSTC may be justified.
Asa way to address the second question, thecorrelations of different metrics and schedulesFigure 3: The correlations of each combination of metric and schedule with the performance ofoptimized polices.316with the performance of optimized policies werecomputed.
The results revealed several insightfulconclusions: 1) Metrics measuring score qualityare more suitable for predicting the performanceof an optimized policy.
2) Evaluation of allconcepts at the same time is more appropriate forpredicting the performance of a sequentialdecision making task involving multipleconcepts in its state.
3) Metrics evaluating onlydiscrimination (e.g., ROC.V2) are inappropriatefor a tracking problem where the score graduallydevelops.
Interesting extensions of this workinclude finding a composite measure ofconventional metrics to obtain a better predictor.A data-driven composition may tell us therelative empirical importance of each metric.
Inspite of several factors which generalize ourconclusions such as handling insufficientexploration, the use of separate test sets andvarious mismatches between test sets, it is stilldesirable to run different policies for live tests inthe future.
Also, since the use of an approximatepolicy evaluation method (e.g.
LSTD) canintroduce systemic errors, more deliberateexperimental setups will be designed for a futurestudy: 1) the application of different RLalgorithms for training and test datasets 2)further experiments on different datasets, e.g.,the datasets for DSTC2 (Henderson et al., 2014).Although the state representation adopted in thiswork is quite common for most systems that usea POMDP model, different state representationscould possibly reveal new insights.ReferencesA.
Black et al., 2011.
Spoken dialog challenge 2010:Comparison of live and control test results.
InProceedings of SIGDIAL.S.
Bradtke and A. Barto, 1996.
Linear Least-Squaresalgorithms for temporal difference learning.Machine Learning, 22, 1-3, 33-57.Y.
Engel, S. Mannor and R. Meir, 2004.
The KernelRecursive Least Squares Algorithm.
IEEETransactions on Signal Processing, 52:2275-2285.M.
Gasic and S. Young, 2011.
Effective handling ofdialogue state in the hidden information statePOMDP-based dialogue manager.
ACMTransactions on Speech and Language Processing,7(3).M.
Gasic, F. Jurcicek, S. Keizer, F. Mairesse, B.Thomson, K. Yu and S. Young, 2010.
GaussianProcesses for Fast Policy Optimisation of POMDP-based Dialogue Managers, In Proceedings ofSIGDIAL, 2010.J.
Henderson, O.
Lemon and K. Georgila, 2008.Hybrid reinforcement/supervised learning ofdialogue policies from fixed data sets.Computational Linguistics, 34(4):487-511.M.
Henderson, B. Thomson and J. Williams, 2014.The Second Dialog State Tracking Challenge.
InProceedings of SIGDIAL, 2014.L.
Hurtado, D. Grial, E. Sanchis and E. Segarra, 2005.A Stochastic Approach to Dialog Management.
InProceedings of ASRU, 2005.M.
Lagoudakis and R. Parr, 2003.
Least-squarespolicy iteration.
Journal of Machine LearningResearch 4, 1107-1149.S.
Lee, 2013.
Structured Discriminative Model ForDialog State Tracking.
In Proceedings of SIGDIAL,2013.S.
Lee and M. Eskenazi, 2012.
Incremental SparseBayesian Method for Online Dialog StrategyLearning.
IEEE Journal of Selected Topics inSignal Processing, 6(8).S.
Lee and M. Eskenazi, 2013.
Recipe For BuildingRobust Spoken Dialog State Trackers: Dialog StateTracking Challenge System Description.
InProceedings of SIGDIAL, 2013.O.
Pietquin, M. Geist, S. Chandramohan and H.Frezza-buet, 2011.
Sample Efficient BatchReinforcement Learning for Dialogue ManagementOptimization.
ACM Transactions on Speech andLanguage Processing, 7(3).O.
Pietquin, M. Geist, and S. Chandramohan, 2011.Sample Efficient On-Line Learning of OptimalDialogue Policies with Kalman TemporalDifferences.
In Proceedings of IJCAI, 2011.A.
Raux, B. Langner, D. Bohus, A. W Black, and M.Eskenazi, 2005.
Let?s Go Public!
Taking a SpokenDialog System to the Real World.
In Proceedingsof Interspeech.J.
Williams, 2013.
Multi-domain learning andgeneralization in dialog state tracking.
InProceedings of SIGDIAL, 2013.J.
Williams, A. Raux, D. Ramachandran and A. Black,2013.
The Dialog State Tracking Challenge.
InProceedings of SIGDIAL, 2013.S.
Young, M. Gasic, B. Thomson and J. Williams2013.
POMDP-based Statistical Spoken DialogueSystems: a Review.
IEEE, 101(5):1160-1179.317
