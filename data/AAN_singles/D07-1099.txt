Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
947?951,Prague, June 2007. c?2007 Association for Computational LinguisticsFast and Robust Multilingual Dependency Parsingwith a Generative Latent Variable ModelIvan TitovUniversity of Geneva24, rue Ge?ne?ral DufourCH-1211 Gene`ve 4, Switzerlandivan.titov@cui.unige.chJames HendersonUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, United Kingdomjames.henderson@ed.ac.ukAbstractWe use a generative history-based model topredict the most likely derivation of a de-pendency parse.
Our probabilistic model isbased on Incremental Sigmoid Belief Net-works, a recently proposed class of la-tent variable models for structure predic-tion.
Their ability to automatically in-duce features results in multilingual pars-ing which is robust enough to achieve accu-racy well above the average for each indi-vidual language in the multilingual track ofthe CoNLL-2007 shared task.
This robust-ness led to the third best overall average la-beled attachment score in the task, despiteusing no discriminative methods.
We alsodemonstrate that the parser is quite fast, andcan provide even faster parsing times with-out much loss of accuracy.1 IntroductionThe multilingual track of the CoNLL-2007 sharedtask (Nivre et al, 2007) considers dependency pars-ing of texts written in different languages.
It re-quires use of a single dependency parsing modelfor the entire set of languages; model parametersare estimated individually for each language on thebasis of provided training sets.
We use a recentlyproposed dependency parser (Titov and Hender-son, 2007b)1 which has demonstrated state-of-the-art performance on a selection of languages from the1The ISBN parser will be soon made downloadable from theauthors?
web-page.CoNLL-X shared task (Buchholz and Marsi, 2006).This parser employs a latent variable model, Incre-mental Sigmoid Belief Networks (ISBNs), to de-fine a generative history-based model of projectiveparsing.
We used the pseudo-projective transforma-tion introduced in (Nivre and Nilsson, 2005) to castnon-projective parsing tasks as projective.
Follow-ing (Nivre et al, 2006), the encoding scheme calledHEAD in (Nivre and Nilsson, 2005) was used to en-code the original non-projective dependencies in thelabels of the projectivized dependency tree.
In thefollowing sections we will briefly discuss our modi-fications to the ISBN parser, experimental setup, andachieved results.2 The Probability ModelOur probability model uses the parsing order pro-posed in (Nivre et al, 2004), but instead of perform-ing deterministic parsing as in (Nivre et al, 2004),this ordering is used to define a generative history-based model, by adding word prediction to the Shiftparser action.
We also decomposed some parser ac-tions into sub-sequences of decisions.
We split arcprediction decisions (Left-Arcr and Right-Arcr) eachinto two elementary decisions: first the parser cre-ates the corresponding arc, then it assigns a relationr to the arc.
Similarly, we decompose the decisionto shift a word into a decision to shift and a pre-diction of the word.
We used part-of-speech tagsand fine-grain word features, which are given in thedata, to further decompose word predictions.
Firstwe predict the fine-grain part-of-speech tag for theword, then the set of word features (treating eachset as an atomic value), and only then the particu-947lar word form.
This approach allows us to both de-crease the effect of sparsity and to avoid normaliza-tion across all the words in the vocabulary, signifi-cantly reducing the computational expense of wordprediction.
When conditioning on words, we treatedeach word feature individually, as this proved to beuseful in (Titov and Henderson, 2007b).The probability of each parser decision, condi-tioned on the complete parse history, is modeledusing a form a graphical model called IncrementalSigmoid Belief Networks.
ISBNs, originally pro-posed for constituent parsing in (Titov and Hender-son, 2007a), use vectors of binary latent variables toencode information about the parse history.
Thesehistory variables are similar to the hidden state ofa Hidden Markov Model.
But unlike the graphi-cal model for an HMM, which would specify con-ditional dependency edges only between adjacentstates in the parse history, the ISBN graphical modelcan specify conditional dependency edges betweenlatent variables which are arbitrarily far apart in theparse history.
The source state of such an edge isdetermined by the partial parse structure built at thetime of the destination state, thereby allowing theconditional dependency edges to be appropriate forthe structural nature of the parsing problem.
In par-ticular, they allow conditional dependencies to belocal in the parse structure, not just local in the his-tory sequence.
In this they are similar to the classof neural networks proposed in (Henderson, 2003)for constituent parsing.
In fact, in (Titov and Hen-derson, 2007a) it was shown that this neural networkcan be viewed as a coarse approximation to the cor-responding ISBN model.Traditional statistical parsing models also condi-tion on features which are local in the parse struc-ture, but these features need to be explicitly definedbefore learning, and require careful feature selec-tion.
This is especially difficult for languages un-known to the parser developer, since the number ofpossible features grows exponentially with the struc-tural distance considered.The ISBN model uses an alternative approach,where latent variables are used to induce featuresduring learning.
The most important problem in de-signing an ISBN is to define an appropriate struc-tural locality for each parser decision.
This is doneby choosing a fixed set of relationships betweenparser states, where the information which is neededto make the decision at the earlier state is also use-ful in making the decision at the later state.
The la-tent variables for these related states are then con-nected with conditional dependency edges in theISBN graphical model.
Longer conditional depen-dencies are then possible through chains of these im-mediate conditional dependencies, but there is an in-ductive bias toward shorter chains.
This bias makesit important that the set of chosen relationships de-fines an appropriate notion of locality.
However,as long as there exists some chain of relationshipsbetween any two states, then any statistical depen-dency which is clearly manifested in the data can belearned, even if it was not foreseen by the designer.This provides a potentially powerful form of featureinduction, which is nonetheless biased toward a no-tion of locality appropriate for the nature of the prob-lem.In our experiments we use the same definition ofstructural locality as was proposed for the ISBN de-pendency parser in (Titov and Henderson, 2007b).The current state is connected to previous states us-ing a set of 7 distinct relationships defined in termsof each state?s parser configuration, which includesof a stack and a queue.
Specifically, the current stateis related to the last previous state whose parser con-figuration has: the same queue, the same stack, astack top which is the rightmost right child of thecurrent stack top, a stack top which is the leftmostleft child of the current stack top, a front of the queuewhich is the leftmost child of the front of the cur-rent queue, a stack top which is the head word ofthe current stack top, a front of the queue which isthe current stack top.
Different model parametersare trained for each of these 7 types of relationship,but the same parameters are used everywhere in thegraphical model where the relationship holds.Each latent variable in the ISBN parser is alsoconditionally dependent on a set of explicit featuresof the parsing history.
As long as these explicit fea-tures include all the new information from the lastparser decision, the performance of the model is notvery sensitive to this design choice.
We used thebase feature model defined in (Nivre et al, 2006)for all the languages but Arabic, Chinese, Czech,and Turkish.
For Arabic, Chinese, and Czech, weused the same feature models used in the CoNLL-X948shared task by (Nivre et al, 2006), and for Turkishwe used again the base feature model but extendedit with a single feature: the part-of-speech tag of thetoken preceding the current top of the stack.3 ParsingExact inference in ISBN models is not tractable, buteffective approximations were proposed in (Titovand Henderson, 2007a).
Unlike (Titov and Hender-son, 2007b), in the shared task we used only thesimplest feed-forward approximation, which repli-cates the computation of a neural network of the typeproposed in (Henderson, 2003).
We would expectbetter performance with the more accurate approxi-mation based on variational inference proposed andevaluated in (Titov and Henderson, 2007a).
We didnot try this because, on larger treebanks it wouldhave taken too long to tune the model with this bet-ter approximation, and using different approxima-tion methods for different languages would not becompatible with the shared task rules.To search for the most probable parse, we use theheuristic search algorithm described in (Titov andHenderson, 2007b), which is a form of beam search.In section 4 we show that this search leads to quiteefficient parsing.To overcome a minor shortcoming of the pars-ing algorithm of (Nivre et al, 2004) we introduce asimple language independent post-processing step.Nivre?s parsing algorithm allows unattached nodesto stay on the stack at the end of parsing, which isreasonable for treebanks with unlabeled attachmentto root.
However, this sometimes happens with lan-guages where only labeled attachment to root is al-lowed.
In these cases (only 35 tokens in Greek, 17in Czech, 1 in Arabic, on the final testing set) weattached them using a simple rule: if there are notokens in the sentence attached to root, then the con-sidered token is attached to root with the most fre-quent root-attachment relation used for its part-of-speech tag.
If there are other root-attached tokens inthe sentence, it is attached to the next root-attachedtoken with the most frequent relation.
Preference isgiven to the most frequent attachment direction forits part-of-speech tag.
This rule guarantees that noloops are introduced by the post-processing.4 ExperimentsWe evaluated the ISBN parser on all the languagesconsidered in the shared task (Hajic?
et al, 2004;Aduriz et al, 2003; Mart??
et al, 2007; Chen etal., 2003; Bo?hmova?
et al, 2003; Marcus et al,1993; Johansson and Nugues, 2007; Prokopidis etal., 2005; Csendes et al, 2005; Montemagni et al,2003; Oflazer et al, 2003).
ISBN models weretrained using a small development set taken out fromthe training set, which was used for tuning learn-ing and decoding parameters, for early stopping andvery coarse feature engineering.2 The sizes of thedevelopment sets were different: starting from lessthan 2,000 tokens for smaller treebanks to 5,000 to-kens for the largest one.
The relatively small sizesof the development sets limited our ability to per-form careful feature selection, but this should nothave significantly affected the model performance,as discussed in section 2.3 We used frequency cut-offs: we ignored any property (word form, lemma,feature) which occurs in the training set less thana given threshold.
We used a threshold of 20 forGreek and Chinese and a threshold of 5 for the rest.Because cardinalities of each of these sets (sets ofword forms, lemmas and features) effect the modelefficiency, we selected the larger threshold when val-idation results with the smaller threshold were com-parable.
For the ISBN latent variables, we used vec-tors of length 80, based on our previous experience.Results on the final testing set are presented in ta-ble 1.
The model achieves relatively high scores oneach individual language, significantly better thaneach average result in the shared task.
This leadsto the third best overall average results in the sharedtask, both in average labeled attachment score andin average unlabeled attachment score.
The absoluteerror increase in labeled attachment score over thebest system is only 0.4%.
We attribute ISBN?s suc-cess mainly to its ability to automatically induce fea-tures, as this significantly reduces the risk of omit-ting any important highly predictive features.
Thismakes an ISBN parser a particularly good baselinewhen considering a new treebank or language, be-2We plan to make all the learning and decoding parametersavailable on our web-page.3Use of cross-validation with our model is relatively time-consuming and, thus, not quite feasible for the shared task.949Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur AveLAS 74.1 75.5 87.4 82.1 77.9 88.4 73.5 77.9 82.3 79.8 79.90UAS 83.2 81.9 93.4 87.9 84.2 89.7 81.2 82.2 86.3 86.2 85.62Table 1: Labeled attachment score (LAS) and unlabeled attachment score (UAS) on the final testing sets78.57979.58080.5810  20  40  60  80  100  120  140AverageLASParsing Time per Token, msFigure 1: Average labeled attachment score onBasque, Chinese, English, and Turkish developmentsets as a function of parsing time per tokencause it does not require much effort in feature en-gineering.
As was demonstrated in (Titov and Hen-derson, 2007b), even a minimal set of local explicitfeatures achieves results which are non-significantlydifferent from a carefully chosen set of explicit fea-tures, given the language independent definition oflocality described in section 2.It is also important to note that the model isquite efficient.
Figure 1 shows the tradeoff be-tween accuracy and parsing time as the width of thesearch beam is varied, on the development set.
Thiscurve plots the average labeled attachment scoreover Basque, Chinese, English, and Turkish as afunction of parsing time per token.4 Accuracy ofonly 1% below the maximum can be achieved withaverage processing time of 17 ms per token, or 60tokens per second.5We also refer the reader to (Titov and Henderson,2007b) for more detailed analysis of the ISBN de-pendency parser results, where, among other things,it was shown that the ISBN model is especially ac-curate at modeling long dependencies.4A piecewise-linear approximation for each individual lan-guage was used to compute the average.
Experiments were runon a standard 2.4 GHz desktop PC.5For Basque, Chinese, and Turkish this time is below 7 ms,but for English it is 38 ms. English, along with Catalan, requiredthe largest beam across all 10 languages.
Note that accuracy inthe lowest part of the curve can probably be improved by vary-ing latent vector size and frequency cut-offs.
Also, efficiencywas not the main goal during the implementation of the parser,and it is likely that a much faster implementation is possible.5 ConclusionWe evaluated the ISBN dependency parser in themultilingual shared task setup and achieved com-petitive accuracy on every language, and the thirdbest average score overall.
The proposed model re-quires minimal design effort because it relies mostlyon automatic feature induction, which is highly de-sirable when using new treebanks or languages.
Theparsing time needed to achieve high accuracy is alsoquite small, making this model a good candidate foruse in practical applications.The fact that our model defines a probabilitymodel over parse trees, unlike the previous state-of-the-art methods (Nivre et al, 2006; McDonald etal., 2006), makes it easier to use this model in ap-plications which require probability estimates, suchas in language processing pipelines or for languagemodeling.
Also, as with any generative model,it should be easy to improve the parser?s accu-racy with discriminative reranking, such as discrim-inative retraining techniques (Henderson, 2004) ordata-defined kernels (Henderson and Titov, 2005),with or even without the introduction of any addi-tional linguistic features.AcknowledgmentsThis work was funded by Swiss NSF grant 200020-109685, UK EPSRC grant EP/E019501/1, and EUFP6 grant 507802 for project TALK.ReferencesA.
Abeille?, editor.
2003.
Treebanks: Building and UsingParsed Corpora.
Kluwer.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
Diaz de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proc.
of the 2nd Workshop on Treebanks and Lin-guistic Theories (TLT), pages 201?204.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.950Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
of the Tenth Conference on Computational Nat-ural Language Learning, New York, USA.K.
Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,and Z. Gao.
2003.
Sinica treebank: Design criteria,representational issues and implementation.
In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.D.
Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor.
2005.The Szeged Treebank.
Springer.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
?Snaidauf, and E. Bes?ka.2004.
Prague Arabic dependency treebank: Develop-ment in data and tools.
In Proc.
of the NEMLAR In-tern.
Conf.
on Arabic Language Resources and Tools,pages 110?117.James Henderson and Ivan Titov.
2005.
Data-definedkernels for parse reranking derived from probabilis-tic models.
In Proc.
43rd Meeting of Association forComputational Linguistics, Ann Arbor, MI.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Proc.joint meeting of North American Chapter of the Asso-ciation for Computational Linguistics and the HumanLanguage Technology Conf., pages 103?110, Edmon-ton, Canada.James Henderson.
2004.
Discriminative training ofa neural network statistical parser.
In Proc.
42ndMeeting of Association for Computational Linguistics,Barcelona, Spain.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.M.
A.
Mart?
?, M.
Taule?, L. Ma`rquez, and M. Bertran.2007.
CESS-ECE: A multilingual and multilevelannotated corpus.
Available for download from:http://www.lsi.upc.edu/?mbertran/cess-ece/.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proc.
of the Tenth Con-ference on Computational Natural Language Learn-ing, New York, USA.S.
Montemagni, F. Barsotti, M. Battista, N. Calzolari,O.
Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,M.
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,D.
Saracino, F. Zanzotto, N. Nana, F. Pianesi, andR.
Delmonte.
2003.
Building the Italian Syntactic-Semantic Treebank.
In Abeille?
(Abeille?, 2003), chap-ter 11, pages 189?210.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proc.
43rd Meeting of Asso-ciation for Computational Linguistics, pages 99?106,Ann Arbor, MI.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proc.
of theEighth Conference on Computational Natural Lan-guage Learning, pages 49?56, Boston, USA.Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,and Svetoslav Marinov.
2006.
Pseudo-projective de-pendency parsing with support vector machines.
InProc.
of the Tenth Conference on Computational Nat-ural Language Learning, pages 221?225, New York,USA.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).K.
Oflazer, B.
Say, D. Zeynep Hakkani-T u?r, and G. Tu?r.2003.
Building a Turkish treebank.
In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.P.
Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-georgiou, and S. Piperidis.
2005.
Theoretical andpractical issues in the construction of a Greek depen-dency treebank.
In Proc.
of the 4th Workshop on Tree-banks and Linguistic Theories (TLT), pages 149?160.Ivan Titov and James Henderson.
2007a.
Constituentparsing with incremental sigmoid belief networks.
InProc.
45th Meeting of Association for ComputationalLinguistics (ACL), Prague, Czech Republic.Ivan Titov and James Henderson.
2007b.
A latent vari-able model for generative dependency parsing.
InProc.
10th Int.
Conference on Parsing Technologies(IWPT), Prague, Czech Republic.951
