Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 332?339,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsAssessing the effectiveness of conversational features for dialoguesegmentation in medical team meetings and in the AMI corpusSaturnino LuzDepartment of Computer ScienceTrinity College Dublinm Irelandluzs@cs.tcd.ieJing SuSchool of Computer Science and StatisticsTrinity College Dublin, Irelandsujing@scss.tcd.ieAbstractThis paper presents a comparison of twosimilar dialogue analysis tasks: segment-ing real-life medical team meetings intopatient case discussions, and segment-ing scenario-based meetings into topics.In contrast to other methods which usetranscribed content and prosodic features(such as pitch, loudness etc), the methodused in this comparison employs only theduration of the prosodic units themselvesas the basis for dialogue representation.
Aconcept of Vocalisation Horizon (VH) al-lows us to treat segmentation as a clas-sification task where each instance to beclassified is represented by the durationof a talk spurt, pause or speech overlapevent in the dialogue.
We report on the re-sults this method yielded in segmentationof medical meetings, and on the implica-tions of the results of further experimentson a larger corpus, the Augmented Multi-party Meeting corpus, to our ongoing ef-forts to support data collection and infor-mation retrieval in medical team meetings.1 IntroductionAs computer mediated communication becomesmore widespread, and data gathering devices startto make their way into the meeting rooms and theworkplace in general, the need arises for mod-elling and analysis of dialogue and human com-municative behaviour in general (Banerjee et al,2005).
The focus of our interest in this area isthe study of multi-party interaction at Multidis-ciplinary Medical Team Meeting (MDTMs), andthe eventual recording of such meetings followedby indexing and structuring for integration intoelectronic health records.
MDTMs share a num-ber of characteristics with more conventional busi-ness meetings, and with the meeting scenarios tar-geted in recent research projects (Renals et al,2007).
However, MDTMs are better structuredthan these meetings, and more strongly influencedby the time pressures placed upon the medical pro-fessionals who take part in them (Kane and Luz,2006).In order for meeting support and review systemsto be truly effective, they must allow users to effi-ciently browse and retrieve information of interestfrom the recorded data.
Browsing in these mediacan be tedious and time consuming because con-tinuous media such as audio and video are difficultto access since they lack natural reference points.A good deal of research has been conducted on in-dexing recorded meetings.
From a user?s point ofview, an important aspect of indexing continuousmedia, and audio in particular, is the task of struc-turing the recorded content.
Banerjee et al (2005),for instance, showed that users took significantlyless time to retrieve answers when they had accessto discourse structure annotation than in a controlcondition in which they had access only to unan-notated recordings.The most salient discourse structure in a meet-ing is the topic of conversation.
The content withina given topic is cohesive and should therefore beviewed as a whole.
In MDTMs, the meeting con-sists basically of successive patient case discus-sions (PCDs) in which the patient?s condition isdiscussed among different medical specialists withthe objective of agreeing diagnoses, making pa-tient management decisions etc.
Thus, the individ-ual PCDs can be regarded as the different ?topics?which make up an MDTM (Luz, 2009).In this paper we explore the use of a content-free approach to the representation of vocalisationevents for segmentation of MDTM dialogues.
Westart by extending the work of Luz (2009) on asmall corpus of MDTM recordings, and then testour approach on a larger dataset, the AMI (Aug-332mented Multi-Party Interaction) corpus (Carletta,2007).
Our ultimate goal is to analyse and applythe insights gained on the AMI corpus to our workon data gathering and representation in MDTMs.2 Related workTopic segmentation and detection, as an aid tomeeting information retrieval and meeting index-ing, has attracted the interest of many researchersin recent years.
The objective of topic segmenta-tion is to locate the beginning and end time of acohesive segment of dialogue which can be sin-gled out as a ?topic?.
Meeting topic segmentationhas been strongly influenced by techniques devel-oped for topic segmentation in text (Hearst, 1997),and more recently in broadcast news audio, eventhough it is generally acknowledged that dialoguesegmentation differs from text and scripted speechin important respects (Gruenstein et al, 2005).In early work (Galley et al, 2003), meetingannotation focused on changes that produce highinter-annotator agreement, with no further specifi-cation of topic label or discourse structure.
Cur-rent work has paid greater attention to discoursestructure, as reflected in two major meeting cor-pus gathering and analysis projects: the AMIproject (Renals et al, 2007) and the ICSI meet-ing project (Morgan et al, 2001).
The AMI cor-pus distinguishes top-level and functional topicssuch as ?presentation?, ?discussion?, ?opening?,?closing?, ?agenda?
which are further specifiedinto sub-topics (Hsueh et al, 2006).
Gruensteinet al (2005) sought to annotated the ICSI cor-pus hierarchically according to topic, identifying,in addition, action items and decision points.
Incontrast to these more general types of meetings,MDTMs are segmented into better defined units(i.e.
PCDs) so that inter-annotator agreement ontopic (patient case discussion) boundaries is less ofan issue, since PCDs are collectively agreed partsof the formal structure of the meetings.Meeting transcripts (either done manually orautomatically) have formed the basis for a num-ber of approaches to topic segmentation (Galleyet al, 2003; Hsueh et al, 2006; Sherman and Liu,2008).
The transcript-based meeting segmentationdescribed in (Galley et al, 2003) adapted the un-supervised lexical cohesion method developed forwritten text segmentation (Hearst, 1997).
Otherapproaches have employed supervised machinelearning methods with textual features (Hsueh etal., 2006).
Prosodic and conversational featureshave also been integrated into text-based represen-tations, often improving segmentation accuracy(Galley et al, 2003; Hsueh and Moore, 2007).However, approaches that rely on transcription,and sometimes higher-level annotation on tran-scripts, as is the case of (Sherman and Liu, 2008),have two shortcomings which limit their applica-bility to MDTM indexing.
First, manual transcrip-tion is unfeasible in a busy hospital setting, andautomatic speech recognition of unconstrained,noisy dialogues falls short of the levels of accu-racy required for good segmentation.
Secondly,the contents of MDTMs are subject to stringentprivacy and confidentiality constraints which limitaccess to training data.
Regardless of such appli-cation constraints, some authors (Malioutov et al,2007; Shriberg et al, 2000) argue for the use ofprosodic features and other acoustic patterns di-rectly from the audio signal for segmentation.
Theapproach investigated in this paper goes a step fur-ther by representing the data solely through whatis, arguably, the simplest form of content-free rep-resentation, namely: duration of talk spurts, si-lences and speech overlaps, optionally comple-mented with speaker role information (e.g.
medi-cal speciality).3 Content-free representationsThere is more to the structure (and even thesemantics) of a dialogue than the textual con-tent of the words exchanged by its participants.The role of prosody in shaping the illocution-ary force of vocalisations, for instance, is welldocumented (Holmes, 1984), and prosodic fea-tures have been used for automatic segmentationof broadcast news data into sentences and topics(Shriberg et al, 2000).
Similarly, recurring audiopatterns have been employed in segmentation ofrecorded lectures (Malioutov et al, 2007).
Worksin the area of social psychology have used the sim-ple conversational features of duration of vocalisa-tions, pauses and overlaps to study the dynamicsof group interaction.
Jaffe and Feldstein (1970)characterise dialogues as Markov processes, andDabbs and Ruback (1987) suggest that a ?content-free?
method based on the amount and structureof vocal interactions could complement group in-teraction frameworks such as the one proposedby Bales (1950).
Pauses and overlap statisticsalone can be used, for instance, to characterise333SpeakersVocalisationevents ...Eventdurations ... ...
...Gaps &OverlapsFigure 1: Vocalisation Horizon for event v.the differences between face-to-face and telephonedialogue (ten Bosch et al, 2005), and a corre-lation between the duration of pauses and topicboundaries has been demonstrated for recordingsof spontaneous narratives (Oliveira, 2002).These works provided the initial motivation forour content-free representation scheme and thetopic segmentation method proposed in this paper.It is easy to verify by inspection of both the corpusof medical team meetings described in Section 4and the AMI corpus that pauses and vocalisationsvary significantly in duration and position on andaround topic boundaries.
Table 1 shows the meandurations of vocalisations that initiate new topicsor PCDs in MDTMs and the scenario-based AMImeetings, as well as the durations of pauses andoverlaps that surround it (within one vocalisationevent to the left and right).
In all cases the dif-ferences were statistically significant.
These re-sults agree with those obtained by Oliveira (2002)for discourse topics, and suggest that an approachbased on representing the duration of vocalisa-tions, pauses and overlaps in the immediate con-text of a vocalisation might be effective for auto-matic segmentation of meeting dialogues into top-ics or PCDs.Table 1: Mean durations in seconds (and standarddeviations) of vocalisation and pauses on and neartopic boundaries in MDTM and AMI meetings.Boundary Non-boundary t-testAMI vocal.
5.3 (8.2) 1.6 (3.5) p < .01AMI pauses 2.6 (4.9) 1.2 (2.8) p < .01AMI overlaps 0.4 (0.4) 0.3 (0.6) p < .01MDTM vocal.
12.0 (15.5) 8.1 (14.7) p < .05MDTM pauses 9.7 (12.7) 8.2 (14.8) p < .05We thus conceptualise meeting topic segmenta-tion as a classification task approachable throughsupervised machine learning.
A meeting canbe pre-segmented into separate vocalisations (i.e.talk spurts uttered by meeting participants) and si-lences, and such basic units (henceforth referredto as vocalisation events) can then be classifiedas to whether they signal a topic transition.
Thebasic defining features of a vocalisation event arethe identity of the speaker who uttered the vocali-sation (or speakers, for events containing speechoverlap) and its duration, or the duration of apause, for silence events.
However, identity la-bels and interval durations by themselves are notenough to enable segmentation.
As we have seenabove, some approaches to meeting segmentationcomplement these basic data with text (keywordsor full transcription) uttered during vocalisationevents, and with prosodic features.
Our proposalis to retain the content-free character of the ba-sic representation by complementing the speakerand duration information for an event with data de-scribing its preceding and succeeding events.
Wethus aim to capture an aspect of the dynamics ofthe dialogue by representing snapshots of vocali-sation sequences.
We call this general representa-tion strategy Vocalisation Horizon (VH).Figure 1 illustrates the basic idea.
Vocalisa-tion events are placed on a time line and com-bine utterances produced by the speakers who tookpart in the meeting.
These events can be labelledwith nominal attributes (s1, s2, .
.
.)
denoting thespeaker (or some other symbolic attribute, such asthe speaker?s role in the meeting).
Silences (gaps)and group talk (overlap) can either be assigned re-served descriptors (such as ?Floor?
and ?Group?
)or regarded as separate annotation layers.
Thegeneral data representation scheme for, say, seg-ment v would involve a data from its left context(v1?, v2?, v3?, .
.
.)
and its right context (v1, v2, v3, .
.
.
)in addition to the data for v itself.
These can be acombination of symbolic labels (in Figure 1, forinstance, s1 for the current speaker, s3, s2, s1, .
.
.for the preceding events and s3, s2, s3, .
.
.
for thefollowing events), durations (d, d1?, d2?, d3?, .
.
.
etc)334SpeakersVoarocaloitnnnirvirV.EedeElratcuropln G rtsce&Onscpeaker lipou.EedeEattilaloit EaccooerotV uloitnnn dgds nnnGipousi tVarsVeleuloitsSpeech signal .........Topic markersFigure 2: Meeting segmentation processing archi-tecture.and gaps or overlaps g1?, g2?, g3?, .
.
.
, g1, g2, g3, .
.
.etc).
Specific representations depend on the typeof annotation available on the speech data and onthe nature of the meeting.
Sections 4 and 5 presentand assess various representation schemes.The general processing architecture for meetingsegmentation assumed in this paper is shown inFigure 2.
The system will received the speech sig-nal, possibly on a single channel, and pre-segmentit into separate channels (one per speaker) withintervals of speech activity and silence labelledfor each stream.
Depending on the quality of therecording and the characteristics of the environ-ment, this initial processing stage can be accom-plished automatically through existing speaker di-arisation methods ?
e.g.
(Ajmera and Wooters,2003).
In the experiments reported below man-ual annotation was employed.
In the AMI cor-pus, speaker and speech activity annotation is doneon the word level and include transcription (Car-letta, 2007).
We parsed these word-level labels,ignoring the transcriptions, in order to build thecontent-free representation described above.
Oncethe data representation has been created it is thenused, along with topic boundary annotations, totrain a probabilistic classifier.
Finally, the topicdetection module uses the models generated in thetraining phase to hypothesise boundaries in unan-notated vocalisation event sequences and, option-ally, performs post-processing of these sequencesbefore returning the final hypothesis.
These mod-ules are described in more detail below.4 MDTM SegmentationThe MDTM corpus was collected over a period ofthree years as part of a detailed ethnographic studyof medical teams (Kane and Luz, 2006).
The cor-pus consists in 28 hours or meetings recorded ina dedicated teleconferencing room at a major pri-mary care hospital.
The audio sources included apressure-zone microphone attached to the telecon-ferencing system and a highly sensitive directionalmicrophone.
Video was gathered through two sep-arate sources: the teleconferencing system, whichshowed the participants and, at times, the medi-cal images (pathology slides, radiology) relevantto the case under discussion, and a high-end cam-corder mounted on a tripod.
All data were im-ported into a multimedia annotation tool and syn-chronised.
Of these, two meetings encompassing54 PCDs were chosen an annotated for vocalisa-tions (including speaker identity and duration) andPCD boundaries.Vocalisation events were encoded as vectorsv = (s, d, s1?, d1?, .
.
.
, sn?, dn?, s1, d1, .
.
.
, sn, dn),where the variables are as explained in Section 3.The speaker labels s, si?
and si are replaced, for thesake of generality, by ?role?
labels denoting med-ical specialties, such as ?radiologist?, ?surgeon?,?clinical oncologist?, ?pathologist?
etc.
In addi-tion to these roles, we reserved the special labels?Pause?
(a period of silence between two vocal-isations by the same speaker), ?SwitchingPause?
(pause between vocalisations by different speak-ers), and ?Group?
(vocalisations containing over-laps, i.e.
speech by more than one speaker).
We seta minimum duration of 1s for a talk spurt to countas a speech vocalisation event and a 0.9s minimumduration for silence period to be a pause.
Shorterintervals (depicted in Figure 1 as the fuzzy ends ofthe speech lines on the top of the chart) are incor-porated into an adjacent vocalisation event.The segmentation process can be defined as theprocess of mapping the set of vocalisation eventsV to {0, 1} where 1 represents a topic bound-ary and 0 represents a non-boundary vocalisationevent.
In order to implement this mapping weemploy a Naive Bayes classifier.
The conditionalprobabilities for the nominal variables (speakerroles) are estimated on the training set by max-imum likelihood and combined into multinomialmodels, while the continuous variables are logtransformed and modelled through Gaussian ker-nels (John and Langley, 1995).These models are used to estimate the probabil-ity, given by equation (1), of a vocalisation beingmarked as a topic boundary given the above de-scribed data representation, and the usual condi-tional independence assumptions applies.P (B = b|V = v) = P (B = b|Sn?
= sn?, Dn?
= dn?,.
.
.
, S = s, .
.
.
, Dn = dn)(1)The model can therefore be represented as asimple Bayesian network where the only depen-335SSSFigure 3: Bayesian model employed for dialoguesegmentation.dencies are between the boundary variable andeach feature of the vocalisation event, as shownin Figure 3.Luz (2009) reports that, for a similar data repre-sentation, horizons of length 2 < n < 6 producedthe best segmentation results.
Following this find-ing, we adopt n = 3 for all our experiments.We tested two variants of the representation: Vpdthat discriminated between pause types (pauses,switching pauses, group pauses, and group switch-ing pauses), as in (Dabbs and Ruback, 1987), andVsp which labelled all pauses equally.
The evalua-tion metrics employed include the standard classi-fication metrics of accuracy (A), the proportion ofcorrectly classified segments, boundary precision(P ), the proportion of correctly assigned bound-aries among all events marked as topic bound-aries, boundary recall (R), the proportion of targetboundaries correctly assigned, and the F1 score,the harmonic mean of P and R.Although these standard metrics provide an ini-tial approximation to segmentation effectiveness,they have been criticised as tools for evaluatingsegmentation because they are hard to interpretand are not sensitive to near misses (Pevzner andHearst, 2002).
Furthermore, due to the highly un-balanced nature of the classification task (bound-ary vocalisation events are only 3.3% of all in-stances), accuracy scores tend to produce over-optimistic results.
Therefore, to give a fairerpicture of the effectiveness of our method, wealso report values for two error metrics proposedspecifically for segmentation: Pk (Beeferman etal., 1999) and WindowDiff, or WD, (Pevzner andHearst, 2002).The Pk metric gives the probability that two vo-calisation events occurring k vocalisations apartand picked otherwise randomly from the datasetare incorrectly identified by the algorithm as be-longing to the same or to different topics.
Pk iscomputed by sliding two pairs of pointers over thereference and the hypothesis sequences and ob-serving whether each pair of pointers rests in thesame or in different segments.
An error is countedif the pairs disagree (i.e.
if they point to the samesegment in one sequence and to different segmentsin the other).WD is as an estimate of inconsistencies betweenreference and hypothesis, obtained by sliding awindow of length equal k segments over the timeline and counting disagreements between true andhypothesised boundaries.
Like the standard IRmetrics, Pk and WD range over the [0, 1] interval.Since they are error metrics, the greater the value,the worse the segmentation.Table 2: PCD segmentation results for 5-fold crossvalidation, horizon n = 3 (mean values).Threshold Filter Data A P R F1 Pk WDMAP no Vsp 0.94 0.20 0.21 0.18 0.33 0.44Vpd 0.95 0.17 0.20 0.16 0.30 0.38yes Vsp 0.95 0.20 0.16 0.16 0.32 0.38Vpd 0.95 0.16 0.12 0.13 0.29 0.34Proport.
no Vsp 0.95 0.28 0.28 0.28 0.26 0.36Vpd 0.95 0.26 0.27 0.26 0.27 0.42yes Vsp 0.95 0.30 0.22 0.25 0.25 0.31Vpd 0.95 0.22 0.14 0.17 0.27 0.33Table 2 shows the results for segmentation ofMDTMs into PCDs under the representationalvariants mentioned above and two different thresh-olding strategies: maximum a posteriori hypothe-sis (MAP) and proportional threshold.
The latteris a strategy that varies the threshold probabilityabove which an event is marked as a boundary ac-cording to the generality of boundaries found inthe training set.
The motivation for testing propor-tional thresholds is illustrated by Figure 4, whichshows a step plot of MAP hypothesis (h) super-imposed on the true segmentation (peaks repre-sent boundaries) and the corresponding values forp(b|v).
It is clear that a number of false positiveswould be removed if the threshold were set abovethe MAP level1 with no effect on the number offalse negatives.Another possible improvement suggested byFigure 4 is the filtering of adjacent boundary hy-potheses.
Wider peaks, such as the ones on in-stances 14 and 172 indicate that two or moreboundaries were hypothesised in immediate suc-cession.
Since this is clearly impossible, astraightforward improvement of the segmentation1I.e.
p(b|v) > 0.5; above the horizontal line in the centre.336hypothesis can be achieved by choosing a sin-gle boundary marker among a cluster of adjacentones.
This has been done as a post-processingstep by choosing a single event with maximal es-timated probability within a cluster of adjacentboundary hypotheses as the new hypothesis.rh1 15 87 110 168 22714 85 107 142 172 198 228 250Figure 4: Segmentation profile showing trueboundaries (r), boundaries hypothesised by a MAPclassifier (h) and probabilities (dotted line).The results suggest that both proportionalthresholding and filtering improve segmentation.As expected, accuracy figures were generally high(an uninformative) reflecting the great imbalancein favour of negative instances and the conser-vative nature of the classifier.
Precision, recalland F1 (for positive instances only) were also pre-dictably low, with Vsp under a proportional thresh-old attaining the best results.
However, in meetingbrowsing marking the topic boundary precisely isfar less important than retrieving the right text is ininformation retrieval or text categorisation, sincethe user can easily scan the neighbouring intervalswith a slider (Banerjee et al, 2005).
Therefore,Pk and WD are the most appropriate measures ofsuccess in this task.
Here our results seem quiteencouraging, given that they all represent greatimprovements over the (rather reasonable) base-lines of Pk = .46 and WD = .51 estimated byMonte Carlo simulation as in (Hsueh et al, 2006)by hypothesising the same proportion of bound-aries found in the training set.
Our results alsocompare favourably with some of the best resultsreported in the meeting segmentation literature todate, namely Pk = 0.32 and WD = 0.36, for a lex-ical cohesion algorithm on the ICSI corpus (Gal-ley et al, 2003), and Pk = 0.34 and WD = 0.36,for a maximum entropy approach combining lexi-cal, conversational and video features on the AMIcorpus (Hsueh et al, 2006).Although these results are promising, they posea question as regards data representation.
WhileVpd yielded the best results under MAP, Vspworked best overall under a proportional thresh-old.
What is the effect of encoding more detailedpause and overlap information?
Unfortunately, theMDTM corpus has not been annotated to the levelof detail required to allow in-depth investigationof this question.
We therefore turn to the far largerand more detailed AMI corpus for our next exper-iments.
In addition to helping clarify the represen-tation issue, testing our method on this corpus willgive us a better idea of how our method performsin a more standard topic segmentation task.5 AMI SegmentationThe AMI corpus is a collection of meetingsrecorded under controlled conditions, many ofwhich have a fixed scenario, goals and assignedparticipant roles.
The corpus is manually tran-scribed, and annotated with word-level timingsand a variety of metadata, including topics andsub-topics (Carletta, 2007).
Transcriptions in theAMI corpus are extracted from redundant record-ing channels (lapel, headset and array micro-phones), and stored separately for each partici-pant.
Because timing information in AMI is sodetailed, we were able to create much richer VHrepresentations, including finer grained pause andoverlap information.The original XML-encoded AMI data wereparsed and collated to produce our variants ofthe VH scheme.
We tested four types of VH:Vv, which includes only vocalisation events; Vg,which includes only pause and speech over-lap events; Va, which includes all vocalisations,pauses and overlaps; and Vr, which is similar toVpd in that it includes speaker roles in addition tovocalisations.
Pauses and overlaps were encodedby the same variable gi, where gi > 0 indicates apause gi < 0 an overlap, as shown in Figure 1.
Un-like MDTM, no arbitrary threshold was imposedon the identification of pause and overlap events.As before, we tested on a horizon n = 3, in orderto allow comparison with MDTM results.The training and boundary inference processalso remained as in the MDTM experiment, ex-cept that the larger amount of meeting data avail-able enabled us to increase the number of folds forcross validation so that the results could be testedfor statistical significance.The error scores and the number of boundariespredicted for the different representational vari-337ants, filtering an thresholding strategies are shownin Table 3.
Although all methods significantlyoutperformed the baseline scores of Pk = 0.473and WD = 0.542 (paired t-tests, p < 0.01, forall conditions), there were hardly any differencesin Pk scores across the different representations,even when conservative boundary filtering is per-formed.
Filtering, however, caused a significantimprovement for WD in all cases, though the com-bined effects of proportional thresholding and fil-tering caused the classifier to err on the side ofunderprediction.
A 3-way analysis of variance in-cluding non-filtered scores for proportional thresh-old resulted in F [4, 235] = 31.82, p < 0.01 forWD scores.
These outcomes agree with the resultsof the smaller-scale MDTM segmentation exper-iment, showing that categorisation based on con-versational features tend to mark clusters of seg-ments around the true topic boundary.
In addition,the trend for better performance of proportionalthresholding exhibited in the MDTM data was notas clearly observed in the AMI data, where onlyWD scores were significantly better than MAP(p < 0.01, Tukey HSD).Table 3: Segmentation results for 16-fold crossvalidation on AMI corpus, horizon n = 3.
Cor-rect number of boundaries in reference is 724.Threshold Filter Data Pk WD # bound.MAP no Va 0.270 0.462 3322Vg 0.278 0.433 1875Vv 0.273 0.449 3075Vr 0.271 0.448 3073yes Va 0.272 0.362 574Vg 0.277 0.391 851Vv 0.275 0.358 468Vr 0.274 0.357 469Proport.
no Va 0.289 0.398 1233Vg 0.290 0.382 735Vv 0.293 0.387 1002Vr 0.293 0.387 1002yes Va 0.293 0.353 241Vg 0.290 0.362 383Vv 0.297 0.350 183Vr 0.297 0.350 182It is noteworthy that the finer-grained represen-tations from which speaker roles were excluded(Vv, Vg, and Va) yielded segmentation perfor-mance comparable to the MDTM segmentationperformance under Vsp and Vpd.
In fact, addingspeaker role information in Vr did not result in im-provement for AMI segmentation.
Also interest-ing is the fact that representations based solely onpause and overlap information also produced goodperformance, thus confirming our initial intuition.5.1 MDTM revisitedSince Vv, Vg and Va seem to perform well with-out including speaker role information (except forthe current vocalisation?s speaker role) we wouldlike to see how a similar representation might af-fect segmentation performance for MDTM.
Wetherefore tested whether excluding preceding andfollowing speaker role information from Vsp andVpd had a positive impact on PCD segmenta-tion performance.
However, contrary to our ex-pectations neither of the modified representationsyielded better scores.
The best results, achievedfor the modified Vpd under proportional thresh-olding (PK = 0.27 and WD = 0.34), failed tomatch the results obtained with the original repre-sentation.
It seems that the various and more spe-cialised speaker roles found in medical meetingscan be good predictors of PCD boundaries.
Forexample: a typical pattern at the start of a PCDis the recounting of the patient?s initial symptomsand clinical findings by the registrar in a narrativestyle.
In AMI, on other hand, the roles are muchfewer, being only acted out by the participants aspart of the given scenario, which might explain theirrelevance of these roles for segmentation.5.2 ConclusionMDTM segmentation differs from topic segmen-tation of the AMI meetings in that PCDs are moreregular in their occurrence than meeting topicsproper.
Speaker role information was also foundto help MDTM segmentation, which was expectedsince there are many more very distinct activespeaker roles in MDTM (10 specialties, in total).Furthermore, Vsp and Vpd represent pauses andoverlaps as reserved roles, so that the informationencoded in Vg and Va as separate variables ap-pear in the speaker role horizon of Vsp and Vpd.It is possible that the finer-grained timing annota-tion of the AMI corpus (including detailed overlapand pause information unavailable in the MDTMdata) contributed to the relatively good segmen-tation performance achieved on AMI even in theabsence of speaker role cues.
It would be inter-esting to investigate whether finer pause and over-lap timings can also improve MDTM segmenta-tion.
This suggests some requirements for MDTMdata collection and pre-processing, such as the useof individual close-talking and the use of a speechrecogniser to derive word-level timings.
We planon conducting further experiments in that regard.338AcknowledgementsThis research was funded by Science Founda-tion Ireland under the Research Frontiers pro-gram.
The presentation was funded by Grant07/CE/1142, Centre for Next Generation Locali-sation (CNGL).ReferencesJ.
Ajmera and C. Wooters.
2003.
A robust speakerclustering algorithm.
In Proceedings of the IEEEWorkshop on Automatic Speech Recognition andUnderstanding, ASRU?03, pages 411?416.
IEEEPress.R.
F. Bales.
1950.
Interaction Process Analysis: AMethod for the Study of Small Groups.
Addison-Wesley, Cambridge, Mass.S.
Banerjee, C. Rose, and A. I. Rudnicky.
2005.
Thenecessity of a meeting recording and playback sys-tem, and the benefit of topic-level annotations tomeeting browsing.
In Proceedings of the 10th In-ternational Conference on Human-Computer Inter-action (INTERACT?05), pages 643?656.D.
Beeferman, A. Berger, and J. Lafferty.
1999.
Statis-tical models for text segmentation.
Machine Learn-ing, 34:177?210, Feb. 10.1023/A:1007506220214.J.
Carletta.
2007.
Unleashing the killer corpus: ex-periences in creating the multi-everything ami meet-ing corpus.
Language Resources and Evaluation,41(2):181?190.J.
M. J. Dabbs and B. Ruback.
1987.
Dimensions ofgroup process: Amount and structure of vocal inter-action.
Advances in Experimental Social Psychol-ogy, 20(123?169).M.
Galley, K. R. McKeown, E. Fosler-Lussier, andH.
Jing.
2003.
Discourse segmentation of multi-party conversation.
In Proceedings of the 41st An-nual Meeting of the ACL, pages 562?569.A.
Gruenstein, J. Niekrasz, and M. Purver.
2005.Meeting structure annotation: Data and tools.
InProceedings of the 6th SIGdial Workshop on Dis-course and Dialogue, pages 117?127, Lisbon, Por-tugal, September.M.
A. Hearst.
1997.
Texttiling: segmenting text intomulti-paragraph subtopic passages.
Comput.
Lin-guist., 23(1):33?64.J.
Holmes.
1984.
Modifying illocutionary force.
Jour-nal of Pragmatics, 8(3):345 ?
365.P.
Hsueh and J. D. Moore.
2007.
Combining multi-ple knowledge sources for dialogue segmentation inmultimedia archives.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics.
ACL Press.P.
Hsueh, J. D. Moore, and S. Renals.
2006.
Automaticsegmentation of multiparty dialogue.
In Proceed-ings of the 11th Conference of the European Chapterof the ACL (EACL), pages 273?277.
ACL Press.J.
Jaffe and S. Feldstein.
1970.
Rhythms of dialogue.Academic Press, New York.G.
H. John and P. Langley.
1995.
Estimating contin-uous distributions in Bayesian classifiers.
In Pro-ceedings of the 11th Conference on Uncertainty inArtificial Intelligence (UAI?95), pages 338?345, SanFrancisco, CA, USA, August.
Morgan KaufmannPublishers.B.
Kane and S. Luz.
2006.
Multidisciplinary med-ical team meetings: An analysis of collaborativeworking with special attention to timing and telecon-ferencing.
Computer Supported Cooperative Work(CSCW), 15(5):501?535.S.
Luz.
2009.
Locating case discussion segmentsin recorded medical team meetings.
In Proceed-ings of the ACM Multimedia Workshop on Search-ing Spontaneous Conversational Speech (SSCS?09),pages 21?30, Beijing, China, October.
ACM Press.I.
Malioutov, A.
Park, R. Barzilay, and J.
Glass.
2007.Making sense of sound: Unsupervised topic seg-mentation over acoustic input.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 504?511, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.N.
Morgan, D. Baron, J. Edwards, D. Ellis, D. Gelbart,A.
Janin, T. Pfau, E. Shriberg, and A. Stolcke.
2001.The meeting project at ICSI.
In Procs.
of HumanLanguage Technologies Conference, San Diego.M.
Oliveira, 2002.
The role of pause occurrence andpause duration in the signaling of narrative struc-ture, volume 2389 of LNAI, pages 43?51.
Springer.L.
Pevzner and M. A. Hearst.
2002.
A critique and im-provement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36, Mar.S.
Renals, T. Hain, and H. Bourlard.
2007.
Recog-nition and interpretation of meetings: The AMIand AMIDA projects.
In Proc.
IEEE Workshop onAutomatic Speech Recognition and Understanding(ASRU ?07).M.
Sherman and Y. Liu.
2008.
Using hidden Markovmodels for topic segmentation of meeting tran-scripts.
In Proceedings of the IEEE Spoken Lan-guage Technology Workshop, pages 185?188.E.
Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech commu-nication, 32(1-2):127?154.L.
ten Bosch, N. Oostdijk, and L. Boves.
2005.
Ontemporal aspects of turn taking in conversational di-alogues.
Speech Communication, 47:80?86.339
