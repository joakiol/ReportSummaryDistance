Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsRelaxed Cross-lingual Projection of Constituent SyntaxWenbin Jiang and Qun Liu and Yajuan Lu?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{jiangwenbin, liuqun, lvyajuan}@ict.ac.cnAbstractWe propose a relaxed correspondence as-sumption for cross-lingual projection of con-stituent syntax, which allows a supposedconstituent of the target sentence to corre-spond to an unrestricted treelet in the sourceparse.
Such a relaxed assumption fundamen-tally tolerates the syntactic non-isomorphismbetween languages, and enables us to learnthe target-language-specific syntactic idiosyn-crasy rather than a strained grammar di-rectly projected from the source language syn-tax.
Based on this assumption, a novel con-stituency projection method is also proposedin order to induce a projected constituent tree-bank from the source-parsed bilingual cor-pus.
Experiments show that, the parser trainedon the projected treebank dramatically out-performs previous projected and unsupervisedparsers.1 IntroductionFor languages with treebanks, supervised modelsgive the state-of-the-art performance in dependencyparsing (McDonald and Pereira, 2006; Nivre et al,2006; Koo and Collins, 2010; Martins et al, 2010)and constituent parsing (Collins, 2003; Charniakand Johnson, 2005; Petrov et al, 2006).
To break therestriction of the treebank scale, lots of works havebeen devoted to the unsupervised methods (Kleinand Manning, 2004; Bod, 2006; Seginer, 2007; Co-hen and Smith, 2009) and the semi-supervised meth-ods (Sarkar, 2001; Steedman et al, 2003; McCloskyet al, 2006; Koo et al, 2008) to utilize the unan-notated text.
In recent years, researchers have alsoconducted many investigations on syntax projection(Hwa et al, 2005; Ganchev et al, 2009; Smith andEisner, 2009; Jiang et al, 2010), in order to borrowsyntactic knowledge from another language.Different from the bilingual parsing (Smith andSmith, 2004; Burkett and Klein, 2008; Zhao et al,2009; Huang et al, 2009; Chen et al, 2010) thatimproves parsing performance with bilingual con-straints, and the bilingual grammar induction (Wu,1997; Kuhn, 2004; Blunsom et al, 2008; Snyder etal., 2009) that induces grammar from parallel text,the syntax projection aims to project the syntac-tic knowledge from one language to another.
Thisseems especially promising for the languages thathave bilingual corpora parallel to resource-rich lan-guages with large treebanks.
Previous works mainlyfocus on dependency projection.
The dependencyrelationship between words in the parsed source sen-tences can be directly projected across the wordalignment to words in the target sentences, follow-ing the direct correspondence assumption (DCA)(Hwa et al, 2005).
Due to the syntactic non-isomorphism between languages, DCA assumptionusually leads to conflicting or incomplete projection.Researchers have to adopt strategies to tackle thisproblem, such as designing rules to handle languagenon-isomorphism (Hwa et al, 2005), and resortingto the quasi-synchronous grammar (Smith and Eis-ner, 2009).For constituency projection, however, the lack ofisomorphism becomes much more serious, since aconstituent grammar describes a language in a moredetailed way.
In this paper we propose a relaxedcorrespondence assumption (RCA) for constituency1192Througha series ofexperimentshe verifiedthe previous hypothesis.???
????
???
??
?dINDT NN INNNSNPPPNPNPPPPRPNPDT JJ NN.VBD NPVPSPNP NNVV ASLC DEGNNPU[VBD][NP-DT-JJ-*][NP][VP][PP][S-PP-*-VP-*][S-PP-NP-VP-*][S]TOP TOP1122 334455Figure 1: An example for constituency projection based on the RCA assumption.
The projection is from Englishto Chinese.
A dash dot line links a projected constituent to its corresponding treelet, which is marked with graybackground; An Arabic numeral relates a directly-projected constituent to its counter-part in the source parse.projection.
It allows a supposed constituent ofthe target sentence to correspond to an unrestrictedtreelet in the source parse.
Such a relaxed as-sumption fundamentally tolerates the syntactic non-isomorphism between languages, and enables us tolearn the target-language-specific syntactic idiosyn-crasy, rather than induce a strained grammar directlyprojected from the source language syntax.
We alsopropose a novel cross-lingual projection method forconstituent syntax based on the RCA assumption.Given a word-aligned source-parsed bilingual cor-pus, a PCFG grammar can be induced for the targetlanguage by maximum likelihood estimation on theexhaustive enumeration of candidate projected pro-ductions, where each nonterminal in a productionis an unrestricted treelet extracted from the sourceparse.
The projected PCFG grammar is then usedto parse each target sentence under the guidance ofthe corresponding source tree, so as to produce anoptimized projected constituent tree.Experiments validate the effectiveness of theRCA assumption and the constituency projectionmethod.
We induce a projected Chinese constituenttreebank from the FBIS Chinese-English parallelcorpus with English sentences parsed by the Char-niak parser.
The Berkeley Parser trained on the pro-jected treebank dramatically outperforms the previ-ous projected and unsupervised parsers.
This pro-vides an promising substitute for unsupervised pars-ing methods, to the resource-scarce languages thathave bilingual corpora parallel to resource-rich lan-guages with human-annotated treebanks.In the rest of this paper we first presents the RCAassumption, and the algorithm used to determine thecorresponding treelet in the source parse for a can-didate constituent in the target sentence.
Then wedescribe the induction of the projected PCFG gram-mar and the projected constituent treebank from theword-aligned source-parsed parallel corpus.
Aftergiving experimental results and the comparison withprevious unsupervised and projected parsers, we fi-nally conclude our work and point out several as-pects to be improved in the future work.2 Relaxed Correspondence AssumptionThe DCA assumption (Hwa et al, 2005) works wellin dependency projection.
A dependency grammardescribes a sentence in a compact manner where thesyntactic information is carried by the dependencyrelationships between pairs of words.
It is reason-able to audaciously assume that the relationship of1193Algorithm 1 Treelet Extraction Algorithm.1: Input: Tf : parse tree of source sentence f2: e: target sentence3: A: word alignment of e and f4: for i, j s.t.
1 ?
i < j ?
|e| do ?
all spans5: t?
EXTTREELET(e, i, j,Tf ,A)6: T?i,j?
?
PRUNETREE(t)7: Output: treelet set T for all spans of e8: function EXTTREELET(e, i, j, T, A)9: if T aligns totally outside ei:j then10: return ?11: if T aligns totally inside ei:j then12: return {T ?
root}13: t?
{T ?
root} ?
partly aligned inside ei:j14: for each subtree s of T do15: t?
t ?
EXTTREELET(e, i, j, s,A)16: return t17: function PRUNETREE(T)18: for each node n in T do19: merge n?s successive empty children20: t?
T21: while t has only one non-empty subtree do22: t?
the non-empty subtree of t23: return ta word pair in the source sentence also holds forthe corresponding word pair in the target sentence.Compared with dependency grammar, constituentgrammar depicts syntax in a more complex way thatgives a sentence a hierarchically branched structure.Therefore the lack of syntactic isomorphism for con-stituency projection becomes much more serious, itwill be hard and inappropriate to directly project thecomplex constituent structure from one language toanother.For constituency projection, we propose a relaxedcorresponding assumption (RCA) to eliminate theinfluence of syntactic non-isomorphism between thesource- and target languages.
This assumption al-lows a supposed constituent of the target sentence tocorrespond to an unrestricted treelet in the sourceparse.
A treelet is a connected subgraph in thesource constituent tree, which covers a discontigu-ous sequence of words of the source sentence.
Thisproperty enables a supposed constituent of the tar-get sentence not necessarily to correspond to exactlya constituent of the source parse, so as to funda-mentally tolerate the syntactic non-isomorphism be-tween languages.
Figure 1 gives an example of re-* *DT JJ *** NPVPS[NP-DT-JJ-*]TOPDT JJ *NP[TOP-[S-*-*-[VP-*-[NP-DT-JJ-*]]-*]](a)(b)* NPDT JJ **NP **S[NP-DT-JJ-*]TOPDT JJ *NP[TOP-[S-*-[NP-[NP-DT-JJ-*]-*]-*-*]]Figure 2: Two examples for treelet pruning.
Asterisksindicate eliminated subtrees, which are represented asempty children of their parent nodes.laxed correspondence.2.1 Corresponding Treelet ExtractionAccording to the word alignment between the sourceand target sentences, we can extract the treelet out ofthe source parse for any possible constituent span ofthe target sentence.
Algorithm 1 shows the treeletextraction algorithm.Given the target sentence e, the parse tree Tf ofthe source sentence f , and the word alignment Abetween e and f , the algorithm extracts the corre-sponding treelet out of Tf for each candidate spanof e (line 4-6).
For a given span ?i, j?, its corre-sponding treelet in Tf can be extracted by a recur-sive top-down traversal in the tree.
If all nodes inthe current subtree T align outside of source subse-quence ei:j , the recursion stops and returns an emptytree ?, indicating that the subtree is eliminated fromthe final treelet (line 9-10).
And, if all nodes in Talign inside ei:j , the root of T is returned as the con-cise representation of the whole subtree (line 11-12).For the third situation, that is to say T aligns partlyinside ei:j , the recursion has to continue to investi-gate the subtrees of T (line 14-15).
The recursivetraversal finally returns a treelet t that exactly corre-1194sponds to the candidate constituent span ?i, j?
of thesource sentence.We can find that even for a smaller span, the recur-sive extraction procedure still starts from the root ofthe source tree.
This leads to a expatiatory treeletwith some redundant nodes on the top.
FunctionPRUNETREE takes charge of the treelet pruning (line6).
It traverses the treelet to merge the successiveempty sibling nodes (marked with asterisks) into one(line 18-19), then conducts a top-down pruning todelete the redundant branches until meeting a branchwith more than one non-empty subtrees (line 20-22).Figure 2 shows the effect of the pruning operationwith two examples.
The pruning operation maps thetwo original treelets into the same simplified ver-sion, that is, the pruned treelet.
The branches prunedout of the original treelet serve as the context of thepruned treelet.
The bracketed representations of thepruned treelets, as shown above the treelet graphs,are used as the nonterminals of the projected targetparses.Since the overall complexity of the algorithm isO(|e|3), it seems inefficient to collect the treeletsfor all spans in the target sentence.
But in fact itruns fast on the realistic corpus in our experiments,we assume that the function EXTTREELET doesn?talways consume O(|e|) because of the more or lessisomorphism between two languages.3 Projected Grammar and TreebankThis section describes how to build a projected con-stituent treebank based on the RCA assumption.
Ac-cording to the last section, each span of the targetsentence could correspond to a treelet in the sourceparse.
If a span ?i, j?
has a corresponding treelet t,a candidate projected constituent can be defined as atriple ?i, j, t?.
For an n-way partition of this span,?i, k1?, ?k1 + 1, k2?, .., ?kn?1 + 1, j?if each sub-span ?kp?1+1, kp?
corresponds to a can-didate constituent ?kp?1+1, kp, tp?, a candidate pro-jected production can then be defined, denoted as?i, j, t?
?
?i, k1, t1?
?k1+1, k2, t2?..
?kn?1+1, j, tn?There may be many candidate projected constituentsbecause of arbitrary combination, the tree projec-tion procedure aims to find the optimum tree fromthe parse forest determined by these candidate con-stituents.
Each production in the optimum treeshould satisfy this principle: the rule used in thisproduction appears in the whole corpus as frequentlyas possible.However, due to translation diversity and wordalignment error, the real constituent tree of the targetsentence may not be contained in the candidate pro-jected constituents.
We propose a relaxed and fault-tolerant tree projection strategy to tackle this prob-lem.
First, based on the distribution of candidateprojected constituents over each single sentence, weestimate the distribution over the whole corpus forthe rules used in these constituents, so as to obtaina projected PCFG grammar.
Then, using a PCFGparser and this grammar, we parse each target sen-tence under the guidance of the candidate projectedconstituent set of the target sentence, so as to ob-tain the optimum projected tree as far as possible.In the following, we first describe the estimation ofthe projected PCFG grammar and then show the treeprojection procedure.3.1 Projected PCFG GrammarFrom a human-annotated treebank, we can induce aPCFG grammar by estimating the frequency of theproduction rules, which are contained in the produc-tions of the trees.
But for each target sentence wedon?t know which candidate productions consist thecorrect constituent tree, so we can?t estimate the fre-quency of the production rules directly.A reasonable hypothesis is, if a candidate pro-jected production for a target sentence happens to bein the correct parse of the sentence, the rule used inthis production will appear frequently in the wholecorpus.
We assume that each candidate projectedproduction may be a part of the correct parse, butwith different probabilities.
If we give each candi-date projected production an appropriate probabil-ity and use this probability as the appearance fre-quency of this production in the correct parse, wecan achieve an approximation of the PCFG gram-mar hidden in the target sentences.
In this work,we restrict the productions to be binarized to reducethe computational complexity.
It results in a bina-rized PCFG grammar, similar to previous unsuper-vised works.To estimate the frequencies of the candidate pro-1195ductions in the correct parse of the target sentence,we need first estimate the frequencies of the candi-date spans, which are described as follows:p(?i, j?|e) = # of trees including ?i, j?# of all trees (1)The count of all binary trees of a target sentence ecan be calculated similar to the ?
value calculationin the inside-outside algorithm.
Without confusion,we adopt the symbol ?
(i, j) to denote the count ofbinary tree for span ?i, j?:?
(i, j) =????????
?1 i = jj?1?k=i?
(i, k) ?
?
(k + 1, j) i < j(2)?
(1, |e|) is the count of binary trees of target sen-tence e. We also need to calculate the count of bi-nary tree fragments that cover the nodes outside span?i, j?.
This is similar to the calculation of the ?
valuein the inside-outside algorithm.
We also adopt thesymbol ?
(i, j) here:?
(i, j) =????????????????????
?1 i = 1, j = |e||e|?k=j+1?
(i, k) ?
?
(k + 1, |e|)+i?1?k=1?
(k, j) ?
?
(k, j ?
1) else(3)For simplicity we omit some conditions in above for-mulas.
The count of trees containing span ?i, j?
is?
(i, j) ?
?
(i, j).
Equation 1 can be rewritten asp(?i, j?|e) = ?
(i, j) ?
?
(i, j)?
(1, |e|) (4)On condition that ?i, j?
is a span in the parse of e,the probability that ?i, j?
has two children ?i, k?
and?k + 1, j?
isp(?i, k?
?k + 1, j?|?i, j?)
= ?
(i, k) ?
?
(k + 1, j)?
(i, j) (5)Therefore, the probability that ?i, j?
is a span in theparse of e and has two children ?i, k?
and ?k + 1, j?can be calculated as follows:p(?i,j?
?
?i, k?
?k + 1, j?|e)= p(?i, j?|e) ?
p(?i, k?
?k + 1, j?|?i, j?
)= ?
(i, j) ?
?
(i, k) ?
?
(k + 1, j)?
(1, |e|)(6)Since each candidate projected span aligns to onetreelet at most, this probability is also the frequencyof the candidate projected production related to thethree spans.The counting approach above is based on the as-sumption that there is a uniform distribution over theprojected trees for every target sentence.
The insideand outside algorithms and the other counting for-mulae are used to calculate the expected counts un-der this assumption.
This looks like a single iterationof EM.A binarized projected PCFG grammar can then beeasily induced by maximum likelihood estimation.Due to word alignment errors, free translation, andexhaustive enumeration of possible projected pro-ductions, such a PCFG grammar may contain toomuch noisy nonterminals and production rules.
Weintroduce a threshold bRULE to filter the grammar.
Aproduction rule can be reserved only if its frequencyis larger than bRULE .3.2 Relaxed Tree ProjectionThe projected PCFG grammar is used in the pro-cedure of constituency projection.
Such a gram-mar, as a kind of global syntactic knowledge, canattenuate the negative effect of word alignment er-ror, free translation and syntactic non-isomorphismfor the constituency projection between each sin-gle sentence pair.
To obtain as optimal a projectedconstituency tree as possible, we have to integratetwo kinds of knowledge: the local knowledge inthe candidate projected production set of the targetsentence, and the global knowledge in the projectedPCFG grammar.The integrated projection strategy can be con-ducted as follows.
We parse each target sentencewith the projected PCFG grammar G, and use thecandidate projected production set D to guide thePCFG parsing.
The parsing procedure aims to findan optimum projected tree, which maximizes boththe PCFG tree probability and the count of produc-tions that also appear in the candidate projected pro-119602000400060008000100001200014000160001  2  4  8  16  32  64  128  256  512 10240102030405060708090#reservedrulesPercentageinall rules# selected NTs# reserved rulesPercentage in all rulesFigure 3: Rule counts corresponding to selected nonter-minal sets, and their frequency summation proportions tothe whole rule set.duction set.
The two optimization objectives can becoordinated as follows:y?
= argmaxy?d?y(p(d|G) ?
e???
(d,D)) (7)Here, d represents a production; ?
is a boolean func-tion that returns 1 if d appears in D and returns 0otherwise; ?
is a weight coefficient that needs to betuned to maximize the quality of the projected tree-bank.4 ExperimentsOur work focuses on the constituency projectionfrom English to Chinese.
The FBIS Chinese-Englishparallel corpus is used to obtain a projected con-stituent treebank.
It contains 239 thousand sentencepairs, with about 6.9/8.9 million Chinese/Englishwords.
We parse the English sentences with theCharniak Parser (Charniak and Johnson, 2005), andtag the Chinese sentences with a POS tagger imple-mented faithfully according to (Collins, 2002) andtrained on the Penn Chinese Treebank 5.0 (Xue etal., 2005).
We perform word alignment by runingGIZA++ (Och and Ney, 2000), and then use thealignment results for constituency projection.Following the previous works of unsupervisedconstituent parsing, we evaluate the projected parseron the subsets of CTB 1.0 and CTB 5.0, which con-tain no more than 10 or 40 words after the removalof punctuation.
The gold-standard POS tags are di-rectly used for testing.
The evaluation for unsu-pervised parsing differs slightly from the standard1015202530351  2  4  8  16  32  64  128  256  512  1024UnlabeledF1(%)# selected NTsFigure 4: Performance curve of the projected PCFGgrammars corresponding to different sizes of nontermi-nal sets.PARSEVAL metrics, it ignores the multiplicity ofbrackets, brackets of span one, and the bracket la-bels.
In all experiments we report the unlabeled F1value which is the harmonic mean of the unlabeledprecision and recall.4.1 Projected PCFG GrammarAn initial projected PCFG grammar can be inducedfrom the word-aligned and source-parsed parallelcorpus according to section 3.1.
Such an initialgrammar is huge and contains a large amount ofprojected nonterminals and production rules, wheremany of them come from free translation and wordalignment errors.
We conservatively set the filtra-tion threshold bRULE as 1.0 to discard the rules withfrequency less than one, the rule count falls dramat-ically from 3.3 millions to 92 thousands.Figure 3 shows the statistics of the remained pro-duction rules.
We sort the projected nonterminalsaccording to their frequencies and select the top 2N(1 ?
N ?
10) best ones, and then discard the rulesthat fall out of the selected nonterminal set.
The fre-quency summation of the rule set corresponding to32 best nonterminals accounts for nearly 90% of thefrequency summation of the whole rule set.We use the developing set of CTB 1.0 (chapter301-325) to evaluate the performance of a series offiltered grammars.
Figure 4 gives the unlabeled F1value of each grammar on all trees in the developingset.
The filtered grammar corresponding to the setof top 32 nonterminals achieves the highest perfor-mance.
We denote this grammar as G32 and use it119736373839400  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5UnlabeledF1(%)Weight coefficientFigure 5: Performance curve of the Berkeley Parsertrained on 5 thousand projected trees.
The weight co-efficient ?
ranges from 0 to 5.in the following tree projection procedure.4.2 Projected Treebank and ParserThe projected grammar G32 provides global syn-tactic knowledge for constituency projection.
Suchglobal knowledge and the local knowledge carriedby the candidate projected production set are inte-grated in a linear weighted manner as in Formula7.
The weight coefficient ?
is tuned to maximizethe quality of the projected treebank, which is in-directly measured by evaluating the performance ofthe parser trained on it.We select the first 5 thousand sentence pairs fromthe Chinese-English FBIS corpus, and induce a se-ries of projected treebanks using different ?, rangingfrom 0 to 5.
Then we train the Berkeley Parser oneach projected treebank, and test it on the develop-ing set of CTB 1.0.
Figure 5 gives the performancecurve, which reports the unlabeled F1 values of theprojected parsers on all sentences of the developingset.
We find that the best performance is achievedwith ?
between 1 and 2.5, with slight fluctuationin this range.
It can be concluded that, the pro-jected PCFG grammar and the candidate projectedproduction set do represent two different kinds ofconstraints, and we can effectively coordinate themby tuning the weight coefficient.
Since different ?values in this range result in slight performance fluc-tuation of the projected parser, we simply set it to 1for the constituency projection on the whole FBIScorpus.There are more than 200 thousand projected trees4545.54646.54747.54848.54949.5UnlabeledF1(%)Scale of treebank5000 10000 20000 40000 80000 160000Figure 6: Performance curve of the Berkeley Parsertrained on different amounts of best project trees.
Thescale of the selected treebank ranges from 5000 to160000.induced from the Chinese-English FBIS corpus.
Itis a heavy burden for a parser to train on so large atreebank.
And on the other hand, the free translationand word alignment errors result in many projectedtrees of poor-quality.
We design a criteria to approx-imate the quality of the projected tree y for the targetsentence x:Q?
(y) = |x|?1?
?d?y(p(d|G) ?
e???
(d,D)) (8)and use an amount of best projected trees instead ofthe whole projected treebank to train the parser.
Fig-ure 6 shows the performance of the Berkeley Parsertrained on different amounts of selected trees.
Theperformance of the Berkeley Parser constantly im-proves along with the increment of selected trees.However, treebanks containing more than 40 thou-sand projected trees can not brings significant im-provement.
The parser trained on 160 thousand treesonly achieves an F1 increment of 0.4 points over theone trained on 40 thousand trees.
This indicates thatthe newly added trees do not give the parser moreinformation due to their projection quality, and alarger parallel corpus may lead to better parsing per-formance.The Berkeley Parser trained on 160 thousand bestprojected trees is used in the final test.
Table 1gives the experimental results and the comparisonwith related works.
This is a sparse table since theexperiments of previous researchers focused on dif-ferent data sets.
Our projected parser significantly1198System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40(Klein and Manning, 2004) ?
46.7 ?
?
(Bod, 2006) ?
47.2 ?
?
(Seginer, 2007) ?
?
54.6 38.0(Jiang et al, 2010) 40.4 ?
?
?our work 52.1 54.4 54.5 49.2Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previousworks on constituency projection and unsupervised parsing.
CTB-TEST-40: sentences?
40 words from CTB standardtest set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ?
10 words from CTB 1.0/CTB 5.0 after theremoval of punctuation; CTB5-ALL-40: sentences ?
40 words from CTB 5.0 after the removal of punctuation.outperforms the parser of Jiang et al (2010), wherethey directly adapt the DCA assumption of (Hwaet al, 2005) from dependency projection to con-stituency projection and resort to a better word align-ment and a more complicated tree projection algo-rithm.
This indicates that the RCA assumption ismore suitable for constituency projection than theDCA assumption, and can induce a better grammarthat much more reflects the language-specific syn-tactic idiosyncrasy of the target language.Our projected parser also obviously surpasses ex-isting unsupervised parsers.
The parser of Seginer(2007) performs slightly better on CTB 5.0 sen-tences no more than 10 words, but obviously fallsbehind on sentences no more than 40 words.
Fig-ure 7 shows the unlabeled F1 of our parser ona series of subsets of CTB 5.0 with different sen-tence length upper limits.
We find that even on thewhole treebank, our parser still gives a promisingresult.
Compared with unsupervised parsing, con-stituency projection can make use of the syntacticinformation of another language, so that it proba-bly induce a better grammar.
Although compar-ing a syntax projection technique to supervised orsemi-supervised techniques seems unfair, it still sug-gests that if a resource-poor language has a bilingualcorpus parallel to a resource-rich language with ahuman-annotated treebank, the constituency projec-tion based on RCA assumption is a promising sub-stitute for unsupervised parsing.5 Conclusion and Future WorksThis paper describes a relaxed correspondence as-sumption (RCA) for constituency projection.
Un-der this assumption a supposed constituent in thetarget sentence can correspond to an unrestricted454647484950515253545510  20  30  40  50  60  70  80  90  100UnlabeledF1(%)Upper limit of sentence length+Figure 7: Performance of the Berkeley Parser on subsetsof CTB 5.0 with different sentence length upper limits.100+ indicates the whole treebank.treelet in the parse of the source sentence.
Differentfrom the direct correspondence assumption (DCA)widely used in dependency projection, the RCA as-sumption is more suitable for constituency projec-tion, since it fundamentally tolerates the syntacticnon-isomorphism between the source and target lan-guages.
According to the RCA assumption we pro-pose a novel constituency projection method.
First, aprojected PCFG grammar is induced from the word-aligned source-parsed parallel corpus.
Then, the treeprojection is conducted on each sentence pair by aPCFG parsing procedure, which integrates both theglobal knowledge in the projected PCFG grammarand the local knowledge in the set of candidate pro-jected productions.Experiments show that the parser trained onthe projected treebank significantly outperforms theprojected parsers based on the DCA assumption.This validates the effectiveness of the RCA assump-tion and the constituency projection method, andindicates that the RCA assumption is more suit-1199able for constituency projection than the DCA as-sumption.
The projected parser also obviously sur-passes the unsupervised parsers.
This suggeststhat if a resource-poor language has a bilingualcorpus parallel to a resource-rich language with ahuman-annotated treebank, the constituency projec-tion based on RCA assumption is an promising sub-stitute for unsupervised methods.Although achieving appealing results, our currentwork is quite coarse and has many aspects to be im-proved.
First, the word alignment is the fundamentalprecondition for projected grammar induction andthe following constituency projection, we can adoptthe better word alignment strategies to improve theword alignment quality.
Second, the PCFG grammaris too weak due to its context free assumption, wecan adopt more complicated grammars such as TAG(Joshi et al, 1975), in order to provide a more pow-erful global syntactic constraints for the tree projec-tion procedure.
Third, the current tree projectionalgorithm is too simple, more bilingual constraintscould lead to better projected trees.
Last but notleast, the constituency projection and the unsuper-vised parsing make use of different kinds of knowl-edge, therefore the unsupervised methods can be in-tegrated into the constituency projection frameworkto achieve better projected grammars, treebanks, andparsers.AcknowledgmentsThe authors were supported by National NaturalScience Foundation of China Contract 90920004,60736014 and 60873167.
We are grateful to theanonymous reviewers for their thorough reviewingand valuable suggestions.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian synchronous grammar induction.
In Pro-ceedings of the NIPS.Rens Bod.
2006.
An all-subtrees approach to unsuper-vised parsing.
In Proceedings of the COLING-ACL.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceedingsof the EMNLP.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminative rerank-ing.
In Proceedings of the ACL.Wenliang Chen, Jun.ichi Kazama, and Kentaro Tori-sawa.
2010.
Bitext dependency parsing with bilingualsubtree constraints.
In Proceedings of the ACL.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofthe NAACL-HLT.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe EMNLP.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext pro-jection constraints.
In Proceedings of the 47th ACL.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of the EMNLP.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.In Natural Language Engineering.Wenbin Jiang, Yajuan Lu?, Yang Liu, and Qun Liu.
2010.Effective constituent projection across languages.
InProceedings of the COLING.A.
K. Joshi, L. S. Levy, and M. Takahashi.
1975.
Treeadjunct grammars.
Journal Computer Systems Sci-ence.Dan Klein and Christopher D. Manning.
2004.
Cor-pusbased induction of syntactic structure: Models ofdependency and constituency.
In Proceedings of theACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the ACL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of the ACL.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of the ACL.Andre?
F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo.
2010.Turbo parsers: Dependency parsing by approximatevariational inference.
In Proceedings of EMNLP.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proceedings of the ACL.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL, pages 81?88.1200Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,and Svetoslav Marinov.
2006.
Labeled pseudoprojec-tive dependency parsing with support vector machines.In Proceedings of CoNLL, pages 221?225.Franz J. Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proceedings of the ACL.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the ACL.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL.Yoav Seginer.
2007.
Fast unsupervised incremental pars-ing.
In Proceedings of the ACL.David Smith and Jason Eisner.
2009.
Parser adaptationand projection with quasi-synchronous grammar fea-tures.
In Proceedings of EMNLP.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using english toparse korean.
In Proceedings of the EMNLP.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proceedings of the ACL.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In Pro-ceedings of the EACL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In Natural Lan-guage Engineering.Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.2009.
Cross language dependency parsing using abilingual lexicon.
In Proceedings of the ACL-IJCNLP.1201
