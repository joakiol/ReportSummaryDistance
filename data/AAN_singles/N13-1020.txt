Proceedings of NAACL-HLT 2013, pages 201?210,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsText Alignment for Real-Time Crowd CaptioningIftekhar Naim, Daniel Gildea, Walter Lasecki and Jeffrey P. BighamDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractThe primary way of providing real-time cap-tioning for deaf and hard of hearing peopleis to employ expensive professional stenogra-phers who can type as fast as natural speak-ing rates.
Recent work has shown that afeasible alternative is to combine the partialcaptions of ordinary typists, each of whomtypes part of what they hear.
In this paper,we describe an improved method for combin-ing partial captions into a final output basedon weighted A?
search and multiple sequencealignment (MSA).
In contrast to prior work,our method allows the tradeoff between accu-racy and speed to be tuned, and provides for-mal error bounds.
Our method outperformsthe current state-of-the-art on Word Error Rate(WER) (29.6%), BLEU Score (41.4%), andF-measure (36.9%).
The end goal is forthese captions to be used by people, and sowe also compare how these metrics correlatewith the judgments of 50 study participants,which may assist others looking to make fur-ther progress on this problem.1 IntroductionReal-time captioning provides deaf or hard of hear-ing people access to speech in mainstream class-rooms, at public events, and on live television.
Tomaintain consistency between the captions beingread and other visual cues, the latency between whena word was said and when it is displayed must beunder five seconds.
The most common approach toreal-time captioning is to recruit a trained stenogra-pher with a special purpose phonetic keyboard, whotranscribes the speech to text within approximately 5seconds.
Unfortunately, professional captionists arequite expensive ($150 per hour), must be recruited inblocks of an hour or more, and are difficult to sched-ule on short notice.
Automatic speech recognition(ASR) (Saraclar et al 2002) attempts to solve thisTXLFNIR[OD]\GRJ&RPELQHUWKHEURZQIR[MXPSHGIR[MXPSHGRYHUWKHOD]\WKHTXLFNEURZQIR[MXPSHGRYHUWKHOD]\GRJ)LQDO&DSWLRQ0HUJLQJ,QFRPSOHWH&DSWLRQV&&&Figure 1: General layout of crowd captioning systems.Captionists (C1, C2, C3) submit partial captions that areautomatically combined into a high-quality output.problem by converting speech to text completely au-tomatically.
However, the accuracy of ASR quicklyplummets to below 30% when used on an untrainedspeaker?s voice, in a new environment, or in the ab-sence of a high quality microphone (Wald, 2006b).An alternative approach is to combine the effortsof multiple non-expert captionists (anyone who cantype) (Lasecki et al 2012; Lasecki and Bigham,2012; Lasecki et al 2013).
In this approach, mul-tiple non-expert human workers transcribe an audiostream containing speech in real-time, and their par-tial input is combined to produce a final transcript(see Figure 1).
This approach has been shown todramatically outperform ASR in terms of both accu-racy and Word Error Rate (WER), even when us-ing captionists drawn from Amazon?s MechanicalTurk.
Furthermore, recall approached and even ex-ceeded that of a trained expert stenographer withseven workers contributing, suggesting that the in-formation is present to meet the performance of astenographer.
However, combining these captionsinvolves real-time alignment of partial captions thatmay be incomplete and that often have spelling er-rors and inconsistent timestamps.
In this paper,we present a more accurate combiner that leverages201Multiple Sequence Alignment (MSA) and NaturalLanguage Processing to improve performance.Gauging the quality of captions is not easy.
Al-though word error rate (WER) is commonly used inspeech recognition, it considers accuracy and com-pleteness, not readability.
As a result, a lower WERdoes not always result in better understanding (Wanget al 2003).
We compare WER with two other com-monly used metrics: BLEU (Papineni et al 2002)and F-measure (Melamed et al 2003), and reporttheir correlation with that of 50 human evaluators.The key contributions of this paper are as follows:?
We have implemented an A?-search based Mul-tiple Sequence Alignment algorithm (Lermenand Reinert, 2000) that can trade-off speed andaccuracy by varying the heuristic weight andchunk-size parameters.
We show that it outper-forms previous approaches in terms of WER,BLEU score, and F-measure.?
We propose a beam-search based technique us-ing the timing information of the captions thathelps to restrict the search space and scales ef-fectively to align longer sequences efficiently.?
We evaluate the correlation of WER, BLEU,and F-measure with 50 human ratings of cap-tion readability, and found that WER was morehighly correlated than BLEU score (Papineniet al 2002), implying it may be a more usefulmetric overall when evaluating captions.2 Related WorkMost of the previous research on real-time caption-ing has focused on Automated Speech Recognition(ASR) (Saraclar et al 2002; Cooke et al 2001;Praz?a?k et al 2012).
However, experiments showthat ASR systems are not robust enough to be ap-plied for arbitrary speakers and in noisy environ-ments (Wald, 2006b; Wald, 2006a; Bain et al 2005;Bain et al 2012; Cooke et al 2001).2.1 Crowd CaptioningTo address these limitations of ASR-based tech-niques, the Scribe system collects partial captionsfrom the crowd and then uses a graph-based in-cremental algorithm to combine them on the fly(Lasecki et al 2012).
The system incrementallybuilds a chain graph, where each node represents aset of equivalent words entered by the workers andthe link between nodes are adjusted according to theorder of the input words.
A greedy search is per-formed to identify the path with the highest confi-dence, based on worker input and an n-gram lan-guage model.
The algorithm is designed to be usedonline, and hence has high speed and low latency.However, due to the incremental nature of the algo-rithm and due to the lack of a principled objectivefunction, it is not guaranteed to find the globally op-timal alignment for the captions.2.2 Multiple Sequence AlignmentThe problem of aligning and combining multipletranscripts can be mapped to the well-studied Mul-tiple Sequence Alignment (MSA) problem (Edgarand Batzoglou, 2006).
MSA is an important prob-lem in computational biology (Durbin et al 1998).The goal is to find an optimal alignment from agiven set of biological sequences.
The pairwisealignment problem can be solved efficiently usingdynamic programming in O(N2) time and space,where N is the sequence length.
The complexity ofthe MSA problem grows exponentially as the num-ber of sequences grows, and has been shown to beNP-complete (Wang and Jiang, 1994).
Therefore,it is important to apply some heuristic to performMSA in a reasonable amount of time.Most MSA algorithms for biological sequencesfollow a progressive alignment strategy that first per-forms pairwise alignment among the sequences, andthen builds a guide tree based on the pairwise simi-larity between these sequences (Edgar, 2004; Do etal., 2005; Thompson et al 1994).
Finally, the inputsequences are aligned according to the order spec-ified by the guide tree.
While not commonly usedfor biological sequences, MSA with A?-style searchhas been applied to these problems by Horton (1997)and Lermen and Reinert (2000).Lasecki et alexplored MSA in the context ofmerging partial captions by using the off-the-shelfMSA tool MUSCLE (Edgar, 2004), replacing the nu-cleotide characters by English characters (Laseckiet al 2012).
The substitution cost for nucleotideswas replaced by the ?keyboard distance?
betweenEnglish characters, learned from the physical lay-out of a keyboard and based on common spelling202errors.
However, MUSCLE relies on a progressivealignment strategy and may result in suboptimal so-lutions.
Moreover, it uses characters as atomic sym-bols instead of words.
Our approach operates on aper-word basis and is able to arrive at a solution thatis within a selectable error-bound of optimal.3 Multiple Sequence AlignmentWe start with an overview of the MSA problem us-ing standard notations as described by Lermen andReinert (2000).
Let S1, .
.
.
, SK ,K ?
2, be the Ksequences over an alphabet ?, and having lengthN1, .
.
.
, NK .
The special gap symbol is denoted by???
and does not belong to ?.
Let A = (aij) be aK ?
Nf matrix, where aij ?
?
?
{?
}, and the ithrow has exactly (Nf ?
Ni) gaps and is identical toSi if we ignore the gaps.
Every column of A musthave at least one non-gap symbol.
Therefore, the jthcolumn of A indicates an alignment state for the jthposition, where the state can have one of the 2K ?
1possible combinations.
Our goal is to find the op-timum alignment matrix AOPT that minimizes thesum of pairs (SOP) cost function:c(A) =?1?i?j?Kc(Aij) (1)where c(Aij) is the cost of the pairwise alignmentbetween Si and Sj according to A. Formally,c(Aij) =?Nfl=1 sub(ail, ajl), where sub(ail, ajl)denotes the cost of substituting ajl for ail.
If ailand ajl are identical, the substitution cost is usu-ally zero.
For the caption alignment task, we treateach individual word as a symbol in our alphabet?.
The substitution cost for two words is estimatedbased on the edit distance between two words.
Theexact solution to the SOP optimization problem isNP-Complete, but many methods solve it approxi-mately.
In this paper, we adapt weighted A?
searchfor approximately solving the MSA problem.3.1 A?
Search for MSAThe problem of minimizing the SOP cost func-tion for K sequences is equivalent to estimating theshortest path between a single source and single sinknode in a K-dimensional lattice.
The total num-ber of nodes in the lattice is (N1 + 1) ?
(N2 +Algorithm 1 MSA-A?
AlgorithmRequire: K input sequences S = {S1, .
.
.
, SK} havinglength N1, .
.
.
, NK , heuristic weight w, beam size b1: start?
0K , goal?
[N1, .
.
.
, NK ]2: g(start)?
0, f(start)?
w ?
h(start).3: Q?
{start}4: while Q 6= ?
do5: n?
EXTRACT-MIN(Q)6: for all s ?
{0, 1}K ?
{0K} do7: ni ?
n + s8: if ni = goal then9: Return the alignment matrix for the reconstructedpath from start to ni10: else if ni 6?
Beam(b) then11: continue;12: else13: g(ni)?
g(n) + c(n, ni)14: f(ni)?
g(ni) + w ?
h(ni)15: INSERT-ITEM(Q, ni, f(ni))16: end if17: end for18: end while1) ?
?
?
?
?
(NK + 1), each corresponding to a dis-tinct position in K sequences.
The source node is[0, .
.
.
, 0] and the sink node is [N1, .
.
.
, NK ].
Thedynamic programming algorithm for estimating theshortest path from source to sink treats each nodeposition [n1, .
.
.
, nK ] as a state and calculates a ma-trix that has one entry for each node.
Assuming thesequences have roughly same length N , the size ofthe dynamic programming matrix is O(NK).
Ateach vertex, we need to minimize the cost over allits 2K ?
1 predecessor nodes, and, for each suchtransition, we need to estimate the SOP objectivefunction that requires O(K2) operations.
Therefore,the dynamic programming algorithm has time com-plexity of O(K22KNK) and space complexity ofO(NK), which is infeasible for most practical prob-lem instances.
However, we can efficiently solve itvia heuristic A?
search (Lermen and Reinert, 2000).We use A?
search based MSA (shown in Algo-rithm 1, illustrated in Figure 2) that uses a prior-ity queue Q to store dynamic programming statescorresponding to node positions in the K dimen-sional lattice.
Let n = [n1, .
.
.
, nK ] be any nodein the lattice, s be the source, and t be the sink.
TheA?
search can find the shortest path using a greedyBest First Search according to an evaluation func-tion f(n), which is the summation of the cost func-203Q Q 7&&&WKH TXLFN EURZQ IR[ MXPSHG RYHU WKH OD]\ GRJWKH EURZQ IR[ MXPSHGTXLFN IR[ OD]\ GRJIR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBWKH EURZQ IR[ MXPSHGTXLFN IR[ OD]\ GRJIR[ MXPSHG RYHU WKH OD]\WKH EURZQ IR[ MXPSHGTXLFN IR[ OD]\ GRJIR[ MXPSHG RYHU WKH OD]\BBBBBB BBBBBBBBBBBBBB WKH EURZQ IR[ MXPSHGTXLFN IR[ OD]\ GRJIR[ MXPSHG RYHU WKH OD]\BBBBBB BBBBBBBBBBBBBBBBBBB WKH EURZQ IR[ MXPSHGTXLFN IR[ OD]\ GRJIR[ MXPSHG RYHU WKH OD]\ BBBBBBBB BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBLBMXPSHG MXPSHG GRJBBBBIR[IR[IR[6 NBBBBRYHUOD]\BBRYHUBBBBBB&DSWLRQ&DSWLRQ&DSWLRQOD]\BB BBFigure 2: A?
MSA search algorithm.
Each branch is one of 2K ?
1 possible alignments for the current input.
Thebranch with minimum sum of the current alignment cost and the expected heuristic value hpair (precomputed).tions g(n) and the heuristic function h(n) for noden.
The cost function g(n) denotes the cost of theshortest path from the source s to the current noden.
The heuristic function h(n) is the approximateestimated cost of the shortest path from n to the des-tination t. At each step of the A?
search algorithm,we extract the node with the smallest f(n) valuefrom the priority queue Q and expand it by one edge.The heuristic function h(n) is admissible if it neveroverestimates the cost of the cheapest solution fromn to the destination.
An admissible heuristic func-tion guarantees that A?
will explore the minimumnumber of nodes and will always find the optimalsolution.
One commonly used admissible heuristicfunction is hpair(n):hpair(n) = L(n ?
t) =?1?i<j?Kc(A?p(?ni , ?nj ))(2)where L(n ?
t) denotes the lower bound on thecost of the shortest path from n to destination t, A?pis the optimal pairwise alignment, and ?ni is the suf-fix of node n in the i-th sequence.
A?
search usingthe pairwise heuristic function hpair significantly re-duces the search space and also guarantees findingthe optimal solution.
We must be able to estimatehpair(n) efficiently.
It may appear that we need toestimate the optimal pairwise alignment for all thepairs of suffix sequences at every node.
However,we can precompute the dynamic programming ma-trix over all the pair of sequences (Si, Sj) once fromthe backward direction, and then reuse these valuesat each node.
This simple trick significantly speedsup the computation of hpair(n).Despite the significant reduction in the searchspace, the A?
search may still need to explore alarge number of nodes, and may become too slowfor real-time captioning.
However, we can furtherimprove the speed by following the idea of weightedA?
search (Pohl, 1970).
We modify the evaluationfunction f(n) = g(n)+hpair(n) to a weighted eval-uation function f ?
(n) = g(n) + whpair(n), wherew ?
1 is a weight parameter.
By setting the valueof w to be greater than 1, we increase the relativeweight of the estimated cost to reach the destina-tion.
Therefore, the search prefers the nodes that arecloser to the destination, and thus reaches the goalfaster.
Weighted A?
search can significantly reducethe number of nodes to be examined, but it also losesthe optimality guarantee of the admissible heuristicfunction.
We can trade-off between accuracy andspeed by tuning the weight parameter w.3.2 Beam Search using Time-stampsThe computational cost of the A?
search algorithmgrows exponentially with increase in the number ofsequences.
However, in order to keep the crowd-sourced captioning system cost-effective, only asmall number of workers are generally recruited ata time (typically K ?
10).
We, therefore, are moreconcerned about the growth in computational cost asthe sequence length increases.In practice, we break down the sequences intosmaller chunks by maintaining a window of a giventime interval, and we apply MSA only to the smallerchunks of captions entered by the workers duringthat time window.
As the window size increases,the accuracy of our MSA based combining systemincreases, but so does the computational cost and la-tency.
Therefore, it is important to apply MSA witha relatively small window size for real-time caption-ing applications.
Another interesting application canbe the offline captioning, for example, captioning anentire lecture and uploading the captions later.For the offline captioning problem, we can fo-cus less on latency and more on accuracy by align-ing longer sequences.
To restrict the search spacefrom exploding with sequence length (N ), we applya beam constraint on our search space using the timestamps of each captioned words.
For example, if we2041.
so now what i want to do is introduce some of the2.
what i wanna do is introduce some of the aspects of the class3.
so now what i want to do is is introduce some of the aspects of the class4.
so now what i want to do is introduce5.
so now what i want to do is introduce some of the operational of the class6.
so i want to introduce some of the operational aspects of the clasC.
so now what i want to do is introduce some of the operational aspects of the classFigure 3: An example of applying MSA-A?
(threshold tv = 2) to combine 6 partial captions (first 6 lines) by humanworkers to obtain the final output caption (C).set the beam size to be 20 seconds, then we ignoreany state in our search space that aligns two wordshaving more than 20 seconds time lag.
Given a fixedbeam size b, we can restrict the number of priorityqueue removals by the A?
algorithm to O(NbK).The maximum size of the priority queue is O(NbK).For each node in the priority queue, for each of theO(2K) successor states, the objective function andheuristic estimation requires O(K2) operations andeach priority queue insertion requires O(log(NbK))i.e.
O(logN + K log b) operations.
Therefore,the overall worst case computational complexity isO(NbK2K(K2 + logN + K log b)).
Note that forfixed beam size b and number of sequences K, thecomputational cost grows as O(N logN) with theincrease in N .
However, in practice, weighted A?search explores much smaller number of states com-pared to this beam-restricted space.3.3 Majority Voting after AlignmentAfter aligning the captions by multiple workers in agiven chunk, we need to combine them to obtain thefinal caption.
We do that via majority voting at eachposition of the alignment matrix containing a non-gap symbol.
In case of tie, we apply the languagemodel to choose the most likely word.Often workers type in nonstandard symbols, ab-breviations, or misspelled words that do not matchwith any other workers?
input and end up as a sin-gle word aligned to gaps in all the other sequences.To filter out such spurious words, we apply a vot-ing threshold (tv) during majority voting and filterout words having less than tv votes.
Typically weset tv = 2 (see the example in Figure 3).
While ap-plying the voting threshold improves the word errorrate and readability, it runs the risk of loosing correctwords if they are covered by only a single worker.3.4 Incorporating an N-gram Language ModelWe also experimented with a version of our systemdesigned to incorporate the score from an n-gramlanguage model into the search.
For this purpose,we modified the alignment algorithm to produce ahypothesized output string as it moves through theinput strings, as opposed to using voting to producethe final string as a post-processing step.
The statesfor our dynamic programming are extended to in-clude not only the current position in each inputstring, but also the last two words of the hypothesisstring (i.e.
[n1, .
.
.
, nK , wi?1, wi?2]) for use in com-puting the next trigram language model probability.We replace our sum-of-all-pairs objective functionwith the sum of the alignment cost of each input withthe hypothesis string, to which we add the log of thelanguage model probability and a feature for the to-tal number of words in the hypothesis.
Mathemati-cally, we consider the hypothesis string to be the 0throw of the alignment matrix, making our objectivefunction:c(A) =?1?i?Kc(A0,i) + wlenNf?l=1I(a0,l 6= ?
)+ wlmNf?l=1logP (a0,l|a0,l?2, a0,l?1)where wlm and wlen are negative constants indicat-ing the relative weights of the language model prob-ability and the length penalty.Extending states with two previous words resultsin a larger computational complexity.
Given K se-quences of length N each, we can have O(NK) dis-tinct words.
Therefore, the number distinct statesis O(NbK(NK)2) i.e.
O(N3K2bK).
Each statecan have O(K2K) successors, giving an overallcomputational complexity of O(N3K3bK2K(K2 +logN + logK + K log b)).
Alternatively, if the vo-205cabulary size |V | is smaller than NK, the number ofdistinct states is bounded by O(NbK |V |2).3.5 Evaluation Metric for Speech to TextCaptioningAutomated evaluation of speech to text captioning isknown to be a challenging task (Wang et al 2003).Word Error Rate (WER) is the most commonly usedmetric that finds the best pairwise alignment be-tween the candidate caption and the ground truthreference sentence.
WER is estimated as S+I+DN ,where S, I , and D is the number of incorrect wordsubstitutions, insertions, and deletions required tomatch the candidate sentence with reference, and Nis the total number of words in the reference.
WERhas several nice properties such as: 1) it is easyto estimate, and 2) it tries to preserve word order-ing.
However, WER does not account for the overall?readability?
of text and thus does not always corre-late well with human evaluation (Wang et al 2003;He et al 2011).The widely-used BLEU metric has been shownto agree well with human judgment for evaluatingtranslation quality (Papineni et al 2002).
However,unlike WER, BLEU imposes no explicit constraintson the word ordering.
BLEU has been criticized asan ?under-constrained?
measure (Callison-Burch etal., 2006) for allowing too much variation in wordordering.
Moreover, BLEU does not directly esti-mate recall, and instead relies on the brevity penalty.Melamed et al(2003) suggest that a better approachis to explicitly measure both precision and recall andcombine them via F-measure.Our application is similar to automatic speechrecognition in that there is a single correct output,as opposed to machine translation where many out-puts can be equally correct.
On the other hand, un-like with ASR, out-of-order output is frequently pro-duced by our alignment system when there is notenough overlap between the partial captions to de-rive the correct ordering for all words.
It may bethe case that even such out-of-order output can beof value to the user, and should receive some sort ofpartial credit that is not possible using WER.
Forthis reason, we wished to systematically compareBLEU, F-measure, and WER as metrics for our task.We performed a study to evaluate the agreementof the three metrics with human judgment.
We ran-Metric Spearman Corr.
Pearson Corr.1-WER 0.5258 0.6282BLEU 0.3137 0.6181F-measure 0.4389 0.6240Table 1: The correlation of average human judgment withthree automated metrics: 1-WER, BLEU, and F-measure.domly extracted one-minute long audio clips fromfour MIT OpenCourseWare lectures.
Each clip wastranscribed by 7 human workers, and then alignedand combined using four different systems: thegraph-based system, and three different versions ofour weighted A?
algorithm with different values oftuning parameters.
Fifty people participated in thestudy and were split in two equal sized groups.
Eachgroup was assigned two of the four audio clips,and each person evaluated all four captions for bothclips.
Each participant assigned a score between 1to 10 to these captions, based on two criteria: 1) theoverall estimated agreement of the captions with theground truth text, and 2) the readability and under-standability of the captions.Finally, we estimated the correlation coefficients(both Spearman and Pearson) for the three metricsdiscussed above with respect to the average scoreassigned by the human participants.
The resultsare presented in Table 1.
Among the three metrics,WER had the highest agreement with the human par-ticipants.
This indicates that reconstructing the cor-rect word order is in fact important to the users, andthat, in this aspect, our task has more of the flavor ofspeech recognition than of machine translation.4 Experimental ResultsWe experiment with the MSA-A?
algorithm for cap-tioning different audio clips, and compare the resultswith two existing techniques.
Our experimental setup is similar to the experiments by Lasecki et al(2012).
Our dataset consists of four 5-minute longaudio clips extracted from lectures available on MITOpenCourseWare.
The audio clips contain speechfrom electrical engineering and chemistry lectures.Each audio clip is transcribed by ten non-expert hu-man workers in real-time.
We then combine theseinputs using our MSA-A?
algorithm, and also com-pare with the existing graph-based system and mul-2060.00.10.20.30.40.50.60.70.58 0.600.360.470.540.00.10.20.30.40.50.60.70.600.630.400.490.410.00.10.20.30.40.50.60.53 0.550.350.45 0.420.00.10.20.30.40.50.60.49 0.510.260.360.300.00.10.20.30.40.50.60.70.53 0.550.440.39 0.370.00.10.20.30.40.50.60.70.56 0.560.450.390.190.00.10.20.30.40.50.60.43 0.44 0.410.350.230.00.10.20.30.40.50.60.430.460.360.290.090.00.10.20.30.40.50.60.70.62 0.640.530.470.550.00.10.20.30.40.50.60.7 0.63 0.630.530.45 0.440.00.10.20.30.40.50.60.52 0.54 0.490.43 0.390.00.10.20.30.40.50.60.53 0.560.460.380.35(1.0-WER) BLEU Score F-MeasureDataSet 1DataSet 2DataSet 3DataSet 4A*-10-t(c=10 sec, threshold=2)A*-15-t(c=15 sec, threshold=2)A*-15(c=15 sec, no threshold)Graph-basedMUSCLEFigure 4: Evaluation of different systems on using threedifferent automated metrics for measuring transcriptionquality: 1- Word Error Rate (WER), BLEU, and F-measure on the four audio clips.tiple sequence alignment using MUSCLE.As explained earlier, we vary the four key pa-rameters of the algorithm: the chunk size (c), theheuristic weight (w), the voting threshold (tv), andthe beam size (b).
The heuristic weight and chunksize parameters help us to trade-off between speedversus accuracy; the voting threshold tv helps im-prove precision by pruning words having less thantv votes, and beam size reduces the search space byrestricting states to be inside a time window/beam.We use affine gap penalty (Edgar, 2004) with dif-ferent gap opening and gap extension penalty.
Weset gap opening penalty to 0.125 and gap extensionpenalty to 0.05.
We evaluate the performance usingthe three standard metrics: Word Error Rate (WER),BLEU, and F-measure.
The performance in terms ofthese metrics using different systems is presented inFigure 4.Out of the five systems in Figure 4, the first threeare different versions of our A?
search based MSAalgorithm with different parameter settings: 1) A?-10-t system (c = 10 seconds, tv = 2), 2) A?-15-t (c =15 seconds, tv = 2), and 3) A?-15 (c = 15 seconds, tv= 1 i.e.
no pruning while voting).
For all three sys-tems, the heuristic weight parameter w is set to 2.5and beam size b = 20 seconds.
The other two sys-tems are the existing graph-based system and mul-tiple sequence alignment using MUSCLE.
Amongthe three A?
based algorithms, both A?-15-t and A?-10-t produce better quality transcripts and outper-form the existing algorithms.
Both systems applythe voting threshold that improves precision.
Thesystem A?-15 applies no threshold and ends up pro-ducing many spurious words having poor agreementamong the workers, and hence it scores worse in allthe three metrics.
The A?-15-t achieves 57.4% aver-age accuracy in terms of (1-WER), providing 29.6%improvement with respect to the graph-based sys-tem (average accuracy 42.6%), and 35.4% improve-ment with respect to the MUSCLE-based MSA sys-tem (average accuracy 41.9%).
On the same set ofaudio clips, Lasecki et al(2012) reported 36.6% ac-curacy using ASR (Dragon Naturally Speaking, ver-sion 11.5 for Windows), which is worse than all thecrowd-based based systems used in this experiment.To measure the statistical significance of this im-provement, we performed a t-test at both the datasetlevel (n = 4 clips) and the word level (n = 2862words).
The improvement over the graph-basedmodel was statistically significant with dataset levelp-value 0.001 and word level p-value smaller than0.0001.
The average time to align each 15 secondchunk with 10 input captions is ?400 milliseconds.We have also experimented with a trigram lan-guage model, trained on the British National Cor-pus (Burnard, 1995) having ?122 million words.The language-model-integrated A?
search provideda negligible 0.21% improvement in WER over theA?-15-t system on average.
The task of combin-ing captions does not require recognizing words; itonly requires aligning them in the correct order.
Thiscould explain why language model did not improveaccuracy, as it does for speech recognition.
Sincethe standard MSA-A?
algorithm (without languagemodel) produced comparable accuracy and fasterrunning time, we used that version in the rest of the2072 3 4 5 6 7 80.420.440.460.480.50.520.540.560.581?WERAvg Running Time (in Seconds)c = 5c = 10c = 15c = 20c = 40c = 60(a) Varying heuristic weights for fixed chunk sizes (c)2 3 4 5 6 7 80.420.440.460.480.50.520.540.560.581?WERAvg Running Time (in Seconds)w = 1.8w = 2w = 2.5w = 3w = 4w = 6w = 8(b) Varying chunk size for fixed heuristic weight (w)Figure 5: The trade-off between speed and accuracy for different heuristic weights and chunk size parameters.experiments.Next, we look at the critical speed versus accuracytrade-off for different values of the heuristic weight(w) and the chunk size (c) parameters.
Since WERhas been shown to correlate most with human judg-ment, we show the next results only with respect toWER.
First, we fix the chunk size at different val-ues, and then vary the heuristic weight parameter:w = 1.8, 2, 2.5, 3, 4, 6, and 8.
The results areshown in Figure 5(a), where each curve representshow time and accuracy changed over the range ofvalues of w and a fixed value of c. We observe thatfor smaller values of w, the algorithm is more accu-rate, but comparatively slower.
As w increases, thesearch reaches the goal faster, but the quality of thesolution degrades as well.
Next, we fix w and varychunk size c = 5, 10, 15, 20, 40, 60 second.
We re-peat this experiment for a range of values of w andthe results are shown in Figure 5(b).
We can see thatthe accuracy improves steeply up to c = 20 seconds,and does not improve much beyond c = 40 seconds.For all these benchmarks, we set the beam size (b)to 20 seconds and voting threshold (tv) to 2.In our tests, the beam size parameter (b) did notplay a significant role in performance, and setting itto any reasonably large value (usually ?
15 seconds)resulted in similar accuracy and running time.
Thisis because the A?
search with hpair heuristic alreadyreduces the the search space significantly, and usu-ally reaches the goal in a number of steps smallerthan the state space size after the beam restriction.Finally, we investigate how the accuracy of ouralgorithm varies with the number of inputs/workers.We start with a pool of 10 input captions for one ofthe audio clips.
We vary the number of input cap-tions (K) to the MSA-A?
algorithm from 2 up to 10.The quality of input captions differs greatly amongthe workers.
Therefore, for each value of K, we re-peat the experiment min(20,(10K))times; each timewe randomly select K input captions out of the totalpool of 10.
Figure 6 shows that accuracy steeplyincreases as the number of inputs increases to 7,and after that adding more workers does not pro-vide much improvement in accuracy, but increasesrunning time.5 Discussion and Future WorkIn this paper, we show that the A?
search basedMSA algorithm performs better than existing algo-rithms for combining multiple captions.
The exist-ing graph-based model has low latency, but it usuallycan not find a near optimal alignment because of itsincremental alignment.
Weighted A?
search on theother hand performs joint multiple sequence align-ment, and is guaranteed to produce a solution hav-ing cost no more than (1 + ?)
times the cost of theoptimal solution, given a heuristic weight of (1+ ?
).Moreover, A?
search allows for straightforward in-tegration of an n-gram language model during thesearch.Another key advantage of the proposed algorithmis the ease with which we can trade-off between2080 2 4 6 8 1000.10.20.30.40.50.6Average(1?WER)Average Running Time (in sec)Figure 6: Experiments showing how the accuracy of thefinal caption by MSA-A?
algorithm varies with the num-ber of inputs from 2 to 10.speed and accuracy.
The algorithm can be tailoredto real-time by using a larger heuristic weight.
Onthe other hand, we can produce better transcripts foroffline tasks by choosing a smaller weight.It is interesting to compare our results with thoseachieved using the MUSCLE MSA tool of Edgar(2004).
One difference is that our system takes a hi-erarchical approach in that it aligns at the word level,but also uses string edit distance at the letter levelas a substitution cost for words.
Thus, it is able totake advantage of the fact that individual transcrip-tions do not generally contain arbitrary fragments ofwords.
More fundamentally, it is interesting to notethat MUSCLE and most other commonly used MSAtools for biological sequences make use of a guidetree formed by a hierarchical clustering of the in-put sequences.
The guide tree produced by the algo-rithms may or may not match the evolutionary treeof the organisms whose genomes are being aligned,but, nevertheless, in the biological application, suchan underlying evolutionary tree generally exists.
Inaligning transcriptions, there is no particular reasonto expect individual pairs of transcriptions to be es-pecially similar to one another, which may make theguide tree approach less appropriate.In order to get competitive results, the A?
searchbased algorithm aligns sequences that are at least 7-10 seconds long.
The delay for collecting the cap-tions within a chunk can introduce latency, however,each alignment usually takes less than 300 millisec-onds, allowing us to repeatedly align the stream ofwords, even before the window is filled.
This pro-vides less accurate but immediate response to users.Finally, when we have all the words entered in achunk, we perform the final alignment and show thecaption to users for the entire chunk.After aligning the input sequences, we obtain thefinal transcript by majority voting at each alignmentposition, which treats each worker equally and doesnot take individual quality into account.
Recently,some work has been done for automatically estimat-ing individual worker?s quality for crowd-based datalabeling tasks (Karger et al 2011; Liu et al 2012).Extending these methods for crowd-based text cap-tioning could be an interesting future direction.6 ConclusionIn this paper, we have introduced a new A?
searchbased MSA algorithm for aligning partial captionsinto a final output stream in real-time.
This methodhas advantages over prior approaches both in for-mal guarantees of optimality and the ability to tradeoff speed and accuracy.
Our experiments on realcaptioning data show that it outperforms prior ap-proaches based on a dependency graph model and astandard MSA implementation (MUSCLE).
An ex-periment with 50 participants explored whether ex-iting automatic metrics of quality matched humanevaluations of readability, showing WER did best.Acknowledgments Funded by NSF awards IIS-1218209 and IIS-0910611.ReferencesKeith Bain, Sara Basson, A Faisman, and D Kanevsky.2005.
Accessibility, transcription, and access every-where.
IBM Systems Journal, 44(3):589?603.Keith Bain, Eunice Lund-Lucas, and Janice Stevens.2012.
22. transcribe your class: Using speech recogni-tion to improve access for at-risk students.
CollectedEssays on Learning and Teaching, 5.Lou Burnard.
1995.
Users Reference Guide British Na-tional Corpus Version 1.0.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of bleu in ma-chine translation research.
In Proceedings of EACL,volume 2006, pages 249?256.209Martin Cooke, Phil Green, Ljubomir Josifovski, and As-cension Vizinho.
2001.
Robust automatic speechrecognition with missing and unreliable acoustic data.Speech Communication, 34(3):267?285.Chuong B Do, Mahathi SP Mahabhashyam, MichaelBrudno, and Serafim Batzoglou.
2005.
Prob-cons: Probabilistic consistency-based multiple se-quence alignment.
Genome Research, 15(2):330?340.Richard Durbin, Sean R Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological sequence analy-sis: probabilistic models of proteins and nucleic acids.Cambridge university press.Robert C Edgar and Serafim Batzoglou.
2006.
Multi-ple sequence alignment.
Current opinion in structuralbiology, 16(3):368?373.Robert C Edgar.
2004.
MUSCLE: multiple sequencealignment with high accuracy and high throughput.Nucleic Acids Research, 32(5):1792?1797.Xiaodong He, Li Deng, and Alex Acero.
2011.
Whyword error rate is not a good metric for speech rec-ognizer training for the speech translation task?
InIEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), 2011, pages 5632?5635.
IEEE.Phillip B Horton.
1997.
Strings, algorithms, and ma-chine learning applications for computational biology.Ph.D.
thesis, University of California, Berkeley.David R Karger, Sewoong Oh, and Devavrat Shah.
2011.Iterative learning for reliable crowdsourcing systems.In Proceedings of Advances in Neural InformationProcessing Systems (NIPS), volume 24, pages 1953?1961.Walter Lasecki and Jeffrey Bigham.
2012.
Online qual-ity control for real-time crowd captioning.
In Pro-ceedings of the 14th international ACM SIGACCESSconference on Computers and accessibility (ASSETS2012), pages 143?150.
ACM.Walter Lasecki, Christopher Miller, Adam Sadilek, An-drew Abumoussa, Donato Borrello, Raja Kushalnagar,and Jeffrey Bigham.
2012.
Real-time captioning bygroups of non-experts.
In Proceedings of the 25rd an-nual ACM symposium on User interface software andtechnology, UIST ?12.Walter Lasecki, Christopher Miller, and Jeffrey Bigham.2013.
Warping time for more effective real-timecrowdsourcing.
In Proceedings of the ACM confer-ence on Human Factors in Computing Systems, CHI?13, page To Appear, New York, NY, USA.
ACM.Martin Lermen and Knut Reinert.
2000.
The prac-tical use of the A* algorithm for exact multiple se-quence alignment.
Journal of Computational Biology,7(5):655?671.Qiang Liu, Jian Peng, and Alex Ihler.
2012.
Varia-tional inference for crowdsourcing.
In Proceedings ofAdvances in Neural Information Processing Systems(NIPS), volume 25, pages 701?709.Dan Melamed, Ryan Green, and Joseph P Turian.
2003.Precision and recall of machine translation.
In Pro-ceedings HLT-NAACL 2003, volume 2, pages 61?63.Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th annual meeting of Association for ComputationalLinguistics, pages 311?318.
Association for Computa-tional Linguistics.Ira Pohl.
1970.
Heuristic search viewed as path findingin a graph.
Artificial Intelligence, 1(3):193?204.Ales?
Praz?a?k, Zdene?k Loose, Jan Trmal, Josef V Psutka,and Josef Psutka.
2012.
Captioning of LiveTV Programs through Speech Recognition and Re-speaking.
In Text, Speech and Dialogue, pages 513?519.
Springer.Murat Saraclar, Michael Riley, Enrico Bocchieri, andVincent Goffin.
2002.
Towards automatic closed cap-tioning: Low latency real time broadcast news tran-scription.
In Proceedings of the International Confer-ence on Spoken Language Processing (ICSLP), pages1741?1744.Julie D Thompson, Desmond G Higgins, and Toby JGibson.
1994.
Clustal w: improving the sensitivityof progressive multiple sequence alignment throughsequence weighting, position-specific gap penaltiesand weight matrix choice.
Nucleic Acids Research,22(22):4673?4680.Mike Wald.
2006a.
Captioning for deaf and hard ofhearing people by editing automatic speech recogni-tion in real time.
Computers Helping People with Spe-cial Needs, pages 683?690.Mike Wald.
2006b.
Creating accessible educational mul-timedia through editing automatic speech recognitioncaptioning in real time.
Interactive Technology andSmart Education, 3(2):131?141.Lusheng Wang and Tao Jiang.
1994.
On the complexityof multiple sequence alignment.
Journal of Computa-tional Biology, 1(4):337?348.Ye-Yi Wang, Alex Acero, and Ciprian Chelba.
2003.
Isword error rate a good indicator for spoken languageunderstanding accuracy.
In IEEE Workshop on Auto-matic Speech Recognition and Understanding, 2003.ASRU?03.
2003, pages 577?582.
IEEE.210
