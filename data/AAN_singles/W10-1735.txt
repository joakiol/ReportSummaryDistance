Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 235?243,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsChunk-based Verb Reordering in VSO Sentences forArabic-English Statistical Machine TranslationArianna Bisazza and Marcello FedericoFondazione Bruno KesslerHuman Language TechnologiesTrento, Italy{bisazza,federico}@fbk.euAbstractIn Arabic-to-English phrase-based statis-tical machine translation, a large numberof syntactic disfluencies are due to wronglong-range reordering of the verb in VSOsentences, where the verb is anticipatedwith respect to the English word order.In this paper, we propose a chunk-basedreordering technique to automatically de-tect and displace clause-initial verbs in theArabic side of a word-aligned parallel cor-pus.
This method is applied to preprocessthe training data, and to collect statisticsabout verb movements.
From this anal-ysis, specific verb reordering lattices arethen built on the test sentences before de-coding them.
The application of our re-ordering methods on the training and testsets results in consistent BLEU score im-provements on the NIST-MT 2009 Arabic-English benchmark.1 IntroductionShortcomings of phrase-based statistical machinetranslation (PSMT) with respect to word reorder-ing have been recently shown on the Arabic-English pair by Birch et al (2009).
An empiri-cal investigation of the output of a strong baselinewe developed with the Moses toolkit (Koehn etal., 2007) for the NIST 2009 evaluation, revealedthat an evident cause of syntactic disfluency is theanticipation of the verb in Arabic Verb-Subject-Object (VSO) sentences ?
a class that is highlyrepresented in the news genre1.Fig.
1 shows two examples where the Arabicmain verb phrase comes before the subject.
Insuch sentences, the subject can be followed byadjectives, adverbs, coordinations, or appositionsthat further increase the distance between the verb1In fact, Arabic syntax admits both SVO and VSO orders.and its object.
When translating into English ?
aprimarily SVO language ?
the resulting long verbreorderings are often missed by the PSMT decodereither because of pure modeling errors or becauseof search errors (Germann et al, 2001): i.e.
theirspan is longer than the maximum allowed distor-tion distance, or the correct reordering hypothesisdoes not emerge from the explored search spacebecause of a low score.
In the two examples, themissed verb reorderings result in different transla-tion errors by the decoder, respectively, the intro-duction of a subject pronoun before the verb and,even worse, a verbless sentence.In Arabic-English machine translation, otherkinds of reordering are of course very frequent: forinstance, adjectival modifiers following their nounand head-initial genitive constructions (Idafa).These, however, appear to be mostly local, there-fore more likely to be modeled through phrase in-ternal alignments, or to be captured by the reorder-ing capabilities of the decoder.
In general there is aquite uneven distribution of word-reordering phe-nomena in Arabic-English, and long-range move-ments concentrate on few patterns.Reordering in PSMT is typically performedby (i) constraining the maximum allowed wordmovement and exponentially penalizing long re-orderings (distortion limit and penalty), and (ii)through so-called lexicalized orientation models(Och et al, 2004; Koehn et al, 2007; Galleyand Manning, 2008).
While the former is mainlyaimed at reducing the computational complexityof the decoding algorithm, the latter assigns ateach decoding step a score to the next sourcephrase to cover, according to its orientation withrespect to the last translated phrase.
In fact, neithermethod discriminates among different reorderingdistances for a specific word or syntactic class.
Toour view, this could be a reason for their inade-quacy to properly deal with the reordering pecu-liarities of the Arabic-English language pair.
In235src: w AstdEt kl mn AlsEwdyp w lybyA w swryASubj sfrA?
hAObj fy AldnmArk .ref: Each of Saudi Arabia , Libya and SyriaSubj recalled their ambassadorsObj from Denmark .MT: He recalled all from Saudi Arabia , Libya and Syria ambassadors in Denmark .src: jdd AlEAhl Almgrby Almlk mHmd AlsAdsSubj dEm hObj l m$rwE Alr}ys Alfrnsyref: The Moroccan monarch King Mohamed VISubj renewed his supportObj to the project of French PresidentMT: The Moroccan monarch King Mohamed VI his support to the French PresidentFigure 1: Examples of problematic SMT outputs due to verb anticipation in the Arabic source.this work, we introduce a reordering techniquethat addresses this limitation.The remainder of the paper is organized as fol-lows.
In Sect.
2 we describe our verb reorderingtechnique and in Sect.
3 we present statistics aboutverb movement collected through this technique.We then discuss the results of preliminary MT ex-periments involving verb reordering of the trainingbased on these findings (Sect.
4).
Afterwards, weexplain our lattice approach to verb reordering inthe test and provide evaluation on a well-knownMT benchmark (Sect.
5).
In the last two sectionswe review some related work and draw the finalconclusions.2 Chunk-based Verb ReorderingThe goal of our work is to displace Arabic verbsfrom their clause-initial position to a position thatminimizes the amount of word reordering neededto produce a correct translation.
In order to re-strict the set of possible movements of a verb andto abstract from the usual token-based movementlength measure, we decided to use shallow syn-tax chunking of the source language.
Full syntac-tic parsing is another option which we have nottried so far mainly because popular parsers that areavailable for Arabic do not mark grammatical re-lations such as the ones we are interested in.We assume that Arabic verb reordering onlyoccurs between shallow syntax chunks, and notwithin them.
For this purpose we annotated ourArabic data with the AMIRA chunker by Diab etal.
(2004)2.
The resulting chunks are generallyshort (1.6 words on average).
We then considera specific type of reordering by defining a produc-tion rule of the kind: ?move a chunk of type Talong with its L left neighbours and R right neigh-bours by a shift of S chunks?.
A basic set of rules2This tool implies morphological segmentation of theArabic text.
All word statistics in this paper refer to AMIRA-segmented text.that displaces the verbal chunk to the right by atmost 10 positions corresponds to the setting:T=?VP?, L=0, R=0, S=1..10In order to address cases where the verb is movedalong with its adverbial, we also add a set of rulesthat include a one-chunk right context in the move-ment:T=?VP?, L=0, R=1, S=1..10To prevent verb reordering from overlappingwith the scope of the following clause, we alwayslimit the maximum movement to the position ofthe next verb.
Thus, for each verb occurrence, thenumber of allowed movements for our setting is atmost 2?
10 = 20.Assuming that a word-aligned translation of thesentence is available, the best movement, if any,will be the one that reduces the amount of distor-tion in the alignment, that is: (i) it reduces thenumber of swaps by 1 or more, and (ii) it mini-mizes the sum of distances between source posi-tions aligned to consecutive target positions, i.e.
?i |ai ?
(ai?1 + 1)| where ai is the index of theforeign word aligned to the ith English word.
Incase several movements are optimal according tothese two criteria, e.g.
because of missing word-alignment links, only the shortest good movementis retained.The proposed reordering method has been ap-plied to various parallel data sets in order to per-form a quantitative analysis of verb anticipation,and to train a PSMT system on more monotonicalignments.3 Analysis of Verb ReorderingWe applied the above technique to two parallelcorpora3 provided by the organizers of the NIST-MT09 Evaluation.
The first corpus (Gale-NW)contains human-made alignments.
As these re-fer to non-segmented text, they were adjusted to3Newswire sections of LDC2006E93 and LDC2009E08,respectively 4337 and 777 sentence pairs.236Figure 2: Percentage of verb reorderings by maxi-mum shift (0 stands for no movement).agree with AMIRA-style segmentation.
For thesecond corpus (Eval08-NW), we filtered out sen-tences longer than 80 tokens in order to makeword alignment feasible with GIZA++ (Och andNey, 2003).
We then used the Intersection ofthe direct and inverse alignments, as computed byMoses.
The choice of such a high-precision, low-recall alignment set is supported by the findings ofHabash (2007) on syntactic rule extraction fromparallel corpora.3.1 The Verb?s DanceThere are 1,955 verb phrases in Gale-NW and11,833 in Eval08-NW.
Respectively 86% and 84%of these do not need to be moved according to thealignments.
The remaining 14% and 16% are dis-tributed by movement length as shown in Fig.
2:most verb reorderings consist in a 1-chunk longjump to the right (8.3% in Gale-NW and 11.6% inEval08-NW).
The rest of the distribution is simi-lar in the two corpora, which indicates a good cor-respondence between verb reordering observed inautomatic and manual alignments.
By increasingthe maximum movement length from 1 to 2, wecan cover an additional 3% of verb reorderings,and around 1% when passing from 2 to 3.
Werecall that the length measured in chunks doesn?tnecessarily correspond to the number of jumpedtokens.
These figures are useful to determine anoptimal set of reordering rules.
From now on wewill focus on verb movements of at most 6 chunks,as these account for about 99.5% of the verb oc-currences.Figure 3: Distortion reduction in the GALE-NWcorpus: jump occurrences grouped by length range(in nb.
of words).3.2 Impact on Corpus Global DistortionWe tried to measure the impact of chunk-basedverb reordering on the total word distortion foundin parallel data.
For the sake of reliability, thisinvestigation was carried out on the manuallyaligned corpus (Gale-NW) only.
Fig.
3 shows thepositive effect of verb reordering on the total dis-tortion, which is measured as the number of wordsthat have to be jumped on the source side in or-der to cover the sentence in the target order (thatis |ai ?
(ai?1 + 1)|).
Jumps have been groupedby length and the relative decrease of jumps perlength is shown on top of each double column.These figures do not prove as we hoped thatverb reordering resolves most of the long range re-orderings.
Thus we manually inspected a sampleof verb-reordered sentences that still contain longjumps, and found out that many of these were dueto what we could call ?unnecessary?
reordering.
Infact, human translations that are free to some ex-tent, often display a global sentence restructuringthat makes distortion dramatically increase.
Webelieve this phenomenon introduces noise in ouranalysis since these are not reorderings that an MTsystem needs to capture to produce an accurateand fluent translation.Nevertheless, we can see from the relative de-crease percentages shown in the plot, that althoughshort jumps are by far the most frequent, verbreordering affects especially medium and longrange distortion.
More precisely, our selectivereordering technique solves 21.8% of the 5-to-6-words jumps, 25.9% of the 7-to-9-words jumpsand 24.2% of the 10-to-14-words jumps, against237only 9.5% of the 2-words jumps, for example.Since our primary goal is to improve the handlingof long reorderings, this makes us think that weare advancing in a promising direction.4 Preliminary ExperimentsIn this section we investigate how verb reorderingon the source language can affect translation qual-ity.
We apply verb reordering both on the trainingand the test data.
However, while the parallel cor-pus used for training can be reordered by exploit-ing word alignments, for the test corpus we needa verb reordering ?prediction model?.
For thesepreliminary experiments, we assumed that optimalverb-reordering of the test data is provided by anoracle that has access to the word alignments withthe reference translations.4.1 SetupWe trained a Moses-based system on a subset ofthe NIST-MT09 Evaluation data4 for a total of981K sentences, 30M words.
We first aligned thedata with GIZA++ and use the resulting Intersec-tion set to apply the technique explained in Sect.
2.We then retrained the whole system ?
from wordalignment to phrase scoring ?
on the reordereddata and evaluated it on two different versions ofEval08-NW: plain and oracle verb-reordered, ob-tained by exploiting word alignments with the firstof the four available English references.
The firstexperiment is meant to measure the impact of theverb reordering procedure on training only.
Thelatter will provide an estimate of the maximum im-provement we can expect from the application tothe test of an optimal verb reordering predictiontechnique.
Given our experimental setting, onecould argue that our BLEU score is biased becauseone of the references was also used to generate theverb reordering.
However, in a series of exper-iments not reported here, we evaluated the samesystems using only the remaining three referencesand observed similar trends as when all four refer-ences are used.Feature weights were optimized through MERT(Och, 2003) on the newswire section of the NIST-MT06 evaluation set (Dev06-NW), in the origi-nal version for the baseline system, in the verb-reordered version for the reordered system.4LDC2007T08, 2003T07, 2004E72, 2004T17, 2004T18,2005E46, 2006E25, 2006E44 and LDC2006E39 ?
the twolast with first reference only.Figure 4: BLEU scores of baseline and reorderedsystem on plain and oracle reordered Eval08-NW.Fig.
4 shows the results in terms of BLEU scorefor (i) the baseline system, (ii) the reordered sys-tem on a plain version of Eval08-NW and (iii) thereordered system on the reordered test.
The scoresare plotted against the distortion limit (DL) usedin decoding.
Because high DL values (8-10) im-ply a larger search space and because we want togive Moses the best possible conditions to prop-erly handle long reordering, we relaxed for theseconditions the default pruning parameter to thepoint that led the highest BLEU score5.4.2 DiscussionThe first observation is that the reordered systemalways performs better (0.5?0.6 points) than thebaseline on the plain test, despite the mismatchbetween training and test ordering.
This may bedue to the fact that automatic word alignmentsare more accurate when less reordering is presentin the data, although previous work (Lopez andResnik, 2006) showed that even large gains inalignment accuracy seldom lead to better trans-lation performances.
Moreover phrase extractionmay benefit from a distortion reduction, since itsheuristics rely on word order in order to expandthe context of alignment links.The results on the oracle reordered test are alsointeresting: a gain of at least 1.2 point absoluteover the baseline is reported in all tested DL condi-tions.
These improvements are remarkable, keep-ing in mind that only 31% of the train and 33% ofthe test sentences get modified by verb reordering.5That is, the histogram pruning maximum stack size wasset to 1000 instead of the default 200.238Figure 5: Reordering lattices for Arabic VSO sentences: word-based and chunk-based.Concerning distortion, although long verbmovements are often observed in parallel corpora,relaxing the DL to high values does not bene-fit the translation, even with our ?generous?
set-ting (wider beam search).
This is probably due tothe fact that, with weakly constrained distortion,the risk of search errors increases as the reorder-ing model fails to properly rank an exponentiallygrowing set of permutations.
Therefore many cor-rect reordering hypotheses receive low scores andget lost in pruning or recombination.5 Verb Reordering LatticesHaving assessed the negative impact of VSO sen-tences on Arabic-English translation performance,we now propose a method to improve the handlingof this phenomenon at decoding time.
As in realworking conditions word alignments of the inputtext are not available, we explore a reordering lat-tice approach.5.1 Lattice ConstructionFirstly conceived to optimally encode multipletranscription hypothesis produced by a speech rec-ognizer, word lattices have later been used to rep-resent various forms of input ambiguity, mainly atthe level of token boundaries (e.g.
word segmenta-tion, morphological decomposition, word decom-pounding (Dyer et al, 2008)).A main problem when dealing with permuta-tions is that the lattice size can grow very quicklywhen medium to long reorderings are represented.We are particularly concerned with this issue be-cause our decoding will perform additional re-ordering on the lattice input.
Thanks to the re-strictions we set on our verb movement reorderingrules described in Sect.
2 ?
i.e.
only reordering be-tween chunks and no overlap between consecutiveverb chunks movement ?
we are able to producequite compact word lattices.Fig.
5 illustrates how a chunk-based reorderinglattice is generated.
Suppose we want to translatethe Arabic sentence ?w >kdt mSAdr rsmyp wjwdrAbT byn AlAEtdA?At?, whose English meaning is?Official sources confirmed that there was a linkbetween the attacks?.
The Arabic main verb >kdt(confirmed) is in pre-subject position.
If we ap-plied word-based rather than chunk-based rules tomove the verb to the right, we would produce thefirst lattice of the figure, containing 7 paths (theoriginal plus 6 verb movements).
With the chunk-based rules, we treat instead chunks as units andget the second lattice.
Then, by expanding eachchunk, we obtain the final, less dense lattice, thatcompared to the first does not contain 3 (unlikely)reordering edges.To be consistent with the reordering applied tothe training data, we use a set of rules that moveeach verb phrase alone or with its following chunkby 1 to 6 chunks to the right.
With this settings,239Figure 6: Structure of a chunk-based reordering lattice for verb reordering, before word expansion.
Edgesin boldface represent the verbal chunk.our lattice generation algorithm computes a com-pact lattice (Fig.
6) that introduces at most 5?
?Schunk edges for each verb chunk, where ?S is thepermitted movement range (6 in this case).Before translation, each edge has to be associ-ated with a weight that the decoder will use as ad-ditional feature.
To differentiate between the orig-inal word order and verb reordering we assign afixed weight of 1 to the edges of the plain path, and0.25 to the other edges.
As future work we will de-vise more discriminative weighting schemes.5.2 EvaluationFor the experiments, we relied on the existingMoses-implementation of non-monotonic decod-ing for word lattices (Dyer et al, 2008) withsome fixes concerning the computation of reorder-ing distance.
The translation system is the sameas the one presented in Sect.
4, to which weadded an additional feature function evaluatingthe lattice weights (weight-i).
Instead of rerun-ning MERT, we directly estimated the additionalfeature-function weight over a suitable interval(0.002 to 0.5), by running the decoder severaltimes on the development set.
The resulting op-timal weight was 0.05.Table 1 presents results on three test sets:Eval08-NW which was used to calibrate the re-ordering rules, Reo08-NW a specific test set con-sisting of the 33% of Eval08-NW sentences thatactually require verb reordering, and Eval09-NWa yet unseen dataset (newswire section of theNIST-MT09 evaluation set, 586 sentences).
Bestresults with lattice decoding were obtained with adistortion limit (DL) of 4, while best performanceof text decoding was obtained with a DL of 6.As we hoped, translating a verb reordering lat-tice yields an additional improvement to the re-ordering of the training corpus: from 43.67%to 44.04% on Eval08-NW and from 48.53% to48.96% on Eval09-NW.
The gap between thebaseline and the score obtainable by oracle verbreordering, as estimated in the preliminary exper-iments on Eval08-NW (44.36%), has been largelyfilled.On the specific test set ?
Reo08-NW ?
we ob-serve a performance drop when reordered modelsare applied to non-reordered (plain) input: from46.90% to 46.64%.
Hence it seems that the mis-match between training and test data is signifi-cantly impacting on the reordering capabilities ofthe system with respect to verbs.
We speculatethat such negative effect is diluted in the full testset (Eval08-NW) and compensated by the positiveinfluence of verb reordering on phrase extraction.Indeed, when the lattice technique is applied weget an improvement of about 0.6 point over thebaseline, which is still a fair result, but not as goodas the one obtained on the general test sets.Finally, our approach led to an overall gain of0.8 point BLEU over the baseline, on Eval09-NW.We believe this is a satisfactory result, given thefairly good starting performance, and given thatthe BLEU metric is known not to be very sensi-tive to word order variations (Callison-Burch etal., 2006).
For the future, we plan to also use spe-cific evaluation metrics that will allow us to isolatethe impact of our approach on reordering, like theones by Birch et al (2010).System DL eval08nw reo08nw eval09nwbaseline 6 43.10 46.90 48.13reord.
training +plain input 6 43.67 46.64 48.53lattice 4 44.04 47.51 48.96oracle reord.
4 44.36 48.25 naTable 1: BLEU scores of baseline and reorderedsystem on plain test and on reordering lattices.2406 Related WorkLinguistically motivated word reordering forArabic-English has been proposed in several re-cent works.
Habash (2007) extracts syntactic re-ordering rules from a word-aligned parallel cor-pus whose Arabic side has been fully parsed.
Therules involve reordering of syntactic constituentsand are applied in a deterministic way (alwaysthe most probable) as preprocessing of trainingand test data.
The technique achieves consistentimprovements only in very restrictive conditions:maximum phrase size of 1 and monotonic decod-ing, thus failing to enhance the existing reorder-ing capabilities of PSMT.
In (Crego and Habash,2008; Elming and Habash, 2009) possible in-put permutations are represented through a wordgraph, which is then processed by a monotonicphrase- or n-gram-based decoder.
Thus, these ap-proaches are conceived as alternatives, rather thanintegrations, to PSMT reordering.
On the contrary,we focused on a single type of significant long re-orderings, in order to integrate class-specific re-ordering methods into a standard PSMT system.To our knowledge, the work by Niehues andKolss (2009) on German-English is the only ex-ample of a lattice-based reordering approach be-ing coupled with reordering at decoding time.
Intheir paper, discontinuous non-deterministic POS-based rules learned from a word-aligned corpusare applied to German sentences in the form ofweighted edges in a word lattice.
Their phrase-based decoder admits local reordering within afixed window of 2 words, while, in our work, weperformed experiments up to a distortion limit of10.
Another major difference is that we used shal-low syntax annotation to effectively reduce thenumber of possible permutations.
A first attemptto adapt our technique to the German language isdescribed in Hardmeier et al (2010).Our work is also tightly related to the prob-lem of noun-phrase subject detection, recently ad-dressed by Green et al (2009).
In fact, detect-ing the extension of the subject can be a suffi-cient condition to guess the optimal reordering ofthe verb.
In their paper, a discriminative classi-fier was trained on a rich variety of linguistic fea-tures to detect the full scope of Arabic NP subjectsin verb-initial clauses.
The authors reported an F-score of 61.3%, showing that the problem is hardto solve even when more linguistic information isavailable.
In order to integrate the output of theclassifier into a PSMT decoder, a specific trans-lation feature was designed to reward hypothesesin which the subject is translated as a contiguousblock.
Unfortunately, no improvement in transla-tion quality was obtained.7 ConclusionsWord reordering remains one of the hardest prob-lems in statistical machine translation.
Based onthe intuition that few reordering patterns wouldsuffice to handle the most significant cases of longreorderings in Arabic-English, we decided to fo-cus on the problem of VSO sentences.Thanks to simple linguistic assumptions on verbmovement, we developed an efficient, low-costtechnique to reorder the training data, on the onehand, and to better handle verb reordering at de-coding time, on the other.
In particular, translationis performed on a compact word lattice that repre-sents likely verb movements.
The resulting systemoutperforms a strong baseline in terms of BLEU,and produces globally more readable translations.However, the problem is not totally solved becausemany verb reorderings are still missed, despitethe suggestions provided by the lattice.
Differentfactors can explain these errors: poor interactionbetween lattice and distortion/orientation modelsused by the decoder; poor discriminative power ofthe target language model with respect to differentreorderings of the source.As a first step to improvement, we will devisea discriminative weighting scheme based on thelength of the reorderings represented in the lat-tice.
For the longer term we are working towardsbringing linguistically informed reordering con-straints inside decoding, as an alternative to thelattice solution.
In addition, we plan to coupleour reordering technique with more informativelanguage models, including for instance syntac-tic analysis of the hypothesis under construction.Finally we would like to compare the proposedchunk-based technique with one that exploits fullsyntactic parsing of the Arabic sentence to furtherdecrease the reordering possibilities of the verb.AcknowledgmentsThis work was supported by the EuroMatrixPlusproject (IST-231720) which is funded by the Eu-ropean Commission under the Seventh FrameworkProgramme for Research and Technological De-velopment.241src: w A$Ar AlsnAtwr AlY dEm h m$rwEA ErD ElY mjls Al$ywxref: The Senator referred to his support for a project proposed to the Senatebase MT: The Senator to support projects presented to the Senatenew MT: Senator noted his support projects presented to the Senatesrc: mn jAnb h hdd >bw mSEb EbdAlwdwd Amyr AlqAEdp b blAd Almgrb AlAslAmy fy nfs Al$ryT b AlqyAmb slslp AEtdA?At w >EmAl <rhAbyp Dd AlmSAlH w Alm&ssAt AljzA}ryp fy AlEdyd mn AlmnATqAljzA}rypref: For his part , Abu Musab Abd al-Wadud , the commander of al-Qaeda in the Islamic Maghreb Countries ,threatened in the same tape to carry out a series of attacks and terrorist actions against Algerian interests andorganizations in many parts of Algeriabase MT: For his part threatened Abu Musab EbdAlwdwd Amir al-Qaeda Islamic Morocco country in the same tape tocarry out a series of attacks and terrorist acts against the interests and the Algerian institutions in many areas ofAlgiersnew MT: For his part , Abu Musab EbdAlwdwd Amir al Qaida threatened to Morocco Islamic country in the same tapeto carry out a series of attacks and terrorist acts against the interests of the Algerian and institutions in manyareas of Algierssrc: w ymtd Alm$rwE 500 km mtr w yrbT Almdyntyn Almqdstyn b mdynp jdp ElY sAHl AlbHr Al>Hmr .ref: The project is 500 kilometers long and connects the two holy cities with the city of Jeddah on the Red Sea coast.base MT: It extends the project 500 km and linking the two holy cities in the city of Jeddah on the Red Sea coast .new MT: The project extends 500 km , linking the two holy cities in the city of Jeddah on the Red Sea coast .Figure 7: Examples showing MT improvements coming from chunk-based verb-reordering.ReferencesAlexandra Birch, Phil Blunsom, and Miles Osborne.2009.
A quantitative analysis of reordering phe-nomena.
In StatMT ?09: Proceedings of the FourthWorkshop on Statistical Machine Translation, pages197?205, Morristown, NJ, USA.
Association forComputational Linguistics.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating re-ordering.
Machine Translation, Published online.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluation the role of BLEUin machine translation research.
In Proceedings ofthe 11th Conference of the European Chapter of theAssociation for Computational Linguistics, Trento,Italy, April.Josep M. Crego and Nizar Habash.
2008.
Using shal-low syntax information to improve word alignmentand reordering for smt.
In StatMT ?08: Proceedingsof the Third Workshop on Statistical Machine Trans-lation, pages 53?61, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.2004.
Automatic Tagging of Arabic Text: FromRaw Text to Base Phrase Chunks.
In Daniel MarcuSusan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Short Papers, pages 149?152,Boston, Massachusetts, USA, May 2 - May 7.
As-sociation for Computational Linguistics.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL-08: HLT, pages 1012?1020, Columbus, Ohio, June.
Association for Com-putational Linguistics.Jakob Elming and Nizar Habash.
2009.
Syntactic re-ordering for English-Arabic phrase-based machinetranslation.
In Proceedings of the EACL 2009 Work-shop on Computational Approaches to Semitic Lan-guages, pages 69?77, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP ?08: Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 848?856, Morristown, NJ, USA.Association for Computational Linguistics.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decodingand optimal decoding for machine translation.
InProceedings of the 39th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages228?335, Toulouse, France.Spence Green, Conal Sathi, and Christopher D. Man-ning.
2009.
NP subject detection in verb-initial Ara-bic clauses.
In Proceedings of the Third Workshopon Computational Approaches to Arabic Script-based Languages (CAASL3), Ottawa, Canada.Nizar Habash.
2007.
Syntactic preprocessing for sta-tistical machine translation.
In Bente Maegaard, ed-itor, Proceedings of the Machine Translation SummitXI, pages 215?222, Copenhagen, Denmark.Christian Hardmeier, Arianna Bisazza, and MarcelloFederico.
2010.
FBK at WMT 2010: Word lat-tices for morphological reduction and chunk-based242reordering.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-rics MATR, Uppsala, Sweden, July.
Association forComputational Linguistics.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.Adam Lopez and Philip Resnik.
2006.
Word-basedalignment, phrase-based translation: What?s thelink?
In 5th Conference of the Association for Ma-chine Translation in the Americas (AMTA), Boston,Massachusetts, August.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 206?214, Athens, Greece,March.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-mada, A. Fraser, S. Kumar, L. Shen, D. Smith,K.
Eng, V. Jain, Z. Jin, and D. Radev.
2004.
A smor-gasbord of features for statistical machine transla-tion.
In Proceedings of the Joint Conference on Hu-man Language Technologies and the Annual Meet-ing of the North American Chapter of the Associ-ation of Computational Linguistics (HLT-NAACL),Boston, MA.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Erhard Hinrichsand Dan Roth, editors, Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 160?167.243
