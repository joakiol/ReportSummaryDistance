Proceedings of the 10th Conference on Parsing Technologies, pages 60?68,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA log-linear model with an n-gram reference distribution for accurate HPSGparsingTakashi NinomiyaInformation Technology CenterUniversity of Tokyoninomi@r.dl.itc.u-tokyo.ac.jpTakuya MatsuzakiDepartment of Computer ScienceUniversity of Tokyomatuzaki@is.s.u-tokyo.ac.jpYusuke MiyaoDepartment of Computer ScienceUniversity of Tokyoyusuke@is.s.u-tokyo.ac.jpJun?ichi TsujiiDepartment of Computer Science, University of TokyoSchool of Informatics, University of ManchesterNaCTeM (National Center for Text Mining)tsujii@is.s.u-tokyo.ac.jpHongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JapanAbstractThis paper describes a log-linear model withan n-gram reference distribution for accurateprobabilistic HPSG parsing.
In the model,the n-gram reference distribution is simplydefined as the product of the probabilitiesof selecting lexical entries, which are pro-vided by the discriminative method with ma-chine learning features of word and POSn-gram as defined in the CCG/HPSG/CDGsupertagging.
Recently, supertagging be-comes well known to drastically improvethe parsing accuracy and speed, but su-pertagging techniques were heuristically in-troduced, and hence the probabilistic mod-els for parse trees were not well defined.We introduce the supertagging probabilitiesas a reference distribution for the log-linearmodel of the probabilistic HPSG.
This is thefirst model which properly incorporates thesupertagging probabilities into parse tree?sprobabilistic model.1 IntroductionFor the last decade, fast, accurate and wide-coverageparsing for real-world text has been pursued insophisticated grammar formalisms, such as head-driven phrase structure grammar (HPSG) (Pollardand Sag, 1994), combinatory categorial grammar(CCG) (Steedman, 2000) and lexical function gram-mar (LFG) (Bresnan, 1982).
They are preferredbecause they give precise and in-depth analysesfor explaining linguistic phenomena, such as pas-sivization, control verbs and relative clauses.
Themain difficulty of developing parsers in these for-malisms was how to model a well-defined proba-bilistic model for graph structures such as featurestructures.
This was overcome by a probabilisticmodel which provides probabilities of discriminat-ing a correct parse tree among candidates of parsetrees in a log-linear model or maximum entropymodel (Berger et al, 1996) with many features forparse trees (Abney, 1997; Johnson et al, 1999; Rie-zler et al, 2000; Malouf and van Noord, 2004; Ka-plan et al, 2004; Miyao and Tsujii, 2005).
Follow-ing this discriminative approach, techniques for effi-ciency were investigated for estimation (Geman andJohnson, 2002; Miyao and Tsujii, 2002; Malouf andvan Noord, 2004) and parsing (Clark and Curran,2004b; Clark and Curran, 2004a; Ninomiya et al,2005).An interesting approach to the problem of parsingefficiency was using supertagging (Clark and Cur-60ran, 2004b; Clark and Curran, 2004a; Wang, 2003;Wang and Harper, 2004; Nasr and Rambow, 2004;Ninomiya et al, 2006; Foth et al, 2006; Foth andMenzel, 2006), which was originally developed forlexicalized tree adjoining grammars (LTAG) (Ban-galore and Joshi, 1999).
Supertagging is a processwhere words in an input sentence are tagged with?supertags,?
which are lexical entries in lexicalizedgrammars, e.g., elementary trees in LTAG, lexicalcategories in CCG, and lexical entries in HPSG.
Theconcept of supertagging is simple and interesting,and the effects of this were recently demonstrated inthe case of a CCG parser (Clark and Curran, 2004a)with the result of a drastic improvement in the pars-ing speed.
Wang and Harper (2004) also demon-strated the effects of supertagging with a statisti-cal constraint dependency grammar (CDG) parserby showing accuracy as high as the state-of-the-artparsers, and Foth et al (2006) and Foth and Menzel(2006) reported that accuracy was significantly im-proved by incorporating the supertagging probabili-ties into manually tuned Weighted CDG.
Ninomiyaet al (2006) showed the parsing model using onlysupertagging probabilities could achieve accuracy ashigh as the probabilistic model for phrase structures.This means that syntactic structures are almost de-termined by supertags as is claimed by Bangaloreand Joshi (1999).
However, supertaggers themselveswere heuristically used as an external tagger.
Theyfilter out unlikely lexical entries just to help parsing(Clark and Curran, 2004a), or the probabilistic mod-els for phrase structures were trained independentlyof the supertagger?s probabilistic models (Wang andHarper, 2004; Ninomiya et al, 2006).
In the case ofsupertagging of Weighted CDG (Foth et al, 2006),parameters for Weighted CDG are manually tuned,i.e., their model is not a well-defined probabilisticmodel.We propose a log-linear model for probabilisticHPSG parsing in which the supertagging probabil-ities are introduced as a reference distribution forthe probabilistic HPSG.
The reference distribution issimply defined as the product of the probabilities ofselecting lexical entries, which are provided by thediscriminative method with machine learning fea-tures of word and part-of-speech (POS) n-gram asdefined in the CCG/HPSG/CDG supertagging.
Thisis the first model which properly incorporates the su-pertagging probabilities into parse tree?s probabilis-tic model.
We compared our model with the proba-bilistic model for phrase structures (Miyao and Tsu-jii, 2005).
This model uses word and POS unigramfor its reference distribution, i.e., the probabilities ofunigram supertagging.
Our model can be regardedas an extension of a unigram reference distributionto an n-gram reference distribution with features thatare used in supertagging.
We also compared with aprobabilistic model in (Ninomiya et al, 2006).
Theprobabilities of their model are defined as the prod-uct of probabilities of supertagging and probabilitiesof the probabilistic model for phrase structures, buttheir model was trained independently of supertag-ging probabilities, i.e., the supertagging probabili-ties are not used for reference distributions.2 HPSG and probabilistic modelsHPSG (Pollard and Sag, 1994) is a syntactic theorybased on lexicalized grammar formalism.
In HPSG,a small number of schemata describe general con-struction rules, and a large number of lexical entriesexpress word-specific characteristics.
The structuresof sentences are explained using combinations ofschemata and lexical entries.
Both schemata andlexical entries are represented by typed feature struc-tures, and constraints represented by feature struc-tures are checked with unification.An example of HPSG parsing of the sentence?Spring has come?
is shown in Figure 1.
First,each of the lexical entries for ?has?
and ?come?is unified with a daughter feature structure of theHead-Complement Schema.
Unification providesthe phrasal sign of the mother.
The sign of thelarger constituent is obtained by repeatedly applyingschemata to lexical/phrasal signs.
Finally, the parseresult is output as a phrasal sign that dominates thesentence.Given a set W of words and a set F of featurestructures, an HPSG is formulated as a tuple, G =?L,R?, whereL = {l = ?w,F ?|w ?
W, F ?
F} is a set oflexical entries, andR is a set of schemata; i.e., r ?
R is a partialfunction: F ?
F ?
F .Given a sentence, an HPSG computes a set ofphrasal signs, i.e., feature structures, as a result of61SpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2head-compHEAD  verbSUBJ  < >COMPS  < >HEAD  nounSUBJ  < >COMPS  < >1=?SpringHEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1hasHEAD  verbSUBJ  <    >COMPS  < >1come2HEAD  verbSUBJ  <    >COMPS  < >1HEAD  verbSUBJ  < >COMPS  < >1subject-headhead-compFigure 1: HPSG parsing.parsing.
Note that HPSG is one of the lexicalizedgrammar formalisms, in which lexical entries deter-mine the dominant syntactic structures.Previous studies (Abney, 1997; Johnson et al,1999; Riezler et al, 2000; Malouf and van Noord,2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)defined a probabilistic model of unification-basedgrammars including HPSG as a log-linear model ormaximum entropy model (Berger et al, 1996).
Theprobability that a parse result T is assigned to agiven sentence w = ?w1, .
.
.
, wn?
is(Probabilistic HPSG)phpsg(T |w) = 1Zw exp(?u?ufu(T ))Zw =?T ?exp(?u?ufu(T ?
)),where ?u is a model parameter, fu is a feature func-tion that represents a characteristic of parse tree T ,and Zw is the sum over the set of all possible parsetrees for the sentence.
Intuitively, the probabilityis defined as the normalized product of the weightsexp(?u) when a characteristic corresponding to fuappears in parse result T .
The model parameters, ?u,are estimated using numerical optimization methods(Malouf, 2002) to maximize the log-likelihood ofthe training data.However, the above model cannot be easily esti-mated because the estimation requires the compu-tation of p(T |w) for all parse candidates assignedto sentence w. Because the number of parse can-didates is exponentially related to the length of thesentence, the estimation is intractable for long sen-tences.
To make the model estimation tractable, Ge-man and Johnson (Geman and Johnson, 2002) andMiyao and Tsujii (Miyao and Tsujii, 2002) proposeda dynamic programming algorithm for estimatingp(T |w).
Miyao and Tsujii (2005) also introduced apreliminary probabilistic model p0(T |w) whose es-timation does not require the parsing of a treebank.This model is introduced as a reference distribution(Jelinek, 1998; Johnson and Riezler, 2000) of theprobabilistic HPSG model; i.e., the computation ofparse trees given low probabilities by the model isomitted in the estimation stage (Miyao and Tsujii,2005), or a probabilistic model can be augmentedby several distributions estimated from the largerand simpler corpus (Johnson and Riezler, 2000).
In(Miyao and Tsujii, 2005), p0(T |w) is defined as theproduct of probabilities of selecting lexical entrieswith word and POS unigram features:(Miyao and Tsujii (2005)?s model)puniref (T |w) = p0(T |w) 1Zw exp(?u?ufu(T ))Zw =?T ?p0(T ?|w) exp(?u?ufu(T ?
))p0(T |w) =n?i=1p(li|wi),where li is a lexical entry assigned to word wi inT and p(li|wi) is the probability of selecting lexicalentry li for wi.In the experiments, we compared our model withother two types of probabilistic models using a su-pertagger (Ninomiya et al, 2006).
The first one isthe simplest probabilistic model, which is definedwith only the probabilities of lexical entry selec-tion.
It is defined simply as the product of the prob-abilities of selecting all lexical entries in the sen-tence; i.e., the model does not use the probabilitiesof phrase structures like the probabilistic models ex-plained above.
Given a set of lexical entries, L, asentence, w = ?w1, .
.
.
, wn?, and the probabilisticmodel of lexical entry selection, p(li ?
L|w, i), thefirst model is formally defined as follows:62HEAD  verbSUBJ  <>COMPS <>HEAD  nounSUBJ  <>COMPS <>HEAD  verbSUBJ  <   >COMPS <>HEAD  verbSUBJ  <   >COMPS <   >HEAD  verbSUBJ  <   >COMPS <>subject-headhead-compSpring/NN has/VBZ come/VBN11 11 22froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>fbinary=head-comp, 1, 0,1, VP, has, VBZ,                    ,1, VP, come, VBN,HEAD  verbSUBJ  <NP>COMPS <VP>HEAD  verbSUBJ  <NP>COMPS <>flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>Figure 2: Example of features.
(Ninomiya et al (2006)?s model 1)pmodel1(T |w) =n?i=1p(li|w, i),where li is a lexical entry assigned to word wi in Tand p(li|w, i) is the probability of selecting lexicalentry li for wi.The probabilities of lexical entry selection,p(li|w, i), are defined as follows:(Probabilistic model of lexical entry selection)p(li|w, i) = 1Zw exp(?u?ufu(li,w, i))Zw =?l?exp(?u?ufu(l?,w, i)),where Zw is the sum over all possible lexical entriesfor the word wi.The second model is a hybrid model of supertag-ging and the probabilistic HPSG.
The probabilitiesare given as the product of Ninomiya et al (2006)?smodel 1 and the probabilistic HPSG.
(Ninomiya et al (2006)?s model 3)pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)In the experiments, we compared our model withMiyao and Tsujii (2005)?s model and Ninomiya etfbinary =?
r, d, c,spl, syl, hwl, hpl, hll,spr, syr, hwr, hpr, hlr?funary = ?r, sy, hw, hp, hl?froot = ?sy, hw, hp, hl?flex = ?wi, pi, li?fsptag =?wi?1, wi, wi+1,pi?2, pi?1, pi, pi+1, pi+2?r name of the applied schemad distance between the head words of the daughtersc whether a comma exists between daughtersand/or inside daughter phrasessp number of words dominated by the phrasesy symbol of the phrasal categoryhw surface form of the head wordhp part-of-speech of the head wordhl lexical entry assigned to the head wordwi i-th wordpi part-of-speech for wili lexical entry for wiTable 1: Feature templates.al.
(2006)?s model 1 and 3.
The features used in ourmodel and their model are combinations of the fea-ture templates listed in Table 1 and Table 2.
Thefeature templates fbinary and funary are defined forconstituents at binary and unary branches, froot is afeature template set for the root nodes of parse trees.flex is a feature template set for calculating the uni-gram reference distribution and is used in Miyao andTsujii (2005)?s model.
fsptag is a feature templateset for calculating the probabilities of selecting lex-ical entries in Ninomiya et al (2006)?s model 1 and3.
The feature templates in fsptag are word trigramsand POS 5-grams.
An example of features appliedto the parse tree for the sentence ?Spring has come?is shown in Figure 2.3 Probabilistic HPSG with an n-gramreference distributionIn this section, we propose a probabilistic modelwith an n-gram reference distribution for probabilis-tic HPSG parsing.
This is an extension of Miyaoand Tsujii (2005)?s model by replacing the unigramreference distribution with an n-gram reference dis-tribution.
Our model is formally defined as follows:63combinations of feature templates for fbinary?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?combinations of feature templates for funary?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?combinations of feature templates for froot?hw, hp, hl?, ?hw, hp?, ?hw, hl?,?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?combinations of feature templates for flex?wi, pi, li?, ?pi, li?combinations of feature templates for fsptag?wi?1?, ?wi?, ?wi+1?,?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,?wi?1, wi?, ?wi, wi+1?,?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)pnref (T |w) =1Znref pmodel1(T |w) exp(?u?ufu(T ))Znref =?T ?pmodel1(T ?|w) exp(?u?ufu(T ?
)).In our model, Ninomiya et al (2006)?s model 1is used as a reference distribution.
The probabilis-tic model of lexical entry selection and its featuretemplates are the same as defined in Ninomiya et al(2006)?s model 1.The formula of our model is the same as Ni-nomiya et al (2006)?s model 3.
But, their modelis not a probabilistic model with a reference distri-bution.
Both our model and their model consist ofthe probabilities for lexical entries (= pmodel1(T |w))and the probabilities for phrase structures (= the restof each formula).
The only difference between ourmodel and their model is the way of how to trainmodel parameters for phrase structures.
In both ourmodel and their model, the parameters for lexical en-tries (= the parameters of pmodel1(T |w)) are first es-timated from the word and POS sequences indepen-dently of the parameters for phrase structures.
Thatis, the estimated parameters for lexical entries arethe same in both models, and hence the probabilitiesof pmodel1(T |w) of both models are the same.
Notethat the parameters for lexical entries will never beupdated after this estimation stage; i.e., the parame-ters for lexical entries are not estimated in the sametime with the parameters for phrase structures.
Thedifference of our model and their model is the esti-mation of parameters for phrase structures.
In ourmodel, given the probabilities for lexical entries, theparameters for phrase structures are estimated so asto maximize the entire probabilistic model (= theproduct of the probabilities for lexical entries andthe probabilities for phrase structures) in the train-ing corpus.
In their model, the parameters for phrasestructures are trained without using the probabili-ties for lexical entries, i.e., the parameters for phrasestructures are estimated so as to maximize the prob-abilities for phrase structures only.
That is, the pa-rameters for lexical entries and the parameters forphrase structures are trained independently in theirmodel.Miyao and Tsujii (2005)?s model also uses a ref-erence distribution, but with word and POS unigramfeatures, as is explained in the previous section.
Theonly difference between our model and Miyao andTsujii (2005)?s model is that our model uses se-quences of word and POS tags as n-gram featuresfor selecting lexical entries in the same way as su-pertagging does.4 ExperimentsWe evaluated the speed and accuracy of parsingby using Enju 2.1, the HPSG grammar for English(Miyao et al, 2005; Miyao and Tsujii, 2005).
Thelexicon of the grammar was extracted from Sec-tions 02-21 of the Penn Treebank (Marcus et al,1994) (39,832 sentences).
The grammar consistedof 3,797 lexical entries for 10,536 words1.
The prob-1An HPSG treebank is automatically generated from thePenn Treebank.
Those lexical entries were generated by apply-ing lexical rules to observed lexical entries in the HPSG tree-bank (Nakanishi et al, 2004).
The lexicon, however, includedmany lexical entries that do not appear in the HPSG treebank.64No.
of tested sentences Total No.
of sentences Avg.
length of tested sentencesSection 23 2,299 (100.00%) 2,299 22.2Section 24 1,245 (99.84%) 1,247 23.0Table 3: Statistics of the Penn Treebank.Section 23 (Gold POSs)LP LR LF UP UR UF Avg.
time(%) (%) (%) (%) (%) (%) (ms)Miyao and Tsujii (2005)?s model 87.26 86.50 86.88 90.73 89.93 90.33 604Ninomiya et al (2006)?s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129Ninomiya et al (2006)?s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379Section 23 (POS tagger)LP LR LF UP UR UF Avg.
time(%) (%) (%) (%) (%) (%) (ms)Miyao and Tsujii (2005)?s model 84.96 84.25 84.60 89.55 88.80 89.17 674Ninomiya et al (2006)?s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154Ninomiya et al (2006)?s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183Matsuzaki et al (2007)?s model 86.93 86.47 86.70 - - - 30our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821Table 4: Experimental results for Section 23.abilistic models were trained using the same portionof the treebank.
We used beam thresholding, globalthresholding (Goodman, 1997), preserved iterativeparsing (Ninomiya et al, 2005) and quick check(Malouf et al, 2000).We measured the accuracy of the predicate-argument relations output of the parser.
Apredicate-argument relation is defined as a tuple?
?,wh, a, wa?, where ?
is the predicate type (e.g.,adjective, intransitive verb), wh is the head word ofthe predicate, a is the argument label (MODARG,ARG1, ..., ARG4), and wa is the head word ofthe argument.
Labeled precision (LP)/labeled re-call (LR) is the ratio of tuples correctly identifiedby the parser2.
Unlabeled precision (UP)/unlabeledrecall (UR) is the ratio of tuples without the pred-icate type and the argument label.
This evaluationscheme was the same as used in previous evaluationsof lexicalized grammars (Hockenmaier, 2003; ClarkThe HPSG treebank is used for training the probabilistic modelfor lexical entry selection, and hence, those lexical entries thatdo not appear in the treebank are rarely selected by the proba-bilistic model.
The ?effective?
tag set size, therefore, is around1,361, the number of lexical entries without those never-seenlexical entries.2When parsing fails, precision and recall are evaluated, al-though nothing is output by the parser; i.e., recall decreasesgreatly.and Curran, 2004b; Miyao and Tsujii, 2005).
Theexperiments were conducted on an AMD Opteronserver with a 2.4-GHz CPU.
Section 22 of the Tree-bank was used as the development set, and the per-formance was evaluated using sentences of ?
100words in Section 23.
The performance of eachmodel was analyzed using the sentences in Section24 of ?
100 words.
Table 3 details the numbersand average lengths of the tested sentences of ?
100words in Sections 23 and 24, and the total numbersof sentences in Sections 23 and 24.The parsing performance for Section 23 is shownin Table 4.
The upper half of the table shows the per-formance using the correct POSs in the Penn Tree-bank, and the lower half shows the performance us-ing the POSs given by a POS tagger (Tsuruoka andTsujii, 2005).
LF and UF in the figure are labeledF-score and unlabeled F-score.
F-score is the har-monic mean of precision and recall.
We evaluatedour model in two settings.
One is implemented witha narrow beam width (?our model 1?
in the figure),and the other is implemented with a wider beamwidth (?our model 2?
in the figure)3.
?our model3The beam thresholding parameters for ?our model 1?
are?0 = 10,??
= 5, ?last = 30, ?0 = 5.0,??
= 2.5, ?last =15.0, ?0 = 10,??
= 5, ?last = 30, ?0 = 5.0,??
=2.5, ?last = 15.0, ?0 = 6.0,??
= 3.5, and ?last = 20.0.6583.00%83.50%84.00%84.50%85.00%85.50%86.00%86.50%87.00%87.50%88.00%0 100 200 300 400 500 600 700 800 900Parsing time (ms/sentence)F-scoreMiyao and Tsujii(2005)'s modelNinomiya et al(2006)'s model 1Ninomiya et al(2006)'s model 3our modelFigure 3: F-score versus average parsing time for sentences in Section 24 of ?
100 words.1?
was introduced to measure the performance withbalanced F-score and speed, which we think appro-priate for practical use.
?our model 2?
was intro-duced to measure how high the precision and re-call could reach by sacrificing speed.
Our mod-els increased the parsing accuracy.
?our model 1?was around 2.6 times faster and had around 2.65points higher F-score than Miyao and Tsujii (2005)?smodel.
?our model 2?
was around 2.3 times slowerbut had around 2.9 points higher F-score than Miyaoand Tsujii (2005)?s model.
We must admit that thedifference between our models and Ninomiya et al(2006)?s model 3 was not as great as the differ-ence from Miyao and Tsujii (2005)?s model, but ?ourmodel 1?
achieved 0.56 points higher F-score, and?our model 2?
achieved 0.8 points higher F-score.When the automatic POS tagger was introduced, F-score dropped by around 2.4 points for all models.We also compared our model with Matsuzaki etal.
(2007)?s model.
Matsuzaki et al (2007) pro-The terms ?
and ?
are the thresholds of the number of phrasalsigns in the chart cell and the beam width for signs in the chartcell.
The terms ?
and ?
are the thresholds of the number andthe beam width of lexical entries, and ?
is the beam width forglobal thresholding (Goodman, 1997).
The terms with suffixes0 are the initial values.
The parser iterates parsing until it suc-ceeds to generate a parse tree.
The parameters increase for eachiteration by the terms prefixed by ?, and parsing finishes whenthe parameters reach the terms with suffixes last.
Details of theparameters are written in (Ninomiya et al, 2005).
The beamthresholding parameters for ?our model 2?
are ?0 = 18,??
=6, ?last = 42, ?0 = 9.0,??
= 3.0, ?last = 21.0, ?0 =18,??
= 6, ?last = 42, ?0 = 9.0,??
= 3.0, ?last = 21.0.In ?our model 2?, the global thresholding was not used.posed a technique for efficient HPSG parsing withsupertagging and CFG filtering.
Their results withthe same grammar and servers are also listed in thelower half of Table 4.
They achieved drastic im-provement in efficiency.
Their parser ran around 6times faster than Ninomiya et al (2006)?s model 3,9 times faster than ?our model 1?
and 60 times fasterthan ?our model 2.?
Instead, our models achievedbetter accuracy.
?our model 1?
had around 0.5 higherF-score, and ?our model 2?
had around 0.8 pointshigher F-score.
Their efficiency is mainly due toelimination of ungrammatical lexical entries by theCFG filtering.
They first parse a sentence with aCFG grammar compiled from an HPSG grammar,and then eliminate lexical entries that are not in theparsed CFG trees.
Obviously, this technique canalso be applied to the HPSG parsing of our mod-els.
We think that efficiency of HPSG parsing withour models will be drastically improved by applyingthis technique.The average parsing time and labeled F-scorecurves of each probabilistic model for the sentencesin Section 24 of ?
100 words are graphed in Fig-ure 3.
The graph clearly shows the difference ofour model and other models.
As seen in the graph,our model achieved higher F-score than other modelwhen beam threshold was widen.
This implies thatother models were probably difficult to reach the F-score of ?our model 1?
and ?our model 2?
for Section23 even if we changed the beam thresholding param-eters.
However, F-score of our model dropped eas-66ily when we narrow down the beam threshold, com-pared to other models.
We think that this is mainlydue to its bad implementation of parser interface.The n-gram reference distribution is incorporatedinto the kernel of the parser, but the n-gram fea-tures and a maximum entropy estimator are definedin other modules; n-gram features are defined in agrammar module, and a maximum entropy estimatorfor the n-gram reference distribution is implementedwith a general-purpose maximum entropy estimatormodule.
Consequently, strings that represent the n-gram information are very frequently changed intofeature structures and vice versa when they go in andout of the kernel of the parser.
On the other hand, Ni-nomiya et al (2006)?s model 3 uses the supertaggeras an external module.
Once the parser acquires thesupertagger?s outputs, the n-gram information nevergoes in and out of the kernel.
This advantage of Ni-nomiya et al (2006)?s model can apparently be im-plemented in our model, but this requires many partsof rewriting of the implemented parser.
We estimatethat the overhead of the interface is around from 50to 80 ms/sentence.
We think that re-implementationof the parser will improve the parsing speed as esti-mated.
In Figure 3, the line of our model crosses theline of Ninomiya et al (2006)?s model.
If the esti-mation is correct, our model will be faster and moreaccurate so that the lines in the figure do not cross.Speed-up in our model is left as a future work.5 ConclusionWe proposed a probabilistic model in which su-pertagging is consistently integrated into the prob-abilistic model for HPSG.
In the model, the n-gramreference distribution is simply defined as the prod-uct of the probabilities of selecting lexical entrieswith machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertag-ging.
We conducted experiments on the Penn Tree-bank with a wide-coverage HPSG parser.
In the ex-periments, we compared our model with the prob-abilistic HPSG with a unigram reference distribu-tion (Miyao and Tsujii, 2005) and the probabilisticHPSG with supertagging (Ninomiya et al, 2006).Though our model was not as fast as Ninomiyaet al (2006)?s models, it achieved the highest ac-curacy among them.
Our model had around 2.65points higher F-score than Miyao and Tsujii (2005)?smodel and around 0.56 points higher F-score thanthe Ninomiya et al (2006)?s model 3.
When we sac-rifice parsing speed, our model achieved around 2.9points higher F-score than Miyao and Tsujii (2005)?smodel and around 0.8 points higher F-score than Ni-nomiya et al (2006)?s model 3.
Our model achievedhigher F-score because parameters for phrase struc-tures in our model are trained with the supertaggingprobabilities, which are not in other models.ReferencesSteven P. Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4):597?618.Srinivas Bangalore and Aravind Joshi.
1999.
Supertag-ging: An approach to almost parsing.
ComputationalLinguistics, 25(2):237?265.Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.Joan Bresnan.
1982.
The Mental Representation ofGrammatical Relations.
MIT Press, Cambridge, MA.Stephen Clark and James R. Curran.
2004a.
The impor-tance of supertagging for wide-coverage CCG parsing.In Proc.
of COLING-04.Stephen Clark and James R. Curran.
2004b.
Parsing theWSJ using CCG and log-linear models.
In Proc.
ofACL?04, pages 104?111.Killian Foth and Wolfgang Menzel.
2006.
Hybrid pars-ing: Using probabilistic models as predictors for asymbolic parser.
In Proc.
of COLING-ACL 2006.Killian Foth, Tomas By, and Wolfgang Menzel.
2006.Guiding a constraint dependency parser with su-pertags.
In Proc.
of COLING-ACL 2006.Stuart Geman and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of stochas-tic unification-based grammars.
In Proc.
of ACL?02,pages 279?286.Joshua Goodman.
1997.
Global thresholding and mul-tiple pass parsing.
In Proc.
of EMNLP-1997, pages11?25.Julia Hockenmaier.
2003.
Parsing with generativemodels of predicate-argument structure.
In Proc.
ofACL?03, pages 359?366.F.
Jelinek.
1998.
Statistical Methods for Speech Recog-nition.
The MIT Press.67Mark Johnson and Stefan Riezler.
2000.
Exploitingauxiliary distributions in stochastic unification-basedgrammars.
In Proc.
of NAACL-2000, pages 154?161.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proc.
of ACL ?99,pages 535?541.R.
M. Kaplan, S. Riezler, T. H. King, J. T. MaxwellIII, and A. Vasserman.
2004.
Speed and accuracyin shallow and deep stochastic parsing.
In Proc.
ofHLT/NAACL?04.Robert Malouf and Gertjan van Noord.
2004.
Widecoverage parsing with stochastic attribute value gram-mars.
In Proc.
of IJCNLP-04 Workshop ?BeyondShallow Analyses?.Robert Malouf, John Carroll, and Ann Copestake.
2000.Efficient feature structure operations without compi-lation.
Journal of Natural Language Engineering,6(1):29?46.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proc.
ofCoNLL-2002, pages 49?55.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Efficient HPSG parsing with supertagging andCFG-filtering.
In Proc.
of IJCAI 2007, pages 1671?1676.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proc.
of HLT2002, pages 292?297.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSG pars-ing.
In Proc.
of ACL?05, pages 83?90.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-jii, 2005.
Keh-Yih Su, Jun?ichi Tsujii, Jong-HyeokLee and Oi Yee Kwong (Eds.
), Natural LanguageProcessing - IJCNLP 2004 LNAI 3248, chapterCorpus-oriented Grammar Development for Acquir-ing a Head-driven Phrase Structure Grammar from thePenn Treebank, pages 684?693.
Springer-Verlag.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2004.
An empirical investigation of the effect of lexi-cal rules on parsing with a treebank grammar.
In Proc.of TLT?04, pages 103?114.Alexis Nasr and Owen Rambow.
2004.
Supertaggingand full parsing.
In Proc.
of the 7th InternationalWorkshop on Tree Adjoining Grammar and RelatedFormalisms (TAG+7).Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,and Jun?ichi Tsujii.
2005.
Efficacy of beam threshold-ing, unification filtering and hybrid parsing in proba-bilistic HPSG parsing.
In Proc.
of IWPT 2005, pages103?114.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.
Ex-tremely lexicalized models for accurate and fast HPSGparsing.
In Proc.
of EMNLP 2006, pages 155?163.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Stefan Riezler, Detlef Prescher, Jonas Kuhn, and MarkJohnson.
2000.
Lexicalized stochastic modeling ofconstraint-based grammars using log-linear measuresand EM training.
In Proc.
of ACL?00, pages 480?487.Mark Steedman.
2000.
The Syntactic Process.
The MITPress.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005.
Bidirec-tional inference with the easiest-first strategy for tag-ging sequence data.
In Proc.
of HLT/EMNLP 2005,pages 467?474.Wen Wang and Mary P. Harper.
2004.
A statistical con-straint dependency grammar (CDG) parser.
In Proc.of ACL?04 Incremental Parsing workshop: BringingEngineering and Cognition Together, pages 42?49.Wen Wang.
2003.
Statistical Parsing and LanguageModeling based on Constraint Dependency Grammar.Ph.D.
thesis, Purdue University.68
