Proceedings of the ACL 2010 Student Research Workshop, pages 73?78,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsAutomatic Selectional Preference Acquisition for Latin verbsBarbara McGillivrayUniversity of PisaItalyb.mcgillivray@ling.unipi.itAbstractWe present a system that automaticallyinduces Selectional Preferences (SPs) forLatin verbs from two treebanks by usingLatin WordNet.
Our method overcomessome of the problems connected with datasparseness and the small size of the inputcorpora.
We also suggest a way to evalu-ate the acquired SPs on unseen events ex-tracted from other Latin corpora.1 IntroductionAutomatic acquisition of semantic informationfrom corpora is a challenge for research on low-resourced languages, especially when semanti-cally annotated corpora are not available.
Latin isdefinitely a high-resourced language for what con-cerns the number of available texts and traditionallexical resources such as dictionaries.
Neverthe-less, it is a low-resourced language from a compu-tational point of view (McGillivray et al, 2009).As far as NLP tools for Latin are concerned,parsing experiments with machine learning tech-niques are ongoing (Bamman and Crane, 2008;Passarotti and Ruffolo, forthcoming), althoughmore work is still needed in this direction, espe-cially given the small size of the training data.As a matter of fact, only three syntactically an-notated Latin corpora are available (and still inprogress): the Latin Dependency Treebank (LDT,53,000 tokens) for classical Latin (Bamman andCrane, 2006), the Index Thomisticus Treebank (IT-TB, 54,000 tokens) for Thomas Aquinas?s works(Passarotti, 2007), and the PROIEL treebank (ap-proximately 100,000 tokens) for the Bible (Haugand J?ndal, 2008).
In addition, a Latin versionof WordNet ?
Latin WordNet (LWN; Minozzi,(2009) ?
is being compiled, consisting of around10,000 lemmas inserted in the multilingual struc-ture of MultiWordNet (Bentivogli et al, 2004).The number and the size of these resources aresmall when compared with the corpora and thelexicons for modern languages, e. g. English.Concerning semantic processing, no seman-tically annotated Latin corpus is available yet;building such a corpus manually would take con-siderable time and energy.
Hence, research incomputational semantics for Latin would benefitfrom exploiting the existing resources and toolsthrough automatic lexical acquisition methods.In this paper we deal with automatic acquisitionof verbal selectional preferences (SPs) for Latin,i.
e. the semantic preferences of verbs on their ar-guments: e. g. we expect the object position of theverb edo ?eat?
to be mostly filled by nouns from thefood domain.
For this task, we propose a methodinspired by Alishahi (2008) and outlined in an ear-lier version on the IT-TB in McGillivray (2009).SPs are defined as probability distributions oversemantic features extracted as sets of LWN nodes.The input data are two subcategorization lexiconsautomatically extracted from the LDT and the IT-TB (McGillivray and Passarotti, 2009).Our main contribution is to create a new tool forsemantic processing of Latin by adapting compu-tational techniques developed for extant languagesto the special case of Latin.
A successful adapta-tion is contingent on overcoming corpus size dif-ferences.
The way our model combines the syntac-tic information contained in the treebanks with thelexical semantic knowledge from LWN allows usto overcome some of the difficulties related to thesmall size of the input corpora.
This is the maindifference from corpora for modern languages, to-gether with the absence of semantic annotation.Moreover, we face the problem of evaluating oursystem?s ability to generalize over unseen cases byusing text occurrences, as access to human linguis-tic judgements is denied for Latin.In the rest of the paper we will briefly summa-rize previous work on SP acquisition and motivate73our approach (section 2); we will then describe oursystem (section 3), report on first results and evalu-ation (section 4), and finally conclude by suggest-ing future directions of research (section 5).2 Background and motivationThe state-of-the-art systems for automatic acqui-sition of verbal SPs collect argument headwordsfrom a corpus (for example, apple, meat, salad asobjects of eat) and then generalize the observedbehaviour over unseen cases, either in the form ofwords (how likely is it to find sausage in the objectposition of eat?)
or word classes (how likely is itto find VEGETABLE, FOOD, etc?
).WN-based approaches translate the generaliza-tion problem into estimating preference probabil-ities over a noun hierarchy and solve it by meansof different statistical tools that use the input dataas a training set: cf.
inter al.
Resnik (1993), Liand Abe (1998), Clark and Weir (1999).
Agirreand Martinez (2001) acquire SPs for verb classesinstead of single verb lemmas by using a semanti-cally annotated corpus and WN.Distributional methods aim at automatically in-ducing semantic classes from distributional data incorpora by means of various similarity measuresand unsupervised clustering algorithms: cf.
e. g.Rooth et al (1999) and Erk (2007).
Bamman andCrane (2008) is the only distributional approachdealing with Latin.
They use an automaticallyparsed corpus of 3.5 million words, then calculateSPs with the log-likelihood test, and obtain an as-sociation score for each (verb, noun) pair.The main difference between these previoussystems and our case is the size of the input cor-pus.
In fact, our dataset consists of subcatego-rization frames extracted from two relatively smalltreebanks, amounting to a little over 100,000 wordtokens overall.
This results in a large number oflow-frequency (verb, noun) associations, whichmay not reflect the actual distributions of Latinverbs.
This state improves if we group the obser-vations into clusters.
Such a method, proposed byAlishahi (2008), proved effective in our case.The originality of this approach is an incre-mental clustering algorithm for verb occurrencescalled frames which are identified by specific syn-tactic and semantic features, such as the numberof verbal arguments, the syntactic pattern, and thesemantic properties of each argument, i. e. theWN hypernyms of the argument?s fillers.
Basedon a probabilistic measure of similarity betweenthe frames?
features, the clustering produces largersets called constructions.
The constructions for averb contribute to the next step, which acquiresthe verb?s SPs as semantic profiles, i. e. probabil-ity distributions over the semantic properties.
Themodel exploits the structure of WN so that predic-tions over unseen cases are possible.3 The modelThe input data are two corpus-driven subcate-gorization lexicons which record the subcatego-rization frames of each verbal token occurringin the corpora: these frames contain morpho-syntactic information on the verb?s arguments, aswell as their lexical fillers.
For example, ?eo+ A (in)Obj[acc]{exsilium}?
represents an activeoccurrence of the verb eo ?go?
with a prepositionalphrase introduced by the preposition in ?to, into?and composed by an accusative noun phrase filledby the lemma exsilium ?exile?, as in the sentence1(1) eatgo:SBJV.PRS.3SGintoexsiliumexile:ACC.N.SG?he goes into exile?.We illustrate how we adapted Alishahi?s defini-tions of frame features and formulae to our case.Alishahi uses a semantically annotated Englishcorpus, so she defines the verb?s semantic prim-itives, the arguments?
participant roles and theirsemantic categories; since we do not have such an-notation, we used the WN semantic information.The syntactic feature of a frame (ft1) is theset of syntactic slots of its verb?s subcategoriza-tion pattern, extracted from the lexicons.
In theabove example, ?A (in)Obj[acc]?.
In addition, thefirst type of semantic features of a frame (ft2)collects the semantic properties of the verb?s ar-guments as the set of LWN synonyms and hy-pernyms of their fillers.
In the previous exam-ple this is {exsilium ?exile?, proscriptio ?proscrip-tion?, rejection, actio, actus ?act?
}.2The secondtype of semantic features of a frame (ft3) col-lects the semantic properties of the verb in theform of the verb?s synsets.
In the above example,these are all synsets of eo ?go?, among which ?
{eo,gradior, grassor, ingredior, procedo, prodeo,1Cicero, In Catilinam, II, 7.2We listed the LWN node of the lemma exsilium, followedby its hypernyms; each node ?
apart from rejection, whichis English and is not filled by a Latin lemma in LWN ?
istranslated by the corresponding node in the English WN.74vado}?
(?
{progress, come on, come along, ad-vance, get on, get alng, shape up}?
in the En-glish WN).3.1 Clustering of framesThe constructions are incrementally built as newframes are included in them; a new frame F is as-signed to a construction K if F probabilisticallyshares some features with the frames in K so thatK = argmaxkP (k|F ) = argmaxkP (k)P (F |k),where k ranges over the set of all constructions,including the baseline k0= {F}.
The priorprobability P (k) is calculated from the number offrames contained in k divided by the total numberof frames.
Assuming that the frame features areindependent, the posterior probability P (F |k) isthe product of three probabilities, each one corre-sponding to the probability that a feature displaysin k the same value it displays in F : Pi(fti(F )|k)for i = 1, 2, 3:P (F |k) =?i=1,2,3Pi(fti(F )|k)We estimated the probability of a match be-tween the value of ft1in k and the value of ft1in F as the sum of the syntactic scores betweenF and each frame h contained in k, divided thenumber nkof frames in k:P (ft1(F )|k) =?h?ksynt score(h, F )nkwhere the syntactic score synt score(h, F ) =|SCS(h)?SCS(F )||SCS(F )|calculates the number of syntac-tic slots shared by h and F over the number ofslots in F .
P (ft1(F )|k) is 1 when all the framesin k contain all the syntactic slots of F .For each argument position a, we estimated theprobability P (ft2(F )|k) as the sum of the seman-tic scores between F and each h in k:P (ft2(F )|k) =?h?ksem score(h, F )nkwhere the semantic score sem score(h, F ) =|S(h)?S(F )||S(F )|counts the overlap between the seman-tic properties S(h) of h (i. e. the LWN hyper-nyms of the fillers in h) and the semantic prop-erties S(F ) of F (for argument a), over |S(F )|.P (ft3(F )|k) =?h?ksyns score(h, F )nkwhere the synset score syns score(h, F ) =|Synsets(verb(h))?Synsets(verb(F ))||Synsets(verb(F ))|calculates theoverlap between the synsets for the verb in h andthe synsets for the verb in F over the number ofsynsets for the verb in F .3We introduced the syntactic and synset scores inorder to account for a frequent phenomenon in ourdata: the partial matches between the values of thefeatures in F and in k.3.2 Selectional preferencesThe clustering algorithm defines the set of con-structions in which the generalization step overunseen cases is performed.
SPs are defined assemantic profiles, that is, probability distributionsover the semantic properties, i. e. LWN nodes.
Forexample, we get the probability of the node actio?act?
in the position ?A (in)Obj[acc]?
for eo ?go?.If s is a semantic property and a an argumentposition for a verb v, the semantic profile Pa(s|v)is the sum of Pa(s, k|v) over all constructions kcontaining v or a WN-synonym of v, i. e. a verbcontained in one or more synsets for v. Pa(s, k|v)is approximated asP (k,v)Pa(s|k,v)P (v), where P (k, v)is estimated asnk?freq(k,v)?k?nk?
?freq(k?,v)To estimate Pa(s|k, v) we consider each frameh in k and account for: a) the similarity between vand the verb in h; b) the similarity between s andthe fillers of h. This is achieved by calculating asimilarity score between h, v, a and s, defined as:syns score(v, V (h)) ?
?f|s ?
S(f)|Nfil(h, a)(1)where V (h) in (1) contains the verbs of h,Nfil(h, a) counts the a-fillers in h, f ranges in theset of a-fillers in h, S(f) contains the semanticproperties for f and |s?S(f)| is 1 when s appearsin S(f) and 0 otherwise.Pa(s|k, v) is thus obtained by normalizing thesum of these similarity scores over all frames ink, divided by the total number of frames in k con-taining v or its synonyms.The similarity scores weight the contributionsof the synonyms of v, whose fillers play a role inthe generalization step.
This is our innovation withrespect to Alishahi (2008)?s system.
It was intro-duced because of the sparseness of our data, where3The algorithm uses smoothed versions of all the previousformulae by adding a very small constant so that the proba-bilities are never 0.75k h1induco + P Sb[acc]{forma}introduco + P Sb{PR}introduco + P Sb{forma}addo +P Sb{praesidium}2induco + A Obj[acc]{forma}immitto + A Obj[acc]{PR},Obj[dat]{antrum}introduco + A Obj[acc]{NP}3introduco + A (in)Obj[acc]{finis},Obj[acc]{copia},Sb{NP}induco + A (in)Obj[acc]{effectus},Obj[acc]{forma}4introduco + A Obj[acc]{forma}induco + A Obj[acc]{perfectio},Sb[nom]{PR}5induco + A Obj[acc]{forma}nimmitto + A Obj[acc]{PR},Obj[dat]{antrum}introduco + A Obj[acc]{NP}Table 1: Constructions (k) for the frames (h) con-taining the verb introduco ?bring in?.many verbs are hapaxes, which makes the gener-alization from their fillers difficult.4 Results and evaluationThe clustering algorithm was run on 15509 framesand it generated 7105 constructions.
Table 1 dis-plays the 5 constructions assigned to the 9 frameswhere the verb introduco ?bring in, introduce?
oc-curs.
Note the semantic similarity between addo?add to, bring to?, immitto ?send against, insert?,induco ?bring forward, introduce?
and introduco,and the similarity between the syntactic patternsand the argument fillers within the same construc-tion.
For example, finis ?end, borders?
and ef-fectus ?result?
share the semantic properties AT-TRIBUTE, COGNITIO ?cognition?, CONSCIENTIA?conscience?, EVENTUM ?event?, among others.The vast majority of constructions contain lessthan 4 frames.
This contrasts with the more gen-eral constructions found by Alishahi (2008) andcan be explained by several factors.
First, the cov-erage of LWN is quite low with respect to thefillers in our dataset.
In fact, 782 fillers out of2408 could not be assigned to any LWN synset;for these lemmas the semantic scores with all theother nouns are 0, causing probabilities lower thanthe baseline; this results in assigning the frame tothe singleton construction consisting of the frameitself.
The same happens for fillers consisting ofverbal lemmas, participles, pronouns and namedentities, which amount to a third of the total num-ber.
Furthermore, the data are not tagged by senseand the system deals with noun ambiguity by list-ing together all synsets of a word n (and their hy-pernyms) to form the semantic properties for n:consequently, each sense contributes to the seman-tic description of n in relation to the number ofhypernyms it carries, rather than to its observedsemantic property probabilityactio ?act?
0.0089actus ?act?
0.0089pars ?part?
0.0089object 0.0088physical object 0.0088instrumentality 0.0088instrumentation 0.0088location 0.0088populus ?people?
0.0088plaga ?region?
0.0088regio ?region?
0.0088arvum ?area?
0.0088orbis ?area?
0.0088external body part ?
0.0088nympha ?nymph?, ?water?
0.0088latex ?water?
0.0088lympha ?water?
0.0088intercapedo ?gap, break?
0.0088orificium ?opening?
0.0088Table 2: Top 20 semantic properties in the seman-tic profile for ascendo ?ascend?
+ A (de)Obj[abl].frequency.
Finally, a common problem in SP ac-quisition systems is the noise in the data, includingtagging and metaphorical usages.
This problemis even greater in our case, where the small sizeof the data underestimates the variance and there-fore overestimates the contribution of noisy obser-vations.
Metaphorical and abstract usages are es-pecially frequent in the data from the IT-TB, dueto the philosophical domain of the texts.As to the SP acquisition, we ran the systemon all constructions generated by the clustering.We excluded the pronouns occurring as argumentfillers, and manually tagged the named entities.For each verb lemma and slot we obtained a proba-bility distribution over the 6608 LWN noun nodes.Table 2 displays the 20 semantic propertieswith the highest SP probabilities as ablative argu-ments of ascendo ?ascend?
introduced by de ?downfrom?, ?out of?.
This semantic profile was cre-ated from the following fillers for the verbs con-tained in the constructions for ascendo and itssynonyms: abyssus ?abyss?, fumus ?smoke?, lacus?lake?, machina ?machine?, manus ?hand?, negoti-atio ?business?, mare ?sea?, os ?mouth?, templum?temple?, terra ?land?.
These nouns are well repre-sented by the semantic properties related to waterand physical places.
Note also the high rank ofgeneral properties like actio ?act?, which are asso-ciated to a large number of fillers and thus gener-ally get a high probability.Regarding evaluation, we are interested in test-ing two properties of our model: calibrationand discrimination.
Calibration is related to themodel?s ability to distinguish between high andlow probabilities.
We verify that our model is76adequately calibrated, since its SP distribution isalways very skewed (cf.
figure 1).
Therefore,the model is able to assign a high probability toa small set of nouns (preferred nouns) and a lowprobability to a large set of nouns (the rest), thusperforming better than the baseline model, definedas the model that assigns the uniform distributionover all nouns (4724 LWN leaf nodes).
Moreover,our model?s entropy is always lower than the base-line: 12.2 vs. the 6.9-11.3 range; by the maximumentropy principle, this confirms that the systemuses some information for estimating the proba-bilities: LWN structure, co-occurrence frequency,syntactic patterns.
However, we have no guaran-tee that the model uses this information sensibly.For this, we test the system?s discrimination po-tential, i. e. its ability to correctly estimate the SPprobability of each single LWN node.noun SP probabilitypars ?part?
0.0029locus ?place?
0.0026forma ?form?
0.0023ratio ?account?
?reason?, ?opinion?
0.0023respectus ?consideration?
0.0022caput ?head?, ?origin?
0.0022anima ?soul?
0.0021animus ?soul?, ?spirit?
0.0020figura ?form?, ?figure?
0.0020spiritus ?spirit?
0.0020causa cause?
?
0.0020corpus ?body?
0.0019sententia ?judgement?
0.0019finitio ?limit?, ?definition?
0.0019species ?sight?, ?appearance?
0.0019Table 3: 15 nouns with the highest probabilities asaccusative objects of dico ?say?.Figure 1: Decreasing SP probabilities of the LWNleaf nodes for the objects of dico ?say?.Table 3 displays the 15 nouns with the highestprobabilities as direct objects for dico ?say?.
Fromtable 3 ?
and the rest of the distribution, repre-sented in figure 1 ?
we see that the model assignsa high probability to most seen fillers for dico inthe corpus: anima ?soul?, corpus ?body?, locus?place?, pars ?part?, etc.For what concerns evaluating the SP probabil-ity assigned to nouns unseen in the training set,Alishahi (2008) follows the approach suggestedby Resnik (1993), using human plausibility judge-ments on verb-noun pairs.
Given the absence ofnative speakers of Latin, we used random occur-rences in corpora, considered as positive examplesof plausible argument fillers; on the other hand, wecannot extract non-plausible fillers from a corpusunless we use a frequency-based criterion.
How-ever, we can measure how well our system predictsthe probability of these unseen events.As a preliminary evaluation experiment, werandomly selected from our corpora a list of 19high-frequency verbs (freq.>51) and 7 medium-frequency verbs (11<freq.<50), for each of whichwe chose an interesting argument slot.
Then werandomly extracted one filler for each such pairfrom two collections of Latin texts (Perseus Dig-ital Library and Corpus Thomisticum), providedthat it was not in the training set.
The semanticscore in equation 1 on page 3 is then calculatedbetween the set of semantic properties of n andthat for f , to obtain the probability of finding therandom filler n as an argument for a verb v.For each of the 26 (verb, slot) pairs, we lookedat three measures of central tendency: mean, me-dian and the value of the third quantile, whichwere compared with the probability assigned bythe model to the random filler.
If this probabil-ity was higher than the measure, the outcome wasconsidered a success.
The successes were 22 forthe mean, 25 for the median and 19 for the thirdquartile.4For all three measures a binomial testfound the success rate to be statistically significantat the 5% level.
For example, table 3 and figure1 show that the filler for dico+A Obj[acc] in theevaluation set ?
sententia ?judgement?
?
is ranked13th within the verb?s semantic profile.5 Conclusion and future workWe proposed a method for automatically acquiringprobabilistic SP for Latin verbs from a small cor-pus using the WN hierarchy; we suggested some4The dataset consists of all LWN leaf nodes n, for whichwe calculated Pa(n|v).
By definition, if we divide the datasetin four equal-sized parts (quartiles), 25% of the leaf nodeshave a probability higher than the value at the third quartile.Therefore, in 19 cases out of 26 the random fillers are placedin the high-probability quarter of the plot, which is a goodresult, since this is where the preferred arguments gather.77new strategies for tackling the data sparseness inthe crucial generalization step over unseen cases.Our work also contributes to the state of the art insemantic processing of Latin by integrating syn-tactic information from annotated corpora with thelexical resource LWN.
This demonstrates the use-fulness of the method for small corpora and therelevance of computational approaches for histor-ical linguistics.In order to measure the impact of the frameclusters for the SP acquisition, we plan to run thesystem for SP acquisition without performing theclustering step, thus defining all constructions assingleton sets containing one frame each.
Finally,an extensive evaluation will require a more com-prehensive set, composed of a higher number ofunseen argument fillers; from the frequencies ofthese nouns, it will be possible to directly compareplausible arguments (high frequency) and implau-sible ones (low frequency).
For this, a larger auto-matically parsed corpus will be necessary.6 AcknowledgementsWe wish to thank Afra Alishahi, Stefano Minozziand three anonymous reviewers.ReferencesE.
Agirre and D. Martinez.
2001.
Learning class-to-class selectional preferences.
In Proceedings of theACL/EACL 2001 Workshop on Computational Nat-ural Language Learning (CoNLL-2001), pages 1?8.A.
Alishahi.
2008.
A probabilistic model of early ar-gument structure acquisition.
Ph.D. thesis, Depart-ment of Computer Science, University of Toronto.D.
Bamman and G. Crane.
2006.
The design and useof a Latin dependency treebank.
In Proceedings ofthe Fifth International Workshop on Treebanks andLinguistic Theories, pages 67?78.
?UFAL MFF UK.D.
Bamman and G. Crane.
2008.
Building a dynamiclexicon from a digital library.
In Proceedings of the8th ACM/IEEE-CS Joint Conference on Digital Li-braries, pages 11?20.L.
Bentivogli, P. Forner, and and Pianta E. Magnini,B.
2004.
Revising wordnet domains hierarchy: Se-mantics, coverage, and balancing.
In Proceedings ofCOLING Workshop on Multilingual Linguistic Re-sources, pages 101?108.S.
Clark and D. Weir.
1999.
An iterative approachto estimating frequencies over a semantic hierarchy.In Proceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora.
University of Maryland,pages 258?265.K.
Erk.
2007.
A simple, similarity-based model forselectional preferences.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics, pages 216?223.D.
T. T. Haug and M. L. J?ndal.
2008.
Creating a par-allel treebank of the old Indo-European Bible trans-lations.
In Proceedings of Language Technologiesfor Cultural Heritage Workshop, pages 27?34.H.
Li and N. Abe.
1998.
Generalizing case framesusing a thesaurus and the MDL principle.
Computa-tional Linguistics, 24(2):217?244.B.
McGillivray and M. Passarotti.
2009.
The devel-opment of the Index Thomisticus Treebank ValencyLexicon.
In Proceedings of the Workshop on Lan-guage Technology and Resources for Cultural Her-itage, Social Sciences, Humanities, and Education,pages 33?40.B.
McGillivray, M. Passarotti, and P. Ruffolo.
2009.The Index Thomisticus treebank project: Annota-tion, parsing and valency lexicon.
TAL, 50(2):103?127.B.
McGillivray.
2009.
Selectional Preferences froma Latin treebank.
In Przepi?orkowski A.
Passarotti,M., S. Raynaud, and F. van Eynde, editors, Proceed-ings of the Eigth International Workshop on Tree-banks and Linguistic Theories (TLT8), pages 131?136.
EDUCatt.S.
Minozzi.
2009.
The Latin Wordnet project.In P. Anreiter and M. Kienpointner, editors, Pro-ceedings of the 15th International Colloquium onLatin Linguistics (ICLL), Innsbrucker Beitraege zurSprachwissenschaft.M.
Passarotti and P. Ruffolo.
forthcoming.
Parsingthe Index Thomisticus Treebank.
some preliminaryresults.
In P. Anreiter and M. Kienpointner, edi-tors, Proceedings of the 15th International Collo-quium on Latin Linguistics, Innsbrucker Beitr?agezur Sprachwissenschaft.M.
Passarotti.
2007.
Verso il Lessico Tomistico Bi-culturale.
La treebank dell?Index Thomisticus.
InR.
Petrilli and D. Femia, editors, Atti del XIII Con-gresso Nazionale della Societ`a di Filosofia del Lin-guaggio, pages 187?205.P.
Resnik.
1993.
Selection and Information: A Class-Based Approach to Lexical Relationships.
Ph.D.thesis, University of Pennsylvania.M.
Rooth, S. Riezler, D. Prescher, G. Carroll, andF.
Beil.
1999.
Inducing a semantically annotatedlexicon via EM-based clustering.
In Proceedings ofthe 37th Annual Meeting of the Association for Com-putational Linguistics, pages 104?111.78
