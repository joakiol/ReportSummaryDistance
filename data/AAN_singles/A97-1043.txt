An Automat ic  Ext rac t ion  o f  Key  ParagraphsBased  on Context  DependencyFumiyo Fukumoto Yoshimi Suzukit Jun'ichi Fukumoto:~Dept.
of Electrical Engineering and Kansai Lab., R & D GroupComputer  Science,  Yamanash i  Un ivers i ty  Oki  E lect r i c  Indust ry  Co. Ltd.,:~4-3-11 Takeda,  Ko fu  400 Japan  1-2-27 Sh i romi ,  Chuo-ku ,  Osaka  540 Japan~fukumoto~skye, ysuzuki~suwa~ }.esi.yamanashi.ac.jp fukumoto~kansai.oki.co.jpAbstractIn this paper, we propose a method for ex-tracting key paragraphs in articles basedon the degree of context dependency.
LikeLuhn's technique, our method assumesthat the words related to theme in an arti-cle appear throughout paragraphs.
Our ex-traction technique of keywords is based onthe degree of context dependency that howstrongly a word is related to a given con-text.
The results of experiments demon-strate the applicability of our proposedmethod.1 IntroductionWith increasing numbers of machine readable doc-uments becoming available, automatic documentsummarisation has become one of the major researchtopics in IR and NLP studies.In the field of an automatic summarisation, thereare at least two approaches.
One is knowledge-basedapproach with particular subject fields (Reimer,1988), (Jacobs, 1990).
This approach, based on deepknowledge of particular subject fields, is useful forrestricted tasks, such as, for example, the construc-tion of 'weather forecasts' summaries.
However,when unrestricted subject matter must be treated,as is often the case in practice, the passage retrievaland text summarisation methods proposed have notproven equal to the need, since deep knowledge ofparticular subject fields is required (Paice, 1990),(Zechner, 1996).The other, alternative strategy is the approachthat relies mainly on corpus statistics (Paice, 1990),(Palce, 1993).
The main task of this approach isthe sentence scoring process.
Typically, weightsare assigned to the individual words in a text, andthe complete sentence scores are then based on theoccurrence characteristics of highly-weighted terms(keywords) in the respective sentences.Term weighting technique has been widely inves-tigated in information retrieval and lots of tech-niques such as location heuristics (Baxendale, 1958),rhetorical relations (Miike, 1994), and title informa-tion (Edmundson, 1969) have been proposed.
Thesetechniques seem to be less dependent on the domain.However, Salton claims that it is difficult to pro-duce high accuracy of retrieval by using these term-weighting approaches (Salton, 1993).The other term weighting technique is based onkeyword frequency (Luhn, 1958).
Keyword fre-quency is further less dependent on the domain thanother weighting methods and therefore, well studied.Major approaches which are based on keyword fre-quency assume on the fact that the keywords of thearticle appear frequently in the article, but appearseldom in other articles (Luhn, 1958), (Nagao, 1976),(Salton, 1993), (Zechner, 1996).
These approachesseem to show the effect in entirely different articles,such as 'weather forecasts', 'medical reports', and'computer manuals'.
Because each different articleis characterised by a larger number of words whichappear frequently in one article, but appear seldomin other articles.
However, in some articles fromthe same domain such as 'weather forecasts', oneencounters quite a number of words which appearfrequently over articles.
Therefore, how to extractkeyword from these words is a serious problem insuch the restricted subject domain.In this paper, we propose a method for extract-ing key paragraphs in articles based on the degree ofcontext dependency and show how the idea of con-text dependency can be used effectively to extractkey paragraphs than other related work.The basic idea of our approach is that whether aword is a key in an article or not depends on the do-main to which the article belongs.
Let 'stake' be akeyword and 'today' not be a keyword in the article.If the article belongs to a restricted subject domain,such as 'Stock market', there are other articles whichare related to the article.
Therefore, the frequency of'stake' and 'today' in other articles are similar witheach other.
Let us consider further a broad coveragedomain such as newspaper articles; i.e.
the articlecontaining the words 'stake' and 'today' belongs toa newspaper which consists of different subject do-mains such as 'Stock market' news, 'International'291news, 'Weather forecasts' news.
'Today' should ap-pear frequently with every article even in such a do-main; i.e.
newspaper articles, while 'stake' shouldnot.
Our technique for extraction of keywords ex-plicitly exploits this feature of context dependencyof word: how strongly a word is related to a givencontext.In the following sections, we first explain con-text dependency using newspaper articles, then wepresent our term weighting method and a methodfor extracting key paragraphs.
Finally, we reportsome experiments o show the effect of our method.2 Context  DependencyLike Luhn's assumption about keywords, ourmethod is based on the fact that a writer normallyrepeats certain words (keywords) as he advances orvaries his arguments and as he elaborates on an as-pect of a subject (Luhn, 1958).
In this paper, wefocus on newspaper articles.
Figure 1 shows thestructure of Wall Street Journal corpus.ArticlerDomain ?
?
?Ee~n~?
In temat~na l$m rParagraph1 Par~raph2 Paragraph31o: K~meFigure 1: The structure of newspaper articlesIn Figure 1, one day's newspaper articles consist ofseveral different topics such as 'Economic news', 'In-ternational news', etc.
We call this Domain, and eachelement ('Economic news', or 'International news')a context.
A particular domain, for example, 'Eco-nomic news', consists of several articles each of whichhas different itle name.
In Figure 1, 'General sig-nal corp.', 'Safecard services inc.', and 'Jostens inc.'show title names.
We call this Article, and eachelement ('General signal corp.' etc) context.
Fur-thermore, a particular article, for example, 'Generalsignal corp.' consists of several paragraphs and key-words of the 'General signal corp.' article appearthroughout paragraphs.
We call each paragraphcontext in the Paragraph .We introduce a degree of context dependency intothe structure of newspaper articles shown in Figure1 in order to extract keywords.
A degree of contextdependency is a measure showing how strongly eachword related to a given context, a particular con-text of Paragraph, Article, or Domain .
In Figure 1,let 'O '  be a keyword in the article 'General signalcorp.'.
According to Luhn's assumption, 'O'  fre-quently appears throughout paragraphs.
Therefore,the deviation value of 'O '  in the Paragraph is small.On the other hand, the deviation value of 'O '  in theArticle is larger than that of the Paragraph, since inArticle, 'O '  appears in a particular element of theArticle, 'General signal corp.'.
Furthermore, the de-viation value of 'O '  in the Domain is larger thanthose of the Article and Paragraph, since in the Do-main, 'O '  appears frequently in a particular context,'Economic news'.
We extracted keywords using thisfeature of the degree of context dependency.
In Fig-ure 1, if a word is a keyword in a given article, itsatisfies the following two conditions:1.
The deviation value of a word in the Paragraphis smaller than that of the Article.2.
The deviation value of a word in the Article issmaller than that of the Domain.3 Term Weight ingEvery sense of words in articles for extracting keyparagraphs is automatically disambiguated in ad-vance.
This is because to disambiguate word-sensesin articles might affect the accuracy of context de-pendent (domain specific) key paragraphs retrieval,since the meaning of a word characterises the do-main in which it is used.
Word-sense disambigua-tion (WSD in short) is a serious problem for NLP,and a variety of approaches have been proposed forsolving it (Brown, 1991), (Yarowsky, 1992).
Ourdisambignation method is based on Niwa's methodwhich uses the similarity between a sentence contain-ing a polysemous noun and a sentence of dictionary-definition (Niwa, 1994).
Furthermore, we linkednouns which are disambignated with their seman-tically similar nouns mainly in order to cope withthe problem of a phrasal exicon.
A phrasal exiconsuch as Atlantic Seaboard, New England gives a neg-ative influence for keywords retrieval, since it cannot be regarded as units, i.e.
each word which isthe element of a phrasal exicon is assigned to eachsemantic ode (Fukumoto, 1996).To the results of WSD and linking methods,we then applied a term weighting method to ex-tract keywords.
There have been several termweighting based on word frequencies, such asTF(Term Frequency), IDF(Inverse Document Fre-quency), TF*IDF, WIDF(Weighted Inverse Doc-ument Frequency) (Luhn, 1957), (Sparck, 1973),(Salton, 1983), (Tokunaga, 1994).
We used Watan-292abe's X2 method for term weighting which is shownin formula (1) (Watanabe, 1996).2 / (='i-ra*J)= if ziy > mii = m,~ (i)Xij 0 otherwiseFormula (1) shows the value of X 2 of the word i inthe domain j. zij in (1) is the frequency of word iin the domain j. mij in (1) is shown in formula (2).E~=I O~ij k (2) mi j  = k 1~i=l~j=a=i~ i=1In formula (2), k is the number of different wordsand l is the number of the domains.
A larger valueof X~ means that the word i appears more frequentlyin thJe domain j than in the other.4 An  Ext rac t ion  of  KeywordsThe first step to extract keywords is to calculateX 2 for each word in the Paragraph, the Article, andthe Domain.
We used formula (1) to calculate thevalue of xP~j, xAi2j, and xD~y, where xP~i, xAi~, andxD~; indicate which word i appears most frequentlyin t~e context j of Paragraph, Article, and Domain,respectively.
For example, xPi2j is shown in formula(3) by using formula (1).xP,:~.
= ( z i j -mo)2  (3)mi jIn formula (3), xlj is the frequency of word i in thecontext j of Paragraph.
miy in formula (3) is shownin (2) where k is the number of different words andl is the number of contexts in Paragraph.The second step is to calculate the degree of word/ in  Paragraph (xP~), Article (xA~), and Domain(xD~).
We defined the degree of word i in Paragraph,Article, and Domain as the deviation value of k con-texts in Paragraph, Article, and Domain, respectively.Here, k is the number of contexts in Paragraph, Ar-ticle, and Domain, respectively.
For example, thedeviation value of the word i in Paragraph is definedas follows:xP~ = Z\]i:1(_;- mi) (4)In formula (4), k is the number of contexts in Para-graph, and mi is the mean value of the total fre-quency of word i in Paragraph which consists of kcontexts.The last step to extract keywords is to calculatethe context dependency ofword i using formula (4).We recall that if i satisfies both 1 and 2 in section2, the word i is regarded as a keyword.xP~:)cA~ < 1 (5)xA~xD ~ < 1 (6)Formulae (5) and (6) shows i, and 2 in section 2,respectively.
In formulae (5) and (6), xP~, xA~, andxD~ are the deviation value of a set of Paragraph,Article, and Domain, respectively.5 An  Ext rac t ion  of  Key ParagraphsThe procedure for extracting key paragraphs has thefollowing three stages:Stage One: Representing every paragraph as a vectorThe goal of this stage is to represent every para-graph in an article as a vector.
Using a term weight-mg method, every paragraph in an article would berepresented by vector of the formm, = (7)where n is the number of nouns in an article and Niyis as follows; {o1(Nj)Nis= 0where I(Nj) is a frequency with which the noun Njappears in paragraph Pi.Stage Two: Clustering methodGiven a vector representation f paragraphs P1,?
.., P,~ as in formula (7), a similarity between twoparagraphs Pi, Pj in an article would be obtainedby using formula (8).
The similarity of Pi and Pj ismeasured by the inner product of their normalisedvectors and is defined as follows:Nj does not appear in PiNj is a keyword and appears in PiNj is not a keyword and appearsin P~v(P,), v(Pj) (8)Sim(P,,Pj) = IV(P,)II v(nj) lThe greater the value of Sim(Pi, Pi) is, the moresimilar these two paragraphs are.
For a set of para-graphs P1, "" ", Pm of an article, we calculate thesemantic similarity value of all possible pairs of para-graphs.
The clustering algorithm is applied to thesets and produces a set of semantic lusters, whichare ordered in the descending order of their seman-tic similarity values.
We adopted non-overlapping,group average method in our clustering technique(Jardine, 1968).Stage Three: Extraction of key paragraphsThe sample results of clustering is shown in Table1.293Table 1: The sample results of clusteringNum Cluster1 (3,4)2 (1,(3,4))3 ((1,(3,4)),2)'Num' in Table 1 shows the order of clusters whichwe have obtained and the number shown under'Cluster' shows the paragraph numbers.
In Table1, if the number of keywords which belonging to thethird paragraph is larger than that of the fourth,the order of key paragraphs i 3 ) 4 > 1 ~ 2,otherwise, 4 > 3 ~ 1 ) 2.6 Exper imentsWe have conducted three experiments to examinethe effect of our method.
The first experiment, Key-words Experiment, is concerned with the keywordsextracting technique and with verifying the effect ofour method which introduces context dependency.The second experiment, Key Paragraphs Experiment,shows how the extracted keywords can be used toextract key paragraphs.
In the third experiment,Comparison to Other Related Work, we applied Zech-ner's key sentences method (Zechner, 1996) to keyparagraphs extraction (we call this method_A), andcompared it with our method.6.1 DataThe corpus we have used is the 1988, 1989 WallStreet J~urnal (Liherman, 1991) in ACL/DCI CD-ROM which consists of about 280,000 part-of-speechtagged sentences (BriU, 1992).
Wall Street Journalconsists of many articles, and each article has a ti-tle name.
These titles are classified into 76 differentdomains.
We selected 10 different domains and usedthem as Domain.
As a test data, we selected 50articles each of which belongs to one of these 10 do-mains.
The selected omain names and the numberof articles are shown in Table 2.Table 2: The selected ataDomain No Domain NoBBK: buybacks 6 BVG: beverages 8DIV: dividends 5 FOD: food products 5STK: stock market 5 RET: retailing 1ARO: aerospace 5 ENV: environment 3PCS: stones, gold 9 CMD: farm products 3There are 3,802 different nouns in 50 articles.
As aresult of WSD and linking methods for these articles,we have obtained 3,707 different nouns.6.2 Keywords ExperimentFormulae (5) and (6) are applied to 50 articles whichare the results of WSD and linking methods, and as aresult, we have obtained 1,047 keywords in all.
Theresult of keyword extraction is shown in Table 3.Table 3: The results of keyword experimentParagraph3(1)4(13)5(6)6(6)7(4)8(3)9(4)lO(2)11(1)Recall~Precision88.9/81.262.7/86.276.7/86.267.3/77.583.2/86.489.0/80.080.3/75.490.2/72.280.1/87.612(1) I00.0/83.714(3) 46.5/50.215(2) 100.0/73.416(2) 89.2/82.017(1) 62.4/89.422(1) 64.3/70.0Total(50) 78.,7/78.1In Table 3, z in 'z(y)'  of 'Paragraph' shows the num-ber of paragraphs in an article, 'y' shows the numberof articles.
For example, 3(1) shows that there is onearticle which consists of three paragraphs.
Recalland Precision in Table 3 are as follows;Number of correct keywords Recall = Number of keywords which are se-lected by humanNumber of correct keywordsPrecision = Number of keywords which are se-lected in our methodRecall and Precision in Table 3 show the means ineach paragraph.
The denominator of Recall is madeby three human judges; i.e.
when more than onehuman judged the word as a keyword, the word isregarded as a keyword.6.3 Key Paragraphs ExperimentFor each article, we extracted 10 ,~50 % of its para-graphs as key paragraphs.
The results of key para-graphs experiment are shown in Table 4.In Table 4, 10 ,~ 50 % indicates the extraction ratioused.
'Para.'
shows the number of paragraphs whichhumans judged to be key paragraphs, and 'Correct'shows the number of these paragraphs which themethod obtained correctly.
Evaluation is performedby three human judges.
When more than one hu-man judges a paragraph as a key paragraph, theparagraph is regarded as a key paragraph.
'*' in Ta-ble 4 shows that the number of the correct data issmaller than that of an extraction ratio.
For exam-ple, in Table 4, the number of paragraphs of 20 %out of 22 is 4.
However, the number of paragraphsthat more than one human judged the paragraph294Table 4: The results of Key Paragraphs ExperimentParagraph Percentage(%)10 20 30 (Article)Para.
Correct Para.
Correct Para .
Correct3(1) 1 1 1 1 1 14(13) 13 12 13 12 13 125(6) 6 5 6 5 *11 86(6) 6 6 6 6 *9 97(4) 4 4 4 4 8 88(3) 3 3 6 6 6 69(4) 4 4 8 8 *8 810(2) 2 2 4 2 *4 211(1) 1 1 2 2 3 312(1) 1 1 2 2 *2 214(3) 3 2 4 3 ,6 415(2) *3 *2 ,3 2 ,3 216(2) ,3 *3 *5 5 5 517(1) 2 2 3 3 *3 322(1) 2 2 ,2 2 *2 2Total(50) 54 50 69 63 84 75% 92.5 91.3 89.240 50 CorrectPara.
Correct Para.
Correct %1 1 2 2 100.013 12 26 21 88.4*10 9 18 14 96.012 10 18 14 88.212 8 16 11 79.5*8 6 12 7 80.016 11 ,18 9 74.08 6 10 7 67.84 3 6 4 81.2*3 3 6 3 78.5,14 7 ,19 10 56.5*8 6 ,14 10 70.912 8 ,16 10 75.6*7 4 *8 4 69.5*4 2 *8 4 66.6132 96 215 13072.7 60.4as a key paragraph was only two.
Therefore, 2 ismarked with a '*'.6.4 Compar i son  to  Other  Re la ted  WorkZechner proposed a method to extract  key sentencesin an art ic le by using simple stat ist ica l  method;  i.e.TF* IDF  te rm weighting method.
In order to showthe appl icabi l i ty  of our method,  we appl ied Zech-ner's key sentences method to key paragraphs  ex-t ract ion and compared it with our method.
In Zech-ner's method,  the sum over all TF* IDF  values of thecontent words for each sentence are calculated, andthe sentences are sorted according to their  weights.F inal ly  a part icu lar  number of sentences are ex-t racted as key sentences.
The data  we used con-sists of 1.92 sentences par  a paragraph and was notso many sentences within a paragraph.
Then,  inorder to apply  his method to key paragraphs  ex-t ract ion,  we calculated the sum over all sentencesfor each paragraph,  and sorted the paragraphs  ac-cording to their  weights.
From these, we extracteda certain number of paragraphs  (method_A).
In ourmethod,  every sense of words in art icles for extract-ing key paragraphs  i d isambiguated in advance andl inking method is performed.
In order to examinewhere the performance comes from, we also com-pared our method to the method which WSD andl inking method are not applied.
The result is shownin Table 5.In Table 5, '%' shows the extract ion rat io,  10 ,-~ 50%and 'Para . '
shows the number  of paragraphs  corre-sponding to each 'Percentage' .
'Our  method' ,  'notWSD',  and 'metbod_h'  shows the results using ourTable 5: The results of comparat ive xper imentOur % Paxa.method(%~10 54 50(92.5)20 69 63(91.3)30 84 75(89.3)40 132 96(72.7)50 215 130(60.4)Total 554 414(74.7)not WSD43(79.6)55(79.7)66(78.5)80(60.6)112(52.8)356(64.2)method_A31(57.4)35(50.7)41(48.8)63(47.7)99(46.0)269(48.6)method,  the method which WSD and l inking are notappl ied, and method_A, respectively.7 D iscuss ion7.1 Keywords  Exper imentE f fec t iveness  o f  the  MethodAccording to Table 3, Recall and Precision valuesrange from 46.5/50.2 to 100.0/89.4, the mean being78.7/78.6.
This shows that  our method is effectiveeven in a restr icted omain such as f inancial articles,e.g.
Wall Street Journal, although the test set wassmall  (50 articles).
Furthermore,  the correct rat iodoes not depend on the number  of paragraphs  inan article.
This shows that  our context dependencymodel  is appl icable for different size of the samples.Prob lem o f  the  MethodAccording to Table 3, the worst results of Recalland Precision was (46.5/50.2) when the number of295paragraphs was 14.
As a result, the result of theextraction of key paragraphs shown in Table 4 wasalso worst (56.5%).
The possible causes of the errorwere summarised the following two points:(1) The formulae of context dependencyThe sample results of keywords of the article,'Abermin sues Geanges in Effort to rescind JointGold Venture' is shown in Table 6.Table 6: Keywords and their X 2 values in the articleKeyword Paragraph Article DomainAbermin 0.582 10.835 663.605Belzberg 1.468 1.548 94.801flin 1.468 1.548 94.801gold5 1.770 2.496 52.865Granges 0.680 15.478 948.007Manitoba 1.468 1.548 94.801mill1 1.706 4.925 94.801ounces 1.765 5.064 284.402reserves 2.912 3.060 94.801suit2 1.099 3.096 189.601supreme1 1.468 1.548 94.801tartan1 0.251 6.191 379.203word237 4.633 5.132 362.887wood238 1.468 1.548 94.801others 15 .
.
.
.
.
.
.
.
.Total average 1.772 2.383 78.161In Table 6, each value of 'Paragraph', Article', and'Domain', shows each X 2 value.
'Total average' showsthe mean of all keywords.
'word237' and 'word238'are representative words which are the result of link-ing noun with their semantically similar nouns.
Ac-cording to Table 6, we can observe that in 'Para-graph', for example, some words whose X 2 valuesare slightly higher than the average (1,772) exist.For example, the X 2 value of 'word237' is 4.633 andslightly higher than 1.772.
However, 'word237' satis-fies the formulae of context dependency.
As a result,'word237' is regarded as a keyword, while this is not.When the extracted ratio was 10%, there were fourarticles whose correct ratio did not attained 100%.Of these, three articles are classified into this typeof the error.From the above observation, we can estimate thatthe formulae of context dependency are weak con-straints in some domains, while they are still effec-tive even in a restricted domain.
In order to getmore accuracy, some other constraints uch as loca-tion heuristics (Baxendale, 1958) or upper-case wordfeature (Kupiec, 1995) might be necessary to be in-troduced into our framework.
(2) The error of WSDWhen the extracted ratio was 10%, there was onearticle out of four articles which could not be ex-tracted correctly because of the error of WSD.
Thetest article and the results of it was shown in Figure,In Figure 2, the headline shows the title name.
Thenumbers show the paragraph number, and the un-derlined words are keywords which are extracted inour method.
The bottom shows the result of keyparagraphs extraction.
According to Figure 2, whenthe extraction ratio was 50%, the paragraphs 3 and4 were extracted and the paragraph 1 was not ex-tracted, although it is a key paragraph.
The key-words and their frequencies of appearance in para-graph 1, 3, and 4 are shown in Table 7.Table 7: The words and their frequenciesPara.
1Ft.
Word1 crystal41 oi145 word2371 word78Para.
3Ft.
Word1 concern21 crystal21 energy41 oi131 rate55 word237Para.
4Ft.
Word1 american21 crystal21 oi135 wood2371 word78word78:word237:Nov., yesterday2exchangel, offer4, notes, shares,stockS, amount4, tradingl, stockl,centsAccording to Table 7, 'crystal' and 'oil in paragraph1 are disambiguated incorrectly and were replacedby 'crystal4' and 'oi14', respectively, while 'crystal 'should have been replaced by 'crystal2' and 'oi1' with'oi13'.
Therefore, the number of words which appearin both paragraph 3 and 4 was larger than any otherpair of paragraphs.
As a result, paragraph 3 and 4are the most semantically similar paragraphs and 1was not extracted as a key paragraph.In our method, the correct ratio of key paragraphsextraction strongly depends on the results of WSD.The correct ratio of our WSD was 78.4% (Fukumoto,1996).
In order to get higher accuracy, it is necessaryto improve our WSD method.7.2 Key  Paragraphs  Exper imentE f fec t iveness  o f  the  MethodIn Key Paragraphs Experiment, the overall resultswere positive, especially when the ratio of extrac-tion was 10,,,30%.
The ratios of correct judgementsin these cases were significantly high; i.e.
92.5%,91.3%, and 89.2%, respectively.
This demonstratesthe applicability of the degree of context depen-dency.L imi ta t ions  o f  the  MethodWhen the ratio of extraction was higher than 30%,the results was 72.7% and 60.4%.
Furthermore, themore paragraphs are in an article, the smaller thenumber of correct judgements.
One possible causeof these results is that the clustering method mighthave a negative ffect on extracting key paragraphs.296Crystal Oil Co. Extends Offer1 Crystal4 oi1__44 co. said it extended to Nov. 17 the exchangel offer4 for all of its non-interest-ear~-ar\]-~-convertible secured notes, due 1997, for shares of its common stockS.2 The offer4 had been set to expire yesterdayl.3 The companyl said about 65.89% of the notes outstanding have been tendered, under theplanh, the notes will be exchanged at a rate5 of 65 shares of crystal2 oil.__33 common for each$1,000 principal amount4 of the notes, the energy4 concern2 said.4 In composite tradingl on the american2 stockl exchangel yesterday2, crystal2 oi13 shares closedat $2.875, up 12.5 cents.The results of Key Paragraph Extraction: 3 ---+ 4 --+ 1 ---+ 2Figure 2: The sample of the articleIn the field of text summarisation, a vector modelwas often used for extracting key sentence or keyparagraph (Tokunaga, 1994), (Zechner, 1996).
Inthis model, the sentences with term weighting aresorted according to their weights and this informa-tion is used to extract a certain ratio of highestweighted paragraph in an article.
We implementedthis model and compared it with our clustering tech-nique.
The results are shown in Table 8.Table 8: Our method and a vector model% Path.
Our method(%) Vector model(%)10 54 50(92.5) 48(88.9)20 69 63(91.3) 58(84.1)30 84 75(89.3) 68(78.6)40 132 96(72.7) 91(69.0)50 215 130(60.4) 128(60.6)In Table 8, '%' shows the extraction ratio, 10 ~ 50%and 'Para. '
shows the mfmber of total paragraphscorresponding to each '%'.
'Our method',  and 'Vec-tor model' shows the results of our method, and us-ing vector model, respectively.Table 8 shows that the results using our methodare highly than those of using the vector model.
Inour method, when the extraction ratio was morethan 30%, the correct ratio decreased.
This phe-nomena is also observed in the vector model.
Fromthe observation, we can estimate that the cause ofthe results was not our clustering technique.
Exam-ining the results of human judges, when the numberof paragraphs was more than 14, the number of para-graphs marked with a '*' is large.
This shows thatit is too difficult even for a human to judge whethera paragraph is a key paragraph or not.
From theobservation, for these articles, there are limitationsto our method based on context dependency.Other  Heur ist icsAs we discussed in Keywords Experiment, it mightbe considered that some heuristics uch as locationof paragraphs are introduced into our method to geta higher accuracy of keywords and key paragraphsextraction, even in these articles.
Table 9 showsthe location of key paragraphs extracted using ourmethod and extracted by humans.
The extractionratio described in Table 9 is 30%.Table 9: The location of key paragraphsArticlesHum.
Method(a)First 39 37(b)First and Last 4 4(c)First, Mid-position, and Last 1 1(d)First and Mid-position 4 4(e)Mid-position 0 1(f) Otherwise 2 3Total 50 50In Table 9, each paragraph (First, Mid-position, andLast paragraph) includes the paragraphs around it.According to Table 9, in human judgement, 39 outof 50 articles' key paragraphs are located in the firstparts, and the ratio attained 78.0%.
This showsthat using only location heuristics (the key para-graph tends to be located in the first parts) is a weakconstraint in itself, since the results of our methodshowed that the correct ratio attained 89.2%.
How-ever, in our method, 2 articles are not extracted cor-rectly, while the key paragraph is located in the firstparts of these articles.
From the observation, in acorpus such as Wall Street Journal, utilising a lo-cation heuristics is useful for extracting key para-graphs.7.3 Compar i son  to Other  Re la ted  WorkAccording to Table 5, the average ratio of ourmethod and method_A was 74.7%, and 48.6%, re-spectively.
This shows that method_A is not moreeffective than our method.
This is because most ofnouns do not contribute to showing the characteris-tic of each domain for given articles.
In the test datawhich consists of 3,802 different nouns, 2,171 nounsappeared in only one article and the frequency ofeach of them is one.
We recall that in method_A,297when word i appears in only one article and the fre-quency of i is one, the value of TF*IDF equals tolog50.
There are 2,955 out of 3,802 nouns whoseTF*IDF value is less than log50, and the percentageattained at 77.7%.
This causes the fact that most ofnouns do not contribute to showing the characteris-tic of each domain for given articles.Comparing the difference ratio of 'Our method'and 'not WSD' to that of 'not WSD' and method_A,the former was 10.5% and the latter was 15.6%.Therefore, our context dependency model con-tributes the extraction of key paragraphs, althoughWSD and linking are still effective.8 ConclusionWe have reported an experimental study for extract-ing key paragraphs based on the degree of contextdependency for a given article and showed how ourcontext dependency model can use effectively to ex-tract key paragraphs, each of which belongs to therestricted subject domain.
In order to cope withthe remaining problems mentioned in section 7 andapply this work to practical use, we will conductfurther experiments.9 AcknowledgmentsThe authors would like to thank the reviewers fortheir valuable comments.ReferencesP.
B. Baxendale, "Man-made index for technical lit-erature - an experiment", IBM J. Res.
Develop.,2(1958)4, pp.
354-361, 1958E.
Brill, "A simple rule-based part of speech tag-ger", In Proc.
of the 3rd conference on appliednatural anguage processing, pp.
152-155, 1992P.
F. Brown et al, "Word-Sense DisambiguationUsing Statistical Methods", In Proc.
of the 29thAnnual Meeting of the ACL, pp.
264-270, 1991H.
P. Edmundson, "New methods in automatic ab-stracting", Journal of ACM, 16(1969)2, pp.
264-285, 1969F.
Fukumoto and Y. Suzuki, "An Automatic Clus-tering of Articles Using Dictionary Definitions",In Proc.
of the 16th COLING, pp.
406-411, 1996N.
Jardine and R. Sibson, "The construction ofhi-erarchic and non-hierarchic classifications", Com-puter Journal, pp.
177-184, 1968P.
S. Jacobs and L. F. Rau, "SCISOR: Extractinginformation from on-line news", Communicationsof the ACM, 33(1990)11, pp.
88-97, 1990J.
Kupiec et al, "A trainable document summa-rizer", In Proc.
of SIGIR'95, pp.
68-73, 1995M.
Liberman, "CD-ROM I Association for Com-putational Linguistics Data Collection Initiative",University of Pennsylvania, 1991H.
P. Luhn, "A statistical approach to mechanizedencoding and searching of literary information",IBM journal, 1(1957)4, pp.
307-319, 1957H.
P. Luhn, "The Automatic Creation of LiteratureAbstracts", IBM journal, 2(1958)1, pp.
159-165,1958S.
Miike et al, "A full-text retrieval system with adynamic abstract generation function", In Proc.of SIGIR'9~, pp.
152-161, 1994M.
Nagao et al, "An Automatic Method of the Ex-traction of Important Words from Japanese Sci-entific Documents (in Japanese)", IPS Japan,17(1976)2, pp.
110-117, 1976Y.
Niwa and Y. Nitta, "Co-occurrence v ctors fromcorpora vs. distance vectors from dictionaries", InProc.
of the 15th COLING, pp.
304-309, 1994C.
D. Paice, "Constructing literature abstractsby computer:Techniques and prospects", Infor-mation Processing and Management, vol.
26, pp.171-186, 1990C.
D. Palce and P. A. Jones, "The identification ofimportant concepts in highly structured technicalpapers", In Proc.
of SIGIR'93, pp.
69-78, 1993U.
Reimer and U. Hahn, "Text condensation asknowledge base abstraction", IEEE Conference onAI Applications, pp.
338-344, 1988G.
Salton and M. J. McGill, "Introduction to Mod-ern Information Retrieval", McGraw-Hill, 1983G.
Salton et al, "Approaches to passage retrievalin full text information systems", In Proc.
of SI-GIR'93, pp.
49-58, 1993K.
J. Sparck, "A statistical interpretation f termspecificity and its application in retrieval", Jour-nal of Documentation, 28(1973)1, pp.
11-21, 1973T.
Tokunaga nd M. Iwayama, "Text Categoriza-tion based on Weighted Inverse Document Fre-quency", SIG-IPSJapan, 100(1994)5, pp.
33-40,1994Y.
Watanabe et al, "Document Classification UsingDomain Specific Kanji Characters Extracted byX 2 Method", In Proc.
of the 16th COLING, pp.794-799, 1996D.
Yarowsky, "Word sense disambiguation using sta-tistical models of Roget's categories trained onlarge corpora", In Proc.
of the 14th COLING, pp.454-460, 1992K.
Zechner, "Fast Generation of Abstracts fromGeneral Domain Text Corpora by Extracting Rel-evant Sentences", In Proc.
of the 16th COLING,pp.
986-989, 1996298
