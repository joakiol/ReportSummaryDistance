Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 499?510,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsResolving Shell NounsVarada KolhatkarDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadavarada@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadagh@cs.toronto.eduAbstractShell nouns, such as fact and problem, oc-cur frequently in all kinds of texts.
Thesenouns themselves are unspecific, and canonly be interpreted together with the shellcontent.
We propose a general approachto automatically identify shell content ofshell nouns.
Our approach exploits lexico-syntactic knowledge derived from the lin-guistics literature.
We evaluate the ap-proach on a variety of shell nouns with avariety of syntactic expectations, achiev-ing accuracies in the range of 62% (base-line = 33%) to 83% (baseline = 74%) oncrowd-annotated data.1 IntroductionShell nouns are abstract nouns, such as fact, issue,idea, and problem, which facilitate efficiency byavoiding repetition of long stretches of text.
Theshell metaphor comes from Schmid (2000), and itcaptures the various functions of these nouns in adiscourse: containment, signalling, pointing, andencapsulating.
Shell nouns themselves are unspe-cific, and can only be interpreted together withtheir shell content, i.e., the propositional contentthey encapsulate in the given context.
The processof identifying this content in the given context isreferred to as shell noun resolution or interpreta-tion.
Examples (1), (2), and (3) show usages of theshell nouns fact and issue.
The shell noun phrasesare resolved to the postnominal that clause, thecomplement wh clause, and the immediately pre-ceding clause, respectively.1,2(1) The fact that a major label hadn?t beenat liberty to exploit and repackage thematerial on CD meant that prices on thevintage LP market were soaring.
(2) The issue that this country and Congressmust address is how to provide optimalcare for all without limiting access forthe many.
(3) Living expenses are much lower in ruralIndia than in New York, but this fact isnot fully captured if prices are convertedwith currency exchange rates.Observe that the relation between shell nounphrases and their shell content is similar tothe relation of abstract anaphora (or cataphora)(Asher, 1993) with backward- or forward-lookingabstract-object antecedents.
For anaphoric shellnoun examples, the shell content precedes theshell noun phrase, and for cataphoric shell nounexamples the shell content follows the shell nounphrase.3Shell nouns as a group occur frequently in argu-mentative texts (Schmid, 2000; Flowerdew, 2003;Botley, 2006).
They play an important role in or-ganizing a discourse and maintaining its coher-ence (Schmid, 2000; Flowerdew, 2003), and re-solving them is an important component of var-ious computational linguistics tasks that rely on1Note that the postnominal that-clause in (1) is not a rela-tive clause: the fact in question is not an argument of exploitand repackage.2All examples in this paper are from the NewYork Times corpus (https://catalog.ldc.upenn.edu/LDC2008T19)3We use the terms cataphoric shell noun and anaphoricshell noun for lack of better alternatives.499discourse structure.
Accordingly, identifying shellcontent can be helpful in summarization, informa-tion retrieval, and ESL learning (Flowerdew, 2003;Hinkel, 2004).Despite their importance in discourse, under-standing of shell nouns from a computational lin-guistics perspective is only in the preliminarystage.
Recently, we proposed an approach to anno-tate and resolve anaphoric cases of six typical shellnouns: fact, reason, issue, decision, question, andpossibility (Kolhatkar et al., 2013b).
This workdrew on the observation that shell nouns followingcataphoric constructions are easy to resolve.
Wemanually developed rules to identify shell contentfor such cases.
Later, we used these cataphoric ex-amples and their shell content as training data toresolve harder anaphoric examples.In this paper, we propose a general algorithm toresolve cataphoric shell noun examples.
Our long-term goal is to build an end-to-end shell-noun res-olution system.
If we want to go beyond the sixshell nouns from our previous work, and general-ize our approach to other shell nouns, first we needto develop an approach to resolve cataphoric shellnoun examples.
A number of challenges are asso-ciated with this seemingly easy task.
The primarychallenges is that this resolution is in many cru-cial respects a semantic phenomenon.
To obtainthe required semantic knowledge, we exploit theproperties of shell nouns and their categorizationdescribed in the linguistics literature.
We evalu-ate our method using crowdsourcing, and demon-strate how far one can get with simple, determin-istic shell content extraction.2 Related workShell-nounhood is a well-established concept inlinguistics (Vendler, 1968; Ivanic, 1991; Asher,1993; Francis, 1994; Schmid, 2000, inter alia).However, understanding of shell nouns from acomputational linguistics perspective is only in thepreliminary stage.Shell nouns take a number of semantic argu-ments.
In this respect, they are similar to the gen-eral class of argument-taking nominals as givenin the NomBank (Meyers et al., 2004).
Simi-larly, there is a small body of literature that ad-dresses nominal semantic role labelling (Gerber etal., 2009) and nominal subcategorization frames(Preiss et al., 2007).
That said, the distinguishingproperty of shell nouns is that one of their seman-tic arguments is the shell content, but the literaturein computational linguistics does not provide anymethod that is able to identify the shell content.The focus of our work is to rectify this.Shell content represents complex and abstractobjects.
So traditional linguistic and psycholin-guistic principles used in pronominal anaphoraresolution (see the survey by Poesio et al.
(2011)),such as gender and number agreement, are not ap-plicable in resolving shell nouns.
That said, thereis a line of literature on annotating and resolvingpersonal and demonstrative pronouns, which typi-cally refer to similar kinds of non-nominal abstractentities (Passonneau, 1989; Eckert and Strube,2000; Byron, 2003; M?ller, 2008; Hedberg etal., 2007; Poesio and Artstein, 2008; Navarretta,2011, inter alia).
Also, there have been attemptsat annotating the shell content of anaphoric occur-rences of shell nouns (e.g., Botley (2006), Kol-hatkar et al.
(2013a)).
However, none of theseapproaches attempt to annotate and resolve cat-aphoric examples such (1) and (2).3 ChallengesA number of challenges are associated with thetask of resolving cataphoric shell noun examples,especially when it comes to developing a holisticapproach for a variety of shell nouns.First, each shell noun has idiosyncrasies.
Dif-ferent shell nouns have different semantic and syn-tactic expectations, and hence they take differenttypes of one or more semantic arguments: one in-troducing the shell content, and others expressingcircumstantial information about the shell noun.For instance, fact typically takes a single factualclause as an argument, which is its shell content,as we saw in example (1), whereas reason expectstwo arguments: the cause and the effect, with thecontent introduced in the cause, as shown in exam-ple (4).4Similarly, decision takes an agent makingthe decision and the shell content is represented asan action or a proposition, as shown in (5).5(4) One reason [that 60 percent of New YorkCity public-school children read belowgrade level]effectis [that many elementaryschools don?t have libraries]cause.4Observe that the postnominal that clause in (4) is not arelative clause, and still it is not the shell content because it isnot the cause argument of the shell noun reason.5Observe that this aspect of shell nouns of taking differentnumbers and kinds of complement clauses is similar to verbshaving different subcategorization frames.500(5) I applaud loudly the decision of[Greenburgh]agentto ban animal per-formances.Second, the relation between a shell noun andits content is in many crucial respects a seman-tic phenomenon.
For instance, resolving the shellnoun reason to its shell content involves identify-ing a) that reason generally expects two semanticarguments: cause and effect, b) that the cause ar-gument (and not the effect argument) representsthe shell content, and c) that a particular con-stituent in the given context represents the causeargument.Third, at the conceptual level, once we knowwhich semantic argument represents shell content,resolving examples such as (4) seems straightfor-ward using syntactic structure, i.e., by extractingthe complement clause.
But at the implementa-tion level, this is a non-trivial problem for two rea-sons.
The first reason is that examples contain-ing shell nouns often follow syntactically complexconstructions, including embedded clauses, coor-dination, and sentential complements.
An auto-matic parser is not always accurate for such ex-amples.
So the challenge is whether the avail-able tools in computational linguistics such as syn-tactic parsers and discourse parsers are able toprovide us with the information that is necessaryto resolve these difficult cases.
The second rea-son is that the shell content can occur in manydifferent constructions, such as apposition (e.g.,parental ownership of children, a concept thatallows .
.
.
), postnominal and complement clauseconstructions, as we saw in examples (1) and (2),and modifier constructions (e.g., the liberal tradepolicy that .
.
.
).
Moreover, in some constructions,the content is indefinite (e.g., A bad idea does notharm until someone acts upon it.)
or None be-cause the example is a non-shell noun usage (e.g.,this week?s issue of Sports Illustrated), and thechallenge is to identify such cases.Finally, whether the postnominal clause intro-duces the shell content or not is dependent onthe context of the shell noun phrase.
The reso-lution can be complicated by complex syntacticconstructions.
For instance, when the shell nounfollows verbs such as expect, it becomes difficultfor an automatic system to identify whether thepostnominal or the complement clause is of theverb or of the shell noun (e.g., they did not expectthe decision to reignite tension in Crown Heightsvs.
no one expected the decision to call an elec-tion).
Similarly, shell noun phrases can be ob-jects of prepositions, and whether the postnomi-nal clause introduces the shell content or not is de-pendent on this preposition.
For instance, for thepattern reason that, the postnominal that clausedoes not generally introduce the shell content, aswe saw in (4); however, this does not hold whenthe shell noun phrase containing reason followsthe preposition for, as shown in (6).
(6) Low tax rates give people an incentive towork, for the simple reason that they getto keep more of what they earn.4 Linguistic frameworkLinguists have studied a variety of shell nouns,their classification, different patterns they follow,and their semantic and syntactic properties in de-tail (Vendler, 1968; Ivanic, 1991; Asher, 1993;Francis, 1994; Schmid, 2000, inter alia).
Schmidpoints out that being a shell noun is a property ofa specific usage of the noun rather than an inher-ent property of the word.
He provides a list of 670English nouns that tend to occur as shell nouns.
Afew frequently occurring ones are: problem, no-tion, concept, issue, fact, belief, decision, point,idea, event, possibility, reason, trouble, question,plan, theory, aim, and principle.4.1 Lexico-syntactic patternsPrecisely defining the notion of shell-nounhoodis tricky.
A necessary property of shell nouns isthat they are capable of taking clausal arguments,primarily with two lexico-syntactic constructions:Noun + postnominal clause and Noun + be + com-plement clause (Vendler, 1968; Biber et al., 1999;Schmid, 2000; Huddleston and Pullum, 2002).Schmid exploits these lexico-syntactic construc-tions to identify shell noun usages.
In particular,he provides a number of typical lexico-syntacticpatterns that are indicative of either anaphoric orcataphoric shell noun occurrences.
Table 1 showsthese patterns with examples.Cataphoric These patterns primarily follow twoconstructions.N-be-clause In this construction, the shellnoun phrase occurs as the subject in a subject-verb-clause construction, with the linking verb be,and the shell content embedded as a wh clause,that clause, or to-infinitive clause.
The linking501Cataphoric1 N-be-to Our plan is to hire and retain the best managers we can.2 N-be-that The major reason is that doctors are uncomfortable with uncertainty.3 N-be-wh Of course, the central, and probably insoluble, issue is whether animal testing is cruel.4 N-to The decision to disconnect the ventilator came after doctors found no brain activity.5 N-that Mr. Shoval left open the possibility that Israel would move into other West Bank cities.6 N-wh If there ever is any doubt whether a plant is a poppy or not, break off a stem and squeeze it.7 N-of The concept of having an outsider as Prime Minister is outdated.Anaphoric8 th-N Living expenses are much lower in rural India than in New York, but this fact is not fullycaptured if prices are converted with currency exchange rates.9 th-be-N People change.
This is a fact.10 Sub-be-N If the money is available, however, cutting the sales tax is a good idea.Table 1: Lexico-grammatical patterns of shell nouns (Schmid, 2000).
Shell noun phrases are underlined,the pattern is marked in boldface, and the shell content is marked in italics.ProportionNoun N-be-to N-be-that N-be-wh N-to N-that N-wh N-of totalidea 7 2 - 5 23 10 53 91,277issue - 1 5 7 14 2 71 55,088concept 1 - - 6 12 - 79 14,301decision - - - 80 12 1 5 55,088plan 5 - - 72 17 - 4 67,344policy 4 1 - 16 25 2 51 24,025Table 2: Distribution of cataphoric patterns for six shell nouns in the New York Times corpus.
Eachcolumn shows the percentage of instances following that pattern.
The last column shows the total numberof cataphoric instances of each noun in the corpus.verb be indicates the semantic identity between theshell noun and its content in the given context.
Theconstruction follows the patterns in rows 1, 2, and3 of Table 1.N-clause This construction includes the cat-aphoric patterns 4?7 in Table 1.
For these patternsthe link between the shell noun and the contentis much less straightforward: whether the post-nominal clause expresses the shell content or notis dependent on the shell noun and the syntac-tic structure under consideration.
For instance,for the shell noun fact, the shell content is em-bedded in the postnominal that clause, as shownin (1), but this does not hold for the shell nounreason in example (4).
The N-of pattern is dif-ferent from other patterns: it follows the con-struction N-prepositional phrase rather than N-clause, and since a prepositional phrase can takedifferent kinds of embedded constituents such as anoun phrase, a sentential complement, and a verbphrase, the pattern offers flexibility in the syntactictype of the shell content.Anaphoric For these patterns, the link betweenthe shell noun and the content is created usinglinguistic elements such as the, this, that, other,same, and such.
For the patterns 8 and 9 the shellcontent does not typically occur in the sentencecontaining the shell noun phrase.
For the pattern10, the shell content is the subject in a subject-verb-N construction.Pattern preferences Different shell nouns havedifferent pattern preferences.
Table 2 shows thedistribution of cataphoric patterns for six shellnouns in the New York Times corpus.
The shellnouns idea, issue, and concept prefer N-of pattern,whereas plan and decision prefer the pattern N-to.Among all instances of the shell noun decision fol-502Idea familySemantic features: [mental], [conceptual]Frame: mental; focus on propositional content of IDEANouns: idea, issue, concept, point, notion, theory, .
.
.Patterns: N-be-that/of, N-that/ofPlan familySemantic features: [mental], [volitional], [manner]Frame: mental; focus on IDEANouns: decision, plan, policy, idea, .
.
.Patterns: N-be-to/that, N-to/thatTrouble familySemantic features: [eventive], [attitudinal], [manner],[deontic]Frame: general eventiveNouns: problem, trouble, difficulty, dilemma, snagPatterns: N-be-toProblem familySemantic features: [factual], [attitudinal], [impeding]Frame: general factualNouns: problem, trouble, difficulty, point, thing, snag,dilemma , .
.
.Patterns: N-be-that/ofThing familySemantic features: [factual]Frame: general factualNouns: fact, phenomenon, point, case, thing, businessPatterns: N-that, N-be-thatReason familySemantic features: [factual], [causal]Frame: causal; attentional focus on CAUSENouns: reason, cause, ground, thingPatterns: N-be-that/why, N-that/whyTable 3: Example families from Schmid (2000).
The nouns in boldface are used to evaluate this work.lowing Schmid?s cataphoric patterns, 80% of theinstances follow the pattern N-to.64.2 Categorization of shell nounsSchmid classifies shell nouns at three levels.
Atthe most abstract level, he classifies shell nounsinto six semantic classes: factual, linguistic, men-tal, modal, eventive, and circumstantial.
Each se-mantic class indicates the type of experience theshell noun is intended to describe.
For instance,the mental class describes ideas and cognitivestates, whereas the linguistic class describes utter-ances, linguistic acts, and products thereof.The next level of classification includes more-detailed semantic features.
Each broad semanticclass is sub-categorized into a number of groups.A group of an abstract class tries to capturethe semantic features associated with the fine-grained differences between different usages ofshell nouns in that class.
For instance, groupsassociated with the mental class are: conceptual,creditive, dubiative, volitional, and emotive.The third level of classification consists of fam-ilies.
A family groups together shell nouns withsimilar semantic features.
Schmid provides 79 dis-tinct families of 670 shell nouns.
Each family isnamed after the primary noun in that family.
Table3 shows six families: Idea, Plan, Trouble, Prob-lem, Thing, and Reason.
A shell noun can be6Table 2 does not include anaphoric patterns, as this pa-per is focused on cataphoric shell noun examples.
Anaphoricpatterns are common for all shell nouns: among all instancesof a shell noun, approximately 50 to 80% are anaphoric.a member of multiple families.
The nouns sub-sumed in a family share semantic features.
Forinstance, all nouns in the Idea family are mentaland conceptual.
They are mental because ideasare only accessible through thoughts, and concep-tual because they represent reflection or an appli-cation of a concept.
Each family activates a se-mantic frame.
The idea of these semantic frames issimilar to that of frames in Frame semantics (Fill-more, 1985) and in semantics of grammar (Talmy,2000).
In particular, Schmid follows Talmy?s con-ception of frames.
A semantic frame describesconceptual structures, its elements, and their in-terrelationships.
For instance, the Reason familyinvokes the causal frame, which has cause and ef-fect as its elements with the attentional focus onthe cause.
According to Schmid, the nouns in afamily also share a number of lexico-syntactic fea-tures.
The patterns attribute in Table 3 shows pro-totypical lexico-syntactic patterns, which attractthe members of the family.
Schmid defines attrac-tion as the degree to which a lexico-grammaticalpattern attracts a certain noun.
For instance, thepatterns N-to and N-that attract the shell nouns inthe Plan family, whereas the N-that pattern attractsthe nouns in the Thing family.
The pattern N-of isrestricted to a smaller group of nouns such as con-cept, problem, and issue.7,87Schmid used the British section of COBUILD?S Bank ofEnglish for his classification.8Schmid?s families could help enrich resources such asFrameNet (Baker et al., 1998) with the shell content informa-tion.5035 Resolution algorithmWith this exposition, the problem of shell nounresolution is identifying the appropriate seman-tic argument of the shell noun representing itsshell content.
This section describes our algorithmto resolve shell nouns following cataphoric pat-terns.
The algorithm addresses the primary chal-lenge of idiosyncrasies of shell nouns by exploit-ing Schmid?s semantic families (see Section 4.2).The input of the algorithm is a shell noun instancefollowing a cataphoric pattern, and the output isits shell content or None if the shell content is notpresent in the given sentence.
The algorithm fol-lows three steps.
First, we parse the given sentenceusing the Stanford parser.9Second, we look forthe noun phrase (NP), where the head of the NP isthe shell noun to be resolved.10Finally, we extractthe appropriate shell content, if it is present in thegiven sentence.5.1 Identifying potentially anaphoricshell-noun constructionsBefore starting the actual resolution, first we iden-tify whether the shell content occurs in the givensentence or not.
According to Schmid, the lexico-syntactic patterns signal the position of the shellcontent.
For instance, if the pattern is of the formN-be-clause, the shell content is more likely tooccur in the complement clause in the same sen-tence.
That said, although on the surface level, theshell noun seems to follow a cataphoric pattern, itis possible that the shell content is not given in apostnominal or a complement clause, as shown in(7).
(7) Just as weekend hackers flock to the golfball most used by PGA Tour players,recreational skiers, and a legion of youthleague racers, gravitate to the skis wornby Olympic champions.
It is the reasonthat top racers are so quick flash their skisfor the cameras in the finish area.Here, the shell noun and its content are linked viathe pronoun it.
For such constructions, the shellnoun phrase and shell content do not occur in thesame sentence.
Shell content occurs in the preced-ing discourse, typically in the preceding sentence.9http://nlp.stanford.edu/software/lex-parser.shtml10We extract the head of an NP following the heuristicsproposed by Collins (1999, p. 238).We identify such cases, and other cases where theshell content is not likely to occur in the postnom-inal or complements clauses, by looking for thepatterns below in the given order, returning theshell content when it occurs in the given sentence.Sub-be-N This pattern corresponds to thelexico-grammatical pattern in Figure 1(a).
If thispattern is found, there are three main possibilitiesfor the subject.
First, if an existential there occursat the subject position, we move to the next pat-tern.
Second, if the subject is it (example (7)), thisor that, we return None, assuming that the con-tent is not present in the given sentence.
Finally,if the first two conditions are not satisfied, i.e., ifthe subject is neither a pronoun not an existentialthere, we assume that subject contains a valid shellcontent, and return it.
An example is shown in (8).Note that in such cases, unlike other patterns, theshell content is expressed as a noun phrase.
(8) Strict liability is the biggest issue whenconsidering what athletes put in their bod-ies.Apposition Another case where shell contentdoes not typically occur in the postnominal orcomplement clause is the case of apposition.
In-definite shell noun phrases often occur in apposi-tion constructions, as shown in (9).
(9) The LH lineup, according to Gale, willfeature ?cab-forward?
design, a conceptthat particularly pleases him.In this step, we check for this construction and re-turn the sentential, verbal, or nominal left siblingof the shell noun phrase.Modifier For shell nouns such as issue, phe-nomenon, and policy, often the shell content isgiven in the modifier of the shell noun, as shownin (10).
(10) But in the 18th century, Leipzig?s centrallocation in German-speaking Europe andthe liberal trade policy of the Saxon courtfostered publishing.We deal with such cases as follows.
First, weextract the modifier phrases by concatenating themodifier words having noun, verb, or adjectivepart-of-speech tags.
To exclude unlikely modi-fier phrases as shell content (e.g., good idea, big504ParentNPSubjectVPVB*form of beNP/NNhead = shell(a) Sub-be-N patternParentNP/NNhead = shellVPVB*form of beSBAR/SINthat/whSclause(b) N-be-clause patternParentNP/NNhead = shellSBAR/SINthat/whSclause(c) N-that/wh patternFigure 1: Lexico-syntactic patterns for shell nounsissue), we extract a list of modifiers for a num-ber of shell nouns and create a stoplist of modi-fiers.
If any of the words in the modifier phrasesis a pronoun or occurs in the stoplist, we move tothe next pattern.
If the modifier phrase passes thestoplist test, to distinguish between non-shell con-tent and shell content modifiers, we examine thehypernym paths of the words in the modifierphrase in WordNet (Fellbaum, 1998).
If the synsetabstraction.n.06 occurs in the path, we considerthe modifier phrase to be valid shell content, as-suming that the shell content of shell nouns mosttypically represents an abstract entity.5.2 Resolving remaining instancesAt this stage we are assuming that the shell con-tent occurs either in the postnominal clause or thecomplement clause.
So we look for the patternsbelow, returning the shell content when found.N-be-clause The lexico-grammatical patterncorresponding to the pattern N-be-clause is shownin Figure 1(b).
This is one of the more reliablepatterns for shell content extraction, as the be verbsuggests the semantic identity between the shellnoun and the complement clause.
The be-verbdoes not necessarily have to immediately followthe shell noun.
For instance, in example (2), thehead of the NP The issue that this country andCongress must address is the shell noun issue, andhence it satisfies the construction in Figure 1(b).N-clause Finally, we look for this pattern.
Anexample of this pattern is shown in Figure 1(c).This is the most common (see Table 2) and tricki-est pattern in terms of resolution, and whether theshell content is given in the postnominal clause ornot is dependent on the properties of the shell noununder consideration and the syntactic constructionof the example.
For instance, for the shell noundecision, the postnominal to-infinitive clause typi-cally represents shell content.
But this did not holdfor the shell noun reason, as shown in (11).
(11) The reason to resist becoming a partici-pant is obvious.Here, Schmid?s semantic families come in thepicture.
We wanted to examine a) the extent towhich the previous steps help in resolution, and b)whether knowledge extracted from Schmid?s fam-ilies add value to the resolution.
So we employtwo versions of this step.Include Schmid?s cues (+SC) This versionexploits the knowledge encoded in Schmid?s se-mantic families (Section 4.2), and extracts post-nominal clauses only if Schmid?s pattern cues aresatisfied.
In particular, given a shell noun, we de-termine the families in which it occurs and list allpossible patterns of these families as shell contentcues.
The postnominal clause is a valid shell con-tent only if it satisfies these cues.
For instance,the shell noun reason occurs in only one family:Reason, with the allowed shell content patterns N-that and N-why.
Schmid?s patterns suggest that thepostnominal to-infinitive clauses are not allowedas shell content for this shell noun, and thus thisstep will return None.
This version helps correctlyresolving examples such as (11) to None.Exclude Schmid?s cues (?SC) This versiondoes not enforce Schmid?s cues in extracting thepostnominal clauses.
For instance, the Problemfamily does not include N-that/wh/to/of patterns,but in this condition, we nonetheless allow thesepatterns in extracting the shell content of the nounsfrom this family.6 Evaluation dataWe claim that our algorithm is able to resolve avariety of shell nouns.
That said, creating eval-uation data for all of Schmid?s 670 English shell505nouns is extremely time-consuming, and is there-fore not pursued further in the current study.
In-stead we create a sample of representative evalua-tion data to examine how well the algorithm worksa) on a variety of shell nouns, b) for shell nounswithin a family, c) for shell nouns across familieswith completely different semantic and syntacticexpectations, and d) for a variety of shell patternsfrom Table 1.6.1 Selection of nounsRecall that each shell noun has its idiosyncrasies.So in order to evaluate whether our algorithm isable to address these idiosyncrasies, the evalua-tion data must contain a variety of shell nouns withdifferent semantic and syntactic expectations.
Toexamine a), we consider the six families shown inTable 3.
These families span three abstract cat-egories: mental, eventive, and factual, and fivedistinct groups: conceptual, volitional, factual,causal, and attitudinal.
Also, the families haveconsiderably different syntactic expectations.
Forinstance, the nouns in the Idea family can havetheir content in that or of clauses occurring in N-clause or N-be-clause constructions, whereas theTrouble and Problem families do not allow N-clause pattern.
The shell content of the nouns inthe Plan family is generally represented with to-infinitive clauses.
To examine b) and c), we choosethree nouns from each of the first four familiesfrom Table 3.
To add diversity, we also includetwo shell nouns from the Thing family and a shellnoun from the Reason family.
So we selected atotal of 12 shell nouns for evaluation: idea, issue,concept, decision, plan, policy, problem, trouble,difficulty, reason, fact, and phenomenon.6.2 Selection of instancesRecall that the shell content varies based on theshell noun and the pattern it follows.
Moreover,shell nouns have pattern preferences, as shown inTable 2.
To examine d), we need shell noun exam-ples following different patterns from Table 1.
Weconsider the New York Times corpus as our basecorpus, and from this corpus extract all sentencesfollowing the lexico-grammatical patterns in Ta-ble 1 for the twelve selected shell nouns.
Then wearbitrarily pick 100 examples for each shell noun,making sure that the selection contains examplesof each cataphoric pattern from Table 1.
Theseexamples consist of 70% examples of each of theseven cataphoric patterns, and the remaining 30%of the examples are picked randomly from the dis-tribution of patterns for that shell noun.6.3 Crowdsourcing annotationWe designed a crowdsourcing experiment to ob-tain the annotated data for evaluation.
We parseeach sentence using the Stanford parser, and ex-tract all possible candidates, i.e., arguments of theshell noun from the parser?s output.
Since our ex-amples include embedding clauses and sententialcomplements, the parser is often inaccurate.
Forinstance, in example (12), the parser attaches onlythe first clause of the coordination (that peoplewere misled) to the shell noun fact.
(12) The fact that people were misled and in-formation was denied, that?s the reasonthat you?d wind up suing.To deal with such parsing errors, we consider the30-best parses given by the parser.
From theseparses, we extract a list of eligible candidates.
Thislist includes the arguments of the shell noun givenin the appositional clauses, modifier phrases, post-nominal that, wh, or to-infinitive clauses, comple-ment clauses, objects of postnominal prepositionsof the shell noun, and subject if the shell noun fol-lows subject-be-N construction.
On average, therewere three candidates per instance.After extracting the candidates, we present theannotators with the sentence, with the shell nounhighlighted, and the extracted candidates.
We askthe annotators to choose the option that providesthe correct interpretation of the highlighted shellnoun.
We also provide them the option None ofthe above, and ask them to select it if the shell con-tent is not present in the given sentence or the shellcontent is not listed in the list of candidates.CrowdFlower We used CrowdFlower11as ourcrowdsourcing platform, which in turn uses vari-ous worker channels such as Amazon MechanicalTurk12.
CrowdFlower offers a number of features.First, it provides a quiz mode which facilitatesfiltering out spammers by requiring an annotatorto pass a certain number of test questions beforestarting the real annotation.
Second, during an-notation, it randomly presents test questions withknown answers to the annotators to keep them ontheir toes.
Based on annotators?
responses to thetest questions, each annotator is assigned a trust11http://crowdflower.com/12https://www.mturk.com/mturk/welcome506?
5 ?
4 ?
3 < 3idea 53 67 95 5issue 44 65 95 5concept 40 56 96 4decision 50 72 98 2plan 41 55 95 5policy 42 61 94 6problem 52 70 100 0trouble 44 69 99 1difficulty 45 61 96 4reason 48 60 93 7fact 52 68 98 2phenomenon 39 56 95 5all 46 63 96 4Table 4: Annotator agreement on shell content.Each column shows the percentage of instances onwhich at least n or fewer than n annotators agreeon a single answer.score: an annotator performing well on the testquestions gets a high trust score.
Finally, Crowd-Flower allows the user to select the permitted de-mographic areas and skills required.Settings We asked for at least 5 annotations perinstance by annotators from the English-speakingcountries.
The evaluation task contained a totalof 1200 instances, 100 instances per shell noun.To maintain the annotation quality, we included105 test questions, distributed among different an-swers.
We paid 2.5 cents per instance and the an-notation task was completed in less than 24 hours.Results Table 4 shows the agreement of thecrowd.
In most cases, at least 3 out of 5 anno-tators agreed on a single answer.
We took this an-swer as the gold standard in our evaluation, anddiscard the instances where fewer than three anno-tators agreed.
The option None of the above wasannotated for about 30% of the cases.
We includethese cases in the evaluation.
In total we had 1,257instances (1,152 instances where at least 3 annota-tors agreed + 105 test questions).7 Evaluation resultsBaseline We evaluate our algorithm againstcrowd-annotated data using a lexico-syntacticclause (LSC) baseline.
Given a sentence con-taining a shell instance and its parse tree, thisbaseline extracts the postnominal or complementclause from the parse tree depending only uponthe lexico-syntactic pattern of the shell noun.
Forinstance, for the N-that and N-be-to patterns, it ex-Nouns LSC A?SC A+SC1 idea 74 82 832 issue 60 75 773 concept 51 67 684 decision 70 71 735 plan 51 63 626 policy 58 70 527 problem 66 69 598 trouble 63 68 509 difficulty 68 75 4910 reason 43 53 7711 fact 43 55 6812 phenomenon 33 62 5013 all 57 69 64Table 5: Shell noun resolution results.
Each col-umn shows the percent accuracy of resolution withthe respective method.
Boldface is best in row.tracts the postnominal that clause and the comple-ment to-infinitive clause, respectively.13Results Table 5 shows the evaluation results forthe LSC baseline, the algorithm without Schmid?scues (A?SC), and the algorithm with Schmid?scues (A+SC).
The A?SC condition in all cases andthe A+SC condition in some cases outperform theLSC baseline, which proves to be rather low, espe-cially for the shell nouns with strict syntactic ex-pectations (e.g., fact and reason).
Thus we see thatour algorithm is adding value.That said, we observe a wide range of per-formance for different shell nouns.
On the upside, adding Schmid?s cues helps resolving theshell nouns with strict syntactic expectations.
TheA+SC results for the shell nouns idea, issue, con-cept, decision, reason, and fact outperform thebaseline and the A?SC results.
In particular, theA+SC results for the shell nouns fact and rea-son are markedly better than the baseline results.These nouns have strict syntactic expectations forthe shell content clauses they take: the familiesThing and Certainty of the shell noun fact allowonly a that clause, and the Reason family of theshell noun reason allows only that and becauseclauses for the shell content.
These cues helpin correctly resolving examples such as (11) toNone, where the postnominal to-infinitive clause13Note that we only extract subordinating clauses (e.g.,(SBAR (IN that) (clause))) and to-infinitive clauses, and notrelative clauses.507describes the purpose or the goal for the reason,but not the shell content itself.On the down side, adding Schmid?s cues hurtsthe performance of more versatile nouns, whichcan take a variety of clauses.
Although the A?SCresults for the shell nouns plan, policy, problem,trouble, difficulty, and phenomenon are well abovethe baseline, the A+SC results are markedly be-low it.
That is, Schmid?s cues were deleterious.Our error analysis revealed that these nouns areversatile in terms of the clauses they take as shellcontent, and Schmid?s cues restrict these clausesto be selected as shell content.
For instance, theshell noun problem occurs in two semantic fami-lies with N-be-that/of and N-be-to as pattern cues(Table 3), and postnominal clauses are not allowedfor this noun.
Although these cues help in filteringsome unwanted cases, we observed a large numberof cases where the shell content is given in post-nominal clauses, as shown in (13).
(13) I was trying to address the problem of un-reliable testimony by experts in capitalcases.Similarly, the Plan family does not allow the N-of pattern.
This cue works well for the shell noundecision from the same family because often thepostnominal of clause is the agent for this shellnoun and not the shell content.
However, it hurtsthe performance of the shell noun policy, as N-of is a common pattern for this shell noun (e.g.,.
.
.
officials in Rwanda have established a policy ofrefusing to protect refugees.
.
.
).
Other failures ofthe algorithm are due to parsing errors and lack ofinclusion of context information.8 Discussion and conclusionIn this paper, we proposed a general method to re-solve shell nouns following cataphoric construc-tions.
This is a first step towards end-to-end shellnoun resolution.
In particular, this method canbe used to create training data for any given shellnoun, which can later be used to resolve harderanaphoric cases of that noun using the method thatwe proposed earlier (Kolhatkar et al., 2013b).The first goal of this work was to point out thedifficulties associated with the resolution of cat-aphoric cases of shell nouns.
The low resolutionresults of the LSC baseline demonstrate the diffi-culties of resolving such cases using syntax alone,suggesting the need for incorporating more lin-guistic knowledge in the resolution.The second goal of this work was to examine towhat extent knowledge derived from the linguis-tics literature helps in resolving shell nouns.
Weconclude that Schmid?s pattern and clausal cuesare useful for resolving nouns with strict syntac-tic expectations (e.g., fact, reason); however, thesecues are defeasible: they miss a number of cases inour corpus.
It is possible to improve on Schmid?scues using crowdsourcing annotation and by ex-ploiting lexico-syntactic patterns associated withdifferent shell nouns from a variety of corpora.One limitation of our approach is that in our res-olution framework, we do not consider the prob-lem of ambiguity of nouns that might not be usedas shell nouns.
The occurrence of nouns with thelexical patterns in Table 1 does not always guaran-tee shell noun usage.
For instance, in our data, weobserved a number of instances of the noun issuewith the publication sense (e.g., this week?s issueof Sports Illustrated).Our algorithm is able to deal with only a re-stricted number of shell noun usage constructions,but the shell content can be expressed in a varietyof other constructions.
A robust machine learningapproach that incorporates context and deeper se-mantics of the sentence, along with Schmid?s cues,could mitigate this limitation.This work opens a number of new research di-rections.
Our next planned task is clustering dif-ferent shell nouns based on the kind of comple-ments they take in different usages similar to verbclustering (Merlo and Stevenson, 2000; Schulte imWalde and Brew, 2002).AcknowledgementsWe thank the anonymous reviewers for their com-ments.
We also thank Suzanne Stevenson, GeraldPenn, Heike Zinsmeister, Kathleen Fraser, AidaNematzadeh, and Ryan Kiros for their feedback.This research was financially supported by theNatural Sciences and Engineering Research Coun-cil of Canada and by the University of Toronto.ReferencesNicholas Asher.
1993.
Reference to Abstract Objectsin Discourse.
Kluwer Academic Publishers, Dor-drecht, Netherlands.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-508ings of the 17th International Conference on Com-putational Linguistics, volume 1 of COLING ?98,pages 86?90, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Douglas Biber, Stig Johansson, Geoffrey Leech, Su-san Conrad, and Edward Finegan.
1999.
LongmanGrammar of Spoken and Written English.
PearsonESL, November.Simon Philip Botley.
2006.
Indirect anaphora: Testingthe limits of corpus-based linguistics.
InternationalJournal of Corpus Linguistics, 11(1):73?112.Donna K. Byron.
2003.
Annotation of pronouns andtheir antecedents: A comparison of two domains.Technical report, University of Rochester.
ComputerScience Department.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Miriam Eckert and Michael Strube.
2000.
Dialogueacts, synchronizing units, and anaphora resolution.Journal of Semantics, 17:51?89.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Charles J. Fillmore.
1985.
Frames and the semantics ofunderstanding.
Quaderni di Semantica, 6(2):222?254.John Flowerdew.
2003.
Signalling nouns in discourse.English for Specific Purposes, 22(4):329?346.Gill Francis.
1994.
Labelling discourse: An aspect ofnominal group lexical cohesion.
In M. Coulthard,editor, Advances in written text analysis, pages 83?101.
Routledge, London.Matthew Gerber, Joyce Chai, and Adam Meyers.
2009.The role of implicit argumentation in nominal srl.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 146?154, Boulder, Colorado, June.Association for Computational Linguistics.Nancy Hedberg, Jeanette K. Gundel, and RonZacharski.
2007.
Directly and indirectly anaphoricdemonstrative and personal pronouns in newspaperarticles.
In Proceedings of DAARC-2007 8th Dis-course Anaphora and Anaphora Resolution Collo-quium, pages 31?36.Eli Hinkel.
2004.
Teaching Academic ESL Writ-ing: Practical Techniques in Vocabulary and Gram-mar (ESL and Applied Linguistics Professional).Lawrence Erlbaum, Mahwah, NJ, London.Rodney D. Huddleston and Geoffrey K. Pullum.
2002.The Cambridge Grammar of the English Language.Cambridge University Press, April.Roz Ivanic.
1991.
Nouns in search of a context: Astudy of nouns with both open- and closed-systemcharacteristics.
International Review of Applied Lin-guistics in Language Teaching, 29:93?114.Varada Kolhatkar, Heike Zinsmeister, and GraemeHirst.
2013a.
Annotating anaphoric shell nounswith their antecedents.
In Proceedings of the 7thLinguistic Annotation Workshop and Interoperabil-ity with Discourse, pages 112?121, Sofia, Bulgaria,August.
Association for Computational Linguistics.Varada Kolhatkar, Heike Zinsmeister, and GraemeHirst.
2013b.
Interpreting anaphoric shell nouns us-ing antecedents of cataphoric shell nouns as trainingdata.
In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing,pages 300?310, Seattle, Washington, USA, October.Association for Computational Linguistics.Paola Merlo and Suzanne Stevenson.
2000.
Automaticverb classification based on statistical distributionsof argument structure.
Computational Linguistics,27(3):373?408.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The nombank project:An interim report.
In In Proceedings of theNAACL/HLT Workshop on Frontiers in Corpus An-notation.Christoph M?ller.
2008.
Fully Automatic Resolution ofIt, This and That in Unrestricted Multi-Party Dialog.Ph.D.
thesis, Universit?t T?bingen.Costanza Navarretta.
2011.
Antecedent and referenttypes of abstract pronominal anaphora.
In Proceed-ings of the Workshop Beyond Semantics: Corpus-based investigations of pragmatic and discoursephenomena, G?ttingen, Germany, Feb.Rebecca J. Passonneau.
1989.
Getting at discourse ref-erents.
In Proceedings of the 27th Annual Meetingof the Association for Computational Linguistics,pages 51?59, Vancouver, British Columbia, Canada.Association for Computational Linguistics.Massimo Poesio and Ron Artstein.
2008.
Anaphoricannotation in the ARRAU corpus.
In Proceedingsof the Sixth International Conference on LanguageResources and Evaluation (LREC?08), Marrakech,Morocco, May.
European Language Resources As-sociation (ELRA).Massimo Poesio, Simone Ponzetto, and Yannick Vers-ley.
2011.
Computational models of anaphora reso-lution: A survey.
Unpublished.Judita Preiss, Ted Briscoe, and Anna Korhonen.
2007.A system for large-scale acquisition of verbal, nom-inal and adjectival subcategorization frames fromcorpora.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 912?919, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.509Hans-J?rg Schmid.
2000.
English Abstract Nouns AsConceptual Shells: From Corpus to Cognition.
Top-ics in English Linguistics 34.
Mouton de Gruyter,Berlin.Sabine Schulte im Walde and Chris Brew.
2002.
In-ducing German semantic verb classes from purelysyntactic subcategorisation information.
In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 223?230,Philadelphia, PA.Leonard Talmy.
2000.
The windowing of attention.In Toward a Cognitive Semantics, volume 1, pages257?309.
The MIT Press.Zeno Vendler.
1968.
Adjectives and Nominalizations.Mouton and Co., The Netherlands.510
