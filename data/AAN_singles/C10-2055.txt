Coling 2010: Poster Volume, pages 481?489,Beijing, August 2010Bilingual lexicon extraction from comparable corpora usingin-domain termsAzniah IsmailDepartment of Computer ScienceUniversity of Yorkazniah@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceUniversity of Yorksuresh@cs.york.ac.ukAbstractMany existing methods for bilinguallexicon learning from comparable corporaare based on similarity of context vectors.These methods suffer from noisy vectorsthat greatly affect their accuracy.
Weintroduce a method for filtering this noiseallowing highly accurate learning ofbilingual lexicons.
Our method is basedon the notion of in-domain terms whichcan be thought of as the most importantcontextually relevant words.
We providea method for identifying such terms.Our evaluation shows that the proposedmethod can learn highly accurate bilin-gual lexicons without using orthographicfeatures or a large initial seed dictionary.In addition, we also introduce a methodfor measuring the similarity betweentwo words in different languages withoutrequiring any initial dictionary.1 IntroductionIn bilingual lexicon extraction, the context-basedapproach introduced by Rapp (1995) is widelyused (Fung, 1995; Diab and Finch, 2000; amongothers).
The focus has been on learning fromcomparable corpora since the late 1990s (Rapp,1999; Koehn and Knight, 2002; among others).However, so far, the accuracy of bilingual lexi-con extraction using comparable corpora is quitepoor especially when orthographic features arenot used.
Moreover, when orthographic featuresare not used, a large initial seed dictionary is es-sential in order to acquire higher accuracy lexicon(Koehn and Knight, 2002).
This means that cur-rent methods are not suitable when the languagepairs are not closely related or when a large initialseed dictionary is unavailable.When learning from comparable corpora, alarge initial seed dictionary does not necessarilyguarantee higher accuracy since the source andtarget texts are poorly correlated.
Thus, inducinghighly accurate bilingual lexicon from compara-ble corpora has so far been an open problem.In this paper, we present a method that is ableto improve the accuracy significantly without re-quiring a large initial bilingual dictionary.
Ourapproach is based on utilising highly associatedterms in the context vector of a source word.For example, the source word powers is highlyassociated with the context word delegation.
Wenote that, firstly, both share context terms such asparliament and affairs.
And, secondly, the trans-lation equivalents of powers and delegation in thetarget language are not only highly associated butthey also share context terms that are the trans-lation equivalents of parliament and affairs (seeFigure 1).2 Related workMost of the early work in bilingual lexicon ex-traction employ an initial seed dictionary.
A largebilingual lexicon with 10k to 20k entries is neces-sary (Fung, 1995; Rapp, 1999).Koehn and Knight (2002) introduce techniquesfor constructing the initial seed dictionary auto-matically.
Their method is based on using identi-cal spelling features.
The accuracy of such initialbilingual lexicon is almost 90.0 percent and canbe increased by restricting the word length (Koehnand Knight, 2002).
Koehn and Knight found ap-proximately 1000 identical words in their German481Figure 1: An example of in-domain terms that co-occur in English and Spanish.
The source word ispowers and the target word is poderes.
The word delegation and delegacion are the highly associatedwords with the source word and the target word respectively.
Their in-domain terms, as shown in themiddle, can be used to map the source word in context of word delegation to its corresponding targetword in context of delegacion.and English monolingual corpora.
They expandedthe lexicon with the standard context-based ap-proach and achieved about 25.0 percent accuracy(Koehn and Knight, 2002).Similar techniques were used in Haghighi etal.
(2008) who employ dimension reduction inthe extraction method.
They recorded 58.0 per-cent as their best F1 score for the context vec-tor approach on non-parallel comparable corporacontaining Wikipedia articles.
However, theirmethod scores less on comparable corpora con-taining distinct sentences derived from the Eu-roparl English-Spanish corpus.3 Learning in-domain termsIn the standard context vector approach, we as-sociate each source word and target word withtheir context vectors.
The source and target con-text vectors are then compared using the initialseed dictionary and a similarity measure.
Learn-ing from comparable corpora is particularly prob-lematic due to data sparsity, as important contextterms may not occur in the training corpora whilesome may occur but with low frequency and canbe missed.
Some limitations may also be due tothe size of the initial seed dictionary being small.The initial seed dictionary can also contributeirrelevant or less relevant features that can mis-lead the similarity measure especially when thenumber of dimensions is large.
The approach weadopt attempts to overcome this problem.In Figure 1, for the source word powers, dele-gation is the highly associated word.
Both powersand delegation share common contextual termssuch as parliament and affairs.
Now the transla-tion equivalent of delegation is delegacion.
Forthe potential translation equivalent poderes, wesee that the common contextual terms shared bypowers and poderes are terms parlamento (par-liament) and asuntos (affairs).482Figure 2: An example of English-Spanish lexicon learnt for the source word powers.
On the top,the system suggested competencias and rejected poderes when powers is associated with community,democracy or independence.
The word poderes is suggested when powers is associated with justice ordelegation.We observe that these common contextualterms are simultaneously the first-order andsecond-order context terms of the target word.They are the shared context terms of the targetword and its highly associated context term.
Wedefine these terms as in-domain terms.
These in-domain terms can be used to map words to theircorresponding translations.
The highly associatedcontext terms can be thought of as sense discrim-inators that differentiate the different uses of thetarget word.
In Figure 2, we show how delegationhelps in selecting between the ?control or influ-ence?
sense of powers while rejecting the ?abilityor skill?
sense.In this paper, our focus is not on sense disam-biguation and we follow current evaluation meth-ods for bilingual lexicon extraction.
However, it isclear that our method can be adapted for buildingsense disambiguated bilingual lexicons.3.1 Identifying highly associated wordsTo identify the context terms CT (WS) of a sourceword WS , as in (Rapp, 1999), we use log-likelihood ratio (LL) Dunning (1993).
We chooseall words with LL > t1 where t1 is a threshold.The highly associated words then are the top khighest ranked context terms.
In our experiments,we only choose the top 100 highest ranked contextterms as our highly associated terms.In order to compute the log-likelihood ratio oftarget word a to co-occur with context word b, wecreate a contingency table.
The contingency tablecontains the observed values taken from a givencorpus.
An example of the contingency table isshown in Table 1.C[i,j] community ?
communitypowers 124 1831 1955 C(powers)?
powers 11779 460218 471997 C(?
powers)11903 462049C(community) C(?
community)Here C[i, j] denotes the count of the number of sentences inwhich i co-occurs with j.Total corpus size: N = 473952 in the aboveTable 1: Contingency table for observed values oftarget word powers and context word community.The LL value of a target word a and contextword b is given by:LL(a, b) =?i?{a,?a},j?
{b,?b}2C(i, j) log C(i, j)NC(i) C(j)3.1.1 Identifying in-domain termsIn our work, to find the translation equivalent of asource word WS , we do not use the context termsCT (WS).
Instead, we use the in-domain termsIDT (WS ,WR).
For each highly associated term483WR, we get different in-domain terms.
Further-more, IDT (WS ,WR) is a subset of CT (WS).The in-domain terms of WS given the contextterms WR is given by:ID(WS ,WR) = CT (WS) ?
CT (WR)Programme and public are some of the examplesof in-domain terms of powers given community asthe highly associated term.3.1.2 Finding translations pairsNote that ID(WS ,WR) is an in-domain termvector in the source language.
Let WT be apotential translation equivalent for WS .
Let,tr(WR) be a translation equivalent for WR.
LetID(WT , tr(WR)) be an in-domain term vector inthe target language.We use tr(WS |WR) to denote the translationproposed for WS given the highly associated termWR.
We compute tr(WS |WR) using:tr(WS |WR) =argmaxWTsim(ID(WS ,WR), ID(WT , tr(WR)))Our method learns translation pairs that areconditioned on highly associated words (WR).
Ta-ble 2 provides a sample of English-Spanish lexi-con learnt for the word power with different WR.English SpanishWS WR tr(WR) WTSimpowerscompetencias 0.9876poderes 0.9744community comunidadindependiente 0.9501competencias 0.9948poderes 0.9915democracy democraciaindependiente 0.9483competencias 0.9939poderes 0.9745independence independenciaindependiente 0.9633poderes 0.9922competencias 0.3450justice justiciaindependiente 0.9296poderes 0.9568competencias 0.9266delegation delegacionindependiente 0.8408Table 2: A sample of translation equivalents learntfor powers.In the next section, we introduce a similaritymeasure that operates on the context vectors in thesource language and the target language withoutrequiring a seed dictionary.4 Rank-binning similarity measureMost existing methods for computing similaritycannot be directly employed for measuring thesimilarity between in-domain term context vec-tors since each context vector is in a different lan-guage.
A bilingual dictionary can be assumedbut that greatly diminishes the practicality of themethod.We address this by making an assumption.
Weassume that the relative distributions of in-domaincontext terms of translation equivalent pairs areroughly comparable in the source language andin the target language.
For example, considerthe log-likelihood values of the in-domain termsfor the translation pair agreement-acuerdo (condi-tioned on the highly associated term association-associacion) given in Figure 3.
We note that thedistribution of in-domain terms are comparable al-though not identical.
Thus, the distribution can beused as a clue to derive translation pairs but weneed a method to compute similarity of the vectorof in-domain terms.Rank-binning or rank histograms are usuallyused as a diagnostic tool to evaluate the spread ofan ensemble rather than as a verification method.Wong (2009) use the method of rank-binning toroughly examine performance of a system onlearning lightweight ontologies.
We apply therank-binning procedure for measuring the similar-ity of word pairs.Pre-processing step:1.
Let WS be a source language word andx1, x2, ..., xn be the set of n context termsranked in descending log-likelihood valuesof WS (see Table 3).2.
We transform the rank values of contextterms xk into the range [0,1] using:zk =rank(xk)?
1n?
1484Figure 3: Similar distribution of in-domain termsfor agreement with association and acuerdo withasociacion.Binning procedureWe divide the interval [0, 1] into g bins1 of equallength.
Let b1, .
.
.
, bg denote the g bins.
Thenwe map the in-domain terms vector ID(WS ,WR)into the binned vector b1, .
.
.
, bg.
For each xk ?ID(WS ,WR), this mapping is done by using thecorresponding zk from the pre-processing step.For each bin, we count the number of different in-domain terms that are mapped into this bin.
Thus,if the range of the first bin b1 is [0, 0.009] then eu-ropean, legislative, parliament are mapped into b1i.e.
b1 = 3.
The bins are normalised by dividingwith | ID(WS ,WR) |.Rank binning similarityWe use Euclidean distance to compute similaritybetween bins.
Given, bins P = p1, .
.
.
, pg andQ = q1, .
.
.
, qg, the Euclidean distance is givenby:dist(P,Q) =???
?g?i=1(pi, qi)21We used the following formula to estimate the numberof bins:g = 1 + 3.3 ?
log (| ID(WS ,WR) |)CT (powers)Context term LL rank zkeuropean 491.33 1 0.00000legislative 482.19 2 0.00406parliament 408.26 3 0.00813: : : :: : : :: : : :public 16.96 245 0.99186programme 15.40 246 0.99593representatives 15.32 247 1.00000n = 247Table 3: Some examples of transformed values ofeach term in CT (powers).In the next section, we describe the setup in-cluding the data, the lexicon and the evaluationused in our experiments.5 Experimental setup5.1 DataFor comparable text, we derive English and Span-ish distinct sentences from the Europarl parallelcorpora.
We split the corpora into three parts ac-cording to year.
We used about 500k sentencesfor each language in the experiments.
This ap-proach is further explained in Ismail and Man-andhar (2009) and is similar to Koehn and Knight(2001) and Haghighi et al (2008).5.2 Pre-processingFor corpus pre-processing, we use sentenceboundary detection and tokenization on the rawtext before we clean the tags and filter stop words.We sort and rank words in the text according totheir frequencies.
For each of these words, wecompute their context term log-likelihood values.5.3 LexiconIn the experiment, a bilingual lexicon is requiredfor evaluation.
We extract our evaluation lexiconfrom the Word Reference2 free online dictio-nary.
This extracted bilingual lexicon has low cov-erage.2http://wordreference.com4855.4 EvaluationIn the experiments, we considered the task ofbuilding a bilingual English-Spanish lexicon be-tween the 2000 high frequency source and targetwords, where we required each individual wordto have at least a hundred highly associated con-text terms that are not part of the initial seed dic-tionary.
Different highly associated WR termsfor a given WT might derive similar (WS ,WT )pairs.
In this case, we only considered one ofthe (WS ,WT ) pairs.
In future work, we wouldlike to keep these for word sense discriminationpurposes.
Note that we only considered proposedtranslation pairs whose similarity values are abovea threshold t2.We used the F1 measure to evaluate the pro-posed lexicon against the evaluation lexicon.
Ifeither WS or WT in the proposed translation pairsis not in the evaluation lexicon, we considered thetranslation pairs as unknown, although the pro-posed translation pairs are correct.
Recall is de-fined as the proportion of the proposed lexicon di-vided by the size of the lexicon and precision isgiven by the number of correct translation pairs ata certain recall value.6 ExperimentsIn this section, we look into how the in-domaincontext vectors affect system performance.
Wealso examine the potential of rank-binning simi-larity measure.6.1 From standard context vector toin-domain context vectorMost research in bilingual lexicon extraction sofar has employed the standard context vector ap-proach.
In order to explore the potential of thein-domain context vectors, we compare the sys-tems that use in-domain approach against systemsthat use the standard approach.
We also employdifferent sets of seed lexicon in each system to beused in the similarity measure:?
Lex700: contains 700 cognate pairs from afew Learning Spanish Cognate websites3.3such as http://www.colorincolorado.organd http://www.language-learning-advisor.com?
Lex100: contains 100 bilingual entries of themost frequent words in the source corpus thathave translation equivalents in the extractedevaluation lexicon.
We select the top onehundred words in the source corpus, so thattheir translation equivalents is within the first2000 high frequency words in the target cor-pus.?
Lex160: contains words with similar spellingthat occur in both corpora.
We used 160word pairs with an edit distance value lessthan 2, where each word is longer than 4characters.Models using the standard approach are de-noted according to the size of the particular lex-icon used in their context similarity measure,i.e.
CV-100 for using Lex100, CV-160 for usingLex160 and CV-700 for using Lex700.
We use IDTto denote our model.
We use lexicon sizes to dis-tinguish the different variants, e.g.
IDT-CV100 forusing Lex100, IDT-CV160 for using Lex160 andIDT-CV700 for using Lex700.With CV-700, the system achieved 52.6 per-cent of the best F1 score.
Using the same seeddictionary, the best F1 score has increased about20 percent points with IDT-CV700 recorded 73.1percent.
IDT-CV100 recorded about 15.0 percenthigher best F1 score than CV-100 with 80.9 and66.4 percent respectively.
Using an automaticallyderived seed dictionary, IDT-CV160 yielded 70.0percent of best F1 score while CV-160 achieved62.4 percent.
Results in Table 4 shows variousprecisions px at recall values x.Model P0.10 P0.25 P0.33 P0.50 BestF1scoreCV-700 58.3 61.2 64.8 55.2 52.6CV-100 52.0 53.0 47.2 44.8 66.4CV-160 68.5 56.8 48.8 48.8 62.4IDT-CV700 83.3 90.2 82.0 66.7 73.1IDT-CV100 80.0 75.8 66.7 69.4 80.9IDT-CV160 90.0 80.6 73.9 69.2 70.0Table 4: Performance of different models.4866.2 Similarity measure using rank-binningWe use RB to denote our model based on therank-binning approach.
Running RB means thatno seed dictionary is involved in the similaritymeasure.
We also ran the similarity measure inthe IDT (IDT-RB160) by employing the derivedLex160 for the in-domain steps.We ran several tests using IDT-RB160 with dif-ferent numbers of bins.
The results are illustratedin Figure 4.
The IDT-RB160 yielded 63.7 percentof best F1 score with 4 bins.
However, the F1score starts to drop from 61.1 to 53.0 percent with6 and 8 bins respectively.
With 3 and 2 bins theIDT-RB160 yielded 63.7 and 62.0 percent of bestF1 score respectively.
Using 1 bin is not be pos-sible as all values fall under one bin.
Thus, therank-binning similarity measure for the rest of theexperiments where RB is mentioned, refers to a 4bins setting.Figure 4: Performance of IDT-RB160 with differ-ent numbers of bins.While systems using the standard context simi-larity measure yielded scores higher than 50.0 per-cent of best F1, the RB achieved only 39.2 per-cent.
However, RB does not employ an initialdictionary and does not use orthographic features.As mentioned above, the system scored higherwhen the similarity measure was used in the IDT(i.e.
IDT-RB160).
Note that Lex160 is derived au-tomatically so the approach can also be consid-ered as unsupervised.
The system performanceis slightly lower compared to the conventionalCV-160.
However, IDT-CV160 outperforms bothof the systems (see Figure 5).Figure 5: Performance of different unsupervisedmodels.Overall, systems that exploit in-domain termsyielded higher F1 scores compared to the conven-tional context vector approach.6.3 Comparison with CCAPrevious work in extracting bilingual lexiconsfrom comparable corpora generally employ theconventional context vector approach.
Haghighiet al (2008) focused on applying canonical cor-relation analysis (CCA), a dimension reductiontechnique, to improve the method.
They were us-ing smaller comparable corpora, taken from thefirst 50k sentences of English Europarl and thesecond 50k of Spanish Europarl, and different ini-tial seed dictionary.
Hence, we tested CCA in ourexperimental setup.
In CV-700 setting, using CCAyields 57.5 percent of the best F1 score comparedto 73.1 percent of the best F1 score with IDT thatwe reported in Section 6.2.7 Discussion7.1 Potential of in-domain termsOur experiments clearly demonstrate that the useof in-domain terms achieves higher F1 scorescompared to conventional methods.
It also showsthat our method improves upon earlier reporteddimension reduction methods.
From our obser-vation, the number of incorrect translation pairs487were further reduced when the context terms werefiltered.
Recall that the in-domain terms in thetarget language were actually the shared contextterms of the target word and its highly associ-ated context terms.
Nevertheless, this approachactually depends on the initial bilingual lexiconin order to translate those highly associated con-text terms into the source language.
Table 5shows some examples of most confidence trans-lation pairs proposed by the IDT-CV100.English Spanish Sim score Correct?principle principio 0.9999 Yesgovernment estado 0.9999 Nogovernment gobierno 0.9999 Yesresources recursos 0.9999 Yesdifficult dificil 0.9999 Yessector competencia 0.9998 Nosector sector 0.9998 Yesprogramme programa 0.9998 Yesprogramme comunidad 0.9998 Noagreement acuerdo 0.9998 YesTable 5: Some examples of most confident trans-lation pairs proposed by IDT-CV100 ranked bysimilarity scores.7.2 Seed dictionary variationThe initial seed dictionary plays a major role inextracting bilingual lexicon from comparable cor-pora.
There are a few different ways for us toderive a seed dictionary.
Recall that Lex700 andLex100, that are used in the experiments, are de-rived using different methods.
The F1 scores ofthe system using Lex100 were much higher com-pared to the system using Lex700.
Thus, extend-ing Lex100 with additional high frequency wordsmay provide higher accuracy.One important reason is that all bilingual en-tries in Lex100 occur frequently in the corpora.Although the size of Lex700 is larger, it is not sur-prising that most of the words never occur in thecorpora, such as volleyball and romantic.
How-ever, using Lex160 is more interesting since it isderived automatically from the corpora, thoughone should realize that the relationship betweenthe language pair used in the respective mono-lingual corpora, English and Spanish, may havelargely affect the results.
Thus, for other sys-tems involving unrelated language pairs, the rank-binning similarity measure might be a good op-tion.7.3 Word sense discrimination abilityAs mentioned in Section 5.4, each source wordmay have more than one highly associated contextterm, WR.
Different WR may suggest differenttarget words for the same source word.
For exam-ple, given the source word powers and the highlyassociated word community, competencias is pro-posed as the best translation equivalent.
On theother hand, for same source word powers, whenthe highly associated word is delegation, the tar-get word poderes is suggested.8 ConclusionWe have developed a method to improve the F1score in extracting bilingual lexicon from compa-rable corpora by exploiting in-domain terms.
Thismethod also performs well without using an ini-tial seed dictionary.
More interestingly, our workreveals the potential of building word sense dis-ambiguated lexicons.ReferencesAria Haghighi, Percy Liang, Taylor Berg-Kirkpatrickand Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In ACL 2008, Colum-bus, Ohio.Azniah Ismail and Suresh Manandhar.
2009.
Utiliz-ing contextually relevant terms in bilingual lexiconextraction In Workshop on Unsupervised and Min-imally Supervised Learning of Lexical Semantics,Boulder, Colorado.Mona Diab and Steve Finch.
2000.
A statistical word-level translation model for comparable corpora.
InProceedings of the Conference on Content-basedmultimedia information access (RIAO).Pascale Fung.
1995.
Compiling bilingual lexicon en-tries from a non-parallel English-Chinese corpus.
InProceedings of the 3rd Annual Workshop on VeryLarge Corpora, Boston, Massachusetts, 173-183.Philipp Koehn and Kevin Knight.
2001.
Knowledgesources for word-level translation models.
In Pro-ceedings of the Conference on empirical method innatural language processing (EMNLP).488Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL 2002, Philadelphia, USA,9-16.Reinhard Rapp.
1995.
Identifying word translationsin non-parallel texts.
In Proceedings of the ACL 33,320-322.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the ACL 37, 519-526.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistic, volume 19(1), 61-74.Wilson Yiksen Wong.
2009.
Learning lightweight on-tologies from text across different domains using theweb as background knowledge.
Ph.D. Thesis.
Uni-versity of Western Australia489
