Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 483?491,Beijing, August 2010Enhancing Cross Document Coreference of Web Documentswith Context Similarity and Very Large Scale Text CategorizationJian HuangInformation Sciences and TechnologyPennsylvania State Universityjhuang@ist.psu.eduPucktada TreeratpitukInformation Sciences and TechnologyPennsylvania State Universitypxt162@ist.psu.eduSarah M. TaylorLockheed Martin IS&GSsarah.m.taylor@lmco.comC.
Lee GilesInformation Sciences and TechnologyPennsylvania State Universitygiles@ist.psu.eduAbstractCross Document Coreference (CDC) isthe task of constructing the coreferencechain for mentions of a person across a setof documents.
This work offers a holisticview of using document-level categories,sub-document level context and extractedentities and relations for the CDC task.We train a categorization component withan efficient flat algorithm using thousandsof ODP categories and over a million webdocuments.
We propose to use ranked cat-egories as coreference information, partic-ularly suitable for web documents that arewidely different in style and content.
Anensemble composite coreference function,amenable to inactive features, combinesthese three levels of evidence for disam-biguation.A thorough feature importance study isconducted to analyze how these threecomponents contribute to the coreferenceresults.
The overall solution is evaluatedusing the WePS benchmark data anddemonstrate superior performance.1 IntroductionCross Document Coreference (CDC) is the taskto determine whether Named Entities (NE) fromdifferent documents refer to the same underlyingidentity.
CDC enables a range of advanced NLPapplications such as automated text summariza-tion and question answering (e.g.
list-type ques-tions).
CDC has mainly been developed from twoperspectives.First, in the Message Understanding Confer-ence (MUC-6), CDC was viewed as an advancedtask performed based on a set of InformationExtraction (IE) artifacts.
IE has been one of thecentral topics in NLP since the 1970s and gainedmuch success in transforming natural languagetext to structured text.
IE on the Web, however,is inherently very challenging.
For one, the Webis comprised of such heterogenous content thatIE systems, many of which are developed ontidy and domain-specific corpora, may achieverelatively limited coverage.
Also, the content ofweb documents may not even be in the naturallanguage form.
Hence, though IE based featuresare quite precise, it is rather difficult to achievegood coverage that?s necessary to disambiguateperson entities on the Web.Recently, there is significant research interest ina related task called Web Person Search (WePS)(Artiles et al, 2007), which seeks to determinewhether two documents refer to the same persongiven a person name search query.
Many systemsemployed the simple vector space model and wordco-occurrence features for this task.
Though morerobust with better coverage, these methods aremore susceptible to irrelevant words with regardto the entity of interest.Rather than relying solely on IE based or wordco-occurrence features, this work adopts a holisticview of the different types of features useful forcross document coreference.
Specifically, themain features of our proposed CDC approach are:483?
The proposed approach covers the entirespectrum of document level, sub-documentcontext level and entity/relation leveldisambiguation evidence.
In particular,we propose to use document categoriesas robust document level evidence.
Thiscomprehensive design naturally combinesstate-of-the-art categorization, informationextraction and IE-driven IR methods andcompensates the limitation of each of them.?
The features used in this work are domain in-dependent and thus are particularly suitablefor coreferencing web documents.?
The composite pairwise coreference functionin this work can readily incorporate a setof heterogenous features that are not alwaysactive or are in different ranges, makingit easily extensible to additional features.Moreover, we thoroughly study the contri-bution of each component and its featuresto gain insight on improving cross documentcoreference performance.In this work, three components specialize ingenerating the aforementioned three levels of fea-tures as coreference decisions.
Thus we refer tothem as experts.
After reviewing prior work onCDC, we describe the methods of each of thesecomponents in detail and present empirical resultswhere appropriate.
We then show how thesecomponents (and its features) are aggregated topredict pairwise coreference using an ensemblemethod.
We evaluate the contribution of eachcomponent and the overall CDC results on abenchmark dataset.
Finally, we conclude anddiscuss future work.2 Related WorkCompared to the traditional (within-document)coreference resolution problem, cross documentcoreference is a much harder problem due to thedivergence of contents and the lack of consistentdiscourse information across documents.
(Bagga and Baldwin, 1998b) presented one ofthe first CDC systems, which relied solely on thecontextual words of the named entities.
(Gooiand Allan, 2004) used a 55-word window asthe context without significant accuracy penalty.As these approaches only considered word co-occurrence, they were more susceptible to genredifferences.
Recent CDC work has sought Infor-mation Extraction (IE) support.
Extracted NEsand relationships were considered in (Niu et al,2004) for improved CDC performance.Many of these earlier CDC methods wereevaluated on small and tidy news articles.
CDCfor Web documents is even more challenging.
(Wan et al, 2005) proposed a web personresolution system called WebHawk, whichextracted several attributes such as title,organization, email and phone number usingpatterns.
These features however only coveredsmall amount of disambiguation evidence andcertain types of web pages (such as personalhome pages).
The more recent Web PersonSearch (WePS) task (Artiles et al, 2007) hascreated a benchmark dataset which is also usedin this work.
Different from CDC which aims toresolve mention level NEs, WePS distinguishesdocuments retrieved by a name search queryaccording to the underlying identity.
The top-performing system (Chen and Martin, 2007)in this task extracted phrasal contextual anddocument-level entities as rich features forcoreference.
Similar IR features are also used byother WePS systems as they are more robust tothe variety of web pages (Artiles et al, 2007).Instead of focusing on local information, (Liet al, 2004) proposed a generative model ofentity co-occurrence to capture global documentlevel information.
However, inference in gen-erative models is expensive for large scale webdata.
Our work instead considers document cat-egories/topics that can be efficiently predictedand easily interpretable by users.
Hand-tunedweights were used in (Baron and Freedman, 2008)and a linear classifier was used in (Li et al,2004) to combine the extracted features.
Ourcomposite pairwise coreference function is basedon an ensemble classifier and is more robust andcapable of handling inactive features.3 Text Categorization Aided CDCConsider the following scenario for motivation.When a user searches for ?Michael Jordan?,the official web page of the basketball player484?Michael Jordan?1 contains mostly his careerstatistics, whereas the homepage of ?MichaelI.
Jordan?
the professor2 contains his titles,contact information and advising students.Neither of these pages contain complete naturallanguage sentences that most IE and NLP toolsare designed to process.
We propose to usedocument categories (trained from a very largescale and general purpose taxonomy, OpenDirectory Project (ODP)) as document levelfeatures for CDC.
In this example, one can easilydifferentiate these namesakes by categorizing theformer as ?Top/Sports/Basketball/Professional?and the latter as ?Top/Computer/ArtificialIntelligence/Machine Learning?.
We firstintroduce the method to categorize Webdocuments; then we show how to combinethese categories for coreferencing.3.1 Very Large Scale Text CategorizationTo handle the web CDC problem, the catago-rization component needs to be able to catego-rize documents of widely different topics.
TheOpen Directory Project (ODP), the largest andmost comprehensive human edited directory ofthe Web3, contains hundreds of thousands ofcategories labeled for 2 million Web pages.
Lever-aging this vast amount of web data and the largeWeb taxonomy has called for the development ofvery efficient text categorization methods.
Thereis significant research interest in scaling up tocategorize millions of pages to thousands of cat-egories and beyond, called the many class classi-fication setting (Madani and Huang, 2008).
Flatclassification methods (e.g.
(Crammer et al,2006; Madani and Huang, 2008)), which treathierarchical categories as flat classes, have beenvery successful due to their superior scalabilityand simplicity compared to classical hierarchicalone-against-rest categorization.
Flat methods alsoachieve high accuracy that is on par with, or betterthan, the traditional counterparts.We adopt a flat multiclass online classificationalgorithm Passive Aggressive (PA) (Crammer etal., 2006) to predict ranked categories for web1See www.nba.com/playerfile/michael jordan/index.html2See www.eecs.berkeley.edu/?jordan/3See http://www.dmoz.org/about.html for details.documents.
For a categorization problem with Ccategories, PA associates each category k with aweight vector wk, called its prototype.
The degreeof confidence for predicting category k with re-spect to an instance x4 (both in online training andtesting) is determined by the similarity betweenthe instance and the prototype ?
the inner productwk ?
x. PA predicts a ranked list of categoriesaccording to this confidence.PA is a family of online and large-margin basedclassifiers.
Given an instance (xt, yt) duringonline learning, the multiclass margin marg inPA5 is the difference between the score of the truecategory yt and that of the highest ranked falsepositive category s, i.e.marg = wyt ?
xt ?ws ?
xt (1)where s = argmaxs 6=yt ws ?
xt.A positive margin value indicates that the algo-rithm makes a correct prediction.
One is howevernot only satisfied with a positive margin value, butalso seeks to achieve a margin value of at least1.
When this is not satisfied, the online algorithmsuffers a multiclass hinge loss:Lmc(w; (xt, yt)) ={0 marg ?
11?marg otherwisewhere w = (w1, ..,wC) denotes the concatena-tion of the C prototypes (into a vector).In an online learning step, the PA-II variantupdates the category prototype with the solutionof this constrained optimization problem,wt+1 = argminw12 ?w ?wt?2 +A?2 (2)s.t.
Lmc(w; (xt, yt)) ?
?.
(3)Essentially, if the margin is met (also imply-ing no misclassification), PA passively acceptsthe current solution.
Otherwise, PA aggressivelylearns the new prototype which satisfies the lossconstraint and stays as close to the one previouslylearned as possible.
To cope with label noise, PA-II introduces a slack variable ?
in the optimization4x is the vector representation of word frequencies of thecorresponding document, L2 normalized.5For brevity of presentation, we consider the single labelmulticlass categorization setting.485for a gentler update, a technique previously em-ployed to derive soft-margin classifiers (Vapnik,1998).
A is a parameter that controls the aggres-siveness of the update.The solution to the above optimization problemamounts to only changing the two prototypesviolating the margin in the update step:wytt+1 = wytt + ?xt wst+1 = wst ?
?xtwhere ?
= Lmc?xt?2+ 12A .To conclude, PA treats the hierarchy as flat cat-egories for multiclass classification.
It is similarto Multiclass Perceptron (Crammer and Singer,2003) but only updates two vectors per iterationand thus is more efficient.3.2 Categories as Coreference EvidenceConceptually, the text categorization componentcan be viewed as a function that maps a documentd to a ranked list of top K categories along withtheir respective confidence scores, i.e.?
(d) = {< c1, s1 >, .., < cK , sK >}We leverage these document categories to mea-sure the pairwise similarity of any two docu-ments, sim(?
(du), ?
(dv)), for entity disambigua-tion.
Given a taxonomy T , we first formallydefine the affinity between a category c and oneof its ancestor category c?
in T as:affinity(c; c?)
= 1?
len(c, c?
)depth(T )where len is the length of the shortest path be-tween the two categories and depth(T) denotes thedepth of the taxonomy.
In other words, affinity isthe complementary of the normalized path lengthbetween c and its ancestor c?.Using graph theory terminology, LCA(c1, c2)denote the lowest common ancestor of two cate-gories c1 and c2 in T .
Given two category lists,?
(du) = {< cu1 , su1 >, .., < cuK , suK >} and?
(dv) = {< cv1, sv1 >, .., < cvK , svK >}, we usethe LCA(cui , cvj ) of each category pair cui and cvjas the basis to measure similarity.
Formally, wetransform ?
(du) to a K ?K dimensional vector:~v(du) = [affinity(cui ;LCA(cui , cvj )) ?
sui ]T (4)where i, j = 1..K. In other words, we project?
(du) into a vector in the space spanned by theLCAs of category pairs.
Using the same bases,we can derive ~v(dv) analogically.With this transformation, ?
(du) and ?
(dv)are expressed in the common bases, i.e.
theirLCAs.
Therefore, the similarity between the topK categories of two documents can be measuredby the inner product of these two vectors:sim(?
(du), ?
(dv)) = ~v(du) ?
~v(dv) (5)3.3 Empirical StudiesTo handle the diverse topics of Web documents,we leverage the ODP data to train the many classcategorization algorithm.
The public ODP datacontains 361,621 categories and links to over 2million pages.
We crawled the original web pagesfrom these links, which yielded 1.9 million pages(50GB in size).
The taxonomy was condensed todepth three6 and then very rare categories (havingless than 5 instances) were discarded.
The dataset is created with these categories and the vectorrepresentation of the term weights of the extractedraw text.
This dataset has 1,889,683 instances and4,891 categories in total.
Finally, stratified 80-20 split was performed on this dataset, i.e.
1.5Mpages for training and 377K pages for testing.Figure 1: Categorization performance at differentpositions in the ODP test set.As we view the taxonomy as a set of flatcategories and we are interested in the top Kcategories, we use the recall at K metric for eval-uation.
Recall at K is defined as the percentageof instances having their true category ranked6The original taxonomy has average depth 7, which istoo deep for the coreference purpose in this work and manycategories have too few instances for training.486among the top K slots in the category list.
Fora single label dataset (most ODP pages have onecategory) and K = 1, this is the accuracy metricin multiclass classification.
Note that in the manyclass setting, recall at 1 is a very strict metricas no credit is given for predicting the parent,children or sibling categories; also, documentsmay have valid secondary topics not labeled byhumans.
Figure 1 shows recall at K in the testset.
We observe that the algorithm is able topredict the category for 58.7% of the instancesin the first rank and more than 77% in top three.There is only diminishing gains when we considerthe categories further down the list.
Hence wechoose to use the similarity of the top 1 and top3 categories (named TC1 and TC3, respectively)and study their contributions for the CDC task.3.4 RemarksIn this section, the entire document in the rep-resentation of its categories is used as a unitof analysis for CDC.
Categorization based CDCworks best with namesakes appearing in docu-ments of relatively heterogenous topics, whichis usually the case for web documents.
Indeed,experienced web searchers would add terms suchas ?baseball player?
to the name search queries formore relevant results; Wikipedia also (manually)disambiguates namesakes by their professions.Categorization can also be adopted as a robustfaceted search system for handling name searchqueries: users select the interested category/facetto efficiently disambiguate and filter out irrelevantresults.
The majority of web persons can bereadily distinguished by the different underlyingcategories of the documents where they appear.For more homogeneous corpora or less benevolentcases, the next sections introduce two comple-mentary CDC strategies.4 Information Extraction for CDCConsider the following two snippets retrievedwith regard to the query ?George Bush?
:[Snippet 1]: ?George W. Bush and Bill Clintonare trying to get Congress to allow Haiti to triplethe number of exports ...?
[Snippet 2]: ?George H. W. Bush succeededReagan as the 41st U.S.
President.
?Using categories alone in this case is insuffi-cient as both will be assigned similar categoriessuch as ?Politics?
or ?History/U.S.?.
Also, it?s notuncommon for these entities to co-occur in thesame document and thus making them even moreconfounding.
Properly disambiguating these twomentions requires the usage of local informa-tion: for instance, the extraction of full names,the detection of co-occurring NEs and contextualinformation.
We introduce an IE system thatextracts precise disambiguation evidence in thissection and describe using the extraction contextas additional information in the next section.Our CDC system leverages a state-of-the-artcommercial IE system AeroText (Taylor, 2004).The IE system employs manually created knowl-edge bases with statistically trained models toextract named entities, detect, classify and linkrelations between NEs.
A summary of the mostimportant IE-based features that we use are listedin Table 1.
Based on the extracted attributes andrelations, we further define their pairwise simi-larity used as coreference features.
This rangesfrom simple compatibility checking for ?gender?,textual soft matching for ?names?, to sophisticatedsemantic matching for ?mentions?
and ?locations?using WordNet.
(Huang et al, 2009) providesmore detailed discussions on the development ofthese IE based coreference features.We note that several existing state-of-the-artIE systems are also capable of extracting thesefeatures.
In particular, Named Entity Recognition(NER) which focuses on a small set of predefinedcategories of named entities (e.g.
persons, orga-nization, location) as well as the detection andtracking of preselected relations have achievedvenerable empirical success in practice7.
Also,within document coreference is a mature andwell-studied technology in NLP (e.g.
(Ng andCardie, 2002)).
Therefore, our CDC system canreadily adopt alternative IE toolkits.5 Context MatchingAs mentioned earlier, achieving high extractionaccuracy and coverage for diverse web documents7The Automatic Content Extraction (ACE) evaluationand the Text Analysis Conference (TAC) also have IE-basedentity tracking tasks that are relevant to this component.487is still a challenging and open research problemeven for the state-of-the-art IE systems.
We notethat one of the natural outcomes from extraction isthe context of the NE of interest, which covers theNE with its surrounding text.
For a specific NE,our CDC system uses the context built from thesentences which form the NE?s within documentcoreference chain.
The context is then representedas a term vector whose terms are weighted by theTF-IDF weighing scheme.
For a pair of NEs, thecontext matching component measures the cosinesimilarity of their context term vectors.Essentially, this component alone is similar tothe method presented in the seminal CDC workin (Bagga and Baldwin, 1998b).
We however notethat simply applying a predetermined threshold onthe context similarity for CDC as in this earlierwork is not sufficient.
First, this method narrowlyfocuses on the local word occurrence and maymiss the big picture, i.e.
the correlation that existsin the global scope of a document.
Also, mereword occurrence is incapable of accounting for thevariation of word choices or placing special em-phases on evidence such as co-occurring namedentities, relations, etc.
The categorization and IEcomponents presented earlier in this work over-come these two pitfalls of the simple IR-basedapproach.
We will further showcase the advantageof our comprehensive approach in section 7.2.6 Composite Pairwise CoreferenceIn the previous sections, we describe the com-ponents to obtain document, sub-document andentity level disambiguation evidence in detail.
Inthis section, we propose to use Random Forest(RF) to combine the experts components into onesingle composite pairwise similarity score.
RF isan ensemble classifier, composed of a collectionof randomized decision trees (Breiman, 2001).Each randomized tree is built on a different boot-strap sample of the training data.
Randomness isalso introduced into the tree construction process:the variable selection for each split is conductednot on the entire feature set, but from a smallrandom subset of features.
Gini index is used asthe criteria in selecting the best split.
Additionally,each tree is unpruned, to keep the predictionbias low.
By aggregating many trees that arelowly-correlated (through bootstrap sampling andrandom variable selection), RF also reduces theprediction variance.An ensemble method such as Random Forestsis very suitable for the CDC task.
First, the col-lection of randomized decision trees is analogousto a panel of different experts, where each makesits decision using different criteria and differentfeatures.
Previously, RF has been used to aggre-gate various features in the author disambiguationtask (Treeratpituk and Giles, 2009).
One of thesignificant challenges in combining these differentfeatures in our CDC setting is that not all of themare always active.
For instance, the IE tool mayextract an employment relation for one entity anda list relation for another.
Also, when the IEtool cannot infer the gender information or whenthe categorization component does not confidentlypredict the top K categories (e.g.
all with lowscores), it?s desirable to not supply those featuresfor coreferencing.
The traditional technique toimpute the missing values, e.g.
by replacing themwith the mean value, is not suitable in this case.In our work, we specify a special level ?NA?
inthe decision tree base learner.
In our developmentset, this treatment improves pairwise coreferenceaccuracy by more than 6%.Figure 2 shows the convergence plot of thecomposite pairwise coreference function based onRandom Forest8.
We observe that the Out-Of-Bag8The R random forest (Liaw and Wiener, 2002) was used.Figure 2: Convergence of OOB errors of thecomposite pairwise coreference function using thetraining portion of the WePS dataset.488(OOB) errors 9 drastically decrease with the first50 trees and then level off (without signs of over-fitting).
Thus we choose to use the model builtwith the first 100 trees for prediction.
Overall, ourmodel can achieve more than 85% accuracy forpairwise coreference prediction.7 ExperimentsWe evaluate our CDC approach with the bench-mark dataset from the ACL-2007 SemEval WebPerson Search (WePS) evaluation campaign (Ar-tiles et al, 2007).
The WePS task is: given a namesearch query, cluster the search result documentsaccording to the underlying referents.
Comparedto the CDC task which clusters mention levelentities, a simplifying assumption is made in thistask that each document refers to only one identitywith respect to the query.
The WePS datasetcontains the training and test set.
The trainingset contains the top 100 web search results of49 names from the Web03 corpus (Mann andYarowsky, 2003), Wikipedia and European Con-ference on Digital Library (ECDL) participants;the test data are comprised of the top 100 docu-ments of 30 names from Wikipedia, US Censusand ACL participants.Table 1: Expert component and their feature sets.Feature Component DescriptionTC1 Categorization Sim.
of the top 1 categoriesTC3 Sim.
of the top 3 categoriesCNTX Context Sim.
of contextNAMEIE (attribute)Sim.
of full/first/last namesMENT Sim.
of mentionsGEND Sim.
of gendersEMPIE (relation)Sim.
of full/first/last namesLIST Sim.
of co-occurring personsLOC Sim.
of locationsFAM Sim.
of family members7.1 Evaluation of Pairwise CoreferenceWe conduct a thorough study of the importanceof the individual expert components and theirfeatures with the WePS training set.
Table 1 showsthe three components of the systems, their mainfeatures and descriptions.The importance of these expert components andtheir features are illustrated in Figure 3.
One of9OOB error is an unbiased estimate of test error in RF(Breiman, 2001), computed as the average misclassificationrates of each tree with samples not used for its construction.Figure 3: Importance of the expert componentsand their features found by Random Forest (notethe small spread in MeanDecreaseAccuracy).the most important features is CNTX, this confirmsthat the prior work on CDC (e.g.
(Bagga andBaldwin, 1998b)) can achieve good results withthe IE-driven context similarity feature (or its vari-ation).
The text categorization component alsocontributes very important features.
In particular,TC3 is more significant than TC1 for reducingthe Gini index because it recalls more correctcategories.
On the other hand, TC1 is slightlymore important than TC3 for its contribution toaccuracy, indicating TC1 is more precise (withless noise categories).
For the IE component,attribute features NAME and MENT are the mostuseful.
As aforementioned, the IE componentmay not always extract the relation features suchas EMP, LIST, LOC and FAM, and hence theyseemingly have limited effect on model learning(with relatively low reduction in Gini index).These relation features are however very accu-rate when extracted and are present for predic-tion.
Therefore, they are strong disambiguationevidence and their removal would significantlyhamper performance.7.2 Evaluation for Web Person SearchUsing the confidence of the pairwise corefer-ence prediction as a distance metric, we adopt adensity-based clustering method DBSCAN (Esteret al, 1996) as in (Huang et al, 2006)10 to inducethe person clusters.
The final set of evaluation isbased on these person clusters generated for theWePS test set.Two sets of metrics are used to evaluate theoverall system.
First, we use the B-CUBED10DBSCAN is a robust and scalable algorithm suitablefor clustering relational data.
In interest of space, we referreaders to (Ester et al, 1996) for the original algorithm.489Table 2: Cross document coreference perfor-mance (I. Pur.
denotes inverse purity).Method Purity I. Pur.
F B-CUBEDCDC 0.812 0.796 0.793 0.775CNTX 0.863 0.601 0.678 0.675TC1+3 0.620 0.776 0.660 0.634OIO 1.000 0.482 0.618 0.618AIO 0.279 1.000 0.389 0.238scores designed in (Bagga and Baldwin, 1998a)for evaluating cross document coreference perfor-mance.
Second, we use the purity, inverse purityand their F score as in WePS (Artiles et al, 2007).Purity penalizes placing noise entities in a cluster,while inverse purity penalizes splitting coreferententities into separate clusters.Table 2 shows the performance of themacro-averaged cross document coreferenceperformance on the WePS test sets.
Note thatthough our evaluation is based on the mentionlevel entities, the baselines One-In-One (OIO,placing each entity in a separate cluster) and All-In-One (AIO, putting all entities in one cluster)have almost identical results as those in theevaluation11.
OIO can yield good performance,indicating that the names in test data are highlyambiguous.
As alluded to in the title, context andcategories both are very useful disambiguationfeatures.
CNTX is essentially very similar to thesystem presented in (Bagga and Baldwin, 1998b)and is a strong baseline12 (outperforming 3/4of the systems in WePS).
Note that CNTX hashigh purity but inferior inverse purity, indicatingthat using the context extracted by the IE systemalone is unable to link many coreferent entities.Interestingly, we observe that using only thetop-K categories (TC1+3) can also achievecompetitive F score, though in a very differentmanner.
TC1+3 recalls much more coreferententities (significantly improving inverse purity),but at the same time also introduces noise.Finally, adding document categories and usingIE results (i.e.
using all features in Table 1),our CDC system achieves 22% and 18% relative11Most person names in this set have only one underlyingidentity per document; thus the results are comparabledespite the simplifying assumption of the WePS evaluation.12We use context similarity 0.2 as the clustering threshold(which has the best performance in training data).improvement compared to CNTX in F (purity)and B-CUBED scores, respectively.
In particular,inverse purity improves by 46% relatively, imply-ing that the additional evidence significantly im-proves the recall of coreferent entities (when thereis a lack of context similarity in the traditionalmethod).
Overall, the comprehensive approachin this work outperforms the top-tiered systems inthe WePS evaluation.8 Conclusion and Future WorkThis work proposes a synergy of three levels ofanalysis for the web cross document coreferencetask.
On the document level, we use text cate-gories, trained from thousands of ODP categoriesand over a million pages, as a concise representa-tion of the documents.
Categorization is a robuststrategy for coreferencing web documents withdiverse topics, formats and when there is a lack ofextraction coverage or word matching.
Two typesof sub-document level evidence are also used inour approach.
First, we apply an information ex-traction system to extract attributes and relationsof named entities from the documents and per-form within document coreference.
Second, weuse the context of the entities, a natural outcomeof the IE system as a focused description of thenamed entity that may miss the extraction process.A CDC system has been implemented based onthe IE and the text categorization componentsto provide a comprehensive solution to the webCDC task.
We demonstrate the importance ofeach component in our system and benchmarkour system with the WePS dataset which showssuperior CDC performance.There are a number of interesting directions forfuture research.
Recently, Open IE was proposedin (Etzioni et al, 2008) for Web informationextraction.
This can be a more powerful alter-native to traditional IE toolkits for Web CDC,though measuring the semantic similarity for avast variety of relations can be another researchissue.
Employing external background knowledgesuch as Wikipedia (Han and Zhao, 2009) whilemaintaining scalability can also be an orthogonaldirection for further improvement.490ReferencesArtiles, Javier, Julio Gonzalo, and Satoshi Sekine.2007.
The SemEval-2007 WePS evaluation:Establishing a benchmark for the web people searchtask.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations (SemEval),pages 64?69.Bagga, Amit and Breck Baldwin.
1998a.
Algorithmsfor scoring coreference chains.
In First Inter-national Conference on Language Resources andEvaluation Workshop on Linguistics Coreference.Bagga, Amit and Breck Baldwin.
1998b.
Entity-basedcross-document coreferencing using the vectorspace model.
In Proceedings of the 36th ACL and17th COLING, pages 79?85.Baron, Alex and Marjorie Freedman.
2008.
Whois who and what is what: experiments in cross-document co-reference.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 274?283.Breiman, Leo.
2001.
Random forests.
MachineLearning, 45(1):5?32.Chen, Ying and James Martin.
2007.
Towards robustunsupervised personal name disambiguation.
InProc.
of EMNLP and CoNLL, pages 190?198.Crammer, Koby and Yoram Singer.
2003.
A family ofadditive online algorithms for category ranking.
J.Machine Learning Research, 3:1025?1058.Crammer, Koby, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research (JMLR), 7:551?585.Ester, M., H. Kriegel, J. Sander, and X. Xu.
1996.
Adensity-based algorithm for discovering clusters inlarge spatial databases with noise.
In Proceedingsof the 2nd KDD Conference, pages 226 ?
231.Etzioni, Oren, Michele Banko, Stephen Soderland,and Daniel S. Weld.
2008.
Open informationextraction from the web.
Communications of ACM,51(12):68?74.Gooi, Chung H. and James Allan.
2004.
Cross-document coreference on a large scale corpus.
InProceedings of HLT-NAACL 2004, pages 9?16.Han, Xianpei and Jun Zhao.
2009.
Named entitydisambiguation by leveraging Wikipedia semanticknowledge.
In Proceedings of the 18th Conf.
onInformation and knowledge management (CIKM),pages 215?224.Huang, Jian, Seyda Ertekin, and C. Lee Giles.2006.
Efficient name disambiguation for large scaledatabases.
In Proc.
of 10th European Conferenceon Principles and Practice of Knowledge Discoveryin Databases (PKDD), pages 536 ?
544.Huang, Jian, Sarah M. Taylor, Jonathan L. Smith,Konstantinos A. Fotiadis, and C. Lee Giles.
2009.Profile based cross-document coreference usingkernelized soft relational clustering.
In Proceedingsof the 47th Annual Meeting of the Association forComputational Linguistics (ACL), pages 414?422.Li, Xin, Paul Morie, and Dan Roth.
2004.
Robustreading: Identification and tracing of ambiguousnames.
In Proceedings of the Human LanguageTechnology Conference and the North AmericanChapter of the Association for ComputationalLinguistics (HLT-NAACL), pages 17?24.Liaw, Andy and Matthew Wiener.
2002.
Classificationand regression by randomforest.
R News, 2(3).Madani, Omid and Jian Huang.
2008.
On updatesthat constrain the features?
connections duringlearning.
In Proceedings of the 14th ACM SIGKDDInternational Conference on Knowledge Discovery& Data Mining (KDD), pages 515?523.Mann, Gideon S. and David Yarowsky.
2003.Unsupervised personal name disambiguation.
InProceedings of the seventh conference on Naturallanguage learning (CoNLL), pages 33?40.Ng, Vincent and Claire Cardie.
2002.
Identifyinganaphoric and non-anaphoric noun phrases to im-prove coreference resolution.
In Proceedings of the19th International Conference on ComputationalLinguistics (COLING), pages 1?7.Niu, Cheng, Wei Li, and Rohini K. Srihari.2004.
Weakly supervised learning for cross-document person name disambiguation supportedby information extraction.
In Proceedings ofthe 42nd Annual Meeting on Association forComputational Linguistics (ACL), pages 598?605.Taylor, Sarah M. 2004.
Information extraction tools:Deciphering human language.
IT Professional,6(6):28 ?
34.Treeratpituk, Pucktada and C. Lee Giles.
2009.Disambiguating authors in academic publicationsusing random forests.
In Proceedings of theACM/IEEE Joint Conference on Digital libraries(JCDL), pages 39?48.Vapnik, V. 1998.
Statistical Learning Theory.
JohnWiley and Sons, Inc., New York.Wan, Xiaojun, Jianfeng Gao, Mu Li, and BinggongDing.
2005.
Person resolution in person searchresults: WebHawk.
In Proceedings of the 14thACM International Conference on Information andKnowledge management (CIKM), pages 163?170.491
