AN INTEGRATED MODEL FOR ANAPHORA RESOLUTIONRuslan MitkovInstitute of MathematicsAcad.
G. Bonchev str.
bl.8, 1113 Sofia, BulgariaABSTRACTThe paper discusses a new knowledge-based and sublanguage-oriented modelfor anaphora resolution, which integratessyntactic, semantic, discourse, domainand heuristical knowledge for thesublanguage of computer science.
Specialattention is paid to a new approach fortracking the center throughout a discoursesegment, which plays an imtx~rtant role inproposing the most likely antecedent tothe anaphor in case of ambiguity.INTRODUCTIONAnaphora resolution is a complicatedproblem in computational linguistics.Considerable research as been done bycomputational linguists (\[Carbonell &Brown 88\], IDahl & Ball 90\],\[Frederking & Gchrke 87\], \[Hayes 81\],\[Hobbs 78\], \[lngria & Stallard 89\],\[Preug et al 9411, \[Rich & LuperFoy 88\[,\[Robert 89\]), but no complete theory hasemerged which offers a resolutionprocedure with success guaranteed.
Allapproaches developed - even if we restrictour attention to pronominal anaphora,which we will do throughout this paper -from purely syntactic ones to highlysemantic and pragmatic ones, onlyprovide a partial treatment of the problem.Given the complexity of the problem, wethink that to secure a comparativelysuccessful handl ing of anaphoraresolution one should adhere to thefollowing principles: l) restriction to adomain (sublanguage) rather than focuson a particular natural language as awhole; 2) maximal use of linguisticinformation integrating it into a uniformarchitecture by means of partial theories.Some more recent reatments of anaphora(\[Carbonell & Brown 88\], \[Preug et al941, \[Rich & LuperFoy 8811) do expressthe idea of "multi-level approach", or"distributed architecture", but their ideasa) do not seem to capture enoughdiscourse and heuristical knowledge andb) do not concentrate on and investigate aconcrete domain, and thus risk being toogeneral.
We have tried nevertheless k)incorporate some of their ideas into ourproposal.THE ANAPttORA RESOLUTIONMODELOur anaphora resolution model integratesmodules containing different types ofknowledge - syntactic, semantic, domain,discourse and heuristical knowledge.
Allthe modules  share a commonrepresentation f the cunent discourse.The syntactic module, for example,knows that the anaphor and antecedentmust agree in number, gender andperson.
It checks if the c-commandconstraints hold and establishes disjointreference.
In cases of syntacticparallelism, it prefers the noun phrasewith the same syntactic role as theanaphor, as the most probable antecedent.It knows when cataphora is possible andcan indicate syntactically topicalized nounphrases, which are more likely to beantecedents than non-topicalized ones.The semantic module checks for semanticconsistency between the anaphor and thepossible antecedent.
It filters outsemantically incompatible candidatesfollowing the cun-ent verb semantics orthe animacy of the candidate.
In cases ofsemantic parallelism, it prefers the nounphrase, having the same semantic role asthe anaphor, as a most likely antecedent.Finally, it generates a set of possibleantecedents whenever necessary.The domain knowlcdge module ispractically a knowlcdge basc of theconcepts of the domain considered and1170thc discourse knowledge module knowshow to track the center throughout hecurrent discourse segment.The heuristical knowledge module cansolnetimes bc helpful in assigning theantecedent.
It has a set of useful rules(e.g.
the antecedent  is to be locatedpreferably in thc current sentence or in theprevious one) and can forestall certainimpractical search procedures.The use of co lnmon sense and worldknowledge is in general commendable,but it requires a huge knowledge base andset of inference rules.
The present versionof the model does not have this mcxtuleimplementcd; its development,  however,is envisaged for later stages of the project.The syntact ic  and semant ic  modulesusually filter the possible candidates anddo not propose an antecedent (with theexcept ion of syntact ic  and semant icparallelism).
Usually the proposal for anantecedent  comes  f rom the domain,heuristical, and discourse modules.
Thelatter plays an important role in trackingthe center and proposes it in many casesas the most probable candidate for anantecedent.Figurc 1 illustrates the general structure ofour anaphom resolution model.IIIiURISTICAI,KNOWI ,I ';l X;t il)omain lleuristicsRating RulesRecency\[ Rl:ilqlilil iNTIA\], l ihJANAPI IOR-~--~.
I  ixptaisstoNSYNTACTIC KNOW\] ,t il )(}l iNumber AgrccmenlGender AgfeelncnlPCI'SOll A~l'Celllelltl)isjoim Reference(~-(~ommaud ConstraintsCataphoraSyntactic ParalldislllSyntactic Topicalization1)OMAIN I)|SCOURSI ~, KNOWI ,El X\]!
iKNOW I ,l ilX\]l'~ l)omain Concept'1 'racking Center Ktlowledgc 1~aseANAPIIORAP,I {SOl ,VI {RANTI iCI il)ENTSt,',MANTI(KNOW1,1 '~I)GESemm~tic ConsistencyCase RolesSemantic ParallelismAnimacySet GeneralionFigure 1: Anaphora resolution modelTHE NEED FOR DISCOURSECRITERIAA l though the syntact ic  and semanticcriteria for the selection of an antecedentare already very strong, they are notalways sufficient o discriminate alnong aset of possible candidates.
Moreover,they serve more as filters to eliminateunsuitable candidates than as protx)sers ofthe most l ikely candidate.
Addit ionalcriteria are therefore needed.As an illustration, considerthe followingtext.Chapter 3 discusses these additional orauxiliary storage devices, wlfieh mcsimilar to our own domestic tapecassettes and record discs.
Figure 2illustrates lheir connection to the maincenlral memory.1171In this discourse segment neither thesyntactic, nor the semantic onstraints caneliminate the ambiguity between "storagedevices", "tape cassettes" or "recorddiscs" as antecedents for "their", and thuscannot urn up a plausible antecedent fromamong these candidates.
A human readerwould be in a better position since hewould be able to identify the centralconcept, which is a primary candidate forpronominalization.
Correct identificationof the antecedent is possible on the basisof the pronominal reference hypothesis: inevery sentence which contains one ormore pronouns must have one of itspronouns refer to the center 1 of theprevious sentence.
Therefore, wheneverwe have to find a referent of a pronounwhich is alone in the sentence, we have tolook for the centered clement in theprevious entence.Fo l low ing  this hypothes is ,  andrecognizing "storage devices" as thecenter, an anaphora resolution modelwould not have problems in picking upthe center of the previous sentence("storage devices") as antecedent for"their".We see now that the main problem whicharises is the tracking of the centerthroughout  the d iscourse segment.Certain ideas and algorithms for trackingfocus or center (e.g.
\[Brennan et al87\])have been proposed, provided that oneknows the focus or center of the firstsentence in the segment.
However, theydo not try to identify this center.
Ourapproach determines the most probablecenter of the first sentence, and thentracks it all the way through the segment,correcting the proposed algorithm at eachstep.TRACKING THE CENTER IN "\['HESUBLANGUAGE OF COMPUTERSCIENCEIdentifying center can be very helpful in1 Though "center" isml uncrancc specific notion,we refer to "sentence nter", because inmanycases the centers of the uttermmes a enlence mayconsist of, coincide.
In a complex sentence,however, we distinguish also "clause centers"anaphora resolution.
Usually a center isthe most  l ike ly  cand idate  forpronominalization.There are different views in literatureregarding the preferred candidate for acenter  ( focus) .
S idner 's  algorithm(\[Sidner 811), which is based on thematicroles, prefers the theme o\[ the previoussentence as the focus of the currentsentence.
This view, in general, isadvocated also in (\[Allen87\]).
PUNDIT,in its current implementation, considersthe entire previous utterance to be thepotential focus (\[Dahl&Ball 901).
Finally,in the centering literature (\[Brennan et al87\]), the subject is generally consideredto be preferred.
We have found,however, that there are many additionalinterrelated factors which influence uponthe location of the center.Wc studied the "behaviour" of center invarious computer  science texts (30different sources totally exceeding 1000pages) and the empirical observationsenab led  us to deve lop  efficientsublanguage-dependent heuristics fortracking the center in the sublanguage ofcomputer science.
We summarize themost important conclusions as follows:1) Consider the primary candidatesfor center from the priority list:subject, object, verb phrase.2) Prefer the NP, representing adomain concept o the NPs, whichare not domain concepts.3) If the verb is a member of theVerb set = {discuss, present,i l lustrate, summarize,  examine,describe, define, show, check,develop, review, report, outline,consider ,  investigate, explore,assess, analyze, synthesize, study,survey, deal, cover}, then considerthe object as a most probablecenter.4) If a verbal adjective is a memberof" the Adj set = {defined, called,so-called}, then consider the NPthey refer to as the probable centerof the subsequent clause/currentsentence.11725) If  the subject is "chapter","section", "table", or a personalpronoun - 'T',  "we", "you", thenconsider the object as most likelycenter.6) If a NP is repeated throughoutthe discourse section, then considerit as the most probable center.7) Kf an NP occurs in the head ofthe section, part of which is thecurrent discourse scgment, thenconsider it as the probable center.8) If  a NP is topicaKized, thenconsider it as a probable center.9) Prefer definite NPs to indefiniteones .K0) Prefer the NPs in the maincKausc to NPs in the subordinateclauses.K 1) If the sentence is complex, thenprefer for an antecedent a nounphrase from the previous chmsewithin the same sentence.As far as rule I is concerned, we foundthat the subject is a primary candidate forcenter in about 73% of the cases.
Thesecond most likely center would be theobject (25%)  and the third most likelyone the verb phrase as a whole (2%).Therefore, the priority list \[subject,object, verb phrase\] is considered interms of the apriori estimated probability.There are certain 'symptoms'  whichdetermine the subject or the object as acenter with very high probability.
Casesin point are 3) and 5).
Other cases are notst) certain, but to some extent quite likely.For example, iK a non-concept NP is insubject position and if a repeated conceptNP, which is also in a head, is in objectposition, it is ahnost certain that the latteris the unambiguous center.
Moreover,certain preferences are stronger thanothers.
For example an NP in subjectposition is preferred over an NP in asection head, but not in subject position.We have made use of our empiricalresults (with approximating probabilitylneasures) and AK techniques to develop aproposer module which identifies themost likely center.
We must point out thateven iK we do not need one for immediateantecedent disambiguation, a center muststill be proposed for each sentence.
O"else we will have to go all the way back totrack it from the beginning of the segmentwhen one is needed later on.The rules 1)- 11) should be orderedaccording to their priority - a problem,which is being currently investigated.Tracking the center in a discoursesegment is very important since knowingthe center of each current sentence helpsin many cases to make correct decisionsabout an antecedent in the event thatsyntactic and semantic onstraints cannotd iscr iminate  among the avai lablecandidates.AN ART IF IC IAL  INTELLIGENCEAPPROACH FOR CALCULATINGTHE PROBABILITY OF A NOUN(VERB) PHRASE.TO BE IN THECENTEROn the basis of the results described in theprevious section, we use an artificialintelligence approach to determine theprobability of a noun (verb) phrase to bethe center of a sentence.
Note that thisapproach allows us to calculate thisprobability in every discourse sentence,including the first one and to propose themost probable center.
This approach,combined with the algorithm tbr trackingthe center (\[Brcnnan et al 87\]), isexpected to yield improvcxl results.Our model incorporatcs an AI algorithmfor calculating the probability of a noun(verb) phrase to be in the center of adiscourse segment.
The algorithm uses aninference engine based on Bayes 'theorem:P(HK) P(AII-IK)P(I t\[dA) .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.P(I KOP(AIH~)forK = 1,2,...Under the conditions of our model Bayes'theorem a l lows  the fo l low ing1173interpretation: there are only two possiblehypotheses for a certain noun (verb)phrase - that it is the center of the currentsentence (clause) or that it is not.
Let Hybe the positive, while HN - the negativehypothesis.
If we call the presence ofsome of the pieces of evidence, describedin the previous section, a "symptom",then let A denote the occurrence of thatsymptom with the examined phrase.P(AIHy) would be the apriori probabilityof the symptom A being observed with anoun (verb) phrase which is the center(we will henceforth refer to this factor asPy).
By analogy P(AIHN) is theprobabi l i ty of the symptom beingobserved with a phrase which is not thecenter (henceforth referred to as PN).
Theaposteriori probability P(HKtA) is definedin the light of the new piece of evidence -the presence of an empirically obtainedsymptom,  ind icat ing the h igherprobability the examined phrase to be inthe center of the discourse segment.In other words, inference ngine based onBayes' theorem draws an inference in thelight of some new piece of evidence.
Thisformula calculates the new probability,given the old probability plus some newpiece of evidence.Cons ider  the fol lowing situation.According to our investigation so far, theprobability of the subject being a center is73%.
Additional evidence (symptom),e.g.
if the subject represents a domainconcept,  will increase the initialprobability.
If this NP is also the head ofthe section, the probability is increasedfurther.
If the NP occurs more than oncein the discourse segment, the probabilitygets even higher.An estimation of the probability of asubject, (direct or indirect) object or verbphrase (the only possible centers in ourtexts) to be centers, can be represented asa predicate with arguments:center (X, PI, \[symptoml (weightfactorl 1' weight factorl2 ) .... symptomN(weight factorN~, weight factolNz)\]where center (X, I, list) represents theestimated probability of X to be the centerof a sentence (clause), X E {subjec t,objectl, object2 .
.
.
.
verb phrase} and Plis the initial probability of X to be thecenter of the sentence (clause).Weight factorl is the probability of thesymptom being observed with a noun(verb) phrase which is the center (Py).Weight factor2 is the probability of thesymptom being observed with a noun(verb) phrase wiaich is not the center(PN).Following our preliminary results, wc canwrite in Prolog notation:center (object, 25, \[sylnptoln (verb_set,40, 3), symptom (subject set, 40,2),symptom (domain concept (95, 80),symptom (repeated, 10, 5), symptom(headline, 10, 9)11, symptoln (topicalizexl,6, 2), symptom (main chmsc (85, 30),symptom (definite.form (90, 7t))\]).center (subject, 73, Isymptonl(domain concept (95, 70), symptom(repeated, 10, 4), symptom (headline,10, 8), symptom (topicalized, 10, 3),symptom (main_clause (85, 30),symptom (definite_form (85, 20)11).The first fact means that the object is thecenter in approximately 25% of the cases.Moreover, it suggests that in 40% of thecases where the center is the object, theverb belongs to the set of verbs {discuss,illustrate, summarize, examine, describe,define...} and it is possible with 3%probability for the verb to be a member ofthis set while the center of the sentence isnot the object.The above Prolog facts are part of asublanguage knowledge base.The process of estimating the probabilityof a given phrase being the center of asentence (clause), is repetitive, beginningwith an initial estimate and graduallyworking towards a more accurate answer.More systematically, the "diagnostic"process is as follows:- start with the initial probability- consider the symptoms one at a time- for each symptom, update the currentprobabil ity, taking into account: a)whether the sentence has the symptomand b) the weight factors Py and PN.1174The probability for an NP to be the centeris calculated by the inference enginerepresented as a Prolog program (left outhere for reasons of space), whichoperates on the basis of the sublanguageknowledge base and the " local"knowledge base.
The latter givesinformation on the current discoursesegment.
Initially, our program workswith manual inputs.
The local knowledgebase can be represented as Prolog facts inthe following way:observcd (lmadlinc).observed ( omailL conccl)O.obsmvcd (repeated).The inference ngine's task is to matchthe expected symptoms of the possiblesyntactic function as center in theknowledge base of the sentence's actualsymptoms,  and produce a list of(reasonably) tx)ssible candidates.THE PROCEDURE:AN INTEGRATED KNOWLEDGEAPPROA CttOur algorithm for assigning (proposing)an antecedent  to an anaphor  issublanguage-oriented because it is basedon rules result ing from studies ofcolnputer science texts.
It is alsoknowledge-based because it uses at leastsyntact ic ,  semant ic  and discourseknowledge.
Discourse knowledge andespecially knowing how to track thecenter play a decisive role in proposingthe most likely antecedent.The initial version of our project handlesonly pronominal anaphors.
However, notall pronomls may have specific reference(as in constructions like "it is necessary","it should be pointed out", "it is clear",.... ).
So be\[ore the input is given to theanaphor esolver, the pronoun is checkedto ensure that it is not a part of suchgrammatical construction.
This function iscarried out by the "referential expressionfilter".The proccdurc  for propos ing anantecedent to an anaphor operates ondiscourse segments and can be describeditffonnally in the lollowing way:l) Propose the center of the firstsentence of the discourse segmentusing the method escribed.2) Use the algorithm proposed inIBrennan et al 871, ilnproved by anadditional estimation of the cotTcctprobability supplied by our method,in order to track the center throughoutthe discourse segment (in case theanaphor is in a complex sentence,identify clause centers too).3) Use syntactic and semanticconstraints to eliminate antecedentcandidates.4) Propose the noun phrase that hasbeen filtered out as the antecedent incase no other candidates have comeup; otherwise propose the center ofthe preceding sentence (clause) as theantecedent.The information obtained in 1) and 2)may not be used; however, it may be vitalfor proposing an antecedent in case ofambiguity.To illustrate how the algorithm works,consider the tollowing sample text:SYSTI '~M PROGRAMSWe should note that, unlike userprogrants, ystem pmgran~s such as thesupervisor and the language translatorshould not have to bc translated everylime they are used, olherwisc lhis wouldresult ill a serious increase ill the timespent in processing a user's program.System t)rogr~|lns are usual ly  written illthe assembly version of the machinelangtmgc and are tnmslated once into IhenladlillC code itself, l;rom thCll oi1 theycan be loaded into memory in machinecode without he need for any immediatetranshuion phases.
They are written byspecialist programmers, who arc "calledsystem programmers and who know agreat deal about the computer and thecomlmtcr system 12)1" which their progrmnsarc wrincn.
They know Ihc exact ntmlberof location which each program willoccupy mid in consequence an make useof these mmlbcrs in the supervisor andt l',qllslalor t)rograms.117,5The proposed center of  the first sentenceis "system programs".
The center emainsthe same in the second, third and forthsentences.
Syntact i c  const ra in ts  aresuff icient to establ ish the antecedent of"they" in the third sentence as "systemprograms" .
In the forth sentence ,syntactic onstraints only,  however,  areinsuff ic ient.
Semant ic  constraints helphere in assigning "system programs" asantecedent to "they".
In the fifth sentenceneither syntactic nor semantic onstraintscan resolve the ambiguity.
The correctdecision comes from proposing the centerof  the previous sentence, in this case"sys tem programmers"  (and  not"programs"! )
,  as the most  l i ke lyantecedent.CONCLUSIONThe mode l  p roposed  has two mainadvantages.
F i rst ,  it is an integratedmodel  of d i f ferent types of  knowledgeand uses existing techniques for anaphoraresolution.
Second, it incorporates a newapproach for tracking the center, whichproposes  centers  and subsequent lyantecedents  with max imal  l ikelihood.S ince we regard  our  results sti l l  asprel iminary, further research is necessaryto conf i rm/ improve the approach/modelpresented.ACKNOWLEDGEMENTI would l ike to express nay gratitude toProf.
P ie ter  Seuren  for his usefu lcomments and to the Machine TranslationUnit, Universiti  Sains Malaysia, Penang,where a considerable part of the describedresearch as been carried out.REFERENCES\[Aonc & MeKee 93\] Ch.
Aonc, D. McKee -Language-independent anaphora resolution systemfor understanding multilingual texts.
ProceeAingsof the 31st Annual Meeting of the ACL, TheOlfio State University, Colmnbus, Ohio, 1993\[Allen87\] J. Al len Natural languageunderstanding.
The Benjamin/CummingsPublishing Company Inc., 1987\[Brennan ct al.
87\] S. Brennml, M. Fridman, C.Pollard - A centering approach to pronouns.Proceedings of the 25th Annual Meeting of fileACL, Statfford, CA, 1987\[Cmbonell & Brown 88\] J. Carbonell, R. 13town- Anaphora resolution: a multi-strategy approach.Proceedings of the 12. International Colfferenccon Computational Linguistics COLING'88,Budapest, 1988\[Dahl & Ball 90\] 1).
Dahl, C. Ball - Referenceresolution in PUNDIT.
Research Report CAIT-SLS-9004, March 1990.
Center for AdvancedInformation Teclmology, Paoli, PA 9301\[Frederking & Gehrkc 87\] R. Frederking, M.Gchrke - Resolving anaphoric references in aDRT-based ialogue system: Part 2: Focus at~lTaxonomic inference.
Siemens AG, WlSBER,Bericht Nr.
17, 1987\[Grosz & Sidncr 86\] B. Grosz, C. Sidncr -Attention, Intention and the Structure ofDiscourse.
Computalional Linguistics, Vol.
12,1986\[Hayes 8111 P.J.
Hayes - Anaphorafor limiteddomain systems.
Proceedings of the 7th IJCAI,V~mcouver, Canada, 1981\[Hirst 81\] G. Hirst - Anaphora in naturallanguage understanding.
Berlin Springer Verlag,1981\[Hobbs 78\] J. IIobbs - Resolving pronounrefere~es.
Lingua, Vol.
44, 1978\[lngria & StaUm'd 8911 R. lngria, D. Stallard - Acomputational mechanism for pronominalreference.
Proceedings of the 27th AnnualMeeting of the ACL, Vancouver, BritishColumbia, 1989\[Mitkov 93\] R. Mitkov - A knowledge-basedandsublanguage-oriented approach for anaphoraresolution.
Proceedings of the Pacific AsiaConference on Formal and ComputationalLinguistics, Taipei, 1993\[Preug et al 94l PrcuB S., Schmitz B.,Hauenschild C., Umbach C. AnaphoraResolution in Machine Translation.
In W.Ramm, P. Schmidt, J. Schiitz (eds.)
Studies inMachine Translation and Natural I.~mguagcProcessing, Volume on "l)iscoursc in MachineTrmlslation"\[Rich & LupcrFoy 88\] 1~.
Rich, S. lalpcrFoy -An architecture for anaphora resolution.Proceedings ofthe Second Conference on ApplitxtNatnral l~anguagc Processing, Austin, Texas,1988\[Robert 8911 M. Robert - RSsolution de formespronominales dans l'interface d'interogation d'unebase de donndes.
Th6sc de doctorat.
Facult6 dessciences de Luminy, 1989\[Sidner 8111 C.L.
Sidncr - Focusing forInterpretation ofPronouns.
Americau Journal ofComputational Linguistics, 7, 1981\[Walker 89\] M. Walker -- Evaluating discourseprocessing algorithms.
Proceedings of the 27thAnnual Meeting of the ACL, Vancouver,Colmnbia, 19897776
