Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 857?864,Sydney, July 2006. c?2006 Association for Computational LinguisticsEvent Extraction in a Plot Advice AgentHarry HalpinSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotland, UKH.Halpin@ed.ac.ukJohanna D. MooreSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotland, UKJ.Moore@ed.ac.ukAbstractIn this paper we present how the auto-matic extraction of events from text canbe used to both classify narrative texts ac-cording to plot quality and produce advicein an interactive learning environment in-tended to help students with story writing.We focus on the story rewriting task, inwhich an exemplar story is read to the stu-dents and the students rewrite the story intheir own words.
The system automati-cally extracts events from the raw text, for-malized as a sequence of temporally or-dered predicate-arguments.
These eventsare given to a machine-learner that pro-duces a coarse-grained rating of the story.The results of the machine-learner and theextracted events are then used to generatefine-grained advice for the students.1 IntroductionIn this paper we investigate how features of a textdiscovered via automatic event extraction can beused in both natural language understanding andadvice generation in the domain of narrative in-struction.
The background application is a fullyautomated plot analysis agent to improve the writ-ing of students could be used by current nar-rative tutoring systems (Robertson and Wiemer-Hastings, 2002).
As shown by participatory de-sign studies, teachers are interested in a plot anal-ysis agent that can give online natural languageadvice and many students enjoy feedback from anautomated agent (Robertson and Cross, 2003).
Weuse automatic event extraction to create a story-independent automated agent that can both ana-lyze the plot of a story and generate appropriateadvice.1.1 The Story Rewriting TaskA task used in schools is the story rewriting task,where a story, the exemplar story, is read to thestudents, and afterwards the story is rewritten byeach student, providing a corpus of rewritten sto-ries.
This task tests the students ability to bothlisten and write, while removing from the studentthe cognitive load needed to generate a new plot.This task is reminiscent of the well-known ?Warof the Ghosts?
experiment used in psychology forstudying memory (Bartlett, 1932) and related towork in fields such as summarization (Lemaire etal., 2005) and narration (Halpin et al, 2004).1.2 Agent DesignThe goal of the agent is to classify each of therewritten stories for overall plot quality.
Thisrating can be used to give ?coarse-grained?
gen-eral advice.
The agent should then provide ?fine-grained?
specific advice to the student on how theirplot could be improved.
The agent should be ableto detect if the story should be re-read or a humanteacher summoned to help the student.To accomplish this task, we extract events thatrepresent the entities and their actions in the plotfrom both the exemplar and the rewritten stories.A plot comparison algorithm checks for the pres-ence or absence of events from the exemplar storyin each rewritten story.
The results of this algo-rithm will be used by a machine-learner to clas-sify each story for overall plot quality and providegeneral ?canned?
advice to the student.
The fea-tures statistically shared by ?excellent?
stories rep-resent the important events of the exemplar story.The results of a search for these important eventsin a rewritten story provides the input needed bytemplates to generate specific advice for a student.8572 CorpusIn order to train our agent, we collected a corpusof 290 stories from primary schools based on twodifferent exemplar stories.
The first is an episodeof ?The Wonderful Adventures of Nils?
by SelmaLagerloff (160 stories) and the second a re-tellingof ?The Treasure Thief?
by Herodotus (130 sto-ries).
These will be referred to as the ?Adventure?and ?Thief?
corpora.2.1 RatingAn experienced teacher, Rater A, designed a ratingscheme equivalent to those used in schools.
Thescheme rates the stories as follows:1.
Excellent: An excellent story shows thatthe student has ?read beyond the lines?
anddemonstrates a deep understanding of thestory, using inference to grasp points thatmay not have been explicit in the story.
Thestudent should be able to retrieve all the im-portant links, and not all the details, but theright details.2.
Good: A good story shows that the studentunderstood the story and has ?read betweenthe lines.?
The student recalls the main eventsand links in the plot.
However, the studentshows no deep understanding of the plot anddoes not make use of inference.
This can of-ten be detected by the student leaving out animportant link or emphasizing the wrong de-tails.3.
Fair: A fair story shows that student haslistened to the story but not understood thestory, and so is only trying to repeat what theyhave heard.
This is shown by the fact that thefair story is missing multiple important linksin the story, including a possibly vital part ofthe story.4.
Poor: A poor story shows the student has hadtrouble listening to the story.
The poor storyis missing a substantial amount of the plot,with characters left out and events confused.The student has trouble connecting the partsof the story.To check the reliability of the rating scheme,two other teachers (Rater B and Rater C) ratedsubsets (82 and 68 respectively) of each of the cor-pora.
While their absolute agreement with Rater AClass Adventure Thief1 (Excellent) .231 .1462 (Good) .300 .3773 (Fair) .156 .2924 (Poor) .313 .185Table 1: Probability Distribution of Ratingsmakes the task appear subjective (58% for B and53% for C), their relative agreement was high, asalmost all disagreements were by one level in therating scheme.
Therefore we use Cronbach?s ?and ?b instead of Cohen?s or Fleiss?
?
to take intoaccount the fact that our scale is ordinal.
BetweenRater A and B there was a Cronbach?s ?
statisticof .90 and a Kendall?s ?b statistic of .74.
BetweenRater B and C there was a Cronbach?s ?
statis-tic of .87 and Kendall?s ?b statistic of .67.
Thesestatistics show the rating scheme to be reliable andthe distribution of plot ratings are given in Table 1.2.2 Linguistic IssuesOne challenge facing this task is the ungrammati-cal and highly irregular text produced by the stu-dents.
Many stories consist of one long run-onsentence.
This leads a traditional parsing systemwith a direct mapping from the parse tree to a se-mantic representation to fail to achieve a parse on35% percent of the stories, and as such could notbe used (Bos et al, 2004).
The stories exhibit fre-quent use of reported speech and the switchingfrom first-person to third-person within a singlesentence.
Lastly, the use of incorrect spelling e.g.,?stalk?
for ?stork?
appearing in multiple storiesin the corpus, the consistent usage of homonymssuch as ?there?
for ?their,?
and the invention ofwords (?torlix?
), all prove to be frequent.3 Plot AnalysisTo automatically rate student writing many tutor-ing systems use Latent Semantic Analysis, a vari-ation on the ?bag-of-words?
technique that usesdimensionality reduction (Graesser et al, 2000).We hypothesize that better results can be achievedusing a ?representational?
account that explicitlyrepresents each event in the plot.
These semanticrelationships are important in stories, e.g., ?Thethief jumped on the donkey?
being distinctly dif-ferent from ?The donkey jumped on the thief.
?What characters participate in an action matter,since ?The king stole the treasure?
reveals a major858misunderstanding while ?The thief stole the trea-sure?
shows a correct interpretation by the student.3.1 Stories as EventsWe represent a story as a sequence of events,p1...ph, represented as a list of predicate-arguments, similar to the event calculus (Mueller,2003).
Our predicate-argument structure is a mini-mal subset of first-order logic (no quantifiers), andso is compatible with case-frame and dependencyrepresentations.
Every event has a predicate (func-tion) p that has one or more arguments, n1...na.In the tradition of Discourse Representation The-ory (Kamp and Reyle, 1993), our current predi-cate argument structure could be converted auto-matically to first order logic by using a defaultexistential quantification over the predicates andjoining them conjunctively.
Predicate names areoften verbs, while their arguments are usually, al-though not exclusively, nouns or adjectives.
Whendescribing a set of events in the story, a superscriptis used to keep the arguments in an event distinct,as n25 is argument 2 in event 5.
The same argumentname may appear in multiple events.
The plot ofany given story is formalized as an event structurecomposed of h events in a partial order, with thepartial order denoting their temporal order:p1(n11, n21, ...na1), ...., ph(n2h, n4h...nch)An example from the ?Thief?
exemplar story is?The Queen nagged the king to build a treasurechamber.
The king decided to have a treasurechamber.?
This can be represented by an eventstructure as:nag(king, queen)build(chamber)decide(king)have(chamber)Note due the ungrammatical corpus we cannot atthis time extract neo-Davidsonian events.
A sen-tence maps onto one, multiple, or no events.
Aunique name and closed-world assumption is en-forced, although for purposes of comparing eventwe compare membership of argument and predi-cate names in WordNet synsets in addition to exactname matches (Fellbaum, 1998).4 Extracting EventsParalleling work in summarization, it is hypothe-sized that the quality of a rewritten story can bedefined by the presence or absence of ?seman-tic content units?
that are crucial details of thetext that may have a variety of syntactic forms(Nenkova and Passonneau, 2004).
We further hy-pothesize these can be found in chunks of thetext automatically identified by a chunker, and wecan represent these units as predicate-arguments inour event structure.
The event structure of eachstory is automatically extracted using an XML-based pipeline composed of NLP processing mod-ules, and unlike other story systems, extract fullevents instead of filling in a frame of a story script(Riloff, 1999).
Using the latest version of theLanguage Technology Text Tokenization Toolkit(Grover et al, 2000), words are tokenized and sen-tence boundaries detected.
Words are given part-of-speech tags by a maximum entropy tagger fromthe toolkit.
We do not attempt to obtain a full parseof the sentence due to the highly irregular natureof the sentences.
Pronouns are resolved using arule-based reimplementation of the CogNIAC al-gorithm (Baldwin, 1997) and sentences are lem-matized and chunked using the Cass Chunker (Ab-ney, 1995).
It was felt the chunking method wouldbe the only feasible way to retrieve portions of thesentences that may contain complete ?semanticcontent units?
from the ungrammatical and irregu-lar text.
The application of a series of rules, mainlymapping verbs to predicate names and nouns toarguments, to the results of the chunker producesevents from chunks as described in our previouswork (McNeill et al, 2006).
The accuracy of ourrule-set was developed by using the grammaticalexemplar stories as a testbed, and a blind judgefound they produced 68% interpretable or ?sen-sible?
events given the ungrammatical text.
Stu-dents usually use the present or past tense exclu-sively throughout the story and events are usuallypresented in order of occurrence.
An inspectionof our corpus showed 3% of stories in our corpusseemed to get the order of events wrong (Hick-mann, 2003).4.1 Comparing StoriesSince the student is rewriting the story using theirown words, a certain variance from the plot of theexemplar story should be expected and even re-warded.
Extra statements that may be true, butare not explicitly stated in the story, can be in-ferred by the students.
Statements that are truebut are not highly relevant to the course of the859plot can likewise be left out.
Word similaritymust be taken into account, so that ?The king isprotecting his gold?
can be recognized as ?Thepharaoh guarded the treasure.?
Characters changein context, as one character that is described asthe ?younger brother?
is from the viewpoint of hismother ?the younger son.?
So, building a modelfrom the events of two stories and simply check-ing equivalence can not be used for comparison,since a wide variety of partial equivalence must betaken into account.Instead of using absolute measures of equiva-lence based on model checking or measures basedon word distribution, we compare each story onthe basis of the presence or absence of events.
Thisapproach takes advantage of WordNet to definesynonym matching and uses the relational struc-ture of the events to allow partial matching ofpredicate functions and arguments.
The eventsof the exemplar story are assumed to be correct,and they are searched for in the rewritten story inthe order in which they occur in the exemplar.
Ifan event is matched (including using WordNet),then in turn each of the arguments attempts to bematched.This algorithm is given more formally in Fig-ure 1.
The complete event structure from the ex-emplar story, E, and the complete event structurefrom the rewritten story R, with each individualevent predicate name labelled as e and r respec-tively, and their arguments labelled as n in eitherNe and Nr.
SYN(x) is the synset of the term x,including hypernyms and hyponyms except upperontology ones.
The results of the algorithm arestored in binary vector F with index i.
1 denotesan exact match or WordNet synset match, and 0 afailure to find any match.4.2 ResultsAs a baseline system LSA produces a similar-ity score for each rewritten story by comparing itto the exemplar, this score is used as a distancemetric for a k-Nearest Neighbor classifier (Deer-wester et al, 1990).
The parameters for LSA wereempirically determined to be a dimensionality of200 over the semantic space given by the rec-ommended reading list for American 6th graders(Landauer and Dumais, 1997).
These parametersresulted in the LSA similarity score having a Pear-son?s correlation of -.520 with Rater A. k wasfound to be optimal at 9.Algorithm 4.1: PLOTCOMPARE(E,R)i?
0f ?
?for e ?
Edo for r ?
Rdo???????????????????????????
?if e = SYN(r)then fi ?
1else fi ?
0for ne ?
Nedo??????????
?for nr ?
Nrdo????
?if ne = SYN(nr)then fi ?
1else fi ?
0i = i + 1Figure 1: Plot Comparison AlgorithmClassifier Corpus Features % Correctk-NN Adventure LSA 47.5Naive Bayes Adventure PLOT 55.6k-NN Thief LSA 41.2Naive Bayes Thief PLOT 45.4Table 2: Machine-Learning ResultsThe results of the plot comparison algorithmwere given as features to machine-learners, withresults produced using ten-fold cross-validation.A Naive Bayes learner discovers the different sta-tistical distributions of events for each rating.
Theresults for both the ?Adventure?
and ?Thief?
sto-ries are displayed in Table 2.
?PLOT?
means theresults of the Plot Comparison Algorithm wereused as features for the machine-learner while?LSA?
means the similarity scores for Latent Se-mantic Analysis were used instead.
Note that thesame machine-learner could not be used to judgethe effect of LSA and PLOT since LSA scores arereal numbers and PLOT a set of features encodedas binary vectors.The results do not seem remarkable at firstglance.
However, recall that the human raters hadan average of 56% agreement on story ratings, andin that light the Naive Bayes learner approachesthe performance of human raters.
Surprisingly,when the LSA score is used as a feature in additionto the results of the plot comparison algorithm forthe Naive Bayes learners, there is no further im-provement.
This shows features given by the event860Class 1 2 3 41 (Excellent) 14 22 0 12 (Good) 5 36 0 73 (Fair) 3 20 0 24 (Poor) 0 11 0 39Table 3: Naive Bayes Confusion Matrix: ?Ad-venture?Class Precision RecallExcellent .64 .38Good .40 .75Fair .00 .00Poor .80 .78Table 4: Naive Bayes Results: ?Adventure?structure better characterize plot structure than theword distribution.
Unlike previous work, the useof both the plot comparison results and LSA didnot improve performance for Naive Bayes, so theresults of using Naive Bayes with both are not re-ported (Halpin et al, 2004).The results for the ?Adventure?
corpus are ingeneral better than the results for the ?Thief?
cor-pus.
However, this is due to the ?Thief?
corpusbeing smaller and having an infrequent number of?Excellent?
and ?Poor?
stories, as shown in Table1.
In the ?Thief?
corpus the learner simply col-lapses most stories into ?Good,?
resulting in verypoor performance.
Another factor may be that the?Thief?
story was more complex than the ?Adven-ture?
story, featuring 9 characters over 5 scenes, asopposed to the ?Adventure?
corpus that featured 4characters over 2 scenes.For the ?Adventure?
corpus, the Naive Bayesclassifier produces the best results, as detailed inTable 4 and the confusion matrix in Figure 3.
Aclose inspection of the results shows that in the?Adventure Corpus?
the ?Poor?
and ?Good?
sto-ries are classified in general fairly well by theNaive Bayes learner, while some of the ?Excel-lent?
stories are classified as correctly.
A signifi-cant number of both ?Excellent?
and most ?Fair?stories are classified as ?Good.?
The ?Fair?
cate-gory, due to its small size in the training corpus,has disappeared.
No ?Poor?
stories are classifiedas ?Excellent,?
and no ?Excellent?
stories are clas-sified as ?Poor.?
The increased difficulty in distin-guishing ?Excellent?
stories from ?Good?
storiesis likely due to the use of inference by ?Excellent?stories, which our system does not use.
An inspec-tion of the rating scale?s wording reveals the sim-ilarity in wording between the ?Fair?
and ?Good?ratings.
This may explain the lack of ?Fair?
sto-ries in the corpus and therefore the inability ofmachine-learners to recognize them.
As given bya survey of five teachers experienced in using thestory rewriting task in schools, this level of perfor-mance is not ideal but acceptable to teachers.Our technique is also shown to be easilyportable over different domains where a teachercan annotate around one hundred sample storiesusing our scale, although performance seems tosuffer the more complex a story is.
Since the NaiveBayes classifier is fast (able to classify stories inonly a few seconds) and the entire algorithm fromtraining to advice generation (as detailed below)is fully automatic once a small training corpus hasbeen produced, this technique can be used in real-life tutoring systems and easily ported to other sto-ries.5 Automated AdviceThe plot analysis agent is not meant to give thestudents grades for their stories, but instead usethe automatic ratings as an intermediate step toproduce advice, like other hybrid tutoring systems(Rose et al, 2002).
The advice that the agent cangenerate from the automatic rating classificationis limited to coarse-grained general advice.
How-ever, by inspecting the results of the plot com-parison algorithm, our agent is capable of givingdetailed fine-grained specific advice from the re-lationships of the events in the story.
One tutor-ing system resembling ours is the WRITE sys-tem, but we differ from it by using event struc-ture to represent the information in the system,instead of using rhetorical features (Burstein etal., 2003).
In this regards it more closely resem-bles the physics tutoring system WHY-ATLAS, al-though we deal with narrative stories of a longerlength than physics essays.
The WHY-ATLASphysics tutor identifies missing information in theexplanations of students using theorem-proving(Rose et al, 2002).5.1 Advice Generation AlgorithmDifferent types of stories need different amountsof advice.
An ?Excellent?
story needs less ad-vice than a ?Good?
story.
One advice statement is?general,?
while the rest are specific.
The system861produces a total of seven advice statements for a?Poor?
story, and two less statements for each rat-ing level above ?Poor.
?With the aid of a teacher, a number of ?canned?text statements offering general advice were cre-ated for each rating class.
These include state-ments such as ?It?s very good!
I only have a fewpointers?
for a ?Good?
story and ?Let?s get helpfrom the teacher?
for ?Poor?
story.
The advicegeneration begins by randomly selecting a state-ment suitable for the rating of the story.
Thosestudents whose stories are rated ?Poor?
are askedif they would like to re-read the story and ask ateacher for help.The generation of specific advice uses the re-sults of the plot-comparison algorithm to producespecific advice.
A number of advice templateswere produced, and the results of the Advice Gen-eration Algorithm fill in the needed values of thetemplate.
The ?
most frequent events in ?Excel-lent?
stories are called the Important Event Struc-ture, which represents the ?important?
events inthe story in temporal order.
Empirical experimentsled us ?
= 10 for the ?Adventure?
story, but forlonger stories like the ?Thief?
story a larger ?would be appropriate.
These events correspond tothe ones given the highest weights by the NaiveBayes algorithm.
For each event in the event struc-ture of a rewritten story, a search for a match inthe important event structure is taken.
If a pred-icate name match is found in the important eventstructure, the search continues to attempt to matchthe arguments.
If the event and the arguments donot match, advice is generated using the structureof the ?important?
event that it cannot find in therewritten story.This advice may use both the predicate nameand its arguments, such as ?Did the stork fly?
?from fly(stork).
If an argument is missing, the ad-vice may be about only the argument(s), like ?Canyou tell me more about the stork??
If the event isout of order, advice is given to the student to cor-rect the order, as in ?I think something with thestork happened earlier in the story.
?This algorithm is formalized in Figure 2, withall variables being the same as in the Plot Anal-ysis Algorithm, except that W is the ImportantEvent Structure composed of events w with theset of arguments Nw.
M is a binary vector usedto store the success of a match with index i. TheADV function, given an event, generates one ad-Algorithm5.1: ADVICEGENERATE(W,R)for w ?Wdo????????????????????????????????????????????????
?M = ?i = 0for r ?
Rdo????????????????????????????????
?if w = r or SY N(r)then mi = 1else mi = 0i = i + 1for nw ?
Nwdo??????????
?for nr ?
Nrdo????????
?if nw = SYN(nr) or nrthen mi ?
1else mi ?
0i = i + 1ADV (w,M)Figure 2: Advice Generation Algorithmvice statement to be given to the student.An element of randomization was used to gen-erate a diversity of types of answers.
An ad-vice generation function (ADV ) takes an impor-tant event (w) and its binary matching vector (M )and generates an advice statement for w. Per im-portant event this advice generation function is pa-rameterized so that it has a 10% chance of deliver-ing advice based on the entire event, 20% chanceof producing advice that dealt with temporal or-der (these being parameters being found ideal af-ter testing the algorithm), and otherwise producesadvice based on the arguments.5.2 Advice EvaluationThe plot advice algorithm is run using a randomlyselected corpus of 20 stories, 5 from each plot rat-ing level using the ?Adventure Corpus.?
This pro-duced matching advice for each story, for a totalof 80 advice statements.5.3 Advice RatingAn advice rating scheme was developed to rate theadvice produced in consultation with a teacher.1.
Excellent: The advice was suitable for thestory, and helped the student gain insight intothe story.2.
Good: The advice was suitable for the story,862Rating % GivenExcellent 0Good 35Fair 60Poor 5Table 5: Advice Rating Resultsand would help the student.3.
Fair: The advice was suitable, but shouldhave been phrased differently.4.
Poor: The advice really didn?t make senseand would only confuse the student further.Before testing the system on students, it was de-cided to have teachers evaluate how well the ad-vice given by the system corresponded to the ad-vice they would give in response to a story.
Ateacher read each story and the advice.
They thenrated the advice using the advice rating scheme.Each story was rated for its overall advice quality,and then each advice statement was given com-ments by the teacher, such that we could derivehow each individual piece of advice contributedto the global rating.
Some of the general ?coarse-grained?
advice was ?Good!
You got all the mainparts of the story?
for an ?Excellent?
story, ?Let?smake it even better!?
for a ?Good?
story, and?Reading the story again with a teacher would behelp!?
for a ?Poor?
story.
Sometimes the ad-vice generation algorithm was remarkably accu-rate.
In one story the connection between a cursebeing lifted by the possession of a coin by thecharacter Nils was left out by a student.
The ad-vice generation algorithm produced the followinguseful advice statement: ?Tell me more about thecurse and Nils.?
Occasionally an automatically ex-tracted event that is difficult to interpret by a hu-man or simply incorrectly is extracted.
This in turncan cause advice that does not make any sensecan be produced, such as ?Tell me more about aspot??.
Qualitative analysis showed that ?missingimportant advice?
to be the most significant prob-lem, followed by ?nonsensical advice.
?5.4 ResultsThe results are given in Table 5.
The majority ofthe advice was rated overall as ?fair.?
Only onestory was given ?poor?
advice, and a few weregiven ?good?
advice.
However, most advice ratedas ?good?
was the advice generated by ?excel-lent?
stories, which generate less advice than othertypes of stories.
?Poor?
stories were given almostentirely ?fair?
advice, although once ?poor?
ad-vice was generated.
In general, the teacher found?coarse-grained?
advice to be very useful, and wasvery pleased that the agent could detect when thestudent needed to re-read the story and when a stu-dent did not need to write any more.
In some casesthe specific advice was shown to help provide a?crucial detail?
and help ?elicit a fact.?
The advicewas often ?repetitive?
and ?badly phrased.?
Thespecific advice came under criticism for often not?being directed enough?
and for being ?too literal?and not ?inferential enough.?
The rater noticedthat ?The program can not differentiate betweenan unfinished story...and one that is confused.?
andthat ?Some why, where and how questions couldbe used?
in the advice.6 Conclusion and Future WorkSince the task involved a fine-grained analysis ofthe rewritten story, the use of events that take plotstructure into account made sense regardless ofits performance.
The use of events as structuredfeatures in a machine-learning classifier outper-formed a classifier that relied on a unstructured?bag-of-words?
as features.
The system achievedclose to human performance on rating the stories.Since each of the events used as a feature in themachine-learner corresponds to a particular eventin the story, the features are easily interpretable byother components in the system and interpretableby humans.
This allows these events to be usedin a template-driven system to generate advice forstudents based on the structure of their plot.Extracting events from text is fraught with er-ror, particularly in the ungrammatical and infor-mal domain used in this experiment.
This is oftena failure of our system to detect semantic contentunits through either not including them in chunksor only partially including a single unit in a chunk.Chunking also has difficulty dealing with preposi-tions, embedded speech, semantic role labels, andcomplex sentences correctly.
Improvement in ourability to retrieve semantics would help both storyclassification and advice generation.Advice generation was impaired by the abil-ity to produce directed questions from the eventsusing templates.
This is because while our sys-tem could detect important events and their or-863der, it could not make explicit their connectionthrough inference.
Given the lack of a large-scaleopen-source accessible ?common-sense?
knowl-edge base and the difficulty in extracting infer-ential statements from raw text, further progressusing inference will be difficult.
Progress in ei-ther making it easier for a teacher to make explicitthe important inferences in the text or improvedmethodology to learn inferential knowledge fromthe text would allow further progress.
Tantaliz-ingly, this ability for a reader to use ?inference tograsp points that may not have been explicit in thestory?
is given as the hallmark of truly understand-ing a story by teachers.ReferencesSteven Abney.
1995.
Chunks and dependencies:Bringing processing evidence to bear on syntax.
InJennifer Cole, Georgia Green, and Jerry Morgan,editors, Computational Linguistics and the Founda-tions of Linguistic Theory, pages 145?164.Breck Baldwin.
1997.
CogNIAC : A High PrecisionPronoun Resolution Engine.F.C.
Bartlett.
1932.
Remembering.
Cambridge Uni-versity Press, Cambridge.Johan Bos, Stephen Clark, Mark Steedman, James Cur-ran, and Julia Hockenmaier.
2004.
Wide-coveragesemantic representations from a CCG parser.
In InProceedings of the 20th International Conference onComputational Linguistics (COLING ?04).
Geneva,Switzerland.Jill Burstein, Daniel Marcu, and Kevin Knight.
2003.Finding the WRITE Stuff: Automatic Identificationof Discourse Structure in Student Essays.
IEEE In-telligent Systems, pages 32?39.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R Harshman.
1990.
Indexing by LatentSemantic Analysis.
Journal of the American SocietyFor Information Science, (41):391?407.Christine Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.A.
Graesser, P. Wiemer-Hastings, K. Wiemer-Hastings,D.
Harter, and N. Person.
2000.
Using latent se-mantic analysis to evaluate the contributions of stu-dents in autotutor.
Interactive Learning Environ-ments, 8:149?169.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
LT TTT - A Flexible Tokenisa-tion Tool.
In Proceedings of the Second LanguageResources and Evaluation Conference.Harry Halpin, Johanna Moore, and Judy Robertson.2004.
Automatic analysis of plot for story rewriting.In In Proceedings of Empirical Methods in NaturalLanguage Processing, Barcelona, Spain.Maya Hickmann.
2003.
Children?s Discourse: per-son, space and time across language.
CambridgeUniversity Press, Cambridge, UK.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer Academic.Thomas.
Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The Latent Semantic Anal-ysis theory of the acquisition, induction, and repre-sentation of knowledge.
Psychological Review.B.
Lemaire, S. Mandin, P. Dessus, and G. Denhire.2005.
Computational cognitive models of summa-rization assessment skills.
In In Proceedings of the27th Annual Meeting of the Cognitive Science Soci-ety, Stressa, Italy.Fiona McNeill, Harry Halpin, Ewan Klein, and AlanBundy.
2006.
Merging stories with shallow seman-tics.
In Proceedings of the Knowledge Representa-tion and Reasoning for Language Processing Work-shop at the European Association for ComputationalLinguistics, Genoa, Italy.Erik T. Mueller.
2003.
Story understanding throughmulti-representation model construction.
In GraemeHirst and Sergei Nirenburg, editors, Text Meaning:Proceedings of the HLT-NAACL 2003 Workshop,pages 46?53, East Stroudsburg, PA. Association forComputational Linguistics.Ani Nenkova and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In In Proceedings of the Joint Con-ference of the North American Association for Com-putational Linguistics and Human Language Tech-nologies.
Boston, USA.E.
Riloff.
1999.
Information extraction as a step-ping stone toward story understanding.
In Ash-win Ram and Kenneth Moorman, editors, Computa-tional Models of Reading and Understanding.
MITPress.Judy Robertson and Beth Cross.
2003.
Children?sperceptions about writing with their teacher and theStoryStation learning environment.
Narrative andInteractive Learning Environments: Special Issueof International Journal of Continuing EngineeringEducation and Life-long Learning.Judy Robertson and Peter Wiemer-Hastings.
2002.Feedback on children?s stories via multiple interfaceagents.
In International Conference on IntelligentTutoring Systems, Biarritz, France.C.
Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-tava, and K. VanLehn.
2002.
A hybrid languageunderstanding approach for robust selection of tutor-ing goals.
In International Conference on IntelligentTutoring Systems, Biarritz, France.864
