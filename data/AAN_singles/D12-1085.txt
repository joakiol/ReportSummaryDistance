Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 928?939, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsGenerating Non-Projective Word Order in Statistical LinearizationBernd Bohnet Anders Bjo?rkelund Jonas Kuhn Wolfgang Seeker Sina Zarrie?Institut fu?r Maschinelle SprachverarbeitungUniversity of Stuttgart{bohnetbd,anders,jonas,seeker,zarriesa}@ims.uni-stuttgart.deAbstractWe propose a technique to generate non-projective word orders in an efficient statisti-cal linearization system.
Our approach pre-dicts liftings of edges in an unordered syntac-tic tree by means of a classifier, and uses aprojective algorithm for tree linearization.
Weobtain statistically significant improvementson six typologically different languages: En-glish, German, Dutch, Danish, Hungarian, andCzech.1 IntroductionThere is a growing interest in language-independentdata-driven approaches to natural language genera-tion (NLG).
An important subtask of NLG is sur-face realization, which was recently addressed in the2011 Shared Task on Surface Realisation (Belz etal., 2011).
Here, the input is a linguistic representa-tion, such as a syntactic dependency tree lacking allprecedence information, and the task is to determinea natural, coherent linearization of the words.The standard data-driven approach is to traversethe dependency tree deciding locally at each node onthe relative order of the head and its children.
Theshared task results have proven this approach to beboth effective and efficient when applied to English.It is what federal support should try to achieveSBJROOT OBJNMOD SBJPRDVC OPRD IMFigure 1: A non-projective example from the CoNLL2009 Shared Task data set for parsing (Hajic?
et al 2009).However, the approach can only generate pro-jective word orders (which can be drawn with-out any crossing edges).
Figure 1 shows a non-projective word order: the edge connecting the ex-tracted wh-pronoun with its head crosses anotheredge.
Once what has been ordered relative toachieve, there are no ways of inserting interveningmaterial.
In this case, only ungrammatical lineariza-tions can be produced from the unordered input tree:(1) a.
*It is federal support should try to what achieveb.
*It is federal support should try to achieve whatc.
*It is try to achieve what federal support shouldAlthough rather infrequent in English, non-projective word orders are quite common in lan-guages with a less restrictive word order.
In theselanguages, it is often possible to find a grammati-cally correct projective linearization for a given in-put tree, but discourse coherence, information struc-ture, and stylistic factors will often make speak-ers prefer some non-projective word order.1 Figure2 shows an object fronting example from Germanwhere the edge between the subject and the finiteverb crosses the edge between the object and the fullverb.
Various other constructions, such as extraposi-tion of (relative) clauses or scrambling, can lead tonon-projectivity.
In languages where word order isdriven to an even larger degree by information struc-ture, such as Czech and Hungarian, non-projectivitycan likewise result from various ordering decisions.These phenomena have been studied extensively in1A categorization of non-projective edges in the PragueDependency Treebank (Bo?hmova?
et al 2000) is presented inHajic?ova?
et al(2004).928the linguistic literature, and for certain languages,work on rule-based generation has addressed certainaspects of the problem.Das Mandat will er zuru?ckgeben .the.ACC mandate.ACC want.3SG he.NOM return.INF .NKOA#?SB OC?
?He wants to return the mandate.
?Figure 2: German object fronting with complex verb in-troducing a non-projective edge.In this paper, we aim for a general data-driven ap-proach that can deal with various causes for non-projectivity and will work for typologically dif-ferent languages.
Our technique is inspired bywork in data-driven multilingual parsing, wherenon-projectivity has received considerable attention.In pseudo-projective parsing (Kahane et al 1998;Nivre and Nilsson, 2005), the parsing algorithm isrestricted to projective structures, but the issue isside-stepped by converting non-projective structuresto projective ones prior to training and application,and then restoring the original structure afterwards.Similarly, we split the linearization task in twostages: initially, the input tree is modified by liftingcertain edges in such a way that new orderings be-come possible even under a projectivity constraint;the second stage is the original, projective lineariza-tion step.
In parsing, projectivization is a determin-istic process that lifts edges based on the linear or-der of a sentence.
Since the linear order is exactlywhat we aim to produce, this deterministic conver-sion cannot be applied before linearization.
There-fore, we use a statistical classifier as our initial lift-ing component.
This classifier has to be trained onsuitable data, and it is an empirical question whetherthe projective linearizer can take advantage of thispreceding lifting step.We present experiments on six languages withvarying degrees of non-projective structures: En-glish, German, Dutch, Danish, Czech and Hungar-ian, which exhibit substantially different word orderproperties.
Our approach achieves significant im-provements on all six languages.
On German, wealso report results of a pilot human evaluation.2 Related WorkAn important concept for tree linearization are wordorder domains (Reape, 1989).
The domains are bagsof words (constituents) that are not allowed to be dis-continuous.
A straightforward method to obtain theword order domains from dependency trees and toorder the words in the tree is to use each word andits children as domain and then to order the domainsand contained words recursively.
As outlined in theintroduction, the direct mapping of syntactic trees todomains does not provide the possibility to obtainall possible correct word orders.Linearization systems can be roughly distin-guished as either rule-based or statistical systems.
Inthe 2011 Shared Task on Surface Realisation (Belzet al 2011), the top performing systems were allstatistical dependency realizers (Bohnet et al 2011;Guo et al 2011; Stent, 2011).Grammar-based approaches map dependencystructures or phrase structures to a tree that repre-sents the linear precedence.
These approaches aremostly able to generate non-projective word orders.Early work was nearly exclusively applied to phrasestructure grammars (e.g.
(Kathol and Pollard, 1995;Rambow and Joshi, 1994; Langkilde and Knight,1998)).
Concerning dependency-based frameworks,Bro?ker (1998) used the concept of word order do-mains to separate surface realization from linearprecedence trees.
Similarly, Duchier and Debus-mann (2001) differentiate Immediate Dominancetrees (ID-trees) from Linear Precedence trees (LP-trees).
Gerdes and Kahane (2001) apply a hierarchi-cal topological model for generating German wordorder.
Bohnet (2004) employs graph grammars tomap between dependency trees and linear prece-dence trees represented as hierarchical graphs.
In theframeworks of HPSG, LFG, and CCG, a grammar-based generator produces word order candidates thatmight be non-projective, and a ranker is used to se-lect the best surface realization (Cahill et al 2007;White and Rajkumar, 2009).Statistical methods for linearization have recentlybecome more popular (Langkilde and Knight, 1998;Ringger et al 2004; Filippova and Strube, 2009;Wan et al 2009; He et al 2009; Bohnet et al 2010;Guo et al 2011).
They typically work by travers-ing the syntactic structure either bottom-up (Filip-929pova and Strube, 2007; Bohnet et al 2010) or top-down (Guo et al 2011; Bohnet et al 2011).
Theselinearizers are mostly applied to English and do notdeal with non-projective word orders.
An excep-tion is Filippova and Strube (2007), who contributea study on the treatment of preverbal and postver-bal constituents for German focusing on constituentorder at the sentence level.
The work most similarto ours is that of Gamon et al(2002).
They usemachine-learning techniques to lift edges in a pre-processing step to a surface realizer.
Their objec-tive is the same as ours: by lifting, they avoid cross-ing edges.
However, contrary to our work, they usephrase-structure syntax and focus on a limited num-ber of cases of crossing branches in German only.3 Lifting Dependency EdgesIn this section, we describe the first of the two stagesin our approach, namely the classifier that lifts edgesin dependency trees.
The classifier we aim to trainis meant to predict liftings on a given unordered de-pendency tree, yielding a tree that, with a perfect lin-earization, would not have any non-projective edges.3.1 PreliminariesThe dependency trees we consider are of the formdisplayed in Figure 1.
More precisely, all words (ornodes) form a rooted tree, where every node has ex-actly one parent (or head).
Edges point from headto dependent, denoted in the text by h?
d, where his the head and d the dependent.
All nodes directlyor transitively depend on an artificial root node (de-picted in Figure 1 as the incoming edge to is).We say that a node a dominates a node d if a isan ancestor of d. An edge h ?
d is projective iffh dominates all nodes in the linear span between hand d. Otherwise it is non-projective.
Moreover,a dependency tree is projective iff all its edges areprojective.
Otherwise it is non-projective.A lifting of an edge h?
d (or simply of the noded) is an operation that replaces h ?
d with g ?
d,given that there exists an edge g ?
h in the tree, andundefined otherwise (i.e.
the dependent d is reat-tached to the head of its head).2 When the lifting2The undefined case occurs only when d depends on theroot, and hence cannot be lifted further; but these edges are bydefinition projective, since the root dominates the entire tree.operation is applied n successive times to the samenode, we say the node was lifted n steps.3.2 TrainingDuring training we make use of the projectivizationalgorithm described by Nivre and Nilsson (2005).It works by iteratively lifting the shortest non-projective edges until the tree is projective.
Here,shortest edge refers to the edge spanning over thefewest number of words.
Since finding the shortestedge relies on the linear order, instead of lifting theshortest edge, we lift non-projective edges orderedby depth in the tree, starting with the deepest nestededge.
A lifted version of the tree from Figure 1 isshown in Figure 3.
The edge of what has been liftedthree steps (the original edge is dotted), and the treeis no longer non-projective.It is what federal support should try to achieveSBJROOT OBJOBJNMOD SBJPRDVC OPRD IMFigure 3: The sentence from Figure 1, where what hasbeen assigned a new head (solid line).
The original edgeis dotted.We model the edge lifting problem as a multi-class classification problem and consider nodes oneat a time and ask the question ?How far should thisedge be lifted?
?, where classes correspond to lifting0, 1, 2, ..., n steps.
To create training instances weuse the projectivization algorithm mentioned above.We traverse the nodes of the tree sorted by depth.For multiple nodes at the same depth, ties are brokenby linear order, i.e.
for multiple nodes at the samedepth, the leftmost is visited first.
When a node isvisited, we create a training instance out of it.
Itsclass is determined by the number of steps it wouldbe lifted by the projectivization algorithm given thelinear order (in most cases the class corresponds tono lifting, since most edges are projective).
As wetraverse the nodes, we also execute the liftings (ifany) and update the tree on the fly.The training instances derived are used to train alogistic regression classifier using the LIBLINEARpackage (Fan et al 2008).
The features used forthe lifting classifier are described in Table 1.
Sincewe use linear classifiers, our feature set al con-tains conjunctions of atomic features.
The features930Atomic features?x ?
{w,wp, wgp, wch, ws, wun} morph(x), label(x), lemma(x), PoS(x)?x ?
{wgc, wne, wco} label(x), lemma(x), PoS(x)Complex features?x ?
{w,wp, wgp} lemma(x)+PoS(x), label(x)+PoS(x), label(x)+lemma(x)?x ?
{wch, ws, wun}, y = w lemma(x)+lemma(y), PoS(y)+lemma(x), PoS(y)+lemma(x)?x ?
{w,wp, wgp}, y = HEAD(x) lemma(x)+lemma(y), lemma(x)+PoS(y), PoS(x)+lemma(y)?x ?
{w,wp, wgp}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)?x ?
{wch, ws, wun}, y = HEAD(x), z = HEAD(y) PoS(x)+PoS(y)+PoS(z), label(x)+label(y)+label(z)Non-binary features?x ?
{w,wp, wgp} SUBTREESIZE(x), RELSUBTREESIZE(x)Table 1: Features used for lifting.
w refers to the word (dependent) in question.
And with respect to w, wp is theparent; wgp is the grandparent; wch are children; ws are siblings; wun are uncles (i.e.
children of the grandparent,excluding the parent); wgc are grandchildren; wne are nephews (i.e.
grandchildren of the parent that are not childrenof w); wco are cousins (i.e.
grandchildren of the grandparent that are not w or siblings of w).
The non-binary featurefunctions refer to: SUBTREESIZE ?
the absolute number of nodes below x, RELSUBTREESIZE ?
the relative size ofthe subtree rooted at x with respect to the whole tree.involve the lemma, dependency edge label, part-of-speech tag, and morphological features of the nodein question, and of several neighboring nodes in thedependency tree.
We also have a few non-binary fea-tures that encode the size of the subtree headed bythe node and its ancestors.We ran preliminary experiments to determine theoptimal architecture.
First, other ways of modelingthe liftings are conceivable.
To find new reattach-ment points, Gamon et al(2002) propose two otherways, both using a binary classifier: applying theclassifier to each node x along the path to the rootasking ?Should d be reattached to x??
; or lifting onestep at a time and applying the classifier iterativelyuntil it says stop.
They found that the latter outper-formed the former.
We tried this method, but foundthat it was inferior to the multi-class model and morefrequently over- or underlifted.Second, to avoid data sparseness for infrequentlifting distances, we introduce a maximum numberof liftings.
We found that a maximum of 3 gave thebest performance.
In the pseudocode below, we re-fer to this number as maxsteps.3 This means that weare able to predict the correct lifting for most (butnot all) of the non-projective edges in our data sets(cf.
Table 3).Third, as Nivre and Nilsson (2005) do for pars-3During training, nodes that are lifted further than maxstepsare assigned to the class corresponding to maxsteps.
This ap-proach worked better than ignoring the training instance ortreating it as a non-lifting (i.e.
a lifting of 0 steps).ing, we experimented with marking edges that werelifted by indicating this on the edge labels.
In thecase of parsing, this step is necessary in order to re-verse the liftings in the parser output.
In our case,it could potentially be beneficial for both the liftingclassifier, and for the linearizer.
However, we foundthat marking liftings at best gave similar results asnot marking, so we kept the original labels withoutmarking.3.3 DecodingIn the decoding stage, an unordered tree is given andthe goal is to lift edges that would be non-projectivewith respect to the gold linear order.
Similarly tohow training instances are derived, the decoding al-gorithm traverses the tree bottom-up and visits everynode once.
Ties between nodes at the same depth arebroken in an arbitrary but deterministic way.
Whena node is visited, the classifier is applied and the cor-responding lifting is executed.
Pseudocode is givenin Algorithm 1.4Different orderings of nodes at the same depthcan lead to different lifts.
The reason is that lift-ings are applied immediately and this influences thefeatures when subsequent nodes are considered.
Forinstance, consider two sibling nodes ni and nj .
Ifni is visited before nj , and ni is lifted, this means4The MIN function is used to guarantee that the edge is notlifted beyond the root node of the tree.
This does not happenin practice though, since the feature set of the classifier includefeatures that implicitly encode the proximity to the root node.931that at the time we visit nj , ni is no longer a siblingof nj , but rather an uncle.
An obvious extension ofthe decoding algorithm presented above is to applybeam search.
This allows us to consider nj both inthe context where ni has been lifted and when it hasnot been lifted.1 N?
NODES(T )2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )3 foreach node ?
N do4 feats?
EXTRACT-FEATURES(node, T )5 steps?
CLASSIFY(feats)6 steps?
MIN(steps,ROOT-DIST(node))7 LIFT(node, T, steps)8 return TAlgorithm 1: Greedy decoding for lifting.Pseudocode for the beam search decoder is givenin Algorithm 2.
The algorithm keeps an agenda oftrees to explore as each node is visited.
For everynode, it clones the current tree and applies every pos-sible lifting.
Every tree also has an associated score,which is the sum of the scores of each lifting so far.The score of a lifting is defined to be the log proba-bility returned from the logistic classifier.
After ex-ploring all trees in the agenda, the k-best new treesfrom the beam are extracted and put back into theagenda.
When all nodes have been visited, the besttree in the agenda is returned.
For the experimentsthe beam size (k in Algorithm 2) was set to 20.1 N?
NODES(T )2 SORT-BY-DEPTH-BREAK-TIES-ARBITRARILY(N,T )3 Tscore ?
04 Agenda?
{T}5 foreach node ?
N do6 Beam?
?7 foreach tree ?
Agenda do8 feats?
EXTRACT-FEATURES(node, tree)9 m?
MIN(maxsteps,ROOT-DIST(node))10 foreach s ?
0 .. maxsteps do11 t?
CLONE(tree)12 score?
GET-LIFT-SCORE(feats, s)13 tscore = tscore + score14 LIFT(node, t, s)15 Beam?
Beam ?
{t}16 Agenda?
EXTRACTKBEST(Beam, k)17 return EXTRACTKBEST(Agenda, 1)Algorithm 2: Beam decoding for lifting.While beam search allows us to explore the searchspace somewhat more thoroughly, a large number ofpossibilities remain unaccounted for.
Again, con-sider the sibling nodes ni and nj when ni is visitedbefore nj .
The beam allows us to consider nj bothwhen ni is lifted and when it is not.
However, thesituation where nj is visited before ni is still neverconsidered.
Ideally, all permutations of nodes at thesame depth should be explored before moving on.Unfortunately this leads to a combinatorial explo-sion of permutations, and exhaustive search is nottractable.
As an approximation, we create two or-derings and run the beam search twice.
The dif-ference between the orderings is that in the secondone all ties are reversed.
As this bibeam consistentlyimproved over the beam in Algorithm 2, we onlypresent these results in Section 5 (there denoted sim-ply Beam).4 LinearizationA linearizer searches for the optimal word ordergiven an unordered dependency tree, where the op-timal word order is defined as the single referenceorder of the dependency tree in the gold standard.We employ a statistical linearizer that is trained on acorpus of pairs consisting of unordered dependencytrees and their corresponding sentences.
The lin-earization method consists of the following steps:Creating word order domains.
In the first step,we build the word order domains dh for all nodesh ?
y of a dependency tree y.
A domain is definedas a node and all of its direct dependents.
For ex-ample, the tree shown in Figure 3 has the followingdomains: {it, be, should}, {what, support, should, try},{federal, support}, {try, to}, {to, achieve}If an edge was lifted before the linearization, thelifted node will end up in the word order domain ofits new head rather than in the domain of its originalhead.
This way, the linearizer can deduce word or-ders that would result in non-projective structures inthe non-lifted tree.Ordering the words of the domains.
In the sec-ond step, the linearizer orders the words of each do-main.
The position of a subtree is determined by theposition of the head of the subtree in the enclosingdomain.
Algorithm 3 shows the tree linearizationalgorithm.
In our implementation, the linearizer tra-verses the tree either top-down or bottom-up.9321 // T is the dependency tree with lifted nodes2 beam-size?
10003 for h ?
T do4 domainh?
GET-DOMAIN(T ,h)5 // initialize the beam with a empty word list6 Agendah?
()7 foreach w ?
domainh do8 // beam for extending word order lists9 Beam?
()10 foreach l ?
Agendah do11 // clone list l and append the word w12 if w 6?
l then13 l?
?
APPEND(l,m)14 Beam?
Beam ?
l?15 score[l?]?
COMPUTE-SCORE(l?
)16 if | Beam | > beam-size then17 SORT-LISTS-DESCENDING-TO-SCORE(Beam,score)18 Agendah?
SUBLIST(0,beam-size,Beam)19 else20 Agendah?
Beam21 foreach l ?
Beam do22 SCOREg[l]?
SCORE[l] +GLOBAL-SCORE(l)23 Agendah?
Beam24 return BeamAlgorithm 3: Dependency Tree Linearization.The linearization algorithm initializes the wordorder beam (agendah) with an empty order () (line6).
It then iterates over the words of a domain (lines7-20).
In the first iteration, the algorithm clones andextends the empty word order list () by each wordof the sentence (line 12-15).
If the beam (beam)exceeds a certain size (beam-size), it is sorted byscore and pruned to maximum beam size (beam-size) (lines 16-20).
The following example illus-trates the extensions of the beam for the top domainshown in Figure 3.Iter.
agendabe0: ()1: ((it) (be) (should))2: ((it be) (it should) (be it) (be should) ...)The beam enables us to apply features that encodeinformation about the first tokens and the last token,which are important for generating, e.g.
the wordorder of questions, i. e. if the last token is a questionmark then the sentence should probably be a ques-tion (cf.
feature set shown in Table 2).
Furthermore,the beam enables us to generate alternative lineariza-tions.
For this, the algorithm iterates over the alter-native word orders of the domains in order to as-semble different word orders on the sentence level.5Finally, when traversing the tree bottom-up, the al-gorithm has to use the different orders of the alreadyordered subtrees as context, which also requires asearch over alternative word orders of the domains.Training of the Linearizer.
We use MIRA(Crammer et al 2006) for the training of the lin-earizer.
The classifier provides a score that we use torank the alternative word orders.
Algorithm 3 callstwo functions to compute the score: compute-score(line 15) for features based on pairs of words and tri-grams and compute-global-score for features basedon word patterns of a domain.
Table 2 shows thefeature set for the two functions.
In the case that thelinearization of a word order domain is incorrect thealgorithm updates its weight vector w. The follow-ing equation shows the update function of the weightvector:w = w + ?h(?
(dh, T, xg)?
?
(dh, T, xp))We update the weight vector w by adding the dif-ference of the feature vector representation of thecorrect linearization xg and the wrongly predictedlinearization xp, multiplied by ?
.
?
is the passive-aggressive update factor as defined below.
The suf-fered lossh is ?
(dh, T, xp)?
?
(dh, T, xg).?
= lossh||?(dh,T,xg)??
(dh,T,xp)||2Creating the word order of a sentence.
The lin-earizer traverses the tree either top-down or bottom-up and assembles the results in the surface order.The bottom-up linearization algorithm can take intoaccount features drawn from the already orderedsubtrees while the top-down algorithm can employas context only the unordered nodes.
However, thebottom-up algorithm additionally has to carry out asearch over the alternative linearization of the sub-domains, as different orders of the subdomain pro-vide different context features.
This leads to a higherlinearization time.
We implemented both, but couldonly find a rather small accuracy difference.
In thefollowing, we therefore present results only for thetop-down method.5The beam also makes it possible to employ a generativelanguage model to rerank alternative linearizations.933Atomic featuresFor nodes w ?domainhlemma(w), label(w), PoS(w), num-children(w), num-grandchildren(w), label-children(w),PoS-children(w)For domaindomainhhead(w1,w2), head(w1,w2,w3), label(head), PoS(head), PoS(w1), label(wn), label(wn?1),contains-?
(domainh)Complex featuresFor bigrams(w1, w2) ?domainhfeat2: label(w1)+label(w2), label(w1)+lemma(w2), lemma(w1)+lemma(w2), PoSw1+PoSw2feat3: label(w1)+num-children(w2)+num-children(w1),PoS-child(w1)+label(w1)+label(w2)feat4: label(w1)+label(w2)+lemma(w2)+PoS(w1), label(w1)+label(w2)+PoS(head)+head(w1,w2)feat5: label(w1)+label(w2)+PoS(head)+label(head)+head(w1,w2)For trigrams(w1, w2, w3) ?domainhfeat3: lemma(w1)+lemma(w2)+lemma(w3)feat4: PoS(w1)+PoS(w2)+PoS(w3)+head(w1,w2,w3)feat5: label(w1)+label(w2)+label(w3)+PoS(w1)+head(w1,w2,w3)For sentence s feat6: label(w1)+label(wn?1)+lemma(head)+lemma(w1)+lemma(wn?1)feat7: PoS(w1)+PoS(w2)+PoS(w3)+PoS(wn?1)+PoS(wn?2)+PoS(wn?3)+contains-?
(s)Table 2: Exemplified features used for scoring linearizations of a word order domain (see Algorithm 3).
Atomicfeatures which represent properties of a node or a domain are conjoined into feature vectors of different lengths.Linearizations are scored based on bigrams, trigrams, and global sentence-level features.5 ExperimentsWe conduct experiments on six European languageswith varying degrees of word order restrictions:While English word order is very restrictive, Czechand Hungarian exhibit few word order constraints.Danish, Dutch, and German (so-called V2, i. e.verb-second, languages) show a relatively free wordorder that is however more restrictive than in Hun-garian or Czech.
The English and the Czech dataare from the CoNLL 2009 Shared Task data sets(Hajic?
et al 2009).
The Danish and the Dutch dataare from the CoNLL 2006 Shared Task data sets(Buchholz and Marsi, 2006).
For Hungarian, we usethe Hungarian Dependency Treebank (Vincze et al2010), and for German, we use a dependency con-version by Seeker and Kuhn (2012).# sent?s np sent?s np edges np ?
3 liftsEnglish 39,279 7.63 % 0.39% 98.39%German 36,000 28.71% 2.34% 94.98%Dutch 13,349 36.44% 5.42% 99.80%Danish 5,190 15.62 % 1.00% 96.72%Hungarian 61,034 15.81% 1.45% 99.82%Czech 38,727 22.42% 1.86% 99.84%Table 3: Size of training sets, percentage of non-projective (np) sentences and edges, percentage of npedges covered by 3 lifting steps.Table 3 shows the sizes of the training corporaand the percentage of non-projective sentences andedges in the data.
Note that the data sets for Dan-ish and Dutch are quite small.
English has the leastpercentage of non-projective edges.
Czech, Ger-man, and Dutch show the highest percentage of non-projective edges.
The last column shows the per-centage of non-projective edges that can be madeprojective by at most 3 lifting steps.5.1 SetupIn our two-stage approach, we first train the liftingclassifier.
The results for this classifier are reportedin Section 5.2.Second, we train the linearizer on the output ofthe lifting classifier.
To assess the impact of thelifting technique on linearization, we built four sys-tems on each language: (a) a linearizer trained onthe original, non-lifted dependency structures (No-lift), two trained on the automatically lifted edges(comparing (b) the beam and (c) greedy decoding),(d) one trained on the oracle, i. e. gold-lifted struc-tures, which gives us an upper bound for the liftingtechnique.
The linearization results are reported inSection 5.3.In this two-stage setup, we have the problem that,if we re-apply the lifting classifier on the data it wastrained on, the input for the linearizer will be betterduring training than during testing.
To provide real-istic training data for the linearizer, we make a 10-fold cross-validation of the lifting classifier on thetraining set, and use this as training data for the lin-earizer.
The lifting classifier that is applied to thetest set is trained on the entire training set.9345.2 Lifting resultsTo evaluate the performance of the lifting classifier,we present precision, recall, and F-measure resultsfor each language.
We also compute the percentageof sentences that were handled perfectly by the lift-ing classifier.
Precision and recall are defined theusual way in terms of true positives, false positives,and false negatives, where true positives are edgesthat should be lifted and were lifted correctly; falsepositives are edges that should not be lifted but wereand edges that should be lifted and were lifted, butwere reattached in the wrong place; false negativesare edges that should be lifted but were not.The performance of both the greedy decoder andthe bibeam decoder are shown in Table 4.
The scoresare taken on the cross-validation on the training set,as this provides more reliable figures.
The scoresare micro-averaged, i.e.
all folds are concatenatedand compared to the entire training set.Although the major evaluation of the lifting isgiven by the performance of the linearizer, Table 4gives us some clues about the lifting.
We see thatprecision is generally much higher than recall.
Webelieve this is related to the fact that some phenom-ena encoded by non-projective edges are more sys-tematic and thus easier to learn than others (e. g. wh-extraction vs. relative clause extraposition).
We alsofind that beam search consistently yields modest in-creases in performance.Greedy BeamP R F1 Perfect P R F1 PerfectEng 77.31 50.45 61.05 95.76 78.85 50.63 61.66 95.83Ger 72.33 63.59 67.68 81.91 72.05 64.41 68.02 81.97Dut 76.66 74.89 75.77 79.28 78.07 76.49 77.27 80.34Dan 85.90 58.55 69.64 92.76 85.90 58.55 69.64 92.74Hun 72.60 61.61 66.66 88.46 73.06 64.77 68.67 88.73Cze 77.79 55.00 64.44 86.28 77.31 55.68 64.74 86.33Table 4: Precision, recall, F-measure and perfect projec-tivization results for the lifting classifier.5.3 Linearization Results and DiscussionWe evaluate the linearizer with standard metrics: n-gram overlap measures (BLEU, NIST), edit distance(Edit), and the proportion of exactly linearized sen-tences (Exact).
As a means to assess the impact oflifting more precisely, we propose the word-basedmeasure Exactlift which only looks at the wordswith an incoming lifted edge.
The Exactlift scorethen corresponds to the percentage of these wordsthat has been realized in the exact same position asin the original sentence.LangLift BLEU NIST Edit Exact Exactlift NliftEngNolift 0.911 15.09 0.922 56.40 0.00 0EngGreedy 0.914 15.10 0.923 57.27 59.87 152EngBeam 0.916 15.11 0.925 58.48 62.82 156EngOracle 0.923 15.14 0.928 60.73 70.42 240GerNolift 0.792 13.76 0.844 40.4 0.00 0GerGreedy 0.811 13.86 0.864 42.9 55.21 480GerBeam 0.813 13.86 0.866 43.3 56.47 487GerOracle 0.843 13.97 0.889 49.95 72.87 634DutNolift 0.743 11.31 0.796 30.05 0.00 0DutGreedy 0.784 11.47 0.797 37.56 41.02 256DutBeam 0.778 11.46 0.8 37.05 47.45 255DutOracle 0.825 11.63 0.848 44.82 70.55 292DanNolift 0.836 11.80 0.886 44.41 0.00 0DanGreedy 0.852 11.88 0.90 45.96 67.65 34DanBeam 0.858 11.90 0.90 48.76 67.65 34DanOracle 0.865 11.92 0.90 50.93 74.42 43HunNolift 0.755 15.70 0.839 30.71 0.00 0HunGreedy 0.764 15.71 0.844 31.98 41,81 1,538HunBeam 0.764 15.71 0.844 31.98 41.37 1,581HunOracle 0.777 15.79 0.849 34.30 57.53 1,933CzeNolift 0.693 14.32 0.789 25.14 0.00 0CzeGreedy 0.711 14.45 0.797 26.85 42.04 923CzeBeam 0.712 14.45 0.795 26.37 41.34 941CzeOracle 0.729 14.52 0.806 28.79 53.12 1,282Table 5: Performance of linearizers using different lift-ings, Exactlift is the exact match for words with an in-coming lifted edge, Nlift is the total number of liftededges.The results are presented in Table 5.
On eachlanguage, the predicted liftings significantly im-prove on the non-lifted baseline (except the greedydecoding in English).6 The differences betweenthe beam and the greedy decoding are not signif-icant.
The scores on the oracle liftings suggestthat the impact of lifting on linearization is heav-ily language-dependent: It is highest on the V2-languages, and somewhat smaller on English, Hun-garian, and Czech.
This is not surprising since theV2-languages (especially German and Dutch) havethe highest proportion of non-projective edges andsentences (see Table 3).
On the other hand, En-glish has a very small number of non-projectiveedges, such that the BLEU score (which capturesthe n-gram level) reflects the improvement by only6We used a t-test, with ?
= 0.01.935a small increase.
However, note that, on the sen-tence level, the percentage of exactly regeneratedsentences increases by 2 points which suggests thata non-negligible amount of non-projective sentencescan now be generated more fluently.50556065707580EngGerDutDanHunCzelanguageaccuracyperiphery left rightFigure 4: Accuracy for the linearization of the sentences?left and right periphery, the bars are upper and lowerbounds of the non-lifted and the gold-lifted baseline.The Exactlift measure refines this picture: Thelinearization of the non-projective edges is relativelyexact in English, and much less precise in Hungarianand Czech where Exactlift is even low on the gold-lifted edges.
The linearization quality is also quitemoderate on Dutch where the lifting leads to con-siderable improvements.
These tendencies point tosome important underlying distinctions in the non-projective word order phenomena over which weare generalizing: In certain cases, the linearizationseems to systematically follow from the fact that theedge has to be lifted, such as wh-extraction in En-glish (Figure 1).
In other cases, the non-projectivelinearization is just an alternative to other grammati-cal, but maybe less appropriate, realizations, such asthe prefield-occupation in German (Figure 2).Since a lot of non-projective word orders affectthe clause-initial or clause-final position, we evalu-ate the exact match of the left periphery (first threewords) and the right periphery (last three words) ofthe sentence.
The accuracies obtained are plottedin Figure 4, where the lower and upper bars corre-spond to the lower and upper bound from the non-lifted and the gold-lifted baseline.
It clearly emergesfrom this figure that the range of improvements ob-tainable from lifting is closely tied to the generallinearization quality, and also to word order prop-erties of the languages.
Thus, the range of sentencesaffected by the lifting is clearly largest for the V2-languages.
The accuracies are high, but the rangesare small for English, whereas the accuracies are lowand the ranges quite small for Czech and Hungarian.System BLEU NIST(Bohnet et al 2011) (ranked 1st) 0.896 13.93(Guo et al 2011) (ranked 2nd) 0.862 13.68Baseline-Non-Lifted + LM 0.896 13.94Beam-Lifted + LM 0.901 13.96Table 6: Results on the development set of the 2011Shared Task on Surface Realisation data, (the test set wasnot officially released).We also evaluated our linearizer on the data of2011 Shared Task on Surface Realisation, which isbased on the English CoNLL 2009 data (like ourprevious evaluations) but excludes information onmorphological realization.
For training and evalu-ation, we used the exact set up of the Shared Task.For the morphological realization, we used the mor-phological realizer of Bohnet et al(2010) that pre-dicts the word form using shortest edit scripts.
Forthe language model (LM), we use a 5-gram modelwith Kneser-Ney (Kneser and Ney, 1995) smoothingderived from 11 million sentences of the Wikipedia.In Table 6, we compare our two linearizers (withand without lifting) to the two top systems of the2011 Shared Task on Surface Realisation, (Bohnet etal., 2011) and (Guo et al 2011).
Without the lifting,our system reaches a score comparable to the top-ranked system in the Shared Task.
With the lifting,we get a small7 but statistically significant improve-ment in BLEU such that our system reaches a higherscore than the top ranked systems.
This shows thatthe improvements we obtain from the lifting carryover to more complex generation tasks which in-clude morphological realization.5.4 Human EvaluationWe have carried out a pilot human evaluation on theGerman data in order to see whether human judgesprefer word orders obtained from the lifting-based7Remember that English has the least percentage of non-projective edges in our data sets, which are however importantto linearize correctly (see Figure 1).936linearizer.
In particular, we wanted to check whetherthe lifting-based linearizer produces more naturalword orders for sentences that had a non-projectivetree in the corpus, and maybe less natural word or-ders on originally projective sentences.
Therefore,we divided the evaluated items into originally pro-jective and non-projective sentences.We asked four annotators to judge 60 sentencepairs comparing the lifting-based against the non-lifted linearizer using the toolkit by Kow and Belz(2012).
All annotators are students, two of themhave a background in linguistics.
The items wererandomly sampled from the subset of the develop-ment set containing those sentences where the lin-earizers produced different surface realizations.
Theitems are subdivided into 30 originally projectiveand 30 originally non-projective sentences.For each item, we presented the original contextsentence from the corpus and the pair of automat-ically produced linearizations for the current sen-tence.
The annotators had to decide on two crite-ria: (i) which sentence do they prefer?
(ii) how flu-ent is that sentence?
In both cases, we used con-tinuous sliders as rating tools, since humans seemto prefer them (Belz and Kow, 2011).
For the firstcriterion, the slider positions were mapped to valuesfrom -50 (preference for left sentence) to 50 (pref-erence for right sentence).
If the slider position iszero, both sentences are equally preferred.
For thesecond criterion, the slider positions were mappedto values from 0 (absolutely broken sentence) to 100(perfectly fluent sentence).Sentences Scores Equal Lifted Non-liftedAll% selected 44.58% 35.0% 20.42%Fluency 56.14 75.77 72.78Preference 0 34.75 31.06Non-Proj.% selected 29.63% 58.33% 12.04%Fluency 43.06 76.27 68.85Preference 0 37.52 24.46Proj.% selected 56.82% 15.91% 27.27%Fluency 61.72 74.29 74.19Preference 0 26.43 33.44Table 7: Results from human evaluation.Table 7 presents the results averaged over all sen-tences, as well as for the subsets of non-projectiveand projective sentences.
We report the percentageof items where the judges selected both, the lifted, ornon-lifted sentence, alongside with the average flu-ency score (0-100) and preference strength (0-50).On the entire set of items, the judges selected bothsentences in almost half of the cases.
However, onthe subset of non-projective sentences, the lifted ver-sion is clearly preferred and has a higher averagefluency and preference strength.
The percentage ofzero preference items is much higher on the sub-set of projective sentences.
Moreover, the averagefluency of the zero preference items is remarkablyhigher on the projective sentences than on the non-projective subset.
We conclude that humans havea strong preference for lifting-based linearizationson non-projective sentences.
We attribute the lowfluency score on the non-projective zero preferenceitems to cases where the linearizer did not get a cor-rect lifting or could not linearize the lifting correctlysuch that the lifted and the non-lifted version werenot appropriate.
On the other hand, incorrect lift-ings on projective sentences do not necessarily seemto result in deprecated linearizations, which leads tothe high percentage of zero preferences with a goodaverage fluency on this subset.6 ConclusionWe have presented a novel technique to linearizesentences for a range of languages that exhibit non-projective word order.
Our approach deals with non-projectivity by lifting edges in an unordered inputtree which can then be linearized by a standard pro-jective linearization algorithm.We obtain significant improvements for thelifting-based linearization on English, German,Dutch, Danish, Czech and Hungarian, and show thatlifting has the largest impact on the V2-languages.In a human evaluation carried out on German wealso show that human judges clearly prefer lifting-based linearizations on originally non-projectivesentences, and, on the other hand, that incorrect lift-ings do not necessarily result in bad realizations ofthe sentence.AcknowledgmentsThis work was funded by the Deutsche Forschungs-gemeinschaft (DFG) via the SFB 732 ?IncrementalSpecification in Context?.
We would also like tothank Anna Ha?tty and our four annotators for theircontribution to the human evaluation.937ReferencesA.
Belz and E. Kow.
2011.
Discrete vs.
Continuous Rat-ing Scales for Language Evaluation in NLP.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 230?235, Portland, Oregon, USA,June.
Association for Computational Linguistics.A.
Belz, M. White, D. Espinosa, D. Hogan, E. Kow, andA.
Stent.
2011.
The First Surface Realisation SharedTask: Overview and Evaluation Results.
In ENLG?11.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2000.The Prague Dependency Treebank: A Three-level an-notation scenario.
In A.
Abeille?, editor, Treebanks:Building and using syntactically annotated corpora.,chapter 1, pages 103?127.
Kluwer Academic Publish-ers, Amsterdam.B.
Bohnet, L. Wanner, S. Mille, and A. Burga.
2010.Broad coverage multilingual deep sentence generationwith a stochastic multi-level realizer.
In Coling 2010,pages 98?106.B.
Bohnet, S. Mille, B. Favre, and L. Wanner.
2011.<stumaba>: From deep representation to surface.
InProceedings of the Generation Challenges Session atthe 13th European Workshop on NLG, pages 232?235,Nancy, France.B.
Bohnet.
2004.
A Graph Grammar Approach to MapBetween Dependency Trees and Topological Models.In IJCNLP, pages 636?645.N.
Bro?ker.
1998.
Separating Surface Order and SyntacticRelations in a Dependency Grammar.
In COLING-ACL 98.S.
Buchholz and E. Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Natu-ral Language Learning, pages 149?164, Morristown,NJ, USA.
Association for Computational Linguistics.A.
Cahill, M. Forst, and C. Rohrer.
2007.
Stochastic real-isation ranking for a free word order language.
ENLG?07, pages 17?24.K.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2006.
Online Passive-Aggressive Algorithms.
Jour-nal of Machine Learning Research, 7:551?585.D.
Duchier and R. Debusmann.
2001.
Topological de-pendency trees: A constraint-based account of linearprecedence.
In Proceedings of the ACL.R.
Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008.LIBLINEAR: A library for large linear classification.Journal of Machine Learning Research, 9:1871?1874.K.
Filippova and M. Strube.
2007.
Generating con-stituent order in german clauses.
In ACL, pages 320?327.K.
Filippova and M. Strube.
2009.
Tree linearization inEnglish: improving language model based approaches.In NAACL, pages 225?228, Morristown, NJ, USA.
As-sociation for Computational Linguistics.M.
Gamon, E. Ringger, R. Moore, S. Corston-Olivier,and Z. Zhang.
2002.
Extraposition: A case study inGerman sentence realization.
In Proceedings of Col-ing 2002.
Association for Computational Linguistics.K.
Gerdes and S. Kahane.
2001.
Word order in german:A formal dependency grammar using a topological hi-erarchy.
In Proceedings of the ACL.Y.
Guo, D. Hogan, and J. van Genabith.
2011.
Dcu atgeneration challenges 2011 surface realisation track.In Proceedings of the Generation Challenges Sessionat the 13th European Workshop on NLG, pages 227?229.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.-A.
Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,J.
Stepa?nek, P. Strana?k, M. Surdeanu, N. Xue, andY.
Zhang.
2009.
The CoNLL-2009 shared task:Syntactic and Semantic dependencies in multiple lan-guages.
In Proceedings of the 13th CoNLL SharedTask, pages 1?18, Boulder, Colorado.E.
Hajic?ova?, J. Havelka, P. Sgall, K.
Vesela?, and D. Ze-man.
2004.
Issues of projectivity in the prague de-pendency treebank.
Prague Bulletin of MathematicalLinguistics, 81.W.
He, H. Wang, Y. Guo, and T. Liu.
2009.
DependencyBased Chinese Sentence Realization.
In Proceedingsof the ACL and of the IJCNLP, pages 809?816.S.
Kahane, A. Nasr, and O. Rambow.
1998.
Pseudo-projectivity: A polynomially parsable non-projectivedependency grammar.
In COLING-ACL, pages 646?652.A.
Kathol and C. Pollard.
1995.
Extraposition via com-plex domain formation.
In Meeting of the Associationfor Computational Linguistics, pages 174?180.R.
Kneser and H. Ney.
1995.
In In Proceedings of theIEEE International Conference on Acoustics, Speechand Signal Processing, pages 181?184.E.
Kow and A. Belz.
2012.
LGRT-Eval: A Toolkit forCreating Online Language Evaluation Experiments.In Proceedings of the 8th International Conference onLanguage Resources and Evaluation (LREC?12).I.
Langkilde and K. Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InCOLING-ACL, pages 704?710.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective de-pendency parsing.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 99?106, Ann Arbor, Michigan,June.
Association for Computational Linguistics.O.
Rambow and A. K. Joshi.
1994.
A formal look atdependency grammars and phrase-structure grammars,with special consideration of word-order phenomena.938In Leo Wanner, editor, Current Issues in Meaning-TextTheory.
Pinter, London, UK.M.
Reape.
1989.
A logical treatment of semi-free wordorder and bounded discontinuous constituency.
InProceedings of the EACL, EACL ?89, pages 103?110.E.
Ringger, M. Gamon, R. C. Moore, D. Rojas, M. Smets,and S. Corston-Oliver.
2004.
Linguistically informedstatistical models of constituent structure for orderingin sentence realization.
In COLING ?04, pages 673?679.W.
Seeker and J. Kuhn.
2012.
Making Ellipses Explicitin Dependency Conversion for a German Treebank.
InProceedings of LREC 2012, Istanbul, Turkey.
Euro-pean Language Resources Association (ELRA).A.
Stent.
2011.
Att-0: Submission to generation chal-lenges 2011 surface realization shared task.
In Pro-ceedings of the Generation Challenges Session at the13th European Workshop on Natural Language Gener-ation, pages 230?231, Nancy, France, September.
As-sociation for Computational Linguistics.V.
Vincze, D. Szauter, A.
Alma?si, G. Mo?ra, Z. Alexin,and J. Csirik.
2010.
Hungarian Dependency Tree-bank.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC 2010), pages 1855?1862, Valletta, Malta.S.
Wan, M. Dras, R. Dale, and C. Paris.
2009.
Improvinggrammaticality in statistical sentence generation: In-troducing a dependency spanning tree algorithm withan argument satisfaction model.
In EACL, pages 852?860.M.
White and R. Rajkumar.
2009.
Perceptron rerankingfor CCG realization.
In EMNLP?09, pages 410?419,Singapore, August.939
