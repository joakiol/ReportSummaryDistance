Proceedings of NAACL-HLT 2013, pages 585?595,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsProcessing Spontaneous OrthographyRamy Eskander, Nizar Habash, Owen Rambow, and Nadi TomehCenter for Computational Learning SystemsColumbia University{reskander,habash,rambow,nadi}@ccls.columbia.eduAbstractIn cases in which there is no standard or-thography for a language or language vari-ant, written texts will display a variety of or-thographic choices.
This is problematic fornatural language processing (NLP) because itcreates spurious data sparseness.
We studythe transformation of spontaneously spelledEgyptian Arabic into a conventionalized or-thography which we have previously proposedfor NLP purposes.
We show that a two-stageprocess can reduce divergences from this stan-dard by 69%, making subsequent processingof Egyptian Arabic easier.1 IntroductionIn areas with diglossia, vernacular spoken variants(?low?)
of a language family co-exist with a largelywritten variant (?high?
), which is often not spokenas a native language.
Traditionally, the low variantshave not been written: written language is reservedfor formal occasions and in those formal occasionsonly the high variant is used.
Prototypical exam-ples of diglossia are the German speaking parts ofSwitzerland, and the Arab world.
The advent of theinternet has changed linguistic behavior: it is nowcommon to find written informal conversations, inthe form of email exchanges, text messages, Twit-ter exchanges, and interactions on blogs and in webforums.
These written conversations are typicallywritten in the low variants (or in a mixture of low andhigh), since conversations in the high variant seemunnatural to the discourse participants.
For naturallanguage processing (NLP), this poses many chal-lenges, one of which is the fact that the low vari-ants have not been written much in the past anddo not have a standard orthography which is gen-erally agreed on by the linguistic community (andperhaps sanctioned by an authoritative institution).Instead, each discourse participant devises a spon-taneous orthography, in which she chooses amongconventions from the high variant to render the spo-ken language.
We are thus faced with a large numberof ways to spell the same word, none of which canbe assumed as ?standard?
since there is no standard.As a result, the increased data sparseness adds to thechallenges of NLP tasks such as machine transla-tion, compared to languages for which orthographyis standardized.In this paper, we work on Egyptian Arabic(EGY).
We follow the conventions which we havepreviously proposed for the normalized orthogra-phy for EGY (Habash et al 2012), called CODA(Conventional Orthography for Dialectal Arabic).
Inthis paper, we investigate how easy it is to con-vert spontaneous orthography of EGY written inArabic script into CODA orthography automatically.We will refer to this process as ?normalization?
or?codafication?.
We present a freely available systemcalled CODAFY, which we propose as a preproces-sor for NLP modules for EGY.
We show that a ?donothing?
baseline achieves a normalization perfor-mance of 75.5%, and CODAFY achieves a normal-ization performance of 92.4%, an error reduction of69.2% over this baseline on an unseen test set.The paper is structured as follows.
We first re-view relevant linguistic facts in Section 2 and thenpresent the conventionalized orthography we use inthis paper.
After reviewing related work in Sec-tion 4, we present our data (Section 5), our approach(Section 6), and our results (Section 7).
We conclude585with a discussion of future work.2 Linguistic Facts2.1 Writing without a Standard OrthographyAn orthography is a specification of how the wordsof a language are mapped to and from a particularscript (in our case, the Arabic script).
In cases whena standard orthography is absent, writers make deci-sions about spontaneous orthography based on vari-ous criteria.
Most prominent among them is phonol-ogy: how can my pronunciation of the word berendered in the chosen writing system, given some(language-specific) assumptions about grapheme-to-phoneme mapping?
Often, these assumptions comefrom the ?high?
variant of the language, or some re-lated language.
Another criterion for choosing or-thography is a cognate in a related language or lan-guage variant (Modern Standard Arabic or MSA,the high variant for EGY), where a cognate pair isa pair of words (or morphemes) in two languagesor language variants which are related by etymol-ogy (in some unspecified manner) and which haveroughly the same meaning.
Finally, the chosenspontaneous orthography can be altered to reflectspeech effects, notably the lengthening of syllablesto represent emphasis or other effects (such as QJJJ?ktyyyyr1 ?very?
).It is important to distinguish typos from sponta-neous orthography.
We define spontaneous orthog-raphy to be an intentional choice of graphemes torender the words in a language or language variant.We define a typographical error (typo) to be an un-intended sequence of graphemes.
For example, @Y?kdA and ?Y?
kdh can be intended spellings for EGY/kida/ ?like this?, while @Q?
krA is not a plausible in-tentional spelling since it neither relates /kida/ to anMSA cognate, nor does the sequence of graphemesrepresent the phonology of EGY using standard as-sumptions.
Instead, we can explain the spelling byassuming that the writer accidentally substituted thegrapheme P for the grapheme X, which look some-what alike and are near each other on some Arabickeyboard layouts.
Of course, when we encounter1Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al 2007): (in alphabetical or-der) Abt?jHxd?rzs?SDTD??
?fqklmnhwy and the additional sym-bols: ?
Z, ?
@, A?
@, A?@, w??
', y?
Z?
', h??, ?
?.a specific spelling in a corpus, it can be, in certaincases, difficult to determine whether it is a consciouschoice or a typo.2.2 Relevant Differences between EGY andMSALexical Variations Lexically, the number of differ-ences is quite significant.
For example, EGY H@Q?mrAt ?wife [of]?
corresponds to MSA?k.
?P zwjh?.
Insuch cases of lexical difference, no cognate spellingis available.Phonological Variations There is an extensiveliterature on the phonology of Arabic dialects (Wat-son, 2002; Holes, 2004; Habash, 2010).
Severalphonological differences exist between EGY andMSA which relate to orthography.
Here, we dis-cuss one representative difference.
The MSA con-sonant H /?/ is pronounced as /t/ in EGY (or /s/in more recent borrowings from MSA).
For exam-ple, MSA Q?Kyk?r ?increase (imperfective)?
is pro-nounced /yak?ur/ in MSA versus /yiktar/ in EGY,giving rise to the EGY phonological spelling Q?Kyktr.Morphological Variations There are a lot ofmorphological differences between MSA and EGY.For orthography, two differences are most relevant.The MSA future proclitic /sa/+ (spelled +?
s+) ap-pears in EGY as /ha/+ or /Ha/+.
The two forms ap-pear in free variation, and we have not been ableto find a variable that predicts which form is usedwhen.
This variation is not a general phonologicalvariation between /h/ and /H/, we find it only in thismorpheme.
Predictably, this leads to two spellingsin EGY: +h H+ and +?
h+.
Negation in EGY isrealized as the circum-clitic /ma?/+ .
.
.
+/?/.
The prin-cipal orthographic question is whether the prefix isa separate word or is part of the main word; bothvariants are found.3 CODACODA is a conventionalized orthography for Arabicdialects (Habash et al 2012).
In this section, wesummarize CODA so that the reader can understandthe goals of this paper.
CODA has five key proper-ties.1.
CODA is an internally consistent and coherentconvention for writing DA: every word has a586single orthographic rendering.2.
CODA is created for computational purposes.3.
CODA uses the Arabic script as used for MSA,with no extra symbols from, for example, Per-sian or Urdu.4.
CODA is intended as a unified framework forwriting all dialects.
In this paper, we only dis-cuss the instantiation of CODA for EGY.5.
CODA aims to maintain a level of dialectaluniqueness while using conventions based onsimilarities between MSA and the dialects.We list some features of CODA relevant to thispaper.Phonological Spelling CODA generally usesphonological spelling for EGY (as MSA spellingdoes for MSA).Etymologically Spelled Consonants A limitednumber of consonants may be spelled differentlyfrom their phonology if the following two conditionsare met: (1) the consonant must be an EGY rootradical and (2) the EGY root must have a cognateMSA root.
If the conditions are met, then we spellthe consonant using the corresponding radical fromthe cognate MSA root of the dialectal word?s root.One such example is the spelling of the EGY verbpronounced /kitir/ as Q?
k?r ?
it increased?.Morphologically Faithful CODA preserves di-alectal morphology and spells the dialectal mor-phemes (clitics and inflections) phonologically.
Forexample, for the attachable future marker clitic, thevariant +h is chosen, not the MSA +?, so thatEGY /Hatiktar/ (and its variant /hatiktar/) are bothspelled Q?Jk Htk?r.
The negation prefix and indi-rect object pronoun (l+pronoun) suffixes are sepa-rated, e.g., ?A??
I??
A?
mA qlt lhA?
/ma?ultilha?
?/ ?Idid not tell her?.Alif-Maqsura The letter ?
?
is often used inEgypt to write word-final ?y and vice versa (evenwhen writing MSA).
In CODA, all rules for us-ing Alif-Maqsura are the same as MSA.
For exam-ple, EGY /maSr?
?/ ?Egyptian?
can be seen in sponta-neous orthography as ?Q???
mSr?, but in CODA itis ?Q???
mSry.Ta-Marbuta As in MSA, the Ta-Marbuta ( ?
h?)
isused morphemically in CODA.
The Ta-Marbuta isalways written as?
h?
in CODA, e.g., /?arba?a/ ?four?is??K.
P@ ?rb?h?
in CODA, though it can be found as??K.
P@ ?rb?h (or ??K.
P@ Arb?h) in spontaneous orthog-raphy.Lexical Exceptions EGY CODA guidelines in-clude a word list specifying ad hoc spellings of EGYwords that may be inconsistent with the default map-ping outlined above.
An example is /kida/ ?like this?,which we find as both @Y?
kdA and ?Y?
kdh in spon-taneous orthography; the CODA spelling is ?Y?
kdh.4 Related WorkTo our knowledge, this paper is the first to discussthe task of automatically providing a conventional-ized spelling for a written Arabic dialect text.
Whilethere is no direct precedent, we discuss here somerelated research.Our proposed work has some similarity to auto-matic spelling correction (ASC) and related taskssuch as post editing for optical character recogni-tion (OCR).
Our task is different from ASC sinceASC work assumes a standard orthography that thewriter is also assumed to aim for.
Both supervisedand unsupervised approaches to this task have beenexplored.
Unsupervised approaches rely on improv-ing the fluency of the text and reducing the percent-age of out-of-vocabulary words using NLP tools, re-sources, and heuristics, e.g., morphological analyz-ers, language models, and edit-distance measure, re-spectively (Kukich, 1992; Oflazer, 1996; Ben Oth-mane Zribi and Ben Ahmed, 2003; Shaalan et al2003; Haddad and Yaseen, 2007; Hassan et al2008; Shaalan et al 2010; Alkanhal et al 2012).Supervised approaches learn models of correctionby training on paired examples of errors and theircorrections.
This data is hard to come by nat-urally, though for applications such as OCR cor-pora can be created from the application itself (Ko-lak and Resnik, 2002; Magdy and Darwish, 2006;Abuhakema et al 2008; Habash and Roth, 2011).There has been some work on conversion of di-alectal Arabic to MSA.
Al-Gaphari and Al-Yadoumi(2010) introduced a rule-based method to convertSanaani dialect to MSA, and Shaalan et al(2007)used a rule-based lexical transfer approach to trans-form from EGY to MSA.
Similarly, both Sawaf587(2010) and Salloum and Habash (2011) showedthat translating dialectal Arabic to MSA can im-prove dialectal Arabic machine translation into En-glish by pivoting on MSA.
A common feature acrossthese conversion efforts is the use of morphologicalanalysis and morphosyntactic transformation rules(for example, Al-Gaphari and Al-Yadoumi (2010)).While all this work is similar to ours in that dialec-tal input is processed, our output is still dialectal,while the work on conversion aims for a transforma-tion into MSA.The work most closely related to ours is that ofDasigi and Diab (2011).
They identify the spellingvariants in a given document and normalize them.However, they do not present a system that con-verts spontaneous spelling to a pre-existing conven-tion such as CODA, and thus their results cannotbe directly related to ours.
Furthermore, their tech-nique is different.
First, similarity metrics based onstring difference are used to identify if two stringsare similar.
Also, a contextual string similarity isused based on the fact that if two words are ortho-graphic variants of each other, then they are boundto appear in similar contexts.
After identifying thesimilar strings, the strings of interest are modeled ina vector space and clustered according to the simi-larity of their vectors.5 DataIn this work, we use a manually annotatedEGY Arabic corpus, developed by the Linguis-tic Data Consortium (LDC), and labeled as ?ARZ?
(Maamouri et al 2012), parts 1, 2, 3, 4 and 5.The corpus consists of about 160K words (excludingnumbers and punctuations), and follows the part-of-speech (POS) guidelines used by the LDC for Egyp-tian Arabic.
The corpus contains a full analysis ofEgyptian Arabic text in spontaneous orthography.The analysis includes the correct CODA orthogra-phy of the raw text, in addition to the full morpho-logical/POS annotations.Data Preparation We divide the ARZ corpus intothree parts: training, development and test, whichare of about 122K, 19K and 19K words, respec-tively.
We only consider the orthographic informa-tion in the ARZ corpus: for every word in the cor-pus, we retain the spontaneous orthographic formand its CODA-compliant form.We manually checked the CODA-compliant an-notations for about 500 words in the developmentcorpus.
We found that the accuracy of the gold an-notations in this subset is about 93%.
We performednext an error analysis for the erroneous gold annota-tions.
About one half of the gold errors are CODAphonological and orthographical errors.
Examplesof the CODA phonological errors include wrong ad-ditions and deletions of @ A and ?y, in addition tothe H/ H t/?
transformations, and the transforma-tions that correspond to the different phonologicalforms of pronouncing the letter ?
h. The CODA or-thographical errors are those errors where a word or-thography looks the same as its pronunciation, whileit should not be, such as the ?/?
h/h?
transformations.One fifth of the gold errors are annotation typos,such as writing H@P???
qTwrAt instead of H@PA??qTArAt ?trains?.
Moreover, 9% of the gold errors arewrong merges for the negation particle A?
mA andthe indirect object pronouns (l+pronouns).
Since weuse the gold in our study, and given the error analy-sis for the gold, we expect a qualitatively better goldstandard to yield better results.Transformation Statistics We observe two typesof transformations when converting from sponta-neous orthography to CODA: character substitu-tions that do not affect the word length, and charac-ter additions/deletions that change the word length.Tables 1 and 2 show the most common charactersubstitution and addition/deletion transformations,respectively, as they appear in the training corpus,associated with their frequencies relative to the oc-currence of transformations.
The character sub-stitutions are dominant, and constitute about 84%of all the transformations in the training corpus.While the classification of the character substitu-tions is automatically generated, the classification ofthe character additions/deletions is done manuallyusing a random sample of 400 additions/deletionsin the training corpus.
This is because many addi-tions/deletions are ambiguous.6 ApproachWe describe next the various approaches for spon-taneous orthography codafication.
Our codaficationtechniques fall into two main categories: contex-588Transformation Frequency %@/@/ @/@ A/?/A?/A?
?
@/@/ @/@ A/?/A?/A?
38.5?y ?
?
?
29.7?
h ??
h?
16.9?
h ?
h H 2.5H ??
H/?
t/s 1.0@ A ?
?
h 0.7?
q ?
@/@/ @/@/Z/?'/Z?'
A/?/A?/A?/?/w?/y?
0.4?
w ?
?
h 0.3X ?
?
X/ 	P d/z 0.3?
h??
H t 0.3@ A ??
h?
0.2?
D ?
X/ 	P/ 	?
d/z/D?
0.2Table 1: Spontaneous to CODA character substitu-tion transformationsTransformation Frequency %Errors in closed class words 22.0Missing space after A?/B/ AKmA/lA/yA 19.0@ A additions & deletions 16.8Gold errors 10.3Speech effects 8.5Missing space before ?
l (+pron) 8.5?/ ?/ @?
w/h/wA ?
?/ ?/ @?
w/h/wA 8.3?y additions & deletions 3.0Table 2: Spontaneous to CODA character addi-tion/deletion transformationstual and non-contextual, where the non-contextualapproaches are a lot faster than the contextual ones.6.1 Speech Effect HandlingBefore applying any codafication techniques, weperform a special preprocessing step for speech ef-fects, which represent redundant repetitions of someletter in sequence.
Sometimes people intend theserepetitions to show affirmation or intensification.This is simply handled by removing the repetitionswhen a letter is repeated more than twice in a row,except for some letters whose repetitions for morethan once indicates a speech effect; these lettersare @ A,@ A?, Z ?, Z?'
y?, ?
?
and?
h?.
Handlingspeech effects on its own corrects about 2% of thenon-CODA spontaneous orthography to its CODA-compliant form, without introducing any new errors.In all experiments we report in this paper, we haveinitially processed speech effects.6.2 Character Edit Classification (CEC)In this approach, a set of transformations is appliedon a character level, where a character may receivea change or not.
As a result, a word changes if oneor more of its characters is changed.
The output ofthese transformations is what constitutes the CODAorthography.
This is a surface modeling techniquethat does not depend on the word context (though itdoes depend on character context inside the word).First, we train classifiers for the most frequenttransformations from EGY spontaneous orthogra-phy to the corresponding CODA, listed in Tables 1and 2 in Section 5.
Second, we apply the trainedclassifiers to generate the CODA output.Training the classifiers For each transformationlisted in the data section, we train a separate classi-fier.
The classifiers are trained on our training cor-pus using the k-nearest neighbor algorithm (k-NN)(Wang et al 2000), which is a method for classi-fying objects based on the closest training examplesin the feature space.
We did experiments using theother classification methods included in the WEKAmachine learning tool (Hall et al 2009), includingSVMs, Na?ve Bayes, and decision trees.
However,k-NN gives the best results for our problem.In the training process, a set of nine static featuresis applied, which are the character that is queried forthe transformation with its preceding and followingtwo characters (a special token indicates a characterposition that does not exist because it is beyond theword boundary), along with the first two and the lasttwo characters in the underlying word.In this model, each data point is a character, wherea classifier determines whether a character shouldreceive a substitution, deletion, or addition of an-other character.
The effect of each classifier is ex-amined separately on the development set.
We thendetermine for each classification whether it helps orweakens the process by comparing its effect to thebaseline which is doing nothing, i.e., no change toa word occurs.
Those classifiers that on their ownperform worse than the baseline are eliminated.
Foreliminating the classifiers, we examine them in a de-scending order according to the frequencies of theircorresponding transformations.
We now discuss theseven classifiers we retain in our system:5891.
The different @ A form ( @/@/ @/@ A/?
/A?/A?)
classifier.The classifier can change any @ A form into anyother @ A form.
The arbitrary selection of thedifferent @ A forms represents the most frequentdivergence from CODA in Arabic spontaneousorthography.2.
The ?/?
y/?
classifier.
The classifier handlestransformations between ?y and ?
?
in bothdirections, as their selection is mostly arbitraryin EGY spontaneous orthography.3.
The ?/?/?
h/h?/w classifier.
The classifier han-dles transformations between ?
h,?
h?
and ?
win both directions.
These transformations arelikely to happen at word endings, since theyrepresent common misspellings in writing ?
h,?
h?
and ?
w, where ?
h is often substituted forthe graphically similar?
h?, and ?
h and ?
w canboth be used to represent the 3rd person mascu-line singular accusative or genitive clitic.
(Notethat in Table 1, we list transformations between?
h and?
h?
as well as between ?
h and ?
w; theremaining transformations are not frequent.)4.
The ?/h h/H classifier.
The transformationfrom ?
h to h H is likely to happen at wordbeginnings, since it represents a common devi-ation in writing the h H future particle.5.
The @ A deletion classifier.
The classifier han-dles the deletion of extra @ A at some positions,which is a common deviation in EGY sponta-neous orthography, where the CODA orthog-raphy requires only short vowels instead.6.
The @ A addition classifier.
The classifier han-dles the addition of @ A in some positions, whereit is mostly omitted in EGY spontaneous or-thography, such as adding @ A after the h H fu-ture particle and the H. b progressive particle(when used with the 1st person singular imper-fective), as well as the ?
w plural pronoun atword endings.
When training this classifier, wetarget the letter after which the @ A should beadded.7.
The space addition classifier.
The classifierhandles the addition of spaces in the middleof words, i.e., splitting a word into two words.This is required to add spaces after A?/B/ AKmA/lA/yA for negation and vocation, and be-fore the indirect object l+pronoun, so that thetext becomes CODA-compliant.Generating CODA Orthography Next, we ap-ply the trained classifiers on the spontaneous-orthography text.
Each classifier determines a setof character corrections, where the characters mayreceive transformations corresponding to those onwhich the classifier is trained.
The classifiers areindependent of one another, so their order of appli-cation is irrelevant.By way of example, we apply the classifiers on theword ??K.
P@ Arb?h, ?four?.
The first classifier, corre-sponding to the different @ A forms, determines thetransformation of @ A to@ ?, while the ?/?
h/h?
clas-sifier determines the correction of ?
h to?
h?.
Theother classifiers are either not involved since they donot work on any of the word characters, or they de-termine that no character transformation should hap-pen for this word.
Thus applying the CEC tech-nique in this case changes the word ??K.
P@ Arb?h to??K.
P@ ?rb?h?, which is the correct CODA form.6.3 Maximum Likelihood Estimate (MLE)Another surface modeling approach for spontaneousorthography codafication is to use a maximum like-lihood model that operates on the word level.
In thisapproach, we build a unigram model that replacesevery word in the spontaneous orthography with itsmost likely CODA form as seen in the training data.This assumes that the underlying word exists in thetraining corpus.
For unseen words, the techniquekeeps them with no change.The MLE approach chooses the correct CODAform for most of the words seen in training, makingthis approach highly dependent on the training data.It is efficient at correcting common misspellings infrequent words, especially those that are from closedclasses.6.4 Morphological TaggerIn addition to the approaches discussed above, weuse a morphological tagger, MADAARZ (Mor-phological Analysis and Disambiguation for Egyp-590tian Arabic) (Habash et al 2013).
AlthoughMADAARZ is originally developed to work as amorphological tagger, it still can help the codafica-tion process, since the choice of a full morpholog-ical analysis for a word in context determines itsCODA spelling.
Therefore, MADAARZ is able tocorrect many word misspellings that are common inspontaneous orthography.
These corrections include( @/@/ @/@ A/?/A?/A?
), ?/?
y/?
and ?/?
h/h?
transforma-tions.
However, MADAARZ , as a codafication tech-nique, uses the context of the word, which makesit a contextual modeling approach unlike CEC andMLE.
It is much slower than they are.6.5 Combined TechniquesThe CEC and MLE techniques can be appliedalone, or they can be applied together in a pipeline ineither order.
This gives a total of four possible com-binations.
Next, we conducted experiments withMADAARZ , running alone and as a pre- or postpro-cessor for a combination of CEC and/or MLE.
Inall cases, when we first apply one module and thenanother on the output of the first, we train the sec-ond module on the training corpus which has beenpassed through the first module.
The results of run-ning the different codafication approaches are dis-cussed next.7 Evaluation7.1 Accuracy EvaluationThe different codafication approaches, discussed inthe previous section, are tested against the develop-ment set, which was not used as part of our train-ing.
The evaluation metric we use is a word accuracymetric, i.e., we evaluate how well we can correctlypredict the CODA form of the input spontaneous or-thography.Table 3 lists the effects of using the differentcodafication approaches.
For each approach, twonumbers are reported; exact and normalized.
Inthe exact evaluation, the output of the codafica-tion approach is exactly matched against the correctCODA orthography, while in the normalized eval-uation, the match is relaxed for the ( @/@/ @/@ A/?/A?/A?
)and ?/?
y/?
alternations, i.e., these differences donot count as errors.
In many NLP applications (suchas machine translation), the input is normalized forthese two phenomena, so that the normalized evalu-ation gives a sense of the relevance of codaficationto downstream processes which normalize.In this evaluation we compare our differentcodafication techniques, CEC, MLE, CEC+MLEand MLE+CEC, against the baseline.
We also showthe effect of using MADAARZ as a codafication sys-tem.
We see that MLE on its own outperforms CEC.Running CEC first and then MLE gives us our bestresult using surface techniques, namely 91.5%, foran error reduction of 63.4% against the baseline.This configuration also gives the highest normalizedaccuracy of 95.2%, for an error reduction of 49.5%against the baseline.We now turn to deep modeling techniques.The performance of MADAARZ on its own as acodafication system is close to the performance ofCEC+MLE, by which it is outperformed in theexact-match accuracy by 0.4%.The best deep modeling (and the best overall)performance is achieved when running MADAARZon top of MLE.
This gives the highest accuracyof 92.6% (exact) and 95.8% (normalized), for er-ror reductions of 68.1% (exact) and 55.8% (nor-malized) against the baseline, respectively.
Notethat the non-contextual modeling techniques CEC(5,584 words/sec) and MLE (6,698 words/sec)are a lot faster than the deep modeling tech-nique MADAARZ (53 words/sec), while their com-bination CEC+MLE+MADAARZ is the slowestamong all the approaches, operating at a rate of 52words/sec.
Thus, a small drop in accuracy results ina large increase in speed.We also evaluated using MADAMSA (v 3.2)(Morphological Analysis and Disambiguation forMSA) (Habash and Rambow, 2005; Habash et al2010).
MADAMSA is able to do some codafication,but it performs far worse than our codafication ap-proaches.Table 4 lists the results of the best perform-ing codafication surface approach, CEC+MLE, anddeep approach, MLE+MADAARZ , when appliedon the test set, which was not used as part of ourtraining or development, i.e., a completely blindtest.
We see that on the test set, the additionof MADAARZ improves results relatively more ascompared to the development set.591ApproachExact Match Norm Matchw/sAcc% ER% Acc% ER%Baseline 76.8 90.5MADAMSA 83.6 29.3 91.7 12.6 70CEC 90.0 56.9 93.9 35.8 5,584MLE 90.5 59.1 94.6 43.2 6,698CEC+MLE 91.5 63.4 95.2 49.5 4,284MLE+CEC 90.7 59.9 94.7 44.2 4,284MADAARZ 91.1 61.6 95.2 49.5 53MADAARZ+CEC 91.5 63.4 95.4 51.6 53MADAARZ+MLE 91.9 65.1 95.8 55.8 53CEC+MADAARZ 92.2 66.4 95.6 53.7 53MLE+MADAARZ 92.6 68.1 95.8 55.8 53MADAARZ+CEC+MLE 91.8 64.7 95.6 53.7 52CEC+MLE+MADAARZ 92.0 65.5 95.8 55.8 52Table 3: Comparison of the performance of the different codafication approaches on the development corpus.Acc stands for Accuracy; ER is error reduction against the Baseline.
w/s is speed (words/sec).ApproachExact Match Norm MatchAcc% ER% Acc% ER%Baseline 75.5 89.7CEC+MLE 91.3 64.5 94.8 49.5MLE+MADAARZ 92.9 71.0 95.5 56.3Table 4: Comparison of the performance of thedifferent codafication approaches on the test cor-pus.
Acc stands for Accuracy; ER is error reductionagainst the Baseline.7.2 Extrinsic EvaluationMorphological Analysis We tested the effect ofcodafication on morphological tagging, specificallyfull POS and lemma determination in context bythe morphological tagger MADAARZ .
Here, weare evaluating MADAARZ not on its conversionto CODA (as above), but on its core functional-ity, namely morphological tagging.
We comparethe performance of MADAARZ against runningCEC+MLE+MADAARZ .
When tested on the de-velopment set, the initial CEC+MLE codaficationstep helps MADAARZ improve the identificationof the complete Arabic (Buckwalter) POS tag from84% to 85.3%, for an error reduction of 8.1%, whilethe correct lemma choice increases from 85.2% to85.7%, for an error reduction of 3.4%.
When testedon the test set, we get improvements on the choiceof the complete Buckwalter POS tag and lemmafrom 84.5% to 85.4% (5.8% error reduction) andfrom 86.3% to 86.7% (2.9% error reduction), re-spectively.Arabic to English MT The goal of this exper-iment is to test the effect of codafication on ma-chine translation from dialectal Arabic to English.We use the open-source Moses toolkit (Koehn etal., 2007) to build a phrase-based SMT system.
Weuse MGIZA++ for word alignment (Gao and Vogel,2008).
Phrase translations of up to 8 words are ex-tracted in the phrase table.
We use SRILM (Stol-cke, 2002) with modified Kneser-Ney smoothing tobuild two 4-gram language models.
The first modelis trained on the English side of the bitext, whilethe other is trained on the English Gigaword data.Feature weights are tuned to maximize BLEU (Pap-ineni et al 2002) on a development set using MERT(Och, 2003).
We perform case-insensitive evalua-tion in terms of the BLEU metric.We train the system on dialectal Arabic-Englishparallel data, obtained from several LDC corpora,which amounts to ?500k sentences with 3.8M unto-kenized words on the Arabic side.
The developmentset, used for tuning the parameters of the MT sys-tem, has 1,547 sentences with 15,585 untokenizedArabic words.
The test set has 1,065 sentences with59212,116 untokenized Arabic words.
Both develop-ment and test sets have two reference translationseach.
The English data is lower-cased and tokenizedusing simple punctuation-based rules.We build two systems which vary in preprocess-ing of the Arabic text.
The baseline system ap-plies only simple punctuation-based rules.
The sec-ond system applies our codafication in addition topunctuation separation.
The Arabic text is Alif/Yanormalized and is kept untokenized in both set-tings.
The baseline system achieves a BLEU scoreof 22.1%.
The system using codafication obtains aBLEU score of 22.6%, and outperforms the baselineby 0.5% absolute BLEU points.
This result showsthat improvements observed in intrinsic evaluationof codafication carry on to the extrinsic task of ma-chine translation.7.3 Error AnalysisWe conducted an error analysis for the best perform-ing codafication approach on the development set.The most frequent error types are listed in Table 5.About two thirds of the errors are CODA phonolog-ical and orthographical errors, denoted by CODA-Phon and CODA-Orth, respectively.
The wrong ad-ditions and deletions of @ A and ?y and the H/ H t/?transformations are examples of CODA phonologi-cal errors.
The CODA orthographic errors includecases such as the ?/?
y/?
transformations.
21% ofthe errors are not real errors in the codafication out-put, but result from gold errors.
Finally, about 13%of the errors are wrong merges and splits for the thenegation particle A?
mA, the vocative particle AKyAand the indirect-object l+pronouns.8 Conclusion and Future WorkWe have presented the problem of transformingspontaneous orthography of the Egyptian Arabic di-alect into a conventionalized form, CODA.
Our besttechnique involves a combination of character trans-formations, whole-word transformations, and theuse of a full morphological tagger.
The tagger canbe omitted for a small decrease in performance anda large increase in speed.2 In future work, we planto extend our approach to other Arabic dialects.
We2Our system will be freely available.
Please contact the au-thors for more information.Error Type Description PercentageGold Error Annotation Error 21.0CODA-Orth ?
h ?
?
h?
13.7CODA-Phon @ A ?
 8.7Merge A?
mA/NEG_PART 7.3CODA-Phon ?
?y 6.8CODA-Phon ?
@ A 5.9CODA-Orth ?y ?
?
?
4.1Merge AKyA/VOC_PART 3.7CODA-Phon ?
h ?
h H 3.2CODA-Phon ?y ?
?
?
3.2Table 5: System Error Analysis: the most frequenterror types.will also investigate incorporating the unsupervisedwork of Dasigi and Diab (2011) into our algorithm,as well as other unsupervised techniques.AcknowledgmentThis paper is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-12-C-0014.Any opinions, findings and conclusions or recom-mendations expressed in this paper are those of theauthors and do not necessarily reflect the views ofDARPA.
We thank three anonymous reviewers forhelpful comments, and Ryan Roth for help with run-ning MADA.593ReferencesG.
Abuhakema, R. Faraj, A. Feldman, and E. Fitzpatrick.2008.
Annotating an Arabic Learner Corpus for Er-ror.
Proceedings of the Sixth International LanguageResources and Evaluation (LREC?08).G Al-Gaphari and M Al-Yadoumi.
2010.
A methodto convert Sana?ani accent to Modern Standard Ara-bic.
International Journal of Information Science andManagement, pages 39?49.Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,Mansour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012.
Automatic Stochastic ArabicSpelling Correction With Emphasis on Space Inser-tions and Deletions.
IEEE Transactions on Audio,Speech & Language Processing, 20:2111?2122.Chiraz Ben Othmane Zribi and Mohammed Ben Ahmed.2003.
Efficient Automatic Correction of MisspelledArabic Words Based on Contextual Information.
InProceedings of the Knowledge-Based Intelligent Infor-mation and Engineering Systems Conference, Oxford,UK.Pradeep Dasigi and Mona Diab.
2011.
CODACT: To-wardsIdentifying Orthographic Variants in DialectalArabic.
In Proceedings of the 5th International JointConference on Natural Language Processing, pages318?326, Chaing Mai, Thailand.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan.Nizar Habash and Ryan Roth.
2011.
Using deep mor-phology to improve automatic error detection in ara-bic handwriting recognition.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 875?884, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.Nizar Habash, Owen Rambow, and Ryan Roth.
2010.MADA+TOKAN Manual.
Technical Report CCLS-10-01, Center for Computational Learning Systems(CCLS), Columbia University.Nizar Habash, Mona Diab, and Owen Rabmow.
2012.Conventional Orthography for Dialectal Arabic.
InProceedings of the Language Resources and Evalua-tion Conference (LREC), Istanbul.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskan-der, and Nadi Tomeh.
2013.
Morphological Analysisand Disambiguation for Dialectal Arabic.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies (NAACL-HLT),Atlanta, GA.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Bassam Haddad and Mustafa Yaseen.
2007.
Detectionand Correction of Non-Words in Arabic: A HybridApproach.
International Journal of Computer Pro-cessing Of Languages (IJCPOL).Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008.Language Independent Text Correction using FiniteState Automata.
In Proceedings of the InternationalJoint Conference on Natural Language Processing(IJCNLP 2008).Clive Holes.
2004.
Modern Arabic: Structures, Func-tions, and Varieties.
Georgetown Classics in Ara-bic Language and Linguistics.
Georgetown UniversityPress.Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-pher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Christopher Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational LinguisticsCompanion Volume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public.Okan Kolak and Philip Resnik.
2002.
OCR error cor-rection using a noisy channel model.
In Proceedingsof the second international conference on Human Lan-guage Technology Research.Karen Kukich.
1992.
Techniques for AutomaticallyCorrecting Words in Text.
ACM Computing Surveys,24(4).Mohamed Maamouri, Ann Bies, Seth Kulick, DalilaTabessi, and Sondos Krouna.
2012.
Egyptian ArabicTreebank Pilot.Walid Magdy and Kareem Darwish.
2006.
Arabic OCRError Correction Using Character Segment Correction,594Language Modeling, and Shallow Morphology.
InProceedings of 2006 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2006),pages 408?414, Sydney, Austrailia.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proceedingsof the 41st Annual Conference of the Association forComputational Linguistics, pages 160?167, Sapporo,Japan.Kemal Oflazer.
1996.
Error-tolerant finite-state recog-nition with applications to morphological analysisand spelling correction.
Computational Linguistics,22:73?90.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA.Wael Salloum and Nizar Habash.
2011.
Dialectalto Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation.
In Proceed-ings of the First Workshop on Algorithms and Re-sources for Modelling of Dialects and Language Va-rieties, pages 10?21, Edinburgh, Scotland.Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the Confer-ence of the Association for Machine Translation in theAmericas (AMTA), Denver, Colorado.Khaled Shaalan, Amin Allam, and Abdallah Gomah.2003.
Towards Automatic Spell Checking for Ara-bic.
In Conference on Language Engineering, ELSE,Cairo, Egypt.K.
Shaalan, Abo Bakr, and I. H. Ziedan.
2007.
Transfer-ring Egyptian Colloquial into Modern Standard Ara-bic.
In International Conference on Recent Advancesin Natural Language Processing (RANLP), Borovets,Bulgaria.K.
Shaalan, R. Aref, and A. Fahmy.
2010.
An approachfor analyzing and correcting spelling errors for non-native Arabic learners.
Proceedings of Informaticsand Systems (INFOS).Andreas Stolcke.
2002.
SRILM - an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing(ICSLP), volume 2, pages 901?904, Denver, CO.Jun Wang, Zucker, and Jean-Daniel.
2000.
Solvingmultiple-instance problem: A lazy learning approach.In Pat Langley, editor, 17th International Conferenceon Machine Learning, pages 1119?1125.Janet C. E. Watson.
2002.
The Phonology and Morphol-ogy of Arabic.
Oxford University Press.595
