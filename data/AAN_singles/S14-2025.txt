Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 166?170,Dublin, Ireland, August 23-24, 2014.CISUC-KIS: Tackling Message Polarity Classification with a Large andDiverse set of FeaturesJo?ao Leal, Sara Pinto, Ana Bento, Hugo Gonc?alo Oliveira, Paulo GomesCISUC, Department of Informatics EngineeringUniversity of CoimbraPortugal{jleal,sarap,arbc}@student.dei.uc.pt, {hroliv,pgomes}@dei.uc.ptAbstractThis paper presents the approach of theCISUC-KIS team to the SemEval 2014task on Sentiment Analysis in Twitter,more precisely subtask B - Message Polar-ity Classification.
We followed a machinelearning approach where a SVM classifierwas trained from a large and diverse setof features that included lexical, syntac-tic, sentiment and semantic-based aspects.This led to very interesting results which,in different datasets, put us always in thetop-7 scores, including second position inthe LiveJournal2014 dataset.1 IntroductionEveryday people transmit their opinion in socialnetworks and microblogging services.
Identifyingthe sentiment transmitted in all those shared mes-sages is of great utility for recognizing trends andsupporting decision making, key in areas such associal marketing.
Sentiment Analysis deals withthe computational treatment of sentiments in nat-ural language text, often normalized to positive ornegative polarities.
It is a very challenging task,not only for machines, but also for humans.SemEval 2014 is a semantic evaluation of Nat-ural Language Processing (NLP) that comprisesseveral tasks.
This paper describes our approachto the Sentiment Analysis in Twitter task, whichcomprises two subtasks: (A) Contextual PolarityDisambiguation; and (B) Message Polarity Clas-sification.
We ended up addressing only task B,which is more sentence oriented, as it targets thepolarity of the full messages and not individualwords in those messages.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/We tackled this task with a machine learning-based approach, in which we first collect severalfeatures from the analysis of the given text at sev-eral levels.
The collected features are then used tolearn a sentiment classification model, which canbe done with different algorithms.
Features werecollected from several different resources, includ-ing: sentiment lexicons, dictionaries and avail-able APIs for this task.
Moreover, since micro-blogging text has particular characteristics that in-crease the difficulty of NLP, we gave special fo-cus on text pre-processing.
Regarding the testedfeatures, they went from low-level ones, such aspunctuation and emoticons, to more high-level,including topics extracted using topic modellingtechniques, as well features from sentiment lexi-cons, some structured on plain words and othersbased on WordNet, and thus structured on wordsenses.
Using the latter, we even explored wordsense disambiguation.
We tested several learn-ing algorithms with all these features, but SupportVector Machines (SVM) led to the best results, soit was used for the final evaluation.In all our runs, a model was learned fromtweets, and no SMS were used for training.
Themodel?s performance was assessed with the F-Score of positive and negative classes, with 10-fold cross validation.
In the official evaluation, weachieved very interesting scores, namely: 74.46%for the LiveJournal2014 (2nd place), 65.9% for theSMS2013 (7th), 67.56% for the Twitter2013 (7th),67.95% for the Twitter2014 (4th) and 55.49%for the Twitter2014Sarcasm (4th) datasets, whichranked us always among the top-7 participations.The next section describes the external re-sources exploited.
Section 3 presents our approachwith more detail, and is followed by section 4,where the experimental results are described.
Sec-tion 5 concludes with a brief balance and the mainlessons learned from our participation.1662 External resourcesWe have used several external resources, includ-ing not only several sentiment lexicons, but alsodictionaries that helped normalizing the text of thetweets, as well as available APIs that already clas-sify the sentiment transmitted by a piece of text.2.1 Sentiment LexiconsWe used several public handcrafted or semi-automatically created sentiment lexicons, whereEnglish words have assigned polarity values.Those included lexicons structured in plain words,namely Bing Liu?s Opinion Lexicon (Hu and Liu,2004) (?2,000 positive and 4,800 negative words),the AFINN list (Nielsen, 2011) (?2,500 wordswith polarities between 5 and -5, 900 positive and1,600 negative), the NRCEmoticon Lexicon (Mo-hammad and Turney, 2010) (?14,000 words,their polarity, ?2,300 positive, ?3,300 negative,and eight basic emotions), MPQA SubjectivityLexicon (Wilson et al., 2005) (?2,700 positiveand ?4,900 negative words), Sentiment140 Lexi-con (Mohammad et al., 2013) (?62,000 unigrams,?677,000 bigrams; ?480,000 pairs), NRC Hash-tag Lexicon (Mohammad et al., 2013) (?54,000unigrams; ?316,000 bigrams; ?308,000 pairs)and labMT 1.0 (Dodds et al., 2011) (?10,000words).We also used two resources with polar-ities assigned automatically to a subset ofPrinceton WordNet (Fellbaum, 1998) synsets,namely SentiWordNet 3.0 (Baccianella et al.,2010) (?117,000 synsets with graded positiveand negatives strengths between 0 and 1), andQ-WordNet (Agerri and Garc?
?a-Serrano, 2010)(?7,400 positive and ?8,100 negative senses).2.2 DictionariesThese included handcrafted dictionaries with themost common abbreviations, acronyms, emoti-cons and web slang used on the Internet and theirmeaning.
Also, a list of regular expressions withelongated words like ?loool?
and ?loloolll?, whichcan be normalized to ?lol?, and a set of idiomaticexpressions and their corresponding polarity.2.3 APIsThree public APIs were used, namelySentiment140 (Go et al., 2009),SentimentAnalyzer1and SentiStrength (Thel-1http://sentimentanalyzer.appspot.com/wall et al., 2012).
All of a them classifya given text snippet as positive or negative.Sentiment140 returns a value which can be 0(negative polarity), 2 (neutral), and 4 (positive).SentimentAnalyzer returns -1 (negative) or 1 (pos-itive), and SentiStrength a strength value between1 and 5 (positive) or -1 and -5 (negative).3 ApproachOur approach consisted of extracting lexical, syn-tactic, semantic and sentiment information fromthe tweets and using it in the form of features, forlearning a sentiment classifier that would detectpolarity in messages.
This is a popular approachfor these types of tasks, followed by other sys-tems, including the winner of SemEval 2013 (Mo-hammad et al., 2013), where a variety of surface-form, semantic, and sentiment features was used.Our set of features is similar for the base classifierare similar, except that we included additional fea-tures that take advantage of word disambiguationto get the polarity of target word senses.3.1 FeaturesAmong the collected features, some were relatedto the content of the tweets and others were ob-tained from the sentiment lexicons.3.1.1 Content FeaturesThe tweets were tokenized and part-of-speech (POS) tagged with the CMU ARKTwitter NLP Tool (Gimpel et al., 2011) andStanford CoreNLP (Toutanova and Manning,2000).
Each tweet was represented as a featurevector containing the following group of features:(i) emoticons (presence or absence, sum of allpositive and negative polarities associated witheach, polarity of the last emoticon of each tweet);(ii) length (total length of the tweet, averagelength per word, number of words per tweet);(iii) elongated words (number of all the wordscontaining a repeated character more than twotimes); (iv) hashtags (total number of hashtags);(v) topic modelling (id of the correspondingtopic); (vi) capital letters (number of words inwhich all letters are capitalized); (vii) nega-tion (number of words that reverse polarity toa negative context, such as ?no?
or ?never?
);(viii) punctuation (number of punctuation se-quences with only exclamation points, questionmarks or both, ASCII code of the most commonpunctuation and of the last punctuation in every167tweet); (ix) dashes and asterisks (number of wordssurrounded by dashes or asterisks, such as ?
*yay*?or ?-me-?
); (x) POS (number of nouns, adjectives,adverbs, verbs and interjections).3.1.2 Lexicon FeaturesA wide range of features were created using thelexicons.
For each tweet and for each lexicon thefollowing set of features were generated: (i) to-tal number of positive and negative opinion words;(ii) sum of all positive/negative polarity values inthe tweet; (iii) the highest positive/negative po-larity value in the tweet; and (iv) the polarityvalue of the last polarity word.
Those featureswere collected for: unigrams, bigrams and pairs(only on the NRC Hashtag Lexicon and Senti-ment140), nouns, adjectives, verbs, interjections,hashtags, all caps tokens (e.g ?GO AWAY?
), elon-gated words, asterisks and dashes tokens.Different approaches were followed to get thepolarity of each word from the wordnets.
FromSentiWordNet, we computed combined scores ofall senses, with decreasing weights for lowerranked senses, as well as the scores of the firstsense only, both considering: (i) positive and neg-ative; (ii) just positive; (iii) just negative scores.Moreover, we performed word sense disambigua-tion using the full WordNet 3.0 to get the previ-ous scores for the selected sense.
For this pur-pose, we applied the Lesk Algorithm adapted towordnets (Banerjee and Pedersen, 2002), using allthe tweet?s content words as the word context, andthe synset words, gloss words and words in relatedsynsets as the synset?s context.
Given that Senti-WordNet is aligned to WordNet 3.0, after select-ing the most adequate sense of the word, we couldget its polarity scores.
From Q-WordNet, similarscores were computed but, since it doesn?t use agraded strength and only classifies word senses aspositive or negative, there were just positive or justnegative scores.3.2 ClassifierIn our final approach we used a SVM (Fan et al.,2008) which is an effective solution in high dimen-sional spaces and proved to be the best learningalgorithm for this task.
We tested various kernels(e.g.
PolyKernel, RBF) and their parameters withcross validation on the training data.
Given the re-sults, we confirmed that the RBF kernel, computedaccording to equation 1, is most effective with aC = 4 and a ?
= 0.0003.K(xi, xj) = ?(xi)T?
(xj) = exp(?
?||xi?xj||2)(1)Considering we are working on a multi-classclassification problem, we implemented the ?one-against-one?
approach (Knerr et al., 1990) where#classes ?
(#classes?
1)/2 classifiers are con-structed and each one trains data from classes.Due to the non-scale invariant nature of SVM al-gorithms, we?ve scaled our data on each attributeto have ?
= 0 and ?
= 1 and took caution againstclass unbalance.4 ExperimentsFor training the SVM classifier, we used a set of9,634 tweets with a known polarity and also 1,281tweets as development test to grid search the bestparameters.
No SMS messages were used as train-ing or as development test.
For the scorer function,we used a macro-averaged F-Score of positive andnegative classes ?
the one made available and usedby the task organizers.4.1 Some ResultsThe results obtained by the system were 70.41%on the training set (using 10-Folds) and 71.03%on the development set, after train on the train-ing set.
When tested against the training set,after train in the same set, we get a score of84.32%, which could indicate a case of under-fitting.
Though, our classifier generalized well,given that we got a 74.46% official score on Live-Journal2014, second in that category.
On the otherhand, our experiments with decision trees showedthat they couldn?t generalize so well, althoughthey achieved scores of >99 on the training set.
Inthe SMS category, our system would benefit froma specific data set in the training phase.
Yet, it stillmanaged to reach 7th place in that category.
In thesarcasm category our submission ranked 4th, witha score of 58.16%, 2.69% below the best rank.
Onthe Twitter2014 dataset, we scored 67.95% (4th),which is slightly below our prediction based ondevelopment tests.
A possible explanation is thatwe might have over-fitted the classifier parameterswhen grid searching.4.2 Features RelevanceIn order to get some insights on the most relevantgroup of features, we did a series of experimentswhere each group of features were removed for168the classification, then tested against the originalscore.
We concluded that the lexicon related fea-tures contribute highly to the performance of oursystem, including the set of features with n-gramsand POS.
Clusters, sport score, asterisks and elon-gated words provide little gains but, on the otherhand, emoticons and hashtags showed some im-portance and provided enough new informationfor the system to learn.
The API information islargely captured by some of our features and thatmakes it much less discriminating than what theywould be on their own, but still worth using forthe small gain.
We also observed that it is best tocreate a diversified set of lexicon features with ex-tra very specific targeted features, such as punc-tuation, instead of focusing on using a specificlexicon alone.
Even though they usually over-lap in information and may perform worse indi-vidually than a hand-refined single dictionary ap-proach, they complement each other and that re-sults in larger gains.4.3 Selected ParametersFor the parameter values, we did a grid searchusing the development set as a test.
We alsofound that large values of C (25) and small ?
val-ues (0.0001) performed worse than smaller valuesof C (4) with a slightly higher ?
(0.0003) whenusing the development set but not when using thetraining set under K-Folds.
For the official eval-uation, we opted for the best-performing resultson the development set.
Using intermediate val-ues accomplished worse results in either case.5 Concluding RemarksWe have described the work developed for the sub-task B of SemEval 2014 Sentiment Analysis inTwitter task.
We followed a machine learning ap-proach, with a diversified set of features, whichtend to complemented each other.
Some of themain takeaways are that the most important fea-tures are the lexicon related ones, including then-grams and POS tags.
Due to time constraints,we could not take strong conclusions on the impactof the word sense disambiguation related featuresalone.
As those are probably the most differentiat-ing features of our classifier, this is something wewish to target in the future.To conclude, we have achieved very interestingresults in terms of overall classification.
Consider-ing that this was our first participation in such anevaluation, we make a very positive balance.
Andof course, we are looking forward for upcomingeditions of this task.AcknowledgementThis work was supported by the iCIS project(CENTRO-07-ST24-FEDER-002003), co-financed by QREN, in the scope of the MaisCentro Program and European Union?s FEDER.ReferencesRodrigo Agerri and Ana Garc??a-Serrano.
2010.
Q-wordnet: Extracting polarity from WordNet senses.In Proceedings of the 7th International Confer-ence on Language Resources and Evaluation, LREC2010, pages 2300?2305, La Valletta, Malta.
ELRA.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the 7th International Confer-ence on Language Resources and Evaluation, LREC2010, pages 2200?2204, Valletta, Malta.
ELRA.Satanjeev Banerjee and Ted Pedersen.
2002.
Anadapted Lesk algorithm for word sense disambigua-tion using WordNet.
In Proceedings of the 3rd Inter-national Conference on Computational Linguisticsand Intelligent Text Processing (CICLing 2002), vol-ume 2276 of LNCS, pages 136?145, London, UK.Springer.Peter Sheridan Dodds, Kameron Decker Harris, Is-abel M. Kloumann, Catherine A. Bliss, and Christo-pher M. Danforth.
2011.
Temporal patterns of hap-piness and information in a global social network:Hedonometrics and Twitter.
PLoS ONE, 6(12), De-cember.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874, June.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flani-gan, and Noah A Smith.
2011.
Part-of-speech tag-ging for Twitter: Annotation, features, and exper-iments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies: short papers-Volume 2, ACL 2011, pages 42?47.
ACL Press.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report, Stanford University.169Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD 2004, pages168?177.Stefan Knerr, L?eon Personnaz, and G?erard Dreyfus.1990.
Single-layer learning revisited: a stepwiseprocedure for building and training neural network.In Proceedings of the NATO Advanced ResearchWorkshop on Neurocomputing, Algorithms, Archi-tectures and Applications, Nato ASI, Computer andSystems Sciences, pages 41?50.
Springer.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: UsingMechanical Turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, CAAGET ?10, pages 26?34,Los Angeles, CA.
ACL Press.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In 2nd JointConference on Lexical and Computational Seman-tics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion, SemEval 2013, pages 321?327, Atlanta, Geor-gia, USA, June.
ACL Press.Finn?Arup Nielsen.
2011.
A new anew: Evaluation ofa word list for sentiment analysis in microblogs.
InProceedings of the ESWC2011 Workshop on ?Mak-ing Sense of Microposts?
: Big things come in smallpackages, pages 93?98, May.Mike Thelwall, Kevan Buckley, and Georgios Pal-toglou.
2012.
Sentiment strength detection for thesocial web.
Journal of the American Society for In-formation Science and Technology, 63(1):163?173,January.Kristina Toutanova and Christopher D Manning.
2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger.
In Proceedingsof the 2000 Joint SIGDAT conference on Empiricalmethods in natural language processing and verylarge corpora, EMNLP 2000, pages 63?70.
ACLPress.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, HLT ?05, pages 347?354, Vancouver, BritishColumbia, Canada.
ACL Press.170
