Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 20?27,Columbus, Ohio, USA June 2008. c?2008 Association for Computational LinguisticsUnsupervised word segmentation for Sesotho using Adaptor GrammarsMark JohnsonBrown UniversityMark Johnson@Brown.eduAbstractThis paper describes a variety of non-parametric Bayesian models of word segmen-tation based on Adaptor Grammars that modeldifferent aspects of the input and incorporatedifferent kinds of prior knowledge, and ap-plies them to the Bantu language Sesotho.While we find overall word segmentation ac-curacies lower than these models achieve onEnglish, we also find some interesting dif-ferences in which factors contribute to betterword segmentation.
Specifically, we found lit-tle improvement to word segmentation accu-racy when we modeled contextual dependen-cies, while modeling morphological structuredid improve segmentation accuracy.1 IntroductionA Bayesian approach to learning (Bishop, 2006) isespecially useful for computational models of lan-guage acquisition because we can use it to studythe effect of different kinds and amounts of priorknowledge on the learning process.
The Bayesianapproach is agnostic as to what this prior knowl-edge might consist of; the prior could encode thekinds of rich universal grammar hypothesised bye.g., Chomsky (1986), or it could express a vaguenon-linguistic preference for simpler as opposed tomore complex models, as in some of the grammarsdiscussed below.
Clearly there?s a wide range ofpossible priors, and one of the exciting possibilitiesraised by Bayesian methods is that we may soon beable to empirically evaluate the potential contribu-tion of different kinds of prior knowledge to lan-guage learning.The Bayesian framework is surprisingly flexible.The bulk of the work on Bayesian inference is onparametric models, where the goal is to learn thevalue of a set of parameters (much as in Chomsky?sPrinciples and Parameters conception of learning).However, recently Bayesian methods for nonpara-metric inference have been developed, in which theparameters themselves, as well as their values, arelearned from data.
(The term ?nonparametric?
isperhaps misleading here: it does not mean that themodels have no parameters, rather it means that thelearning process considers models with different setsof parameters).
One can think of the prior as pro-viding an infinite set of possible parameters, fromwhich a learner selects a subset with which to modeltheir language.If one pairs each of these infinitely-many pa-rameters with possible structures (or equivalently,rules that generate such structures) then these non-parametric Bayesian learning methods can learnthe structures relevant to a language.
Determiningwhether methods such as these can in fact learn lin-guistic structure bears on the nature vs. nurture de-bates in language acquisition, since one of the argu-ments for the nativist position is that there doesn?tseem to be a way to learn structure from the inputthat children receive.While there?s no reason why these methods can?tbe used to learn the syntax and semantics of humanlanguages, much of the work to date has focused onlower-level learning problems such as morphologi-cal structure learning (Goldwater et al, 2006b) andword segmentation, where the learner is given un-segmented broad-phonemic utterance transcriptions20and has to identify the word boundaries (Goldwateret al, 2006a; Goldwater et al, 2007).
One reason forthis is that these problems seem simpler than learn-ing syntax, where the non-linguistic context plausi-bly supplies important information to human learn-ers.
Virtually everyone agrees that the set of possiblemorphemes and words, if not infinite, is astronom-ically large, so it seems plausible that humans usesome kind of nonparametric procedure to learn thelexicon.Johnson et al (2007) introduced Adaptor Gram-mars as a framework in which a wide varietyof linguistically-interesting nonparametric inferenceproblems can be formulated and evaluated, includ-ing a number of variants of the models described byGoldwater (2007).
Johnson (2008) presented a vari-ety of different adaptor grammar word segmentationmodels and applied them to the problem of segment-ing Brent?s phonemicized version of the Bernstein-Ratner corpus of child-directed English (Bernstein-Ratner, 1987; Brent, 1999).
The main results of thatpaper were the following:1. it confirmed the importance of modeling con-textual dependencies above the word level forword segmentation (Goldwater et al, 2006a),2. it showed a small but significant improvementto segmentation accuracy by learning the possi-ble syllable structures of the language togetherwith the lexicon, and3.
it found no significant advantage to learningmorphological structure together with the lex-icon (indeed, that model confused morphologi-cal and lexical structure).Of course the last result is a null result, and it?s pos-sible that a different model would be able to usefulycombine morphological learning with word segmen-tation.This paper continues that research by applyingthe same kinds of models to Sesotho, a Bantu lan-guage spoken in Southern Africa.
Bantu languagesare especially interesting for this kind of study, asthey have rich productive agglutinative morpholo-gies and relatively transparent phonologies, as com-pared to languages such as Finnish or Turkish whichhave complex harmony processes and other phono-logical complexities.
The relative clarity of Bantuhas inspired previous computational work, such asthe algorithm for learning Swahili morphology byHu et al (2005).
The Hu et al algorithm usesa Minimum Description Length procedure (Rissa-nen, 1989) that is conceptually related to the non-parametric Bayesian procedure used here.
However,the work here is focused on determining whether theword segmentation methods that work well for En-glish generalize to Sesotho and whether modelingmorphological and/or syllable structure improvesSesotho word segmentation, rather than learningSesotho morphological structure per se.The rest of this paper is structured as follows.Section 2 informally reviews adaptor grammars anddescribes how they are used to specify differentBayesian models.
Section 3 describes the Sesothocorpus we used and the specific adaptor grammarswe used for word segmentation, and section 5 sum-marizes and concludes the paper.2 Adaptor grammarsOne reason why Probabilistic Context-Free Gram-mars (PCFGs) are interesting is because they arevery simple and natural models of hierarchical struc-ture.
They are parametric models because eachPCFG has a fixed number of rules, each of whichhas a numerical parameter associated with it.
Oneway to construct nonparametric Bayesian models isto take a parametric model class and let one or moreof their components grow unboundedly.There are two obvious ways to construct nonpara-metric models from PCFGs.
First, we can let thenumber of nonterminals grow unboundedly, as in theInfinite PCFG, where the nonterminals of the gram-mar can be indefinitely refined versions of a basePCFG (Liang et al, 2007).
Second, we can fix theset of nonterminals but permit the number of rulesor productions to grow unboundedly, which leads toAdaptor Grammars (Johnson et al, 2007).At any point in learning, an Adaptor Grammar hasa finite set of rules, but these can grow unbound-edly (typically logarithmically) with the size of thetraining data.
In a word-segmentation applicationthese rules typically generate words or morphemes,so the learner is effectively learning the morphemesand words of its language.The new rules learnt by an Adaptor Grammar are21compositions of old ones (that can themselves becompositions of other rules), so it?s natural to thinkof these new rules as tree fragments, where eachentire fragment is associated with its own proba-bility.
Viewed this way, an adaptor grammar canbe viewed as learning the tree fragments or con-structions involved in a language, much as in Bod(1998).
For computational reasons adaptor gram-mars require these fragments to consist of subtrees(i.e., their yields are terminals).We now provide an informal description of Adap-tor Grammars (for a more formal description seeJohnson et al (2007)).
An adaptor grammar con-sists of terminals V , nonterminals N (including astart symbol S), initial rules R and rule probabilitiesp, just as in a PCFG.
In addition, it also has a vec-tor of concentration parameters ?, where ?A ?
0 iscalled the (Dirichlet) concentration parameter asso-ciated with nonterminal A.The nonterminals A for which ?A > 0 areadapted, which means that each subtree for A thatcan be generated using the initial rules R is consid-ered as a potential rule in the adaptor grammar.
If?A = 0 then A is unadapted, which means it ex-pands just as in an ordinary PCFG.Adaptor grammars are so-called because theyadapt both the subtrees and their probabilities to thecorpus they are generating.
Formally, they are Hi-erarchical Dirichlet Processes that generate a distri-bution over distributions over trees that can be de-fined in terms of stick-breaking processes (Teh et al,2006).
It?s probably easiest to understand them interms of their conditional or sampling distribution,which is the probability of generating a new tree Tgiven the trees (T1, .
.
.
, Tn) that the adaptor gram-mar has already generated.An adaptor grammar can be viewed as generatinga tree top-down, just like a PCFG.
Suppose we havea node A to expand.
If A is unadapted (i.e., ?A = 0)then A expands just as in a PCFG, i.e., we pick arule A ?
?
?
R with probability pA??
and recur-sively expand ?.
If A is adapted and has expandednA times before, then:1.
A expands to a subtree ?
with probabilityn?/(nA+?A), where n?
is the number of timesA has expanded to subtree ?
before, and2.
A expands to ?
where A ?
?
?
R with prob-ability ?A pA?
?/(nA + ?A).Thus an adapted nonterminal A expands to a previ-ously expanded subtree ?
with probability propor-tional to the number n?
of times it was used before,and expands just as in a PCFG (i.e., using R) withprobability proportional to the concentration param-eter ?A.
This parameter specifies how likely A is toexpand into a potentially new subtree; as nA and n?grow this becomes increasingly unlikely.We used the publically available adaptor gram-mar inference software described in Johnson et al(2007), which we modified slightly as described be-low.
The basic algorithm is a Metropolis-within-Gibbs or Hybrid MCMC sampler (Robert andCasella, 2004), which resamples the parse tree foreach sentence in the training data conditioned on theparses for the other sentences.
In order to producesample parses efficiently the algorithm constructs aPCFG approximation to the adaptor grammar whichcontains one rule for each adapted subtree ?, anduses a Metropolis accept/reject step to correct for thedifference between the true adaptor grammar dis-tribution and the PCFG approximation.
With thedatasets described below less than 0.1% of proposalparses from this PCFG approximation are rejected,so it is quite a good approximation to the adaptorgrammar distribution.On the other hand, at convergence this algorithmproduces a sequence of samples from the posteriordistribution over adaptor grammars, and this poste-rior distribution seems quite broad.
For example,at convergence with the most stable of our models,each time a sentence?s parse is resampled there isan approximately 25% chance of the parse chang-ing.
Perhaps this is not surprising given the com-paratively small amount of training data and the factthat the models only use fairly crude distributionalinformation.As just described, adaptor grammars require theuser to specify a concentration parameter ?A foreach adapted nonterminal A.
It?s not obvious howthis should be done.
Previous work has treated ?Aas an adjustable parameter, usually tying all of the?A to some shared value which is adjusted to opti-mize task performance (say, word segmentation ac-curacy).
Clearly, this is undesirable.Teh et al (2006) describes how to learn the con-22centration parameters ?, and we modified their pro-cedure for adaptor grammars.
Specifically, we puta vague Gamma(10, 0.1) prior on each ?A, and af-ter each iteration through the training data we per-formed 100 Metropolis-Hastings resampling stepsfor each ?A from an increasingly narrow Gammaproposal distribution.
We found that the perfor-mance of the models with automatically learnedconcentration parameters ?
was generally as goodas the models where ?
was tuned by hand (althoughadmittedly we only tried three or four different val-ues for ?
).3 Models of Sesotho word segmentationWe wanted to make our Sesotho corpus as similaras possible to one used in previous work on wordsegmentation.
We extracted all of the non-childutterances from the LI?LV files from the Sesothocorpus of child speech (Demuth, 1992), and usedthe Sesotho gloss as our gold-standard corpus (wedid not phonemicize them as Sesotho orthographyis very close to phonemic).
This produced 8,503utterances containing 21,037 word tokens, 30,200morpheme tokens and 100,113 phonemes.
By com-parison, the Brent corpus contains 9,790 utterances,33,399 word tokens and 95,809 phonemes.
Thusthe Sesotho corpus contains approximately the samenumber of utterances and phonemes as the Brentcorpus, but far fewer (and hence far longer) words.This is not surprising as the Sesotho corpus involvesan older child and Sesotho, being an agglutinativelanguage, tends to have morphologically complexwords.In the subsections that follow we describe a vari-ety of adaptor grammar models for word segmenta-tion.
All of these models were given same Sesothodata, which consisted of the Sesotho gold-standardcorpus described above with all word boundaries(spaces) and morpheme boundaries (hyphens) re-moved.
We computed the f-score (geometric aver-age of precision and recall) with which the modelsrecovered the words or the morphemes annotated inthe gold-standard corpus.3.1 Unigram grammarWe begin by describing an adaptor grammar thatsimulates the unigram word segmentation modelModel word f-score morpheme f-scoreword 0.431 0.352colloc 0.478 0.387colloc2 0.467 0.389word ?
syll 0.502 0.349colloc?
syll 0.476 0.372colloc2?
syll 0.490 0.393word ?morph 0.529 0.321word ?
smorph 0.556 0.378colloc?
smorph 0.537 0.352Table 1: Summary of word and morpheme f-scores forthe different models discussed in this paper.proposed by Goldwater et al (2006a).
In this modeleach utterance is generated as a sequence of words,and each word is a sequence of phonemes.
Thisgrammar contains three kinds of rules, includingrules that expand the nonterminal Phoneme to all ofthe phonemes seen in the training data.Sentence ?
Word+Word ?
Phoneme+Adapted non-terminals are indicated by underlin-ing, so in the word grammar only the Word nonter-minal is adapted.
Our software doesn?t permit reg-ular expressions in rules, so we expand all Kleenestars in rules into right-recursive structures over newunadapted nonterminals.
Figure 1 shows a sampleparse tree generated by this grammar for the sen-tence:u-SM-e-OM-nk-take-il-PERF-eINkaewhere?You took it from where?
?This sentence shows a typical inflected verb, with asubject marker (glossed SM), an object marker (OM),perfect tense marker (PERF) and mood marker (IN).In order to keep the trees a managable size, we onlydisplay the root node, leaf nodes and nodes labeledwith adapted nonterminals.The word grammar has a word segmentation f-score of 43%, which is considerably below the 56%f-score the same grammar achieves on the Brent cor-pus.
This difference presumably reflects the fact thatSesotho words are longer and more complex, and sosegmentation is a harder task.We actually ran the adaptor grammar sampler for23SentenceWordu e n k i l eWordk a eFigure 1: A sample (correct) parse tree generated by theword adaptor grammar for a Sesotho utterance.the word grammar four times (as we did for all gram-mars discussed in this paper).
Because the sampleris non-deterministic, each run produced a differentseries of sample segmentations.
However, the av-erage segmentation f-score seems to be very stable.The accuracies of the final sample of the four runsranges between 42.8% and 43.7%.
Similarly, onecan compute the average f-score over the last 100samples for each run; the average f-score ranges be-tween 42.6% and 43.7%.
Thus while there maybe considerable uncertainty as to where the wordboundaries are in any given sentence (which is re-flected in fact that the word boundaries are verylikely to change from sample to sample), the aver-age accuracy of such boundaries seems very stable.The final sample grammars contained the initialrules R, together with between 1,772 and 1,827 ad-ditional expansions for Word, corresponding to thecached subtrees for the adapted Word nonterminal.3.2 Collocation grammarGoldwater et al (2006a) showed that incorporating abigram model of word-to-word dependencies signif-icantly improves word segmentation accuracy in En-glish.
While it is not possible to formulate such a bi-gram model as an adaptor grammar, Johnson (2008)showed that a similar improvement can be achievedin an adaptor grammar by explicitly modeling col-locations or sequences of words.
The colloc adaptorgrammar is:Sentence ?
Colloc+Colloc ?
Word+Word ?
Phoneme+This grammar generates a Sentence as a sequenceof Colloc(ations), where each Colloc(ation) is a se-quence of Words.
Figure 2 shows a sample parse treegenerated by the colloc grammar.
In terms of wordsegmentation, this grammar performs much worseSentenceCollocWordu eWordnWordk i l eCollocWordk aCollocWordeFigure 2: A sample parse tree generated by the collocgrammar.
The substrings generated by Word in fact tendto be morphemes and Colloc tend to be words, which ishow they are evaluated in Table 1.than the word grammar, with an f-score of 27%.In fact, it seems that the Word nonterminals typ-ically expand to morphemes and the Colloc nonter-minals typically expand to words.
It makes sensethat for a language like Sesotho, when given a gram-mar with a hierarchy of units, the learner would usethe lower-level units as morphemes and the higher-level units as words.
If we simply interpret the Wordtrees as morphemes and the Colloc trees as wordswe get a better word segmentation accuracy of 48%f-score.3.3 Adding more levelsIf two levels are better than one, perhaps three levelswould be better than two?
More specifically, per-haps adding another level of adaptation would per-mit the model to capture the kind of interword con-text dependencies that improved English word seg-mentation.
Our colloc2 adaptor grammar includesthe following rules:Sentence ?
Colloc+Colloc ?
Word+Word ?
Morph+Morph ?
Phoneme+This grammar generates sequences of Wordsgrouped together in collocations, as in the previousgrammar, but each Word now consists of a sequenceof Morph(emes).
Figure 3 shows a sample parse treegenerated by the colloc2 grammar.Interestingly, word segmentation f-score is46.7%, which is slightly lower than that obtainedby the simpler colloc grammar.
Informally, it seemsthat when given an extra level of structure thecolloc2 model uses it to describe structure internal24SentenceCollocWordMorphuMorpheWordMorphn k iMorphl eWordMorphk aMorpheFigure 3: A sample parse tree generated by the colloc2grammar.to the word, rather than to capture interword depen-dencies.
Perhaps this shouldn?t be surprising, sinceSesotho words in this corpus are considerably morecomplex than the English words in the Brent corpus.4 Adding syllable structureJohnson (2008) found a small but significant im-provement in word segmentation accuracy by usingan adaptor grammar that models English words asa sequence of syllables.
The word?
syll grammarbuilds in knowledge that syllables consist of an op-tional Onset, a Nuc(leus) and an optional Coda, andknows that Onsets and Codas are composes of con-sonants and that Nucleii are vocalic (and that syl-labic consonsants are possible Nucleii), and learnsthe possible syllables of the language.
The rules inthe adaptor grammars that expand Word are changedto the following:Word ?
Syll+Syll ?
(Onset) Nuc (Coda)Syll ?
SCOnset ?
C+Nuc ?
V+Coda ?
C+In this grammar C expands to any consonant and Vexpands to any vowel, SC expands to the syllablicconsonants ?l?, ?m?
?n?
and ?r?, and parentheses indi-cate optionality.
Figure 4 shows a sample parse treeproduced by the word ?
syll adaptor grammar (i.e.,where Words are generated by a unigram model),while Figure 5 shows a sample parse tree generatedby the corresponding colloc?
syll adaptor grammar(where Words are generated as a part of a Colloca-tion).SentenceWordSylluSylleSylln k iSylll eWordSyllk a eFigure 4: A sample parse tree generated by theword?
syll grammar, in which Words consist of se-quences of Syll(ables).SentenceCollocWordSylluWordSylleWordSylln k iSylll eCollocWordSyllk a eFigure 5: A sample parse tree generated by thecolloc?
syll grammar, in which Colloc(ations) consist ofsequences of Words, which in turn consist of sequencesof Syll(ables).Building in this knowledge of syllable struc-ture does improve word segmentation accuracy,but the best performance comes from the simplestword ?
syll grammar (with a word segmentation f-score of 50%).4.1 Tracking morphological positionAs we noted earlier, the various Colloc grammarswound up capturing a certain amount of morpholog-ical structure, even though they only implement arelatively simple unigram model of morpheme wordorder.
Here we investigate whether we can im-prove word segmentation accuracy with more so-phisticated models of morphological structure.The word?morph grammar generates a word asa sequence of one to five morphemes.
The relevantproductions are the following:Word ?
T1 (T2 (T3 (T4 (T5))))T1 ?
Phoneme+T2 ?
Phoneme+T3 ?
Phoneme+T4 ?
Phoneme+T5 ?
Phoneme+25SentenceWordT1u eT2n k i l eT3k a eFigure 6: A sample parse tree generated by theword?morph grammar, in which Words consist of mor-phemes T1?T5, each of which is associated with specificlexical items.While each morpheme is generated by a unigramcharacter model, because each of these five mor-pheme positions is independently adapted, the gram-mar can learn which morphemes prefer to appear inwhich position.
Figure 6 contains a sample parsegenerated by this grammar.
Modifying the gram-mar in this way significantly improves word seg-mentation accuracy, achieving a word segmentationf-score of 53%.Inspired by this, we decided to see what wouldhappen if we built-in some specific knowledge ofSesotho morphology, namely that a word consists ofa stem plus an optional suffix and zero to three op-tional prefixes.
(This kind of information is oftenbuilt into morphology learning models, either ex-plicitly or implicitly via restrictions on the searchprocedure).
The resulting grammar, which we callword ?
smorph, generates words as follows:Word ?
(P1 (P2 (P3))) T (S)P1 ?
Phoneme+P2 ?
Phoneme+P3 ?
Phoneme+T ?
Phoneme+S ?
Phoneme+Figure 7 contains a sample parse tree generatedby this grammar.
Perhaps not surprisingly, with thismodification the grammar achieves the highest wordsegmentation f-score of any of the models examinedin this paper, namely 55.6%.Of course, this morphological structure is per-fectly compatible with models which posit higher-level structure than Words.
We can replace the Wordexpansion in the colloc grammar with one just given;the resulting grammar is called colloc?
smorph,and a sample parse tree is given in Figure 8.
Interest-SentenceWordP1uP2eTn kSi l eWordTk aSeFigure 7: A sample parse tree generated by theword?
smorph grammar, in which Words consist of upto five morphemes that satisfy prespecified ordering con-straints.SentenceCollocWordP1u eTnSk i l eWordTk aSeFigure 8: A sample parse tree generated by thecolloc?
smorph grammar, in which Colloc(ations) gen-erate a sequence of Words, which in turn consist of upto five morphemes that satisfy prespecified ordering con-straints.ingly, this grammar achieves a lower accuracy thaneither of the two word-based morphology grammarswe considered above.5 ConclusionPerhaps the most important conclusion to be drawnfrom this paper is that the methods developed forunsupervised word segmentation for English alsowork for Sesotho, despite its having radically dif-ferent morphological structures to English.
Just aswith English, more structured adaptor grammars canachieve better word-segmentation accuracies thansimpler ones.
While we find overall word segmen-tation accuracies lower than these models achieveon English, we also found some interesting differ-ences in which factors contribute to better word seg-mentation.
Perhaps surprisingly, we found littleimprovement to word segmentation accuracy whenwe modeled contextual dependencies, even thoughthese are most important in English.
But includ-ing either morphological structure or syllable struc-ture in the model improved word segmentation accu-26racy markedly, with morphological structure makinga larger impact.
Given how important morphology isin Sesotho, perhaps this is no surprise after all.AcknowledgmentsI?d like to thank Katherine Demuth for the Sesothodata and help with Sesotho morphology, my collabo-rators Sharon Goldwater and Tom Griffiths for theircomments and suggestions about adaptor grammars,and the anonymous SIGMORPHON reviewers fortheir careful reading and insightful comments on theoriginal abstract.
This research was funded by NSFawards 0544127 and 0631667.ReferencesN.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Rens Bod.
1998.
Beyond grammar: an experience-basedtheory of language.
CSLI Publications, Stanford, Cal-ifornia.M.
Brent.
1999.
An efficient, probabilistically soundalgorithm for segmentation and word discovery.
Ma-chine Learning, 34:71?105.Noam Chomsky.
1986.
Knowledge of Language: ItsNature, Origin and Use.
Praeger, New York.Katherine Demuth.
1992.
Acquisition of Sesotho.In Dan Slobin, editor, The Cross-Linguistic Studyof Language Acquisition, volume 3, pages 557?638.Lawrence Erlbaum Associates, Hillsdale, N.J.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006a.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 673?680, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006b.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466,Cambridge, MA.
MIT Press.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2007.
Distributional cues to word boundaries:Context is important.
In David Bamman, TatianaMagnitskaia, and Colleen Zaller, editors, Proceedingsof the 31st Annual Boston University Conference onLanguage Development, pages 239?250, Somerville,MA.
Cascadilla Press.Sharon Goldwater.
2007.
Nonparametric Bayesian Mod-els of Lexical Acquisition.
Ph.D. thesis, Brown Uni-versity.Yu Hu, Irina Matveeva, John Goldsmith, and ColinSprague.
2005.
Refining the SED heuristic for mor-pheme discovery: Another look at Swahili.
In Pro-ceedings of the Workshop on PsychocomputationalModels of Human Language Acquisition, pages 28?35,Ann Arbor, Michigan, June.
Association for Computa-tional Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Adaptor Grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-vances in Neural Information Processing Systems 19,pages 641?648.
MIT Press, Cambridge, MA.Mark Johnson.
2008.
Using adaptor grammars to identi-fying synergies in the unsupervised acquisition of lin-guistic structure.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 688?697.Rissanen.
1989.
Stochastic Complexity in Statistical In-quiry.
World Scientific Company, Singapore.Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods.
Springer.Y.
W. Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hier-archical Dirichlet processes.
Journal of the AmericanStatistical Association, 101:1566?1581.27
