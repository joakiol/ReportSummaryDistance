Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1?8,Sydney, July 2006. c?2006 Association for Computational LinguisticsExtracting Opinions, Opinion Holders, and Topics Expressed inOnline News Media TextSoo-Min Kim and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{skim, hovy}@ISI.EDUAbstractThis paper presents a method for identi-fying an opinion with its holder andtopic, given a sentence from online newsmedia texts.
We introduce an approach ofexploiting the semantic structure of asentence, anchored to an opinion bearingverb or adjective.
This method uses se-mantic role labeling as an intermediatestep to label an opinion holder and topicusing data from FrameNet.
We decom-pose our task into three phases: identify-ing an opinion-bearing word, labelingsemantic roles related to the word in thesentence, and then finding the holder andthe topic of the opinion word among thelabeled semantic roles.
For a broadercoverage, we also employ a clusteringtechnique to predict the most probableframe for a word which is not defined inFrameNet.
Our experimental results showthat our system performs significantlybetter than the baseline.1 IntroductionThe challenge of automatically identifying opin-ions in text automatically has been the focus ofattention in recent years in many different do-mains such as news articles and product reviews.Various approaches have been adopted in subjec-tivity detection, semantic orientation detection,review classification and review mining.
Despitethe successes in identifying opinion expressionsand subjective words/phrases, there has been lessachievement on the factors closely related to sub-jectivity and polarity, such as opinion holder,topic of opinion, and inter-topic/inter-opinionrelationships.
This paper addresses the problemof identifying not only opinions in text but alsoholders and topics of opinions from online newsarticles.Identifying opinion holders is important espe-cially in news articles.
Unlike product reviews inwhich most opinions expressed in a review arelikely to be opinions of the author of the review,news articles contain different opinions of differ-ent opinion holders (e.g.
people, organizations,and countries).
By grouping opinion holders ofdifferent stance on diverse social and politicalissues, we can have a better understanding of therelationships among countries or among organi-zations.An opinion topic can be considered as an ob-ject an opinion is about.
In product reviews, forexample, opinion topics are often the productitself or its specific features, such as design andquality (e.g.
?I like the design of iPod video?,?The sound quality is amazing?).
In news arti-cles, opinion topics can be social issues, gov-ernment?s acts, new events, or someone?s opin-ions.
(e.g., ?Democrats in Congress accused vicepresident Dick Cheney?s shooting accident.
?,?Shiite leaders accused Sunnis of a mass killingof Shiites in Madaen, south of Baghdad.?
)As for opinion topic identification, little re-search has been conducted, and only in a verylimited domain, product reviews.
In most ap-proaches in product review mining, given aproduct (e.g.
mp3 player), its frequently men-tioned features (e.g.
sound, screen, and design)are first collected and then used as anchor points.In this study, we extract opinion topics fromnews articles.
Also, we do not pre-limit topics inadvance.
We first identify an opinion and thenfind its holder and topic.
We define holder as anentity who holds an opinion, and topic, as whatthe opinion is about.In this paper, we propose a novel method thatemploys Semantic Role Labeling, a task of iden-tifying semantic roles given a sentence.
We de-1compose the overall task into the followingsteps:?
Identify opinions.?
Label semantic roles related to the opin-ions.?
Find holders and topics of opinionsamong the identified semantic roles.?
Store <opinion, holder, topic> triplesinto a database.In this paper, we focus on the first three subtasks.The main contribution of this paper is to pre-sent a method that identifies not only opinionholders but also opinion topics.
To achieve thisgoal, we utilize FrameNet data by mapping targetwords to opinion-bearing words and mappingsemantic roles to holders and topics, and then usethem for system training.
We demonstrate thatinvestigating semantic relations between an opin-ion and its holder and topic is crucial in opinionholder and topic identification.This paper is organized as follows: Section 2briefly introduces related work both in sentimentanalysis and semantic role labeling.
Section 3describes our approach for identifying opinionsand labeling holders and topics by utilizing Fra-meNet1 data for our task.
Section 4 reports ourexperiments and results with discussions andfinally Section 5 concludes.2 Related WorkThis section reviews previous works in bothsentiment detection and semantic role labeling.2.1 Subjectivity and Sentiment DetectionSubjectivity detection is the task of identifyingsubjective words, expressions, and sentences(Wiebe et al, 1999; Hatzivassiloglou and Wiebe,2000; Riloff et al, 2003).
Identifying subjectiv-ity helps separate opinions from fact, which maybe useful in question answering, summarization,etc.
Sentiment detection is the task of determin-ing positive or negative sentiment of words (Hat-zivassiloglou and McKeown, 1997; Turney,2002; Esuli and Sebastiani, 2005), phrases andsentences (Kim and Hovy, 2004; Wilson et al,2005), or documents (Pang et al, 2002; Turney,2002).Building on this work, more sophisticatedproblems such as opinion holder identificationhave also been studied.
(Bethard et al, 2004)identify opinion propositions and holders.
Their1 http://framenet.icsi.berkeley.edu/work is similar to ours but different because theiropinion is restricted to propositional opinion andmostly to verbs.
Another related works are (Choiet al, 2005; Kim and Hovy, 2005).
Both of themuse the MPQA corpus 2  but they only identifyopinion holders, not topics.As for opinion topic identification, little re-search has been conducted, and only in a verylimited domain, product reviews.
(Hu and Liu,2004; Popescu and Etzioni, 2005) present prod-uct mining algorithms with extracting certainproduct features given specific product types.Our paper aims at extracting topics of opinion ingeneral news media text.2.2 Semantic Role LabelingSemantic role labeling is the task of identifyingsemantic roles such as Agent, Patient, Speaker,or Topic, in a sentence.
A statistical approach forsemantic role labeling was introduced by (Gildeaand Jurafsky, 2002).
Their system learned se-mantic relationship among constituents in a sen-tence from FrameNet, a large corpus of semanti-cally hand-annotated data.
The FrameNet annota-tion scheme is based on Frame Semantics (Fill-more, 1976).
Frames are defined as ?schematicrepresentations of situations involving variousframe elements such as participants, props, andother conceptual roles.?
For example, given asentence ?Jack built a new house out of bricks?,a semantic role labeling system should identifythe roles for the verb built such as ?
[Agent Jack]built [Created_entity  a new house] [Component out ofbricks]?3.
In our study, we build a semantic rolelabeling system as an intermediate step to labelopinion holders and topics by training it on opin-ion-bearing frames and their frame elements inFrameNet.3 Finding Opinions and Their Holdersand TopicsFor the goal of this study, extracting opinionsfrom news media texts with their holders andtopics, we utilize FrameNet data.
The basic ideaof our approach is to explore how an opinionholder and a topic are semantically related to anopinion bearing word in a sentence.
Given a sen-tence and an opinion bearing word, our methodidentifies frame elements in the sentence and2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/3 The verb ?build?
is defined under the frame ?Build-ing?
in which Agent, Created_entity, and Componentsare defined as frame elements.2searches which frame element corresponds to theopinion holder and which to the topic.
The ex-ample in Figure 1 shows the intuition of our al-gorithm.We decompose our task in 3 subtasks: (1) col-lect opinion words and opinion-related frames,(2) semantic role labeling for those frames, and(3) finally map semantic roles to holder andtopic.
Following subsections describe each sub-task.3.1 Opinion Words and Related FramesWe describe the subtask of collecting opinionwords and related frames in 3 phases.Phase 1: Collect Opinion WordsIn this study, we consider an opinion-bearing(positive/negative) word is a key indicator of anopinion.
Therefore, we first identify opinion-bearing word from a given sentence and extractits holder and topic.
Since previous studies indi-cate that opinion-bearing verbs and adjectives areespecially efficient for opinion identification, wefocus on creating a set of opinion-bearing verbsand adjectives.
We annotated 1860 adjectivesand 2011 verbs4 by classifying them into posi-tive, negative, and neutral classes.
Words in thepositive class carry positive valence whereas4 These were randomly selected from 8011 Englishverbs and 19748 English adjectives.those in negative class carry negative valence.Words that are not opinion-bearing are classifiedas neutral.Note that in our study we treat word sentimentclassification as a three-way classification prob-lem instead of a two-way classification problem(i.e.
positive and negative).
By adding the thirdclass, neutral, we can prevent the classifier as-signing either positive or negative sentiment toweak opinion-bearing word.
For example, theword ?central?
that Hatzivassiloglou and McKe-own (1997) marked as a positive adjective is notclassified as positive by our system.
Instead wemark it as ?neutral?, since it is a weak clue for anopinion.
For the same reason, we did not con-sider ?able?
classified as a positive word by Gen-eral Inquirer5 , a sentiment word lexicon, as apositive opinion indicator.
Finally, we collected69 positive and 151 negative verbs and 199 posi-tive and 304 negative adjectives.Phase 2: Find Opinion-related FramesWe collected frames related to opinion wordsfrom the FrameNet corpus.
We used FrameNet II(Baker et al, 2003) which contains 450 semanticframes and more than 3000 frame elements (FE).A frame consists of lexical items, called LexicalUnit (LU), and related frame elements.
For in-stance, LUs in ATTACK frame are verbs such asassail, assault, and attack, and nouns such as in-vasion, raid, and strike.
FrameNet II contains5 http://www.wjh.harvard.edu/~inquirer/homecat.htmTable 1: Example of opinion related framesand lexical unitsFramename Lexical units Frame elementsDesiringwant, wish, hope,eager, desire,interested,Event,Experiencer,Location_of_eventEmotion_directedagitated, amused,anguish, ashamed,angry, annoyed,Event, TopicExperiencer,Expressor,Mental_propertyabsurd, brilliant,careless, crazy,cunning, foolishBehavior,Protagonist,Domain, DegreeSubject_stimulusdelightful, amazing,annoying, amusing,aggravating,Stimulus, DegreeExperiencer,Circumstances,Figure 1: An overview of our algorithm3approximately 7500 lexical units and over100,000 annotated sentences.For each word in our opinion word set de-scribed in Phase 1, we find a frame to which theword belongs.
49 frames for verbs and 43 framesfor adjectives are collected.
Table 1 shows ex-amples of selected frames with some of the lexi-cal units those frames cover.
For example, oursystem found the frame Desiring from opinion-bearing words want, wish, hope, etc.
Finally, wecollected 8256 and 11877 sentences related toselected opinion bearing frames for verbs andadjectives respectively.Phase 3: FrameNet expansionEven though Phase 2 searches for a correlatedframe for each verb and adjective in our opinion-bearing word list, not all of them are defined inFrameNet data.
Some words such as criticize andharass in our list have associated frames (Case1), whereas others such as vilify and maltreat donot have those (Case 2).
For a word in Case 2,we use a clustering algorithms CBC (ClusteringBy Committee) to predict the closest (most rea-sonable) frame of undefined word from existingframes.
CBC (Pantel and Lin, 2002) was devel-oped based on the distributional hypothesis (Har-ris, 1954) that words which occur in the samecontexts tend to be similar.
Using CBC, for ex-ample, our clustering module computes lexicalsimilarity between the word vilify in Case 2 andall words in Case 1.
Then it picks criticize as asimilar word, so that we can use for vilify theframe Judgment_communication to which criti-cize belongs and all frame elements defined un-der Judgment_ communication.3.2 Semantic Role LabelingTo find a potential holder and topic of an opinionword in a sentence, we first label semantic rolesin a sentence.Modeling: We follow the statistical ap-proaches for semantic role labeling (Gildea andJurafsky, 2002; Fleischman et.
al, 2003) whichseparate the task into two steps: identify candi-dates of frame elements (Step 1) and assign se-mantic roles for those candidates (Step 2).
Liketheir intuition, we treated both steps as classifica-tion problems.
We first collected all constituentsof the given sentence by parsing it using theCharniak parser.
Then, in Step 1, we classifiedcandidate constituents of frame elements fromnon-candidates.
In Step 2, each selected candi-date was thus classified into one of frame ele-ment types (e.g.
Stimulus, Degree, Experiencer,etc.).
As a learning algorithm for our classifica-tion model, we used Maximum Entropy (Bergeret al, 1996).
For system development, we usedMEGA model optimization package6, an imple-mentation of ME models.Data: We collected 8256 and 11877 sentenceswhich were associated to opinion bearing framesfor verbs and adjectives from FrameNet annota-tion data.
Each sentence in our dataset containeda frame name, a target predicate (a word whosemeaning represents aspects of the frame), andframe elements labeled with element types.
Wedivided the data into 90% for training and 10%for test.Features used: Table 2 describes features thatwe used for our classification model.
The targetword is an opinion-bearing verb or adjectivewhich is associated to a frame.
We used theCharniak parser to get a phrase type feature of aframe element and the parse tree path feature.We determined a head word of a phrase by analgorithm using a tree head table7, position fea-ture by the order of surface words of a frameelement and the target word, and the voice fea-ture by a simple pattern.
Frame name for a target6 http://www.isi.edu/~hdaume/megam/index.html7 http://people.csail.mit.edu/mcollins/papers/headsTable 2: Features used for our semantic rolelabeling model.Feature Descriptiontarget wordA predicate whose meaningrepresents the frame (a verbor an adjective in our task)phrase type Syntactic type of the frame element (e.g.
NP, PP)head word Syntactic head of the frame element phraseparse treepathA path between the frameelement and target word inthe parse treepositionWhether the element phraseoccurs before or after the tar-get wordvoice The voice of the sentence (active or passive)frame name one of our opinion-related frames4word was selected by methods described inPhase 2 and Phase 3 in Subsection 3.1.3.3 Map Semantic Roles to Holder andTopicAfter identifying frame elements in a sentence,our system finally selects holder and topic fromthose frame elements.
In the example in Table 1,the frame ?Desiring?
has frame elements such asEvent (?The change that the Experiencer wouldlike to see?
), Experiencer (?the person or sentientbeing who wishes for the Event to occur?
), Loca-tion_of_event (?the place involved in the desiredEvent?
), Focal_participant (?entity that the Ex-periencer wishes to be affected by some Event?
).Among these FEs, we can consider that Experi-encer can be a holder and Focal_participant canbe a topic (if any exists in a sentence).
Wemanually built a mapping table to map FEs toholder or topic using as support the FE defini-tions in each opinion related frame and the anno-tated sample sentences.4 Experimental ResultsThe goal of our experiment is first, to see howour holder and topic labeling system works onthe FrameNet data, and second, to examine howit performs on online news media text.
The firstdata set (Testset 1) consists of 10% of data de-scribed in Subsection 3.2 and the second (Testset2) is manually annotated by 2 humans.
(see Sub-section 4.2).
We report experimental results forboth test sets.4.1 Experiments on Testset 1Gold Standard: In total, Testset 1 contains 2028annotated sentences collected from FrameNetdata set.
(834 from frames related to opinionverb and 1194 from opinion adjectives) Wemeasure the system performance using precision(the percentage of correct holders/topics amongsystem?s labeling results), recall (the percentageof correct holders/topics that system retrieved),and F-score.Baseline: For the baseline system, we appliedtwo different algorithms for sentences whichhave opinion-bearing verbs as target words andfor those that have opinion-bearing adjectives astarget words.
For verbs, baseline system labeleda subject of a verb as a holder and an object as atopic.
(e.g.
?
[holder He] condemned [topic the law-yer].?)
For adjectives, the baseline marked thesubject of a predicate adjective as a holder (e.g.?
[holder I] was happy?).
For the topics of adjec-tives, the baseline picks a modified word if thetarget adjective is a modifier (e.g.
?That was astupid [topic mistake]?.)
and a subject word if theadjective is a predicate.
([topic The view] isbreathtaking in January.
)Result: Table 3 and 4 show evaluation resultsof our system and the baseline system respec-tively.
Our system performed much better thanthe baseline system in identifying topic andholder for both sets of sentences with verb targetwords and those with adjectives.
Especially inrecognizing topics of target opinion-bearingwords, our system improved F-score from 30.4%to 66.5% for verb target words and from 38.2%to 70.3% for adjectives.
It was interesting to seethat the intuition that ?A subject of opinion-bearing verb is a holder and an object is a topic?which we applied for the baseline achieved rela-tively good F-score (56.9%).
However, our sys-tem obtained much higher F-score (78.7%).Holder identification task achieved higher F-score than topic identification which implies thatidentifying topics of opinion is a harder task.We believe that there are many complicatedsemantic relations between opinion-bearingwords and their holders and topics that simplerelations such as subject and object relations arenot able to capture.
For example, in a sentence?Her letter upset me?, simply looking for thesubjective and objective of the verb upset is notenough to recognize the holder and topic.
It isnecessary to see a deeper level of semantic rela-Table 3.
Precision (P), Recall (R), and F-score (F) of Topic and Holder identificationfor opinion verbs (V) and adjectives (A) onTestset 1.Topic  HolderP (%) R (%) F (%) P (%) R (%) F (%)V  69.1 64.0 66.5 81.9 75.7 78.7A  67.5 73.4 70.3 66.2 77.9 71.6Table 4.
Baseline system on Testset 1.Topic  HolderP (%) R (%) F (%) P (%) R (%) F (%)V 85.5 18.5 30.4 73.7 46.4 56.9A  68.2 26.5 38.2 12.0 49.1 19.35tions: ?Her letter?
is a stimulus and ?me?
is anexperiencer of the verb upset.4.2 Experiments on Testset 2Gold Standard: Two humans 8  annotated 100sentences randomly selected from news mediatexts.
Those news data is collected from onlinenews sources such as The New York Times, UNOffice for the Coordination of Humanitarian Af-fairs, and BBC News 9 , which contain articlesabout various international affaires.
Annotatorsidentified opinion-bearing sentences with mark-ing opinion word with its holder and topic if theyexisted.
The inter-annotator agreement in identi-fying opinion sentences was 82%.Baseline: In order to identify opinion-bearingsentences for our baseline system, we used theopinion-bearing word set introduced in Phase 1in Subsection 3.1.
If a sentence contains an opin-ion-bearing verb or adjective, the baseline sys-tem started looking for its holder and topic.
Forholder and topic identification, we applied the8 We refer them as Human1 and Human2 for the rest of thispaper.9 www.nytimes.com, www.irinnews.org, andwww.bbc.co.uksame baseline algorithm as described in Subsec-tion 4.1 to Testset 2.Result: Note that Testset 1 was collected fromsentences of opinion-related frames in FrameNetand therefore all sentences in the set containedeither opinion-bearing verb or adjective.
(i.e.
Allsentences are opinion-bearing) However, sen-tences in Testset 2 were randomly collected fromonline news media pages and therefore not all ofthem are opinion-bearing.
We first evaluated thetask of opinion-bearing sentence identification.Table 5 shows the system results.
When we markall sentences as opinion-bearing, it achieved 43%and 38% of accuracy for the annotation result ofHuman1 and Human2 respectively.
Our systemperformance (64% and 55%) is comparable withthe unique assignment.We measured the holder and topic identifica-tion system with precision, recall, and F-score.As we can see from Table 6, our system achievedmuch higher precision than the baseline systemfor both Topic and Holder identification tasks.However, we admit that there is still a lot ofroom for improvement.The system achieved higher precision for topicidentification, whereas it achieved higher recallfor holder identification.
In overall, our systemattained higher F-score in holder identificationtask, including the baseline system.
Based on F-score, we believe that identifying topics of opin-ion is much more difficult than identifying hold-ers.
It was interesting to see the same phenome-non that the baseline system mainly assumingthat subject and object of a sentence are likely tobe opinion holder and topic, achieved lowerscores for both holder and topic identificationtasks in Testset 2 as in Testset 1.
This impliesthat more sophisticated analysis of the relation-ship between opinion words (e.g.
verbs and ad-jectives) and their topics and holders is crucial.4.3 Difficulties in evaluationWe observed several difficulties in evaluatingholder and topic identification.
First, the bound-ary of an entity of holder or topic can be flexible.For example, in sentence ?Senator Titus Olupitanwho sponsored the bill wants the permission.
?,not only ?Senator Titus Olupitan?
but also?Senator Titus Olupitan who sponsored the bill?is an eligible answer.
Second, some correct hold-ers and topics which our system found wereevaluated wrong even if they referred the sameentities in the gold standard because human an-notators marked only one of them as an answer.Table 5.
Opinion-bearing sentence identifica-tion on Testset 2.
(P: precision, R: recall, F:F-score, A: Accuracy, H1: Human1, H2:Human2)P (%) R (%) F (%) A (%)H1 56.9 67.4 61.7 64.0H2 43.1 57.9 49.4 55.0Table 6: Results of Topic and Holder identi-fication on Testset 2.
(Sys: our system, BL:baseline)Topic HolderP(%) R(%) F(%) P(%) R(%) F(%)H1 64.7 20.8 31.5 47.9 34.0 39.8SysH2 58.8 7.1 12.7 36.6 26.2 30.5H1 12.5 9.4 10.7 20.0 28.3 23.4BLH2 23.2 7.1 10.9 14.0 19.0 16.16In the future, we need more annotated data forimproved evaluation.5 Conclusion and Future WorkThis paper presented a methodology to identifyan opinion with its holder and topic given a sen-tence in online news media texts.
We introducedan approach of exploiting semantic structure of asentence, anchored to an opinion bearing verb oradjective.
This method uses semantic role label-ing as an intermediate step to label an opinionholder and topic using FrameNet data.
Ourmethod first identifies an opinion-bearing word,labels semantic roles related to the word in thesentence, and then finds a holder and a topic ofthe opinion word among labeled semantic roles.There has been little previous study in identi-fying opinion holders and topics partly because itrequires a great amount of annotated data.
Toovercome this barrier, we utilized FrameNet databy mapping target words to opinion-bearingwords and mapping semantic roles to holders andtopics.
However, FrameNet has a limited numberof words in its annotated corpus.
For a broadercoverage, we used a clustering technique to pre-dict a most probable frame for an unseen word.Our experimental results showed that our sys-tem performs significantly better than the base-line.
The baseline system results imply that opin-ion holder and topic identification is a hard task.We believe that there are many complicated se-mantic relations between opinion-bearing wordsand their holders and topics which simple rela-tions such as subject and object relations are notable to capture.In the future, we plan to extend our list ofopinion-bearing verbs and adjectives so that wecan discover and apply more opinion-relatedframes.
Also, it would be interesting to see howother types of part of speech such as adverbs andnouns affect the performance of the system.ReferenceBaker, Collin F. and Hiroaki Sato.
2003.
The Frame-Net Data and Software.
Poster and Demonstrationat Association for Computational Linguistics.
Sap-poro, Japan.Berger, Adam, Stephen Della Pietra, and VincentDella Pietra.
1996.
A maximum entropy approachto natural language processing, Computational Lin-guistics, (22-1).Bethard, Steven, Hong Yu, Ashley Thornton, Va-sileios Hatzivassiloglou, and Dan Jurafsky.
2004.Automatic Extraction of Opinion Propositions andtheir Holders, AAAI Spring Symposium on Explor-ing Attitude and Affect in Text: Theories and Ap-plications.Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S.2005.
Identifying Sources of Opinions with Condi-tional Random Fields and Extraction Patterns.
Pro-ceedings of HLT/EMNLP-05.Esuli, Andrea and Fabrizio Sebastiani.
2005.
Deter-mining the semantic orientation of terms throughgloss classification.
Proceedings of CIKM-05, 14thACM International Conference on Information andKnowledge Management, Bremen, DE, pp.
617-624.Fillmore, C. Frame semantics and the nature of lan-guage.
1976.
In Annals of the New York Academyof Sciences: Conferences on the Origin and Devel-opment of Language and Speech, Volume 280: 20-32.Fleischman, Michael, Namhee Kwon, and EduardHovy.
2003.
Maximum Entropy Models for Fra-meNet Classification.
Proceedings of EMNLP,Sapporo, Japan.Gildea, D. and Jurafsky, D. Automatic Labeling ofsemantic roles.
2002.
In Computational Linguis-tics.
28(3), 245-288.Harris, Zellig, 1954.
Distributional structure.
Word,10(23) :146--162.Hatzivassiloglou, Vasileios and Kathleen McKeown.1997.
Predicting the Semantic Orientation of Ad-jectives.
Proceedings of 35th Annual Meeting ofthe Assoc.
for Computational Linguistics (ACL-97): 174-181Hatzivassiloglou, Vasileios and Wiebe, Janyce.
2000.Effects of Adjective Orientation and Gradability onSentence Subjectivity.
Proceedings of Interna-tional Conference on Computational Linguistics(COLING-2000).
Saarbr?cken, Germany.Hu, Minqing and Bing Liu.
2004.
Mining and summa-rizing customer reviews".
Proceedings of the ACMSIGKDD International Conference on KnowledgeDiscovery & Data Mining (KDD-2004), Seattle,Washington, USA.Kim, Soo-Min and Eduard Hovy.
2004.
Determiningthe Sentiment of Opinions.
Proceedings of COL-ING-04.
pp.
1367-1373.
Geneva, Switzerland.Kim, Soo-Min and Eduard Hovy.
2005.
IdentifyingOpinion Holders for Question Answering in Opin-ion Texts.
Proceedings of AAAI-05 Workshop onQuestion Answering in Restricted DomainsPang, Bo, Lillian Lee, and Shivakumar Vaithyana-than.
2002.
Thumbs up?
Sentiment Classificationusing Machine Learning Techniques, Proceedingsof EMNLP-2002.7Pantel, Patrick and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
Proceedings of ACM Con-ference on Knowledge Discovery and Data Mining.(KDD-02).
pp.
613-619.
Edmonton, Canada.Popescu, Ana-Maria and Oren Etzioni.
2005.Extracting Product Features and Opinions fromReviews , Proceedings of HLT-EMNLP 2005.Riloff, Ellen, Janyce Wiebe, and Theresa Wilson.2003.
Learning Subjective Nouns Using ExtractionPattern Bootstrapping.
Proceedings of SeventhConference on Natural Language Learning(CoNLL-03).
ACL SIGNLL.
Pages 25-32.Turney, Peter D. 2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised clas-sification of reviews, Proceedings of ACL-02,Philadelphia, Pennsylvania, 417-424Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P.O'Hara.
1999.
Development and use of a gold stan-dard data set for subjectivity classifications.
Pro-ceedings of ACL-99.
University of Maryland, June,pp.
246-253.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
Proceedings ofHLT/EMNLP 2005, Vancouver, Canada8
