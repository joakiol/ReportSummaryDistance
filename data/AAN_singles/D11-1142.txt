Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1535?1545,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsIdentifying Relations for Open Information ExtractionAnthony Fader, Stephen Soderland, and Oren EtzioniUniversity of Washington, Seattle{afader,soderlan,etzioni}@cs.washington.eduAbstractOpen Information Extraction (IE) is the taskof extracting assertions from massive corporawithout requiring a pre-specified vocabulary.This paper shows that the output of state-of-the-art Open IE systems is rife with uninfor-mative and incoherent extractions.
To over-come these problems, we introduce two sim-ple syntactic and lexical constraints on bi-nary relations expressed by verbs.
We im-plemented the constraints in the REVERBOpen IE system, which more than doubles thearea under the precision-recall curve relativeto previous extractors such as TEXTRUNNERand WOEpos.
More than 30% of REVERB?sextractions are at precision 0.8 or higher?compared to virtually none for earlier systems.The paper concludes with a detailed analysisof REVERB?s errors, suggesting directions forfuture work.11 Introduction and MotivationTypically, Information Extraction (IE) systems learnan extractor for each target relation from la-beled training examples (Kim and Moldovan, 1993;Riloff, 1996; Soderland, 1999).
This approach to IEdoes not scale to corpora where the number of targetrelations is very large, or where the target relationscannot be specified in advance.
Open IE solves thisproblem by identifying relation phrases?phrasesthat denote relations in English sentences (Bankoet al, 2007).
The automatic identification of rela-1The source code for REVERB is available at http://reverb.cs.washington.edu/tion phrases enables the extraction of arbitrary re-lations from sentences, obviating the restriction to apre-specified vocabulary.Open IE systems have achieved a notable measureof success on massive, open-domain corpora drawnfrom the Web, Wikipedia, and elsewhere.
(Banko etal., 2007; Wu and Weld, 2010; Zhu et al, 2009).
Theoutput of Open IE systems has been used to supporttasks like learning selectional preferences (Ritter etal., 2010), acquiring common sense knowledge (Linet al, 2010), and recognizing entailment (Schoen-mackers et al, 2010; Berant et al, 2011).
In ad-dition, Open IE extractions have been mapped ontoexisting ontologies (Soderland et al, 2010).We have observed that two types of errors are fre-quent in the output of Open IE systems such as TEX-TRUNNER and WOE: incoherent extractions and un-informative extractions.Incoherent extractions are cases where the ex-tracted relation phrase has no meaningful interpre-tation (see Table 1 for examples).
Incoherent ex-tractions arise because the learned extractor makes asequence of decisions about whether to include eachword in the relation phrase, often resulting in incom-prehensible predictions.
To solve this problem, weintroduce a syntactic constraint: every multi-wordrelation phrase must begin with a verb, end with apreposition, and be a contiguous sequence of wordsin the sentence.
Thus, the identification of a relationphrase is made in one fell swoop instead of on thebasis of multiple, word-by-word decisions.Uninformative extractions are extractions thatomit critical information.
For example, consider thesentence ?Faust made a deal with the devil.?
Previ-1535ous Open IE systems return the uninformative(Faust, made, a deal)instead of(Faust, made a deal with, the devil).This type of error is caused by improper handlingof relation phrases that are expressed by a combi-nation of a verb with a noun, such as light verbconstructions (LVCs).
An LVC is a multi-word ex-pression composed of a verb and a noun, with thenoun carrying the semantic content of the predi-cate (Grefenstette and Teufel, 1995; Stevenson et al,2004; Allerton, 2002).
Table 2 illustrates the widerange of relations expressed this way, which are notcaptured by existing open extractors.
Our syntacticconstraint leads the extractor to include nouns in therelation phrase, solving this problem.Although the syntactic constraint significantly re-duces incoherent and uninformative extractions, itallows overly-specific relation phrases such as is of-fering only modest greenhouse gas reduction targetsat.
To avoid overly-specific relation phrases, we in-troduce an intuitive lexical constraint: a binary rela-tion phrase ought to appear with at least a minimalnumber of distinct argument pairs in a large corpus.In summary, this paper articulates two simple butsurprisingly powerful constraints on how binary re-lationships are expressed via verbs in English sen-tences, and implements them in the REVERB OpenIE system.
We release REVERB and the data used inour experiments to the research community.The rest of the paper is organized as follows.
Sec-tion 2 analyzes previous work.
Section 3 defines ourconstraints precisely.
Section 4 describes REVERB,our implementation of the constraints.
Section 5 re-ports on our experimental results.
Section 6 con-cludes with a summary and discussion of futurework.2 Previous WorkOpen IE systems like TEXTRUNNER (Banko et al,2007), WOEpos, and WOEparse (Wu and Weld, 2010)focus on extracting binary relations of the form(arg1, relation phrase, arg2) from text.
These sys-tems all use the following three-step method:1.
Label: Sentences are automatically labeledwith extractions using heuristics or distant su-pervision.Sentence Incoherent RelationThe guide contains dead linksand omits sites.contains omitsThe Mark 14 was central to thetorpedo scandal of the fleet.was central torpedoThey recalled that Nungesserbegan his career as a precinctleader.recalled beganTable 1: Examples of incoherent extractions.
In-coherent extractions make up approximately 13% ofTEXTRUNNER?s output, 15% of WOEpos?s output, and30% of WOEparse?s output.is is an album by, is the author of, is a city inhas has a population of, has a Ph.D. in, has a cameo inmade made a deal with, made a promise totook took place in, took control over, took advantage ofgave gave birth to, gave a talk at, gave new meaning togot got tickets to, got a deal on, got funding fromTable 2: Examples of uninformative relations (left) andtheir completions (right).
Uninformative relations oc-cur in approximately 4% of WOEparse?s output, 6% ofWOEpos?s output, and 7% of TEXTRUNNER?s output.2.
Learn: A relation phrase extractor is learnedusing a sequence-labeling graphical model(e.g., CRF).3.
Extract: the system takes a sentence as in-put, identifies a candidate pair of NP arguments(arg1, arg2) from the sentence, and then usesthe learned extractor to label each word be-tween the two arguments as part of the relationphrase or not.The extractor is applied to the successive sentencesin the corpus, and the resulting extractions are col-lected.This method faces several challenges.
First,the training phase requires a large number of la-beled training examples (e.g., 200, 000 heuristically-labeled sentences for TEXTRUNNER and 300, 000for WOE).
Heuristic labeling of examples obviateshand labeling but results in noisy labels and distortsthe distribution of examples.
Second, the extrac-tion step is posed as a sequence-labeling problem,where each word is assigned its own label.
Becauseeach assignment is uncertain, the likelihood that theextracted relation phrase is flawed increases withthe length of the sequence.
Finally, the extractor1536chooses an extraction?s arguments heuristically, andcannot backtrack over this choice.
This is problem-atic when a word that belongs in the relation phraseis chosen as an argument (for example, deal fromthe ?made a deal with?
sentence).Because of the feature sets utilized in previouswork, the learned extractors ignore both ?holistic?aspects of the relation phrase (e.g., is it contiguous?
)as well as lexical aspects (e.g., how many instancesof this relation are there?).
Thus, as we show in Sec-tion 5, systems such as TEXTRUNNER are unableto learn the constraints embedded in REVERB.
Ofcourse, a learning system, utilizing a different hy-pothesis space, and an appropriate set of training ex-amples, could potentially learn and refine the con-straints in REVERB.
This is a topic for future work,which we consider in Section 6.The first Open IE system was TEXTRUNNER(Banko et al, 2007), which used a Naive Bayesmodel with unlexicalized POS and NP-chunk fea-tures, trained using examples heuristically generatedfrom the Penn Treebank.
Subsequent work showedthat utilizing a linear-chain CRF (Banko and Et-zioni, 2008) or Markov Logic Network (Zhu et al,2009) can lead to improved extraction.
The WOEsystems introduced by Wu and Weld make use ofWikipedia as a source of training data for their ex-tractors, which leads to further improvements overTEXTRUNNER (Wu and Weld, 2010).
Wu and Weldalso show that dependency parse features result in adramatic increase in precision and recall over shal-low linguistic features, but at the cost of extractionspeed.Other approaches to large-scale IE have includedPreemptive IE (Shinyama and Sekine, 2006), On-Demand IE (Sekine, 2006), and weak supervisionfor IE (Mintz et al, 2009; Hoffmann et al, 2010).Preemptive IE and On-Demand IE avoid relation-specific extractors, but rely on document and en-tity clustering, which is too costly for Web-scale IE.Weakly supervised methods use an existing ontol-ogy to generate training data for learning relation-specific extractors.
While this allows for learn-ing relation-specific extractors at a larger scale thanwhat was previously possible, the extractions arestill restricted to a specific ontology.Many systems have used syntactic patterns basedon verbs to extract relation phrases, usually rely-ing on a full dependency parse of the input sentence(Lin and Pantel, 2001; Stevenson, 2004; Specia andMotta, 2006; Kathrin Eichler and Neumann, 2008).Our work differs from these approaches by focus-ing on relation phrase patterns expressed in termsof POS tags and NP chunks, instead of full parsetrees.
Banko and Etzioni (Banko and Etzioni, 2008)showed that a small set of POS-tag patterns cover alarge fraction of relationships in English, but neverincorporated the patterns into an extractor.
This pa-per reports on a substantially improved model of bi-nary relation phrases, which increases the recall ofthe Banko-Etzioni model (see Section 3.3).
Further,while previous work in Open IE has mainly focusedon syntactic patterns for relation extraction, we in-troduce a lexical constraint that boosts precision andrecall.Finally, Open IE is closely related to semantic rolelabeling (SRL) (Punyakanok et al, 2008; Toutanovaet al, 2008) in that both tasks extract relations andarguments from sentences.
However, SRL systemstraditionally rely on syntactic parsers, which makesthem susceptible to parser errors and substantiallyslower than Open IE systems such as REVERB.
Thisdifference is particularly important when operatingon the Web corpus due to its size and heterogeneity.Finally, SRL requires hand-constructed semantic re-sources like Propbank and Framenet (Martha andPalmer, 2002; Baker et al, 1998) as input.
In con-trast, Open IE systems require no relation-specifictraining data.
ReVerb, in particular, relies on its ex-plicit lexical and syntactic constraints, which haveno correlate in SRL systems.
For a more detailedcomparison of SRL and Open IE, see (Christensenet al, 2010).3 Constraints on Relation PhrasesIn this section we introduce two constraints on re-lation phrases: a syntactic constraint and a lexicalconstraint.3.1 Syntactic ConstraintThe syntactic constraint serves two purposes.
First,it eliminates incoherent extractions, and second, itreduces uninformative extractions by capturing rela-tion phrases expressed by a verb-noun combination,including light verb constructions.1537V | V P | VW ?PV = verb particle?
adv?W = (noun | adj | adv | pron | det)P = (prep | particle | inf.
marker)Figure 1: A simple part-of-speech-based regular expres-sion reduces the number of incoherent extractions likewas central torpedo and covers relations expressed vialight verb constructions like gave a talk at.The syntactic constraint requires the relationphrase to match the POS tag pattern shown in Fig-ure 1.
The pattern limits relation phrases to be eithera verb (e.g., invented), a verb followed immediatelyby a preposition (e.g., located in), or a verb followedby nouns, adjectives, or adverbs ending in a preposi-tion (e.g., has atomic weight of).
If there are multiplepossible matches in a sentence for a single verb, thelongest possible match is chosen.
Finally, if the pat-tern matches multiple adjacent sequences, we mergethem into a single relation phrase (e.g., wants to ex-tend).
This refinement enables the model to readilyhandle relation phrases containing multiple verbs.
Aconsequence of this pattern is that the relation phrasemust be a contiguous span of words in the sentence.The syntactic constraint eliminates the incoherentrelation phrases returned by existing systems.
Forexample, given the sentenceExtendicare agreed to buy Arbor Health Care forabout US $432 million in cash and assumed debt.TEXTRUNNER returns the extraction(Arbor Health Care, for assumed, debt).The phrase for assumed is clearly not a valid rela-tion phrase: it begins with a preposition and splicestogether two distant words in the sentence.
The syn-tactic constraint prevents this type of error by sim-ply restricting relation phrases to match the patternin Figure 1.The syntactic constraint reduces uninformativeextractions by capturing relation phrases expressedvia LVCs.
For example, the POS pattern matchedagainst the sentence ?Faust made a deal with theDevil,?
would result in the relation phrase made adeal with, instead of the uninformative made.Finally, we require the relation phrase to appearbetween its two arguments in the sentence.
This is acommon constraint that has been implicitly enforcedin other open extractors.3.2 Lexical ConstraintWhile the syntactic constraint greatly reduces unin-formative extractions, it can sometimes match rela-tion phrases that are so specific that they have only afew possible instances, even in a Web-scale corpus.Consider the sentence:The Obama administration is offering only modestgreenhouse gas reduction targets at the conference.The POS pattern will match the phrase:is offering only modest greenhouse gas reduction targets at(1)Thus, there are phrases that satisfy the syntactic con-straint, but are not relational.To overcome this limitation, we introduce a lexi-cal constraint that is used to separate valid relationphrases from overspecified relation phrases, like theexample in (1).
The constraint is based on the in-tuition that a valid relation phrase should take manydistinct arguments in a large corpus.
The phrase in(1) is specific to the argument pair (Obama admin-istration, conference), so it is unlikely to represent abona fide relation.
We describe the implementationdetails of the lexical constraint in Section 4.3.3 LimitationsOur constraints represent an idealized model of re-lation phrases in English.
This raises the question:How much recall is lost due to the constraints?To address this question, we analyzed Wu andWeld?s set of 300 sentences from a set of randomWeb pages, manually identifying all verb-based re-lationships between noun phrase pairs.
This resultedin a set of 327 relation phrases.
For each rela-tion phrase, we checked whether it satisfies our con-straints.
We found that 85% of the relation phrasesdo satisfy the constraints.
Of the remaining 15%,we identified some of the common cases where theconstraints were violated, summarized in Table 3.Many of the example relation phrases shown inTable 3 involve long-range dependencies betweenwords in the sentence.
These types of dependen-cies are not easily representable using a pattern overPOS tags.
A deeper syntactic analysis of the inputsentence would provide a much more general lan-guage for modeling relation phrases.
For example,one could create a model of relations expressed in1538Binary Verbal Relation Phrases85% Satisfy Constraints8% Non-Contiguous Phrase StructureCoordination: X is produced and maintained by YMultiple Args: X was founded in 1995 by YPhrasal Verbs: X turned Y off4% Relation Phrase Not Between ArgumentsIntro.
Phrases: Discovered by Y, X .
.
.Relative Clauses: .
.
.
the Y that X discovered3% Do Not Match POS PatternInterrupting Modifiers: X has a lot of faith in YInfinitives: X to attack YTable 3: Approximately 85% of the binary verbal relationphrases in a sample of Web sentences satisfy our con-straints.terms of dependency parse features that would cap-ture the non-contiguous relation phrases in Table 3.Previous work has shown that dependency paths doindeed boost the recall of relation extraction systems(Wu and Weld, 2010; Mintz et al, 2009).
While us-ing dependency path features allows for a more flex-ible model of relations, it significantly increases pro-cessing time, which is problematic for Web-scale ex-traction.
Further, we have found that this increasedrecall comes at the cost of lower precision on Webtext (see Section 5).The results in Table 3 are similar to Banko and Et-zioni?s findings that a set of eight POS patterns covera large fraction of binary verbal relation phrases.However, their analysis was based on a set of sen-tences known to contain either a company acquisi-tion or birthplace relationship, while our results areon a random sample of Web sentences.
We appliedBanko and Etzioni?s verbal patterns to our randomsample of 300 Web sentences, and found that theycover approximately 69% of the relation phrases inthe corpus.
The gap in recall between this and the85% shown in Table 3 is largely due to LVC relationphrases (made a deal with) and phrases containingmultiple verbs (refuses to return to), which their pat-terns do not cover.In sum, our model is by no means complete.However, we have empirically shown that the ma-jority of binary verbal relation phrases in a sampleof Web sentences are captured by our model.
Byfocusing on this subset of language, our model canbe used to perform Open IE at significantly higherprecision than before.4 REVERBThis section introduces REVERB, a novel open ex-tractor based on the constraints defined in the previ-ous section.
REVERB first identifies relation phrasesthat satisfy the syntactic and lexical constraints, andthen finds a pair of NP arguments for each identifiedrelation phrase.
The resulting extractions are thenassigned a confidence score using a logistic regres-sion classifier.This algorithm differs in three important waysfrom previous methods (Section 2).
First, the re-lation phrase is identified ?holistically?
rather thanword-by-word.
Second, potential phrases are fil-tered based on statistics over a large corpus (theimplementation of our lexical constraint).
Finally,REVERB is ?relation first?
rather than ?argumentsfirst?, which enables it to avoid a common errormade by previous methods?confusing a noun in therelation phrase for an argument, e.g.
the noun deal inmade a deal with.4.1 Extraction AlgorithmREVERB takes as input a POS-tagged and NP-chunked sentence and returns a set of (x, r, y)extraction triples.2 Given an input sentence s,REVERB uses the following extraction algorithm:1.
Relation Extraction: For each verb v in s,find the longest sequence of words rv such that(1) rv starts at v, (2) rv satisfies the syntacticconstraint, and (3) rv satisfies the lexical con-straint.
If any pair of matches are adjacent oroverlap in s, merge them into a single match.2.
Argument Extraction: For each relationphrase r identified in Step 1, find the nearestnoun phrase x to the left of r in s such that x isnot a relative pronoun, WHO-adverb, or exis-tential ?there?.
Find the nearest noun phrase yto the right of r in s. If such an (x, y) pair couldbe found, return (x, r, y) as an extraction.We check whether a candidate relation phraserv satisfies the syntactic constraint by matching itagainst the regular expression in Figure 1.2REVERB uses OpenNLP for POS tagging and NP chunk-ing: http://opennlp.sourceforge.net/1539To determine whether rv satisfies the lexical con-straint, we use a large dictionary D of relationphrases that are known to take many distinct argu-ments.
In an offline step, we construct D by find-ing all matches of the POS pattern in a corpus of500 million Web sentences.
For each matching re-lation phrase, we heuristically identify its arguments(as in Step 2 above).
We set D to be the set of allrelation phrases that take at least k distinct argumentpairs in the set of extractions.
In order to allow forminor variations in relation phrases, we normalizeeach relation phrase by removing inflection, auxil-iary verbs, adjectives, and adverbs.
Based on ex-periments on a held-out set of sentences, we foundthat a value of k = 20 works well for filtering outoverspecified relations.
This results in a set of ap-proximately 1.7 million distinct normalized relationphrases, which are stored in memory at extractiontime.As an example of the extraction algorithm in ac-tion, consider the following input sentence:Hudson was born in Hampstead, which is asuburb of London.Step 1 of the algorithm identifies three relationphrases that satisfy the syntactic and lexical con-straints: was, born in, and is a suburb of.
The firsttwo phrases are adjacent in the sentence, so they aremerged into the single relation phrase was born in.Step 2 then finds an argument pair for each relationphrase.
For was born in, the nearest NPs are (Hud-son, Hampstead).
For is a suburb of, the extractorskips over the NP which and chooses the argumentpair (Hampstead, London).
The final output ise1: (Hudson, was born in, Hampstead)e2: (Hampstead, is a suburb of, London).4.2 Confidence FunctionThe extraction algorithm in the previous section hashigh recall, but low precision.
Like with previousopen extractors, we want way to trade recall for pre-cision by tuning a confidence threshold.
We use alogistic regression classifier to assign a confidencescore to each extraction, which uses the featuresshown in Table 4.
All of these features are efficientlycomputable and relation independent.
We trainedthe confidence function by manually labeling the ex-tractions from a set of 1, 000 sentences from the Weband Wikipedia as correct or incorrect.Weight Feature1.16 (x, r, y) covers all words in s0.50 The last preposition in r is for0.49 The last preposition in r is on0.46 The last preposition in r is of0.43 len(s) ?
10 words0.43 There is a WH-word to the left of r0.42 r matches VW*P from Figure 10.39 The last preposition in r is to0.25 The last preposition in r is in0.23 10 words < len(s) ?
20 words0.21 s begins with x0.16 y is a proper noun0.01 x is a proper noun-0.30 There is an NP to the left of x in s-0.43 20 words < len(s)-0.61 r matches V from Figure 1-0.65 There is a preposition to the left of x in s-0.81 There is an NP to the right of y in s-0.93 Coord.
conjunction to the left of r in sTable 4: REVERB uses these features to assign a confi-dence score to an extraction (x, r, y) from a sentence susing a logistic regression classifier.Previous open extractors require labeled trainingdata to learn a model of relations, which is then usedto extract relation phrases from text.
In contrast,REVERB uses a specified model of relations for ex-traction, and requires labeled data only for assigningconfidence scores to its extractions.
Learning a con-fidence function is a much simpler task than learninga full model of relations, using two orders of magni-tude fewer training examples than TEXTRUNNER orWOE.4.3 TEXTRUNNER-RThe model of relation phrases used by REVERBis specified, but could a TEXTRUNNER-like sys-tem learn this model from training data?
Whileit is difficult to answer such a question for allpossible permutations of features sets, training ex-amples, and learning biases, we demonstrate thatTEXTRUNNER itself cannot learn REVERB?s modeleven when re-trained using the output of REVERBas labeled training data.
The resulting system,TEXTRUNNER-R, uses the same feature representa-tion as TEXTRUNNER, but different parameters, anda different set of training examples.To generate positive instances, we ran REVERB1540on the Penn Treebank, which is the same datasetthat TEXTRUNNER is trained on.
To generate neg-ative instances from a sentence, we took each nounphrase pair in the sentence that does not appear asarguments in a REVERB extraction.
This processresulted in a set of 67, 562 positive instances, and356, 834 negative instances.
We then passed theselabeled examples to TEXTRUNNER?s training proce-dure, which learns a linear-chain CRF using closed-class features like POS tags, capitalization, punctu-ation, etc.TEXTRUNNER-R uses the argument-firstextraction algorithm described in Section 2.5 ExperimentsWe compare REVERB to the following systems:?
REVERB?lex - The REVERB system describedin the previous section, but without the lexicalconstraint.
REVERB?lex uses the same confi-dence function as REVERB.?
TEXTRUNNER - Banko and Etzioni?s 2008 ex-tractor, which uses a second order linear-chainCRF trained on extractions heuristically gener-ated from the Penn Treebank.
TEXTRUNNERuses shallow linguistic features in its CRF,which come from the same POS tagger and NP-chunker that REVERB uses.?
TEXTRUNNER-R - Our modification toTEXTRUNNER, which uses the same extrac-tion code, but with a model of relations trainedon REVERB extractions.?
WOEpos - Wu and Weld?s modification toTEXTRUNNER, which uses a model of re-lations learned from extractions heuristicallygenerated from Wikipedia.?
WOEparse - Wu and Weld?s parser-based ex-tractor, which uses a large dictionary of depen-dency path patterns learned from heuristic ex-tractions generated from Wikipedia.Each system is given a set of sentences as input,and returns a set of binary extractions as output.
Wecreated a test set of 500 sentences sampled from theWeb, using Yahoo?s random link service.3 After run-3http://random.yahoo.com/bin/rylREVERB REVERB WOE TEXT- WOE TEXT-0.00.10.20.30.40.5AreaUnderPRCurve?lexparseRUNNER-RposRUNNERAreaUnderPRCurveFigure 2: REVERB outperforms state-of-the-art openextractors, with an AUC more than twice that ofTEXTRUNNER or WOEpos, and 38% higher thanWOEparse.0.0 0.1 0.2 0.3 0.4 0.5 0.6Recall0.00.20.40.60.81.0PrecisionComparison of REVERB-Based SystemsREVERBREVERB?lexTEXTRUNNER-RPrecisionFigure 3: The lexical constraint gives REVERBa boost in precision and recall over REVERB?lex.TEXTRUNNER-R is unable to learn the model used byREVERB, which results in lower precision and recall.ning each extractor over the input sentences, two hu-man judges independently evaluated each extractionas correct or incorrect.
The judges reached agree-ment on 86% of the extractions, with an agreementscore of ?
= 0.68.
We report results on the subsetof the data where the two judges concur.The judges labeled uninformative extractions con-servatively.
That is, if critical information wasdropped from the relation phrase but included in thesecond argument, it is labeled correct.
For example,both the extractions (Ackerman, is a professor of, bi-ology) and (Ackerman, is, a professor of biology) areconsidered correct.Each system returns confidence scores for its ex-tractions.
For a given threshold, we can measurethe precision and recall of the output.
Precisionis the fraction of returned extractions that are cor-rect.
Recall is the fraction of correct extractions in15410.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Recall0.00.20.40.60.81.0PrecisionExtractionsREVERBWOEparseWOEposTEXTRUNNERPrecisionFigure 4: REVERB achieves significantly higher preci-sion than state-of-the-art Open IE systems, and compara-ble recall to WOEparse.the corpus that are returned.
We use the total num-ber of extractions labeled as correct by the judgesas our measure of recall for the corpus.
In order toavoid double-counting, we treat extractions that dif-fer superficially (e.g., different punctuation or drop-ping inessential modifiers) as a single extraction.
Wecompute a precision-recall curve by varying the con-fidence threshold, and then compute the area underthe curve (AUC).5.1 ResultsFigure 2 shows the AUC of each system.
REVERBachieves an AUC that is 30% higher than WOEparseand is more than double the AUC of WOEpos orTEXTRUNNER.
The lexical constraint provides asignificant boost in performance, with REVERBachieving an AUC 23% higher than REVERB?lex.REVERB proves to be a useful source of train-ing data, with TEXTRUNNER-R having an AUC71% higher than TEXTRUNNER and performingon par with WOEpos.
From the training data,TEXTRUNNER-R was able to learn a model thatpredicts contiguous relation phrases, but still re-turned incoherent relation phrases (e.g., starting witha preposition) and overspecified relation phrases.These errors are due to TEXTRUNNER-R overfittingthe training data and not having access to the lexicalconstraint.Figure 3 shows the precision-recall curves of thesystems introduced in this paper.
TEXTRUNNER-Rhas much lower precision than REVERB and0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Recall0.00.20.40.60.81.0PrecisionRelations OnlyREVERBWOEparseWOEposTEXTRUNNERPrecisionFigure 5: On the subtask of identifying relations phrases,REVERB is able to achieve even higher precision and re-call than other systems.REVERB?lex at all levels of recall.
The lexi-cal constraint gives REVERB a boost in precisionover REVERB?lex, reducing overspecified extrac-tions from 20% of REVERB?lex?s output to 1% ofREVERB?s.
The lexical constraint also boosts recallover REVERB?lex, since REVERB is able to find acorrect relation phrase where REVERB?lex finds anoverspecified one.Figure 4 shows the precision-recall curves ofREVERB and the external systems.
REVERB hasmuch higher precision than the other systems atnearly all levels of recall.
In particular, more than30% of REVERB?s extractions are at precision 0.8or higher, compared to virtually none for the othersystems.
WOEparse achieves a slightly higher recallthan REVERB (0.62 versus 0.64), but at the cost oflower precision.In order to highlight the role of the relationalmodel of each system, we also evaluate their per-formance on the subtask of extracting just the rela-tion phrases from the input text.
Figure 5 shows theprecision-recall curves for each system on the rela-tion phrase-only evaluation.
In this case, REVERBhas both higher precision and recall than the othersystems.REVERB?s biggest improvement came from theelimination of incoherent extractions.
Incoher-ent extractions were a large fraction of the errorsmade by previous systems, accounting for approxi-mately 13% of TEXTRUNNER?s extractions, 15% ofWOEpos?s, and 30% of WOEparse?s.
Uninformative1542REVERB - Incorrect Extractions65% Correct relation phrase, incorrect arguments16% N-ary relation8% Non-contiguous relation phrase2% Imperative verb2% Overspecified relation phrase7% Other, including POS/chunking errorsTable 5: The majority of the incorrect extractions re-turned by REVERB are due to errors in argument extrac-tion.extractions had a smaller effect on other systems?precision, accounting for 4% of WOEparse?s extrac-tions, 5% of WOEpos?s, and 7% of TEXTRUNNER?s,while only appearing in 1% of REVERB?s extrac-tions.
REVERB?s reduction in uninformative extrac-tions resulted in a boost in recall, capturing manyLVC relation phrases missed by other systems (likethose shown in Table 2).To test the systems?
speed, we ran each extrac-tor on a set of 100, 000 sentences using a Pen-tium 4 machine with 4GB of RAM.
The process-ing times were 16 minutes for REVERB, 21 min-utes for TEXTRUNNER, 21 minutes for WOEpos, and11 hours for WOEparse.
The times for REVERB,TEXTRUNNER, and WOEpos are all approximatelythe same, since they all use the same POS-taggingand NP-chunking software.
WOEparse processeseach sentence with a dependency parser, resultingin much longer processing time.5.2 REVERB Error AnalysisTo better understand the limitations of REVERB, weperformed a detailed analysis of its errors in pre-cision (incorrect extractions returned by REVERB)and its errors in recall (correct extractions thatREVERB missed).Table 5 summarizes the types of incorrect extrac-tions that REVERB returns.
We found that 65% ofthe incorrect extractions returned by REVERB werecases where a relation phrase was correctly identi-fied, but the argument-finding heuristics failed.
Theremaining errors were cases where REVERB ex-tracted an incorrect relation phrase.
One commonmistake that REVERB made was extracting a rela-tion phrase that expresses an n-ary relationship viaa ditransitive verb.
For example, given the sentenceREVERB - Missed Extractions52% Could not identify correct arguments23% Relation filtered out by lexical constraint17% Identified a more specific relation8% POS/chunking errorTable 6: The majority of extractions that were missed byREVERB were cases where the correct relation phrasewas found, but the arguments were not correctly identi-fied.
?I gave him 15 photographs,?
REVERB extracts (I,gave, him).
These errors are due to the fact thatREVERB only models binary relations.Table 6 summarizes the correct extractions thatwere extracted by other systems and were not ex-tracted by REVERB.
As with the false positive ex-tractions, the majority of false negatives (52%) weredue to the argument-finding heuristics choosing thewrong arguments, or failing to extract all possible ar-guments (in the case of coordinating conjunctions).Other sources of failure were due to the lexical con-straint either failing to filter out an overspecified re-lation phrase or filtering out a valid relation phrase.These errors hurt both precision and recall, sinceeach case results in the extractor overlooking a cor-rect relation phrase and choosing another.5.3 Evaluation At ScaleSection 5.1 shows that REVERB outperforms ex-isting Open IE systems when evaluated on a sam-ple of sentences.
Previous work has shown thatthe frequency of an extraction in a large corpus isuseful for assessing the correctness of extractions(Downey et al, 2005).
Thus, it is possible a pri-ori that REVERB?s gains over previous systems willdiminish when extraction frequency is taken into ac-count.In fact, we found that REVERB?s advantage overTEXTRUNNER when run at scale is qualitativelysimilar to its advantage on single sentences.
We ranboth REVERB and TEXTRUNNER on Banko and Et-zioni?s corpus of 500 million Web sentences and ex-amined the effect of redundancy on precision.As Downey?s work predicts, precision increasedin both systems for extractions found multipletimes, compared with extractions found only once.However, REVERB had higher precision than1543TEXTRUNNER at all frequency thresholds.
In fact,REVERB?s frequency 1 extractions had a precisionof 0.75, which TEXTRUNNER could not approacheven with frequency 10 extractions, which had aprecision of 0.34.
Thus, REVERB is able to returnmore correct extractions at a higher precision thanTEXTRUNNER, even when redundancy is taken intoaccount.6 Conclusions and Future WorkThe paper?s contributions are as follows:?
We have identified and analyzed the problemsof incoherent and uninformative extractions forOpen IE systems, and shown their prevalencefor systems such as TEXTRUNNER and WOE.?
We articulated general, easy-to-enforce con-straints on binary, verb-based relation phrasesin English that ameliorate these problems andyield richer and more informative relations(see, for example, Table 2).?
Based on these constraints, we designed, im-plemented, and evaluated the REVERB extrac-tor, which substantially outperforms previousOpen IE systems in both recall and precision.?
We make REVERB and the data used in ourexperiments available to the research commu-nity.4In future work, we plan to explore utilizing ourconstraints to improve the performance of learnedCRF models.
Roth et al have shown how to incor-porate constraints into CRF learners (Roth and Yih,2005).
It is natural, then, to consider whether thecombination of heuristically labeled training exam-ples, CRF learning, and our constraints will resultin superior performance.
The error analysis in Sec-tion 5.2 also suggests natural directions for futurework.
For instance, since many of REVERB?s errorsare due to incorrect arguments, improved methodsfor argument extraction are in order.AcknowledgmentsWe would like to thank Mausam, Dan Weld, YoavArtzi, Luke Zettlemoyer, members of the KnowItAll4http://reverb.cs.washington.edugroup, and the anonymous reviewers for their help-ful comments.
This research was supported in partby NSF grant IIS-0803481, ONR grant N00014-08-1-0431, and DARPA contract FA8750-09-C-0179,and carried out at the University of Washington?sTuring Center.ReferencesDavid J. Allerton.
2002.
Stretched Verb Constructions inEnglish.
Routledge Studies in Germanic Linguistics.Routledge (Taylor and Francis), New York.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceedingsof the 17th international conference on Computationallinguistics, pages 86?90.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT, pages 28?36, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the web.
In In the Proceedingsof the 20th International Joint Conference on ArtificialIntelligence, pages 2670?2676, January.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of ACL, Portland, OR.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2010.
Semantic role labeling foropen information extraction.
In Proceedings of theNAACL HLT 2010 First International Workshop onFormalisms and Methodology for Learning by Read-ing, FAM-LbR ?10, pages 52?60, Stroudsburg, PA,USA.
Association for Computational Linguistics.Doug Downey, Oren Etzioni, and Stephen Soderland.2005.
A probabilistic model of redundancy in infor-mation extraction.
In IJCAI, pages 1034?1041.Gregory Grefenstette and Simone Teufel.
1995.
Corpus-based method for automatic identification of supportverbs for nominalizations.
In Proceedings of the sev-enth conference on European chapter of the Associa-tion for Computational Linguistics, pages 98?103, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.2010.
Learning 5000 relational extractors.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, ACL ?10, pages 286?295, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.1544Holmer Hemsen Kathrin Eichler and Gnter Neu-mann.
2008.
Unsupervised relation extractionfrom web documents.
In LREC.
http://www.lrec-conf.org/proceedings/lrec2008/.J.
Kim and D. Moldovan.
1993.
Acquisition of semanticpatterns for information extraction from corpora.
InProcs.
of Ninth IEEE Conference on Artificial Intelli-gence for Applications, pages 171?176.Dekang Lin and Patrick Pantel.
2001.
DIRT-Discoveryof Inference Rules from Text.
In Proceedings ofACM Conference on Knowledge Discovery and DataMining(KDD-01), pages pp.
323?328.Thomas Lin, Mausam, and Oren Etzioni.
2010.
Identify-ing Functional Relations in Web Text.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1266?1276, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Paul Kingsbury Martha and Martha Palmer.
2002.
Fromtreebank to propbank.
In In Proceedings of LREC-2002.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In ACL-IJCNLP ?09: Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume2, pages 1003?1011, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).E.
Riloff.
1996.
Automatically constructing extractionpatterns from untagged text.
In Procs.
of the Thir-teenth National Conference on Artificial Intelligence(AAAI-96), pages 1044?1049.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A La-tent Dirichlet Allocation Method for Selectional Pref-erences.
In ACL.Dan Roth and Wen-tau Yih.
2005.
Integer linear pro-gramming inference for conditional random fields.
InProceedings of the 22nd international conference onMachine learning, ICML ?05, pages 736?743, NewYork, NY, USA.
ACM.Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,and Jesse Davis.
2010.
Learning first-order hornclauses from web text.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?10, pages 1088?1098,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proceedings of the COLING/ACL on Mainconference poster sessions, pages 731?738, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive Information Extraction using Unrestricted Rela-tion Discovery.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, MainConference, pages 304?311, New York City, USA,June.
Association for Computational Linguistics.Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,Mausam, and Oren Etzioni.
2010.
Adapting open in-formation extraction to domain-specific relations.
AIMagazine, 31(3):93?102.S.
Soderland.
1999.
Learning Information ExtractionRules for Semi-Structured and Free Text.
MachineLearning, 34(1-3):233?272.Lucia Specia and Enrico Motta.
2006.
M.: A hybridapproach for extracting semantic relations from texts.In In.
Proceedings of the 2 nd Workshop on OntologyLearning and Population, pages 57?64.Suzanne Stevenson, Afsaneh Fazly, and Ryan North.2004.
Statistical measures of the semi-productivityof light verb constructions.
In 2nd ACL Workshop onMultiword Expressions, pages 1?8.M.
Stevenson.
2004.
An unsupervised WordNet-basedalgorithm for relation extraction.
In Proceedings ofthe ?Beyond Named Entity?
workshop at the FourthInternational Conference on Language Resources andEvalutaion (LREC?04).Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Fei Wu and Daniel S. Weld.
2010.
Open information ex-traction using Wikipedia.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 118?127, Morristown,NJ, USA.
Association for Computational Linguistics.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, andJi-Rong Wen.
2009.
StatSnowball: a statistical ap-proach to extracting entity relationships.
In WWW ?09:Proceedings of the 18th international conference onWorld wide web, pages 101?110, New York, NY, USA.ACM.1545
