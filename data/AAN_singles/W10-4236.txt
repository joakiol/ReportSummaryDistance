Helping Our Own:Text Massaging for Computational Linguistics as a New Shared TaskRobert DaleCentre for Language TechnologyMacquarie UniversitySydney, AustraliaRobert.Dale@mq.edu.auAdam KilgarriffLexical Computing LtdBrightonUnited Kingdomadam@lexmasterclass.comAbstractIn this paper, we propose a new shared taskcalled HOO: Helping Our Own.
The aim isto use tools and techniques developed in com-putational linguistics to help people writingabout computational linguistics.
We describea text-to-text generation scenario that poseschallenging research questions, and deliverspractical outcomes that are useful in the firstcase to our own community and potentiallymuch more widely.
Two specific factors makeus optimistic that this task will generate usefuloutcomes: one is the availability of the ACLAnthology, a large corpus of the target texttype; the other is that CL researchers who arenon-native speakers of English will be moti-vated to use prototype systems, providing in-formed and precise feedback in large quantity.We lay out our plans in detail and invite com-ment and critique with the aim of improvingthe nature of the planned exercise.1 IntroductionA forbidding challenge for many scientists whosefirst language is not English is the writing of ac-ceptable English prose.
There is a concern?perhaps sometimes imagined, but real enough to bea worry?that papers submitted to conferences andjournals may be rejected because the use of languageis jarring and makes it harder for the reader to followwhat the author intended.
While this can be a prob-lem for native speakers as well, non-native speakerstypically face a greater obstacle.The Association for Computational Linguistics?mentoring service is one part of a response.1 A men-toring service can address a wider range of problemsthan those related purely to writing; but a key moti-vation behind such services is that an author?s mate-rial should be judged on its research content, not onthe author?s skills in English.This problem will surface in any discipline whereauthors are required to provide material in a lan-guage other than their mother tongue.
However, asa discipline, computational linguistics holds a priv-ileged position: as scientists, language (of differentvarieties) is our object of study, and as technologists,language tasks form our agenda.
Many of the re-search problems we focus on could assist with writ-ing problems.
There is already existing work thataddresses specific problems in this area (see, for ex-ample, (Tetreault and Chodorow, 2008)), but to begenuinely useful, we require a solution to the writingproblem as a whole, integrating existing solutions tosub-problems with new solutions for problems as yetunexplored.Our proposal, then, is to initiate a shared task thatattempts to tackle the problem head-on; we want to?help our own?
by developing tools which can helpnon-native speakers of English (NNSs) (and maybesome native ones) write academic English prose ofthe kind that helps a paper get accepted.The kinds of assistance we are concernedwith here go beyond that which is provided bycommonly-available spelling checkers and grammarcheckers such as those found in Microsoft Word(Heidorn, 2000).
The task can be simply expressedas a text-to-text generation exercise:1See http://acl2010.org/mentoring.htm.Given a text, make edits to the text to im-prove the quality of the English it con-tains.This simple characterisation masks a number ofquestions that must be answered in order to fullyspecify a task.
We turn to these questions in Sec-tion 3, after first elaborating on why we think thistask is likely to deliver useful results.2 Why This Will Work2.1 Potential UsersWe believe this initiative has a strong chance of suc-ceeding simply because there will be an abundanceof committed, serious and well-informed users togive feedback on proposed solutions.
A famil-iar problem for technological developments in aca-demic research is that of capturing the time and in-terest of potential users of the technology, to obtainfeedback about what works in a real world task set-ting, with an appropriate level of engagement.It is very important to NNS researchers that theirpapers are not rejected because the English is notgood or clear enough.
They expect to invest largeamounts of time in honing the linguistic aspects oftheir papers.
One of us vividly recalls an explana-tion by a researcher that, prior to submitting a pa-per, he took his draft and submitted each sentencein turn, in quotation marks (to force exact matchesonly), to Google.
If there were no Google hits, itwas unlikely that the sentence was satisfactory En-glish and it needed reworking; if there were hits, thehits needed checking to ascertain whether they ap-peared to be written by another non-native speaker.2To give that researcher a tool that improves on thissituation should not be too great a challenge.For HOO, we envisage that the researchers them-selves, as well as their colleagues, will want to usethe prototype systems when preparing their confer-ence and journal submissions.
They will have theskills and motivation to integrate the use of proto-types into their paper-writing.2See the Microsoft ESL Assistant athttp://www.eslassistant.com as an embodiment ofa similar idea.2.2 The ACL AnthologyOver a number of years, the ACL has sponsoredthe ongoing development of the ACL Anthology, alarge collection of papers in the domain of computa-tional linguistics.
This provides an excellent sourcefor the construction of language models for the taskdescribed here.
The more recently-prepared ACLAnthology Reference Corpus (Bird et al, 2008), inwhich 10,921 of the Anthology texts (around 40 mil-lion words) have been made available in plain textform, has also been made accessible via the SketchEngine, a leading corpus query tool.3The corpus is not perfect, of course: not every-thing in the ACL Anthology is written in flawlessEnglish; the ARC was prepared in 2007, so new top-ics, vocabulary and ideas in CL will not be repre-sented; and the fact that the texts have been auto-matically extracted from PDF files means that thereare errors from the conversion process.3 The Task in More Detail3.1 How Do We Measure Quality?To be able to evaluate the performance of systemswhich attempt to improve the quality of a text,we require some means of measuring text quality.One approach would be to develop measures, ormake use of existing measures, of characteristicsof text quality such as well-formedness and read-ability (see, for example, (Dale and Chall, 1948;Flesch, 1948; McLaughlin, 1969; Coleman andLiau, 1975)).
Given a text and a version of that textthat had been subjected to rewriting, we could thencompare both texts using these metrics.
However,there is always a concern that the metrics may not re-ally measure what they are intended to measure (see,for example, (Le Vie Jr, 2000)); readability metricshave often been criticised for not being good mea-sures of actual readability.
The measures also tendto be aggregate measures (for example, providing anaverage readability level across an entire text), whenthe kinds of changes that we are interested in evalu-ating are often very local in nature.Given these concerns, we opt for a different route:for the initial pilot run of the proposed task, we in-tend to provide a set of development data consisting3See http://sketchengine.co.uk/open.of 10 conference papers in two versions: an originalversion of the paper, and an improved version whereerrors in expression and language use have been cor-rected.
We envisage that participants will focus ondeveloping techniques that attempt to replicate thekinds of corrections found in the improved versionsof the papers.
For evaluation, we will provide a fur-ther ten papers in their original versions, and eachparticipant?s results will then be compared against aheld-back set of corrected versions for these papers.We would expect the evaluation to assess the follow-ing:?
Has the existence of each error annotated in themanually revised versions been correctly iden-tified??
Have the spans or extents of the errors been ac-curately identified??
Has the type of error, as marked in the annota-tions, been correctly identified??
How close is the automatically-produced cor-rection to the manually-produced correction??
What corrections are proposed that do not cor-respond to errors identified in the manually-corrected text?With respect to this last point: we anticipate lookingclosely at all such machine-proposed-errors, sincesome may indeed be legitimate.
Either the humanannotators may have missed them, or may not haveconsidered them significant enough to be marked.
Ifthere are many such cases, we will need to reviewhow we handle ?prima facie false positives?
in theevaluation metrics.Evaluation of the aspects described above canbe achieved automatically; there is also scope, ofcourse, for human evaluation of the overall relativequality of the system-generated texts, although thisis of course labour intensive.3.2 Where Does the Source Data Come From?We have two candidates which we aim to exploreas sources of data for the exercise.
It is almost cer-tain the first of these two options will yield mate-rial which is denser in errors, and closer to the kindsof source material that any practical application willhave to work with; however, the pragmatics of thesituation mean that we may have to fall back on oursecond option.First, we intend to approach the Mentoring Chairsfor the ACL conferences over the last few years withour proposal; then, with their permission, we ap-proach the authors of papers that were submitted formentoring.
If these authors are willing, we use theirinitial submissions to the mentoring process as theoriginal document set.If this approach yields an insufficient number ofpapers (it may be that some authors are not willingto have their drafts made available in this way, andit would not be possible to make them anonymous)then we will source candidate papers from the ACLAnthology.
The process we have in mind is this:?
Identify a paper whose authors are non-nativeEnglish speakers.?
If a quick reading of the paper reveals a mod-erately high density of correctable errors within the first page, that paper becomes a candi-date for the data set; if it contains very few cor-rectable errors, the paper is ruled as inappropri-ate.?
Repeat this process until we have a sufficientlylarge data set.We then contact the authors to determine whetherthey are happy for their papers to be used in this ex-ercise.
If they are not, the paper is dropped and thenext paper?s author is asked.3.3 Where do the Corrections Come From?For the initial pilot, two copy-editors (who may ormay not be the authors of this paper) hand-correctthe papers in both the development and evaluationdata sets.
For a full-size exercise there should bemore than two such annotators, just as there shouldbe more than ten papers in each of the developmentand evaluation sets, but our priority here is to test themodel before investing further in it.The copy-editors will then compare corrections,and discuss differences.
The possible cases are:1.
One annotator identifies a correction that theother does not.2.
Both annotators identify different correctionsfor the same input text fragment.We propose to deal with instances of the first type asfollows:?
The two annotators will confer to determinewhether one has simply made a mistake?asmany authors can testify, no proofreader willfind all the errors in a text.?
If agreement on the presence or absence of anerror cannot be reached, the instance will bedealt with as described below for cases of thesecond type, with absence of an error beingconsidered a ?null correction?.Instances of the second type will be handled as fol-lows:?
If both annotators agree that both alternativesare acceptable, then both alternatives will beprovided in the gold standard.?
If no agreement can be reached, then neitheralternative will be provided in the gold standard(which effectively means that a null correctionis recorded).Other strategies, such as using a third annotator asa tie-breaker, can be utilised if the task generates acritical mass of interest and volunteer labour.3.4 What Kinds of Corrections?Papers can go through very significant changes andrevisions during the course of their production: largeportions of the material can be added or removed,the macro-structure can be re-organised substan-tially, arguments can be refined or recast.
Ideally, awriting advisor might help with large-scale concernssuch as these; however, we aim to start at a muchsimpler level, focussing on what is sometimes re-ferred to as a ?light copy-edit?.
This involves a rangeof phenomena which can be considered sentence-internal:?
domain- and genre-specific spelling errors, in-cluding casing errors;?
dispreferred or suboptimal lexical choices;?
basic grammatical errors, including commonESL problems like incorrect preposition anddeterminer usage;?
reduction of syntactic complexity;?
stylistic infelicities which, while not grammati-cally incorrect, are unwieldy and impact on flu-ency and ease of reading.The above are all identifiable and correctable withinthe context of a single sentence; however, we also in-tend to correct inconsistencies across the documentas whole:?
consistency of appropriate tense usage;?
spelling and hyphenation instances where thereis no obvious correct answer, but a uniformityis required.We envisage that the process of marking up the gold-standard texts will allow us to develop more formalguidelines and taxonomic descriptions for use sub-sequent to the pilot exercise.
There are, of course,existing approaches to error markup that can pro-vide a starting point here, in particular the schemesused in the large-scale exercises in learner errorannotation undertaken at CECL, Louvain-la-Neuve(Dagneaux et al, 1996) and at Cambridge ESOL(Nicholls, 2003).3.5 How Should the Task be Approached?There are many ways in which the task could be ad-dressed; it is open to both rule-based and statisticalsolutions.
An obvious way to view the task is as amachine translation problem from poor English tobetter English; however, supervised machine learn-ing approaches may be ruled out by the absence ofan appropriately large training corpus, something wemay not see until the task has generated significantmomentum (or more volunteer annotators at an earlystage!
).There is clearly a wealth of existing research ongrammar and style checking that can be broughtto bear.
Although grammar and style checkinghas been in the commercial domain now for threedecades, the task may provide a framework for thefirst comparative test of many of these applications.Because the nature of errors is so diverse, thistask offers the opportunity to exercise a broad rangeof approaches to the problem, and also allows fornarrowly-focussed solutions that attempt to addressspecific problems with high accuracy.4 Some Potential ProblemsOur proposal is not without possible problems anddetrimental side effects.Clearly there are ethical issues that need to beconsidered carefully; even if an author is happy fortheir data to be used in this way, one might find ret-rospective embarrassment at eponynmous error de-scriptions entering the common vocabulary in thefield?it?s one thing to be acknowledged for Kneser-Ney smoothing, but perhaps less appealing to be fa-mous for the Dale-Kilgarriff adjunct error.Our suggestion that the ACL Anthology might beused as a source for language modelling brings itsown downsides: in particular, if anything is likelyto increase the oft-complained-about sameness ofCL papers, this will!
There is also an ethical is-sue around the fine line between what such systemswill do and plagiarism; one might foresee the adventof a new scholastic crime labelled ?machine-assistedstyle plagiarism?.There are no doubt other issues we have not yetconsidered; again, feedback on potential pitfalls iseagerly sought.5 Next StepsOur aim is to obtain feedback on this proposal fromconference participants and others, with the aim ofrefining our plan in the coming months.
If we sensethat there is a reasonable degree of interest in thetask, we would aim to publish the initial data set wellbefore the end of the year, with a first evaluation tak-ing place in 2011.In the name of better writing, CLers of the worldunite?you have nothing to lose but your worst sen-tences!AcknowledgementsWe thank the two anonymous reviewers for usefulfeedback on this proposal, and Anja Belz for encour-aging us to develop the idea.ReferencesSteven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,Mark Joseph, Min-Yen Kan, Dongwon Lee, BrettPowley, Dragomir Radev, and Yee Fan Tan.
2008.
Theacl anthology reference corpus: A reference dataset forbibliographic research in computational linguistics.
InProceedings of the Language Resources and Evalua-tion Conference (LREC 2008), location = Marrakesh,Morocco.Meri Coleman and T. L. Liau.
1975.
A computer read-ability formula designed for machine scoring.
Journalof Applied Psychology, 60:283?284.E Dagneaux, S Denness, S Granger, and F Meunier.1996.
Error tagging manual version 1.1.
Technicalreport, Centre for English Corpus Linguistics, Univer-site?
Catholique de Louvain.Edgar Dale and Jeanne S. Chall.
1948.
A formula forpredicting readability.
Educational research bulletin,27:11?20.Rudolph Flesch.
1948.
A new readability yardstick.Journal of Applied Psychology, 32:221?233.George Heidorn.
2000.
Intelligent writing assistance.
InR Dale, H Moisl, and H Somers, editors, Handbook ofNatural Language Processing, pages 181?207.
MarcelDekker Inc.Donald S. Le Vie Jr. 2000.
Documentation metrics:What do you really want to measure?
Intercom.G.
Harry McLaughlin.
1969.
SMOG grading ?
a newreadability formula.
Journal of Reading, pages 639?646.D Nicholls.
2003.
The cambridge learner corpus: errorcoding and analysis for lexicography and ELT.
In Pro-ceedings of the Corpus Linguistics 2003 Conference(CL 2003), page 572.J R Tetreault and M S Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of the 22nd International Conferenceon Computational Linguistics.
