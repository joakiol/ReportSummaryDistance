Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 387?391,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsIIITH: Domain Specific Word Sense DisambiguationSiva ReddyIIIT HyderabadIndiagvsreddy@students.iiit.ac.inDiana McCarthyLexical Computing Ltd.United Kingdomdiana@dianamccarthy.co.ukAbhilash InumellaIIIT HyderabadIndiaabhilashi@students.iiit.ac.inMark StevensonUniversity of SheffieldUnited Kingdomm.stevenson@dcs.shef.ac.ukAbstractWe describe two systems that participatedin SemEval-2010 task 17 (All-words WordSense Disambiguation on a Specific Do-main) and were ranked in the third andfourth positions in the formal evaluation.Domain adaptation techniques using thebackground documents released in thetask were used to assign ranking scores tothe words and their senses.
The test datawas disambiguated using the PersonalizedPageRank algorithm which was appliedto a graph constructed from the whole ofWordNet in which nodes are initializedwith ranking scores of words and theirsenses.
In the competition, our systemsachieved comparable accuracy of 53.4 and52.2, which outperforms the most frequentsense baseline (50.5).1 IntroductionThe senses in WordNet are ordered according totheir frequency in a manually tagged corpus, Sem-Cor (Miller et al, 1993).
Senses that do not oc-cur in SemCor are ordered arbitrarily after thosesenses of the word that have occurred.
It is knownfrom the results of SENSEVAL2 (Cotton et al,2001) and SENSEVAL3 (Mihalcea and Edmonds,2004) that first sense heuristic outperforms manyWSD systems (see McCarthy et al (2007)).
Thefirst sense baseline?s strong performance is due tothe skewed frequency distribution of word senses.WordNet sense distributions based on SemCor areclearly useful, however in a given domain thesedistributions may not hold true.
For example, thefirst sense for ?bank?
in WordNet refers to ?slop-ing land beside a body of river?
and the secondto ?financial institution?, but in the domain of ?fi-nance?
the ?financial institution?
sense would beexpected to be more likely than the ?sloping landbeside a body of river?
sense.
Unfortunately, itis not feasible to produce large manually sense-annotated corpora for every domain of interest.McCarthy et al (2004) propose a method to pre-dict sense distributions from raw corpora and usethis as a first sense heuristic for tagging text withthe predominant sense.
Rather than assigning pre-dominant sense in every case, our approach aimsto use these sense distributions collected from do-main specific corpora as a knowledge source andcombine this with information from the context.Our approach focuses on the strong influence ofdomain for WSD (Buitelaar et al, 2006) and thebenefits of focusing on words salient to the do-main (Koeling et al, 2005).
Words are assigneda ranking score based on its keyness (salience) inthe given domain.
We use these word scores asanother knowledge source.Graph based methods have been shown toproduce state-of-the-art performance for unsu-pervised word sense disambiguation (Agirre andSoroa, 2009; Sinha and Mihalcea, 2007).
Theseapproaches use well-known graph-based tech-niques to find and exploit the structural propertiesof the graph underlying a particular lexical knowl-edge base (LKB), such as WordNet.
These graph-based algorithms are appealing because they takeinto account information drawn from the entiregraph as well as from the given context, makingthem superior to other approaches that rely onlyon local information individually derived for eachword.Our approach uses the Personalized PageRankalgorithm (Agirre and Soroa, 2009) over a graph387representing WordNet to disambiguate ambigu-ous words by taking their context into consider-ation.
We also combine domain-specific informa-tion from the knowledge sources, like sense distri-bution scores and keyword ranking scores, into thegraph thus personalizing the graph for the givendomain.In section 2, we describe domain sense ranking.Domain keyword ranking is described in Section3.
Graph construction and personalized page rankare described in Section 4.
Evaluation results overthe SemEval data are provided in Section 5.2 Domain Sense RankingMcCarthy et al (2004) propose a method forfinding predominant senses from raw text.
Themethod uses a thesaurus acquired from automat-ically parsed text based on the method describedby Lin (1998).
This provides the top k nearestneighbours for each target word w, along with thedistributional similarity score between the targetword and each neighbour.
The senses of a wordw are each assigned a score by summing over thedistributional similarity scores of its neighbours.These are weighted by a semantic similarity score(using WordNet Similarity score (Pedersen et al,2004) between the sense of w and the sense of theneighbour that maximizes the semantic similarityscore.More formally, let Nw= {n1, n2, .
.
.
nk}be the ordered set of the top k scoringneighbours of w from the thesaurus withassociated distributional similarity scores{dss(w, n1), dss(w, n2), .
.
.
dss(w, nk)}.
Letsenses(w) be the set of senses of w. For eachsense of w (wsi?
senses(w)) a ranking score isobtained by summing over the dss(w, nj) of eachneighbour (nj?
Nw) multiplied by a weight.This weight is the WordNet similarity score(wnss) between the target sense (wsi) and thesense of nj(nsx?
senses(nj)) that maximizesthis score, divided by the sum of all such WordNetsimilarity scores for senses(w) and nj.
Eachsense wsi?
senses(w) is given a sense rankingscore srs(wsi) usingsrs(wsi) =?njNwdss(w, nj)?wnss(wsi, nj)?wsisenses(w)wnss(wsi, nj)where wnss(wsi, nj) =maxnsx?senses(nj)(wnss(wsi, nsx))Since this approach requires only raw text,sense rankings for a particular domain can be gen-erated by simply training the algorithm using acorpus representing that domain.
We used thebackground documents provided to the partici-pants in this task as a domain specific corpus.
Ingeneral, a domain specific corpus can be obtainedusing domain-specific keywords (Kilgarriff et al,2010).
A thesaurus is acquired from automaticallyparsed background documents using the StanfordParser (Klein and Manning, 2003).
We used k = 5to built the thesaurus.
As we increased k we foundthe number of non-domain specific words occur-ring in the thesaurus increased and negatively af-fected the sense distributions.
To counter this, oneof our systems IIITH2 used a slightly modifiedranking score by multiplying the effect of eachneighbour with its domain keyword ranking score.The modified sense ranking msrs(wsj) score ofsense wsiismsrs(wsi) =?njNwdss(w, nj)?wnss(wsi, nj)?wsisenses(w)wnss(wsi, nj)?krs(nj)where krs(nj) is the keyword ranking score ofthe neighbour njin the domain specific corpus.
Inthe next section we describe the way in which wecompute krs(nj).WordNet::Similarity::lesk (Pedersen et al,2004) was used to compute word similarity wnss.IIITH1 and IIITH2 systems differ in the waysenses are ranked.
IIITH1 uses srs(wsj) whereasIIITH2 system uses msrs(wsj) for computingsense ranking scores in the given domain.3 Domain Keyword RankingWe extracted keywords in the domain by compar-ing the frequency lists of domain corpora (back-ground documents) and a very large general cor-pus, ukWaC (Ferraresi et al, 2008), using themethod described by Rayson and Garside (2000).For each word in the frequency list of the domaincorpora, words(domain), we calculated the log-likelihood (LL) statistic as described in Raysonand Garside (2000).
We then normalized LL tocompute keyword ranking score krs(w) of wordw words(domain) using388krs(w) =LL(w)?wi?words(domain)LL(wi)The above score represents the keyness of theword in the given domain.
Top ten keywords (indescending order of krs) in the corpora providedfor this task are species, biodiversity, life, habitat,natura1, EU, forest, conservation, years, amp2.4 Personalized PageRankOur approach uses the Personalized PageRank al-gorithm (Agirre and Soroa, 2009) with WordNetas the lexical knowledge base (LKB) to performWSD.
WordNet is converted to a graph by repre-senting each synset as a node (synset node) and therelationships in WordNet (hypernymy, hyponymyetc.)
as edges between synset nodes.
The graph isinitialized by adding a node (word node) for eachcontext word of the target word (including itself)thus creating a context dependent graph (person-alized graph).
The popular PageRank (Page et al,1999) algorithm is employed to analyze this per-sonalized graph (thus the algorithm is referred aspersonalized PageRank algorithm) and the sensefor each disambiguous word is chosen by choos-ing the synset node which gets the highest weightafter a certain number of iterations of PageRankalgorithm.We capture domain information in the personal-ized graph by using sense ranking scores and key-word ranking scores of the domain to assign initialweights to the word nodes and their edges (word-synset edge).
This way we personalize the graphfor the given domain.4.1 Graph Initialization MethodsWe experimented with different ways of initial-izing the graph, described below, which are de-signed to capture domain specific information.Personalized Page rank (PPR): In this method,the graph is initialized by allocating equal prob-ability mass to all the word nodes in the contextincluding the target word itself, thus making thegraph context sensitive.
This does not include do-main specific information.1In background documents this word occurs in reports de-scribing Natura 2000 networking programme.2This new word ?amp?
is created by our programs whileextracting body text from background documents.
TheHTML code ?&amp;?
which represents the symbol?&?
isconverted into this word.Keyword Ranking scores with PPR (KRS +PPR): This is same as PPR except that contextwords are initialized with krs.Sense Ranking scores with PPR (SRS + PPR):Edges connecting words and their synsets are as-signed weights equal to srs.
The initialization ofword nodes is same as in PPR.KRS + SRS + PPR: Word nodes are initializedwith krs and edges are assigned weights equal tosrs.In addition to the above methods of unsuper-vised graph initialization, we also initialized thegraph in a semi-supervised manner.
WordNet (ver-sion 1.7 and above) have a field tag cnt for eachsynset (in the file index.sense) which representsthe number of times the synset is tagged in vari-ous semantic concordance texts.
We used this in-formation, concordance score (cs) of each synset,with the above methods of graph initialization asdescribed below.Concordance scores with PPR (CS + PPR): Thegraph initialization is similar to PPR initializationadditionally with concordance score of synsets onthe edges joining words and their synsets.CS + KRS + PPR: The initialization graph ofKRS + PPR is further initialized by assigning con-cordance scores to the edges connecting words andtheir synsets.CS + SRS + PPR: Edges connecting words andtheir synsets are assigned weights equal to sum ofthe concordance scores and sense ranking scoresi.e.
cs + srs.
The initialization of word nodes issame as in PPR.CS + KRS + SRS + PPR: Word nodes are ini-tialized with krs and edges are assigned weightsequal to cs + srs.PageRank was applied to all the above graphs todisambiguate a target word.4.2 Experimental details of PageRankTool: We used UKB tool3(Agirre and Soroa,2009) which provides an implementation of per-sonalized PageRank.
We modified it to incorpo-rate our methods of graph initialization.
The LKBused in our experiments is WordNet3.0 + Glosswhich is provided in the tool.
More details of thetools used can be found in the Appendix.Normalizations: Sense ranking scores (srs) andkeyword ranking scores (krs) have diverse ranges.We found srs generally in the range between 0 to3http://ixa2.si.ehu.es/ukb/389Precision RecallUnsupervised Graph InitializationPPR 37.3 36.8KRS + PPR 38.1 37.6SRS + PPR 48.4 47.8KRS + SRS + PPR 48.0 47.4Semi-supervised Graph InitializationCS + PPR 50.2 49.6CS + KRS + PPR 50.1 49.5* CS + SRS + PPR 53.4 52.8CS + KRS + SRS + PPR 53.6 52.9Others1stsense 50.5 50.5PSH 49.8 43.2Table 1: Evaluation results on English test data of SemEval-2010 Task-17.
* represents the system whichwe submitted to SemEval and is ranked 3rd in public evaluation.1 and krs in the range 0 to 0.02.
Since these scoresare used to assign initial weights in the graph,these ranges are scaled to fall in a common rangeof [0, 100].
Using any other scaling method shouldnot effect the performance much since PageRank(and UKB tool) has its own internal mechanismsto normalize the weights.5 Evaluation ResultsTest data released for this task is disambiguatedusing IIITH1 and IIITH2 systems.
As describedin Section 2, IIITH1 and IIITH2 systems differ inthe way the sense ranking scores are computed.Here we project only the results of IIITH1 sinceIIITH1 performed slightly better than IIITH2 in allthe above settings.
Results of 1stsense system pro-vided by the organizers which assigns first sensecomputed from the annotations in hand-labeledcorpora is also presented.
Additionally, we alsopresent the results of Predominant Sense Heuristic(PSH) which assigns every word w with the sensewsj(wsj?
senses(w)) which has the highestvalue of srs(wsj) computed in Section 2 similarto (McCarthy et al, 2004).Table 1 presents the evaluation results.
We usedTreeTagger4to Part of Speech tag the test data.POS information was used to discard irrelevantsenses.
Due to POS tagging errors, our precisionvalues were not equal to recall values.
In the com-petition, we submitted IIITH1 and IIITH2 systemswith CS + SRS + PPR graph initialization.
IIITH14http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/and IIIH2 gave performances of 53.4 % and 52.2% precision respectively.
In our later experiments,we found CS + KRS + SRS + PPR has given thebest performance of 53.6 % precision.From the results, it can be seen when srs in-formation is incorporated in the graph, precisionimproved by 11.1% compared to PPR in unsuper-vised graph initialization and by 3.19% comparedto CS + PPR in semi-supervised graph initializa-tion.
Also little improvements are seen when krsinformation is added.
This shows that domainspecific information like sense ranking scores andkeyword ranking scores play a major role in do-main specific WSD.The difference between the results in unsu-pervised and semi-supervised graph initializationsmay be attributed to the additional information thesemi-supervised graph is having i.e.
the sense dis-tribution knowledge of non-domain specific words(common words).6 ConclusionThis paper proposes a method for domain specificWSD.
Our method is based on a graph-based al-gorithm (Personalized Page Rank) which is mod-ified to include information representing the do-main (sense ranking and key word ranking scores).Experiments show that exploiting this domain spe-cific information within the graph based methodsproduces better results than when this informationis used individually.390AcknowledgementsThe authors are grateful to Ted Pedersen for hishelpful advice on the WordNet Similarity Pack-age.
We also thank Rajeev Sangal for supportingthe authors Siva Reddy and Abhilash Inumella.ReferencesEneko Agirre and Aitor Soroa.
2009.
Personaliz-ing pagerank for word sense disambiguation.
InEACL ?09: Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 33?41, Morristown, NJ,USA.
Association for Computational Linguistics.Paul Buitelaar, Bernardo Magnini, Carlo Strapparava,and Piek Vossen.
2006.
Domain-specific wsd.
InWord Sense Disambiguation.
Algorithms and Appli-cations, Editors: Eneko Agirre and Philip Edmonds.Springer.Scott Cotton, Phil Edmonds, Adam Kilgarriff, andMartha Palmer.
2001.
Senseval-2.
http://www.sle.sharp.co.uk/senseval2.A.
Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-dini.
2008.
Introducing and evaluating ukwac,a very large web-derived corpus of english.
InProceed-ings of the WAC4 Workshop at LREC 2008,Marrakesh, Morocco.Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and Avi-nesh PVS.
2010.
A corpus factory for many lan-guages.
In LREC 2010, Malta.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL ?03: Proceedingsof the 41st Annual Meeting on Association for Com-putational Linguistics, pages 423?430, Morristown,NJ, USA.
Association for Computational Linguis-tics.Rob Koeling, Diana McCarthy, and John Carroll.2005.
Domain-specific sense distributions and pre-dominant sense acquisition.
In HLT ?05: Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 419?426, Morristown, NJ, USA.Association for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of the 17thinternational conference on Computational linguis-tics, pages 768?774, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In ACL ?04: Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics, page 279, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2007.
Unsupervised acquisition of pre-dominant word senses.
Computational Linguistics,33(4):553?590.Rada Mihalcea and Phil Edmonds, editors.
2004.Proceedings Senseval-3 3rd International Workshopon Evaluating Word Sense Disambiguation Systems.ACL, Barcelona, Spain.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of the ARPA Workshop on Human Lan-guage Technology, pages 303?308.
Morgan Kauf-man.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report1999-66, Stanford InfoLab, November.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity: measuring the re-latedness of concepts.
In HLT-NAACL ?04: Demon-stration Papers at HLT-NAACL 2004 on XX, pages38?41, Morristown, NJ, USA.
Association for Com-putational Linguistics.Paul Rayson and Roger Garside.
2000.
Comparingcorpora using frequency profiling.
In Proceedingsof the workshop on Comparing corpora, pages 1?6, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-basedword sense disambiguation using mea-sures of word semantic similarity.
In ICSC ?07: Pro-ceedings of the International Conference on Seman-tic Computing, pages 363?369, Washington, DC,USA.
IEEE Computer Society.AppendixDomain Specific Thesaurus, Sense RankingScores and Keyword Ranking Scores are accessi-ble athttp://web.iiit.ac.in/?gvsreddy/SemEval2010/Tools Used:?
UKB is used with options ?ppr ?dict weight.
Dictio-nary files which UKB uses are automatically generatedusing sense ranking scores srs.?
Background document words are canonicalized usingKSTEM, a morphological analyzer?
The Stanford Parser is used to parse background docu-ments to build thesaurus?
Test data is part of speech tagged using TreeTagger.391
