Proceedings of the ACL 2010 Conference Short Papers, pages 307?312,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDecision detection using hierarchical graphical modelsTrung H. BuiCSLIStanford UniversityStanford, CA 94305, USAthbui@stanford.eduStanley PetersCSLIStanford UniversityStanford, CA 94305, USApeters@csli.stanford.eduAbstractWe investigate hierarchical graphicalmodels (HGMs) for automatically detect-ing decisions in multi-party discussions.Several types of dialogue act (DA) aredistinguished on the basis of their roles informulating decisions.
HGMs enable usto model dependencies between observedfeatures of discussions, decision DAs, andsubdialogues that result in a decision.
Forthe task of detecting decision regions, anHGM classifier was found to outperformnon-hierarchical graphical models andsupport vector machines, raising theF1-score to 0.80 from 0.55.1 IntroductionIn work environments, people share informationand make decisions in multi-party conversationsknown as meetings.
The demand for systems thatcan automatically process information containedin audio and video recordings of meetings is grow-ing rapidly.
Our own research, and that of othercontemporary projects (Janin et al, 2004) aim atmeeting this demand.We are currently investigating the automatic de-tection of decision discussions.
Our approach in-volves distinguishing between different dialogueact (DA) types based on their role in the decision-making process.
These DA types are called De-cision Dialogue Acts (DDAs).
Groups of DDAscombine to form a decision region.Recent work (Bui et al, 2009) showed thatDirected Graphical Models (DGMs) outperformother machine learning techniques such as Sup-port Vector Machines (SVMs) for detecting in-dividual DDAs.
However, the proposed mod-els, which were non-hierarchical, did not signifi-cantly improve identification of decision regions.This paper tests whether giving DGMs hierarchi-cal structure (making them HGMs) can improvetheir performance at this task compared with non-hierarchical DGMs.We proceed as follows.
Section 2 discusses re-lated work, and section 3 our data set and anno-tation scheme for decision discussions.
Section4 summarizes previous decision detection exper-iments using DGMs.
Section 5 presents the HGMapproach, and section 6 describes our HGM exper-iments.
Finally, section 7 draws conclusions andpresents ideas for future work.2 Related workUser studies (Banerjee et al, 2005) have con-firmed that meeting participants consider deci-sions to be one of the most important meetingoutputs, and Whittaker et al (2006) found thatthe development of an automatic decision de-tection component is critical for re-using meet-ing archives.
With the new availability of sub-stantial meeting corpora such as the AMI cor-pus (McCowan et al, 2005), recent years haveseen an increasing amount of research on decision-making dialogue.
This research has tackled is-sues such as the automatic detection of agreementand disagreement (Galley et al, 2004), and ofthe level of involvement of conversational partic-ipants (Gatica-Perez et al, 2005).
Recent workon automatic detection of decisions has been con-ducted by Hsueh and Moore (2007), Ferna?ndez etal.
(2008), and Bui et al (2009).Ferna?ndez et al (2008) proposed an approachto modeling the structure of decision-making di-alogue.
These authors designed an annotationscheme that takes account of the different rolesthat utterances can play in the decision-makingprocess?for example it distinguishes betweenDDAs that initiate a decision discussion by rais-ing an issue, those that propose a resolution of theissue, and those that express agreement to a pro-posed resolution.
The authors annotated a por-tion of the AMI corpus, and then applied what307they refer to as ?hierarchical classification.?
Here,one sub-classifier per DDA class hypothesizes oc-currences of that type of DDA and then, basedon these hypotheses, a super-classifier determineswhich regions of dialogue are decision discus-sions.
All of the classifiers, (sub and super), werelinear kernel binary SVMs.
Results were bet-ter than those obtained with (Hsueh and Moore,2007)?s approach?the F1-score for detecting de-cision discussions in manual transcripts was 0.58vs.
0.50.
Purver et al (2007) had earlier detectedaction items with the approach Ferna?ndez et al(2008) extended to decisions.Bui et al (2009) built on the promising resultsof (Ferna?ndez et al, 2008), by employing DGMsin place of SVMs.
DGMs are attractive becausethey provide a natural framework for modeling se-quence and dependencies between variables, in-cluding the DDAs.
Bui et al (2009) were espe-cially interested in whether DGMs better exploitnon-lexical features.
Ferna?ndez et al (2008) ob-tained much more value from lexical than non-lexical features (and indeed no value at all fromprosodic features), but lexical features have limi-tations.
In particular, they can be domain specific,increase the size of the feature space dramatically,and deteriorate more in quality than other featureswhen automatic speech recognition (ASR) is poor.More detail about decision detection using DGMswill be presented in section 4.Beyond decision detection, DGMs are used forlabeling and segmenting sequences of observa-tions in many different fields?including bioin-formatics, ASR, Natural Language Processing(NLP), and information extraction.
In particular,Dynamic Bayesian Networks (DBNs) are a pop-ular model for probabilistic sequence modelingbecause they exploit structure in the problem tocompactly represent distributions over multi-stateand observation variables.
Hidden Markov Mod-els (HMMs), a special case of DBNs, are a classi-cal method for important NLP applications suchas unsupervised part-of-speech tagging (Gael etal., 2009) and grammar induction (Johnson et al,2007) as well as for ASR.
More complex DBNshave been used for applications such as DA recog-nition (Crook et al, 2009) and activity recogni-tion (Bui et al, 2002).Undirected graphical models (UGMs) are alsovaluable for building probabilistic models for seg-menting and labeling sequence data.
ConditionalRandom Fields (CRFs), a simple UGM case, canavoid the label bias problem (Lafferty et al, 2001)and outperform maximum entropy Markov mod-els and HMMs.However, the graphical models used in theseapplications are mainly non-hierarchical, includ-ing those in Bui et al (2009).
Only Sutton et al(2007) proposed a three-level HGM (in the form ofa dynamic CRF) for the joint noun phrase chunk-ing and part of speech labeling problem; theyshowed that this model performs better than a non-hierarchical counterpart.3 DataFor the experiments reported in this study, weused 17 meetings from the AMI Meeting Corpus1,a freely available corpus of multi-party meetingswith both audio and video recordings, and a widerange of annotated information including DAs andtopic segmentation.
The meetings last around 30minutes each, and are scenario-driven, whereinfour participants play different roles in a com-pany?s design team: project manager, marketingexpert, interface designer and industrial designer.We use the same annotation scheme asFerna?ndez et al (2008) to model decision-makingdialogue.
As stated in section 2, this scheme dis-tinguishes between a small number of DA typesbased on the role which they perform in the for-mulation of a decision.
Besides improving the de-tection of decision discussions (Ferna?ndez et al,2008), such a scheme also aids in summarizationof them, because it indicates which utterances pro-vide particular types of information.The annotation scheme is based on the observa-tion that a decision discussion typically containsthe following main structural components: (a) Atopic or issue requiring resolution is raised; (b)One or more possible resolutions are considered;(c) A particular resolution is agreed upon, and soadopted as the decision.
Hence the scheme dis-tinguishes between three main DDA classes: issue(I), resolution (R), and agreement (A).
Class R isfurther subdivided into resolution proposal (RP)and resolution restatement (RR).
I utterances in-troduce the topic of the decision discussion, ex-amples being ?Are we going to have a backup?
?and ?But would a backup really be necessary??
inTable 1.
In comparison, R utterances specify theresolution which is ultimately adopted as the deci-1http://corpus.amiproject.org/308(1) A: Are we going to have a backup?
Or we dojust?B: But would a backup really be necessary?A: I think maybe we could just go for thekinetic energy and be bold and innovative.C: Yeah.B: I think?
yeah.A: It could even be one of our selling points.C: Yeah ?laugh?.D: Environmentally conscious or something.A: Yeah.B: Okay, fully kinetic energy.D: Good.Table 1: An excerpt from the AMI dialogueES2015c.
It has been modified slightly for pre-sentation purposes.sion.
RP utterances propose this resolution (e.g.
?Ithink maybe we could just go for the kinetic energy.
.
.
?
), while RR utterances close the discussion byconfirming/summarizing the decision (e.g.
?Okay,fully kinetic energy?).
Finally, A utterances agreewith the proposed resolution, signaling that it isadopted as the decision, (e.g.
?Yeah?, ?Good?
and?Okay?).
Unsurprisingly, an utterance may be as-signed to more than one DDA class; and within adecision discussion, more than one utterance canbe assigned to the same DDA class.We use manual transcripts in the experimentsdescribed here.
Inter-annotator agreement was sat-isfactory, with kappa values ranging from .63 to.73 for the four DDA classes.
The manual tran-scripts contain a total of 15,680 utterances, and onaverage 40 DDAs per meeting.
DDAs are sparsein the transcripts: for all DDAs, 6.7% of the total-ity of utterances; for I,1.6%; for RP, 2%; for RR,0.5%; and for A, 2.6%.
In all, 3753 utterances (i.e.,23.9%) are tagged as decision-related utterances,and on average there are 221 decision-related ut-terances per meeting.4 Prior Work on Decision Detectionusing Graphical ModelsTo detect each individual DDA class, Bui et al(2009) examined the four simple DGMs shownin Fig.
1.
The DDA node is binary valued, withvalue 1 indicating the presence of a DDA and 0its absence.
The evidence node (E) is a multi-dimensional vector of observed values of non-lexical features.
These include utterance features(UTT) such as length in words2, duration in mil-liseconds, position within the meeting (as percent-age of elapsed time), manually annotated dialogueact (DA) features3 such as inform, assess, suggest,and prosodic features (PROS) such as energy andpitch.
These features are the same as the non-lexical features used by Ferna?ndez et al (2008).The hidden component node (C) in the -mix mod-els represents the distribution of observable evi-dence E as a mixture of Gaussian distributions.The number of Gaussian components was hand-tuned during the training phase.DDAEa) BN-simDDAEb) BN-mixCDDAtime t-1 time tEDDAEc) DBN-simDDAtime t-1 time tEDDAEd) DBN-mixCCFigure 1: Simple DGMs for individual decisiondialogue act detection.
The clear nodes are hidden,and the shaded nodes are observable.More complex models were constructed fromthe four simple models in Fig.
1 to allow for de-pendencies between different DDAs.
For exam-ple, the model in Fig.
2 generalizes Fig.
1c witharcs connecting the DDA classes based on analy-sis of the annotated AMI data.Atime t-1 time tE EI RP RR AI RP RRFigure 2: A DGM that takes the dependencies be-tween decision dialogue acts into account.Decision discussion regions were identified us-ing the DGM output and the following two simplerules: (1) A decision discussion region begins withan Issue DDA; (2) A decision discussion regioncontains at least one Issue DDA and one Resolu-tion DDA.2This feature is a manual count of lexical tokens; but wordcount was extracted automatically from ASR output by Buiet al (2009).
We plan experiments to determine how muchusing ASR output degrades detection of decision regions.3The authors used the AMI DA annotations.309The authors conducted experiments using theAMI corpus and found that when using non-lexical features, the DGMs outperform the hierar-chical SVM classification method of (Ferna?ndez etal., 2008).
The F1-score for the four DDA classesincreased between 0.04 and 0.19 (p < 0.005),and for identifying decision discussion regions, by0.05 (p > 0.05).5 Hierarchical graphical modelsAlthough the results just discussed showed graph-ical models are better than SVMs for detecting de-cision dialogue acts (Bui et al, 2009), two-levelgraphical models like those shown in Figs.
1 and 2cannot exploit dependencies between high-leveldiscourse items such as decision discussions andDDAs; and the ?superclassifier?
rule (Bui et al,2009) used for detecting decision regions did notsignificantly improve the F1-score for decisions.We thus investigate whether HGMs (structuredas three or more levels) are superior for discov-ering the structure and learning the parametersof decision recognition.
Our approach composesgraphical models to increase hierarchy with an ad-ditional level above or below previous ones, or in-serts a new level such as for discourse topics intothe interior of a given model.Fig.
3 shows a simple structure for three-levelHGMs.
The top level corresponds to high-leveldiscourse regions such as decision discussions.The segmentation into these regions is representedin terms of a random variable (at each DR node)that takes on discrete values: {positive, negative}(the utterance belongs to a decision region or not)or {begin, middle, end, outside} (indicating theposition of the utterance relative to a decision dis-cussion region).
The middle level corresponds tomid-level discourse items such as issues, resolu-tion proposals, resolution restatements, and agree-ments.
These classes (C1, C2, ..., Cn nodes) arerepresented as a collection of random variables,each corresponding to an individual mid-level ut-terance class.
For example, the middle level of thethree-level HGM Fig.
3 could be the top-level ofthe two-level DGM in Fig.
2, each middle levelnode containing random variables for the DDAclasses I, RP, RR, and A.
The bottom level cor-responds to vectors of observed features as before,e.g.
lexical, utterance, and prosodic features.CnCCnCDR DRC1E ELevel 1Level 2Level 3current utterance next utteranceC1Figure 3: A simple structure of a three-levelHGM: DRs are high-level discourse regions;C1, C2, ..., Cn are mid-level utterance classes; andEs are vectors of observed features.6 ExperimentsThe HGM classifier in Figure 3 was implementedin Matlab using the BNT software4.
The classifierhypothesizes that an utterance belongs to a deci-sion region if the marginal probability of the ut-terance?s DR node is above a hand-tuned thresh-old.
The threshold is selected using the ROC curveanalysis5 to obtain the highest F1-score.
To evalu-ate the accuracy of hypothesized decision regions,we divided the dialogue into 30-second windowsand evaluated on a per window basis.The best model structure was selected by com-paring the performance of various handcraftedstructures.
For example, the model in Fig.
4b out-performs the one in Fig.
4a.
Fig.
4b explicitlymodels the dependency between the decision re-gions and the observed features.I RP RR ADREI RP RR ADREa) b)Figure 4: Three-level HGMs for recognition of de-cisions.
This illustrates the choice of the structurefor each time slice of the HGM sequence models.Table 2 shows the results of 17-fold cross-validation for the hierarchical SVM classifica-tion (Ferna?ndez et al, 2008), rule-based classifi-cation with DGM output (Bui et al, 2009), andour HGM classification using the best combina-tion of non-lexical features.
All three methods4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html5http://en.wikipedia.org/wiki/Receiver operating characteristic310were implemented by us using exactly the samedata and 17-fold cross-validation.
The featureswere selected based on the best combination ofnon-lexical features for each method.
The HGMclassifier outperforms both its SVM and DGMcounterparts (p < 0.0001)6.
In fact, even when theSVM uses lexical as well as non-lexical features,its F1-score is still lower than the HGM classifier.Classifier Pr Re F1SVM 0.35 0.88 0.50DGM 0.39 0.93 0.55HGM 0.69 0.96 0.80Table 2: Results for detection of decision dis-cussion regions by the SVM super-classifier,rule-based DGM classifier, and HGM clas-sifier, each using its best combination ofnon-lexical features: SVM (UTT+DA), DGM(UTT+DA+PROS), HGM (UTT+DA).In contrast with the hierarchical SVM and rule-based DGM methods, the HGM method identifiesdecision-related utterances by exploiting not justDDAs but also direct dependencies between deci-sion regions and UTT, DA, and PROS features.
Asmentioned in the second paragraph of this section,explicitly modeling the dependency between deci-sion regions and observable features helps to im-prove detection of decision regions.
Furthermore,a three-level HGM can straightforwardly modelthe composition of each high-level decision regionas a sequence of mid-level DDA utterances.
Whilethe hierarchical SVM method can also take depen-dency between successive utterances into account,it has no principled way to associate this depen-dency with more extended decision regions.
Inaddition, this dependency is only meaningful forlexical features (Ferna?ndez et al, 2008).The HGM result presented in Table 2 wascomputed using the three-level DBN model (seeFig.
4b) using the combination of UTT and DAfeatures.
Without DA features, the F1-score de-grades from 0.8 to 0.78.
However, this differenceis not statistically significant (i.e., p > 0.5).7 Conclusions and Future WorkTo detect decision discussions in multi-party dia-logue, we investigated HGMs as an extension of6We used the paired t test for computing statistical signif-icance.
http://www.graphpad.com/quickcalcs/ttest1.cfmthe DGMs studied in (Bui et al, 2009).
Whenusing non-lexical features, HGMs outperform thenon-hierarchical DGMs of (Bui et al, 2009) andalso the hierarchical SVM classification methodof Ferna?ndez et al (2008).
The F1-score foridentifying decision discussion regions increasedto 0.80 from 0.55 and 0.50 respectively (p <0.0001).In future work, we plan to (a) investigate cas-caded learning methods (Sutton et al, 2007) toimprove the detection of DDAs further by usingdetected decision regions and (b) extend HGMsbeyond three levels in order to integrate useful se-mantic information such as topic structure.AcknowledgmentsThe research reported in this paper was spon-sored by the Department of the Navy, Office ofNaval Research, under grants number N00014-09-1-0106 and N00014-09-1-0122.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the Office ofNaval Research.ReferencesSatanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.2005.
The necessity of a meeting recording andplayback system, and the benefit of topic-level anno-tations to meeting browsing.
In Proceedings of the10th International Conference on Human-ComputerInteraction.H.
H. Bui, S. Venkatesh, and G. West.
2002.
Pol-icy recognition in the abstract hidden markov model.Journal of Artificial Intelligence Research, 17:451?499.Trung Huu Bui, Matthew Frampton, John Dowding,and Stanley Peters.
2009.
Extracting decisions frommulti-party dialogue using directed graphical mod-els and semantic similarity.
In Proceedings of the10th Annual SIGDIAL Meeting on Discourse andDialogue (SIGdial09).Nigel Crook, Ramon Granell, and Stephen Pulman.2009.
Unsupervised classification of dialogue actsusing a dirichlet process mixture model.
In Pro-ceedings of SIGDIAL 2009: the 10th Annual Meet-ing of the Special Interest Group in Discourse andDialogue, pages 341?348.Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,Matthew Purver, and Stanley Peters.
2008.
Mod-elling and detecting decisions in multi-party dia-logue.
In Proceedings of the 9th SIGdial Workshopon Discourse and Dialogue.311Jurgen Van Gael, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsu-pervised PoS tagging.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 678?687.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:Use of Bayesian networks to model pragmatic de-pendencies.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Lin-guistics (ACL).Daniel Gatica-Perez, Ian McCowan, Dong Zhang, andSamy Bengio.
2005.
Detecting group interest levelin meetings.
In Proceedings of ICASSP.Pey-Yun Hsueh and Johanna Moore.
2007.
Automaticdecision detection in meeting speech.
In Proceed-ings of MLMI 2007, Lecture Notes in Computer Sci-ence.
Springer-Verlag.Adam Janin, Jeremy Ang, Sonali Bhagat, RajdipDhillon, Jane Edwards, Javier Marc?
?as-Guarasa,Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,Andreas Stolcke, Chuck Wooters, and Britta Wrede.2004.
The ICSI meeting project: Resources and re-search.
In Proceedings of the 2004 ICASSP NISTMeeting Recognition Workshop.Mark Johnson, Thomas Griffiths, and Sharon Gold-water.
2007.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Proceedings ofHuman Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 139?146,Rochester, New York, April.
Association for Com-putational Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning, pages 282?289.
Morgan Kaufmann.Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,S.
Bourban, M. Flynn, M. Guillemot, T. Hain,J.
Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,M.
Lincoln, A. Lisowska, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI Meeting Corpus.
InProceedings of Measuring Behavior, the 5th Inter-national Conference on Methods and Techniques inBehavioral Research, Wageningen, Netherlands.Matthew Purver, John Dowding, John Niekrasz,Patrick Ehlen, Sharareh Noorbaloochi, and StanleyPeters.
2007.
Detecting and summarizing actionitems in multi-party dialogue.
In Proceedings of the8th SIGdial Workshop on Discourse and Dialogue,Antwerp, Belgium.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
Journal of MachineLearning Research, 8:693?723.Steve Whittaker, Rachel Laban, and Simon Tucker.2006.
Analysing meeting records: An ethnographicstudy and technological implications.
In S. Renalsand S. Bengio, editors, Machine Learning for Multi-modal Interaction: Second International Workshop,MLMI 2005, Revised Selected Papers, volume 3869of Lecture Notes in Computer Science, pages 101?113.
Springer.312
