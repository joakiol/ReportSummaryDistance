USE OF PROSODY IN SYNTACTIC DISAMBIGUATION:A N  ANALYSIS-BY-SYNTHESIS APPROACHC.
W. Wightman, N. M. Veilleuz, M. OstendorfBoston University44 Cumrnington St.Boston, MA 02215ABSTRACT of seven types of structural ambiguity.
In addition, it ap-~ ~ ~ ~ f i ~ ~ t ~  have shown that prosody is used by human ]is- peared that the relative size and location of prosodic phraseteners to disambiguate spoken language and, in particular, that boundaries provided the principal prosodic clue for resolv-the relative size and location of prosodic phrase boundaries pro- ing ambiguities.
Thus, it seems likely that automaticallyvides a cue for resolving syntactic ambiguity.
Therefore, auto- detected prosodic phrase breaks could be used by speechmatically detected prosodic phrase boundaries can provide infor- understanding systems to reduce syntactic ambiguity.mation useful in speech understanding for choosing among severalcandidate parses.
Here, we propose a scoring algorithm to rankcandidate parses based on an analysis-by-synthesis method whichcompares the observed prosodic phrase structure with the pre-dicted structure for each candidate parse.
In experiments witha small corpus of ambiguous sentences spoken by FM radio an-nouncers, we have achieved disambiguation performance close tothe performance of human subjects in perceptual experiments.INTRODUCTIONSpoken language processing is a difficult problem, in partbecause of the many ambiguities inherent in natural lan-guage.
Syntactic ambiguity arises when a given expressioncan be described by more than one syntactic structure, andrepresents an important problem in natural language pro-cessing.
In particular, attachment ambiguities occur fre-quently in language, e.g.,"Show the fares for [the cheapest flights on the screen].
""Show [the fares for the cheapest flights] on the screen.
"Several factors may be involved in resolving such ambigu-ities, including semantics, discourse and syntactic bias.
Inspoken language, prosody, or the suprasegmental informa-tion in an utterance, is an important additional cue.In previous work [lo], a hierarchical set of break indiceswas proposed as a representation of prosodic phrase bound-aries and automatically detected break indices were used ina parser to provide constraints on rules that would preventprosodically inconsistent parses.
Here, we propose a scoringalgorithm to rank candidate parses based on an analysis-by-synthesis method which involves: (1) using an algorithm topredict prosodic break locations for each candidate syntac-tic structure; (2) automatically detecting prosodic breaksin the spoken utterance; and (3) ranking the parses accord-ing to a similarity score between predicted and observedprosodic structure.
Using the database of ambiguous sen-tences from [13], this approach achieves performance closeto that of the human subjects.The following section describes the speech corpus andprosodic break index representation.
We then examine syn-thesis algorithms for predicting the phrase structure of asentence and evaluate them in relation to the hand-labeledspeech corpus.
Next, we describe an automatic methodof labeling the prosodic phrase structure and a measure ofthe similarity between the predicted and detected prosodicstructures.
We then present experimental results, based onthe ambiguous sentence corpus, demonstrating the utilityof this approach.
In conclusion, we discuss future work sug-gested by these results.Experimental evidence has shown that listeners can re-solve several types of syntactic ambiguities by using prosodic CORPUS AND LABELINGinformation [13,8].
In [13], ambiguous sentences were read As mentioned above, the experiments here are based onin contexts in which only one interpretation was reason- the corpus of ambiguous sentences described in [13].
Anable, and the recordings edited to remove the context.
Hu- advantage of using this database is in the availability ofman subjects then listened to the ambiguous sentences, and perceptual experiment results, which provide an interestingwere asked to select the intended meaning, which they were performance baseline for comparison with results from ourable to do reliably (86% correct identification) for six out algorithm.
The corpus and associated prosodic labeling isdescribed briefly here; readers are referred to \[13\] for furtherdetails.Four professional FM radio announcers were asked toread 35 pairs of sentences, where members of a pair werephonetically similar but associated with different syntacticstructures and therefore different meanings.
The sentencesincluded five examples of each of seven types of structuralambiguity: (1) parenthetical c auses vs. non-parentheticalsubordinate clauses, (2) appositions vs. attached noun (orprepositional) phrases, (3) main clauses linked by coordi-nating conjunctions vs. a main clause and a subordinateclause, (4) tag questions vs. attached noun phrases, (5) farvs.
near attachment of final phrase, (6) left vs. right attach-ment of middle phrase, and (7) particles vs. prepositions.In presentation, the target sentence was preceded by a dis-ambiguating context of one or two sentences.
The targetsentence was edited out of context for analysis and for theperceptual experiments.The utterances were phonetically labeled and segmentedusing the SRI Decipher system \[15\], given the sentence tran-scription, and the associated phoneme durations are usedhere for automatically detecting prosodic phrase breaks.In addition, the utterances have been hand-labeled withprosodic phrase break indices at each word boundary, wherea break index corresponds to the amount of prosodic de-coupling between words.
We use a hierarchy of breaks, from0 for word boundaries within clitic groups through 6 for sen-tence boundaries.SYNTHESISOur goal in this work is to quantitatively measure thesimilarity between predicted and observed prosodic struc-tures.
Therefore, the prosodic phrase synthesis algorithmmust not only predict he locations of prosodic phrase breaks,but also associate a numerical value to indicate hierarchicalstructure, as the perceptual labeling does.
Below we de-scribe algorithms that are appropriate for this application,together with some of our own modifications.Previous WorkGee and Grosjean \[6\] proposed the Phi algorithm tobuild a prosodic tree from syntactic structure.
(Their goalwas to predict psycholinguistic "performance structures;"however, we will only be interested in the prosodic tree.
)The algorithm consists of a sequence of rules that progres-sively groups words and phrases based on syntactic struc-ture and constituent length constraints.
The rules are con-strained to operate within, but not across, basic sentenceclauses.
First, function words and simple modifiers aregrouped into ?-phrases using a right-branching structure.These ~b-phrases are then grouped into I-phrases accord-ing to syntactic onstituency, again using a right-branchingstructure.
The exception is verb phrases which are groupedwith either the subject or the verb's subcategorized comple-ments, depending upon the size of these units, N(-), mea-sured in number of branches (words):o If N(X) + N(V) >_ N(Y) --~ X\[VY\]o otherwise ~ \[XV\]YThese constituents are then further bundled using a left-branching rule until all elements in the clause are included,and then clauses are bundled in a left branching structure.The degree of separation between two words, which we willrefer to as a ~b-break, is given by the number of nodes in thetree dominated by and including the node at this boundary.A second performance structure algorithm, the Psy Al-gorithm, is proposed by van Wijk \[14\] as being more di-rectly tied to linguistic notions of prosodic structure.
ThePsy algorithm requires knowledge of the location of intona-tional phrase boundaries and is based on a flatter prosodicstructure.
Unfortunately, prediction of intonational phraseboundary location is a difficult problem, so this approachwas not investigated here.
However, recent work in this areashows much promise \[17,16\], and the Psy algorithm mightbe interesting to pursue in the future.Modifications to the Phi algorithm have been proposedby Bachenko and Fitzpatrick for speech synthesis applica-tions \[1\].
The main difference lies in Bachenko and Fitz-patrick's claim that prosodic phrase boundaries can extendacross syntactic boundaries, including clause boundaries,.provided that balancing constituent length requires it.
Specif-ically, they have modified Gee and Grosjean's verb balanc-ing rule to include a wider range of syntactic onstituentsavailable for grouping in the verb phrase.
In addition, con-stituent length is determined by the number of phonologicalwords, rather than the number of words.
(A phonologi-cal word is defined as a single content word or a contentword combined with one or more function words that areorthographically distinct but are not separated by prosodicboundaries).
In this paper, we will use "Bachenko/Fitzpatrickalgorithm" to refer to the C-break prediction algorithm whichincorporates their modifications, noting that their work wasnot aimed at predicting numerical break indices.Limitations and Modif icationsAn analysis of the ?-breaks predicted by the Phi algo-rithm and the Bachenko/Fitzpatrick algorithm for the am-biguous sentence corpus identified some weaknesses in thealgorithms which we discuss below.
In addition, we describemodifications to the Bachenko/Fitzpatrick algorithm, whichwe found to more often reflect observed prosodic structure.The verb balancing rule, using either method of count-ing constituent length, did not always yield breaks that were385consistent with our data.
We often observed the verb group-ing with the subject when i t  was predicted by both algo-rithms to group with the following &group.
For example,consider the labeling of the sentenceMarge would never dea l  i n  any guise.5 0  1  3 1  0The largest break, after "Marge", was not perceived in anyof the four spoken renditions.
A more appropriate labelingwould beMarge vould never dea l  i n  any guise.3  0  1 5 1  0Based on this and other examples, we proposed the follow-ing revised verb balancing rule:0 If N ( X )  + N ( V )  2 N ( V )  + N ( Y )-+ X[VYlo otherwise -+ [ X V YUsing this algorithm with a constituent counting functionbased on words rather than on phonological words seemedto be somewhat more consistent with our data, but thisaspect should be confirmed through further study.A second area where problems occured was in allowingprosodic units to contain clause boundaries.
Although itis in general a positive feature of the Bachenko/Fitzpatrickalgorithm, the predicted phrase breaks are not always con-sistent with the observed data, as in.
.
.
, only I knew my Dad would be angry.8 1 0  4 0  6 1 0The larger break was perceived after "knew" rather than af-ter "Dad" in our data.
(Even though the predicted phrasingmight be acceptable, for our purposes it is important thatit be typical.)
This particular problem could be handled byadding the rule:If Y  is NULL, X K S K Y  -+ X K [ S & ]where S is the subject corresponding to %.
The resulting&break labels are then:.
.
.
, only I knew my Dad would be angry.8 1 0  6 0  4 1 0Again, this rule needs further investigation because it mayrequire an associated constituent length constraint.A further limitation with both previous algorithms is thetreatment of parenthetical phrases.Algorithm CorrelationGee & GrosjeanBachenko/FitzpatrickB-F + new verb rulesTable 1: Correlation of predicted +-breaks with hand-labeledperceived breaks for different synthesis algorithms.They know, you r e a l i z e ,  your goals.0  3  0  5 0In our observations, parenthetical phrases are bracketed bynearly equal breaks.
We therefore added a rule to increasethe smaller &break at  a parenthetical boundary to the sizeof the break at  the other side of the parenthetical phrase,as inThey know, you r e a l i z e ,  your goals.0  5 0  5 0EvaluationThe different synthesis algorithms were evaluated bycomputing the correlation between the predicted &breaksand the hand-labeled break indices.
A potential problemis that the hand-labeled indices are constrained to rangefrom 0 to 6, while the &breaks are theoretically unbounded.However, there seemed to be a roughly linear associationbetween the two labeling schemes in principle, apart fromthe specific rules for predicting groupings, and therefore itwas felt that correlation would be a meaningful measure.The correlations given in Table 1 represent the average overseventy sentences from each of four speakers.The original Phi algorithm actually had the highest per-formance, although average performance was similar for allfour algorithms.
The Phi algorithm predictions are morehighly correlated to observed data in most syntactic cat-egories in our database.
Relative to the Phi algorithm,the Bachenko/Fitzpatrick algorithm offered slight improve-ments for parentheticals, main-main structures, and far at-tachments.
Our modified algorithm was similar to the Phialgorithm, but having better performance for parentheticalsand non-tags and somewhat worse performance for non-parentheticals and left attachments.
Our algorithm wasgenerally better than the Bachenko/Fitzpatrick algoritl~m,except for a significant performance degradation for left at-tachments.Overall, results indicate that, while relaxation of clauseboundary constraints is useful, a more conservative set ofrules may more accurately reflect observations.
The verb as-sociation rule introduced here addresses one problem, thatof verb attachment across a clause boundary.
In addition,the length constraints that influence prosodic grouping be-come more important with more flexible syntactic onstraints,which explains the improvement associated with the revisedverb balancing rule.ANALYS ISAfter the synthesis component predicts the prosodic breaksof candidate parses, the analysis component uses a similar-ity measure to compare the match between the predictedand observed prosodic breaks for different possible inter-pretations.
Clearly, in a speech understanding system, theobserved prosodic breaks must be automatically detectedand the algorithm used is described below.
Given sequencesof predicted and automatically detected breaks, many dif-ferent similarity measures are possible.
The results of \[13\],which suggest he importance of relative break size, moti-vate the correlation measure investigated here.Automatic LabelingOther work has reported an algorithm for automaticallydetecting prosodic break indices using a seven-state hiddenMarkov model \[10\], where each state represented a differentbreak index.
The feature used in that system was normal-ized duration in word-final syllable rhyme; a measure ofthe duration lengthening many researchers have observedat phrasal boundaries (e.g., \[7,5\]).
Though pre-boundaxylengthening is a particularly important cue, several otheracoustic ues are also used to mark prosodic phrase bound-aries, including breaths, pauses, boundary tones, and rhythmchanges.
In order to make use of these more diverse cuesand increase the accuracy of our break detection algorithm,we have recently modified the algorithm to use a discreteHMM with a binary tree quantizer that can incorporatemultiple non-homogeneous features.
The algorithm is de-scribed briefly here; further details can be found in \[18\].As in previous work, the first step of processing is todetermine phoneme durations.
These can be obtained fromthe output of the speech recognizer.
Since inherent phoneduration is the main contributor to variance in duration\[7\], segment durations axe normalized according to phone-dependent means and variances.
The means and variancesare themselves adapted according to an estimate of the long-term speaking rate, using an algorithm motivated by thespeaking rate differences given in the data in \[5\].
(This issomewhat different from the tracking Mgorithm reported in\[18\].
)The current system can combine several different fea-tures; we have thus far investigated the following:?
absolute duration of following pause;?
average normalized uration of the phonemes in theword-final syllable rhyme (pre-boundary lengthening);?
difference between average normalized uration of syl-lable rhyme and offset (to distinguish boundaries fromphrasal prominence \[4\]);?
difference between the averages of normalized ura-tion before and after the boundary (rhythm changes);and?
a flag indicating whether or not the word contains anystressed syllables (which was not included in \[18\]).The use of a classification tree \[3\] provides a means ofclassifying feature vectors with non-homogeneous elementsand, in fact, the quantizer can be designed jointly with theHMM \[11\].
Once the feature vectors for each word boundaryare available, we uncover the sequence of break indices mostlikely to have produced them by using Viterbi decoding torecover the state sequence.ScoringIn order to evaluate aJternative interpretations of an ut-terance, we need to be able to compare the synthesisedprosodic breaks with the automatically labeled break in-dices in some quantitative way.
One measure might be aHamming distance between binary sequences where a "1"indicates the location of a major prosodic phrase break.
Thedifficulty with this approach is that it has been shown thatmajor phrase breaks alone are often insufficient o disam-biguate an utterance \[13\].
Thus we need to assign a scorebased on the agreement between the synthesisized break hi-erarchy and the automatic labels for an utterance.The simplest method, and the one used here, is to com-pute the correlation between the two sets of labels.
Forexample, consider the sentence They may wear down theword.
The word down may be either a particle or a prepo-sition in this sentence.
The Gee and Grosjean $-breaks forthese two interpretations are (1, 1,0, 3, 0) and (1, 0, 4, 1, 0),respectively.
The break indices assigned to one reading ofthis sentence are (1, 1, 4, 1, 0), and the correlations with theparticle and preposition interpretations are -0.27 and 0.96,respectively.
Thus, we select the parse in which down func-tions as a preposition as representing the speaker's intendedmeaning.This scoring method is effectively a matched filter detec-tion system, with the exception that we are not normalizingfor "signal energy".
Using this interpretation, it might bepossible to incorporate the greater salience of intonationalphrase boundaries (4,5) \[13\] through a weighted (as opposedto Euclidean) distance measure.Maximum correlation can be used as a criterion for choos-ing among candidate parses.
Occasionally, the correlations387for two candidates will be almost identical.
In this case,we can either allow the algorithm to equivocate (assumingsome other level of processing can resolve the ambiguity),or we can arbitrari ly choose one parse as we do in the ex-periments described here.
Another alternative would be touse the correlations to rank parses or sentence hypotheses.The rank or score might be used in combination with otherknowledge sources, as in \[9\], to choose the correct sentenceinterpretation.EXPERIMENTSWe have tested our analysis-by-synthesis approach byusing it to perform the same task that the human sub-jects in \[13\] were asked to perform.
Specifically, we attemptto select which of two interpretations was intended by thespeaker.
For each test utterance, we use the automatic la-beling algorithm to label the break indices in the utteranceand the synthesis algorithm to generate the prosodic breaksfor the two candidate parses.
We then compute the correla-tion between the labeled break indices and the synthesizedprosodic breaks for each candidate parse and select he parsewith the largest correlation.
In the event of a tie, the firstsentence in the pair is chosen.The models used for the automatic labeling algorithmwere speaker-independent models trained using data fromthree speakers.
Rotation (train on 3 speakers, test on 1)was used to obtain results averaged over all four.
The treequantizer had a codebook size of 70.To gain insight into the effect of break index labelingerrors on the performance ofour disambiguation scheme, wealso conducted the experiment using the hand labeled breakindices in the corpus.
The results of these experiments aresummarized in Table 2 for each of the 14 types (7 pairs) ofsyntactic ambiguity.
For comparison, Table 2 also containsthe results Price et.
al \[13\] report for the human subjects.The results based on the hand-labeled break indices againshow that there is very little difference between the synthe-sis algorithms.
As indicated by the correlation with handlabels (see Table 1), the Gee and Grosjean algorithm gavethe best performance.
The identification accuracy is compa-rable to humans in all but two cases: the non-parentheticaland non-apposition categories.
This could be a weakness ofeither the synthesis algorithm, the similarity measure, or anartifact of the tie-breaking rule.When we use automatically abeled break indices, thereis a loss in performance.
Even so, the algorithm correctlydisambiguates 74% of the sentences, and this represents 88%of the human performance and 89% of the performance ob-tained with hand labels.
Moreover, if we exclude the paren-theticals and appositions, the automatic algorithm achieves79% disambiguation ascompared to human performance of81% for the same categories.DISCUSSIONIn summary, we have demonstrated that automaticallydetected prosodic break indices contain enough informationto achieve disambiguation close to the level of human perfor-mance.
We have considered ifferent synthesis algorithmswhich appear to be quite useful for this task.
Little dif-ference was observed between the synthesis algorithms, butevaluation on a larger task domain would probably yieldmore insight into this issue.While these results demonstrate f asibility of the analysis-by-synthesis approach to disambiguation, the work needsto be extended in several ways.
First, the current synthe-sis algorithm is not implemented automatically because wedid not have access to machine parses for these sentences.Automatic implementation of the synthesis algorithm andintegration with a parser is an important next step.
Asmentioned earlier, additional modifications to the synthesisalgorithm or investigation of a variation based on the Psyalgorithm might also be useful.Second, the automatic break index labeling algorithmneeds to be extended to achieve closer agreement with thehand labels.
Although the correlation between the two isalready 0.86, there is a loss of disambiguation performance.The principal reason for this loss can been seen by not-ing that the machine label differs from the hand label by nomore than one 93% of the time for all the boundaries exceptthose with hand labels of 3 and 4.
These boundaries cor-respond to intermediate and intonational phrases \[12\] andin these cases, the current algorithm produces labels within1 of the hand labels only 57% of the time.
This is hardlysurprising since intermediate and intonational phrases aremarked by intonation \[2\] and our labeling algorithm cur-rently has no pitch features.
Thus a principal extensionwhich needs to be investigated, is the inclusion of intona-tion features uch as boundary tones.
Since these are theprincipal cue for the larger breaks, we expect that their in-clusion will improve performance considerably.In addition, it might be useful to investigate other simi-larity measures.
In particular, a measure which more highlyweighted the larger break indices might be useful.
Finally; itwill be important o consider spontaneous speech domains,which may require an entirely different synthesis algorithmfor predicting phrase breaks.ACKNOWLED GEMENTSThe authors gratefully acknowledge Patt i  Price and Ste-fanie Shattuck-Hufnagel for their valuable suggestions andinsights.
Thanks also to John Butzberger and Hy Murvietat SRI for their help in obtaining the phonetic alignments.This research was jointly funded by NSF and DARPA underNSF grant number IRI-8905249.388Ambiguity+ Parenthetical- Parenthetical+ Apposition- AppositionMain-MainMain-Subordinate+ Tag- TagFar AttachNear AttachLeg AttachRight AttachParticlePrepositionAverage \]Hand Labdse-G l B-F M-I I M-II50 80 80 7590 40 35 4590 90 90 10055 70 70 6565 100 85 8585 45 55 5590 100 100 100100 80 95 95100 60 65 654O 60 5O 5O100 10'0 100 100100 100 100 100100 100 1~0 10095 95 95 958318~ 8o I 81Machine Human& G-G Perception50' 7765 9690 9235 9185 8895 5490 95100 8180 7845 6390 9470 9565 8270 81\[ 74 84Table  2: Percent correct disambiguation asa function of different syntactic ambiguities for: different synthesis algorithms comparingto hand-labeled breaks (G-G: Gee/Grosjean, B-F: Bachenko/Fitzpatrick, M-I: B-F with verb rule modifications, M-II: B-F with allmodifications); the best-case synthesis algorithm comparing to automatically abeled breaks; and human perceptual results.REFERENCES1.
J. Bachenko and E. Fitzpatrick "A Computational Grammarof Discourse-Neutral Prosodic Phrasing in English", Com-putational Linguistics, Vol.
16, No.
3, pp.
155-170 (1990).2.
M. Beckman and J. Pierrehumbert (1986) "IntonationalStructure in Japanese and English," Phonology Yearbook 3,ed.
J. Ohala, pp.
255-309.3.
L. Breiman, J. H. Friedman, R. A. Olshen, and C. J.
Stone(1984) Classification and Regression Trees.
Wadsworth andBrooks/Cole Advanced Books and Software, Monterey, CA.4.
W. N. Campbell (1990).
"Evidence for a Syllable-BasedModel of Speech Timing," Proceedings Int.
Conf.
SpkokenLanguage Processing, pp.
9-12, Kobe, Japan.5.
T. H. Crystal and A. S. House (1988), "Segmental durationsin connected-speech signals: Current results" Journal o.f theAcoustical Society of America, Vol.
83, No.
4, pp.1553-1573.6.
J. P. Gee and F. Grosjean (1983) "Performance Struc-tures: A Psycholingulstic and Linguistic Appraisal," Cog-nifi~e Psychology, Vol.
15, pp.
411-458.7.
D. Klatt (1975) "Vowel Lengthening is Syntactically Deter-rained in a Connected Discourse," J. Phonetics 3, 129-140.8.
I. Lehiste (1973) "Phonetic Disambiguation of SyntacticAmbiguity," Glossa 7:2.9.
M. Ostendoff, A. Kannan, S. Austin, O. Kimball, R.Schwartz and J. R. Roldlcek (1991), "Integration of DiverseRecognition Methodologies Through Reevaluation of N-BestSentence Hypotheses," this proceedings.10.
M. Ostendorf, P. Price, J.
Bear and C. W. Wightman (1990)"The Use of Relative Duration in Syntactic Disambigua-tion," Proceedings of the ~th DARPA Workshop and Speechand Natural Language, pp.
26-31.
A shorter version ap-pears in Proceedings Int.
Conf.
Spkoken Language Process-ing, pp.
13-16, Kobe, Japan.11.
M. Ostendorf and R. Rohllcek (1990) "Joint Quantizer De-sign and Parameter Estimation for Discrete Hidden MarkovModels," Proc.
IEEE Int.
Conf.
Acoust., Speech, SignalProcessing, pp.
705-708, Albuquerque, NM.12.
J. Pierrehumbert (1980) The Phonology and Phonetics ofEnglish Intonation.
PhD Thesis, Massachusetts Institute ofTechnology.13.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, C. Fong "TheUse of Prosody in Syntactic Disambiguation," manuscriptsubmitted to the Journal of the Acoustical Society of Amer-ica.
A shorter version appears in this proceedings.14.
C. van Wijk (1987), "The PSY behind the PHI: A Psy-chollngulstic Model for Performance Structures," Journalof Psycholinguistic Research 16:2, pp.
185-199.15.
M. Weintraub, H. Murveit, M. Cohen, P. Price, J. Bem-stein, G. Baldwin and D. Bell (1989) "Linguistic Con-straints in Hidden Markov Model Based Speech Recogni-tion," Proe.
IEEE Int.
Conf.
Acoust., Speech, Signal Pro-cessing, pp.
699-702, Glasgow, Scotland.16.
N. Veilleux and M. Ostendoff, "A Hierarchical StochasticStructure for Automatic Prediction of Prosodic BoundaryLocation," manuscript.17.
Wang and J. Hirschberg (1991), "Predicting IntonationalBoundaries Automatically from Text: the ATIS Do-main" ,this proceedings.18.
C. W. Wightman and M. Ostendorf (1991), "AutomaticRecognition of Prosodic Phrases," Proc.
IEEE Int.
Conf.Aeoust., Speech, Signal Processing, Toronto, Canada.389
