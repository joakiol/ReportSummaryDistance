Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 374?379,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIBC-C: A Dataset for Armed Conflict Event AnalysisAndrej?Zukov-Gregori?c and Bartal Veyhe and Zhiyuan LuoDepartment of Computer ScienceRoyal Holloway, University of LondonEgham TW20 0EX{andrej.zukovgregoric.2010, bartal.veyhe.2014}@live.rhul.ac.ukzhiyuan@cs.rhul.ac.ukAbstractWe describe the Iraq Body Count Cor-pus (IBC-C) dataset, the first substan-tial armed conflict-related dataset whichcan be used for conflict analysis.
IBC-C provides a ground-truth dataset for con-flict specific named entity recognition, slotfilling, and event de-duplication.
IBC-C is constructed using data collected bythe Iraq Body Count project which hasbeen recording incidents from the ongoingwar in Iraq since 2003.
We describe thedataset?s creation, how it can be used forthe above three tasks and provide initialbaseline results for the first task (namedentity recognition) using Hidden MarkovModels, Conditional Random Fields, andRecursive Neural Networks.1 IntroductionMany reports about armed conflict related inci-dents are published every day.
However, these re-ports on the deaths and injuries of civilians andcombatants often get forgotten or go unnoticed forlong periods of time.
Automatically extracting ca-sualty counts from such reports would help bettertrack ongoing conflicts and understand past ones.One popular approach of discovering incidentsis to identify them from textual reports and extractcasualty, and other, information from them.
Thiscan either be done by hand or automatically.
TheIraq Body Count (IBC) project has been directlyrecording casualties since 2003 for the ongoingconflict in Iraq (IBC, 2016; Hicks et al, 2011).IBC staff collect reports, link them to unique in-cidents, extract casualty information, and save theinformation on a per incident basis as can be seenin Table 2.Direct recording by hand is a slow process andnotable efforts to do so have tended to lag behindthe present.
Information extraction systems capa-ble of automating this process must explicitly orimplicitly successfully solve three tasks: (1) findand extract casualty information in reports (2) de-tect events mentioned in reports (3) deduplicatedetected events into unique events which we callincidents.
The three tasks correspond to namedentity recognition, slot filling, and de-duplication.In this work we introduce the report based IBC-C dataset.1Each report can contain one or moresections; each section, one or more sentences;each sentence, one or more words.
Each word istagged with one of nine entity tags in the inside-outside-beginning (IOB) style.
A visual represen-tation of the dataset can be seen in Figure 1 and itsstatistics in Table 1.To the best of our knowledge apart from thesignificantly smaller MUC-3 and MUC-4 datasets(which aren?t casualty-specific) there are no otherpublicly available datasets made specifically fortasks (1), (2) or (3).
The IBC-C dataset can beused to train supervised models for all three tasks.We provide baseline results for task (1) whichwe posit as a sequence-classification problem andsolve using an HMM, a CRF, and an RNN.Since the 1990s the conflict analysis andNLP/IE communities have diverged.
With theIBC-C dataset we hope to bring the two commu-nities closer again.2 Related WorkExtracting information from conflict related re-ports has been a topic of interest at various timesfor both the conflict analysis, information extrac-tion, and natural language processing communi-1More information about the IBC-C dataset can be foundon: http://andrejzg.github.io/ibcc/374IncidentIncidentIncidentIncidentSectionSectionReportSectionReportSectionSectionSectionReportKilled: 5Injured: 2Location: BaghdadKilled: 22Injured: 10Location: TikritKilled: 13Injured: 5Location: BaghdadDate: March 12thKilled: 1Injured: 0Location: FallujahDate: Last weekFigure 1: The IBC-C dataset visualised.
A re-port is split into one or more non overlapping sec-tions.
A section is comprised of sentences whichare comprised of words.
Each section is linked toexactly one incident which in turn can be linked toone or more sections.ties.The 1990s saw a series of message understand-ing conferences (MUCs) of which MUC-3 andMUC-4 are closely related to our work and containreports of terrorist incidents in Central and SouthAmerica.
MUC data is most often used for slotfilling and although MUC-3 and MUC-4 containmore slots than IBC-C they are at the same timemuch smaller (MUC4 contains 1,700 reports) andcannot be used for incident de-duplication.Although various ACE, CoNNL, and TAC-KBPtasks contain within them conflict-related reports,none of them are specific to conflict and haven?tbeen studied for conflict-related information ex-traction specifically.Studies more directly related to our dataset in-clude work by Tanev and Piskorski (Tanev et al,2008) who use pattern matching to count casu-alties.
They report a 93% accuracy on count-ing the wounded.
However, they have access toonly 29 unique conflict events.
Other non-casualtyconflict-related work in the domain also suffersfrom a lack of data, for example, (King and Lowe,2003) only deal with 711 reports.Despite work in the NLP and IE communities,the conflict analysis community is still reliant onElement Countincidents 9,184sections 18,379reports 16,405sentences 35,295words 857,465KNUM 13,597INUM 6,689KSUB 14,395ISUB 1,036KOTHER 1,192IOTHER 495LOCATION 25,251DATE 4,765WEAPON 35,617Table 1: Dataset statistics.
Fully capitalised wordsindicate named entity tags.datasets created by hand.
These include IBC (IBC,2016), ACLED (Raleigh et al, 2010), EDACS(Chojnacki et al, 2012), UCDP (Gleditsch et al,2002), and GTD (GTD, 2015).To the best of our knowledge there are no effortsto fully automate casualty counting.
However, ef-forts using NLP/IE tools to automate incident de-tection do exist but their ability to de-deduplicateincidents has been called into question (Weller andMcCubbins, 2014).Three notable such efforts originating in theconflict analysis community are GDELT (Lee-taru and Schrodt, 2013), ICEWS (Obrien, 2010),and OEDA (Schrodt, 2016).
All three use pat-tern matching software such as TABARI (Schrodt,2001) and to categorise reports using the CAMEOcoding scheme (Schrodt et al, 2008).3 Creating the IBC-C Dataset3.1 PreprocessingThe Iraq Body Count project (IBC) has beenrecording conflict-related incidents from the Iraqwar since 2003.
An incident is a unique event re-lated to war or other forms of violence which ledto the death or injury of people.
An example canbe seen in Table 2.The recording of incidents by the IBC worksas follows: IBC staff first collect relevant reportsbefore highlighting sections of them which theydeem relevant to individual incidents.
Parts ofthe report outside the highlighted sections are dis-carded.
Sections can be seen in Figure 1.
Becauseof the way IBC staff highlight sections there are nooverlapping sections in the IBC-C dataset.
Eventsare then recognised from the highlighted sectionsand de-duplicated into incidents.
A final descrip-375Incident ID Start date End dated3473 22 Mar 2003 22 Mar 2003Min killed Max killed Min injured2 2 8Max injured Location Cause of death9 Khurmal Suicide car bombSources Town ProvinceBBC 23 MarDPA 23 MarKhurmal SulaymaniyahAlt.
province District Alt district/ Halabja /Killed SubjectsPerson 1, Person 2, ...Injured SubjectsPerson 3, Person 4, ...Report SectionsBBC: ?On Saturday Person 1 died in Khurmal ...?DPA: ?2 people died yesterday afternoon...?Table 2: An example of an incident hand codedby IBC staff.
Min and max values represent theminimum and maximum figures quoted in reportsections linked to the incident.tion of the incident (e.g.
death and injury counts,location and date) is agreed upon after multiplerounds of human checking.In the preprocessing step we gathered all inci-dents which occurred between March 20th, 2003and December 31st, 2013.
We removed spuri-ous incidents (e.g.
where the minimum numberkilled is larger than the maximum number killed)and cleaned the section text by removing all for-matting and changing all written-out numbers intotheir numeric form (e.g.
?three?
to 3).3.2 AnnotationUsing the information extracted by the IBC (seeTable 2) we annotated each section word with oneof ten tags: KNUM and INUM for numbers repre-senting the number killed and injured respectively;KSUB and ISUB for named individuals were killedor injured; KOTHER and IOTHER for unnamedpeople who were killed or injured (for example?The doctor was injured yesterday.?
); LOCATIONfor the location in which an incident occurred;WEAPON for any weapons used in an attack;DATE for words which identify when the incidenthappened; and, O for all other words.Our data generation process can be thought ofas a form of distant supervision (Mintz et al,2009) where we use agreed upon knowledge aboutan incident to label words contained within itssections instead of having hand-labeled individualwords.
This inevitably introduces errors which wetry to mitigate using a filtration step where we re-move ambiguous data.Incident FiltrationPreprocessing + AnnotationIBC dataIBC-C datasetSection FiltrationSentence FiltrationFigure 2: A visualisation of the different stepstaken to create the dataset.3.3 FiltrationSimply annotating words based on the informationin Table 2 can lead to wrong annotations.
For ex-ample, if two people were recorded as having diedin an incident, then, if another number two appearsin the same sentence, this might lead to a wrongannotation.
The sentence, ?2 civilians were killedafter 2 rockets hit the compound?
could lead to thesecond ?2?
being annotated as a KNUM.
The ac-tual cardinality of a number makes little differenceto a sequence classifier compared to the differencea misannotated number would make.
To min-imise such misannotations we remove sentencesand reports which do not pass all filtration criteria.Our filtration criteria consist of boolean functionsover sentences, sections and incidents which re-turn false if a test isn?t passed.The goal of filtration is to remove as much am-biguously labelled data as possible without bias-ing against any particular set of linguistic forms.There is thus a tradeoff which must be struck be-tween linguistic richness and the quality of anno-tation.In our case we found that simple combinationsof pattern matching and semantic functions, as in3, worked well.
No syntactic functions were used.3.3.1 Incident FiltrationIncidents are filtered using a single criterion: if theminimum number of people killed or injured doesnot equal the maximum number of people killedor injured, respectively, (Table 2) then the inci-dent is removed.
We do this so as to minimise anyambiguity in our named entity tagging (the onlytask for which we provide baseline results).
Thishas the adverse effect of removing any incidentswhere reports mention different casualty counts.To compile a dataset which disregards this crite-rion, or considers a permissible window of casual-376hasKNUMisKillSentencehasOneTaggedAsKNUMhasNumberotherKNUMsInSectiontoConsider#+ + + + + - 2,445+ + + + - + 7,526+ + - + + - 14,624+ + - + - + 30,204+ - + + + - 2,119+ - + + - - 1,498+ - - + + - 4,282+ - - + - - 4,648- + - + + + 2,757- + - + - - 67,402- + - - + + 3,360- + - - - + 43,006- - - + + + 7,573- - - + - + 47,736- - - - + + 19,749- - - - - + 125,010Table 3: Filtration criteria.
An example of a set ofboolean functions (columns one through five) ap-plied to sentences to filter out ambiguous KNUMannotations.
Sentences which we wish to allow areidentified by a ?+?
in the toConsider column.
Sen-tence counts are given in the last column.
Onlyrows with non-zero counts are shown.
Shadedrows indicate sentences which are ambiguous areshaded and identified by a ?-?.
We show only theKNUM table due to lack of space.ties, a parameter in our dataset generating programmay be changed.3.3.2 Sentence FiltrationFiltering sentences is by far the hardest step.
Itis here where we must be careful to not biasagainst any linguistic forms.
A separate set ofboolean functions are applied to each sentence forthe KNUM and INUM entity tags.
An example forthe KNUM tag can be seen in Table 3.
Every sen-tence passes through four boolean functions (thefirst four columns) and is then labeled as eitherhaving passed or failed the test (fifth column).
Thefifth column was decided upon by us in advance.In the case of Table 3: hasKNUM indicateswhether the sentence contains a word tagged asKNUM; isKillSentence indicates whether any ofits words are connected to death or killing (bymatching them against a list of predefined words);hasOneTaggedAsKNUM indicates whether thenumber ?1?
is tagged as a KNUM (remember thatwe convert written out numbers such as ?three?to ?3?
and that ?one?, and thus ?1?, can also bea pronoun); hasNumber indicates whether a sen-tence has a number; and, otherKNUMsInSectionindicates whether there are other words tagged asKNUM in the section.3.3.3 Report FiltrationReport filtering is simple and again done usingonly one rule.
If any sentence a report containsfails to pass a single sentence-level test, then thewhole report is removed.3.4 Tasks3.4.1 Named Entity RecognitionEach word in the IBC-C dataset is tagged with oneof nine (excluding O) entity tags as can be seen inTable 1 which can be thought of as subsets of morecommon named entity tags such as person or loca-tion.
The dataset can be used to train a supervisedNER model for conflict-specific named entity tags.This is important for relationship extraction whichrelies on good named entity tags.3.4.2 Slot Filling and Relationship ExtractionEach IBC-C event can be thought of as a 9-slotevent template where each slot is named after anentity tag.
The important thing to keep in mind isthat a report may contain more than one section sojust correctly recognising the entities isn?t enoughto solve the slot filling task.
Instead, if a reportmentions two events then two separate templatesmust be created and their slots filled.A common sub-problem of slot filling is rela-tionship extraction.
Because we know which in-cident every section refers to, generating ground-truth relationships is trivial because we may besure that an entity which appears in one of thesections is related to every other entity in thatsame section.
For example, finding a KSUBand a LOCATION means that we can build akilled in(KSUB, LOCATION) relationship.3.4.3 Event De-duplicationSince the IBC-C dataset preserves the links be-tween sections and incidents it may be used asa ground-truth training set for training event de-duplication models.377HMM CRF 13-window RNN 13-windowTag Precision Recall F1 Precision Recall F1 Precision Recall F1KNUM 0.63 0.86 0.73 0.91 0.94 0.92 0.90 0.85 0.88INUM 0.50 0.39 0.44 0.95 0.93 0.94 0.87 0.91 0.89KSUB 0.73 0.68 0.70 0.82 0.76 0.79 0.86 0.53 0.66ISUB 0.00 0.00 0.00 0.89 0.24 0.38 0.80 0.06 0.12KOTHER 0.39 0.19 0.25 0.83 0.54 0.66 0.41 0.36 0.38IOTHER 0.00 0.00 0.00 0.80 0.61 0.69 0.55 0.50 0.52LOCATION 0.75 0.70 0.73 0.85 0.77 0.80 0.86 0.70 0.77DATE 0.75 0.64 0.69 0.75 0.64 0.69 0.41 0.30 0.35WEAPON 0.98 0.89 0.93 0.98 0.90 0.94 0.97 0.87 0.92Overall 0.57 0.53 0.55 0.88 0.73 0.78 0.74 0.57 0.61Table 4: Results for various models4 ExperimentsBaseline results were computed for the named en-tity recognition task using an 80:20 tag split acrosssentences (we ignore report or section bound-aries).
We compare three different sequence-classification models as seen in Table 4: a Hid-den Markov Model (Zhou and Su, 2002), a Con-ditional Random Field (McCallum and Li, 2003),and a Elman-style Recursive Neural Network sim-ilar to the one used in (Mesnil et al, 2013).For the HMM we use bigram features in combi-nation with the current word and the current basenamed entity features2.
We trained the HMM inCRF form using LBFGS.For the CRF we find that using bigram fea-tures and a 13-word window, across words andbase named entities, gives us the best result.
Wetrain the CRF using LBFGS.
All CRF training,including the HMM, was done using CRFSuite(Okazaki, 2007).For the Elman-style recurrent network we userandomly initialised 100 dimensional word vec-tors as input, the network has 100 hidden units,and we use a 13-word context window again.
TheRNN was implemented using Theano (Bastien etal., 2012).
We train the RNN using stochastic gra-dient descent on a single GPU.4.1 EvaluationThe first thing which strikes us is how low theISUB scores are.
The CRF returns a recall score of0.24.
At the same time, the precision is relativelyhigh at 0.89.
Low recall indicates a lot of falsenegative classifications - i.e.
there were many in-jured people who were mistakingly tagged as un-injured.
A high precision rate means a low false2Base named entities such as PERSON and LOCATIONwere found using Stanford?s named entity recogniser (Finkelet al, 2005).positive rate - i.e.
most uninjured people were cor-rectly tagged as uninjured.
In short, the classifierwas too generous with tagging people as havingbeen injured.
Looking at the dataset we realisethat in contrast to KSUBS, words which we asso-ciate with injury such as ?wounded?
or ?injured?are often very far away from an ISUB.
Increasingthe window size with the CRF didn?t help (suchlarge features are often never expressed during thetest phase).Low recall scores across multiple tags indi-cate that long-distance dependencies determine aword?s classification.
K/INUM recall is excep-tionally high because K/INUMs are usually sur-rounded by words such as ?killed?.
We were sur-prised to see the RNN perform relatively poorlyand expected it to be able to factor in long-distancedependencies.
We believe this has more to do withour hyper-parameter settings than deficiencies inthe actual model.5 ConclusionWe present IBC-C, a new dataset for armed con-flict analysis which can be used for entity recogni-tion, slot filling, and incident de-duplication.6 AcknowledgementsWe would like to thank members of the IBC, es-pecially Hamit Dardagan for his help with procur-ing and helping us understand the data collectedby the IBC.
We would also like to thank GregoryChockler, Mike Spagat, and Andrew Evans fortheir insightful discussions and suggestions.
Thiswork was partially supported by EPSRC grantEP/K033344/1 (?Mining the Network Behaviourof Bots?
).378ReferencesFr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian Goodfellow, Arnaud Bergeron,Nicolas Bouchard, David Warde-Farley, and YoshuaBengio.
2012.
Theano: new features and speed im-provements.
arXiv preprint arXiv:1211.5590.Sven Chojnacki, Christian Ickler, Michael Spies, andJohn Wiesel.
2012.
Event data on armed con-flict and security: New perspectives, old challenges,and some solutions.
International Interactions,38(4):382?401.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Nils Petter Gleditsch, Peter Wallensteen, Mikael Eriks-son, Margareta Sollenberg, and H?avard Strand.2002.
Armed conflict 1946-2001: A new dataset.Journal of peace research, 39(5):615?637.GTD.
2015.
Global terrorism database.
http://www.start.umd.edu/gtd.
(Accessed on02/23/2016).Madelyn Hsiao-Rei Hicks, Hamit Dardagan,Gabriela Guerrero Serd?an, Peter M Bagnall,John A Sloboda, and Michael Spagat.
2011.
Vio-lent deaths of iraqi civilians, 2003?2008: analysisby perpetrator, weapon, time, and location.
PLoSMed, 8(2):e1000415.IBC.
2016.
Iraq body count.
https://www.iraqbodycount.org/database/.
(Accessed on 02/23/2016).Gary King and Will Lowe.
2003.
An automated infor-mation extraction tool for international conflict datawith performance as good as human coders: A rareevents evaluation design.
International Organiza-tion, 57(03):617?642.Kalev Leetaru and Philip A Schrodt.
2013.
Gdelt:Global data on events, location, and tone, 1979?2012.
In ISA Annual Convention, volume 2.
Cite-seer.Andrew McCallum and Wei Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 188?191.
Association for Computational Lin-guistics.Gr?egoire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spo-ken language understanding.
In INTERSPEECH,pages 3771?3775.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Naoaki Okazaki.
2007.
Crfsuite: a fast implementa-tion of conditional random fields (crfs).
(Accessedon 02/23/2016).Sean P Obrien.
2010.
Crisis early warning anddecision support: Contemporary approaches andthoughts on future research.
International StudiesReview, 12(1):87?104.Clionadh Raleigh, Andrew Linke, H?avard Hegre, andJoakim Karlsen.
2010.
Introducing acled: Anarmed conflict location and event dataset specialdata feature.
Journal of peace research, 47(5):651?660.Philip A Schrodt, Om?ur Yilmaz, Deborah J Gerner, andDennis Hermreck.
2008.
The cameo (conflict andmediation event observations) actor coding frame-work.
In 2008 Annual Meeting of the InternationalStudies Association.Philip A Schrodt.
2001.
Automated coding of interna-tional event data using sparse parsing techniques.
Inannual meeting of the International Studies Associ-ation, Chicago.Philip A Schrodt.
2016.
Open event data alliance(oeda).
(Accessed on 02/23/2016).Hristo Tanev, Jakub Piskorski, and Martin Atkinson.2008.
Real-time news event extraction for globalcrisis monitoring.
In Natural Language and Infor-mation Systems, pages 207?218.
Springer.Nicholas Weller and Kenneth McCubbins.
2014.Open event data alliance (oeda)raining on the pa-rade: Some cautions regarding the global databaseof events, language and tone dataset.
(Accessed on05/18/2016).GuoDong Zhou and Jian Su.
2002.
Named entityrecognition using an hmm-based chunk tagger.
Inproceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 473?480.Association for Computational Linguistics.379
