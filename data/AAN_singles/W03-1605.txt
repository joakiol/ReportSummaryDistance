Interrogative Reformulation Patterns and Acquisition of QuestionParaphrasesNoriko TomuroDePaul UniversitySchool of Computer Science, Telecommunications and Information Systems243 S. Wabash Ave.Chicago, IL 60604 U.S.A.tomuro@cs.depaul.eduAbstractWe describe a set of paraphrase patternsfor questions which we derived from acorpus of questions, and report the resultof using them in the automatic recogni-tion of question paraphrases.
The aimof our paraphrase patterns is to factor outdifferent syntactic variations of interroga-tive words, since the interrogative part ofa question adds a syntactic superstructureon the sentence part (i.e., the rest of thequestion), thereby making it difficult foran automatic system to analyze the ques-tion.
The patterns we derived are ruleswhich map surface syntactic structures tosemantic case frames, which serve as thecanonical representation of questions.
Wealso describe the process in which weacquired question paraphrases, which weused as the test data.
The results obtainedby using the patterns in paraphrase recog-nition were quite promising.1 IntroductionThe phenomenon of paraphrase in human languagesis essentially the inverse of ambiguity ?
a givensentence could ambiguously have several meanings,while any given meaning could be formulated intoseveral paraphrases using various words and syntac-tic constructions.
For this reason, paraphrase posesa great challenge for many Natural Language Pro-cessing (NLP) tasks, just as ambiguity does, notablyin text summarization and NL generation (Barzilayand Lee, 2003; Pang et al, 2003).The problem of paraphrase is important inQuestion-Answering systems as well, because thesystems must return the same answer to questionswhich ask for the same thing but are expressed indifferent ways.
Recently there have been severalwork which utilized reformulations of questions asa way to fill the chasm between words in a questionand those in a potential answer sentence (Hermjakobet al, 2002; Murata and Isahara, 2001; Agichtei etal., 2001).
In general, paraphrasing a question, be itfor recognition or generation, is more difficult thana declarative sentence, because interrogative wordscarry a meaning of their own, which is subject to re-formulation, in addition to the rest (or the sentencepart) of the question.
Reformulations of the interrog-ative part of questions have some interesting char-acteristics which are distinct from reformulations ofthe sentence part or declarative sentences.
First,paraphrases of interrogatives are strongly lexical andidiosyncratic, containing many keywords, idioms orfixed expressions.
For example, for a question ?Howcan I clean teapots??
one can easily think of somevariations of the ?how?
part while fixing the sentencepart:- ?In what way should I clean teapots?
?- ?What do I have to do to clean teapots?
?- ?What is the best way to clean teapots?
?- ?What method is used for cleaning teapots?
?- ?How do I go about cleaning teapots?
?- ?What is involved in cleaning teapots?- ?What should I do if I want to clean teapots?Second, reformulation patterns of interrogativesseem to be governed by question types.
For exam-ple, the variation patterns above apply to almost all?how-to?
questions, while ?why?
questions undergoa different set of transformations (e.g.
?Why ..?,?For what reason ..?, ?What was the reason why ..?etc.).
Also, further observations suggest that ques-tions of the same question type have the same se-mantic empty category: something (or some things)which a question is asking.In this paper, we describe the set of para-phrase/reformulation patterns we derived from acorpus of questions, and report the result of usingthem in the automatic recognition of question para-phrases.
We also describe the process in which weacquired paraphrases, which we used as the test data.Our approaches to constructing those resources weremanual ?
the transformation patterns were derivedby inspecting an existing large corpus of questions,and the paraphrases were collected by asking webusers to type in reformulations of sample questions.Our work here is focused on the reformulations ofthe interrogative part of questions in contrast to otherwork in question-answering where major emphasesare placed on the reformulations of phrases or wordsin the sentence part (Lin and Pantel, 2001; Herm-jakob et al, 2002).
The patterns we derived areessentially rules which map surface syntactic struc-tures to semantic case frame representations.
We usethose case frame representations when we comparequestions for similarity.
The results obtained by theuse of the patterns in paraphrase recognition werequite promising.The motivation behind the work we present hereis to improve the retrieval accuracy of our systemcalled FAQFinder (Burke et al, 1997).
FAQFinder isa web-based, natural language question-answeringsystem which uses Usenet Frequently Asked Ques-tions (FAQ) files to answer users?
questions.
EachFAQ file contains a list of question-and-answer(Q&A) pairs on a particular subject.
Given a user?squestion as a query, FAQFinder tries to find an an-swer by matching the user?s question against thequestion part of each Q&A pair, and displays 5 FAQquestions which are ranked the highest by the sys-tem?s similarity measure.
Thus, FAQFinder?s task isto identify FAQ questions which are the best para-phrases of the user?s question.
Figure 1 shows ascreen snapshot of FAQFinder where a user?s queryFigure 1: The 5 best-matching FAQ questions re-turned by FAQFinder?What do I have to do to clean teapots??
is matchedagainst the Q&A pairs in ?drink tea faq?.
The cur-rent similarity measure used in the system is a com-bination of four independent metrics: term vectorsimilarity, coverage, semantic similarity, and ques-tion type similarity (Lytinen and Tomuro, 2002).
Al-though those metrics are additive and complementalto each other, they cannot capture the relations andinteractions between them.
The idea of paraphrasepatterns proposed in this paper is a first step in devel-oping an alternative, integrated similarity measurefor question sentences.2 Paraphrasing Patterns for Questions2.1 Training DataParaphrasing patterns were extracted from a largecorpus of question sentences which we had used inour previous work (Tomuro and Lytinen, 2001; Lyti-nen and Tomuro, 2002).
It consisted of 12938 exam-ple questions taken from 485 Usenet FAQ files.
Inthe current work, we used a subset of that corpusconsisting of examples whose question types werePRC (procedure), RSN (reason) or ATR (atrans).Those question types are members of the 12 ques-tion types we had defined in our previous work (To-muro and Lytinen, 2001).
As described in that paper,PRC questions are typical ?how-to?
questions andRSN questions are ?why?
questions.
The type ATR;(1) how can/do .. anyVerb(defpattern prc-how 1(:WH how) (:S <NPS>) (:V <V>) (:O <NPO>)=>(:proc ?)
(:actor <NPS>) (:verb <V>) (:theme <NPO>));(2) how can/do .. obtain(defpattern atr-1-how-obtainV 3(:WH how) (:S <NPS>) (:V <obtainV>) (:O <NPO>)=>(:source ?)
(:proc ?)
(:actor <NPS>) (:verb <obtainV>) (:theme <NPO>));(3) what is the .. method for obtaining(defpattern atr-1-what-is-method 4(:WH what) (:S NIL) (:V <beV>) (:O <methodN>) (:VG <obtainV>) (:NP <NPO>)=>(:source ?)
(:proc ?)
(:actor I) (:verb <obtainV>) (:theme <NPO>));(4) who sells(defpattern atr-who-sourceNP 4(:WH who) (:S NIL) (:V <sellV>) (:O <NPO>)=>(:source ?)
(:proc ?)
(:actor I) (:verb obtain) (:theme <NPO>))Figure 2: Example Paraphrase Patterns(for ATRANS in Conceptual Dependency (Schank,1973)) is essentially a special case of PRC, wherethe (desire for the) transfer of possession is stronglyimplied.
An example question of this type wouldbe ?How can I get tickets for the Indy 500??.
Notonly do ATR questions undergo the paraphrasingpatterns of PRC questions, they also allow reformu-lations which ask for the (source or destination) lo-cation or entity of the thing(s) being sought, for in-stance, ?Where can I get tickets for the Indy 500?
?and ?Who sells tickets for the Indy 500??.
Wehad observed that such ATR questions were in factasked quite frequently in question-answering sys-tems.1 Also those question types seem to have aricher set of paraphrasing patterns than other types(such as definition or simple reference questionsgiven in TREC competitions (Voorhees, 2000)) withregard to the interrogative reformulation.
In the cor-pus, there were 2417, 1022 and 968 questions oftype PRC, RSN, ATR respectively, and they consti-tuted the training data in the current work.1Although we did not use it in the current work, wealso had access to the user log of AskJeeves system(http://www.askjeeves.com).
We observed that a largeportion of the user questions were ATR questions.2.2 Paraphrase PatternsThe aim of our paraphrasing patterns is to accountfor different syntactic variations of interrogativewords.
As we showed examples in section 1, theinterrogative part of a question adds a syntactic su-perstructure to the sentence part, thereby making itdifficult for an automatic system to get to the core ofthe question.
By removing this syntactic overhead,we can derive the canonical representations of ques-tions, and by using them we can perform a many-to-one matching instead of many-to-many when wecompare questions for similarity.In the pre-processing stage, we first applied ashallow parser to each question in the training dataand extracted its phrase structure.
The parser weused is customized for interrogative sentences, andits complexity is equivalent to a finite-state machine.The output of the parser is a list of phrases in whicheach phrase is labeled with its syntactic functionin the question (subject, verb, object etc.).
Passivequestions are converted to active voice in the laststep of the parser by inverting the subject and objectnoun phrases.
Then using the pre-processed data,we manually inspected all questions and defined pat-terns which seemed to apply to more than two in-stances.
By this enumeration process, we deriveda total of 127 patterns, consisting of 18, 23 and 86patterns for PRC, RSN and ATR respectively.Each pattern is expressed in the form of a rule,where the left-hand side (LHS) expresses the phrasestructure of a question, and the right-hand side(RHS) expresses the semantic case frame represen-tation of the question.
When a rule is matchedagainst a question, the LHS of the rule is comparedwith the question first, and if they match, the RHS isgenerated using the variable binding obtained fromthe LHS.
Figure 2 shows some example patterns.In a pattern, both LHS and RHS are a set of slot-value tuples.
In each tuple, the first element, whichis always prefixed with :, is the slot name and theremaining elements are the values.
Slots nameswhich appear on the LHS (:S, :V, :O, etc.)
relateto syntactic phrases, while those on the RHS (:ac-tor, :theme, :source etc.)
indicate semantic cases.
Aslot value could be either a variable, indicated by asymbol enclosed in .. (e.g.
NPS), or a con-stant (e.g.
how).
A variable could be either con-strained (e.g.
obtainV) or unconstrained (e.g.NPS, NPO).
Constrained variables are de-fined separately, and they specify that a phrase tobe matched must satisfy certain conditions.
Mostof the conditions are lexical constraints ?
a phrasemust contain a word of a certain class.
For instance,obtainV denotes a word class ?obtainV?
and itincludes words such as ?obtain?, ?get?, ?buy?
and?purchase?.
Word classes are groupings of wordsappeared in the training data which have similarmeanings (i.e., synonyms), and they were developedin tandem with the paraphrase patterns.
Whetherconstrained or unconstrained, a variable gets boundwith one or more words in the matched question (ifpossible for constrained variables).
A constant indi-cates a word and requires the word to exist in the tu-ple.
?NIL?
and ???
are special constants where ?NIL?requires the tuple (phrase in the matched question)to be empty, and ???
indicates that the slot is anempty category.
Each rule is also given a prioritylevel (e.g.
3 in pattern (2)), with a large number in-dicating a high priority.In the example patterns shown in Figure 2, pat-tern (1) matches a typical ?how-to?
question suchas ?How do I make beer??.
Its meaning, accord-ing to the case frame generated by the RHS, wouldbe ?I?
for the actor, ?make?
for the verb, ?beer?
forthe theme, and the empty category is :proc (for pro-Figure 3: Paraphrase Entry Sitecedure).
Patterns (2) through (4) are rules for ATRquestions.
Notice they all have two empty categories?
:proc and :source ?
as consistent with our defini-tion of type ATR.
Also notice the semantic case rolesare taken from various syntactic phrases: pattern (2)takes the actor and theme from syntactic subject andobject straight-forwardly, while pattern (3), whichmatches a question such as ?What is a good way tobuy tickets for the Indy 500?, takes the theme fromthe object in the infinitival phrase (:NP) and fills theactor with ?I?
which is implicit in the question.
Pat-tern (4), which matches a question such as ?Whosells tickets for the Indy 500?, changes the verb to?obtain?
as well as filling the implicit actor with ?I?.This way, ATR paraphrases are mapped to identicalcase frames (modulo variable binding).3 Acquisition of Question ParaphrasesTo evaluate the question paraphrase patterns, weused the set of question paraphrases which we hadacquired in our previous work (Tomuro and Lytinen,2001) for the test data.
In that work, we obtainedquestion paraphrases in the following way.
First weselected a total of 35 questions from 5 FAQ cate-gories: astronomy, copyright, gasoline, mutual-fund and tea.
Then we created a web site whereusers could enter paraphrases for any of the 35 ques-tions.
Figure 3 shows a snapshot of the site when theastronomy FAQ is displayed.2 After keeping the sitepublic for two weeks, a total of 1000 paraphraseswere entered.
Then we inspected each entry and dis-carded ill-formed ones (such as keywords or booleanqueries) and incorrect paraphrases.
This process leftus with 714 correct paraphrases (including the orig-inal 35 questions).Figure 4 shows two sets of example paraphrasesentered by the site visitors.
In each set, the firstsentence in bold-face is the original question (andits question type).
In the paraphrases of the firstquestion, we see more variations of the interroga-tive part of ATR questions.
For instance, 1c ex-plicitly refers to the source location/entity as ?store?and 1d uses ?place?.
Those words are essentiallyhyponyms/specializations of the concept ?location?.Paraphrases of the second question, on the otherhand, show variations in the sentence part of thequestions.
The expression ?same face?
in the origi-nal question is rephrased as ?one side?
(2a), ?sameside?
(2b), ?not .. other side?
(2c) and ?darkside?
(2f).
The verb is changed from ?show?
to?face?
(2b), ?see?
(2c, 2d) and ?look?
(2e).
Thoserephrasings are rather subtle, requiring deep seman-tic knowledge and inference beyond lexical seman-tics, that is, the common-sense knowledge.To see the kinds of rephrasing the web users en-tered, we categorized the 679 (= 714 - 35) para-phrased questions roughly into the following 6 cate-gories.3(1) Lexical substitution ?
synonyms; involves noor minimal sentence transformation(2) Passivization(3) Verb denominalization ?
e.g.
?destroy?
vs.?destruction?
(4) Lexical semantics & inference ?
e.g.
?show?vs.
?see?
(5) Interrogative reformation ?
variations in the in-terrogative part(6) Common-sense ?
e.g.
?dark side of the Moon?Table 1 shows the breakdown by those categories.As you see, interrogative transformation had the2In order to give a context to a question, we put a link(?wanna know the answer??)
to the actual Q&A pair in theFAQ file for each sample question.3If a paraphrase fell under two or more categories, the onewith the highest number was chosen.Table 1: Breakdown of the paraphrases by para-phrase categoryCategory # of paraphrases(1) Lexical substitution 168 (25 %)(2) Passivization 37 (5 %)(3) Verb denominalization 18 (3 %)(4) Lexical semantics & inference 107 (16 %)(5) Interrogative reformation 339 (50 %)(6) Common-sense 10 (1 %)Total 679 (100 %)largest proportion.
This was partly because all trans-formations to questions that start with ?What?
wereclassified as this category.
But the data indeed con-tained many instances of transformation betweendifferent interrogatives (why  how  where who etc.).
From the statistics above, we can thussee the importance of understanding the reformula-tions of the interrogatives.
As for other categories,lexical substitution had the next largest proportion.This means a fair number of users entered rela-tively simple transformations.
On this, (Lin and Pan-tel, 2001) makes a comment on manually generatedparaphrases (as versus automatically extracted para-phrases): ?It is difficult for humans to generate a di-verse list of paraphrases, given a starting formula-tion and no context?.
Our data is in agreement withtheir observations indeed.4 EvaluationUsing the paraphrase data described in the previoussection, we evaluated our question reformulationpatterns on coverage and in the paraphrase recogni-tion task.
From the data, we selected all paraphrasesderived from the original questions of type PRC,RSN and ATR.
There were 306 such examples, andthey constituted the testset for the evaluation.4.1 CoverageWe first applied the transformation patterns to all ex-amples in the testset and generated their case framerepresentations.
In the 306 examples, 289 of themfound at least one pattern.
If an example matchedwith two or more patterns, the one with the highestpriority was selected.
Thus the coverage was 94%.However after inspecting the results, we observedthat in some successful matches, the syntactic struc-ture of the question did not exactly correspond to1.
Where can I get British tea in the United States?
[ATR]a.
How can I locate some British tea in the United States?b.
Who sells English tea in the U.S.?c.
What stores carry British tea in the United States?d.
Where is the best place to find English tea in the U.S.?e.
Where exactly should I go to buy British tea in the U.S.?f.
How can an American find British tea?2.
Why does the Moon always show the same face to the Earth?
[RSN]a.
What is the reason why the Moon show only one side to the Earth?b.
Why is the same side of the Moon facing the Earth all the time?c.
How come we do not see the other side of the Moon from Earth?d.
Why do we always see the same side of the Moon?e.
Why do the Moon always look the same from here?f.
Why is there the dark side of Moon?Figure 4: Examples of question paraphrases entered by the web usersthe pattern as intended.
For example, ?How can Ilearn to drink less tea and coffee?
?4 matched thepattern (1) shown in Figure 2 and produced a framewhere ?I?
was the actor, ?learn?
was the verb andthe theme was null (because the shallow parser an-alyzed ?to drink less tea and coffee?
to be a verbmodifier).
Although the difficulty with this examplewas incurred by inadequate pre-processing or inher-ent difficulty in shallow parsing, the end result was aspurious match nonetheless.
In the 289 matches, 15of them were such false matches.As for the 17 examples which failed to matchwith any patterns, one example is ?What internet re-sources exist regarding copyright?
?5 ?
there werepatterns that matched the interrogative part (?Whatinternet resources?
), but all of them had constrainedvariables for the verb which did not match ?exist?.Other failed matches were because of elusive para-phrasing.
For example, for an original question?Why is evaporative emissions a problem?
?, webusers entered ?What?s up with evaporative emis-sions??
and ?What is wrong with evaporative emis-sions??.
Those paraphrases seem to be keyed offfrom ?problem?
rather than ?why?.4The original question for this paraphrase was ?How can Iget rid of a caffeine habit?
?.5This question can be paraphrased as ?Where can I find in-formation about copyright on the internet?
?4.2 Paraphrase RecognitionUsing the case frame representations derived fromthe first experiment, we applied a frame similaritymeasure for all pairs of frames.
This measure israther rudimentary, and we are planning to fine-tuneit in the future work.
This measure focuses on theeffect of paraphrase patterns ?
how much the canon-ical representations, after the variations of interrog-atives are factored out, can bring closer the (true)paraphrases (i.e., questions generated from the sameoriginal question), thereby possibly improving therecognition of paraphrases.The frame similarity between a pair of frames isdefined as a weighted sum of two similarity scores:one for the interrogative part (which we call inter-rogative similarity) and another for the sentence part(which we call case role similarity).
The interrog-ative similarity is obtained by computing the av-erage slot-wise correspondence of the empty cate-gories (slots whose value is ???
), where the corre-spondence value of a slot is 1 if both frames have???
for the slot or 0 otherwise.
The case role simi-larity, on the other hand, is obtained by computingthe distance between two term vectors, where termsare the union of words that appeared in the remain-ing slots (i.e., non-empty category slots) of the twoframes.
Those terms/words are considered as a bagof words (as in Information Retrieval), irrespectiveof the order or the slots in which they appeared.
We00.10.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1RejectionRecall FrSim_0.5FrSim_0.0SentFigure 5: Recall vs. Rejectionchose this scheme for the non-empty category slotsbecause our current work does not address the issueof paraphrases in the sentence part of the questions(as we mentioned earlier).
Value of each term in aframe is either 1 if the word is present in the frameor 0 otherwise, and the cosine of the two vectors isreturned as the distance.
The final frame similarityvalue, after applying weights which sum to 1, wouldbe between 0 and 1, where 1 indicates the strongestsimilarity.6Using the frame similarity measure, we computedtwo versions ?
one with 0.5 for the weight of the in-terrogative similarity and another with 0.0.
In addi-tion, we also computed a baseline metric, sentencesimilarity.
It was computed as the term vector simi-larity where terms in the vectors were taken from thephrase representation of the questions (i.e., syntacticphrases generated by the shallow parser).
Thus theterms here included various wh-interrogative wordsas well as words that were dropped or changed inthe paraphrase patterns (e.g.
words instantiated withmethodN in pattern (3) in Figure 2).
This metricproduces a value between 0 and 1, thus it is compa-rable to the frame similarity.The determination of whether or not two frames(or questions) are paraphrase of each other dependson the threshold value ?
if the similarity value isabove a certain threshold, the two frames/questionsare determined to be paraphrases.
With the 306 caseframes in the testset, there were a total of 46665 (=) distinct combinations of frames, and 38116If either one of the frames is null (for which the pattern-matching failed), the frame similarity is 0.of them were (true) paraphrases.
After computingthe three metrics (two versions of frame similarity,plus sentence similarity) for all pairs, we evaluatedtheir performance by examining the trade-off be-tween recall and rejection for varying threshold val-ues.
Recall is defined in the usual way, as the ratio oftrue positives # classified as paraphrase# true paraphrases , and re-jection is defined as the ratio of true negatives # classified as non-paraphrase# true non-paraphrases .
We chose to userejection instead of precision or accuracy becausethose measures are not normalized for the number ofinstances in the classification category (# true para-phrases vs. # true non-paraphrases); since our test-set had a skewed distribution (8% paraphrases, 92%non-paraphrases), those measures would have onlygiven scores in which the results for paraphrases wasovershadowed by those for non-paraphrases.Figure 5 shows the recall vs. rejection curves forthe three metrics.
As you see, both versions of theframe similarity (FrSim 0.5 and FrSim 0.0 in thefigure) outperformed the sentence similarity (Sent),suggesting that the use of semantic representationwas very effective in recognizing paraphrases com-pared to syntactic representation.
For example, Fr-Sim 0.5 correctly recognized 90% of the true para-phrases while making only a 10% error in recogniz-ing false positives, whereas Sent made a slightlyover 20% error in achieving the same 90% recalllevel.
This is a quite encouraging result.The figure also shows that FrSim 0.5 performedmuch better than FrSim 0.0.
This means that ex-plicit representation of empty categories (or questiontypes) contributed significantly to the paraphraserecognition.
This also underscores the importanceof considering the formulations of interrogatives inanalyzing question sentences.5 Conclusions and Future WorkIn this paper, we showed that automatic recogni-tion of question paraphrases can benefit from un-derstanding the various formulations of the interrog-ative part.
Our paraphrase patterns remove thosevariations and produce canonical forms which re-flect the meaning of the questions (i.e., case frames).Not only does this semantic representation facili-tates simple and straight-forward ways to computethe similarity of questions, it also produces more ac-curate results than syntactic phrase representation.Our immediate future work is to define paraphrasepatterns for other question types.
While doing so,we would also like to look into ways to automati-cally extract patterns.
A good starting point wouldbe (Agichtei et al, 2001), which looked for commonn-grams anchored at the beginning of questions.Once the syntactic superstructure of the interrog-ative part is factored out, the next task is to tacklereformulations of the sentence part of questions.Lately several interesting efforts have been made toextract paraphrase expressions automatically, for in-stance (Lin and Pantel, 2001; Shinyama et al, 2002).We would like to experiment doing the same withthe web as the resource.Finally, we would like to synthesize the reformu-lation patterns of the two parts of questions and de-velop unified paraphrase patterns.
Then we will in-corporate this new approach in FAQFinder and con-duct end-to-end question-answering experiments inorder to see how much the use of paraphrase patternscan improve the performance of the system.ReferencesE.
Agichtei, S. Lawrence, and L. Gravano.
2001.
Learn-ing search engine specific query transformations forquestion answering.
In Proceedings of the 10th In-ternational World Wide Web Conference (WWW10),Hong Kong.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In Proceedings of the DARPA Human Lan-guage Technologies (HLT-2003).R.
Burke, K. Hammond, V. Kulyukin, S. Lytinen, N. To-muro, and S. Schoenberg.
1997.
Question answeringfrom frequently asked question files: Experiences withthe faqfinder system.
AI Magazine, 18(2).U.
Hermjakob, E. Abdessamad, and D. Marcu.
2002.Natural language based reformulation resource andweb exploitation for question answering.
In Proceed-ings of TREC-2002.D.
Lin and P. Pantel.
2001.
Discovery of inference rulesfor question answering.
Natural Language Engineer-ing, 7(4):343?360.S.
Lytinen and N. Tomuro.
2002.
The use of questiontypes to match questions in faqfinder.
In Papers fromthe 2002 AAAI Spring Symposium on Mining Answersfrom Texts and Knowledge Bases.M.
Murata and H. Isahara.
2001.
Universal modelfor paraphrasing using transformation based on a de-fined criteria.
In Proceedings of the workshop on Au-tomatic Paraphrasing at NLP Pacific Rim (NLPRS-2001), Tokyo, Japan.B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedalignment of multiple translations: Extracting para-phrases and generating new sentences.
In Proceedingsof the DARPA Human Language Technologies (HLT-2003).R.
Schank.
1973.
Identification of conceptualiza-tions underlying natural language.
In R. Schank andK.
Colby, editors, Computer Models of Thought andLanguage.
Freeman.Y.
Shinyama, S. Sekine, K. Sudo, and R. Grishman.2002.
Automatic paraphrase acquisition from newsarticles.
In Proceedings of Human Language Technol-ogy Conference (HLT-2002).N.
Tomuro and S. Lytinen.
2001.
Selecting features forparaphrasing question sentences.
In Proceedings ofthe workshop on Automatic Paraphrasing at NLP Pa-cific Rim (NLPRS-2001), Tokyo, Japan.E.
Voorhees.
2000.
The trec-9 question answering trackreport.
In Proceedings of TREC-9.
