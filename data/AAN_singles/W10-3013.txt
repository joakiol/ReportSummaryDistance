Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 92?99,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHedge Detection and Scope Finding by Sequence Labelingwith Normalized Feature Selection?Shaodian Zhang12, Hai Zhao123?, Guodong Zhou3 and Bao-Liang Lu121Center for Brain-Like Computing and Machine IntelligenceDept of Computer Science and Engineering, Shanghai Jiao Tong University2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent SystemsShanghai Jiao Tong University3School of Computer Science and Technology, Soochow Universityzhangsd.sjtu@gmail.com, zhaohai@cs.sjtu.edu.cngdzhou@suda.edu.cn, blu@cs.sjtu.edu.cnAbstractThis paper presents a system which adoptsa standard sequence labeling technique forhedge detection and scope finding.
Forthe first task, hedge detection, we formu-late it as a hedge labeling problem, whilefor the second task, we use a two-step la-beling strategy, one for hedge cue label-ing and the other for scope finding.
In par-ticular, various kinds of syntactic featuresare systemically exploited and effectivelyintegrated using a large-scale normalizedfeature selection method.
Evaluation onthe CoNLL-2010 shared task shows thatour system achieves stable and competi-tive results for all the closed tasks.
Fur-thermore, post-deadline experiments showthat the performance can be much furtherimproved using a sufficient feature selec-tion.1 IntroductionHedges are linguistic devices representing spec-ulative parts of articles.
Previous works such as(Hyland, 1996; Marco and Mercer, 2004; Light etal., 2004; Thompson et al, 2008) present researchon hedge mainly as a linguistic phenomenon.Meanwhile, detecting hedges and their scopes au-tomatically are increasingly important tasks in nat-ural language processing and information extrac-tion, especially in biomedical community.
Theshared task of CoNLL-2010 described in Farkaset al (2010) aims at detecting hedges (task 1)and finding their scopes (task 2) for the literature?
This work is partially supported by the NationalNatural Science Foundation of China (Grants 60903119,60773090, 90820018 and 90920004), the National Basic Re-search Program of China (Grant No.
2009CB320901), andthe National High-Tech Research Program of China (GrantNo.2008AA02Z315).
?corresponding authorfrom BioScope corpus (Szarvas et al, 2008) andWikipedia.
This paper describes a system adopt-ing sequence labeling which performs competitivein the official evaluation, as well as further test.In addition, a large-scale feature selection proce-dure is applied in training and development.
Con-sidering that BioScope corpus is annotated by twoindependent linguists according to a formal guide-line (Szarvas, 2008), while Wikipedia weasels aretagged by netizens who are diverse in backgroundand various in evaluation criterion, it is needed tohandle them separately.
Our system selects fea-tures for Wikipedia and BioScope corpus indepen-dently and evaluate them respectively, leading tofine performances for all of them.The rest of the paper is organized as follows.The next section presents the technical details ofour system of hedge detection and scope finding.Section 3 gives information of features.
Section4 shows the evaluation results, including officialresults and further ones after official outputs col-lection.
Section 5 concludes the paper.2 MethodsBasically, the tasks are formulated as sequence la-beling in our approach.
The available label set dif-fers between task 1 and 2.
In addition, it is neededto introduce an indicator in order to find scopes forthe multi-hedge sentences properly.2.1 Hedge detectionThe valid label set of task 1, hedge detection, con-tains only two labels: ?Hedge?
and ?
?, whichrepresent that a word is in a hedge cue or notrespectively.
Since results of hedge detection inthis shared task are evaluated at sentence level, asentence will be classified as ?uncertain?
in thepost-process if it has one or more words labeled?Hedge?
in it and otherwise ?certain?.922.2 Scope findingThe second task is divided into two steps in oursystem.
The first step is quite the same as whatthe system does in task 1: labeling the words as inhedge cues or not.
Then the scope of each hedgewill be labeled by taking advantage of the resultof the first step.
A scope can be denoted by abeginning word and an ending word to representthe first and the last element.
In scope finding theavailable label set contains ?Begin?, ?End?, ?Mid-dle?
and ?
?, representing the first and last word inthe scope, in-scope and out-of-scope.
As an exam-ple, a sentence with hedge cue and scope labelingis given in Table 1.
Hedge cue ?indicating?
withits scope from ?indicating?
itself to ?transcription?are labeled.
While evaluating outputs, only ?Be-gin?s and ?End?s will be taken into considerationand be treated as the head and tail tokens of thescopes of specific hedge cues.Furthermore ..., ...inhibition ...can ...be ...blocked ...by ...actinomycin ...D ..., ...indicating ...
Hedge Begina ... Middlerequirement ... Middlefor ... Middlede ... Middlenovo ... Middletranscription ... End.
...Table 1: A sentence with hedge cue and scope la-belingIt seems that the best labeling result of task 1can be used directly to be the proper intermediaterepresentation of task 2.
However, the complexityof scope finding for multi-hedge sentences forcesus to modify the intermediate result of task 2 forthe sake of handling the sentences with more thanone hedge cue correctly.
Besides, since task 1 isa sentence classification task essentially, while thegoal of the first step of task 2 is to label the wordsas accurately as possible, it is easy to find thatthe optimal labeling results of task 1 may not beoptimal to be the intermediate representations fortask 2.
This problem can be solved if sentence-level hedge detection and intermediate representa-tion finding are treated as two separate tasks withindependent feature selection procedures.
The de-tails of feature selection will be given in section3.2.3 Scope finding for multi-hedge casesSentences with more than one hedge cue are quitecommon in both datasets of BioScope corpus andWikipedia.
By counting hedges in every sentence,we find that about one fourth of the sentences withhedges have more than one hedge cue in all threedata sources (Table 2).
In Morante and Daele-mans (2009), three classifiers predict whether eachtoken is Begin, End or None and a postprocess-ing is needed to associate Begins and Ends withtheir corresponding hedge cues.
In our approach,in order to decrease ambiguous or illegal outputse.g.
inequivalent numbers of Begins and Ends, apair of Begin and End without their correspond-ing hedge cue between them, etc., sentences withmore than one hedge cue will be preprocessed bymaking copies as many as the number of hedgesand be handled separately.The sentence which is selected as a sample hastwo hedge cues: ?suggesting?
and ?may?, so oursystem preprocesses the sentence into two single-hedge ones, which is illustrated in Table 3.
Now itcomes to the problem of finding scope for single-hedge sentence.
The two copies are labeled sep-arately, getting one scope from ?suggesting?
to?mitogenesis?
for the hedge cue ?suggesting?
andthe other from ?IFN-alpha?
to ?mitogenesis?
for?may?.
Merging the two results will give the finalscope resolution of the sentence.However, compared with matching Begins andEnds in postprocessing given by Morante andDaelemans (2009), the above method gives riseto out of control of projections of the scopes,i.e.
scopes of hedges may partially overlap aftercopies are merged.
Since scopes should be in-tact constituents of sentences, namely, subtrees insyntax tree which never partly overlap with eachother, results like this are linguistically illegal andshould be discarded.
We solve this problem by in-troducing an instructional feature called ?Indica-tor?.
For sentences with more than one hedge cue,namely more than one copy while finding scopes,words inside the union of existing (labeled) scopeswill be tagged as ?Indicator?
in unhandled copiesbefore every labeling.
For example, after findingscope for the first copy in Table 3 and words from93Dataset # Sentence # No-hedge ratio # One-hedge ratio # Multi-hedge ratioBiomedical Abstracts 11871 9770 82.3% 1603 13.5% 498 4.2%Biomedical Fulltexts 2670 2151 80.6% 385 14.4% 134 5.0%Wikipedia 11111 8627 77.6% 1936 17.4% 548 4.9%Table 2: Statistics of hedge amountIFN-alpha IFN-alphaalso alsosensitized sensitizedT Tcells cellsto toIL-2-induced IL-2-inducedproliferation proliferation, ,further furthersuggesting Hedge suggestingthat thatIFN-alpha IFN-alphamay may Hedgebe beinvolved involvedin inthe theregulation regulationof ofT-cell T-cellmitogenesis mitogenesis.
.Table 3: An example of 2-hedge sentence beforescope finding?suggesting?
to ?mitogenesis?
are put in the scopeof cue ?suggesting?, these words should be tagged?Indicator?
in the second copy, whose result is il-lustrated in Table 4.
If not in a scope, any word istagged ?
?
as the indicator.
The ?Indicator?s tag-ging from ?suggesting?
to ?mitogenesis?
in Table4 mean that no other than the situations of a) ?Be-gin?
is after or at ?suggesting?
and ?End?
is beforeor at ?mitogenesis?
b) Both ?Begin?
and ?End?
arebefore ?suggesting?
c) Both next ?Begin?
and next?End?
are after ?mitogenesis?
can be accepted.
Inother words, new labeling should keep the projec-tions of scopes in the result.
Although it is onlyan instructional indicator and does not have anycoerciveness, the evaluation result of experimentshows it effective.3 Feature selectionSince hedge and scope finding are quite noveltasks and it is not easy to determine the effectivefeatures by experience, a greedy feature selectionis conducted.
As it mentioned in section 2, oursystem divides scope finding into two sub-tasks:IFN-alpha ...also ...sensitized ...T ...cells ...to ...IL-2-induced ...proliferation ..., ...further ...suggesting ... Indicatorthat ... IndicatorIFN-alpha ... Indicator Beginmay ... Indicator Hedge Middlebe ... Indicator Middleinvolved ... Indicator Middlein ... Indicator Middlethe ... Indicator Middleregulation ... Indicator Middleof ... Indicator MiddleT-cell ... Indicator Middlemitogenesis ... Indicator End.
...Table 4: Scope resolution with instructional fea-ture: ?Indicator?a) Hedge cue labelingb) Scope labelingThe first one is the same as hedge detection taskin strategy, but quite distinct in target of featureset, because hedge detection is a task of sentenceclassification while the first step of scope find-ing aims at high accuracy of labeling hedge cues.Therefore, three independent procedures of fea-ture selection are conducted for BioScope corpusdataset.
AsWikipedia is not involved in the task ofscope finding, it only needs one final feature set.About 200 feature templates are initially con-sidered for each task.
We mainly borrow ideas andare enlightened by following sources while initial-izing feature template sets:a) Previous papers on hedge detection andscope finding (Light et al, 2004; Medlock,2008; Medlock and Briscoe, 2008; Kilicogluand Bergler, 2008; Szarvas, 2008; Ganterand Strube, 2009; Morante and Daelemans,2009);94b) Related works such as named entity recog-nition (Collins, 1999) and text chunking(Zhang et al, 2001);c) Some literature on dependency parsing(Nivre and Scholz, 2004; McDonald et al,2005; Nivre, 2009; Zhao et al, 2009c; Zhaoet al, 2009a);3.1 Notations of Feature TemplateA large amount of advanced syntactic features in-cluding syntactic connections, paths, families andtheir concatenations are introduced.
Many of thesefeatures come from dependency parsing, whichaims at building syntactic tree expressed by depen-dencies between words.
More details about de-pendency parsing are given in Nivre and Scholz(2004) and McDonald et al (2005).
The parserin Zhao et al (2009a) is used to construct de-pendency structures in our system, and some ofthe notations in this paper adopt those presentedin Zhao et al (2009c).
Feature templates are fromvarious combinations or integrations of the follow-ing basic elements.Word Property.
This part of features includesword form (form), lemma (lemma), part-of-speechtag (pos), syntactic dependency (dp) , syntactic de-pendency label (dprel).Syntactic Connection.
This includes syntactichead (h), left(right) farthest(nearest) child (lm, ln,rm and rn) and high (low) support verb, noun orpreposition.
Here we specify the last one as anexample, support verb(noun/preposition).
From agiven word to the syntactic root along the syntac-tic tree, the first verb/noun/preposition that is metis called its low support verb/noun/preposition,and the nearest one to the root(farthest tothe given word) is called as its high supportverb/noun/preposition.
The concept of supportverb was broadly used (Toutanova et al, 2005;Xue, 2006; Jiang and Ng, 2006), and it is extendedto nouns and prepositions in Zhao et al (2009b).In addition, a slightly modified syntactic head, pp-head, is introduced, it returns the left most siblingof a given word if the word is headed by a prepo-sition, otherwise it returns the original head.Path.
There are two basic types of path.
Oneis the linear path (linePath) in the sequence, theother is the path in the syntactic parsing tree (dp-Path).
For example, m:n|dpPath represents thedependency path from word m to n. Assumingthat the two paths from m and n to the root arepm and pn, m:n|dpPathShare, m:n|dpPathPredand m:n|dpPathArgu represent the common partof pm and pn, part of pm which does not belongto pn and part of pn which does not belong to pm,respectively.Family.
A children set includes all syntacticchildren(children) are used in the template nota-tions.Concatenation of Elements.
For all collectedelements according to dpPath, children and so on,we use three strategies to concatenate all thosestrings to produce the feature value.
The first isseq, which concatenates all collected strings with-out doing anything.
The second is bag, whichremoves all duplicated strings and sort the rest.The third is noDup, which removes all duplicatedneighbored strings.Hedge Cue Dictionary and Scope Indicator.Hedge cues in the training set are collected and putin a dictionary.
Whether a word in the training ortesting set is in the dictionary (dic) is introducedinto feature templates.
As the evaluation is non-open, we do not put in any additional hedge cuesfrom other resources.
An indicator (indicator) isgiven for multi-hedge scope finding, as specifiedin section 2.At last, in feature set for scope label-ing, hedge represents that the word is in a hedgecue.At last, we take x as current token to be labeled,and xm to denote neighbor words.
m > 0 repre-sents that it is a word goes mth after current wordand m < 0 for word ?mth before current word.3.2 Feature template sets for each taskAs optimal feature template subsets cannot be ex-pected to be extracted from so large sets by hand,greedy feature selections according to Zhao et al(2009b) are applied.
The normalized feature selec-tion has been proved to be effective in quite a lotof NLP tasks and can often successfully select anoptimal or very close to optimal feature set from alarge-scale superset.
Although usually it needs 3to 4 loops denoted by ?While?
in the Algorithm 1of Zhao et al (2009b) to get the best template set,we only complete one before official outputs col-lection because of time limitation, which to a largeextent hinders the performance of the system.Three template sets are selected for BioScopecorpus.
One with the highest accuracy forsentence-level hedge detection (Set B), one withthe best performance for word-level hedge cue la-95beling (Set H) and another one with the maximalF-score for scope finding (Set S).
In addition, oneset is discovered for sentence-level hedge detec-tion of Wikipedia (Set W)1 .
Table 52 lists someselected feature templates which are basic word orhedging properties for the three sets of BioScopecorpus and Wikipedia.
From the table we can seeit is clear that the combinations of lemma, POSand word form of words in context, which are usu-ally basic and common elements in NLP, are alsoeffective for hedge detection.
And as we expected,the feature that represents whether the word is inthe hedge list or not is very useful especially inhedge cue finding, indicating that methods basedon a hedge cue lists (Light et al, 2004) or keywordselection (Szarvas, 2008) are quite significant wayto accomplish such tasks.Some a little complicated syntactic featuresbased on dependencies are systemically exploitedas features for tasks.
Table 6 enumerates some ofthe syntactic features which proves to be highlyeffective.
We noticed that lowSupportNoun, high-SupportNoun and features derived from dpPath isnotably useful.
It can be explained by the aware-ness that hedge labeling and scope finding are toprocess literatures in the level of semantics wheresyntactic features are often helpful.We continue our feature selection proceduresfor BioScope corpus after official outputs collec-tion and obtain feature template sets that bring bet-ter performance.
Table 7 gives some of the fea-tures in the optimized sets for BioScope corpusresolution.
One difference between the new setsand the old ones is the former contain more syntac-tic elements, indicating that exploiting syntacticfeature is a correct choice.
Another difference isthe new sets assemble more information of wordsbefore or after the current word, especially wordslinearly far away but close in syntax tree.
Appear-ance of combination of these two factors such asx?1.lm.form seems to provide an evidence of theinsufficiency training and development of our sys-tem submitted to some extent.4 Evaluation resultsTwo tracks (closed and open challenges) are pro-vided for CoNLL-2010 shared task.
We partici-pated in the closed challenge, select features based1num in the set of Wikipedia represents the sequentialnumber of word in the sentence2Contact the authors to get the full feature lists, as well asentire optimized sets in post-deadline experiment- x.lemma + x1.lemma + x?1.lemma+ x.dic + x1.dic + x?1.dic- x.lemma + x1.pos + x?1.pos + x.pos+ x1.lemma + x?1.lemma- x.formSet B x.pos + x1.pos + x?1.pos + x2.pos+ x?2.pos- x.dic + x1.dic + x?1.dic- x1.pos- x.dic + x1.dic + x?1.dic + x2.dic+ x?2.dic- x.pos + x?1.pos- x.dicSet H x.dic + x.lemma + x.pos + x.form- x.pos + x1.pos + x?1.pos + x2.pos+ x?2.pos- x?2.form + x?2.lemma- x?1.form + x.form- x.dic + x1.dic + x?1.dic- x.dic + x1.dic + x?1.dic + x2.dic+ x?2.dic + x3.dic + x?3.dic- x.indicator- x.hedge + x1.hedge + x?1.hedgeSet S x.lemma + x1.pos + x?1.pos + x.pos+ x1.lemma + x?1.lemma- x.pos + x.hedge + x.dp + x.dprel- x1.pos- x.pos + x1.pos + x?1.pos + x2.pos+ x?2.pos- x.lemma + x1.lemma + x?1.lemma- + x.dic + x1.dic + x?1.dic- x.lemma + x1.lemma + x?1.lemma+x2.lemma + x?2.lemma + x.dic+ x1.dic + x?1.dic + x2.dic + x?2.dic- x.lemma + x1.lemmaSet W x.hedge + x1.hedge + x?1.hedge+ x2.hedge + x?2.hedge + x3.hedge+ x?3.hedge- x.pos + x1.pos + x?1.pos +x2.pos+ x?2.pos + x.dic + x1.dic + x?1.dic+ x2.dic + x?2.dic- x.pos + x.dic- x.num + x.dicTable 5: Selected feature template sets96- x.lowSupportNoun:x | dpPathArgu.dprel.seq- x.lowSupportNoun:x|dpPathArgu.dprel.seq+ x.lowSupportProp:x|dpPathArgu.dprel.seq- x.lowSupoortNoun.pos- x.pos + x.children.dprel.bag- x.rm.dprel + x.formSet B x.pphead.lemma- x.form + x.children.dprel.bag- x.lowSupportNoun:x?dpTreeRelation- x.lowSupportProp.lemma- x.form + x.children.dprel.noDup- x.highSupportNoun:x|dpTreeRelation + x.form- x.lowSupportVerb.form- x.lowSupportProp:x|dpPathShared.dprel.seq- x.lowSupportProp:x|dpPathShared.pos.seq- x.highSupportNoun.pos- x.highSupportNoun:x|dpTreeRelation- x.highSupportNoun:x|dpPathArgu.dprel.seqSet H + x.highSupportProp:x|dpPathArgu.dprel.seq- xlowSupportProp.lemma- x.rm.dprel- x.lm.form- x.lemma + x.pphead.form- x.lowSupportVerb.form- x.rm.lemma + x.rm.form- x.children.dprel.noDup- x.children.dprel.bag- x.highSupportNoun:x|dpTreeRelation- x.lemma + x.pphead.formSet S x.highSupportNoun:x|dpTreeRelation + x.form- x.lowSupportVerb.form- x.lowSupportVerb.lemma- x.h.children.dprel.bag- x.highSupportVerb.form- x.lm.form- x.lemma + x.pphead.form- x.lm.dprel + x.pos- x.lowSupportProp:x|dpPathPred.dprel.seq- x.pphead.lemmaSet W x.rm.lemma- x.lowSupportProp:x|dpTreeRelation- x.lowSupportVerb:x|dpPathPred.dprel.seq- x.lowSupportVerb:x|dpPathPred.pos.seq- x.lowSupportVerb:x|dpPathShared.pos.seq- x.lowSupportProp:x|dpPathShared.pos.seq- x.lowSupportProp.formTable 6: Syntactic features- x?1.lemma- x.dic + x1.dic + x?1.dic + x2.dic+ x?2.dic + x3.dic + x?3.dic- x?1.pos + x1.posSet H x.rm.lemma- x.rm.dprel- x.lm.dprel + x.pos- x.lowSupportNoun:x | dpPathArgu.dprel.seq- x.lowSupportNoun:x|dpPathArgu.dprel.seq+ x.lowSupportProp:x|dpPathArgu.dprel.seq- x?1.lemma- x.lemma + x1.lemma + x?1.lemma + x.dic+ x1.dic + x?1.dic- x.form + x.lemma + x.pos + x.dicSet B x?2.form + x?1.form- x.highSupportNoun:x|dpTreeRelation- x.highSupportNoun:x|dpPathArgu.dprel.seq- x.lowSupportProp:x|dpPathShared.dprel.seq- x?1.lm.form- x1.form- x.pos + x.dic- x.hedge + x1.hedge + x?1.hedge- x.pos + x1.pos + x?1.pos + x2.pos + x?2.posSet S x.children.dprel.bag- x.lemma + x.pphead.form- x.highSupportVerb.form- x.highSupportNoun:x|dpTreeRelation + x.form- x.lowSupportNoun:x|dpTreeRelation + x.formTable 7: Selected improved feature template setsfor BioScope corpuson the in-domain data and evaluated our systemon the in-domain and cross-domain evaluation set.All the experiments are implemented and run byMaximum Entropy Markov Models (McCallum,2000).4.1 Official resultsThe official results for tasks are in Table 8, inwhich three in-domain tests and cue matchingresult for biomedical texts are listed.
For thefirst task for BioCorpus, our system gives F-score0.8363 in in-domain test and for Wikipedia wegive F-score 0.5618 in closed evaluation.
For thesecond task, our system gives results in closed andopen test, with F-score 0.4425 and 0.4441 respec-tively.We compare the F-score of our system with thebest in the final result in Table 9.
We rank prettyhigh in Wikipedia hedge detection, while otherthree are quite steady but not prominent.
This ismainly due to two reasons:1.
Feature selection procedures are not perfectlyconducted.2.
Abstracts and fulltexts in BioScope are mixedto be the training set, which proves quite in-appropriate when the evaluation set contains97only fulltext literature, since abstract and full-text are quite different in terms of hedging.Dataset F-score BestTask1-closed 0.8363 0.8636BioScope Task2-closed 0.4425 0.5732Cue-matching 0.7853 0.8134Wikipedia Task1-closed 0.5618 0.6017Table 9: Comparing results with the best4.2 Further resultsIntact feature selection procedures for BioScopecorpus are conducted after official outputs collec-tions.
The results of evaluation with completelyselected features compared with the incompleteone are given in Table 7.
The system performs ahigher score on evaluation data (Table 10), whichis more competitive in both tasks on BioScope cor-pus.
The improvement for task 2 is significant, butthe increase of performance of hedge cue detec-tion is less remarkable.
We believe that a largerfulltext training set and a more considerate train-ing plan will help us to do better job in the futurework.Dataset Complete IncompleteTask1-closed 0.8522 0.8363BioScope Task2-closed 0.5151 0.4425Cue-matching 0.7990 0.7853Table 10: Comparing improved outputs with thebest5 ConclusionWe describe the system that uses sequence label-ing with normalized feature selection and rich fea-tures to detect hedges and find scopes for hedgecues.
Syntactic features which are derived fromdependencies are exploited, which prove to bequite favorable.
The evaluation results show thatour system is steady in performance and doespretty good hedging and scope finding in both Bio-Scope corpus and Wikipedia, especially when thefeature selection procedure is carefully and totallyconducted.
The results suggest that sequence la-beling and a feature-oriented method are effectivein such NLP tasks.ReferencesMichael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Viola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 173?176, Suntec, Singapore, 4,August.Ken Hyland.
1996.
Writing without conviction: Hedg-ing in science research articles.
Applied Linguistics,17:433?54.Zheng Ping Jiang and Hwee Tou Ng.
2006.
Semanticrole labeling of NomBank: A maximum entropy ap-proach.
In Proceedings of the EMNLP-2006, pages138?145, Sydney, Australia.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9.Marc Light, Xin Ying Qiu, and Padimini Srinivasan.2004.
The language of bioscience: Facts, specula-tions, and statements in between.
In Proc.
of theBioLINK 2004, pages 17?24.Chrysanne Di Marco and Robert E. Mercer.
2004.Hedging in scientific articles as a means of classify-ing citations.
In Working Notes of the AAAI SpringSymposium on Exploring Attitude and Affect in Text:Theories and Applications, pages 50?54.Andrew McCallum.
2000.
Maximum entropy markovmodels for information extraction and segmentation.In Proceedings of ICML 2000, pages 591?598, Stan-ford, California.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceed-ings of HLT/EMNLP 05, pages 523?530, Vancouver,Canada, October.Ben Medlock and Ted Briscoe.
2008.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of 45th Annual Meetingof the ACL, pages 992?999, Prague, Czech Repub-lic, June.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Infor-matics, 41:636?654.98Dataset TP FP FN precision recall F-scoreBioScope Task1-closed 669 141 121 0.8259 0.8468 0.8363Task2-closed 441 519 592 0.4594 0.4269 0.4425Cue-matching 788 172 259 0.8208 0.7526 0.7853Wikipedia Task1-closed 991 303 1243 0.7658 0.4436 0.5618Table 8: Official results of our submission for in-domain tasksRoser Morante andWalter Daelemans.
2009.
Learningthe scope of hedge cues in biomedical texts.
In Pro-ceedings of the Workshop on BioNLP, pages 28?36,Boulder, Colorado, June.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedingsof COLING-2004, pages 64?70, Geneva, Switzer-land, August 23rd-27th.Joakim Nivre.
2009.
Non-projective dependency pars-ing in expected linear time.
In Proceedings of ACL-IJCNLP 2009, pages 351?359, Suntec, Singapore,2-7 August.Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas, andJa?nos Csirik.
2008.
The BioScope corpus: anno-tation for negation, uncertainty and their scope inbiomedical texts.
In Proceedings of BioNLP 2008,pages 38?45, Columbus, Ohio, USA, June.Gyo?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of ACL-08, pages 281?289, Columbus, Ohio, USA, June.Paul Thompson, Giulia Venturi, John McNaught,Simonetta Montemagni, and Sophia Ananiadou.2008.
Categorising modality in biomedical texts.
InProc.
of the LREC 2008 Workshop on Building andEvaluating Resources for Biomedical Text Mining,pages 27?34, Marrakech.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL-2005, pages589?596, Ann Arbor, USA.Nianwen Xue.
2006.
Semantic role labeling of nom-inalized predicates in Chinese.
In Proceedings ofthe Human Language Technology Conference of theNAACL (NAACL-2006), pages 431?438, New YorkCity, USA, June.Tong Zhang, Fred Damerau, and David Johnson.
2001.Text chunking using regularized winnow.
In Pro-ceedings of the 39th Annual Meeting on Associa-tion for Computational Linguistics, pages 539?546,Toulouse, France.Hai Zhao, Wenliang Chen, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009a.
Multi-lingual dependency learning: Exploiting rich fea-tures for tagging syntactic and semantic dependen-cies.
In Proceedings of CoNLL-2009, June 4-5,Boulder, Colorado, USA.Hai Zhao, Wenliang Chen, and Chunyu Kit.
2009b.Semantic dependency parsing of NomBank andPropBank: An efficient integrated approach via alarge-scale feature selection.
In Proceedings ofEMNLP-2009, pages 30?39, Singapore.Hai Zhao, Wenliang Chen, Chunyu Kit, and GuodongZhou.
2009c.
Multilingual dependency learning:A huge feature engineering method to semantic de-pendency parsing.
In Proceedings of CoNLL-2009,June 4-5, Boulder, Colorado, USA.99
