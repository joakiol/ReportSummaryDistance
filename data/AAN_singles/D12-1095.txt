Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1038?1047, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsForest Reranking through Subtree RankingRicha?rd Farkas, Helmut SchmidInstitute for Natural Language ProcessingUniversity of Stuttgart{farkas,schmid}@ims.uni-stuttgart.deAbstractWe propose the subtree ranking approach toparse forest reranking which is a general-ization of current perceptron-based rerankingmethods.
For the training of the reranker,we extract competing local subtrees, hencethe training instances (candidate subtree sets)are very similar to those used during beam-search parsing.
This leads to better param-eter optimization.
Another chief advantageof the framework is that arbitrary learning torank methods can be applied.
We evaluatedour reranking approach on German and En-glish phrase structure parsing tasks and com-pared it to various state-of-the-art rerankingapproaches such as the perceptron-based for-est reranker.
The subtree ranking approachwith a Maximum Entropy model significantlyoutperformed the other approaches.1 IntroductionReranking has become a popular technique forsolving various structured prediction tasks, suchas phrase-structure (Collins, 2000) and depen-dency parsing (Hall, 2007), semantic role labeling(Toutanova et al 2008) and machine translation(Shen et al 2004).
The idea is to (re)rank candi-dates extracted by a base system exploiting a richfeature set and operating at a global (usually sen-tence) level.
Reranking achieved significant gainsover the base system in many tasks because it hasaccess to information/features which are not com-putable in the base system.
Reranking also outper-forms discriminative approaches which try to han-dle the entire candidate universe (cf.
Turian et al(2006)) because the base system effectively and ef-ficiently filters out many bad candidates and makesthe problem tractable.The standard approach for reranking is the n-bestlist ranking procedure, where the base system ex-tracts its top n global-level candidates with associ-ated goodness scores that define an initial ranking.Then the task is to rerank these candidates by us-ing a rich feature set.
The bottleneck of this ap-proach is the small number of candidates consid-ered.
Compared to n-best lists, packed parse forestsencode more candidates in a compact way.
For-est reranking methods have been proposed, whichcan exploit the richer set of candidates and theyhave been successfully applied for phrase-structure(Huang, 2008), dependency (Hayashi et al 2011)parsing and machine translation (Li and Khudanpur,2009) as well.Huang (2008) introduced the perceptron-basedforest reranking approach.
The core of the algo-rithm is a beam-search based decoder operating onthe packed forest in a bottom-up manner.
It followsthe assumption that the feature values of the wholestructure are the sum of the feature values of the lo-cal elements and they are designed to the usage ofthe perceptron update.
Under these assumptions a1-best Viterbi or beam-search decoder can be effi-ciently employed at parsing and training time.
Dur-ing training, it decodes the 1-best complete parsethen it makes the perceptron update against the or-acle parse, i.e.
the perceptron is trained at the global(sentence) level.We propose here a subtree ranker approachwhich can be regarded as a generalization of this for-1038est reranking procedure.
In contrast to updating ona single (sub)tree per sentence using only the 1-bestparse (perceptron-based forest reranking), the sub-tree ranker exploits subtrees of all sizes from a sen-tence and trains a (re)ranker utilising several deriva-tions of the constituent in question.
During parsingwe conduct a beam-search extraction by asking theranker to select the k best subtrees among the pos-sible candidates of each forest node.
The chief mo-tivation for this approach is that in this way, train-ing and prediction are carried out on similar localcandidate lists which we expect to be favorable tothe learning mechanism.
We empirically prove thatthe trained discriminative rankers benefit from hav-ing access to a larger amount of subtree candidates.Moreover, in this framework any kind of learningto rank methods can be chosen as ranker, includingpair-wise and list-wise classifiers (Li, 2011).The contributions of this paper are the following:?
We extend the perceptron-based forestrerankers to the subtree ranker forest rerankingframework which allows to replace the per-ceptron update by any kind of learning to rankprocedure.?
We report experimental results on Germanand English phrase-structure parsing compar-ing subtree rerankers to various other rerankersshowing a significant improvement over theperceptron-based forest reranker approach.2 Related WorkOur method is closely related to the work of Huang(2008), who introduced forest-based reranking forphrase structure parsing.
The proposed frame-work can be regarded as an extension of this ap-proach.
It has several advantages compared withthe perceptron-based forest reranker.
In this paperwe focus on the most important one ?
and brieflydiscuss two others in Section 5 ?
which is enablingthe use of any kind of learning to rank approaches.While the perceptron is fast to train, other machinelearning approaches usually outperform it.
Most ofthe existing learning to rank approaches are built onlinear models and evaluate the candidates indepen-dently of each other (such as MaxEnt (Charniak andJohnson, 2005), SVMRank (Joachims, 2002), Soft-Rank (Guiver and Snelson, 2008)).
Thus the choiceof the learning method does not influence parsingtime.
We believe that the real bottleneck of parsingapplications is parsing time and not training time.On the other hand, they can learn a better model(at the cost of higher training time) than the Per-ceptron.
In theory, we can imagine learning to rankapproaches which can not be reduced to the indi-vidual scoring of candidates at prediction time, forinstance a decision tree-based pairwise ranker.
Al-though such methods would also fit into the generalsubtree framework, they are not employed in prac-tice (Li, 2011).The subtree ranking approach is a generalizationof the perceptron-based approach.
If the rankingalgorithm is the Averaged Perceptron, the parsingalgorithm reduces to perceptron-based forest pars-ing.
If the ?selection strategy?
utilizes the base sys-tem ranking and training starts with a filtering stepwhich keeps only candidate sets from the root nodeof the forest we get the offline version of the trainingprocedure of the perceptron-based forest reranker ofHuang (2008).As our approach is based on local ranking (localupdate in the online learning literature), it is highlyrelated to early update which looks for the first lo-cal decision point where the oracle parse falls outfrom the beam.
Early update was introduced byCollins and Roark (2004) for incremental parsingand adopted to forest reranking by Wang and Zong(2011).Besides phrase structure parsing, the forestreranking approach was successfully applied for de-pendency parsing as well.
Hayashi et al(2011) in-troduced a procedure where the interpolation of agenerative and a forest-based discriminative parseris exploited.From the algorithmic point of view, our approachis probably most closely related to Searn (Daume?et al 2009) and Magerman (1995) as we also em-ploy a particular machine learned model for a se-quence of local decisions.
The topological order ofthe parse forest nodes can form the ?sequence ofchoices?
of Searn.
The biggest differences betweenour approach and Searn are that we propose an ap-proach employing beam search and the ?policy?
is aranker in our framework instead of a multiclass clas-sifier as there are no ?actions?
here, instead we haveto choose from candidate sets in the forest reranking1039framework.
In a wider sense, our approach can beregarded ?
like Searn ?
as an Inverse ReinforcementLearning approach where ?one is given an environ-ment and a set of trajectories and the problem is tofind a reward function such that an agent acting opti-mally with respect to the reward function would fol-low trajectories that match those in the training set?
(Neu and Szepesva?ri, 2009).
Neu and Szepesva?ri(2009) introduced the top-down parsing Markov De-cision Processes and experiment with several inversereinforcement learning methods.
The forest rerank-ing approaches are bottom-up parsers which wouldrequire a new (non-straightforward) definition of acorresponding Markov Decision Process.3 Subtree Ranking-based ForestRerankingA packed parse forest is a compact representationof possible parses for a given sentence.
A forest hasthe structure of a hypergraph, whose nodes V are theelementary units of the underlying structured predic-tion problem and the hyperedges E are the possibledeductive steps from the nodes.
In this paper weexperimented with phrase-structure parse reranking.In this framework nodes correspond to constituentsspanning a certain scope of the input sentence and ahyperedge e links a parent node head(e) to its chil-dren tails(e) (i.e.
a hyperedge is a CFG rule in con-text).The forest is extracted from the chart of a basePCFG parser, usually employing a heavy pruningstrategy.
Then the goal of a forest reranker is to findthe best parse of the input sentence exploiting a fea-ture representation of (sub)trees.We sketch the parsing procedure of the subtreeranker in Algorithm 1.
It is a bottom-up beam-search parser operating on the hypergraph.
At eachnode v we store the k best subtrees S(v) headed bythe node.
The S(v) lists contain the k top-rankedsubtrees by the ranker R among the candidates in thebeam.
The set of candidate subtrees at a node is theunion of the candidates at the different hyperedges.The set of candidate subtrees at a certain hyperedge,in turn, is formed by the Cartesian product ?S(vi)of the k-best subtrees stored at the child nodes vi.The final output of forest ranking is the 1-best sub-tree headed by the goal node S1(vgoal).Algorithm 1 Subtree RankingRequire: ?V,E?
forest, R rankerfor all v ?
V in bottom-up topological order doC ?
?for all e ?
E, head(e) = v doC ?
C ?
(?S(vi)) , vi ?
tails(e)end forS(v)?
Rk(C)end forreturn S1(vgoal)For training the ranker we propose to extract lo-cal candidate lists from the forests which share thecharacteristics of the candidates at parsing time.
Al-gorithm 2 depicts the training procedure of the sub-tree ranker.As forests sometimes do not contain the gold stan-dard tree, we extract an oracle tree instead, whichis the closest derivable tree in the forest to the goldstandard tree (Collins, 2000).
Then we optimize theparser for ranking the oracle tree at the top.
This pro-cedure is beneficial to training since the objective isa reachable state.
In Algorithm 2, we extract the ora-cle tree from the parses encoded in the forest ?V,E?ifor the ith training sentence, which is the tree withthe highest F-score when compared to the gold stan-dard tree yi.
For each of the training sentences wecalculate the oracle subtrees for each node {Ov} ofthe corresponding parse forest.
We follow the dy-namic programming approach of Huang (2008) forthe extraction of the forest oracle.
The goal of thisalgorithm is to extract the full oracle tree, but as aside product it calculates the best possible subtreefor all nodes including the nodes outside of the fulloracle tree as well.After computing the oracle subtrees, we crawlthe forests bottom-up and extract a training instance?C,Ov?
at each node v which consists of the candi-date set C and the oracle Ov at that node.
The cre-ation of candidate lists is exactly the same as it wasat parsing time.
Then we create training instancesfrom each of the candidate lists and form the set ofsubtrees S(v) which is stored for candidate extrac-tion at the higher levels of the forest (later steps inthe training instance extraction).A crucial design question is how to form the S(v)sets during training, which is the task of the selection1040Algorithm 2 Subtree Ranker TrainingRequire: {?V,E?i, yi}N1 , SS selection strategyT ?
?for all i?
1...N doO ?
oracle extractor(?V,E?i, yi)for all v ?
Vi in bottom-up topological orderdoC ?
?for all e ?
E, head(e) = v doC ?
C ?
(?S(vj)) , vj ?
tails(e)end forT ?
T ?
?C,Ov?S(v)?
SS(C,Ov)end forend forR?
train reranker(T )return Rstrategy SS.
One possible solution is to keep the kbest oracle subtrees, i.e.
the k subtrees closest to thegold standard parse, which is analogous to using thegold standard labels in Maximum Entropy MarkovModels for sequence labeling problems (we referthis selection strategy as ?oracle subtree?
later on).The problem with this solution is that if the rankershave been trained on the oracle subtrees potentiallyleads to a suboptimal performance as the outputs ofthe ranker at prediction time are noisy.
Note thatthis approach is not a classical beam-based decod-ing anymore as the ?beam?
is maintained accordingto the oracle parses and there is no model which in-fluences that.
An alternative solution ?
beam-baseddecoding ?
is to use a ranker model to extract theS(v) set in training time as well.
In the generalreranking approach, we assume that the ranking ofthe base parser is reliable.
So we store the k bestsubtrees according to the base system in S(v) (the?base system ranking?
selection strategy).
Note thatthe general framework keeps this question open andlets the implementations define a selection strategySS.After extracting the training instances T we cantrain an arbitrary ranker R offline.
Note that theextraction of candidate lists is exactly the same inAlgorithm 1 and 2 while the creation of Sv can bedifferent.4 ExperimentsWe carried out experiments on English and Germanphrase-structure reranking.
As evaluation metric, weused the standard evalb implementation of PAR-SEVAL on every sentence without length limitationand we start from raw sentences without gold stan-dard POS tagging.
As the grammatical functions ofconstituents are important from a downstream ap-plication point of view ?
especially in German ?
wealso report PARSEVAL scores on the conflation ofconstituent labels and grammatical functions.
Thesescores are shown in brackets in Table 2.4.1 DatasetsWe used the Wall Street Journal subcorpus of theOntonotes v4.0 corpus (Weischedel et al 2011)1 forEnglish.
As usual sections 2-21, 23 and 24 served astraining set (30,060 sentences), test set (1,640 sen-tences), and development set (1,336 sentences), re-spectively.
Using the Ontonotes version enables usto assess parser robustness.
To this end, we eval-uated our models also on the weblog subcorpus ofthe Ontonotes v4.0 corpus which consists of 15,103sentences.For German we used the Tiger treebank (Brantset al 2002).
We take the first 40,474 sentences ofthe Tiger treebank as training data, the next 5,000sentences as development data, and the last 5,000sentences as test data.4.2 Implementation of the Generic FrameworkWe investigate the Averaged Perceptron and a Maxi-mum Entropy ranker as the reranker R in the subtreeranking framework.
The Maximum Entropy rankermodel is optimized with a loss function which isthe negative log conditional likelihood of the ora-cle trees relative to the candidate sets.
In the case ofmultiple oracles we optimize for the sum of the ora-cle trees?
posterior probabilities (Charniak and John-son, 2005).In our setup the parsing algorithm is identicalto the perceptron-based forest reranker of Huang(2008) because both the Averaged Perceptron andthe Maximum Entropy rankers score the local sub-tree candidates independently of each other using1Note that it contains less sentences and a slightly modifiedannotation schema than the Penn Treebank.1041a linear model.
There is no need to compute theglobal normalization constant of the Maximum En-tropy model because we only need the ranking andnot the probabilities.
Hence the difference is in howto train the ranker model.We experimented with both the ?oracle subtree?and the ?base system ranking?
selection strategies(see Section 3).4.3 Five Methods for Forest-based RerankingWe conducted comparative experiments employingthe proposed subtree ranking approach and state-of-the-art methods for forest reranking.
Note that theyare equivalent in parsing time as each of them usesbeam-search with a linear classifier, on the otherhand they are radically different in their training.?
The original perceptron-based forest rerankerof Huang (2008) (?perceptron with global train-ing?).?
The same method employing the early-updateupdating mechanism instead of the global up-date.
Wang and Zong (2011) reported a signif-icant gain using this update over the standardglobal update (?perceptron with early update?).?
Similar to learning a perceptron at the globallevel and then applying it at local decisions,we can train a Maximum Entropy ranker at theglobal level utilizing the n-best full parse can-didates of the base parser, then use this modelfor local decision making.
So we train thestandard n-best rerankers (Charniak and John-son, 2005) and then apply them in the beam-search-based Viterbi parser (?n-best list train-ing?).
Applying the feature weights adjusted inthis approach in the forest-based decoding out-performs the standard n-best list decoding byan F-score of 0.3 on the German dataset.?
The subtree ranker method using the AveragedPerceptron reranker.
This is different from the?perceptron with global training?
as we conductupdates at every local decision point and we dooffline training (?subtree ranking by AvgPer?).?
The subtree ranker method using MaximumEntropy training (?subtree ranking by Max-Ent?
).We (re)implemented these methods and used thesame forests and the same feature sets for the com-parative experiments.4.4 Implementation DetailsWe used the first-stage PCFG parser of Charniakand Johnson (2005) for English and BitPar (Schmid,2004) for German.
BitPar employs a grammar engi-neered for German (for details please refer to Farkaset al(2011)).
These two parsers are state-of-the-artPCFG parsers for English and German, respectively.For German the base parser and the reranker oper-ate on the conflation of constituent labels and gram-matical functions.
For English, we used the forestextraction and pruning code of Huang (2008).
Thepruning removes hyperedges where the differencebetween the cost of the best derivation using this hy-peredge and the cost of the globally best derivationis above some threshold.
For German, we used thepruned parse forest of Bitpar (Schmid, 2004).
Af-ter computing the posterior probability of each hy-peredge given the input sentence, Bitpar prunes theparse forest by deleting hyperedges whose posteriorprobability is below some threshold.
(We used thethreshold 0.01).We employed an Averaged Perceptron (for ?per-ceptron with global training?, ?perceptron with earlyupdate?
and ?subtree ranking by AvgPer?)
and aMaximum Entropy reranker (for ?subtree rankingby MaxEnt?
and ?n-best list training?).
For the per-ceptron reranker, we used the Joshua implementa-tion2.
The optimal number of iterations was deter-mined on the development set.
For the MaximumEntropy reranker we used the RankMaxEnt imple-mentation of the Mallet package (McCallum, 2002)modified to use the objective function of Charniakand Johnson (2005) and we optimized the L2 regu-larizer coefficient on the development set.The beam-size were set to 15 (the value suggestedby Huang (2008)) during parsing and the trainingof the ?perceptron with global training?
and ?percep-tron with early update?
models.
We used k = 3 fortraining the ?subtree ranking by AvgPer?
and ?sub-tree ranking by MaxEnt?
rankers (see Section 5 fora discussion on this).In the English experiments, we followed (Huang,2http://joshua.sourceforge.net/Joshua/1042Tiger test WSJ dev WSJ test WBbase system (1-best) 76.84 (65.91) 89.29 88.63 81.86oracle tree 90.66 (80.38) 97.31 97.30 94.18Table 1: The lower and upper bounds for rerankers on the four evaluation datasets.
The numbers in brackets refers toevaluation with grammatical function labels on the German dataset.Tiger test WSJ dev WSJ test WBperceptron with global training 78.39 (67.79) 90.58 89.60 82.87perceptron with early update 78.83 (68.05) 90.81?
90.01 83.03?n-best list training 78.75 (68.04) 90.89 90.11 83.55subtree ranking by AvgPer 78.54?
(67.97?)
90.65?
89.97 83.04?subtree ranking by MaxEnt 79.36 (68.72) 91.14 90.32 83.83Table 2: The results achieved by various forest rerankers.
The difference between the scores marked by ?
and the?perceptron with global training?
were not statistically significant with p < 0.005 according to the the McNemar test.All other results are statistically different from this baseline.2008) and selectively re-implemented feature tem-plates from (Collins, 2000) and Charniak and John-son (2005).
For German we re-implemented thefeature templates of Versley and Rehbein (2009)which is the state-of-the-art feature set for German.It consists of features constructed from the lexical-ized parse tree and its typed dependencies alongwith features based on external statistical informa-tion (such as the clustering of unknown words ac-cording to their context of occurrence and PP attach-ment statistics gathered from the automatically POStagged DE-WaC corpus, a 1.7G words sample of theGerman-language WWW).
We filtered out rare fea-tures which occurred in less than 10 forests (we usedthe same non-tuned threshold for the English andGerman training sets as well).We also re-implemented the oracle extraction pro-cedure of Huang (2008) and extended its convolu-tion and translation operators for using the base sys-tem score as tie breaker.4.5 ResultsTable 1 shows the results of the 1-best parse of thebase system and the oracle scores ?
i.e.
the lowerand upper bounds for the rerankers ?
for the fourevaluation datasets used in our experiments.
TheGerman and the weblog datasets are more difficultfor the parsers.The following table summarizes the characteris-tics of the subtree ranker?s training sample of theGerman and English datasets by employing the ?or-acle subtree?
selection strategy:Tiger train WSJ train#candidate lists 266,808 1,431,058avg.
size of cand.
lists 3.2 5.7#features before filtering 2,683,552 22,164,931#features after filtering 94,164 858,610Table 3: The sizes of the subtree ranker training datasetsat k = 3.Using this selection strategy the training datasetis smaller than the training dataset of the n-best listrankers ?
where offline trainers are employed as well?
as the total number of candidates is similar (andeven less in the Tiger corpus) while there are fewerfiring features at the subtrees than at full trees.Table 2 summarizes the results achieved by vari-ous forest rerankers.
Both subtree rankers used theoracle subtrees as the selection strategy of Algo-rithm 2.
The ?subtree ranking by MaxEnt?
methodsignificantly outperformed the perceptron-based for-est reranking algorithms at each of the datasets andseems to be more robust as its advantage on the out-domain data ?WB?
is higher compared with the in-domain ?WSJ?
datasets.
The early update improvesthe perceptron based forest rerankers which is in linewith the results reported by Wang and Zong (2011).The ?n-best list training?
method works surprisinglywell.
It outperforms both perceptron-based forest1043rerankers on the English datasets (while achieving asmaller F-score than the perceptron with early up-date on the Tiger corpus) which demonstrates thepotential of utilizing larger candidate lists for dis-criminative training of rerankers.
The comparison ofthe ?subtree ranking by AvgPer?
row and the ?subtreeranking by MaxEnt?
row shows a clear advantage ofthe Maximum Entropy training mechanism over theAveraged Perceptron.Besides the ?oracle subtree?
selection strategy wealso experimented with the ?base system ranking?selection strategy with subtree Maximum Entropyranker.
Table 4 compares the accuracies of the twostrategies.
The difference between the two strate-gies varies among datasets.
In the German dataset,they are competitive and the prediction of grammati-cal functions benefits from the ?base system ranking?strategy, while it performs considerably worse at theEnglish datasets.Tiger test WSJ test WBoracle SS 79.36 (68.72) 90.32 83.83base sys SS 79.34 (68.84) 89.97 83.34Table 4: The results of the two selection strategies.
Usingthe oracle trees proved to be better on each of the datasets.Extracting candidate lists from each of the localdecision points might seem to be redundant.
To gainsome insight into this question, we investigated theeffect of training instance filtering strategies on theTiger treebank.
We removed the training instancesfrom the training sample T where the F-score ofthe oracle (sub)tree against the gold standard tree isless than a certain threshold (this data selection pro-cedure was inspired by Li and Khudanpur (2008)).The idea behind this data selection is to eliminatebad training examples which might push the learnerinto the wrong direction.
Figure 1 depicts the resultson the Tiger treebank as a function of this data se-lection threshold.With this data selection strategy we could furthergain 0.22 F-score percentage points achieving 79.58(68.87) and we can conclude that omitting candidatesets far from the gold-standard tree helps training.Figure 1 also shows that too strict filtering hurts theperformance.
The result with threshold=90 is worsethan the result without filtering.
We should notethat similar data selection methods can be applied0 20 40 60 8079.2579.3579.4579.55data filtering threshold (F?score)PARSEVAL F?scoreFigure 1: The effect of data selection on the Tiger test set.to each of the baseline systems and the comparisonto them would be fair with conducting that.
Thuswe consider our results without data selection to befinal.5 DiscussionWe experimentally showed in the previous sectionthat the subtree forest reranking approach with Max-imum Entropy models significantly outperforms theperceptron-based forest reranking approach.
Thisimprovement must be the result of differences in thetraining algorithms because there is no differencebetween the two approaches at parse time, as we dis-cussed in Section 4.2.There are two sources of these improvements.
(i) We use local subtrees as training instances in-stead of using the global parses exclusively.
Themost important difference between the training ofthe perceptron-based forest reranker and the subtreeforest reranker is that we train on subtrees (extractcandidate sets) outside of the Viterbi parses as well,i.e.
our intuition is that the training of the discrimi-native model can benefit from seeing good and badsubtrees far from the best parses as well.
(ii) Thesubtree ranker framework enables us to employ theMaximum Entropy ranker on multiple candidates,which usually outperforms the Averaged Perceptron.The results of Table 2 can be considered as twopaths from the ?perceptron with global training?to the ?subtree ranking by MaxEnt?
applying these1044sources of improvements.
If we use (i) and stay withthe Averaged Perceptron as learning algorithm weget ?subtree ranking by AvgPer?.
If we additionallyreplace the Averaged Perceptron by Maximum En-tropy ?
i.e.
follow (ii) ?
we arrive at ?subtree rankingby MaxEnt?.
On the other hand, the ?n-best training?uses global trees and Maximum Entropy for train-ing, so the reason of the difference between ?per-ceptron with global training?
and ?n-best training?
is(ii).
Then we arrive at ?subtree ranking by MaxEnt?by (i).
This line of thoughts and the figures of Ta-ble 2 indicate that the added value of (i) and (ii) aresimilar in magnitude.5.1 Error AnalysisFor understanding the added value of the proposedsubtree ranking method, we manually investigatedsentences from the German development set andcompared the parses of the ?perceptron with globaltraining?
with the ?subtree ranking by MaxEnt?.
Wecould not found any linguistic phenomena whichwas handled clearly better by the subtree ranker3,but it made considerably more fixes than errors inthe following cases:?
the attachment of adverbs,?
the unary branching verbal phrases and?
extremely short sentences which does not con-tain any verb (fragments).5.2 Novel Opportunities with the SubtreeRanking FrameworkA generalization issue of the subtree ranking ap-proach is that it allows to use any kind of featurerepresentation and arbitrary aggregation of localfeatures.
The basic assumption of training on theglobal (sentence) level in the perceptron rerankingframework is that the feature vector of a subtree isthe sum of the feature vectors of the children andthe features extracted from the root of the subtreein question.
This decomposability assumption pro-vides a fine framework in the case of binary featureswhich fire if a certain linguistic phenomenon occurs.On the other hand, this is not straightforward in the3We believe that this might be the case only if we wouldintroduce new information (e.g.
features) for the system.presence of real valued features.
For example, Ver-sley and Rehbein (2009) introduce real-valued fea-tures for supporting German PP-attachment recogni-tion ?
the mutual information of noun and preposi-tion co-occurrence estimated from a huge unlabeledcorpus ?
and this single feature template (about 80features) could achieve a gain of 1 point in phrasestructure parsing accuracy while the same improve-ment can be achieved by several feature templatesand millions of binary features.
The aggregation ofsuch feature values can be different from summing,for instance the semantics of the feature can demandaveraging, minimum, maximum or introducing newfeatures etc.
Another opportunity for extending cur-rent approaches is to employ utility functions on topof the sum of the binary feature values.
Each of theseextensions fits into the proposed framework.The subtree ranking framework also enables theusage of different models at different kinds ofnodes.
For example, different models can be trainedfor ranking subtress headed by noun phrases and forverb phrases.
This is not feasible in the perceptron-based forest ranker which sums up features and up-dates feature weights at the sentence level while theranker R in Algorithm 2 can refer to several modelsbecause we handle local decisions separately.
Thisapproach would not hurt parsing speed as one par-ticular model is asked at each node, but it multipliesmemory requirements.
This is an approach whichthe subtree ranking framework allows, but whichwould not fit to the global level updates of the per-ceptron forest rerankers.As a first step in this direction of research we ex-perimented with training three different MaximumEntropy models using the same feature representa-tion, the first only on candidate lists extracted fromnoun phrase nodes, the second on verb phrase nodesand the third on all nodes (i.e.
the third model isequivalent to the ?subtree MaxEnt?
model).
Then atprediction time, we ask that model (out of the three)which is responsible for ranking the candidates ofthe current type of node.
This approach performedworse than the single model approach achieving anF-scores of 79.24 (68.46) on the Tiger test dataset.This negative results ?
compared with 79.36 (68.72)achieved by a single model ?
is probably due to datasparsity problems.
The amount of training samplesfor noun phrases is 6% of the full training sample1045and it seems that a better model can be learned froma much bigger but more heterogeneous dataset.5.3 On the Efficiency of Subtree RankingIn subtree ranking, we extract a larger numberof training instances (candidate lists) than theperceptron-based approach which extracts exactlyone instance from a sentence.
Moreover, the can-didate lists are longer than the perceptron-based ap-proach (where 2 ?candidates?
are compared againsteach other).
Training on this larger set (refer Table 3for concrete figures) consumes more space and time.In our implementation, we keep the whole train-ing dataset in the memory.
With this implementationthe whole training process (feature extraction, can-didate extraction and training the Maximum Entropyranker) takes 3 hours and uses 10GB of memory atk = 1 and it takes 20 hours and uses 60GB of mem-ory at k = 3 ((Huang, 2008) reported 5.3 and 27.3hours at beam-sizes of 1 and 15 respectively but itused only 1.2GB of memory).
The in-depth investi-gation of the effect of k is among our future plans.6 ConclusionsWe presented a subtree ranking approach to parseforest reranking, which is a generalization of currentreranking methods.
The main advantages of our ap-proach are: (i) The candidate lists used during train-ing are very similar to those used during parsing,which leads to better parameter optimization.
(ii)Arbitrary ranking methods can be applied in our ap-proach.
(iii) The reranking models need not to bedecomposable.We evaluated our parse reranking approach onGerman and English phrase structure parsing tasksand compared it to various state-of-the-art rerank-ing approaches such as the perceptron-based for-est reranker (Huang, 2008).
The subtree rerankingapproach with a Maximum Entropy model signifi-cantly outperformed the other approaches.We conjecture two reasons for this result: (i) Bytraining on all subtrees instead of Viterbi parses orn-best parses only, we use the available trainingdata more effectively.
(ii) The subtree ranker frame-work allows us to use a standard Maximum Entropylearner in parse-forest training instead of the Percep-tron, which is usually superior.AcknowledgementsWe thank Liang Huang to provide us the modi-fied version of the Charniak parser, which output apacked forest for each sentence along with his forestpruning code.This work was founded by the DeutscheForschungsgemeinschaft grant SFB 732, project D4.1046ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories, pages 24?41.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 173?180.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages 111?118.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of the Seven-teenth International Conference onMachine Learning,ICML ?00, pages 175?182.Hal Daume?, III, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75(3):297?325, June.Richa`rd Farkas, Bernd Bohnet, and Helmut Schmid.2011.
Features for phrase-structure reranking fromdependency parses.
In Proceedings of the 12th Inter-national Conference on Parsing Technologies, pages209?214.John Guiver and Edward Snelson.
2008.
Learning torank with softrank and gaussian processes.
In Pro-ceedings of the 31st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, SIGIR ?08, pages 259?266.Keith Hall.
2007.
K-best spanning tree parsing.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 392?399, June.Katsuhiko Hayashi, Taro Watanabe, Masayuki Asahara,and Yuji Matsumoto.
2011.
Third-order varia-tional reranking on packed-shared dependency forests.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1479?1488.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-08: HLT, pages 586?594.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In ACM SIGKDD Conference onKnowledge Discovery and Data Mining (KDD), pages133?142.Zhifei Li and Sanjeev Khudanpur.
2008.
Large-scalediscriminative n-gram language models for statisticalmachine translation.
In Proceedings of the 8th AMTAconference, pages 133?142.Z.
Li and S. Khudanpur, 2009.
GALE book chapter on?MT From Text?, chapter Forest reranking for machinetranslation with the perceptron algorithm.Hang Li.
2011.
Learning to Rank for Information Re-trieval and Natural Language Processing.
SynthesisLectures on Human Language Technologies.
Morgan& Claypool Publishers.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguis-tics, pages 276?283, June.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Gergely Neu and Csaba Szepesva?ri.
2009.
Trainingparsers by inverse reinforcement learning.
MachineLearning, 77(2?3):303?337.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProceedings of Coling 2004, pages 162?168.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages177?184.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34(2):161?191.Joseph P. Turian, Benjamin Wellington, and I. DanMelamed.
2006.
Scalable discriminative learning fornatural language parsing and translation.
In NIPS,pages 1409?1416.Yannick Versley and Ines Rehbein.
2009.
Scalable dis-criminative parsing for german.
In Proceedings of the11th International Conference on Parsing Technolo-gies (IWPT?09), pages 134?137.Zhiguo Wang and Chengqing Zong.
2011.
Parse rerank-ing based on higher-order lexical dependencies.
InProceedings of 5th International Joint Conference onNatural Language Processing, pages 1251?1259.Ralph Weischedel, Eduard Hovy, Martha Palmer, MitchMarcus, Robert Belvin, Sameer Pradhan, LanceRamshaw, and Nianwen Xue, 2011.
Handbook of Nat-ural Language Processing and Machine Translation.,chapter OntoNotes: A Large Training Corpus for En-hanced Processing.1047
