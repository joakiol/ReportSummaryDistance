Aggregate and mixed-order Markov models forstatistical language processingLawrence Saul  and Fernando  Pere i ra{Isaul, pereira}?research, att.
comAT~T Labs - Research180 Park  Ave,  D-130F lo rham Park ,  N J  07932Abst rac tWe consider the use of language modelswhose size and accuracy are intermedi-ate between different order n-gram models.Two types of models are studied in partic-ular.
Aggregate Markov models are class-based bigram models in which the map-ping from words to classes is probabilis-tic.
Mixed-order Markov models combinebigram models whose predictions are con-ditioned on different words.
Both typesof models are trained by Expectation-Maximization (EM) algorithms for maxi-mum likelihood estimation.
We examinesmoothing procedures in which these mod-els are interposed between different ordern-grams.
This is found to significantly re-duce the perplexity of unseen word combi-nations.1 In t roduct ionThe purpose of a statistical language model is to as-sign high probabilities to likely word sequences andlow probabilities to unlikely ones.
The challengehere arises from the combinatorially arge numberof possibilities, only a fraction of which can ever beobserved.
In general, language models must learnto recognize word sequences that are functionallysimilar but lexically distinct.
The learning problem,one of generalizing from sparse data, is particularlyacute for large-sized vocabularies (Jelinek, Mercer,and Roukos, 1992).The simplest models of natural language are n-gram Markov models.
In these models, the prob-ability of each word depends on the n -  1 wordsthat precede it.
The problems in estimating ro-bust models of this form are well-documented.
Thenumber of parameters--or transition probabilities--scales as V n, where V is the vocabulary size.
Fortypical models (e.g., n = 3, V = 104), this num-ber exceeds by many orders of magnitude the totalnumber of words in any feasible training corpus.The transition probabilities in n-gram models areestimated from the counts of word combinations inthe training corpus.
Maximum likelihood (ML) esti-mation leads to zero-valued probabilities for unseenn-grams.
In practice, one adjusts or smoothes (Chenand Goodman, 1996) the ML estimates o thatthe language model can generalize to new phrases.Smoothing can be done in many ways--for example,by introducing artificial counts, backing off to lower-order models (Katz, 1987), or combining models byinterpolation (Jelinek and Mercer, 1980).Often a great deal of information:is lost in thesmoothing procedure.
This is due to the great dis-crepancy between -gram models of different order.The goal of this paper is to investigate models thatare intermediate, in both size and accuracy, betweendifferent order n-gram models.
We show that suchmodels can "intervene" between different order n-grams in the smoothing procedure.
Experimentally,we find that this significantly reduces the perplexityof unseen word combinations.The language models in this paper were evalu-ated on the ARPA North American Business News(NAB) corpus.
All our experiments used a vo-cabulary of sixty-thousand words, including tokensfor punctuation, sentence boundaries, and an un-known word token standing for all out-of-vocabularywords.
The training data consisted of approxi-mately 78 million words (three million sentences);the test data, 13 million words (one-half millionsentences).
All sentences were drawn randomlywithout replacement from the NAB corpus.
Allperplexity figures given in the paper are com-puted by combining sentence probabilities; the prob-ability of sentence wow1 .
.
.w~wn+l is given byyIn+lP(wilwo ..wi-1), where w0 and wn+l are i=1the start- and end-of-sentence markers, respectively.Though not reported below, we also confirmed thatthe results did not vary significantly for different ran-domly drawn test sets of the same size.The organization of this paper is as follows.In Section 2, we examine aggregate Markov mod-els, or class-based bigram models (Brown et al,1992) in which the mapping from words to classes81is probabilistic.
We describe an iterative algo-rithm for discovering "soft" word classes, based onthe Expectation-Maximization (EM) procedure formaximum likelihood estimation (Dempster, Laird,and Rubin, 1977).
Several features make this algo-rithm attractive for large-vocabulary language mod-eling: it has no tuning parameters, converges mono-tonically in the log-likelihood, and handles proba-bilistic constraints in a natural way.
The numberof classes, C, can be small or large depending onthe constraints of the modeler.
Varying the numberof classes leads to models that are intermediate be-tween unigram (C = 1) and bigram (C = V) models.In Section 3, we examine another sort of "inter-mediate" model, one that arises from combinationsof non-adjacent words.
Language models using suchcombinations have been proposed by Huang et al(1993), Ney, Essen, and Kneser (1994), and Rosen-feld (1996), among others.
We consider specificallythe skip-k transition matrices, M(wt_k, wt), whosepredictions are conditioned on the kth previous wordin the sentence.
(The value of k determines howmany words one "skips" back to make the predic-tion.)
These predictions, conditioned on only a sin-gle previous word in the sentence, are inherentlyweaker than those conditioned on all k previouswords.
Nevertheless, by combining several predic-tions of this form (for different values of k), we cancreate a model that is intermediate in size and ac-curacy between bigram and trigram models.Mixed-order Markov models express the predic-tions P(wt\[wt-1, wt-2,..., Wt-m) as a convex com-bination of skip-k transition matrices, M(wt-k, wt).We derive an EM algorithm to learn the mixing co-efficients, as well as the elements of the transitionmatrices.
The number of transition probabilities inthese models scales as mV 2, as opposed to V m+l.Mixed-order models are not as powerful as trigrammodels, but they can make much stronger predic-tions than bigram models.
The reason is that quiteoften the immediately preceding word has less pre-dictive value than earlier words in the same sentence.In Section 4, we use aggregate and mixed-ordermodels to improve the probability estimates fromn-grams.
This is done by interposing these modelsbetween different order n-grams in the smoothingprocedure.
We compare our results to a baseline tri-gram model that backs off to bigram and unigrammodels.
The use of "intermediate" models is foundto reduce the perplexity of unseen word combina-tions by over 50%.In Section 5, we discuss some extensions to thesemodels and some open problems for future research.We conclude that aggregate and mixed-order modelsprovide a compelling alternative to language modelsbased exclusively on n-grams.822 Aggregate  Markov  mode lsIn this section we consider how to construct class-based bigram models (Brown et al, 1992).
Theproblem is naturally formulated as one of hiddenvariable density estimation.
Let P(clwl ) denote theprobability that word wl is mapped into class c.Likewise, let P(w21c) denote the probability thatwords in class c are followed by the word w2.
Theclass-based bigram model predicts that word wl isfollowed by word w2 with probabilitycP(w21wl) = Z P(w21c)P(clwx)' (1)c=lwhere C is the total number of classes.
The hiddenvariable in this problem is the class label c, whichis unknown for each word wl.
Note that eq.
(1)represents the V 2 elements of the transition matrixP(w21wa) in terms of the 2CV elements of P(w2\]c)and P(clwl ).The Expectation-Maximization (EM) algorithm(Dempster, Laird, and Rubin, 1977) is an iterativeprocedure for estimating the parameters of hiddenvariable models.
Each iteration consists of two steps:an E-step which computes tatistics over the hiddenvariables, and an M-step which updates the param-eters to reflect these statistics.The EM algorithm for aggregate Markov modelsis particularly simple.
The E-step is to compute, foreach bigram WlW 2 in the training set, the posteriorprobabilityP(w2\]c)P(C\[Wl) (2)P(ClWl, w2) = ~c, P(w2lc')P(c'lwl)"Eq.
(2) gives the probability that word wl was as-signed to class c, based on the observation that itwas followed by word w2.
The M-step uses theseposterior probabilities to re-estimate the model pa-rameters.
The updates for aggregate Markov modelsare:~w N(wl, w)P(ClWl, w)P(clwl) ~ ~wc, N(wl ' , , w)P(c \[wl, w) (3)Ew N(w, w2)P(clw, w~)P(w2\[c) ~- Eww'g(w,w')P(clw, ')' (4)where N(Wl, w2) denotes the number of counts ofwlw2 in the training set.
These updates are guar-anteed to increase the overall og-likelihood,g= Z N(Wl'W2)lnP(w21wl)' (5)WlW2at each iteration.
In general, they converge to a local(though not global) maximum of the log-likelihood.The perplexity V* is related to the log-likelihood byV* : e -~/N ,  where N is the total number of wordsprocessed.Though several algorithms (Brown et al, 1992;Pereira, Tishby, and Lee, 1993) have been proposed100(9o(80(4O(20(1000goo80~411112@ 5 10 15 20 25 30 5 10 15 20 25 30iteration of EM iteration of EM(a) (b)Figure 1: Plots of (a) training and (b) test perplexity versus number of iterations of the EM algorithm, forthe aggregate Markov model with C = 32 classes.C train test1 964.7 964.92 771.2 772.24 541.9 543.68 399.5 401.516 328.8 331.832 278.9 283.2V 123.6 - -Table 1: Perplexities of aggregate Markov models onthe training and test sets; C is the number of classes.The case C = 1 corresponds to a ML unigram model;C = V, to a ML bigram model.0.2 0.4 0.6 0.8winning assignment probabilityFigure 2: Histogram of the winning assignmentprobabilities, maxc P(clw), for the three hundredmost commonly occurring words.for performing the decomposition i  eq.
(1), it isworth noting that only the EM algorithm directlyoptimizes the log-likelihood in eq.
(5).
This has ob-vious advantages if the goal of finding word classes isto improve the perplexity of a language model.
TheEM algorithm also handles probabilistic onstraintsin a natural way, allowing words to belong to morethan one class if this increases the overall ikelihood.Our approach differs in important ways from theuse of hidden Markov models (HMMs) for class-based language modeling (Jelinek et al, 1992).While HMMs also use hidden variables to representword classes, the dynamics are fundamentally dif-ferent.
In HMMs, the hidden state at time t ?
1 ispredicted (via the state transition matrix) from thehidden state at time t. On the other hand, in aggre-gate Markov models, the hidden state at time t + 1is predicted (via the matrix P(ct+llwt)) from theword at time t. The state-to-state versus word-to-state dynamics lead to different learning algorithms.For example, the Baum-Welch algorithm for HMMsrequires forward and backward passes through eachtraining sentence, while the EM algorithm we usedoes not.We trained aggregate Markov models with 2, 4,8, 16, and 32 classes.
Figure 1 shows typical plotsof the training and test set perplexities versus thenumber of iterations of the EM algorithm.
Clearly,the two curves are very close, and the monotonicdecrease in test set perplexity strongly suggests lit-tle if any overfitting, at least when the number ofclasses is small compared to the number of words inthe vocabulary.
Table 1 shows the final perplexities(after thirty-two iterations of EM) for various ag-gregate Markov models.
These results confirm thataggregate Markov models are intermediate in accu-racy between unigram (C = 1) and bigram (C = V)models.The aggregate Markov models were also observedto discover meaningful word classes.
Table 2 shows,for the aggregate model with C = 32 classes, the83las cents made make takeago day earfier Friday Monday month quarterreported said Thursday trading TuesdayWednesday (...)even get tobased days down home months up work yearsthose (,) (--)(.)
(?
)eighty fifty forty ninety seventy sixty thirty19 bilfion hundred million nineteen20 did (") (')21 but called San (:) (start-of-sentence)2223bank board chairman end group membersnumber office out part percent price prices ratesales shares usea an another any dollar each first good her his itsmy old our their this24 long Mr. year7 twenty (0 (') 258 can could may should to will would9 about at just only or than (&) (;)i 10 economic high interest much no such tax united i 27well11 president12 because do how if most say so then think verywhat when where 2913 according back expected going him plan used way15 don't I people they we you \[Bush company court department more officials \] 30 16 pofice retort spokesman \[17 former theAmerican big city federal general house mifitary18 national party political state union York ibusiness California case companies corporationdollars incorporated industry law moneythousand time today war week 0) (unknown)26 also government he it market she that therewhich whoA.
B. C. D. E. F. G. I. L. M. N. P. R. S. T. U.28 both foreign international major many new oilother some Soviet stock these west worldafter all among and before between by during forfrom in including into like of off on over sincethrough told under until while witheight fifteen five four half last next nine oh onesecond seven several six ten third three twelvetwo zero (-)31 are be been being had has have is it's not stillwas were32 chief exchange news public service tradeTable 2: Most probable assignments for the 300 most frequent words in an aggregate Markov model withC = 32 classes.
Class 14 is absent because it is not the most probable class for any of the selected words.
)most probable class assignments of the three hun-dred most commonly occurring words.
To be precise,for each class c*, we have listed the words for whichc* = arg maxe P(c\]w).
Figure 2 shows a histogram ofthe winning assignment probabilities, maxe P(c\[w),for these words.
Note that the winning assignmentprobabilities are distributed broadly over the inter-val \[-~, 1\].
This demonstrates the utility of allowing"soft" membership classes: for most words, the max-imum likelihood estimates of P(clw ) do not corre-spond to a winner-take-all assignment, and thereforeany method that assigns each word to a single class("hard" clustering), such as those used by Brown etal.
(1992) or Ney, Essen, and Kneser (1994), wouldlose information.We conclude this section with some final com-ments on overfitting.
Our models were trained bythirty-two iterations of EM, allowing for nearly com-plete convergence in the log-likelihood.
Moreover,we did not implement any flooring constraints 1 onthe probabilities P(clwl ) or P(w21c).
Nevertheless,in all our experiments, the ML aggregate Markovlit is worth noting, in this regard, that individualzeros in the matrices P(w2\[c) and P(c\[wl) do not nec-essarily give rise to zeros in the matrix P(w21wt), ascomputed from eq.
(1).models assigned non-zero probability to all the bi-grams in the test set.
This suggests that for largevocabularies there is a useful regime 1 << C << Vin which aggregate models do not suffer much fromoverfitting.
In this regime, aggregate models can berelied upon to compute the probabilities of unseenword combinations.
We will return to this point inSection 4, when we consider how to smooth n-gramlanguage models.3 Mixed-order  Markov  mode lsOne of the drawbacks of n-gram models is that theirsize grows rapidly with their order.
In this section,we consider how to make predictions based on a con-vex combination of'pairwise correlations.
This leadsto language models whose size grows linearly in thenumber of words used for each prediction.For each k > 0, the ski_p-k transition matrixM(wt-k, wt) predicts the current word from thekth previous word in the sentence.
A mixed-orderMarkov model combines the information in thesematrices for different values of k. Let m denotethe number of bigram models being combined.
Theprobability distribution for these models has theform:P(wdwt-1,..., wt_~) = (6)84k-1f i  Ak(wt-k) Mk(wt-k,Wt) II\[1- Aj(w,_~)\].k=l  j= lThe terms in this equation have a simple interpreta-tion.
The V x V matrices Mk (w, w') in eq.
(6) de-fine the skip-k stochastic dependency of w' at someposition t on w at position t - k; the parametersAk (w) are mixing coefficients that weight the predic-tions from these different dependencies.
The value ofAk (w) can be interpreted as the probability that themodel, upon seeing the word wt-k, looks no furtherback to make its prediction (Singer, 1996).
Thus themodel predicts from wt-1 with probability A1 (wt-1),from wt-2 with probability \[1 - Al(wt-1)\]A2(wt-~),and so on.
Though included in eq.
(6) for cosmeticreasons, the parameters Am (w) are actually fixed tounity so that the model never looks further than mwords back.We can view eq.
(6) as a hidden variable model.Imagine that we adopt the following strategy to pre-dict the word at time t. Starting with the previousword, we toss a coin (with bias Ai(Wt_i) ) to see ifthis word has high predictive value.
If the answeris yes, then we predict from the skip-1 transitionmatrix, Ml(Wt-l,Wt).
Otherwise, we shift our at-tention one word to the  left and repeat the process.If after m-  1 tosses we have not settled on a pre-diction, then as a last resort, we make a predictionusing Mm(wt-m, wt).
The hidden variables in thisprocess are the outcomes of the coin tosses, whichare unknown for each word wt-k.Viewing the model in this way, we can derive anEM algorithm to learn the mixing coefficients Ak (w)and the transition matrices 2 Mk(w, w').
The E-stepof the algorithm is to compute, for each word in thetraining set, the posterior probability that it wasgenerated by Mk(wt-k, wt).
Denoting these poste-rior probabilities by Ck(t), we have:Ck(t) = (7)Aa(wt-a)Mk(wt-k wt) k-1 ,P(wt Iw,-1, w,-2,..., w,_~)where the denominator is given by eq.
(6).
TheM-step of the algorithm is to update the parame-ters Ak(W) and Mk(w, w') to reflect the statistics ineq.
(7).
The updates for mixed-order Markov modelsare given by:,s(w, wt-k)?k (0 A (w) (8)~Note that the ML estimates of Mk(w,w') do notdepend only on the raw counts of k-separated bigrams;they are also coupled to the values of the mixing coef-ficients, Aa(w).
In particular, the EM algorithm adaptsthe matrix elements to the weighting of word combina-tions in eq.
(6).
The raw counts of k-separated bigrams,however, do give good initial estimates.11C10510~"~ 95858G757G1 2 3 4iteration of EMFigure 3: Plot of (training set) perplexity versusnumber of iterations of the EM algorithm.
The re-sults are for the m = 4 mixed-order Markov model.m train missing1 123.2 0.0452 89.4 0.0143 77.9 0.00634 72.4 0.0037Table 3: Results for ML mixed-order models; m de-notes the number of bigrams that were mixed intoeach prediction.
The first column shows the per-plexities on the training set.
The s.ec0nd shows thefraction of words in the test set that were assignedzero probability.
The case m = 1 corresponds to aML bigram model.Mk(w, W') +- ~t  ~(W, Wt-k)~(W', Wt)?k(t)E,  w,-k)?k(t) , (9)where the sums are over all the sentences in thetraining set, and J(w, w') = 1 iff w = w'.We trained mixed-order Markov models with 2 <m _< 4.
Figure 3 shows a typical plot of the train-ing set perplexity as a function of the number ofiterations of the EM algorithm.
Table 3 shows thefinal perplexities on the training set (after four iter-ations of EM).
Mixed-order models cannot be useddirectly on the test set because they predict zeroprobability for unseen word combinations.
Unlikestandard n-gram models, however, the number ofunseen word combinations actually decreases withthe order of the model.
The reason for this is thatmixed-order models assign finite probability to all n-grams wlw~ ... wn for which any of the k-separatedbigrams wkwn are observed in the training set.
Toillustrate this point, Table 3 shows the fraction ofwords in the test set that were assigned zero proba-bility by the mixed-order model.
As expected, thisfraction decreases monotonically with the number ofbigrams that are mixed into each prediction.Clearly, the success of mixed-order models de-pends on the ability to gauge the predictive valueof each word, relative to earlier words in the samesentence.
Let us see how this plays out for the850.1 < Al(w) < 0.7(-) and of (") or (;) to (,) (&) by with S. fromnine were for that eight low seven the (() (:) sixare not against was four between a their twothree its (unknown) S. on as is (--) five 0) intoC.
M. her him over than A.0.96 < Al(w) < 1officials prices which go way he last they earlieran Tuesday there foreign quarter she formerfederal don't days Friday next Wednesday (%)Thursday I Monday Mr. we half based partUnited it's years going nineteen thousand months(.)
million very cents San ago U. percent billion(?)
according (.
)Table 4: Words with low and high values of Al(w)in an m = 2 mixed order model.second-order (m = 2) model in Table 3.
In thismodel, a small value for ~l(w) indicates that theword w typically carries less information that theword that precedes it.
On the other hand, a largevalue for Al(w) indicates that the word w is highlypredictive.
The ability to learn these relationshipsis confirmed by the results in Table 4.
Of the three-hundred most common words, Table 4 shows thefifty with the lowest and highest values of Al(w).Note how low values of Al(w) are associated withprepositions, mid-sentence punctuation marks, andconjunctions, while high values are associated with"contentful" words and end-of-sentence markers.
(Aparticularly interesting dichotomy arises for the twoforms "a" and "an" of the indefinite article; the lat-ter, because it always precedes a word that beginswith a vowel, is inherently more predictive.)
Theseresults underscore the importance of allowing thecoefficients Al(w) to depend on the context w, asopposed to being context-independent (Ney, Essen,and Kneser, 1994).4 Smooth ingSmoothing plays an essential role in language modelswhere ML predictions are unreliable for rare events.In n-gram modeling, it is common to adopt a re-cursive strategy, smoothing bigrams by unigrams,trigrams by bigrams, and so on.
Here we adopt asimilar strategy, using the (m - 1)th mixed-ordermodel to smooth the ruth one.
At the "root" ofour smoothing procedure, however, lies not a uni-gram model, but an aggregate Markov model withC > 1 classes.
As shown in Section 2, these modelsassign finite probability to all word combinations,even those that are not observed in the training set.Hence, they can legitimately replace unigrams as thebase model in the smoothing procedure.Let us first examine the impact of replacing uni-gram models by aggregate models at the root of theC12481632validation test unseen163.615162.982161.513161.327160.034159.247167.112166.193164.363164.104162.686161.683293175259360200067190178164673150958Table 5: Perplexities of bigram models smoothed byaggregate Markov models with different numbers ofclasses (C).smoothing procedure.
To this end, a held-out inter-polation algorithm (Jelinek and Mercer, 1980) wasused to smooth an ML bigram model with the aggre-gate Markov models from Section 2.
The smoothingparameters, one for each row of the bigram transi-tion matrix, were estimated from a validation set thesame size as the test set.
Table 5 gives the final per-plexities on the validation set, the test set, and theunseen bigrams in the test set.
Note that smooth-ing with the C = 32 aggregate Markov model hasnearly halved the perplexity of unseen bigrams, ascompared to smoothing with the unigram model.Let us now examine the recursive use of mixed-order models to obtain smoothed probability esti-mates.
Again, a held-out interpolation algorithmwas used to smooth the mixed-order Markov modelsfrom Section 3.
The ruth mixed-order model hadmV smoothing parameters 0"k (w), corresponding tothe V rows in each skip-k transition matrix.
Themth mixed-order model was smoothed by discount-ing the weight of each skip-k prediction, then fill-ing in the leftover probability mass by a lower-ordermodel.
In particular, the discounted weight of theskip-k prediction was given byk-1\[1 - O'k(wt-k) lAk(Wt-k) HI1  --)~j(wt-j)\] , (10)j=lleaving a total mass ofk-1f i  O'k(Wt-k)~k(W,-k) H\[1-- ,~j(W,_j)\] (11)k=l  j=lfor the (m-  1)th mixed-order model.
(Note thatthe m = 1 mixed-order model corresponds to a MLbigram model.
)Table 6 shows the perplexities of the smoothedmixed-order models on the validation and test sets.An aggregate Markov model with C = 32 classeswas used as the base model in the smoothing proce-dure.
The first row corresponds to a bigram modelsmoothed by a aggregate Markov model; the secondrow corresponds to an m = 2 mixed-order model,smoothed by a ML bigram model, smoothed by anaggregate Markov model; the third row corresponds86m validation test1 160.1 161.32 135.3 136.93 131.4 133.54 131.2 133.7Table 6: Perplexities of smoothed mixed-order mod-els on the validation and test sets.to an m = 3 mixed-order model, smoothed by am = 2 mixed-order model, smoothed by a ML bi-gram model, etc.
A significant decrease in perplex-ity occurs in moving to the smoothed m = 2 mixed-order model.
On the other hand, the difference inperplexity for higher values of m is not very dra-matic.Our last experiment looked at the smoothing ofa trigram model.
Our baseline was a ML trigrammodel that backed off 3 to bigrams (and when nec-essary, unigrams) using the Katz backoff procedure(Katz, 1987).
In this procedure, the predictions ofthe ML trigram model are discounted by an amountdetermined by the Good-Turing coefficients; the left-over probability mass is then filled in by the backoffmodel.
We compared this to a trigram model thatbacked off to the m = 2 model in Table 6.
This washandled by a slight variant of the Katz procedure(Dagan, Pereira, and Lee, 1994) in which the mixed-order model substituted for the backoff model.One advantage of this smoothing procedure is thatit is straightforward to assess the performance of dif-ferent backoff models.
Because the backoff modelsare only consulted for unseen word combinations,the perplexity on these word combinations serves asa reasonable figure-of-merit.Table 7 shows those perplexities for the twosmoothed trigram models (baseline and backoff).The mixed-order smoothing was found to reducethe perplexity of unseen word combinations by 51%.Also shown in the table are the perplexities on theentire test set.
The overall perplexity decreasedby 16%--a significant amount considering that only24% of the predictions involved unseen word com-binations and required backing off from the trigrammodel.The models in Table 7 were constructed from alln-grams (1 < n < 3) observed in the training data.Because many n-grams occur very infrequently, anatural question is whether truncated models, whichomit low-frequency n-grams from the training set,can perform as well as untruncated ones.
The ad-vantage of truncated models is that they do not needto store nearly as many non-zero parameters as un-truncated models.
The results in Table 8 were ob-~We used a backoff procedure (instead of interpo-lation) to avoid the estimation of trigram smoothingparameters.backoff test unseenbaseline 95.2 2799mixed 79.8 1363Table 7: Perplexities of two smoothed trigram mod-els on the test set and the subset of unseen wordcombinations.
The baseline model backed off to bi-grams and unigrams; the other backed off to them = 2 model in Table 6.t baseline mixed trigrams(?
105) missing1 95.2 79.8 25.4 0.242 98.6 78.3 6.1 0.323 101.7 79.6 3.3 0.364 104.2 81.1 2.3 0.385 106.2 82.4 1.7 0.41Table 8: Effect of truncating trigrams that occurless than t times.
The table shows the baseline andmixed-order perplexities on the test set, the num-ber of distinct trigrams with t or more counts, andthe fraction of trigrams in the test set that requiredbacking off.tained by dropping trigrams that occurred less thant times in the training corpus.
The t = 1 row cor-responds to the models in Table 7.
The most in-teresting observation from the table is that omittingvery low-frequency trigrams does not decrease thequality of the mixed-order model, and may in factslightly improve it.
This contrasts with the standardbackoff model, in which truncation causes ignificantincreases in perplexity.5 Discuss ionOur results demonstrate he utility of language mod-els that are intermediate in size and accuracy be-tween different order n-gram models.
The twomodels considered in this paper were hidden vari-able Markov models trained by EM algorithms formaximum likelihood estimation.
Combinations ofintermediate-order models were also investigated byRosenfeld (1996).
His experiments used the 20,000-word vocabulary Wall Street Journal corpus, a pre-decessor of the NAB corpus.
He trained a maximum-entropy model consisting of unigrams, bigrams, tri-grams, skip-2 bigrams and trigrams; after selectinglong-distance bigrams (word triggers) on 38 millionwords, the model was tested on a held-out 325 thou-sand word sample.
Rosenfeld reported a test-setperplexity of 86, a 19% reduction from the 105 per-plexity of a baseline trigram backoff model.
In ourexperiments, the perplexity gain of the mixed-ordermodel ranged from 16% to 22%, depending on theamount of truncation in the trigram model.While Rosenfeld's results and ours are not di-87rectly comparable, both demonstrate the utility ofmixed-order models.
It is worth discussing, how-ever, the different approaches to combining infor-mation from non-adjacent words.
Unlike the max-imum entropy approach, which allows one to com-bine many non-independent features, ours calls fora careful Markovian decomposition.
Rosenfeld ar-gues at length against naive linear combinations infavor of maximum entropy methods.
His argumentsdo not apply to our work for several reasons.
First,we use a large number of context-dependent mixingparameters to optimize the overall ikelihood of thecombined model.
Thus, the weighting in eq.
(6) en-sures that the skip-k predictions are only invokedwhen the context is appropriate.
Second, we adjustthe predictions of the skip-k transition matrices (byEM) so that they match the contexts in which theyare invoked.
Hence, the count-based models are in-terpolated in a way that is "consistent" with theireventual use.Training efficiency is another issue in evaluatinglanguage models.
The maximum entropy methodrequires very long training times: e.g., 200 CPU-days in Rosenfeld's experiments.
Our methods re-quire significantly less; for example, we trained thesmoothed m = 2 mixed-order model, from start tofinish, in less than 12 CPU-hours (while using alarger training corpus).
Even accounting for differ-ences in processor speed, this amounts to a signifi-cant mismatch in overall training time.In conclusion, let us mention some open problemsfor further research.
Aggregate Markov models canbe viewed as approximating the full bigram tran-sition matrix by a matrix of lower rank.
(Fromeq.
(1), it should be clear that the rank of the class-based transition matrix is bounded by the num-ber of classes, C.) As such, there are interestingparallels between Expectation-Maximization (EM),which minimizes the approximation error as mea-sured by the KL divergence, and singular value de-composition (SVD), which minimizes the approxi-mation error as measured by the L2 norm (Presset al, 1988; Schiitze, 1992).
Whereas SVD finds aglobal minimum in its error measure, however, EMonly finds a local one.
It would clearly be desirableto improve our understanding of this fundamentalproblem.In this paper we have focused on bigram models,but the ideas and algorithms generalize in a straight-forward way to higher-order n-grams.
Aggregatemodels based on higher-order n-grams (Brown et al,1992) might be able to capture multi-word struc-tures such as noun phrases.
Likewise, trigram-basedmixed-order models would be useful complements o4-gram and 5-gram models, which are not uncom-mon in large-vocabulary language modeling.A final issue that needs to be addressed isscaling--that is, how the performance of these mod-els depends on the vocabulary size and amountof training data.
Generally, one expects that thesparser the data, the more helpful are models thatcan intervene between different order n-grams.
Nev-ertheless, it would be interesting to see exactly howthis relationship lays out for aggregate and mixed-order Markov models.AcknowledgmentsWe thank Michael Kearns and Yoram Singer for use-ful discussions, the anonymous reviewers for ques-tions and suggestions that helped improve the paper,and Don Hindle for help with his language modelingtools, which we used to build the baseline modelsconsidered in the paper.ReferencesP.
Brown, V. Della Pietra, P. deSouza, J. Lai, and R.Mercer.
1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics 18(4):467-479.S.
Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InProceedings of the 34th Meeting of the Associationfor Computational Linguistics.I.
Dagan, F. Pereira, and L. Lee.
1994.
Similarity-based estimation of word co-occurrence probabili-ties.
In Proceedings of the 32nd Annual Meeting ofthe Association for Computational Linguistics.A.
Dempster, N. Laird, and D. Rubin.
1977.
Max-imum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical SocietyB39:1-38.X.
Huang, F. Alleva, H. Hon, M.-Y.
Hwang, K.-F.Lee, and R. Rosenfeld.
1993.
The SPHINX-If  speechrecognition system: an overview.
Computer Speechand Language, 2:137-148.F.
Jelinek and R. Mercer.
1980.
Interpolated es-timation of Markov source parameters from sparsedata.
In Proceedings of the Workshop on PatternRecognition in Practice.F.
Jelinek, R. Mercer, and S. Roukos.
1992.
Princi-ples of lexical language modeling for speech recogni-tion.
In S. Furui and M. Sondhi, eds.
Advances inSpeech Signal Processing.
Mercer Dekker, Inc.S.
Katz.
1987.
Estimation of probabilities fromsparse data for the language model component ofa speech recognizer.
IEEE Transactions on ASSP35(3):400-401.H.
Ney, U. Essen, and R. Kneser.
1994.
On structur-ing probabilistic dependences in stochastic languagemodeling.
Computer Speech and Language 8:1-38.88F.
Pereira, N. Tishby, and L. Lee.
1993.
Distribu-tional clustering of English words.
In Proceedingsof the 30th Annual Meeting of the Association forComputational Linguistics.W.
Press, B. Flannery, S. Teukolsky, and W. Vet-terling.
1988.
Numerical Recipes in C. CambridgeUniversity Press: Cambridge.R.
Rosenfeld.
1996.
A Maximum Entropy Approachto Adaptive Statistical Language Modeling.
Com-puter Speech and Language, 10:187-228.H.
Schfitze.
1992.
Dimensions of Meaning.
In Pro-ceedings of Supereomputing, 787-796.
MinneapolisMN.Y.
Singer.
1996.
Adaptive Mixtures of ProbabilisticTransducers.
In D. Touretzky, M. Mozer, and M.Hasselmo (eds).
Advances in Neural InformationProcessing Systems 8:381-387.
MIT Press: Cam-bridge, MA.89
