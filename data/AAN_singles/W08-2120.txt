CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158Manchester, August 2008Trainable Speaker-Based Referring Expression GenerationGiuseppe Di Fabbrizio and Amanda J. Stent and Srinivas BangaloreAT&T Labs - Research, Inc.180 Park AvenueFlorham Park, NJ 07932, USA{pino,stent,srini}@research.att.comAbstractPrevious work in referring expression gen-eration has explored general purpose tech-niques for attribute selection and surfacerealization.
However, most of this workdid not take into account: a) stylistic dif-ferences between speakers; or b) trainablesurface realization approaches that com-bine semantic and word order information.In this paper we describe and evaluate sev-eral end-to-end referring expression gener-ation algorithms that take into considera-tion speaker style and use data-driven sur-face realization techniques.1 IntroductionNatural language generation (NLG) systems havetypically decomposed the problem of generatinga linguistic expression from a conceptual specifi-cation into three major steps: content planning,text planning and surface realization (Reiter andDale, 2000).
The task in content planning is toselect the information that is to be conveyed tomaximize communication efficiency.
The task intext planning and surface realization is to use theavailable linguistic resources (words and syntax) toconvey the selected information using well-formedlinguistic expressions.During a discourse (whether written or spoken,monolog or dialog), a number of entities are in-troduced into the discourse context shared by thereader/hearer and the writer/speaker.
Construct-ing linguistic references to these entities efficientlyand effectively is a problem that touches on allc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.parts of an NLG system.
Traditionally, this prob-lem is split into two parts.
The task of selectingthe attributes to use in referring to an entity is theattribute selection task, performed during contentplanning or sentence planning.
The actual con-struction of the referring expression is part of sur-face realization.There now exist numerous general-purpose al-gorithms for attribute selection (e.g., (Dale and Re-iter, 1995; Krahmer et al, 2003; Belz and Gatt,2007; Siddharthan and Copestake, 2004)).
How-ever, these algorithms by-and-large focus on thealgorithmic aspects of referring expression gener-ation rather than on psycholinguistic factors thatinfluence language production.
For example, weknow that humans exhibit individual differences inlanguage production that can be quite pronounced(e.g.
(Belz, 2007)).
We also know that thelanguage production process is subject to lexicalpriming, which means that words and concepts thathave been used recently are likely to appear again(Levelt, 1989).In this paper, we look at attribute selection andsurface realization for referring expression gener-ation using the TUNA corpus 1, an annotated cor-pus of human-produced referring expressions thatdescribe furniture and people.
We first explorethe impact of individual style and priming on at-tribute selection for referring expression genera-tion.
To get an idea of the potential improvementwhen modeling these factors, we implemented aversion of full brevity search that uses speaker-specific constraints, and another version that alsouses recency constraints.
We found that usingspeaker-specific constraints led to big performancegains for both TUNA domains, while the use of re-1http://www.csd.abdn.ac.uk/research/tuna/151cency constraints was not as effective for TUNA-style tasks.
We then modified Dale and Reiter?sclassic attribute selection algorithm (Dale and Re-iter, 1995) to model individual differences in style,and found performance gains in this more greedyapproach as well.Then, we look at surface realization for re-ferring expression generation.
There are sev-eral approaches to surface realizations describedin the literature (Reiter and Dale, 2000) rang-ing from hand-crafted template-based realizers todata-driven syntax-based realizers (Langkilde andKnight, 2000; Bangalore and Rambow, 2000).Template-based realization provides a straightfor-ward method to fill out pre-defined templates withthe current attribute values.
Data-driven syntax-based methods employ techniques that incorporatethe syntactic relations between words which canpotentially go beyond local adjacency relations.Syntactic information also helps in eliminating un-grammatical sentence realizations.
At the other ex-treme, there are techniques that exhaustively gen-erate possible realizations with recourse to syntaxin as much as it is reflected in local n-grams.
Suchtechniques have the advantage of being robust al-though they are inadequate to capture long-rangedependencies.
We explore three techniques forthe task of referring expression generation that aredifferent hybrids of hand-crafted and data-drivenmethods.The layout of this paper is as follows: In Sec-tion 2, we describe the TUNA data set and the taskof identifying target entities in the context of dis-tractors.
In Section 3, we present our algorithmsfor attribute selection.
Our algorithms for sur-face realization are presented in Section 4.
Ourevaluation of these methods for attribute selectionand surface realization are presented in Sections 5and 6.2 The TUNA CorpusThe TUNA corpus was constructed using a web-based experiment.
Participants were presentedwith a sequence of web pages, on each of whichthey saw displayed a selection of 7 pictures of ei-ther furniture (e.g.
Figure 1) or people (e.g.
Fig-ure 2) sparsely placed on a 3 row x 5 columngrid.
One of the pictures (the target) was high-lighted; the other 6 objects (the distractors) wererandomly selected from the object database.
Par-ticipants were told that they were interacting with acomputer system to remove all but the highlightedpicture from the screen.
They entered a descriptionof the object using natural language to identify theobject to the computer system.The section of the TUNA corpus we used wasthat provided for the REG 2008 Challenge2.
Thetraining data includes 319 referring expressions inthe furniture domain and 274 in the people domain.The development data (which we used for testing)includes 80 referring expressions in the furnituredomain and 68 in the people domain.Figure 1: Example of data from the furniture do-main (The red couch on top).Figure 2: Example of data from the people domain(The bald subject on the bottom with the whitebeard).3 Attribute Selection AlgorithmsGiven a set of entities with attributes appropriateto a domain (e.g., cost of flights, author of a book,2http://www.nltg.brighton.ac.uk/research/reg08/.
Prelimi-nary versions of these algorithms were used in this challengeand presented at INLG 2008.152color of a car) that are in a discourse context, and atarget entity that needs to be identified, the task ofattribute selection is to select a subset of the at-tributes that uniquely identifies the target entity.
(Note that there may be more than one such at-tribute set.)
The efficacy of attribute selection canbe measured based on the minimality of the se-lected attribute set as well as its ability to deter-mine the target entity uniquely.
There are varia-tions however in terms of what makes an attributeset more preferable to a human.
For example, ina people identification task, attributes of faces aregenerally more memorable than attributes pertain-ing to outfits.
In this paper, we demonstrate thatthe attribute set is speaker dependent.In this section, we present two different attributeselection algorithms.
The Full Brevity algorithmselects the attribute set by exhaustively searchingthrough all possible attribute sets.
In contrast, Daleand Reiter algorithm orders the attributes basedon a heuristic (motivated by human preference)and selects the attributes in that order until the tar-get entity is uniquely determined.
We elaborate onthese algorithms below.Full Brevity (FB) We implemented a version offull brevity search.
It does the following: first,it constructs AS, the set of attribute sets thatuniquely identify the referent given the distrac-tors.
Then, it selects an attribute set ASu ?
ASbased on one of the following four criteria: 1) Theminimality (FB-m) criterion selects from amongthe smallest elements of AS at random.
2) Thefrequency (FB-f) criterion selects the element ofAS that occurred most often in the training data.3) The speaker frequency (FB-sf) criterion se-lects the element of AS used most often by thisspeaker in the training data, backing off to FB-f ifnecessary.
This criterion models individual speak-ing/writing style.
4) Finally, the speaker recency(FB-sr) criterion selects the element of AS usedmost recently by this speaker in the training data,backing off to FB-sf if necessary.
This criterionmodels priming.Dale and Reiter We implemented two variantsof the classic Dale & Reiter attribute selection(Dale and Reiter, 1995) algorithm.
For Dale &Reiter basic (DR-b), we first build the preferredlist of attributes by sorting the attributes accordingto frequency of use in the training data.
We keepseparate lists based on the ?LOC?
condition (if itsvalue was ?+LOC?, the participants were told thatthey could refer to the target using its location onthe screen; if it was ?-LOC?, they were instructednot to use location on the screen) and backoff toa global preferred attribute list if necessary.
Next,we iterate over the list of preferred attributes andselect the next one that rules out at least one en-tity in the contrast set until no distractors are left.Dale & Reiter speaker frequency (DR-sf) usesa different preferred attribute list for each speaker,backing off to the DR-b preferred list if an attributehas never been observed in the current speaker?spreferred attribute list.
For the purpose of this task,we did not use any external knowledge (e.g.
tax-onomies).4 Surface Realization ApproachesA surface realizer for referring expression genera-tion transforms a set of attribute-value pairs into alinguistically well-formed expression.
Our surfacerealizers, which are all data-driven, involve fourstages of processing: (a) lexical choice of wordsand phrases to realize attribute values; (b) genera-tion of a space of surface realizations (T ); (c) rank-ing the set of realizations using a language model(LM ); (d) selecting the best scoring realization.In general, the best ranking realization (T?)
is de-scribed by equation 1:T ?
= Bestpath(Rank(T,LM)) (1)We describe three different methods for creatingthe search space of surface realizations ?
Template-based, Dependency-based and Permutation-basedmethods.
Although these techniques share thesame method for ranking, they differ in the meth-ods used for generating the space of possible sur-face realizations.4.1 Generating possible surface realizationsIn order to transform the set of attribute-valuepairs into a linguistically well-formed expression,the appropriate words that realize each attributevalue need to be selected (lexical choice) and theselected words need to be ordered according tothe syntax of the target language (lexical order).We present different models for approximating thesyntax of the target language.
All three modelstightly integrate the lexical choice and lexical re-ordering steps.1534.1.1 Template-Based RealizerIn the template-based approach, surface realiza-tions from our training data are used to infer a setof templates.
In the TUNA data, each attribute ineach referring expression is annotated with its at-tribute type (e.g.
in ?the large red sofa?
the sec-ond word is labeled ?size?, the third ?color?
andthe fourth ?type?).
We extract the annotated re-ferring expressions from each trial in the trainingdata and replace each attribute value with its type(e.g.
?the size color type?)
to create a tem-plate.
Each template is indexed by the lexicograph-ically sorted list of attribute types it contains (e.g.color size type).
If an attribute set is notfound in the training data (e.g.
color size)but a superset of that set is (e.g.
color sizetype), then the corresponding template(s) may beused, with the un-filled attribute types deleted priorto output.At generation time, we find all possible realiza-tions (l) (from the training data) of each attributevalue (a) in the input attribute set (AS), and fill ineach possible template (t) with each combinationof the attribute realizations.
The space of possiblesurface realizations is represented as a weightedfinite-state automaton.
The weights are computedfrom the prior probability of each template andthe prior probability of each lexical item realizingan attribute (Equation 2).
We have two versionsof this realizer: one with speaker-specific lexi-cons and templates (Template-S), and one without(Template).
We report results for both.P (T |AS) =?tP (t|AS)?
?a?t?lP (l|a, t) (2)4.1.2 Dependency-Based RealizerTo construct our dependency-based realizer, wefirst parse all the word strings from the train-ing data using the dependency parser describedin (Bangalore et al, 2005; Nasr and Rambow,2004).
Then, for every pair of words wi, wj thatoccur in the same referring expression (RE) in thetraining data, we compute: freq(i < j), the fre-quency with which wi precedes wj in any RE;freq(dep(wi, wj) ?
i < j), the frequency withwhich wi depends on and precedes wj in any RE,and freq(dep(wi, wj)?j < i), the frequency withwhich wi depends on and follows wj in any RE.At generation time, we find all possible realiza-tions of each attribute value in the input attributeset, and for each combination of attribute realiza-tions, we find the most likely set of dependenciesand precedences given the training data.
In otherwords, we bin the selected attribute realizationsaccording to whether they are most likely to pre-cede, depend on and precede, depend on and fol-low, or follow, the head word they are closest to.The result is a set of weighted partial orderings onthe attribute realizations.
As with the template-based surface realizer, we implemented speaker-specific and speaker-independent versions of thedependency-based surface realizer.
Once again,we encode the space of possible surface realiza-tions as a weighted finite-state automaton.4.1.3 Permute and Rank RealizerIn this method, the lexical items associated witheach attribute value to be realized are treated as adisjunctive set of tokens.
This disjunctive set isrepresented as a finite-state automaton with twostates and transitions between them labeled withthe tokens of the set.
The transitions are weightedby the negative logarithm of the probability of thelexical token (l) being associated with that attributevalue (a): (?log(P (l|a))).
These sets are treatedas bags of tokens; we create permutations of thesebags of tokens to represent the set of possible sur-face realizations.In general, the number of states of the minimalpermutation automaton of even a linear automaton(finite-state representation of a string) grows expo-nentially with the number of words of the string.Although creating the full permutation automatonfor full natural language generation tasks couldbe computationally prohibitive, most attribute setsin our two domains contain no more than five at-tributes.
So we choose to explore the full permu-tation space.
A more general approach might con-strain permutations to be within a local window ofadjustable size (also see (Kanthak et al, 2005)).Figure 3 shows the minimal permutation au-tomaton for an input sequence of 4 words and awindow size of 2.
Each state of the automaton isindexed by a bit vector of size equal to the numberof words/phrases of the target sentence.
Each bitof the bit vector is set to 1 if the word/phrase inthat bit position is used on any path from the initialto the current state.
The next word for permutationfrom a given state is restricted to be within the win-dow size (2 in our case) positions counting fromthe first as-yet uncovered position in that state.
Forexample, the state indexed with vector ?1000?
rep-154000010001010021100210103111103110141111432Figure 3: Locally constraint permutation automaton for a sentence with 4 positions and a window sizeof 2.resents the fact that the word/phrase at position 1has been used.
The next two (window=2) posi-tions are the possible outgoing arcs from this statewith labels 2 and 3 connecting to state ?1100?
and?1010?
respectively.
The bit vectors of two statesconnected by an arc differ only by a single bit.Note that bit vectors elegantly solve the problem ofrecombining paths in the automaton as states withthe same bit vectors can be merged.
As a result, afully minimized permutation automaton has only asingle initial and final state.4.2 Ranking and Recovering a SurfaceRealizationThese three methods for surface realization createa space of possible linguistic expressions given theset of attributes to be realized.
These expressionsare encoded as finite-state automata and have to beranked based on their syntactic well-formedness.We approximate the syntactic well-formedness ofan expression by the n-gram likelihood score ofthat expression.
We use a trigram model trainedon the realizations in the training corpus.
Thislanguage model is also represented as a weightedfinite-state automaton.
The automaton represent-ing the space of possible realizations and the onerepresenting the language model are composed.The result is an automaton that ranks the possiblerealizations according to their n-gram likelihoodscores.
We then produce the best-scoring realiza-tion as the target realization of the input attributeset.We introduce a parameter ?
which allows usto control the importance of the prior score rela-tive to the language model scores.
We weight thefinite-state automata according to this parameter asshown in Equation 3.T ?
= Bestpath(?
?
T ?
(1 ?
?)
?
LM) (3)DICE MASI Acc.
Uniq.
Min.FurnitureFB-m .36 .16 0 1 1FB-f .81 .58 .40 1 0FB-sf .95 .87 .79 1 0FB-sr .93 .81 .71 1 0DR-b .81 .60 .45 1 0DR-sf .86 .64 .45 1 .04PeopleFB-m .26 .12 0 1 1FB-f .58 .37 .28 1 0FB-sf .94 .88 .84 1 .01FB-sr .93 .85 .79 1 .01DR-b .70 .45 .25 1 0DR-sf .78 .55 .35 1 0OverallFB-m .32 .14 0 1 1FB-f .70 .48 .34 1 0FB-sf .95 .87 .81 1 .01FB-sr .93 .83 .75 1 .01DR-b .76 .53 .36 1 0DR-sf .82 .60 .41 1 .02Table 1: Results for attribute selection5 Attribute Selection ExperimentsData Preparation The training data were usedto build the models outlined above.
The develop-ment data were then processed one-by-one.Metrics We report performance using the met-rics used for the REG 2008 competition.
TheMASI metric is a metric used in summarizationthat measures agreement between two annotators(or one annotator and one system) on set-valueditems (Nenkova et al, 2007).
Values range from0 to 1, with 1 representing perfect agreement.The DICE metric is also a measure of associationwhose value varies from 0 (no association) to 1 (to-tal association) (Dice, 1945).
The Accuracy met-ric is binary-valued: 1 if the attribute set is iden-tical to that selected by the human, 0 otherwise.The Uniqueness metric is also binary-valued: 1 ifthe attribute set uniquely identifies the target refer-ent among the distractors, 0 otherwise.
Finally, theMinimality metric is 1 if the selected attribute setis as small as possible (while still uniquely identi-fying the target referent), and 0 otherwise.
We note155that attribute selection algorithms such as Dale &Reiter?s are based on the observation that humansfrequently do not produce minimal referring ex-pressions.Results Table 1 shows the results for variationsof full brevity.
As we would expect, all approachesachieve a perfect score on uniqueness.
For bothcorpora, we see a large performance jump whenwe use speaker constraints for all metrics otherthan minimality.
However, when we incorporaterecency constraints as well performance declinesslightly.
We think this is due to two factors: first,the speakers are not in a conversation, and self-priming may have less impact than other-priming;and second, we do not always have the most recentprior utterance for a given speaker in the trainingdata.Table 1 also shows the results for variations ofDale & Reiter?s algorithm.
When we incorporatespeaker constraints, we again see a performancejump for most metrics, although compared to thebest possible case (full brevity) there is still roomfor improvement.We conclude that speaker constraints can be suc-cessfully used in standard attribute selection algo-rithms to improve performance on this task.The most relevant previous research is the workof (Gupta and Stent, 2005), who modified Daleand Reiter?s algorithm to model speaker adaptationin dialog.
However, this corpus does not involvedialog so there are no cross-speaker constraints,only within-speaker constraints (speaker style andpriming).6 Surface Realization ExperimentsData Preparation We first normalized the train-ing data to correct misspellings and remove punc-tuation and capitalization.
We then extracted aphrasal lexicon.
For each attribute value we ex-tracted the count of all realizations of that value inthe training data.
We treated locations as a spe-cial case, storing separately the realizations of x-y coordinate pairs and single x- or y-coordinates.We added a small number of realizations by handto cover possible attribute values not seen in thetraining data.Realization We ran two realization experiments.In the first experiment, we used the human-selected attribute sets in the development data asthe input to realization.
If we want to maxi-?
SED ACC Bleu NISTFurniturePermute&Rank 0.01 3.54 0.14 0.311 3.87Dependency 0.90 4.51 0.09 0.206 3.29Dependency-S 0.60 4.30 0.11 0.232 3.91Template 0.10 3.59 0.13 0.328 3.93Template-S 0.10 2.80 0.28 0.403 4.67PeoplePermute&Rank 0.04 4.37 0.10 0.227 3.15Dependency 0.70 6.10 0.00 0.072 2.35Dependency-S 0.50 5.84 0.02 0.136 3.05Template 0.80 3.87 0.07 0.250 3.18Template-S 0.70 3.79 0.15 0.265 3.59OverallPermute&Rank .01/.04 3.92 0.12 0.271 4.02Dependency 0.9/0.7 5.24 0.05 0.146 3.23Dependency-S 0.6/0.5 5.01 0.07 0.187 3.98Template 0.1/0.8 3.77 0.10 0.285 4.09Template-S 0.1/0.7 3.26 0.22 0.335 4.77Table 2: Results for realization using speakers?
at-tribute selection (SED: String Edit Distance, ACC:String Accuracy)mize humanlikeness, then using these attribute setsshould give us an idea of the best possible perfor-mance of our realization methods.
In the secondexperiment, we used the attribute sets output byour best-performing attribute selection algorithms(FB-sf and DR-sf) as the input to realization.Metrics We report performance of our surfacerealizers using the metrics used for the REG 2008shared challenge and standard metrics used in thenatural language generation and machine trans-lation communities.
String Edit Distance (SED)is a measure of the number of words that wouldhave to be added, deleted, or replaced in order totransform the generated referring expression intothe one produced by the human.
As used in theREG 2008 shared challenge, it is unnormalized, soits values range from zero up.
Accuracy (ACC)is binary-valued: 1 if the generated referring ex-pression is identical to that produced by the hu-man (after spelling correction and normalization),and 0 otherwise.
Bleu is an n-gram based met-ric that counts the number of 1, 2 and 3 gramsshared between the generated string and one ormore (preferably more) reference strings (Papeniniet al, 2001).
Bleu values are normalized and rangefrom 0 (no match) to 1 (perfect match).
Finally,the NIST metric is a variation on the Bleu met-ric that, among other things, weights rare n-gramshigher than frequently-occurring ones (Dodding-ton, 2002).
NIST values are unnormalized.156SED ACC Bleu NISTFurnitureFB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sfPermute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22Template-S 3.26 3.90 0.19 0.12 .362 .294 4.41 4.07PeoplePermute&Rank 4.75 5.82 0.09 0.03 .171 .110 2.70 2.31Dependency 6.35 6.91 0.00 0.00 .068 .073 1.81 1.86Dependency-S 5.94 6.18 0.01 0.00 .108 .113 2.73 2.41Template 3.62 4.24 0.07 0.04 .231 .138 2.88 1.35Template-S 3.76 4.38 0.12 0.06 .201 .153 2.76 1.88OverallPermute&Rank 4.33 4.96 0.09 0.05 .236 .235 3.73 3.72Dependency 5.51 6.00 0.02 0.01 .136 .091 2.97 2.50Dependency-S 5.36 5.67 0.04 0.02 .159 .136 3.77 3.25Template 3.76 4.41 0.08 0.05 .258 .180 3.69 2.89Template-S 3.48 4.12 0.16 0.09 .288 .229 4.15 3.58Table 3: Results for realization with different attribute selection algorithmsFurniture PeopleFB-sf DR-sf FB-sf DR-sfPermute&Rank .01 .05 .05 .04Dependency .9 .9 .9 .1Dependency-S .2 .2 .4 .4Template .8 .8 .8 .8Template-S .6 .8 .8 .8Table 4: Optimal ?
values with different attributeselection algorithmsResults Our experimental results are shown inTables 2 and 3.
(These results are the resultsobtained with the language model weighting thatgives best performance; the weights are shown inTables 2 and 4.)
Our approaches work better forthe furniture domain, where there are fewer at-tributes, than for the people domain.
For bothdomains, for automatic and human attribute se-lection, the speaker-dependent Template-based ap-proach seems to perform the best, then the speaker-independent Template-based approach, and thenthe Permute&Rank approach.
However, we findautomatic metrics for evaluating generation qual-ity to be unreliable.
We looked at the output of thesurface realizers for the two examples in Section 2.The best output for the example in Figure 1 is fromthe FB-sf template-based speaker-dependent algo-rithm, which is the big red sofa.
The worst out-put is from the DR-sf dependency-based speaker-dependent algorithm, which is on the left red chairwith three seats.
The best output for the exam-ple in Figure 2 is from the FB-sf template-basedspeaker-independent algorithm, which is the manwith the white beard.
The worst output is from theFB-sf dependency-based speaker-dependent algo-rithm, which is beard man white.Discussion The Template-S approach achievesthe best string edit distance scores, but it is not veryrobust.
If no examples are found in the trainingdata that realize (a superset of) the input attributeset, neither Template approach will produce anyoutput.The biggest cause of errors for the Permute andReorder approach is missing determiners and miss-ing modifiers.
The biggest cause of errors for theDependency approach is missing determiners andreordered words.
The Template approach some-times has repeated words (e.g.
?middle?, where?middle?
referred to both x- and y-coordinates).Here we report performance using automaticmetrics, but we find these metrics to be unreliable(particularly in the absence of multiple referencetexts).
Also, we are not sure that people would ac-cept from a computer system output that is veryhuman-like in this domain, as the human-like out-put is often ungrammatical and telegraphic (e.g.
?grey frontal table?).
We plan to do a human eval-uation soon to better analyze our systems?
perfor-mance.7 ConclusionsWhen building computational models of language,knowledge about the factors that influence humanlanguage production can prove very helpful.
Thisknowledge can be incorporated in frequentist andheuristic approaches as constraints or features.
Inthe experiments described in this paper, we used157data-driven, speaker-aware approaches to attributeselection and referring expression realization.
Weshowed that individual speaking style can be use-fully modeled even for quite ?small?
generationtasks, and confirmed that data-driven approachesto surface realization can work well using a rangeof lexical, syntactic and semantic information.We plan to explore the impact of human visualsearch strategies (Rayner, 1998) on the referringexpression generation task.
In addition, we areplanning a human evaluation of the generation sys-tems?
output.
Finally, we plan to apply our algo-rithms to a conversational task.AcknowledgmentsWe thank Anja Belz, Albert Gatt, and Eric Kowfor organizing the REG competition and providingdata, and Gregory Zelinsky for discussions aboutvisually-based constraints.ReferencesBangalore, S. and O. Rambow.
2000.
Exploiting aprobabilistic hierarchical model for generation.
InProc.
COLING.Bangalore, S., A. Emami, and P. Haffner.
2005.
Fac-toring global inference by enriching local represen-tations.
Technical report, AT&T Labs-Research.Belz, A. and A. Gatt.
2007.
The attribute selection forGRE challenge: Overview and evaluation results.
InProc.
UCNLG+MT at MT Summit XI.Belz, A.
2007.
Probabilistic generation of weatherforecast texts.
In Proc.
NAACL/HLT.Dale, R. and E. Reiter.
1995.
Computational interpre-tations of the Gricean maxims in the generation ofreferring expressions.
Cognitive Science, 19(2).Dice, L. 1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26.Doddington, G. 2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proc.
HLT.Gupta, S. and A. Stent.
2005.
Automatic evaluationof referring expression generation using corpora.
InProc.
UCNLG.Kanthak, S., D. Vilar, E. Matusov, R. Zens, and H. Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In Proc.
ACL Work-shop on Building and Using Parallel Texts.Krahmer, E., S. van Erk, and A. Verleg.
2003.
Graph-based generation of referring expressions.
Computa-tional Linguistics, 29(1).Langkilde, I. and K. Knight.
2000.
Forest-based statis-tical sentence generation.
In Proc.
NAACL.Levelt, W., 1989.
Speaking: From intention to articu-lation, pages 222?226.
MIT Press.Nasr, A. and O. Rambow.
2004.
Supertagging andfull parsing.
In Proc.
7th International Workshop onTree Adjoining Grammar and Related Formalisms(TAG+7).Nenkova, A., R. Passonneau, and K. McKeown.
2007.The Pyramid method: incorporating human con-tent selection variation in summarization evaluation.ACM Transactions on speech and language process-ing, 4(2).Papenini, K., S. Roukos, T. Ward, andW.-J.
Zhu.
2001.BLEU: A method for automatic evaluation of ma-chine translation.
In Proc.
ACL.Rayner, K. 1998.
Eye movements in reading and infor-mation processing: 20 years of research.
Psycholog-ical Bulletin, 124(3).Reiter, E. and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.Siddharthan, A. and A. Copestake.
2004.
Generat-ing referring expressions in open domains.
In Proc.ACL.158
