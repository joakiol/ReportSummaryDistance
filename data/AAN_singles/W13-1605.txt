Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 29?37,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsThe perfect solution for detecting sarcasm in tweets #notChristine LiebrechtCentre for Language StudiesRadboud University NijmegenP.O.
Box 9103NL-6500 HD Nijmegenc.liebrecht@let.ru.nlFlorian KunnemanCentre for Language StudiesRadboud University NijmegenP.O.
Box 9103NL-6500 HD Nijmegenf.kunneman@let.ru.nlAntal van den BoschCentre for Language StudiesRadboud University NijmegenP.O.
Box 9103NL-6500 HD Nijmegena.vandenbosch@let.ru.nlAbstractTo avoid a sarcastic message being understoodin its unintended literal meaning, in microtextssuch as messages on Twitter.com sarcasm isoften explicitly marked with the hashtag ?#sar-casm?.
We collected a training corpus of about78 thousand Dutch tweets with this hashtag.Assuming that the human labeling is correct(annotation of a sample indicates that about85% of these tweets are indeed sarcastic), wetrain a machine learning classifier on the har-vested examples, and apply it to a test set ofa day?s stream of 3.3 million Dutch tweets.Of the 135 explicitly marked tweets on thisday, we detect 101 (75%) when we remove thehashtag.
We annotate the top of the ranked listof tweets most likely to be sarcastic that do nothave the explicit hashtag.
30% of the top-250ranked tweets are indeed sarcastic.
Analysisshows that sarcasm is often signalled by hy-perbole, using intensifiers and exclamations;in contrast, non-hyperbolic sarcastic messagesoften receive an explicit marker.
We hypothe-size that explicit markers such as hashtags arethe digital extralinguistic equivalent of non-verbal expressions that people employ in liveinteraction when conveying sarcasm.1 IntroductionIn the general area of sentiment analysis, sarcasmplays a role as an interfering factor that can flip thepolarity of a message.
Unlike a simple negation, asarcastic message typically conveys a negative opin-ion using only positive words ?
or even intensifiedpositive words.
The detection of sarcasm is there-fore important, if not crucial, for the developmentand refinement of sentiment analysis systems, butis at the same time a serious conceptual and tech-nical challenge.
In this paper we introduce a sar-casm detection system for tweets, messages on themicroblogging service offered by Twitter.1In doing this we are helped by the fact that sar-casm appears to be a well-understood concept byTwitter users, as seen by the relatively accurate useof an explicit marker of sarcasm, the hashtag ?#sar-casm?.
Hashtags in messages on Twitter (tweets) areexplicitly marked keywords, and often act as cate-gorical labels or metadata in addition to the bodytext of the tweet.
By using the explicit hashtag anyremaining doubt a reader may have is taken away:the message is intended as sarcastic.In communication studies, sarcasm has beenwidely studied, often in relation with, or encom-passed by concepts such as irony as a broader cate-gory term, and in particular in relation with (or syn-onymous to) verbal irony.
A brief overview of def-initions, hypotheses and findings from communica-tion studies regarding sarcasm and verbal irony mayhelp clarify what the hashtag ?#sarcasm?
conveys.1.1 DefinitionsMany researchers treat irony and sarcasm asstrongly related (Attardo, 2007; Brown, 1980; Gibbsand O?Brien, 1991; Kreuz and Roberts, 1993;Muecke, 1969; Mizzau, 1984), and sometimes evenequate the terms in their studies in order to workwith an usable definition (Grice, 1978; Tsur et al2010).
We are interested in sarcasm as a linguisticphenomenon, and how we can detect it in social me-1http://www.twitter.com29dia messages.
Yet, Brown (1980) warns that sarcasm?is not a discrete logical or linguistic phenomenon?(p.
111), while verbal irony is; we take the libertyof using the term sarcasm while verbal irony wouldbe the more appropriate term.
Even then, accordingto Gibbs and Colston (2007) the definition of ver-bal irony is still a ?problem that surfaces in the ironyliterature?
(p. 584).There are many different theoretical approachesto verbal irony.
Burgers (2010), who provides anoverview of approaches, distinguishes a number offeatures in ironic utterances that need to be includedin an operational definition of irony: (1) irony is al-ways implicit (Giora, 1995; Grice, 1978), (2) ironyis evaluative (Attardo, 2000; Kotthoff, 2003; Sper-ber and Wilson, 1995), it is possible to (3) distin-guish between a non-ironic and an ironic reading ofthe same utterance (Grice, 1975; Grice, 1978), (4)between which a certain type of opposition may beobserved (see also Kawakami, 1984, 1988, summa-rized in (Hamamoto, 1998; Partington, 2007; Seto,1998).
Burgers?
own definition of verbal irony is ?anevaluative utterance, the valence of which is implic-itly reversed between the literal and intended evalu-ation?
(Burgers, 2010, p. 19).Thus, a sarcastic utterance involves a shift in eval-uative valence, which can go two ways: it could bea shift from a literally positive to an intended neg-ative meaning, or a shift from a literally negativeto an intended positive evaluation.
Since Reyes etal.
(2012b) also argue that users of social media of-ten use irony in utterances that involve a shift inevaluative valence, we use Burgers?
(2010) defini-tion of verbal irony in this study on sarcasm, andwe use both terms synonymously.
The definition ofirony as saying the opposite of what is meant is com-monly used in previous corpus-analytic studies, andis reported to be reliable (Kreuz et al 1996; Leigh,1994; Srinarawat, 2005).Irony is used relatively often in dialogic interac-tion.
Around 8% of conversational turns betweenAmerican college friends contains irony (Gibbs,2007).
According to Gibbs (2007), group membersuse irony to ?affirm their solidarity by directing com-ments at individuals who are not group members andnot deemed worthy of group membership?
(p. 341).When an individual sees a group?s normative stan-dards violated, he uses sarcasm to vent frustration.Sarcasm is also used when someone finds a situa-tion or object offensive (Gibbs, 2007).
Sarcasm orirony is always directed at someone or something;its target.
A target is the person or object againstwhom or which the ironic utterance is directed (Liv-nat, 2004).
Targets can be the sender himself, theaddressee or a third party (or a combination of thethree).
Burgers (2010) showed that in Dutch writtencommunication, the target of the ironic utterance isoften a third party.
These findings may be interest-ing for our research, in which we study microtextsof up to 140 characters from Twitter.Sarcasm in written and spoken interaction maywork differently (Jahandarie, 1999).
In spoken in-teraction, sarcasm is often marked with a specialintonation (Attardo et al 2003; Bryant and Tree,2005; Rockwell, 2007) or an incongruent facial ex-pression (Muecke, 1978; Rockwell, 2003; Attardoet al 2003).
Burgers (2010) argues that in writ-ten communication, authors do not have clues like ?aspecial intonation?
or ?an incongruent facial expres-sion?
at their disposal.
Since sarcasm is more diffi-cult to comprehend than a literal utterance (Gibbs,1986; Giora, 2003; Burgers, 2010), it is likely thataddressees do not pick up on the sarcasm and in-terpret the utterances literally.
Acoording to Gibbsand Izett (2005), sarcasm divides its addressees intotwo groups; a group of people who understand sar-casm (the so-called group of wolves) and a groupof people who do not understand sarcasm (the so-called group of sheep).
In order to ensure that the ad-dressees detect the sarcasm in the utterance, sendersuse linguistic markers in their utterances.
Accordingto Attardo (2000) those markers are clues a writercan give that ?alert a reader to the fact that a sen-tence is ironical?
(p. 7).
On Twitter, the hashtag?#sarcasm?
is a popular marker.1.2 IntensifiersThere are sarcastic utterances which would still bequalified as sarcastic when all markers were re-moved from it (Attardo et al 2003), for examplethe use of a hyperbole (Kreuz and Roberts, 1995).It may be that a sarcastic utterance with a hyper-bole (?fantastic weather?
when it rains) is identi-fied as sarcastic with more ease than a sarcastic ut-terance without a hyperbole (?the weather is good?when it rains).
While both utterances convey a lit-30erally positive attitude towards the weather, the ut-terance with the hyperbolic ?fantastic?
may be eas-ier to interpret as sarcastic than the utterance withthe non-hyperbolic ?good?.
Such hyperbolic wordswhich strengthen the evaluative utterance are calledintensifiers.
Bowers (1964) defines language inten-sity as ?the quality of language which indicates thedegree to which the speaker?s attitude toward a con-cept deviates from neutrality?
(p. 416).
Accordingto Van Mulken and Schellens (2012), an intensifieris a linguistic element that can be removed or re-placed while respecting the linguistic correctness ofthe sentence and context, but resulting in a weakerevaluation.
A commonly used way to intensify ut-terances is by using word classes such as adverbs(?very?)
or adjectives (?fantastic?
instead of ?good?
).It may be that senders use such intensifiers in theirtweets to make the utterance hyperbolic and therebysarcastic, without using a linguistic marker such as?#sarcasm?.1.3 OutlineIn this paper we describe the design and imple-mentation of a sarcasm detector that marks unseentweets as being sarcastic or not.
We analyse the pre-dictive performance of the classifier by testing its ca-pacity on test tweets that are explicitly marked withthe hashtag #sarcasme (Dutch for ?sarcasm?
), leftout during testing, and its capacity to rank likely sar-castic tweets that do not have the #sarcasme mark.We also provide a qualitative linguistic analysis ofthe features that the classifier thinks are the mostdiscriminative.
In a further qualitative analysis ofsarcastic tweets in the test set we find that the useof an explicit hashtag marking sarcasm occurs rela-tively often without other indicators of sarcasm suchas intensifiers or exclamations.2 Related ResearchThe automatic classification of communicative con-structs in short texts has become a widely researchedsubject in recent years.
Large amounts of opinions,status updates and personal expressions are postedon social media platforms such as Twitter.
The au-tomatic labeling of their polarity (to what extent atext is positive or negative) can reveal, when aggre-gated or tracked over time, how the public in gen-eral thinks about certain things.
See Montoyo et al(2012) for an overview of recent research in senti-ment analyis and opinion mining.A major obstacle for automatically determiningthe polarity of a (short) text are constructs in whichthe literal meaning of the text is not the intendedmeaning of the sender, as many systems for the de-tection of polarity primarily lean on positive andnegative words as markers.
The task to identifysuch constructs can improve polarity classification,and provide new insights into the relatively newgenre of short messages and microtexts on socialmedia.
Previous works describe the classificationof irony (Reyes et al 2012b), sarcasm (Tsur et al2010), satire (Burfoot and Baldwin, 2009), and hu-mor (Reyes et al 2012a).Most common to our research are the works byReyes et al(2012b) and Tsur et al(2010).
Reyes etal.
(2012b) collect a training corpus of irony basedon tweets that consist of the hashtag #irony in orderto train classifiers on different types of features (sig-natures, unexpectedness, style and emotional sce-narios) and try to distinguish #irony-tweets fromtweets containing the hashtags #education, #hu-mour, or #politics, achieving F1-scores of around70.
Tsur et al(2010) focus on product reviews onthe World Wide Web, and try to identify sarcasticsentences from these in a semi-supervised fashion.Training data is collected by manually annotatingsarcastic sentences, and retrieving additional train-ing data based on the annotated sentences as queries.Sarcasm is annotated on a scale from 1 to 5.
As fea-tures, Tsur et allook at the patterns in these sen-tences, consisting of high-frequency words and con-tent words.
Their system achieves an F1-score of 79on a testset of product reviews, after extracting andannotating a sample of 90 sentences classified as sar-castic and 90 sentences classified as not sarcastic.In the two works described above, a system istested in a controlled setting: Reyes et al(2012b)compare irony to a restricted set of other topics,while Tsur et al(2010) took from the unlabeledtest set a sample of product reviews with 50% ofthe sentences classified as sarcastic.
In contrast, weapply a trained sarcasm detector to a real-world testset representing a realistically large sample of tweetsposted on a specific day of which the vast majority isnot sarcastic.
Detecting sarcasm in social media is,31arguably, a needle-in-a-haystack problem (of the 3.3million tweets we gathered on a single day, 135 areexplicitly marked with the hashtag #sarcasm), and itis only reasonable to test a system in the context of atypical distribution of sarcasm in tweets.
Like in theresearch of (Reyes et al 2012b), we train a classifierbased on tweets with a specific hashtag.3 Experimental Setup3.1 DataFor the collection of tweets for this study we makeuse of a database provided by the Netherlands e-Science Centre, consisting of a substantial portionof all Dutch tweets posted from December 2010 on-wards.2 From this database, we collected all tweetsthat contained the marker ?#sarcasme?, the Dutchword for sarcasm with the hashtag prefix.
This re-sulted in a set of 77,948 tweets.
We also collectedall tweets posted on a single day, namely February1, 2013.3 This set of tweets contains approximately3,3 million tweets, of which 135 carry the hashtag#sarcasme.3.2 Winnow classificationBoth the collected tweets with a #sarcasme hash-tag and the tweets that were posted on a single daywere tokenized and stripped of punctuation.
Capi-tals were not removed, as they might be used to sig-nal sarcasm (Burgers, 2010).
We made use of worduni-, bi- and trigrams as features.
Terms that oc-curred three times or less or in two tweets or less inthe whole set were removed, as well as the hashtag#sarcasme.
Features were weighted by the ?2 met-ric.As classification algorithm we made use of Bal-anced Winnow (Littlestone, 1988) as implementedin the Linguistic Classification System.4 This algo-rithm is known to offer state-of-the-art results in textclassification, and produces interpretable per-classweights that can be used to, for example, inspectthe highest-ranking features for one class label.
The?
and ?
parameters were set to 1,05 and 0,95 re-spectively.
The major threshold (?+) and the minor2http://twiqs.nl/3All tweets from February 1, 2013 onwards were removedfrom the set of sarcasm tweets.4http://www.phasar.cs.ru.nl/LCS/threshold (??)
were set to 2,5 and 0,5.
The numberof iterations was bounded to a maximum of three.3.3 ExperimentIn order to train the classifier on distinctive featuresof sarcasm in tweets, we combined the set of 78thousand sarcasm tweets with a random sample ofother tweets posted on February 1, 2013 as back-ground corpus.
We made sure the background cor-pus did not contain any of the 135 explicitly markedsarcasm tweets posted that day.
As the size of abackground corpus can influence the performance ofthe classifier (in doubt, a classifier will be biased bythe skew of the distribution of classes in the training-data), we performed a comparitive experiment withtwo distributions between sarcasm-labeled tweetsand background tweets: in the first variant, the di-vision between the two is 50%?50%, in the second,25% of the tweets are sarcasm-labeled, and 75% arebackground.4 ResultsTo evaluate the outcome of our machine learning ex-periment, we ran two evaluations.
The first evalu-ation focuses on the 135 tweets with explicit #sar-casme hastags posted on February 1, 2013.
We mea-sured how well these tweets were identified usingthe true positive rate (TPR), false positive rate (FPR,also known as recall), and their joint score, the areaunder the curve (AUC).
AUC is a common evalua-tion metric that is argued to be more resistant to skewthan F-score, due to using TPR rather than precision(Fawcett, 2004).
Results are displayed in Table 1.The first evaluation, on the variant with a balanceddistribution of the two classes, leads to a retrieval of101 of the 135 sarcasm-tweets (75%), while nearly500 thousand tweets outside of these were also clas-sified as being sarcastic.
When a quarter of the train-ing tweets has a sarcasm label, a smaller amount of76 sarcasm tweets are retrieved.
The AUC scores forthe two ratios indicates that the 50%?50% balanceleads to the highest AUC score (0.79) for sarcasm.Our subsequent analyses are based on the outcomeswhen using this distribution in training.Besides generating an absolute winner-take-allclassification, our Balanced Winnow classifier alsoassigns scores to each label that can be seen as its32Pos/Neg Ratio # Training # TestTraining Examples Label tweets tweets TPR FPR AUC Classified Correct50/50 sarcasm 77,948 135 0,75 0,16 0,79 487,955 101background 77,499 3,246,806 0,79 0,25 0,77 2,575,206 2,575,17325/75 sarcasm 77,948 135 0,56 0,05 0,75 162,400 76background 233,834 3,090,472 0,92 0,43 0,74 2,830,103 2,830,045Table 1: Scores on the test set with two relative sizes of background tweets (TPR = True Positive Rate, FPR = FalsePositive Rate, AUC = Area Under the Curveconfidence in that label.
We can rank its predictionsby the classifier?s confidence on the ?sarcasm?
la-bel and inspect manually which of the top-rankingtweets is indeed sarcastic.
We generated a list of the250 most confident ?sarcasm?-labeled tweets.
Threeannotators (the authors of this paper) made a judge-ment for these tweets as being either sarcastic or not.In order to test for intercoder reliability, Cohen?sKappa was used.
In line with Siegel and Castellan(1988), we calculated a mean Kappa based on pair-wise comparisons of all possible coder pairs.
Themean intercoder reliability between the three possi-ble coder pairs is substantial (?
= .79).When taking the majority vote of the three an-notators as the golden label, a curve of the preci-sion at all points in the ranking can be plotted.
Thiscurve is displayed in Figure 1.
As can be seen, theoverall performance is poor (the average precisionis 0.30).
After peaking at 0.50 after 22 tweets, pre-cision slowly decreases when descending to lowerrankings.
During the first five tweets, the curve is at0.0; these tweets, receiving the highest overall con-fidence scores, are relatively short and contain onestrong sarcasm feature in the classifier without anynegative feature.5 AnalysisOur first closer analysis of our results concerns thereliability of the user-generated hastag #sarcasme asa golden label, as Twitter users cannot all be as-sumed to be experts in sarcasm or understand whatsarcasm is.
The three annotators who annotated theranked classifier output also coded a random sam-ple of 250 tweets with the #sarcasme hashtag fromthe training set.
The average score of agreement be-tween the three possible coder pairs turned out to bemoderate (?
= .54).
Taking the majority vote overFigure 1: Precision at {1 .
.
.
250} on the sarcasm classthe three annotations as the reference labeling, 85%(212) of the 250 annotated #sarcasme tweets werefound to be sarcastic.While the classifier performance gives an impres-sion of its ability to distinguish sarcastic tweets, thestrong indicators of sarcasm as discovered by theclassifier may provide additional insight into the us-age of sarcasm by Twitter users: in particular, thetypical targets of sarcasm, and the different linguis-tic markers that are used.
We thus set out to ana-lyze the feature weights assigned by the BalancedWinnow classifier ranked by the strength of theirconnection to the sarcasm label, taking into accountthe 500 words and n-grams with the highest positiveweight towards the sarcasm class.
These words andn-grams provide insight into the topics Twitter usersare talking about: their targets.
People often talkabout school and related subjects such as homework,books, exams, classes (French, chemistry, physics),teachers, the school picture, sports day, and (return-ing from) vacation.
Another popular target of sar-33casm is the weather: the temperature, rain, snow,and sunshine.
Apart from these two common top-ics, people tend to be sarcastic about social mediaitself, holidays, public transport, soccer, televisionprograms (The Voice of Holland), celebrities (JustinBieber), the church, the dentist and vacuum clean-ing.
Many of these topics are indicative of the youngage, on average, of Twitter users.The strongest linguistic markers of sarcastic ut-terances are markers that can be seen as syn-onyms for #sarcasme, such as sarcasme (without#), #ironie and ironie (irony), #cynisme and cynisme(cynicism), or words that are strongly related tothose concepts by marking the opposite of the ex-pressed utterance: #humor, #LOL, #joke (grapje),and #NOT.Second, the utterances contain much positive ex-clamations that make the utterance hyperbolic andthereby sarcastic.
Examples of those markers inDutch are (with and without # and/or capitals): jip-pie, yes, goh, joepie, jeej, jeuj, yay, woehoe, andwow.We suspected that the sarcastic utterances con-tained intensifiers to make the tweets hyperbolic.The list of strongest predictors show that some inten-sifiers are indeed strong predictors of sarcasm, suchas geweldig (awesome), heerlijk (lovely), prachtig(wonderful), natuurlijk (of course), gelukkig (for-tunately), zoooo (soooo), allerleukste (most fun),fantastisch (fantastic), and heeel (veeery).
Besidesthese intensifiers many unmarked positive words oc-cur in the list of strongest predictors as well, suchas fijn (nice), gezellig (cozy), leuk (fun), origi-neel (original), slim (smart), favoriet (favorite), nut-tig (useful), and chill.
Considerably less negativewords occur as strong predictors.
This supports ourhypothesis that the utterances are mostly positive,while the opposite meaning is meant.
This find-ing corresponds with the results of Burgers (2010),who show that 77% of the ironic utterances in Dutchcommunication are literally positive.To inspect whether sarcastic tweets are always in-tensified to be hyperbolic, we need to further analysethe sarcastic tweets our classifier correctly identifies.Analyzing the 76 tweets that our classifier correctlyidentifies in the top-250 tweets the classifier rates assarcastic, we see that intensifiers do not dominatein occurrence; supporting numbers are listed in Ta-RelativeoccurrenceType (%)Marker only 34.2Intensifier only 9.2Exclamation only 17.1Marker + Intensifier 10.5Marker + Exclamation 9.2Intensifier + Exclamation 10.5Marker + Intensifier + Exclamation 2.6Other 6.6Total 100Table 2: Relative occurrence (%) of word types and theircombinations in the tweets annotated as sarcastic by amajority vote.ble 2.
About one in three sarcastic tweets, 34.2%,are not hyperbolic at all: they are only explicitlymarked, most of the times with a hashtag.
A major-ity of 59.2% of the tweets does contain hyperbole-inducing elements, such as an intensifier or an ex-clamation, or combinations of these elements.
A fullcombination of explicit markers, intensifiers, and ex-clamations only rarely occurs, however (2.6%).
Thethree categories of predictive word types do cover93.4% of the tweets.6 ConclusionIn this study we developed and tested a system thatdetects sarcastic tweets in a realistic sample of 3.3million Dutch tweets posted on a single day, trainedon a set of nearly 78 thousand tweets, harvestedover time, marked by the hashmark #sarcasme bythe senders.
The classifier is able to correctly detect101 of the 135 tweets among the 3.3 million thatwere explicitly marked with the hashtag, with thehashtag removed.
Testing the classifier on the top250 of the tweets it ranked as most likely to be sar-castic, it attains only a 30% average precision.
Wecan conclude that it is fairly hard to distinguish sar-castic tweets from literal tweets in an open setting,though the top of the classifier?s ranking does iden-tify many sarcastic tweets which were not explicitlymarked with a hashtag.An additional linguistic analysis provides someinsights into the characteristics of sarcasm on Twit-ter.
We found that most tweets contain a literally34positive message, take common teenager topics astarget (school, homework, family life) and furthercontain three types of words: explicit markers (theword sarcasme and pseudo-synonyms, with or with-out the hashmark #), intensifiers, and exclamations.The latter two categories of words induce hyper-bole, but together they only occur in about 60%of sarcastic tweets; in 34% of the cases, sarcastictweets are not hyperbolic, but only have an explicitmarker, most of which hashtags.
This indicates thatthe hashtag can and does replace linguistic markersthat otherwise would be needed to mark sarcasm.Arguably, extralinguistic elements such as hashtagscan be seen as the social media equivalent of non-verbal expressions that people employ in live inter-action when conveying sarcasm.
As Burgers (2010)show, the more explicit markers an ironic utterancecontains, the better the utterance is understood, theless its perceived complexity is, and the better it israted.
Many Twitter users already seem to apply thisknowledge.Although in this research we focused on theDutch language, our findings may also apply to lan-guages similar to Dutch, such as English and Ger-man.
Future research would be needed to chart theprediction of sarcasm in languages that are more dis-tant to Dutch.
Sarcasm may be used differently inother cultures (Goddard, 2006).
Languages may usethe same type of marker in different ways, such asa different intonation in spoken sarcasm by Englishand Cantonese speakers (Cheang and Pell, 2009).Such a difference between languages in the use ofthe same marker may also apply to written sarcasticutterances.Another strand of future research would be to ex-pand our scope from sarcasm to other more sub-tle variants of irony, such as understatements, eu-phemisms, and litotes.
Based on Giora et al(2005),there seems to be a spectrum of degrees of ironyfrom the sarcastic ?Max is exceptionally bright?
viathe ironic ?Max is not exceptionally bright?, the un-derstatement ?Max is not bright?
to the literal ?Maxis stupid?.
In those utterances, there is a gap betweenwhat is literally said and the intended meaning of thesender.
The greater the gap or contrast, the easier itis to perceive the irony.
But the negated not brightis still perceived as ironic; more ironic than the lit-eral utterance (Giora et al 2005).
We may need tocombine the sarcasm detection task with the prob-lem of the detection of negation and hedging mark-ers and their scope (Morante et al 2008; Moranteand Daelemans, 2009) in order to arrive at a compre-hensive account of polarity-reversing mechanisms,which in sentiment analysis is still highly desirable.ReferencesS.
Attardo, J. Eisterhold, J. Hay, and I. Poggi.
2003.Visual markers of irony and sarcasm.
Humor,16(2):243?260.S.
Attardo.
2000.
Irony as relevant inappropriateness.Journal of Pragmatics, 32(6):793?826.S.
Attardo.
2007.
Irony as relevant inappropriateness.In R. W. Gibbs, R. W. Gibbs Jr., and H. Colston, ed-itors, Irony in language and thought: A cognitive sci-ence reader, pages 135?170.
Lawrence Erlbaum, NewYork, NY.J.
W. Bowers.
1964.
Some correlates of language in-tensity.
Quarterly Journal of Speech, 50(4):415?420,December.R.
L. Brown.
1980.
The pragmatics of verbal irony.
InR.
W. Shuy and A. Shnukal, editors, Language useand the uses of language, pages 111?127.
GeorgetownUniversity Press, Washington, DC.G.
A. Bryant and J. E. Fox Tree.
2005.
Is there an ironictone of voice?
Language and Speech, 48(3):257?277.C.
Burfoot and T. Baldwin.
2009.
Automatic satire de-tection: Are you having a laugh?
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,pages 161?164.
Association for Computational Lin-guistics.C.
F. Burgers.
2010.
Verbal irony: Use and effects inwritten discourse.
Ipskamp, Nijmegen, The Nether-lands.H.
S. Cheang and M. D. Pell.
2009.
Acoustic markers ofsarcasm in Cantonese and English.
The Journal of theAcoustical Society of America, 126:1394.T.
Fawcett.
2004.
ROC graphs: Notes and practicalconsiderations for researchers.
Technical Report HPL-2003-4, Hewlett Packard Labs.R.
W. Gibbs and H. Colston.
2007.
Irony as persua-sive communication.
In R. W. Gibbs, R. W. Gibbs Jr.,and H. Colston, editors, Irony in language and thougt:A cognitive science reader, pages 581?595.
LawrenceErlbaum, New York, NY.R.
W. Gibbs and C. Izett.
2005.
Irony as persuasive com-munication.
In H. Colston and A. Katz, editors, Fig-urative language comprehension: Social and culturalinfluences, pages 131?151.
Lawrence Erlbaum, NewYork, NY.35R.
W. Gibbs and J. O?Brien.
1991.
Psychological as-pects of irony understanding.
Journal of pragmatics,16(6):523?530.R.
W. Gibbs.
1986.
On the psycholinguistics of sar-casm.
Journal of Experimental Psychology: General,115(1):3.R.
W. Gibbs.
2007.
On the psycholinguistics of sarcasm.In R. W. Gibbs, R. W. Gibbs Jr., and H. Colston, ed-itors, Irony in language and thougt: A cognitive sci-ence reader, pages 173?200.
Lawrence Erlbaum, NewYork, NY.R.
Giora, O. Fein, J. Ganzi, N. Levi, and H. Sabah.
2005.On negation as mitigation: the case of negative irony.Discourse Processes, 39(1):81?100.R.
Giora.
1995.
On irony and negation.
Discourse pro-cesses, 19(2):239?264.R.
Giora.
2003.
On our mind: Salience, context, andfigurative language.
Oxford University Press.C.
Goddard.
2006.
?lift your game Martina!?
: Dead-pan jocular irony and the ethnopragmatics of Aus-tralian English.
APPLICATIONS OF COGNITIVELINGUISTICS, 3:65.H.
Grice.
1975.
Logic and conversation.
In P. Cole andJ.
Morgan, editors, Speech acts: Syntax and semantics,pages 41?58.
Academic Press, New York, NY.H.
Grice.
1978.
Further notes on logic and conversation.In P. Cole, editor, Pragmatics: syntax and semantics,pages 113?127.
Academic Press, New York, NY.H.
Hamamoto.
1998.
Irony from a cognitive perspective.In R. Carston and S. Uchida, editors, Relevance the-ory: Applications and implications, pages 257?270.John Benjamins, Amsterdam, The Netherlands.K.
Jahandarie.
1999.
Spoken and written discourse: Amulti-disciplinary perspective.
Greenwood PublishingGroup.H.
Kotthoff.
2003.
Responding to irony in different con-texts: On cognition in conversation.
Journal of Prag-matics, 35(9):1387?1411.R.
J. Kreuz and R. M. Roberts.
1993.
The empiri-cal study of figurative language in literature.
Poetics,22(1):151?169.R.
J. Kreuz and R. M. Roberts.
1995.
Two cues forverbal irony: Hyperbole and the ironic tone of voice.Metaphor and symbol, 10(1):21?31.R.
Kreuz, R. Roberts, B. Johnson, and E. Bertus.
1996.Figurative language occurrence and co-occurrence incontemporary literature.
In R. Kreuz and M. Mac-Nealy, editors, Empirical approaches to literature andaesthetics, pages 83?97.
Ablex, Norwood, NJ.J.
H Leigh.
1994.
The use of figures of speech in printad headlines.
Journal of Advertising, pages 17?33.N.
Littlestone.
1988.
Learning quickly when irrelevantattributes abound: A new linear-threshold algorithm.Machine Learning, 2:285?318.Z.
Livnat.
2004.
On verbal irony, meta-linguistic knowl-edge and echoic interpretation.
Pragmatics & Cogni-tion, 12(1):57?70.M.
Mizzau.
1984.
L?ironia: la contraddizione consen-tita.
Feltrinelli, Milan, Italy.A.
Montoyo, P.
Mart?
?nez-Barco, and A. Balahur.
2012.Subjectivity and sentiment analysis: An overview ofthe current state of the area and envisaged develop-ments.
Decision Support Systems.R.
Morante and W. Daelemans.
2009.
Learning thescope of hedge cues in biomedical texts.
In Proceed-ings of the Workshop on BioNLP, pages 28?36.
Asso-ciation for Computational Linguistics.R.
Morante, A. Liekens, and W. Daelemans.
2008.Learning the scope of negation in biomedical texts.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 715?724.D.
C. Muecke.
1969.
The compass of irony.
OxfordUniv Press.D.
C. Muecke.
1978.
Irony markers.
Poetics, 7(4):363?375.A.
Partington.
2007.
Irony and reversal of evaluation.Journal of Pragmatics, 39(9):1547?1569.A.
Reyes, P. Rosso, and D. Buscaldi.
2012a.
From hu-mor recognition to irony detection: The figurative lan-guage of social media.
Data & Knowledge Engineer-ing.A.
Reyes, P. Rosso, and T. Veale.
2012b.
A multidimen-sional approach for detecting irony in twitter.
Lan-guage Resources and Evaluation, pages 1?30.P.
Rockwell.
2003.
Empathy and the expression andrecognition of sarcasm by close relations or strangers.Perceptual and motor skills, 97(1):251?256.P.
Rockwell.
2007.
Vocal features of conversationalsarcasm: A comparison of methods.
Journal of psy-cholinguistic research, 36(5):361?369.K.-i.
Seto.
1998.
On non-echoic irony.
In R. Car-son and S. Uchida, editors, Relevance theory: Appli-cations and implications, pages 239?255.
John Ben-jamins, Amsterdam, The Netherlands.S.
Siegel and N. Castellan.
1988.
Nonparametric statis-tics for the behavioral sciences.
McGraw Hill, NewYork.D.
Sperber and D. Wilson.
1995.
Relevance: Commu-nication and cognition.
Blackwell Publishers, Oxford,UK, 2nd edition.D.
Srinarawat.
2005.
Indirectness as a politeness strat-egy of Thai speakers.
In R. Lakoff and S. Ide, editors,Broadening the horizon of linguistic politeness, pages175?193.
John Benjamins, Amsterdam, The Nether-lands.36O.
Tsur, D. Davidov, and A. Rappoport.
2010.
Icwsm?a great catchy name: Semi-supervised recognition ofsarcastic sentences in online product reviews.
In Pro-ceedings of the Fourth International AAAI Conferenceon Weblogs and Social Media, pages 162?169.M.
Van Mulken and P. J. Schellens.
2012.
Over lood-zware bassen en wapperende broekspijpen.
gebruik enperceptie van taalintensiverende stijlmiddelen.
Tijd-schrift voor taalbeheersing, 34(1):26?53.37
