Unsupervised Learning of Dependency Structure for Language ModelingJianfeng GaoMicrosoft Research, Asia49 Zhichun Road, Haidian DistrictBeijing 100080 Chinajfgao@microsoft.comHisami SuzukiMicrosoft ResearchOne Microsoft WayRedmond WA 98052 USAhisamis@microsoft.comAbstractThis paper presents a dependency languagemodel (DLM) that captures linguistic con-straints via a dependency structure, i.e., a setof probabilistic dependencies that expressthe relations between headwords of eachphrase in a sentence by an acyclic, planar,undirected graph.
Our contributions arethree-fold.
First, we incorporate the de-pendency structure into an n-gram languagemodel to capture long distance word de-pendency.
Second, we present an unsuper-vised learning method that discovers thedependency structure of a sentence using abootstrapping procedure.
Finally, weevaluate the proposed models on a realisticapplication (Japanese Kana-Kanji conver-sion).
Experiments show that the best DLMachieves an 11.3% error rate reduction overthe word trigram model.1 IntroductionIn recent years, many efforts have been made toutilize linguistic structure in language modeling,which for practical reasons is still dominated bytrigram-based language models.
There are twomajor obstacles to successfully incorporating lin-guistic structure into a language model: (1) captur-ing longer distance word dependencies leads tohigher-order n-gram models, where the number ofparameters is usually too large to estimate; (2)capturing deeper linguistic relations in a languagemodel requires a large annotated training corpusand a decoder that assigns linguistic structure,which are not always available.This paper presents a new dependency languagemodel (DLM) that captures long distance linguisticconstraints between words via a dependencystructure, i.e., a set of probabilistic dependenciesthat capture linguistic relations between headwordsof each phrase in a sentence.
To deal with the firstobstacle mentioned above, we approximatelong-distance linguistic dependency by a model thatis similar to a skipping bigram model in which theprediction of a word is conditioned on exactly oneother linguistically related word that lies arbitrarilyfar in the past.
This dependency model is then in-terpolated with a headword bigram model and aword trigram model, keeping the number of pa-rameters of the combined model manageable.
Toovercome the second obstacle, we used an unsu-pervised learning method that discovers the de-pendency structure of a given sentence using anExpectation-Maximization (EM)-like procedure.
Inthis method, no manual syntactic annotation isrequired, thereby opening up the possibility forbuilding a language model that performs well on awide variety of data and languages.
The proposedmodel is evaluated using Japanese Kana-Kanjiconversion, achieving significant error rate reduc-tion over the word trigram model.2 MotivationA trigram language model predicts the next wordbased only on two preceding words, blindly dis-carding any other relevant word that may lie threeor more positions to the left.
Such a model is likelyto be linguistically implausible: consider the Eng-lish sentence in Figure 1(a), where a trigram modelwould predict cried from next seat, which does notagree with our intuition.
In this paper, we define adependency structure of a sentence as a set ofprobabilistic dependencies that express linguisticrelations between words in a sentence by an acyclic,planar graph, where two related words are con-nected by an undirected graph edge (i.e., we do notdifferentiate the modifier and the head in a de-pendency).
The dependency structure for the sen-tence in Figure 1(a) is as shown; a model that usesthis dependency structure would predict cried frombaby, in agreement with our intuition.
(a) [A baby] [in the next seat] cried [throughout the flight](b) [/] [/	] [/	] [/] [] [/]Figure 1.
Examples of dependency structure.
(a) Adependency structure of an English sentence.
Squarebrackets indicate base NPs; underlined words are theheadwords.
(b) A Japanese equivalent of (a).
Slashesdemarcate morpheme boundaries; square bracketsindicate phrases (bunsetsu).A Japanese sentence is typically divided intonon-overlapping phrases called bunsetsu.
As shownin Figure 1(b), each bunsetsu consists of one con-tent word, referred to here as the headword H, andseveral function words F. Words (more precisely,morphemes) within a bunsetsu are tightly boundwith each other, which can be adequately capturedby a word trigram model.
However, headwordsacross bunsetsu boundaries also have dependencyrelations with each other, as the diagrams in Figure1 show.
Such long distance dependency relationsare expected to provide useful and complementaryinformation with the word trigram model in the taskof next word prediction.In constructing language models for realisticapplications such as speech recognition and Asianlanguage input, we are faced with two constraintsthat we would like to satisfy: First, the model mustoperate in a left-to-right manner, because (1) thesearch procedures for predicting words that corre-spond to the input acoustic signal or phonetic stringwork left to right, and (2) it can be easily combinedwith a word trigram model in decoding.
Second, themodel should be computationally feasible both intraining and decoding.
In the next section, we offera DLM that satisfies both of these constraints.3 Dependency Language ModelThe DLM attempts to generate the dependencystructure incrementally while traversing the sen-tence left to right.
It will assign a probability toevery word sequence W and its dependency struc-ture D. The probability assignment is based on anencoding of the (W, D) pair described below.Let W be a sentence of length n words to whichwe have prepended <s> and appended </s> so thatw0 = <s>, and wn+1 = </s>.
In principle, a languagemodel recovers the probability of a sentence P(W)over all possible D given W by estimating the jointprobability P(W, D): P(W) = ?D P(W, D).
In prac-tice, we used the so-called maximum approximationwhere the sum is approximated by a single termP(W, D*):?
?
?=DDWPDWPWP ),(),()( .
(1)Here, D* is the most probable dependency structureof the sentence, which is generally discovered bymaximizing P(W, D):DDWPD ),(maxarg=?
.
(2)Below we restrict the discussion to the most prob-able dependency structure of a given sentence, andsimply use D to represent D*.
In the remainder ofthis section, we first present a statistical dependencyparser, which estimates the parsing probability atthe word level, and generates D incrementally whiletraversing W left to right.
Next, we describe theelements of the DLM that assign probability to eachpossible W and its most probable D, P(W, D).
Fi-nally, we present an EM-like iterative method forunsupervised learning of dependency structure.3.1 Dependency parsingThe aim of dependency parsing is to find the mostprobable D of a given W by maximizing the prob-ability P(D|W).
Let D be a set of probabilistic de-pendencies d, i.e.
d ?
D. Assuming that the de-pendencies are independent of each other, we have?
?=DdWdPWDP )|()|((3)where P(d|W) is the dependency probability condi-tioned by a particular sentence.1 It is impossible toestimate P(d|W) directly because the same sentenceis very unlikely to appear in both training and testdata.
We thus approximated P(d|W) by P(d), andestimated the dependency probability from thetraining corpus.
Let dij = (wi, wj) be the dependency1The model in Equation (3) is not strictly probabilisticbecause it drops the probabilities of illegal dependencies(e.g., crossing dependencies).between wi and wj.
The maximum likelihood esti-mation (MLE) of P(dij) is given by),(),,()(jijiijwwCRwwCdP =(4)where C(wi, wj, R) is the number of times wi and wjhave a dependency relation in a sentence in trainingdata, and C(wi, wj) is the number of times wi and wjare seen in the same sentence.
To deal with the datasparseness problem of MLE, we used the backoffestimation strategy similar to the one proposed inCollins (1996), which backs off to estimates thatuse less conditioning context.
More specifically, weused the following three estimates:444323223111 ???????
?=++== EEE ,(5)Where),,(1 RwwC ji=?
, ),(1 ji wwC=?
,),*,(2 RwC i=?
, ,*)(2 iwC=?
,),(*,3 RwC j=?
, )(*,3 jwC=?
,)(*,*,4 RC=?
, (*,*)4 C=?
.in which * indicates a wild-card matching anyword.
The final estimate E is given by linearlyinterpolating these estimates:))1()(1( 42232111 EEEE ????
?+?+=  (6)where ?1 and ?2 are smoothing parameters.Given the above parsing model, we used an ap-proximation parsing algorithm that is O(n2).
Tradi-tional techniques use an optimal Viterbi-style algo-rithm (e.g., bottom-up chart parser) that is O(n5).2Although the approximation algorithm is notguaranteed to find the most probable D, we optedfor it because it works in a left-to-right manner, andis very efficient and simple to implement.
In ourexperiments, we found that the algorithm performsreasonably well on average, and its speed and sim-plicity make it a better choice in DLM trainingwhere we need to parse a large amount of trainingdata iteratively, as described in Section 3.3.The parsing algorithm is a slightly modifiedversion of that proposed in Yuret (1998).
It reads asentence left to right; after reading each new word2For parsers that use bigram lexical dependencies, Eis-ner and Satta (1999) presents parsing algorithms that areO(n4) or O(n3).
We thank Joshua Goodman for pointingthis out.wj, it tries to link wj to each of its previous words wi,and push the generated dependency dij into a stack.When a dependency crossing or a cycle is detectedin the stack, the dependency with the lowest de-pendency probability in conflict is eliminated.
Thealgorithm is outlined in Figures 2 and 3.DEPENDENCY-PARSING(W)1 for j ?
1 to LENGTH(W)2 for i ?
j-1 downto 13 PUSH dij = (wi, wj) into the stack Dj4 if a dependency cycle (CY) is detected in Dj(see Figure 3(a))5 REMOVE d, where )(minarg dPdCYd?=6 while a dependency crossing (CR) is detectedin Dj (see Figure 3(b)) do7 REMOVE d, where )(minarg dPdCRd?=8 OUTPUT(D)Figure 2.
Approximation algorithm of dependencyparsing(a) (b)Figure 3.
(a) An example of a dependency cycle: giventhat P(d23) is smaller than P(d12) and P(d13), d23 isremoved (represented as dotted line).
(b) An example ofa dependency crossing: given that P(d13) is smaller thanP(d24), d13 is removed.Let the dependency probability be the measure ofthe strength of a dependency, i.e., higher probabili-ties mean stronger dependencies.
Note that when astrong new dependency crosses multiple weakdependencies, the weak dependencies are removedeven if the new dependency is weaker than the sumof the old dependencies.
3  Although this actionresults in lower total probability, it was imple-mented because multiple weak dependencies con-nected to the beginning of the sentence often pre-3This operation leaves some headwords disconnected; insuch a case, we assumed that each disconnected head-word has a dependency relation with its precedingheadword.w1 w2 w3 w1w2 w3 w4vented a strong meaningful dependency from beingcreated.
In this manner, the directional bias of theapproximation algorithm was partially compen-sated for.43.2 Language modelingThe DLM together with the dependency parserprovides an encoding of the (W, D) pair into a se-quence of elementary model actions.
Each actionconceptually consists of two stages.
The first stageassigns a probability to the next word given the leftcontext.
The second stage updates the dependencystructure given the new word using the parsingalgorithm in Figure 2.
The probability P(W, D) iscalculated as:=),( DWP  (7)?=??????
?njjjjjjjjj wDWDPDWwP111111 )),,(|()),(|(=????
)),,(|( 111 jjjjj wDWDP  (8)?=??
?jijijjjji ppDWpP11111 ),...,,,|( .Here (Wj-1, Dj-1) is the word-parse (j-1)-prefix thatDj-1 is a dependency structure containing only thosedependencies whose two related words are includedin the word (j-1)-prefix, Wj-1.
wj is the word to bepredicted.
Dj-1j is the incremental dependencystructure that generates Dj = Dj-1 || Dj-1j (|| stands forconcatenation) when attached to Dj-1; it is the de-pendency structure built on top of Dj-1 and thenewly predicted word wj (see the for-loop of line 2in Figure 2).
pij denotes the ith action of the parser atposition j in the word string: to generate a newdependency dij, and eliminate dependencies withthe lowest dependency probability in conflict (seelines 4 ?
7 in Figure 2).
?
is a function that maps thehistory (Wj-1, Dj-1) onto equivalence classes.The model in Equation (8) is unfortunately in-feasible because it is extremely difficult to estimatethe probability of pij due to the large number ofparameters in the conditional part.
According to theparsing algorithm in Figure 2, the probability of4Theoretically, we should arrive at the same dependencystructure no matter whether we parse the sentence left toright or right to left.
However, this is not the case with theapproximation algorithm.
This problem is called direc-tional bias.each action pij  depends on the entire history (e.g.for detecting a dependency crossing or cycle), soany mapping ?
that limits the equivalence classi-fication to less context suitable for model estima-tion would be very likely to drop critical conditionalinformation for predicting pij.
In practice, we ap-proximated P(Dj-1j| ?
(Wj-1, Dj-1), wj) by P(Dj|Wj) ofEquation (3), yielding P(Wj, Dj) ?
P(Wj| ?
(Wj-1,Dj-1)) P(Dj|Wj).
This approximation is probabilisti-cally deficient, but our goal is to apply the DLM to adecoder in a realistic application, and the perform-ance gain achieved by this approximation justifiesthe modeling decision.Now, we describe the way P(wj|?
(Wj-1,Dj-1)) isestimated.
As described in Section 2, headwordsand function words play different syntactic andsemantic roles capturing different types of de-pendency relations, so the prediction of them canbetter be done separately.
Assuming that each wordtoken can be uniquely classified as a headword or afunction word in Japanese, the DLM can be con-ceived of as a cluster-based language model withtwo clusters, headword H and function word F. Wecan then define the conditional probability of wjbased on its history as the product of two factors:the probability of the category given its history, andthe probability of wj given its category.
Let hj or fj bethe actual headword or function word in a sentence,and let Hj or Fj be the category of the word wj.P(wj|?
(Wj-1,Dj-1)) can then be formulated as:=???
)),(|( 11 jjj DWwP   (9))),,(|()),(|( 1111 jjjjjjj HDWwPDWHP ????
???
)),,(|()),(|( 1111 jjjjjjj FDWwPDWFP ????
??
?+ .We first describe the estimation of headwordprobability P(wj | ?
(Wj-1, Dj-1), Hj).
Let HWj-1 be theheadwords in (j-1)-prefix, i.e., containing onlythose headwords that are included in Wj-1.
BecauseHWj-1 is determined by Wj-1, the headword prob-ability can be rewritten as P(wj | ?
(Wj-1, HWj-1, Dj-1),Hj).
The problem is to determine the mapping ?
soas to identify the related words in the left contextthat we would like to condition on.
Based on thediscussion in Section 2, we chose a mapping func-tion that retains (1) two preceding words wj-1 andwj-2 in Wj-1, (2) one preceding headword hj-1 inHWj-1, and (3) one linguistically related word wiaccording to Dj-1.
wi is determined in two stages:First, the parser updates the dependency structureDj-1 incrementally to Dj assuming that the next wordis wj.
Second, when there are multiple words thathave dependency relations with wj in Dj, wi is se-lected using the following decision rule:),|(maxarg),(:RwwPw ijDwwwijjii ?= , (10)where the probability P(wj | wi, R) of the word wjgiven its linguistic related word wi is computedusing MLE by Equation (11):?=jwjijiij RwwCRwwCRwwP ),,(),,(),|( .
(11)We thus have the mapping function ?
(Wj-1, HWj-1,Dj-1) = (wj-2, wj-1, hj-1, wi).
The estimate of headwordprobability is an interpolation of three probabilities:=???
)),,(|( 11 jjjj HDWwP   (12)),|(( 121 jjj HhwP ???
)),|()1( 2 RwwP ij?
?+),,|()1( 121 jjjj HwwwP ??
?+ ?
.Here P(wj|wj-2, wj-1, Hj) is the word trigram prob-ability given that wj is a headword, P(wj|hj-1, Hj) isthe headword bigram probability, and ?1, ?2 ?
[0,1]are  the interpolation weights optimized on held-outdata.We now come back to the estimate of the otherthree probabilities in Equation (9).
Following thework in Gao et al (2002b), we used the unigramestimate for word category probabilities, (i.e.,P(Hj|?
(Wj-1, Dj-1)) ?
P(Hj) and P(Fj | ?
(Wj-1, Dj-1)) ?P(Fj)), and the standard trigram estimate for func-tion word probability (i.e., P(wj |?
(Wj-1,Dj-1),Fj) ?P(wj | wj-2, wj-1, Fj)).
Let Cj be the category of wj; weapproximated P(Cj)?
P(wj|wj-2, wj-1, Cj) by P(wj | wj-2,wj-1).
By separating the estimates for the probabili-ties of headwords and function words, the finalestimate is given below:P(wj | ?
(Wj-1, Dj-1))= (13))|()((( 121 ?jjj hwPHP ??
)),|()1( 2 RwwP ij?
?+),|()1( 121 ??
?+ jjj wwwP?wj: headword),|( 12 ??
jjj wwwP   ????????
?wj: function wordAll conditional probabilities in Equation (13) areobtained using MLE on training data.
In order todeal with the data sparseness problem, we used abackoff scheme (Katz, 1987) for parameter estima-tion.
This backoff scheme recursively estimates theprobability of an unseen n-gram by utilizing(n?1)-gram estimates.
In particular, the probabilityof Equation (11) backs off to the estimate ofP(wj|R), which is computed as:NRwCRwP jj),()|( = , (14)where N is the total number of dependencies intraining data, and C(wj, R) is the number of de-pendencies that contains wj.
To keep the model sizemanageable, we removed all n-grams of count lessthan 2 from the headword bigram model and theword trigram model, but kept all long-distancedependency bigrams that occurred in the trainingdata.3.3 Training data creationThis section describes two methods that were usedto tag raw text corpus for DLM training: (1) amethod for headword detection, and (2) an unsu-pervised learning method for dependency structureacquisition.In order to classify a word uniquely as H or F,we used a mapping table created in the followingway.
We first assumed that the mapping frompart-of-speech (POS) to word category is uniqueand fixed;5 we then used a POS-tagger to generate aPOS-tagged corpus, which are then turned into acategory-tagged corpus.6 Based on this corpus, wecreated a mapping table which maps each word to aunique category: when a word can be mapped toeither H or F, we chose the more frequent categoryin the corpus.
This method achieved a 98.5% ac-curacy of headword detection on the test data weused.Given a headword-tagged corpus, we then usedan EM-like iterative method for joint optimizationof the parsing model and the dependency structureof training data.
This method uses the maximumlikelihood principle, which is consistent with lan-5The tag set we used included 1,187 POS tags, of which102 counted as headwords in our experiments.6Since the POS-tagger does not identify phrases (bun-setsu), our implementation identifies multiple headwordsin phrases headed by compounds.guage model training.
There are three steps in thealgorithm: (1) initialize, (2) (re-)parse the trainingcorpus, and (3) re-estimate the parameters of theparsing model.
Steps (2) and (3) are iterated untilthe improvement in the probability of training datais less than a threshold.Initialize: We set a window of size N and assumedthat each headword pair within a headword N-gramconstitutes an initial dependency.
The optimal valueof N is 3 in our experiments.
That is, given aheadword trigram (h1, h2, h3), there are 3 initialdependencies: d12, d13, and d23.
From the initialdependencies, we computed an initial dependencyparsing model by Equation (4).
(Re-)parse the corpus: Given the parsing model,we used the parsing algorithm in Figure 2 to selectthe most probable dependency structure for eachsentence in the training data.
This provides an up-dated set of dependencies.Re-estimate the parameters of parsing model:We then re-estimated the parsing model parametersbased on the updated dependency set.4 Evaluation MethodologyIn this study, we evaluated language models on theapplication of Japanese Kana-Kanji conversion,which is the standard method of inputting Japanesetext by converting the text of a syllabary-basedKana string into the appropriate combination ofKanji and Kana.
This is a similar problem to speechrecognition, except that it does not include acousticambiguity.
Performance on this task is measured interms of the character error rate (CER), given by thenumber of characters wrongly converted from thephonetic string divided by the number of charactersin the correct transcript.For our experiments, we used two newspapercorpora, Nikkei and Yomiuri Newspapers, both ofwhich have been pre-word-segmented.
We builtlanguage models from a 36-million-word subset ofthe Nikkei Newspaper corpus, performed parameteroptimization on a 100,000-word subset of the Yo-miuri Newspaper (held-out data), and tested ourmodels on another 100,000-word subset of theYomiuri Newspaper corpus.
The lexicon we usedcontains 167,107 entries.Our evaluation was done within a framework ofso-called ?N-best rescoring?
method, in which a listof hypotheses is generated by the baseline languagemodel (a word trigram model in this study), whichis then rescored using a more sophisticated lan-guage model.
We use the N-best list of N=100,whose ?oracle?
CER (i.e., the CER of the hy-potheses with the minimum number of errors) ispresented in Table 1, indicating the upper bound onperformance.
We also note in Table 1 that the per-formance of the conversion using the baseline tri-gram model is much better than the state-of-the-artperformance currently available in the marketplace,presumably due to the large amount of training datawe used, and to the similarity between the trainingand the test data.Baseline Trigram Oracle of 100-best3.73% 1.51%Table 1.
CER results of baseline and 100-best list5 ResultsThe results of applying our models to the task ofJapanese Kana-Kanji conversion are shown inTable 2.
The baseline result was obtained by using aconventional word trigram model (WTM).7 HBMstands for headword bigram model, which does notuse any dependency structure (i.e.
?2 = 1 in Equation(13)).
DLM_1 is the DLM that does not use head-word bigram (i.e.
?2 = 0 in Equation (13)).
DLM_2is the model where the headword probability isestimated by interpolating the word trigram prob-ability, the headword bigram probability, and theprobability given one previous linguistically relatedword in the dependency structure.Although Equation (7) suggests that the wordprobability P(wj|?
(Wj-1,Dj-1)) and the parsing modelprobability can be combined through simple multi-plication, some weighting is desirable in practice,especially when our parsing model is estimatedusing an approximation by the parsing scoreP(D|W).
We therefore introduced a parsing modelweight PW: both DLM_1 and DLM_2 models werebuilt with and without PW.
In Table 2, the PW-prefix refers to the DLMs with PW = 0.5, and theDLMs without PW- prefix refers to DLMs with PW= 0.
For both DLM_1 and DLM_2, models with theparsing weight achieve better performance; we7For a detailed description of the baseline trigram model,see Gao et al (2002a).therefore discuss only DLMs with the parsingweight for the rest of this section.Model ?1 ?2 CER CER reductionWTM ---- ---- 3.73% ----HBM 0.2 1 3.40% 8.8%DLM_1  0.1 0 3.48% 6.7%PW-DLM_1 0.1 0 3.44% 7.8%DLM_2 0.3 0.7 3.33% 10.7%PW-DLM_2 0.3 0.7 3.31% 11.3%Table 2.
Comparison of CER resultsBy comparing both HBM and PW-LDM_1 modelswith the baseline model, we can see that the use ofheadword dependency contributes greatly to theCER reduction: HBM outperformed the baselinemodel by 8.8% in CER reduction, and PW-LDM_1by 7.8%.
By combining headword bigram anddependency structure, we obtained the best modelPW-DLM_2 that achieves 11.3% CER reductionover the baseline.
The improvement achieved byPW-DLM_2 over the HBM is statistically signifi-cant according to the t test (P<0.01).
These resultsdemonstrate the effectiveness of our parsing tech-nique and the use of dependency structure for lan-guage modeling.6 DiscussionIn this section, we relate our model to previousresearch and discuss several factors that we believeto have the most significant impact on the per-formance of DLM.
The discussion includes: (1) theuse of DLM as a parser, (2) the definition of themapping function ?, and (3) the method of unsu-pervised dependency structure acquisition.One basic approach to using linguistic structurefor language modeling is to extend the conventionallanguage model P(W) to P(W, T), where T is a parsetree of W. The extended model can then be used as aparser to select the most likely parse by T* = arg-maxT P(W, T).
Many recent studies (e.g., Chelbaand Jelinek, 2000; Charniak, 2001; Roark, 2001)adopt this approach.
Similarly, dependency-basedmodels (e.g., Collins, 1996; Chelba et al, 1997) usea dependency structure D of W instead of a parsetree T, where D is extracted from syntactic trees.Both of these models can be called grammar-basedmodels, in that they capture the syntactic structureof a sentence, and the model parameters are esti-mated from syntactically annotated corpora such asthe Penn Treebank.
DLM, on the other hand, is anon-grammar-based model, because it is not basedon any syntactic annotation: the dependency struc-ture used in language modeling was learned directlyfrom data in an unsupervised manner, subject to twoweak syntactic constraints (i.e., dependency struc-ture is acyclic and planar).8 This resulted in cap-turing the dependency relations that are not pre-cisely syntactic in nature within our model.
Forexample, in the conversion of the string below, theword  ban 'evening' was correctly predicted inDLM by using the long-distance bigram ~asa~ban 'morning~evening', even though these twowords are not in any direct syntactic dependencyrelationship:	'asks for instructions in the morning and submitsdaily reports in the evening'Though there is no doubt that syntactic dependencyrelations provide useful information for languagemodeling, the most linguistically related word in theprevious context may come in various linguisticrelations with the word being predicted, not limitedto syntactic dependency.
This opens up new possi-bilities for exploring the combination of differentknowledge sources in language modeling.Regarding the function ?
that maps the leftcontext onto equivalence classes, we used a simpleapproximation that takes into account only onelinguistically related word in left context.
An al-ternative is to use the maximum entropy (ME)approach (Rosenfeld, 1994; Chelba et al, 1997).Although ME models provide a nice framework forincorporating arbitrary knowledge sources that canbe encoded as a large set of constraints, training andusing ME models is extremely computationallyexpensive.
Our working hypothesis is that the in-formation for predicting the new word is dominatedby a very limited set of words which can be selectedheuristically: in this paper, ?
is defined as a heu-ristic function that maps D to one word in D that hasthe strongest linguistic relation with the word beingpredicted, as in (8).
This hypothesis is borne out by8In this sense, our model is an extension of a depend-ency-based model proposed in Yuret (1998).
However,this work has not been evaluated as a language modelwith error rate reduction.an additional experiment we conducted, where weused two words from D that had the strongest rela-tion with the word being predicted; this resulted in avery limited gain in CER reduction of 0.62%, whichis not statistically significant (P>0.05 according tothe t test).The EM-like method for learning dependencyrelations described in Section 3.3 has also beenapplied to other tasks such as hidden Markov modeltraining (Rabiner, 1989), syntactic relation learning(Yuret, 1998), and Chinese word segmentation(Gao et al, 2002a).
In applying this method, twofactors need to be considered: (1) how to initializethe model (i.e.
the value of the window size N), and(2) the number of iterations.
We investigated theimpact of these two factors empirically on the CERof Japanese Kana-Kanji conversion.
We built aseries of DLMs using different window size N anddifferent number of iterations.
Some sample resultsare shown in Table 3: the improvement in CERbegins to saturate at the second iteration.
We alsofind that a larger N results in a better initial modelbut makes the following iterations less effective.The possible reason is that a larger N generatesmore initial dependencies and would lead to a betterinitial model, but it also introduces noise that pre-vents the initial model from being improved.
AllDLMs in Table 2 are initialized with N = 3 and arerun for two iterations.Iteration N = 2 N = 3 N = 5 N = 7 N = 10Init.
3.552% 3.523% 3.540% 3.514 % 3.511%1 3.531% 3.503% 3.493% 3.509% 3.489%2 3.527% 3.481% 3.483% 3.492% 3.488%3 3.526% 3.481% 3.485% 3.490% 3.488%Table 3.
CER of DLM_1 models initialized with dif-ferent window size N, for 0-3 iterations7 ConclusionWe have presented a dependency language modelthat captures linguistic constraints via a dependencystructure ?
a set of probabilistic dependencies thatexpress the relations between headwords of eachphrase in a sentence by an acyclic, planar, undi-rected graph.
Promising results of our experimentssuggest that long-distance dependency relations canindeed be successfully exploited for the purpose oflanguage modeling.There are many possibilities for future im-provements.
In particular, as discussed in Section 6,syntactic dependency structure is believed to cap-ture useful information for informed languagemodeling, yet further improvements may be possi-ble by incorporating non-syntax-based dependen-cies.
Correlating the accuracy of the dependencyparser as a parser vs. its utility in CER reductionmay suggest a useful direction for further research.ReferenceCharniak, Eugine.
2001.
Immediate-head parsing forlanguage models.
In ACL/EACL 2001, pp.124-131.Chelba, Ciprian and Frederick Jelinek.
2000.
StructuredLanguage Modeling.
Computer Speech and Language,Vol.
14, No.
4. pp 283-332.Chelba, C, D. Engle, F. Jelinek, V. Jimenez, S. Khu-danpur, L. Mangu, H. Printz, E. S. Ristad, R.Rosenfeld, A. Stolcke and D. Wu.
1997.
Structure andperformance of a dependency language model.
InProcessing of Eurospeech, Vol.
5, pp 2775-2778.Collins, Michael John.
1996.
A new statistical parserbased on bigram lexical dependencies.
In ACL34:184-191.Eisner, Jason and Giorgio Satta.
1999.
Efficient parsingfor bilexical context-free grammars and headautomaton grammars.
In ACL 37: 457-464.Gao, Jianfeng, Joshua Goodman, Mingjing Li andKai-Fu Lee.
2002a.
Toward a unified approach to sta-tistical language modeling for Chinese.
ACM Trans-actions on Asian Language Information Processing,1-1: 3-33.Gao, Jianfeng, Hisami Suzuki and Yang Wen.
2002b.Exploiting headword dependency and predictiveclustering for language modeling.
In EMNLP 2002:248-256.Katz, S. M. 1987.
Estimation of probabilities from sparsedata for other language component of a speech recog-nizer.
IEEE transactions on Acoustics, Speech andSignal Processing, 35(3): 400-401.Rabiner, Lawrence R. 1989.
A tutorial on hidden Markovmodels and selected applications in speech recognition.Proceedings of IEEE 77:257-286.Roark, Brian.
2001.
Probabilistic top-down parsing andlanguage modeling.
Computational Linguistics, 17-2:1-28.Rosenfeld, Ronald.
1994.
Adaptive statistical languagemodeling: a maximum entropy approach.
Ph.D. thesis,Carnegie Mellon University.Yuret, Deniz.
1998.
Discovery of linguistic relationsusing lexical attraction.
Ph.D. thesis, MIT.
