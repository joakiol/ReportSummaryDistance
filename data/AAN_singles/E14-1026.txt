Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239?248,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsSource-side Preordering for Translation using Logistic Regression andDepth-first Branch-and-Bound Search?Laura Jehl?Adri`a de Gispert?Mark Hopkins?William Byrne??Dept.
of Computational Linguistics, Heidelberg University.
69120 Heidelberg, Germanyjehl@cl.uni-heidelberg.de?SDL Research.
East Road, Cambridge CB1 1BH, U.K.{agispert,mhopkins,bbyrne}@sdl.comAbstractWe present a simple preordering approachfor machine translation based on a feature-rich logistic regression model to predictwhether two children of the same nodein the source-side parse tree should beswapped or not.
Given the pair-wise chil-dren regression scores we conduct an effi-cient depth-first branch-and-bound searchthrough the space of possible children per-mutations, avoiding using a cascade ofclassifiers or limiting the list of possi-ble ordering outcomes.
We report exper-iments in translating English to Japaneseand Korean, demonstrating superior per-formance as (a) the number of crossinglinks drops by more than 10% absolutewith respect to other state-of-the-art pre-ordering approaches, (b) BLEU scores im-prove on 2.2 points over the baseline withlexicalised reordering model, and (c) de-coding can be carried out 80 times faster.1 IntroductionSource-side preordering for translation is the taskof rearranging the order of a given source sen-tence so that it best resembles the order of the tar-get sentence.
It is a divide-and-conquer strategyaiming to decouple long-range word movementfrom the core translation task.
The main advan-tage is that translation becomes computationallycheaper as less word movement needs to be con-sidered, which results in faster and better transla-tions, if preordering is done well and efficiently.Preordering also can facilitate better estimationof alignment and translation models as the paral-lel data becomes more monotonically-aligned, and?This work was done during an internship of the first au-thor at SDL Research, Cambridge.translation gains can be obtained for various sys-tem architectures, e.g.
phrase-based, hierarchicalphrase-based, etc.For these reasons, preordering has a clear re-search and commercial interest, as reflected by theextensive previous work on the subject (see Sec-tion 2).
From these approaches, we are particu-larly interested in those that (i) involve little or nohuman intervention, (ii) require limited computa-tional resources at runtime, and (iii) make use ofavailable linguistic analysis tools.In this paper we propose a novel preorderingapproach based on a logistic regression modeltrained to predict whether to swap nodes inthe source-side dependency tree.
For each pairof sibling nodes in the tree, the model uses afeature-rich representation that includes lexicalcues to make relative reordering predictions be-tween them.
Given these predictions, we conducta depth-first branch-and-bound search throughthe space of possible permutations of all siblingnodes, using the regression scores to guide thesearch.
This approach has multiple advantages.First, the search for permutations is efficient anddoes not require specific heuristics or hard limitsfor nodes with many children.
Second, the inclu-sion of the regression prediction directly into thesearch allows for finer-grained global decisions asthe predictions that the model is more confidentabout are preferred.
Finally, the use of a singleregression model to handle any number of childnodes avoids incurring sparsity issues, while al-lowing the integration of a vast number of featuresinto the preordering model.We empirically contrast our proposed methodagainst another preordering approach based onautomatically-extracted rules when translating En-glish into Japanese and Korean.
We demonstratea significant reduction in number of crossing linksof more than 10% absolute, as well as translationgains of over 2.2 BLEU points over the baseline.239We also show it outperforms a multi-class classifi-cation approach and analyse why this is the case.2 Related workOne useful way to organize previous preorderingtechniques is by how they incorporate linguisticknowledge.On one end of the spectrum we find those ap-proaches that rely on syntactic parsers and hu-man knowledge, typically encoded via a set ofhand-crafted rules for parse tree rewriting or trans-formation.
Examples of these can be foundfor French-English (Xia and McCord, 2004),German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007), English-Arabic (Badret al., 2009), English-Hindi (Ramanathan et al.,2009), English-Korean (Hong et al., 2009), andEnglish-Japanese (Lee et al., 2010; Isozaki etal., 2010).
A generic set of rules for transform-ing SVO to SOV languages has also been de-scribed (Xu et al., 2009).
The main advantage ofthese approaches is that a relatively small set ofgood rules can yield significant improvements intranslation.
The common criticism they receive isthat they are language-specific.On the other end of the spectrum, there are pre-ordering models that rely neither on human knowl-edge nor on syntactic analysis, but only on wordalignments.
One such approach is to form a cas-cade of two translation systems, where the firstone translates the source to its preordered ver-sion (Costa-juss`a and Fonollosa, 2006).
Alterna-tively, one can define models that assign a cost tothe relative position of each pair of words in thesentence, and search for the sequence that opti-mizes the global score as a linear ordering prob-lem (Tromble and Eisner, 2009) or as a travel-ing salesman problem (Visweswariah et al., 2011).Yet another line of work attempts to automaticallyinduce a parse tree and a preordering model fromword alignments (DeNero and Uszkoreit, 2011;Neubig et al., 2012).
These approaches are at-tractive due to their minimal reliance on linguisticknowledge.
However, their findings reveal that thebest performance is obtained when using human-aligned data which is expensive to create.Somewhere in the middle of the spectrum areworks that rely on automatic source-language syn-tactic parses, but no direct human intervention.Preordering rules can be automatically extractedfrom word alignments and constituent trees (Liet al., 2007; Habash, 2007; Visweswariah etal., 2010), dependency trees (Genzel, 2010) orpredicate-argument structures (Wu et al., 2011),or simply part-of-speech sequences (Crego andMari?no, 2006; Rottmann and Vogel, 2007).
Rulesare assigned a cost based on Maximum En-tropy (Li et al., 2007) or Maximum Likelihood es-timation (Visweswariah et al., 2010), or directlyon their ability to make the training corpus moremonotonic (Genzel, 2010).
The latter performsvery well in practice but comes at the cost of abrute-force extraction heuristic that cannot incor-porate lexical information.
Recently, other ap-proaches treat ordering the children of a node asa learning to rank (Yang et al., 2012) or discrimi-native multi-classification task (Lerner and Petrov,2013).
These are appealing for their use of finer-grained lexical information, but they struggle toadequately handle nodes with multiple children.Our approach is closely related to this latterwork, as we are interested in feature-rich discrim-inative approaches that automatically learn pre-ordering rules from source-side dependency trees.Similarly to Yang et al.
(2012) we train a largediscriminative linear model, but rather than modeleach child?s position in an ordered list of children,we model a more natural pair-wise swap / no-swappreference (like Tromble and Eisner (2009) did atthe word level).
We then incorporate this modelinto a global, efficient branch-and-bound searchthrough the space of permutations.
In this way, weavoid an error-prone cascade of classifiers or anylimit on the possible ordering outcomes (Lernerand Petrov, 2013).3 Preordering using logistic regressionand branch-and-bound searchLike Genzel (2010), our method starts with depen-dency parses of source sentences (which we con-vert to shallow constituent trees; see Figure 1 foran example), and reorders the source text by per-muting sibling nodes in the parse tree.
For eachnon-terminal node, we first apply a logistic regres-sion model which predicts, for each pair of childnodes, the probability that they should be swappedor kept in their original order.
We then applya depth-first branch-and-bound search to find theglobal optimal reordering of children.240VBheNN1couldMD2standVB3NN4theDTsmellNNnsubjauxHEADdobjdet HEADFigure 1: Shallow constituent tree generated fromthe dependency tree.
Non-terminal nodes inheritthe tag from the head.3.1 Logistic regressionWe build a regression model that assigns a prob-ability of swapping any two sibling nodes, a andb, in the source-side dependency tree.
The proba-bility of swapping them is denoted p(a, b) and theprobability of keeping them in their original orderis 1 ?
p(a, b).
We use LIBLINEAR (Fan et al.,2008) for training an L1-regularised logistic re-gression model based on positively and negativelylabelled samples.3.1.1 Training dataWe generate training examples for the logistic re-gression from word-aligned parallel data which isannotated with source-side dependency trees.
Foreach non-terminal node, we extract all possiblepairs of child nodes.
For each pair, we obtain abinary label y ?
{?1, 1} by calculating whetherswapping the two nodes would reduce the numberof crossing alignment links.
The crossing score ofhaving two nodes a and b in the given order iscs(a, b) := |{(i, j) ?
Aa?Ab: i > j}|where Aaand Abare the target-side positions towhich the words spanned by a and b are aligned.The label is then given asy(a, b) ={1 , cs(a, b) > cs(b, a)?1 , cs(b, a) > cs(a, b)Instances for which cs(a, b) = cs(b, a) are notincluded in the training data.
This usually happensif either Aaor Abis empty, and in this case thealignments provide no indication of which orderis better.
We also discard any samples from nodesthat have more than 16 children, as these are rarecases that often result from parsing errors.12 3 42 3221. .
.. .
.Figure 2: Branch-and-bound search: Partial searchspace of permutations for a dependency tree nodewith four children.
The gray node marks a goalnode.
For the root node of the tree in Figure 1, thepermutation corresponding to this path (1,4,3,2)would produce ?he the smell stand could?.3.1.2 FeaturesUsing a machine learning setup allows us to in-corporate fine-grained information in the form offeatures.
We use the following features to charac-terise pairs of nodes:l The dependency labels of each nodet The part-of-speech tags of each node.hw The head words and classes of each node.lm, rm The left-most and right-most words and classesof a node.dst The distances between each node and the head.gap If there is a gap between nodes, the left-mostand right-most words and classes in the gap.In order to keep the size of our feature spacemanageable, we only consider features which oc-cur at least 5 times1.
For the lexical features, weuse the top 100 vocabulary items from our trainingdata, and 51 clusters generated by mkcls (Och,1999).
Similarly to previous work (Genzel, 2010;Yang et al., 2012), we also explore feature con-junctions.
For the tag and label classes, we gen-erate all possible combinations up to a given size.For the lexical and distance features, we explicitlyspecify conjunctions with the tag and label fea-tures.
Results for various feature configurationsare discussed in Section 4.3.1.3.2 SearchFor each non-terminal node in the source-side de-pendency tree, we search for the best possible1Additional feature selection is achieved through L1-regularisation.241permutation of its children.
We define the scoreof a permutation pi as the product of the proba-bilities of its node pair orientations (swapped orunswapped):score(pi) =?1?i<j?k|pi[i]>pi[j]p(i, j)??1?i<j?k|pi[i]<pi[j]1?
p(i, j)Here, we represent a permutation pi of k nodesas a k-length sequence containing each integer in{1, ..., k} exactly once.
Define a partial permu-tation of k nodes as a k?< k length sequencecontaining each integer in {1, ..., k} at most once.We can construct a search space over partial per-mutations in the natural way (see Figure 2).
Theroot node represents the empty sequence  and hasscore 1.
Then, given a search node representinga k?-length partial permutation pi?, its successornodes are obtained by extending it by one element:score(pi??
?i?)
= score(pi?)?
?j?V |i>jp(i, j)?
?j?V |i<j1?
p(i, j)where V = {1, ..., k}\(pi??
?i?)
is the set of sourcechild positions that have not yet been visited.
Ob-serve that the nodes at search depth k correspondexactly to the set of complete permutations.
Tosearch this space, we employ depth-first branch-and-bound (Balas and Toth, 1983) as our searchalgorithm.
The idea of branch-and-bound is toremember the best scoring goal node found thusfar, abandoning any partial paths that cannot leadto a better scoring goal node.
Algorithm 1 givespseudocode for the algorithm2.
If the initial bound(bound0) is set to 0, the search is guaranteed tofind the optimal solution.
By raising the bound,which acts as an under-estimate of the best scor-ing permutation, search can be faster but possiblyfail to find any solution.
All our experiments weredone with bound0= 0, i.e.
exact search, but wediscuss search time in detail and pruning alterna-tives in Section 4.3.2.Since we use a logistic regression model and in-corporate its predictions directly as swap probabil-ities, our search prefers those permutations withswaps which the model is more confident about.2See (Poole and Mackworth, 2010) for more details and aworked example.Algorithm 1 Depth-first branch-and-boundRequire: k: maximum sequence length, : empty sequence,bound0: initial boundprocedure BNBSEARCH(, bound0, k)best path?
?bound?
bound0SEARCH(??
)return best pathend procedureprocedure SEARCH(pi?
)if score(pi?)
> bound thenif |pi?| = k thenbest path?
?pi??bound?
score(pi?
)returnelsefor each i ?
{1, ..., k}\pi?doSEARCH(pi??
?i?
)end forend ifend ifend procedure4 Experiments4.1 SetupWe report translation results in English-to-Japanese/Korean.
Our corpora are comprised ofgeneric parallel data extracted from the web, withsome documents extracted manually and some au-tomatically crawled.
Both have about 6M sentencepairs and roughly 100M words per language.The dev and test sets are also generic.
Sourcesentences were extracted from the web and onetarget reference was produced by a bilingualspeaker.
These sentences were chosen to evenlyrepresent 10 domains, including world news,chat/SMS, health, sport, science, business, andothers.
The dev/test sets contain 602/903 sen-tences and 14K/20K words each.
We do Englishpart-of-speech tagging using SVMTool (Gim?enezand M`arquez, 2004) and dependency parsing us-ing MaltParser (Nivre et al., 2007).For translation experiments, we use a phrase-based decoder that incorporates a set of standardfeatures and a hierarchical reordering model (Gal-ley and Manning, 2008) with weights tuned us-ing MERT to optimize the character-based BLEUscore on the dev set.
The Japanese and Korean lan-guage models are 5-grams estimated on > 350Mwords of generic web text.For training the logistic regression model, weautomatically align the parallel training data andintersect the source-to-target and target-to-sourcealignments.
We reserve a random 5K-sentence242approach EJ cs (%) EK cs (%)rule-based (Genzel, 2010) 61.9 64.2multi-class 65.2 -df-bnb 51.4 51.8Table 1: Percentage of the original crossing scoreon the heldout set, obtained after applying eachpreordering approach in English-Japanese (EJ,left) and Korean (EK, right).
Lower is better.subset for intrinsic evaluation of preordering, anduse the remainder for model parameter estimation.We evaluate our preordering approach with lo-gistic regression and depth-first branch-and-boundsearch (in short, ?df-bnb?)
both in terms of reorder-ing via crossing score reduction on the heldout set,and in terms of translation quality as measured bycharacter-based BLEU on the test set.4.2 Preordering baselinesWe contrast our work against two data-driven pre-ordering approaches.
First, we implemented therule-based approach of Genzel (2010) and opti-mised its multiple parameters for our task.
Wereport only the best results achieved, which corre-spond to using ?100K training sentences for ruleextraction, applying a sliding window width of 3children, and creating rule sequences of?60 rules.This approach cannot incorporate lexical featuresas that would make the brute-force rule extractionalgorithm unmanageable.We also implemented a multi-class classifica-tion setup where we directly predict complete per-mutations of children nodes using multi-class clas-sification (Lerner and Petrov, 2013).
While thisis straightforward for small numbers of children,it leads to a very large number of possible per-mutations for larger sets of children nodes, mak-ing classification too difficult.
While Lerner andPetrov (2013) use a cascade of classifiers and im-pose a hard limit on the possible reordering out-comes to solve this, we follow Genzel?s heuristic:rather than looking at the complete set of children,we apply a sliding window of size 3 starting fromthe left, and make classification/reordering deci-sions for each window separately.
Since the win-dows overlap, decisions made for the first windowaffect the order of nodes in the second window,etc.
We address this by soliciting decisions fromthe classifier on the fly as we preorder.
One lim-Figure 3: Crossing scores and classification accu-racy improve with training data size.itation of this approach is that it is able to movechildren only within the window.
We try to rem-edy this by applying the method iteratively, eachtime re-training the classifier on the preordereddata from the previous run.4.3 Crossing scoreWe now report contrastive results in the intrin-sic preordering task, as measured by the num-ber of crossing links (Genzel, 2010; Yang et al.,2012) on the 5K held-out set.
Without preorder-ing, there is an average of 22.2 crossing links inEnglish-Japanese and 20.2 in English-Korean.
Ta-ble 1 shows what percentage of these links re-main after applying each preordering approach tothe data.
We find that the ?df-bnb?
method out-performs the other approaches in both languagepairs, achieving more than 10 additional percent-age points reduction over the rule-based approach.Interestingly, the multi-class approach is not ableto match the rule-based approach despite using ad-ditional lexical cues.
We hypothesise that this isdue to the sliding window heuristic, which causesa mismatch in train-test conditions: while samplesare not independent of each other at test time dueto window overlaps, they are considered to be sowhen training the classifier.4.3.1 Impact of training size and featureconfigurationWe now report the effects of feature configura-tion and training data size for the English-Japanesecase.
We assess our ?df-bnb?
approach in terms ofthe classification accuracy of the trained logistic243features used acc (%) cs (%)l,t,hw,lm,rm,dst,gap 82.43 51.3l,t,hw,lm,rm,dst 82.44 51.4l,t,hw,lm,rm 82.32 53.1l,t,hw 82.02 55l,t 81.07 58.4Table 2: Ablation tests showing crossing scoresand classification accuracy as features are re-moved.
All models were trained on 8M samples.regression model (using it to predict ?1 labels inthe held-out set) and by the percentage of crossingalignment links reduced by preordering.Figure 3 shows the performance of the logisticregression model over different training set sizes,extracted from the training corpus as described inSection 3.
We observe a constant increase in pre-diction accuracy, mirrored by a steady decrease incrossing score.
However, gains are less for morethan 8M training examples.
Note that a small vari-ation in accuracy can produce a large variation incrossing score if two nodes are swapped whichhave a large number of crossing alignments.Table 2 shows an ablation test for various fea-ture configurations.
We start with all features, in-cluding head word and class (hw), left-most andright-most word in each node?s span (lm, rm), eachnode?s distance to the head (dst), and left-mostand right-most word of the gap between nodes(gap).
We then proceed by removing features toend with only label and tag features (l,t), as inGenzel (2010).
For each configuration, we gener-ated all tag- and label- combinations of size 2.
Wethen specified combinations between tag and labeland all other features.
For the lexical features wealways used conjunctions of the word itself, and itsclass.
Class information is included for all words,not just those in the top 100 vocabulary.
Table 2shows that lexical and distance feature groups con-tribute to prediction accuracy and crossing score,except for the gap features, which we omit fromfurther experiments.4.3.2 Run timeWe now demonstrate the efficiency of branch-and-bound search for the problem of finding the opti-mum permutation of n children at runtime.
Eventhough in the worst case the search could ex-plore all n!
permutations, making it prohibitive forFigure 4: Average number of nodes explored inbranch-and-bound search by number of children.nodes with many children, in practice this doesnot happen.
Many low-scoring paths are discardedearly by branch-and-bound search so that the opti-mal solution can be found quickly.
The top curvein Figure 4 shows the average number of nodesexplored in searches run on our validation set (5Ksentences) as a function of the number of children.All instances are far from the worst case3.In our experiments, the time needed to conductexact search (bound0= 0) was not a problem ex-cept for a few bad cases (nodes with more than 16children), which we simply chose not to preorder;in our data, 90% of the nodes have less than 6 chil-dren, while only 0.9% have 10 children or more, sothis omission does not affect performance notice-ably.
We verified this on our held-out set, by car-rying out exhaustive searches.
We found that notpreordering nodes with 16 children did not worsenthe crossing score.
In fact, setting a harsher limitof 10 nodes would still produce a crossing scoreof 51.9%, compared to the best score of 51.4%.There are various ways to speed up the search,if needed.
First, one could impose a hard limiton the number of explored nodes4.
As shownin Figure 4, a limit of 4K would still allow ex-act search on average for permutations of up to11 children, while stopping search early for morechildren.
We tested this for limits of 1K/4K nodesand obtained crossing scores of 51.9/51.5%.
Al-ternatively, one could define a higher initial bound;since the score of a path is a product of proba-bilities, one would select a threshold probability3Note that 12!
?479M nodes, whereas our search finds theoptimal permutation path after exploring <10K nodes.4As long as the limit exceeds the permutation length, asolution will always be found as search is depth-first.244d approach ?LRM ?
+LRM ?baseline 25.39 - 26.62 -rule-based 25.93 +0.54 27.65 +1.0310multi-class 25.60 +0.21 26.10 ?0.52df-bnb 26.73 +1.34 28.09 +1.47baseline 25.07 - 25.92 -rule-based 26.35 +1.28 27.54 +1.624multi-class 25.37 +0.30 26.31 +0.39df-bnb 26.98 +1.91 28.13 +2.21Table 3: English-Japanese BLEU scores with var-ious preordering approaches (and improvementover baseline) under two distortion limits d. Re-sults reported both excluding and including lexi-calised reordering model features (LRM).p and calculate a bound depending on the size nof the permutation as bound0= pn?(n?1)2.
Exam-ples of this would be the lower curves of Figure 4.The curve labels show the crossing score producedwith each threshold, and in parenthesis the per-centage of searches that fail to find a solution witha better score than bound0, in which case childrenare left in their original order.
As shown, this strat-egy proves less effective than simply limiting thenumber of explored nodes, because the more fre-quent cases with less children remain unaffected.4.4 Translation performanceTable 3 reports English-Japanese translation re-sults for two different values of the distortion limitd, i.e.
the maximum number of source words thatthe decoder is allowed to jump during search.
Wedraw the following conclusions.
Firstly, all thepreordering approaches outperform the baselineand the BLEU score gain they provide increases asthe distortion limit decreases.
This is further anal-ysed in Figure 5, where we report BLEU as a func-tion of the distortion limit in decoding for bothEnglish-Japanese and English-Korean.
This re-veals the power of preordering as a targeted strat-egy to obtain high performance at fast decodingtimes, since d can be drastically reduced with-out performance degradation which leads to hugedecoding speed-ups; this is consistent with theobservations in (Xu et al., 2009; Genzel, 2010;Visweswariah et al., 2011).
We also find that withpreordering it is possible to apply harsher pruningconditions in decoding while still maintaining theFigure 5: BLEU scores as a function of distor-tion limit in decoder (+LRM case).
Top: English-Japanese.
Bottom: English-Korean.exact same performance, achieving further speed-ups.
With preordering, our system is able to de-code 80 times faster while producing translationoutput of the same quality.Secondly, we observe that the preorderinggains, which are correlated with the crossing scorereductions of Table 1, are largely orthogonal tothe gains obtained when incorporating a lexi-calised reordering model (LRM).
In fact, preorder-ing gains are slightly larger with LRM, suggest-ing that this reordering model can be better esti-mated with preordered text.
This echoes the notionthat reordering models are particularly sensitiveto alignment noise (DeNero and Uszkoreit, 2011;Neubig et al., 2012; Visweswariah et al., 2013),and that a ?more monotonic?
training corpus leadsto better translation models.Finally, ?df-bnb?
outperforms all other preorder-ing approaches, and achieves an extra 0.5?0.8BLEU over the rule-based one even at zero distor-tion limit.
This is consistent with the substantialcrossing score reductions reported in Section 4.3.We argue that these improvements are due tothe usage of lexical features to facilitate finer-grained ordering decisions, and to our bettersearch through the children permutation spacewhich is not restricted by sliding windows, does245Example1reference [1?????]Barlow[2???
]the smell[3??]endure[4??????]could[5???]hoped[6?
]source [1Barlow] [5hoped] he [4could] [3stand] [2the smell] [6.
]preordered [1Barlow] he [2the smell] [3stand] [4could] [5hoped] [6.
]Example2reference [1????
]my own[2??]experience[3????
]in, [4???????
]Rosa Parks[5???]called[6???]black[7???
]woman, [8???
]one day[9????????]somehow[10???
]bus of[11?????
]back seat in[12??]sit???
[13????
]told being[14???]of[15?????
]was fed up with?source [3In] [1my own] [2experience] , a [6black] [7woman] [5named] [4Rosa Parks] [14was just tired] [8one day][14of] [13being told] [12to sit] [11in the back] [10of the bus] .rule-based [1my own] [2experience] [3In] [14was just tired] [13being told] [10the bus of] [11the back in] [12sit to] [14of][8one day] , [6a black] [7woman] [4Rosa Parks] [5named] .df-bnb [1my own] [2experience] [3In] , [5named] [6a black] [7woman] [4Rosa Parks] [10the bus of] [11the back in][12sit to] [13told being] [14of] [8one day] [14was just tired] .Example3reference [1????]we?[2????]quite[3???]Xi?an[4??]like[5?]to[6?????
]come have?source [1we] [6have come] [5to] [2quite] [4like] [1xi?an] .rule-based [1we] [2quite] [4like] [3xi?an] [5to] [6come have] .df-bnb [1we] have [2quite] [3xi?an] [4like] [5to] [6come] .baseline ???????????????
?rule-based ?????????????????
?df-bnb ???????????????
?Table 4: Examples from our test data illustrating the differences between the preordering approaches.not depend heavily on getting the right decisionin a multi-class scenario, and which incorporatesregression to carry out a score-driven search.4.5 AnalysisTable 4 gives three English-Japanese examplesto illustrate the different preordering approaches.The first, very short, example is preordered cor-rectly by the rule-based and the df-bnb approach,as the order of the brackets matches the order ofthe Japanese reference.For longer sentences we see more differencesbetween approaches, as illustrated by Example 2.In this case, both approaches succeed at movingprepositions to the back of the phrase (?my expe-rience in?, ?the bus of?).
However, while the df-bnb approach correctly moves the predicate of thesecond clause (?was just tired?)
to the back, therule-based approach incorrectly moves the subject(?a black woman named Rosa Parks?)
to this posi-tion - possibly because of the verb ?named?
whichoccurs in the phrase.
This could be an indicationthat the df-bnb is better suited for more compli-cated constructions.
With the exception of phrases4 and 8, all other phrases are in the correct orderin the df-bnb reordering.
None of the approachesmanage to reorder ?a black woman named RosaParks?
to the correct order.Example 3 shows that the translations intoJapanese also reflect preordering quality.
Theoriginal source results in ?like?
being translatedas the main verb (which is incorrectly interpretedas ?to be like, to be equal to?).
The rule-basedversion correctly moves ?have come?
to the end,but fails to swap ?xi?an?
and ?like?, resulting in?come?
being interpreted as a full verb, rather thanan auxiliary.
Only the df-bnb version achieves al-most perfect reordering, resulting in the correctword choice of ??
(to get to, to become) for?have come to?.55 ConclusionWe have presented a novel preordering approachthat estimates a preference for swapping or notswapping pairs of children nodes in the source-side dependency tree by training a feature-richlogistic regression model.
Given the pair-wisescores, we efficiently search through the spaceof possible children permutations using depth-firstbranch-and-bound search.
The approach is ableto incorporate large numbers of features includ-ing lexical cues, is efficient at runtime even witha large number of children, and proves superior toother state-of-the-art preordering approaches bothin terms of crossing score and translation perfor-mance.5This translation is still not perfect, since it uses the wronglevel of politeness, an important distinction in Japanese.246ReferencesIbrahim Badr, Rabih Zbib, and James Glass.
2009.Syntactic Phrase Reordering for English-to-ArabicStatistical Machine Translation.
In Proceedings ofEACL, pages 86?93, Athens, Greece.Egon Balas and Paolo Toth.
1983.
Branch andBound Methods for the Traveling Salesman Prob-lem.
Carnegie-Mellon Univ.
Pittsburgh PA Manage-ment Sciences Research Group.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan.Marta R. Costa-juss`a and Jos?e A. R. Fonollosa.
2006.Statistical Machine Reordering.
In Proceedings ofEMNLP, pages 70?76, Sydney, Australia.Josep M. Crego and Jos?e B. Mari?no.
2006.
Integra-tion of POStag-based Source Reordering into SMTDecoding by an Extended Search Graph.
In Pro-ceedings of AMTA, pages 29?36, Cambridge, Mas-sachusetts.John DeNero and Jakob Uszkoreit.
2011.
InducingSentence Structure from Parallel Corpora for Re-ordering.
In Proceedings of EMNLP, pages 193?203, Edinburgh, Scotland, UK.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
Journalof Machine Learning Research, 9:1871?1874.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase Reorder-ing Model.
In Proceedings of EMNLP, pages 847?855, Honolulu, Hawaii.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine trans-lation.
In Proceedings of COLING, pages 376?384,Beijing, China.Jes?us Gim?enez and Llu?
?s M`arquez.
2004.
SVMTool:A general POS tagger generator based on SupportVector Machines.
In Proceedings of LREC, Lisbon,Portugal.Nizar Habash.
2007.
Syntactic Preprocessing for Sta-tistical Machine Translation.
In Proceedings of MT-Summit, pages 215?222, Copenhagen, Denmark.Gumwon Hong, Seung-Wook Lee, and Hae-ChangRim.
2009.
Bridging Morpho-Syntactic Gap be-tween Source and Target Sentences for English-Korean Statistical Machine Translation.
In Proceed-ings of ACL-IJCNLP, pages 233?236, Suntec, Sin-gapore.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010.
Head Finalization: A Simple Re-ordering Rule for SOV Languages.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 244?251, Up-psala, Sweden.Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.2010.
Constituent Reordering and Syntax Modelsfor English-to-Japanese Statistical Machine Trans-lation.
In Proceedings of COLING, pages 626?634,Beijing, China.Uri Lerner and Slav Petrov.
2013.
Source-Side Clas-sifier Preordering for Machine Translation.
In Pro-ceedings of EMNLP, Seattle, USA.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A ProbabilisticApproach to Syntax-based Reordering for StatisticalMachine Translation.
In Proceedings of ACL, pages720?727, Prague, Czech Republic.Graham Neubig, Taro Watanabe, and Shinsuke Mori.2012.
Inducing a Discriminative Parser to OptimizeMachine Translation Reordering.
In Proceedings ofEMNLP-CoNLL, pages 843?853, Jeju Island, Korea.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proceedings ofEACL, pages 71?76, Bergen, Norway.David L. Poole and Alan K. Mackworth.
2010.
Ar-tificial Intelligence: Foundations of ComputationalAgents.
Cambridge University Press.
Full text on-line at http://artint.info.Ananthakrishnan Ramanathan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.
2009.Case markers and Morphology: Addressing the cruxof the fluency problem in English-Hindi SMT.
InProceedings of ACL-IJCNLP, pages 800?808, Sun-tec, Singapore.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In Proceedings ofTMI, pages 171?180, Sk?ovde, Sweden.Roy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proceed-ings of EMNLP, pages 1007?1016, Singapore.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax based reordering with auto-matically derived rules for improved statistical ma-chine translation.
In Proceedings of COLING, pages1119?1127, Beijing, China.247Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A word reordering model forimproved machine translation.
In Proceedings ofEMNLP, pages 486?496, Edinburgh, United King-dom.Karthik Visweswariah, Mitesh M. Khapra, and Anan-thakrishnan Ramanathan.
2013.
Cut the noise: Mu-tually reinforcing reordering and alignments for im-proved machine translation.
In Proceedings of ACL,pages 1275?1284, Sofia, Bulgaria.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese Syntactic Reordering for StatisticalMachine Translation.
In Proceedings of EMNLP-CoNLL, pages 737?745, Prague, Czech Republic.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
ExtractingPre-ordering Rules from Predicate-Argument Struc-tures.
In Proceedings of IJCNLP, pages 29?37, Chi-ang Mai, Thailand.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proceedings of COLING,Geneva, Switzerland.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a Dependency Parser to ImproveSMT for Subject-Object-Verb Languages.
In Pro-ceedings of HTL-NAACL, pages 245?253, Boulder,Colorado.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.2012.
A ranking-based approach to word reorderingfor statistical machine translation.
In Proceedings ofACL, pages 912?920, Jeju Island, Korea.248
