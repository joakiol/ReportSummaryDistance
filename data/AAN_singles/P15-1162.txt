Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1681?1691,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDeep Unordered Composition Rivals Syntactic Methodsfor Text ClassificationMohit Iyyer,1Varun Manjunatha,1Jordan Boyd-Graber,2Hal Daum?e III11University of Maryland, Department of Computer Science and UMIACS2University of Colorado, Department of Computer Science{miyyer,varunm,hal}@umiacs.umd.edu, Jordan.Boyd.Graber@colorado.eduAbstractMany existing deep learning models fornatural language processing tasks focus onlearning the compositionality of their in-puts, which requires many expensive com-putations.
We present a simple deep neuralnetwork that competes with and, in somecases, outperforms such models on sen-timent analysis and factoid question an-swering tasks while taking only a fractionof the training time.
While our model issyntactically-ignorant, we show significantimprovements over previous bag-of-wordsmodels by deepening our network and ap-plying a novel variant of dropout.
More-over, our model performs better than syn-tactic models on datasets with high syn-tactic variance.
We show that our modelmakes similar errors to syntactically-awaremodels, indicating that for the tasks we con-sider, nonlinearly transforming the input ismore important than tailoring a network toincorporate word order and syntax.1 IntroductionVector space models for natural language process-ing (NLP) represent words using low dimensionalvectors called embeddings.
To apply vector spacemodels to sentences or documents, one must firstselect an appropriate composition function, whichis a mathematical process for combining multiplewords into a single vector.Composition functions fall into two classes: un-ordered and syntactic.
Unordered functions treat in-put texts as bags of word embeddings, while syntac-tic functions take word order and sentence structureinto account.
Previously published experimentalresults have shown that syntactic functions outper-form unordered functions on many tasks (Socheret al, 2013b; Kalchbrenner and Blunsom, 2013).However, there is a tradeoff: syntactic functionsrequire more training time than unordered compo-sition functions and are prohibitively expensive inthe case of huge datasets or limited computing re-sources.
For example, the recursive neural network(Section 2) computes costly matrix/tensor productsand nonlinearities at every node of a syntactic parsetree, which limits it to smaller datasets that can bereliably parsed.We introduce a deep unordered model that ob-tains near state-of-the-art accuracies on a variety ofsentence and document-level tasks with just min-utes of training time on an average laptop computer.This model, the deep averaging network (DAN),works in three simple steps:1. take the vector average of the embeddingsassociated with an input sequence of tokens2.
pass that average through one or more feed-forward layers3.
perform (linear) classification on the finallayer?s representationThe model can be improved by applying a noveldropout-inspired regularizer: for each training in-stance, randomly drop some of the tokens?
embed-dings before computing the average.We evaluate DANs on sentiment analysis and fac-toid question answering tasks at both the sentenceand document level in Section 4.
Our model?s suc-cesses demonstrate that for these tasks, the choiceof composition function is not as important as ini-tializing with pretrained embeddings and using adeep network.
Furthermore, DANs, unlike morecomplex composition functions, can be effectivelytrained on data that have high syntactic variance.
A1681qualitative analysis of the learned layers suggeststhat the model works by magnifying tiny but mean-ingful differences in the vector average throughmultiple hidden layers, and a detailed error analy-sis shows that syntactically-aware models actuallymake very similar errors to those of the more na?
?veDAN.2 Unordered vs. Syntactic CompositionOur goal is to marry the speed of unordered func-tions with the accuracy of syntactic functions.In this section, we first describe a class of un-ordered composition functions dubbed ?neural bag-of-words models?
(NBOW).
We then explore morecomplex syntactic functions designed to avoidmany of the pitfalls associated with NBOW mod-els.
Finally, we present the deep averaging network(DAN), which stacks nonlinear layers over the tradi-tional NBOW model and achieves performance onpar with or better than that of syntactic functions.2.1 Neural Bag-of-Words ModelsFor simplicity, consider text classification: map aninput sequence of tokens X to one of k labels.
Wefirst apply a composition function g to the sequenceof word embeddings vwfor w ?
X .
The output ofthis composition function is a vector z that servesas input to a logistic regression function.In our instantiation of NBOW, g averages wordembeddings1z = g(w ?
X) =1|X|?w?Xvw.
(1)Feeding z to a softmax layer induces estimatedprobabilities for each output labely?
= softmax(Ws?
z + b), (2)where the softmax function issoftmax(q) =exp q?kj=1exp qj(3)Wsis a k ?
d matrix for a dataset with k outputlabels, and b is a bias term.We train the NBOW model to minimize cross-entropy error, which for a single training instancewith ground-truth label y is`(y?)
=k?p=1yplog(y?p).
(4)1Preliminary experiments indicate that averaging outper-forms the vector sum used in NBOW from Kalchbrenner et al(2014).Before we describe our deep extension of theNBOW model, we take a quick detour to discusssyntactic composition functions.
Connections toother representation frameworks are discussed fur-ther in Section 4.2.2 Considering Syntax for CompositionGiven a sentence like ?You?ll be more entertainedgetting hit by a bus?, an unordered model likeNBOW might be deceived by the word ?entertained?to return a positive prediction.
In contrast, syn-tactic composition functions rely on the order andstructure of the input to learn how one word orphrase affects another, sacrificing computationalefficiency in the process.
In subsequent sections,we argue that this complexity is not matched by acorresponding gain in performance.Recursive neural networks (RecNNs) are syntac-tic functions that rely on natural language?s inher-ent structure to achieve state-of-the-art accuracieson sentiment analysis tasks (Tai et al, 2015).
As inNBOW, each word type has an associated embed-ding.
However, the composition function g nowdepends on a parse tree of the input sequence.
Therepresentation for any internal node in a binaryparse tree is computed as a nonlinear function ofthe representations of its children (Figure 1, left).A more powerful RecNN variant is the recursiveneural tensor network (RecNTN), which modifiesg to include a costly tensor product (Socher et al,2013b).While RecNNs can model complex linguisticphenomena like negation (Hermann et al, 2013),they require much more training time than NBOWmodels.
The nonlinearities and matrix/tensor prod-ucts at each node of the parse tree are expen-sive, especially as model dimensionality increases.RecNNs also require an error signal at every node.One root softmax is not strong enough for themodel to learn compositional relations and leadsto worse accuracies than standard bag-of-wordsmodels (Li, 2014).
Finally, RecNNs require rela-tively consistent syntax between training and testdata due to their reliance on parse trees and thuscannot effectively incorporate out-of-domain data,as we show in our question-answering experiments.Kim (2014) shows that some of these issues canbe avoided by using a convolutional network in-stead of a RecNN, but the computational complex-ity increases even further (see Section 4 for runtimecomparisons).What contributes most to the power of syntactic1682Predatorc1isc2ac3masterpiecec4z1 = f(W[c3c4]+ b)z2 = f(W[c2z1]+ b)z3 = f(W[c1z2]+ b)softmaxsoftmaxsoftmaxRecNNPredatorc1isc2ac3masterpiecec4av =4?i=1ci4h1 = f(W1 ?
av + b1)h2 = f(W2 ?
h1 + b2)softmaxDANFigure 1: On the left, a RecNN is given an input sentence for sentiment classification.
Softmax layersare placed above every internal node to avoid vanishing gradient issues.
On the right is a two-layer DANtaking the same input.
While the RecNN has to compute a nonlinear representation (purple vectors) forevery node in the parse tree of its input, this DAN only computes two nonlinear layers for every possibleinput.functions: the compositionality or the nonlineari-ties?
Socher et al (2013b) report that removing thenonlinearities from their RecNN models drops per-formance on the Stanford Sentiment Treebank byover 5% absolute accuracy.
Most unordered func-tions are linear mappings between bag-of-wordsfeatures and output labels, so might they sufferfrom the same issue?
To isolate the effects of syn-tactic composition from the nonlinear transforma-tions that are crucial to RecNN performance, weinvestigate how well a deep version of the NBOWmodel performs on tasks that have recently beendominated by syntactically-aware models.3 Deep Averaging NetworksThe intuition behind deep feed-forward neural net-works is that each layer learns a more abstract rep-resentation of the input than the previous one (Ben-gio et al, 2013).
We can apply this concept to theNBOW model discussed in Section 2.1 with the ex-pectation that each layer will increasingly magnifysmall but meaningful differences in the word em-bedding average.
To be more concrete, take s1asthe sentence ?I really loved Rosamund Pike?s per-formance in the movie Gone Girl?
and generate s2and s3by replacing ?loved?
with ?liked?
and thenagain by ?despised?.
The vector averages of thesethree sentences are almost identical, but the aver-ages associated with the synonymous sentences s1and s2are slightly more similar to each other thanthey are to s3?s average.Could adding depth to NBOW make small suchdistinctions as this one more apparent?
In Equa-tion 1, we compute z, the vector representation forinput text X , by averaging the word vectors vw?X.Instead of directly passing this representation to anoutput layer, we can further transform z by addingmore layers before applying the softmax.
Supposewe have n layers, z1...n. We compute each layerzi= g(zi?1) = f(Wi?
zi?1+ bi) (5)and feed the final layer?s representation, zn, to asoftmax layer for prediction (Figure 1, right).This model, which we call a deep averaging net-work (DAN), is still unordered, but its depth allowsit to capture subtle variations in the input betterthan the standard NBOW model.
Furthermore, com-puting each layer requires just a single matrix multi-plication, so the complexity scales with the numberof layers rather than the number of nodes in a parsetree.
In practice, we find no significant differencebetween the training time of a DAN and that of theshallow NBOW model.3.1 Word Dropout Improves RobustnessDropout regularizes neural networks by randomlysetting hidden and/or input units to zero with someprobability p (Hinton et al, 2012; Srivastava etal., 2014).
Given a neural network with n units,dropout prevents overfitting by creating an ensem-ble of 2ndifferent networks that share parameters,where each network consists of some combinationof dropped and undropped units.
Instead of drop-ping units, a natural extension for the DAN model isto randomly drop word tokens?
entire word embed-dings from the vector average.
Using this method,1683which we call word dropout, our network theoreti-cally sees 2|X|different token sequences for eachinput X .We posit a vector r with |X| independentBernoulli trials, each of which equals 1 with prob-ability p. The embedding vwfor token w in X isdropped from the average if rwis 0, which expo-nentially increases the number of unique examplesthe network sees during training.
This allows us tomodify Equation 1:rw?
Bernoulli(p) (6)?X = {w|w ?
X and rw> 0} (7)z = g(w ?
X) =?w??Xvw|?X|.
(8)Depending on the choice of p, many of the?dropped?
versions of an original training instancewill be very similar to each other, but for shorterinputs this is less likely.
We might drop a veryimportant token, such as ?horrible?
in ?the crabrangoon was especially horrible?
; however, sincethe number of word types that are predictive of theoutput labels is low compared to non-predictiveones (e.g., neutral words in sentiment analysis), wealways see improvements using this technique.Theoretically, word dropout can also be appliedto other neural network-based approaches.
How-ever, we observe no significant performance differ-ences in preliminary experiments when applyingword dropout to leaf nodes in RecNNs for senti-ment analysis (dropped leaf representations are setto zero vectors), and it slightly hurts performanceon the question answering task.4 ExperimentsWe compare DANs to both the shallow NBOWmodel as well as more complicated syntactic mod-els on sentence and document-level sentiment anal-ysis and factoid question answering tasks.
The DANarchitecture we use for each task is almost identi-cal, differing across tasks only in the type of outputlayer and the choice of activation function.
Ourresults show that DANs outperform other bag-of-words models and many syntactic models with verylittle training time.2On the question-answeringtask, DANs effectively train on out-of-domain data,while RecNNs struggle to reconcile the syntacticdifferences between the training and test data.2Code at http://github.com/miyyer/dan.Model RT SST SST IMDB Timefine bin (s)DAN-ROOT ?
46.9 85.7 ?
31DAN-RAND 77.3 45.4 83.2 88.8 136DAN 80.3 47.7 86.3 89.4 136NBOW-RAND 76.2 42.3 81.4 88.9 91NBOW 79.0 43.6 83.6 89.0 91BiNB ?
41.9 83.1 ?
?NBSVM-bi 79.4 ?
?
91.2 ?RecNN?77.7 43.2 82.4 ?
?RecNTN??
45.7 85.4 ?
?DRecNN ?
49.8 86.6 ?
431TreeLSTM ?
50.6 86.9 ?
?DCNN??
48.5 86.9 89.4 ?PVEC??
48.7 87.8 92.6 ?CNN-MC 81.1 47.4 88.1 ?
2,452WRRBM??
?
?
89.2 ?Table 1: DANs achieve comparable sentiment accu-racies to syntactic functions (bottom third of table)but require much less training time (measured astime of a single epoch on the SST fine-grained task).Asterisked models are initialized either with differ-ent pretrained embeddings or randomly.4.1 Sentiment AnalysisRecently, syntactic composition functions haverevolutionized both fine-grained and binary (pos-itive or negative) sentiment analysis.
We conductsentence-level sentiment experiments on the Rot-ten Tomatoes (RT) movie reviews dataset (Pangand Lee, 2005) and its extension with phrase-levellabels, the Stanford Sentiment Treebank (SST) in-troduced by Socher et al (2013b).
Our model isalso effective on the document-level IMDB moviereview dataset of Maas et al (2011).4.1.1 Neural BaselinesMost neural approaches to sentiment analysis arevariants of either recursive or convolutional net-works.
Our recursive neural network baselinesinclude standard RecNNs (Socher et al, 2011b),RecNTNs, the deep recursive network (DRecNN)proposed by?Irsoy and Cardie (2014), and theTREE-LSTM of (Tai et al, 2015).
Convolu-tional network baselines include the dynamic con-volutional network (Kalchbrenner et al, 2014,DCNN) and the convolutional neural network multi-channel (Kim, 2014, CNN-MC).
Our other neu-ral baselines are the sliding-window based para-graph vector (Le and Mikolov, 2014, PVEC)3and3PVEC is computationally expensive at both training andtest time and requires enough memory to store a vector forevery paragraph in the training data.1684the word-representation restricted Boltzmann ma-chine (Dahl et al, 2012, WRRBM), which onlyworks on the document-level IMDB task.44.1.2 Non-Neural BaselinesWe also compare to non-neural baselines, specif-ically the bigram na?
?ve Bayes (BINB) and na?
?veBayes support vector machine (NBSVM-BI) mod-els introduced by Wang and Manning (2012), bothof which are memory-intensive due to huge featurespaces of size |V |2.4.1.3 DAN ConfigurationsIn Table 1, we compare a variety of DAN and NBOWconfigurations5to the baselines described above.
Inparticular, we are interested in not only comparingDAN accuracies to those of the baselines, but alsohow initializing with pretrained embeddings and re-stricting the model to only root-level labels affectsperformance.
With this in mind, the NBOW-RANDand DAN-RAND models are initialized with ran-dom 300-dimensional word embeddings, while theother models are initialized with publicly-available300-d GloVe vectors trained over the CommonCrawl (Pennington et al, 2014).
The DAN-ROOTmodel only has access to sentence-level labels forSST experiments, while all other models are trainedon labeled phrases (if they exist) in addition to sen-tences.
We train all NBOW and DAN models usingAdaGrad (Duchi et al, 2011).We apply DANs to documents by averaging theembeddings for all of a document?s tokens andthen feeding that average through multiple layersas before.
Since the representations computed byDANs are always d-dimensional vectors regardlessof the input size, they are efficient with respect toboth memory and computational cost.
We find thatthe hyperparameters selected on the SST also workwell for the IMDB task.4.1.4 Dataset DetailsWe evaluate over both fine-grained and binarysentence-level classification tasks on the SST, andjust the binary task on RT and IMDB.
In the fine-grained SST setting, each sentence has a label fromzero to five where two is the neutral class.
For thebinary task, we ignore all neutral sentences.64The WRRBM is trained using a slow Metropolis-Hastingsalgorithm.5Best hyperparameters chosen by cross-validation: three300-d ReLu layers, word dropout probability p = 0.3, L2regularization weight of 1e-5 applied to all parameters6Our fine-grained SST split is {train: 8,544, dev: 1,101,test: 2,210}, while our binary split is {train: 6,920, dev:872,4.1.5 ResultsThe DAN achieves the second best reported resulton the RT dataset, behind only the significantlyslower CNN-MC model.
It?s also competitive withmore complex models on the SST and outperformsthe DCNN and WRRBM on the document-levelIMDB task.
Interestingly, the DAN achieves goodperformance on the SST when trained with onlysentence-level labels, indicating that it does notsuffer from the vanishing error signal problem thatplagues RecNNs.
Since acquiring labelled phrasesis often expensive (Sayeed et al, 2012; Iyyer etal., 2014b), this result is promising for large ormessy datasets where fine-grained annotation isinfeasible.4.1.6 Timing ExperimentsDANs require less time per epoch and?in general?require fewer epochs than their syntactic coun-terparts.
We compare DAN runtime on the SSTto publicly-available implementations of syntacticbaselines in the last column of Table 1; the reportedtimes are for a single epoch to control for hyper-parameter choices such as learning rate, and allmodels use 300-d word vectors.
Training a DANon just sentence-level labels on the SST takes underfive minutes on a single core of a laptop; whenlabeled phrases are added as separate training in-stances, training time jumps to twenty minutes.7All timing experiments were performed on a singlecore of an Intel I7 processor with 8GB of RAM.4.2 Factoid Question AnsweringDANs work well for sentiment analysis, but howdo they do on other NLP tasks?
We shift gearsto a paragraph-length factoid question answeringtask and find that our model outperforms otherunordered functions as well as a more complexsyntactic RecNN model.
More interestingly, wefind that unlike the RecNN, the DAN significantlybenefits from out-of-domain Wikipedia trainingdata.Quiz bowl is a trivia competition in which play-ers are asked four-to-six sentence questions aboutentities (e.g., authors, battles, or events).
It is anideal task to evaluate DANs because there is priortest:1,821}.
Split sizes increase by an order of magnitudewhen labeled phrases are added to the training set.
For RT,we do 10-fold CV over a balanced binary dataset of 10,662sentences.
Similarly, for the IMDB experiments we use theprovided balanced binary training set of 25,000 documents.7We also find that DANs take significantly fewer epochs toreach convergence than syntactic models.1685Model Pos 1 Pos 2 Full Time(s)BoW-DT 35.4 57.7 60.2 ?IR 37.5 65.9 71.4 N/AQANTA 47.1 72.1 73.7 314DAN 46.4 70.8 71.8 18IR-WIKI 53.7 76.6 77.5 N/AQANTA-WIKI 46.5 72.8 73.9 1,648DAN-WIKI 54.8 75.5 77.1 119Table 2: The DAN achieves slightly lower accu-racies than the more complex QANTA in muchless training time, even at early sentence posi-tions where compositionality plays a bigger role.When Wikipedia is added to the training set (bot-tom half of table), the DAN outperforms QANTAand achieves comparable accuracy to a state-of-the-art information retrieval baseline, which highlightsa benefit of ignoring word order for this task.llllll6970710.0 0.1 0.2 0.3 0.4 0.5Dropout ProbabilityHistory QB Accuracy Effect of Word DropoutFigure 2: Randomly dropping out 30% of wordsfrom the vector average is optimal for the quiz bowltask, yielding a gain in absolute accuracy of almost3% on the quiz bowl question dataset compared tothe same model trained with no word dropout.work using both syntactic and unordered modelsfor quiz bowl question answering.
In Boyd-Graberet al (2012), na?
?ve Bayes bag-of-words models(BOW-DT) and sequential language models workwell on easy questions but poorly on harder ones.A dependency-tree RecNN called QANTA proposedin Iyyer et al (2014a) shows substantial improve-ments, leading to the hypothesis that correctly mod-eling compositionality is crucial for answering hardquestions.4.2.1 Dataset and Experimental SetupTo test this, we train a DAN over the history ques-tions from Iyyer et al (2014a).8This dataset is aug-8The training set contains 14,219 sentences over 3,761questions.
For more detail about data and baseline systems,mented with 49,581 sentence/page-title pairs fromthe Wikipedia articles associated with the answersin the dataset.
For fair comparison with QANTA,we use a normalized tanh activation function at thelast layer instead of ReLu, and we also change theoutput layer from a softmax to the margin rank-ing loss (Weston et al, 2011) used in QANTA.
Weinitialize the DAN with the same pretrained 100-d word embeddings that were used to initializeQANTA.We also evaluate the effectiveness of worddropout on this task in Figure 2.
Cross-validationindicates that p = 0.3 works best for question an-swering, although the improvement in accuracy isnegligible for sentiment analysis.
Finally, continu-ing the trend observed in the sentiment experiments,DAN converges much faster than QANTA.4.2.2 DANs Improve with Noisy DataTable 2 shows that while DAN is slightly worsethan QANTA when trained only on question-answerpairs, it improves when trained on additional out-of-domain Wikipedia data (DAN-WIKI), reachingperformance comparable to that of a state-of-the-artinformation retrieval system (IR-WIKI).
QANTA,in contrast, barely improves when Wikipedia data isadded (QANTA-WIKI) possibly due to the syntacticdifferences between Wikipedia text and quiz bowlquestion text.The most common syntactic structures in quizbowl sentences are imperative constructions suchas ?Identify this British author who wrote Wuther-ing Heights?, which are almost never seen inWikipedia.
Furthermore, the subject of most quizbowl sentences is a pronoun or pronomial mentionreferring to the answer, a property that is not trueof Wikipedia sentences (e.g., ?Little of Emily?swork from this period survives, except for poemsspoken by characters.?).
Finally, many Wikipediasentences do not uniquely identify the title of thepage they come from, such as the following sen-tence from Emily Bront?e?s page: ?She does notseem to have made any friends outside her family.
?While noisy data affect both DAN and QANTA, thelatter is further hampered by the syntactic diver-gence between quiz bowl questions and Wikipedia,which may explain the lack of improvement in ac-curacy.see Iyyer et al (2014a).1686010203040500 1 2 3 4 5LayerPerturbationResponsecoolokaythe worstunderwhelmingPerturbation Response vs. LayerFigure 3: Perturbation response (difference in 1-norm) at each layer of a 5-layer DAN after replac-ing awesome in the film?s performances were awe-some with four words of varying sentiment polarity.While the shallow NBOW model does not show anymeaningful distinctions, we see that as the networkgets deeper, negative sentences are increasinglydifferent from the original positive sentence.lll l l l llll ll83848586870 2 4 6Number of LayersBinaryClassificationAccuracyllDANDAN?ROOTEffect of Depth on Sentiment AccuracyFigure 4: Two to three layers is optimal for theDAN on the SST binary sentiment analysis task, butadding any depth at all is an improvement over theshallow NBOW model.5 How Do DANs Work?In this section we first examine how the deep layersof the DAN amplify tiny differences in the vector av-erage that are predictive of the output labels.
Next,we compare DANs to DRecNNs on sentences thatcontain negations and contrastive conjunctions andfind that both models make similar errors despitethe latter?s increased complexity.
Finally, we an-alyze the predictive ability of unsupervised wordembeddings on a simple sentiment task in an effortto explain why initialization with these embeddingsimproves the DAN.5.1 Perturbation AnalysisFollowing the work of?Irsoy and Cardie (2014), weexamine our network by measuring the response ateach hidden layer to perturbations in an input sen-tence.
In particular, we use the template the film?sperformances were awesome and replace the fi-nal word with increasingly negative polarity words(cool, okay, underwhelming, the worst).
For eachperturbed sentence, we observe how much the hid-den layers differ from those associated with theoriginal template in 1-norm.Figure 3 shows that as a DAN gets deeper, the dif-ferences between negative and positive sentencesbecome increasingly amplified.
While nonexistentin the shallow NBOW model, these differences arevisible even with just a single hidden layer, thusexplaining why deepening the NBOW improves sen-timent analysis as shown in Figure 4.5.2 Handling Negations and ?but?
: WhereSyntax is Still NeededWhile DANs outperform other bag-of-words mod-els, how can they model linguistic phenomena suchas negation without considering word order?
Toevaluate DANs over tougher inputs, we collect 92sentences, each of which contains at least one nega-tion and one contrastive conjunction, from the devand test sets of the SST.9Our fine-grained accuracyis higher on this subset than on the full dataset,improving almost five percent absolute accuracyto 53.3%.
The DRecNN model of?Irsoy and Cardie(2014) obtains a similar accuracy of 51.1%, con-trary to our intuition that syntactic functions shouldoutperform unordered functions on sentences thatclearly require syntax to understand.10Are these sentences truly difficult to classify?
Aclose inspection reveals that both the DAN and theDRecNN have an overwhelming tendency to pre-dict negative sentiment (60.9% and 55.4% of thetime for the DAN and DRecNN respectively) whenthey see a negation compared to positive sentiment(35.9% for DANs, 34.8% for DRecNNs).
If we fur-ther restrict our subset of sentences to only thosewith positive ground truth labels, we find that whileboth models struggle, the DRecNN obtains 41.7%accuracy, outperforming the DAN?s 37.5%.To understand why a negation or contrastive con-junction triggers a negative sentiment prediction,9We search for non-neutral sentences containing not / n?t,and but.
48 of the sentences are positive while 44 are negative.10Both models are initialized with pretrained 300-d GloVeembeddings for fair comparison.1687Sentence DAN DRecNN Ground Trutha lousy movie that?s not merely unwatchable , but alsounlistenablenegative negative negativeif you?re not a prepubescent girl , you?ll be laughing atbritney spears ?
movie-starring debut whenever it does n?thave you impatiently squinting at your watchnegative negative negativeblessed with immense physical prowess he may well be, butahola is simply not an actorpositive neutral negativewho knows what exactly godard is on about in this film , buthis words and images do n?t have to add up to mesmerizeyou.positive positive positiveit?s so good that its relentless , polished wit can withstandnot only inept school productions , but even oliver parker ?smovie adaptationnegative positive positivetoo bad , but thanks to some lovely comedic moments andseveral fine performances , it?s not a total lossnegative negative positivethis movie was not good negative negative negativethis movie was good positive positive positivethis movie was bad negative negative negativethe movie was not bad negative negative positiveTable 3: Predictions of DAN and DRecNN models on real (top) and synthetic (bottom) sentences thatcontain negations and contrastive conjunctions.
In the first column, words colored red individually predictthe negative label when fed to a DAN, while blue words predict positive.
The DAN learns that the negatorsnot and n?t are strong negative predictors, which means it is unable to capture double negation as in thelast real example and the last synthetic example.
The DRecNN does slightly better on the synthetic doublenegation, predicting a lower negative polarity.we show six sentences from the negation subset andfour synthetic sentences in Table 3, along with bothmodels?
predictions.
The token-level predictions inthe table (shown as colored boxes) are computed bypassing each token through the DAN as separate testinstances.
The tokens not and n?t are strongly pre-dictive of negative sentiment.
While this simplified?negation?
works for many sentences in the datasetswe consider, it prevents the DAN from reasoningabout double negatives, as in ?this movie was notbad?.
The DRecNN does slightly better in this caseby predicting a lesser negative polarity than theDAN; however, we theorize that still more powerfulsyntactic composition functions (and more labelledinstances of negation and related phenomena) arenecessary to truly solve this problem.5.3 Unsupervised Embeddings CaptureSentimentOur model consistently converges slower to a worsesolution (dropping 3% in absolute accuracy oncoarse-grained SST) when we randomly initializethe word embeddings.
This does not apply to justDANs; both convolutional and recursive networksdo the same (Kim, 2014;?Irsoy and Cardie, 2014).Why are initializations with these embeddings socrucial to obtaining good performance?
Is it pos-sible that unsupervised training algorithms are al-ready capturing sentiment?We investigate this theory by conducting a sim-ple experiment: given a sentiment lexicon contain-ing both positive and negative words, we train alogistic regression to discriminate between the asso-ciated word embeddings (without any fine-tuning).We use the lexicon created by Hu and Liu (2004),which consists of 2,006 positive words and 4,783negative words.
We balance and split the datasetinto 3,000 training words and 1,000 test words.Using 300-dimensional GloVe embeddings pre-trained over the Common Crawl, we obtain over95% accuracy on the unseen test set, supporting thehypothesis that unsupervised pretraining over largecorpora can capture properties such as sentiment.Intuitively, after the embeddings are fine-tunedduring DAN training, we might expect a decreasein the norms of stopwords and an increase in the1688norms of sentiment-rich words like ?awesome?
or?horrible?.
However, we find no significant dif-ferences between the L2norms of stopwords andwords in the sentiment lexicon of Hu and Liu(2004).6 Related WorkOur DAN model builds on the successes of bothsimple vector operations and neural network-basedmodels for compositionality.There are a variety of element-wise vector op-erations that could replace the average used in theDAN.
Mitchell and Lapata (2008) experiment withmany of them to model the compositionality ofshort phrases.
Later, their work was extended totake into account the syntactic relation betweenwords (Erk and Pad?o, 2008; Baroni and Zampar-elli, 2010; Kartsaklis and Sadrzadeh, 2013) andgrammars (Coecke et al, 2010; Grefenstette andSadrzadeh, 2011).
While the average works best forthe tasks that we consider, Banea et al (2014) findthat simply summing word2vec embeddings out-performs all other methods on the SemEval 2014phrase-to-word and sentence-to-phrase similaritytasks.Once we compute the embedding average in aDAN, we feed it to a deep neural network.
In con-trast, most previous work on neural network-basedmethods for NLP tasks explicitly model word or-der.
Outside of sentiment analysis, RecNN-basedapproaches have been successful for tasks suchas parsing (Socher et al, 2013a), machine trans-lation (Liu et al, 2014), and paraphrase detec-tion (Socher et al, 2011a).
Convolutional net-works also model word order in local windows andhave achieved performance comparable to or bet-ter than that of RecNNs on many tasks (Collobertand Weston, 2008; Kim, 2014).
Meanwhile, feed-forward architectures like that of the DAN havebeen used for language modeling (Bengio et al,2003), selectional preference acquisition (Van deCruys, 2014), and dependency parsing (Chen andManning, 2014).7 Future WorkIn Section 5, we showed that the performance ofour DAN model worsens on sentences that con-tain lingustic phenomena such as double negation.One promising future direction is to cascade clas-sifiers such that syntactic models are used onlywhen a DAN is not confident in its prediction.
Wecan also extend the DAN?s success at incorporatingout-of-domain training data to sentiment analysis:imagine training a DAN on labeled tweets for clas-sification on newspaper reviews.
Another poten-tially interesting application is to add gated unitsto a DAN,as has been done for recurrent and recur-sive neural networks (Hochreiter and Schmidhuber,1997; Cho et al, 2014; Sutskever et al, 2014; Taiet al, 2015), to drop useless words rather thanrandomly-selected ones.8 ConclusionIn this paper, we introduce the deep averaging net-work, which feeds an unweighted average of wordvectors through multiple hidden layers before clas-sification.
The DAN performs competitively withmore complicated neural networks that explicitlymodel semantic and syntactic compositionality.
Itis further strengthened by word dropout, a regu-larizer that reduces input redundancy.
DANs ob-tain close to state-of-the-art accuracy on both sen-tence and document-level sentiment analysis andfactoid question-answering tasks with much lesstraining time than competing methods; in fact, allexperiments were performed in a matter of min-utes on a single laptop core.
We find that bothDANs and syntactic functions make similar errorsgiven syntactically-complex input, which motivatesresearch into more powerful models of composi-tionality.AcknowledgmentsWe thank Ozan?Irsoy not only for many insight-ful discussions but also for suggesting some ofthe experiments that we included in the paper.We also thank the anonymous reviewers, RichardSocher, Arafat Sultan, and the members of theUMD ?Thinking on Your Feet?
research group fortheir helpful comments.
This work was supportedby NSF Grant IIS-1320538.
Boyd-Graber is alsosupported by NSF Grants CCF-1409287 and NCSE-1422492.
Any opinions, findings, conclusions, orrecommendations expressed here are those of theauthors and do not necessarily reflect the view ofthe sponsor.1689ReferencesCarmen Banea, Di Chen, Rada Mihalcea, Claire Cardie, andJanyce Wiebe.
2014.
Simcompass: Using deep learn-ing word embeddings to assess cross-level similarity.
InSemEval.Marco Baroni and Roberto Zamparelli.
2010.
Nouns arevectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedings ofEmpirical Methods in Natural Language Processing.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Chris-tian Jauvin.
2003.
A neural probabilistic language model.Journal of Machine Learning Research.Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013.Representation learning: A review and new perspectives.IEEE Transactions on Pattern Analysis and Machine Intel-ligence, 35(8):1798?1828.Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum?eIII.
2012.
Besting the quiz master: Crowdsourcing incre-mental classification games.
In Proceedings of EmpiricalMethods in Natural Language Processing.Danqi Chen and Christopher D Manning.
2014.
A fast andaccurate dependency parser using neural networks.
InProceedings of Empirical Methods in Natural LanguageProcessing.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Fethi Bougares, Holger Schwenk, and Yoshua Bengio.2014.
Learning phrase representations using rnn encoder-decoder for statistical machine translation.
In Proceedingsof Empirical Methods in Natural Language Processing.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010.Mathematical foundations for a compositional distribu-tional model of meaning.
Linguistic Analysis (LambekFestschirft).Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neuralnetworks with multitask learning.
In Proceedings of theInternational Conference of Machine Learning.George E Dahl, Ryan P Adams, and Hugo Larochelle.
2012.Training restricted boltzmann machines on word observa-tions.
In Proceedings of the International Conference ofMachine Learning.John Duchi, Elad Hazan, and Yoram Singer.
2011.
Adaptivesubgradient methods for online learning and stochasticoptimization.
Journal of Machine Learning Research.Katrin Erk and Sebastian Pad?o.
2008.
A structured vectorspace model for word meaning in context.
In Proceedingsof Empirical Methods in Natural Language Processing.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.
Ex-perimental support for a categorical compositional distri-butional model of meaning.
In Proceedings of EmpiricalMethods in Natural Language Processing.Karl Moritz Hermann, Edward Grefenstette, and Phil Blun-som.
2013.
?not not bad?
is not ?bad?
: A distributionalaccount of negation.
Proceedings of the ACL Workshop onContinuous Vector Space Models and their Compositional-ity.Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, IlyaSutskever, and Ruslan Salakhutdinov.
2012.
Improvingneural networks by preventing co-adaptation of featuredetectors.
CoRR, abs/1207.0580.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
Long short-term memory.
Neural computation.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In Knowledge Discovery and DataMining.Ozan?Irsoy and Claire Cardie.
2014.
Deep recursive neuralnetworks for compositionality in language.
In Proceedingsof Advances in Neural Information Processing Systems.Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,Richard Socher, and Hal Daum?e III.
2014a.
A neuralnetwork for factoid question answering over paragraphs.In Proceedings of Empirical Methods in Natural LanguageProcessing.Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and PhilipResnik.
2014b.
Political ideology detection using recursiveneural networks.
In Proceedings of the Association forComputational Linguistics.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrent convo-lutional neural networks for discourse compositionality.
InACL Workshop on Continuous Vector Space Models andtheir Compositionality.Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.2014.
A convolutional neural network for modelling sen-tences.
In Proceedings of the Association for Computa-tional Linguistics.Dimitri Kartsaklis and Mehrnoosh Sadrzadeh.
2013.
Priordisambiguation of word tensors for constructing sentencevectors.
In Proceedings of Empirical Methods in NaturalLanguage Processing.Yoon Kim.
2014.
Convolutional neural networks for sentenceclassification.
In Proceedings of Empirical Methods inNatural Language Processing.Quoc V Le and Tomas Mikolov.
2014.
Distributed represen-tations of sentences and documents.
In Proceedings of theInternational Conference of Machine Learning.Jiwei Li.
2014.
Feature weight tuning for recursive neuralnetworks.
CoRR, abs/1412.3714.Shujie Liu, Nan Yang, Mu Li, and Ming Zhou.
2014.
Arecursive recurrent neural network for statistical machinetranslation.
In Proceedings of the Association for Compu-tational Linguistics.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, DanHuang, Andrew Y. Ng, and Christopher Potts.
2011.
Learn-ing word vectors for sentiment analysis.
In Proceedings ofthe Association for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of the Associationfor Computational Linguistics.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization with respectto rating scales.
In Proceedings of the Association forComputational Linguistics.1690Jeffrey Pennington, Richard Socher, and Christopher Manning.2014.
Glove: Global vectors for word representation.
InProceedings of Empirical Methods in Natural LanguageProcessing.Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and AmyWeinberg.
2012.
Grammatical structures for word-levelsentiment detection.
In North American Association ofComputational Linguistics.Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.Ng, and Christopher D. Manning.
2011a.
Dynamic Pool-ing and Unfolding Recursive Autoencoders for ParaphraseDetection.
In Proceedings of Advances in Neural Informa-tion Processing Systems.Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.Ng, and Christopher D. Manning.
2011b.
Semi-SupervisedRecursive Autoencoders for Predicting Sentiment Distri-butions.
In Proceedings of Empirical Methods in NaturalLanguage Processing.Richard Socher, John Bauer, Christopher D. Manning, andAndrew Y. Ng.
2013a.
Parsing With Compositional VectorGrammars.
In Proceedings of the Association for Compu-tational Linguistics.Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,Christopher D Manning, Andrew Y Ng, and ChristopherPotts.
2013b.
Recursive deep models for semantic com-positionality over a sentiment treebank.
In Proceedings ofEmpirical Methods in Natural Language Processing.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, IlyaSutskever, and Ruslan Salakhutdinov.
2014.
Dropout: Asimple way to prevent neural networks from overfitting.Journal of Machine Learning Research, 15(1).Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.
Se-quence to sequence learning with neural networks.
InProceedings of Advances in Neural Information ProcessingSystems.Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations from tree-structured long short-term memory networks.Tim Van de Cruys.
2014.
A neural network approach to selec-tional preference acquisition.
In Proceedings of EmpiricalMethods in Natural Language Processing.Sida I. Wang and Christopher D. Manning.
2012.
Baselinesand bigrams: Simple, good sentiment and topic classifica-tion.
In Proceedings of the Association for ComputationalLinguistics.Jason Weston, Samy Bengio, and Nicolas Usunier.
2011.Wsabie: Scaling up to large vocabulary image annotation.In International Joint Conference on Artificial Intelligence.1691
