Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
343?350, Prague, June 2007. c?2007 Association for Computational LinguisticsImproving Statistical Machine Translation Performance byTraining Data Selection and OptimizationYajuan L?, Jin Huang and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100080, China{lvyajuan, huangjin,liuqun}@ict.ac.cnAbstractParallel corpus is an indispensable resourcefor translation model training in statisticalmachine translation (SMT).
Instead of col-lecting more and more parallel trainingcorpora, this paper aims to improve SMTperformance by exploiting full potential ofthe existing parallel corpora.
Two kinds ofmethods are proposed: offline data optimi-zation and online model optimization.
Theoffline method adapts the training data byredistributing the weight of each trainingsentence pairs.
The online method adaptsthe translation model by redistributing theweight of each predefined submodels.
In-formation retrieval model is used for theweighting scheme in both  methods.
Ex-perimental results show that without usingany additional resource, both methods canimprove SMT performance significantly.1 IntroductionStatistical machine translation relies heavily on theavailable training data.
Typically, the more data isused to estimate the parameters of the translationmodel, the better it can approximate the true trans-lation probabilities, which will obviously lead to ahigher translation performance.
However, largecorpora are not easily available.
The collected cor-pora are usually from very different areas.
Forexample, the parallel corpora provided by LDCcome from quite different domains, such asHongkong laws, Hangkong Hansards andHongkong news.
This results in the problem that atranslation system trained on data from a particulardomain(e.g.
Hongkong Hansards) will performpoorly when translating text from a differentdomain(e.g.
news articles).
Our experiments alsoshow that simply putting all these domain specificcorpora together will not always improvetranslation quality.
From another aspect, largeramount of training data also requires largercomputational resources.
With the increasing oftraining data, the improvement of translationquality will become smaller and smaller.
Therefore,while keeping collecting more and more parallelcorpora, it is also important to seek effective waysof making better use of available parallel trainingdata.There are two cases when we train a SMTsystem.
In one case, we know the target test set ortarget test domain, for example, when building aspecific domain SMT system or when participatingthe NIST MT evaluation1.
In the other case, we areunaware of any information of the testing data.This paper presents two methods to exploit fullpotential of the available parallel corpora in thetwo cases.
For the first case, we try to optimize thetraining data offline to make it match the test databetter in domain, topic and style, thus improvingthe translation performance.
For the second case,we first divide the training data into several do-mains and train submodels for each domain.
Then,in the translation process, we try to optimize thepredefined models according to the online inputsource sentence.
Information retrieval model isused for similar sentences retrieval in both meth-ods.
Our preliminary experiments show that bothmethods can improve SMT performance withoutusing any additional data.1 http://www.nist.gov/speech/tests/mt/343The remainder of this paper is organized as fol-lows: Section 2 describes the offline data selectionand optimization method.
Section 3 describes theonline model optimization method.
The evaluationand discussion are given in section 4.
Related workis introduced before concluding.2 Offline training data optimizationIn offline training data optimization, we assumethat the target test data or target test domain isknown before building the translation model.
Wefirst select sentences similar to the test text usinginformation retrieval method to construct a smalland adapted training data.
Then the extracted simi-lar subset is used to optimize the distribution of thewhole training data.
The adapted and the optimizedtraining data will be used to train new translationmodels.2.1 Similar data selection using TF-IDFWe use information retrieval method for similardata retrieval.
The standard TF-IDF (Term Fre-quency and Inverse Document Frequency) termweighting scheme is used to measure the similaritybetween the test sentence and the training sentence.TF-IDF is a similarity measure widely used in in-formation retrieval.
Each document i is representedas a vector  ,  is the size of thevocabulary.
is calculate as follows:D),...,,( 21 inii www nijw)log( jijij idftfw ?=where,ij  is the term frequency(TF) of the j-th wordin the vocabulary in the document , i.e.
thenumber of occurrences;tfiDj  is the inverse document frequency(IDF)of the j-th word calculated as below:idfth term-j  ##containingdocumentsdocumentsidf j = .The similarity between two documents is thendefined as the cosine of the angle between the twovectors.We perform information retrieval using the Le-mur toolkit2.
The source language part of the par-allel training data is used as the document collec-tion.
Each sentence represents one document.
Eachsentence from the test data or test domain is usedas one separate query.
In the sentence retrieval2 http://www.cs.cmu.edu/~lemur/process, both the query and the document are con-verted into vectors by assigning a term weight toeach word.
Then the cosine similarity is calculatedproportional to the inner product of the two vectors.All retrieved sentences are ranked according totheir similarity with the query.
We pair each of theretrieved sentences with the corresponding targetpart and the top N most similar sentences pairs areput together to form an adapted parallel data.
Nranges from one to several thousand in our experi-ments.
Since Lemur toolkit gives the similarityscore for each retrieved sentences, it is also possi-ble to select the most similar sentences accordingto the similarity score.Note that the selected similar data can containduplicate sentences as the top N retrieval resultsfor different test sentences can contain the sametraining sentences.
The duplicate sentences willforce the translation probability towards the moreoften seen words.
Intuitively, this could help.
Inexperiment section, we will compare experimentalresults by keeping or removing duplicates to seehow the duplicate sentences affect the translations.The selected subset contains the similar sen-tences with the test data or test domain.
It matchesthe test data better in domain, topic and style.Hopefully, training translation model using thisadapted parallel data may helpful for improvingtranslation performance.
In addition, the translationmodel trained using the selected subset is usuallymuch smaller than that trained using the wholetranslation data.
Limiting the size of translationmodel is very important for some real applications.Since SMT systems usually require large computa-tion resource.
The complexity of standard trainingand decoding algorithm depends mainly on the sizeof the parallel training data and the size of thetranslation model.
Limiting the size of the trainingdata with the similar translation performancewould also reduce the memories and speed up thetranslations.In the information retrieval process, we only usethe source language part for document indexingand query generating.
It is easy to get source partof the test data.
This is different from the commonlanguage model adaptation methods, which have todo at lease one pass machine translation to get thecandidate English translation as query(Zhao 2004,Zhang 2006).
So our method has the advantagethat it is independent from the quality of baselinetranslation system.3442.2 Training data optimizationThere are two factors on training data that influ-ence the translation performance of SMT system:the scale and the quality.
In some sense, we im-prove the quality of the training data by selectingthe similar sentence to form an adapted training set.However, we also reduce the scale of the trainingdata at the same time.
Although this is helpful forsome small device applications, it is also possibleto induce the data sparseness problem.
Here, weintroduce a method to optimize between the scaleand the quality of the training data.The basic idea is that we still use all the avail-able training data; by redistributing the weight ofeach sentence pairs we adapt the whole trainingdata to the test domain.
In our experiments, wesimply combine the selected small similar subsetand the whole training data.
The weights of eachsentence pairs are changed accordingly.
Figure 1shows the procedure of the optimization.Figure 1.
Training data optimizationAs can be seen, through the optimization, theweight of the similar sentence pairs are increased,while the general sentence pairs still have an ordi-nary weight.
This make the translation model in-clined to give higher probabilities to the adaptedwords, and at the same time avoid the data sparse-ness problem.
Since we only change the weight ofthe sentence pairs, and no new training data is in-troduced, the translation model size trained on theoptimized data will keep as the same as the origi-nal one.
We use GIZA++ toolkit3 for word align-3 http://www.fjoch.com/GIZA++.htmlment training in the training process.
The inputtraining file formats for GIZA++ is as follows:Each training sentence pair is stored in three lines.The first line is the number of times this sentencepair occurred.
The second line is the source sen-tence where each token is replaced by its uniqueinteger id and the third is the target sentence in thesame format.
To deal with our optimized trainingdata, we only need to change the number of sen-tence pairs in the first line accordingly.
This willnot call for extra training time and memory for thewhole training process.It might be beneficial to investigate other so-phisticated weighting schemes under the similaridea, such as to give more precise fractionalweights to the sentences according the retrievalsimilarity scores.3 Online model optimizationIn most circumstances, we don?t know exactly thetest data or the test domain when we train a ma-chine translation system.
This results in the factthat the performance of the translation systemhighly depends on the training data and the testdata it is used in.
To alleviate this blindfold statusand maximize the potential of the available train-ing corpora, we propose a novel online model op-timization method.The basic idea is that: several candidate transla-tion models are prepared in training stage.
In par-ticularly, a general model is also prepared.
Then, inthe translation process, the similarity between theinput sentence and the predefined models is calcu-lated online to get the weights of each model.
Theoptimized model is used to translate the input sen-tence.There are two problems in the method: how toprepare submodels in training process and how tooptimize the model weight online in translationprocess.3.1 Prepare the submodelsThere are several ways to prepare submodels intraining process.
If the training data comes fromvery different sources, we can divide the data ac-cording to its origins.
Otherwise, we can use clus-tering method to separate the training corpus intoseveral classes.
In addition, our offline data adapta-tion method can also be used for submodel prepa-ration.
For each candidate domain, we can use the345source side of a small corpus as queries to extract adomain specific training set.
In this case, a sen-tence pair in the training data may occur in severalsub training data, but this doesn?t matter.
The gen-eral model is used when the online input is notsimilar to any prepared submodels.
We can use allavailable training data to train the general modelsince generally larger data can get better modeleven there are some noises.3.2 Online model weightingWe also use TF-IDF information retrieval methodfor online model weighting.
The procedure is asfollows:For each input sentence:1.
Do IR on training data collection, using theinput sentence as query.2.
Determine the weights of submodels accord-ing to the retrieved sentences.3.
Use the optimized model to translate the sen-tence.The information retrieval process is the same asthe offline data selection except that each retrievedsentence is attached with the sub-corpus informa-tion, i.e.
it belongs to which sub-models in thetraining process.With the sub-corpus information, we can calcu-late the weights of submodels.
We get the top Nmost similar sentences, and then calculate propor-tions of each submodel?s sentences.
The proportioncan be calculated use the count of the sentences orthe similarity score of the sentences.
The weight ofeach submodel can be determined according to theproportions.Our optimized model is the log linear interpola-tion of the sub-models as follows:?=?=Miiicepcepcep10 )|()|()|(?
0??
?=+=Miiiecepcepe100 )))|(log())|(log((maxarg?
?
?where, 0 is the probability of general model, ip isthe probability of submodel i.
0p?
is the weight ofgeneral model.
i?
is the weight of submodel i. Eachmodel i is also implemented using log linear model inour SMT system.
So after the log operation, the sub-models are interpolated linearly.In our experiments, the interpolation factor i?
isdetermined using the following four simple weight-ing schemes:Weighting scheme 1:;0     ;1     ;00 === ?max_modelimax_model ??
?Weighting scheme 2:if  Proportion(max_model) > 0.5Use weighting scheme1;else;0    ;1     0 == i?
?Weighting scheme 3:);(Proportion;00ii model==?
?Weighting scheme 4:if  Proportion(max_model) > 0.5Use weighting scheme3;else);( Proportion5.0;5.0     0ii model?==?
?where, modeli is the i-th submodel, .Proportion (model)...1( Mi =i) is the proportion of modeli inthe retrieved results.
We use count for proportioncalculation.
max_model is the submodel with themax proportion score.The training and translation procedure of onlinemodel optimization is illustrated in Figure 2.Figure 2.
Online model optimization346The online model optimization method makesit possible to select suitable models for each indi-vidual test sentence.
Since the IR process is doneon a fixed training data, the size of the index datais quite small compared with the web IR.
The IRprocess will not take much time in the translation.4 Experiments and evaluation4.1 Experimental settingWe conduct our experiments on Chinese-to-English translation tasks.
The baseline system is avariant of the phrase-base SMT system, imple-mented using log-linear translation model (He et al2006).
The baseline SMT system is used in all ex-periments.
The only difference between them isthat they are trained on different parallel trainingdata.In training process, we use GIZA++4 toolkit forword alignment in both translation directions, andapply ?grow-diag-final?
method to refine it (Koehnet al, 2003).
We change the preprocess part ofGIZA++ toolkit to make it accept the weightedtraining data.
Then we use the same criterion assuggested in (Zens et al, 2002) to do phrase ex-traction.
For the log-linear model training, we takeminimum-error-rate training method as describedin (Och, 2003).
The language model is trained us-ing Xinhua portion of Gigaword with about 190Mwords.
SRI Language Modeling toolkit5 is used totrain a 4-gram model with modified Kneser-Neysmoothing(Chen and Goodman, 1998).
All ex-periments use the same language model.
This en-sures that any differences in performance arecaused only by differences in the parallel trainingdata.Our training data are from three LDC corpora asshown in Table 1.
We random select 200,000 sen-tence pairs from each corpus and combine themtogether as the baseline corpus, which includes16M Chinese words and 19M English words intotal.
This is the usual case when we train a SMTsystem, i.e.
we simply combine all corpora fromdifferent origins to get a larger training corpus.We use the 2002 NIST MT evaluation test dataas our development set, and the 2005 NIST MTtest data as the test set in offline data optimizationexperiments.
In both data, each sentence has four4 http://www.fjoch.com/GIZA++.html5 http://www.speech.sri.com/projects/srilm/human translations as references.
The translationquality is evaluated by BLEU metric (Papineni etal., 2002), as calculated by mteval-v11b.pl6 withcase-sensitive matching of n-grams.Corpus LDC No.
Description # sent.
pairsFBIS LDC2003E14 FBIS Multilanguage Texts 200000HK_Hansards LDC2004T08 Hong Kong Hansards Text 200000HK_News LDC2004T08 Hong Kong News Text 200000Baseline - All above data 600000Table 1.
Training corpora4.2 Baseline experimentsWe first train translation models on each sub train-ing corpus and the baseline corpus.
The develop-ment set is used to tune the feature weights.
Theresults on test set are shown in Table 2.System BLEU on dev set BLEU on test setFBIS 0.2614 0.2331HK_Hansards 0.1679 0.1624HK_News 0.1748 0.1608Baseline 0.2565 0.2363Table 2.
Baseline resultsFrom the results we can see that although thesize of each sub training corpus is similar, thetranslation results from the corresponding systemare quite different on the same test set.
It seemsthat the FBIS corpus is much similar to the test setthan the other two corpora.
In fact, it is the case.The FBIS contains text mainly from Chinamainland news stories, while the 2005 NIST testset alo include lots of China news text.
The resultsillustrate the importance of selecting suitable train-ing data.When combining all the sub corpora together,the baseline system gets a little better result thanthe sub systems.
This indicates that larger data isuseful even it includes some noise data.
However,compared with the FBIS corpus, the baseline cor-pus contains three times larger data, while the im-provement of translation result is not significant.This indicates that simply putting different corporatogether is not a good way to make use of theavailable corpora.6http://www.nist.gov/speech/tests/mt/resources/scoring.htm3474.3 Offline data optimization experimentsWe use baseline corpus as initial training corpus,and take Lemur toolkit to build document index onChinese part of the corpus.
The Chinese sentencesin development set and test set are used as queries.For each query, N = 100, 200, 500, 1000, 2000similar sentences are retrieved from the indexedcollection.
The extracted similar sentence pairs areused to train the new adapted translation models.Table 3 illustrates the results.
We give the distinctpair numbers for each adapted set and compare thesize of the translation models.
To illustrate the ef-fect of duplicate sentences, we also give the resultswith duplicates and without duplicates (distinct).System Distinct pairsSize oftrans modelBLEU onduplicatesBLEU ondistinctBaseline 600000 2.41G 0.2363 0.2363Top100 91804 0.43G 0.2306 0.2346Top200 150619 0.73G 0.2360 0.2345Top500 261003 1.28G 0.2415 0.2370Top1000 357337 1.74G 0.2463 0.2376Top2000 445890 2.11G 0.2351 0.2346Table 3.
Offline data adaptation resultsThe results show that:1.
By using similar data selection, it is possibleto use much smaller training data to get compara-ble or even better results than the baseline system.When N=200, using only 1/4 of the training dataand 1/3 of the model size, the adapted translationmodel achieves comparable result with the baselinemodel.
When N=500, the adapted model outper-forms the baseline model with much less trainingdata.
The results indicate that relevant data is betterdata.
The method is particular useful for SMT ap-plications on small device.2.
In general, using duplicate data achieves bet-ter results than using distinct data.
This justifiesour idea that give a higher weight to more similardata will benefit.3.
With the increase of training data size,   thetranslation performance tends to improve also.However, when the size of corpus achieves a cer-tain scale, the performance may drop.
This maybebecause that with the increase of the data, noisydata may also be included.
More and more in-cluded noises may destroy the data.
It is necessaryto use a development set to determine an optimalsize of N.We combine each adapted data with the baselinecorpus to get the optimized models.
The results areshown in Table 4.
We also compare the adaptedmodels (TopN) and the optimized models (TopN+)in the table.Without using any additional data, the optimizedmodels achieve significant better results than thebaseline model by redistributing the weight oftraining sentences.
The optimized models also out-perform adapted models when the size of theadapted data is small since they make use of all theavailable data which decrease the influence of datasparseness.
However, with the increase of theadapted data, the performance of optimized modelsis similar to that of the adapted models.System Distinct pairsBLEU onTopNBLEU onTopN+Baseline 600000 0.2363 0.2363Top100+ 600000 0.2306 0.2387Top200+ 600000 0.2360 0.2443Top500+ 600000 0.2415 0.2461Top1000+ 600000 0.2463 0.2431Top2000+ 600000 0.2351 0.2355Table 4.
Offline data optimization results4.4 Online model optimization experimentsSince 2005 NIST MT test data tends bias to FBIScorpus too much, we build a new test set to evalu-ate the online model optimization method.
We ran-domly select 500 sentences from extra part of FBIS,HK_Hansards and HK_News corpus respectively(i.e the selected 1500 test sentences are not in-cluded in any of the training set).
The correspond-ing English part is used as translation reference.Note that there is only one reference for each testsentence.
We also include top 500 sentence andtheir first reference translation of 2005 NIST MTtest data in the new test set.
So in total, the new testcontains 2000 test sentences with one translationreference for each sentence.
The test set is used tosimulate SMT system?s online inputs which maycome from various domains.The baseline translation results are shown in Ta-ble 5.
We also give results on each sub test set (de-notes as Xcorpus_part).
Please note that the abso-lute BLEU scores are not comparable to the previ-ous experiments since there is only one referencein this test set.348As expected, using the same domain data fortraining and testing achieves the best results as in-dicate by bold fonts.
The results demonstrateagain that relevant data is better data.To test our online model optimization method,we divide the baseline corpus according to the ori-gins of sub corpus.
That is, the FBIS, HK_ Han-sards and HK_News models are used as three sub-models and the baseline model is used as generalmodel.
The four weighting schemes described insection 3.2 are used as online weighting schemesindividually.
The experimental results are shown inTable 6.
S_i indicates the system using weightingscheme i.SystemTest data FBISHK_HansardsHK_News BaselineFBIS-part 0.1096 0.0687 0.0622 0.1030HK_Hans_part 0.0726 0.0918 0.0846 0.0897HK_News_part 0.0664 0.0801 0.0936 0.0870MT05_part 0.1130 0.0805 0.0776 0.1116Whole test set 0.0937 0.0799 0.0781 0.0993Table 5.
Baseline results on new test setSystemTest data S_1 S_2 S_3 S_4FBIS-part 0.1090 0.1090 0.1089 0.1089HK_Hans_part 0.0906 0.0903 0.0902 0.0902HK_News_part 0.0952 0.0950 0.0933 0.0934MT05_part 0.1119 0.1123 0.1149 0.1151Whole test set 0.1034 0.1034 0.1038 0.1038Table 6.
Online model optimization resultsDifferent weighting schemes don?t show signifi-cant improvements from each other.
However, allthe four weighting schemes achieve better resultsthan the baseline system.
The improvements areshown not only on the whole test set but also oneach part of the sub test set.
The results justify theeffectiveness of our online model optimizationmethod.5 Related workMost previous research on SMT training data isfocused on parallel data collection.
Some worktries to acquire parallel sentences from web (Nie etal.
1999; Resnik and Smith 2003; Chen et al 2004).Others extract parallel sentences from comparableor non-parallel corpora (Munteanu and Marcu2005, 2006).
These work aims to collect moreparallel training corpora, while our work aims tomake better use of existing parallel corpora.Some research has been conducted on paralleldata selection and adaptation.
Eck et al (2005)propose a method to select more informative sen-tences based on n-gram coverage.
They use n-grams to estimate the importance of a sentence.The more previously unseen n-grams in the sen-tence the more important the sentence is.
TF-IDFweighting scheme is also tried in their method, butdidn?t show improvements over n-grams.
Thismethod is independent of test data.
Their goal is todecrease the amount of training data to make SMTsystem adaptable to small devices.
Similar to ourwork, Hildebrand et al (2005) also use informationretrieval method for translation model adaptation.They select sentences similar to the test set fromavailable in-of-domain and out-of-domain trainingdata to form an adapted translation model.
Differ-ent from their work, our method further use thesmall adapted data to optimize the distribution ofthe whole training data.
It takes the full advantageof larger data and adapted data.
In addition, wealso propose an online translation model optimiza-tion method, which make it possible to selectadapted translation model for each individual sen-tence.Since large scale monolingual corpora are easierto obtain than parallel corpora.
There has someresearch on language model adaptation recentyears.
Zhao et al (2004) and Eck et al(2004) in-troduce information retrieval method for languagemodel adaptation.
Zhang et al(2006)  and  Mauseret al(2006) use adapted language model for SMTre-ranking.
Since language model is built for targetlanguage in SMT, one pass translation is usuallyneeded to generate n-best translation candidates inlanguage model adaptation.
Translation model ad-aptation doesn?t need a pre-translation procedure.Comparatively, it is more direct.
Language modeladaptation and translation model adaptation aregood complement to each other.
It is possible thatcombine these two adaptation approaches couldfurther improve machine translation performance.6 Conclusion and future workThis paper presents two new methods to im-prove statistical machine translation performanceby making better use of the available parallel train-ing corpora.
The offline data selection method349adapts the training corpora to the test domain byretrieving similar sentence pairs and redistributingtheir weight in the training data.
Experimental re-sults show that the selected small subset achievescomparable or even better performance than thebaseline system with much less training data.
Theoptimized training data can further improve trans-lation performance without using any additionalresource.
The online model optimization methodadapts the translation model to the online testsentence by redistributing the weight of eachpredefined submodels.
Preliminary results showthe effectiveness of the method.
Our work alsodemonstrates that in addition to larger training data,more relevant training data is also important forSMT model training.In future work, we will improve our methods inseveral aspects.
Currently, the similar sentence re-trieval model and the weighting schemes are verysimple.
It might work better by trying other sophis-ticated similarity measure models or using someoptimization algorithms to determine submodel?sweights.
Introducing language model optimizationinto our system might further improve translationperformance.AcknowledgementThis work was supported by National Natural Sci-ence Foundation of China, Contract No.
60603095and 60573188.ReferencesJisong Chen, Rowena Chau, Chung-Hsing Yeh 2004.Discovering Parallel Text from the World Wide Web.ACSW Frontiers 2004: 157-161Stanley F. Chen and Joshua Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
Technical Report TR-10-98, Harvard Uni-versity Center for Research in Computing Technol-ogy.Matthias Eck, Stephan Vogel, and Alex Waibel 2004.Language Model Adaptation for Statistical MachineTranslation Based on Information Retrieval.
Proceed-ings of Fourth International Conference on LanguageResources and Evaluation:327-330Matthias Eck, Stephan Vogel,  Alex Waibel 2005.
Lowcost portability for statistical machine translationbased on n-gram coverage.
MT Summit X: 227-234.Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, andQun Liu 2006.
ICT System Description for the 2006TC-STAR Run#2 SLT Evaluation.
Proceedings of TC-STAR Workshop on Speech-to-Speech Translation:63-68Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
Proceedings ofHLT-NAACL 2003: 127?133.Arne Mauser, Richard Zens, Evgeny Matusov, SasaHasan, Hermann Ney 2006.
The RWTH StatisticalMachine Translation System for the IWSLT 2006Evaluation.
Proceedings of International Workshopon Spoken Language Translation.
:103-110Dragos Stefan Munteanu and Daniel Marcu 2005.
Im-proving Machine Translation Performance by Ex-ploiting Comparable Corpora.
Computational Lin-guistics, 31 (4): 477-504Dragos Stefan Munteanu and Daniel Marcu 2006.
Ex-tracting Parallel Sub-Sentential Fragments fromComparable Corpora.
ACL-2006: 81-88Jian-Yun Nie, Michel Simard, Pierre Isabelle, RichardDurand 1999.
Cross-Language Information Retrievalbased on Parallel Texts and Automatic Mining ofParallel Texts in the Web.
SIGIR-1999: 74-81Franz Josef Och 2003.
Minimum Error Rate Training inStatistical Machine Translation.
ACL-2003:160-167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
ACL-2002: 311?318Philip Resnik and Noah A. Smith 2003.
The Web as aParallel Corpus.
Computational Linguistics 29(3):349-380Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel 2005.
Adaptation of the TranslationModel for Statistical Machine Translation based onInformation Retrieval.
Proceedings of EAMT 2005:133-142.Richard Zens, Franz Josef Och, Hermann Ney 2002.Phrase-Based Statistical Machine Translation.
An-nual German Conference on AI, KI 2002, Vol.
LNAI2479: 18-32Ying Zhang, Almut Silja Hildebrand, Stephan Vogel2006.
Distributed Language Modeling for N-best ListRe-ranking.
EMNLP-2006:216-223Bing Zhao, Matthias Eck, Stephan Vogel 2004.
Lan-guage Model Adaptation for Statistical MachineTranslation with structured query models.
COLING-2004350
