Mining Very-Non-Parallel Corpora:Parallel Sentence and Lexicon Extraction via Bootstrapping and EMPascale Fung and Percy CheungHuman Language Technology Center,University of Science & Technology (HKUST),Clear Water Bay, Hong Kong{pascale,eepercy}@ee.ust.hkAbstractWe present a method capable of extractingparallel sentences from far more disparate?very-non-parallel corpora?
than previous?comparable corpora?
methods, by exploitingbootstrapping on top of IBM Model 4 EM.
Step1 of our method, like previous methods, usessimilarity measures to find matching documentsin a corpus first, and then extracts parallelsentences as well as new word translations fromthese documents.
But unlike previous methods,we extend this with an iterative bootstrappingframework based on the principle of?find-one-get-more?, which claims thatdocuments found to contain one pair of parallelsentences must contain others even if thedocuments are judged to be of low similarity.We re-match documents based on extractedsentence pairs, and refine the mining processiteratively until convergence.
This novel?find-one-get-more?
principle allows us to addmore parallel sentences from dissimilardocuments, to the baseline set.
Experimentalresults show that our proposed method is nearly50% more effective than the baseline methodwithout iteration.
We also show that our methodis effective in boosting the performance of theIBM Model 4 EM lexical learner as the latter,though stronger than Model 1 used in previouswork, does not perform well on data fromvery-non-parallel corpus.Figure1.
Parallel sentence and lexicon extractionvia Bootstrapping and EMThe most challenging task is to extract bilingualsentences and lexicon from very-non-parallel data.Recent work (Munteanu et al, 2004, Zhao and Vogel,2002) on extracting parallel sentences fromcomparable data, and others on extractingparaphrasing sentences from monolingual corpora(Barzilay and Elhadad 2003) are based on the?find-topic-extract-sentence?
principle which claimsthat parallel sentences only exist in document pairswith high similarity.
They all use lexical information(e.g.
word overlap, cosine similarity) to matchdocuments first, before extracting sentences fromthese documents.1.
IntroductionParallel sentences are important resources fortraining and improving statistical machine translationand cross-lingual information retrieval systems.Various methods have been previously proposed toextract parallel sentences from multilingual corpora.Some of them are described in detail in (Manningand Sch?tze, 1999, Wu, 2001, Veronis 2001).
Thechallenge of these tasks varies by the degree ofparallel-ness of the input multilingual documents.However, the non-parallel corpora used so far inthe previous work tend to be quite comparable.
Zhaoand Vogel (2002) used a corpus of Chinese andEnglish versions of news stories from the XinhuaNews agency, with ?roughly similar sentence orderof content?.
This corpus can be more accuratelydescribed as noisy parallel corpus.
Barzilay andElhadad (2003) mined paraphrasing sentences fromweather reports.
Munteanu et al, (2004) used newsarticles published within the same 5-day window.
Allthese corpora have documents in the same, matchingtopics.
They can be described as on-topicdocuments.
In fact, both Zhao and Vogel (2002) andBarzilay and Elhadad (2003) assume similarsentence orders and applied dynamic programmingin their work.In our work, we try to find parallel sentences fromfar more disparate, very-non-parallel corpora than inany previous work.
Since many more multilingualtexts available today contain documents that do nothave matching documents in the other language, wepropose finding more parallel sentences fromoff-topic documents, as well as on-topic documents.An example is the TDT corpus, which is anaggregation of multiple news sources from differenttime periods.
We suggest the ?find-one-get-more?principle, which claims that as long as twodocuments are found to contain one pair of parallelsentence, they must contain others as well.
Based onthis principle, we propose an effective Bootstrappingmethod to accomplish our task (Figure 1).We also apply the IBM Model 4 EM lexicallearning to find unknown word translations from theextracted parallel sentences from our system.
TheIBM models are commonly used for word alignmentin statistical MT systems.
This EM method differsfrom some previous work, which used a seed-wordlexicon to extract new word translations or wordsenses from comparable corpora (Rapp 1995, Fung& McKeown 1997, Grefenstette 1998, Fung and Lo1998, Kikui 1999, Kaji 2003).2.
Bilingual Sentence AlignmentThere have been conflicting definitions of theterm ?comparable corpora?
in the researchcommunity.
In this paper, we contrast and analyzedifferent bilingual corpora, ranging from theparallel, noisy parallel, comparable, tovery-non-parallel corpora.A parallel corpus is a sentence-aligned corpuscontaining bilingual translations of the samedocument.
The Hong Kong Laws Corpus is aparallel corpus with manually aligned sentences, andis used as a parallel sentence resource for statisticalmachine translation systems.
There are 313,659sentence pairs in Chinese and English.
Alignment ofparallel sentences from this type of database hasbeen the focus of research throughout the last decadeand can be accomplished by many off-the-shelf,publicly available alignment tools.A noisy parallel corpus, sometimes also called a?comparable?
corpus, contains non-alignedsentences that are nevertheless mostly bilingualtranslations of the same document.
(Fung andMcKeown 1997, Kikui 1999, Zhao and Vogel 2002)extracted bilingual word senses, lexicon and parallelsentence pairs from such corpora.
A corpus such asHong Kong News contains documents that are infact rough translations of each other, focused on thesame thematic topics, with some insertions anddeletions of paragraphs.Another type of comparable corpus is one thatcontains non-sentence-aligned, non-translatedbilingual documents that are topic-aligned.
Forexample, newspaper articles from two sources indifferent languages, within the same window ofpublished dates, can constitute a comparable corpus.Rapp (1995), Grefenstette (1998), Fung and Lo(1998), and Kaji (2003) derived bilingual lexicons orword senses from such corpora.
Munteanu et al,(2004) constructed a comparable corpus of Arabicand English news stories by matching the publishingdates of the articles.Finally, a very-non-parallel corpus is one thatcontains far more disparate, very-non-parallelbilingual documents that could either be on the sametopic (in-topic) or not (off-topic).
The TDT3 Corpusis such a corpus.
It contains transcriptions of variousnews stories from radio broadcasting or TV newsreport from 1998-2000 in English and Chinese.
Inthis corpus, there are about 7,500 Chinese and12,400 English documents, covering more around 60different topics.
Among these, 1,200 Chinese and4,500 English documents are manually marked asbeing in-topic.
The remaining documents are markedas off-topic as they are either only weakly relevantto a topic or irrelevant to all topics in the existingdocuments.
From the in-topic documents, most arefound to have high similarity.
A few of the Chineseand English passages are almost translations of eachother.
Nevertheless, the existence of a considerableamount of off-topic document gives rise to morevariety of sentences in terms of content andstructure.
Overall, the TDT 3 corpus contains110,000 Chinese sentences and 290,000 Englishsentences.
Some of the bilingual sentences aretranslations of each other, while some others arebilingual paraphrases.
Our proposed method is a firstapproach that can extract bilingual sentence pairsfrom this type of very-non-parallel corpus.3.
Comparing bilingual corporaTo quantify the parallel-ness or comparability ofbilingual corpora, we propose using a lexicalmatching score computed from the bilingual wordpairs occurring in the bilingual sentence pairs.Matching bilingual sentence pairs are extracted fromdifferent corpora using existing and the proposedmethods.We then identify bilingual word pairs that appearin the matched sentence pairs by using a bilinguallexicon (bilexicon).
The lexical matching score isthen defined as the sum of the mutual informationscore of a known set of word pairs that appear in thecorpus:?==),(),()()(),(),(ec WWallececececWWSSWfWfWWfWWSwhere f(Wc,We) is the co-occurrence frequency ofbilexicon pair (Wc,We) in the matched sentencepairs.
f(Wc) and f(We) are the occurrencefrequencies of Chinese word Wc and English wordWe, in the bilingual corpus.Corpus Parallel Comparable Quasi-ComparableLexicalmatchingscore359.1 253.8 160.3Table 1: Bilingual lexical matching scores ofdifferent corporaTable 1 shows the lexical matching scores of theparallel corpus (Hong Kong Law), a comparablenoisy parallel corpus (Hong Kong News), and avery-non-parallel corpus (TDT 3).
We can see thatthe more parallel or comparable the corpus, thehigher the overall lexical matching score is.4.
Comparing alignment principlesIt is well known that existing work on sentencealignment from parallel corpus makes use of one ormultiple of the following principles (Manning andSch?tze, 1999, Somers 2001):?
A bilingual sentence pair are similar in length inthe two languages;?
Sentences are assumed to correspond to thoseroughly at the same position in the otherlanguage;?
A pair of bilingual sentences which containmore words that are translations of each othertend to be translations themselves.
Conversely,the context sentences of translated word pairsare similar.For noisy parallel corpora, sentence alignment isbased on embedded content words.
The wordalignment principles used in previous work are asfollows:?
Occurrence frequencies of bilingual word pairsare similar;?
The positions of bilingual word pairs are similar;?
Words have one dominant sense/translation percorpus.Different sentence alignment algorithms based onthe above principles can be found in Manning andSch?tze (1999), Somers (2001), Wu (2000), and1.
Initial document matchingFor all documents in the comparable corpus D:Gloss Chinese documents using the bilingual lexicon (Bilex);For every pair of glossed Chinese document and English documents,compute document similarity =>S(i,j);Obtain all matched bilingual document pairs whose S(i,j)> threshold1=>D22.
Sentence matchingFor each document pair in D2:For every pair of glossed Chinese sentence and English sentence,compute sentence similarity =>S2(i,j);Obtain all matched bilingual sentence pairs whose S2(i,j)> threshold2=>C13.
EM learning of new word translationsFor all bilingual sentences pairs in C1, do:Compute translation lexicon probabilities of all bilingual word pairs =>S3(i,j);Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j)> threshold3=>L1, and update Bilex;Compute sentence alignment scores=>S4; if (S4 does not change) return C1 and L1, otherwise continue;4.
Document re-matchingFind all pairs of glossed Chinese and English documents which contain parallel sentences (anchor sentences) fromC1=>D3;Expand D2 by finding documents similar to each of the document in D2;D2:=D3;Goto 2;Figure 2.
Bootstrapping with EMVeronis (2002).
These methods have also beenapplied recently in a sentence alignment shared taskat NAACL 20031.
We have also learned that asbilingual corpora become less parallel, it is better torely on lexical information rather than sentencelength and position information.For comparable corpora, the alignment principlemade in previous work is as follows:?
Parallel sentences only exist in document pairswith high similarity scores ?
?find-topic-extract-sentence?We take a step further and propose a newprinciple for our task:?
Documents that are found to contain at least onepair of parallel sentences are likely to containmore parallel sentences ?
?find-one-get-more?5.
Extracting Bilingual Sentences fromVery-Non-Parallel CorporaExisting algorithms such as Zhao and Vogel,(2002), Barzilay and Elhadad, (2003), Munteanu etal., (2004) for extracting parallel or paraphrasingsentences from comparable documents, are based onthe ?find-topic-extract-sentence?
principle whichlooks for document pairs with high similarities, andthen look for parallel sentences only from thesedocuments.Based on our proposed ?find-one-get-more?principle, we suggest that there are other, dissimilardocuments that might contain more parallelsentences.
We can iterate this whole process forimproved results using a Bootstrapping method.Figure 2 outlines the algorithm in more detail.
In thefollowing sections 5.1-5.5, we describe thedocument pre-processing step followed by the foursubsequent iterative steps of our algorithm.5.1.
Document preprocessingThe documents are word segmented with theLanguage Data Consortium (LDC) Chinese-Englishdictionary 2.0.Then the Chinese document is glossedusing all the dictionary entries.
When a Chineseword has multiple possible translations in English, itis disambiguated by a method extended from (Funget al 1999).5.2.
Initial document matchingThis initial step is based on the same?find-topic-extract-sentence?
principle as in earlierworks.
The aim of this step is to roughly match theChinese-English documents pairs that have the sametopic, in order to extract parallel sentences from1 http://www.cs.unt.edu/~rada/wpt/them.
Similar to previous work, comparability isdefined by cosine similarity between documentvectors.Both the glossed Chinese document and Englishare represented in word vectors, with term weights.We evaluated different combinations of termweighting of each word in the corpus: termfrequency (tf); inverse document frequency (idf);tf.idf; and the product of a function of tf and idf.The ?documents?
here are sentences.
We find thatusing idf alone gives the best sentence pair rank.This is probably due to the fact that frequencies ofbilingual word pairs are not comparable in avery-non-parallel corpus.Pair-wise similarities are calculated for allpossible Chinese-English document pairs, andbilingual documents with similarities above a certainthreshold are considered to be comparable.
Forvery-non-parallel corpora, this document-matchingstep also serves as topic alignment.5.3.
Sentence matchingAgain based on the ?find-topic-extract-sentence?principle, we extract parallel sentences from thematched English and Chinese documents.
Eachsentence is again represented as word vectors.
Foreach extracted document pair, pair-wise cosinesimilarities are calculated for all possibleChinese-English sentence pairs.
Sentence pairsabove a set threshold are considered parallel andextracted from the documents.
Sentence similarity isbased on the number of words in the two sentencesthat are translations of each other.
The better ourbilingual lexicon is, the more accurate the sentencesimilarity will be.
In the following section, wediscuss how to find new word translations.5.4.
EM lexical learning from matched sentencepairsThis step updates the bilingual lexicon accordingto the intermediate results of parallel sentenceextraction.
New bilingual word pairs are learnedfrom the extracted sentence pairs based on an EMlearning method.
We use the GIZA++ (Och andNey, 2000) implementation of the IBM statisticaltranslation lexicon Model 4 (Brown et al, 1993) forthis purpose.This model is based on the conditional probabilityof a source word being generated by the target wordin the other language, based on EM estimation fromaligned sentences.
Zhao and Vogel (2002) showedthat this model lends itself to adaptation and canprovide better vocabulary coverage and bettersentence alignment probability estimation.
In ourwork, we use this model on the intermediate resultsof parallel sentence extraction, i.e.
on a set ofaligned sentence pairs that may or may not trulycorrespond to each other.We found that sentence pairs with high alignmentscores are not necessarily more similar than others.This might be due to the fact that EM estimation ateach intermediate step is not reliable, since we onlyhave a small amount of aligned sentences that aretruly parallel.
The EM learner is therefore weakwhen applied to bilingual sentences fromvery-non-parallel corpus.
We decided to try usingparallel corpora to initialize the EM estimation, as inZhao and Vogel (2002).
The results are discussed inSection 6.5.5.
Document re-matching: find-one-get-moreThis step augments the earlier matched documentsby the ?find-one-get-more?
principle.
From the setof aligned sentence pairs, we look for otherdocuments, judged to be dissimilar in the first step,that contain one or more of these sentence pairs.
Wefurther find other documents that are similar to eachof the monolingual documents found.
This new setof documents is likely to be off-topic, yet containssegments that are on-topic.
Following our newalignment principle, we believe that these documentsmight still contain more parallel sentence candidatesfor subsequent iterations.
The algorithm then iteratesto refine document matching and parallel sentenceextraction.5.6.
ConvergenceThe IBM model parameters, including sentencealignment score and word alignment scores, arecomputed in each iteration.
The parameter valueseventually stay unchanged and the set of extractedbilingual sentence pairs also converges to a fixedsize.
The system then stops and gives the last set ofbilingual sentence pairs as the final output.6.
EvaluationWe evaluate our algorithm on a very-non-parallelcorpus of TDT3 data, which contains various newsstories transcription of radio broadcasting or TVnews report from 1998-2000 in English and ChineseChannels.
We compare the results of our proposedmethod against a baseline method that is based onthe conventional, ?find-topic-extract-sentence?principle only.
We investigate the performance ofthe IBM Model 4 EM lexical learner on data fromvery-non-parallel corpus, and evaluate how ourmethod can boost its performance.
The results aredescribed in the following sub-sections.6.1.
Baseline methodSince previous works were carried out on differentcorpora, in different language pairs, we cannotdirectly compare our method against them.However, we implement a baseline method thatfollows the same ?find-topic-extract-sentence?principle as in earlier work.
The baseline methodshares the same preprocessing, document matchingand sentence matching steps with our proposedmethod.
However, it does not iterate to update thecomparable document set, the parallel sentence set,or the bilingual lexicon.Human evaluators manually check whether thematched sentence pairs are indeed parallel.
Theprecision of the parallel sentences extracted is 42.8%for the top 2,500 pairs, ranked by sentence similarityscores.6.2.
Bootstrapping performs much betterThere are 110,000 Chinese sentences and290,000 English sentences in TDT3,  which lead tomore than 30 billion  possible sentence pairs.
Fewof the sentence pairs turn out to be exact translationsof each other, but many are bilingual paraphrases.For example, in the following extracted sentencepair, the English sentence has the extra phrase?under the agreement?, which is missing from theChinese sentence:?
???????????
??
(Hun Sen becomes Cambodia ' s sole primeminister)?
Under the agreement, Hun Sen becomesCambodia ' s sole prime minister.Another example of translation versus bilingualparaphrases is as follows:?
???????????????????
(The Chinese president Jiang Zemin arrived inJapan today for a state visit)(Translation) Chinese president Jiang Zeminarrived in Japan today for a landmark state visit.?
???????????????
(This is afirst visit by a Chinese head of state to Japan)(Paraphrase) Mr Jiang is the first Chinese head ofstate to visit the island country.The precision of parallel sentences extraction is65.7% for the top 2,500 pairs using our method,which has a 50% improvement over the baseline.
Inaddition, we also found that the precision of parallelsentence pair extraction increases steadily over eachiteration, until convergence.6.3.
Bootstrapping can boost a weak EM lexicallearner6.4.
Bootstrapping is significantly more usefulthan new word translations for miningparallel sentences In this section, we discuss experimental resultsthat lead to the claim that our proposed method canboost a weak IBM Model 4 EM lexical learner.It is important for us to gauge the effects of thetwo main ideas in our algorithm, Bootstrapping andEM lexicon learning, on the extraction parallelsentences from very-non-parallel corpora.
Thebaseline experiment shows that without iteration, theperformance is at 42.8%.
We carried out another setof experiment of using Bootstrapping where thebilingual lexicon is not updated in each iteration.The bilingual sentence extraction accuracy of the top2500 sentence pairs in this case dropped to 65.2%,with only 1% relative degradation.6.3.1.
EM lexical learning is weak on bilingualsentences from very-non-parallel corporaWe compare the performances of the IBM Model4 EM lexical learning on parallel data (130ksentence pairs from Hong Kong News) andvery-non-parallel data (7200 sentence pairs fromTDT3) by looking at a common set of source wordsand their top-N translation candidates extracted.
Wefound that the IBM Model 4 EM learning performsmuch worse on TDT3 data.
Figure 3 shows that theEM learner performs about 30% worse on averageon the TDT3 data.Based on the above, we conclude that EM lexicallearning has little effect on the overall parallelsentence extraction output.
This is probably due tothe fact that whereas EM does find new wordtranslations (such as ???
?/Pinochet), this haslittle effect on the overall glossing of the Chinesedocument since such new words are rare.7.
ConclusionPrevious work on extracting bilingual ormonolingual sentence pairs from comparablecorpora has only been applied to documents that arewithin the same topic, or have very similarpublication dates.
One principle for previousmethods is ?find-topic-extract-sentence?
whichclaims that parallel or similar sentences can only befound in document pairs with high similarity.
Wepropose a new, ?find-one-get-more?
principle whichclaims that document pairs that contain at least onepair of matched sentences must contain others, evenif these document pairs do not have high similarityscores.
Based on this, we propose a novelBootstrapping method that successfully extractsparallel sentences from a far more disparate andvery-non-parallel corpus than reported in previouswork.
This very-non-parallel corpus, TDT3 data,includes documents that are off-topic, i.e.
documentswith no corresponding topic in the other language.This is a completely unsupervised method.Evaluation results show that our approach achieves65.7% accuracy and a 50% relative improvementfrom baseline.
This shows that the proposedmethod is promising.
We also find that the IBMModel 4 lexical learner is weak on data fromvery-non-parallel corpus, and that its performancecan be boosted by our Multilevel Bootstrappingmethod, whereas using parallel corpus for adaptationis not nearly as useful.Figure 3.
EM lexical learning performance6.3.2.
Multilevel Bootstrapping is significantlybetter than adaptation data in boosting theweak EM lexical learnerSince the IBM model parameters can be betterestimated if the input sentences are more parallel, wehave tried to add parallel sentences to the extractedsentence pairs in each iteration step, as proposed byZhao and Vogel (2002).
However, our experimentsshowed that adding parallel corpus gives noimprovement on the final output.
This is likely dueto (1) the parallel corpus is not in the same domainas the TDT corpus; and (2) there are simply notenough parallel sentences extracted at each step forthe reliable estimation of model parameters.In contrast, Figure 3 shows that when we applyBootstrapping to the EM lexical learner, thebilingual lexicon extraction accuracy is improved by20% on the average, evaluated on top-N translationcandidates of the same source words, showing thatour proposed method can boost a weak EM lexicallearner even on data from very-non-parallel corpus.In addition, we compare and contrast a number ofbilingual corpora, ranging from the parallel, tocomparable, and to very-non-parallel corpora.
Theparallel-ness of each type of corpus is quantified bya lexical matching score calculated for the bi-lexiconpair distributed in the aligned bilingual sentencepairs.
We show that this scores increases as theparallel-ness or comparability of the corpusincreases.Finally, we would like to suggest thatBootstrapping can in the future be used inconjunction with other sentence or word alignmentlearning methods to provide better mining results.For example, methods for learning a classifier todetermine sentence parallel-ness such as thatproposed by Munteanu et al, (2004) can beincorporated into our Bootstrapping framework.ReferencesRegina Barzilay and Noemie Elhadad, SentenceAlignment for Monolingual Comparable Corpora,Proc.
of EMNLP, 2003, Sapporo, Japan.Peter F. Brown, S.A. Della Pietra, V.J.
Della Pietra,and R.L.
Mercer.
The mathematics of statisticalmachine translation: parameter estimation, inComputational Linguistics, 19-2, 1993.Pascale Fung and Kathleen Mckeown.
Findingterminology translations from non-parallelcorpora.
In The 5th Annual Workshop on VeryLarge Corpora.
pages 192--202, Hong Kong,Aug.
1997.Pascale Fung and Lo Yuen Yee.
An IR Approach forTranslating New Words from Nonparallel,Comparable Texts.
In COLING/ACL  1998Pascale Fung, Liu, Xiaohu, and Cheung, Chi Shun.Mixed-language Query Disambiguation.
InProceedings of ACL ?99, Maryland: June 1999Gale, W A and Kenneth W.Church.
A Program forAligning Sentences in Bilingual Corpora.Computatinal Linguistics.
vol.19 No.1 March,1993.Gregory Grefenstette, editor.
Cross-LanguageInformation Retrieval.
Kluwer AcademicPublishers, 1998.Hiroyuki Kaji, Word sense acquisition frombilingual comparable corpora, in Proceedings ofthe NAACL, 2003, Edmonton, Canada, pp111-118.Genichiro Kikui.
Resolving translation ambiguityusing non-parallel bilingual corpora.
InProceedings of ACL99 Workshop onUnsupervised Learning in Natural LanguageChristopher D. Manning and Hinrich Sch?tze.Foundations of Statistical Natural LanguageProcessing.
The MIT Press.Dragos Stefan Munteanu, Alexander Fraser, DanielMarcu, 2004.
Improved Machine TranslationPerformance via Parallel Sentence Extractionfrom Comparable Corpora.
In Proceedings of theHuman Language Technology and NorthAmerican Association for ComputationalLinguistics Conference (HLT/NAACL 2004).Franz Josef Och and Hermann Ney.
Improvedstatistical alignment models, in Proceedings ofACL-2000.Reinhard Rapp.
Identifying word translations innon-parallel texts.
Proceedings of the 33rdMeeting of the Association for ComputationalLinguistics.
Cambridge, MA, 1995.
320-322Philip Resnik and Noah A. Smith.
The Web as aParallel Corpus.
Computational Linguistics29(3), pp.
349-380, September 2003.Frank Smadja.
Retrieving collocations from text:Xtract.
In Computational Linguistics,19(1):143-177,1993Harold Somers.
Bilingual Parallel Corpora andLanguage Engineering.
Anglo-Indian workshop"Language Engineering for South-Asianlanguages" (LESAL), (Mumbai, April 2001).Jean Veronis (editor), Parallel Text Processing:Alignment and Use of Translation Corpora.Dordrecht: Kluwer.
ISBN 0-7923-6546-1.
Aug2000.Dekai Wu.
Alignment.
In Robert Dale, HermannMoisl, and Harold Somers (editors), Handbook ofNatural Language Processing.
415-458.
NewYork: Marcel Dekker.
ISBN 0-8247-9000-6.
Jul2000.Bing Zhao, Stephan Vogel.
Adaptive ParallelSentences Mining from Web Bilingual NewsCollections.
In Proceedings of the IEEEWorkshop on Data Mining 2002.
