Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 499?509,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLinear Mixture Models for Robust Machine TranslationMarine Carpuat, Cyril Goutte and George FosterMultilingual Text ProcessingNational Research CouncilOttawa, ON K1A0R6, Canadafirstname.lastname@nrc.caAbstractAs larger and more diverse parallel textsbecome available, how can we lever-age heterogeneous data to train robustmachine translation systems that achievegood translation quality on various testdomains?
This challenge has been ad-dressed so far by repurposing techniquesdeveloped for domain adaptation, suchas linear mixture models which combineestimates learned on homogeneous sub-domains.
However, learning from largeheterogeneous corpora is quite differentfrom standard adaptation tasks with cleardomain distinctions.
In this paper, weshow that linear mixture models can re-liably improve translation quality in veryheterogeneous training conditions, evenif the mixtures do not use any domainknowledge and attempt to learn genericmodels rather than adapt them to the tar-get domain.
This surprising finding opensnew perspectives for using mixture mod-els in machine translation beyond clear cutdomain adaptation tasks.1 IntroductionWhile machine translation tasks used to be de-fined by drawing training and test data from a sin-gle well-defined domain, current systems have todeal with increasingly heterogeneous data, both attraining and at test time.
As larger and more di-verse parallel texts become available, how can weleverage heterogeneous data to train statistical ma-chine translation (SMT) systems that achieve goodtranslation quality on various test domains?So far, this challenge has been addressed by re-purposing techniques developed for more clear-cutdomain adaptation scenarios, such as linear mix-ture models (Koehn and Schroeder, 2007; Fosterand Kuhn, 2007; Sennrich, 2012b).
Instead of es-timating models on the whole training corpus atonce, linear mixture models are built as follows:(1) partition the training corpus into homogeneousdomain-based component, (2) train one model percomponent, (3) linearly mix models using weightslearned to adapt to the test domain, (4) replace re-sulting model in translation system.In this paper, we aim to gain a better under-standing of the benefits of linear mixture models inheterogeneous data conditions, by examining keyuntested assumptions:?
Should mixture component capture domaininformation?
Previous work assumes thattraining data should be organized into do-mains.
When manual domain distinctions arenot available, previous work uses clusteringapproaches to approximate manual domaindistinctions (Sennrich, 2012a).
However, itis unclear whether it is necessary to use ormimic domain distinctions in order to definemixture components.?
Mixture models are usually assumed to im-prove translation quality by giving moreweight to parts of the training corpus that aremore relevant to the test domain.
Is this intu-ition still valid in our more complex hetero-geneous training conditions?
If not, how domixture models affect translation probabilityestimates?In order to answer these questions, we proposeto study several variants of linear mixture mod-els that reflect different modeling assumptions anddifferent levels of domain knowledge.
We first499consider two methods for setting mixture weights:adaptation to the test domain via maximum like-lihood, and uniform mixtures that make no as-sumption about the domain of interest (Section 2).Then, we will describe a wide range of tech-niques that can be used to define mixture com-ponents (Section 3).
Again, these techniques re-flect opposite modeling assumptions: manuallydefined domains and automatic clusters attempt toorganize heterogeneous training sets into homo-geneous groups that represent distinct domains,while random samples capture no domain infor-mation and simply provide different views of thetraining set.We present an empirical investigation of all thevariations outlined above using a strong systemtrained on large and diverse training corpora, fortwo language pairs and two distinct test domains.Our results show that linear mixtures reliably androbustly improve the quality of machine transla-tion (Section 5).
While they were originally de-veloped for domain adaptation tasks, linear mix-tures that have no domain knowledge can performas well as traditional mixtures meant to performdomain adaptation.
This suggests that improve-ments do not stem from domain modeling per se,but from better generic estimates from the hetero-geneous training data.
Further analysis shows thatthe linear mixture estimates are very different fromestimates obtained using more explicit smoothingschemes (Section 6).2 Linear Mixtures for TranslationModelsDoes domain knowledge yield better translationquality when learning linear mixture weights forthe translation model of a phrase-based MT sys-tem?
We leave the study of linear mixtures forlanguage and reordering models for future work.2.1 Maximum Likelihood MixturesIn the standard domain adaptation scenario, thelinear mixture combines translation probabilitieslearned on distinct sub-domains in the trainingcorpus.
The conditional translation probability ofphrase t given s is defined as:p(t|s) =K?k=1?kpk(t|s) (1)where pk(t|s) is a conditional translation proba-bility learned on subset k of the training corpus.Note that for all phrase pairs (s, t) that are not ob-served in component k of the training corpus, wewill have pk(t|s) = 0.
As a result, the resultingdistributions are not normalized.The weights ?kare learned to adapt the transla-tion model to a development set, which representsthe domain of interest.
First, we extract all phrasepairs from the development set, using the sametechnique used to extract phrases from the trainingset as part of standard phrase-based MT training.This yields a joint distribution p?
(s, t), which canbe used to define a maximum likelihood objective:??
= argmax??s,tp?
(s, t) logK?k=1?kpk(s|t).
(2)We use the Expectation Maximization algo-rithm to solve this maximization problem.2.2 Uniform MixturesWe will consider uniform mixtures where all com-ponents are weighted equally:p(t|s) =1KK?k=1pk(t|s).
(3)In contrast with maximum likelihood mixtures,uniform mixtures are not meant to adapt the trans-lation model to a specific test domain.
Instead,they combine estimates learned on various subsetsof the data in the hope of obtaining a better es-timate of the translation probability distributionsfrom the (possibly heterogeneous) training domainas a whole.2.3 Why Not Use Loglinear Mixtures?In current machine translation systems, there aretwo straightforward ways to combine estimatesfrom heterogneous training data: linear and loglin-ear mixtures.
We argue that linear mixtures are abetter model for combining domain-specific prob-abilities, since they sum translation probabilities,while loglinear mixtures multiply probabilities.
Ina loglinear mixture, a translation candidate t for aphrase s will only be scored highly if all compo-nents agree that it is highly probable.
In contrast,in a linear mixture, t can be a top translation can-didate overall even if it is not a preferred transla-tion in some of the components.
When the train-ing data is very heterogeneous, linear mixtures aretherefore preferable.500Previous work provides empirical evidence sup-porting this.
For instance, Foster et al.
(2010)found that linear mixtures outperform log linearmixtures when adapting a French-English systemto the medical domain, as well as on a Chinese-English NIST translation task.2.4 Estimating Conditional TranslationProbabilitiesWithin each mixture component, we extract allphrase-pairs, compute relative frequencies, anduse Kneser-Ney smoothing (Chen et al., 2011) toproduce the final estimate of conditional transla-tion probabilities pk(t|s).
Per-component proba-bilities are then combined in Eq.
1 and 3.
Simi-larly, baseline translation probabilities are learnedusing Kneser-Ney smoothed frequencies collectedon the entire training set.3 Defining Mixture ComponentsWe assume that the heterogeneous training corpuscan be split into basic elements that will be or-ganized in various ways to define the K mixturecomponents.
Basic components could be docu-ments or sets of sentences defined along variouscriteria.
Sennrich (2012a) show that using iso-lated sentences as basic elements might not pro-vide sufficient information, as smoothing com-ponent assignments using neighboring sentencesbenefits translation quality.
In our experiments,basic elements are sets of parallel sentences whichshare the same provenance, genre and dialect, aswe will see in Section 4.We consider four very different ways of defin-ing mixture components by grouping the basiccorpus elements: (1) manual partition of the train-ing corpus into domains, (2) automatically learn-ing homogeneous domains using text clusteringalgorithms, (3) random partitioning, (4) samplingwith replacement.3.1 Manually Defined DomainsHeterogeneous training data is usually groupedinto domains manually using provenance informa-tion.
In most previous work, such domain dis-tinctions are very clear and easy to define.
Forinstance, Haddow (2013) uses European parlia-ment proceedings to improve translation of textin the movie subtitles and News Commentary do-mains; Sennrich (2012a) aims to translate AlpineClub reports using components trained on Euro-pean parliament proceedings and movie subtitles.Foster et al.
(2010) work with a slightly differ-ent setting when defining mixture components forthe NIST Chinese-English translation task: whilethere is no single obvious ?in-domain?
componentin the NIST training set, homogeneous domainscan still be defined in a straightforward fashionbased on the provenance of the data (e.g., HongKong Hansards vs. Hong Kong Law vs. News ar-ticles from FBIS, etc.).
We take a similar approachin our experiments.
However, we will see thatsince our training data is very heterogeneous, wetake into account other dimensions beyond prove-nance, such as genre and dialect information (Sec-tion 4).3.2 Induced Domains Using AutomaticClustering AlgorithmsWe propose to use automatic text clustering tech-niques to organize basic elements into homoge-neous clusters that are seen as sub-domains.
In ourexperiments, we apply clustering algorithms to thetarget (English) side of the corpus only.Each corpus element is transformed into avector-space format by constructing a tf.idf vectorrepresentation.
After indexing, we filter out stop-words as well as words occuring in a single doc-ument.
We then weight each word token by thelog of its frequency in the document, combinedwith an inverse document frequency (Salton andMcGill, 1983) followed by a normalization to unitlength.
The cosine similarity between each pairof elements is obtained by simply computing thescalar product, resulting in aN?N similarity ma-trix, where N is the number of corpus elements.For clustering, we used Ward?s hierarchicalclustering algorithm (Ward, 1963).
We start withone cluster per corpus element, i.e.
N clusters.From the similarity matrix, we identify the twomost similar clusters and merge them into a sin-gle one, resulting in N ?1 clusters.
The similaritymatrix is updated using Ward?s method to form a(N?1)?
(N?1) similarity matrix.
The process isrepeated on the new set of clusters, until we reachthe target number of clusters K.3.3 Random PartitioningWe consider random partitions of the training cor-pus.
They are generated by using a random num-ber generator to assign each basic element to oneof K clusters.
Resulting components therefore donot capture any domain information.
Each com-501Arabic-English Training Conditionssegs src entrain 8.5M 262M 207MTest Domain 1: Webforumsegs src endev (tune) 4.1k 66k 72kweb1 (eval) 2.2k 35k 38kweb2 (eval) 2.4k 37k 40kTest Domain 2: Newssegs src endev (tune) 1664 54k 51knews (eval) 813 32k 29kTable 1: Statistics for Arabic-English data: Num-ber of segments (segs), source tokens (src) and En-glish tokens (en) for each corpus.
For English devand test sets, word counts averaged across 2 refer-ences.ponent can potentially be as heterogeneous as thefull training set.3.4 Random Sampling with ReplacementAll previous techniques assume that the trainingcorpus should be partitioned into distinct clus-ters.
We now consider mixture components thatbreak this assumption, and simply represent sev-eral, possibly overlapping, views of the trainingcorpus.
They are defined by sampling basic corpuselements uniformly with replacement.
This ap-proach simply requires defining a number of sam-ples K and the size n of each sample.
We set thesample size n to the average size of the manualclusters.
We do not fix K in advance: in order toprovide a fair comparison with corpus partitioningtechniques where components achieve coverage ofthe entire training set by definition, we keep gen-erating samples until all basic elements have beenused, and use all resulting K components.When using uniform linear mixtures, this ap-proach is similar to bootstrap aggregating (bag-ging) for regression (Breiman, 1996), where amore stable model is learned by averaging K es-timates obtained by sampling the training set uni-formly and with replacement.4 Experiment SettingsWe evaluate our linear mixture models on twodifferent language pairs, Arabic-English andChinese-English, and two different test domains.Chinese-English Training Conditionssegs src entrain 11M 234M 253MTest Domain 1: Webforumsegs src endev (tune) 2.7k 61k 77kweb1 (eval) 1.4k 31k 38kweb2 (eval) 1.2k 29k 36kTest Domain 2: Newssegs src endev (tune) 1.7k 39k 24knews (eval) 0.7k 19k 19kTable 2: Statistics for Chinese-English data: Num-ber of segments (segs), source tokens (src) and En-glish tokens (en) for each corpus.
For English devand test sets, word counts averaged across 4 refer-ences.4.1 Training ConditionsWe use the large-scale heterogeneous training con-ditions defined in the DARPA BOLT project.
Datastatistics for both language pairs are given in Ta-bles 1 and 2.
Training corpora cover a wide varietyof sources, genres, dialects, domains, topics.For instance, for the Arabic task, the trainingcorpus is originally bundled into 48 files repre-senting different provenance and epochs.
Thedata spans 15 genres (defined based on dataprovenance, they range from lexicon to newswire,United Nations, and many variants of web datasuch as webforum, weblog, newsgroup, etc.)
and4 automatically tagged dialects (Egyptian, Levan-tine, Modern Standard Arabic, and untagged).
Thedistribution along each of these dimensions is veryunbalanced, and each corpus file often containstext in more than one genre, epoch or dialect.As a result, we divide the large training corpusinto basic elements, based on the available meta-data.
We define basic corpus elements as a subsetof sentences from the same provenance (i.e.
cor-pus file), dialect and genre.
For Arabic, splittingthe original 48 files along these dimensions yields82 basic elements.
Similarly, the Chinese data wassplit into a set of 101 basic elements, using genre,dialects, as well as time span information to splitthe original files.
Figure 1 shows the wide range ofcomponent sizes in the Arabic and Chinese collec-tion.
For Arabice, notice that several componentsare very small, from 6 lines and 90 words to 5.3million lines and 137M words.5020 20 40 60 801e+011e+05Arabic corpus componentscorpus component#lines/ #wordslllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll #lines  #words0 20 40 60 80 1001e+031e+051e+07Chinese corpus componentscorpus component#lines/ #wordsllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll #lines  #wordsFigure 1: Sizes of the 82 Arabic-English (top)and 101 Chinese-English (bottom) corpus compo-nents.4.2 Definition of Mixture ComponentsManual partitions were created first by the systemdevelopers, based on intuitions on the nature of thetest domain and manual inspection of the trainingdata.
The main goal was to group data into com-ponents that are large enough to reliably estimatetranslation probabilities, but small enough to behomogeneous.
This resulted in Km= 10 clustersfor Arabic, and Km= 17 for Chinese.Automatic partitions are created as describedin Section 3.
Preliminary experiments with thehierarchical agglomerative clustering algorithmshowed that the number of clusters used did nothave a big impact on translation quality,1so wewill only present results that use the same num-ber of clusters as in the manual partitions (10 forArabic and 17 for Chinese).Results for random partitions are averagedacross experiments run with four random seeds.4.3 Test DomainsWe consider two test domains, as described in Ta-bles 1 and 2: webforum and news.The webforum test domain is defined by devel-opment test sets made available through BOLT.
It1We tried K = {2, 4, .
.
.
, 18, 20} for Arabic and K ={12, 14, .
.
.
, 20} for Chinese, plus all basic components.contains very informal text drawn from online dis-cussion of various topics.
Taking these data setsas the definition of the target domain, there is nosingle obvious in-domain section of the trainingcorpus.
For instance, for Arabic, the dev set sen-tences are almost exclusively written in the Egyp-tian dialect.
Therefore, Egyptian webforum datais presumably the closest to the test domain, butEgyptian weblogs or mixed-dialect broadcast con-versations could potentially be useful as well.We also test the Arabic and Chinese systems onthe news domain.
The goal of these experiments isto evaluate the robustness of linear mixtures acrossdifferent test domains.
We use publicly availabletest sets from the NIST evaluation.
The dev setused to learn maximum likelihood mixtures andtune the translation system is the NIST section ofthe 2006 test set.
We evaluate system performanceon the newswire section of the NIST 2008 test set.4.4 Machine Translation SystemWe use an in-house implementation of a Phrase-based Statistical Machine Translation system(Koehn et al., 2007) to build strong baseline sys-tems for both language pairs.
Translation hypothe-ses are scored according to the following features:?
4 phrase-table scores: Kneser-Ney smoothedphrasal translation probabilites and lexicalweights, in both translation directions (Chenet al., 2011)2?
6 hierarchical lexicalized reordering scores(Galley and Manning, 2008)?
a word penalty, and a word-displacement dis-tortion penalty?
a Good-Turing smoothed 4-gram languagemodel trained on the Gigaword corpus,Kneser-Ney smoothed 5-gram models trainedon the English side of the training corpus, andan additional 5-gram model trained on mono-lingual webforum data.Weights for these features are learned using abatch version of the MIRA algorithm (Chiang,2012).
Phrase pairs are extracted from severalword alignments of the training set: HMM, IBM2,and IBM4.
Word alignments are kept constantacross all experiments.We apply our linear mixture models to bothtranslation probability scores, in each direction.The reordering and language models are not2The Arabic-English system uses 6 additional binary fea-tures which fire if a phrase-pair was generated by one of the3 word alignment methods in each translation direction.503Test domain WebforumArabic eval Forum1 Forum2Linear mix 39.67 40.60Loglinear mix 37.53 38.80Chinese eval Forum1 Forum2Linear mix 30.17 26.86Loglinear mix 27.65 23.78Table 3: Impact of mixture type on translationquality as measured by BLEU.adapted.
Note that systems used to translate theweb1 and web2 test sets are always tuned on thewebforum tuning set, while systems used to trans-late data in the news domain are tuned on a newsdevelopment set.
The relevant tuning set is alsoused for learning maximum likelkihood mixtureswhen appropriate.5 Findings: Impact on TranslationQuality5.1 Linear vs. Loglinear MixturesBefore focusing exclusively on linear mixtures,we confirm that they outperform loglinear mix-tures.
This comparison was conducted on the web-forum domain, using manually defined domainsas components.
For linear mixtures, we trainedthe weights using maximum likelihood.
Loglin-ear mixture weights are trained by MIRA.
Table 3shows that linear mixtures yield consistently andsignificantly higher BLEU scores than loglinearmixtures, which is consistent with existing results(Foster et al., 2010, inter alia).5.2 Impact of Mixture ComponentsWe now focus on linear mixtures and measure theimpact on translation quality of the various com-ponent types described in Section 3.
In all cases,mixtures weights are estimated by maximum like-lihood.
Results are summarized in Table 4 for bothArabic and Chinese.The main result is that all mixture models con-sidered significantly improve on the ?no mix?baseline for both languages.
Directly using the101 basic elements for Chinese and the 82 basicelements for Arabic significantly improves on thebaseline.
Grouping the basic elements into coarserclusters can further improve BLEU.
For Arabic,automatic partitioning (randomly or by clustering)yields better BLEU scores than manual partition-Test domain Webforum NewsArabic eval web1 web2 newsCluster domains 40.11 40.60 57.95Random partition 40.43 40.63 57.78Random sample 39.94 40.36 57.85Manual domains 39.67 40.60 57.63Basic elements 39.83 40.63 57.57No mix 38.64 39.21 56.59Chinese eval web1 web2 newsCluster domains 29.82 26.34 37.22Random partition 29.50 26.21 36.83Random sample 29.47 26.17 36.70Manual domains 30.17 26.86 36.90Basic elements 29.29 26.25 36.17No mix 28.61 25.63 35.96Table 4: Impact of mixture component definitionon BLEU score: there is no clear benefit to explic-itly modeling domains.ing, while the manual and cluster-based domainsyield the highest BLEU scores for Chinese.5.3 Impact of Mixture WeightsDoes domain knowledge yield better translationquality when learning linear mixture weights?
Weanswer this question by comparing the transla-tion quality obtained with maximum likelihoodvs.
uniform mixtures.
The maximum likelihoodweights are set once per domain, using the rele-vant domain development set, while the uniformmixture is the same across all test domains.Table 5 shows that maximum likelihoodweights generally have a slight advantage overuniform weights, especially in the Webforum do-main.
On ?basic elements?
in Arabic, the gain isa massive 5 BLEU points, which we attribute tothe fact that, as shown in Figure 1, there are manymore very small components in Arabic.
Those geta disproportionate influence in the uniform mix-ture, hurting the overall performance.
On the otherhand, the uniform mixture performs better in theNews domain.
This might be explained by the factthat the tune and test sets are more distant in Newsthan in Webforum, as suggested by the fact that thetuning BLEU scores are not as good at predicingtest BLEU rankings in the news domain as in thewebforum domain.Overall, the difference in performance betweenthe best linear mixture and the ?no mix?
baselineis 1.4 to 1.6 BLEU on Arabic, and 0.7 to 1.3 BLEU504on Chinese.
By comparison, the delta between thetwo weight setting approaches (maximum likeli-hood vs. uniform), depending on the partition-ing technique, is below 0.4 BLEU for Arabic (ex-cept for Basic elements, +3.6 BLEU) and below0.57 BLEU for Chinese.
It is therefore clear thatthe gain from using linear mixtures is much largerthan the influence of the mixture weight setting,except in the one specific case discussed above.Taken together, these results show that lin-ear mixtures can reliably and robustly improvethe quality of machine translation.
But surpris-ingly, linear mixtures that have no domain knowl-edge (random partition + uniform weights) cansometimes perform as well as traditional mixturesmeant to perform domain adaptation.
This sug-gests that improvements cannot be only explainedby improved domain modeling.Test domain Webforum NewsArabic eval web1 web2 newsCluster domains 40.11 40.60 57.95w/ uniform mix 39.63 40.15 58.21Random partition 40.43 40.63 57.78w/ uniform mix 40.31 40.15 58.18Random sample 39.94 40.36 58.06w/ uniform mix 39.88 40.56 58.65Manual domains 39.67 40.60 57.63w/ uniform mix 39.93 40.18 58.00Basic elements 39.83 40.63 57.57w/ uniform mix 34.84 35.82 58.46No mix 38.64 39.21 56.59Chinese eval web1 web2 newsCluster domains 29.82 26.34 37.22w/ uniform mix 29.44 25.94 37.47Random partition 29.50 26.21 36.83w/ uniform mix 29.43 25.89 36.95Random sample 29.47 26.17 36.70w/ uniform mix 28.47 25.54 36.61Manual domains 30.17 26.86 36.90w/ uniform mix 29.25 26.36 36.95Basic elements 29.29 26.25 36.17w/ uniform mix 29.23 25.81 36.63No mix 28.61 25.63 35.96Table 5: Impact of linear mixture weights on trans-lation quality as measured by BLEU: using do-main knowledge when setting weights has an un-reliable impact.6 Findings: Impact on TranslationProbability EstimatesThus far, all our experiments have measured theimpact of different types of linear mixtures onoverall translation quality.
But what is the im-pact of these various estimations methods on thelearned phrasal translation probability distribu-tions themselves?
More specifically, how do trans-lation probabilities estimated using linear mixturesdiffer from global ?no mix?
estimates?
If linearmixtures do not only capture domain knowledgeas suggested by Section 5, do they simply performa form of smoothing?
If so, how does this im-plicit smoothing compare to more explicit smooth-ing schemes for translation probabilities?6.1 How do linear mixtures affect translationprobabilities?Let us compare translation probabilities estimateddirectly on the entire corpus Pnomix(t|s), with lin-ear mixtures pmix(t|s) =?Kk=1?kpk(t|s).
Thedifference between pmix(t|s) and pnomix(t|s) ishard to represent analytically in the general case,but studying a few particular cases can help us gaina better understanding.First, we observe that linear mixtures scaledown the contribution of component-specificsource phrases.
Assume that the phrase s oc-curs only once in the training corpus, with trans-lation t. By definition, there is a single mixturecomponent k such that pmix(t|s) = ?k, which islikely to be smaller than pnomix(t|s) = 1.
In theslightly more general case where s occurs morethan once, but always in the same component k,then pmix(t|s) = ?kpnomix(t|s), which has no im-pact on the ranking of translation candidates for s,but yields a smaller feature value for the decoder.Second, let us consider the case of very fre-quent ?general language?
phrases.
They shouldhave roughly the same translation distributions inall mixture components: If the pk(t|s) distribu-tions are the same in each component, the ?kval-ues learned do not matter, they have no impact onpmix(t|s) = pnomix(t|s).In between these extremes, the impact of linearmixtures depends on the frequency and ambiguityof translation candidates t across mixture compo-nents.
For instance, let us assume that the mixturecomponents are somehow defined such that theypartition the translate candidates t of a phrase sinto separate clusters.
In that case, for each t, there5054 8 16 32 64 128 512 2048 8192 327680.000.020.040.060.080.10frequency binJSdivnomix unsmoothed vs. nomixFigure 2: Comparing translation probability dis-tributions with and without Kneser Ney smooth-ing for Chinese phrase-tables: boxplots of Jensen-Shannon divergences binned by source phrase fre-quency.
For instance, the box and whisker at x = 8represent the distribution of the values of Jensen-Shannon divergence between the unsmoothed andsmoothed translation probability distribution forall Chinese phrases seen between 5 and 8 timesduring phrase extraction.is a k such that pk(t|s) = pnomix(t|s).
The rank-ing of translation candidates t for s according topmix(t|s) can be very different from pnomix(t|s),as controlled by the ?
values used.6.2 Smoothing EffectsAs a basis for comparison, let us analyze thedifference between unsmoothed relative frequen-cies and smoothed translation probabilities usinga conventional smoothing scheme.
We focus onthe Kneser-Ney smoothing scheme (Chen et al.,2011), since it is used to smooth translation prob-abilities in the ?nomix?
baseline as well as in allmixture components.For seen phrase pairs (with f(s, t) > 0), thedifference between Kneser-Ney estimates pkn(t|s)and relative frequency estimates prf(t|s) can bewritten as:prf(t|s)?
pkn(t|s) =Df(s)?D ?
n(s) ?
pb(t)f(s)(4)where D is a discount coefficient, f(s) is the rawfrequency for source phrase s, n(s) is the num-ber of translation candidates for s in the phrase-table, pb(t) is a back-off distribution proportionalto n(t).
The first term is a discount that increaseswhen s is rare, while the second term adds someprobability mass back, based on the frequency anddegree of ambiguity of the target phrase t. There-fore, Kneser-Ney smoothing has primarily a dis-count effect, applied on rare source phrases.
In ad-dition, for more frequent and ambiguous phrases,the relative frequency can be adjusted up or downdepending on how ambiguous s and t are.Overall, there are some similarities between theimpact of Kneser-Ney smoothing and linear mix-tures, since one can expect that the translationdistributions will diverge more from global rela-tive frequencies for rare phrases than for frequentphrases.
However, the discounting / down-scalingeffects are controlled by very different parametersin linear mixtures than in Kneser-New smoothing.In order to better understand these differences inpractice, an empirical analysis is required.6.3 Empirical ComparisonHow do linear mixtures and smoothing affecttranslation probabilities p(t|s) in practice?
Weuse the Jensen-Shannon divergence (Lin, 1991) toquantify the distance between (a) various mixturemodel estimates and (b) the global smoothed rela-tive frequency estimates used in our baseline ?nomix?
experiments.
In addition, we also comparethe Kneser-Ney smoothed translation probabilitieswith unsmoothed relative frequencies, in order tohighlight the difference between standard smooth-ing techniques and linear mixture models.Figures 2 and 3 show the distributions of di-vergence values by source phrase frequencies forChinese-English phrase-tables.
The divergencefrom the global estimate is the largest for rarephrases in all cases, as expected based on previousSections.
However, the Figures also highlight thedifferent behavior of linear mixtures compared toKneser-Ney smoothing.
The divergence values aremuch higher overall for the linear mixtures thanfor smoothing (note that the difference in rangeon the y axis in Figure 2 vs.
Figure 3).
In addi-tion, linear mixtures have a large impact on trans-lation probabilities not only on the rarest sourcephrases but also on relatively frequent phrases: inFigure 3, the median Jensen-Shannon divergenceremains high for source phrases extracted up to128 times from the training set3, while the medianvalue drops significantly as the frequency range in-3Recall that we use multiple word alignment methods, soextraction counts are summed across all alignment methods.5064 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivmix manual domains vs. nomix4 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivmix cluster domains vs. nomix4 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivmix random partitions vs. nomix4 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivavg manual domains vs. nomix4 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivavg cluster domains vs. nomix4 8 16 32 64 128 512 2048 8192 327680.00.10.20.30.40.5frequency binJSdivavg random partitions vs. nomixFigure 3: Comparing translation probability distributions of mixtures vs. ?nomix?
on Chinese webforumdata, including EM weights (top row) and uniform weights (bottom row).creases in Figure 2.
In addition, uniform mixtureshave an even higher impact on frequent phrasesthan mixtures based on EM weights.Furthermore, the nature of mixture componentsused has a visible impact on the divergence distri-butions in Figure 3: random partitions yield lowerdivergences for very frequent source phrases.Overall, the linear mixtures result in very differ-ent translation probability distributions than globalestimates, including smoothed estimates.
Thissuggests that standard smoothing techniques canbe improved when learning from heterogeneoustraining data, and that mixture components arebeneficial even when they do not explicitly cap-ture domain distinctions.7 Related workMost previous work on domain adaptation in ma-chine translation presupposes a clear-cut distinc-tion between in-domain and out-of-domain data(Koehn and Schroeder, 2007; Foster and Kuhn,2007; Duh et al., 2010; Bisazza et al., 2011; Had-dow and Koehn, 2012; Sennrich, 2012b; Haddowand Koehn, 2012; Clark et al., 2012, among manyothers).
We focused instead on a different less-studied question: how can we leverage trainingdata drawn from a wide variety of sources, genres,time periods, to translate a domain represented bya small development set?Many approaches focus on mapping the test do-main to a single subset of the training data.
In con-trast, we show that the test domain can be flex-ibly represented by a mixture of many compo-nents.
Yamamoto and Sumita (2007) cluster theparallel data using bilingual representations, andassign data to a single cluster at test time.
Wanget al.
(2012) show how to detect a known domainat test time in order to configure a generic transla-tion system with domain-specific feature weights.Others select a subset of training data that is rele-vant to the test domain, using e.g., IR techniques(Hildebrand et al., 2005) or language model cross-entropy (Axelrod et al., 2011).Closer to this work, Sennrich (2012a) proposesa sentence-level clustering approach to automati-cally recover domain distinctions in a heteoroge-neous corpus obtained by concatenating data froma small number of very distant domains.
The tar-507get domain was Alpine Club reports, while outof domain data sets comprised European parlia-ment proceedings and movie subtitles.
We addresstraining conditions where the dimensions for or-ganizing the training data are not as clear-cut, andshow that partitions that do not attempt to mimickdomain distinctions can improve translation qual-ity.
It would be interesting to see whether our con-clusion holds in these more artificial training set-tings, and whether sentence-level corpus organiza-tion could help translation quality in our settings.Finally, recent work shows that linear mixtureweights can be optimized for BLEU, either di-rectly (Haddow, 2013), or by simulating discrim-inative training (Foster et al., 2013).
In this pa-per, we limited our studies to maximum likelihoodand uniform mixtures, however, the various mix-ture component definitions proposed here can alsobe applied when maximizing BLEU.8 ConclusionWe have presented an extensive study of lin-ear mixtures for training translation models onvery heterogeneous data on Arabic-English andChinese-English translation tasks.
In addition, weevaluated the robustness of our models across twodistinct domains on the Arabic-English task.Our results show that linear mixtures reliablyand robustly improve the quality of machine trans-lation.
Improvements on the mixture-free base-line system range from 0.7 to 1.6 BLEU pointsdepending on the components and weights used.While linear mixture translation models were orig-inally proposed for domain adaptation tasks, weshowed that linear mixtures that have no domainknowledge can perform as well or better than tra-ditional mixtures meant to perform domain adap-tation.
This suggests that improvements with lin-ear mixture models do not only stem from givingmore weight to sections of the training data thatare relevant to the test domain, as is assumed ina standard domain adaptation task.
Improvementsalso come from averaging better generic estimatesfrom the heterogeneous training data.
In otherwords, in heterogeneous training settings, linearmixture models improve translation quality eventhough they do not perform domain adaptation.Finally, we show that while linear mixtures canbe viewed as a smoothing technique, linear mix-ture estimates do not diverge from global estimatesin the same way as Kneser-Ney smoothed transla-tion probabilities.
In particular, while smoothingprimarily has a large discounting effect for raresource phrases, linear mixtures yield differencesin translation probabilities for phrases with a widerrange of frequencies.These surprising results encourage us to rethinkthe use of mixture models, and opens up new waysof conceptualizing learning from heterogeneousdata beyond domain adaptation.
In future work,we will extend this study by varying the gran-ularity of basic elements used to define mixturecomponents, including sentences and phrases, andwill explore how they compare with more generalsmoothing techniques.AcknowledgmentsThis research was supported in part by DARPAcontract HR0011-12-C-0014 under subcontract toRaytheon BBN Technologies.
The authors wouldlike to thank the reviewers and the PORTAGEgroup at the National Research Council.ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 355?362.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus interpolation methods forphrase-based SMT adaptation.
International Work-shop on Spoken Language Translation (IWSLT).Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Boxing Chen, Roland Kuhn, George Foster, andHoward Johnson.
2011.
Unpacking and transform-ing feature functions: New ways to smooth phrasetables.
In Proceedings of Machine Translation Sum-mit.David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
Journalof Machine Learning Research, 13(1):1159?1187,April.Jonathan H. Clark, Alon Lavie, and Chris Dyer.
2012.One system, many domains: Open-domain statisti-cal machine translation via feature augmentation.
InProceedings of the Conference of the Association forMachine Translation in the Americas.Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.2010.
Analysis of translation model adaptation instatistical machine translation.508George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, pages 128?135.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing.George Foster, Boxing Chen, and Roland Kuhn.
2013.Simulating discriminative training for linear mixtureadaptation in statistical machine translation.
In Pro-ceedings of the XIV Machine Translation Summit,pages 183?190.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08, pages 848?856.Barry Haddow and Philipp Koehn.
2012.
Analysingthe effect of out-of-domain data on SMT systems.
InProceedings of the Seventh Workshop on StatisticalMachine Translation, pages 422?432.Barry Haddow.
2013.
Applying pairwise ranked opti-misation to improve the interpolation of translationmodels.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 342?347.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
2005.
Adaptation of the transla-tion model for statistical machine translation basedon information retrieval.
In European Associationfor Machine Translation.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin Domain Adaptation for Statistical Machine Trans-lation.
In Workshop on Statistical Machine Transla-tion.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Annual Meeting of the Association for Computa-tional Linguistics (ACL), demonstration session.Jianhua Lin.
1991.
Divergence measures basedon the shannon entropy.
IEEE Trans.
Inf.
Theor.,37(1):145?151, September.Rico Sennrich.
2012a.
Mixture-modeling with un-supervised clusters for domain adaptation in statis-tical machine translation.
In 16th Conference ofthe European Association for Machine Translation(EAMT).Rico Sennrich.
2012b.
Perplexity minimization fortranslation model adaptation in statistical machinetra.
In Thirteenth Conference of the European Chap-ter of the Association for Computational Linguistics(EACL).Wei Wang, Klaus Macherey, Wolfgang Macherey,Franz Och, and Peng Xu.
2012.
Improved do-main adaptation for statistical machine translation.In 10th biennial conference of the Association forMachine Translation in the Americas (AMTA).Hirofumi Yamamoto and Eiichiro Sumita.
2007.
Bilin-gual cluster based models for statistical machinetranslation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 514?523.509
