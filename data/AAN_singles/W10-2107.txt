Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 43?51,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsYou talking to me?
A predictive model for zero auxiliary constructionsAndrew CainesComputation, Cognition & Language GroupRCEAL, University of Cambridge, UKapc38@cam.ac.ukPaula ButteryComputation, Cognition & Language GroupRCEAL, University of Cambridge, UKpjb48@cam.ac.ukAbstractAs a consequence of the established prac-tice to prefer training data obtained fromwritten sources, NLP tools encounterproblems in handling data from the spo-ken domain.
However, accurate modelsof spoken data are increasingly in demandfor naturalistic speech generation and ma-chine translations in speech-like contexts(such as chat windows and SMS).
Thereis a widely held assumption in the lin-guistic field that spoken language is animpoverished form of written language.However, we show that spoken data isnot unpredictably irregular and that lan-guage models can benefit from detailedconsideration of spoken language features.This paper considers one specific con-struction which is largely restricted to thespoken domain - the ZERO AUXILIARY -and makes a predictive model of that con-struction for native speakers of British En-glish.
The model can predict zero auxil-iary occurrence in the BNC with 96.9%accuracy.
We will demonstrate how thismodel can be integrated into existing pars-ing tools, increasing the number of suc-cessful parses for this zero auxiliary con-struction by around 30%, and thus improv-ing the performance of NLP applicationswhich rely on parsing.1 IntroductionUp to this point, statistical Natural Language Pro-cessing (NLP) tools have generally been trainedon corpora that are representative of written ratherthan spoken language.
A major factor behind thisdecision to use written data is that it is far easier tocollect than spoken data.
Newswire, for instance,may be harvested readily and in abundance.
Oncecollected, written language requires relatively lit-tle processing before it can be used for training astatistical model.Processing of spoken data, on the other hand,involves at the very least transcription - which usu-ally requires a human transcriber.
Since transcrip-tion is a slow and laborious task, the collection ofspoken data is highly resource intensive.
But thisrelative difficulty in collection is not the only rea-son that spoken language data has been sidelined.Had spoken data been considered to be crucial tothe production of NLP applications greater effortsmight have been made to obtain it.
However, onaccount of some of its characteristic features suchas hesitations, interruptions and ellipsis, spokenlanguage is often dismissed as nothing more thana noisy approximation to ?real?
or ?intended?
lan-guage.In some forums, written language is held upas an idealised form of language toward whichspeakers aspire and onto which spoken lan-guage should be retrofitted.
This is an arte-fact of the theoretical notion of a ?competence?-?performance?
dichotomy (Chomsky 1965) withthe latter deemed irrelevant and ignored in main-stream linguistic research.The consequence of the established practice tosideline spoken data is that NLP tools are inher-ently error prone when handling data from the spo-ken domain.
With increasing calls for speech to beconsidered the primary form of language and to betreated as such (Sampson 2001: 7 1; Cerma?k 2009:115 2; Haugh 2009: 74 3) and a growing trend forNLP techniques to be integrated into cognitive andneurolinguistic research as well as forensic appli-1Speech is ?unquestionably the more natural, basic modeof language behaviour?.2?From a linguistic point of view, spoken corpora shouldbe primary for research but that has not been the case so far?.3Haugh observes that ?spoken language and interactionlie at the core of human experience?
but bemoans the ?relativeneglect of spoken language in corpora to date?.43cations, there are now compelling reasons to ex-amine spoken data more closely.
Accurate mod-els of spoken data are increasingly in demand fornaturalistic speech generation and machine trans-lations in speech-like contexts (such as human-machine dialogue, chat windows and SMS).The main research aim of our work is to showthat spoken data should not be considered errorprone and therefore unpredictably irregular.
Weshow that language models can be improved in in-crements as we deepen our understanding of spo-ken language features.
We investigate ZERO AUX-ILIARY progressive aspect constructions - thosewhich do not feature the supposedly obligatoryauxiliary verb, as in (1a) below (cf.
1b):(1a) What you doing?
Who you lookingfor?
You been working?
(1b) What are you doing?
Who are youlooking for?
Have you been working?The zero auxiliary is a non-standard featurewhich for the most part is known to be restrictedto speech.
A corpus study of spoken British En-glish indicates that in progressive aspect interroga-tives with second person subjects (as in (1) above)the auxiliary occurs in zero form in 27% of con-structions found.
The equivalent figure from thewritten section of the corpus is just 5.4%.
Con-sequently, existing NLP techniques - since theyare based on written training data - are unlikelyto deal appropriately with zero auxiliary construc-tions.
We report below on the corpus study in fulland use the results of logistic regression to designa predictive model of zero auxiliary occurrencein spoken English.
The model is based on con-textual grammatical features and can predict zeroauxiliary occurrence in the British National Cor-pus (BNC; 2007) with 96.9% accuracy.
Finally,we discuss how this model can be used to improvethe performance of NLP techniques in the spokendomain, demonstrating its implementation in theRASP system (Robust Accurate Statistical Pars-ing; (Briscoe, Carroll and Watson, 2006)).This paper underlines why awareness of non-standard linguistic features matters.
Targeted dataextraction from large corpus resources allows theconstruction of more informed language modelswhich have been trained on naturalistic spokenusage rather than standard and restricted rules ofwritten language.
Such work has only been madepossible with the advent of large spoken languagecorpora such as the BNC.
Even so, the resource-heavy nature of spoken data collection means thatspeech transcriptions constitute only one tenth ofthis 100 million word corpus 4.
Nevertheless, itis an invaluable resource made up of a range ofspeech genres including spontaneous face-to-faceconversation, a fact which makes it unique amongcorpora.
Since conversational dialogue is the pre-dominant language medium, the BNC offers thebest chance of modelling speech as it occurs natu-rally.This work has important implications for bothcomputational and theoretical linguistics.
On theone hand, we can improve various NLP techniqueswith more informed language models, and on theother hand we are reminded that the space ofgrammatical possibility is not restricted and thatcontinued empirical investigation is key in orderto arrive at the fullest possible description of lan-guage use.2 Spoken and written languageIn the modern mainstream fields of linguisticresearch, based on Chomsky?s ?ideal speaker-listener?
(1965), spoken language has been alltoo easily dismissed from consideration on thegrounds that it is more error-prone and less impor-tant than written language.
In this idealisation, thespeaker-listener is ?unaffected by such grammati-cally irrelevant conditions as memory limitations,distractions, shifts of attention and interest, and er-rors (random or characteristic)?
(Chomsky, 1965).The ?errors?
Chomsky refers to are features ofspeech production such as pauses, filled silence,hesitation, repetition and elision, or of dialoguesuch as backgrounding, overlap and truncation be-tween speakers.
Thus ?error?
is essentially here de-fined as that which is not normally found in well-formed written data, that which is ?noisy?
and ?un-predictable?.
It is on these grounds - the gram-matical rigidity of the written medium relative tospeech - that the divide between spoken and writ-ten language modelling has grown up.The opposing, usage-based view is that spokenlanguage is systematic and that it should be mod-elled as it is rather than as a crude approximationof the written form.
On this view, the speech pro-duction and dialogue features listed above are not4Cerma?k estimates our experience with each languagemedium is in fact this ratio in reverse - 90:10 spoken to writ-ten (2009: 115).44considered mistakes but ?regular products of thesystem of spoken English?
(Halliday, 1994).
?Er-ror?
is thus seen as a misnomer for these featuresbecause they are in fact all regular in some way.For example, the fact that people tend to put fillerpauses in specific places.We propose a middle way: that which buildson the NLP tools available, even though they aretrained on written data, and on top of these modelsthe features of spoken language as ?noise?
in thecommunicative channel.
This is a pragmatic ap-proach which recognises the considerable amountof work underpinning existing NLP tools and seesno value in discarding these and starting againfrom scratch.
We demonstrate that spoken lan-guage is model-able and predictable, even witha feature which would not be seen as ?correct?in written form.
For practical purposes we needto recognise the regularities in the apparently ?in-correct?
features of speech and build these intothe functioning language models we already havethrough statistical analysis of corpus distributionsand appropriate adjustment to parser tools.3 The zero auxiliary constructionAccording to standard grammatical rules, the aux-iliary verb is an obligatory feature of progressiveaspect constructions.
However, this rule is basedon norms of written language and is in fact not al-ways adhered to in the production of speech.
As aresult, some progressive constructions do not fea-ture an auxiliary verb.
These are termed ?zero aux-iliary?
constructions and have been previously ex-amined in studies of dialect (Labov, 1969; Ander-sen, 1995) and first language acquisition (Brown,1973; Rizzi, 1993/1994; Wexler, 1994; Lieven etal, 2003; Wilson, 2003; Theakston et al 2005).There are copious anecdotal examples of thezero auxiliary:(2) You talking to me?
Travis Bickle inTaxi Driver (1976).
(3) Where he going?
Avon Barksdalein The Wire, Season 1: ?Game Day?(2002).
(4) What you doing?
Holly Golightly inBreakfast at Tiffany?s (1961).Natural language data taken from the spokensection of the British National Corpus (sBNC)shows that the zero auxiliary features in 1330(27%) of the 4923 second person progressive in-terrogative constructions; as in (1), (2), (4) above.In first person singular declaratives (cf.
(5a) and(5b)), in contrast, the proportion of zero auxiliaryoccurrence is just 0.9% (158 of 17,838 construc-tions).
This already indicates the way that the zeroauxiliary occurs in predictable contexts and howgrammatical properties will feature in the predic-tive model.
(5a) What I saying?
I annoying you?Why I doing this?
(5b) What am I saying?
Am I annoyingyou?
Why am I doing this?Subject person, subject number, subject type(pronoun or other noun) and clause type (declar-ative or interrogative) are four of the eight syntac-tic properties incorporated in the predictive model.The four other properties are clause tense (6), per-fect or non-perfect aspect (7), clause polarity (8)and presence or absence of subject (9).
(6) You are debugging.
You were de-bugging.
(7) We have been looking for a present.We are looking for a present.
(8) She is watching the grand prix.
Heis not watching the grand prix.
(9) I am going to town in a minute.
Go-ing to town in a minute.We employ logistic regression to investigate theprecise nature of the relationships between zeroauxiliary use and these various linguistic vari-ables.
This allows us to build a predictive modelof zero auxiliary occurrence in spoken languagewhich will be useful for several reasons relatingto parsing of natural spoken language.
Firstly,for automatic parsing of spoken data being ableto predict when a zero auxiliary is likely to oc-cur enables the parser to relax its normal ruleswhich are based on written standards.
Secondly,as technology improves and interaction with com-puters becomes more humanistic the need to repli-cate human-like communication increases in im-portance: by knowing in which contexts the aux-iliary verb might be absent, researchers can builda language model which is more realistic and sothe user experience is improved and made morenaturalistic.
Thirdly, a missing auxiliary might beproblematic for machine translation since it could45result in the loss of tense and aspect information,but with the ability to predict where a zero auxil-iary might occur, the auxiliary can be restored sothat translation can be performed with appropriatetense and aspect.For all these reasons, the zero auxiliary in spo-ken English is an appropriate case study for find-ing the common ground between NLP and Lin-guistics.
Awareness of this particular linguis-tic phenomenon through corpus study allows theconstruction of more informed language modelswhich in turn enhance relevant NLP techniques.The cross-pollination of research from NLP andlinguistics benefits both fields and ties in with theemergence of linguistic theories that ?conceive ofstructure as gradient, malleable and probabilistic?and incorporate ?knowledge of the frequency andprobability of use of these categories in speakers?experience?
(Tily et al 2009).
These are collec-tively known as ?usage-based?
approaches to lan-guage theory and are exerting a growing influ-ence on the field (e.g.
Barlow and Kemmer 2000;Bybee and Hopper 2001; Bod, Hay and Jannedy2003).4 Corpus studyTraining data was obtained through manual anno-tation of progressive constructions in the BritishNational Corpus (2007).
A preliminary study ofinterrogatives with second person subjects con-firmed that the zero auxiliary is more a featureof the spoken rather than the written domain (Ta-ble 1).
Therefore a focus on the spoken section ofthe corpus (sBNC) was justified and so we under-took a comprehensive study of all progressive con-structions in sBNC.
The genres contained in sBNCinclude a range of settings and levels of formality- from academic lectures to radio programmes tospontaneous, face-to-face conversation.We extracted 93,253 sentences featuring a pro-gressive construction from sBNC and each wasmanually annotated for auxiliary realisation andthe eight syntactic properties described in Ta-ble 2.
In Table 3 the progressive constructions areclassified by auxiliary realisation.
With approxi-mately 4.2% occurrence in progressive construc-tions, zero auxiliaries are a low frequency featureof spoken language but ones which are significantfor the fact that existing NLP tools cannot suc-cessfully parse them, thus one in twenty-five pro-gressive constructions will not be fully parsed.
WeCorpus AuxiliaryFull Contracted ZerowBNC 3220 27 187sBNC 3498 95 1330Table 1: Auxiliary realisation in second personprogressive interrogatives in the BNC.use the annotated corpus of these progressive con-structions to design the predictive model describedbelow.Properties Value encodingsAux realisationZero auxiliary full(0), contracted(1), zero(2)VariablesSubject person 1st(1), 2nd(2), 3rd(3)Subject number singular(0), plural(1)Subject type other noun(0), pronoun(1)Subj supplied zero subj(0), subj supplied(1)Clause type declarative(0), interrogative(1)Clause tense present(0), past(1)Perfect aspect non-perfect(0), perfect(1)Polarity positive(0), negative(1)Table 2: Syntactic features and their encodings inthe annotated sBNC Progressive Corpus5 ModelTo predict the zero auxiliary in spoken languagewe use logistic regression.
To train this modelwe took a 90% sample from our corpus of 93,253progressive constructions extracted from the spo-ken section of the BNC, as described above and inCaines 2010.
The dataset was split into two cat-egories: those sentences which exhibited the zeroauxiliary and those which did not5.
A logistic re-gression was then performed to ascertain the prob-ability of category membership using the eightpreviously described syntactic properties.
Notethat subject person is arguably not a scalar vari-5Contracted auxiliaries thus belong in the ?not zero auxil-iary?
category.Corpus Full Contracted ZerosBNC 38,015 51,295 3943Table 3: Auxiliary realisation in progressive con-structions in sBNC.46able and therefore is re-analysed as three booleanvariables with separate binary values for use of thefirst, second and third person.
However, the threesubject person variables are dependent (ie.
If thesubject is not first or second person it will be in thethird).
Thus the eight syntactic properties becomenine explanatory variables in the predictive model,as reported in Table 4.Corpus Predictor Coefficientsubject person: 1st 0.171subject person: 2nd 1.280plural subject -0.300pronoun subject -0.470zero subject 5.711interrogative clause 2.139past tense clause -4.852perfect aspect -0.280negated clause -1.163constant -4.033Table 4: Predictor coefficients for the presence ofa zero auxiliary construction.5.1 Model EvaluationThe logistic function is defined by:f(Z) =11 + e?z(1)The variable z is representative of the set of pre-dictors and is defined by:z = ?0 + ?1x1 + ?2x2 + .
.
.+ ?kxk (2)where ?0, ?1, ?2 ... ?k are the regression coeffi-cients of predictors x1, x2 .. xk respectively.
Thepredictors explored in this paper are encodings ofthe syntactic properties of the annotated sentences.The predictors and their encodings are indicated inTable 2.The logistic function is constrained to valuesbetween 0 and 1 and represents the probabilityof membership of one of the two categories (zeroauxiliary or auxiliary supplied).
In our case anf(z) > 0.5 indicates that there is likely to be azero auxiliary.The logistic function defined by the coefficientsin Table 4 is able to predict correct category mem-bership for 96.9% of the sentences in the annotatedcorpus.
All coefficients are highly significant tothe logistic function (p<0.001) with the exceptionof perfect aspect and first person subject - whichare both significant nevertheless (p<0.05).For this model, positive coefficients indicatethat the associated syntactic properties raise theprobability of a zero auxiliary occurring.
Largecoefficients more strongly influence the probabil-ity of the zero auxiliary whereas near-zero coeffi-cients have little influence.
From the coefficientsin Table 4 we see that the strongest predictor ofa zero auxiliary is the occurrence of a zero sub-ject (as in the utterance, ?leaving now.?).
An inter-rogative utterance is also a good candidate, as isthe second person subject (e.g.
?you eating thoseolives??).
However, a past tense utterance is an un-likely candidate for a zero auxiliary construction,as is a negated utterance.6 Discussion ?
using the predictivemodel to aid parsingAs mentioned above, since parsers are trained onwritten data they can often display poor perfor-mance on text transcribed from the spoken do-main.
From the results of our corpus study weknow that the zero auxiliary occurs in approxi-mately 4.2% of progressive constructions in spo-ken language and we can extrapolate that it willoccur in less than 1% (approximately 0.8%) ofall progressive constructions in written language.A statistical parser trained on written languagewill therefore be prone to undergo parsing fail-ure for every one in twenty-five progressive sen-tences.
This is no insignificant problem, espe-cially when it is remarked that the progressive isin high frequency usage (there are one thousandING-forms featuring in progressive constructionsfor every one million words of sBNC) and that itsuse is known to be spreading (Leech et al 2009).Compounded with those parser breakdownscaused by other speech phenomena (for instance,repetition and elision), high numbers of parse fail-ures on progressive constructions will render NLPaccuracy on spoken language intolerable for anyapplications which rely on accurate parsing as afoundation.
However, we have shown above thatfeatures of spoken language such as the zero aux-iliary should not be thought of as errors or as un-predictable deviations from the written form, butrather can be considered to be consistent and pre-dictable events.
In this section we illustrate howour predictive model for zero auxiliary occurrence47(|relation| |head| |dependant|) (3)(|ncsubj| |play + ing : V V G| |you : PPY |)(4)(|obj| |play + ing : V V G| |what : DDQ|) (5)(|aux| |play + ing : V V G| |be+ : V BR|) (6)(|arg| |play + ing : V V G| |you : PPY |) (7)(|relation| |verb : V V G| |dependant|) (8)Figure 1: Example grammatical relations fromRASP.may be integrated into a parser pipeline in orderto aid the parsing of spoken language.
In this waywe build on the increasingly robust engineering ofstatistical NLP tools trained on written languageby allowing them to adapt to the spoken domainon the basis of the linguistic study of speech phe-nomena.In general the notion of ?parsing?
an utteranceinvolves a chain of several processes: utteranceboundary detection, tokenization, part-of-speechtagging, and then parsing.
We suggest that when itis known that the language to be parsed is from thespoken domain the pipeline of processes should berun in a SPEECH AWARE MODE.
Extra functional-ity would be incorporated into each of the stagesaccording to the findings of linguistic research intospoken language.
In other work we have adaptedthe tokenization and tagging stages of the pipelinebased on predictors that indicate when interjec-tions (e.g.
?umm?, ?err?
and ?ah?)
have been ?used?as punctuation or lexical items.
We also incorpo-rate intonation phrases as predictors for utteranceboundary detection (Buttery and Caines: in prepa-ration).
Here, we augment the parsing stage of thepipeline by allowing an informed re-parse of ut-terances in which a parse failure is likely to havebeen caused by a zero auxiliary.We present this section with reference to thespecific mechanics and output formats of theRASP system but our algorithm is by no meansparser specific and could be adapted for otherparsers quite easily.
Utterances parsed withRASP may be expressed as ?grammatical rela-tions?.
RASP?s grammatical relations are theory-general, binary relations between lexical terms andare expressed in the form of head-dependancy re-lations as shown in (3), Figure 1.Consider the utterance ?what are you playing?
?.When we parse this with RASP we get grammati-cal relations (4), (5) and (6) in Figure 1.
The capi-tal letter codes following the ?:?
symbols are part-of-speech tags (from the CLAWS-2 tagset (Gar-side, 1987)) which have been assigned to the lexi-cal tokens by the tagger of the RASP system.
HerePPY indicates the pronoun ?you?
; VVG indicatesthe ING-form of lexical verb; VBR indicates ?be?in 3rd person present tense; and DDQ indicatesa wh-determiner.
The relation (4) tells us that?you?
is the subject of ?playing?
; relation (5) tellsus that ?what?
is taking the place of the object be-ing played; and relation (6) tells us that there is anauxiliary relationship between ?are?
and ?playing?.This is much as we would expect.
However, if wetry to parse ?what you playing??
the parse fails.The single relation (4) is returned where ideallywe would like both (4) and (5), as we did whenthe auxiliary was present.For the utterance, ?you playing??
RASP returnsthe under-specified grammatical relation (7) whichis simply indicating that ?you?
is an argument of?playing?
but not which type of argument (whethera subject, direct object, etc).
Ideally we wouldlike to retrieve at least (4) as we would have if weparsed the utterance ?are you playing??.
For theseexamples, we shall consider the failure to identifythe correct subject and object of the progressiveverb to be a parsing failure.We integrate the zero auxiliary predictive modelwith parsing technology to improve the parsing ofzero auxiliaries in spoken language.
Note that weuse the RASP system but our algorithm is by nomeans parser specific.
The only prerequisite isthat the parser must be able to identify relationsof some kind between the subject noun and ING-form (possibly via a parsing rule) and also be ableextract values for the predictors (through either arich tagset or from the identification of key speechtokens).
The illustrative method we discuss hereis integrated into the parsing pipeline in the eventof a parse failure but there are several alternativemethods that might also be considered.For instance, by using the predictive modelearlier in the parsing system pipeline a modifiedtagset could be used which updates the ING-formtag with a new tag to indicate that there is also amissing auxiliary.
Another method might involvealtering rule probabilities or adding extra parserrules so that parsing only has to occur once.
Ourother work in this area suggests that the final deci-48sion on where to add the spoken language modifi-cations within the parsing pipeline will largely de-pend on the interaction of the phenomena in ques-tion with other speech phenomena.6With the proviso that it is a preliminary integra-tion of the predictive model into a parsing system,we propose the following algorithm for zero aux-iliaries in spoken language.
When ?speech awaremode?
is activated, if we encounter a parse failurethen we first check the part-of-speech tags of theutterance to ascertain if the sentence contains theING-form requisite for a progressive construction:?
IF no ING-form is found: STOP.
Ourmodel predicts zero auxiliaries in progressiveconstructions?there is nothing more we cando with the input.?
ELSE: An ING-form is found.
Extract allgrammatical relations that were obtained bythe parse which contained the ING-form inthe head position (these would be grammat-ical relations that have the general formatof (8) in Figure 1).
We will refer to this set ofgrammatical relations as GRS.?
IF there is an auxiliary relationpresent in GRS: STOP.
If at least oneof the extracted grammatical relations isan auxiliary relation, similar to (6) inFigure 1, an auxiliary is present?we donot have a zero auxiliary construction.7?
ELSE: The utterance is a candidate forzero auxiliary.Having determined a possible candidate for zeroauxiliary we carry out the following steps:1.
Ascertain values for the zero auxiliary pre-dictors (explained in more detail below).2.
Calculate the value of the logistic functionf(z) using the obtained predictor values withtheir coefficients (shown in Table 4).3.
If f(z) > 0.5, assume an auxiliary is miss-ing.6Although, another major consideration is the overallcomputational efficiency of the parsing system.7This step is actually subtly more complicated?auxiliaryrelations involving ?been?
are allowed to be present in GRS(this allows us to capture zero auxiliaries in the perfect suchas ?been coming here long??)
but if there is any other auxil-iary relation present in GRS then we STOP here.4.
Add the auxiliary to the sentence (choos-ing which auxiliary based on the predictorvalues?see below).5.
Re-parse the sentence.6.
Remove (or flag) the auxiliary grammaticalrelation from the newly obtained parser out-put.8For step 1 above properties of the current utter-ance have to be obtained.
The subject person, plu-ral subject, zero subject and pronoun subject prop-erties are ascertained by looking at the part-of-speech of the dependant noun/pronoun within anysubject relations occurring in the set GRS (gram-matical relations headed by the ING-form).
Sub-ject relations would look similar to (4) in Figure 1.If there is no subject grammatical relation, any un-derspecified ?arg?
relation (such as (7) in Figure 1)are considered.
If neither of these relations arepresent in GRS then a zero subject is inferred.The person and plurality of the subject noun is en-coded within its CLAWS2 part-of-speech tag.
Forinstance, a PPHS1 tag, which is used to indicate?him?
or ?her?
would tell us we have a third per-son, singular pronoun.9The other properties are all ascertained by thepresence or absence of a token within the utter-ance: interrogative property is inferred when theutterance ends with a question mark; the nega-tion property when either ?not?
or ?n?t?
(which aretagged XX) is present; the perfect is inferred fromthe presence of the word ?been?
; and past tense isascertained from a set of temporal marker lexicalitems (e.g.
?yesterday, ?before?).
Once extractedthe properties are encoded as shown in Table 4 foruse as the predictor values in the logistic function.In order to select the correct auxiliary and loca-tion for insertion in step 4 the utterance values areconsulted.
For instance, an interrogative utterancein the present tense, not in perfect aspect, with asecond person singular subject will require inser-tion of the auxiliary ?are?
after the subject.
A zerosubject zero auxiliary, on the other hand, requiresrestoration of both subject and auxiliary.
Where aquestion mark indicates it has been used in an in-terrogative clause the subject is assumed to be sec-8We also remove (or flag) the subject relation in caseswhere a subject also had to be added in step 4.
This wouldoccur when the original utterance exhibited a zero subject.9All common nouns are assumed to be 3rd person and allinstances of ?you?
were considered to be singular (as was thecase during corpus annotation).49ond person - as is the case in most questions - andso the auxiliary-subject combination ?are you?
isrestored before the ING-form.
Without a questionmark, the clause is assumed to be declarative andso the first person singular subject-auxiliary com-bination ?I am?
is restored before the ING-form 10.We withheld 10% of the zero auxiliary corpusfor test purposes.
The integration of the predic-tive model into the parser allowed us to success-fully parse 31.4% of previously unparsable zero-auxilaries.
On cleaned spoken transcripts (i.e.with speech phenomena other than the zero aux-iliary, such as repetitions, removed) this algorithmallows us to retrieve the correct subject-object re-lations for an extra 1238 utterances within ourannotated corpus (which again accounts for ap-proximately one third of the previously unparsablezero-auxilaries).
This is a significant step forwardfor any applications building on top of a parsinginfrastructure.7 ConclusionWe have shown how awareness of a specificlinguistic phenomenon enables improvements inNLP techniques.
The zero auxiliary is mainly afeature of spoken language and so is not on thewhole handled successfully by existing parsers,trained as they are on written data.
As a so-lution, rather than proposing the construction ofnew models specifically designed for spoken lan-guage, thereby doing away with all previous workon NLP tools and starting again from scratch, wedemonstrated how new training data from a spo-ken source could be applied to an existing parser- RASP.
We designed a predictive model of zeroauxiliary occurrence based on logistic regressionwith nine syntactic variables.
The data came froman annotated corpus of 93,253 progressive con-structions which showed zero auxiliary frequencyto be 4.2%.
Without this new predictive informa-tion in the parser, the status quo would continuewhereby one in twenty-five progressive construc-tions would continue to be mis-parsed.
We foundthat instead the noise was regular and could bemodelled, and we illustrated how this specific lin-guistic data could be integrated into existing NLPtechnology.
This is a case study of one specificlinguistic phenomenon.
Our belief is that other10A sample of one hundred zero subject declarative zeroauxiliaries indicates that the first person singular is the ap-propriate subject type to restore on 60% of occasions.such spoken language phenomena can be mod-elled in the same way, given an appropriate corpusresource, accurate annotation and implementationinto a parser.By running in a ?speech aware mode?
whichsupplements existing parsing architecture we ben-efit from the training that has already been under-taken on a large scale based on written data andcomplement it with specialized and predictablelinguistic properties of speech.
Ideally, we wouldlike to train an entire parsing system on spokenlanguage but until spoken corpora become morereadily available this is not a practical option: theresulting parser would suffer greatly from datasparsity issues.
Frustratingly, there is a circularproblem in generating corpora of an appropriatesize for training since until highly accurate mod-els for spoken language are built we can not expectspeech-to-text systems to provide highly accuratetranscripts.
But to build these highly accuratemodels of spoken language in the first place a largeamount of data is required.
Augmenting the ex-isting statistical NLP tools trained on written lan-guage with specialized linguistic knowledge fromthe spoken domain is a pragmatic short-term fixfor this problem.We should note that tailoring parsers to dealwith spoken language is by no means unheard of:the RASP system itself, for example (which parsesusing a probabilistic context-free grammar), al-ready has several rules in its grammar which aremore appropriate for parsing spoken language.However, use of these rules can contribute to muchover-generation and complexity in the parse for-est (the parser internal structure which holds allthe possible parses for an utterance).
In conse-quence, the specialized rules have to be expertlyselected or deselected when configuring the parser.This work - and our research program as a whole- would instead allow parser configuration deci-sions and algorithmic adaptions to be made non-expertly and on-the-fly when running in ?speechaware mode?.
All rule activations and algorithmadaptions would be made based on predictionsconstructed from expert linguistic analysis of thespoken domain.AcknowledgementsThis work was supported by the AHRC.
We thankthree anonymous reviewers for their comments,Andrew Rice and Barbara Jones.50ReferencesGisle Andersen.
1995.
Omission of the primary verbsBE and HAVE in London teenage speech - a so-ciolinguistic study.
Hovedfag thesis, University ofBergen, Norway.Michael Barlow and Suzanne Kemmer.
2000.
Usage-based models of language.
Chicago: CSLI.Rens Bod, Jennifer Hay and Stefanie Jannedy (eds.).2003.
Probabilistic Linguistics.
Cambridge, MA:MIT Press.Ted Briscoe, John Carroll and Rebecca Watson.
2007.The second release of the RASP system.
Proceed-ings of the COLING/ACL on Interactive presenta-tion sessions, July 17-18, 2006, Sydney, Australia.The British National Corpus, version 3.
2007.Distributed by Oxford University Computing Ser-vices on behalf of the BNC Consortium.
URL:http://www.natcorp.ox.ac.uk/Roger Brown.
1973.
A First Language: the earlystages.
London: George Allen and Unwin.Paula Buttery and Andrew Caines.
In preparation.
AnEmpirical Approach to First Language Acquisition.Cambridge: Cambridge University Press.Joan Bybee and Paul Hopper (eds.).
2001.
Frequencyand the emergence of linguistic structure.
Amster-dam: John Benjamins.Andrew Caines.
2010.
You talking to me?
Zero aux-iliary constructions in British English.
Ph.D thesis,University of Cambridge.Frantisek Cerma?k.
2009.
Spoken corpora design.Their constitutive parameters.
International Journalof Corpus Linguistics 14: 113-123.Noam Chomsky.
1965.
Aspects of the Theory of Syn-tax.
Cambridge, MA: MIT Press.Roger Garside.
1987.
The CLAWS Word-tagging Sys-tem.
In: Roger Garside, Geoffrey Leech and Ge-offrey Sampson (eds.
), The Computational Analy-sis of English: A Corpus-based Approach.
London:Longman.Michael A. K. Halliday.
1994.
Spoken and Writ-ten Modes of Meaning.
In: David Graddol andOliver Boyd-Barrett (eds.
), Media Texts: Authorsand Readers.
Clevedon: Multilingual Matters.Michael Haugh.
2009.
Designing a MultimodalSpoken Component of the Australian National Cor-pus.
In: Michael Haugh, Kate Burridge, Jean Mul-der, and Pam Peters (eds.
), Selected Proceedings ofthe 2008 HCSNet Workshop on Designing the Aus-tralian National Corpus.
Somerville, MA: Cas-cadilla Proceedings Project.William Labov.
1969.
Contraction, deletion, and in-herent variability of the English copula.
Language45: 715-762.Geoffrey Leech, Marianne Hundt, Christian Mair andNicholas Smith.
2009.
Change in ContemporaryEnglish: a grammatical study.
Cambridge: Cam-bridge University Press.Elena Lieven, Heike Behrens, Jennifer Speares andMichael Tomasello.
2003.
Early syntactic creativ-ity: a usage-based approach.
Journal of Child Lan-guage 30: 333-370.Luigi Rizzi.
1993/1994.
Some notes on linguistic the-ory and language development: The case of root in-finitives.
Language Acquisition 3: 371-393.Geoffrey Sampson.
2001.
Empirical Linguistics.
Lon-don: Continuum.Anna Theakston, Elena Lieven, Julian Pine and Caro-line Rowland.
2005.
The acquisition of auxiliarysyntax: BE and HAVE.
Cognitive Linguistics 16:247-277.Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,Anubha Kothari and Joan Bresnan.
2009.
Syntacticprobabilities affect pronunciation variation in spon-taneous speech.
Language and Cognition 1: 147-165.Kenneth Wexler.
1994.
Optional Infinitives, headmovement and the economy of derivations.
In:David Lightfoot and Norbert Hornstein (eds.
), VerbMovement.
Cambridge: Cambridge UniversityPress.Stephen Wilson.
2003.
Lexically specific construc-tions in the acquisition of inflection in English.Journal of Child Language 30: 75-115.51
