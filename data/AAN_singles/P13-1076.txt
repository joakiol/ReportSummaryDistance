Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770?779,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGraph-based Semi-Supervised Model for Joint Chinese WordSegmentation and Part-of-Speech TaggingXiaodong Zeng?
Derek F. Wong?
Lidia S. Chao?
Isabel Trancoso?
?Department of Computer and Information Science, University of Macau?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugalnlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,isabel.trancoso@inesc-id.ptAbstractThis paper introduces a graph-based semi-supervised joint model of Chinese wordsegmentation and part-of-speech tagging.The proposed approach is based on agraph-based label propagation technique.One constructs a nearest-neighbor simi-larity graph over all trigrams of labeledand unlabeled data for propagating syn-tactic information, i.e., label distribution-s.
The derived label distributions are re-garded as virtual evidences to regular-ize the learning of linear conditional ran-dom fields (CRFs) on unlabeled data.
Aninductive character-based joint model isobtained eventually.
Empirical results onChinese tree bank (CTB-7) and MicrosoftResearch corpora (MSR) reveal that theproposed model can yield better result-s than the supervised baselines and othercompetitive semi-supervised CRFs in thistask.1 IntroductionWord segmentation and part-of-speech (POS) tag-ging are two critical and necessary initial proce-dures with respect to the majority of high-levelChinese language processing tasks such as syn-tax parsing, information extraction and machinetranslation.
The traditional way of segmentationand tagging is performed in a pipeline approach,first segmenting a sentence into words, and thenassigning each word a POS tag.
The pipeline ap-proach is very simple to implement, but frequentlycauses error propagation, given that wrong seg-mentations in the earlier stage harm the subse-quent POS tagging (Ng and Low, 2004).
The join-t approaches of word segmentation and POS tag-ging (joint S&T) are proposed to resolve these t-wo tasks simultaneously.
They effectively allevi-ate the error propagation, because segmentationand tagging have strong interaction, given thatmost segmentation ambiguities cannot be resolvedwithout considering the surrounding grammaticalconstructions encoded in a POS sequence (Qianand Liu, 2012).In the past years, several proposed supervisedjoint models (Ng and Low, 2004; Zhang andClark, 2008; Jiang et al, 2009; Zhang and Clark,2010) achieved reasonably accurate results, but theoutstanding problem among these models is thatthey rely heavily on a large amount of labeled data,i.e., segmented texts with POS tags.
However, theproduction of such labeled data is extremely time-consuming and expensive (Jiao et al, 2006; Jianget al, 2009).
Therefore, semi-supervised join-t S&T appears to be a natural solution for easily in-corporating accessible unlabeled data to improvethe joint S&T model.
This study focuses on usinga graph-based label propagation method to builda semi-supervised joint S&T model.
Graph-basedlabel propagation methods have recently shownthey can outperform the state-of-the-art in sever-al natural language processing (NLP) tasks, e.g.,POS tagging (Subramanya et al, 2010), knowl-edge acquisition (Talukdar et al, 2008), shallowsemantic parsing for unknown predicate (Das andSmith, 2011).
As far as we know, however, thesemethods have not yet been applied to resolvethe problem of joint Chinese word segmentation(CWS) and POS tagging.Motivated by the works in (Subramanya et al,2010; Das and Smith, 2011), for structured prob-lems, graph-based label propagation can be em-ployed to infer valuable syntactic information (n-gram-level label distributions) from labeled datato unlabeled data.
This study extends this intui-tion to construct a similarity graph for propagatingtrigram-level label distributions.
The derived labeldistributions are regarded as prior knowledge toregularize the learning of a sequential model, con-ditional random fields (CRFs) in this case, on both770labeled and unlabeled data to achieve the semi-supervised learning.
The approach performs theincorporation of the derived labeled distributionsby manipulating a ?virtual evidence?
function asdescribed in (Li, 2009).
Experiments on the da-ta from the Chinese tree bank (CTB-7) and Mi-crosoft Research (MSR) show that the proposedmodel results in significant improvement over oth-er comparative candidates in terms of F-score andout-of-vocabulary (OOV) recall.This paper is structured as follows: Section2 points out the main differences with the re-lated work of this study.
Section 3 reviews thebackground, including supervised character-basedjoint S&T model based on CRFs and graph-basedlabel propagation.
Section 4 presents the details ofthe proposed approach.
Section 5 reports the ex-periment results.
The conclusion is drawn in Sec-tion 6.2 Related WorkPrior supervised joint S&T models present ap-proximate 0.2% - 1.3% improvement in F-scoreover supervised pipeline ones.
The state-of-the-art joint models include reranking approaches (Shiand Wang, 2007), hybrid approaches (Nakagawaand Uchimoto, 2007; Jiang et al, 2008; Sun,2011), and single-model approaches (Ng and Low,2004; Zhang and Clark, 2008; Kruengkrai et al,2009; Zhang and Clark, 2010).
The proposed ap-proach in this paper belongs to the single-modeltype.There are few explorations of semi-supervisedapproaches for CWS or POS tagging in previ-ous works.
Xu et al (2008) described a Bayesiansemi-supervised CWS model by considering thesegmentation as the hidden variable in machinetranslation.
Unlike this model, the proposed ap-proach is targeted at a general model, instead ofone oriented to machine translation task.
Sun andXu (2011) enhanced a CWS model by interpolat-ing statistical features of unlabeled data into theCRFs model.
Wang et al (2011) proposed a semi-supervised pipeline S&T model by incorporatingn-gram and lexicon features derived from unla-beled data.
Different from their concern, our em-phasis is to learn the semi-supervised model byinjecting the label information from a similaritygraph constructed from labeled and unlabeled da-ta.The induction method of the proposed approachalso differs from other semi-supervised CRFs al-gorithms.
Jiao et al (2006), extended by Mannand McCallum (2007), reported a semi-supervisedCRFs model which aims to guide the learningby minimizing the conditional entropy of unla-beled data.
The proposed approach regularizes theCRFs by the graph information.
Subramanya etal.
(2010) proposed a graph-based self-train stylesemi-supervised CRFs algorithm.
In the proposedapproach, an analogous way of graph constructionintuition is applied.
But overall, our approach dif-fers in three important aspects: first, novel featuretemplates are defined for measuring the similari-ty between vertices.
Second, the critical property,i.e., sparsity, is considered among label propaga-tion.
And third, the derived label information fromthe graph is smoothed into the model by optimiz-ing a modified objective function.3 Background3.1 Supervised Character-based ModelThe character-based joint S&T approach is oper-ated as a sequence labeling fashion that each Chi-nese character, i.e., hanzi, in the sequence is as-signed with a tag.
To perform segmentation andtagging simultaneously in a uniform framework,according to Ng and Low (2004), the tag is com-posed of a word boundary part, and a POS part,e.g., ?B NN?
refers to the first character in a wordwith POS tag ?NN?.
In this paper, 4 word bound-ary tags are employed: B (beginning of a word),M (middle part of a word), E (end of a word) andS (single character).
As for the POS tag, we shal-l use the 33 tags in the Chinese tree bank.
Thus,the potential composite tags of joint S&T consistof 132 (4?33) classes.The first-order CRFs model (Lafferty et al,2001) has been the most common one in thistask.
Given a set of labeled examples Dl ={(xi, yi)}li=1, where xi = x1ix2i ...xNi is the se-quence of characters in the ith sentence, and yi =y1i y2i ...yNi is the corresponding label sequence.The goal is to learn a CRFs model in the form,p(yi|xi; ?)
=1Z(xi; ?
)exp{N?j=1K?k=1?kfk(yj?1i , yji , xi, j)}(1)where Z(xi; ?)
is the partition function that nor-malizes the exponential form to be a probabilitydistribution, and fk(yj?1i , yji , xi, j).
In this study,771the baseline feature templates of joint S&T arethe ones used in (Ng and Low, 2004; Jiang et al,2008), as shown in Table 1. ?
= {?1?2...?K} ?RK are the weight parameters to be learned.
In su-pervised training, the aim is to estimate the ?
thatmaximizes the conditional likelihood of the train-ing data while regularizing model parameters:L(?)
=l?i=1log p(yi|xi; ?)?R(?)
(2)R(?)
can be any standard regularizer on parame-ters, e.g., R(?)
=?
?
?
/2?2, to limit overfittingon rare features and avoid degeneracy in the caseof correlated features.
This objective function canbe optimized by the stochastic gradient method orother numerical optimization methods.Type Font SizeUnigram Cn(n = ?2,?1, 0, 1, 2)Bigram CnCn+1(n = ?2,?1, 0, 1)Date, Digit andAlphabetic LetterT (C?2)T (C?1)T (C0)T (C1)T (C2)Table 1: The feature templates of joint S&T.3.2 Graph-based Label PropagationGraph-based label propagation, a critical subclassof semi-supervised learning (SSL), has been wide-ly used and shown to outperform other SSL meth-ods (Chapelle et al, 2006).
Most of these algo-rithms are transductive in nature, so they cannotbe used to predict an unseen test example in the fu-ture (Belkin et al, 2006).
Typically, graph-basedlabel propagation algorithms are run in two mainsteps: graph construction and label propagation.The graph construction provides a natural way torepresent data in a variety of target domains.
Oneconstructs a graph whose vertices consist of la-beled and unlabeled examples.
Pairs of verticesare connected by weighted edges which encodethe degree to which they are expected to have thesame label (Zhu et al, 2003).
Popular graph con-struction methods include k-nearest neighbors (k-NN) (Bentley, 1980; Beygelzimer et al, 2006),b-matching (Jebara et al, 2009) and local recon-struction (Daitch et al, 2009).
Label propaga-tion operates on the constructed graph.
The pri-mary objective is to propagate labels from a fewlabeled vertices to the entire graph by optimiz-ing a loss function based on the constraints orproperties derived from the graph, e.g., smooth-ness (Zhu et al, 2003; Subramanya et al, 2010;Talukdar et al, 2008), or sparsity (Das and Smith,2012).
State-of-the-art label propagation algo-rithms include LP-ZGL (Zhu et al, 2003), Ad-sorption (Baluja et al, 2008), MAD (Talukdarand Crammer, 2009) and Sparse Inducing Penal-ties (Das and Smith, 2012).4 MethodThe emphasis of this work is on building a jointS&T model based on two different kinds of datasources, labeled and unlabeled data.
In essence,this learning problem can be treated as incorporat-ing certain gainful information, e.g., prior knowl-edge or label constraints, of unlabeled data intothe supervised model.
The proposed approach em-ploys a transductive graph-based label propagationmethod to acquire such gainful information, i.e.,label distributions from a similarity graph con-structed over labeled and unlabeled data.
Then,the derived label distributions are injected as vir-tual evidences for guiding the learning of CRFs.Algorithm 1 semi-supervised joint S&T inductionInput:Dl = {(xi, yi)}li=1 labeled sentencesDu = {(xi)}l+ui=l+1 unlabeled sentencesOutput:?
: a set of feature weights1: Begin2: {G} = construct graph (Dl,Du)3: {q0} = init labelDist ({G})4: {q} = propagate label ({G}, {q0})5: {?}
= train crf (Dl ?
Du, {q})6: EndThe model induction includes the followingsteps (see Algorithm 1): firstly, given labeledand unlabeled data, i.e., Dl = {(xi, yi)}li=1with l labeled sentences and Du = {(xi)}l+ui=l+1with u unlabeled sentences, a specific similaritygraph G representing Dl and Du is constructed(construct graph).
The vertices (Section 4.1) inthe constructed graph consist of all trigrams thatoccur in labeled and unlabeled sentences, and edgeweights between vertices are computed using thecosine distance between pointwise mutual infor-mation (PMI) statistics.
Afterwards, the estimatedlabel distributions q0 of vertices in the graph G arerandomly initialized (init labelDist).
Subsequently,772the label propagation procedure (propagate label)is conducted for projecting label distributions qfrom labeled vertices to the entire graph, usingthe algorithm of Sparse-Inducing Penalties (Dasand Smith, 2012) (Section 4.2).
The final step(train crf) of the induction is incorporating the in-ferred trigram-level label distributions q into CRFsmodel (Section 4.3).4.1 Graph ConstructionIn most graph-based label propagation tasks, thefinal effect depends heavily on the quality ofthe graph.
Graph construction thus plays a cen-tral role in graph-based label propagation (Zhu etal., 2003).
For character-based joint S&T, unlikethe unstructured learning problem whose verticesare formed directly by labeled and unlabeled in-stances, the graph construction is non-trivial.
Dasand Petrov (2011) mentioned that taking individu-al characters as the vertices would result in variousambiguities, whereas the similarity measurementis still challenging if vertices corresponding to en-tire sentences.This study follows the intuitions of graph con-struction from Subramanya et al (2010) in whichvertices are represented by character trigrams oc-curring in labeled and unlabeled sentences.
For-mally, given a set of labeled sentences Dl, and un-labeled onesDu, whereD , {Dl,Du}, the goal isto form an undirected weighted graph G = (V,E),where V is defined as the set of vertices whichcovers all trigrams extracted from Dl and Du.Here, V = Vl ?
Vu, where Vl refers to trigramsthat occurs at least once in labeled sentences andVu refers to trigrams that occur only in unlabeledsentences.
The edges E ?
Vl ?
Vu, connect allthe vertices.
This study makes use of a symmet-ric k-NN graph (k = 5) and the edge weights aremeasured by a symmetric similarity function (E-quation (3)):wi,j ={sim(xi, xj) if j ?
K(i) or i ?
K(j)0 otherwise(3)where K(i) is the set of the k nearest neighbors ofxi(|K(i) = k, ?i|) and sim(xi, xj) is a similari-ty measure between two vertices.
The similarityis computed based on the co-occurrence statistic-s over the features in Table 2.
Most features weadopted are selected from those of (Subramanyaet al, 2010).
Note that a novel feature in the lastrow encodes the classes of surrounding character-s, where four types are defined: number, punctu-ation, alphabetic letter and other.
It is especiallyhelpful for the graph to make connections with tri-grams that may not have been seen in labeled databut have similar label information.
The pointwisemutual information values between the trigram-s and each feature instantiation that they have incommon are summed to sparse vectors, and theircosine distances are computed as the similarities.Description FeatureTrigram + Context x1x2x3x4x5Trigram x2x3x4Left Context x1x2Right Context x4x5Center Word x3Trigram - Center Word x2x4Left Word + Right Context x2x4x5Right Word + Left Context x1x2x3Type of Trigram: number,punctuation, alphabetic letterand othert(x2)t(x3)t(x4)Table 2: Features employed to measure the sim-ilarity between two vertices, in a given tex-t ?x1x2x3x4x5?, where the trigram is ?x2x3x4?.The nature of the similarity graph enforces thatthe connected trigrams with high weight appearingin different texts should have similar syntax con-figurations.
Thus, the constructed graph is expect-ed to provide additional information that cannotbe expressed directly in a sequence model (Subra-manya et al, 2010).
One primary benefit of thisproperty is on enriching vocabulary coverage.
Inother words, the new features of various trigram-s only occurring in unlabeled data can be discov-ered.
As the excerpt in Figure 1 shows, the trigram?????
(Tianjin port) has no any label informa-tion, as it only occurs in unlabeled data, but for-tunately its neighborhoods with similar syntax in-formation, e.g., ?????
(Shanghai port), ?????
(Guangzhou port), can assist to infer the cor-rect tag ?M NN?.4.2 Label PropagationIn order to induce trigram-level label distributionsfrom the graph constructed by the previous step,a label propagation algorithm, Sparsity-InducingPenalties, proposed by Das and Smith (2012), isemployed.
This algorithm is used because it cap-tures the property of sparsity that only a few labels773Figure 1: An excerpt from the similarity graphover trigrams on labeled and unlabeled data.are typically associated with a given instance.
Infact, the sparsity is also a common phenomenonamong character-based CWS and POS tagging.The following convex objective is optimized onthe similarity graph in this case:argminql?j=1?
qj ?
rj ?2+?l+u?i=1,k?N (i)wik ?
qi ?
qk ?2 +?l+u?i=1?
qi ?2s.t.
qi ?
0, ?i ?
V(4)where rj denotes empirical label distributions oflabeled vertices, and qi denotes unnormalized es-timate measures in every vertex.
The wik refers tothe similarity between the ith trigram and the kthtrigram, and N (i) is a set of neighbors of the ithtrigram.
?
and ?
are two hyperparameters whosevalues are discussed in Section 5.
The squared-loss criterion1 is used to formulate the objectivefunction.
The first term in Equation (4) is the seedmatch loss which penalizes the estimated label dis-tributions qj , if they go too far away from the em-pirical labeled distributions rj .
The second termis the edge smoothness loss that requires qi shouldbe smooth with respect to the graph, such that twovertices connected by an edge with high weightshould be assigned similar labels.
The final termis a regularizer to incorporate the prior knowledge,e.g., uniform distributions used in (Talukdar et al,2008; Das and Smith, 2011).
This study appliesthe squared norm of q to encourage sparsity pervertex.
Note that the estimated label distribution1It can be seen as a multi-class extension of quadratic costcriterion (Bengio et al, 2006) or as a variant of the objectivein (Zhu et al, 2003).
An entropic distance measure could alsobe used, e.g., KL-divergence (Subramanya et al, 2010; Dasand Smith, 2012).qi in Equation (4) is relaxed to be unnormalized,which simplifies the optimization.
Thus, the objec-tive function can be optimized by L-BFGS-B (Zhuet al, 1997), a generic quasi-Newton gradient-based optimizer.
The partial derivatives of Equa-tion (4) are computed for each parameter of q andthen passed on to the optimizer that updates themsuch that Equation (4) is maximized.4.3 Semi-Supervised CRFs TrainingThe trigram-level label distributions inferred in thepropagation step can be viewed as a kind of valu-able ?prior knowledge?
to regularize the learningon unlabeled data.
The final step of the induc-tion is thus to incorporate such prior knowledgeinto CRFs.
Li (2009) generalizes the use of vir-tual evidence to undirected graphical models and,in particular, to CRFs for incorporating externalknowledge.
By extending the similar intuition, asillustrated in Figure 2, we modify the structure ofa regular linear-chain CRFs on unlabeled data forsmoothing the derived label distributions, wherevirtual evidences, i.e., q in our case, are donatedby {v1, v2, .
.
.
, vT }, in parallel with the state vari-ables {y1, y2, .
.
.
, yT }.
The modified CRFs modelallows us to flexibly define the interaction betweenestimated state values and virtual evidences by po-tential functions.
Therefore, given labeled and un-labeled data, the learning objective is defined asfollows:L(?)
+l+u?i=l+1Ep(yi|xi,vi;?g)[log p(yi, vi|xi; ?
)](5)where the conditional probability in the secondterm is denoted asp(yi, vi|xi; ?)
=1Z ?
(xi; ?
)exp{N?j=1K?k=1?kfk(yj?1i , yji , xi, j)+?N?t=1s(yti , vti)}(6)The first term in Equation (5) is the same as E-quation (2), which is the traditional CRFs learn-ing objective function on the labeled data.
Thesecond term is the expected conditional likelihoodof unlabeled data.
It is directed to maximize theconditional likelihood of hidden states with thederived label distributions on unlabeled data, i.e.,p(y, v|x), where y and v are jointly modeled but774the probability is still conditional on x. Here,Z ?
(x; ?)
is the partition function of normalizationthat is achieved by summing the numerator overboth y and v. A virtual evidence feature functionof s(yti , vti) with pre-defined weight ?
is definedto regularize the conditional distributions of statesover the derived label distributions.
The learningis impacted by the derived label distributions as E-quation (7): firstly, if the trigram xt?1i xtixt+1i atcurrent position does have no corresponding de-rived label distributions (vti = null), the value ofzero is assigned to all state hypotheses so that theposteriors would not affected by the derived infor-mation.
Secondly, if it does have a derived labeldistribution, since the virtual evidence in this caseis a distribution instead of a specific label, the la-bel probability in the distribution under the currentstate hypothesis is assigned.
This means that thevalues of state variables are constrained to agreewith the derived distributions.s(yti , vti) ={qxt?1i xtixt+1i (yti) if vti 6= null0 else(7)The second term in Equation (5) can be op-timized by using the expectation maximization(EM) algorithm in the same fashion as in thegenerative approach, following (Li, 2009).
Onecan iteratively optimize the Q function Q(?)
=?y p(yi|xi; ?g) log p(yi, vi|xi; ?
), in which ?g isthe model estimated from the previous iteration.Here the gradient of the Q function can be mea-sured by:?Q(?)?
?k=?t?yt?1i ,ytifk(yt?1i , yti , xi, t).
(p(yt?1i , yti |xi, vi; ?)?
p(yt?1i , yti |xi; ?
))(8)The forward-backward algorithm is used to mea-sure p(yt?1i , yti |xi, vi; ?)
and p(yt?1i , yti |xi; ?
).Thus, the objective function Equation (5) is op-timized as follows: for the instances i = 1, 2, ..., l,the parameters ?
are learned as the supervisedmanner; for the instances i = l+1, l+2, ..., u+ l,in the E-step, the expected value of Q function iscomputed, based on the current model ?g.
In theM-step, the posteriors are fixed and updated ?
thatmaximizes Equation (5).Figure 2: Modified linear-chain CRFs integratingvirtual evidences on unlabeled data.5 Experiment5.1 SettingThe experimental data are mainly taken from theChinese tree bank (CTB-7) and Microsoft Re-search (MSR)2.
CTB-7 consists of over one mil-lion words of annotated and parsed text from Chi-nese newswire, magazine news, various broadcastnews and broadcast conversation programs, webnewsgroups and weblogs.
It is a segmented, POStagged3 and fully bracketed corpus.
The train, de-velopment and test sets4 from CTB-7 and theircorresponding statistics are reported in Table 3.To satisfy the characteristic of the semi-supervisedlearning problem, the train set, i.e., the labeled da-ta, is formed by a relatively small amount of an-notated texts sampled from CTB-7.
For the un-labeled data in this experiment, a greater amountof texts is extracted from CTB-7 and MSR, whichcontains 53,108 sentences with 2,418,690 charac-ters.The performance measurement indicators forword segmentation and POS tagging (joint S&T)are balance F-score, F = 2PR/(P+R), the harmon-ic mean of precision (P) and recall (R), and out-of-vocabulary recall (OOV-R).
For segmentation,a token is regarded to be correct if its boundariesmatch the ones of a word in the gold standard.For the POS tagging, it is correct only if both theboundaries and the POS tags are perfect matches.The experimental platform is implementedbased on two toolkits: Mallet (McCallum andKachites, 2002) and Junto (Talukdar and Pereira,2010).
Mallet is a java-based package for s-tatistical natural language processing, which in-cludes the CRFs implementation.
Junto is a graph-2It can be download at: www.sighan.org/bakeoff2005.3There is a total of 33 POS tags in CTB-7.4The extracted sentences in train, development and test setwere assigned with the composite tags as described in Section3.1.775based label propagation toolkit that provides sev-eral state-of-the-art algorithms.Data #Sent #Word #Char #OOVTrain 17,968 374,697 596,360Develop 1,659 46,637 79,283 0.074Test 2,037 65,219 104,502 0.089Table 3: Training, development and testing data.5.2 Baseline and Proposed ModelsIn the experiment, the baseline supervised pipelineand joint S&T models are built only on the traindata.
The proposed model will also be comparedwith the semi-supervised pipeline S&T model de-scribed in (Wang et al, 2011).
In addition, twostate-of-the-art semi-supervised CRFs algorithms,Jiao?s CRFs (Jiao et al, 2006) and Subramanya?sCRFs (Subramanya et al, 2010), are also used tobuild joint S&T models.
The corresponding set-tings of the above candidates are listed below:?
Baseline I: a supervised CRFs pipeline S&Tmodel.
The feature templates are from Zhaoet al (2006) and Wu et al (2008).?
Wang?s model: a semi-supervised CRFspipeline S&T model.
The same feature tem-plates in (Wang et al, 2011) are used, i.e.,?+n-gram+cluster+lexicon?.?
Baseline II: a supervised CRFs joint S&Tmodel.
The feature templates introduced inSection 3.1 are used.?
Jiao?s model: a semi-supervised CRFs jointS&T model trained using the entropy regular-ization (ER) criteria (Jiao et al, 2006).
Theoptimization method proposed by Mann andMcCallum (2007) is applied.?
Subramanya?s model: a self-train stylesemi-supervised CRFs joint S&T modelbased on the same parameters used in (Sub-ramanya et al, 2010).?
Our model: several parameters in our modelare needed to tune based on the developmentset, e.g., ?, ?
and ?.In all the CRFs models above, the Gaussian reg-ularizer and stochastic gradient descent methodare employed.5.3 Main ResultsThis experiment yielded a similarity graph thatconsists of 462,962 trigrams from labeled and un-labeled data.
The majority (317,677 trigrams) oc-curred only in unlabeled data.
Based on the de-velopment data, the hyperparameters of our mod-el were tuned among the following settings: forthe graph propagation, ?
?
{0.2, 0.5, 0.8} and?
?
{0.1, 0.3, 0.5, 0.8}; for the CRFs training,?
?
{0.1, 0.3, 0.5, 0.7, 0.9}.
The best performedjoint settings are ?
= 0.5, ?
= 0.3 and ?
= 0.7.With the chosen set of hyperparameters, the testdata was used to measure the final performance.Model Segmentation POS TaggingF1 OOV-R F1 OOV-RBaseline I 94.27 60.12 91.08 51.72Wang?s 95.17 63.10 91.64 53.29Baseline II 95.14 61.52 91.61 52.29Jiao?s 95.58 63.05 92.11 53.27Subramanya?s 96.30 67.12 92.46 57.15Our model 96.85 68.09 92.89 58.36Table 4: The performance of segmentation andPOS tagging on testing data.Table 4 summarizes the performance of seg-mentation and POS tagging on the test data, incomparison with the other five models.
First-ly, as expected, for the two supervised baselines,the joint model outperforms the pipeline one, e-specially on segmentation.
It obtains 0.92% and2.32% increase in terms of F-score and OOV-Rrespectively.
This outcome verifies the commonlyaccepted fact that the joint model can substantiallyimprove the pipeline one, since POS tags provideadditional information to word segmentation (Ngand Low, 2004).
Secondly, it is also noticed thatall four semi-supervised models are able to benefitfrom unlabeled data and greatly improve the re-sults with respect to the baselines.
On the whole,for segmentation, they achieve average improve-ments of 1.02% and 6.8% in F-score and OOV-R;whereas for POS tagging, the average incrementsof F-sore and OOV-R are 0.87% and 6.45%.
Aninteresting phenomenon is found among the com-parisons with baselines that the supervised jointmodel (Baseline II) is even competitive with semi-supervised pipeline one (Wang et al, 2011).
Thisillustrates the effects of error propagation in thepipeline approach.
Thirdly, in what concerns thesemi-supervised approaches, the three joint S&Tmodels, i.e., Jiao?s, Subramanya?s and our mod-el, are superior to the pipeline model, i.e., Wang?s776model.
Moreover, the two graph-based approach-es, i.e., Subramanya?s and our model, outperformthe others.
Most importantly, the boldface num-bers in the last row illustrate that our model doesachieve the best performance.
Overall, for wordsegmentation, it obtains average improvements of1.43% and 8.09% in F-score and OOV-R over oth-ers; for POS tagging, it achieves average improve-ments of 1.09% and 7.73%.0 10,000 20,000 30,000 40,000 50,00094.094.595.095.596.096.597.097.5Wang'sJiao'sSubramanya'sOurF-scoreNumber of unlabeled sentences0 10,000 20,000 30,000 40,000 50,00091.091.592.092.593.093.5Wang'sJiao'sSubramanya'sOurF-scoreNumber of unlabeled sentences0 10,000 20,000 30,000 40,000 50,00060.062.565.067.570.0Wang'sJiao'sSubramanya'sOurOOV-RNumber of unlabeled sentences0 10,000 20,000 30,000 40,000 50,00051.052.554.055.557.058.5Wang'sJiao'sSubramanya'sOurOOV-RNumber of unlabeled sentencesFigure 3: The learning curves of semi-supervisedmodels on unlabeled data, where left graphs aresegmentation and the right ones are tagging.5.4 Learning CurveAn additional experiment was conducted to inves-tigate the impact of unlabeled data for the foursemi-supervised models.
Figure 3 illustrates thecurves of F-score and OOV-R for segmentationand tagging respectively, as the unlabeled datasize is progressively increased in steps of 6,000sentences.
It can be clearly observed that al-l curves of our model are able to mount up steadi-ly and achieve better gains over others consistent-ly.
The most competitive performance of the oth-er three candidates is achieved by Subramanya?smodel.
This strongly reveals that the knowledgederived from the similarity graph does effectivelystrengthen the model.
But in Subramanya?s mod-el, when the unlabeled size ascends to approxi-mately 30,000 sentences the curves become nearlyasymptotic.
The semi-supervised pipeline model,Wang?s model, presents a much slower growth onall curves over the others and also begins to over-fit with large unlabeled data sizes (>25,000 sen-tences).
The figure also shows an erratic fluctu-ation of Jiao?s model.
Since this approach aimsat minimizing conditional entropy over unlabeleddata and encourages finding putative labelings forunlabeled data, it results in a data-sensitive mod-el (Li et al, 2009).5.5 Analysis & DiscussionA statistical analysis of the segmentation and tag-ging results of the supervised joint model (Base-line II) and our model is carried out to comprehendthe influence of the graph-based semi-supervisedbehavior.
For word segmentation, the most signif-icant improvement of our model is mainly concen-trated on two kinds of words which are known fortheir difficulties in terms of CWS: a) named enti-ties (NE), e.g., ?????
(Tianjin port) and ?????
(free tax zone); and b) Chinese numbers (CN),e.g., ??????
(eight hundred and fifty million)and ????????
(seventy two percent).
Veryoften, these words do not exist in the labeled data,so the supervised model is hard to learn their fea-tures.
Part of these words, however, may occur inthe unlabeled data.
The proposed semi-supervisedapproach is able to discover their label informationwith the help of a similarity graph.
Specifically, itlearns the label distributions from similar words(neighborhoods), e.g., ?????
(Shanghai port),?????
(protection zone), ??????
(ninehundred and seventy million).
The statistics in Ta-ble 5 demonstrate significant error reductions of50.44% and 48.74% on test data, corresponding toNE and CN respectively.Type #word #baErr #gbErr ErrDec%NE 471 226 112 50.44CN 181 119 61 48.74Table 5: The statistics of segmentation error fornamed entities (NE) and Chinese numbers (CN)in test data.
#baErr and #gbErr denote the countof segmentations by Baseline II and our model;ErrDec% denotes the error reduction.On the other hand, to better understand the tag-ging results, we summarize the increase and de-crease of the top five common tagging error pat-terns of our model over Baseline II for the cor-rectly segmented words, as shown in Table 6.
Theerror pattern is defined by ?A?B?
that refers thetrue tag of ?A?
is annotated by a tag of ?B?.
Theobvious improvement brought by our model oc-curs with the tags ?NN?, ?CD?, ?NR?, ?JJ?
and?NR?, where errors are reduced 60.74% on aver-777Pattern #baErr ?
Pattern #baErr ?NN?VV 58 38 NN?NR 13 6CD?NN 41 27 IJ?ON 9 5NR?VV 29 17 VV?NN 4 3JJ?NN 18 11 NR?NN 1 3NR?VA 19 10 JJ?AD 1 2Table 6: The statistics of POS tagging error pat-terns in test data.
#baErr denote the count of tag-ging error by Baseline II, while ?
and ?
denotesthe number of error reduced or increased by ourmodel.age.
More impressively, there is a large portion offixed error pattern instances stemming from OOVwords.
Meanwhile, it is also observed that the dis-ambiguation of error patterns in the right portionof the table slightly suffers from our approach.
Inreality, it is impossible and unrealistic to requesta model to be ?no harms but only benefits?
underwhatever circumstances.6 ConclusionThis study introduces a novel semi-supervised ap-proach for joint Chinese word segmentation andPOS tagging.
The approach performs the semi-supervised learning in the way that the trigram-level distributions inferred from a similarity graphare used to regularize the learning of CRFs modelon labeled and unlabeled data.
The empirical re-sults indicate that the similarity graph informationand the incorporation manner of virtual evidencespresent a positive effect to the model induction.AcknowledgmentsThe authors are grateful to the Science and Tech-nology Development Fund of Macau and the Re-search Committee of the University of Macaufor the funding support for our research, un-der the reference No.
017/2009/A and RG060/09-10S/CS/FST.
The authors also wish to thank theanonymous reviewers for many helpful comments.ReferencesShumeet Baluja, Rohan Seth, D. Sivakumar, YushiJing, Jay Yagnik, Shankar Kumar, Deepak Ravich,and Mohamed Aly.
2008.
Video suggestion anddiscovery for youtube: taking random walks throughthe view graph.
In Proceedings of WWW, pages 895-904, Beijing, China.Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.2006.
Manifold regularization.
Journal of machinelearning research, 7:2399?2434.Yoshua Bengio, Olivier Delalleau, and Nicolas LeRoux.
2006.
Label propogation and quadratic crite-rion.
MIT Press.Jon Louis Bentley.
1980.
Multidimensional divide-and-conquer.
Communications of the ACM, 23(4):214 -229.Alina Beygelzimer, Sham Kakade, and John Langford.2006.
Cover trees for nearest neighbor.
In Proceed-ings of ICML, pages 97-104, New York, USAOlivier Chapelle, Bernhard Scho?
lkopf, and AlexanderZien.
2006.
Semi-supervised learning.
MIT Press.Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.Spielman.
2009.
Fitting a graph to vector data.
InProceedings of ICML, 201-208, NY, USA.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised framesemantic parsing for unknownpredicates.
In Proceedings of ACL, pages 1435-1444, Portland, Oregon, USA.Dipanjan Das and Slav Petrov.
2011.
UnsupervisedPart-of-Speech Tagging with Bilingual Graph-basedProjections.
In Proceedings of ACL, pages 1435-1444, Portland, Oregon, USA.Dipanjan Das and Noah A. Smith.
2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In Proceedings of NAACL, pages 677-687, Montre?al,Canada.Tony Jebara, Jun Wang, and Shih-Fu Chang.
2009.Graph construction and b-matching for semi-supervised learning.
In Proceedings of ICML, 441-448, New York, USA.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.2008.
A Cascaded Linear Model for Joint ChineseWord Segmentation and Part-of-Speech Tagging.
InProceedings of ACL, pages 897-904, Columbus, O-hio.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic Adaptation of Annotation Standards: Chi-nese Word Segmentation and POS Tagging ?
A CaseStudy.
In Proceedings of he ACL and the 4th IJC-NLP of the AFNLP, pages 522?530, Suntec, Singa-pore.Feng Jiao, Shaojun Wang, and Chi-Hoon Lee.
2006.Semi-supervised conditional random fields for im-proved sequence segmentation and labeling.
In InProceedings of ACL, pages 209?216, Sydney, Aus-tralia.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint Chinese word segmentation andPOS tagging.
In Proceedings of ACL and IJCNLPof the AFNLP, pages 513- 521, Suntec, SingaporeAugust.778John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Field: Prob-abilistic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of ICML, pages 282-289, Williams College, USA.Xiao Li.
2009.
On the use of virtual evidence in con-ditional random fields.
In Proceedings of EMNLP,pages 1289-1297, Singapore.Xiao Li, Ye-Yi Wang, and Alex Acero.
2009.
Extract-ing structured information from user queries withsemi-supervised conditional random fields In Pro-ceedings of ACM SIGIR, pages 572-579, Boston,USA.Gideon S. Mann and Andrew McCallum.
2007.
Ef-ficient computation of entropy gradient for semi-supervised conditional random fields.
In Proceed-ings of NAACL, pages 109-112, New York, USA.McCallum and Andrew Kachites.
2002.
MALLET: AMachine Learning for Language Toolkit.
Softwareat http://mallet.cs.umass.edu.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.
Ahybrid approach to word segmentation and POS tag-ging.
In Proceedings of ACL Demo and Poster Ses-sion, pages 217?220, Prague, Czech Republic.Hwee Tou Ng and Jin Kiat Low 2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Proceedings ofEMNLP, Barcelona, Spain.Xian Qian and Yang Liu.
2012.
Joint Chinese WordSegmentation, POS Tagging and Parsing.
In Pro-ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-land, Korea.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layerCRF based joint decoding method for cascade seg-mentation and labelling tasks.
In Proceedings of IJ-CAI, Hyderabad, India.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervised learning of structured tagging models.In Proceedings of EMNLP, pages 167-176, Mas-sachusetts, USA.Weiwei Sun.
2011.
A Stacked Sub-Word Modelfor Joint Chinese Word Segmentation and Part-of-Speech Tagging.
In Proceedings of ACL, pages1385?1394, Portland, Oregon.Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of EMNLP, pages 970-979, Scotland, UK.Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-ca, Deepak Ravichandran, Rahul Bhagat, and Fer-nando Pereira.
2008.
Weakly Supervised Acquisi-tion of Labeled Class Instances using Graph Ran-dom Walks.
In Proceedings of EMNLP, pages 582-590, Hawaii, USA.Partha Pratim Talukdar and Koby Crammer.
2009.New Regularized Algorithms for TransductiveLearning.
In Proceedings of ECML-PKDD, pages442 - 457, Bled, Slovenia.Partha Pratim Talukdar and Fernando Pereira.
2010.Experiments in graph-based semi-supervised learn-ing methods for class-instance acquisition.
In Pro-ceedings of ACL, pages 1473-1481, Uppsala, Swe-den.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Torisa-wa.
2011.
Improving Chinese word segmentationand POS tagging with semi-supervised methods us-ing large auto-analyzed data.
In Proceedings of IJC-NLP, pages 309?317, Chiang Mai, Thailand.Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee.
2008.Description of the NCU Chinese Word Segmenta-tion and Part-of-Speech Tagging for SIGHAN Bake-off.
In Proceedings of the SIGHAN Workshop onChinese Language Processing, pages 161-166, Hy-derabad, India.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-n Ney.
2008.
Bayesian semi-supervised chineseword segmentation for statistical machine transla-tion.
In Proceedings of COLING, pages 1017-1024,Manchester, UK.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and POS tagging using a single percep-tron.
In Proceedings of EMNLP, pages 888-896,Columbus, Ohio.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging usinga single discriminative model.
In Proceedings ofEMNLP, pages 843-852, Massachusetts, USA.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2006.
Effective tag set selection in Chineseword segmentation via conditional random fieldmodeling.
In Proceedings of PACLIC, pages 87-94,Wuhan, China.Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-ty.
2003.
Semi-supervised learning using Gaussianfields and harmonic functions.
In Proceedings ofICML, pages 912?919, Washington DC, USA.Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and JorgeNocedal.
1997.
L-BFGS-B: Fortran subroutines forlarge scale bound constrained optimization.
ACMTransactions on Mathematical Software, 23:550-560.779
