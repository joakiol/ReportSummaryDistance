Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938?947,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPWeb-Scale Distributional Similarity and Entity Set ExpansionPatrick Pantel?, Eric Crestan?, Arkady Borkovsky?, Ana-Maria Popescu?, Vishnu Vyas??Yahoo!
LabsSunnyvale, CA 94089{ppantel,ecrestan}@yahoo-inc.com{amp,vishnu}@yahoo-inc.com?Yandex LabsBurlingame, CA 94010arkady@yandex-team.ruAbstractComputing the pairwise semantic similaritybetween all words on the Web is a compu-tationally challenging task.
Parallelizationand optimizations are necessary.
We pro-pose a highly scalable implementationbased on distributional similarity, imple-mented in the MapReduce framework anddeployed over a 200 billion word crawl ofthe Web.
The pairwise similarity between500 million terms is computed in 50 hoursusing 200 quad-core nodes.
We apply thelearned similarity matrix to the task of au-tomatic set expansion and present a largeempirical study to quantify the effect onexpansion performance of corpus size, cor-pus quality, seed composition and seedsize.
We make public an experimentaltestbed for set expansion analysis that in-cludes a large collection of diverse entitysets extracted from Wikipedia.1 IntroductionComputing the semantic similarity between termshas many applications in NLP including word clas-sification (Turney and Littman 2003), word sensedisambiguation (Yuret and Yatbaz 2009), context-spelling correction (Jones and Martin 1997), factextraction (Pa?ca et al 2006), semantic role labe-ling (Erk 2007), and applications in IR such asquery expansion (Cao et al 2008) and textual ad-vertising (Chang et al 2009).For commercial engines such as Yahoo!
andGoogle, creating lists of named entities found onthe Web is critical for query analysis, documentcategorization, and ad matching.
Computing termsimilarity is typically done by comparing co-occurrence vectors between all pairs of terms(Sarmento et al 2007).
Scaling this task to theWeb requires parallelization and optimizations.In this paper, we propose a large-scale term si-milarity algorithm, based on distributional similari-ty, implemented in the MapReduce framework anddeployed over a 200 billion word crawl of theWeb.
The resulting similarity matrix between 500million terms is applied to the task of expandinglists of named entities (automatic set expansion).We provide a detailed empirical analysis of thediscovered named entities and quantify the effecton expansion accuracy of corpus size, corpusquality, seed composition, and seed set size.2 Related WorkBelow we review relevant work in optimizing si-milarity computations and automatic set expansion.2.1 Computing Term SimilaritiesThe distributional hypothesis (Harris 1954), whichlinks the meaning of words to their contexts, hasinspired many algorithms for computing term simi-larities (Lund and Burgess 1996; Lin 1998; Lee1999; Erk and Pad?
2008; Agirre et al 2009).Brute force similarity computation compares allthe contexts for each pair of terms, with complexi-ty O(n2m) where n is the number of terms and m isthe number of possible contexts.
More efficientstrategies are of three kinds:938Smoothing: Techniques such as Latent SemanticAnalysis reduce the context space by applyingtruncated Singular Value Decomposition (SVD)(Deerwester et al 1990).
Computing the matrixdecomposition however does not scale well toweb-size term-context matrices.
Other currentlyunscalable smoothing techniques include Probabil-istic Latent Semantic Analysis (Hofmann 1999),Iterative Scaling (Ando 2000), and Latent DirichletAllocation (Blei et al 2003).Randomized Algorithms: Randomized tech-niques for approximating various similarity meas-ures have been successfully applied to term simi-larity (Ravichandran et al 2005; Gorman and Cur-ran 2006).
Common techniques include RandomIndexing based on Sparse Distributed Memory(Kanerva 1993) and Locality Sensitive Hashing(Broder 1997).Optimizations and Distributed Processing:Bayardo et al (2007) present a sparse matrix opti-mization strategy capable of efficiently computingthe similarity between terms which?s similarityexceeds a given threshold.
Rychl?
and Kilgarriff(2007), Elsayed et al (2008) and Agirre et al(2009) use reverse indexing and the MapReduceframework to distribute the similarity computa-tions across several machines.
Our proposed ap-proach combines these two strategies and efficient-ly computes the exact similarity (cosine, Jaccard,Dice, and Overlap) between all pairs.2.2 Entity extraction and classificationBuilding entity lexicons is a task of great interestfor which structured, semi-structured and unstruc-tured data have all been explored (GoogleSets;Sarmento et al 2007; Wang and Cohen 2007; Bu-nescu and Mooney 2004; Etzioni et al 2005; Pa?caet al 2006).
Our own work focuses on set expan-sion from unstructured Web text.
Apart from thechoice of a data source, state-of-the-art entity ex-traction methods differ in their use of numerous,few or no labeled examples, the open or targetednature of the extraction as well as the types of fea-tures employed.
Supervised approaches (McCal-lum and Li 2003, Bunescu and Mooney 2004) relyon large sets of labeled examples, perform targetedextraction and employ a variety of sentence- andcorpus-level features.
While very precise, thesemethods are typically used for coarse grained enti-ty classes (People, Organizations, Companies) forwhich large training data sets are available.
Unsu-pervised approaches rely on no labeled data anduse either bootstrapped class-specific extractionpatterns (Etzioni et al 2005) to find new elementsof a given class (for targeted extraction) or corpus-based term similarity (Pantel and Lin 2002) to findterm clusters (in an open extraction framework).Finally, semi-supervised methods have showngreat promise for identifying and labeling entities(Riloff and Shepherd 1997; Riloff and Jones 1999;Banko et al 2007; Downey et al 2007; Pa?ca et al2006; Pa?ca 2007a; Pa?ca 2007b; Pa?ca and Durme2008).
Starting with a set of seed entities, semi-supervised extraction methods use either class-specific patterns to populate an entity class or dis-tributional similarity to find terms similar to theseed set (Pa?ca?s work also examines the advan-tages of combining these approaches).
Semi-supervised methods (including ours) are useful forextending finer grain entity classes, for which largeunlabeled data sets are available.2.3 Impact of corpus on system performancePrevious work has examined the effect of usinglarge, sometimes Web-size corpora, on system per-formance in the case of familiar NLP tasks.
Bankoand Brill (2001) show that Web-scale data helpswith confusion set disambiguation while Lapataand Keller (2005) find that the Web is a goodsource of n-gram counts for unsupervised models.Atterer and Schutze (2006) examine the influenceof corpus size on combining a supervised approachwith an unsupervised one for relative clause andPP-attachment.
Etzioni et al (2005) and Pantel etal.
(2004) show the advantages of using largequantities of generic Web text over smaller corporafor extracting relations and named entities.
Overall,corpus size and quality are both found to be impor-tant for extraction.
Our paper adds to this body ofwork by focusing on the task of similarity-basedset expansion and providing a large empiricalstudy quantify the relative corpus effects.2.4 Impact of seeds on extraction performancePrevious extraction systems report on the size andquality of the training data or, if semi-supervised,the size and quality of entity or pattern seed sets.Narrowing the focus to closely related work, Pa?ca(2007a; 2007b) and Pa?ca and Durme (2008) showthe impact of varying the number of instances rep-resentative of a given class and the size of theattribute seed set on the precision of class attributeextraction.
An example observation is that good939quality class attributes can still be extracted using20 or even 10 instances to represent an entity class.Among others, Etzioni et al (2005) shows that asmall pattern set can help bootstrap useful entityseed sets and reports on the impact of seed setnoise on final performance.
Unlike previous work,empirically quantifying the influence of seed setsize and quality on extraction performance of ran-dom entity types is a key objective of this paper.3 Large-Scale Similarity ModelTerm semantic models normally invoke the distri-butional hypothesis (Harris 1985), which links themeaning of terms to their contexts.
Models arebuilt by recording the surrounding contexts foreach term in a large collection of unstructured textand storing them in a term-context matrix.
Me-thods differ in their definition of a context (e.g.,text window or syntactic relations), or by a meansto weigh contexts (e.g., frequency, tf-idf, pointwisemutual information), or ultimately in measuringthe similarity between two context vectors (e.g.,using Euclidean distance, Cosine, Dice).In this paper, we adopt the following methodol-ogy for computing term similarity.
Our variousweb crawls, described in Section 6.1, are POS-tagged using Brill?s tagger (1995) and chunkedusing a variant of the Abney chunker (Abney1991).
Terms are NP chunks with some modifiersremoved; their contexts (i.e., features) are definedas their rightmost and leftmost stemmed chunks.We weigh each context f using pointwise mutualinformation (Church and Hanks 1989).
Let PMI(w)denote a pointwise mutual information vector, con-structed for each term as follows: PMI(w) = (pmiw1,pmiw2, ?, pmiwm), where pmiwf is the pointwisemutual information between term w and feature f:???
?===mjwjniifwfwfccNcpmi11logwhere cwf is the frequency of feature f occurring forterm w, n is the number of unique terms and N isthe total number of features for all terms.Term similarities are computed by comparingthese pmi context vectors using measures such ascosine, Jaccard, and Dice.3.1 Large-Scale ImplementationComputing the similarity between terms on a largeWeb crawl is a non-trivial problem, with a worstcase cubic running time ?
O(n2m) where n is thenumber of terms and m is the dimensionality of thefeature space.
Section 2.1 introduces several opti-mization techniques; below we propose an algo-rithm for large-scale term similarity computationwhich calculates exact scores for all pairs of terms,generalizes to several different metrics, and is scal-able to a large crawl of the Web.Our optimization strategy follows a generalizedsparse-matrix multiplication approach (Sarawagiand Kirpal 2004), which is based on the well-known observation that a scalar product of twovectors depends only on the coordinates for whichboth vectors have non-zero values.
Further, weobserve that most commonly used similarity scoresfor feature vectors xrand yr, such as cosine andDice, can be decomposed into three values: onedepending only on features of xr, another depend-ing only on features of yr, and the third dependingon the features shared both by xrand yr. More for-mally, commonly used similarity scores ( )yxF rr,can be expressed as:( ) ( ) ( ) ( )?????
?= ?
yfxfyxffyxFiiirrrr3210 ,,,,Table 1 defines f0, f1, f2, and f3 for some commonsimilarity functions.
For each of these scores, f2 =f3.
In our work, we compute all of these scores, butreport our results using only the cosine function.Let A and B be two matrices of PMI feature vec-tors.
Our task is to compute the similarity betweenall vectors in A and all vectors in B.
In computingthe similarity between all pairs of terms, A = B.Figure 1 outlines our algorithm for computingthe similarity between all elements of A and B. Ef-ficient computation of the similarity matrix can beachieved by leveraging the fact that ( )yxF rr,  is de-termined solely by the features shared by xrand yr(i.e., f1(0,x) = f1(x,0) = 0 for any x) and that most ofTable 1.
Definitions for f0, f1, f2, and f3 for commonly usedsimilarity scores.METRIC ( )zyxf ,,0  ( )yxf ,1  ( ) ( )xfxf rr 32 =Overlap x  1 0Jaccard* xzyx?+( )yx ,min  ?iixDice*zyx+2yx ?
?iix2Cosine zyx?yx ?
?iix2*weighted generalization940the feature vectors are very sparse (i.e., most poss-ible contexts never occur for a given term).
In thiscase, calculating f1(x, y) is only required when bothfeature vectors have a shared non-zero feature, sig-nificantly reducing the cost of computation.
De-termining which vectors share a non-zero featurecan easily be achieved by first building an invertedindex for the features.
The computational cost ofthis algorithm is ?
2iN , where Ni is the number ofvectors that have a non-zero ith coordinate.
Itsworst case time complexity is O(ncv) where n isthe number of terms to be compared, c is the max-imum number of non-zero coordinates of any vec-tor, and v is the number of vectors that have a non-zero ith coordinate where i is the coordinate whichis non-zero for the most vectors.
In other words,the algorithm is efficient only when the density ofthe coordinates is low.
On our datasets, we ob-served near linear running time in the corpus size.Bayardo et al (2007) described a strategy thatpotentially reduces the cost even further by omit-ting the coordinates with the highest number ofnon-zero value.
However, their algorithm gives asignificant advantage only when we are interestedin finding solely the similarity between highly sim-ilar terms.
In our experiments, we compute the ex-act similarity between all pairs of terms.Distributed ImplementationThe pseudo-code in Figure 1 assumes that A can fitinto memory, which for large A may be impossible.Also, as each element of B is processed indepen-dently, running parallel processes for non-intersecting subsets of B makes the processingfaster.
In this section, we outline our MapReduceimplementation of Figure 1 deployed using Ha-doop1, the open-source software package imple-menting the MapReduce framework and distri-buted file system.
Hadoop has been shown to scaleto several thousands of machines, allowing users towrite simple ?map?
and ?reduce?
code, and toseamlessly manage the sophisticated parallel ex-ecution of the code.
A good primer on MapReduceprogramming is in (Dean and Ghemawat 2008).Our implementation employs the MapReducemodel by using the Map step to start M?N Maptasks in parallel, each caching 1/Mth part of A asan inverted index and streaming 1/Nth part of Bthrough it.
The actual inputs are read by the tasks1 Hadoop, http://lucene.apache.org/hadoop/directly from HDFS (Hadoop Distributed File Sys-tem).
Each part of A is processed N times, and eachpart of B is processed M times.
M is determined bythe amount of memory dedicated for the invertedindex, and N should be determined by trading offthe fact that as N increases, more parallelism canbe obtained at the increased cost of building thesame inverse index N times.The similarity algorithm from Figure 1 is run ineach task of the Map step of a MapReduce job.The Reduce step is used to group the output by bi.4 Application to Set ExpansionCreating lists of named entities is a critical prob-lem at commercial engines such as Yahoo!
andGoogle.
The types of entities to be expanded areoften not known a priori, leaving supervised clas-sifiers undesirable.
Additionally, list creators typi-cally need the ability to expand sets of varyinggranularity.
Semi-supervised approaches are pre-dominantly adopted since they allow targeted ex-pansions while requiring only small sets of seedentities.
State-of-the-art techniques first computeterm-term similarities for all available terms andthen select candidates for set expansion fromamongst the terms most similar to the seeds (Sar-mento et al 2007).Input: Two matrices A and B of feature vectors.## Build an inverted index for A (optimiza-## tion for data sparseness)AA = an empty hash-tablefor i in (1..n):F2[i] = f2(A[i]) ## cache values of f2(x)for k in non-zero features of A[i]:if k not in AA: AA[k] = empty-set## append <vector-id, feature-value>## pairs to the set of non-zero## values for feature kAA[k].append( (i,A[i,k]) )## Process the elements of Bfor b in B:F1 = {} ## the set of Ai that have non-zero similarity with bfor k in non-zero features of b:for i in AA[k]:if i not in sim: sim[i] = 0F1[i] += f1( AA[k][i], b[k])F3 = f3(b)for i in sim:print i, b, f0( F1[i], F2[i], F3)Output: A matrix containing the similarity betweenall elements in A and in B.Figure 1.
Similarity computation algorithm.941Formally, we define our expansion task as:Task Definition: Given a set of seed entities S ={s1, s2, ?, sk} of a class C = {s1, s2, ?, sk, ?,, sn} andan unlabeled textual corpus T, find all members ofthe class C.For example, consider the class of Bottled WaterBrands.
Given the set of seeds S = {Volvic, SanPellegrino, Gerolsteiner Brunnen, Bling H2O}, ourtask is to find all other members of this class, suchas {Agua Vida, Apenta, Culligan, Dasani, EthosWater, Iceland Pure Spring Water, Imsdal, ?
}4.1 Set Expansion AlgorithmOur goal is not to propose a new set expansion al-gorithm, but instead to test the effect of using ourWeb-scale term similarity matrix (enabled by thealgorithm proposed in Section 3) on a state-of-the-art distributional set expansion algorithm, namely(Sarmento et al 2007).We consider S as a set of prototypical examplesof the underlying entity set.
A representation forthe meaning of S is computed by building a featurevector consisting of a weighted average of the fea-tures of its seed elements s1, s2, ?, sk, a centroid.
Forexample, given the seed elements {Volvic, San Pel-legrino, Gerolsteiner Brunnen, Bling H2O}, theresulting centroid consists of (details of the featureextraction protocol are in Section 6.1):brand, mineral water, monitor,lake, water, take over, ?Centroids are represented in the same space asterms allowing us to compute the similarity be-tween centroids and all terms in our corpus.
Ascored and ranked set for expansion is ultimatelygenerated by sorting all terms according to theirsimilarity to the seed set centroid, and applying acutoff on either the similarity score or on the totalnumber of retrieved terms.
In our reported experi-ments, we expanded over 22,000 seed sets usingour Web similarity model from Section 3.5 Evaluation MethodologyIn this section, we describe our methodology forevaluating Web-scale set expansion.5.1 Gold Standard Entity SetsEstimating the quality of a set expansion algorithmrequires a random sample from the universe of allentity sets that may ever be expanded, where a setrepresents some concept such as Stage Actors.
Anapproximation of this universe can be extractedfrom the ?List of?
pages in Wikipedia2.Upon inspection of a random sample of the ?Listof?
pages, we found that several lists were compo-sitions or joins of concepts, for example ?List ofWorld War II aces from Denmark?
and ?List ofpeople who claimed to be God?.
We addressed thisissue by constructing a quasi-random sample asfollows.
We randomly sorted the list of every nounoccurring in Wikipedia2.
Then, for each noun weverified whether or not it existed in a Wikipedialist, and if so we extracted this list.
If a noun be-longed to multiple lists, the authors chose the listthat seemed most appropriate.
Although this doesnot generate a perfect random sample, diversity isensured by the random selection of nouns and rele-vancy is ensured by the author adjudication.The final gold standard consists of 50 sets, in-cluding: classical pianists, Spanish provinces,Texas counties, male tennis players, first ladies,cocktails, bottled water brands, and Archbishops ofCanterbury.
For each set, we then manuallyscraped every instance from Wikipedia keepingtrack also of the listed variants names.The gold standard is available for download at:http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=sse-gold/wikipedia.20071218.goldsets.tgzThe 50 sets consist on average of 208 instances(with a minimum of 11 and a maximum of 1,116)for a total of 10,377 instances.5.2 TrialsIn order to analyze the corpus and seed effects onperformance, we created 30 copies of each of the50 sets and randomly sorted each copy.
Then, foreach of the 1500 copies, we created a trial for eachof the following 23 seed sizes: 1, 2, 5, 10, 20, 30,40, ?, 200.
Each trial of seed size s was created bytaking the first s entries in each of the 1500 randomcopies.
For sets that contained fewer than 200items, we only generated trials for seed sizes2 In this paper, extractions from Wikipedia are takenfrom a snapshot of the resource in December 2008.942smaller than the set size.
The resulting trial datasetconsists of 20,220 trials3.5.3 JudgmentsSet expansion systems consist of an expansion al-gorithm (such as the one described in Section 4.1)as well as a corpus (such as Wikipedia, a newscorpus, or a web crawl).
For a given system, eachof the 20,220 trials described in the previous sec-tion are expanded.
In our work, we limited the totalnumber of system expansions, per trial, to 1000.Before judgment of an expanded set, we firstcollapse each instance that is a variant of another(determined using the variants in our gold stan-dard) into one single instance (keeping the highestsystem score)4.
Then, each expanded instance isjudged as correct or incorrect automaticallyagainst the gold standard described in Section 5.1.5.4 Analysis MetricsOur experiments in Section 6 consist of precisionvs.
recall or precision vs. rank curves, where:a) precision is defined as the percentage of correctinstances in the expansion of a seed set; andb) recall is defined as the percentage of non-seedgold standard instances retrieved by the system.Since the gold standard sets vary significantly insize, we also provide the R-precision metric tonormalize for set size:c) R-precision is defined as the average precisionof all trials where precision is taken at rank R ={size of trial?s associated gold standard set},thereby normalizing for set size.3 Available for download at http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=sse-gold/wikipedia.20071218.trials.tgz.4 Note also that we do not allow seed instances nor theirvariants to appear in an expansion set.For the above metrics, 95% confidence bounds arecomputed using the randomly generated samplesdescribed in Section 5.2.6 Experimental ResultsOur goal is to study the performance gains on setexpansion using our Web-scale term similarity al-gorithm from Section 3.
We present a large empir-ical study quantifying the importance of corpusand seeds on expansion accuracy.6.1 Experimental SetupWe extracted statistics to build our model fromSection 3 using four different corpora, outlined inTable 2.
The Wikipedia corpus consists of a snap-shot of the English articles in December 20085.The Web100 corpus consists of an extraction froma large crawl of the Web, from Yahoo!, of over600 million English webpages.
For each crawleddocument, we removed paragraphs containingfewer than 50 tokens (as a rough approximation ofthe narrative part of a webpage) and then removedall duplicate sentences.
The resulting corpus con-sists of over 200 billion words.
The Web020 cor-pus is a random sample of 1/5th of the sentences inWeb100 whereas Web004 is a random sample of1/25th of Web100.For each corpus, we tagged and chunked eachsentence as described in Section 3.
We then com-puted the similarity between all noun phrasechunks using the model of Section 3.1.6.2 Quantitative AnalysisOur proposed optimization for term similaritycomputation produces exact scores (unlike rando-mized techniques) for all pairs of terms on a largeWeb crawl.
For our largest corpus, Web100, wecomputed the pairwise similarity between over 500million words in 50 hours using 200 four-core ma-chines.
Web004 is of similar scale to the largestreported randomized technique (Ravichandran etal.
2005).
On this scale, we compute the exact si-milarity matrix in a little over two hours whereasRavichandran et al (2005) compute an approxima-tion in 570 hours.
On average they only find 73%5 To avoid biasing our Wikipedia corpus with the testsets, Wikipedia ?List of?
pages were omitted from ourstatistics as were any page linked to gold standard listmembers from ?List of?
pages.Table 2.
Corpora used to build our expansion models.CORPORAUNIQUESENTENCES(MILLIONS)TOKENS(MILLIONS)UNIQUEWORDS(MILLIONS)Web100 5,201 217,940 542Web020?
1040 43,588 108Web004?
208 8,717 22Wikipedia6 30 721 34?Estimated from Web100 statistics.943of the top-1000 similar terms of a random termwhereas we find all of them.For set expansion, experiments have been run oncorpora as large as Web004 and Wikipedia (Sar-mento et al 2007), a corpora 300 times smallerthan our Web crawl.
Below, we compare the ex-pansion accuracy of Sarmento et al (2007) on Wi-kipedia and our Web crawls.Figure 2 illustrates the precision and recall tra-deoff for our four corpora, with 95% confidenceintervals computed over all 20,220 trials describedin Section 4.2.
Table 3 lists the resulting R-precision along with the system precisions at ranks25, 50, and 100 (see Figure 2 for detailed precisionanalysis).
Why are the precision scores so low?Compared with previous work that manually selectentity types for expansion, such as countries andcompanies, our work is the first to evaluate over alarge set of randomly selected entity types.
On justthe countries class, our R-Precision was 0.816 us-ing Web100.The following sections analyze the effects ofvarious expansion variables: corpus size, corpusquality, seed size, and seed quality.6.2.1 Corpus Size and Corpus Quality EffectNot surprisingly, corpus size and quality have asignificant impact on expansion performance.
Fig-ure 2 and Table 3 quantify this expectation.
On ourWeb crawl corpora, we observe that the full 200+billion token crawl (Web100) has an average R-precision 13% higher than 1/5th of the crawl(Web020) and 53% higher than 1/25th of the crawl.Figure 2 also illustrates that throughout the fullprecision/recall curve, Web100 significantly out-performs Web020, which in turn significantly out-performs Web004.The higher text quality Wikipedia corpus, whichconsists of roughly 60 times fewer tokens thanWeb020, performs nearly as well as Web020 (seeFigure 2).
We omitted statistics from Wikipedia?List of?
pages in order to not bias our evaluationto the test set described in Section 5.1.
Inspectionof the precision vs. rank graph (omitted for lack ofspace) revealed that from rank 1 thru 550, Wikipe-dia had the same precision as Web020.
From rank550 to 1000, however, Wikipedia?s precisiondropped off significantly compared with Web020,accounting for the fact that the Web corpus con-tains a higher recall of gold standard instances.
TheR-precision reported in Table 3 shows that thisprecision drop-off results in a significantly lowerR-precision for Wikipedia compared with Web020.6.2.2  The Effect of Seed SelectionIntuitively, some seeds are better than others.
Westudy the impact of seed selection effect by in-specting the system performance for several ran-domly selected seed sets of fixed size and we findthat seed set composition greatly affects perfor-mance.
Figure 3 illustrates the precision vs. recalltradeoff on our best performing corpus Web100 for30 random seed sets of size 10 for each of our 50gold standard sets (i.e., 1500 trials were tested.
)Each of the trials performed better than the averagesystem performance (the double-lined curve lowestin Figure 3).
Distinguishing between the variousdata series is not important, however important tonotice is the very large gap between the preci-sion/recall curves of the best and worst performingrandom seed sets.
On average, the best performingseed sets had 42% higher precision and 39% higherrecall than the worst performing seed set.
SimilarTable 3.
Corpora analysis: R-precision and Precision at var-ious ranks.
95% confidence bounds are all below 0.005?.CORPORA R-PREC PREC@25 PREC@50 PREC@100Web100 0.404 0.407 0.347 0.278Web020 0.356 0.377 0.319 0.250Web004 0.264 0.353 0.298 0.239Wikipedia 0.315 0.372 0.314 0.253?95% confidence bounds are computed over all trials described in Section 5.2.Figure 2.
Corpus size and quality improve performance.00.10.20.30.40.50.60 0.1 0.2 0.3 0.4 0.5 0.6RecallPrecisionCorpora?Analysis(Precision?vs.
?Recall)Web100Web020Web004WikipediaCORPORA R-PREC PREC@25 PREC@50 PREC@100Web100 0.404 0.407 0.347 0.278Web020 0.356 0.377 0.319 0.250Web004 0.264 0.353 0.298 0.239Wikipedia 0.315 0.372 0.314 0.253?95% confidence bounds are computed over all trials described in Section 5.2.944curves were observed for inspected seed sets ofsize 5, 20, 30, and 40.Although outside of the scope of this paper, weare currently investigating ways to automaticallydetect which seed elements are better than others inorder to reduce the impact of seed selection effect.6.2.3 The Effect of Seed SizeHere we aim to confirm, with a large empiricalstudy, the anecdotal claims in (Pa?ca and Durme2008) that few seeds are necessary.
We found thata) very small seed sets of size 1 or 2 are not suffi-cient for representing the intended entity set; b) 5-20 seeds yield on average best performance; and c)surprisingly, increasing the seed set size beyond20 or 30 on average does not find any new correctinstances.We inspected the effect of seed size on R-precision over the four corpora.
Each seed sizecurve is computed by averaging the system per-formance over the 30 random trials of all 50 sets.For each corpus, R-precision increased sharplyfrom seed size 1 to 10 and the curve flattened outfor seed sizes larger than 20 (figure omitted forlack of space).
Error analysis on the Web100 cor-pus shows that once our model has seen 10-20seeds, the distributional similarity model seems tohave enough statistics to discover as many newcorrect instances as it could ever find.
Some enti-ties could never be found by the distributional si-milarity model since they either do not occur orinfrequently occur in the corpus or they occur incontexts that vary a great deal from other set ele-ments.
Figure 4 illustrates this behavior by plottingfor each seed set size the rate of increase in discov-ery of new correct instances (i.e., not found insmaller seed set sizes).We see that most gold standard instances arediscovered with the first 5-10 seeds.
After the 30thseed is introduced, no new correct instances arefound.
An important finding is that the error ratedoes not increase with increased seed set size (seeFigure 5).
This study shows that only few seeds(10-20) yield best performance and that addingmore seeds beyond this does not on average affectperformance in a positive or negative way.Figure 3.
Seed set composition greatly affects system performance (with 30 different seed samples of size 10).Figure 4.
Few new instances are discovered with morethan 5-20 seeds on Web100 (with 95% confidence).Figure 5.
Percentage of errors does not increase asseed size increases on Web100 (with 95% confidence).00.511.522.530 20 40 60 80 100 120 140 160 180 200Rate?of?New?Correct?Seed?SizeRate?of?New?Correct?Expansionsvs.
?Seed?Size00.20.40.60.810 20 40 60 80 100 120 140 160 180 200%?of?ErrorSeed?SizeSeed?Size?vs.
?%?of?Errors00.20.40.60.80 0.2 0.4 0.6 0.8 1RecallPrecisionWeb100:?Seed?Selection?EffectPrecision?vs.
?RecallWeb100 s010s010.t01 s010.t02s010.t03 s010.t04s010.t05 s010.t06s010.t07 s010.t08s010.t09 s010.t10s010.t11 s010.t12s010.t13 s010.t14s010.t15 s010.t16s010.t17 s010.t18s010.t19 s010.t20s010.t21 s010.t22s010.t23 s010.t24s010.t25 s010.t26s010.t27 s010.t28s010.t29 s010.t309457 ConclusionWe proposed a highly scalable term similarity al-gorithm, implemented in the MapReduce frame-work, and deployed over a 200 billion word crawlof the Web.
The pairwise similarity between 500million terms was computed in 50 hours using 200quad-core nodes.
We evaluated the impact of thelarge similarity matrix on a set expansion task andfound that the Web similarity matrix gave a largeperformance boost over a state-of-the-art expan-sion algorithm using Wikipedia.
Finally, we re-lease to the community a testbed for experimental-ly analyzing automatic set expansion, which in-cludes a large collection of nearly random entitysets extracted from Wikipedia and over 22,000randomly sampled seed expansion trials.ReferencesAbney, S. Parsing by Chunks.
In: Robert Berwick, Ste-ven Abney and Carol Tenny (eds.
), Principle-BasedParsing.
Kluwer Academic Publishers, Dordrecht.1991.Agirre, E.; Alfonseca, E.; Hall, K.; Kravalova, J.; Pasca,M.
; and Soroa, A.. 2009.
A Study on Similarity andRelatedness Using Distributional and WordNet-basedApproaches.
In Proceedings of NAACL HLT 09.Ando, R. K. 2000.
Latent semantic space: Iterative scal-ing improves precision of interdocument similaritymeasurement.
In Proceedings of SIGIR-00.
pp.
216?223.Atterer, M. and Schutze, H., 2006.
The Effect of CorpusSize when Combining Supervised and UnsupervisedTraining for Disambiguation.
In Proceedings of ACL-06.Banko, M. and Brill, E. 2001.
Mitigating the paucity ofdata problem.
In Proceedings of HLT-2001.
San Di-ego, CA.Banko, M.; Cafarella, M.; Soderland, S.; Broadhead, M.;Etzioni, O.
2007.
Open Information Extraction fromthe Web.
In Proceedings of IJCAI.Bayardo, R. J.; Ma, Y.; Srikant, R. 2007.
Scaling UpAll-Pairs Similarity Search.
In Proceedings of WWW-07.
pp.
131-140.
Banff, Canada.Blei, D. M.; Ng, A. Y.; and Jordan, M. I.
2003.
LatentDirichlet Allocation.
Journal of Machine LearningResearch, 3:993?1022.Brill, E. 1995.
Transformation-Based Error-DrivenLearning and Natural Language Processing: A CaseStudy in Part of Speech Tagging.
ComputationalLinguistics.Broder, A.
1997.
On the resemblance and containmentof documents.
In Compression and Complexity ofSequences.
pp.
21-29.Bunescu, R. and Mooney, R. 2004 Collective Informa-tion Extraction with Relational Markov Networks.
InProceedings of ACL-04, pp.
438-445.Cao, H.; Jiang, D.; Pei, J.; He, Q.; Liao, Z.; Chen, E.;and Li, H. 2008.
Context-aware query suggestion bymining click-through and session data.
In Proceed-ings of KDD-08.
pp.
875?883.Chang, W.; Pantel, P.; Popescu, A.-M.; and Gabrilovich,E.
2009.
Towards intent-driven bidterm suggestion.In Proceedings of WWW-09 (Short Paper), Madrid,Spain.Church, K. and Hanks, P. 1989.
Word associationnorms, mutual information, and lexicography.
InProceedings of ACL89.
pp.
76?83.Dean, J. and Ghemawat, S. 2008.
MapReduce: Simpli-fied Data Processing on Large Clusters.
Communica-tions of the ACM, 51(1):107-113.Deerwester, S. C.; Dumais, S. T.; Landauer, T. K.; Fur-nas, G. W.; and Harshman, R. A.
1990.
Indexing bylatent semantic analysis.
Journal of the American So-ciety for Information Science, 41(6):391?407.Downey, D.; Broadhead, M; Etzioni, O.
2007.
LocatingComplex Named Entities in Web Text.
In Proceed-ings of IJCAI-07.Elsayed, T.; Lin, J.; Oard, D. 2008.
Pairwise DocumentSimilarity in Large Collections with MapReduce.
InProceedings of ACL-08: HLT, Short Papers (Com-panion Volume).
pp.
265?268.
Columbus, OH.Erk, K. 2007.
A simple, similarity-based model for se-lectional preferences.
In Proceedings of ACL-07.
pp.216?223.
Prague, Czech Republic.Erk, K. and Pad?, S. 2008.
A structured vector spacemodel for word meaning in context.
In Proceedingsof EMNLP-08.
Honolulu, HI.Etzioni, O.; Cafarella, M.; Downey.
D.; Popescu, A.;Shaked, T; Soderland, S.; Weld, D.; Yates, A.
2005.Unsupervised named-entity extraction from the Web:An Experimental Study.
In Artificial Intelligence,165(1):91-134.Gorman, J. and Curran, J. R. 2006.
Scaling distribution-al similarity to large corpora.
In Proceedings of ACL-06.
pp.
361-368.946Harris, Z.
1985.
Distributional Structure.
In: Katz, J.
J.(ed.
), The Philosophy of Linguistics.
New York: Ox-ford University Press.
pp.
26-47.Hindle, D. 1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL-90.
pp.268?275.
Pittsburgh, PA.Hofmann, T. 1999.
Probabilistic Latent Semantic Index-ing.
In Proceedings of SIGIR-99.
pp.
50?57, Berke-ley, California.Kanerva, P. 1993.
Sparse distributed memory and re-lated models.
pp.
50-76.Lapata, M. and Keller, F., 2005.
Web-based Models forNatural Language Processing, In ACM Transactionson Speech and Language Processing (TSLP), 2(1).Lee, Lillian.
1999.
Measures of Distributional Similarity.In Proceedings of ACL-93.
pp.
25-32.
College Park,MD.Lin, D. 1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING/ACL-98.pp.
768?774.
Montreal, Canada.Lund, K., and Burgess, C. 1996.
Producing high-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments,and Computers, 28(2):203?208.McCallum, A. and Li, W. Early Results for NamedEntity Recognition with Conditional Random Fields,Feature Induction and Enhanced Lexicons.
In Pro-ceedings of CoNLL-03.McQueen, J.
1967.
Some methods for classification andanalysis of multivariate observations.
In Proceedingsof 5th Berkeley Symposium on Mathematics, Statisticsand Probability, 1:281?298.Pa?ca, M. 2007a.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of CIKM-07.
pp.
683-690.Pa?ca, M. 2007b.
Organizing and Searching the WorldWide Web of Facts ?
Step Two: Harnessing the Wis-dom of the Crowds.
In Proceedings of WWW-07.Pa?ca, M. and Durme, B.J.
2008.
Weakly-supervisedAcquisition of Open-Domain Classes and ClassAttributes from Web Documents and Query Logs.
InProceedings of ACL-08.Pa?ca, M.; Lin, D.; Bigham, J.; Lifchits, A.; Jain, A.2006.
Names and Similarities on the Web: Fast Ex-traction in the Fast Lane.
In Proceedings of ACL-2006.
pp.
113-120.Pantel, P. and Lin, D. 2002.
Discovering Word Sensesfrom Text.
In Proceedings of KDD-02.
pp.
613-619.Edmonton, Canada.Pantel, P.; Ravichandran, D; Hovy, E.H. 2004.
Towardsterascale knowledge acquisition.
In proceedings ofCOLING-04.
pp 771-777.Ravichandran, D.; Pantel, P.; and Hovy, E. 2005.
Ran-domized algorithms and NLP: Using locality sensi-tive hash function for high speed noun clustering.
InProceedings of ACL-05.
pp.
622-629.Riloff, E. and Jones, R. 1999 Learning Dictionaries forInformation Extraction by Multi-Level Boostrapping.In Proceedings of AAAI/IAAAI-99.Riloff, E. and Shepherd, J.
1997.
A corpus-based ap-proach for building semantic lexicons.
In Proceed-ings of EMNLP-97.Rychl?, P. and Kilgarriff, A.
2007.
An efficient algo-rithm for building a distributional thesaurus (and oth-er Sketch Engine developments).
In Proceedings ofACL-07, demo sessions.
Prague, Czech Republic.Sarawagi, S. and Kirpal, A.
2004.
Efficient set joins onsimilarity predicates.
In Proceedings of SIGMOD '04.pp.
74 ?754.
New York, NY.Sarmento, L.; Jijkuon, V.; de Rijke, M.; and Oliveira, E.2007.
?More like these?
: growing entity classes fromseeds.
In Proceedings of CIKM-07.
pp.
959-962.
Lis-bon, Portugal.Turney, P. D., and Littman, M. L. 2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems, 21(4).Wang, R.C.
and Cohen, W.W. 2008.
Iterative Set Ex-pansion of Named Entities using the Web.
In Pro-ceedings of ICDM 2008.
Pisa, Italy.Wang.
R.C.
and Cohen, W.W. 2007 Language-Independent Set Expansion of Named Entities Usingthe Web.
In Proceedings of ICDM-07.Yuret, D., and Yatbaz, M. A.
2009.
The noisy channelmodel for unsupervised word sense disambiguation.Computational Linguistics.
Under review.947
