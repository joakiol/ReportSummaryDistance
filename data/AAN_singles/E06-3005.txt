Developing an approach for why-question answeringSuzan VerberneDept.
of Language and SpeechRadboud University Nijmegens.verberne@let.ru.nlAbstractIn the current project, we aim atdeveloping an approach for automaticallyanswering why-questions.
We created adata collection for research, developmentand evaluation of a method forautomatically answering why-questions(why-QA) The resulting collectioncomprises 395 why-questions.
For eachquestion, the source document and one ortwo user-formulated answers areavailable in the data set.
The resultingdata set is of importance for our researchand it will contribute to and stimulateother research in the field of why-QA.We developed a question analysismethod for why-questions, based onsyntactic categorization and answer typedetermination.
The quality of the outputof this module is promising for futuredevelopment of our method for why-QA.1 IntroductionUntil now, research in the field of automaticquestion answering (QA) has focused on factoid(closed-class) questions like who, what, whereand when questions.
Results reported for the QAtrack of the Text Retrieval Conference (TREC)show that these types of wh-questions can behandled rather successfully (Voorhees 2003).In the current project, we aim at developing anapproach for automatically answering why-questions.
So far, why-questions have largelybeen ignored by researchers in the QA field.
Onereason for this is that the frequency of why-questions in a QA context is lower than that ofother questions like who- and what-questions(Hovy et al, 2002a).
However, although why-questions are less frequent than some types offactoids (who, what and where), their frequencyis not negligible: in a QA context, they compriseabout 5 percent of all wh-questions (Hovy, 2001;Jijkoun, 2005) and they do have relevance in QAapplications (Maybury, 2002).
A second reasonfor ignoring why-questions until now, is that ithas been suggested that the techniques that haveproven to be successful in QA for closed-classquestions are not suitable for questions thatexpect a procedural answer rather than a nounphrase (Kupiec, 1999).
The current paper aims tofind out whether the suggestion is true thatfactoid-QA techniques are not suitable for why-QA.
We want to investigate whether principledsyntactic parsing can make QA for why-questions feasible.In the present paper, we report on the work thathas been carried out until now.
Morespecifically, sections 2 and 3 describe theapproach taken to data collection and questionanalysis and the results that were obtained.
Then,in section 4, we discuss the plans and goals forthe work that will be carried out in the remainderof the project.2 Data for why-QAIn research in the field of QA, data sources ofquestions and answers play an important role.Appropriate data collections are necessary for thedevelopment and evaluation of QA systems(Voorhees, 2000).
While in the context of theQA track of TREC data collections in support offactoid questions have been created, so far, noresources have been created for why-QA.
For thepurpose of the present research therefore, wehave developed a data collection comprising aset of questions and corresponding answers.
Indoing so, we have extended the time testedprocedures previously developed in the TRECcontext.In this section, we describe the requirementsthat a data set must meet to be appropriate fordevelopment and we discuss a number ofexisting sources of why-questions.
Then wedescribe the method employed for data collection39and the main characteristics of the resulting dataset.The first requirement for an appropriate data setconcerns the nature of the questions.
In thecontext of the current research, a why-question isdefined as an interrogative sentence in which theinterrogative adverb why (or one of itssynonyms) occurs in (near) initial position.
Weconsider the subset of why-questions that couldbe posed in a QA context and for which theanswer is known to be present in the relateddocument set.
This means that the data set shouldonly comprise why-questions for which theanswer can be found in a fixed collection ofdocuments.
Secondly, the data set should notonly contain questions, but also thecorresponding answers and source documents.The answer to a why-question is a clause orsentence (or a small number of coherentsentences) that answers the question withoutgiving supplementary context.
The answer is notliterally present in the source document, but canbe deduced from it.
For example, a possibleanswer to the question Why are 4300 additionalteachers required?, based on the source snippetThe school population is due to rise by 74,000,which would require recruitment of an additional4,300 teachers, is Because the school populationis due to rise by a further 74,000.Finally, the size of the data set should be largeenough to cover all relevant variation that occurin why-questions in a QA context.There are a number of existing sources ofwhy-questions that we may consider for use inour research.
However, for various reasons, noneof these appear suitable.Why-questions from corpora like the BritishNational Corpus (BNC, 2002), in whichquestions typically occur in spoken dialogues,are not suitable because the answers are notstructurally available with the questions, or theyare not extractable from a document that hasbeen linked to the question.
The same holds forthe data collected for the Webclopedia project(Hovy et al, 2002a), in which neither theanswers nor the source documents wereincluded.
One could also consider questions andanswers from frequently asked questions (FAQ)pages, like the large data set collected byValentin Jijkoun (Jijkoun, 2005).
However, inFAQ lists, there is no clear distinction betweenthe answer itself (a clause that answers thequestion) and the source document that containsthe answer.The questions in the test collections from theTREC-QA track do contain links to the possibleanswers and the corresponding sourcedocuments.
However, these collections containtoo few why-questions to qualify as a data setthat is appropriate for developing why-QA.Given the lack of available data that match ourrequirements, a new data set for QA researchinto why-questions had to be compiled.
In orderto meet the given requirements, it would be bestto collect questions posed in an operational QAenvironment, like the compilers of the TREC-QA test collections did: they extracted factoidand definition questions from search logsdonated by Microsoft and AOL (TREC, 2003).Since we do not have access to comparablesources, it was decided to revert to the procedureused in earlier TRECs, and imitate a QAenvironment in an elicitation experiment.
Weextended the conventional procedure bycollecting user-formulated answers in order toinvestigate the range of possible answers to eachquestion.
We also added paraphrases of collectedquestions in order to extend the syntactic andlexical variation in the data collection.In the elicitation experiment, ten nativespeakers of English were asked to read five textsfrom Reuters?
Textline Global News (1989) andfive texts from The Guardian on CD-ROM(1992).
The texts were around 500 words each.The experiment was conducted over the Internet,using a web form and some CGI scripts.
In orderto have good control over the experiment, weregistered all participants and gave them a codefor logging in on the web site.
Every time aparticipant logged in, the first upcoming text thathe or she did not yet finish was presented.
Theparticipant was asked to formulate one to sixwhy-questions for this text, and to formulate ananswer to each of these questions.
Theparticipants were explicitly told that it wasessential that the answers to their questions couldbe found in the text.
After submitting the form,the participant was presented the questions posedby one of the other participants and he or she wasasked to formulate an answer to these questionstoo.
The collected data was saved in text format,grouped per participant and per sourcedocument, so that the source information isavailable for each question.
The answers havebeen linked to the questions.In this experiment, 395 questions and 769corresponding answers were collected.
Thenumber of answers would have been twice the40number of questions if all participants wouldhave been able to answer all questions that wereposed by another participant.
However, for 21questions (5.3%), the second participant was notable to answer the first participant?s question.Note that not every question in the elicitationdata set has a unique topic1: on average, 38questions were formulated per text, coveringaround twenty topics per text.The collected questions have been formulatedby people who had constant access to the sourcetext.
As a result of that, the chosen formulationsoften resemble the original text, both in the useof vocabulary and sentence structure.
In order toexpand the dataset, a second elicitationexperiment was set up, in which five participantsfrom the first experiment were asked toparaphrase some of the original why-questions.The 166 unique questions were randomlyselected from the original data set.
Theparticipants formulated 211 paraphrases in totalfor these questions.
This means that somequestions have more than one paraphrase.
Theparaphrases were saved in a text file that includesthe corresponding original questions and thecorresponding source documents.We studied the types of variation that occuramong questions covering the same topic.
First,we collected the types of variation that occur inthe original data set and then we compared theseto the variation types that occur in the set ofparaphrases.In the original data set, the following types ofvariation occur between different questions onthe same topic:Lexical variation, e.g.for the second year running vs.again;Verb tense variation, e.g.have risen vs. have been rising;Optional constituents variation, e.g.class sizes vs. class sizes inEngland and Wales;Sentence structure variation, e.g.would require recruitment vs.need to be recruitedIn the set of paraphrases, the same types ofvariation occur, but as expected the differencesbetween the paraphrases and the source1 The topic of a why-question is the proposition that isquestioned.
A why-question has the form ?WHY P?, inwhich P is the topic.sentences are slightly bigger than the differencesbetween the original questions and the sourcesentences.
We measured the lexical overlapbetween the questions and the source texts as thenumber of content words that are in both thequestion and the source text.
The average relativelexical overlap (the number of overlapping wordsdivided by the total number of words in thequestion) between original questions and sourcetext is 0.35; the average relative lexical overlapbetween paraphrases and source text is 0.31.The size of the resulting collection (395 originalquestions, 769 answers, and 211 paraphrases ofquestions) is large enough to initiate seriousresearch into the development of why-QA.Our collection meets the requirements thatwere formulated with regard to the nature of thequestions and the presence of the answers andsource documents for every question.3 Question analysis for why-QAThe goal of question analysis is to create arepresentation of the user?s information need.The result of question analysis is a query thatcontains all information about the answer thatcan be extracted from the question.
So far, noquestion analysis procedures have been createdfor why-QA specifically.
Therefore, we havedeveloped an approach for proper analysis ofwhy-questions.
Our approach is based on existingmethods of analysis of factoid questions.
Thiswill allow us to verify whether methods used inhandling factoid questions are suitable for usewith procedural questions.
In this section, wedescribe the components of successful methodsfor the analysis of factoid questions.
Then wepresent the method that we used for the analysisof why-questions and indicate the quality of ourmethod.The first (and most simple) component in currentmethods for question analysis is keywordextraction.
Lexical items in the question giveinformation on the topic of the user?sinformation need.
In keyword selection, severaldifferent approaches may be followed.
Moldovanet al (2000), for instance, select as keywords allnamed entities that were recognized as propernouns.
In almost all approaches to keywordextraction, syntax plays a role.
Shallow parsingis used for extracting noun phrases, which areconsidered to be relevant key phrases in theretrieval step.
Based on the query?s keywords,41one or more documents or paragraphs can beretrieved that may possibly contain the answer.A second, very important, component inquestion analysis is determination of thequestion?s semantic answer type.
The answertype of a question defines the type of answer thatthe system should look for.
Often-cited work onquestion analysis has been done by Moldovan etal.
(1999, 2000), Hovy et al (2001), and Ferret etal.
(2002).
They all describe question analysismethods that classify questions with respect totheir answer type.
In their systems for factoid-QA, the answer type is generally deduceddirectly from the question word (who, when,where, etc.
): who leads to the answer typeperson; where leads to the answer type place,etc.
This information helps the system in thesearch for candidate answers to the question.Hovy et al find that, of the question analysiscomponents used by their system, thedetermination of the semantic answer type makesby far the largest contribution to the performanceof the entire QA system.For determining the answer type, syntacticanalysis may play a role.
When implementing asyntactic analysis module in a working QAsystem, the analysis has to be performed fullyautomatically.
This may lead to concessions withregard to either the degree of detail or the qualityof the analysis.
Ferret et al implement asyntactic analysis component based on shallowparsing.
Their syntactic analysis module yields asyntactic category for each input question.
Intheir system, a syntactic category is a specificsyntactic pattern, such as ?WhatDoNP?
(e.g.What does a defibrillator do?)
or?WhenBePNborn?
(e.g.
When was Rosa Parkborn?).
They define 80 syntactic categories likethese.
Each input question is parsed by a shallowparser and hand-written rules are applied fordetermining the syntactic category.
Ferret et alfind that the syntactic pattern helps indetermining the semantic answer type (e.g.company, person, date).
They unfortunately donot describe how they created the mappingbetween syntactic categories and answer types.As explained above, determination of thesemantic answer type is the most important taskof existing question analysis methods.
Therefore,the goal of our question analysis method is topredict the answer type of why-questions.In the work of Moldovan et al (2000), allwhy-questions share the single answer typereason.
However, we believe that it is necessaryto split this answer type into sub-types, because amore specific answer type helps the systemselect potential answer sentences or paragraphs.The idea behind this is that every sub-type has itsown lexical and syntactic cues in a source text.Based on the classification of adverbialclauses by Quirk (1985:15.45), we distinguishthe following sub-types of reason: cause,motivation, circumstance (which combinesreason with conditionality), and purpose.Below, an example of each of these answertypes is given.Cause:The flowers got dry because ithadn?t rained in a month.Motivation:I water the flowers because Idon?t like to see them dry.Circumstance:Seeing that it is only three,we should be able to finishthis today.Purpose:People have eyebrows to preventsweat running into their eyes.The why-questions that correspond to the reasonclauses above are respectively Why did theflowers get dry?, Why do you water the flowers?,Why should we be able to finish this today?, andWhy do people have eyebrows?.
It is not alwayspossible to assign one of the four answer sub-types to a why-question.
We will come back tothis later.Often, the question gives information on theexpected answer type.
For example, compare thetwo questions below:Why did McDonald's write Mr.Bocuse a letter?Why have class sizes risen?Someone asking the former question expects asan answer McDonald?s motivation for writing aletter, whereas someone asking the latterquestion expects the cause for rising class sizesas answer.The corresponding answer paragraphs doindeed contain the equivalent answer sub-types:McDonald's has acknowledgedthat a serious mistake wasmade.
"We have written toapologise and we hope to reach42a settlement with Mr. Bocusethis week," said Marie-PierreLahaye, a spokeswoman forMcDonald's France, whichoperates 193 restaurants.Class sizes in schools inEngland and Wales have risenfor the second year running,according to figures releasedtoday by the Council of LocalEducation Authorities.
Thefigures indicate that althoughthe number of pupils in schoolshas risen in the last year bymore than 46,000, the number ofteachers fell by 3,600.We aim at creating a question analysis modulethat is able to predict the expected answer type ofan input question.
In the analysis of factoidquestions, the question word often gives theneeded information on the expected answer type.In case of why, the question word does not giveinformation on the answer type since all why-questions have why as question word.
Thismeans that other information from the question isneeded for determining the answer sub-type.We decided to use Ferret?s approach, in whichsyntactic categorization helps in determining theexpected answer type.
In our question analysismodule, the TOSCA (TOols for SyntacticCorpus Analysis) system (Oostdijk, 1996) isexplored for syntactic analysis.
TOSCA?ssyntactic parser takes a sequence ofunambiguously tagged words and assignsfunction and category information to allconstituents in the sentence.
The parser yieldsone or more possible output trees for (almost) allinput sentences.
For the purpose of evaluatingthe maximum contribution to a classificationmethod that can be obtained from a principledsyntactic analysis, the most plausible parse treefrom the parser?s output is selected manually.For the next step of question analysis, wecreated a set of hand-written rules, which areapplied to the parse tree in order to choose thequestion?s syntactic category.
We defined sixsyntactic categories for this purpose:Action questions, e.g.Why did McDonald's write Mr.Bocuse a letter?Process questions, e.g.Why has Dixville grown famoussince 1964?Intensive complementation questions, e.g.Why is Microsoft Windows asuccess?Monotransitive have questions, e.g.Why did compilers of the OEDhave an easier time?Existential there questions, e.g.Why is there a debate aboutclass sizes?Declarative layer questions, e.g.Why does McDonald's spokeswomanthink the mistake was made?The choice for these categories is based theinformation that is available from the parser, andthe information that is needed for determiningthe answer type.For some categories, the question analysismodule only needs fairly simple cues forchoosing a category.
For example, a main verbwith the feature intens leads to the category?intensive complementation question?
and thepresence of the word there with the syntacticcategory EXT leads to the category ?existentialthere question?.
For deciding on declarative layerquestions, action questions and processquestions, complementary lexical-semanticinformation is needed.
In order to decide whetherthe question contains a declarative layer, themodule checks whether the main verb is in a listthat corresponds to the union of the verb classessay and declare from Verbnet (Kipper et al,2000), and whether it has a clausal object.
Thedistinction between action and process questionsis made by looking up the main verb in a list ofprocess verbs.
This list contains the 529 verbsfrom the causative/inchoative alternation class(verbs like melt and grow) from the Levin verbindex (Levin, 1993); in an intransitive context,these verbs are process verbs.We have not yet developed an approach forpassive questions.Based on the syntactic category, the questionanalysis module tries to determine the answertype.
Some of the syntactic categories leaddirectly to an answer type.
All process questionswith non-agentive subjects get the expectedanswer type cause.
All action questions withagentive subjects get the answer type motivation.We extracted information on agentive and non-agentive nouns from WordNet: all nouns that arein the lexicographer file noun.person wereselected as agentive.Other syntactic categories need further analysis.Questions with a declarative layer, for example,43are ambiguous.
The question Why did they saythat migration occurs?
can be interpreted in twoways: Why did they say it?
or Why doesmigration occur?.
Before deciding on the answertype, our question analysis module tries to findout which of these two questions is supposed tobe answered.
In other words: the module decideswhich of the clauses has the question focus.
Thisdecision is made on the basis of the semantics ofthe declarative verb.
If the declarative is a factiveverb ?
a verb that presupposes the truth of itscomplements ?
like know, the module decidesthat the main clause has the focus.
The questionconsequently gets the answer type motivation.
Incase of a non-factive verb like think, the focus isexpected to be on the subordinate clause.
Inorder to predict the answer type of the question,the subordinate clause is then treated the sameway as the complete question was.
For example,consider the question Why do the school councilsbelieve that class sizes will grow even more?.Since the declarative (believe) is non-factive, thequestion analysis module determines the answertype for the subordinate clause (class sizes willgrow even more), which is cause, and assigns itto the question as a whole.Special attention is also paid to questions with amodal auxiliary.
Modal auxiliaries like can andshould, have an influence on the answer type.For example, consider the questions below, inwhich the only difference is the presence orabsence of the modal auxiliary can:Why did McDonalds not useactors to portray chefs inamusing situations?Why can McDonalds not useactors to portray chefs inamusing situations?The former question expects a motivation asanswer, whereas the latter question expects acause.
We implemented this difference in ourquestion analysis module: CAN (can, could) andHAVE TO (have to, has to, had to) lead to theanswer type cause.
Furthermore, the modalauxiliary SHALL (shall, should) changes theexpected answer type to motivation.When choosing an answer type, our questionanalysis module follows a conservative policy: incase of doubt, no answer type is assigned.We did not yet perform a complete evaluation ofour question analysis module.
For properevaluation of the module, we need a reference setof questions and answers that is different fromthe data set that we collected for development ofour system.
Moreover, for evaluating therelevance of our question analysis module foranswer retrieval, further development of ourapproach is needed.However, to have a general idea of theperformance of our method for answer typedetermination, we compared the output of themodule to manual classifications.
We performedthese reference classifications ourselves.First, we manually classified 130 why-questions from our development set with respectto their syntactic category.
Evaluation of thesyntactic categorization is straightforward: 95percent of why-questions got assigned the correctsyntactic category using ?perfect?
parse trees.The erroneous classifications were due todifferences in the definitions of the specific verbtypes.
For example, argue is not in the list ofdeclarative verbs, as a result of which a questionwith argue as main verb is classified as actionquestion instead of declarative layer question.Also, die and cause are not in the list of processverbs, so questions with either of these verbs asmain verb are labeled as action questions insteadof process questions.Secondly, we performed a manual classificationinto the four answer sub-types (cause,motivation, circumstance and purpose).
For thisclassification, we used the same set of 130questions as we did for the syntacticcategorization, combined with the correspondinganswers.
Again, we performed this classificationourselves.During the manual classification, we assignedthe answer type cause to 23.3 percent of thequestions and motivation to 40.3 percent.
Wewere not able to assign an answer sub-type to theremaining pairs (36.4 percent).
These questionsare in the broader class reason and not in one ofthe specific sub-classes None of the question-answer pairs was classified as circumstance orpurpose.
Descriptions of purpose are very rare innews texts because of their generic character(e.g.
People have eyebrows to prevent sweatrunning into their eyes).
The answer typecircumstance, defined by Quirk (cf.
section15.45) as a combination of reason withconditionality, is also rare as well as difficult torecognize.For evaluation of the question analysismodule, we mainly considered the questions that44did get assigned a sub-type (motivation or cause)in the manual classification.
Our questionanalysis module succeeded in assigning thecorrect answer sub-type to 62.2 percent of thesequestions, the wrong sub-type to 2.4 percent, andno sub-type to the other 35.4 percent.
The set ofquestions that did not get a sub-type from ourquestion analysis module can be divided in fourgroups:(a) Action questions for which the subject wasincorrectly not marked as agentive (mostlybecause it was an agentive organization likeMcDonald?s, or a proper noun that was not inWordNet?s list of nouns denoting persons, likeHenk Draijen);(b) questions with an action verb as main verbbut a non-agentive subject (e.g.
Why willrestrictions on abortion damage women'shealth?
);(c) passive questions, for which we have notyet developed an approach (e.g.
Why was theSupreme Court reopened?
);(d) Monotransitive have questions.
Thiscategory contains too few questions to formulatea general rule.Group (a), which is by far the largest of thesefour (covering half of the questions without sub-type), can be reduced by expanding the list ofagentive nouns, especially with names oforganizations.
For groups (c) and (d), generalrules may possibly be created in a later stage.With this knowledge, we are confident that wecan reduce the number of questions without sub-type in the output of our question analysismodule.These first results predict that it is possible toreach a relatively high precision in answer typedetermination.
(Only 2 percent of questions gotassigned a wrong sub-type.)
A high precisionmakes the question analysis output useful andreliable in the next steps of the questionanswering process.
On the other hand, it seemsdifficult to get a high recall.
In this test, only62.2 percent of the questions that were assignedan answer type in the reference set, was assignedan answer type by the system ?
this is 39.6percent of the total.4 Conclusions and further researchWe created a data collection for research intowhy-questions and for development of a methodfor why-QA.
The collection comprises asufficient amount of why-questions.
For eachquestion, the source document and one or twouser-formulated answers are available in the dataset.
The resulting data set is of importance forour research as well as other research in the fieldof why-QA.We developed a question analysis method forwhy-questions, based on syntactic categorizationand answer type determination.
In-depthevaluation of this module will be performed in alater stage, when the other parts of our QAapproach have been developed, and a test set hasbeen collected.
We believe that the first testresults, which show a high precision and lowrecall, are promising for future development ofour method for why-QA.We think that, just as for factoid-QA, answertype determination can play an important role inquestion analysis for why-questions.
Therefore,Kupiec?
suggestion that conventional questionanalysis techniques are not suitable for why-QAcan be made more precise by saying that thesemethods may be useful for a (potentially small)subset of why-questions.
The issue of recall, bothfor human and machine processing, needs furtheranalysis.In the near future, our work will focus ondevelopment of the next part of our approach forwhy-QA.Until now we have focused on the first of foursub-tasks in QA, viz.
(1) question analysis (2)retrieval of candidate paragraphs; (3) paragraphanalysis and selection; and (4) answergeneration.
Of the remaining three sub-tasks, wewill focus on paragraph analysis (3).
In order toclarify the relevance of the paragraph analysisstep, let us briefly discuss the QA-processes thatfollows question analysis.The retrieval module, which comes directlyafter the question analysis module, uses theoutput of the question analysis module forfinding candidate answer paragraphs (ordocuments).
Paragraph retrieval can bestraightforward: in existing approaches forfactoid-QA, candidate paragraphs are selectedbased on keyword matching only.
For the currentresearch, we do not aim at creating our ownparagraph selection technique.More interesting than paragraph retrieval isthe next step of QA: paragraph analysis.
Theparagraph analysis module tries to determinewhether the candidate paragraphs containpotential answers.
In case of who-questions,noun phrases denoting persons are potential45answers; in case of why-questions, reasons arepotential answers.
In the paragraph analysisstage, our answer sub-types come into play.
Thequestion analysis module determines the answertype for the input question, which is motivation,cause, purpose, or circumstance.
The paragraphanalysis module uses this information forsearching candidate answers in a paragraph.
Ashas been said before, the procedure for assigningthe correct sub-type needs further investigationin order to increase the coverage and thecontribution that answer sub-type classificationcan make to the performance of why-questionanswering.Once the system has extracted potentialanswers from one or more paragraphs with thesame topic as the question, the eventual answerhas to be delimited and reformulated ifnecessary.ReferencesBritish National Corpus, 2002.
The BNC Sampler.Oxford University Computing Services.Fellbaum, C.
(Ed.
), 1998.
WordNet: An ElectronicLexical Database.
Cambridge, Mass.
: MIT Press.Ferret O., Grau B., Hurault-Plantet M., Illouz G.,Monceaux L., Robba I., and Vilnat A., 2002.Finding An Answer Based on the Recognition ofthe Question Focus.
In Proceedings of The TenthText REtrieval Conference (TREC 2001).Gaithersburg, Maryland: NIST Special PublicationSP 500-250.Hovy, E.H., Gerber, L., Hermjakob, U., Lin, C-J, andRavichandran, D., 2001.
Toward Semantics-BasedAnswer Pinpointing.
In Proceedings of the DARPAHuman Language Technology Conference (HLT).San Diego, CAHovy, E.H., Hermjakob, U., and Ravichandran, D.,2002a.
A Question/Answer Typology with SurfaceText Patterns.
In Proceedings of the HumanLanguage Technology conference (HLT).
SanDiego, CA.Jijkoun, V. and De Rijke, M., 2005.
RetrievingAnswers from Frequently Asked Questions Pageson the Web.
In: Proceedings CIKM-2005, toappear.Kipper, K., Trang Dang, H., and Palmer, M., 2000.Class-Based Construction of a Verb Lexicon.AAAI-2000 Seventeenth National Conference onArtificial Intelligence, Austin, TX.Kupiec, J.M., 1999.
MURAX: Finding andOrganizing Answers from Text Search.
InStrzalkowski, T.
(ed.)
Natural LanguageInformation Retrieval.
311-332.
Dordrecht,Netherlands: Kluwer Academic.Levin, B., 1993.
English Verb Classes andAlternations - A Preliminary Investigation.
TheUniversity of Chicago Press.Litkowski, K. C., 1998.
Analysis of SubordinatingConjunctions, CL Research Technical Report 98-01 (Draft).
CL Research, Gaithersburg, MD.Maybury , M., 2003.
Toward a Question AnsweringRoadmap.
In New Directions in QuestionAnswering 2003: 8-11Moldovan, D., S., Harabagiu, M., Pas?a, R.,Mihalcea, R., G?rju, R., Goodrum, R., and Rus, V.1999.
Lasso: A Tool for Surfing the Answer Net.175-184.
In E. Voorhees and D. Harman (Eds.
),NIST Special Publication 500-246.
The Eight TextREtrieval Conference.
Dept.
of Commerce, NIST.Moldovan, D., S., Harabagiu, M., Pas?a, R.,Mihalcea, R., G?rju, R., Goodrum, R., and Rus, V.2000.
The Structure and Performance of an OpenDomain Question Answering System.
InProceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics (ACL-2000): 563-570.Oostdijk, N., 1996.
Using the TOSCA analysis systemto analyse a software manual corpus.
In: R.Sutcliffe, H. Koch and A. McElligott (eds.
),Industrial Parsing of Software Manuals.Amsterdam: Rodopi.
179-206.Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J.,1985.
A comprehensive grammar of the Englishlanguage.
London: Longman.Text Retrieval Conference (TREC) QA track, 2003.http://trec.nist.gov/data/qamain.htmlVoorhees, E. & Tice, D., 2000.
Building a QuestionAnswering Test Collection.
In Proceedings ofSIGIR-2000: 200-207Voorhees, E., 2003.
Overview of the TREC 2003Question Answering Track.
In Overview of TREC2003: 1-1346
