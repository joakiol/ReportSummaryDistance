Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2109?2115,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsThere?s No Comparison: Reference-less Evaluation Metricsin Grammatical Error CorrectionCourtney Napoles,1 Keisuke Sakaguchi,1 and Joel Tetreault21Center for Language and Speech Processing, Johns Hopkins University2Grammarly{napoles,keisuke}@cs.jhu.edu, joel.tetreault@grammarly.comAbstractCurrent methods for automatically evaluatinggrammatical error correction (GEC) systemsrely on gold-standard references.
However,these methods suffer from penalizing gram-matical edits that are correct but not in thegold standard.
We show that reference-lessgrammaticality metrics correlate very stronglywith human judgments and are competitivewith the leading reference-based evaluationmetrics.
By interpolating both methods, weachieve state-of-the-art correlation with hu-man judgments.
Finally, we show that GECmetrics are much more reliable when they arecalculated at the sentence level instead of thecorpus level.
We have set up a CodaLab sitefor benchmarking GEC output using a com-mon dataset and different evaluation metrics.1 IntroductionGrammatical error correction (GEC) has been evalu-ated by comparing the changes made by a system tothe corrections made in gold-standard annotations.Following the recent shared tasks in this field (e.g.,Ng et al (2014)), several papers have critiqued GECmetrics and proposed new methods.
Existing met-rics depend on gold-standard corrections and there-fore have a notable weakness: systems are penalizedfor making corrections that do not appear in the ref-erences.1 For example, the following output has lowmetric scores even though three appropriate correc-tions were made to the input:1We refer to the gold-standard corrections as references be-cause gold standard suggests just one accurate correction.However , people now can contact communicatewith anyone people all over the world who can usecomputers at any time , and there is no time delay ofmessages .These changes (in red) were not seen in the refer-ences and therefore the metrics GLEU and M2 (de-scribed in ?2) score this output worse than 75% of15,000 other generated sentences.While grammaticality-based, reference-less met-rics have been effective in estimating the qualityof machine translation (MT) output, the utility ofsuch metrics has not been investigated previously forGEC.
We hypothesize that such methods can over-come this weakness in reference-based GEC met-rics.
This paper has four contributions: 1) We de-velop three grammaticality metrics that are com-petitive with current reference-based measures andcorrelate very strongly with human judgments.
2)We achieve state-of-the-art performance when in-terpolating a leading reference-based metric with agrammaticality metric.
3) We identify an interest-ing result that the mean of sentence-level scores issubstantially better for evaluating systems than thesystem-level score.
4) We release code for twogrammaticality metrics and establish an online plat-form for evaluating GEC output.2 Prior workTo our knowledge, this is the first work to evalu-ate GEC without references.
Within MT, this taskis called quality estimation.
MT output is evalu-ated by its fluency, or adherence to accepted con-ventions of grammaticality and style, and adequacy,which is the input?s meaning conveyed in the output.2109Quality estimation targets fluency by estimating theamount of post-editing needed by the output.
Thishas been the topic of recent shared tasks, e.g.
Bojaret al (2015).
Specia et al (2010) evaluated the qual-ity of translations using sentence-level features fromthe output but not the references, predicting discreteand continuous scores.
A strong baseline, QuEst,uses support vector regression trained over 17 fea-tures extracted from the output (Specia et al, 2013).Most closely related to our work, Parton et al (2011)applied features from Educational Testing Service?se-rater R?
(Attali and Burstein, 2006) to evaluate MToutput with a ranking SVM, without references, andimproved performance by including features derivedfrom MT metrics (BLEU, TERp, and METEOR).Within the GEC field, recent shared tasks haveprompted the development and scrutiny of new met-rics for evaluating GEC systems.
The Helping OurOwn shared tasks evaluated systems using precision,recall, and F-score against annotated gold-standardcorrections (Dale and Kilgarriff, 2011; Dale et al,2012).
The subsequent CoNLL Shared Tasks onGEC (Ng et al, 2013; Ng et al, 2014) were scoredwith the MaxMatch metric (M2), which capturesword- and phrase-level edits by calculating the F-score over an edit lattice (Dahlmeier and Ng, 2012).Felice and Briscoe (2015) identified shortcomings ofM2 and proposed I-measure to address these issues.I-measure computes the accuracy of a token-levelalignment between the original, generated, and gold-standard sentences.
These precision- and recall-based metrics measure fluency and adequacy by pe-nalizing inappropriate changes, which alter mean-ing or introduce other errors.
Changes consistentwith the annotations indicate improved fluency andno change in meaning.Unlike these metrics, GLEU scores output by pe-nalizing n-grams found in the input and output butnot the reference (Napoles et al, 2015).
Like BLEU(Papineni et al, 2002), GLEU captures both fluencyand adequacy with n-gram overlap.
Recent work hasshown that GLEU has the strongest correlation withhuman judgments compared to the GEC metrics de-scribed above (Sakaguchi et al, 2016).
These GECmetrics are all defined at the corpus level, mean-ing that the statistics are accumulated over the en-tire output and then used to calculate a single systemscore.3 Explicitly evaluating grammaticalityGLEU, I-measure, and M2 are calculated basedon comparison to reference corrections.
TheseReference-Based Metrics (RBMs) credit correctionsseen in the references and penalize systems for ig-noring errors and making bad changes (changing aspan of text in an ungrammatical way or introduc-ing errors to grammatical text).
However, RBMsmake two strong assumptions: that the annotationsin the references are correct and that they are com-plete.
We argue that these assumptions are invalidand point to a deficit in current evaluation practices.In GEC, the agreement between raters can be lowdue to the challenging nature of the task (Bryant andNg, 2015; Rozovskaya and Roth, 2010; Tetreaultand Chodorow, 2008), indicating that annotationsmay not be correct or complete.An exhaustive list of all possible correctionswould be time-consuming, if not impossible.
As aresult, RBMs penalize output that has a valid correc-tion that is not present in the references or that ad-dresses an error not corrected in the references.
Theexample in ?1 has low GLEU and M2 scores, eventhough the output addresses two errors (GLEU=0.43and M2 = 0.00, in the bottom half and quartile of 15ksystem outputs, respectively).To address these concerns, we propose three met-rics to evaluate the grammaticality of output withoutcomparing to the input or a gold-standard sentence(Grammaticality-Based Metrics, or GBMs).
We ex-pect GBMs to score sentences, such as our examplein ?1, more highly.
The first two metrics are scoredby counting the errors found by existing grammat-ical error detection tools.
The error count score issimply calculated: 1 ?
# errors# tokens .
Two different toolsare used to count errors: e-rater R?
?s grammaticalerror detection modules (ER) and Language Tool(Mi?kowski, 2010) (LT).
We choose these because,while e-rater R?
is a large-scale, robust tool that de-tects more errors than Language Tool,2 it is propri-etary whereas Language Tool is publicly availableand open sourced.For our third method, we estimate a grammat-icality score with a linguistic feature-based model(LFM), which is our implementation of Heilman et2In the data used for this work, e-rater R?
detects approxi-mately 15 times more errors than Language Tool.2110al.
(2014).3 The LFM score is a ridge regressionover a variety of linguistic features related to gram-maticality, including the number of misspellings,language model scores, OOV counts, and PCFG andlink grammar features.
It has been shown to effec-tively assess the grammaticality of learner writing.LFM predicts a score for each sentence while ERand LT, like the RBMs, can be calculated with ei-ther sentence- or document-level statistics.
To beconsistent with LFM, for all metrics in this workwe score each sentence individually and report thesystem score as the mean of the sentence scores.We discuss the effects of modifying metrics from acorpus-level to a sentence-level in ?5.Consistent with our hypothesis, ER and LT scorethe ?1 example in the top quartile of outputs andLFM ranks it in the top half.3.1 A hybrid metricThe obvious deficit of GBMs is that they do not mea-sure the adequacy of generated sentences, so theycould easily be manipulated with grammatical out-put that is unrelated to the input.
An ideal GECmetric would measure both the grammaticality of agenerated sentence and its meaning compared to theoriginal sentence, and would not necessarily needreferences.
The available data of scored system out-puts are insufficient for developing a new metric dueto their limited size, thus we turn to interpolation todevelop a sophisticated metric that jointly capturesgrammaticality and adequacy.To harness the advantage of RBMs (adequacy)and GBMs (fluency), we build combined metrics,interpolating each RBM with each GBM.
For a sen-tence of system output, the interpolated score (SI) ofthe GBM score (SG) and RBM score (SR) is com-puted as follows:SI = (1?
?
)SG + ?SRAll values of SG and SR are in the interval [0, 1], ex-cept for I-measure, which falls between [?1, 1], andthe distribution varies for each metric.4 The systemscore is the average SI of all generated sentences.3Our implementation is slightly modified in that it does notuse features from the PET HPSG parser.4Mean scores are GLEU 0.52 ?
0.21, M2 0.21 ?
0.34, IM0.10?0.30, ER 0.91?0.10, LFM 0.50?0.16, LT 1.00?0.01.Metric Spearman?s ?
Pearson?s rGLEU 0.852 0.838ER 0.852 0.829LT 0.808 0.811I-measure 0.769 0.753LFM 0.780 0.742M2 0.648 0.641Table 1: Correlation between the human and metric rankings.4 ExperimentsTo assess the proposed metrics, we apply the RBMs,GBMs, and interpolated metrics to score the out-put of 12 systems participating in the CoNLL-2014Shared Task on GEC (Ng et al, 2014).
Recent workshave evaluated RBMs by collecting human rankingsof these system outputs and comparing them to themetric rankings (Grundkiewicz et al, 2015; Napoleset al, 2015).
In this section, we compare each met-ric?s ranking to the human ranking of Grundkiewiczet al (2015, Table 3c).
We use 20 references forscoring with RBMs: 2 original references, 10 ref-erences collected by Bryant and Ng (2015), and8 references collected by Sakaguchi et al (2016).The motivations for using 20 references are twofold:the best GEC evaluation method uses these 20 ref-erences with the GLEU metric (Sakaguchi et al,2016), and work in machine translation shows thatmore references are better for evaluation (Finch etal., 2004).
Due to the low agreement discussed in ?3,having more references can be beneficial for evalu-ating a system when there are multiple viable waysof correcting a sentence.
Unlike previous GEC eval-uations, all metrics reported here use the mean of thesentence-level scores for each system.Results are presented in Table 1.
The error-countmetrics, ER and LT, have stronger correlation thanall RBMs except for GLEU, which is the currentstate of the art.
GLEU has the strongest correlationwith the human ranking (?
= 0.852, r = 0.838), fol-lowed closely by ER, which has slightly lower Pear-son correlation (r = 0.829) but equally as strongrank correlation, which is arguably more importantwhen comparing different systems.
I-measure andLFM have similar strength correlations, and M2 isthe lowest performing metric, even though its corre-lation is still strong (?
= 0.648, r = 0.641).Next we compare the interpolated scores with thehuman ranking, testing 101 different values of ?2111ranked by Spearman?s rank coefficient (?)
ranked by Pearson?s correlation coefficient (r)ER LFM LT ER LFM LTno intrpl.
0.852 (0) 0.780 (0) 0.808 (0) no intrpl.
0.829 (0) 0.742 (0) 0.811 (0)GLEU 0.852 (1) 0.885 (0.03) 0.874 (0.27) 0.857 (0.04) 0.838 (1) 0.867 (0.27) 0.845 (0.84) 0.867 (0.09)I-m. 0.769 (1) 0.874 (0.19) 0.863 (0.37) 0.852 (0.01) 0.753 (1) 0.837 (0.02) 0.791 (0.22) 0.828 (0.01)M2 0.648 (1) 0.868 (0.01) 0.852 (0.05) 0.808 (0.00) 0.641 (1) 0.829 (0.00) 0.754 (0.04) 0.811 (0.00)Table 2: Oracle correlations between interpolated metrics and the human rankings.
The ?
value for each metric is in parentheses.GLEU Intrpl.rank rank Output sentence1 2 Genectic testing is a personal decision ,with the knowledge that there is a possi-blity that one could be a carrier or not .2 3 Genectic testing is a personal decision ,the kowledge that there is a possiblity thatone could be a carrier or not .3 1 Genetic testing is a personal decision ,with the knowledge that there is a possi-bility that one could be a carrier or not .Table 3: An example of system outputs ranked by GLEU andGLEU interpolated with ER.
Words in italics are misspelled.
[0,1] to find the oracle value.
Table 2 shows thecorrelations between the human judgments and theoracle interpolated metrics.
Correlations of interpo-lated metrics are the upper bound and the correla-tions of uninterpolated metrics (in the first columnand first row) are the lower bound.
InterpolatingGLEU and IM with GBMs has stronger correlationthan any uninterpolated metric (i.e.
?
= 0 or 1),and the oracle interpolation of ER and GLEU mani-fests the strongest correlation with the human rank-ing (?
= 0.885, r = 0.867).5 M2 has the weakestcorrelation of all uninterpolated metrics and, whencombined with GBMs, three of the interpolated met-rics have ?
= 0, meaning the interpolated score isequivalent to the GBM and M2 does not contribute.Table 3 presents an example of how interpolationcan help evaluation.
The top two sentences rankedby GLEU have misspellings that were not correctedin the NUCLE references.
Interpolating with a GBMrightly ranks the misspelled output below the cor-rected output.Since these experiments use a large number ofreferences (20), we determine how different refer-ence sizes affect the interpolated metrics by system-5To verify that these metrics cannot be gamed, we interpo-lated GBMs with RBMs scored against randomized references,and got scores 15% lower than un-gamed scores, on average.0 5 10 15 20Number of References0.800.820.840.860.88Spearman's?ERGLEUGLEU+ERFigure 1: The mean correlation of oracle interpolated GLEUand ER scores across different reference sizes, compared to theuninterpolated metrics.
Bars indicate a 95% confidence interval.atically increasing the number of references from 1to 20 and randomly choosing n references to use asa gold standard when calculating the RBM scores,repeating 10 times for each value n (Figure 1).
Thecorrelation is nearly as strong with 3 and 20 refer-ences (?
= 0.884 v. 0.885), and interpolating withjust 1 reference is nearly as good (0.878) and im-proves over any uninterpolated metric.We acknowledge that using GBMs is in effect us-ing GEC systems to score other GEC systems.
In-terestingly, we find that even if the GBMs are im-perfect, they still boost performance of the RBMs.GBMs have been trained to recognize errors in dif-ferent contexts and, conversely, can identify cor-rect grammatical constructions in diverse contexts,where the RBMs only recognize corrections madethat appear in the gold standards, which are limited.Therefore GBMs can make a nice complement toshortcomings that RBMs may have.5 Sentence-level evaluationIn the course of our experiments, we noticed thatI-measure and GLEU have stronger correlationswith the expert human ranking when using the2112Corpus SentenceMetric ?
r ?
rGLEU 0.725 0.724 0.852 0.838I-m. -0.055?
0.061 0.769 0.753M2 0.692 0.617 0.648 0.641Table 4: Correlation with human ranking when using corpus-and sentence-level metrics.
?
indicates a significant differencefrom the corresponding sentence-level correlation (p < 0.05).7mean sentence-level score (Table 4).6 Most no-tably, I-measure does not correlate at all as a corpus-level metric but has a very strong correlation at thesentence-level (specifically, ?
improves from -0.055to 0.769).
This could be because corpus-level statis-tics equally distribute counts of correct annotationsover all sentences, even those where the output ne-glects to make any necessary corrections.
Sentence-level statistics would not average the correct countsover all sentences in this same way.
As a result,a corpus-level statistic may over-estimate the qual-ity of system output.
Deeper investigation into thisphenomenon is needed to understand why the meansentence-level scores do better.6 SummaryWe have identified a shortcoming of reference-basedmetrics: they penalize changes made that do not ap-pear in the references, even if those changes are ac-ceptable.
To address this problem, we developedmetrics to explicitly measure grammaticality with-out relying on reference corrections and showed thatthe error-count metrics are competitive with the bestreference-based metric.
Furthermore, by interpolat-ing RBMs with GBMs, the system ranking has evenstronger correlation with the human ranking.
TheER metric, which was derived from counts of er-rors detected using e-rater R?, is nearly as good asthe state-of-the-art RBM (GLEU) and the interpo-lation of these metrics has the strongest reportedcorrelation with the human ranking (?
= 0.885,r = 0.867).
However, since e-rater R?
is not widelyavailable to researchers, we also tested a metric us-ing Language Tool, which does nearly as well wheninterpolated with GLEU (?
= 0.857, r = 0.867).6The correlations in Table 4 differ from what was reported inGrundkiewicz et al (2015) and Napoles et al (2015) due to thereferences and sentence-level computation used in this work.7Significance is found by applying a two-tailed t-test to theZ-scores attained using Fisher?s z-transformation.Two important points should be noted: First, dueto the small sample size (12 system outputs), noneof the rankings significantly differ from one anotherexcept for the corpus-level I-measure.
Secondly,GLEU and the other RBMs already have strong tovery strong correlation with the human judgments,which makes it difficult for any combination of met-rics to perform substantially higher.
The best unin-terpolated and interpolated metrics use an extremelylarge number of references (20), however Figure 1shows that interpolating GLEU using just one ref-erence has stronger correlation than any uninterpo-lated metric.
This supports the use of interpolationto improve GEC evaluation in any setting.This work is the first exploration into applyingfluency-based metrics to GEC evaluation.
We be-lieve that, for future work, fluency measures couldbe further improved with other methods, such as us-ing existing GEC systems to detect errors, or evenusing an ensemble of systems to improve coverage(indeed, ensembles have been useful in the GEC taskitself (Susanto et al, 2014)).
There is also recentwork from the MT community, such as the use ofconfidence bounds (Graham and Liu, 2016) or un-certainty measurement (Beck et al, 2016), whichcould be adopted by the GEC community.Finally, in the course of our experiments, we de-termined that metrics calculated on the sentence-level is more reliable for evaluating GEC output, andwe suggest that the GEC community adopt this mod-ification to better assess systems.To facilitate GEC evaluation, we have set up anonline platform8 for benchmarking system output onthe same set of sentences evaluated using differentmetrics and made the code for calculating LT andLFM available.9AcknowledgmentsWe would like to thank Matt Post, Martin Chodorow,and the three anonymous reviews for their commentsand feedback.
We also thank Educational TestingService for providing e-rater R?
output.
This materialis based upon work partially supported by the NSFGRF under Grant No.
1232825.8https://competitions.codalab.org/competitions/127319https://github.com/cnap/grammaticality-metrics2113ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater R?
v. 2.
The Journal of Technology,Learning and Assessment, 4(3).Daniel Beck, Lucia Specia, and Trevor Cohn.
2016.Exploring prediction uncertainty in machine transla-tion quality estimation.
In Proceedings of The 20thSIGNLL Conference on Computational Natural Lan-guage Learning, pages 208?218, Berlin, Germany,August.
Association for Computational Linguistics.Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop onStatistical Machine Translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation,pages 1?44, Sofia, Bulgaria, August.
Association forComputational Linguistics.Ondr?ej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ales?
Tam-chyna.
2014.
Findings of the 2014 Workshop on Sta-tistical Machine Translation.
In Proceedings of theNinth Workshop on Statistical Machine Translation,pages 12?58, Baltimore, Maryland, USA, June.
As-sociation for Computational Linguistics.Ondr?ej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp, PhilippKoehn, Varvara Logacheva, Christof Monz, MatteoNegri, Matt Post, Carolina Scarton, Lucia Specia, andMarco Turchi.
2015.
Findings of the 2015 Workshopon Statistical Machine Translation.
In Proceedings ofthe Tenth Workshop on Statistical Machine Transla-tion, pages 1?46, Lisbon, Portugal, September.
Asso-ciation for Computational Linguistics.Christopher Bryant and Hwee Tou Ng.
2015.
How farare we from fully automatic high quality grammaticalerror correction?
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Linguis-tics and the 7th International Joint Conference on Nat-ural Language Processing (Volume 1: Long Papers),pages 697?707, Beijing, China, July.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical MachineTranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 10?51, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 568?572, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Robert Dale and Adam Kilgarriff.
2011.
Helping ourown: The HOO 2011 pilot shared task.
In Proceedingsof the Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 242?249, Nancy, France, September.
Associa-tion for Computational Linguistics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the Seventh Workshop on Building EducationalApplications Using NLP, pages 54?62, Montre?al,Canada, June.
Association for Computational Linguis-tics.Mariano Felice and Ted Briscoe.
2015.
Towards astandard evaluation method for grammatical error de-tection and correction.
In Proceedings of the 2015Conference of the North American Chapter of the As-sociation for Computational Linguistics, pages 578?587, Denver, Colorado, June.
Association for Compu-tational Linguistics.Andrew M. Finch, Yasuhiro Akiba, and Eiichiro Sumita.2004.
How does automatic machine translation eval-uation correlate with human scoring as the numberof reference translations increases?
In Proceedingsof the Fourth International Conference on LanguageResources and Evaluation, LREC 2004, May 26-28,2004, Lisbon, Portugal.Yvette Graham and Qun Liu.
2016.
Achieving accurateconclusions in evaluation of automatic machine trans-lation metrics.
In Proceedings of the 2016 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1?10, San Diego, California, June.
As-sociation for Computational Linguistics.Roman Grundkiewicz, Marcin Junczys-Dowmunt, andEdward Gillian.
2015.
Human evaluation of gram-matical error correction systems.
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 461?470, Lisbon, Portu-gal, September.
Association for Computational Lin-guistics.Michael Heilman, Aoife Cahill, Nitin Madnani, MelissaLopez, Matthew Mulholland, and Joel Tetreault.
2014.Predicting grammaticality on an ordinal scale.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 2: ShortPapers), pages 174?180, Baltimore, Maryland, June.Association for Computational Linguistics.2114Marcin Mi?kowski.
2010.
Developing an open-source,rule-based proofreading tool.
Software: Practice andExperience, 40(7):543?566.Courtney Napoles, Keisuke Sakaguchi, Matt Post, andJoel Tetreault.
2015.
Ground truth for grammaticalerror correction metrics.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 2: Short Pa-pers), pages 588?593, Beijing, China, July.
Associa-tion for Computational Linguistics.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 Shared Task on grammatical error correction.
InProceedings of the Seventeenth Conference on Com-putational Natural Language Learning: Shared Task,pages 1?12, Sofia, Bulgaria, August.
Association forComputational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon grammatical error correction.
In Proceedings ofthe Eighteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?14, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-tin Chodorow.
2011.
E-rating machine translation.In Proceedings of the Sixth Workshop on StatisticalMachine Translation, pages 108?115.
Association forComputational Linguistics.Alla Rozovskaya and Dan Roth.
2010.
Annotating ESLerrors: Challenges and rewards.
In Proceedings of theNAACL HLT 2010 Fifth Workshop on Innovative Useof NLP for Building Educational Applications, pages28?36, Los Angeles, California, June.
Association forComputational Linguistics.Keisuke Sakaguchi, Courtney Napoles, Matt Post, andJoel Tetreault.
2016.
Reassessing the goals of gram-matical error correction: Fluency instead of grammat-icality.
Transactions of the Association for Computa-tional Linguistics, 4:169?182.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.
Ma-chine translation evaluation versus quality estimation.Machine translation, 24(1):39?50.Lucia Specia, Kashif Shah, Jose G.C.
de Souza, andTrevor Cohn.
2013.
QuEst ?
a translation quality esti-mation framework.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics: System Demonstrations, pages 79?84, Sofia, Bul-garia, August.
Association for Computational Linguis-tics.Raymond Hendy Susanto, Peter Phandi, and Hwee TouNg.
2014.
System combination for grammatical er-ror correction.
In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 951?962, Doha, Qatar, October.Association for Computational Linguistics.Joel Tetreault and Martin Chodorow.
2008.
Native judg-ments of non-native usage: Experiments in preposi-tion error detection.
In Coling 2008: Proceedings ofthe workshop on Human Judgements in ComputationalLinguistics, pages 24?32, Manchester, UK, August.Coling 2008 Organizing Committee.2115
