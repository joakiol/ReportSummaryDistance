Proceedings of the Workshop on Statistical Machine Translation, pages 78?85,New York City, June 2006. c?2006 Association for Computational LinguisticsPartitioning Parallel Documents Using Binary SegmentationJia Xu and Richard Zens and Hermann NeyChair of Computer Science 6Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen Germany{xujia,zens,ney}@cs.rwth-aachen.deAbstractIn statistical machine translation, largenumbers of parallel sentences are requiredto train the model parameters.
However,plenty of the bilingual language resourcesavailable on web are aligned only at thedocument level.
To exploit this data,we have to extract the bilingual sentencesfrom these documents.The common method is to break the doc-uments into segments using predefinedanchor words, then these segments arealigned.
This approach is not error free,incorrect alignments may decrease thetranslation quality.We present an alternative approach to ex-tract the parallel sentences by partitioninga bilingual document into two pairs.
Thisprocess is performed recursively until allthe sub-pairs are short enough.In experiments on the Chinese-EnglishFBIS data, our method was capable ofproducing translation results comparableto those of a state-of-the-art sentencealigner.
Using a combination of the twoapproaches leads to better translation per-formance.1 IntroductionCurrent statistical machine translation systems usebilingual sentences to train the parameters of thetranslation models.
The exploitation of more bilin-gual sentences automatically and accurately as wellas the use of these data with the limited computa-tional requirements become crucial problems.The conventional method for producing parallelsentences is to break the documents into sentencesand to align these sentences using dynamic program-ming.
Previous investigations can be found in workssuch as (Gale and Church, 1993) and (Ma, 2006).A disadvantage is that only the monotone sentencealignments are allowed.Another approach is the binary segmentationmethod described in (Simard and Langlais, 2003),(Xu et al, 2005) and (Deng et al, 2006), whichseparates a long sentence pair into two sub-pairs re-cursively.
The binary reordering in alignment is al-lowed but the segmentation decision is only opti-mum in each recursion step.Hence, a combination of both methods is ex-pected to produce a more satisfying result.
(Denget al, 2006) performs a two-stage procedure.
Thedocuments are first aligned at level using dynamicprogramming, the initial alignments are then refinedto produce shorter segments using binary segmen-tation.
But on the Chinese-English FBIS trainingcorpus, the alignment accuracy and recall are lowerthan with Champollion (Ma, 2006).We refine the model in (Xu et al, 2005) usinga log-linar combination of different feature func-tions and combine it with the approach of (Ma,2006).
Here the corpora produced using both ap-proaches are concatenated, and each corpus is as-signed a weight.
During the training of the wordalignment models, the counts of the lexicon entries78are linear interpolated using the corpus weights.
Inthe experiments on the Chinese-English FBIS cor-pus the translation performance is improved by 0.4%of the BLEU score compared to the performanceonly with Champollion.The remainder of this paper is structured as fol-lows: First we will briefly review the baseline statis-tical machine translation system in Section 2.
Then,in Section 3, we will describe the refined binary seg-mentation method.
In Section 4.1, we will introducethe methods to extract bilingual sentences from doc-ument aligned texts.
The experimental results willbe presented in Section 4.2 Review of the Baseline StatisticalMachine Translation SystemIn this section, we briefly review our translation sys-tem and introduce the word alignment models.In statistical machine translation, we are givena source language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible tar-get language sentences, we will choose the sentencewith the highest probability:e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )}= argmaxI,eI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (1)The decomposition into two knowledge sources inEquation 1 allows independent modeling of tar-get language model Pr(eI1) and translation modelPr(fJ1 |eI1)1.
The translation model can be furtherextended to a statistical alignment model with thefollowing equation:Pr(fJ1 |eI1) =?aJ1Pr(fJ1 , aJ1 |eI1)The alignment model Pr(fJ1 , aJ1 |eI1) introduces a?hidden?
word alignment a = aJ1 , which describes amapping from a source position j to a target positionaj .1The notational convention will be as follows: we use thesymbol Pr(?)
to denote general probability distributions with(nearly) no specific assumptions.
In contrast, for model-basedprobability distributions, we use the generic symbol p(?
).Monotone Non-monotoneTarget B APositions C DSource PositionsFigure 1: Two Types of AlignmentThe IBM model 1 (IBM-1) (Brown et al, 1993)assumes that all alignments have the same probabil-ity by using a uniform distribution:p(fJ1 |eI1) =1IJ ?J?j=1I?i=1p(fj |ei) (2)We use the IBM-1 to train the lexicon parametersp(f |e), the training software is GIZA++ (Och andNey, 2003).To incorporate the context into the translationmodel, the phrase-based translation approach (Zenset al, 2005) is applied.
Pairs of source and tar-get language phrases are extracted from the bilin-gual training corpus and a beam search algorithm isimplemented to generate the translation hypothesiswith maximum probability.3 Binary Segmentation Method3.1 ApproachHere a document or sentence pair (fJ1 , eI1) 2 is repre-sented as a matrix.
Every element in the matrix con-tains a lexicon probability p(fj |ei), which is trainedon the original parallel corpora.
Each position di-vides a matrix into four parts as shown in Figure 1:the bottom left (C), the upper left (A), the bottomright (D) and the upper right (B).
We use m to de-note the alignment direction, m = 1 means that thealignment is monotone, i.e.
the bottom left part isconnected with the upper right part, and m = 0means the alignment is non-monotone, i.e.
the upperleft part is connected with the bottom right part, asshown in Figure 1.3.2 Log-Linear ModelWe use a log-linear interpolation to combine differ-ent models: the IBM-1, the inverse IBM-1, the an-2Sentences are equivalent to segments in this paper.79chor words model as well as the IBM-4.
K denotesthe total number of models.We go through all positions in the bilingual sen-tences and find the best position for segmenting thesentence:(?i, j?, m?)
= argmaxi,j,m{ K?k=1?khk(j, i,m|fJ1 , eI1)},where i ?
[1, I ?
1] and j ?
[1, J ?
1] are posi-tions in the source and target sentences respectively.The feature functions are described in the follow-ing sections.
In most cases, the sentence pairs arequite long and even after one segmentation we maystill have long sub-segments.
Therefore, we separatethe sub-segment pairs recursively until the length ofeach new segment is less than a defined value.3.3 Normalized IBM-1The function in Equation 2 can be normalized bythe source sentence length with a weighting ?
as de-scribed in (Xu et al, 2005):The monotone alignment is calculated ash1(j, i, 1|fJ1 , eI1) = log(p(f j1 |ei1)??1j+(1??)
(3)?p(fJj+1|eIi+1)??1J?j+(1??
)),and the non-monotone alignment is formulated inthe same way.We also use the inverse IBM-1 as a feature, by ex-changing the place of ei1 and f j1 its monotone align-ment is calculated as:h2(j, i, 1|fJ1 , eI1) = log(p(ei1|f j1 )??1i+(1??)
(4)?p(eIi+1|fJj+1)??1I?i+(1??
))3.4 Anchor WordsIn the task of extracting parallel sentences fromthe paragraph-aligned corpus, selecting some anchorwords as preferred segmentation positions can ef-fectively avoid the extraction of incomplete segmentpairs.
Therefore we use an anchor words model toprefer the segmentation at the punctuation marks,where the source and target words are identical:h3(j, i,m|fJ1 , eI1) ={ 1 : fj = ei ?
ei ?
A0 : otherwiseA is a user defined anchor word list, here we useA={.,??;}.
If the corresponding model scaling factor?3 is assigned a high value, the segmentation posi-tions are mostly after anchor words.3.5 IBM-4 Word AlignmentIf we already have the IBM-4 Viterbi word align-ments for the parallel sentences and need to retrainthe system, for example to optimize the training pa-rameters, we can include the Viterbi word align-ments trained on the original corpora into the binarysegmentation.
In the monotone case, the model isrepresented ash4(j, i, 1|fJ1 , eI1) =log(N(f j1 , ei1) +N(fJj+1, eIi+1)N(fJ1 , eI1)),where N(f j1 , ei1) denotes the number of the align-ment links inside the matrix (1, 1) and (j, i).
In thenon-monotone case the model is formulated in thesame way.3.6 Word Alignment ConcatenationAs described in Section 2, our translation is based onphrases, that means for an input sentence we extractall phrases matched in the training corpus and trans-late with these phrase pairs.
Although the aim ofsegmentation is to split parallel text into translatedsegment pairs, but the segmentation is still not per-fect.
During sentence segmentation we might sep-arate a phrase into two segments, so that the wholephrase pair can not be extracted.To avoid this, we concatenate the word align-ments trained with the segmentations of one sen-tence pair.
During the segmentation, the position ofeach segmentation point in the sentence is memo-rized.
After training the word alignment model withthe segmented sentence pairs, the word alignmentsare concatenated again according to the positions oftheir segments in the sentences.
The original sen-tence pairs and the concatenated alignments are thenused for the phrase extraction.80Table 1: Corpus Statistics: NISTChinese EnglishTrain Sentences 8.64 MRunning Words 210 M 226 MAverage Sentence Length 24.4 26.3Vocabulary 224 268 359 623Singletons 98 842 156 493Segmentation Sentences 17.9 MRunning Words 210 M 226 MAverage Sentence Length 11.7 12.6Vocabulary 221 517 353 148Singletons 97 062 152 965Segmentation with Additional Data Sentences 19.5 MRunning Words 230 M 248 MAdded Running Words 8.0% 8.2%Evaluation Sentences 878 3 512Running Words 24 111 105 516Vocabulary 4 095 6 802OOVs (Running Words) 8 6584 Translation Experiments4.1 Bilingual Sentences Extraction MethodsIn this section, we describe the different methods toextract the bilingual sentence pairs from the docu-ment aligned corpus.Given each document pair, we assume that theparagraphs are aligned one to one monotone if boththe source and target language documents containthe same number of paragraphs; otherwise the para-graphs are aligned with the Champollion tool.Starting from the parallel paragraphs we extractthe sentences using three methods:1.
Binary segmentationThe segmentation method described in Sec-tion 3 is applied by treating the paragraph pairsas long sentence pairs.
We can use the anchorwords model described in Section 3.4 to prefersplitting at punctuation marks.The lexicon parameters p(f |e) in Equation 2are estimated as follows: First the sentences arealigned roughly using the dynamic program-ming algorithm.
Training on these aligned sen-tences, we get the initial lexicon parameters.Then the binary segmentation algorithm is ap-plied to extract the sentences again.2.
ChampollionAfter a paragraph is divided into sentences atpunctuation marks, the Champollion tool (Ma,2006) is used, which applies dynamic program-ming for the sentence alignment.3.
CombinationThe bilingual corpora produced by the binarysegmentation and Champollion methods areconcatenated and are used in the training of thetranslation model.
Each corpus is assigned aweight.
During the training of the word align-ment models, the counts of the lexicon en-tries are linearly interpolated using the corpusweights.4.2 Translation TasksWe will present the translation results on twoChinese-English tasks.1.
On the large data track NIST task (NIST,2005), we will show improvements using therefined binary segmentation method.81Table 2: Corpus Statistics: FBISSegmentation ChampollionChinese English Chinese EnglishTrain Sentences 739 899 177 798Running Words 8 588 477 10 111 752 7 659 776 9 801 257Average Sentence Length 11.6 13.7 43.1 55.1Vocabulary 34 896 56 573 34 377 55 775Singletons 4 775 19 283 4 588 19 004Evaluation Sentences 878 3 513 878 3 513Running Words 24 111 105 516 24 111 105 516Vocabulary 4 095 6 802 4 095 6 802OOVs (Running Words) 109 2 257 119 2 3092.
On the FBIS corpus, we will compare the dif-ferent sentence extraction methods described inSection 4.1 with respect to translation perfor-mance.
We do not apply the extraction meth-ods on the whole NIST corpora, because somecorpora provided by the LDC (LDC, 2005) aresentence aligned but not document aligned.4.3 Corpus StatisticsThe training corpora used in NIST task are a set ofindividual corpora including the FBIS corpus.
Thesecorpora are provided by the Linguistic Data Consor-tium (LDC, 2005), the domains are news articles.The translation experiments are carried out on theNIST 2002 evaluation set.As shown in Table 1, there are 8.6 million sen-tence pairs in the original corpora of the NIST task.The average sentence length is about 25.
After seg-mentation, there are twice as many sentence pairs,i.e.
17.9 million, and the average sentence lengthis around 12.
Due to a limitation of GIZA++, sen-tences consisting of more than one hundred wordsare filtered out.
Segmentation of long sentences cir-cumvents this restriction and allows us include moredata.
Here we were able to add 8% more Chineseand 8.2% more English running words to the train-ing data.
The training time is also reduced.Table 2 presents statistics of the FBIS data.
Af-ter the paragraph alignment described in Section 4.1we have nearly 81 thousand paragraphs, 8.6 millionChinese and 10.1 million English running words.One of the advantages of the binary segmentation isthat we do not loose words during the bilingual sen-tences extraction.
However, we produce sentencepairs with very different lengths.
Using Champol-lion we loose 10.8% of the Chinese and 3.1% of theEnglish words.4.4 Segmentation ParametersWe did not optimize the log-linear model scalingfactors for the binary segmentation but used the fol-lowing fixed values: ?1 = ?2 = 0.5 for the IBM-1models in both directions; ?3 = 108, if the anchorwords model is is used; ?4 = 30, if the IBM-4 modelis used.
The maximum sentence length is 25.4.5 Evaluation CriteriaWe use four different criteria to evaluate the transla-tion results automatically:?
WER (word error rate):The WER is computed as the minimum num-ber of substitution, insertion and deletion oper-ations that have to be performed to convert thegenerated sentence into the reference sentence,divided by the reference sentence length.?
PER (position-independent word error rate):A shortcoming of the WER is that it requires aperfect word order.
The word order of an ac-ceptable sentence can be differ from that of thetarget sentence, so that the WER measure alonecould be misleading.
The PER compares thewords in the two sentences ignoring the wordorder.?
BLEU score:This score measures the precision of unigrams,820 0.2 0.4 0.6 0.8 131.831.93232.132.2Weight for the Binary SegmentationBLEU[%]Figure 2: Translation performance as a function ofthe weight for the binary segmentation ?
( weightfor Champollion: 1?
?
)bigrams, trigrams and fourgrams with a penaltyfor too short sentences.
(Papineni et al, 2002).?
NIST score:This score is similar to BLEU, but it usesan arithmetic average of N-gram counts ratherthan a geometric average, and it weights moreheavily those N-grams that are more informa-tive.
(Doddington, 2002).The BLEU and NIST scores measure accuracy,i.e.
larger scores are better.
In our evaluation thescores are measured as case insensitive and with re-spect to multiple references.4.6 Translation ResultsFor the segmentation of long sentences into shortsegments, we performed the experiments on theNIST task.
Both in the baseline and the segmenta-tion systems we obtain 4.7 million bilingual phrasesduring the translation.
The method of alignmentconcatenation increases the number of the extractedbilingual phrase pairs from 4.7 million to 4.9 mil-lion, the BLEU score is improved by 0.1%.
Byincluding the IBM-4 Viterbi word alignment, theNIST score is improved.
The training of the base-line system requires 5.9 days, after the sentence seg-mentation it requires only 1.5 days.
Moreover, thesegmentation allows the inclusion of long sentencesthat are filtered out in the baseline system.
Usingthe added data, the translation performance is en-hanced by 0.3% in the BLEU score.
Because ofthe long translation period, the translation parame-ters are only optimized on the baseline system withrespect to the BLEU score, we could expect a furtherimprovement if the parameters were also optimizedon the segmentation system.Our major objective here is to introduce anotherapproach to parallel sentence extraction: binary seg-mentation of the bilingual texts recursively.
We usethe paragraph-aligned corpus as a starting point.
Ta-ble 4 presents the translation results on the train-ing corpora generated by the different methods de-scribed in Section 4.1.
The translation parametersare optimized with the respect to the BLEU score.We observe that the binary segmentation methodsare comparable to Champollion and the segmenta-tion with anchors outperforms the one without an-chors.
By combining the methods of Champol-lion and the binary segmentation with anchors, theBLEU score is improved by 0.4% absolutely.We optimized the weightings for the binary seg-mentation method, the sum of the weightings forboth methods is one.
As shown in Figure 2, usingone of the methods alone does not produce the bestresult.
The maximum BLEU score is attained whenboth methods are combined with equal weightings.5 Discussion and Future WorkWe successfully applied the binary sentence seg-mentation method to extract bilingual sentence pairsfrom the document aligned texts.
The experimentson the FBIS data show an enhancement of 0.4% ofthe BLEU score compared to the score obtained us-ing a state-of-art sentence aligner.
In addition to theencouraging results obtained, further improvementscould be achieved in the following ways:1.
By extracting bilingual paragraphs from thedocuments, we lost running words using Cham-pollion.
Applying the segmentation approachto paragraph alignment might avoid the loss ofthis data.2.
We combined a number of different models inthe binary segmentation, such as IBM-1, andanchor words.
The model weightings could beoptimized with respect to translation quality.83Table 3: Translation Results using Refined Segmentation Methods on NIST taskError Rate[%] AccuracyWER PER NIST BLEU[%]Baseline 62.7 42.1 8.95 33.5Segmentation 62.6 42.4 8.80 33.5Segmentation + concatenation 62.4 42.3 8.84 33.6Segmentation + concatenation + IBM-4 62.8 42.4 8.91 33.6Segmentation + added data 62.9 42.5 9.00 33.9Table 4: Translation Results on Sentence Alignment Task with FBIS Training CorpusError Rate[%] AccuracyWER PER NIST BLEU[%]Champollion 64.2 43.7 8.61 31.8Segmentation without Anchors 64.3 44.4 8.57 31.8Segmentation with Anchors 64.0 43.9 8.58 31.9Champollion + Segmentation with Anchors 64.3 44.2 8.57 32.23.
In the binary segmentation method, an incor-rect segmentation results in further mistakesin the segmentation decisions of all its sub-segments.
An alternative method (Wu, 1997)makes decisions at the end but has a high com-putational requirement.
A restricted expansionof the search space might better balance seg-mentation accuracy and the efficiency.6 AcknowledgmentsThis work was supported by the European Unionunder the integrated project TC-Star (Technologyand Corpora for Speech to Speech Translation,IST-2002-FP6-506738, http://www.tc-star.org) andthe Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.Y.
Deng, S. Kumar, and W. Byrne.
2006.
Segmenta-tion and alignment of parallel text for statistical ma-chine translation.
Natural Language Engineering, Ac-cepted.
To appear.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proceedings of Human Language Technology,pages 128?132, San Diego, California, March.W.
A. Gale and K. W. Church.
1993.
A program foraligning sentences in bilingual corpora.
Computa-tional Linguistics, 19(1):75?90.LDC.
2005.
Linguistic data consortium resource homepage.
http://www.ldc.upenn.edu/Projects/TIDES.X.
Ma.
2006.
Champollion: A robust parallel textsentence aligner.
In Proceedings of the fifth interna-tional conference on Language Resources and Evalu-ation (LREC), Genoa, Italy, Accepted.
To appear.NIST.
2005.
Machine translation home page.http://www.nist.gov/speech/tests/mt/index.htm.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51, March.K.
A. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318, Philadelphia, July.M.
Simard and P. Langlais.
2003.
Statistical transla-tion alignment with compositionality constraints.
InNAACL 2003 Workshop on Building and Using Paral-lel Texts: Data Driven Machine Translation and Be-yond, Edmonton, Canada, May.84D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?403, September.J.
Xu, R. Zens, and H. Ney.
2005.
Sentence segmentationusing IBM word alignment model 1.
In Proceedings ofEAMT 2005 (10th Annual Conference of the EuropeanAssociation for Machine Translation), pages 280?287,Budapest, Hungary, May.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHphrase-based statistical machine translation system.
InProceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 155?162, Pitts-burgh, PA, October.85
