Proceedings of the Second Workshop on Statistical Machine Translation, pages 171?176,Prague, June 2007. c?2007 Association for Computational LinguisticsAnalysis of statistical and morphological classes to generate weightedreordering hypotheses on a Statistical Machine Translation systemMarta R. Costa-jussa` and Jose?
A. R. FonollosaDepartment of Signal Theory and CommunicationsTALP Research Center (UPC)Barcelona 08034, Spain(mruiz,adrian)@gps.tsc.upc.eduAbstractOne main challenge of statistical machine trans-lation (SMT) is dealing with word order.
Themain idea of the statistical machine reordering(SMR) approach is to use the powerful tech-niques of SMT systems to generate a weightedreordering graph for SMT systems.
This tech-nique supplies reordering constraints to an SMTsystem, using statistical criteria.In this paper, we experiment with different graphpruning which guarantees the translation qualityimprovement due to reordering at a very low in-crease of computational cost.The SMR approach is capable of generalizing re-orderings, which have been learned during train-ing, by using word classes instead of wordsthemselves.
We experiment with statistical andmorphological classes in order to choose thosewhich capture the most probable reorderings.Satisfactory results are reported in the WMT07Es/En task.
Our system outperforms in terms ofBLEU the WMT07 Official baseline system.1 IntroductionNowadays, statistical machine translation is mainly basedon phrases (Koehn et al, 2003).
In parallel to this phrase-based approach, the use of bilingual n-grams gives com-parable results, as shown by Crego et al (2005).
Twobasic issues differentiate the n-gram-based system fromthe phrase-based: training data is monotonically seg-mented into bilingual units; and, the model considers n-gram probabilities rather than relative frequencies.
Then-gram-based system follows a maximum entropy ap-proach, in which a log-linear combination of multiplemodels is implemented (Marin?o et al, 2006), as an al-ternative to the source-channel approach.Introducing reordering capabilities is important in bothsystems.
Recently, new reordering strategies have beenproposed such as the reordering of each source sentenceto match the word order in the corresponding target sen-tence, see Kanthak et al (2005) and Marin?o et al (2006).These approaches are applied in the training set and theylack of reordering generalization.Applied both in the training and decoding step, Collinset al (2005) describe a method for introducing syntac-tic information for reordering in SMT.
This approach isapplied as a pre-processing step.Differently, Crego et al (2006) presents a reorderingapproach based on reordering patterns which is coupledwith decoding.
The reordering patterns are learned di-rectly from word alignment and all reorderings have thesame probability.In our previous work (Costa-jussa` and Fonollosa,2006) we presented the SMR approach which is basedon using the powerful SMT techniques to generate a re-ordered source input for an SMT system both in train-ing and decoding steps.
One step further, (Costa-jussa`et al, 2007) shows how the SMR system can generate aweighted reordering graph, allowing the SMT system tomake the final reordering decision.In this paper, the SMR approach is used to train theSMT system and to generate a weighted reordering graphfor the decoding step.
The SMR system uses word classesinstead of words themselves and we analyze both statisti-cal and morphological classes.
Moreover, we present ex-periments regarding the reordering graph efficiency: weanalyze different graph pruning and we show the very lowincrease in computational cost (compared to a monotonictranslation).
Finally, we compare the performance oursystem in terms of BLEU with the WMT07 baseline sys-tem.This paper is organized as follows.
The first two sec-tions explain the SMT and the SMR baseline systems,respectively.
Section 4 reports the study of statistical and171morphological classes.
Section 5 describes the experi-mental framework and discusses the results.
Finally, Sec-tion 6 presents the conclusions and some further work.2 Ngram-based SMT SystemThis section briefly describes the Ngram-based SMT (forfurther details see (Marin?o et al, 2006)).
The Ngram-based SMT system uses a translation model based onbilingual n-grams.
It is actually a language model ofbilingual units, referred to as tuples, which approxi-mates the joint probability between source and target lan-guages by using bilingual n-grams.
Tuples are extractedfrom any word alignment according to the following con-straints:1. a monotonic segmentation of each bilingual sen-tence pairs is produced,2.
no word inside the tuple is aligned to words outsidethe tuple, and3.
no smaller tuples can be extracted without violatingthe previous constraints.As a result of these constraints, only one segmentationis possible for a given sentence pair.In addition to the bilingual n-gram translation model,the baseline system implements a log-linear combinationof feature functions, which are described as follows:?
A target language model.
This feature consists ofa 4-gram model of words, which is trained from thetarget side of the bilingual corpus.?
A class target language model.
This feature con-sists of a 5-gram model of words classes, which istrained from the target side of the bilingual corpususing the statistical classes from (Och, 1999).?
A word bonus function.
This feature introducesa bonus based on the number of target words con-tained in the partial-translation hypothesis.
It is usedto compensate for the system?s preference for shortoutput sentences.?
A source-to-target lexicon model.
This feature,which is based on the lexical parameters of the IBMModel 1 (Brown et al, 1993), provides a comple-mentary probability for each tuple in the translationtable.
These lexicon parameters are obtained fromthe source-to-target algnments.?
A target-to-source lexicon model.
Similarly to theprevious feature, this feature is based on the lexicalparameters of the IBM Model 1 but, in this case,these parameters are obtained from target-to-sourcealignments.Figure 1: SMR block diagram.3 SMR Baseline SystemAs mentioned in the introduction, SMR and SMT arebased on the same principles.3.1 ConceptThe aim of SMR consists in using an SMT system to dealwith reordering problems.
Therefore, the SMR systemcan be seen as an SMT system which translates from anoriginal source language (S) to a reordered source lan-guage (S?
), given a target language (T).3.2 DescriptionFigure 1 shows the SMR block diagram and an exam-ple of the input and output of each block inside theSMR system.
The input is the initial source sentence(S) and the output is the reordered source sentence (S?
).There are three blocks inside SMR: (1) the class replac-ing block; (2) the decoder, which requires an Ngrammodel containing the reordering information; and, (3) thepost-processing block which either reorders the sourcesentence given the indexes of the decoder output 1-best(training step) or transforms the decoder output graph toan input graph for the SMT system (decoding step).The decoder in Figure 1 requires a translation modelwhich is an Ngram model.
Given a training parallel cor-pus this model has been built following the next steps:1.
Select source and target word classes.2.
Align parallel training sentences at the word level inboth translation directions.
Compute the union ofthe two alignments to obtain a symmetrized many-to-many word alignment.3.
Use the IBM1 Model to obtain a many-to-one wordalignment from the many-to-many word alignment.4.
Extract translation units from the computed many-to-one alignment.
Replace source words by their172Figure 2: SMR approach in the (A) training step (B) inthe test step (the weight of each arch is in brackets).classes and target words by the index of the linkedsource word.
An example of a translation unit hereis: C61 C28 C63#2 0 1, where # divides source(word classes) and target (positions).5.
Compute the sequence of the above units and learnthe language modelFor further information about the SMR training proce-dure see (Costa-jussa` and Fonollosa, 2006).3.3 Improving SMT trainingFigure 2 (A) shows the corresponding block diagramfor the training corpus: first, the given training corpusS is translated into the reordered training source corpusS?
with the SMR system.
Then, this reordered trainingsource corpus S?
and the given training target corpus Tare used to build the SMT systemThe main difference here is that the training is com-puted with the S?2T task instead of the S2T given task.Figure 3 (A) shows an example of the word alignmentcomputed on the given training parallel corpus S2T.
Fig-ure 3 (B) shows the same links but with the reorderedsource training corpus S?.
Although the quality in align-ment is the same, the tuples that can be extracted change(notice that tuple extraction is monotonic).
We now areable to extract smaller tuples which reduce the transla-tion vocabulary sparseness.
These new tuples are used tobuild the SMT system.3.4 Generation of multiple weighted reorderinghypothesesThe SMR system, having its own search, can generate ei-ther an output 1-best or an output graph.
In decoding, theSMR technique generates an output graph which is usedas an input graph by the SMT system.
Figure 2 (B) showsthe corresponding block diagram in decoding: the SMRoutput graph is given as an input graph to the SMT sys-tem.
Hereinafter, this either SMR output graph or SMTinput graph will be referred to as (weighted) reorderinggraph.
The monotonic search in the SMT system is ex-tended with reorderings following this reordering graph.Figure 3: Alignment and tuple extraction (A) originaltraining source corpus (B) reordered training source cor-pus.This reordering graph has multiple paths and each pathhas its own weight.
This weight is added as a featurefunction in the log-linear model.4 Morphological vs Statistical ClassesPrevious SMR studies (Costa-jussa` and Fonollosa,2006) (Costa-jussa` et al, 2007) considered only statisti-cal classes.
On the one hand, these statistical classes per-formed fairly well and had the advantage of being suit-able for any language.
On the other hand, it should betaken into account the fact of training them in the train-ing set alows for unknown words in the development orin the test set.
Additionally, they do not have any reorder-ing information because they are trained on a monolin-gual set.The first problem, unknown words which appear inthe development or in the test set, may be solved by us-ing a disambiguation technique.
Unknown words can beassigned to one class by taking into account their owncontext.
The second problem, incorporating informationabout order, might be solved by training classes in thereordered training source corpus.
In other words, wemonotonized the training corpus with the alignment in-formation (i.e.
reorder the source corpus in the way thatmatches the target corpus under the alignment links cri-terion).
After that, we train the statistical classes, here-inafter, called statistical reordered classes.In some pair of languages, as for example En-glish/Spanish, the reordering that may be performed isrelated to word?s morphology (i.e.
TAGS).
Some TAGSrules (with some lexical exceptions) can be extracted asin (Popovic and Ney, 2006) where they were appliedwith reordering purposes as a preprocessing step.
An-other approach that has related TAGS and reordering waspresented in (Crego and Marin?o, 2006) where instead ofrules, they learned reordering patterns based on TAGS asnamed in this paper?s introduction.
Hence, the SMR tech-173Spanish EnglishTrain Sentences 1,3MWords 37,9M 35,5MVocabulary 138,9k 133kDev Sentences 2 000 2 000Words 60.5k 58.7kVocabulary 8.1k 6.5kTest Sentences 2 000 2 000Words 60,2k 58kVocabulary 8,2k 6,5kTable 1: Corpus Statistics.nique may take advantage of the morphological informa-tion.
Notice that an advantage is that there is a TAG foreach word, hence there are not unknown words.5 Evaluation Framework5.1 Corpus StatisticsExperiments were carried out using the data in the secondevaluation campaign of the WMT07 1.This corpus consists in the official version of thespeeches held in the European Parliament Plenary Ses-sions (EPPS), as available on the web page of the Eu-ropean Parliament.
Additionally, there was available asmaller corpus (News-Commentary).
Our training cor-pus was the catenation of both.
Table 1 shows the corpusstatistics.5.2 Tools and preprocessingThe system was built similarly to (Costa-jussa` et al,2007).
The SMT baseline system uses the Ngram-based approach, which has been explained in Section 2.Tools used are defined as follows: word alignments werecomputed using GIZA++ 2; language model was esti-mated using SRILM 3; decoding was carried out withMARIE4; an n-best re-ranking strategy is implementedwhich is used for optimization purposes just as pro-posed in http://www.statmt.org/jhuws/ using the simplexmethod (Nelder and Mead, 1965) and BLEU as a lossfunction.The SMT system we use a 4gram translation languagemodel, a 5gram target language model and a 5gram classtarget language model.Spanish data have been processed so that the pronounswhich are attached to verbs are split up.
Additionally,several article and prepositions words are separated (i.e.1http://www.statmt.org/wmt07/2http://www.fjoch.com/GIZA++.html3http://www.speech.sri.com/projects/srilm/4http://gps-tsc.upc.es/veu/soft/soft/marie/Figure 5: Perplexity over the manually aligned test setgiven the SMR Ngram length.del goes into de el).
This preprocessing was performedusing Freeling software (Atserias et al, 2006).
Trainingand evaluation were both true-case.5.3 Classes and Ngram length Study for theSMR-Graph generationThis section evaluates several types of classes and n-gramlengths in the SMR model in order to choose the SMRconfiguration which provides the best results in trans-lation in terms of quality.
To accomplish this evalua-tion, we have designed the following experiment.
Given500 manually aligned parallel sentences of the EPPS cor-pora (Lambert et al, 2006), we order the source test inthe way that better matches the target set.
This orderedsource set is considered our reference as it is based onmanual alignments.
On the other hand, the 500 sen-tences set is translated using the SMR configurations tobe tested.
Finally, the Word Error Rate (WER) is used asquality measure.Figure 4 shows the WER behavior given different typesof classes.
As statistical classes (cl50,cl100,cl200) weused the Och monolingual classes (Och, 1999), whichcan be performed using ?mkcls?
(a tool available withGIZA).
Also we used the statistical reordered classes(cl100mono) which were explained in Section 4.
Bothstatistical and statistical reordered classes used the dis-amb tool of SRILM in order to classify unknown words.As morphological classes we used the TAGS provided byFreeling.
Clearly, statistical classes perform better thanTAGS and best results can be achieved with 100 and 200classes and an n-gram length of 5.For the sake of completeness, we have evaluated theperplexity of the SMR Ngram model over the aligned testset above and choosing 200 classes.
Figure 5 is coherentwith the WER results above and it shows that perplexityis not reduced for an n-gram length greater than 5.174Figure 4: WER over the reference given various sets of classes and Ngram lengths.5.4 Graph pruningThe more complex is the reordering graph, the less effi-cient is the decoding.
That is why, in this section, we ex-periment with several ways of graph pruning.
Addition-ally, for each pruning we see the influence of consideringthe graph weights (i.e.
reordering feature importance).Given that the reordering graph is the output of a beamsearch decoder, we can consider pruning the reorderinggraph by limiting the SMR beam, i.e.
limiting the size ofhypothesis stacks.Given a reordering graph, another option is to prunestates and arches only used in paths s times worse thanthe best path.Table 2 gives the results of the proposed pruning.
Notethat computational time is given in terms of the mono-tonic translation time (and it is the same for both direc-tions).
It is shown that graph pruning guarantees the effi-ciency of the system and even increases the translation?squality.
Similar results are obtained in terms of BLEU forboth types of pruning.
In this task and for both translationdirections, it seems more appropriate to limit directly thebeam search in the SMR step to 5.As expected, the influence of the reordering feature,which takes into account the graph weights, tends to bemore important as pruning decreases (i.e.
when the graphhas more paths).Pruning Wr BLEUEn2Es BLEUEs2En TIMEb5 yes 31.32 32.64 2.4Tmb5 no 31.25 31.82 2.5Tmb50 yes 30.95 32.28 5.3Tmb50 no 30.90 27.44 4.8Tmb50 s10 yes 31.19 32.20 1.5Tmb50 s10 no 31.07 32.41 1.4TmTable 2: Performance in BLEU in the test set of differentgraph pruning (b stands for beam and s for states); theuse of reordering feature function (Wr indicates its use);and the time increase related to Tm (monotonic transla-tion time).5.5 Results and discussionTable 3 shows the performance of our Ngram-based system using the SMR technique.
Firstrow is the WMT07 baseline system which canbe reproduced following the instructions inhttp://www.statmt.org/wmt07/baseline.html.
Thisbaseline system uses a non-monotonic search.
Secondrow shows the results of the Ngram-based systempresented in section 2 using the weighted reorderinggraph trained with the best configuration found in theabove section (200 statistical classes and an Ngram oflength 5).175System BLEUes2en BLEUen2esWMT07 Of.
Baseline 31.21 30.74Ngram-based 32.64 31.32Table 3: BLEU Results.6 Conclusions and further workThe proposed SMR technique can be used both in trainingand test steps in a SMT system.
Applying the SMR tech-nique in the training step reduces the sparseness in thetranslation vocabulary.
Applying SMR technique in thetest step allows to generate a weighted reordering graphfor SMT system.The use of classes plays an important role in the SMRtechnique, and experiments have shown that statisticalclasses are better than morphological ones.Moreover, we have experimented with different graphpruning showing that best translation results can beachieved at a very low increase of computational costwhen comparing to the monotonic translation computa-tional cost.Finally, we have shown that our translation system us-ing the SMR technique outperforms the WMT07 Officialbaseline system (which uses a non-monotonic search) interms of BLEU.As further work, we want to introduce the SMR tech-nique in a state-of-the-art phrase-based system.7 AcknowledgmentsThis work has been funded by the European Union underthe TC-STAR project (IST- 2002-FP6-506738) and theSpanish Government under grant TEC2006-13964-C03(AVIVAVOZ project).ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonza?lez,L.
Padro?, and M. Padro?.
2006.
Freeling 1.3: Syntacticand semantic services in an open-source nlp library.
In5th Int.
Conf.
on Language Resource and Evaluation(LREC), pages 184?187.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine transla-tion.
Computational Linguistics, 19(2):263?311.M.
Collins, P. Koehn, and I. Kucerova?.
2005.
Clauserestructuring for statistical machine translation.
In 43stAnnual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 531 ?
540, Michigan.M.R.
Costa-jussa` and J.A.R.
Fonollosa.
2006.
Statisticalmachine reordering.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 71?77, Sydney.M.
R. Costa-jussa`, P. Lambert, J.M.
Crego, M. Khalilov,J.A.R.
Fonollosa, J.B. Marin?o, and R. Banchs.
2007.Ngram-based statistical machine translation enhancedwith multiple weighted reordering hypotheses.
InACL: Workshop of Statistical Machine Translation(WMT07), Prague.J.M.
Crego and J.B. Marin?o.
2006.
Reordering exper-iments for n-gram-based smt.
Ist IEEE/ACL Inter-national Workshop on Spoken Language Technology(SLT?06), pages 242?245.J.
M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.Fonollosa.
2005.
Ngram-based versus phrase-based statistical machine translation.
In Proc.
ofthe Int.
Workshop on Spoken Language Translation,IWSLT?05, pages 177?184, Pittsburgh, October.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In Proceedings of theACL Workshop on Building and Using Parallel Texts:Data-Driven Machine Translation and Beyond, pages167?174, Ann Arbor, MI, June.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proc.
of the HumanLanguage Technology Conference, HLT-NAACL?2003,pages 48 ?
54, Edmonton, Canada, May.P.
Lambert, A. de Gispert, R. Banchs, and J. Marin?o.2006.
Guidelines for word alignment and man-ual alignment.
Language Resources and Evaluation,39(4):267?285.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram based machine translation.
Computa-tional Linguistics, 32(4):527?549.J.A.
Nelder and R. Mead.
1965.
A simplex method forfunction minimization.
The Computer Journal, 7:308?313.F.J.
Och.
1999.
An efficient method for determiningbilingual word classes.
In 9th Conf.
of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 71?76, June.M.
Popovic and H. Ney.
2006.
Pos-based word reorder-ings for statistical machine translation.
In 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC), pages 1278?1283, Genova, May.176
