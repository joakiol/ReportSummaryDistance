Word Sense Disambiguation using Static and Dynamic SenseVectorsJong-Hoon Oh, and Key-Sun ChoiComputer Science Division, Dept.
of EECS, Korea Advanced Institute of Science & Technology(KAIST) / Korea Terminology Research Center for Language and Knowledge Engineering(KORTERM), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, KoreaEmail: {rovellia,kschoi}@world.kaist.ac.krAbstractIt is popular in WSD to use contextualinformation in training sense tagged data.Co-occurring words within a limitedwindow-sized context support one senseamong the semantically ambiguous ones ofthe word.
This paper reports on word sensedisambiguation of English words usingstatic and dynamic sense vectors.
First,context vectors are constructed usingcontextual words 1  in the training sensetagged data.
Then, the words in the contextvector are weighted with local density.Using the whole training sense tagged data,each sense of a target word2 is representedas a static sense vector in word space, whichis the centroid of the context vectors.
Thencontextual noise is removed using aautomatic selective sampling.
A automaticselective sampling method use informationretrieval technique, so as to enhance thediscriminative power.
In each test case, aautomatic selective sampling methodretrieves N relevant training samples toreduce noise.
Using them, we constructanother sense vectors for each sense of thetarget word.
They are called dynamic sensevectors because they are changed accordingto a target word and its context.
Finally, aword sense of a target word is determinedusing static and dynamic sense vectors.
TheEnglish SENSEVAL test suit is used for thisexperimentation and our method producesrelatively good results.1?Contextual words?
is defined as a list of contentwords in context.2In this paper, a target word ?Wt?
is a semantically1.
IntroductionIt is popular in WSD to use contextualinformation in training data (Agirre, et al,19963; Escudero, et al, 2000; Gruber, 1991;Schutze, 1998).
Co-occurring words within alimited window-sized context support one senseamong the semantically ambiguous ones of theword.
The problem is to find the most effectivepatterns in order to capture the right sense.
It istrue that they have similar context andco-occurrence information when words are usedwith the same sense (Rigau, et al, 1997).
It isalso true that contextual words nearby anambiguous word give more effective patterns orfeatures than those far from it (Chen, et al,1998).
In this paper, we represent each sense ofa word as a vector in word space.
First,contextual words in the training sense taggeddata4 are represented as context vectors.
Then,ambiguous word in a given context of ?Wt?.
Thiscontext may consist of several sentences and it isrepresented by ?contextual words?.3Agirre et al, (1996) defines a term ?conceptualdensity?
based on how many nodes are hit betweenWordNet node and target words+contexts.
Unlike?Conceptual density?, ?local density?
used in thispaper does not use any semantic net like WordNetbut use only the contextual words surrounding thegiven target word..4In this paper, the English SENSEVAL-2 data forthe lexical sample task is used as training sensetagged data.
It is sampled from BNC-2, the PennTreebank (comprising components from the WallStreet Journal, Brown, and IBM manuals) and so on.All items in the lexical sample are specific to oneword class; noun, verb or adjective.
Training sensetagged data is composed of training samples thatsupport a certain sense of a target word.
They containthe words in the context vector are weightedwith local density.
Then, each sense of a targetword can be represented as a sense vector, whichis the centroid of the context vectors in wordspace.However, if training samples contain noise, it isdifficult to capture effective patterns for WSD(Atsushi, et al, 1998).
Word occurrences in thecontext are too diverse to capture the rightpattern for WSD.
It means that the dimension ofcontextual words will be very large when wewill use all words in the training samples forWSD.
To avoid the problems, we use anautomatized hybrid version of selectivesampling that will be called ?automatic selectivesampling?.
This automatization is based oncosine similarity for the selection.
For a giventarget word and its context, this method retrievesN-best relevant training samples using the cosinesimilarity.
Using them, we can construct anothersense vectors for each sense of the target word.The relevant training samples are retrieved bycomparing cosine similarities between givencontexts and indexed context vectors of trainingsamples.
The ?automatic selective sampling?method makes it possible to use traning sampleswhich have higher discriminative power.This paper is organized as follows: section 2shows details of our method.
Section 3 dealswith experiments.
Conclusion and future worksare drawn in sections 4.2 Word Sense DisambiguationMethod2.1 Overall System DescriptionFigure 1 shows the overall system description.The system is composed of a training phase anda test phase.
In the training phase, words in thelimited context window of training samples,which contains a target word and its sense, areextracted and the words are weighted with localdensity concept (section 2.2).
Then, contextvectors, which represent each training sample,are indexed and static sense vectors for eacha target word , its sense and its context.
But the senseof contexual words is not annotated in the trainingsamples (SENSEVAL-2, 2001)sense are constructed.
A static sense vector is thecentroid of context vectors of training sampleswhere a target word is used as a certain sense(section 2.3).
For example, two sense vectors of?bank?
can be constructed using context vectorsof training samples where ?bank?
is used as?business establishment?
and those where ?bank?is used as ?artificial embankment?.
Each contextvector is indexed for  ?automatic selectivesampling?.Training samplesWord extraction inlocal contextTerm weighting withlocal densityIndexing contextvectorContext vectorsfor each trainingsampleConstructing staticsense vectors foreach senseIndex foreach trainingsampleStatic sensevectors foreach senseAutomatic selectivesamplingEstimating a wordsenseDynamic sensevectors for the givencontextTest samplesWord SensesConstructingDynamic vectorsRetrieved N trainingsamplesTraining TestingMorphologicalanalyzerFig.
1 The overall system descriptionIn the test phase, contextual words are extractedwith the same manner as in the training phase(section 2.5).
Then, the ?automatic selectivesampling?
module retrieves top-N trainingsamples.
Cosine similarity between indexedcontext vectors of training samples, and thecontext vector of a given test sample providesrelevant training samples.
Then we can makeanother sense vectors for each sense using theretrieved context vectors.
Since, the sensevectors produced by the automatic selectivesampling method are changed according to testsamples and their context, we call them dynamicsense vectors in this paper (section 2.4) (Notethat, the sense vectors produced in the trainingphase are not changed according to test samples.Thus, we call them static sense vectors.
)The similarities between dynamic sense vectors,and a context vector of a test sample, and thosebetween static sense vectors and the contextvector of the test sample are estimated by cosinemeasure.
The sense with the highest similarity isselected as the relevant word sense.Our proposed method can be summarized asfollowsTraining Phase1) Constructing context vectors usingcontextual words in training sensetagged data.2) Local density to weight terms incontext vectors.3) Creating static sense vectors, whichare the centroid of the contextvectors.Test Phase1) Constructing context vectors usingcontextual words in test data.2) Automatic selective sampling oftraining vectors in each test case toreduce noise.3) Creating dynamic sense vectors,which are the centroid of thetraining vectors for each sense.4) Estimating word senses using staticand dynamic sense vectors.2.2 Representing Training Samples as aContext Vector with Local DensityIn WSD, context must reflect various contextualcharacteristics5.
If the window size of context istoo large, the context cannot contain relevantinformation consistently (Kilgarriff et al, 2000).Words in this context window6 can be classifiedinto nouns, verbs, and adjectives.
The classifiedwords within the context window are assumed toshow the co-occurring behaviour with the targetword.
They provide a supporting vector for acertain sense.
Contextual words nearby a targetword give more relevant information to decideits sense than those far from it.
Distance from atarget word is used for this purpose and it iscalculated by the assumption that the targetwords in the context window have the samesense (Yarowsky, 1995).Each word in the training samples can beweighted by formula (1).
Let Wij(tk) represent aweighting function for a term tk, which appearsin the jth training sample for the ith sense, tfijk5  POS, collocations, semantic word associations,subcategorization information, semantic roles,selectional preferences and frequency of senses areuseful for WSD (Agirre et al, 2001).6  Since, the length of context window wasconsidered when SENSEVAL-2 lexical sample datawere constructed, we use a training sample itself ascontext window.represent the frequency of a term tk in the jthtraining sample for the ith sense, dfik  representthe number of training samples for the ith sensewhere a term tk appears, Dijk represent theaverage distance of a term tk from the targetword in the jth training sample for the ith sense,and Ni represent the number of training samplesfor the ith sense.ijijkkij ZZtW =)(     (1)where,??????????
?=ikikijkijkijk NNDFdfDtfZ 1??
==sensesallikksensesalli dfDFNN__,( )?==termofkijkij ZZ__#12In formula (1), Z is a normalization factor,which forces all values of Wij(tk) to fall intobetween 0 and 1, inclusive (Salton et al, 1983).Formula (1) is a variation of tf-idf.
We regardeach training sample as indexed documents,which we want to retrieve and a test sample as aquery in information retrieval system.
Becausewe know a target word in training samples andtest samples, we can restrict search space intotraining samples, which contain the target wordwhen we find relevant samples.
We also takeinto account distance from the target word.Dijk and dfik in formula (1) support a localdensity concept.
In this paper, ?local density?
ofa target word ?Wt?
is defined by the densityamong contextual words of ?Wt?
in terms of theirin-between distance and relative frequency.
First,the distance factor is one of the important cluesbecause contextual words surrounding a targetword frequently support a certain sense: forexample, ?money?
in ?money in a bank?.Second, if contextual words frequently co-occurwith a target word of a certain sense, they maybe a strong evidence to decide what word senseis correct.
Therefore, contextual words, whichmore frequently appear near a target word andappear with a certain sense of a target word,have a higher local density.With the local density concept, context oftraining samples can be represented by a vectorwith context words and their weight, such that(wij(t1),wij(t2),?.,wij(tn)).
When Wij(tk) is 1, itmeans that tk is strong evidence for the ith sense.
(Zijk are much larger than others.
)2.3 Constructing Static Sense VectorsNow, we can represent each training sample ascontext vectors using contextual words such thatvij=(wij(t1),wij(t2),?.,wij(tn)) where vij represents acontext vector of the jth training sample for the ithsense and wij(tk) is the weight of a term tkcalculated by formula (1).||||1iNjiji NvSVi?==(2)Context vectorsfor Sense 1Context vectorsfor Sense 2Context vectorfor Sense n?2SV1SVnSVFig.2 A graphical representation of static sensevectorsThroughout clustering the context vectors, eachsense can be represented as sense vectors.
Let Nirepresent the number of training samples for theith sense, and vij represent the context vector ofthe jth training sample for the ith sense.
The staticsense vector for the ith sense, SVi, can berepresented by formula (2) (Park, 1997).
Informula (2), SVi is the centroid of contextvectors of training samples for the ith sense asshown in figure 2.
In figure 2, there are n sensesand context vectors, which represent eachtraining sample.
We can categorize each contextvector according to a sense of a target word.Then, each sense vectors are acquired usingformula (2).
Because the sense vectors are notchanged according to test samples, we call thema static sense vector in this paper (note that sensevectors, which we will describe in section 2.4,are changed depending on the context of testsamples).2.4 Automatic selective sampling: DynamicSense VectorsIt is important to capture effective patterns andfeatures from the training sense tagged data inWSD.
However, if there is noise in the trainingsense tagged data, it makes difficult todisambiguate word senses effectively.
To reduceits negative effects, we use a automatic selectivesampling method using cosine similarity.
Figure3 shows the process of a automatic selectivesampling method.
The upper side showsretrieval process and the lower side shows agraphical representation of dynamic sensevectors.Sense 1 Sense 2 Sense n..RetrievedTrainingSamples....A target wordDSV1 DSV2 DSVn?
?A Context vector for a test sampleIndexedTrainingSamplesContext vectorsfor Sense 1Context vectorsfor Sense 2Context vectorsfor Sense n?2DSV1DSVnDSV A context vectorof a test sampleRetrieved top-Ntraining sampleFig.
3 A graphical representation of an automaticselective sampling methodFor example, let ?bank?
have two senses(?business establishment?, ?artificialembankment?).
Now, there are indexed trainingsamples for the two senses.
Then top-N trainingsamples can be acquired for a given test samplecontaining a target word ?bank?.
The retrievedtraining samples can be clustered as DynamicSense Vectors according to a sense of theirtarget word.
Since, the sense vectors producedby a automatic selective sampling method arechanged according to the context vector of a testsample, we call them dynamic sense vectors inthis paper.Let RTi represent the number of training samplesfor the ith sense in the retrieved top-N, and vijrepresent a context vector of the jth trainingsample for the ith sense in the top-N. Thedynamic sense vector for the ith sense of a targetword, DSVi, is formulated by formula (3).
Informula (3), DSVi means the centroid of theretrieved context vectors of training samples forthe ith sense as shown in the lower side offigure.3||||1iRTjiji RTvDSVi?==(3)2.5 Context Vectors of a Test SampleContextual words in a test sample are extractedas the same manner as in the training phase.
Theclassified words in the limited window size ?nouns, verbs, and adjectives ?
offer componentsof context vectors.
When a term tk appears in thetest sample, the value of tk in a context vector ofthe test sample will be 1, in contrary, when tkdoes not appear in the test sample, the value of tkin a context vector of the test sample will be 0.Let contextual words of a test sample be ?bank?,?river?
and ?water?, and dimension of contextvector be (?bank?, ?commercial?, ?money?,?river?, ?water?).
Then we can acquire a contextvector, CV =(1,0,0,1,1), from the test sample.Henceforth we will denote CVi as a contextvector for the ith test sample.2.6 Estimating a Word Sense: ComparingSimilarityWe described the method for constructing staticsense vectors, dynamic sense vectors andcontext vectors of a test sample.
Next, we willdescribe the method for estimating a word senseusing them.
The similarity in informationretrieval area is the measure of how alike twodocuments are, or how alike a document and aquery are.
In a vector space model, this isusually interpreted as how close theircorresponding vector representations are to eachother.
A popular method is to compute thecosine of the angle between the vectors (Saltonet al, 1983).
Since our method is based on avector space model, the cosine measure (formula(4)) will be used as the similarity measure.Throughout comparing similarity between SViand CVj and between DSVi and CVj for the ithsense and the jth test sample, we can estimate therelevant word sense for the given context vectorof the test sample.
Formula (5) shows acombining method of sim(SVi,CVj) andsim(DSVi,CVj).
Let CVj represent the contextvector of the jth test sample, si represent the ithsense of a target word, and Score(si,CVj)represent score between the ith  sense and thecontext vector of the jth test sample.??
?====Ni iNi iNi iiwvwvwvsim12121),(  (4)where, N represents the dimension of the vectorspace, v and w represent vectors.),()1(),(),(maxargjijijisCVDSVsimCVSVsimCVsScorei??+?=??
(5)where ?
is a weighting parameter.Because the value of cosine similarity falls intobetween 0 and 1, that of Score(si,CVj) also existsbetween 0 and 1.
When similarity value is 1 itmeans perfect consensus, in contrary, whensimilarity value is 0 it means there is no part ofagreement at all.
After all, the sense havingmaximum similarity by formula (5) is decided asthe answer.3.
Experiment3.1 Experimental SetupIn this paper, we compared six systems asfollows.The system that assigns a word sensewhich appears most frequently in thetraining samples (Baseline)The system by the Na?ve Bayesianmethod (A) (Gale, et al, 1992)The system that is trained byco-occurrence information directlywithout changing.
(only with termfrequency) (B)The system with local density andwithout automatic selective sampling(C)The system with automatic selectivesampling and without local density (D)The system with local density andautomatic selective sampling (E)System A was used to compare our method withthe other method.
System B, C, D, and E willshow the performance of each component in ourproposed method.
To evaluate performance inthe condition of ?without local density (system Band D)?, we weight each word with its frequencyin the context of training samples.The test suit used is the English lexical samplesreleased for SENSEVAL-2 in 2001.
This testsuit supplies training sense tagged data and testdata for noun, verb and adjective(SENSEVAL-2, 2001).Cross-validation on training sense tagged data isused to determine the parameters ?
?
informula (5) and top-N in constructing dynamicsense vectors.
We divide training sense taggeddata into ten folds with the equal size, anddetermine each parameter, which makes the bestresult in average from ten-fold validation.
Thevalues, we used, are 2.0=?
, and 50 =N .The results were evaluated by precision rates(Salton, et al, 1983).
The precision rate isdefined as the proportion of the correct answersto the generated results.3.2 Experimental ResultsNoun Verb Adjective TotalBaseline 50.97% 40.34% 58.04% 47.60%A 44.04% 32.48% 43.43% 39.09%B 24.33% 21.31% 26.92% 23.50%C 44.44% 33.81% 45.38% 40.15%D 65.47% 49.64% 66.84% 59.09%E 66.89% 53.74% 70.74% 62.07%Table 1.
Experimental resultsTable 1 shows experimental results.
In the result ,all systems and baseline show higherperformance on noun and adjective than verb.This indicates that the disambiguation of verb ismore difficult than others in this test suit.
Inanalysing errors, we found that we did notconsider important information fordisambiguating verb senses such as adverbs,which can be used as idioms with the verbs.
Forexample, ?carry out?, ?pull out?
and so on.
It isnecessary to handle them for more effectiveWSD.System B, C, D, and E show how effective localdensity and dynamic vectors are in WSD.
Theperformance increase was shown about 70%with local density (system C) and about 150%with dynamic vectors (system D), when they arecompared with system B ?
without local densityand dynamic vectors.
This shows that localdensity is more effective than term frequency.This also shows that automatic selectivesampling of training samples in each test sampleis very important.Combining local density and dynamic vectors(system E), we acquire about 62% performance.Our method also shows higher performance thanbaseline and system A (the Na?ve Bayesianmethod) ?
about 30% for baseline and about58% for system A.As a result of this experiment, we proved thatco-occurrence information throughout the localdensity and the automatic selective sampling ismore suitable and discriminative in WSD.
Thistechniques lead up to 70% ~ 150% performanceimprovement in the experimentation comparingthe system without local density and automaticselective sampling.4.
ConclusionThis paper reported about word sensedisambiguation for English words using staticand dynamic sense vectors.
Content words ?noun, verb, and adjective ?
in the context wereselected as contextual words.
Local density wasused to weight words in the contextual window.Then we constructed static sense vectors foreach sense.
A automatic selective samplingmethod was used to construct dynamic sensevectors, which had more discriminative power,by reducing the negative effects of noise in thetraining sense tagged data.
The answer wasdecided by comparing similarity.
Our method issimple but effective for WSD.Our method leads up to 70~150% precisionimprovement in the experimentation comparingthe system without local density and automaticselective sampling.
We showed that our methodis simple but effective.
Our method wassomewhat language independent, because ourmethod used only POS information.
Syntacticand semantic features such as dependencyrelations, approximated word senses ofcontextual words and so on may be useful toimprove the performance of our method.ReferencesAgirre, E. and G. Rigau (1996) Word SenseDisambiguation using Conceptual Density,Proceedings of 16th International Conference onComputational Linguistics(COLING96),Copenhagen, Denmark.Agirre, E. and D. Martinez, (2001) KnowledgeSources for Word Sense Disambiguation,Proceedings of the Fourth InternationalConference (TSD 2001).Fujii, Atsushi , Kentaro Inui, Takenobu Tokunaga,and Hozumi Tanaka, (1998) Selective Sampling forExample-based Word Sense Disambiguation,Computational Linguistics, 24(4), pp.
573-597.Escudero, G., L. M?rquez and G. Rigau (2000)Boosting Applied to Word Sense Disambiguation,Proceedings of the 11th European Conference onMachine Learning (ECML 2000) Barcelona, Spain.2000.
Lecture Notes in Artificial Intelligence 1810.R.
L. de M?ntaras and E. Plaza (Eds.).
SpringerVerlag.Gale, William A., Kenneth W. Church, and DavidYarowsky (1992) A Method for DisambiguatingWord Senses in a Large Corpus.
Computers andHumanities, 26, 415-439.Gruber, T. R. (1991) Subject-DependentCo-occurrence and Word Sense Disambiguation,Proceedings of 29th Annual Meeting of theAssociation for Computational Linguistics.Schutze, Hinrich (1998) Automatic Word SenseDiscrimination.
Computational Linguistics, 24(1),97-123.Chen , Jen Nan and Jason S. Chang (1998) AConcept-based Adaptive Approach to Word SenseDisambiguation, Proceedings of 36th AnnualMeeting of the Association for ComputationalLinguistics and  17th International Conference onComputational Linguistics (COLING/ACL-98) pp237-243.Kilgarriff, A. and J. Rosenzweig, (2000) EnglishSENSEVAL: Report and Results, Proceedings of2nd International Conference on LanguageResources & Evaluation (LREC 2000), Athens.Park,Y.C (1997) ?Building word knowledge forinformation retrieval using statistical information?,Ph.D.
thesis, Department of Computer Science,Korea Advanced Institute of Science andTechnology.Rigau, G., J. Atserias and E. Agirre, (1997)Combining Unsupervised Lexical KnowledgeMethods for Word Sense Disambiguation,Proceedings of joint 35th Annual Meeting of theAssociation for Computational Linguistics and 8thConference of the European Chapter of theAssociation for Computational Linguistics(ACL/EACL?97), Madrid, Spain.Salton, G. and M.  McGill, (1983) Introduction toModern Information Retrieval, McGraw-Hill, NewYork.SENSEVAL-2 (2001)http://www.sle.sharp.co.uk/senseval2/Yarowsky, D. (1995) Unsupervised Word SenseDisambiguation Rivaling Supervised Methods, InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics,Cambridge, MA, 189-196.
