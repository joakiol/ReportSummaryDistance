Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 100?109,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsAggregating Continuous Word Embeddings for Information RetrievalSte?phane ClinchantXerox Research Centre Europestephane.clinchant@xrce.xerox.comFlorent PerronninXerox Research Centre Europeflorent.perronnin@xrce.xerox.comAbstractWhile words in documents are generallytreated as discrete entities, they can beembedded in a Euclidean space whichreflects an a priori notion of similaritybetween them.
In such a case, a textdocument can be viewed as a bag-of-embedded-words (BoEW): a set of real-valued vectors.
We propose a noveldocument representation based on suchcontinuous word embeddings.
It con-sists in non-linearly mapping the word-embeddings in a higher-dimensional spaceand in aggregating them into a document-level representation.
We report retrievaland clustering experiments in the casewhere the word-embeddings are computedfrom standard topic models showing sig-nificant improvements with respect to theoriginal topic models.1 IntroductionFor many tasks such as information retrieval (IR)or clustering, a text document is represented bya vector, where each dimension corresponds toa given word and where each value encodes theword importance in the document (Salton andMcGill, 1983).
This Vector Space Model (VSM)or bag-of-words (BoW) representation is at theroot of topic models such as Latent Semantic In-dexing (LSI) (Deerwester, 1988), Probablistic La-tent Semantic Analysis (PLSA) (Hofmann, 1999)or Latent Dirichlet Allocation (LDA) (Blei et al2003).
All these topic models consist in ?pro-jecting?
documents on a set of topics generallylearned in an unsupervised manner.
During thelearning stage, as a by-product of the projec-tion of the training documents, one also obtainsan embedding of the words in a typically small-dimensional continuous space.
The distance be-tween two words in this space translates the mea-sure of similarity between words which is capturedby the topic models.
For LSI, PLSA or LDA, theimplicit measure is the number of co-occurrencesin the training corpus.In this paper, we raise the following question:if we were provided with an embedding of wordsin a continuous space, how could we best use it inIR/clustering tasks?
Especially, could we developprobabilistic models which would be able to bene-fit from this a priori information on the similaritybetween words?
When the words are embeddedin a continuous space, one can view a documentas a Bag-of-Embedded-Words (BoEW).
We there-fore draw inspiration from the computer visioncommunity where it is common practice to rep-resent an image as a bag-of-features (BoF) whereeach real-valued feature describes local proper-ties of the image (such as its color, texture orshape).
We model the generative process of em-bedded words using a mixture model where eachmixture component can be loosely thought of asa ?topic?.
To transform the variable-cardinalityBoEW into a fixed-length representation which ismore amenable to comparison, we make use of theFisher kernel framework of Jaakkola and Haussler(Jaakkola and Haussler, 1999).
We will show thatthis induces a non-linear mapping of the embed-ded words in a higher-dimensional space wheretheir contributions are aggregated.We underline that our contribution is not theapplication of the FK to text analysis (see (Hof-mann, 2000) for such an attempt).
Knowing thatwords can be embedded in a continuous space,our main contribution is to show that we canconsequently represent a document as a bag-of-embedded-words.
The FK is just one possible wayto subsequently transform this bag representationinto a fixed-length vector which is more amenableto large-scale processing.The remainder of the article is organized as fol-100lows.
In the next section, we review related works.In section 3, we describe the proposed frameworkbased on embedded words, GMM topic modelsand the Fisher kernel.
In section 4, we report anddiscuss experimental results on clustering and re-trieval tasks before concluding in section 5.2 Related WorksWe provide a short review of the literature on thosetopics which are most related to our work: topicmodels, word embeddings and bag-of-patches rep-resentations in computer vision.Topic models.
Statistical topic models build onthe idea of Latent Semantic Indexing (LSI) in aprobabilistic way.
The PLSA model proposed byHoffman (Hofmann, 1999) can be thought of as aconstrained matrix factorization problem equiva-lent to NMF (Lee and Seung, 1999; Gaussier andGoutte, 2005).
Latent Dirichlet Allocation (LDA)(Blei et al 2003) , the generative counterpart ofPLSA, has played a major role in the developmentof probabilistic models for textual data.
As a re-sult, it has been extended or refined in a count-less studies (Griffiths et al 2004; Eisenstein et al2011; Hoffman et al 2010).
Statistical topic mod-els are often evaluated with the perplexity mea-sure on a held-out dataset but it has been shownthat perplexity only correlates weakly with humanpreference (Chang et al 2009).
Moreover, sev-eral studies reported that LDA does not generallyoutperform LSI in IR or sentiment analysis tasks(Wang et al 2011; Maas et al 2011).Nevertheless, LSI has known a resurging inter-est.
Supervised Semantic Indexing (SSI) (Bai etal., 2009) learns low-rank projection matrices onquery-document pairs so as to minimize a rank-ing loss.
Similarly, (Wang et al 2011) studies theinfluence of `1 and `2 regularization on the pro-jection matrices and shows how to distribute thealgorithm using Map-Reduce.Word embeddings.
Parrallel to the large devel-opment of statistical topic models, there has beenan increasing amount of literature on word em-beddings where it has been proposed to includehigher-level dependencies between words, eithersyntactic or semantic.
We note that topic mod-els such as LSI, PLSA or LDA implicitly performsuch an embedding (jointly with the embedding ofdocuments) and that the measure of similarity isthe co-occurrence of words in the training corpus.A seminal work in this field is the one by Col-lobert and Weston (Collobert and Weston, 2008)where a neural network is trained by stochasticgradient descent in order to minimize a loss func-tion on the observed n-grams.
This work has laterthen been refined in (Bengio et al 2009).
Proba-bilistic methods have also been proposed to learnlanguage models such as the HLBL embedding(Mnih and Hinton, 2007).Similarly, (Maas et al 2011) parametrizes aprobabilistic model in order to capture word repre-sentations, instead of modeling individually latenttopics, which lead to significant improvementsover LDA in sentiment analysis.
Furthermore,(Dhillon et al 2011) uses the Canonical Correla-tion Analysis technique between the left and rightcontext vectors of a word to learn word embed-dings.
Lastly, (Turian et al 2010) proposes anempirical comparison of several word embeddingtechniques in a named entity recognition task andprovides an excellent state-of-the-art of word rep-resentation.
Except (Maas et al 2011) , there hasbeen very little work to your knowledge bridgingthe statistical topic models with the word embed-ding techniques.Computer vision.
In modern computer vision,an image is usually described by a set of local de-scriptors extracted from small image patches suchas SIFT.
This local representation provides someinvariance to changes in viewpoint, lighting orocclusion.
The local descriptors characterize thelow-level properties of the image such as its color,texture or shape.
Since it is computationally in-tensive to handle (e.g.
to match) sets of descrip-tors of variable cardinality, it has been proposed toaggregate the local descriptors into a global vectorwhich is more amenable to retrieval and classifica-tion.The most popular aggregation mechanism wasdirectly inspired by the work in text analysis.
Itmakes use of an intermediate representation ?
thevisual vocabulary ?
which is a set of prototypicaldescriptors ?
the visual words ?
obtained through aclustering algorithm such as k-means (Leung andMalik, 1999; Sivic and Zisserman, 2003; Csurkaet al 2004).
Given an image, each of its de-scriptors is assigned to its closest visual word andthe image is described by the histogram of visualwords frequencies.
This representation is referredto as the Bag-of-Visual-words (BoV).Some works pushed the analogy with text anal-ysis even further.
For instance, in large-scale re-101trieval, Sivic and Zisserman proposed to use a tf-idf weighting of the BoV vector and an invertedfile for efficient matching (Sivic and Zisserman,2003).
As another example, pLSA, LDA and theirmany variations have been extensively applied toproblems such as image classification (Quelhaset al 2005) or object discovery (Russell et al2006).
However, it has been noted that the quan-tization process mentioned above incurs a loss ofinformation since a continuous descriptor is trans-formed into a discrete value (the index of the clos-est visual word).
To overcome this limitation, sev-eral improvements have been proposed which de-part from the pure discrete model.
These improve-ments include the soft assignment of descriptorsto visual words (Farquhar et al 2005; Philbin etal., 2008; Gemert et al 2008) or the use of moreadvanced coding techniques than vector quanti-zation such as sparse coding (Yang et al 2009)or locality-constrained linear coding (Wang et al2010).All the previous techniques can be understood(with some simplifications) as simple countingmechanisms (computation of 0-order statistics).
Ithas been proposed to take into account higher-order statistics (first and second order for instance)which encode more descriptor-level informationand therefore incur a lower loss of information.This includes the Fisher vector (Perronnin andDance, 2007; Perronnin et al 2010), which wasdirectly inspired by the Fisher kernel of Jaakkolaand Haussler (Jaakkola and Haussler, 1999).
In anutshell, the Fisher vector consists in modeling thedistribution of patches in any image with a Gaus-sian mixture model (GMM) and then in describingan image by its deviation from this average prob-ability distribution.
In a recent evaluation (Chat-field et al 2011), it has been shown experimen-tally that the Fisher vector was the state-of-the-artrepresentation for image classification.
However,in this work we question the treatment of wordsas discrete entities.
Indeed, intuitvely some wordsare closer to each other from a semantic standpointand words can be embedded in a continuous spaceas is done for instance in LSA.3 The Bag-of-Embedded-Words (BoEW)In this work, we draw inspiration from the workin the computer vision community: we model thegeneration process of words with continuous mix-ture models and use the FK for aggregation.The proposed bag-of-embedded-words pro-ceeds as follows: Learning phase.
Given an un-labeled training set of documents:1.
Learn an embedding of words in a low-dimensional space, i.e.
lower-dimensionalthan the VSM.
After this operation, eachwordw is then represented by a vector of sizee:w ?
Ew = [Ew,1, .
.
.
, Ew,e].
(1)2.
Fit a probabilistic model ?
e.g.
a mixturemodel ?
on the continuous word embeddings.Document representation.
Given a documentwhose BoW representation is {w1, .
.
.
, wT }:1.
Transform the BoW representation into aBoEW:{w1, .
.
.
, wT } ?
{Ew1 , .
.
.
, EwT } (2)2.
Aggregate the continuous word embeddingsEwt using the FK framework.Since the proposed framework is independent ofthe particular embedding technique, we will firstfocus on the modeling of the generation processand on the FK-based aggregation.
We will thencompare the proposed continuous topic model tothe traditional LSI, PLSA and LDA topic models.3.1 Probabilistic modeling and FKaggregationWe assume that the continuous word embeddingsin a document have been generated by a ?univer-sal?
(i.e.
document-independent) probability den-sity function (pdf).
As is common practice forcontinuous features, we choose this pdf to be aGaussian mixture model (GMM) since any con-tinuous distribution can be approximated with ar-bitrary precision by a mixture of Gaussians.
Inwhat follows, the pdf is denoted u?
where ?
={?i, ?i,?i, i = 1 .
.
.K} is the set of parametersof the GMM.
?i, ?i and ?i denote respectivelythe mixture weight, mean vector and covariancematrix of Gaussian i.
For computational reasons,we assume that the covariance matrices are diag-onal and denote ?2i the variance vector of Gaus-sian i, i.e.
?2i = diag(?i).
In practice, theGMM is estimated offline with a set of continu-ous word embeddings extracted from a represen-tative set of documents.
The parameters ?
are es-timated through the optimization of a Maximum102Likelihood (ML) criterion using the Expectation-Maximization (EM) algorithm.Let us assume that a document contains Twords and let us denote by X = {x1, .
.
.
, xT } theset of continuous word embeddings extracted fromthe document.
We wish to derive a fixed-lengthrepresentation (i.e.
a vector whose dimensionalityis independent of T ) that characterizes X with re-spect to u?.
A natural framework to achieve thisgoal is the FK (Jaakkola and Haussler, 1999).
Inwhat follows, we use the notation of (Perronnin etal., 2010).Given u?
one can characterize the sampleX us-ing the score function:GX?
= ?T?
log u?(X).
(3)This is a vector whose size depends only on thenumber of parameters in ?.
Intuitively, it describesin which direction the parameters ?
of the modelshould be modified so that the model u?
better fitsthe data.
Assuming that the word embeddings xtare iid (a simplifying assumption), we get:GX?
=T?t=1??
log u?(xt).
(4)Jaakkola and Haussler proposed to measure thesimilarity between two samplesX and Y using theFK:K(X,Y ) = GX??F?1?
GY?
(5)where F?
is the Fisher Information Matrix (FIM)of u?:F?
= Ex?u?[??
log u?(x)??
log u?(x)?]
.
(6)As F?
is symmetric and positive definite, ithas a Cholesky decomposition F?
= L??L?
andK(X,Y ) can be rewritten as a dot-product be-tween normalized vectors G?
with:GX?
= L?GX?
.
(7)(Perronnin et al 2010) refers to GX?
as the FisherVector (FV) of X .
Using a diagonal approxima-tion of the FIM, we obtain the following formulafor the gradient with respect to ?i 1:GXi =1?
?iT?t=1?t(i)(xt ?
?i?i).
(8)1we only consider the partial derivatives with respect tothe mean vectors since the partial derivatives with respect tothe mixture weights and variance parameters carry little ad-ditional information (we confirmed this fact in preliminaryexperiments).where the division by the vector ?i should be un-derstood as a term-by-term operation and ?t(i) =p(i|xt, ?)
is the soft assignment of xt to Gaussian i(i.e.
the probability that xt was generated by Gaus-sian i) which can be computed using Bayes?
for-mula.
The FV GX?
is the concatenation of the GXi ,?i .
Let e be the dimensionality of the continuousword descriptors and K be the number of Gaus-sians.
The resulting vector is e?K dimensional.3.2 Relationship with LSI, PLSA, LDARelationship with LSI.
Let n be the number ofdocuments in the collection and t be the numberof indexing terms.
Let A be the t ?
n documentmatrix.
In LSI (or NMF), A decomposes as:A ?
U?V ?
(9)where U ?
Rt?e, ?
?
Re?e is diagonal, V ?Rn?e and e is the size of the embedding space.
Ifwe choose V ?
as the LSI document embeddingmatrix ?
which makes sense if we accept the dot-product as a measure of similarity between docu-ments since A?A ?
(V ?
)(V ?)?
?
then we haveV ?
?
A?U .
This means that the LSI embeddingof a document is approximately the sum of the em-bedding of the words, weighted by the number ofoccurrences of each word.Similarly, from equations (4) and (7), it is clearthat the FV GX?
is a sum of non-linear mappings:xt ?
L???
log u?
(xt) =[?t(1)?
?1xt ?
?1?1, .
.
.
,?t(K)?
?Kxt ?
?K?K]](10)computed for each embedded-word xt.
When thenumber of Gaussians K = 1, the mapping simpli-fies to a linear one:xt ?xt ?
?1?1(11)and the FV is simply a whitened version of thesum of word-embeddings.
Therefore, if we chooseLSI to perform word-embeddings in our frame-work, the Fisher-based representation is similar tothe LSI document embedding in the one Gaus-sian case.
This does not come as a surprise inthe case of LSI since Singular Value Decomposi-tion (SVD) can be viewed as a the limite case of aprobabilistic model with a Gaussian noise assump-tion (Salakhutdinov and Mnih, 2007).
Hence, theproposed framework enables to model documentswhen the word embeddings are non-Gaussian.103Another advantage is that the proposed frameworkis rotation and scale invariant.
Indeed, while it?makes sense?
to use V ?
as the document em-bedding, in practice better results can be obtainedwhen using simply V .
Our framework is indepen-dent of such an arbitrary choice.Relationship with PLSA and LDA.
There isalso a strong parallel between topic models on dis-crete word occurrences such as PLSA/LDA andthe proposed model for continuous word embed-dings.
Indeed, both generative models include alatent variable which indicates which mixture gen-erates which words.
In the LDA case, each topic ismodeled by a multinomial distribution which indi-cates the frequency of each word for the particu-lar topic.
In the mixture model case, each mixturecomponent can be loosely understood as a ?topic?.Therefore, one could wonder if the proposedframework is not somehow equivalent to topicmodels such PLSA/LDA.
The major difference isthat PLSA, LDA and other topic models on wordcounts jointly perform the embedding of wordsand the learning of the topics.
A major deficiencyof such approaches is that they cannot deal withwords which have not been seen at training time.In the proposed framework, these two steps are de-coupled.
Hence, we can cope with words whichhave not been seen during the training of the prob-abilistic model.
We will see in section 4.3.1 thatthis yields a major benefit: the mixture model canbe trained efficiently on a small subset of the cor-pus and yet generalize to unseen words.In the same manner, our work is significantlydifferent from previous attempts at applying theFK framework to topic models such as PLSA(Hofmann, 2000; Chappelier and Eckard, 2009)or LDA (Chandalia and Beal, 2006) (we will referto such combinations as FKPLSA and FKLDA).Indeed, while FKPLSA and FKLDA can improveover PLSA and LDA respectively, they inherit thedeficiencies of the original PLSA and LDA ap-proaches, especially their unability to deal withwords unseen at training time.
We note also thatFKPLSA is extremely computationally intensive:in the recent (Chappelier and Eckard, 2009), thelargest corpus which could be handled containedbarely 7,466 documents.
In contrast, we can eas-ily handle on a single machine corpora containinghundreds of thousands of documents (see section4.2).Collection #docs #terms #classes20NG 19,995 32,902 20TDT 4,214 8,978 18(a) ClusteringCollection #docs #terms #queriesROBUST 490,779 87,223 250TREC1&-3 741,856 108,294 150CLEF03 166,754 79,008 60(b) IRTable 1: Characteristics of the clustering and IRcollections4 ExperimentsThe experiments aim at demonstrating that theproposed continuous model is competitive withexisting topic models on discrete words.
We focusour experiments on the case where the embeddingof the continuous words is obtained by LSI as itenables us to compare the quality of the documentrepresentation obtained originally by LSI and theone derived by our framework on top of LSI.
Inwhat follows, we will refer to the FV on the LSIembedding simply as the FV.We assessed the performance of the FV onclustering and ad-hoc IR tasks.
We used twodatasets for clustering and three for IR.
Using theLemur toolkit (Ogilvie and Callan, 2001), we ap-plied a standard processing pipeline on all thesedatasets including stopword removal, stemming orlemmatization and the filtering of rare words tospeed up computations.
The GMMs were trainedon 1,000,000 word occurences, which representsroughly 5,000 documents for the collections wehave used.
In what follows, the cosine similar-ity was used to compare FVs and LSI documentvectors.4.1 ClusteringWe used two well-known and publicly avail-able datasets which are 20 NewsGroup(20NG) and a subset of one TDT dataset(http://www.ldc.upenn.edu/ProjectsTDT2004,2004).
The 20NG is a classical dataset whenevaluating classifiers or clustering methods.
Forthe TDT dataset we retain only topics with morethan one hundred documents, which resulted in 18classes.
After preprocessing, the 20NG collectionhas approximately 20,000 documents and 33,000unique words and the TDT has approximately4,000 documents and 9,000 unique words.
Table104Collection Model ARI NMI20NGPLSA 41.0 57.4LDA 40.7 57.9LSI 41.0 59.5FV 45.2 60.7TDTPLSA 64.2 84.5LDA 69.4 86.4LSI 72.1 88.5FV 70.4 88.2Table 2: Clustering experiments on 20NG and theWebKB TDT Corpus: Mean performance over 20runs (in %).1 (a) gives the general statistics of the two datasetsafter preprocessing.We use 2 standard evaluation metrics to assessthe quality of the clusters, which are the AdjustedRand Index (ARI) (Hubert and Arabie, 1985) andNormalized Mutual Information (NMI) (Manningand Schu?tze, 1999).
These measures compare theclusters with respect to the partition induced bythe category information.
The ARI and NMI rangebetween 0 and 1 where 1 indicates a perfect matchwith the categories.
For all the clustering methods,the number of clusters is set to the true numberof classes of the collections and the performanceswere averaged over 20 runs.We compared spherical k-means on the FV doc-ument representations to topic models such asPLSA and LDA2.
We choose a priori an embed-ding of size e = 20 for both datasets for LSI andtherefore for the FV.
LDA and PLSA were trainedon the whole dataset.
For the FV, we varied thenumber of Gaussians (K) to analyze the evolutionof performances.
Table 2 shows the best resultsfor the FV and compares them to LSI, PLSA andLDA.
First, LDA has lower performance than LSIin our experiments as reported by several stud-ies which showed that LDA does not necessarilyoutperform LSI (Wang et al 2011; Maas et al2011).
Overall, the FV outperforms all the othermodels on 20NG and probabilistic topic modelson TDT.2We use Blei?s implementation available athttp://www.cs.princeton.edu/ blei/lda-c/4.2 RetrievalWe used three IR collections, from two evalua-tion campaigns: TREC3 and CLEF4: Table 1 (b)gives the statistics of the collections we retained:(i) ROBUST (TREC), (ii) the English subpart ofCLEF03 AdHoc Task and (iii) the TREC 1&2 col-lection, with 150 queries corresponding to topics51 to 200.
For the ROBUST and TREC 1&2 col-lections, we used standard Porter stemming.
ForCLEF, words were lemmatized.
We removed rarewords to speed up the computation of LSI.
Per-formances were measured with the Mean Aver-age Precision (MAP) over the top 1,000 retrieveddocuments.
All the collections have more than80,000 unique words and approximately 166,000documents for CLEF, 500,000 for ROBUST and741,000 for TREC.
LSI was computed on thewhole dataset and the GMMs were trained on arandom subset of 5,000 documents.
We then com-puted the FVs for all documents in the collection.Note that we did not compute topic models withLDA on these datasets as LSI provides similar per-formances to LDA (Wang et al 2011; Bai et al2009).Table 3 shows the evolution of the MAP for theLSI baseline with respect to the size of the latentspace.
Note that we use Matlab to compute sin-gular valued decompositions and that some num-bers are missing in this table because of the mem-ory limitations of our machine.
Figure 1 showsthe evolution of the MAP for different numbers ofGaussians (K) for respectively the CLEF, TRECand ROBUST datasets.
For all these plots, FV per-formances are displayed with a circle and LSI withcrosses.
We tested an embedding of size e = 50and e = 200 for the CLEF dataset, an embeddingof size e = 100 and e = 200 for the TREC datasetand e = 100 and e = 300 for ROBUST.
All thesefigures show the same trend: a) the performanceof the FV increases up to 16 Gaussians and thenreaches a plateau and b) the FV significantly out-performs LSI (since it is able to double LSI?s per-formance in several cases).
In addition, the LSIresults in table 3 (a) indicate that LSI with moredimensions will not reach the level of performanceobtained by the FV.3trec.nist.gov4www.clef-campaign.org105e 50 100 200 300 400 500CLEF 4.0 6.7 9.2 11.0 13.0 13.9TREC-1 &2 2.2 4.3 6.5 8.3 - -ROBUST 1.3 2.4 3.6 4.5 - -Table 3: LSI MAP (%) for the IR datasets for sev-eral sizes of the latent subspace.4.3 DiscussionIn the previous section we validated the good be-havior of the proposed continuous document rep-resentation.
In the following parts, we conduct ad-ditional experiments to further show the strengthsand weaknesses of the proposed approach.IR Baselines.
If the FV based on LSI word em-beddings significantly outperforms LSI, it is out-performed by strong IR baselines such as Diver-gence From Randomness (DFR) models (Amatiand Rijsbergen, 2002) or Language Models (Ponteand Croft, 1998).
This is what we show in table4 with the PL2 DFR model compared to standardTFIDF, the best FV and LSI.Collection PL2 TFIDF FV LSICLEF?03 35.7 16.4 23.7 9.2TREC-1&2 22.6 12.4 10.8 6.5ROBUST 24.8 12.6 10.5 4.5Table 4: Mean Average Precision(%) for the PL2and TFIDF model on the three IR Collectionscompared to Fisher Vector and LSIThese results are not surprising as it has beenshown experimentally in many studies that latent-based approaches such as LSI are generally out-performed by state-of-the-art IR models in Ad-Hoc tasks.
There are a significant gap in per-formances between LSI and TFIDF and betweenTFIDF and the PL2 model.
The first gap is due tothe change in representation, from a vector spacemodel to latent based representation, while thesecond one is only due to a ?better?
similarity asboth methods operate in a similar space.
In away, the FV approach offers a better similarityfor latent representations even if several improve-ments could be further proposed (pivoted docu-ment length normalization, combination with ex-act representation).05101520251  2  4  8  16  32  64 0510152025MAPKCLEFLSI e=50FV e=50LSI e=200FV e=200(a) CLEF024681012141  2  4  8  16  32  64 02468101214MAPKTRECLSI e=100FV e=100LSI e=200FV e=200(b) TREC024681012141  2  4  8  16  32  64MAPKROBUST LSI e=100FV e=100LSI e=300FV e=300(c) ROBUSTFigure 1: MAP(%) for the FV with different num-bers of Gaussians against LSI on the CLEF, TRECand ROBUST datasets4.3.1 Influence of Training Set Size andUnseen Words.One of the main benefits of our method is its abil-ity to cope with unseen words: our frameworkallows to assign probabilities for words unseenwhile training the topic model assuming that theycan be embedded in the Euclidean space.
Thus,one can train the probabilistic model on a subpartof the collection without having to discard unseenwords at test time.
Therefore, we can easily ad-dress large-scale collections as we can restrict theGMM learning step on a subset of a collection ofdocuments.
This is something that LDA cannotcope with as the vocabulary is frozen at training1060.780.80.820.840.860.880.90.921  2  4  8  16  32  64  0.780.80.820.840.860.880.90.92NMIKTDTFV e=50Yule LinearFigure 2: NMI for the FV with different Numberof Gaussians against averaging word embeddingson TDT with the Yule measuretime.
We show in figure 5 that our model is robustto the number of word occurences used for train-ing the GMM.
We illustrate this point using theTREC 1-&2 collection with an embedding of sizee = 100.
We varied the number of documents totrain the GMM.
Figure 5 shows that increasing thenumber of documents does not lead to improve-ments and the performance remains stable.
There-fore, these empirical results indeed confirm thatwe can adress large-scale collections as we can re-strict the learning step on a small subset of a col-lection of documents.4.3.2 Beyond LSI EmbeddingWhile we focused in the experimental sectionon word embeddings obtained with LSI, we nowshow that the proposed framework can be ap-plied to other word embeddings.
To do so, weuse a word embedding based on the Yule associ-ation measure (Jagarlamudi et al 2011) which isclosely related to the Mutual Information but relieson the raw frequencies rather than on probabilities.We use this measure to compute a similarity ma-trix between words.
Then, we applied a sphericalkmeans on this matrix to find e = 50 word clustersand used the cluster centroids as the word embed-ding matrix.
A simple baseline is to use as docu-ment representation the average word embeddingas is the case of LSI.
The baseline gets 82% NMIwherease the FV with 32 Gaussians reaches 88%.The non-linear mapping induced by the FV alwaysoutperforms the simple averaging.
Therefore, it isworthwhile to learn non-linear mappings.5 ConclusionIn this work, we proposed to treat documentsas bags-of-embedded-words (BoEW) and to learnM # docs MAP TREC0.5M ?
2,700 11.01M ?
5,400 11.05M ?
27,000 10.610M ?
54,000 10.6Table 5: Model performance for different subsetsused to train the GMM.
M refers to a million wordoccurencesprobabilistic mixture models once words were em-bedded in a Euclidean space.
This is a signifi-cant departure from the vast majority of the worksin the machine learning and information retrievalcommunities which deal with words as discreteentities.
We assessed our framework on severalclustering and ad-hoc IR collections and the exper-iments showed that our model is able to yield ef-fective descriptors of textual documents.
In partic-ular, the FV based on LSI embedding was shownto significantly outperform LSI for retrieval tasks.There are many possible applications and gen-eralizations of our framework.
In this study, we fo-cused on the LSI embedding and showed prelim-inary results with the Yule embedding.
Since webelieve that the word embedding technique is ofcrucial importance, we would like to experimentwith recent embedding techniques such as the Col-lobert and Weston embedding (Collobert and We-ston, 2008) which has been shown to scale well inseveral NLP tasks.Moreover, another significant advantage of theproposed framework is that we could deal seam-lessly with collections of multilingual documents.This requires the ability to embedd the words ofdifferent languages and techniques exist to per-form such an embedding including Canonical Cor-relation Analysis.
Finally, the GMM still has sev-eral theoretical limitations to model textual docu-ments appropriately so that one could design a bet-ter statistical model for bags-of-embedded-words.ReferencesGianni Amati and Cornelis Joost Van Rijsbergen.2002.
Probabilistic models of information retrievalbased on measuring the divergence from random-ness.
ACM Trans.
Inf.
Syst., 20(4):357?389.B.
Bai, J. Weston, D. Grangier, R. Collobert,K.
Sadamasa, Y. Qi, O. Chapelle, and K. Wein-berger.
2009.
Supervised semantic indexing.
InProceeding of the 18th ACM CIKM.107Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.2009.
Curriculum learning.
In ICML.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
JMLR.Gaurav Chandalia and Matthew J. Beal.
2006.
Usingfisher kernels from topic models for dimensionalityreduction.Jonathan Chang, Jordan Boyd-graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNIPS.Jean-Ce?dric Chappelier and Emmanuel Eckard.
2009.Plsi: The true fisher kernel and beyond.
InECML/PKDD (1).K.
Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisser-man.
2011.
The devil is in the details: an evaluationof recent feature encoding methods.
In BMVC.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In ICML.G.
Csurka, C. Dance, L Fan, J. Willamowski, andC.
Bray.
2004.
Visual categorization with bags ofkeypoints.
In Proc.
of ECCV Workshop on Statisti-cal Learning for Computer Vision.Scott Deerwester.
1988.
Improving Information Re-trieval with Latent Semantic Indexing.
In Proceed-ings of (ASIS ?88).Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.2011.
Multi-view learning of word embeddings viacca.
In NIPS, volume 24.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse additive generative models of text.
In LiseGetoor and Tobias Scheffer, editors, ICML, pages1041?1048.
Omnipress.Jason Farquhar, Sandor Szedmak, Hongying Meng,and John Shawe-Taylor.
2005.
Improving ?bag-of-keypoints?
image categorisation: Generative mod-els and pdf-kernels.
Technical report, University ofSouthampton.E?ric Gaussier and Cyril Goutte.
2005.
Relation be-tween PLSA and NMF and implications.
In SIGIR.Jan Van Gemert, Jan-Mark Geusebroek, Cor Veenman,and Arnold Smeulders.
2008.
Kernel codebooks forscene categorization.
In European Conference onComputer Vision (ECCV).Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2004.
Integrating top-ics and syntax.
In NIPS.Matthew D. Hoffman, David M. Blei, and FrancisBach.
2010.
Online learning for latent dirichlet allocation.
In In NIPS.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In SIGIR.
ACM.T.
Hofmann.
2000.
Learning the similarity of docu-ments: An information geometric approach to doc-ument retrieval and categorization.
In Neural Infor-mation Processing Systems.http://www.ldc.upenn.edu/ProjectsTDT2004.
2004.TDT: Annotation manual - version 1.2.Lawrence Hubert and Phipps Arabie.
1985.
Compar-ing partitions.
Journal of Classification.Tommi S. Jaakkola and David Haussler.
1999.
Ex-ploiting generative models in discriminative classi-fiers.
In NIPS, Cambridge, MA, USA.
MIT Press.Jagadeesh Jagarlamudi, Raghavendra Udupa,Hal Daume?
III, and Abhijit Bhole.
2011.
Im-proving bilingual projections via sparse covariancematrices.
In EMNLP, pages 930?940.D.
D. Lee and H. S. Seung.
1999.
Learning the partsof objects by non-negative matrix factorization.
Na-ture, 401(6755):788?791, October.T.
Leung and J. Malik.
1999.
Recognizing surfacesusing three-dimensional textons.
In IEEE Interna-tional Conference on Computer Vision (ICCV).Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning word vectors for sentiment analysis.In ACL, pages 142?150.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Andriy Mnih and Geoffrey E. Hinton.
2007.
Threenew graphical models for statistical language mod-elling.
In Zoubin Ghahramani, editor, ICML, vol-ume 227 of ACM International Conference Proceed-ing Series, pages 641?648.
ACM.Paul Ogilvie and James P. Callan.
2001.
ExperimentsUsing the Lemur Toolkit.
In TREC.Florent Perronnin and Christopher R. Dance.
2007.Fisher kernels on visual vocabularies for image cat-egorization.
In CVPR.Florent Perronnin, Yan Liu, , Jorge Sa?nchez, and Herve?Poirier.
2010.
Large-scale image retrieval withcompressed fisher vectors.
In CVPR.James Philbin, Ondrej Chum, Michael Isard, JosefSivic, and Andrew Zisserman.
2008.
Lost in quanti-zation: Improving particular object retrieval in largescale image databases.
In IEEE International Con-ference on Computer Vision and Pattern Recognition(CVPR).Jay M. Ponte and W. Bruce Croft.
1998.
A languagemodeling approach to information retrieval.
In SI-GIR, pages 275?281.
ACM.108P.
Quelhas, F. Monay, J.-M. Odobez, D. Gatica-Perez,T.
Tuytelaars, and L. Van Gool.
2005.
Modelingscenes with local descriptors and latent aspects.
InIEEE International Conference on Computer Vision(ICCV).B.
Russell, A. Efros, J. Sivic, W. Freeman, and A. Zis-serman.
2006.
Using multiple segmentations to dis-cover objects and their extent in image collections.In IEEE International Conference on Computer Vi-sion and Pattern Recognition (CVPR).R.
Salakhutdinov and A. Mnih.
2007.
Probabilisticmatrix factorization.
In NIPS.G.
Salton and M. J. McGill.
1983.
Introduction toModern Information Retrieval.
McGraw-Hill, Inc.,New York, NY, USA.J.
Sivic and A. Zisserman.
2003.
Video google: A textretrieval approach to object matching in videos.
InIEEE International Conference on Computer Vision(ICCV).Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-gio.
2010.
Word representations: A simple and gen-eral method for semi-supervised learning.
In ACL,pages 384?394.J.
Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong.2010.
Locality-constrained linear coding for imageclassification.
In IEEE International Conference onComputer Vision and Pattern Recognition (CVPR).Quan Wang, Jun Xu, Hang Li, and Nick Craswell.2011.
Regularized latent semantic indexing.
In SI-GIR?11.J.
Yang, K. Yu, Y. Gong, and T. Huang.
2009.
Lin-ear spatial pyramid matching using sparse coding forimage classification.
In IEEE International Confer-ence on Computer Vision and Pattern Recognition(CVPR).109
