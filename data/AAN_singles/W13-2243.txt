Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352?358,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsFBK-UEdin participation to the WMT13 Quality Estimation shared-taskJose?
G. C. de SouzaFBK-irstUniversity of TrentoTrento, Italydesouza@fbk.euChristian BuckSchool of InformaticsUniversity of EdinburghEdinburgh, UKchristian.buck@ed.ac.ukMarco Turchi, Matteo NegriFBK-irstTrento, Italy{turchi,negri}@fbk.euAbstractIn this paper we present the approach andsystem setup of the joint participation ofFondazione Bruno Kessler and Universityof Edinburgh in the WMT 2013 QualityEstimation shared-task.
Our submissionswere focused on tasks whose aim was pre-dicting sentence-level Human-mediatedTranslation Edit Rate and sentence-levelpost-editing time (Task 1.1 and 1.3, re-spectively).
We designed features thatare built on resources such as automaticword alignment, n-best candidate transla-tion lists, back-translations and word pos-terior probabilities.
Our models consis-tently overcome the baselines for bothtasks and performed particularly well forTask 1.3, ranking first among seven parti-cipants.1 IntroductionQuality Estimation (QE) for Machine Transla-tion (MT) is the task of evaluating the qualityof the output of an MT system without relyingon reference translations.
The WMT 2013 QEShared Task defined four different tasks coveringboth word and sentence level QE.
In this workwe describe the Fondazione Bruno Kessler (FBK)and University of Edinburgh approach and systemsetup of our participation to the shared task.
Wedeveloped models for two sentence-level tasks:Task 1.1: Scoring and ranking for post-editing ef-fort, and Task 1.3: Predicting post-editing time.The first task aims at predicting the Human-mediated Translation Edit Rate (HTER) (Snoveret al 2006) between a suggestion generated bya machine translation system and its manuallypost-edited version.
The data set contains 2,754English-Spanish sentence pairs post-edited by onetranslator (2,254 for training and 500 for test).
Weparticipated only in the scoring mode of this task.The second task requires to predict the time, inseconds, that was required to post edit a transla-tion given by a machine translation system.
Par-ticipants are provided with 1,087 English-Spanishsentence pairs, source and suggestion, along withtheir respective post-edited sentence and post-editing time in seconds (803 data points for train-ing and 284 for test).For both tasks we applied supervised learningmethods and made use of information about wordalignments, n-best diversity scores, word posteriorprobabilities, pseudo-references, and back trans-lation to train our models.
In the remainder ofthis paper we describe the features designed forour participation (Section 2), the learning methodsused to build our models (Section 3), the experi-ments that led to our submitted systems (Section4), and we briefly conclude our experience in thisevaluation task (Section 5).2 Features2.1 Word AlignmentInformation about word alignments is used to ex-tract quantitative (amount and distribution of thealignments) and qualitative features (importanceof the aligned terms) under the assumption thatfeatures that explore what is aligned can bring im-provements to tasks where sentence-level seman-tic relations need to be identified.
Among the pos-sible applications, Souza et al(2013) recently in-vestigated with success their application in Cross-lingual Textual Entailment for content synchro-nization (Mehdad et al 2012; Negri et al 2013).For our experiments in both tasks we built wordalignment models using the resources made avail-able for the evaluation campaign.
To train theword alignment models we used the MGIZA++implementation (Gao and Vogel, 2008) of the IBMmodels (Brown et al 1993) and the concatenationof Europarl, News Commentary, MultiUN, paral-352lel corpora made available for task 1.3.
The train-ing data comprises about 12.8 million sentencepairs.The word alignment features are divided intothree main groups: AL, POS and IDF.
TheAL group regards quantitative information aboutaligned and unaligned words between sourcesentence (src) and machine translation output(tgt).
The features of this group are computedfor both src and tgt:?
proportion of aligned words;?
number of contiguous unaligned words nor-malized by the length of the sentence;?
length of the longest sequence ofaligned/unaligned words normalized bythe length of the sentence;?
average length of aligned/unaligned se-quences of words;?
position of the first/last unaligned word nor-malized by the length of the sentence;?
proportion of aligned n-grams in the sen-tence.To compute the features of the POS groupwe use part-of-speech (PoS) information for eachword in src and tgt.
Training and test data forboth tasks were preprocessed with the TreeTag-ger (Schmid, 1995) and mapped to a more coarse-grained set of part-of-speech tags (P ) based on theuniversal PoS tag set by Petrov et al(2012).
Inthis group there are two different types of features:one is computed for the alignments (the mappingbetween a word in src and a word in tgt) andthe other is computed for aligned words (words insrc that are aligned to one or more words in tgtand vice-versa).
The features computed over thealignments are:?
proportion of alignments connecting wordswith the same PoS tag;?
proportion of alignments connecting wordswith the same PoS tag for each tag p ?
P .The features implemented for aligned wordsare:?
proportion of aligned words tagged with p inthe sentence (p ?
P ).
This feature is pro-cessed for both src and tgt;?
proportion of words in src aligned withwords in tgt that share the same PoS tag(and vice-versa);?
proportion of words tagged with p in src andthat are aligned to words with the same tagp in tgt (and vice-versa).
This is done forevery p ?
P .The last group, IDF, is composed by one fea-ture that explores the notion of inverse documentfrequency as another source of qualitative infor-mation.
The idea is that rare words (with higherIDF) are more informative than frequent words.The IDF scores for each word are calculated forEnglish and Spanish on each side of the parallelcorpora used to build the alignment models.
Thisfeature is calculated for both src and tgt (at teststage, the average IDF value of each language isassigned to unseen terms):?
summation of the IDF scores of alignedwords in src divided by the sum of IDFscores of the aligned words in tgt (and vice-versa).Preliminary experiments have been executed tofind the best word alignment algorithm for eachtask.
We explored three different word alignmentalgorithms: the hidden Markov model (HMM)(Vogel et al 1996) and IBM models 3 and 4(Brown et al 1993).
We also tried three sym-metrization models (Koehn et al 2005): union,intersection, and grow-diag-final-and, a morecomplex symmetrization method which combinesintersection with some alignments from the union.The best alignment and symmetrization combina-tion found for Task 1.1 was IBM4 with intersec-tion and for task 1.3 was HMM with intersec-tion.
These experiments were carried out in 10-fold cross-validation on the training set and usedonly the alignment features.2.2 N-best Diversity scoresOur n-best diversity features are based on the intu-ition that a large number of possible choices gen-erally leads to more errors.
While a similar notioncan be expressed locally by counting the transla-tion options for each word or phrase, we considern-best lists as a good approximation of the searchspace.
This allows us to circumvent problems as-sociated with the local measures, such as ambigu-ous alignment and segmentation, and limitations353of using the search graph directly such as the in-ability compute edit distance between hypotheses.Thus, to quantify the coherence of translationoptions we compute a (symmetrical) matrix ofpairwise Levenshtein distances, either on token orcharacter level, for n-best lists of size up to 100k1using the baseline system and the systems we de-scribe in Section 2.4.
For this matrix the followingfeatures are produced:1.
The index of the central hypothesis, i.e.
thetranslation with the minimum average dis-tance to all other entries.2.
The average edit distance between the cen-tral hypothesis and all other entries normal-ized by the length of top scoring hypothesis.3.
Edit distance between top scoring and centralhypothesis4.
Number of hypotheses with an edit distanceto the top-scoring hypothesis below a setthreshold.2.3 Word Posterior ProbabilitiesFollowing previous work on word posterior prob-abilities (WPPs) (Ueffing et al 2003) we com-puted the sequence of edit operations needed totransform the MT suggestion into all entries of ann-best list in which we normalized the logarithmicmodel scores to resemble probabilities.
Tokens areconsidered incorrect is the operation is either in-sert or substitute, otherwise the probability of thehypothesis counts towards the correctness of theword.
These word-level features were then nor-malized by taking the geometric mean of the in-dividual probabilities.
We did this for all systemsdescribed in Section 2.4 and varying sizes of n be-tween 10 and 100k.2.4 Pseudo-references and back-translationMotivated by the success of pseudo-reference fea-tures (Soricut et al 2012) we employed three ad-ditional MT systems: one similar to the originalsystem but trained on more data, a hierarchicalphrase-based system, and a Spanish-English sys-tem to translate back into English.
All models1Computing the pair-wise edit-distances between all 100kentries is computationally expensive.
However, we found then-best lists to be highly repetitive, so that on average only3.7% of the values had to be computed.
The computation isalso trivially parallel.have been estimated using publicly available soft-ware (SRILM (Stolcke, 2002), Moses (Koehn etal., 2007)), and corpora (Europarl, News Com-mentary, MultiUN, Gigaword).
Using the predic-tions of the English-Spanish systems as pseudo-references and likewise the original source as ref-erence for the back-translation system we com-puted a number of automatic metrics includingBLEU (Papineni et al 2002), GTM (Turian et al2003), PER (Tillmann et al 1997), TER (Snoveret al 2006) and Meteor (Denkowski and Lavie,2011).3 Learning algorithmsTo build our models using the features presentedin Section 2 we tried different learning algorithms.After some preliminary experiments for both taskswe decided to use mainly two: support vectormachines (SVM) and extremely randomized trees(Geurts et al 2006).
For all experiments pre-sented in this paper we use the Scikit-learn (Pe-dregosa et al 2011) implementations of the abovealgorithms.In preliminary experiments we noticed that thenumber of features that we were using for bothtasks was leading to poor results when using theSVM regression (SVR) models.
In order to copewith this problem we performed feature selectionprior to the SVM regression training.
For thatwe used Randomized Lasso, or stability selec-tion (Meinshausen and Bu?hlmann, 2010).
It re-samples the training data several times and fits aLasso regression model on each sample.
Featuresthat appear in a given number of samples are re-tained.
Both the fraction of the data to be sam-pled and the threshold to select the features can beconfigured.
In our experiments we set the sam-pling fraction to 75%, the selection threshold to25% and the number of re-samples to 200.To optimize the SVR with radial basis function(RBF) kernel hyper-parameters we used randomsearch (Bergstra and Bengio, 2012) instead of thetraditional grid search procedure.
We found ran-dom search to be as efficient or better than gridsearch and it drastically reduced the time requiredto compute the best parameter combination.Finally, we trained an extremely randomizedforest, i.e.
an ensemble of extremely randomizedtrees.
Each tree can be parameterized differently.The results of the individual trees are combined byaveraging their predictions.
When a tree is built,354System Features MAE RMSE Predict.
Interval ParametersSVR Base 0.127 0.163 [0.046, 0.671] 347.5918, 0.001, 0.0001SVR Base + All 0.121 0.155 [0.090, 0.714] 0.4052, 0.0753, 0.0010RL + SVR Sel(Base + All) 0.119 0.1534 [0.084, 0.745] 40.5873, 0.0484, 0.0002ET Base + All 0.123 0.156 [0.142, 0.708] 100ET Base + All 0.122 0.155 [0.164, 0.712] 1000Table 1: Experiments results for Task 1.1 on 10-fold cross-validation.
?Base?
are the 17 baseline features.?All?
corresponds to all the features described in Section 2 in a total of 141 features.
?SVR?
is supportvector regression, ?RL?
is randomized Lasso and ?ET?
is extremely randomized trees.
MAE stands forthe average mean absolute error and RMSE is the root mean squared error.
Parameters for SVR are C, ,?
and for ET is the number of estimators.the node splitting step is done at random by pick-ing the best split among a random subset of theinput features.4 ExperimentsFor both tasks we set up a baseline system thatuses the same 17 black box ?baseline?
featuresprovided for the WMT 2012 QE shared task(Callison-Burch et al 2012).
The baseline modelis trained with an SVM regression with RBF ker-nel and optimized parameters.
Parameter opti-mization for SVM regression models was per-formed with 1000 iterations of random search forwhich the process was set to minimize the meanabsolute error (MAE)2.
The parameters of SVRwith RBF kernel (the penalty parameter C, thewidth of the insensitivity zone , and the RBF pa-rameter ?)
are sampled from an exponential distri-bution.Experiments for both tasks were run using10-fold cross-validation on the training set.
InTask 1.3 some data points were annotated by2 or more post-editors and, in a normal cross-validation scheme, the same data point might ap-pear in the training and test set but annotated bydifferent post-editors.
To address this characteris-tic we implemented a cross-validation that dividesalong source sentences, so that all translations of asource segment end up in either the training or testportion of a split.
The number of features availablefor both tasks is not the same (112 for Task 1.1and 141 for Task 1.3) because there were fewer n-best diversity, pseudo-references and word poste-rior probability based features developed with dif-ferent parameters due to time constraints.2Given by MAE =?Ni=1|H(si)?V (si)|N , where H(si)is the hypothesis score for the entry si and V (si) is the goldstandard value for si in a dataset with N entries.During our experiments with the training set,the best model for Task 1.1 was the combinationof randomized Lasso feature selection with SVR(0.119 MAE).
The extremely randomized treespresented results around 0.12 MAE worse than thefigures obtained by the SVR models.
Results ob-tained for Task 1.1 are summarized in Table 1.As for Task 1.3, training results are presented inTable 2.
The best model combines feature selec-tion (randomized Lasso) with SVR.
During train-ing it obtained the lowest average MAE (38.6).Compared to the models built with extremely ran-domized trees, the prediction interval of this sys-tem is narrower.
This indicates that the tree-basedmodels cover a wider range of data points than theSVR-based models.In the official results released by the organiz-ers our submissions had close performances forTask 1.1.
The difference between the SVR and theextremely randomized tree models is very small(around 0.0012 MAE points).
For Task 1.3 ourbest submission is the one based on ensembles oftrees, a trend that was not observed during train-ing.
Our hypothesis is that the tree-based ensem-ble model was capable of generalizing the train-ing data better than the SVR-based ones and thatdespite the low number of employed features thelatter was prone to overfitting.Table 3 presents the official evaluation numbersfor both tasks.4.1 Feature analysisTo gain some insight about the relevance of thefeatures we explored in our submissions, we com-pared the output of the randomized Lasso withthe most important features computed by the ex-tremely randomized tree algorithm.
Below wepresent the features that appear in the intersection355System Features MAE RMSE Predict.
Interval ParametersSVR Base 41.3 69.2 [5.6, 315.7] 138.7359, 2.3331, 0.0185SVR Base + All 40.2 70.6 [8.6, 335.6] 308.3817, 0.2194, 0.0009RL + SVR Sel(Base + All) 38.6 69.1 [11.5, 332.0] 161.5705, 7.3370, 0.0460ET Base + All 44.1 72.2 [11.9, 446.2] 100ET Base + All 43.7 72.0 [12.6, 446.2] 1000Table 2: Experiments results for Task 1.3 on 10-fold cross-validation.
?Base?
are the 17 baseline features.?All?
corresponds to all the features described in Section 2 in a total of 141 features.
?SVR?
is supportvector regression, ?RL?
is randomized Lasso and ?ET?
is extremely randomized trees.
MAE stands forthe average mean absolute error and RMSE is the root mean squared error.
Parameters for SVR are C, ,?
and for ET is the number of estimators.System MAE RMSETask 1.1Official Baseline 0.1491 0.1822RL + SVR 0.1450 0.1773ET 0.1438 0.1768Task 1.3Official Baseline 51.93 93.35RL + SVR 47.92 86.66ET 47.52 82.60Table 3: Official results for tasks 1.1 and 1.3 onthe test set.of these two sets for each task.In Task 1.1, the feature selection algorithm re-tained 29 out of 112 features.
We take the intersec-tion of this set with the 29 most relevant featurescomputed by the ensemble tree-based method.This selection comes from features based on dif-ferent resources:?
proportion of words in src aligned withwords in tgt that share the same PoS tag;?
average number of translations per sourceword according to IBM Model 1 thresholdedP (t|s) > 0.01;?
average number of translations per sourceword according to IBM Model 1 thresholdedP (t|s) > 0.2;?
average source sentence token length;?
number of times the top-scoring hypothesis isrepeated in an 10k-best list;?
position of the first unaligned word normal-ized by the length of the sentence for srcand tgt;?
position of the last unaligned word normal-ized by the length of the sentence for srcand tgt;?
summation of the IDF scores of alignedwords in tgt divided by the summation ofIDF scores of the aligned words in src;?
length of the longest sequence of unalignedwords normalized by the length of the src;?
percentage of bigrams in the 4th quartile offrequency of the source language corpus;?
percentage of trigrams in the 4th quartile offrequency of the source language corpus;?
proportion of alignments connecting wordswith the same PoS tag;?
proportion of aligned words in src.For Task 1.3, the randomized Lasso selectionreduced the input feature vector from 141 fea-tures to 19.
We compared these features with the19 most important features computed by the ex-tremely randomized tree algorithm.
As above theintersection of both sets utilizes many resources:?
proportion of aligned words in src with theadjective PoS tag.?
rank of central hypothesis (see Section 2.2)and average edit distance to all other entriesin 10k-best list of Spanish-English backtrans-lation system;?
language model probability for tgt;?
length of the longest sequence of alignedwords in tgt;356?
number of occurrences of the target wordwithin the target hypothesis averaged for allwords in the hypothesis;?
percentage of bigrams in the 4th quartile offrequency of the source language corpus;?
percentage of trigrams in the 4th quartile offrequency of the source language corpus;?
number of contiguous unaligned words intgt normalized by the length of tgt.5 ConclusionThis paper presented the participation of FBKand University of Edinburgh to the WMT 2013Quality Estimation shared task.
Our approachexplored features based on word alignment, n-best diversity scores, pseudo-references and back-translations, and word posterior probabilities.
Weexperimented with two different learning methods,SVR and extremely randomized trees for predict-ing sentence-level post-editing time and HTER.Our submitted systems were particularly suc-cessful for predicting sentence-level post-editingtime, ranking 1st among seven participants.
Thesubmitted models for predicting HTER consis-tently overcome the baseline for the task.
In addi-tion to the description of our approach and systemsetup, we presented a first analysis of the featuresused in our models with the objective of assess-ing the importance of the features used either forpredicting time or HTER.6 AcknowledgmentsThis work was partially funded by the EuropeanCommission under the project MateCat, Grant287688.
The authors want to thank Philipp Koehnfor training two of the models used in Section 2.2.ReferencesJames Bergstra and Yoshua Bengio.
2012.
RandomSearch for Hyper-Parameter Optimization.
Journalof Machine Learning Research, 13:281?305, March.Peter F. E Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation :Parameter Estimation.
Computational Linguistics,19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the 7th Work-shop on Statistical Machine Translation, pages 10?51, Montreal, Canada, June.
Association for Com-putational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance, SETQA-NLP ?08, pages 49?57, Stroudsburg, PA, USA.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine Learn-ing, 63(1):3?42, March.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation evalu-ation.
In Proceedings of the International Workshopon Spoken Language Translation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zenz, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL 2007 Demo and Poster Sessions, pages 177?180, June.Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Detecting Semantic Equivalence and Infor-mation Disparity in Cross?lingual Documents.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (ACL?12),pages 120?124, Jeju Island, Korea.Nicolai Meinshausen and Peter Bu?hlmann.
2010.Stability selection.
Journal of the Royal Statis-tical Society: Series B (Statistical Methodology),72(4):417?473, July.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2013.Semeval-2013 Task 8: Cross-lingual Textual Entail-ment for Content Synchronization.
In Proceedingsof the 7th International Workshop on Semantic Eval-uation (SemEval 2013).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,July.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, and357E.
Duchesnay.
2011.
Scikit-learn : Machine Learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A Universal Part-of-Speech Tagset.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istan-bul, Turkey, May.
European Language ResourcesAssociation (ELRA).Helmut Schmid.
1995.
Improvements in Part-of-Speech Tagging with an Application to German.
InProceedings of the ACL SIGDAT-Workshop.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Association for Machine Translation inthe Americas, pages 223?231.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL Language Weaver Systems in the WMT12Quality Estimation Shared Task.
In Proceedings ofthe 7th Workshop on Statistical Machine Transla-tion, pages 145?151.Jose?
G. C. de Souza, Miquel Espla`-Gomis, MarcoTurchi, and Matteo Negri.
2013.
Exploiting qualita-tive information from automatic word alignment forcross-lingual nlp tasks.
In The 51st Annual Meetingof the Association for Computational Linguistics -Short Papers (ACL Short Papers 2013).Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,pages 901?904, Denver, Colorado.Christoph Tillmann, Stephan Vogel, Hermann Ney,Alex Zubiaga, and Hassan Sawaf.
1997.
Accel-erated DP Based Search for Statistical Translation.In Proceedings of the Fifth European Conferenceon Speech Communication and Technology, pages2667?2670, Rhodos, Greece.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003.
Evaluation of machine translation and itsevaluation.
In In Proceedings of MT Summit IX,pages 386?393, New Orleans, LA, USA.Nicola Ueffing, Klaus Macherey, and Hermann Ney.2003.
Confidence measures for statistical machinetranslation.
In In Procedings of Machine Transla-tion Summit IX.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proceedings of the 16th conferenceon Computational linguistics-Volume 2, pages 836?841, Morristown, NJ, USA.
Association for Compu-tational Linguistics.358
