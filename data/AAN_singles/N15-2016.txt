Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 118?125,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsDiscourse and Document-level Information for Evaluating Language OutputTasksCarolina ScartonDepartment of Computer Science, University of SheffieldRegent Court, 211 Portobello, Sheffield, S1 4DP, UKc.scarton@sheffield.ac.ukAbstractEvaluating the quality of language outputtasks such as Machine Translation (MT) andAutomatic Summarisation (AS) is a chal-lenging topic in Natural Language Processing(NLP).
Recently, techniques focusing only onthe use of outputs of the systems and sourceinformation have been investigated.
In MT,this is referred to as Quality Estimation (QE),an approach that uses machine learning tech-niques to predict the quality of unseen data,generalising from a few labelled data points.Traditional QE research addresses sentence-level QE evaluation and prediction, disregard-ing document-level information.
Document-level QE requires a different set up fromsentence-level, which makes the study of ap-propriate quality scores, features and modelsnecessary.
Our aim is to explore document-level QE of MT, focusing on discourse infor-mation.
However, the findings of this researchcan improve other NLP tasks, such as AS.1 IntroductionEvaluation metrics for Machine Translation (MT)and Automatic Summarisation (AS) tasks should beable to measure quality with respect to different as-pects (e.g.
fluency and adequacy) and they shouldbe fast and scalable.
Human evaluation seems to bethe most reliable (although it might introduce biasesof reviewers).
However, it is expensive and cumber-some for large datasets; it is also not practical forcertain scenarios, such as gisting in MT and sum-marisation of webpages.Automatic evaluation metrics (such as BLEU (Pa-pineni et al, 2002) and ROUGE (Lin and Och,2004)), based on human references, are widely usedto evaluate MT and AS outputs.
One limitation ofthese metrics is that if the MT or AS system out-puts a translation or summary considerably differentfrom the references, it does not really mean that it isa bad output.
Another problem is that these metricscannot be used in scenarios where the output of thesystem is to be used directly by end-users, for exam-ple a user reading the output of Google Translate1for a given news text cannot count on a reference forthat translated text.Quality Estimation (QE) approaches aim to pre-dict the quality of MT systems without using refer-ences.
Instead, features (that may be or may not berelated to the MT system that produced this trans-lations) are applied to source and target documents(Blatz et al, 2004; Bojar et al, 2013).
The only re-quirement is data points with scores (e.g.
: Human-targeted Translation Error Rate (HTER) (Snover etal., 2006) or even BLEU-style metrics).
These datapoints can be used to train supervised machine learn-ing models (regressors or classifiers) to predict thescores of unseen data.
The advantage of these ap-proaches is that we do not need to have all the words,sentences or documents of a task evaluated manu-ally, we just need enough data points to train themachine learning model.QE systems predict scores that reflect how gooda translation is for a given scenario.
For example, awidely predicted score in QE is HTER, which mea-sures the effort needed to post-edit a sentence.
A1https://translate.google.com/118user of a QE system predicting HTER could de-cide whether to post-edit or translate sentences fromscratch based on the score predicted for each sen-tence.The vast majority of work done on QE is at sen-tence level.
Document-level predictions, on theother hand, are interesting in scenarios where onewants to evaluate the overall score of an MT systemor where the end-user is interested in the quality ofthe document as whole.
In addition, document-levelfeatures can also correlate well with quality scores,mainly because state-of-the-art MT systems trans-late documents at sentence level, disregarding dis-course information.
Therefore, it is expected thatthe outputs of these systems may contain discourseproblems.In this work we focus on document-level QE.Regarding features, discourse phenomena are be-ing considered since they are linguistic phenom-ena that often manifest document-wide.
These phe-nomena are related to how sentences are connected,how genre and domain of a document are identified,anaphoric pronouns, etc.Regarding document-level prediction, we focuson finding the ideal quality label for the task.
Tradi-tional evaluation metrics tend to yield similar scoresfor different documents.
This leads to low variationbetween the document quality scores with all thesescores being close to the mean score.
Therefore,a quality label that captures document quality in amore sensitive way is needed.Research on the use of linguistic features for QEand the use of discourse for improving MT and MTevaluation are presented in Section 2.
Section 3presents the work done so far and the directions thatwe intend to follow.
Conclusions and future workare presented in Section 42 Document-level information for QE andMTTraditional systems translate documents at sen-tence level, disregarding document-wide informa-tion.
This means that sentences are translated with-out considering the relations in the whole document.Therefore, information such as discourse structurescan be lost in this process.QE is also traditionally done at sentence levelmainly because the majority of MT systems translatetexts at this level.
Another reason is that sentence-level approaches have more applications than othergranularity levels, because they can explore the pe-culiarities of each sentence, being very useful for thepost-edition task.
On the other hand, sentence-levelapproaches do not consider the document as a wholeand information regarding discourse is disregarded.Moreover, for scenarios in which post-edition is notpossible, for example, gisting, quality predictionsfor the entire documents are more useful.In this section we present related work on QE andthe first research towards document-level QE.
Re-search on the use of discourse phenomena for MTimprovement and MT evaluation are also presented.2.1 Quality Estimation of Machine TranslationPrevious work on QE has used supervised machinelearning (ML) approaches (mainly regression algo-rithms).
Besides the specific ML method adopted,the choice of features is also a design decision thatplays a crucial role.Sentences (or documents) from source and targetand also information from the MT system are usedfor designing features.
The features extracted areused as input to train a QE model.
In this trainingphase supervised ML techniques, such as regression,can be applied.
A training set with quality labelsis provided for an ML model.
These quality labelsare the scores that the QE model will learn to pre-dict.
Therefore, the QE model will be able to predicta quality score for a new, unseen data points.
Thequality labels can be likert scores, HTER, BLEU,just to cite some widely used examples.
Also theML algorithm can vary (SVM and Gaussian Processare the state-of-the-art algorithms for QE).Some work in the area include linguistic infor-mation as features for QE (Avramidis et al, 2011;Pighin and M`arquez, 2011; Hardmeier, 2011; Fe-lice and Specia, 2012; Almaghout and Specia, 2013)at sentence level.
Only Scarton and Specia (2014)(predicting quality at document level) and Rubino etal.
(2013) (sentence level) focus on the use of dis-course information for QE.It is important to notice that frameworks likeQuEst2(Specia et al, 2013) are available for QE at2http://www.quest.dcs.shef.ac.uk119sentence level.
QuEst has modules to extract severalfeatures for QE from source and target documentsand to experiment with ML techniques for predict-ing QE.
Features are divided in two types: glass-box(dependent on the MT system) and black-box (inde-pendent on the MT system).At document level, Soricut and Echihabi (2010)explore document-level QE prediction to rank doc-uments translated by a given MT system, predictingBLEU scores.
Features include text-based, languagemodel-based, pseudo-reference-based, example-based and training-data-based.
Pseudo-referencefeatures are BLEU scores based on pseudo-references from an off-the-shelf MT system, for boththe target and the source languages.Scarton and Specia (2014) explore lexical cohe-sion and LSA (Latent Semantic Analysis) (Landaueret al, 1998) cohesion for document-level QE.
Thelexical cohesion features are repetitions (Wong andKit, 2012) and the LSA cohesion is achieved fol-lowing the work of Graesser et al (2004).
Pseudo-reference features are also applied in this work, ac-cording to the work of Soricut and Echihabi (2010).BLEU and TER (Snover et al, 2006) are used asquality labels.
The best results were achieved withpseudo-reference features.
However, LSA cohesionfeatures alone also showed improvements over thebaseline.2.2 Discourse phenomena in MTIn the MT area, there have been attempts to use dis-course information that can be used as inspirationsource for QE features.
The need of document-levelinformation for improving MT is a widely acceptedfact.
However, it is hard to integrate discourse in-formation into traditional state-of-the-art sentence-level MT systems.
It is also challenging to build adocument-level or discourse-based MT system fromscratch.
Therefore, the initiatives focus on the in-tegration of discourse as features into the decodingphase or previously annotate discourse phenomenain the parallel corpora.Lexical Cohesion is related to word usage: wordrepetitions, synonyms repetitions and collocations.Besides initiatives to improve MT system and out-puts with lexical cohesion (Ture et al, 2012; Xiaoet al, 2011; Ben et al, 2013), Wong and Kit (2012)apply lexical cohesion metrics for evaluation of MTsystems at document level.Coreference is related to coherence clues, suchas pronominal anaphora and connectives.
Machinetranslation can break coreference chains since it isdone at sentence level.
Initiatives for improvementof coreference in MT include anaphora resolution(Gim?enez et al, 2010; LeNagard and Kohen, 2010;Hardmeier and Federico, 2010; Hardmeier, 2014)and connectives (Popescu-Belis et al, 2012; Meyerand Popescu-Belis, 2012; Meyer et al, 2012; Li etal., 2014).RST (Rhetorical Structure Theory) (Mann andThompson, 1987) is a linguistic theory that corre-lates macro and micro units of discourse in a co-herent way.
The correlation is made among EDUs(Elementary Discourse Units).
EDUs are defined atsentence, phrase or paragraph-level.
These correla-tions are represented in the form of a tree.
Marcu etal.
(2000) explore RST focusing on identifying thefeasibility of building a discourse-based MT system.Guzm?an et al (2014) use RST trees comparison forMT evaluation.Topic models capture word usage, although theyare more robust than lexical cohesion structures be-cause they can correlate words that are not repeti-tions or do not present any semantic relation.
Thesemethods can measure if a document follows a topic,is related to a genre or belongs to a specific domain.Work on improving MT that uses topic models in-clude Zhengxian et al (2010) and Eidelman et al(2012).3 Planned WorkIn this paper, we describe the three main researchquestions that we aim to answer in this PhD work:1.
How to address document-level QE?2.
Are discourse models appropriate to be used forQE at document level?
Are these models appli-cable for different languages?3.
How can we use the discourse informationfor the evaluation of Automatic Summarisationand Readability Assessment?In this section, we summarise how we are ad-dressing these research questions.1203.1 Document-level Quality EstimationAs mentioned previously, one aim of this PhD is toidentify a suitable quality label for document-levelQE.
Our hypothesis is that document quality is morecomplex than a simple aggregation of sentence qual-ity.
In order to exemplify this assumption, considerdocument A and document B.
Documents A and Bhave the same number of sentences (10 sentences)and score the same value when we access quality asan average of HTER at sentence level, 0.5.
How-ever, 5 sentences of document A score 1 and theother five sentences score 0.
On the other hand,document B shows a more smooth distribution ofscores among sentences (the majority of the sen-tences score a value close to 0.5).
Are document Aand B comparable just because the averaged HTERsare the same?
Our assumption is that a real scoreat document level or a more clever combination ofsentence-level scores are the more suitable ways toevaluate documents.Another drawback of averaging sentence-levelscores is that sentences have different importanceinside a document, they contain different informa-tion across a document.
Therefore, documents thathave important sentences badly translated should bepenalised more heavily.
The way we propose to ad-dress this problem is by using summarisation or in-formation retrieval techniques in order to identifythe most important sentences (or even paragraphs)and assign different weights according to the rele-vance of the sentence.Moreover, we studied several traditional evalua-tion metrics as quality labels for QE at documentlevel and found out that, on average, all the doc-uments seem to be similar.
Part of this study isshowed in Table 1 for 9 documents of WMT2013QE shared task corpus (English-Spanish transla-tions) and for 119 documents of LIG corpus (Potetet al, 2012) (French-English translations, with post-editions).3The quality metrics considered wereBLEU, TER, METEOR (Banerjee and Lavie, 2005)and an average of HTER scores at sentence level.All traditional MT evaluation metrics showed lowstandard deviation (STDEV) in both corpora.
Alsothe HTER at sentence level averaged to obtain adocument-score showed low variation.
This means3Both corpora were translated by only one SMT system.that all documents in the corpora seem similar interms of quality.
Our hypothesis is that this evalu-ation is wrong and other factors should be consid-ered in order to achieve a suitable quality label fordocument-level prediction.Besides quality scores, another issue indocument-level QE is the features to be used.Thus far, the majority of features for QE are atword or sentence level.
Since a document can beviewed as a combination of words and sentencesone way to explore document-level features isto combine word- or sentence-level features (byaveraging them, for example).
Another way is toexplore linguistic phenomena document-wide.
Thisis discussed on the next subsection.New features and prediction at document levelcan be included in existing frameworks, such asQuEst.
This is the first step to integrate document-level and sentence-level prediction and features.3.2 Modelling discourse for Quality EstimationDiscourse phenomena happen document-wide and,therefore, these can be considered a strong candi-date for the extraction of document-level features.
Adocument is not only a bag of words and sentences,although the words and sentences are in fact organ-ised in a logical way by using linguistic clues.
Dis-course was already studied in the MT field, aimingto improve MT systems and/or MT outputs and alsoto automatically evaluate MT against human refer-ences.
However, for QE, we should be able to dealwith evaluation for several language pairs, consid-ering features for source and target.
Another issueis that QE features should correlate with the qualityscore used.
Therefore, the use of discourse for QEpurposes deserves further investigation.We intend to model discourse for QE by applyinglinguistic and statistical knowledge.
Two cases arebeing explored:3.2.1 Linguistic-based modelsCertain discourse theories could be used tomodel discourse for QE purposes, such as such asthe Rhetorical Structure Theory (RST) (Mann andThompson, 1987) and Entity-Grid models (Barzilayand Lapata, 2008; Elsner, 2011).
We refer to thesetwo theories mainly because they can be readily ap-plied, for English language, given the existence of121WMT LIGAverage STDEV Average STDEVBLEU (?)
0.26 0.046 0.27 0.052TER (?)
0.52 0.049 0.53 0.069METEOR-ex (?)
0.46 0.050 0.29 0.031METEOR-st (?)
0.43 0.050 0.30 0.030Averaged HTER (?)
0.25 0.071 0.21 0.032Table 1: Average values of evaluation metrics in the WMT and LIG corporaparsers (RST parser (Joty et al, 2013) and EntityGrid parser).4Although these resources are onlyavailable for English, it is important in this stage tostudy the impact of this information for document-level QE, considering English as source or targetlanguage.
In this scenario, we intend to exploresource and target features isolated (source featureswill be applied only when English is source lan-guage and target features only when English is tar-get).Moreover, other linguistic information could beused to model discourse for QE.
Anaphoric infor-mation, co-reference resolution and discourse con-nectives classification could be used.
(Scarton andSpecia, 2014) explore lexical cohesion features forQE.
These features are based on repetitions of wordsor lemmas.
Looking at more complex structures,such as synonym in order to count repetitions be-yond word matching can lead to improvements inthe results.We have also studied linguistic phenomena andtheir correlations with HTER values at documentlevel on the LIG corpus.
Results are shown in Fig-ure 1.
This figure shows four scenarios with differ-ent numbers of documents.
The first scenario hasten documents: the five best documents and the fiveworst (in terms of averaged HTER).
The second sce-nario considers the ten best and ten worst, the thirdthe 20 best and 20 worst and the fourth the 40 bestand 40 worst.
The last scenario considers all thedata.
The bars are Pearson?s r correlation valuesbetween a given feature and the real HTER value.Features were: number of connectives, number ofpronouns, number of RST nucleus relations, num-ber of RST satellite relations, number of elemen-tary discourse units (EDUs) breaks, lexical cohe-sion (LC) features and LSA features from the workof (Scarton and Specia, 2014).
The most success-4https://bitbucket.org/melsner/browncoherenceful features of QuEst framework were also consid-ered: QuEst1 - number of tokens, QuEst2 - languagemodel probability, QuEst3 - number of occurrencesof the target word within the target hypothesis (av-eraged for all words in the hypothesis - type/tokenratio) and QuEst4 - number of punctuation marks.Features were only applied for target (English) dueto resources availability.Mainly for scenarios with 10 and 20 documents,features considering discourse phenomena countsperformed better than QuEst features.
In the otherscenarios LC and LSA features were the best.
Itis worth mentioning that this study is on-going andmuch more can be extracted from discourse infor-mation such as RST, than only simple counts.3.2.2 Latent variable modelsLatent variable models such as Latent DirichletAllocation (LDA) (Blei et al, 2003) and Latent Se-mantic Analysis (LSA) (Landauer et al, 1998) havebeen widely used to extract topic models from cor-pora.
The idea behind these methods is that a matrixof words versus sentences is built and mathemati-cal transformations are applied, in order to achievecorrelations among the word vectors.
However, asGraesser et al (2004) suggests, they can also beused to find lexical cohesion information within doc-uments.
In fact, topic modelling approaches have al-ready been used to improve MT and also for QE atsentence level.
Their advantage is that they are fast,language independent and do not require robust re-sources (such as discourse parsers).
Previous workhas used LSA and LDA for QE purposes (Scartonand Specia, 2014; Rubino et al, 2013).We could also use latent variable models to findhow close a machine translated document is fromoriginal documents in the same language, genre anddomain.12210 documents 20 documents 40 documents 80 documents 119 documentsBins0.60.40.20.00.20.40.60.8Person?sr** *** ****ConnectivesPronounsRST - NucleusRST - SatelliteEDUsLC - Argument targetLC - Lemma targetLC - Noun targetLSA - Adjacent targetLSA - All targetQuEst 1QuEst 2QuEst3QuEst 4Figure 1: Impact of discourse features on document-level QE - ?*?
means p-value < 0.053.3 Using discourse models for other NLP tasksOne of our aims is to evaluate whether the dis-course models built for QE can be used for the eval-uation of other tasks in NLP.
AS evaluation couldbenefit from QE: to an extent, AS outputs couldbe viewed as ?translations?
from ?source language?into ?source summarised language?.
Up to now,only (Louis and Nenkova, 2013) proposed an ap-proach for evaluating summaries without references(by using pseudo-references).
Moreover, discourseevaluation of AS outputs is expected to show morecorrelation with quality scores than MT because ofthe nature of the tasks.
While MT outputs are de-pendent on the source language (and, as shown byCarpuat and Simard (2012), they tend to preservediscourse constituents of the source), AS outputs arebuilt by choosing sentences from one or more doc-uments trying to keep as much relevant informationas possible.
The combination of text from multipledocuments can lead to loss of coherence of auto-matic summaries more than MT does to translatedtexts.Another task in NLP that could benefit from ad-vances in QE is Readability Assessment (RA).
Thistask consists in evaluating the complexity of doc-uments for a given audience (therefore, the task isan evaluation per se).
Several studies have alreadyexplored discourse information for RA (Graesser etal., 2004; Pitler and Nenkova, 2008; Todirascu et al,2013).
QE techniques can benefit RA in scenarioswhere we need to compare texts produced by or fornative speakers or second language learners (SLL)or texts produced by or for mentally impaired patientcompared to healthy subjects (in these scenarios, thedocuments produced by or for the ?experts?
couldbe considered as source documents and documentsproduced by or for ?inexpert or mentally impaired?as target documents).4 ConclusionIn this paper we presented a proposal to address todocument-level quality estimation.
This includes thestudy of quality labels for document-level predictionand also document-level features.
We intend to fo-cus on discourse features, because of the nature ofdiscourse phenomena.We showed that traditional MT evaluation metricsare not suitable for QE at document level becausethey cannot measure quality of documents accordingto relevance of sentences.Discourse features were also evaluated fordocument-level QE showing higher correlation withHTER scores than the most successful features fromQuEst framework.
This is sign that discourse infor-mation can help in document-level prediction.Finally, we discussed ways to use the discoursemodels developed for QE to improve evaluation ofother NLP task: AS and RA.AcknowledgmentsThis work was supported by the EXPERT (EUMarie Curie ITN No.
317471) project.123ReferencesHala Almaghout and Lucia Specia.
2013.
A CCG-based Quality Estimation Metric for Statistical Ma-chine Translation.
In The XIV Machine TranslationSummit, pages 223?230, Nice, France.Eleftherios Avramidis, Maja Popovic, David Vilar Torres,and Aljoscha Burchardt.
2011.
Evaluate with Confi-dence Estimation: Machine ranking of translation out-puts using grammatical features.
In The Sixth Work-shop on Statistical Machine Translation, pages 65?70,Edinburgh, UK.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In TheACL 2005 Workshop on Intrinsic and Extrinsic Eval-uation Measures for MT and/or Summarization, pages65?72, Ann Harbor, MI.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Gousheng Ben, Deyi Xiong, Zhiyang Teng, Yajuan Lu,and Qun Liu.
2013.
Bilingual Lexical CohesionTrigger Model for Document-Level Machine Transla-tion.
In The 51st Annual Meeting of the Associationfor Computational Linguistics, pages 382?386, Sofia,Bulgaria.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence Estimation forMachine Translation.
In The 20th International Con-ference on Computational Linguistics, pages 315?321,Geneva, Switzerland.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
The Journal of Ma-chine Learning research, 3:993?1022.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop on Sta-tistical Machine Translation.
In The Eighth Workshopon Statistical Machine Translation, pages 1?44, Sofia,Bulgaria.Marine Carpuat and Michel Simard.
2012.
The Trou-ble with SMT Consistency.
In The Seventh Workshopon Statistical Machine Translation, pages 442?449,Montr?eal, Quebec, Canada.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic Models of Dynamic TranslationModel Adaptation.
In The 50th Annual Meeting ofthe Association for Computational Linguistics, pages115?119, Jeju Island, Korea.Micha Elsner.
2011.
Generalizing Local CoherenceModeling.
Ph.D. thesis, Department of Computer Sci-ence, Brown University, Providence, Rhode Island.Mariano Felice and Lucia Specia.
2012.
Linguistic Fea-tures for Quality Estimation.
In The Seventh Work-shop on Statistical Machine Translation, pages 96?103, Montr?eal, Quebec, Canada.Jes?us Gim?enez, Llu?
?s M`arquez, Elisabet Comelles, IreneCatell?on, and Victoria Arranz.
2010.
Document-levelAutomatic MT Evaluation based on Discourse Repre-sentations.
In The Joint 5th Workshop on StatisticalMachine Translation and MetricsMATR, pages 333?338, Uppsala, Sweden.Arthur C. Graesser, Danielle S. McNamara, Max M.Louwerse, and Zhiqiang Cai.
2004.
Coh-Metrix:Analysis of text on cohesion and language.
Behav-ior Research Methods, Instruments, and Computers,36:193?202.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez, andPreslav Nakov.
2014.
Using Discourse Structure Im-proves Machine Translation Evaluation.
In The 52ndAnnual Meeting of the Association for ComputationalLinguistics, pages 687?698, Baltimore, MD.Christian Hardmeier and Marcello Federico.
2010.
Mod-elling Pronominal Anaphora in Statistical MachineTranslation.
In The 7th International Workshop onSpoken Language Translation, pages 283?289.Christian Hardmeier.
2011.
Improving machine trans-lation quality prediction with syntatic tree kernels.
InProceedings of the 15th conference of the EuropeanAssoaciation for Machine Translation (EAMT 2011),pages 233?240, Leuven, Belgium.Christian Hardmeier.
2014.
Discourse in Statistical Ma-chine Translation.
Ph.D. thesis, Department of Lin-guistics and Philology, Uppsala University, Sweden.Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, andYashar Mehdad.
2013.
Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Dis-course Analysis.
In The 51st Annual Meeting ofthe Association for Computational Linguistics, pages486?496, Sofia, Bulgaria.Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.1998.
An Introduction to Latent Semantic Analysis.Discourse Processes, 25:259?284.Ronan LeNagard and Philipp Kohen.
2010.
Aiding Pro-noun Translation with Co-Reference Resolution.
InThe Joint 5th Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 252?261, Uppasala,Sweden.Junyi Jessy Li, Marine Carpuat, and Ani Nenkova.
2014.Assessing the Discourse Factors that Influence theQuality of Machine Translation.
In The 52nd AnnualMeeting of the Association for Computational Linguis-tics, pages 283?288, Baltimore, MD.Chin-Yew Lin and Franz J. Och.
2004.
Automatic Eval-uation of Machine Translation Quality Using Longest124Common Subsequence and Skip-Bigram Statics.
InThe 42nd Meeting of the Association for Computa-tional Linguistics, pages 605?612, Barcelona, Spain.Annie Louis and Ani Nenkova.
2013.
AutomaticallyAssessing Machine Summary Content Without a GoldStandard.
Computational Linguistics, 39(2):267?300,June.Willian C. Mann and Sandra A. Thompson.
1987.Rhetorical Structure Theory: A Theory of Text Organi-zation.
Cambridge University Press, Cambridge, UK.Daniel Marcu, Lynn Carlson, and Maki Watanabe.
2000.The automatic translation of discourse structures.
InThe 1st North American chapter of the Associationfor Computational Linguistics conference, pages 9?17.Association for Computational Linguistics, April.Thomas Meyer and Andrei Popescu-Belis.
2012.
Us-ing Sense-labeled Discourse Connectives for Statisti-cal Machine Translation.
In The Joint Workshop onExploiting Synergies between Information Retrievaland Machine Translation (ESIRMT) and Hybrid Ap-proaches to Machine Translation (HyTra), pages 129?138, Avignon, France.Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,and Andrea Gesmundo.
2012.
Machine translation oflabeled discourse connectives.
In The Tenth BiennialConference of the Association for Machine Translationin the Americas, San Diego, CA.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In The 40th AnnualMeeting of the Association for Computational Linguis-tics, pages 311?318, Philadelphia, PA.Daniele Pighin and Llu?
?s M`arquez.
2011.
AutomaticProjection of Semantic Structures: an Application toPairwise Translation R anking.
In The Fifth Work-shop on Syntax, Semantics and Structure in StatisticalTranslation, pages 1?9, Portland, OR.Emily Pitler and Ani Nenkova.
2008.
Revisiting Read-ability: A Unified Framework for Predicting TextQuality.
In The Conference on Empirical Meth-ods in Natural Language Processing, pages 186?195,Waikiki, Honolulu, Hawaii.Andrei Popescu-Belis, Thomas Meyer, Jeevanthi Liyana-pathirana, Bruno Cartoni, and Sandrine Zufferey.2012.
Discourse-level Annotation over Europarl forMachine Translation: Connectives and Pronouns.
InThe Eighth International Conference on Language Re-sources and Evaluation, pages 2716?2720, Istanbul,Turkey.Marion Potet, Emmanuelle Esperanc?a-Rodier, LaurentBesacier, and Herv?e Blanchon.
2012.
Collection ofa Large Database of French-English SMT Output Cor-rections.
In The 8th International Conference on Lan-guage Resources and Evaluation, pages 23?25, Istan-bul, Turkey.Raphael Rubino, Jos G. C. de Souza, Jennifer Foster, andLucia Specia.
2013.
Topic Models for TranslationQuality Estimation for Gisting Purposes.
In The XIVMachine Translation Summit, pages 295?302, Nice,France.Carolina Scarton and Lucia Specia.
2014.
Document-level translation quality estimation: exploring dis-course and pseudo-references.
In The 17th AnnualConference of the European Association for MachineTranslation, pages 101?108, Dubrovnik, Croatia.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In The Seventh Biennial Conference of theAssociation for Machine Translation in the Americas,pages 223?231, Cambridge, MA.Radu Soricut and Abdessamad Echihabi.
2010.TrustRank: Inducing Trust in Automatic Translationsvia Ranking.
In The 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 612?621,Uppsala, Sweden.Lucia Specia, Kashif Shah, Jose G.C.
de Souza, andTrevor Cohn.
2013.
QuEst - A translation quality es-timation framework.
In The 51st Annual Meeting ofthe Association for Computational Linguistics: SystemDemonstrations, pages 79?84, Sofia, Bulgaria.Amalia Todirascu, Thomas Franc?o?
?, Nuria Gala, C?edricFairon, Anne-Laure Ligozat, and Delphine Bernhard.2013.
Coherence and Cohesion for the Assessment ofText Readability.
In The 10th International Workshopon Natural Language Processing and Cognitive Sci-ence, pages 11?19, Marseille, France.Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012.Encouraging Consistent Translation Choices.
In The12th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 417?426, Montr?eal, Quebec, Canada.Billy T. M. Wong and Chunyu Kit.
2012.
Extending Ma-chine Translation Evaluation Metrics with Lexical Co-hesion to Document Level.
In The 2012 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 1060?1068, Jeju Island, Korea.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.2011.
Document-level Consistency Verification inMachine Translation.
In The XII Machine TranslationSummit, pages 131?138, Xiamen, China.Gong Zhengxian, Zhang Yu, and Zhou Guodong.
2010.Statistical Machine Translation Based on LDA.
InThe 4th International Universal Communication Sym-posium, pages 279?283, Beijing, China.125
