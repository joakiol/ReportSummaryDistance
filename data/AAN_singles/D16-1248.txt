Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2278?2282,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsWhy Neural Translations are the Right LengthXing Shi1, Kevin Knight1, and Deniz Yuret21Information Sciences Institute & Computer Science DepartmentUniversity of Southern California{xingshi,knight}@isi.edu2Computer Engineering, Koc?
Universitydyuret@ku.edu.trAbstractWe investigate how neural, encoder-decodertranslation systems output target strings of ap-propriate lengths, finding that a collection ofhidden units learns to explicitly implementthis functionality.1 IntroductionThe neural encoder-decoder framework for machinetranslation (Neco and Forcada, 1997; Castan?o andCasacuberta, 1997; Sutskever et al, 2014; Bahdanauet al, 2014; Luong et al, 2015) provides new toolsfor addressing the field?s difficult challenges.
In thisframework (Figure 1), we use a recurrent neural net-work (encoder) to convert a source sentence into adense, fixed-length vector.
We then use another re-current network (decoder) to convert that vector intoa target sentence.
In this paper, we train long short-term memory (LSTM) neural units (Hochreiter andSchmidhuber, 1997) trained with back-propagationthrough time (Werbos, 1990).A remarkable feature of this simple neural MT(NMT) model is that it produces translations of theright length.
When we evaluate the system on previ-ously unseen test data, using BLEU (Papineni et al,2002), we consistently find the length ratio betweenMT outputs and human references translations to bevery close to 1.0.
Thus, no brevity penalty is in-curred.
This behavior seems to come for free, with-out special design.By contrast, builders of standard statistical MT(SMT) systems must work hard to ensure correctlength.
The original mechanism comes from theIBM SMT group, whose famous Models 1-5 in-cluded a learned table (y|x), with x and y beingthe lengths of source and target sentences (Brownet al, 1993).
But they did not deploy this table whendecoding a foreign sentence f into an English sen-tence e; it did not participate in incremental scoringand pruning of candidate translations.
As a result(Brown et al, 1995):?However, for a given f, if the goal is to discoverthe most probable e, then the product P(e) P(f|e) istoo small for long English strings as compared withshort ones.
As a result, short English strings are im-properly favored over longer English strings.
Thistendency is counteracted in part by the followingmodification: Replace P(f|e) with clength(e) ?
P(f|e)for some empirically chosen constant c. This modifi-cation is treatment of the symptom rather than treat-ment of the disease itself, but it offers some tempo-rary relief.
The cure lies in better modeling.
?More temporary relief came from MinimumError-Rate Training (MERT) (Och, 2003), which au-tomatically sets c to maximize BLEU score.
MERTalso sets weights for the language model P(e), trans-lation model P(f|e), and other features.
The lengthfeature combines so sensitively with other featuresthat MERT frequently returns to it as it revises oneweight at a time.NMT?s ability to correctly model length is re-markable for these reasons:?
SMT relies on maximum BLEU training to ob-tain a length ratio that is prized by BLEU, whileNMT obtains the same result through genericmaximum likelihood training.?
Standard SMT models explicitly ?cross off?2278Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al, 2014).
Here, a source sentenceC B A (fed in reverse as A B C) is translated into a target sentence W X Y Z.
At each step, an evolving real-valued vector summarizesthe state of the encoder (left half) and decoder (right half).source words and phrases as they are translated,so it is clear when an SMT decoder has finishedtranslating a sentence.
NMT systems lack thisexplicit mechanism.?
SMT decoding involves heavy search, so if oneMT output path delivers an infelicitous ending,another path can be used.
NMT decoding ex-plores far fewer hypotheses, using a tight beamwithout recombination.In this paper, we investigate how length regulationworks in NMT.2 A Toy Problem for Neural MTWe start with a simple problem in which sourcestrings are composed of symbols a and b.
The goalof the translator is simply to copy those strings.Training cases look like this:a a a b b a <EOS> ?
a a a b b a <EOS>b b a <EOS> ?
b b a <EOS>a b a b a b a a <EOS> ?
a b a b a b a a <EOS>b b a b b a b b a <EOS> ?
b b a b b a b b a <EOS>The encoder must summarize the content of anysource string into a fixed-length vector, so that thedecoder can then reconstruct it.1 With 4 hiddenLSTM units, our NMT system can learn to solvethis problem after being trained on 2500 randomlychosen strings of lengths up to 9.2 3To understand how the learned system works,we encode different strings and record the resultingLSTM cell values.
Because our LSTM has four hid-den units, each string winds up at some point in four-1We follow Sutskever et al (2014) in feeding the input stringbackwards to the encoder.2Additional training details: 100 epochs, 100 minibatchsize, 0.7 learning rate, 1.0 gradient clipping threshold.3We use the toolkit: https://github.com/isi-nlp/Zoph RNNdimensional space.
We plot the first two dimensions(unit1 and unit2) in the left part of Figure 2, and weplot the other two dimensions (unit3 and unit4) in theright part.
There is no dimension reduction in theseplots.
Here is what we learn:?
unit1 records the approximate length of thestring.
Encoding a string of length 7 may gen-erate a value of -6.99 for unit1.?
unit2 records the number of b?s minus the num-ber of a?s, thus assigning a more positive valueto b-heavy strings.
It also includes a +1 bonusif the string ends with a.?
unit3 records a prefix of the string.
If its valueis less than 1.0, the string starts with b. Other-wise, it records the number of leading a?s.?
unit4 has a more diffuse function.
If its value ispositive, then the string consists of all b?s (witha possible final a).
Otherwise, its value corre-lates with both negative length and the prepon-derance of b?s.For our purposes, unit1 is the interesting one.
Fig-ure 3 shows the progression of ?a b a b b b?
as it getsencoded (top figure), then decoded (bottom two fig-ures).
During encoding, the value of unit1 decreasesby approximately 1.0 each time a letter is read.
Dur-ing decoding, its value increases each time a letter iswritten.
When it reaches zero, it signals the decoderto output <EOS>.The behavior of unit1 shows that the translator in-corporates explicit length regulation.
It also explainstwo interesting phenomena:?
When asked to transduce previously-unseenstrings up to length 14, the system occasionallymakes a mistake, mixing up an a or b. How-ever, the output length is never wrong.44Machine translation researchers have also noticed that22798 6 4 2420246baabbbbabbbaabbaaaabbbbabbbbbbababaaaaabbaaaaababbbbbaabbbaabbbbabbabbaaaaabbbabaaaaabbaaabbaaabbbabaaaabbabaababbbbbaaabaaabbabbabbbbbabaabbababaabbabaaababaabaababbabaababbaabbbaababababaaaabaabaabababbabbaaaaaababaaabbabbaaaaaabababaaaaaaaaabaaaabaaabbbaabbabbaababaaaaababaaaaabbabbbabbbbaababbbababbbbbabbbabbbbbabaabbbabbaaabaaabaaaabaabbbbbbabbaaaabbbbaabbabbbabababaabbabbabbbabaaaabba aaabbbaabbabbabab0 1 2 3 4 5 6 7 88765432101baabbbbabbbaabbaaaabbbbabbbbbbaaaaababbbaabbbabbaaaaabbbabaa aaabbaaabbaaabbbbabaababbbbbaaabaaabbabbaabbababaabaababaaababaababababaaaabaababaaababaaabbabbaaaaaabababaaaaaaaaabaaabbaaaaaaaabbabbaababbbabbbbabaabaabaaaabaabbababaaaabbaaaabbbabbababFigure 2: After learning, the recurrent network can convert any string of a?s and b?s into a 4-dimensional vector.
The left plotshows the encoded strings in dimensions described by the cell states of LSTM unit1 (x-axis) and unit2 (y-axis).
unit1 learns torecord the length of the string, while unit2 records whether there are more b?s than a?s, with a +1 bonus for strings that end in a.The right plot shows the cell states of LSTM unit3 (x-axis) and unit4 (y-axis).
unit3 records how many a?s the string begins with,while unit4 correlates with both length and the preponderance of b?s.
Some text labels are omitted for clarity.?
When we ask the system to transduce very longstrings, beyond what it has been trained on, itsoutput length may be slightly off.
For example,it transduces a string of 28 b?s into a string of27 b?s.
This is because unit1 is not incrementedand decremented by exactly 1.0.3 Full-Scale Neural Machine TranslationNext we turn to full-scale NMT.
We train ondata from the WMT 2014 English-to-French task,consisting of 12,075,604 sentence pairs, with303,873,236 tokens on the English side, and348,196,030 on the French side.
We use 1000 hid-den LSTM units.
We also use two layers of LSTMunits between source and target.5After the LSTM encoder-decoder is trained, wesend test-set English strings through the encoderportion.
Every time a word token is consumed, werecord the LSTM cell values and the length of thewhen the translation is completely wrong, the length is still cor-rect (anonymous).5Additional training details: 8 epochs, 128 minibatch size,0.35 learning rate, 5.0 gradient clipping threshold.Top 10 units by ... 1st layer 2nd layerIndividual R2 0.868 0.947Greedy addition 0.968 0.957Beam search 0.969 0.958Table 1: R2 values showing how differently-chosen sets of 10LSTM hidden units correlate with length in the NMT encoder.string so far.
Over 143,379 token observations, weinvestigate how the LSTM encoder tracks length.With 1000 hidden units, it is difficult to build andinspect a heat map analogous to Figure 3.
Instead,we seek to predict string length from the cell values,using a weighted, linear combination of the 1000LSTM cell values.
We use the least-squares methodto find the best predictive weights, with resulting R2values of 0.990 (for the first layer, closer to sourcetext) and 0.981 (second layer).
So the entire networkrecords length very accurately.However, unlike in the toy problem, no single unittracks length perfectly.
The best unit in the secondlayer is unit109, which correlates with R2=0.894.We therefore employ three mechanisms to locate22801234<S> b b b a b aEncoder cell state-6-4-20241234<S> a b a b b bDecoder cell state-6-4-2024<EOS>ba<S> a b a b b bDecoder output probability0.0010.010.11Figure 3: The progression of LSTM state as the recurrent net-work encodes the string ?a b a b b b?.
Columns show the in-puts over time and rows show the outputs.
Red color indicatespositive values, and blue color indicates negative.
The valueof unit1 decreases during the encoding phase (top figure) andincreases during the decoding phase (middle figure).
The bot-tom figure shows the decoder?s probability of ending the targetstring (<EOS>).k Best subset of LSTM?s 1000 units R21 109 0.8942 334, 109 0.9363 334, 442, 109 0.9424 334, 442, 109, 53 0.9475 334, 442, 109, 53, 46 0.9516 334, 442, 109, 53, 46, 928 0.9537 334, 442, 109, 53, 46, 433, 663 0.955Table 2: Sets of k units chosen by beam search to optimallytrack length in the NMT encoder.
These units are from theLSTM?s second layer.0 5 10 15 20 25 0 5 10 15 20 25 30201510505Encoding DecodingUnit 109Unit 334log P(<EOS>)Figure 4: Action of translation unit109 and unit334 during theencoding and decoding of a sample sentence.
Also shown is thesoftmax log-prob of output <EOS>.a subset of units responsible for tracking length.
Weselect the top k units according to: (1) individualR2 scores, (2) greedy search, which repeatedly addsthe unit which maximizes the set?s R2 value, and (3)beam search.
Table 1 shows different subsets we ob-tain.
These are quite predictive of length.
Table 2shows how R2 increases as beam search augmentsthe subset of units.4 Mechanisms for DecodingFor the toy problem, Figure 3 (middle part) showshow the cell value of unit1 moves back to zero as thetarget string is built up.
It also shows (lower part)how the probability of target word <EOS> shoots uponce the correct target length has been achieved.MT decoding is trickier, because source and tar-get strings are not necessarily the same length, and2281target length depends on the words chosen.
Figure 4shows the action of unit109 and unit334 for a samplesentence.
They behave similarly on this sentence,but not identically.
These two units do not form asimple switch that controls length?rather, they arehigh-level features computed from lower/previousstates that contribute quantitatively to the decisionto end the sentence.Figure 4 also shows the log P(<EOS>) curve,where we note that the probability of outputting<EOS> rises sharply (from 10?8 to 10?4 to 0.998),rather than gradually.5 ConclusionWe determine how target length is regulated in NMTdecoding.
In future work, we hope to determine howother parts of the translator work, especially withreference to grammatical structure and transforma-tions.AcknowledgmentsThis work was supported by ARL/ARO (W911NF-10-1-0533), DARPA (HR0011-15-C-0115), and theScientific and Technological Research Council ofTurkey (TU?BI?TAK) (grants 114E628 and 215E201).ReferencesD.
Bahdanau, K. Cho, and Y. Bengio.
2014.
Neural ma-chine translation by jointly learning to align and trans-late.
In Proc.
ICLR.P.
Brown, S. della Pietra, V. della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263?311.P.
F. Brown, J. Cocke, S. della Pietra, V. della Pietra,F.
Jelinek, J. C. Lai, and R. L. Mercer.
1995.
Methodand system for natural language translation.
US Patent5,477,451.M.
A. Castan?o and F. Casacuberta.
1997.
A con-nectionist approach to machine translation.
In EU-ROSPEECH.S.
Hochreiter and J. Schmidhuber.
1997.
Lstm can solvehard long time lag problems.
Advances in neural in-formation processing systems, pages 473?479.M.
Luong, H. Pham, and C. Manning.
2015.
Effectiveapproaches to attention-based neural machine transla-tion.
In Proc.
EMNLP.R.
Neco and M. Forcada.
1997.
Asynchronous transla-tions with recurrent neural nets.
In International Conf.on Neural Networks, volume 4, pages 2535?2540.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
ACL.I.
Sutskever, O. Vinyals, and Q. V. Le.
2014.
Sequenceto sequence learning with neural networks.
In Proc.NIPS.P.
J. Werbos.
1990.
Backpropagation through time: whatit does and how to do it.
Proceedings of the IEEE,78(10):1550?1560.2282
