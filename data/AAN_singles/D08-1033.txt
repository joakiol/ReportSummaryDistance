Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314?323,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSampling Alignment Structure under a Bayesian Translation ModelJohn DeNero, Alexandre Bouchard-Co?te?
and Dan KleinComputer Science DepartmentUniversity of California, Berkeley{denero, bouchard, klein}@cs.berkeley.eduAbstractWe describe the first tractable Gibbs sam-pling procedure for estimating phrase pairfrequencies under a probabilistic model ofphrase alignment.
We propose and evalu-ate two nonparametric priors that successfullyavoid the degenerate behavior noted in previ-ous work, where overly large phrases mem-orize the training data.
Phrase table weightslearned under our model yield an increase inBLEU score over the word-alignment basedheuristic estimates used regularly in phrase-based translation systems.1 IntroductionIn phrase-based translation, statistical knowledgeof translation equivalence is primarily captured bycounts of how frequently various phrase pairs occurin training bitexts.
Since bitexts do not come seg-mented and aligned into phrase pairs, these countsare typically gathered by fixing a word alignmentand applying phrase extraction heuristics to thisword-aligned training corpus.
Alternatively, phrasepair frequencies can be learned via a probabilisticmodel of phrase alignment, but this approach haspresented several practical challenges.In this paper, we address the two most signifi-cant challenges in phrase alignment modeling.
Thefirst challenge is with inference: computing align-ment expectations under general phrase models is#P-hard (DeNero and Klein, 2008).
Previous phrasealignment work has sacrificed consistency for effi-ciency, employing greedy hill-climbing algorithmsand constraining inference with word alignments(Marcu and Wong, 2002; DeNero et al, 2006; Birchet al, 2006).
We describe a Gibbs sampler that con-sistently and efficiently approximates expectations,using only polynomial-time computable operators.Despite the combinatorial complexity of the phrasealignment space, our sampled phrase pair expecta-tions are guaranteed to converge to the true poste-rior distributions under the model (in theory) and doconverge to effective values (in practice).The second challenge in learning phrase align-ments is avoiding a degenerate behavior of the gen-eral model class: as with many models which canchoose between large and small structures, the largerstructures win out in maximum likelihood estima-tion.
Indeed, the maximum likelihood estimate ofa joint phrase alignment model analyzes each sen-tence pair as one large phrase with no internal struc-ture (Marcu andWong, 2002).
We describe two non-parametric priors that empirically avoid this degen-erate solution.Fixed word alignments are used in virtually ev-ery statistical machine translation system, if not toextract phrase pairs or rules directly, then at leastto constrain the inference procedure for higher-levelmodels.
We estimate phrase translation featuresconsistently using an inference procedure that is notconstrained by word alignments, or any other heuris-tic.
Despite this substantial change in approach, wereport translation improvements over the standardword-alignment-based heuristic estimates of phrasetable weights.
We view this result as an importantstep toward building fully model-based translationsystems that rely on fewer procedural heuristics.2 Phrase Alignment ModelWhile state-of-the-art phrase-based translation sys-tems include an increasing number of features,translation behavior is largely driven by the phrasepair count ratios ?
(e|f) and ?
(f |e).
These featuresare typically estimated heuristically using the countsc(?e, f?)
of all phrase pairs in a training corpus thatare licensed by word alignments:?
(e|f) =c(?e, f?)?e?
c(?e?, f?
).314Gracias,lohar?demuybuengrado.youdo soThank , I shallgladly.youdo soThank , I shallgladly.Gracias,lohar?demuybuengrado.
(a) example word alignment (b) example phrase alignmentFigure 1: In this corpus example, the phrasealignment model found the non-literal translationpair ?gladly, de muy buen grado?
while heuristically-combined word alignment models did not.
(a) is a grow-diag-final-and combined IBM Model 4 word alignment;(b) is a phrase alignment under our model.In contrast, a generative model that explicitlyaligns pairs of phrases ?e, f?
gives us well-foundedalternatives for estimating phrase pair scores.
Forinstance, we could use the model?s parameters astranslation features.
In this paper, we compute theexpected counts of phrase pairs in the training dataaccording to our model, and derive features fromthese expected counts.
This approach endows phrasepair scores with well-defined semantics relative to aprobabilistic model.
Practically, phrase models candiscover high-quality phrase pairs that often eludeheuristics, as in Figure 1.
In addition, the model-based approach fits neatly into the framework of sta-tistical learning theory for unsupervised problems.2.1 Generative Model DescriptionWe first describe the symmetric joint model ofMarcu and Wong (2002), which we will extend.
Atwo-step generative process constructs an orderedset of English phrases e1:m, an ordered set of for-eign phrases f1:n, and a phrase-to-phrase alignmentbetween them, a = {(j, k)} indicating that ?ej , fk?is an aligned pair.1.
Choose a number of components ` and generateeach of ` phrase pairs independently.2.
Choose an ordering for the phrases in the for-eign language; the ordering for English is fixedby the generation order.11We choose the foreign to reorder without loss of generality.In this process, m = n = |a|; all phrases in bothsentences are aligned one-to-one.We parameterize the choice of ` using a geometricdistribution, denoted PG, with stop parameter p$:P (`) = PG(`; p$) = p$ ?
(1 ?
p$)`?1 .Each aligned phrase pair ?e, f?
is drawn from amultinomial distribution ?J which is unknown.
Wefix a simple distortion model, setting the probabilityof a permutation of the foreign phrases proportionalto the product of position-based distortion penaltiesfor each phrase:P (a|{?e, f?})
??a?a?(a)?
(a = (j, k)) = b|pos(ej)?pos(fk)?s| ,where pos(?)
denotes the word position of the startof a phrase, and s the ratio of the length of the En-glish to the length of the foreign sentence.
This po-sitional distortion model was deemed to work bestby Marcu and Wong (2002).We can now state the joint probability for aphrase-aligned sentence consisting of ` phrase pairs:P ({?e, f?
}, a) = PG(`; p$)P (a|{?e, f?})??e,f?
?J(?e, f?)
.While this model has several free parameters in ad-dition to ?J, we fix them to reasonable values to fo-cus learning on the phrase pair distribution.22.2 Unaligned PhrasesSentence pairs do not always contain equal informa-tion on both sides, and so we revise the generativestory to include unaligned phrases in both sentences.When generating each component of a sentence pair,we first decide whether to generate an aligned phrasepair or, with probability p?, an unaligned phrase.3Then, we either generate an aligned phrase pair from?J or an unaligned phrase from ?N, where ?N is amultinomial over phrases.
Now, when generatinge1:m, f1:n and alignment a, the number of phrasesm+ n can be greater than 2 ?
|a|.2Parameters were chosen by hand during development on asmall training corpus.
p$ = 0.1, b = 0.85 in experiments.3We strongly discouraged unaligned phrases in order toalign as much of the corpus as possible: p?
= 10?10 in ex-periments.315To unify notation, we denote unaligned phrases asphrase pairs with one side equal to null: ?e, null?
or?null, f?.
Then, the revised model takes the form:P ({?e, f?
},a) = PG(`; p$)P (a|{?e, f?})?
?e,f?PM(?e, f?
)PM(?e, f?)
= p?
?N(?e, f?)
+ (1 ?
p?
)?J(?e, f?)
.In this definition, the distribution ?N gives non-zero weight only to unaligned phrases of the form?e, null?
or ?null, f?, while ?J gives non-zeroweight only to aligned phrase pairs.3 Model Training and ExpectationsOur model involves observed sentence pairs, whichin aggregate we can call x, latent phrase segmenta-tions and alignments, which we can call z, and pa-rameters ?J and ?N, which together we can call ?.A model such as ours could be used either for thelearning of the key phrase pair parameters in ?, orto compute expected counts of phrase pairs in ourdata.
These two uses are very closely related, butwe focus on the computation of phrase pair expecta-tions.
For exposition purposes, we describe a Gibbssampling algorithm for computing expected countsof phrases under P (z|x, ?)
for fixed ?.
Such ex-pectations would be used, for example, to computemaximum likelihood estimates in the E-step of EM.In Section 4, we instead compute expectations underP (z|x), with ?
marginalized out entirely.In a Gibbs sampler, we start with a completephrase segmentation and alignment, state z0, whichsets all latent variables to some initial configuration.We then produce a sequence of sample states zi,each of which differs from the last by some smalllocal change.
The samples zi are guaranteed (in thelimit) to consistently approximate the conditionaldistribution P (z|x, ?)
(or P (z|x) later).
Therefore,the average counts of phrase pairs in the samplesconverge to expected counts under the model.
Nor-malizing these expected counts yields estimates forthe features ?
(e|f) and ?
(f |e).Gibbs sampling is not new to the natural languageprocessing community (Teh, 2006; Johnson et al,2007).
However, it is usually used as a search pro-cedure akin to simulated annealing, rather than forapproximating expectations (Goldwater et al, 2006;Finkel et al, 2007).
Our application is also atypicalfor an NLP application in that we use an approxi-mate sampler not only to include Bayesian prior in-formation (section 4), but also because computingphrase alignment expectations exactly is a #P-hardproblem (DeNero and Klein, 2008).
That is, wecould not run EM exactly, even if we wanted maxi-mum likelihood estimates.3.1 Related WorkExpected phrase pair counts under P (z|x, ?)
havebeen approximated before in order to run EM.Marcu and Wong (2002) employed local searchfrom a heuristic initialization and collected align-ment counts during a hill climb through the align-ment space.
DeNero et al (2006) instead proposedan exponential-time dynamic program pruned usingword alignments.
Subsequent work has relied heav-ily on word alignments to constrain inference, evenunder reordering models that admit polynomial-timeE-steps (Cherry and Lin, 2007; Zhang et al, 2008).None of these approximations are consistent, andthey offer no method of measuring their biases.Gibbs sampling is not only consistent in the limit,but also allows us to add Bayesian priors conve-niently (section 4).
Of course, sampling has liabili-ties as well: we do not know in advance how long weneed to run the sampler to approximate the desiredexpectations ?closely enough.
?Snyder and Barzilay (2008) describe a Gibbs sam-pler for a bilingual morphology model very similarin structure to ours.
However, the basic samplingstep they propose ?
resampling all segmentationsand alignments for a sequence at once ?
requires a#P-hard computation.
While this asymptotic com-plexity was apparently not prohibitive in the case ofmorphological alignment, where the sequences areshort, it is prohibitive in phrase alignment, where thesentences are often very long.3.2 Sampling with the SWAP OperatorOur Gibbs sampler repeatedly applies each of fiveoperators to each position in each training sentencepair.
Each operator freezes all of the current state ziexcept a small local region, determines all the waysthat region can be reconfigured, and then chooses a(possibly) slightly different zi+1 from among thoseoutcomes according to the conditional probability ofeach, given the frozen remainder of the state.
This316frozen region of the state is called a Markov blanket(denoted m), and plays a critical role in proving thecorrectness of the sampler.The first operator we consider is SWAP, whichchanges alignments but not segmentations.
It freezesthe set of phrases, then picks two English phrases e1and e2 (or two foreign phrases, but we focus on theEnglish case).
All alignments are frozen except thephrase pairs ?e1, f1?
and ?e2, f2?.
SWAP chooses be-tween keeping ?e1, f1?
and ?e2, f2?
aligned as theyare (outcome o0), or swapping their alignments tocreate ?e1, f2?
and ?e2, f1?
(outcome o1).SWAP chooses stochastically in proportion toeach outcome?s posterior probability: P (o0|m,x, ?
)and P (o1|m,x, ?).
Each phrase pair in each out-come contributes to these posteriors the probabilityof adding a new pair, deciding whether it is null, andgenerating the phrase pair along with its contribu-tion to the distortion probability.
This is all capturedin a succinct potential function ?
(?e, f?)
={(1?p$) (1?p?)
?J(?e, f?)
?
(?e, f?)
e & f non-null(1?p$) ?
p?
?
?N(?e, f?)
otherwise.Thus, outcome o0 is chosen with probabilityP (o0|m,x, ?)
=?
(?e1, f1?)?
(?e2, f2?)?
(?e1, f1?)?
(?e2, f2?)
+ ?
(?e1, f2?)?
(?e2, f1?
).Operators in a Gibbs sampler require certain con-ditions to guarantee the correctness of the sampler.First, they must choose among all possible configu-rations of the unfrozen local state.
Second, imme-diately re-applying the operator from any outcomemust yield the same set of outcome options as be-fore.4 If these conditions are not met, the samplermay no longer be guaranteed to yield consistent ap-proximations of the posterior distribution.A subtle issue arises with SWAP as defined:should it also consider an outcome o2 of ?e1, null?and ?e2, null?
that removes alignments?
No partof the frozen state is changed by removing thesealignments, so the first Gibbs condition dictates thatwe must include o2.
However, after choosing o2,when we reapply the operator to positions e1 and4These are two sufficient conditions to guarantee that theMetropolis-Hastings acceptance ratio of the sampling step is 1.
(b) FLIP(a) SWAP(c) TOGGLE(d) FLIP TWO(e) MOVEFigure 2: Each local operator manipulates a small portionof a single alignment.
Relevant phrases are exaggeratedfor clarity.
The outcome sets (depicted by arrows) of eachpossible configuration are fully connected.
Certain con-figurations cannot be altered by certain operators, such asthe final configuration in SWAP.
Unalterable configura-tions for TOGGLE have been omitted for space.e2, we freeze all alignments except ?e1, null?
and?e2, null?, which prevents us from returning to o0.Thus, we fail to satisfy the second condition.
Thispoint is worth emphasizing because some prior workhas treated Gibbs sampling as randomized searchand, intentionally or otherwise, proposed inconsis-tent operators.Luckily, the problem is not with SWAP, but withour justification of it: we can salvage SWAP by aug-menting its Markov blanket.
Given that we have se-lected ?e1, f1?
and ?e2, f2?, we not only freeze allother alignments and phrase boundaries, but also thenumber of aligned phrase pairs.
With this count heldinvariant, o2 is not among the possible outcomes ofSWAP given m. Moreover, regardless of the out-come chosen, SWAP can immediately be reappliedat the same location with the same set of outcomes.All the possible starting configurations and out-come sets for SWAP appear in Figure 2(a).317The boys areElloscomenCurrent StateIncludes segmentationsand alignments for allsentence pairsMarkov BlanketFreezes most of thesegmentations andalignments, along withthe alignment countOutcomesAn exhaustive set ofpossibilities giventhe Markov blanketeating?
?Apply the FLIP operatorto English position 11Compute the conditionalprobability of each outcome2Finally, select a new state proportionalto its conditional probability3?Figure 3: The three steps involved in applying the FLIPoperator.
The Markov blanket freezes all segmentationsexcept English position 1 and all alignments except thosefor Ellos and The boys.
The blanket alo freezes the num-ber of alignments, which disallows the lower right out-come.3.3 The FLIP operatorSWAP can arbitrarily shuffle alignments, but weneed a second operator to change the actual phraseboundaries.
The FLIP operator changes the status ofa single segmentation position5 to be either a phraseboundary or not.
In this sense FLIP is a bilingualanalog of the segmentation boundary flipping oper-ator of Goldwater et al (2006).Figure 3 diagrams the operator and its Markovblanket.
First, FLIP chooses any between-word po-sition in either sentence.
The outcome sets for FLIPvary based on the current segmentation and adjacentalignments, and are depicted in Figure 2.Again, for FLIP to satisfy the Gibbs conditions,we must augment its Markov blanket to freeze notonly all other segmentation points and alignments,but also the number of aligned phrase pairs.
Oth-erwise, we end up allowing outcomes from which5A segmentation position is a position between two wordsthat is also potentially a boundary between two phrases in analigned sentence pair.we cannot return to the original state by reapply-ing FLIP.
Consequently, when a position is alreadysegmented and both adjacent phrases are currentlyaligned, FLIP cannot unsegment the point becauseit can?t create two aligned phrase pairs with the onelarger phrase that results (see bottom of Figure 2(b)).3.4 The TOGGLE operatorBoth SWAP and FLIP freeze the number of align-ments in a sentence.
The TOGGLE operator, on theother hand, can add or remove individual alignmentlinks.
In TOGGLE, we first choose an e1 and f1.
If?e1, f1?
?
a or both e1 and f1 are null, we freezeall segmentations and the rest of the alignments, andchoose between including ?e1, f1?
in the alignmentor leaving both e1 and f1 unaligned.
If only one ofe1 and f1 are aligned, or they are not aligned to eachother, then TOGGLE does nothing.3.5 A Complete SamplerTogether, FLIP, SWAP and TOGGLE constitute acomplete Gibbs sampler that consistently samplesfrom the posterior P (z|x, ?).
Not only are theseoperators valid Gibbs steps, but they also can forma path of positive probability from any source stateto any target state in the space of phrase alignments(formally, the induced Markov chain is irreducible).Such a path can at worst be constructed by unalign-ing all phrases in the source state with TOGGLE,composing applications of FLIP to match the targetphrase boundaries, then applying TOGGLE to matchthe target algnments.We include two more local operators to speed upthe rate at which the sampler explores the hypothesisspace.
In short, FLIP TWO simultaneously flips anEnglish and a foreign segmentation point (to make alarge phrase out of two smaller ones or vice versa),while MOVE shifts an aligned phrase boundary tothe left or right.
We omit details for lack of space.3.6 Phrase Pair Count EstimationWith our sampling procedure in place, we can nowestimate the expected number of times a givenphrase pair occurs in our data, for fixed ?, using aMonte-Carlo average,1NN?i=1count?e,f?
(x, zi)a.s.??
E[count?e,f?
(x, ?
)].318The left hand side is simple to compute; we countaligned phrase pairs in each sample we generate.In practice, we only count phrase pairs after apply-ing every operator to every position in every sen-tence (one iteration).6 Appropriate normalizationsof these expected counts can be used either in an M-step as maximum likelihood estimates, or to com-pute values for features ?
(f |e) and ?
(e|f).4 Nonparametric Bayesian PriorsThe Gibbs sampler we presented addresses the infer-ence challenges of learning phrase alignment mod-els.
With slight modifications, it also enables us toinclude prior information into the model.
In this sec-tion, we treat ?
as a random variable and shape itsprior distribution in order to correct the well-knowndegenerate behavior of the model.4.1 Model DegeneracyThe structure of our joint model penalizes explana-tions that use many small phrase pairs.
Each phrasepair token incurs the additional expense of genera-tion and distortion.
In fact, the maximum likelihoodestimate of the model puts mass on ?e, f?
pairs thatspan entire sentences, explaining the training corpuswith one phrase pair per sentence.Previous phrase alignment work has primarilymitigated this tendency by constraining the in-ference procedure, for example with word align-ments and linguistic features (Birch et al, 2006),or by disallowing large phrase pairs using a non-compositional constraint (Cherry and Lin, 2007;Zhang et al, 2008).
However, the problem lies withthe model, and therefore should be corrected in themodel, rather than the inference procedure.Model-based solutions appear in the literature aswell, though typically combined with word align-ment constraints on inference.
A sparse Dirichletprior coupled with variational EM was explored byZhang et al (2008), but it did not avoid the degen-erate solution.
Moore and Quirk (2007) proposed anew conditional model structure that does not causelarge and small phrases to compete for probabil-ity mass.
May and Knight (2007) added additionalmodel terms to balance the cost of long and shortderivations in a syntactic alignment model.6For experiments, we ran the sampler for 100 iterations.4.2 A Dirichlet Process PriorWe control this degenerate behavior by placing aDirichlet process (DP) prior over ?J, the distributionover aligned phrase pairs (Ferguson, 1973).If we were to assume a maximum number K ofphrase pair types, a (finite) Dirichlet distributionwould be an appropriate prior.
A draw from a K-dimensional Dirichlet distribution is a list of K realnumbers in [0, 1] that sum to one, which can be in-terpreted as a distribution overK phrase pair types.However, since the event space of possible phrasepairs is in principle unbounded, we instead use aDirichlet process.
A draw from a DP is a countablyinfinite list of real numbers in [0, 1] that sum to one,which we interpret as a distribution over a countablyinfinite list of phrase pair types.7The Dirichlet distribution and the DP distributionhave similar parameterizations.
A K-dimensionalDirichlet can be parameterized with a concentrationparameter ?
> 0 and a base distribution M0 =(?1, .
.
.
, ?K?1), with ?i ?
(0, 1).8 This parameteri-zation has an intuitive interpretation: under these pa-rameters, the average of independent samples fromthe Dirichlet will converge toM0.
That is, the aver-age of the ith element of the samples will convergeto ?i.
Hence, the base distributionM0 characterizesthe sample mean.
The concentration parameter ?only affects the variance of the draws.Similarly, we can parameterize the Dirichlet pro-cess with a concentration parameter ?
(that affectsonly the variance) and a base distribution M0 thatdetermines the mean of the samples.
Just as in thefinite Dirichlet case,M0 is simply a probability dis-tribution, but now with countably infinite support:all possible phrase pairs in our case.
In practice, wecan use an unnormalized M0 (a base measure) byappropriately rescaling ?.In our model, we select a base measure thatstrongly prefers shorter phrases, encouraging themodel to use large phrases only when it has suffi-cient evidence for them.
We continue the model:7Technical note: to simplify exposition, we restrict the dis-cussion to settings such as ours where the base measure of theDP has countable support.8This parametrization is equivalent to the standard pseudo-counts parametrization of K positive real numbers.
The bi-jection is given by ?
=PKi=1 ?
?i and ?i = ?
?i/?, where(?
?1, .
.
.
, ?
?K) are the pseudo-counts.319?J ?
DP (M0, ?
)M0(?e, f?)
= [Pf (f)PWA(e|f) ?
Pe(e)PWA(f |e)]12Pf (f) = PG(|f |; ps) ?
(1nf)|f |Pe(e) = PG(|e|; ps) ?
(1ne)|e|..PWA is the IBM model 1 likelihood of one phraseconditioned on the other (Brown et al, 1994).
Pfand Pe are uniform over types for each phraselength: the constants nf and ne denote the vocab-ulary size of the foreign and English languages, re-spectively, and PG is a geometric distribution.Above, ?J is drawn from a DP centered on the ge-ometric mean of two joint distributions over phrasepairs, each of which is composed of a monolingualunigram model and a lexical translation component.This prior has two advantages.
First, we pressurethe model to use smaller phrases by increasing ps(ps = 0.8 in experiments).
Second, we encour-age good phrase pairs by incorporating IBM Model1 distributions.
This use of word alignment distri-butions is notably different from lexical weightingor word alignment constraints: we are supplyingprior knowledge that phrases will generally followword alignments, though with enough corpus evi-dence they need not (and often do not) do so in theposterior samples.
The model proved largely insen-sitive to changes in the sparsity parameter ?, whichwe set to 100 for experiments.4.3 Unaligned phrases and the DP PriorIntroducing unaligned phrases invites further degen-erate megaphrase behavior: a sentence pair can begenerated cheaply as two unaligned phrases thateach span an entire sentence.
We attempted to placea similar DP prior over ?N, but surprisingly, thismodeling choice invoked yet another degenerate be-havior.
The DP prior imposes a rich-get-richer prop-erty over the phrase pair distribution, strongly en-couraging the model to reuse existing pairs ratherthan generate new ones.
As a result, commonwords consistently aligned to null, even while suit-able translations were present, simply because eachnull alignment reinforced the next.
For instance, thewas always unaligned.Instead, we fix ?N to a simple unigram model thatis uniform over word types.
This way, we discour-age unaligned phrases while focusing learning on ?J.For simplicity, we reuse Pf (f) and Pe(e) from theprior over ?J.
?N(?e, f?)
={12 ?
Pe(e) if f = null12 ?
Pf (f) if e = null .The 12 represents a choice of whether the alignedphrase is in the foreign or English sentence.4.4 Collapsed Sampling with a DP PriorOur entire model now has the general formP (x, z, ?J); all other model parameters have beenfixed.
Instead of searching for a suitable ?J,9 wesample from the posterior distribution P (z|x) with?J marginalized out.To this end, we convert our Gibbs sampler intoa collapsed Gibbs sampler10 using the ChineseRestaurant Process (CRP) representation of the DP(Aldous, 1985).
With the CRP, we avoid the prob-lem of explicitely representing samples from theDP.
CRP-based samplers have served the commu-nity well in related language tasks, such as word seg-mentation and coreference resolution (Goldwater etal., 2006; Haghighi and Klein, 2007).Under this representation, the probability of eachsampling outcome is a simple expression in termsof the state of the rest of the training corpus (theMarkov blanket), rather than explicitly using ?J.Let zm be the set of aligned phrase pair tokens ob-served in the rest of the corpus.
Then, when ?e, f?
isaligned (that is, neither e nor f are null), the condi-tional probability for a pair ?e, f?
takes the form:?
(?e, f?|zm) =count?e,f?
(zm) + ?
?M0(?e, f?
)|zm| + ?,where count?e,f?
(zm) is the number of times that?e, f?
appears in zm.
We can write this expressionthanks to the exchangeability of the model.
For fur-ther exposition of this collapsed sampler posterior,9For instance, using approximate MAP EM.10A collapsed sampler is simply one in which the model pa-rameters have been marginalized out.32002550751002007 20081 x 11 x 2, 2 x 12 x 22 x 3, 3 x 23+ x 3+02550751001x1 1x2 & 2x1 1x3 & 3x1 2x2 2x3 & 3x2 3x3 and upMinimal extracted phrasesSampled phrasesAll extracted phrasesFigure 4: The distribution of phrase pair sizes (denotedEnglish length x foreign length) favors small phrases un-der the model.see Goldwater et al (2006).11The sampler remains exactly the same as de-scribed in Section 3, except that the posterior con-ditional probability of each outcome uses a revisedpotential function ?DP(?e, f?)
={(1?p$) (1?p?)
?
(?e, f?)
?
(?e, f?)
e & f non-null(1?p$) ?
p?
?
?N(?e, f?)
otherwise .
?DP is like ?, but the fixed ?J is replaced with theconstantly-updated ?
function.4.5 Degeneracy AnalysisFigure 4 shows a histogram of phrase pair sizes inthe distribution of expected counts under the model.As reference, we show the size distribution of bothminimal and all phrase pairs extracted from wordalignments using the standard heuristic.
Our modeltends to select minimal phrases, only using largerphrases when well motivated.12This result alone is important: a model-basedsolution with no inference constraint has yieldeda non-degenerate distribution over phrase lengths.Note that our sampler does find the degenerate solu-tion quickly under a uniform prior, confirming thatthe model, and not the inference procedure, is select-ing these small phrases.11Note that the expression for ?
changes slightly under con-ditions where two phrase pairs being changed simultaneouslycoincidentally share the same lexical content.
Details of thesefringe conditions have been omitted for space, but were in-cluded in our implementation.12The largest phrase pair found was 13 English words by 7Spanish words.4.6 A Hierarchical Dirichlet Process PriorWe also evaluate a hierarchical Dirichlet process(HDP) prior over ?J, which draws monolingual dis-tributions ?E and ?F from a DP and ?J from theircross-product:?J ?
DP (M?0, ?
)M ?0(?e, f?)
= [?F(f)PWA(e|f) ?
?E(e)PWA(f |e)]12?F ?
DP (Pf , ??
)?E ?
DP (Pe, ??)
.This prior encourages novel phrase pairs to be com-posed of phrases that have been used before.
In thesampler, we approximate table counts for ?E and?F with their expectations, which can be computedfrom phrase pair counts (see the appendix of Gold-water et al (2006) for details).
The HDP prior givesa similar distribution over phrase sizes.5 Translation ResultsWe evaluate our new estimates using the baselinetranslation pipeline from the 2007 Statistical Ma-chine Translation Workshop shared task.5.1 Baseline SystemWe trained Moses on all Spanish-English Europarlsentences up to length 20 (177k sentences) usingGIZA++ Model 4 word alignments and the grow-diag-final-and combination heuristic (Koehn et al,2007; Och and Ney, 2003; Koehn, 2002), whichperformed better than any alternative combinationheuristic.13 The baseline estimates (Heuristic) comefrom extracting phrases up to length 7 from the wordalignment.
We used a bidirectional lexicalized dis-tortion model that conditions on both foreign andEnglish phrases, along with their orientations.
Our5-gram language model was trained on 38.3 millionwords of Europarl using Kneser-Ney smoothing.
Wereport results with and without lexical weighting,denoted lex.We tuned and tested on development corpora forthe 2006 translation workshop.
The parameters foreach phrase table were tuned separately using min-imum error rate training (Och, 2003).
Results are13Sampling iteration time scales quadratically with sentencelength.
Short sentences were chosen to speed up our experimentcycle.321Phrase ExactPair NIST MatchEstimate Count BLEU METEORHeuristic 4.4M 29.8 52.4DP 0.6M 28.8 51.7HDP 0.3M 29.1 52.0DP-composed 3.7M 30.1 52.7HDP-composed 3.1M 30.1 52.6DP-smooth 4.8M 30.1 52.5HDP-smooth 4.6M 30.2 52.7Heuristic + lex 4.4M 30.5 52.9DP-smooth + lex 4.8M 30.4 53.0HDP-smooth + lex 4.6M 30.7 53.2Table 1: BLEU results for learned distributions improveover a heuristic baseline.
Estimate labels are describedfully in section 5.3.
The label lex indicates the additionof a lexical weighting feature.scored with lowercased, tokenized NIST BLEU, andexact match METEOR (Papineni et al, 2002; Lavieand Agarwal, 2007).The baseline system gives a BLEU score of 29.8,which increases to 30.5 with lex, as shown in Table1.
For reference, training on all sentences of lengthless than 40 (the shared task baseline default) gives32.4 BLEU with lex.5.2 Learned Distribution PerformanceWe initialized the sampler with a configuration de-rived from the word alignments generated by thebaseline.
We greedily constructed a phrase align-ment from the word alignment by identifying min-imal phrase pairs consistent with the word align-ment in each region of the sentence.
We then ranthe sampler for 100 iterations through the trainingdata.
Each iteration required 12 minutes under theDP prior, and 30 minutes under the HDP prior.
Totalrunning time for the HDP model neared two days onan eight-processor machine with 16 Gb of RAM.Estimating phrase counts under the DP prior de-creases BLEU to 28.8, or 29.1 under the HDP prior.This gap is not surprising: heuristic extraction dis-covers many more phrase pairs than sampling.
Notethat sacrificing only 0.7 BLEU while shrinking thephrase table by 92% is an appealing trade-off inresource-constrained settings.5.3 Increasing Phrase Pair CoverageThe estimates DP-composed and HDP-composed inTable 1 take expectations of a more liberal countfunction.
While sampling, we count not only alignedphrase pairs, but also larger ones composed of two ormore contiguous aligned pairs.
This count functionis similar to the phrase pair extraction heuristic, butnever includes unaligned phrases in any way.
Expec-tations of these composite phrases still have a proba-bilistic interpretation, but they are not the structureswe are directly modeling.
Notably, these estimatesoutperform the baseline by 0.3 BLEU without everextracting phrases from word alignments, and per-formance increases despite a reduction in table size.We can instead increase coverage by smooth-ing the learned estimates with the heuristic counts.The estimates DP-smooth and HDP-smooth addcounts extracted from word alignments to the sam-pler?s running totals, which improves performanceby 0.4 BLEU over the baseline.
This smoothing bal-ances the lower-bias sampler counts with the lower-variance heuristics ones.6 ConclusionOur novel Gibbs sampler and nonparametric pri-ors together address two open problems in learn-ing phrase alignment models, approximating infer-ence consistently and efficiently while avoiding de-generate solutions.
While improvements are mod-est relative to the highly developed word-alignment-centered baseline, we show for the first time com-petitive results from a system that uses word align-ments only for model initialization and smoothing,rather than inference and estimation.
We view thismilestone as critical to eventually developing a cleanprobabilistic approach to machine translation thatunifies model structure across both estimation anddecoding, and decreases the use of heuristics.ReferencesDavid Aldous.
1985.
Exchangeability and related topics.In E?cole d?e?te?
de probabilitie?s de Saint-Flour, Berlin.Springer.Alexandra Birch, Chris Callison-Burch, and Miles Os-borne.
2006.
Constraining the phrase-based, jointprobability statistical translation model.
In The Con-322ference for the Association for Machine Translation inthe Americas.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.Colin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.
InThe Annual Conference of the North American Chap-ter of the Association for Computational LinguisticsWorkshop on Syntax and Structure in Statistical Trans-lation.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In The Annual Confer-ence of the Association for Computational Linguistics:Short Paper Track.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In The Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics Workshop on Statistical Ma-chine Translation.Thomas S Ferguson.
1973.
A bayesian analysis of somenonparametric problems.
In Annals of Statistics.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In The Annual Con-ference of the Association for Computational Linguis-tics.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In The Annual Conference of theAssociation for Computational Linguistics.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In The Annual Conference of the Associationfor Computational Linguistics.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In The Annual Conference of theAssociation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In The An-nual Conference of the Association for ComputationalLinguistics.Philipp Koehn.
2002.
Europarl: A multilingual corpusfor evaluation of machine translation.Alon Lavie and Abhaya Agarwal.
2007.
Meteor: Anautomatic metric for mt evaluation with high levelsof correlation with human judgments.
In The AnnualConference of the Association for Computational Lin-guistics Workshop on Statistical Machine Translation.Daniel Marcu and Daniel Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In The Conference on Empirical Methods inNatural Language Processing.Jonathan May and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In TheConference on Empirical Methods in Natural Lan-guage Processing.Robert Moore and Chris Quirk.
2007.
An iteratively-trained segmentation-free phrase translation model forstatistical machine translation.
In The Annual Confer-ence of the Association for Computational LinguisticsWorkshop on Statistical Machine Translation.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In The Annual Confer-ence of the Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In The Annual Confer-ence of the Association for Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In The Annual Conference of the Associationfor Computational Linguistics.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In The AnnualConference of the Association for Computational Lin-guistics.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InThe Annual Conference of the Association for Compu-tational Linguistics.323
