Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 255?262, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 3: Spatial Role LabelingOleksandr Kolomiyets?, Parisa Kordjamshidi?,Steven Bethard?
and Marie-Francine Moens?
?KU Leuven, Celestijnenlaan 200A, Heverlee 3001, Belgium?University of Colorado, Campus Box 594 Boulder, Colorado, USAAbstractMany NLP applications require informationabout locations of objects referenced in text,or relations between them in space.
For ex-ample, the phrase a book on the desk containsinformation about the location of the objectbook, as trajector, with respect to another ob-ject desk, as landmark.
Spatial Role Label-ing (SpRL) is an evaluation task in the infor-mation extraction domain which sets a goalto automatically process text and identify ob-jects of spatial scenes and relations betweenthem.
This paper describes the task in Se-mantic Evaluations 2013, annotation schema,corpora, participants, methods and results ob-tained by the participants.1 IntroductionSpatial Role Labeling at SemEval-2013 is the sec-ond iteration of the task, which was initially in-troduced at SemEval-2012 (Kordjamshidi et al2012a).
The second iteration extends the previouswork with an additional training corpus, which con-tains besides ?static?
spatial relations, annotated mo-tions.
Motion detection is a novel task for annotatingtrajectors (objects, which are moving), landmarks(spatial context in which the motion is performed),motion indicators (lexical triggers which signals tra-jector?s motion), paths (a path along which the mo-tion is performed), directions (absolute or relativedirections of trajector?s motion) and distances (adistance as a product of motion).
For annotatingmotions the existing annotation scheme has beenadapted with additional markables which are, all to-gether, described below.2 Spatial Annotation SchemaIn this Section we describe the annotation format ofspatial markables in text, and annotation guidelinesfor the annotators.2.1 Spatial Annotation FormatBuilding upon the previous work, we used the no-tions of trajectors, landmarks and spatial indicatorsas introduced by Kordjamshidi et al(2010).
In ad-dition, we further expanded the set of spatial roleslabels with motion indicators, paths, directions anddistances to capture fine-grained spatial semantics ofstatic spatial relations (as the ones which do not in-volve motions), and to accommodate dynamic spa-tial relations (the ones which do involve motions).2.1.1 Static Spatial Relations and their RolesStatic spatial relations are defined as relations be-tween still objects, whereas one object plays a cen-tral role in the spatial scene, which is called tra-jector, and the second one plays a secondary role,and it is called landmark.
In language, a spatial re-lation between two objects is usually implementedby a preposition (in, on, at, etc.)
or a prepositionalphrase (on top of, inside of, etc.
).A static spatial relation is defined as a tuple thatcontains a trajector, a landmark and a spatial indica-tor.
In the annotation schema, these annotations aredefined as follows:Trajector: Trajector is a spatial role label as-signed to a word or a phrase that denotes a centralobject of a spatial scene.
For example:?
[Trajector a lake] in the forest255?
[Trajector a flag] on top of the buildingLandmark: Landmark is a spatial role label as-signed to a word or a phrase that denotes a secondaryobject of a spatial scene, to which a possible spatialrelation (as between two objects in space) can be es-tablished.
For example:?
a lake in [Landmark the forest]?
a flag on top of [Landmark the building]Spatial Indicator: Spatial Indicator is a spatialrole label assigned to a word or a phrase that sig-nals a spatial relation between objects (trajectors andlandmarks) of a spatial scene.
For example:?
a lake [Sp indicator in] the forest?
a flag] [Sp indicator on top of ] the buildingSpatial Relation: Spatial Relation is a relationthat holds between spatial markables in text as, e.g.,between a trajector and a landmark and triggered bya spatial indicator.
In spatial information theory therelations and properties are usually grouped into thedomains of topological, directional, and distance re-lations and also shape (Stock, 1998).
Three semanticclasses for spatial relations were proposed:?
Region.
This type refers to a region of spacewhich is always defined in relation to a land-mark, e.g., the interior or exterior.
For exam-ple:a lake in the forest =?
?Region, [Sp indicatorin], [Trajector a lake], [Landmark the forest]??
Direction.
This relation type denotes a direc-tion along the axes provided by the differentframes of reference, in case the trajector of mo-tion is not characterized in terms of its relationto the region of a landmark.
For example:a flag on top of the building =?
?Direction,[Sp indicator on top of ], [Trajector a flag],[Landmark the building]??
Distance.
Type Distance states informationabout the spatial distance of the objects andcould be a qualitative expression, such as close,far or quantitative, such as 12 km.
For example:the kids are close to the blackboard =?
?Distance, [Distance close], [Trajector the kids],[Landmark the blackboard]?2.1.2 Dynamic Spatial RelationsIn addition to static spatial relations and theirroles, SpRL-2013 introduces new spatial roles tocapture dynamic spatial relations which involvemotions.
Let us demonstrate this with the followingexample:(1) In Brazil coming from the North-East Istepped into the small forest and followed down adried creek.The text above describes a motion, and the readercan identify a number of concepts which are pecu-liar for motions: there is an object whose locationis changing, the motion is performed in a specificspatial context, with a specific direction, and with anumber of locations related to the object?s motion.There has been an enormous effort in formalizingand annotating motions in natural language.
Whileannotating motions was out of scope for the previ-ous SpRL task and SpatialML (Mani et al 2010),the most recent work on the Dynamic Interval Tem-poral Logic (DITL) (Pustejovsky and Moszkowicz,2011) presents a framework for modeling motionsas a change of state, which adapts linguistic back-ground considering path constructions and manner-of-motion constructions.
On this basis the Spa-tiotemporal Markup Language (STML) has been in-troduced for annotating motions in natural language.In STML, a motion is treated as a change of locationover time, while differentiating between a numberof spatial configurations along the path.
Being well-defined for the formal representations of motion andreasoning, in which representations either take ex-plicit reference to temporal frames or reify a spatialobject for a path, all the previous work seems to bedifficult to apply in practice when annotating mo-tions in natural language.
It can be attributed to pos-sible vague descriptions of path in natural languagewhen neither clear temporal event ordering, nor dis-tinction between the start, end or intermediate pathpoint can be made.In SpRL-2013, we simplify the previously intro-duced notion of path in order to provide practicalmotion annotations.
For dynamic spatial relationswe introduce the following roles:256Trajector: Trajector is a spatial role label as-signed to a word or a phrase which denotes an objectwhich moves, starts, interrupts, resumes a motion, oris forcibly involved in a motion.
For example:?
... coming from the North-East [Trajector I]stepped into ...Motion Indicator: Motion indicator is a spatialrole label assigned to a word or a phrase which sig-nals a motion of the trajector along a path.
In Exam-ple (1), a number of motion indicators can be identi-fied:?
... [Motion coming] from the North-East I[Motion stepped into] ... and [Motion followeddown] ...Path: Path is a spatial role label assigned to a wordor phrase that denotes the path of the motion as thetrajector is moving along, starting in, arriving in ortraversing it.
In SpRL-2013, as opposite to STML,the notion of path does not have the temporal dimen-sion, thus whenever the motion is performed along apath, for which either a start, an intermediate, an endpath point, or an entire path can be identified in text,they are labeled as path.
In Example (1), a numberof path labels can be identified:?
... coming [Path from the North-East] I steppedinto [Path the small forest] and followed down[Path a dried creek].Landmark: The notion of path should not be con-fused with landmarks.
For spatial annotations, land-mark has been introduced as a spatial role label fora secondary object of the spatial scene.
Being ofgreat importance for static spatial relations, in dy-namic spatial relations, landmarks are used to cap-ture a spatial context of a motion as for example:?
In [Landmark Brazil] coming from the North-East ...Distance: In contrast to the previous SpRL anno-tation standard, in which distances and directionshave been uniformly treated as signals, in SpRL-2013 if the motion is performed for a certain dis-tance, and such a distance is mentioned in text, thecorresponding textual span is labeled as distance.Distance is a spatial role label assigned to a wordor a phrase that denotes an absolute or relative dis-tance of motion, or the distance between a trajectorand a landmark in case of a static spatial scene.
Forexample:?
[Distance 25 km]?
[Distance about 100 m]?
[Distance not far away]?
[Distance 25 min by car]Direction: Additionally, if the motion is per-formed in a certain (absolute or relative) direction,and such a direction is mentioned in text, the corre-sponding textual span is annotated as direction.
Di-rection is a spatial role label assigned to a word ora phrase that denotes an absolute or relative direc-tion of motion, or a spatial arrangement between atrajector and a landmark.
For example:?
[Direction the North-West]?
[Direction northwards]?
[Direction west]?
[Direction the left-hand side]Spatial Relation: Similarly to static spatial rela-tions, dynamic spatial relations are annotated by re-lations that hold between a number of spatial roles.The major difference to static spatial relations is themandatory motion indicator1.
For example:?
In Brazil coming from the North-East I ...=?
?Direction, [Sp indicator In], [Trajector I],[Landmark Brazil], [Motion coming],[Path fromthe North-East]??
...
I stepped into the small forest and ...=?
?Direction, [Trajector I], [Motion steppedinto],[Path the small forest]??
...
I [...] and followed down a dried creek.=?
?Direction, [Trajector I], [Motion followeddown],[Path a dried creek]?1All dynamic spatial relations were annotated with type Di-rection.257Corpus Files Sent.
TR LM SI MI Path Dir Dis RelationIAPR TC-12Training 1 600 716 661 670 - - - - 765Evaluation 1 613 872 743 796 - - - - 940ConfluenceProjectTraining 95 1422 1701 1037 879 1039 945 223 307 2105Evaluation 22 367 497 316 247 305 240 37 87 598Table 1: Corpus statistics for SpRL-2013 with respect to annotated spatial roles (trajectors (TR), landmarks (LM),spatial indicators (SI), motion indicators (MI), paths (Path), directions (Dir) and distances (Dis)) and spatial relations.3 CorporaThe data for the shared task comprises two differentcorpora.3.1 IAPR TC-12 Image Benchmark CorpusThe first corpus is a subset of the IAPR TC-12 imagebenchmark corpus (Grubinger et al 2006).
It con-tains 613 text files that include 1213 sentences in to-tal, and represents an extension of the dataset previ-ously used in (Kordjamshidi et al 2011).
The orig-inal corpus was available free of charge and withoutcopyright restrictions.
The corpus contains imagestaken by tourists with descriptions in different lan-guages.
The texts describe objects, and their abso-lute and relative positions in the image.
This makesthe corpus a rich resource for spatial information,however, the descriptions are not always limited tospatial information.
Therefore, they are less domain-specific and contain free explanations about the im-ages.
For training we released 600 sentences (about50% of the corpus), and used remaining 613 sen-tences for evaluations.3.2 Confluence Project CorpusThe second corpus comes from the Confluenceproject that targets the description of locations sit-uated at each of the latitude and longitude inte-ger degree intersection in the world.
This corpuscontains user-generated content produced by, some-times, non-native English speakers.
We gathered thecontent by keeping the original orthography and for-mating.
In addition, we stored the URLs of the de-scriptions and extracted the coordinates of the de-scribed confluence point, which might be interest-ing for further research.
In total, the entire corpuscontains 117 files with 1789 sentences (about 40,000tokens).
For training we released 95 annotated fileswith 1422 sentences, 2105 annotated relations in to-tal.
For evaluation we used 22 annotated files with367 sentences.
The statistics on both corpora areprovided in Table 1.3.3 Data FormatOne important change to the data was made inSpRL-2013.
In contrast to SpRL-2012, where spa-tial roles were annotated over ?head words?
whoseindexes were part of unique identifiers, in SpRL-2013 we switched to span-based annotations.
More-over, in order to provide a single data format forthe task, we transformed SpRL-2012 data into span-based annotations, in course of which, we identifieda number of annotation errors and made further im-provements for about 50 annotations.For annotating the Confluence Project corpus weused a freely available annotation tool MAE createdby Amber Stubbs (Stubbs, 2011).
The resulting dataformat uses the same annotation tags as in SpRL-2012, but each role annotation refers to a characteroffset in the original text2.
Spatial relations are com-posed of references to annotations by their uniqueidentifiers.
Similarly to SpRL-2012, we allowedannotators to provide non-consuming annotations,where entity mentions, for which spatial roles canbe identified, are omitted in text but necessary for aspatial relation triggered by either a spatial indicatoror a motion indicator.
Two spatial roles are eligiblefor non-consuming annotations: trajectors and land-marks.4 Tasks DescriptionsFor the sake of consistency with SpRL-2012, inSpRL-2013 we proposed the following tasks:2Due to paper length constraints we omit the BNF specifica-tions for spatial roles and relations.
For further data format in-formation we refer the reader to the task description web page:www.cs.york.ac.uk/semeval-2013/task3/258?
Task A: Identification of markable spans forthree types of spatial annotations such as tra-jector, landmark and spatial indicator.?
Task B: Identification of tuples (triplets) thatconnect trajectors, landmarks and spatial indi-cators identified in Task A into spatial relations.That is, identification of spatial relations withthree markables connected, and without se-mantic relation classification.?
Task C: Identification of markable spans for allspatial annotations such as trajector, landmark,spatial indicator, motion indicator, path, direc-tion and distance.?
Task D: Identification of n-tuples that connectspatial markables identified in Task C into spa-tial relations.
That is, identification of spatialrelations with as many participating mark-ables as possible, and without semantic rela-tion classification.?
Task E: Semantic classification of spatial rela-tions identified in Task D.5 Evaluation Criteria and MetricsSystem outputs were evaluated against the goldannotations, which had to conform to the role?sBackus-Naur form.
For Tasks A and C, the systemannotations are spatial roles: spans of text associatedwith spatial role types.
A system annotation of arole is considered correct if it has a minimal overlapof one character with a gold annotation and matchesthe role type of the gold annotation.
For Tasks B andD, the system annotations are spatial relation tuples(of length 3 in task B, of length 3 to 5 in Task D) ofreferences to markable annotations.
A system anno-tation of a spatial relation tuple is considered correctif it is of the same length as the gold annotation, andif each spatial role in the system tuple matches eachrole in the gold tuple.
A spatial role estimated by asystem is considered correct if it matches a gold ref-erence when having the same character offsets andmarkable types (strict evaluation settings).
In ad-dition we introduced relaxed evaluation settings, inwhich a minimal overlap of one character betweena system and a gold markable references is requiredfor a positive match under condition that the rolesmatch.
For Task E, the system annotations are spa-tial relation tuples of length 3 to 5, along with re-lation type labels.
A system annotation of a spatialrelation is considered correct if the spatial relationtuple is correct under the evaluation of Task D andthe relation type of the system relation is the sameas the relation type of the gold relation.Systems were evaluated for each of the tasks interms of precision (P), recall (R) and F1-score whichare defined as follows:Precision =tptp + fp(1)Recall =tptp + fn(2)where tp is the number of true positives (the num-ber of instances that are correctly found), fp is thenumber of false positives (number of instances thatare predicted by the system but not a true instance),and fn is the number of false negatives (missing re-sults).F1 = 2 ?Precision ?RecallPrecision + Recall(3)6 System Description and EvaluationResultsUNITOR.
The UNITOR-HMM-TK system ad-dressed Tasks A,B and C (Bastianelli et al 2013).In Tasks A and C, roles are labeled by a sequence-based classifier: each word in a sentence is classi-fied with respect to the possible spatial roles.
Anapproach based on the SVM-HMM learning algo-rithm, formulated in (Tsochantaridis et al 2006),was used.
It is in line with other methods basedon sequence-based classifier for Spatial Role La-beling, such as Conditional Random Fields (Kord-jamshidi et al 2011), and the same SVM-HMMlearning algorithm (Kordjamshidi et al 2012b).UNITOR?s labeling approach has been inspired bythe work in (Croce et al 2012), where an SVM-HMM learning algorithm has been applied to theclassical FrameNet-based Semantic Role Labeling.The main contribution of the proposed approach isthe adoption of shallow grammatical features insteadof the full syntax of the sentence, in order to avoidover-fitting on the training data.
Moreover, lexicalinformation has been generalized through the use259Run Task Evaluation Label P R F1-scoreUNITOR.Run1.1Task A relaxedTR 0.684 0.681 0.682LM 0.741 0.835 0.785SI 0.967 0.889 0.926Task Brelaxed Relation 0.551 0.391 0.458strict Relation 0.431 0.306 0.358UNITOR.Run1.2Task A relaxedTR 0.682 0.493 0.572LM 0.801 0.560 0.659SI 0.968 0.585 0.729Task Brelaxed Relation 0.551 0.391 0.458strict Relation 0.431 0.306 0.358UNITOR.Run2.1Task A relaxedTR 0.565 0.317 0.406LM 0.661 0.476 0.554SI 0.612 0.481 0.538Task C relaxedTR 0.565 0.317 0.406LM 0.662 0.476 0.554SI 0.609 0.479 0.536MI 0.892 0.294 0.443Path 0.775 0.295 0.427Dir 0.312 0.229 0.264Dis 0.946 0.331 0.490Table 2: Results of UNITOR for SpRL-2013 tasks (Task A, B and C).of Word Space ?
a Distributional Model of Lexi-cal Semantics derived from the unsupevised anal-ysis of an unlabeled large-scale corpus (Sahlgren,2006).
Similarly to the approaches demonstratedin SpRL-2012, the proposed approach first classi-fies spatial and motion indicators, then, using theseoutcomes further spatial roles are determined.
Forclassifying indicators, the classifier makes use oflexical and grammatical features like lemmas, part-of-speech tags and lexical context representations.The remaining spatial roles are estimated by anotherclassifier additionally employing the lemma of theindicator, distance and relative position to the indi-cator, and the number of tokens composing the indi-cator as features.In Task B, all roles found in a sentence for Task Aare combined to generate candidate relations, whichare verified by a Support Vector Machine (SVM)classifier.
As the entire sentence is informativeto determine the proper conjunction of all roles, aSmoothed Partial Tree Kernel (SPTK) within theclassifier that enhances both syntactic and lexical in-formation of the examples was applied (Croce et al2011).
This is a convolution kernel that measures thesimilarity between syntactic structures, which arepartially similar and whose nodes can be different,but are, nevertheless, semantically related.
Each ex-ample is represented as a tree-structure which is di-rectly derived from the sentence dependency parse,and thus allows for avoiding manual feature engi-neering as in contrast to the work of Roberts andHarabagiu (2012).
In the end, the similarity scorebetween lexical nodes is measured by the WordSpace model.UNITOR submitted two runs for the IAPR TC-12 Image benchmark corpus (we refer to themas to UNITOR.Run1.1 and UNITOR.Run1.2) andone run for the Confluence Project corpus (UN-ITOR.Run2.1), based on the models individuallytrained on the different corpora.
The differencebetween UNITOR.Run1.1 and UNITOR.Run1.2 isthat for UNITOR.Run1.1 the results are obtained forall spatial roles (also the ones that have no spatialrelation), and UNITOR.Run1.2 only provided theroles for which also spatial relations were identified.The results are presented in Table 2.260Although, not directly comparable to the results inSpRL-2012, one may observe some common trends.First, similarly to the previous findings, the perfor-mance for recognition of landmarks and spatial in-dicators (Task A) on the IAPR TC-12 Image bench-mark corpus is better than trajectors (F1-scores of0.785, 0.926 and 0.682 respectively), and spatial in-dicators is the ?easiest?
spatial role to recognize (F1-score of 0.926).In contrast, spatial role labeling on the ConfluenceProject corpus performs worse than on the IAPRTC-12 Image benchmark corpus (with F1-scores of0.406, 0.538 and 0.554 for trajectors, spatial indica-tors and landmarks respectively).
Interestingly, theperformance for landmarks is generally higher thanfor trajectors, which is in line with previous findingsin SpRL-2012.
The performance drop on the newcorpus can be attributed to more complex text anddescriptions, whereas multiple roles can be identi-fied for the same span (for example, a path whichspans over trajectors, landmarks and spatial indica-tors).
For the new spatial roles of motion indicators,paths, directions and distances, the performance lev-els are overall higher than for trajectors with an ex-ception of directions.
Yet, the precision levels fornew roles is much higher than the recall (0.892 vs.0.294 for motion indicators, 0.775 vs. 0.295 forpaths and 0.946 vs. 0.331 for distances).
Directionsturned out to be the most difficult role to classify(0.312, 0.229 and 0.264 for P , R and F1-score re-spectively).7 ConclusionIn this paper we described an evaluation task on Spa-tial Role Labeling in the context of Semantic Evalu-ations 2013.
The task sets a goal to automaticallyprocess text and identify objects of spatial scenesand relations between them.
Building largely uponthe previous evaluation campaign, SpRL-2012, inSpRL-2013 we introduced additional spatial rolesand relations for capturing motions in text.
In ad-dition, a new annotated corpus for spatial roles (in-cluding annotated motions) was produced and re-leased to the participants.
It comprises a set of 117files with about 40,000 tokens in total.With the registered number of 10 participants andthe final number of submissions (only one) we canconclude that spatial role labeling is an interestingtask within the research community, however some-times underestimated in its complexity.
Our furthersteps in promoting spatial role labeling will be a de-tailed description of the annotation scheme and an-notation guidelines, analysis of the corpora and ob-tained results.AcknowledgmentsThe presented research was supporter by the PARISproject (IWT - SBO 110067), TERENCE (EU FP7?257410) and MUSE (EU FP7?296703).ReferencesEmanuele Bastianelli, Danilo Croce, Roberto Basili, andDaniele Nardi.
2013.
UNITOR-HMM-TK: Struc-tured Kernel-based learning for Spatial Role Labeling.In Proceedings of the Seventh International Workshopon Semantic Evaluation (SemEval 2013).
Associationfor Computational Linguistics.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured Lexical Similarity via ConvolutionKernels on Dependency Trees.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1034?1046.
Association forComputational Linguistics.Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-tianelli.
2012.
Structured Learning for Semantic RoleLabeling.
Intelligenza Artificiale, 6(2):163?176.Michael Grubinger, Paul Clough, Henning Mu?ller, andThomas Deselaers.
2006.
The IAPR TC-12 Bench-mark: A New Evaluation Resource for Visual Informa-tion Systems.
In International Workshop OntoImage,pages 13?23.Parisa Kordjamshidi, Marie-Francine Moens, and Mar-tijn van Otterlo.
2010.
Spatial Role Labeling: TaskDefinition and Annotation Scheme.
In Proceedingsof the Seventh Conference on International LanguageResources and Evaluation (LREC?10), pages 413?420.Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-Francine Moens.
2011.
Spatial Role Labeling: To-wards Extraction of Spatial Relations from NaturalLanguage.
ACM Transactions on Speech and Lan-guage Processing (TSLP), 8(3):4.Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens.
2012a.
Semeval-2012 Task 3: Spa-tial Role Labeling.
In Proceedings of the Sixth In-ternational Workshop on Semantic Evaluation, pages365?373.
Association for Computational Linguistics.Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-terlo, Marie-Francine Moens, and Luc De Raedt.2612012b.
Relational Learning for Spatial Relation Ex-traction from Natural Language.
In Inductive LogicProgramming, pages 204?220.
Springer.Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-man, Rob Quimby, Justin Richer, Ben Wellner, ScottMardis, and Seamus Clancy.
2010.
SpatialML: Anno-tation Scheme, Resources, and Evaluation.
LanguageResources and Evaluation, 44(3):263?280.James Pustejovsky and Jessica L Moszkowicz.
2011.The Qualitative Spatial Dynamics of Motion in Lan-guage.
Spatial Cognition & Computation, 11(1):15?44.Kirk Roberts and Sanda M Harabagiu.
2012.
UTD-SpRL: A Joint Approach to Spatial Role Labeling.
InProceedings of the Sixth International Workshop onSemantic Evaluation, pages 419?424.
Association forComputational Linguistics.Magnus Sahlgren.
2006.
The Word-space Model.
Ph.D.thesis, Stockholm University.Oliviero Stock.
1998.
Spatial and Temporal Reasoning.Springer-Verlag New York Incorporated.Amber Stubbs.
2011.
MAE and MAI: Lightweight An-notation and Adjudication Tools.
In Proceedings ofthe 5th Linguistic Annotation Workshop, LAW V ?11,pages 129?133, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, Yasemin Altun, and Yoram Singer.
2006.
LargeMargin Methods for Structured and InterdependentOutput Variables.
Journal of Machine Learning Re-search, 6(2):1453.262
