Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 48?55,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPA Parse-and-Trim Approach with Information Significancefor Chinese Sentence CompressionWei Xu           Ralph GrishmanComputer Science DepartmentNew York UniversityNew York, NY, 10003, USA{xuwei,grishman}@cs.nyu.eduAbstractIn this paper, we propose an event-based ap-proach for Chinese sentence compressionwithout using any training corpus.
We en-hance the linguistically-motivated heuristicsby exploiting event word significance andevent information density.
This is shown toimprove the preservation of important infor-mation and the tolerance of POS and parsingerrors, which are more common in Chinesethan English.
The heuristics are only requiredto determine possibly removable constituentsinstead of selecting specific constituents forremoval, and thus are easier to develop andport to other languages and domains.
The ex-perimental results show that around 72% ofour automatic compressions are grammaticallyand semantically correct, preserving around69% of the most important information on av-erage.1 IntroductionThe goal of sentence compression is to shortensentences while preserving their grammaticalityand important information.
It has recently at-tracted much attention because of its wide rangeof applications, especially in summarization(Jing, 2000) and headline generation (which canbe viewed as summarization with very shortlength requirement).
Sentence compression canimprove extractive summarization in coherenceand amount of information expressed within afixed length.An ideal sentence compression will includecomplex paraphrasing operations, such as worddeletion, substitution, insertion, and reordering.In this paper, we focus on the simpler instantia-tion of sentence simplification, namely word de-letion, which has been proved a success in theliterature (Knight and Marcu, 2002; Dorr et al2003; Clarke and Lapata, 2006).In this paper, we present our technique forChinese sentence compression without the needfor a sentence/compression parallel corpus.
Wecombine linguistically-motivated heuristics andword significance scoring together to trim theparse tree, and rank candidate compressions ac-cording to event information density.
In contrastto probabilistic methods, the heuristics are morelikely to produce grammatical and fluent com-pressed sentences.
We reduce the difficulty andlinguistic skills required for composing heuristicsby only requiring these heuristics to identify pos-sibly removable constituents instead of selectingspecific constituents for removal.
The word sig-nificance helps to preserve informative constitu-ents and overcome some POS and parsing errors.In particular, we seek to assess the event infor-mation during the compression process, accord-ing to the previous successes in event-basedsummarization (Li et al 2006) and a new event-oriented 5W summarization task (Parton et al2009).The next section presents previous approachesto sentence compression.
In section 3, we de-scribe our system with three modules, viz.
lin-guistically-motivated heuristics, word signific-ance scoring and candidate compression selec-tion.
We also develop a heuristics-only approachfor comparison.
In section 4, we evaluate thecompressions in terms of grammaticality, infor-48mativeness and compression rate.
Finally, Sec-tion 5 concludes this paper and discusses direc-tions of future work.2 Previous WorkMost previous studies relied on a parallel cor-pus to learn the correspondences between origi-nal and compressed sentences.
Typically sen-tences are represented by features derived fromparsing results, and used to learn the transforma-tion rules or estimate the parameters in the scorefunction of a possible compression.
A variety ofmodels have been developed, including but notlimited to the noisy-channel model (Knight andMarcu, 2002; Galley and McKeown, 2007), thedecision-tree model (Knight and Marcu, 2002),support vector machines (Nguyen et al 2004)and large-margin learning (McDonald, 2006;Cohn and Lapata 2007).Approaches which do not employ parallel cor-pora are less popular, even though the parallelsentence/compression corpora are not as easy toobtain as multilingual corpora for machine trans-lation.
Only a few studies have been done requir-ing no or minimal training corpora (Dorr et al2003; Hori and Furui, 2004; Turner and Char-niak, 2005).
The scarcity of parallel corpora alsoconstrains the development in languages otherthan English.
To the best of our knowledge, nostudy has been done on Chinese sentence com-pression.An algorithm making limited use of trainingcorpora was proposed originally by Hori and Fu-rui (2004) for spoken text in Japanese, and latermodified by Clarke and Lapata (2006) for Eng-lish text.
Their model searches for the compres-sion with highest score according to the signific-ance of each word, the existence of Subject-Verb-Object structures and the language modelprobability of the resulting word combination.The weight factors to balance the three mea-surements are experimentally optimized by aparallel corpus or estimated by experience.Turner and Charniak (2005) present semi-supervised and unsupervised variants of the noi-sy channel model.
They approximate the rules ofcompression from a non-parallel corpus (e.g.
thePenn Treebank) based on probabilistic contextfree grammar derivation.Our approach is most similar to the HedgeTrimmer for English headline generation (Dorr etal, 2003), in which linguistically-motivated heu-ristics are used to trim the parse tree.
This me-thod removes low content components in a presetorder until the desired length requirement isreached.
It reduces the risk of deleting subordi-nate clauses and prepositional phrases by delay-ing these operations until no other rules can beapplied.
This fixed order of applying rules limitsthe flexibility and capability for preserving in-formative constituents during deletions.
It is like-ly to fail by producing a grammatical but seman-tically useless compressed sentence.
Anothermajor drawback is that it requires considerablelinguistic skill to produce proper rules in a properorder.3 Algorithms for Sentence CompressionOur system takes the output of a Chinese Tree-bank-style syntactic parser (Huang and Harper,2009) as input and performs tree trimming opera-tions to obtain compression.
We propose andcompare two approaches.
One uses only linguis-tically-motivated heuristics to delete words andgets the compression result directly.
The otherone uses heuristics to determine which nodes inthe parse tree are potentially removable.
Then allremovable nodes are deleted one by one accord-ing to their significance weights to generate aseries of candidate compressions.
Finally, thebest compression is selected based on sentencelength and informativeness criteria.3.1 Linguistically-motivated HeuristicsThis module aims to identify the nodes in theparse tree which may be removed without severeloss in grammaticality and information.
Based onan analysis of the Penn Treebank corpus andhuman-produced compression, we decided thatthe following parse constituents are potential lowcontent units.Set 0 ?
basic:?
Parenthetical elements?
Adverbs except negative, some temporaland degree adverbs?
Adjectives except when the modified nounconsists of only one character?
DNPs (which are formed by variousphrasal categories plus ???
and appear asmodifiers of NP in Chinese)?
DVPs (which are formed by variousphrasal categories plus ???
in Chinese,and appear as modifiers of VP in Chinese)?
All nodes in noun coordination phrasesexcept the first noun49Set 1 ?
fixed:?
All children of NP nodes except temporalnouns and proper nouns and the last nounword?
All simple clauses (IP) except the first one,if the sentence consists of more than oneIP?
Prepositional phrases except those thatmay contain location or date information,according to a hand-made list of preposi-tionsSet 2 ?
flexible:?
All nodes in verb coordination phrases ex-cept the first one.?
Relative clauses?
Appositive clauses?
All prepositional phrases?
All children of NP nodes except the lastnoun word?
All simple clauses, if the sentence consistsof more than one IP (at least one clause isrequired to be preserved in later trimming)Set 0 lists all the fundamental constituents thatmay be removed and is used in both approaches.Set 1 and Set 2 are designed to handle morecomplex constituents for the two approaches re-spectively.The heuristics-only approach exploits Set 0and Set 1.
It can be viewed as the Chinese ver-sion of Hedge Trimmer (Dorr et al 2003), butdiffers in the following ways:1) Chinese has different language construc-tions and grammar from English.2) We eliminate the strict compressionlength constraint in order to yield morenatural compressions with varying length.3) We do not remove time expressions onpurpose to benefit further applications,such as event extraction.The heuristics-only approach deletes low con-tent units mechanically while preserving syntac-tic correctness, as long as parsing is accurate.Our preliminary experiments showed that theheuristics in Set 0 and Set 1 can generate a com-paratively satisfying compression, but is sensi-tive to part-of-speech and parsing errors, e.g.
theproper noun ???
(Hyundai)?
as motor compa-ny is tagged as an adjective (shown in Figure 1)and thus removed since its literal meaning is ???(modern)?.
Moreover, the rules in Set 1 reducethe sentence length in a gross manner, riskingserious information or grammaticality loss.
Forexample, the first clause may not be a completegrammatical sentence, and is not always the mostimportant clause in the sentence though that isusually the case.
We also want to point out thatthe heuristics tend to reduce the sentence lengthand preserve the grammar by removing most ofthe modifiers, even though modifiers may con-tain a lot of important information.To address the above problems of heuristics,we exploit word significance to measure the im-portance of each constituent.
Set 2 was created towork with Set 0 to identify removable low con-tent units.
The heuristics in this approach areused only to detect all possible candidates fordeletion and thus are more general and easier tocreate than Set 1.
For instance, we do not need tocarefully determine which kinds of prepositionalphrases are safe or dangerous to delete but in-stead mark all of them as potentially removable.The actual word deletion is performed later bya compression generation and selection module,taking word significance and compression rateinto consideration.
The heuristics in Set 2 areable to cover more risky constituents than Set 1,e.g.
clauses and parallel structures, since the riskwill be controlled by the later processes.
( (IP(NP(*NP (NR ??))
South Korean(#*ADJP (JJ ??))
Hyundai(NP(#*NN ??)
motor(NN ??)))
company(VP (VC ?)
is(NP (#*DNP (NP (NR ???))
Volvo(DEG ?))
?s(#*ADJP (JJ ??))
potential(NP (NN ??))))
buyer(PU .
)))Figure 1.
Parse tree trimming by heuristics(#: nodes trimmed out by Set0 & Set1;*: nodes labeled as removable by Set0 & Set2.
)Figure 1 shows an example of applying heuris-tics to the parse tree of the sentence ???????????????????
(The SouthKorean Hyundai Motor Company is a potentialbuyer of Volvo.).
The heuristics-only approachproduces ?????????
(The South Koreancompany is a buyer.
), which is grammatical butsemantically meaningless.
We will see how wordsignificance and information density scoringproduce a better compression in section 3.3.503.2 Event-based Word SignificanceBased on our observations, a human-compressedsentence primarily describes an event or a set ofrelevant events and contains a large proportion ofnamed entities, especially in the news articledomain.
Similar to event-based summarization(Li et al 2006), we consider only the eventterms, namely verbs and nouns, with a prefe-rence for proper nouns.The word significance score Ij(wi) indicateshow important a word wi is to a document j.
It isa tf-idf weighting scheme with additional weightfor proper nouns:????????
?otherwisenounproperiswifidftfnouncommonorverbiswifidftfwI iiijiiijij,0,,)( ?
(1)wherewi : a word in the sentence of document jtfij :  term frequency of wi in document jidfi : inverse document frequency of wi?
: additional weight for proper noun.The nodes in the parse tree are then weightedby the word significance for leaves or the sum ofthe children?s weights for internal nodes.
Theweighting depends on the word itself regardlessof its part-of-speech tags in order to overcomesome part-of-speech errors.3.3 Compression Generation and SelectionIn this module, we first apply a greedy algorithmto trim the weighted parse tree to obtain a seriesof candidate compressions.
Recall that the heu-ristics Set 0 and 2 have provided the removabili-ty judgment for each node in the tree.
The parsetree trimming algorithm is as follows:1) remove one node with the lowest weightand get a candidate compressed sentence2) update  the weights of all ancestors ofthe removed node3) repeat until no node is removableThe selection among candidate compressions is atradeoff between sentence length and amount ofinformation.
Inspired by headlines in news ar-ticles, most of which contain a large proportionof named entities, we create an information den-sity measurement D(sk) for sentence sk to selectthe best compression:)()()(kPwik sLwIsD i???
(2)whereP : the set of words whose significance scoresare larger  than ?
in (1)I(wi) : the significance score of word wiL(sk) : the length of sentence in charactersTable 1 shows the effectiveness of informationdensity to select a proper compression with abalance between length and meaningfulness.
Ta-ble 1 lists all candidate compressions in sequencegenerated from the parse tree in Figure 1.
Thewords in bold are considered in information den-sity.
The underlined compression is picked asfinal output as ???????????????
(The South Korean Hyundai company is abuyer of Volvo.
), which makes more sense thanthe one produced by heuristics-only approach as?????????
(The South Korean companyis a buyer.).
In our approach, ???
(Hyundai)?tagged as adjective and ?????(Volvo?s)?
asa modifier to buyer are preserved successfully.D(s) Sentence0.254 ????????????????
?.The South Korean Hyundai Motor Companyis a potential buyer of Volvo.0.288 ??????????????
?.The South Korean Hyundai Motor Company isa buyer of Volvo.0.332 ????????????
?.The South Korean Hyundai Company is a buy-er of Volvo.0.282 ??????????
?.The South Korean company is a buyer of Vol-vo.0.209 ????????
?.The company is a buyer of Volvo.0.0 ????
?.The company is a buyer.Table 1.
Compression generation and selectionfor the sentence in Figure 1The compression with highest informationdensity is chosen as system output.
To achieve abetter compression rate and avoids overly con-densed sentences (i.e.
very short sentences withonly a proper noun), we further constrain thecompression to a limited but varying lengthrange [min_length, max_length] according to thelength of the original sentence:???
?????otherwiselengthoriginallengthoriginaliflengthorigmax_lengthlengthoriginalmin_length,__,_},_min{????
(3)51whereorig_length : the length of original sentence incharacters?,?
: fixed lengths in charactersIn contrast to a fixed limitation of length, thisvarying length simulates human behavior increating compression and avoid the overcom-pression caused by the density selection schema.4 Experiments4.1 Experiment SetupOur experiments were designed to evaluate thequality of automatic compression.
The evaluationcorpus is 79 documents from Chinese newswires,and the first sentence of each news article iscompressed.The compression of the first sentences in theChinese news articles is a comparatively chal-lenging task.
Unlike English, Chinese often con-nects two or more self-complete sentences to-gether without any indicating word or punctua-tion; this is extremely frequent for the first sen-tence of news text.
The average length of the firstsentences in the 79 documents is 61.5 characters,compared to 46.8 characters for the sentences inthe body of these news articles.We compare the compressions generated byfour different methods:?
Human [H]: A native Chinese speaker isasked to generate a headline-like compres-sion (must be a complete sentence, not afragment, and need not preserve originalSVO structure) based on the first sentenceof each news article.
Only word deletionoperations are allowed.?
Heuristics [R]: The heuristics-only ap-proach mentioned in section 2.1.?
Heuristics + Word Significance [W]: Theapproach combines heuristics and wordsignificance.
The parameter ?
in (1) is setto be 1, which is an upper bound of word?stf-idf value throughout the corpus.?
Heuristics + Word Significance + LengthConstraints [L]: Compression is con-strained to a limited but varying length, asmentioned in section 2.3.
The length pa-rameters ?
and ?
in (3) are set roughly tobe 10 and 20 characters based on our ex-perience.4.2 Human EvaluationSentence compression is commonly evaluatedby human judgment.
Following the literature(Knight and Marcu, 2002; Dorr et al 2003;Clarke and Lapata, 2006; Cohn and Lapata2007), we asked three native Chinese speakers torate the grammaticality of compressions usingthe 1 to 5 scale.
We find that all three non-linguist human judges tend to take semantic cor-rectness into consideration when scoring gram-maticality.We also asked these three judges to give a listof keywords from the original sentence beforeseeing compressions, which they would preserveif asked to create a headline based on the sen-tence.
Instead of a subjective score, the informa-tiveness is evaluated by measuring the keywordcoverage of the target compression on a percen-tage scale.
The three judges give different num-bers of keywords varying from 3.33 to 6.51 onaverage over the 79 sentences.The compression rate is the ratio of the num-ber of Chinese characters in a compressed sen-tence to that in its original sentence.The experimental results in Table 2 show thatour automatically generated compressions pre-serve grammaticality, with an average score ofabout 4 out of 5, because of the use of linguisti-cally-motivated heuristics.Compres-sion RateGrammat-icality(1 ~ 5)Informa-tiveness(0~100%)Human 38.5% 4.962 90.7%Heuristics 54.1% 4.114 64.9%Heu+Sig 52.8% 3.854 68.8%Heu+Sig+L 34.3% 3.664 56.1%Table 2.
Mean rating from human evaluation onfirst sentence compressionEvent-based word significance and informa-tion density increase the amount of importantinformation by 6% with similar sentence length,but decreases the average grammaticality scoreby 6.5%.
This is because the method using wordsignificance sacrifices grammaticality to reducethe linguistic complexity of the heuristics.
None-theless, this method does improve grammaticali-ty for 16 of the 79 compressed sentences, typi-cally for those with POS or parsing errors.The compression rates of the two basic auto-matic approaches are around 53%, while it is38.5% for manual compression.
This is partiallybecause our heuristics only trim the parse tree52but do not transform the structure of it, while ahuman may change the grammatical structure,remove more linking words and even abbreviatesome words.
The length constraint boosts thecompression rate of our combined approach by35% with a loss of 18.5% in informativeness and5% in grammaticality.Grammaticali-ty(1 ~ 5)Number ofSentenceCompres-sion RateInforma-tiveness(0~100%)Heuristics > 4.5 45 64.1% 75.9%Heuristics >= 4 62 54.5% 70.6%Heu+Sig  > 4.5 35 59.8% 81.8%Heu+Sig  >= 4 57 56.7% 75.8%Table 3.
Compressions with good grammarWe further investigate the performance of ourautomatic system by considering only relativelygrammatical compressions, as shown in Table 3.The compressions which receive an averagescore of more than 4.5 are comparatively reada-ble.
The combined approach generates 35 suchcompressions among a total of 79 sentences, pre-serving 81.8% important information on average,which is quite satisfying since human-generatedcompression only achieves 90.7%.The infomativeness score of human-generatedcompression also demonstrates the difficulty ofthis task.
We compare our automatically generat-ed event words list with the keywords picked byhuman judges.
61.8% of human-selected key-words are included in the event words list, thusconsidered when calculating information signi-ficance.
This fact demonstrates some success butalso potential room for improving keyword se-lection.4.3 Some ExamplesWe illustrate several representative samples ofour system output in Table 4.
In the first example,all three automatic compressions are acceptable,though different in preserving important infor-mation.
[W] and [L] concisely contain the WHO,WHAT, WHOM information of the event, while[R] further preserves the WHY and WHEN in-formation.In the second example, the heuristics-only ap-proach produced a decent compression by keep-ing only the first self-complete sub-sentence.
Theweight of word ???
(White House)?
is some-what overwhelming and resulted in dense com-pressions in [W] and [L], which are too short tobe good.
Besides, [W] and [L] in this exampleshow that not all the prepositional phrases, nounmodifiers etc.
can be removed in Chinese with-out affecting grammaticality, though in mostcases the removals are safe.
This is one of themain reasons for grammar errors in the compres-sion results except POS and parsing errors.The third example shows how the combinedapproach overcomes POS errors and how lengthconstraints avoid overcompression.
In [R], ????(Nadal)?
is deleted because it is mistakenlytagged as an adverb modifying the action ?claimthe victory and progress through?.
Since Nadal istagged as proper noun somewhere else in thedocument, its significance makes it survive thecompression process.
[L] produces a perfectcompression with proper length, information andgrammar, just as human-made compression.
[W]selects a very condensed version of compressionbut loses some information.1.
[O] ?????????????,?????????????????????
?.Because both sides were immovable on the drawing of maritimeborders, a three-day high-level military meeting between Northand South Korea broke up in discord today.[H]?????????????
?.A high-level military meeting between two Koreas broke up indiscord today.[R]??????,?????????????????
?.Because both sides were immovable, a three-day high-levelmeeting between two Koreas broke up in discord today.[L]?????????
?.A high-level meeting between two Koreas broke up in discord.[W]?????????
?.A high-level meeting between two Koreas broke up in discord.2.[O]??????????????,?????????????;????????????????????????,?????
?.The White House today called for nuclear inspectors to be sentas soon as possible to monitor North Korea?s closure of its nuc-lear reactors.
The White House made this call after US PresidentBush had telephone conversations with South Korean PresidentRoh Moo-hyun.
[H] ???????????????????
?.The White House today called for inspectors to be sent to moni-tor North Korea?s closure of its nuclear reactors.[R]??????????,?????????
?.The White House today called for inspectors to be sent to moni-tor North Korea?s closure of its reactors.[L]?????????
?, ??
?, ???
?.The White House today called for inspectors to be sent.
TheWhite House is,  made this call.[W]???,???
?.The White House is, made this call.3.[O]??????????,??????,???,????????????????
?.Fourth seed Djokovic withdrew from the game, and allowedsecond seed Nadal , who was leading 3-6 , 6-1 , 4-1 , to claim thevictory and progress through.[H]?????????????
?.Djokovic withdrew from the game, and allowed Nadal to claimthe victory and progress through.53[R]??????,?????,???,?????????????
?.Djokovic withdrew from the game, and allowed second seed,who was leading 3-6 , 6-1 , 4-1 , to claim the victory andprogress through.[L]???????????????
?.Djokovic withdrew from the game, and allowed seed Nadal toclaim the victory and progress through.[W]?????
?.Djokovic withdrew from the game.4.[O]???
7 ?
31 ?????
30 ????????????????????
?.Chinanews.com , July 31 On the 30th Chen Shui-bian questionedthat members of the judiciary on the island may have tried to getinvolved in elections for leaders in the Taiwan region.[H]??????????????????
?.Chen Shui-bian questioned that members of the judiciary mayget involved in elections for leaders in the Taiwan region.[R]???
7 ?
31 ?????
30 ??????????????????
?.Chinanews.com , July 31 On the 30th Chen Shui-bian questionedthat members on the island may have tried to get involved inelections for leaders in the Taiwan region.[L]???
30????????????????
?.On the 30th Chen Shui-bian questioned that members may havetried to get involved in elections for leaders in the Taiwan re-gion.[W]???
30 ????????????????
?.On the 30th Chen Shui-bian questioned that members may havetried to get involved in elections for leaders in the Taiwan re-gion.5.[O]??????????????????,?????,???????????????
?.Patil is India?s first woman presidential candidate, if she iselected, she will become India?s first woman president in history.
[H] ?????????????????
?.Patil is India?s first woman presidential candidate.[R]?????????????
?.Patil is the first candidate in the history of India.[L]???????,????????????
?.Patil is the candidate, she will become president of Indian histo-ry.[W]??????
?.Patil is the candidate.Table 4.
Compression examples including humanand system results, with reference translation(O: Original sentence)The fourth sample indicates an interesting lin-guistic phenomenon.
The head of the nounphrase ???????
(members of the judiciaryon the island)?, ???(members)?
cannot standalone making a fluent and valid sentence, thoughall the compressions are grammatically correct.Our human assessors also show a preference of[R] to [L, W] in grammaticality evaluation, tak-ing semantic correctness into consideration aswell.
This is probably a reason that our combinedapproach performs worse than heuristic-only ap-proach in grammaticality.
The combined ap-proach tends to remove risky constituents, but itis hard for word significance to control this riskproperly in every case.
This is another of themain reasons for bad compression.In the fifth sample, all the automatic compres-sions are grammatically correct preserving wellthe heads of subject and object, but are semanti-cally incorrect.
This case should be hard to han-dle by any compression approach.5 Conclusions and Future WorkIn this paper, we propose a novel approach tocombine linguistically-motivated heuristics andword significance scoring for Chinese sentencecompression.
We take advantage of heuristics topreserve grammaticality and not rely on a paral-lel corpus.
We reduce the complexity involved inpreparing complicated deterministic rules forconstituent deletion, requiring people only todetermine potentially removable constituents.Therefore, this approach can be easily extendedto languages or domains for which parallel com-pression corpora are scarce.
The word signific-ance scoring is used to control the word deletionprocess, pursuing a balance between sentencelength and information loss.
The exploitation ofevent information improves the mechanical rule-based approach in preserving event-relatedwords and overcomes some POS and parsingerrors.The experimental results prove that this com-bined approach is competitive with a finely-tuned heuristics-only approach to grammaticality,and includes more important information in thecompressions of the same length.In the future, we plan to apply the compres-sion to Chinese summarization and headline gen-eration tasks.
A careful study on keyword selec-tion and word weighting may further improve theperformance of the current system.
We also con-sider incorporating language models to producefluent and natural compression and reduce se-mantically invalid cases.Another important future direction lies increating a parallel compression corpus in Chi-nese and exploiting statistical and machine learn-ing techniques.
We also expect that an abstrac-tive approach involving paraphrasing operationsbesides word deletion will create more naturalcompression than an extractive approach.AcknowledgmentsThis work was supported in part by the DefenseAdvanced Research Projects Agency (DARPA)under Contract HR0011-06-C-0023.
Any opi-nions, findings, conclusions, or recommenda-54tions expressed in this material are the authors'and do not necessarily reflect those of the U.S.Government.ReferencesJ.
Clarke and M. Lapata, 2006.
Models for SentenceCompression: A Comparison across Domains,Training Requirements and Evaluation Measures.In Proceedings of the COLING/ACL 2006, Syd-ney, Australia, pp.
377-384.T.
Cohn and M. Lapata.
2007.
Large Margin Syn-chronous generation and its application to sentencecompression.
In the Proceedings of the EMNLP/CoNLL 2007, Pragure, Czech Republic, pp.
73-82.B.
Dorr, D. Zajic and R. Schwartz.
2003.
HedgeTrimmer: A Parse-and-Trim Approach to HeadlineGeneration.
In the Proceedings of theNAACL/HLT text summarization workshop, Ed-monton, Canada, pp.
1-8.M.
Galley and K. McKeown, 2007.
Lexicalized Mar-kov Grammars for Sentence Compression.
In theProceedings of NAACL/HLT 2007, Rochester,NY, pp.
180-187.C.
Hori and S. Furui.
2004.
Speech Summarization:An Approach through Word Extraction and a Me-thod for Evaluation.
IEICE Transactions on Infor-mation and Systems, E87-D(1): 15-25.Z.
Huang and M. Harper, 2009.
Self-training PCFGGrammars with Latent Annotations Across Lan-guages.
In the proceedings of EMNLP 2009, Sin-gapore.H.
Jing.
2000.
Sentence Reduction for AutomaticText Summarization.
In Proceedings of the 6thANLP, Seattle, WA, pp.
310-315.K.
Knight and D. Marcu, 2002.
Summarizationbeyond Sentence Extraction: a Probabilistic Ap-proach to Sentence Compression.
Artificial Intelli-gence, 139(1): 91-107.W.
Li, W. Xu, M. Wu, C. Yuan and Q. Lu.
2006.
Ex-tractive Summarization using Inter- and Intra-Event Relevance.
In the Proceedings of COL-ING/ACL 2006, Sydney, Australia, pp 369-376.R.
McDonald.
2006.
Discriminative Sentence Com-pression with Soft Syntactic Constraints.
In theProceedings of 11th EACL, Trento, Italy, pp.
297-304.M.
L. Nguyen, A. Shimazu, S. Horiguchi, T. B. Hoand M. Fukushi.
2004.
Probabilistic Sentence Re-duction using Support Vector Machines.
In Pro-ceedings of the 20th COLING, Geneva, Switzer-land, pp.
743-749.K.
McKeown, R. Barzilay, S. Blair-Goldensohn, D.Evans, V. Hatzivassiloglou, J. Klavans, A. Nenko-va, B. Schiffman and S. Sigelman.
2002.
The Co-lumbia Multi-Document Summarizer for DUC2002.
In the Proceedings of the ACL workshop onDocument Understanding Conference (DUC)workshop, Philadelphia, PA, pp.
1-8.K.
Parton, K. McKeown, R. Coyne, M. Diab, R.Grishman, D. Hakkani-T?r, M. Harper, H. Ji, W.Ma, A. Meyers, S. Stolbach, A.
Sun, G. Tur, W.Xu and S. Yaman.
2009. Who, What, When,Where, Why?
Comparing Multiple Approaches tothe Cross-Lingual 5W Task.
In the Proceedings ofACL-IJCNLP, Singapore.J.
Turner and E. Charniak.
2005.
Supervised and Un-supervised Learning for Sentence Compression.
Inthe Proceedings of 43rd ACL, Ann Arbor, MI, pp.290-297.55
