A Rhetorical Status Classifier for Legal Text SummarisationBen Hachey and Claire GroverSchool of InformaticsUniversity of Edinburgh{bhachey,grover}@inf.ed.ac.ukAbstractWe describe a classifier which determines therhetorical status of sentences in texts from a corpusof judgments of the UK House of Lords.
Our sum-marisation system is based on the work of Teufeland Moens where sentences are classified for rhetor-ical status to aid sentence selection.
We experi-ment with a variety of linguistic features with resultscomparable to Teufel and Moens, thereby demon-strating the feasibility of porting this kind of systemto a new domain.1 IntroductionLaw reports form an interesting domain for auto-matic summarisation.
They are texts which recordthe proceedings of a court and, due to the role thatprecedents play in English law, easy access to themis essential for a wide range of people.
For this rea-son, they are frequently manually summarised bylegal experts, with summaries varying according totarget audience (e.g.
students, solicitors).In the SUM project, we are exploring methods forgenerating flexible summaries of legal documents,taking as our point of departure the Teufel andMoens (2002; 1999a; 1999b) approach to automaticsummarisation (henceforth T&M).
We have chosento work with law reports for three main reasons: (a)the existence of manual summaries means that wehave evaluation material for the final summarisationsystem; (b) the existence of differing target audi-ences allows us to explore the issue of tailored sum-maries; and (c) the texts have much in common withthe academic papers that T&M worked with, whileremaining challengingly different in many respects.Our general aims are comparable with those of theSALOMON project (Moens et al, 1997), which alsodeals with summarisation of legal texts, but ourchoice of methodology is designed to test the porta-bility of the T&M approach to a new domain.The T&M approach is an instance of what Spa?rckJones (1999) terms text extraction where a sum-mary typically consists of sentences selected fromthe source text, with some smoothing to increasethe coherence between the sentences.
Since the aca-demic texts they use are rather long and the aimis to produce flexible summaries of varying lengthand for various audiences, T&M go beyond sim-ple sentence selection and classify source sentencesaccording to their rhetorical status (e.g.
a descrip-tion of the main result, a criticism of someone else?swork, etc.).
With sentences classified in this man-ner, different kinds of summaries can be generated.Sentences can be reordered, since they have rhetor-ical roles associated with them, or they can be sup-pressed if a user is not interested in certain types ofrhetorical roles.In the second stage of our project we will exploretechniques for sentence selection.
Following theT&M methodology, we will annotate sentences inthe corpus for ?relevance?.
For our corpus we hopeto be able to compute relevance by using automatictechniques to pair up sentences from manually cre-ated abstracts with sentences in the source text.
Theaddition of this layer of annotation will provide thetraining and testing material for sentence extraction,with the rhetorical role labels helping to constrainthe type of summary generated.In this paper we focus on our rhetorical statusclassifier.
This is a key part of the summarisationprocess and our work can be thought of as a test ofportability of the T&M approach to a new domain.At the same time, our methods differ in importantrespects from those of T&M and in reporting ourwork we will attempt to draw comparisons wher-ever possible.In Section 2 we describe the House of Lords cor-pus we have gathered and annotated.
We explainthe rhetorical role annotation scheme that we havedeveloped and contrast it with the T&M schemefor academic articles.
We provide inter-annotationagreement results for the annotation scheme.
InSection 2.3 we give an overview of the tools andtechniques we have used in the automatic linguis-tic processing of the judgments.
Section 3 describesour sentence classifier.
In Section 3.1 we review thekinds of features that can be used by a classifier anddescribe the set of features used in our experiments.In Section 3.2 we present the results of experimentswith four classifiers and discuss the relative effec-tiveness of the methods and the feature sets.
Finally,in Section 4 we draw some conclusions and outlinefuture work.2 The HOLJ Corpus2.1 Corpus OverviewThe texts in our corpus are judgments of the Houseof Lords1, which we refer to as HOLJ.
These textscontain a header providing structured information,followed by a sequence of Law Lord?s judgmentsconsisting of free-running text.
The structured partof the document contains information such as the re-spondent, appellant and the date of the hearing.
Thedecision is given in the opinions of the Law Lords,at least one of which is a substantial speech.
Thisoften starts with a statement of how the case camebefore the court.
Sometimes it will move to a reca-pitulation of the facts, moving on to discuss one ormore points of law, and then offer a ruling.We have gathered a corpus of 188 judgmentsfrom the years 2001?2003 from the House of Lordswebsite.
(For 153 of these, manually created sum-maries are available2 and will be used for systemevaluation).
The raw HTML documents are pro-cessed through a sequence of modules which auto-matically add layers of annotation.
The first stageconverts the HTML to an XML format which we referto as HOLXML.
In HOLXML, a House of Lords Judg-ment is defined as a J element whose BODY elementis composed of a number of LORD elements (usu-ally five).
Each LORD element contains the judg-ment of one individual lord and is composed of asequence of paragraphs (P elements) inherited fromthe original HTML.
The total number of words inthe BODY elements in the corpus is 2,887,037 andthe total number of sentences is 98,645.
The aver-age sentence length is approx.
29 words.
A judg-ment contains an average of 525 sentences while anindividual LORD speech contains an average of 105sentences.All annotation is computed automatically exceptfor manual annotation of sentences for their rhetori-cal status.
The automatic processing is divided intotwo stages, tokenisation, which also includes part-of-speech (POS) tagging and sentence boundary dis-ambiguation, followed by linguistic annotation (de-scribed in detail in Section 2.3 below).
The humanannotation of rhetorical roles is performed on the1http://www.parliament.uk/judicial_work/judicial_work.cfm2http://www.lawreports.co.uk/documents after tokenisation has identified the sen-tences.
This annotation is work in progress and sofar we have 40 manually annotated documents.
Theclassifiers described in this paper have been trainedand evaluated on this manually annotated subset ofthe corpus.Our working subset of the corpus is similar in sizeto the corpus reported in (Teufel and Moens, 2002):the T&M corpus consists of 80 conference articleswhile ours consists of 40 HOLJ documents.
TheT&M corpus contains 12,188 sentences and 285,934words while ours contains 10,169 sentences and290,793 words.
The experimental results reportedin this paper were obtained using 10-fold cross val-idation over the 40 documents.2.2 Rhetorical Status AnnotationThe rhetorical roles that it would be appropriateto assign to sentences3 vary from domain to do-main and reflect the argumentative structure of thetexts.
Teufel and Moens (2002) describe a set oflabels which reflect regularities in the argumenta-tive structure of research articles following fromthe authors?
communicative goals.
The scientificarticle rhetorical roles include labels such as AIM,which is assigned to sentences indicating the goalsof the paper, and BACKGROUND, which is assignedto sentences describing generally accepted scientificbackground.For the legal domain, the communicative goal isslightly different; the author?s primary communica-tive goal is to convince his peers that his position islegally sound, having considered the case with re-gard to all relevant points of law.
We have anal-ysed the structure of typical documents in our do-main and derived from this seven rhetorical rolecategories, as illustrated in Table 1.
The secondcolumn shows the frequency of occurrence of eachlabel in the manually annotated subset of the cor-pus.
Apart from the OTHER category, the most in-frequently assigned category is TEXTUAL while themost frequent is BACKGROUND.
The distributionacross categories is more uniform than that of theT&M labels: Teufel and Moens (2002) report thattheir most frequent category (OWN) is assigned to67% of sentences while three other labels (BASIS,TEXTUAL and AIM) are each assigned to only 2%of sentences.3We take the sentence as the level of processing for rhetor-ical role annotation.
While clause-level annotation might al-low more detailed discourse information, there are consider-ably more clauses in the HOLJ documents than sentences andannotating at the clause level would be significantly more ex-pensive.
Moreover, clause boundary identification is less reli-able than sentence boundary identification.Label Freq.
DescriptionFACT 862 The sentence recounts the events or circumstances which gave rise(8.5%) to legal proceedings.E.g.
On analysis the package was found to contain 152 milligramsof heroin at 100% purity.PROCEEDINGS 2434 The sentence describes legal proceedings taken in the lower courts.
(24%) E.g.
After hearing much evidence, Her Honour Judge Sander, sitting atPlymouth County Court, made findings of fact on 1 November 2000.BACKGROUND 2813 The sentence is a direct quotation or citation of source of law material.
(27.5%) E.g.
Article 5 provides in paragraph 1 that a group of producers mayapply for registration .
.
.FRAMING 2309 The sentence is part of the law lord?s argumentation.
(23%) E.g.
In my opinion, however, the present case cannot be brought withinthe principle applied by the majority in the Wells case.DISPOSAL 935 A sentence which either credits or discredits a claim or previous ruling.
(9%) E.g.
I would allow the appeal and restore the order of the Divisional Court.TEXTUAL 768 A sentence which has to do with the structure of the document or with(7.5%) things unrelated to a case.E.g.
First, I should refer to the facts that have given rise to this litigation.OTHER 48 A sentence which does not fit any of the above categories.
(0.5%) E.g.
Here, as a matter of legal policy, the position seems to me straightforward.Table 1: Rhetorical annotation scheme for legal judgmentsThe 40 judgments in our manually annotated sub-set were annotated by two annotators using guide-lines which were developed by one of the authors,one of the annotators and a law professional.
Elevenfiles were doubly annotated in order to measureinter-annotator agreement.4 We used the kappa co-efficient of agreement as a measure of reliability.This showed that the human annotators distinguishthe seven categories with a reproducibility of K=.83(N=1,955, k=2; where K is the kappa co-efficient,N is the number of sentences and k is the numberof annotators).
This is slightly higher than that re-ported by T&M and above the .80 mark which Krip-pendorf (1980) suggests is the cut-off for good reli-ability.In striving to achieve high quality summarisation,it is tempting to consider using an annotation sys-tem which reflects a more sophisticated analysis ofrhetorical roles.
However, our kappa co-efficient iscurrently on the bottom end of the range suggestedas indicating ?good?
reliability.
Therefore, we sus-pect that the methods we are using may not scale tomore refined distinctions.
The decisions we madewith the annotation scheme reflect a desire to bal-ance quality of annotation against detail.
Also, asmentioned earlier, the cost of annotation for thesecomplex legal documents is not insignificant.4The doubly annotated files were used only for computingkappa.
For the experiments, we trained and tested on the 40annotated files produced by the main annotator.2.3 Linguistic AnalysisOne of the aims of the SUM project is to create anannotated corpus in the legal domain which will beavailable to NLP researchers.
With this aim in mindwe have used the HOLXML format for the corpusand we encode all the results of linguistic process-ing as XML annotations.
Figure 1 shows the broaddetails of the automatic processing that we perform,with the processing divided into an initial tokenisa-tion module and a later linguistic annotation mod-ule.
The architecture of our system is one where arange of NLP tools is used in a modular, pipelinedway to add linguistic knowledge to the XML docu-ment markup.In the tokenisation module we convert from thesource HTML to HOLXML and then pass the datathrough a sequence of calls to a variety of XML-based tools from the LT TTT and LT XML toolsets(Grover et al, 2000; Thompson et al, 1997).
Thecore program in our pipelines is the LT TTT pro-gram fsgmatch, a general purpose transducer whichprocesses an input stream and adds annotations us-ing rules provided in a hand-written grammar file.The other main LT TTT program is ltpos, a statisti-cal combined part-of-speech (POS) tagger and sen-tence boundary disambiguation module (Mikheev,1997).
The first step in the tokenisation modulesuses fsgmatch to segment the contents of the para-graphs into word tokens encoded in the XML as Welements.
Once the word tokens have been iden-tified, the next step uses ltpos to mark up the sen-HOLXMLConversion todocumentHTMLAutomaticallyannotatedHOLXMLdocumentRecognitionNamedEntityIdentificationChunking& ClauseVerb &subjectfeaturessationLemmati?TokenisationPOS Tagging& SentenceIdentificationTOKENISATION MODULELINGUISTIC ANALYSIS MODULEFigure 1: HOLJ processing stagestences as SENT elements and to add part of speechattributes to word tokens.The motivation for the module that performs fur-ther linguistic analysis is to compute information tobe used to provide features for the sentence classi-fier.
However, the information we compute is gen-eral purpose, making the data useful for a range ofNLP research activities.The first step in the linguistic analysis modulelemmatises the inflected words using Minnen et al?s(2000) morpha lemmatiser.
This program is notXML-aware so we use xmlperl (McKelvie, 1999) toprovide a wrapper so that it can be incorporated inthe XML pipeline.
We use a similar mechanism forthe other non-XML components.The next stage, described in Figure 1 as NamedEntity Recognition, is in fact a more complex layer-ing of two kinds of named entity recognition.
Thedocuments in our domain contain the standard kindsof entities familiar from the MUC and CoNLL com-petitions (Chinchor, 1998; Roth and van den Bosch,2002; Daelemans and Osborne, 2003), such as per-son, organisation, location and date.
However, theyalso contain entities which are are specific to the do-main.
Table 2 shows examples of the entities wehave marked up in the corpus (in our annotationscheme these are noun groups (NG) with specifictype and subtype attributes).
In the top two blocksof the table are examples of domain-specific entitiessuch as courts, judges, acts and judgments, while inthe third block we show examples of non-domain-specific entity types.
We use different strategiesfor the identification of the two classes of entities:for the domain-specific ones we use hand-craftedLT TTT rules, while for the non-domain-specificones we use the C&C named entity tagger (Curranand Clark, 2003) trained on the MUC7 data set.
Forsome entities, the two approaches provide compet-ing analyses and in all cases the domain-specific la-bel is to be preferred since it provides finer-grainedinformation.
However, while the rule-based recog-niser can operate incrementally over data which al-ready contains some entity markup, the C&C taggeris trained to operate over unlabelled sentences.
Forthis reason we run the C&C tagger first and encodeits results as attributes on the words.
We then run thedomain-specific tagger, encoding its results as XMLelements enclosing the words, and finish with a sim-ilar encoding of whichever C&C entities can still berealised in the unlabelled subparts of the sentences(these are labelled as subtype=?fromCC?
).Part of the rule-based entity recognition com-ponent builds an ?on-the-fly?
lexicon from namesfound in the header of the document.
Here thenames of the lords who are judging the case arelisted as well as the names of the respondent andappellant.
Since instances of these three entities oc-curring in the body of the judgment are likely to bedistributed differently across sentences with differ-ent rhetorical roles, it is useful to mark them up ex-plicitly.
We create an expanded lexicon from the?on-the-fly?
lexicon containing entries for consecu-tive substrings of the original entry in order to per-form a more flexible lexical look-up.
Thus the entityCommission is recognised as an appellant substringentity in the document where Northern Ireland Hu-man Rights Commission has been identified as anappellant entity.As future work, we plan to create a named entitygold standard for the HOLJ domain and evaluate thenamed entity recognition we are performing.
Fornow, we can use rhetorical status classification as atask-based evaluation to estimate the utility of entityrecognition.
The generic C&C entity recognition to-gether with the hand-crafted rules for the HOLJ do-main prove to be the third most effective feature setafter the cue phrase and location features (Table 3).The next stage in the linguistic analysis moduleperforms noun group and verb group chunking us-ing fsgmatch with the specialised hand-written rulesets which were the core part of LT CHUNK (Finchand Mikheev, 1997).
The noun group and verbgroup mark-up plus POS tags provide the relevant<NG type=?enamex-pers?
subtype=?committee-lord?> Lord Rodger of EarlsferryLord Hutton<NG type=?caseent?
subtype=?appellant?> Northern Ireland Human Rights Commission<NG type=?caseentsub?
subtype=?appellant?> Commission<NG type=?caseent?
subtype=?respondent?> URATEMP VENTURES LIMITED<NG type=?caseentsub?
subtype=?respondent?> Uratemp Ventures<NG type=?enamex-pers?
subtype=?judge?> Collins JPotter and Hale LJJ<NG type=?enamex-org?
subtype=?court?> European Court of JusticeBristol County Court<NG type=?legal-ent?
subtype=?act?> Value Added Tax Act 1994Adoption Act 1976<NG type=?legal-ent?
subtype=?section?> section 18(1)(a)para 3.1<NG type=?legal-ent?
subtype=?judgment?> Turner J [1996] STC 1469Apple and Pear Development Council v Commissionersof Customs and Excise (Case 102/86) [1988] STC 221<NG type=?enamex-loc?
subtype=?fromCC?> Oakdene RoadKuwait Airport<NG type=?enamex-pers?
subtype=?fromCC?> Irfan ChoudhryJohn MacDermott<NG type=?enamex-org?
subtype=?fromCC?> PowergenGrayan Building Services LtdTable 2: Named entities in the corpusfeatures for the next processing step.
In a previouspaper we showed that a range of information aboutthe main verb group of the sentence was likely toprovide important clues as to the rhetorical statusof the sentence (e.g.
a present tense active verb willcorrelate more highly with BACKGROUND or DIS-POSAL sentences while a simple past tense sentenceis more likely to be found in a FACT sentence).
In or-der to find the main verb group of a sentence, how-ever, we need to establish its clause structure.
Wedo this with a clause identifier (Hachey, 2002) builtusing the CoNLL-2001 shared task data (Sang andDe?jean, 2001).
Clause identification is performedin three steps.
First, two maximum entropy classi-fiers (Berger et al, 1996) are applied, where the firstpredicts clause start labels and the second predictsclause end labels.
In the the third step clause seg-mentation is inferred from the predicted starts andends using a maximum entropy model whose solepurpose is to provide confidence values for poten-tial clauses.The final stages of linguistic processing use hand-written LT TTT components to compute features ofverb and noun groups.
For all verb groups, attributesencoding tense, aspect, modality and negation areadded to the mark-up: for example, might not havebeen brought is analysed as <VG tense=?pres?, as-pect=?perf?, voice=?pass?, modal=?yes?, neg=?yes?>.In addition, subject noun groups are identified andlemma information from the head noun of the sub-ject and the head verb of the verb group are propa-gated to the verb group attribute list.3 The Sentence Classifier3.1 Feature SetsThe feature set described in Teufel andMoens (2002) includes many of the featureswhich are typically used in sentence extractionapproaches to automatic summarisation as well ascertain other features developed specifically forrhetorical role classification.
Briefly, the T&Mfeature set includes such features as: location of asentence within the document and its subsectionsand paragraphs; sentence length; whether thesentence contains words from the title; whether itcontains significant terms as determined by tf*idf ;whether it contains a citation; linguistic featuresof the first finite verb; and cue phrases (describedas meta-discourse features in Teufel and Moens,2002).
The features that we have been experiment-ing with for the HOLJ domain are broadly similarto those used by T&M and are described in theremainder of this section.Location.
For sentence extraction in the news do-main, sentence location is an important feature and,though it is less dominant for T&M?s scientific arti-cle domain, they did find it to be a useful indicator.T&M calculate the position of a sentence relative tosegments of the document as well as sections andparagraphs.
In our system, location is calculatedrelative to the containing paragraph and LORD el-ement and is encoded in six integer-valued features:paragraph number after the beginning of the LORDelement, paragraph number before the end of theLORD, sentence number after the beginning of theLORD element, sentence number before the end ofthe LORD, sentence number after the beginning ofthe paragraph, and sentence number before the endof the paragraph.Thematic Words.
This feature is intended tocapture the extent to which a sentence containsterms which are significant, or thematic, in the doc-ument.
The thematic strength of a sentence is cal-culated as a function of the tf*idf measure on words(tf =?term frequency?, idf =?inverse document fre-quency?
): words which occur frequently in the doc-ument but rarely in the corpus as a whole have ahigh tf*idf score.
The thematic words feature inTeufel and Moens (2002) records whether a sen-tence contains one or more of the 18 highest scoringwords.
In our system we summarise the thematiccontent of a sentence with a real-valued thematicsentence feature, whose value is the average tf*idfscore of the sentence?s terms.Sentence Length.
In T&M, this feature describessentences as short or long depending on whetherthey are less than or more than twelve words inlength.
We implement an integer-valued sentencelength feature which is a count of the number of to-kens in the sentence.Quotation.
This feature, which does not havea direct counterpart in T&M, encodes the percent-age of sentence tokens inside an in-line quote andwhether or not the sentence is inside a block quote.Entities.
T&M do not incorporate full-scalenamed entity recognition in their system, thoughthey do have a feature reflecting the presence orabsence of citations.
We recognise a wide rangeof named entities and generate binary-valued entitytype features which take the value 0 or 1.Cue Phrases.
The term ?cue phrase?
covers thekinds of stock phrases which are frequently good in-dicators of rhetorical status (e.g.
phrases such as Theaim of this study in the scientific article domain andIt seems to me that in the HOLJ domain).
T&M in-vested a considerable amount of effort in compilinglists of such cue phrases and building hand-craftedlexicons where the cue phrases are assigned to oneof a number of fixed categories.
A primary aim ofthe current research is to investigate whether the ef-fects of T&M?s cue phrase features can be achievedusing automatically computable linguistic features.If they can, then this helps to relieve the burden in-volved in porting systems such as these to new do-mains.Our preliminary cue phrase feature set includessyntactic features of the main verb (voice, tense, as-pect, modality, negation), which we have shown tobe correlated with rhetorical status (Grover et al,2003).
We also use features indicating sentenceinitial part-of-speech and sentence initial word fea-tures to roughly approximate formulaic expressionswhich are sentence-level adverbial or prepositionalphrases.
Subject features include the head lemma,entity type, and entity subtype.
These features ap-proximate the hand-coded agent features of T&M.A main verb lemma feature simulates T&M?s typeof action and a feature encoding the part-of-speechafter the main verb is meant to capture basic subcat-egorisation information.3.2 Classifier Results and DiscussionWe ran experiments for four classifiers in the Wekapackage:5 C4.5 decision trees, na?
?ve Bayes (NB)incorporating nonparametric density estimation ofcontinuous variables, the Winnow6 algorithm formistake-driven learning of a linear separator, andthe sequential minimal optimization algorithm fortraining support vector machines (SVM) using poly-nomial kernels.
Default parameter settings are usedfor all algorithms.Micro-averaged7 F-scores for each classifier arepresented in Table 3.
The I columns contain in-dividual scores for each feature type and the Ccolumns contain cumulative scores which incorpo-rate features incrementally.
C4.5 performs very well(65.4%) with location features only, but is not ableto successfully incorporate other features for im-proved performance.
SVMs perform second best(60.6%) with all features.
NB is next (51.8%)with all but thematic word features.
Winnow hasthe poorest performance with all features giving amicro-averaged F-score of 41.4%.For the most part, these scores are consider-ably lower than T&M, where they achieve a micro-averaged F-score of 72.
However, the picture isslightly different when we consider the systems inthe context of their respective baselines.
Teufel andMoens (2002) report a macro-averaged F-score of11 for always assigning the most frequent rhetori-cal class, similar to the simple baseline they use inearlier work.
This score is 54 when micro-averaged5http://www.cs.waikato.ac.nz/ml/weka/6To evaluate Winnow, we use the Weka implementation ofthe MDL discretization method which recursively splits inter-vals at the cut-off point that minimizes entropy.7Micro-averaging weights categories by their prior proba-bility.
By contrast, macro-averaging puts equal weight on eachclass regardless of how sparsely populated it might be.C4.5 NB Winnow SVMI C I C I C I CCue Phrases 47.8 47.8 39.6 39.6 31.1 31.1 52.1 52.1Location 65.4 54.9 34.9 47.5 34.2 40.2 35.9 55.0Entities 35.5 54.4 32.6 48.8 26.0 40.2 33.1 56.5Sent.
Length 27.2 55.1 20.0 49.1 27.0 40.4 12.0 56.8Quotations 28.4 59.5 29.7 51.8 23.3 41.1 27.8 60.2Them.
Words 30.4 59.7 21.2 51.7 25.7 41.4 12.0 60.6Baseline 12.0Table 3: Micro-averaged F-score results for rhetorical classificationbecause of the skewed distribution of rhetorical cat-egories (67% of sentences fall into the most frequentcategory).8With the more uniform distribution of rhetori-cal categories in the HOLJ corpus, we get baselinenumbers of 6.2 (macro-averaged) and 12.0 (micro-averaged).
Thus, the actual per-sentence (micro-averaged) F-score improvement is relatively high,with our system achieving an improvement of be-tween 29.4 and 53.4 points (to 41.4 and 65.4 respec-tively for the optimal Winnow and C4.5) where theT&M system achieves an improvement of 18 points.Like T&M, our cue phrase features are the mostsuccessful feature subset (excepting C4.5 decisiontrees).
We find these results very encouraging giventhat we have not invested any time in developing thehand-crafted cue phrase features that proved mostuseful for T&M, but rather have attempted to simu-late these through fully automatic, largely domain-independent linguistic information.The fact that C4.5 decision trees outperform allalgorithms on location features led us to believewe might be using an inferior representation for lo-cation features.
To test this, we encoded our lo-cation features in the same way as T&M.
Thisgave improved F scores for SVMs (41.5) and na?
?veBayes (41.0) but worse scores for Winnow and dra-matically worse scores for C4.5, indicating, as onewould expect, that the discrete T&M location fea-tures lose information present in our non-discretizedlocation features.Maximum entropy (ME) modelling is anothermachine learning method which allows the integra-tion of diverse information sources.
ME approaches8T&M use macro-averaging in order to down-weight theirlargest category which was the least interesting for their sum-maries.
With our more uniform distribution of rhetorical cat-egories and without any reason, as yet, to expect the numberof summary sentences coming from any one category to be farout of proportion, we believe it better to report micro-averagedscores.
If we compare macro-averaged F scores, the SVM clas-sifier achieves a score (52) a bit higher than T&M (50).
C4.5outperforms both by a considerable amount, achieving a macro-averaged F score of 58.explicitly model the dependence between featuresand have proven highly effective in similar naturallanguage tasks such as text categorisation, part-of-speech tagging, and named entity recognition.
Thenext step in our research will be to experiment withmaximum entropy modelling and to compare it withthe techniques reported here.4 Conclusions and Future WorkWe have presented new work on the summarisationof legal texts for which we are developing a new cor-pus of UK House of Lords judgments with detailedlinguistic markup in addition to rhetorical status andsentence extraction annotation.We have effectively laid the ground work for de-tailed experiments with robust and generic meth-ods for capturing cue phrase information.
Thisis favourable as it can be automatically ported tonew text summarisation domains where the tools areavailable for linguistic analysis, as opposed to rely-ing on cue phrases which need to be hand-craftedfor each domain.
Hand-crafted cue phrase lists arenecessarily more fragile and more susceptible toover-fitting in large-scale applications.Future experiments will use maximum entropymodelling to incorporate our diverse range of sparselinguistic and textual features.
We plan to ex-periment with maximum entropy for sentence-levelrhetorical status prediction in both standard classifi-cation and sequence modelling frameworks.We also intend to incorporate bootstrappednamed entity recognition systems.
While genericlinguistic analysis tools (e.g.
part-of-speech tag-ging, chunking) are easy to come by in many lan-guages, domain-specific named entity recognitionis not.
We have invested a considerable amountof time in writing named entity rules by hand forthe HOLJ domain.
However, current research is in-vestigating methods for bootstrapping named en-tity systems from small amounts of seed data.
Ef-fective methods will make our linguistic featuresfully domain-independent for domains and lan-guages where linguistic analysis tools are available.For future work, we are considering active learn-ing and co-training.
Active learning (Cohn et al,1994) would seem the appropriate starting point forour task as we currently have no gold standard databut we do have annotation resources.
We may alsobenefit from co-training (Blum and Mitchell, 1998)and rule induction (Riloff and Jones, 1999) with theseed data set from the initial annotation for activelearning.We have also performed a preliminary experi-ment with hypernym features for subject and verblemmas which should allow better generalisationover cue phrase information.
This is a rather noisyfeature as we are not performing word sense disam-biguation, but adding all WordNet hypernyms of thefirst three senses as features.
Nevertheless, this hasshown an improvement with the na?
?ve Bayes classi-fier from 24.75 for the cue phrase features sets (mi-nus lemma features) to 27.45 when hypernyms areincluded.
Future work will further investigate hy-pernym features.AcknowledgmentsThis work is supported by EPSRC grant GR/N35311.The corpus annotation was carried out by VasilisKaraiskos and Hui-Mei Liao using the NITE XMLToolkit (NXT), with assistance from Jonathan Kilgour inconfiguring NXT.
We are grateful to Stephen Clark andthree anonymous reviewers for their valuable comments.ReferencesAdam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the 11th Annual Conference on Compu-tational Learning Theory, Madison, Wisconsin.Nancy A. Chinchor.
1998.
Proceedings of the SeventhMessage Understanding Conference (MUC-7).
Fair-fax, Virginia.David Cohn, Les Atlas, and Richard Ladner.
1994.
Im-proving generalization with active learning.
MachineLearning, 15(2):201?221.James R. Curran and Stephen Clark.
2003.
Languageindependent NER using a maximum entropy tagger.In Proceedings of CoNLL-2003, pages 164?167.
Ed-monton, Canada.Walter Daelemans and Miles Osborne.
2003.
Proceed-ings of the Seventh Workshop on Computational Lan-guage Learning (CoNLL-2003).
Edmonton, Canada.Steve Finch and Andrei Mikheev.
1997.
A workbenchfor finding structure in texts.
In Proceedings of theFifth Conference on Applied Natural Language Pro-cessing (ANLP-97).
Washington D.C.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
LT TTT?a flexible tokenisationtool.
In LREC 2000?Proceedings of the 2nd Interna-tional Conference on Language Resources and Evalu-ation, pages 1147?1154.Claire Grover, Ben Hachey, and Chris Korycinski.
2003.Summarising legal texts: Sentential tense and argu-mentative roles.
In HLT-NAACL 2003 Workshop: TextSummarization (DUC03), pages 33?40, Edmonton,Canada.Ben Hachey.
2002.
Recognising clauses using symbolicand machine learning approaches.
Master?s thesis,University of Edinburgh.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to its Methodology.
Sage Publications, Bev-erly Hills, CA.David McKelvie.
1999.
Xmlperl 1.0.4 XML process-ing software.
http://www.cogsci.ed.ac.uk/?dmck/xmlperl.Andrei Mikheev.
1997.
Automatic rule induction forunknown word guessing.
Computational Linguistics,23(3):405?423.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust, applied morphological generation.
In Pro-ceedings of 1st International Natural Language Gen-eration Conference (INLG?2000).Marie-Francine Moens, Caroline Uyttendaele, and JosDumortier.
1997.
Abstracting of legal cases: The SA-LOMON experience.
In Proceedings of the Sixth In-ternational Conference on Artificial Intelligence andLaw, ACM, pages 114?122.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proceedings of the 16th National Con-ference on Artificial Intelligence (AIII-99), Orlando,Florida.Dan Roth and Antal van den Bosch.
2002.
Proceedingsof the Sixth Workshop on Computational LanguageLearning (CoNLL-2002).
Taipei, Taiwan.Erik Tjong Kim Sang and Herve?
De?jean.
2001.
Intro-duction to the CoNLL-2001 shared task: clause iden-tification.
In Proceedings of the Fifth Workshop onComputational Language Learning, pages 53?57.Karen Sp a?rck-Jones.
1998.
Automatic summarising:factors and directions.
In Advances in Automatic TextSummarisation, pages 1?14.
MIT Press.Simone Teufel and Marc Moens.
1999a.
Argumentativeclassification of extracted sentences as a first step to-wards flexible abstracting.
In Advances in AutomaticText Summarization, pages 137?175.
MIT Press.Simone Teufel and Marc Moens.
1999b.
Discourse-levelargumentation in scientific articles: human and auto-matic annotation.
In Towards Standards and Tools forDiscourse Tagging, pages 84?93.
ACL Workshop.Simone Teufel and Marc Moens.
2002.
Summaris-ing scientific articles?experiments with relevanceand rhetorical status.
Computational Linguistics,28(4):409?445.Henry Thompson, Richard Tobin, David McKelvie,and Chris Brew.
1997.
LT XML.
software APIand XML toolkit.
http://www.ltg.ed.ac.uk/software/.
