Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 1?9,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPResearcher affiliation extraction from homepagesIstva?n Nagy1, Richa?rd Farkas1,2, Ma?rk Jelasity2Nagy.Istvan@gmail.com, {rfarkas,jelasity}@inf.u-szeged.hu1 University of Szeged, Department of InformaticsA?rpad te?r 2., H-6720 Szeged, Hungary2 Hungarian Academy of Sciences, Research Group on Artificial IntelligenceAradi ve?rtanuk tere 1., H-6720 Szeged, HungaryAbstractOur paper discusses the potential use ofWeb Content Mining techniques for gath-ering scientific social information from thehomepages of researchers.
We will intro-duce our system which seeks [affiliation,position, start year, end year] informationtuples on these homepages along with pre-liminary experimental results.
We believethat the lessons learnt from these experi-ments may be useful for further scientificsocial web mining.1 IntroductionScientific social network analysis (Yang et al,2009; Said et al, 2008) seeks to discover globalpatterns in the network of researchers working ina particular field.
Common approaches uses bibli-ographic/scholarly data as the basis for this anal-ysis.
In this paper, we will discuss the poten-tial of exploiting other resources as an informa-tion source, such as the homepages of researchers.The homepage of a researcher contains severaluseful pieces of scientific social information likethe name of their supervisor, affiliations, academicranking and so on.The information on homepages may be presentin a structured or natural text form.
Here weshall focus on the detection and analysis of fulltext regions of the homepages as they may con-tain a huge amount of information while requiresmore sophisticated analysis than that for struc-tured ones.
We will show that this kind of Web-based Relation Extraction requires different tech-niques than the state-of-the-art seed-based ap-proaches as it has to acquire information from thelong-tail of the World Wide Web.As a case study, we chose one particular sci-entific social information type and sought to ex-tract information tuples concerning the previousand current affiliations of the researcher in ques-tion.
We defined ?affiliation?
as the current andprevious physical workplaces and higher edu-cational institutes of the researcher in question.Our aim is to use this kind of information todiscover collegial relationships and workplace-changing behaviour which may be complementaryto the items of information originating from bibli-ographic databases.Based on a manually annotated corpus we car-ried out several information extraction experi-ments.
The architecture of the complex systemand the recognised problems will be discussed inSection 3, while our empirical results will be pre-sented in Section 4.
In the last two sections wewill briefly discuss our results and then draw ourmain conclusions.2 Related workThe relationship to previous studies will be dis-cussed from a scientific social network analysis asan application point of view and from a Web Con-tent Mining point of view as well.2.1 Researcher affiliation extractionScientific social network analysis has become agrowing area in recent years ((Yang et al, 2009;Robardet and Fleury, 2009; Said et al, 2008)just to name a few in recent studies).
Its goal isto provide a deeper insight into a research fieldor into the personal connections among fields byanalysing relationships among researchers.
Theexisting studies use the co-authorship (e.g.
(New-man, 2001; Baraba?si et al, 2002)) or/and the cita-tion (Goodrum et al, 2001; Teufel et al, 2006) in-formation ?
generally by constructing a graph withnodes representing researchers ?
as the basis fortheir investigations.Apart from publication-related relationships?
which are presented in structured scholarlydatasets ?, useful scientific social information can1be gathered from theWWW.
Take, for instance thehomepage of a researchers where they summarisetheir topic of interest, list supervisors and students,nationality, age, memberships and so on.
Our goalis to develop an automatic Web Content Miningsystemwhich crawls the homepages of researchersand extracts useful social information from them.A case study will be outlined here, where theprevious and current affiliations of the researcherin question were gathered automatically.
Havinga list of normalised affiliations for each researcherof a field (i) we ought to be able to discover col-legial relationships (whether they worked with thesame group at the same time) which may differfrom the co-authorship relation and (ii) we hopeto be able to answer questions like ?Do Americanor European researchers change their workplacemore often?
?.2.2 Information extraction from homepagesFrom a technology point of view our procedureis a Web Content Mining tool, but it differs fromthe popular techniques used nowadays.
The aimof Web Content Mining (Liu and Chen-Chuan-Chang, 2004) is to extract useful information fromthe natural language-written parts of websites.The first attempts on Web Content Mining be-gan with the Internet around 1998-?99 (Adelberg,1998; Califf and Mooney, 1999; Freitag, 1998;Kosala and Blockeel, 2000).
They were expertsystems with hand-crafted rules or induced rulesused in a supervised manner and based on labeledcorpora.The next generation of approaches on the otherhand work in weakly-supervised settings (Etzioniet al, 2005; Sekine, 2006; Bellare et al, 2007).Here, the input is a seed list of target informationpairs and the goal is to gather a set of pairs whichare related to each other in the same manner as theseed pairs.
These pairs may contain related enti-ties (for example, country - capital city in (Etzioniet al, 2005) and celebrity partnerships in (Chenget al, 2009)) or form an entity-attribute pair (likeNobel Prize recipient - year in (Feiyu Xu, 2007))or may be concerned with retrieving all availableattributes for entities (Bellare et al, 2007; Pas?ca,2009).
These systems generally download webpages which contain the seed pairs then learn syn-tactical/semantical rules from the sentences of thepairs (they generally use the positive instances forone case as negative instances for another case).According to these patterns, they can download anew set of web pages and parse them to acquirenew pairs.These seed-based systems exploit the redun-dancy of the WWW.
They are based on the hy-pothesis that important information can be foundat several places and in several forms on the Web,hence a few accurate rules can be used to collectthe required lists.
Their goal is to find and recog-nise (at least) one occurrence of the target infor-mation and not to find their every occurrence onthe Web.
But this is not the case in our scenario.Several pieces of social information for the re-searchers are available just on their homepages (ornowhere).
Thus here we must capture each men-tion of the information.
The weakly-supervised(redundancy-based) systems can build on high-precision and lower recall information extraction,while we have to have target a perfect recall.
Forthe evaluation of such a system we constructed amanually annotated corpus of researchers?
home-pages.
This corpus was also used as a training cor-pus for the preliminary information extraction ex-periments described in this paper.3 The architecture of the systemThe general task of our system is to gather sci-entific social information from the homepages ofresearchers.
In the use case presented in this pa-per, the input is a set of researchers?
names whowork in a particular research field (later on, thislist can be automatically gathered, for example,from a call for papers) and the output is a list ofaffiliations for each researcher.
Here the affiliationis a tuple of affiliation, position type and start/enddates.
We think that the lessons learnt from affili-ation extraction will be useful for the developmentof a general social information extraction system.The system has to solve several subproblemswhich will be described in the following subsec-tions.3.1 Locating the homepage of the researcherHomepage candidates can be efficiently found byusing web search engine queries for the givenname.
In our case study the homepage of theresearcher (when it existed) were among the top10 responses of the Google API1 in each case.However, selecting the correct homepage fromthe top 10 responses is a harder task.
Among1http://code.google.com/apis/2these sites there are (i) publication-related ones(books/articles written by the researchers, call forpapers), sites of the institute/group associated withthe researcher and (ii) homepages of people shar-ing the same name.In our preliminary experiments, we ignoredthese two basic problems and automatically parsedeach website.
However in the future we plan todevelop a two-stage approach to solve them.
Inthe first stage a general homepage detection model?
a binary classification problem with classeshomepage/non-homepage ?
will be applied.In the second stage we will attempt to automati-cally extract textual clues for the relations amongthe researchers (e.g.
the particular field they workin) from the homepage candidates and utilise thesecues for name disambiguation along with other bi-ographical cues.
For a survey of state-of-the-artname disambiguation, see (Artiles et al, 2009).3.2 Locating the relevant parts of the siteThe URL got from the search engine usuallypoints to the main page of the homepage site.
Anideal system should automatically find every pagewhich might contain scientific social informationlikeCurriculum Vitae, Research interests, Projectsetc.
This can be done by analysing the text of thelinks or even the linked page.
In our case study wesimply parsed the pages to a depth of 1 (i.e.
themain page and each page which was linked fromit).The located web pages usually have their con-tent arranged in sections.
The first step of infor-mation extraction may be a relevant section se-lection module.
For example, in the affiliationextraction task the Positions Held and Educationtype sections are relevant while Selected Papersis not.
Having several relevant sections with theirtextual positions, an automatic classification sys-tem can filter out a huge number of probably irrel-evant sections.
In our experiments, we statisticallycollected a few ?relevant keywords?
and filteredout sections and paragraphs which did not containany of these keywords.3.3 Extracting information tuplesPieces of scientific social information are usuallypresent on the homepages and in the CVs even inan itemised (structured) form or in a natural lan-guage full text form.
Information extraction isperformed from the structured parts of the docu-ments by automatically constructed rules based onthe HTML tags and keywords.
This field is calledWrapper Induction (Kushmerick, 2000).We shall focus on the information extractionfrom raw texts here because we found that morepages express content in textual form than in astructured one in the researchers?
homepages ofour case study and this task still has several un-solved problems.
We mentioned above that sci-entific social information extraction has to cap-ture each occurrence of the target information.We manually labeled homepages for the evalua-tion of these systems.
We think that the DOMstructure of the homepages (e.g.
formatting tags,section headers) could provide useful information,hence the labeling was carried out in their origi-nal HTML form (Farkas et al, 2008).
In our pre-liminary experiments we also used this corpus totrain classification models (they were evaluated ina one-researcher-leave-out scheme).
The purposeof these supervised experiments was to gain an in-sight into the nature of the problem, but we suggestthat a real-world system for this task should workin a weakly-supervised setting.3.4 NormalisationThe output of the extraction phase outlined aboveis a list of affiliations for each researcher in theform that occurred in the documents.
However, forscientific social network analysis, several normal-isation steps should be performed.
For example,for collegial relationship extraction, along withthe matching of various transliteration of researchgroups (like Massachusetts Institute of Technologyand MIT AI Lab), we have to identify the appropri-ate institutional level where two researchers prob-ably still have a personal contact as well.4 ExperimentsNow we will present the affiliation corpus whichwas constructed manually for evaluation purposesalong with several preliminary experiments on af-filiation extraction.4.1 The affiliation corpusWe manually constructed a web page corpuscontaining HTML documents annotated for pub-licly available information about researchers.
Wedownloaded 455 sites, 5282 pages for 89 re-searchers (who form the Programme Committeeof the SASO07 conference2), and two indepen-2http://projects.csail.mit.edu/saso2007/tmc.html3dent annotators carried out their manual labelingin the original (HTML) format of the web pages,following an annotation guideline (Farkas et al,2008).
All the labels that were judged inconsis-tent were collected together from the corpus for areview by the two annotators and the chief annota-tor.
We defined a three-level deep annotation hier-archy with 44 classes (labels).
The wide range ofthe labels and the inter-annotator agreement bothsuggest that the automatic reproduction of this fulllabelling is a hard task.We selected one particular information class,namely affiliation from our class hierarchy for ourcase study.
We defined ?affiliation?
as the currentand previous physical workplaces and higher ed-ucational institutes of the researcher in questionas we would like to use this kind of informationto discover collegial relationships and workplace-changing behaviour.
Here institutes related to re-view activities, awards, or memberships are not re-garded as affiliations.
We call position the tu-ple of <affiliation, position types,years>, as for example in <National Depart-ment of Computer Science and Operational Re-search at the University of Montreal, adjunct Pro-fessor, {1995, 2002}>3.
Among the four slotsjust the affiliation slot is mandatory (it isthe head) as the others are usually missing in realhomepages.The problem of finding the relevant pages of ahomepage site originating from a seed URL wasnot addressed in this study.
We found that pagesholding affiliation information was the one re-trieved by Google in 135 cases and directly linkedto the main page in 50 cases.
We found affilia-tion information for all of the 89 researchers of ourcase study in the depth of 1, but we did not checkwhether deeper crawling could have yielded newinformation.The affiliation information (like every piece ofscientific social information) can be present onweb pages in an itemised or natural text format.We manually investigated our corpus and foundthat the 47% of the pages contained affiliation in-formation exclusively in a textual form, 24% ex-clusively in an itemised form and 29% were hy-brid.
Information extraction from these two for-mats requires different methods.
We decided toaddress the problem of affiliation extraction just3the example is extracted fromhttp://bcr2.uwaterloo.ca/?rboutaba/biography2.htmby using the raw text parts of the homepages.We partitioned each downloaded page at HTMLbreaking tags and kept the parts (paragraphs)which were regarded as ?raw text?.
Here we usedthe following rule: a textual paragraph has to belonger than 40 characters and contain at least oneverb.
Certainly this rule is far from perfect (para-graphs describing publication and longer items oflists are still present), but it seems to be a reason-able one as it extracts paragraphs even from ?hy-brid?
pages.
We found 86,735 paragraphs in the5282 downloaded pages and used them in experi-ments in a raw txt format (HTML tags were re-moved).Table 4.1 summarises the size-related figuresfor the part of this textual corpus which containsaffiliation information (these paragraphs containmanually labeled information).
The corpus isfreely available for non-commercial use4.# researchers 59# pages 103# paragraph 151# sentences 181# affiliation 374# position type 326# year 212Table 1: The size of the textual corpus which con-tains affiliation information.4.2 The multi-stage model of relationextractionOur relation extraction system follows the archi-tecture described in the previous section.
We fo-cus on the relevant part location and informationextraction steps in this study.
We applied simplerules to recognise the relevant parts of the home-pages.
We extract textual paragraphs as describedabove and then filter out probably irrelevant ones(Section 4.3).Preliminary supervised information extractionexperiments were carried out in our case study inorder to get an insight into the special nature ofthe problem.
We used a one-researcher-leave-outevaluation setting (i.e.
the train sets consisted ofthe paragraphs of 88 researchers and the test setsconcerned 1 researcher), thus we avoided the situ-ations where a training set contained possibly re-4www.inf.u-szeged.hu/rgai/homepagecorpus4dundant information about the subject of the testtexts.A two-stage information extraction system wasapplied here.
In the first phase, a model shouldrecognise each possible slot/entities of the targetinformation tuples (Section 4.4).
Then the tupleshave to be filled, i.e.
the roles have to be assignedand irrelevant entities should be ignored (Section4.5).4.3 Paragraph filteringBecause just a small portion of extracted textualparagraphs contained affiliation information, wecarried out experiments on filtering out probablyirrelevant paragraphs.Our filtering method exploited the paragraphscontaining position (positive paragraphs).We calculated the P (word|positive) conditionalprobabilities and the best words based on this mea-sure (e.g.
university, institute and professor) thenformed the so-called positive wordset.
The para-graphs which did not contain any word from thepositive wordset were removed.
Note that stan-dard positive and negative sample-based classifi-cation is not applicable here as the non-positiveparagraphs may contain these indicative words,but in an irrelevant context or with a connectionto people outside of our scope of interest.
Our 1-DNF hypothesis described above uses just positiveexamples and it was inspired by (Yu et al, 2002).After performing this procedure we kept 14,686paragraphs (from the full set of 86,735), but wedid not leave out any annotated text.
Hence the in-formation extraction module could then work witha smaller and less noisy dataset.4.4 Detecting possible slotsWe investigated a Named Entity Recognition(NER) tool for detecting possible actors of aposition tuple.
But note that this task is not aclassical NER problem because our goal here is torecognise just those entities which may play a rolein a position event.
For example there weremany year tokens in the text ?
having the sameorthographic properties ?
but only a few were re-lated to affiliation information.
The contexts of thetokens should play an important role in this kindof an NER targeting of very narrow semantic NEclasses.For training and evaluating the NER systems,we used each 151 paragraphs containing at leastone manually labeled position along with 200other manually selected paragraphs which do notcontain any labeled position.
We decided touse just this 151+200 paragraphs instead of thefull set of 86,735 paragraphs for CPU time rea-sons.
Manual selection ?
instead of random sam-pling ?
was required as there were several para-graphs which contained affiliation information un-related to the researcher in question, thus introduc-ing noise.
In our multi-stage architecture, the NERmodel trained on this reduced document set wasthan predicated for the full set of paragraphs andfalse positives (note that the paragraphs outside theNER-train do not contain any gold-standard anno-tation) has to be eliminated.We employed the Condition Random Fields(Lafferty et al, 2001) (implementation MALLET(McCallum, 2002)) for our NER experiments.The feature set employed was developed for gen-eral NER and includes the following categories(Szarvas et al, 2006):orthographical features: capitalisation, wordlength, bit information about the word form(contains a digit or not, has uppercase char-acter inside the word, and so on), characterlevel bi/trigrams,dictionaries of first names, company types, de-nominators of locations,frequency information: frequency of the token,the ratio of the token?s capitalised and low-ercase occurrences, the ratio of capitalisedand sentence beginning frequencies of the to-ken which was derived from the Gigaworddataset5,contextual information: sentence position, trig-ger words (the most frequent and unambigu-ous tokens in a window around the NEs) fromthe train text, the word between quotes, andso on.This basic set was extended by two domain-specific gazetteers, namely a list of universitynames and position types.
We should add thata domain-specific exception list (containing e.g.Dr., Ph.D.) for augmenting a general sentencesplitter was employed here.Table 2 lists the phrase-level F?=1 results ob-tained by CRF in the one-researcher-leave-out5Linguistic Data Consortium (LDC),catalogId: LDC2003T055evaluation scheme, while Table 3 lists the resultsof a baseline method which labels each memberof the university and position type gazetteers andidentifies years using regular expressions.
Thiscomparison highlights the fact that labeling eachoccurrences of this easily recognisable classescannot be applied.
It gives an extremely low pre-cision thus contextual information has to be lever-aged.Precision Recall F?=1affiliation 66.78 53.28 59.27position type 87.50 70.22 77.91year 86.42 69.31 76.92TOTAL 78.73 62.88 69.92Table 2: The results achieved by CRF.Precision Recall F?=1affiliation 21.43 9.68 13.33position type 23.27 66.77 34.51year 65.77 98.99 79.03TOTAL 32.16 44.08 37.19Table 3: NER baseline results.4.5 The assignment of rolesWhen we apply the NER module to unknown doc-uments we have to decide whether the recognisedentities have any connection with the particularperson as downloaded pages often contain infor-mation about other researchers (supervisors, stu-dents, etc.)
as well.
The subject of the informa-tion is generally expressed by a proper noun atthe beginning of the page or paragraph and thenanaphoric references are used.
We assumed herethat each position tuple in a paragraph was re-lated to exactly one person and when the subject ofthe first sentence of the paragraph was a personalpronoun I, she, he then the paragraph belonged tothe author of the page.To automatically find the subject of the para-graphs we tried out two procedures and evaluatedthem on the predictions of the NER model intro-duced in the previous subsection.
First, we applieda NER trained on the person names of the CoNLL-2003 corpus (Tjong Kim Sang and De Meulder,2003).
The names predicted by this method werethen compared to the owner of the homepage us-ing name normalisation techniques.
If no namewas found by the tagger we regarded the para-graph as belonging to the author.
Its errors had twosources; the NER trained on an out-domain corpusmade a lot of false negatives and the normalisationmethod had to deal with incorrect ?names?
(likePaul Hunter Curator as a name phrase) as well.The second method was simpler.
We keptthe position tuples whose paragraph containedany part of the researcher name or any of the ?I?,?she?, ?he?
personal pronouns.
Its errors came, forinstance, from finding the ?Paul?
string for ?PaulRobertson?
in the text snippet ?Paul Berger?.We applied these two subject detection meth-ods to the predictions of our slot detection NERmodul.
Table 4 summarises the accuracies of thesystems, i.e.
whether they made the correct deci-sion on ?is this forecasted affiliation correspondsto the researcher in question?.
The columns ofthis table shows how many affiliation pre-diction was carried out by the slot detection sys-tem, i.e.
how many times has to made a de-cision.
?name.
det?
and ?p.
pronouns?
referto the two methods, to the name detection-basedand to the personal pronoun-matcher ones.
Weinvestigated their performance on the paragraphswhich contained manually labeled information,on the paragraphs which did contained any butthe slot detection module forecasted at least oneaffiliation here and on the union of thesesets of paragraphs.
The figures of the table showsthat the personal pronoun detection approach per-forms significantly better on the paragraphs whichreally contains affiliation information.
This is dueto the fact that this method removes less predic-tion compared to the name based one and there arejust a few forecast which has to be removed on theparagraphs which contain information.#pred name det.
p. pronounsannotated 165 66.9 87.8non-ann.
214 71.5 61.2full set 379 69.4 73.4Table 4: Accuracies of subject detection methods.To find relationships among the other types ofpredicated entities (affiliation, position type, startyear, end year) we used a very simple heuristic.As the affiliation slot is the head of the tuplewe simply assigned every other detected entity tothe nearest affiliation and regarded the ear-lier preidcated year token as the start year.6This method made the correct decision in the91.3% and 71.8% of the cases applied on the gold-standard annotation and the predicated entities, re-spectively.
We should add that using the predictedlabels during the evaluation, the false positives ofthe NER counts automatically an error in relationdetection as well.5 DiscussionThe first step of the information extraction sys-tem of this case study was the localisation of rele-vant information.
We found that Web search en-gines are efficient tools for finding homepages.We empirically showed that a very simple crawl-ing (downloading everything to a depth of 1) canbe applied, because the irrelevant contents can beremoved later.
The advantage of focused crawl-ing (i.e.
making a decision before download-ing a linked page) is that it can avoid the time-consuming analysis of pages.
However makingthe decision of whether the linked document mightcontain relevant information is a hard task.
On theother hand we showed that the requested informa-tion is reachable in depth 1 and that a fast string-matching based filtering method can significantlyreduce the amount of texts which have to be anal-ysed without losing any information.
Moreover,the positive example-based filtering approach canbe employed in a seed-driven setting as well.For the information extraction phase we thinkthat a high-recall system has to be developed.
Weconstructed a corpus with contextual occurrencesfor evaluation issues.
The extraction can be re-lationship detection-based (e.g.
the state-of-the-art seed-driven approaches seek to acquire syntac-tic/semantic patterns which are typical of the re-lationship itself) or entity-based (like our method,these approaches first identify possible actors thenlook for relationships among them).
We expectthat the latter one is more suitable for high-recalltasks.The NER system of this case study achievedsignificantly better results than those for the base-line method.
We experimentally showed thatit could exploit the contextual information andthat the labeled entities were those which wereaffiliation-related.
However, the overall systemhas to be improved in the future.
We manuallyanalysed the errors on a part of the corpus andfound a few typical errors were present.
Ourannotation guide said that the geographical loca-tion of the affiliation was a part of the affilia-tion as it sometimes identifies the department (e.g.
?Hewlett-Packard Labs in Palo Alto?).
This ex-tension of the phrase proved to be difficult becausethere were several cases with the same ortho-graphic features (e.g.
Ph.D. from MIT in Physics).The acronyms immediately after the affiliation area similar case, which we regard as part of the nameand it is difficult for the NER to handle (e.g.
Cen-tre for Policy Modelling (CPM)).
As there is nopartial credit; an incorrect entity boundary is pe-nalised both as a false positive and as a false neg-ative.These points also explain the surprisingly lowprecision of the baseline system as it labeled uni-versity names without more detailed identifica-tion of the unit (e.g.
Department of ComputerScience, [Waterloo University]BASELINE).
Weshould add that these two annotation guidelinesare questionable, but we expect that informationmight get lost without them.
Moreover, there isan another reason for the low recall, it is that ourhuman annotators found textual clues for positiontypes on verbs as well (e.g.
I leadTY PE the Dis-tributed Systems Group).
The context of these la-beled examples are clearly different from that ofthe usual position type.Comparing the two subject detection methods,we see that the name detection model which learnton an out-domain corpus made a lot of mistakes,thus the method based on it judged more para-graphs as irrelevant ones.
The name detectioncould be improved by a domain corpus (for exam-ple the training corpus did not contain any Prof.NAME example) and by applying more sophisti-cated name normalisation techniques.
When wemanually analysed the errors of these procedureswe found that each false negative of the sim-pler subject detection method was due to the er-rors of the textual paragraph identification defini-tion used.
There were several itemisations whoseheader was type of ?Previously I worked for:?
andthe textual items themselves did not contain thesubject of the affiliation information.
The falsepositives often originated from pages which didnot belong to the researcher in question but con-tained him name (e.g.
I am a Ph.D. Student work-ing under the supervision of Prof. NAME).Lastly, an error analysis of the affiliation headseeking heuristic revealed that the 44% of thepredicted position type and year entities?s7sentences did not contain any affiliationprediction.
With the gold-standard labeling, therewere 6 sentences without affiliation labelsand only one of them used an anaphoric refer-ence, the others were a consequence of the erro-neous automatic sentence splitting of the HTMLdocuments.
The prediction of the NER sys-tem contained many more sentences without anyaffiliation label.
These could be fixedby forcing a second forecast phase to predictaffiliation in these sentences or by remov-ing these labels in a post-processing step.The remaining errors of the affiliation head as-signment could be avoided just by employing aproper syntactic analyser.
The most important lin-guistic phenomena which should be automaticallyidentify for this problem is enumeration.
For in-stance, we should distinguish between the enumer-ation and clause splitting roles of ?and?
(e.g.
?I?ma senior researcher and leader of the GROUP?and ?He got his PhD fromUNIVERSITY1 in YEARand has a Masters from UNIVERSITY2?).
Thisrequires a deep syntactic analysis, i.e.
the use ofa dependency parser which has to make accuratepredictions on several certain types of dependen-cies is probably needed.6 ConclusionsIn this paper we introduced a Web Content Miningsystem for gathering affiliation information fromthe homepages of researchers.
The affiliation in-formation collected from this source might be ofgreat value for scientific social network analysis.We discussed the special nature of this taskcompared to common Web-based relation extrac-tion approaches and identified several subtasks ofthe system during our preliminary experiments.We argued that the evaluation of this kind of sys-tem should be carried out on a manually labeledreference corpus.
We introduced simple but ef-fective solutions for the subproblems along withempirical results on a corpus.
We achieved rea-sonable results with an overall phrase-level F?=1score of 70% on the possible slot detection andan accuracy of 61% on relation extraction (as anaggregation of the subject detection and the affil-iation head selection procedures).
However eachsubproblem requires more sophisticated solutions,which we plan to address in the near future.AcknowledgmentsThis work was supported in part by the NKTHgrant of the Jedlik A?nyos R&D Programme(project codename TEXTREND) of the Hungar-ian government.
The authors would like to thankthe annotators of the corpus for their devoted ef-forts.ReferencesBrad Adelberg.
1998.
Nodose - a tool for semi-automatically extracting structured and semistruc-tured data from text documents.
ACM SIGMOD,27(2):283?294.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.2009.
Weps 2 evaluation campaign: overview of theweb people search clustering task.
In 2nd Web Peo-ple Search Evaluation Workshop (WePS 2009), 18thWWW Conference.A.
L. Baraba?si, H. Jeong, Z. Ne?da, E. Ravasz, A. Schu-bert, and T. Vicsek.
2002.
Evolution of the so-cial network of scientific collaborations.
Physica A:Statistical Mechanics and its Applications, 311(3-4):590 ?
614.Kedar Bellare, Partha Talukdar, Giridhar Kumaran,Fernando Pereira, Mark Liberman, Andrew McCal-lum, and Mark Dredze.
2007.
Lightly-supervisedattribute extraction for web search.
In Proceedingsof NIPS 2007 Workshop on Machine Learning forWeb Search.Mary Elaine Califf and Raymond J. Mooney.
1999.Relational learning of pattern-match rules for in-formation extraction.
In Proceedings of the Six-teenth National Conference on Artificial Intelli-gence, pages 328?334.Xiwen Cheng, Peter Adolphs, Feiyu Xu, Hans Uszko-reit, and Hong Li.
2009.
Gossip galore ?
a self-learning agent for exchanging pop trivia.
In Pro-ceedings of the Demonstrations Session at EACL2009, pages 13?16, Athens, Greece, April.
Associa-tion for Computational Linguistics.Oren Etzioni, Michael Cafarella, Doug Downey,Ana maria Popescu, Tal Shaked, Stephen Soderl,Daniel S. Weld, and Er Yates.
2005.
Unsupervisednamed-entity extraction from the web: An experi-mental study.
Artificial Intelligence, 165:91?134.Richa?rd Farkas, Ro?bert Orma?ndi, Ma?rk Jelasity, andJa?nos Csirik.
2008.
A manually annotated html cor-pus for a novel scientific trend analysis.
In Proc.
ofThe Eighth IAPR Workshop on Document AnalysisSystems.Hong Li Feiyu Xu, Hans Uszkoreit.
2007.
A seed-driven bottom-up machine learning framework for8extracting relations of various complexity.
In Pro-ceedings of ACL 2007, 45th Annual Meeting of theAssociation for Computational Linguistics, Prague,Czech Republic, 6.Dayne Freitag.
1998.
Information extraction fromhtml: Application of a general machine learning ap-proach.
In Proceedings of the Fifteenth NationalConference on Artificial Intelligence, pages 517?523.A.
A Goodrum, K. W McCain, S. Lawrence, and C. LGiles.
2001.
Scholarly publishing in the internetage: a citation analysis of computer science liter-ature.
Information Processing and Management,37:661?675, September.Raymond Kosala and Hendrik Blockeel.
2000.
Webmining research: A survey.
SIGKDD Explorations,2:1?15.Nicholas Kushmerick.
2000.
Wrapper induction: Ef-ficiency and expressiveness.
Artificial Intelligence,118:2000.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
18th International Conf.
onMachine Learning, pages 282?289.
Morgan Kauf-mann, San Francisco, CA.Bing Liu and Kevin Chen-Chuan-Chang.
2004.
Edito-rial: special issue on web content mining.
SIGKDDExplor.
Newsl., 6(2):1?4.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.M.
E. J. Newman.
2001.
The structure of scientificcollaboration networks.
In Proceedings NationalAcademy of Sciences USA, pages 404?418.Marius Pas?ca.
2009.
Outclassing Wikipedia in open-domain information extraction: Weakly-supervisedacquisition of attributes over conceptual hierarchies.In Proceedings of the 12th Conference of the Eu-ropean Chapter of the ACL (EACL 2009), Athens,Greece, March.Celine Robardet and Eric Fleury.
2009.
Communi-ties detection and the analysis of their dynamics incollaborative networks.
Int.
J.
Web Based Commu-nities, 5(2):195?211.Yasmin H. Said, Edward J. Wegman, Walid K. Shara-bati, and John T. Rigsby.
2008.
Social networksof author-coauthor relationships.
ComputationalStatistics & Data Analysis, 52(4):2177?2184.Satoshi Sekine.
2006.
On-demand information ex-traction.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 731?738,Sydney, Australia, July.
Association for Computa-tional Linguistics.Gyo?rgy Szarvas, Richa?rd Farkas, and Andra?s Kocsor.2006.
A multilingual named entity recognition sys-tem using boosting and c4.5 decision tree learningalgorithms.
DS2006, LNAI, 4265:267?278.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
An annotation scheme for citation function.In Proceedings of the 7th SIGdial Workshop on Dis-course and Dialogue, pages 80?87, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 142?147.
Edmon-ton, Canada.Y.
Yang, C. M. Au Yeung, M. J. Weal, and H. Davis.2009.
The researcher social network: A social net-work based on metadata of scientific publications.Hwanjo Yu, Jiawei Han, and Kevin Chen-ChuanChang.
2002.
Pebl: positive example based learn-ing for web page classification using svm.
In KDD?02: Proceedings of the eighth ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 239?248, New York, NY, USA.ACM.9
