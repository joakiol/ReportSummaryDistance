Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1506?1515,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCollective Classification of Congressional Floor-Debate TranscriptsClinton Burfoot, Steven Bird and Timothy BaldwinDepartment of Computer Science and Software EngineeringUniversity of Melbourne, VIC 3010, Australia{cburfoot, sb, tim}@csse.unimelb.edu.auAbstractThis paper explores approaches to sentimentclassification of U.S. Congressional floor-debate transcripts.
Collective classificationtechniques are used to take advantage of theinformal citation structure present in the de-bates.
We use a range of methods based onlocal and global formulations and introducenovel approaches for incorporating the outputsof machine learners into collective classifica-tion algorithms.
Our experimental evaluationshows that the mean-field algorithm obtainsthe best results for the task, significantly out-performing the benchmark technique.1 IntroductionSupervised document classification is a well-studiedtask.
Research has been performed across manydocument types with a variety of classification tasks.Examples are topic classification of newswire ar-ticles (Yang and Liu, 1999), sentiment classifica-tion of movie reviews (Pang et al, 2002), and satireclassification of news articles (Burfoot and Baldwin,2009).
This and other work has established the use-fulness of document classifiers as stand-alone sys-tems and as components of broader NLP systems.This paper deals with methods relevant to super-vised document classification in domains with net-work structures, where collective classification canyield better performance than approaches that con-sider documents in isolation.
Simply put, a networkstructure is any set of relationships between docu-ments that can be used to assist the document clas-sification process.
Web encyclopedias and scholarlypublications are two examples of document domainswhere network structures have been used to assistclassification (Gantner and Schmidt-Thieme, 2009;Cao and Gao, 2005).The contribution of this research is in four parts:(1) we introduce an approach that gives better thanstate of the art performance for collective classifica-tion on the ConVote corpus of congressional debatetranscripts (Thomas et al, 2006); (2) we provide acomparative overview of collective document classi-fication techniques to assist researchers in choosingan algorithm for collective document classificationtasks; (3) we demonstrate effective novel approachesfor incorporating the outputs of SVM classifiers intocollective classifiers; and (4) we demonstrate effec-tive novel feature models for iterative local classifi-cation of debate transcript data.In the next section (Section 2) we provide a for-mal definition of collective classification and de-scribe the ConVote corpus that is the basis for ourexperimental evaluation.
Subsequently, we describeand critique the established benchmark approach forcongressional floor-debate transcript classification,before describing approaches based on three alterna-tive collective classification algorithms (Section 3).We then present an experimental evaluation (Sec-tion 4).
Finally, we describe related work (Section 5)and offer analysis and conclusions (Section 6).2 Task Definition2.1 Collective ClassificationGiven a network and an object o in the network,there are three types of correlations that can be used1506to infer a label for o: (1) the correlations betweenthe label of o and its observed attributes; (2) the cor-relations between the label of o and the observed at-tributes and labels of nodes connected to o; and (3)the correlations between the label of o and the un-observed labels of objects connected to o (Sen et al,2008).Standard approaches to classification generallyignore any network information and only take intoaccount the correlations in (1).
Each object is clas-sified as an individual instance with features derivedfrom its observed attributes.
Collective classificationtakes advantage of the network by using all threesources.
Instances may have features derived fromtheir source objects or from other objects.
Classifi-cation proceeds in a joint fashion so that the labelgiven to each instance takes into account the labelsgiven to all of the other instances.Formally, collective classification takes a graph,made up of nodes V = {V1, .
.
.
, Vn} and edgesE.
The task is to label the nodes Vi ?
V froma label set L = {L1, .
.
.
, Lq}, making use of thegraph in the form of a neighborhood function N ={N1, .
.
.
, Nn}, where Ni ?
V \ {Vi}.2.2 The ConVote CorpusConVote, compiled by Thomas et al (2006), is acorpus of U.S. congressional debate transcripts.
Itconsists of 3,857 speeches organized into 53 debateson specific pieces of legislation.
Each speech istagged with the identity of the speaker and a ?for?or ?against?
label derived from congressional votingrecords.
In addition, places where one speaker citesanother have been annotated, as shown in Figure 1.We apply collective classification to ConVote de-bates by letting V refer to the individual speakers in adebate and populatingN using the citation graph be-tween speakers.
We set L = {y, n}, correspondingto ?for?
and ?against?
votes respectively.
The textof each instance is the concatenation of the speechesby a speaker within a debate.
This results in a corpusof 1,699 instances with a roughly even class distri-bution.
Approximately 70% of these are connected,i.e.
they are the source or target of one or more cita-tions.
The remainder are isolated.3 Collective Classification TechniquesIn this section we describe techniques for perform-ing collective classification on the ConVote cor-pus.
We differentiate between dual-classifier anditerative-classifier approaches.Dual-classifier approach: This approach usesa collective classification algorithm that takes inputsfrom two classifiers: (1) a content-only classifier thatdetermines the likelihood of a y or n label for an in-stance given its text content; and (2) a citation clas-sifier that determines, based on citation information,whether a given pair of instances are ?same class?
or?different class?.Let ?
denote a set of functions representing theclassification preferences produced by the content-only and citation classifiers:?
For each Vi ?
V , ?i ?
?
is a function ?i: L ?R+ ?
{0}.?
For each (Vi, Vj) ?
E, ?ij ?
?
is a function?ij : L ?
L ?
R+ ?
{0}.Later in this section we will describe three collec-tive classification algorithms capable of performingoverall classification based on these inputs: (1) theminimum-cut approach, which is the benchmark forcollective classification with ConVote, establishedby Thomas et al; (2) loopy belief propagation; and(3) mean-field.
We will show that these latter twotechniques, which are both approximate solutionsfor Markov random fields, are superior to minimum-cut for the task.Figure 2 gives a visual overview of the dual-classifier approach.Iterative-classifier approach: This approachincorporates content-only and citation features intoa single local classifier that works on the assump-tion that correct neighbor labels are already known.This approach represents a marked deviation fromthe dual-classifier approach and offers unique ad-vantages.
It is fully described in Section 3.4.Figure 3 gives a visual overview of the iterative-classifier approach.For a detailed introduction to collective classifica-tion see Sen et al (2008).1507Debate 006Speaker 400378 [against]Mr. Speaker, .
.
.
all over Washington and in the country, people are talking today about themajority?s last-minute decision to abandon .
.
.. .
.Speaker 400115 [for].
.
.Mr.
Speaker, .
.
.
I just want to say to the gentlewoman from New York that every single memberof this institution .
.
.. .
.Figure 1: Sample speech fragments from the ConVote corpus.
The phrase gentlewoman from New York by speaker400115 is annotated as a reference to speaker 400378.Debate contentCitation vectorsContent-only vectorsContent-only classifications Citation classificationsContent-only andcitation scoresOverall classificationsExtract features Extract featuresSVM SVMNormaliseNormaliseMF/LBP/MincutFigure 2: Dual-classifier approach.Debate contentContent-only vectorsContent-only classificationsLocal vectorsLocal classificationsOverall classificationsExtract featuresSVMCombine content-onlyand citation featuresSVMUpdate citation featuresTerminate iterationFigure 3: Iterative-classifier approach.3.1 Dual-classifier Approach withMinimum-cutThomas et al use linear kernel SVMs as their baseclassifiers.
The content-only classifier is trained topredict y or n based on the unigram presence fea-tures found in speeches.
The citation classifier istrained to predict ?same class?
or ?different class?labels based on the unigram presence features foundin the context windows (30 tokens before, 20 tokensafter) surrounding citations for each pair of speakersin the debate.The decision plane distance computed by thecontent-only SVM is normalized to a positive realnumber and stripped of outliers:?i(y) =????
?1 di > 2?i;(1 + di2?i)/2 |di| ?
2?i;0 di < ?2?iwhere ?i is the standard deviation of the decisionplane distance, di, over all of the instances in thedebate and ?i(n) = 1??i(y).
The citation classifieroutput is processed similarly:1?ij(y, y) =??
?0 dij < ?;?
?
dij/4?ij ?
?
dij ?
4?ij ;?
dij > 4?ijwhere ?ij is the standard deviation of the decisionplane distance, dij over all of the citations in the de-bate and ?ij(n, n) = ?ij(y, y).
The ?
and ?
vari-ables are free parameters.A given class assignment v is assigned a cost thatis the sum of per-instance and per-pair class costsderived from the content-only and citation classifiersrespectively:c(v) =?Vi?V?i(v?i) +?
(Vi,Vj)?E:vi 6=vj?ij(vi, vi)where vi is the label of node Vi and v?i denotes thecomplement class of vi.1Thomas et al classify each citation context window sep-arately, so their ?
values are actually calculated in a slightlymore complicated way.
We adopted the present approach forconceptual simplicity and because it gave superior performancein preliminary experiments.1508The cost function is modeled in a flow graphwhere extra source and sink nodes represent the yand n labels respectively.
Each node in V is con-nected to the source and sink with capacities ?i(y)and ?i(n) respectively.
Pairs classified in the ?sameclass?
class are linked with capacities defined by ?.An exact optimum and corresponding overallclassification is efficiently computed by finding theminimum-cut of the flow graph (Blum and Chawla,2001).
The free parameters are tuned on a set ofheld-out data.Thomas et al demonstrate improvements overcontent-only classification, without attempting toshow that the approach does better than any alter-natives; the main appeal is the simplicity of the flowgraph model.
There are a number of theoretical lim-itations to the approach, which we now discuss.As Thomas et al point out, the model has no wayof representing the ?different class?
output from thecitation classifier and these citations must be dis-carded.
This, to us, is the most significant problemwith the model.
Inspection of the corpus shows thatapproximately 80% of citations indicate agreement,meaning that for the present task the impact of dis-carding this information may not be large.
However,the primary utility in collective approaches lies intheir ability to fill in gaps in information not pickedup by content-only classification.
All available linkinformation should be applied to this end, so weneed models capable of accepting both positive andnegative information.The normalization techniques used for convertingSVM outputs to graph weights are somewhat arbi-trary.
The use of standard deviations appears prob-lematic as, intuitively, the strength of a classificationshould be independent of its variance.
As a case inpoint, consider a set of instances in a debate all clas-sified as similarly weak positives by the SVM.
Useof ?i as defined above would lead to these being er-roneously assigned the maximum score because oftheir low variance.The minimum-cut approach places instances ineither the positive or negative class depending onwhich side of the cut they fall on.
This meansthat no measure of classification confidence is avail-able.
This extra information is useful at the veryleast to give a human user an idea of how much totrust the classification.
A measure of classificationconfidence may also be necessary for incorporationinto a broader system, e.g., a meta-classifier (An-dreevskaia and Bergler, 2008; Li and Zong, 2008).Tuning the ?
and ?
parameters is likely to becomea source of inaccuracy in cases where the tuning andtest debates have dissimilar link structures.
For ex-ample, if the tuning debates tend to have fewer, moreaccurate links the ?
parameter will be higher.
Thiswill not produce good results if the test debates havemore frequent, less accurate links.3.2 Heuristics for Improving Minimum-cutBansal et al (2008) offer preliminary work describ-ing additions to the Thomas et al minimum-cut ap-proach to incorporate ?different class?
citation clas-sifications.
They use post hoc adjustments of graphcapacities based on simple heuristics.
Two of thethree approaches they trial appear to offer perfor-mance improvements:The SetTo heuristic: This heuristic worksthrough E in order and tries to force Vi and Vj intodifferent classes for every ?different class?
(dij < 0)citation classifier output where i < j.
It does this byaltering the four relevant content-only preferences,?i(y), ?i(n), ?j(y), and ?j(n).
Assume withoutloss of generality that the largest of these values is?i(y).
If this preference is respected, it follows thatVj should be put into class n. Bansal et al instanti-ate this chain of reasoning by setting:?
?
?i(y) = max(?, ?i(y))?
?
?j(n) = max(?, ?j(n))where ??
is the replacement content-only function,?
is a free parameter ?
(.5, 1], ?
?i(n) = 1 ?
?
?i(y),and ?
?j(y) = 1?
?
?j(y).The IncBy heuristic: This heuristic is a moreconservative version of the SetTo heuristic.
Insteadof replacing the content-only preferences with fixedconstants, it increments and decrements the previousvalues so they are somewhat preserved:?
?
?i(y) = min(1, ?i(y) + ?)?
?
?j(n) = min(1, ?j(n) + ?
)There are theoretical shortcomings with these ap-proaches.
The most obvious problem is the arbitrarynature of the manipulations, which produce a flow1509graph that has an indistinct relationship to the out-puts of the two classifiers.Bensal et al trial a range of ?
values, with vary-ing impacts on performance.
No attempt is made todemonstrate a method for choosing a good ?
value.It is not clear that the tuning approach used to set ?and ?
would be successful here.
In any case, havinga third parameter to tune would make the processmore time-consuming and increase the risks of in-correct tuning, described above.As Bansal et al point out, proceeding through Ein order means that earlier changes may be undonefor speakers who have multiple ?different class?
ci-tations.Finally, we note that the confidence of the cita-tion classifier is not embodied in the graph structure.The most marginal ?different class?
citation, classi-fied just on the negative side of the decision plane, istreated identically to the most confident one furthestfrom the decision plane.3.3 Dual-classifier Approach with MarkovRandom Field ApproximationsA pairwise Markov random field (Taskar et al,2002) is given by the pair (G,?
), where G and ?are as previously defined, ?
being re-termed as a setof clique potentials.
Given an assignment v to thenodes V , the pairwise Markov random field is asso-ciated with the probability distribution:P (v) =1Z?Vi?V?i(vi)?
(Vi,Vj)?E?ij(vi, vj)where:Z =?v??Vi?V?i(v?i)?
(Vi,Vj)?E?ij(v?i, v?j)and v?i denotes the label of Vi for an alternative as-signment in v?.In general, exact inference over a pairwiseMarkov random field is known to be NP-hard.
Thereare certain conditions under which exact inferenceis tractable, but real-world data is not guaranteed tosatisfy these.
A class of approximate inference al-gorithms known as variational methods (Jordan etal., 1999) solve this problem by substituting a sim-pler ?trial?
distribution which is fitted to the Markovrandom field distribution.Loopy Belief Propagation: Applied to a pair-wise Markov random field, loopy belief propagationis a message passing algorithm that can be conciselyexpressed as the following set of equations:mi?j(vj) = ?
?vi?L{?ij(vi, vj)?i(vi)?Vk?Ni?V\Vjmk?i(vi),?vj ?
L}bi(vi) = ?
?i(vi)?Vj?Ni?Vmj?i(vi),?vi ?
Lwhere mi?j is a message sent by Vi to Vj and ?
isa normalization constant that ensures that each mes-sage and each set of marginal probabilities sum to 1.The algorithm proceeds by making each node com-municate with its neighbors until the messages sta-bilize.
The marginal probability is then derived bycalculating bi(vi).Mean-Field: The basic mean-field algorithm canbe described with the equation:bj(vj) = ?
?j(vj)?Vi?Nj?V?vi?L?bi(vi)ij (vi, vj), vj ?
Lwhere ?
is a normalization constant that ensures?vjbj(vj) = 1.
The algorithm computes the fixedpoint equation for every node and continues to do sountil the marginal probabilities bj(vj) stabilize.Mean-field can be shown to be a variationalmethod in the same way as loopy belief propagation,using a simpler trial distribution.
For details see Senet al (2008).Probabilistic SVM Normalisation: Unlikeminimum-cut, the Markov random field approacheshave inherent support for the ?different class?
out-put of the citation classifier.
This allows us to ap-ply a more principled SVM normalisation technique.Platt (1999) describes a technique for converting theoutput of an SVM classifier to a calibrated posteriorprobability.
Platt finds that the posterior can be fitusing a parametric form of a sigmoid:P (y = 1|d) =11 + exp(Ad+B)This is equivalent to assuming that the output ofthe SVM is proportional to the log odds of a positiveexample.
Experimental analysis shows error rate is1510improved over a plain linear SVM and probabilitiesare of comparable quality to those produced using aregularized likelihood kernel method.By applying this technique to the base classifiers,we can produce new, simpler ?
functions, ?i(y) =Pi and ?ij(y, y) = Pij where Pi is the probabilis-tic normalized output of the content-only classifierand Pij is the probabilistic normalized output of thecitation classifier.This approach addresses the problems with theThomas et al method where the use of standarddeviations can produce skewed normalizations (seeSection 3.1).
By using probabilities we also openup the possibility of replacing the SVM classifierswith any other model than can be made to producea probability.
Note also that there are no parametersto tune.3.4 Iterative Classifier ApproachThe dual-classifier approaches described above rep-resent global attempts to solve the collective classifi-cation problem.
We can choose to narrow our focusto the local level, in which we aim to produce thebest classification for a single instance with the as-sumption that all other parts of the problem (i.e.
thecorrect labeling of the other instances) are solved.The Iterative Classification Algorithm (Bilgic etal., 2007), defined in Algorithm 1, is a simple tech-nique for performing collective classification usingsuch a local classifier.
After bootstrapping with acontent-only classifier, it repeatedly generates newestimates for vi based on its current knowledge ofNi.
The algorithm terminates when the predictionsstabilize or a fixed number of iterations is com-pleted.
Each iteration is completed using a newlygenerated ordering O, over the instances V .We propose three feature models for the localclassifier.Citation presence and Citation count: Giventhat the majority of citations represent the ?sameclass?
relationship (see Section 3.1), we can an-ticipate that content-only classification performancewill be improved if we add features to represent thepresence of neighbours of each class.We define the function c(i, l) =?vj?Ni?V?vj ,lgiving the number of neighbors for node Vi with la-bel l, where ?
is the Kronecker delta.
We incorporatethese citation count values, one for the supportingAlgorithm 1 Iterative Classification Algorithmfor each node Vi ?
V do {bootstrapping}compute ~ai using only local attributes of nodevi ?
f(~ai)end forrepeat {iterative classification}randomly generate ordering O over nodes in Vfor each node Vi ?
O do{compute new estimate of vi}compute ~ai using current assignments to Nivi ?
f(~ai)end foruntil labels have stabilized or maximum iterationsreachedclass and one for the opposing class, obtaining a newfeature vector (u1i , u2i , .
.
.
, uji , c(i, y), c(i, n)) whereu1i , u2i , .
.
.
, uji are the elements of ~ui, the binary un-igram feature vector used by the content-only clas-sifier to represent instance i.Alternatively, we can represent neighbor labelsusing binary citation presence values where anynon-zero count becomes a 1 in the feature vector.Context window: We can adopt a more nu-anced model for citation information if we incor-porate the citation context window features into thefeature vector.
This is, in effect, a synthesis ofthe content-only and citation feature models.
Con-text window features come from the product spaceL ?
C, where C is the set of unigrams used in ci-tation context windows and ~ci denotes the contextwindow features for instance i.
The new feature vec-tor becomes: (u1i , u2i , .
.
.
, uji , c1i , c2i , .
.
.
, cki ).
Thisapproach implements the intuition that speakers in-dicate their voting intentions by the words they useto refer to speakers whose vote is known.
Becauseneighbor relations are bi-directional the reverse isalso true: Speakers indicate other speakers?
votingintentions by the words they use to refer to them.As an example, consider the context window fea-ture AGREE-FOR, indicating the presence of theagree unigram in the citation window I agree withthe gentleman from Louisiana, where the label forthe gentleman from Louisiana instance is y. Thisfeature will be correctly correlated with the y label.Similarly, if the unigram were disagree the featurewould be correlated with the n label.15114 ExperimentsIn this section we compare the performance of ourdual-classifier and iterative-classifier approaches.We also evaluate the performance of the three fea-ture models for local classification.All accuracies are given as the percentages ofinstances correctly classified.
Results are macro-averaged using 10 ?
10-fold cross validation, i.e.10 runs of 10-fold cross validation using differentrandomly assigned data splits.Where quoted, statistical significance has beencalculated using a two-tailed paired t-test measuredover all 100 pairs with 10 degrees of freedom.
SeeBouckaert (2003) for an experimental justificationfor this approach.Note that the results presented in this sectionare not directly comparable with those reported byThomas et al and Bansal et al because their exper-iments do not use cross-validation.
See Section 4.3for further discussion of experimental configuration.4.1 Local ClassificationWe evaluate three models for local classification: ci-tation presence features, citation count features andcontext window features.
In each case the SVMclassifier is given feature vectors with both content-only and citation information, as described in Sec-tion 3.4.Table 1 shows that context window performs thebest with 89.66% accuracy, approximately 1.5%ahead of citation count and 3.5% ahead of citationpresence.
All three classifiers significantly improveon the content-only classifier.These relative scores seem reasonable.
Knowingthe words used in citations of each class is betterthan knowing the number of citations in each class,and better still than only knowing which classes ofcitations exist.These results represent an upper-bound for theperformance of the iterative classifier, which re-lies on iteration to produce the reliable informationabout citations given here by oracle.4.2 Collective ClassificationTable 2 shows overall results for the three collectiveclassification algorithms.
The iterative classifier wasrun separately with citation count and context win-Method Accuracy (%)Majority 52.46Content-only 75.29Citation presence 85.01Citation count 88.18Context window 89.66Table 1: Local classifier accuracy.
All three localclassifiers are significant over the in-isolation classifier(p < .001).dow citation features, the two best performing localclassification methods, both with a threshold of 30iterations.Results are shown for connected instances, iso-lated instances, and all instances.
Collective clas-sification techniques can only have an impact onconnected instances, so these figures are most im-portant.
The figures for all instances show the per-formance of the classifiers in our real-world task,where both connected and isolated instances need tobe classified and the end-user may not distinguishbetween the two types.Each of the four collective classifiers outperformthe minimum-cut benchmark over connected in-stances, with the iterative classifier (context win-dow) (79.05%) producing the smallest gain of lessthan 1% and mean-field doing best with a nearly6% gain (84.13%).
All show a statistically signif-icant improvement over the content-only classifier.Mean-field shows a statistically significant improve-ment over minimum-cut.The dual-classifier approaches based on loopybelief propagation and mean-field do better thanthe iterative-classifier approaches by an average ofabout 3%.Iterative classification performs slightly betterwith citation count features than with context win-dow features, despite the fact that the context win-dow model performs better in the local classifierevaluation.
We speculate that this may be due to ci-tation count performing better when given incorrectneighbor labels.
This is an aspect of local classi-fier performance we do not otherwise measure, so aclear conclusion is not possible.
Given the closenessof the results it is also possible that natural statisticalvariation is the cause of the difference.1512The performance of the minimum-cut method isnot reliably enhanced by either the SetTo or IncByheuristics.
Only IncBy(.15) gives a very small im-provement (0.14%) over plain minimum-cut.
Allof the other combinations tried diminished perfor-mance slightly.4.3 A Note on Error Propagation andExperimental ConfigurationEarly in our experimental work we noticed that per-formance often varied greatly depending on the de-bates that were allocated to training, tuning and test-ing.
This observation is supported by the per-foldscores that are the basis for the macro-average per-formance figures reported in Table 2, which tendto have large standard deviations.
The absolutestandard deviations over the 100 evaluations for theminimum-cut and mean-field methods were 11.19%and 8.94% respectively.
These were significantlylarger than the standard deviation for the content-only baseline, which was 7.34%.
This leads us toconclude that the performance of collective classifi-cation methods is highly variable.Bilgic and Getoor (2008) offer a possible expla-nation for this.
They note that the cost of incor-rectly classifying a given instance can be magnifiedin collective classification, because errors are prop-agated throughout the network.
The extent to whichthis happens may depend on the random interactionbetween base classification accuracy and networkstructure.
There is scope for further work to morefully explain this phenomenon.From these statistical and theoretical factors weinfer that more reliable conclusions can be drawnfrom collective classification experiments that usecross-validation instead of a single, fixed data split.5 Related workSomasundaran et al (2009) use ICA to improve sen-timent polarity classification of dialogue acts in acorpus of multi-party meeting transcripts.
Link fea-tures are derived from annotations giving frame re-lations and target relations.
Respectively, these re-late dialogue acts based on the sentiment expressedand the object towards which the sentiment is ex-pressed.
Somasundaran et al provides another ar-gument for the usefulness of collective classification(specifically ICA), in this case as applied at a dia-logue act level and relying on a complex system ofannotations for link information.Somasundaran and Wiebe (2009) propose an un-supervised method for classifying the stance of eachcontribution to an online debate concerning the mer-its of competing products.
Concessions to otherstances are modeled, but there are no overt citationsin the data that could be used to induce the networkstructure required for collective classification.Pang and Lee (2005) use metric labeling to per-form multi-class collective classification of moviereviews.
Metric labeling is a multi-class equiva-lent of the minimum-cut technique in which opti-mization is done over a cost function incorporat-ing content-only and citation scores.
Links are con-structed between test instances and a set of k near-est neighbors drawn only from the training set.
Re-stricting the links in this way means the optimizationproblem is simple.
A similarity metric is used to findnearest neighbors.The Pang and Lee method is an instance of im-plicit link construction, an approach which is be-yond the scope of this paper but nevertheless an im-portant area for future research.
A similar techniqueis used in a variation on the Thomas et al experi-ment where additional links between speeches areinferred via a similarity metric (Burfoot, 2008).
Incases where both citation and similarity links arepresent, the overall link score is taken as the sum ofthe two scores.
This seems counter-intuitive, giventhat the two links are unlikely to be independent.
Inthe framework of this research, the approach wouldbe to train a link meta-classifier to take scores fromboth link classifiers and output an overall link prob-ability.Within NLP, the use of LBP has not been re-stricted to document classification.
Examples ofother applications are dependency parsing (Smithand Eisner, 2008) and alignment (Cromires andKurohashi, 2009).
Conditional random fields(CRFs) are an approach based on Markov randomfields that have been popular for segmenting andlabeling sequence data (Lafferty et al, 2001).
Werejected linear-chain CRFs as a candidate approachfor our evaluation on the grounds that the arbitrar-ily connected graphs used in collective classificationcan not be fully represented in graphical format, i.e.1513Connected Isolated AllMajority 52.46 46.29 50.51Content only 75.31 78.90 76.28Minimum-cut 78.31 78.90 78.40Minimum-cut (SetTo(.6)) 78.22 78.90 78.32Minimum-cut (SetTo(.8)) 78.01 78.90 78.14Minimum-cut (SetTo(1)) 77.71 78.90 77.93Minimum-cut (IncBy(.05)) 78.14 78.90 78.25Minimum-cut (IncBy(.15)) 78.45 78.90 78.46Minimum-cut (IncBy(.25)) 78.02 78.90 78.15Iterative-classifier (citation count) 80.07?
78.90 79.69?Iterative-classifier (context window) 79.05 78.90 78.93Loopy Belief Propagation 83.37?
78.90 81.93?Mean-Field 84.12?
78.90 82.45?Table 2: Speaker classification accuracies (%) over connected, isolated and all instances.
The marked results arestatistically significant over the content only benchmark (?
p < .01, ?
p < .001).
The mean-field results are statisticallysignificant over minimum-cut (p < .05).linear-chain CRFs do not scale to the complexity ofgraphs used in this research.6 Conclusions and future workBy applying alternative models, we have demon-strated the best recorded performance for collectiveclassification of ConVote using bag-of-words fea-tures, beating the previous benchmark by nearly 6%.Moreover, each of the three alternative approachestrialed are theoretically superior to the minimum-cutapproach approach for three main reasons: (1) theysupport multi-class classification; (2) they supportnegative and positive citations; (3) they require noparameter tuning.The superior performance of the dual-classifierapproach with loopy belief propagation and mean-field suggests that either algorithm could be consid-ered as a first choice for collective document classi-fication.
Their advantage is increased by their abil-ity to output classification confidences as probabili-ties, while minimum-cut and the local formulationsonly give absolute class assignments.
We do not dis-miss the iterative-classifier approach entirely.
Themost compelling point in its favor is its ability tounify content only and citation features in a singleclassifier.
Conceptually speaking, such an approachshould allow the two types of features to inter-relatein more nuanced ways.
A case in point comes fromour use of a fixed size context window to build acitation classifier.
Future approaches may be ableto do away with this arbitrary separation of featuresby training a local classifier to consider all words interms of their impact on content-only classificationand their relations to neighbors.Probabilistic SVM normalization offers a conve-nient, principled way of incorporating the outputs ofan SVM classifier into a collective classifier.
An op-portunity for future work is to consider normaliza-tion approaches for other classifiers.
For example,confidence-weighted linear classifiers (Dredze et al,2008) have been shown to give superior performanceto SVMs on a range of tasks and may therefore be abetter choice for collective document classification.Of the three models trialled for local classifiers,context window features did best when measured inan oracle experiment, but citation count features didbetter when used in a collective classifier.
We con-clude that context window features are a more nu-anced and powerful approach that is also more likelyto suffer from data sparseness.
Citation count fea-tures would have been the less effective in a scenariowhere the fact of the citation existing was less infor-mative, for example, if a citation was 50% likely toindicate agreement rather than 80% likely.
There ismuch scope for further research in this area.1514ReferencesAlina Andreevskaia and Sabine Bergler.
2008.
Whenspecialists and generalists work together: Overcom-ing domain dependence in sentiment tagging.
In ACL,pages 290?298.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.
Thepower of negative thinking: Exploiting label disagree-ment in the min-cut classification framework.
In COL-ING, pages 15?18.Mustafa Bilgic and Lise Getoor.
2008.
Effective labelacquisition for collective classification.
In KDD, pages43?51.Mustafa Bilgic, Galileo Namata, and Lise Getoor.
2007.Combining collective classification and link predic-tion.
In ICDM Workshops, pages 381?386.
IEEEComputer Society.Avrim Blum and Shuchi Chawla.
2001.
Learning fromlabeled and unlabeled data using graph mincuts.
InICML, pages 19?26.Remco R. Bouckaert.
2003.
Choosing between twolearning algorithms based on calibrated tests.
InICML, pages 51?58.Clint Burfoot and Timothy Baldwin.
2009.
Automaticsatire detection: Are you having a laugh?
In ACL-IJCNLP Short Papers, pages 161?164.Clint Burfoot.
2008.
Using multiple sources of agree-ment information for sentiment classification of polit-ical transcripts.
In Australasian Language TechnologyAssociation Workshop 2008, pages 11?18.
ALTA.Minh Duc Cao and Xiaoying Gao.
2005.
Combiningcontents and citations for scientific document classifi-cation.
In 18th Australian Joint Conference on Artifi-cial Intelligence, pages 143?152.Fabien Cromires and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In EACL, pages166?174.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
InICML, pages 264?271.Zeno Gantner and Lars Schmidt-Thieme.
2009.
Auto-matic content-based categorization of Wikipedia ar-ticles.
In 2009 Workshop on The People?s WebMeets NLP: Collaboratively Constructed SemanticResources, pages 32?37.Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola,Lawrence Saul, and David Heckerman.
1999.
An in-troduction to variational methods for graphical mod-els.
Machine Learning, 37:183?233.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Shoushan Li and Chengqing Zong.
2008.
Multi-domainsentiment classification.
In ACL, pages 257?260.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In ACL, pages 115?124.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification using ma-chine learning techniques.
In EMNLP, pages 79?86.John C. Platt.
1999.
Probabilistic outputs for supportvector machines and comparisons to regularized likeli-hood methods.
In A. Smola, P. Bartlett, B. Scholkopf,and D. Schuurmans, editors, Advances in Large Mar-gin Classifiers, pages 61?74.
MIT Press.Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.2008.
Collective classification in network data.
AIMagazine, 29:93?106.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP, pages 145?156.Swapna Somasundaran and Janyce Wiebe.
2009.
Rec-ognizing stances in online debates.
In ACL-IJCNLP,pages 226?234.Swapna Somasundaran, Galileo Namata, Janyce Wiebe,and Lise Getoor.
2009.
Supervised and unsupervisedmethods in employing discourse relations for improv-ing opinion polarity classification.
In EMNLP, pages170?179.Ben Taskar, Pieter Abbeel, and Daphne Koller.
2002.Discriminative probabilistic models for relational data.In UAI.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In EMNLP, pages327?335.Yiming Yang and Xin Liu.
1999.
A re-examination oftext categorization methods.
In Proceedings ACM SI-GIR, pages 42?49.1515
