Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1445?1455,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCommonsense Knowledge Base CompletionXiang Li?
?Aynaz Taheri?Lifu Tu?Kevin Gimpel?
?University of Chicago, Chicago, IL, 60637, USA?University of Illinois at Chicago, Chicago, IL, 60607, USA?Toyota Technological Institute at Chicago, Chicago, IL, 60637, USAlix1@uchicago.edu, ataher2@uic.edu, {lifu,kgimpel}@ttic.eduAbstractWe enrich a curated resource of common-sense knowledge by formulating the prob-lem as one of knowledge base comple-tion (KBC).
Most work in KBC focuseson knowledge bases like Freebase that re-late entities drawn from a fixed set.
How-ever, the tuples in ConceptNet (Speer andHavasi, 2012) define relations between anunbounded set of phrases.
We developneural network models for scoring tupleson arbitrary phrases and evaluate them bytheir ability to distinguish true held-outtuples from false ones.
We find strongperformance from a bilinear model usinga simple additive architecture to modelphrases.
We manually evaluate our trainedmodel?s ability to assign quality scores tonovel tuples, finding that it can propose tu-ples at the same quality level as medium-confidence tuples from ConceptNet.1 IntroductionMany ambiguities in natural language process-ing (NLP) can be resolved by using knowledgeof various forms.
Our focus is on the type ofknowledge that is often referred to as ?common-sense?
or ?background?
knowledge.
This knowl-edge is rarely expressed explicitly in textual cor-pora (Gordon and Van Durme, 2013).
Some re-searchers have developed techniques for inferringthis knowledge from patterns in raw text (Gor-don, 2014; Angeli and Manning, 2014), while oth-ers have developed curated resources of common-sense knowledge via manual annotation (Lenatand Guha, 1989; Speer and Havasi, 2012) orgames with a purpose (von Ahn et al, 2006).Curated resources typically have high preci-sion but suffer from a lack of coverage.
For cer-relation right term conf.MOTIVATEDBYGOAL relax 3.3USEDFOR relaxation 2.6MOTIVATEDBYGOAL your muscle be sore 2.3HASPREREQUISITE go to spa 2.0CAUSES get pruny skin 1.6HASPREREQUISITE change into swim suit 1.6Table 1: ConceptNet tuples with left term ?soak inhotspring?
; final column is confidence score.tain resources, researchers have developed meth-ods to automatically increase coverage by infer-ring missing entries.
These methods are com-monly categorized under the heading of knowl-edge base completion (KBC).
KBC is widely-studied for knowledge bases like Freebase (Bol-lacker et al, 2008) which contain large sets of enti-ties and relations among them (Mintz et al, 2009;Nickel et al, 2011; Riedel et al, 2013; West etal., 2014), including recent work using neural net-works (Socher et al, 2013; Yang et al, 2014).We improve the coverage of commonsense re-sources by formulating the problem as one ofknowledge base completion.
We focus on a par-ticular curated commonsense resource called Con-ceptNet (Speer and Havasi, 2012).
ConceptNetcontains tuples consisting of a left term, a rela-tion, and a right term.
The relations come froma fixed set.
While terms in Freebase tuples are en-tities, ConceptNet terms can be arbitrary phrases.Some examples are shown in Table 1.
An NLP ap-plication may wish to query ConceptNet for infor-mation about soaking in a hotspring, but may usedifferent words from those contained in the Con-ceptNet tuples.
Our goal is to do on-the-fly knowl-edge base completion so that queries can be an-swered robustly without requiring the precise lin-guistic forms contained in ConceptNet.To do this, we develop neural network mod-els to embed terms and provide scores to arbi-1445trary tuples.
We train them on ConceptNet tuplesand evaluate them by their ability to distinguishtrue and false held-out tuples.
We consider sev-eral functional architectures, comparing two com-position functions for embedding terms and twofunctions for converting term embeddings into tu-ple scores.
We find that all architectures are ableto outperform several baselines and reach similarperformance on classifying held-out tuples.We also experiment with several training ob-jectives for KBC, finding that a simple cross en-tropy objective with randomly-generated negativeexamples performs best while also being fastest.We manually evaluate our trained model?s abil-ity to assign quality scores to novel tuples, find-ing that it can propose tuples at the same qual-ity level as medium-confidence tuples from Con-ceptNet.
We release all of our resources, includ-ing our ConceptNet KBC task data, large sets ofrandomly-generated tuples scored with our model,training code, and pretrained models with code forcalculating the confidence of novel tuples.12 Related WorkOur methods are similar to past work onKBC (Mintz et al, 2009; Nickel et al, 2011; Laoet al, 2011; Nickel et al, 2012; Riedel et al, 2013;Gardner et al, 2014; West et al, 2014), particu-larly methods based on distributed representationsand neural networks (Socher et al, 2013; Bordeset al, 2013; Bordes et al, 2014a; Bordes et al,2014b; Yang et al, 2014; Neelakantan et al, 2015;Gu et al, 2015; Toutanova et al, 2015).
Most priorwork predicts new relational links between termsdrawn from a fixed set.
In a notable exception,Neelakantan and Chang (2015) add new entitiesto KBs using external resources along with prop-erties of the KB itself.
Relatedly, Yao et al (2013)induce an unbounded set of entity categories andassociate them with entities in KBs.Several researchers have developed techniquesfor discovering commonsense knowledge fromtext (Gordon et al, 2010; Gordon and Schu-bert, 2012; Gordon, 2014; Angeli and Manning,2014).
Open information extraction systems likeREVERB (Fader et al, 2011) and NELL (Carl-son et al, 2010) find tuples with arbitrary termsand relations from raw text.
In contrast, we startwith a set of commonsense facts to use for train-1Available at http://ttic.uchicago.edu/?kgimpel/commonsense.html.ing, though our methods could be applied to theoutput of these or other extraction systems.Our goals are similar to those of the Analogy-Space method (Speer et al, 2008), which uses ma-trix factorization to improve coverage of Concept-Net.
However, AnalogySpace can only return aconfidence score for a pair of terms drawn fromthe training set.
Our models can assign scores totuples that contain novel terms (as long as theyconsist of words in our vocabulary).Though we use ConceptNet, similar techniquescan be applied to other curated resources likeWordNet (Miller, 1995) and FrameNet (Baker etal., 1998).
For WordNet, tuples can contain lexi-cal entries that are linked via synset relations (e.g.,?hypernym?).
WordNet contains many multi-word entries (e.g., ?cold sweat?
), which can bemodeled compositionally by our term models; al-ternatively, entire glosses could be used as terms.To expand frame relationships in FrameNet, tuplescan draw relations from the frame relation types(e.g., ?is causative of?)
and terms can be framelexical units or their definitions.Several researchers have used commonsenseknowledge to improve language technologies, in-cluding sentiment analysis (Cambria et al, 2012;Agarwal et al, 2015), semantic similarity (Caro etal., 2015), and speech recognition (Lieberman etal., 2005).
Our hope is that our models can en-able many other NLP applications to benefit fromcommonsense knowledge.Our work is most similar to that of Angeli andManning (2013).
They also developed methodsto assess the plausibility of new facts based ona training set of facts, considering commonsensedata from ConceptNet in one of their settings.Like us, they can handle an unbounded set of termsby using (simple) composition functions for novelterms, which is rare among work in KBC.
One keydifference is that their best method requires iterat-ing over the KB at test time, which can be com-putationally expensive with large KBs.
Our mod-els do not require iterating over the training set.We compare to several baselines inspired by theirwork, and we additionally evaluate our model?sability to score novel tuples derived from bothConceptNet and Wikipedia.3 ModelsOur goal is to represent commonsense knowledgesuch that it can be used for NLP tasks.
We as-1446sume this knowledge is given in the form of tuples?t1, R, t2?, where t1is the left term, t2is the rightterm, and R is a (directed) relation that exists be-tween the terms.
Examples are shown in Table 1.2Given a set of tuples, our goal is to develop aparametric model that can provide a confidencescore for new, unseen tuples.
That is, we wantto design and train models that define a functionscore(t1, R, t2) that provides a quality score foran arbitrary tuple ?t1, R, t2?.
These models will beevaluated by their ability to distinguish true held-out tuples from false ones.We describe two model families for scoring tu-ples.
We assume that we have embeddings forwords and define models that use these word em-beddings to score tuples.
So our models are lim-ited to tuples in which terms consist of wordsin the word embedding vocabulary, though futurework could consider character-based architecturesfor open-vocabulary modeling (Huang et al, 2013;Ling et al, 2015).3.1 Bilinear ModelsWe first consider bilinear models, since they havebeen found useful for KBC in past work (Nickelet al, 2011; Jenatton et al, 2012; Garc?
?a-Dur?an etal., 2014; Yang et al, 2014).
A bilinear model hasthe following form for a tuple ?t1, R, t2?
:v>1MRv2where v1?
Rris the (column) vector representingt1, v2?
Rris the vector for t2, and MR?
Rr?ris the parameter matrix for relation R.To convert terms t1and t2into term vectors v1and v2, we consider two possibilities: word aver-aging and a bidirectional long short-term memory(LSTM) recurrent neural network (Hochreiter andSchmidhuber, 1997).
This provides us with twomodels: Bilinear AVG and Bilinear LSTM.One downside of this architecture is that as thelength of the term vectors grows, the size of the re-lation matrices grows quadratically.
This can slowdown training while requiring more data to learnthe large numbers of parameters in the matrices.To address this, we include an additional nonlin-ear transformation of each term:ui= a(W(B)vi+ b(B))2These examples are from the Open Mind Common Sense(OMCS) part of ConceptNet version 5 (Speer and Havasi,2012).
In our experiments below, we only use OMCS tuples.where a is a nonlinear activation function (tunedamong ReLU, tanh, and logistic sigmoid) andwhere we have introduced additional parametersW(B)and b(B).
This gives us the following model:scorebilinear(t1, R, t2) = u>1MRu2When using the LSTM, we tune the decision abouthow to produce the final term vectors to pass to thebilinear model, including possibly using the finalvectors from each direction and the output of maxor average pooling.
We use the same LSTM pa-rameters for each term.3.2 Deep Neural Network ModelsOur second family of models is based on deep neu-ral networks (DNNs).
While bilinear models havebeen shown to work well for KBC, their functionalform makes restrictions about how terms can inter-act.
DNNs make no such restrictions.As above, we define two models, one based onusing word averaging for the term model (DNNAVG) and one based on LSTMs (DNN LSTM).For the DNN AVG model, we obtain the term vec-tors v1and v2by averaging word vectors in the re-spective terms.
We then concatenate v1, v2, and arelation vector vRto form the input of the DNN,denoted vin.
The DNN uses a single hidden layer:u = a(W(D1)vin+ b(D1))scoreDNN(t1, R, t2) = W(D2)u+ b(D2)(1)where a is again a (tuned) nonlinear activationfunction.
The size of the hidden vector u istuned, but the output dimensionality (the numbersof rows in W(D2)and b(D2)) is fixed to 1.
We donot use a nonlinear activation for the final layersince our goal is to output a scalar score.For the DNN LSTM model, we first create asingle vector for the two terms using an LSTM.That is, we concatenate t1, a delimiter token, andt2to create a single word sequence.
We use a bidi-rectional LSTM to convert this word sequence toa vector, again possibly using pooling (the deci-sion is tuned; details below).
We concatenate theoutput of this bidirectional LSTM with the rela-tion vector vrto create the DNN input vector vin,then use Eq.
1 to obtain a score.
We found thisto work better than separately using an LSTM oneach term.
We can not try this for the BilinearLSTM model since its functional form separatesthe two term vectors.1447The relation vectors vRare learned in additionto the DNN parameters W(D1), W(D2), b(D1),b(D2), and the LSTM parameters (in the case ofthe DNN LSTM model).
Also, word embeddingparameters are updated in all settings.4 TrainingGiven a tuple training set T , we train our modelsusing two different loss functions: hinge loss anda binary cross entropy function.
Both rely on waysof generating negative examples (Section 4.3).Both also use regularization (Section 4.4).4.1 Hinge LossGiven a training tuple ?
= ?t1, R, t2?, the hingeloss seeks to make the score of ?
larger than thescore of negative examples by a margin of at least?.
This corresponds to minimizing the followingloss, summed over all examples ?
?
T :losshinge(?)
=max{0, ?
?
score(?)
+ score(?neg(t1))}+ max{0, ?
?
score(?)
+ score(?neg(R))}+ max{0, ?
?
score(?)
+ score(?neg(t2))}where ?neg(t1)is the negative example obtained byreplacing t1in ?
with some other t1, and ?neg(R)and ?neg(t2)are defined analogously for the rela-tion and right term.
We describe how we generatethese negative examples in Section 4.3 below.4.2 Binary Cross EntropyThough we only have true tuples in our trainingset, we can create a binary classification problemby assigning a label of 1 to training tuples and alabel of 0 to negative examples.
Then we can min-imize cross entropy (CE) as is common when us-ing neural networks for classification.
To generatenegative examples, we consider the methods de-scribed in Section 4.3 below.
We also need to con-vert our models?
scores into probabilities, whichwe do by using a logistic sigmoid ?
on score.
Wedenote the label as `, where the label is 1 if the tu-ple is from the training set and 0 if it is a negativeexample.
Then the loss is defined:lossCE(?, `) =?` log ?(score(?))?
(1?`) log(1?
?(score(?
)))When using this loss, we generate three negativeexamples for each positive example (one for swap-ping each component of the tuple, as in the hingeloss).
For a mini-batch of size ?, there are ?
pos-itive examples and 3?
negative examples used fortraining.
The loss is summed over these 4?
exam-ples yielded by each mini-batch.4.3 Negative ExamplesFor the loss functions above, we need ways of au-tomatically generating negative examples.
For ef-ficiency, we consider using the current mini-batchonly, as our models are trained using optimiza-tion on mini-batches.
We consider the follow-ing three strategies to construct negative examples.Each strategy constructs three negative examplesfor each positive example ?
: one by replacing t1,one by replacing R, and one by replacing t2.Random sampling.
We create the three negativeexamples for ?
by replacing each component withits counterpart in a randomly-chosen tuple in thesame mini-batch.Max sampling.
We create the three negative ex-amples for ?
by replacing each component withits counterpart in some other tuple in the mini-batch, choosing the substitution to maximize thescore of the resulting negative example.
For ex-ample, when swapping out t1in ?
= ?t1, R, t2?,we choose the substitution t?1as follows:t?1= argmaxt:?t,R?,t?2??
?\?score(t, R, t2)where ?
is the current mini-batch of tuples.
Weperform the analogous procedure for R and t2.Mix sampling.
This is a mixture of the above,using random sampling 50% of the time and maxsampling the remaining 50% of the time.4.4 RegularizationWe use L2regularization.
For the DNN models,we add the penalty term ???
?2to the losses, where?
is the regularization coefficient and ?
contains allother parameters.
However, for the bilinear mod-els we regularize the relation matrices MRtowardthe identity matrix instead of all zeroes, adding thefollowing to the loss:?1??
?2+ ?2?R?MR?
Ir?22where Iris the r ?
r identity matrix, the summa-tion is performed over all relations R, and ?
repre-sents all other parameters.14485 Experimental SetupWe now evaluate our tuple models.
We measurewhether our models can distinguish true and falsetuples by training a model on a large set of tuplesand testing on a held-out set.5.1 Task DesignThe tuples are obtained from the Open Mind Com-mon Sense (OMCS) entries in the ConceptNet5 dataset (Speer and Havasi, 2012).
They aresorted by a confidence score.
The most confident1200 tuples were reserved for creating our test set(TEST).
The next most confident 600 tuples (i.e.,those numbered 1201?1800) were used to build adevelopment set (DEV1) and the next most confi-dent 600 (those numbered 1801?2400) were usedto build a second development set (DEV2).For each set S (S ?
{DEV1, DEV2, TEST}), foreach tuple ?
?
S, we created a negative exampleand added it to S. So each set doubled in size.
Tocreate a negative example from ?
?
S, we ran-domly swapped one of the components of ?
withanother tuple ???
S. One third of the time weswapped t1in ?
for t1in ?
?, one third of the timewe swapped their R?s, and the remaining third ofthe time we swapped their t2?s.
Thus, distinguish-ing positive and negative examples in this task issimilar to the objectives optimized during training.Each of DEV1 and DEV2 has 1200 tuples (600positive examples and 600 negative examples),while TEST has 2400 tuples (1200 positive and1200 negative).
For training data, we selected100,000 tuples from the remaining tuples (num-bered 2401 and beyond).The task is to separate the true and false tuplesin our test set.
That is, the labels are 1 for truetuples and 0 for false tuples.
Given a model forscoring tuples, we select a threshold by maximiz-ing accuracy on DEV1 and report accuracies onDEV2.
This is akin to learning the bias featureweight (using DEV1) of a linear classifier that usesour model?s score as its only feature.
We tunedseveral choices?including word embeddings, hy-perparameter values, and training objectives?onDEV2 and report final performance on TEST.
Oneannotator (a native English speaker) attempted thesame classification task on a sample of 100 tu-ples from DEV2 and achieved an accuracy of 95%.We release these datasets to the community so thatothers can work on this same task.5.2 Word EmbeddingsOur tuple models rely on initial word embeddings.To help our models better capture the common-sense knowledge in ConceptNet, we generatedword embedding training data using the OMCSsentences underlying our training tuples (we ex-cluded the top 2400 tuples which were used forcreating DEV1, DEV2, and TEST).
We createdtraining data by merging the information in thetuples and their OMCS sentences.
Our goal wasto combine the grammatical context of the OMCSsentences with the words in the actual terms, so asto ensure that we learn embeddings for the wordsin the terms.
We also insert the relations into theOMCS sentences so that we can learn embeddingsfor the relations themselves.We describe the procedure by example and alsorelease our generated data for ease of replication.The tuple ?soak in a hotspring, CAUSES, get prunyskin?
was automatically extracted/normalized (bythe ConceptNet developers) from the OMCS sen-tence ?The effect of [soaking in a hotspring] is[getting pruny skin]?
where brackets surroundterms.
We replace the bracketed portions withtheir corresponding terms and insert the relationbetween them: ?The effect of soak in a hotspringCAUSES get pruny skin?.
We do this for all train-ing tuples.3We used the word2vec (Mikolov et al, 2013)toolkit to train skip-gram word embeddings onthis data.
We trained for 20 iterations, using adimensionality of 200 and a window size of 5.We refer to these as ?CN-trained?
embeddings forthe remainder of this paper.
Similar approacheshave been used to learn embeddings for partic-ular downstream tasks, e.g., dependency pars-ing (Bansal et al, 2014).
We use our CN-trainedembeddings within baseline methods and also pro-vide the initial word embeddings of our models.For all of our models, we update the initial wordembeddings during learning.In the baseline methods described below, wecompare our CN-trained embeddings to pretrainedword embeddings.
We use the GloVe (Penning-ton et al, 2014) embeddings trained on 840 bil-lion tokens of Common Crawl web text and thePARAGRAM-SimLex embeddings of Wieting et al(2015), which were tuned to have strong perfor-mance on the SimLex-999 task (Hill et al, 2015).3For reversed relations, indicated by an asterisk in theOMCS sentences, we swap t1and t2in the tuple.14495.3 BaselinesWe consider three baselines inspired by those ofAngeli and Manning (2013):?
Similar Fact Count (Count): For each tuple?
= ?t1, R, t2?
in the evaluation set, we countthe number of similar tuples in the training set.A training tuple ?
?= ?t?1, R?, t?2?
is considered?similar?
to ?
if R = R?, one of the termsmatches exactly, and the other term has the samehead word.
That is, (R = R?)
?
(t1= t?1) ?
(head(t2) = head(t?2)), or (R = R?)
?
(t2=t?2) ?
(head(t1) = head(t?1)).
The head wordfor a term was obtained by running the StanfordParser (Klein and Manning, 2003) on the term.This baseline does not use word embeddings.?
Argument Similarity (ArgSim): This baselinecomputes the cosine similarity of the vectors fort1and t2, ignoring the relation.
Vectors for t1and t2are obtained by word averaging.?
Max Similarity (MaxSim): For tuple ?
in anevaluation set, this baseline outputs the maxi-mum similarity between ?
and any tuple in thetraining set.
The similarity is computed by con-catenating the vectors for t1, R, and t2, thencomputing cosine similarity.
As in ArgSim,we obtain vectors for terms by averaging theirwords.
We only consider R when using ourCN-trained embeddings since they contain em-beddings for the relations.
When using GloVeand PARAGRAM embeddings for this baseline,we simply use the two term vectors (still con-structed via averaging the words in each term).We chose these baselines because they can all han-dle unbounded term sets but differ in their otherrequirements.
ArgSim and MaxSim use wordembeddings while Count does not.
Count andMaxSim require iterating over the training set dur-ing inference while ArgSim does not.For each baseline, we tuned a threshold onDEV1 to maximize classification accuracy thentested on DEV2 and TEST.5.4 Training and TuningWe used AdaGrad (Duchi et al, 2011) for opti-mization, training for 20 epochs through the train-ing tuples.
We separately tuned hyperparametersfor each model and training objective.
We tunedthe following hyperparameters: the relation ma-trix size r for the bilinear models (also the lengthof the transformed term vectors, denoted u1andu2above), the activation a, the hidden layer size gfor the DNN models, the relation vector length dfor the DNN models, the LSTM hidden vector sizeh for models with LSTMs, the mini-batch size ?,the regularization parameters ?, ?1, and ?2, andthe AdaGrad learning rate ?.All tuning used early stopping: periodicallyduring training, we used the current model tofind the optimal threshold on DEV1 and evaluatedon DEV2.
Due to computational limitations, wewere unable to perform thorough grid searches forall hyperparameters.
We combined limited gridsearches with greedy hyperparameter tuning basedon regions of values that were the most promising.For the Bilinear LSTM and DNN LSTM, wedid hyperparameter tuning by training on the fulltraining set of 100,000 tuples for 20 epochs, com-puting DEV2 accuracy once per epoch.
For the av-eraging models, we tuned by training on a subsetof 1000 tuples with ?
= 200 for 20 epochs; theaveraging models showed more stable results anddid not require the full training set for tuning.
Be-low are the tuned hyperparameter values:?
Bilinear AVG: for CE: r = 150, a = tanh,?
= 200, ?
= 0.01, ?1= ?2= 0.001.
Hingeloss: same values as above except ?
= 0.005.?
Bilinear LSTM: for CE: r = 50, a = ReLU,h = 200, ?
= 800, ?
= 0.02, and ?1= ?2=0.00001.
To obtain vectors from the term bidi-rectional LSTMs, we used max pooling.
Forhinge loss: r = 50, a = tanh, h = 200,?
= 400, ?
= 0.007, ?1= 0.00001, and?2= 0.01.
To obtain vectors from the termbidirectional LSTMs, we used the concatena-tion of average pooling and final hidden vectorsin each direction.
For each sampling methodand loss function, ?
was tuned by grid searchwith the others fixed to the above values.?
DNN AVG: for both losses: a = ReLU, d =200, g = 1000, ?
= 600, ?
= 0.01, ?
= 0.001.?
DNN LSTM: for both losses: a = ReLU, d =200, bidirectional LSTM hidden layer size h =200, hidden layer dimension g = 800, ?
= 400,?
= 0.005, and ?
= 0.00005.
To get vectorsfrom the term LSTMs, we used max pooling.6 ResultsWord Embedding Comparison.
We first evalu-ate the quality of our word embeddings trained onthe ConceptNet training tuples.
Table 2 compares1450GloVe PARAGRAM CN-trainedArgSim 68 69 73MaxSim 73 70 82Table 2: Accuracies (%) on DEV2 of two base-lines using three different sets of word embed-dings.
Our ConceptNet-trained embeddings out-perform GloVe and PARAGRAM embeddings.DNN AVG DNN LSTMCE hinge CE hingerandom 124 230 710 783mix 20755 21045 25928 26380max 39338 41867 49583 49427Table 4: Loss function runtime comparison (sec-onds per epoch) of the DNN models.accuracies on DEV2 for the two baselines thatuse embeddings: ArgSim and MaxSim.
We findthat pretrained GloVe and PARAGRAM embed-dings perform comparably, but both are outper-formed by our ConceptNet-trained embeddings.We use the latter for the remaining experiments inthis paper.Training Comparison.
Table 3 shows the re-sults of our models with the two loss functions andthree sampling strategies.
We find that the binarycross entropy loss with random sampling performsbest across models.
We note that our conclusiondiffers from some prior work that found max ormix sampling to be better than random (Wietinget al, 2016).
We suspect that this difference maystem from characteristics of the ConceptNet train-ing data.
It may often be the case that the max-scoring negative example in the mini-batch is ac-tually a true fact, due to the generic nature of thefacts expressed.Table 4 shows a runtime comparison of thelosses and sampling strategies.4We find randomsampling to be orders of magnitude faster than theothers while also performing the best.Final Results.
Our final results are shown in Ta-ble 5.
We show the DEV2 and TEST accuracies forour baselines and for the best configuration (tunedon DEV2) for each model.
All models outperformall baselines handily.
Our models perform simi-larly, with the Bilinear models and the DNN AVGmodel all exceeding 90% on both DEV2 and TEST.We note that the AVG models performedstrongly compared to those that used LSTMs for4These experiments were performed using 2 threads on a3.40-GHz Intel Core i7-3770 CPU with 8 cores.DEV2 TESTCount 75.4 79.0ArgSim 72.9 74.2MaxSim 81.9 83.5Bilinear AVG 90.3 91.7Bilinear LSTM 90.8 90.7DNN AVG 91.3 92.0DNN LSTM 88.1 89.2Bilinear AVG + data 91.8 92.5human ?95.0 ?Table 5: Accuracies (%) of baselines and finalmodel configurations on DEV2 and TEST.
?+ data?uses enlarged training set of size 300,000, and thendoubles this training set by including tuples withconjugated forms; see text for details.
Human per-formance on DEV2 was estimated from a sampleof size 100.modeling terms.
We suggest two reasons for this.The first is that most terms are short, with an aver-age term length of 2.3 words in our training tu-ples.
An LSTM may not be needed to capturelong-distance properties.
The second reason maybe due to hyperparameter tuning.
Recall that weused a greedy search for optimal hyperparametervalues; we found that models with LSTMs takemore time per epoch, more epochs to converge,and exhibit more hyperparameter sensitivity com-pared to models based on averaging.
This mayhave contributed to inferior hyperparameter valuesfor the LSTM models.We also trained the Bilinear AVG model on alarger training set (row labeled ?Bilinear AVG +data?).
We note that the ConceptNet tuples typ-ically contain unconjugated forms; we sought touse both conjugated and unconjugated words.
Webegan with a larger training set of 300,000 tuplesfrom ConceptNet, then augmented them to includeconjugated word forms as in the following exam-ple.
For the tuple ?soak in a hotspring, CAUSES,get pruny skin?
obtained from the OMCS sentence?The effect of [soaking in a hotspring] is [get-ting pruny skin]?, we generated an additional tuple?soaking in a hotspring, CAUSES, getting prunyskin?.
We thus created twice as many training tu-ples.
The results with this larger training set im-proved from 91.7 to 92.5 on the test set.
We re-lease this final model to the research community.We note that the strongest baseline, MaxSim, isa nonparametric model that requires iterating overall training tuples to provide a score to new tuples.This is a serious bottleneck for use in NLP appli-cations that may need to issue large numbers of1451Bilinear AVG Bilinear LSTM DNN AVG DNN LSTMCE hinge CE hinge CE hinge CE hingerandom 90 84 91 83 91 87 88 57mix 90 83 90 87 90 78 82 63max 86 75 65 66 61 52 56 52Table 3: Accuracies (%) on DEV2 of models trained with two loss functions (cross entropy (CE) andhinge) and three sampling strategies (random, mix, and max).
The best accuracy for each model is shownin bold.
Cross entropy with random sampling is best across models and is also fastest (see Table 4).queries.
Our models are parametric models thatcan compress a large training set into a fixed num-ber of parameters.
This makes them extremely fastfor answering queries, particularly the AVG mod-els, enabling use in downstream NLP applications.7 Generating and Scoring Novel TuplesWe now measure our model?s ability to score noveltuples generated automatically from ConceptNetand Wikipedia.
We first describe simple pro-cedures to generate candidate tuples from thesetwo datasets.
We then score the tuples using ourMaxSim baseline and the trained Bilinear AVGmodel.5We evaluate the highly-scoring tuples us-ing a small-scale manual evaluation.The DNN AVG and Bilinear AVG modelsreached the highest TEST accuracies in our evalua-tion, though in preliminary experiments we foundthat the Bilinear AVG model appeared to performbetter when scoring the novel tuples described be-low.
We suspect this is because the DNN func-tion class has more flexibility than the bilinear one.When scoring novel tuples, many of which may behighly noisy, it appears that the constrained struc-ture of the Bilinear AVG model makes it more ro-bust to the noise.7.1 Generating Tuples From ConceptNetIn order to get new tuples, we automatically mod-ify existing ConceptNet tuples.
We take an exist-ing tuple and randomly change one of the threefields (t1, t2, orR), ensuring that the result is not atuple existing in ConceptNet.
We then score thesetuples using MaxSim and the Bilinear AVG modeland analyze the results.7.2 Generating Tuples from WikipediaWe also propose a simple method to extract candi-date tuples from raw text.
We first run the Stan-ford part-of-speech (POS) tagger (Toutanova et5For the results in this section, we used the Bilinear AVGmodel that achieved 91.7 on TEST rather than the one aug-mented with additional data.t1, R, t2scorebus, ISA, public transportation 0.95bus, ISA, public transit 0.90bus, ISA, mass transit 0.79bus, ATLOCATION, downtown area 0.98bus, ATLOCATION, subway station 0.98bus, ATLOCATION, city center 0.94bus, CAPABLEOF, low cost 0.72bus, CAPABLEOF, local service 0.65bus, CAPABLEOF, train service 0.63Table 6: Top Wikipedia tuples for 3 relations witht1= bus, scored by Bilinear AVG model.al., 2003) on the terms in our ConceptNet trainingtuples.
We enumerate the 50 most frequent termpair tag sequences for each relation.
We do lim-ited manual filtering of the frequent tag sequences,namely removing the sequences ?DT NN NN?
and?DT JJ NN?
for the ISA relation.
We do this in or-der to reduce noise in the extracted tuples.
To fo-cus on finding nontrivial tuples, for each relationwe retain the top 15 POS tag sequences in whicht1or t2has at least two words.We then run the tagger on sentences from En-glish Wikipedia.
We extract word sequence pairscorresponding to the relation POS tag sequencepairs, requiring that there be a gap of at least oneword between the two terms.
We then removeword sequence pairs in which one term is solelyone of the following words: be, the, always, there,has, due, however.
We also remove tuples con-taining words that are not in the vocabulary of ourConceptNet-trained embeddings.
We require thatone term does not include the other term.
We cre-ate tuples consisting of the two terms and all pos-sible relations that occur with the POS sequencesof those two terms.
Finally, we remove tuples thatexactly match our ConceptNet training tuples.We use our trained Bilinear AVG model toscore these tuples.
We extract term pairs that oc-cur within the same sentence because we hope thatthese will have higher precision than if we were topair together arbitrary pairs.
Some example tuplesfor t1= bus are shown in Table 6.14527.3 Manual Analysis of Novel TuplesTo evaluate our models on newly generated tu-ples, we rank them using different models andmanually score the high-ranking tuples for qual-ity.
We first randomly sampled 3000 tuples fromeach set of novel tuples.
We do so due to the timerequirements of the MaxSim baseline, which re-quires iterating through the entire training set foreach candidate tuple.
We score these sampled tu-ples using MaxSim and the Bilinear AVG modeland rank them by their scores.
The top 100 tu-ples under each ranking were given to an annota-tor who is a native English speaker.
The annota-tor assigned a quality score to each tuple, usingthe same 0-4 annotation scheme as Speer et al(2010): 0 (?Doesn?t make sense?
), 1 (?Not true?
),2 (?Opinion/Don?t know?
), 3 (?Sometimes true?
),and 4 (?Generally true?).
We report the averagequality score across each set of 100 tuples.The results are shown in Table 7.
To calibratethe scores, we also gave two samples of Concept-Net (CN) tuples to the annotator: a sample of 100high-confidence tuples (first row) and a sampleof 100 medium-confidence tuples (second row).We find the high-confidence tuples to be of highquality, recording an average of 3.68, though themedium-confidence tuples drop to 3.14.The next two rows show the quality scoresof the MaxSim baseline and the Bilinear AVGmodel.
The latter outperforms the baseline andmatches the quality of the medium-confidenceConceptNet tuples.
Since our novel tuples are notcontained in ConceptNet, this result suggests thatour model can be used to add medium-confidencetuples to ConceptNet.The novel Wikipedia tuples (top 100 tuplesranked by Bilinear AVG model) had a lower qual-ity score (2.78), but this is to be expected due tothe difference in domain.
Since Wikipedia con-tains a wide variety of text, we found the noveltuples to be noisier than those from ConceptNet.Still, we are encouraged that on average the tuplesare judged to be close to ?sometimes true.
?7.4 Text Analysis with Commonsense TuplesWe note that our method of tuple extraction andscoring could be used as an aid in applicationsthat require sentence understanding.
Two examplesentences are shown in Table 8, along with the toptuples extracted and scored using our method.
Thetuples capture general knowledge about phrasestuples qualityhigh-confidence CN tuples 3.68medium-confidence CN tuples 3.14novel CN tuples, ranked by MaxSim 2.74novel CN tuples, ranked by Bilinear AVG 3.20novel Wiki tuples, ranked by Bilinear AVG 2.78Table 7: Average quality scores from manual eval-uation of novel tuples.
Each row corresponds to adifferent set of tuples.
See text for details.After nine years of primary school, students can go tothe high school or to an educational institution.t1, R, t2scoreschool, HASPROPERTY, educational 0.89school, ISA, educational institution 0.80school, ISA, institution 0.78school, HASPROPERTY, high 0.77high school, ISA, institution 0.71On March 14, 1964, Ruby was convicted of murder withmalice, for which he received a death sentence.t1, R, t2scoremurder, CAUSES, death?1.00murder, CAUSES, death sentence 0.86murder, HASSUBEVENT, death 0.84murder, CAPABLEOF, death 0.51Table 8: Top ranked tuples extracted from twoexample sentences and scored by Bilinear AVGmodel.
?
= contained in ConceptNet.contained in the sentence, rather than necessarilyindicating what the sentence means.
This proce-dure could provide relevant commonsense knowl-edge for a downstream application that seeks tounderstand the sentence.
We leave further investi-gation of this idea to future work.8 ConclusionWe proposed methods to augment curated com-monsense resources using techniques from knowl-edge base completion.
By scoring novel tuples,we showed how we can increase the applicabilityof the knowledge contained in ConceptNet.
In fu-ture work, we will explore how to use our modelto improve downstream NLP tasks, and considerapplying our methods to other knowledge bases.We have released all of our resources?code, data,and trained models?to the research community.6AcknowledgmentsWe thank the anonymous reviewers, John Wieting,and Luke Zettlemoyer.6Available at http://ttic.uchicago.edu/?kgimpel/commonsense.html.1453ReferencesBasant Agarwal, Namita Mittal, Pooja Bansal, andSonal Garg.
2015.
Sentiment analysis usingcommon-sense and context information.
Comp.
Int.and Neurosc.Gabor Angeli and Christopher Manning.
2013.Philosophers are mortal: Inferring the truth of un-seen facts.
In Proc.
of CoNLL.Gabor Angeli and D. Christopher Manning.
2014.NaturalLI: Natural logic inference for commonsense reasoning.
In Proc.
of EMNLP.Collin F Baker, Charles J Fillmore, and John B Lowe.1998.
The Berkeley FrameNet project.
In Proc.
ofCOLING.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proc.
of ACL.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proc.
of the ACM SIGMODInternational Conference on Management of Data.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Advances in NIPS.Antoine Bordes, Sumit Chopra, and Jason Weston.2014a.
Question answering with subgraph embed-dings.
In Proc.
of EMNLP.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2014b.
A semantic matching en-ergy function for learning with multi-relational data.Machine Learning, 94(2).Erik Cambria, Daniel Olsher, and Kenneth Kwok.2012.
Sentic activation: A two-level affective com-mon sense reasoning framework.
In Proc.
of AAAI.Andrew Carlson, Justin Betteridge, Bryan Kisiel,Burr Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In Proc.
of AAAI.Luigi Di Caro, Alice Ruggeri, Loredana Cupi, andGuido Boella.
2015.
Common-sense knowledgefor natural language understanding: Experiments inunsupervised and supervised settings.
In Proc.
ofAI*IA.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
JMLR.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proc.
of EMNLP.Alberto Garc?
?a-Dur?an, Antoine Bordes, and NicolasUsunier.
2014.
Effective blending of two and three-way interactions for modeling multi-relational data.In Machine Learning and Knowledge Discovery inDatabases.Matt Gardner, Partha Pratim Talukdar, Jayant Krish-namurthy, and Tom Mitchell.
2014.
Incorporat-ing vector space similarity in random walk inferenceover knowledge bases.
In Proc.
of EMNLP.Jonathan Gordon and Lenhart K Schubert.
2012.Using textual patterns to learn expected event fre-quencies.
In Proc.
of Joint Workshop on Auto-matic Knowledge Base Construction and Web-scaleKnowledge Extraction.Jonathan Gordon and Benjamin Van Durme.
2013.Reporting bias and knowledge acquisition.
In Proc.of Workshop on Automated Knowledge Base Con-struction.Jonathan Gordon, Benjamin Van Durme, andLenhart K Schubert.
2010.
Learning from theweb: Extracting general world knowledge fromnoisy text.
In Collaboratively-Built KnowledgeSources and AI.Jonathan Gordon.
2014.
Inferential CommonsenseKnowledge from Text.
Ph.D. thesis, University ofRochester.Kelvin Gu, John Miller, and Percy Liang.
2015.Traversing knowledge graphs in vector space.
InProc.
of EMNLP.Felix Hill, Roi Reichart, and Anna Korhonen.
2015.SimLex-999: Evaluating semantic models with(genuine) similarity estimation.
Computational Lin-guistics.Sepp Hochreiter and J?urgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search usingclickthrough data.
In Proc.
of CIKM.Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes,and Guillaume R Obozinski.
2012.
A latent factormodel for highly multi-relational data.
In Advancesin NIPS.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
of ACL.Ni Lao, Tom Mitchell, and William W Cohen.
2011.Random walk inference and learning in a large scaleknowledge base.
In Proc.
of EMNLP.Douglas B Lenat and Ramanathan V Guha.
1989.Building large knowledge-based systems: represen-tation and inference in the Cyc project.
Addison-Wesley Longman Publishing Co., Inc.1454Henry Lieberman, Alexander Faaborg, Waseem Daher,and Jos?e Espinosa.
2005.
How to wreck a nicebeach you sing calm incense.
In Proc.
of IUI.Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-coso, Ramon Fermandez, Silvio Amir, Luis Marujo,and Tiago Luis.
2015.
Finding function in form:Compositional character models for open vocabu-lary word representation.
In Proc.
of EMNLP.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in NIPS.George A Miller.
1995.
WordNet: a lexical databasefor English.
Communications of the ACM, 38(11).Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proc of ACL.Arvind Neelakantan and Ming-Wei Chang.
2015.
In-ferring missing entity type instances for knowledgebase completion: New dataset and methods.
InProc.
of NAACL.Arvind Neelakantan, Benjamin Roth, and Andrew Mc-Callum.
2015.
Compositional vector space modelsfor knowledge base completion.
In Proc.
of ACL.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proc.
of ICML.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing YAGO: Scalable ma-chine learning for linked data.
In Proc.
of WWW.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: Global vectors forword representation.
In Proc.
of EMNLP.Sebastian Riedel, Limin Yao, Andrew McCallum, andM.
Benjamin Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InProc.
of NAACL-HLT.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in NIPS.Robert Speer and Catherine Havasi.
2012.
Represent-ing general relational knowledge in ConceptNet 5.In Proc.
of LREC.Robert Speer, Catherine Havasi, and Henry Lieberman.2008.
AnalogySpace: Reducing the dimensionalityof common sense knowledge.
In Proc.
of AAAI.Robert Speer, Catherine Havasi, and Harshit Surana.2010.
Using verbosity: Common sense data fromgames with a purpose.
In Proc.
of FLAIRS.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proc.
of NAACL-HLT.Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-fung Poon, Pallavi Choudhury, and Michael Gamon.2015.
Representing text for joint embedding of textand knowledge bases.
In Proc.
of EMNLP.Luis von Ahn, Mihir Kedia, and Manuel Blum.
2006.Verbosity: a game for collecting common-sensefacts.
In Proc.
of CHI.Robert West, Evgeniy Gabrilovich, Kevin Murphy,Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014.Knowledge base completion via search-based ques-tion answering.
In Proc.
of WWW.John Wieting, Mohit Bansal, Kevin Gimpel, KarenLivescu, and Dan Roth.
2015.
From paraphrasedatabase to compositional paraphrase model andback.
Transactions of the ACL.John Wieting, Mohit Bansal, Kevin Gimpel, and KarenLivescu.
2016.
Towards universal paraphrastic sen-tence embeddings.
In Proc.
of ICLR.Bishan Yang, Wen-tau Yih, Xiaodong He, JianfengGao, and Li Deng.
2014.
Embedding entities andrelations for learning and inference in knowledgebases.
arXiv preprint arXiv:1412.6575.Limin Yao, Sebastian Riedel, and Andrew McCallum.2013.
Universal schema for entity type prediction.In Proc.
of Workshop on Automated Knowledge BaseConstruction.1455
