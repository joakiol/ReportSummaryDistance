A Robust Dialogue System with Spontaneous SpeechUnderstanding and Cooperative ResponseToshihiko ITOH, Akihiro DENDA,  Satoru KOGURE and Seiichi NAKAGAWADepartment  of Informat ion and Computer  SciencesToyohashi  University of TechnologyTenpaku-cho, Toyohashi-shi, Aichi-ken, 441, JapanE-mail  address ?
{itoh, akihiro, kogure, nakagawa}@slp.tut ics.tut .ac.
jp1 IntroductionA spoken dialogue system that can understand spon-taneous peech needs to handle extensive range ofspeech in comparision with the read speech that hasbeen studied so far.
The spoken language has looserrestriction of the grammar than the written languageand has ambiguous phenomena such as interjections,ellipses, inversions, repairs, unknown words and soon.
It must be noted that the recognition rate of thespeech recognizer is limited by the trade-off betweenthe looseness of linguistic constraints and recogni-tion precision , and that the recognizer may out-put a sentence as recognition results which humanwould never say.
Therefore, the interpreter that re-ceives recognized sentences must cope not only withspontaneous sentences but also with illegal sentenceshaving recognition errors.
Some spoken languagesystems focus on robust matching to handle ungram-matical utterances and illegal sentences.The Template Matcher (TM) at the Stanford Re-search Institute (Jackson et al, 91) instantiates com-peting templates, each of which seeks to fill its slotswith appropriate words and phrases from the utter-ance.
The template with the highest score yields thesemantic representation.
Carnegie Mellon Univer~sity's Phoenix (Ward and Young, 93) uses RecursiveTransition Network formalism; word patterns corre-spond to semantic tokens, some of which appear asslots in frame structures.
The system fills slots indifferent frames in parallel, using a form of dynamicprogramming beam search.
The score for frame isthe number of input words it accounts for.Recently many multi-modal systems, which com-bine speech with touch screen, have been developed.For example, Tell and Bellik developed the tool fordrawing coloured geometric objects on a computerdisplay using speech, tactile and a mouse (Tell andBellik, 91).
We also developed a multi-modal dia-logue system based on the robust spoken dialoguesystem.In Section 2, we present an overview of our spo-ken dialogue system through multi-modalities.
InSection 3, we describe the robust interpreter f omerrorful speech recognition results and illegal sen-tences, and in Section 4, we describe the coopera-tive response generator.
In Section 5, we show theresults of the evaluation experiments.2 A Mu l t i -Moda l  D ia logue  SystemThe domain of our dialogue system is "Mt.
Fujisightseeing uidance (the vocabulary size is 292words for the recognizer and 948 words for the inter-preter, and the test-set word perplexity is 103)", Thedialogue system is composed of 4 parts: Input byspeech recognizer and touch screen, graphical userinterface, interpreter, and response generator.
Thelatter two parts are described in Sections 3 and 4.2.1 Spontaneous Speech RecognizerThe speech recognizer uses a frame-synchronous nepass Viterbi algorithm and Earley like parser forcontext-free grammar, while using HMMs as sylla-ble units.
Input speech is analyzed by the followingconditions :Sampling frequency :Hamming window size :Frame period :LPC analysis :Feature parameter :12kHz21.33ms(256 samples)8ms14th order10 LPC Mel-cepstram coefficientsand regression coefficients (ACEP)The acoustic models consist of 113 syllable basedHMMs, which have 5 states, 4 Gaussian densitiesand 4 discrete duration distributions.
The speaker-independent HMMs were adapted to the test speakerusing 20 utterances for the adaptation.
The gram-mar used in our speech recognizer is represented byacontext-free grammar which describes the syntacticand semantic information.Our recognizer integrates the acoustic processwith linguistic process directly without the phraseor word lattice.
We could say that this architec-ture is better for not only cooperatively read speechbut spontaneous speech rather than hierarchical r-chitectures interleaved with phrase lattice (Kai andNakagawa, 95).
Furthermore, the recognizer pro-cesses interjections and restarts based on an un-known word processing technique.
The unknownword processing part uses HMM's likelihood scoresfor arbitrary syllable sequences.A context free grammar is made to be able toaccept sentences with omitted post-positions andinversion of word in order to recognize sponta-neous peech.
We assume that the interjections and57restarts occur at the phrase boundaries.
Thus, ourspeech recognizer for read speech was improved todeal with spontaneous speech.2.2 Touch screen (pointing device)The touch panel used here is an electrostatic typeproduced by Nissya International System Inc. andthe resolution is 1024 x 1024 points.
This panel isattached on the 21 inch display of SPARC-10, whichhas coordinate axes of 1152 x 900 and a transmissionspeed of 180 points/sec.The input by touch screen is used to designate thelocation of map around Mt.Fuji (which is a main lo-cation related to our task) on the display or to selectthe desired item from the menu which consists ofthe set of items responded by a speech synthesizer.The response through the speech synthesizer is con-venient, however, user cannot memorize the contentwhen the content includes many items.
Therefore,we use the display output (map and menu) as well asspeech synthesis for the response.
User can only usethe positioning/selecting put and speech input atthe same time.
For example, user can utter "Is here... ~" while positioning the location or menu.
Inthis case, system regard the demonstartive "here'asa keyword that user has positioned/selected.2.3 Graphical User InterfaceOn man-machine communication, user wants toknow his or machine situation what informationhe gets from the dialogue or how machine inter-prets/understands hi utterances, as well as thespeech recognition result.
Therefore our system dis-plays the history of dialogue.
This function helpsto eliminate user uneasiness.
Figure 1 illustrates anexample of map, menu and history.
A multi-modalresponse algorithm is very simple, because the sys-tem is sure to respond to user through speech synthe-sizer and if the system is possible to respond throughgraphical information, the system does use these.Figure 1: An Example of Map, Menu, and Historyfor an Input Utterance( Input : How much is the entrance fee for Fujikyu-Highland ?
)58I-"E 1?Do~&o=1 on synthesized voiceFigure 2: Spoken Dialogue SystemThe  In terpreterProcessing illegal Utterances33.1Whole process is carried out as below:1.
The steps in the following process are carriedout one by one.
When one of the steps ucceeds,go to process 2.
If all of the processes fail, goto process 4.
(a) syntax and semantics analysis for legal sen-tence without omission of post-positionsand inversion of word order.
(b) syntax and semantics analysis for sentenceincluding omission of post-positions.
(c) syntax and semantics analysis for sentenceincluding omission of post-positions and in-version of word order.
(d) syntax and semantics analysis for sen-tence including invalid (misrecognized)post-positions and inversion of word order.2.
Fundamental contextual processing is per-formed.
(a) Replace demonstrative word with adequatewords registered for a demonstrative worddatabase(b) Unify different semantic networks using de-fault knowledges, which are considered tobe semantically equivalent to each other(processing for semantically omissions).3.
Semantic representation of the sentence ischecked using contextual knowledge (we call itfiltering hereafter).
(a) correct case: Output the semantic repre-sentation of the analysis result (end of anal-ysis).
(b) incorrect case: If there are some heuristicsfor correcting, apply them to the seman-tic representation.
The corrected semanticrepresentation is the result of analysis (endof analysis).
If there aren't any applicableheuristics, go to process 4.4.
Keyword analysis (later mentioned) is per-formed by using a partial result of the analysis.First, the interpreter assumes that there are noomissions and inversions in the sentence(l-a).
Sec-ond, when the analysis fails, the interpreter uses theheuristics which enable to recover about 90% of in-versions and post-position omissions(Yamamoto etal., 92)(1-b,c).
Furthermore, when the interpreterfails the analysis using the heuristics, it assumesthat the post-position is wrong.
Post-positions as-sumed to be wrong are ignored and the correct post-position is guessed using above heuristics(i-d).
Theinterpreter gives the priority to the interpretationwhere the number of post-position assumed to bewrong is a few as possible.Human agents can recover illegal sentences by us-ing general syntactical knowledge and/or contextualknowledge.
To do this process by computer, we re-alized a filtering process(3-b).
Contextually disal-lowable semantic representations are registered asfilters.
This process has 2 functions.
One is to blocksemantic networks including the same as the regis-tered networks for wrong patterns.
The other is tomodify networks so that they can be accepted assemantically correct.
If the input pattern matcheswith one of the registered patterns, its semantic rep-resentation is rejected, and the correction procedureis applied if possible.
The patterns are specifiedas semantic representations i cluding variables, andthe matching algorithm works a unification-like.When no network is generated at this stage, theinterpreter checks the sentence using keyword basedmethod(4).
The interpreter has several dozens oftemplate networks which have semantic onditionson some nodes.
If one of them is satisfied by somewords in the sentence, it is accepted as the corre-sponding semantic network.4 The Cooperative ResponseGeneratorDialogue system through natural anguage must bedesigned so that it can cooperatively response tousers.
For example, if a user's query doesn't haveenough conditions/information t  answer the ques-tion by sysytem, or if there is much retrieved in-formation from the knowledge database for user'squestion, the dialogue manager queries the user toget necessary conditions or to select the candidate,respectively.
Further, if the system can't retrieveany information related to the user's question, thegenerator proposes an alternative plan.
Based onthese considerations, we developed a cooperative re-sponse generator in the dialogue system.The response generator is composed of dialoguemanager, intention(focus) analyzer, problem solver,knowledge databases, and response sentence gener-ator as shown in Figure 2 (lower part).Firstly, the dialogue manager eceives a semanticrepresentation (that is,semantic network) throughthe semantic interpreter for the user's utterance.The dialogue manager is a component which carriesout some operations uch as dialogue management,control of contextual information and query to users.Secondly, to get intention for managing dialogues,the dialogue manager passes emantic network to in-tention(M) analyzer which extracts a dialogue inten-tion and conditions/information of a user's query.Then, the dialogue manager decides a flow of di-alogue using the intention that is sent back fromthe intention analyzer and acquires available infor-mation from dialogue history as contextual informa-tion.
Thirdly, the dialogue manager passes a seman-tic network and contextual information to problemsolver to retrieve any information from the knowl-edge database.
Further, if the problem solver can'tretrieve any information related to the user's ques-tion, the problem solver proposes an alternative plan(information) by changing a part of conditions ofusr's query and send it back to dialgoue manager.Then the dialogue manager counts a number ofretrieved information.
If there is much retrievedinformation from the knowledge database for user'squestion, the dialogue manager queries further con-ditions to the user to select the information.
If thenumber of these is adequate, the dialogue managergives a semantic network and retrieved informationto the response sentence generator.Finally, the response sentence generator decidesa response form from the received inputs and thenforms response sentence networks according to thisform.
After this process was finished, the responsesentence generator converts these networks into re-sponse sentences.5 Evaluation Experiment5.1 OverviewIn order to evaluate our dialogue system with themulti-modal interfaces, we investigated its perfor-mance through the evaluation experiments, payingattention to "usefulness of our system".We gave a task of making some plans of Mt.Fujisightseeing to 10 users\[A ... J\] ( 6 users where eval-uation of language processing part ) who did notknow about this system\[novises\] in advance.
Thenumber of items that user should fill in using oursystem in this experiment is eight: "Where to go"and "What to do" in first day and second day, and"Where to stay", "Kind of accommodation", "Ac-commodation name", and "Accommodation fee" infirst night.
We explained this dialogue system tothem and asked them to speak to the system freelyand spontaneously.And we gave three dialogue modes to every sub-jects, as shown in below :mode-A Using only speech input and output (ourconventional system)59mode-B Using speech input and multi-modal out-put (graphical output on display andspeech output)mode-C Using multi-modal input and output (in-put : speech and using touch screen, out-put : speech and graphic on display)Users used three systems on-line mode at the com-puter room.In this experiment, the performances (recognition/ comprehension rate, dialogue time, number of ut-terances) of three systems were not seen explicit dif-ferences, because the system is imperfect.5.2 Evaluation of the language processingpart through the experimental resultTable 1 shows the performance of our systemthrough experiments using mode-A system, whichinvestigated the performance of the language pro-cessing parts.The column of "Speech input" is the result thatexperiments was done in practice.
And the columnof "Text input" is the perforamnce of our system,when system inputted a transcription of user's ut-terances that the recognition rate of the speech rec-ognizer was assumed as 100%.
"Semicorrect Recog"means the recognition rate that permitted somerecognition errors of particles.
"Data presentation"is the rate that the system offered the valuable infor-mation to user.
"System query" is the rate that thesystem queried the user to get necessary conditionsand to select the information.
"Alternative plan"is the rate that the system proposed the alternativeplan.
"Correct response" is the sum of "Data pre-sentation", System query", "Alternative plan" andrate that the intepreter was unsuccessful in gener-ating a semantic network.
"Retrieval failure" is therate that the system could not offer the valuable in-formation to user although the interpreter has beensuccessful in generating a semantic network.The number of total utterances was 101.
81 outof 101 were acceptable by the grammar of the rec-ognizer.
12 unacceptable out of 20 utterances werecaused by unknown words, so we considered that itwas very important to solve the unknown word prob-lem.
And, 8 out of 20 were not acceptable by thegrammar.
The recognition rate of the speech recog-nizer on the spontaneous speech was 20.8%.
In thespeech input, the system unterstood about 55% ofthe all utterances and offered the available informa-tion to user about 55% (42.6%+9.0%+3.0%).
Andin the text input, these rates were 90% and 80%,respectively.
These rates show that the languageprocessing part worked well.6 Conc lus ionWe developed the robust interpreter that can ac-cept not only spontaneous speech but also misrecog-nized sentences.
The interpreter was implemented toour dialogue system for spontaneous speech whichworked in the task domain of "Mt.Fuji sightsee-ing guidance".
Further more, based on that dialogsystem through natural anguage must be designedTableEvaluationSubjects(users)UtterancesCorrect recognitionSemicorrect RecogInterpretationCorrect responseData presentationSystem queryAlternative planRetrieval failureEvaluation resultsSpeech input Text inputsentence(%) sentences(%)6 users101(100%)21(20.8% ~56q 55.4% b56155:4%81180.2% I43(42.6%)9(9.0% :3(3.0%:4(4.0%.~90(89.1%)87(86.1%)64(63.4%)12(12.0%)~(5.0%), 9(8.9%)so that it can cooperatively response to users, wedevloped a cooperative response generator in thedialogue system.
This dialogue system was inte-grated with a touch screen input method.
Exper-iments howed that our interpretation mechanism issuitable for understanding the recognition result ofspontaneous speech.
And we found that the multi-modal interface with spontaneous speech and touchscreen was user-friendly.Re ferencesE.Jackson, J.Bear, R.Moore, and A.Podlozny: "Atemplate matcher for robust NL interpretation"Proc.
Speech and Natural Language Workshop,Morgan Kaufmann Inc., pp.190-194, Feb.19-22(1991).W.Ward and S.Young: "Flexible use of seman-tic constraints in speech recognition," Proc.
Int.Conf.
Acoustics, Speech & Signal Process, vol.
II,pp.49-50, Minneapolis (1993).D.Teil and Y.Bellik : Multimodal dialogue inter-face on a workstation, Venaco Workshop andETRW on "The structure of multimodal dia-logue", Maratea (1991).A.Kai and S.Nakagawa : "Investigation on un-known word processing and strategies for sponta-neous peech understanding", Proceedings of EU-ROSPEECH 95, pp.2095-2098 (1995).M.Yamamoto, S.Kobayashi, S.Nakagawa: "Ananalysis and parsing method of the omissionof post-position and inversion on japanese spo-ken sentence in dialog", Transactions of Informa-tion Processing Society of Japan Vol.33~ No.11,pp.1322-1330(1992), in Japanese.T.Itoh, M.ttidano, M.Yamamoto and S.Nakagawa :"Spontaneous Speech Understanding for a RobustDialogue System", Proceeding of Natural Lan-guage Processing Pacific Rim Symposium '95, Vol-ume 2, pp.538-543 (1995).A.Denda, T.Itoh, and S.Nakagawa : "A RobustDialogue System with Spontaneous Speech andTouch Screen", Proceeding of the First Interna-tional Conference on Multumodal Interface '96,pp.144-151 ( 996).60
