Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 251?259,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsUsing N-gram and Word Network Features forNative Language IdentificationShibamouli Lahiri Rada MihalceaComputer Science and EngineeringUniversity of North TexasDenton, TX 76207, USAshibamoulilahiri@my.unt.edu, rada@cs.unt.eduAbstractWe report on the performance of two differentfeature sets in the Native Language IdentificationShared Task (Tetreault et al 2013).
Our featuresets were inspired by existing literature on nativelanguage identification and word networks.
Exper-iments show that word networks have competitiveperformance against the baseline feature set, whichis a promising result.
We also present a discussionof feature analysis based on information gain, and anoverview on the performance of different word net-work features in the Native Language Identificationtask.1 IntroductionNative Language Identification (NLI) is a well-established problem in NLP, where the goal is toidentify a writer?s native language (L1) from his/herwriting in a second language (L2), usually English.NLI is generally framed as a multi-class classifi-cation problem (Koppel et al 2005; Brooke andHirst, 2011; Wong and Dras, 2011), where nativelanguages (L1) are considered class labels, and writ-ing samples in L2 are used as training and test data.The NLI problem has recently seen a big surge ininterest, sparked in part by three influential early pa-pers on this problem (Tomokiyo and Jones, 2001;van Halteren and Oostdijk, 2004; Koppel et al2005).
Apart from shedding light on the way non-native learners (also called ?L2 learners?)
learn anew language, the NLI task allows constrastive anal-ysis (Wong and Dras, 2009), study of different typesof errors that people make while learning a new lan-guage (Kochmar, 2011; Bestgen et al 2012; Jarviset al 2012), and identification of language trans-fer patterns (Brooke and Hirst, 2012a; Jarvis andCrossley, 2012), thereby helping L2-students im-prove their writing styles and expediting the learn-ing process.
It also helps L2 educators to concen-trate their efforts on particular areas of a languagethat cause the most learning difficulty for differentL1s.The NLI task is closely related to traditional NLPproblems of authorship attribution (Juola, 2006; Sta-matatos, 2009; Koppel et al 2009) and author pro-filing (Kes?elj et al 2003; Estival et al 2007a; Esti-val et al 2007b; Bergsma et al 2012), and sharesmany of the same features.
Like authorship attri-bution, NLI is greatly benefitted by having functionwords and character n-grams as features (Brookeand Hirst, 2011; Brooke and Hirst, 2012b).
Nativelanguages form a part of an author?s socio-culturaland psychological profiles, thereby being related toauthor profiling (van Halteren and Oostdijk, 2004;Torney et al 2012).Researchers have used different types of featuresfor the NLI problem, including but not limited tofunction words (Brooke and Hirst, 2012b); char-acter, word and POS n-grams (Brooke and Hirst,2012b); spelling and syntactic errors (Koppel et al2005); CFG productions (Brooke and Hirst, 2012b);Tree Substitution Grammar productions (Swansonand Charniak, 2012); dependencies (Brooke andHirst, 2012b); Adaptor Grammar features (Wong etal., 2012); L1-influence (Brooke and Hirst, 2012a);stylometric features (Golcher and Reznicek, 2011;251Crossley and McNamara, 2012; Jarvis et al 2012);recurrent n-grams on words and POS (Bykh andMeurers, 2012); and features derived from topicmodels (Wong et al 2011).
State-of-the-art re-sults are typically in the 80%-90% range, with re-sults above 90% reported in some cases (Brookeand Hirst, 2012b).
Note, however, that results varygreatly across different datasets, depending on thenumber of languages being considered, size and dif-ficulty of data, etc.2 Our ApproachThe NLI 2013 Shared Task (Tetreault et al 2013)marks an effort in bringing together the NLI researchcommunity to share and compare their results andevaluations on a common dataset - TOEFL11 (Blan-chard et al 2013) - consisting of 12,100 unique En-glish essays written by non-native learners of elevendifferent languages.1 The dataset has 9,900 essaysfor training, 1,100 essays for test, and 1,100 essaysfor development.
Each of the three sets is balancedacross different L1s.Inspired by previous work in NLI, in our differentNLI systems submissions we used several differenttypes of character, word, and POS n-gram features(cf.
Section 2.1).
Although not included in the sys-tems submitted, we also experimented with a familyof new features derived from a word network repre-sentation of natural language text (cf.
Section 2.2).We used Weka (Hall et al 2009) for all our classifi-cation experiments.
The systems that were submit-ted gave best 10-fold cross-validation accuracy ontraining data among different feature-classifier com-binations (Section 3).
Word network features - al-though competitive against the baseline n-gram fea-tures - were not able to beat the baseline featureson the training set, so we did not submit that sys-tem for evaluation.
Section 2.1 discusses our n-gramfeatures, followed by a discussion of word networkfeatures in Section 2.2.2.1 N-gram FeaturesWe used several baseline n-gram features based onwords, characters, and POS.
We experimented withthe raw frequency, normalized frequency, and binary1Arabic, Chinese, French, German, Hindi, Italian, Japanese,Korean, Spanish, Telugu and Turkish.presence/absence indicator on top 100, 200, 500 and1000 n-grams:21. word n-grams (n = 1, 2, 3), with and withoutpunctuation.2.
character n-grams (n = 1, 2, 3), with and with-out space characters.3.
POS n-grams (n = 1, 2, 3), with and withoutpunctuation.3We experimented with punctuation because pre-vious research indicates that punctuation is help-ful (Wong and Dras, 2009; Kochmar, 2011).
In total,there are 216 types of n-gram feature vectors (withdimensions 100, 200, 500 and 1000) for a particulardocument.
Because of size restrictions (e.g., some n-gram dictionaries are smaller than the specified fea-ture vector dimensions), we ended up with 168 typesof feature vectors per document (cf.
Tables 2 to 4).2.2 Word NetworksA ?word network?
of a particular document is a net-work (graph) of unique words found in that docu-ment.
Each node (vertex) in this network is a word.Edges between two nodes (unique words) can beconstructed in several different ways.
The simplesttype of edge connects word A to word B, if wordA is followed by word B in the document at leastonce.
In our work, we have assumed a directed edgewith direction from word A to word B.
Note that wecould have used undirected edges as well (cf.
(Mi-halcea and Tarau, 2004)).
Moreover, edges canbe weighted/unweighted.
We assumed unweightededges.A deeper issue with this network constructionprocess concerns what we should do with stopwords.Should we keep them, or should we remove them?Since stopwords and function words have proved tobe of special importance in previous native languageidentification studies (Wong and Dras, 2009; Brookeand Hirst, 2012b), we chose to keep them in ourword networks.Two other choices we made in the constructionof our word networks concern sentence boundaries2Note that these most frequent n-grams were extracted fromthe training+development set.3We used CRFTagger (Phan, 2006) for POS tagging.252Figure 1: Word network of the sentence ?the quick brownfox jumped over the lazy dog?.and word co-occurrence.
Word networks can beconstructed either by respecting sentence boundaries(where the last word of sentence 1 does not linkto the first word of sentence 2), or by disregard-ing them.
In our case, we disregarded all sentenceboundaries.
Moreover, a network edge can eitherlink two words that appeared side-by-side in theoriginal document, or it can link two words that ap-peared within a window of n words in the document(cf.
(Mihalcea and Tarau, 2004)).
In our case, wechose the first option - linking unique words that ap-peared side-by-side at least once.
Finally, we didnot perform any stemming/morphological analysisto retain subtle cues that might be revealed from in-flected/derived words.The word network of an example sentence (?thequick brown fox jumped over the lazy dog?)
isshown in Figure 1.
Note that the word ?the?
ap-peared twice in this sentence, so the correspond-ing network contains a cycle that starts at ?the?and ends at ?the?.
In a realistic word network ofa large document, there can be many such cycles.In addition, it is observed that such word networksshow power-law degree distribution and a small-world structure (i Cancho and Sole?, 2001; Matsuoet al 2001).Once the word networks have been constructed,we extract a set of simple features from these net-works4 that represent local properties of individualnodes.
We have extracted ten local features for eachnode in a word network:1. in-degree, out-degree and degree2.
in-coreness, out-coreness and coreness53.
in-neighborhood size (order 1), out-neighborhood size (order 1) and neighborhoodsize (order 1)4. local clustering coefficientWe take a set of representative words, and converta document into a local feature vector - each localfeature pertaining to one word in the set of repre-sentative words.
For example, when we use the top200 most frequent words as the representative set,a document can be represented as the degree vec-tor of these 200 words in the document?s word net-work, or as the local clustering coefficient vector ofthese words in the word network, or as the corenessvector of the words (and so on).
A document canalso be represented as a concatenation (mixture) ofthese vectors.
For example, it can be representedas concat(degree vector, coreness vector) of top200 most frequent words.
We are yet to explorehow such mixed feature sets perform in the NLItask, and this constitutes a part of our future work(Section 4).
We experimented with top k most fre-quent words (with k = 100, 200, 500, 1000) on train-ing+development data as our representative word-set.3 ResultsTable 1 describes the three systems we submitted.The first two systems (UNT-closed-1.csv and UNT-closed-2.csv) were based on a bag of words modelusing all the words from the training set.
Thesystems used a home-grown implementation of theNa?
?ve Bayes classifier, and achieved 10-fold cross-validation accuracy of 64.5% and 65.1% respec-tively, on the training set.
The first system used raw4We used the igraph (Csardi and Nepusz, 2006) softwarepackage for graph feature extraction.5Coreness is an index given to a particular vertex basedon its position in the k-core decomposition of the word net-work (Batagelj and Zaversnik, 2003).253Submitted System10-fold CV Accuracy on Accuracy onDescriptionTraining Set (%) Test Set (%)UNT-closed-1.csv 64.50 63.20Raw frequency of all words in the training setincluding stopwords.
Na?
?ve Bayes classifier.UNT-closed-2.csv 65.10 63.70Raw frequency of all words in the training setexcept stopwords.
Na?
?ve Bayes classifier.UNT-closed-3.csv 62.46 64.50Raw frequency of 1000 most frequent wordsin the training+development set including punctuation.SVM (SMO) classifier.Table 1: Performance summary and description of the systems we submitted.term frequency of all words including stopwords asfeatures, and the second system used raw term fre-quency of all words except stopwords.
These twosystems achieved test set accuracy of 63.2% and63.7%, respectively.The third system we submitted (UNT-closed-3.csv) was based on n-gram features (cf.
Sec-tion 2.1).
We used the raw frequency of top 1000word unigrams, including punctuation, as features.The Weka SMO implementation of SVM (Hall etal., 2009) was used as classifier with default param-eter settings.
This system gave us the best 10-foldcross-validation accuracy of 62.46% in the trainingset, among all n-gram features.
Note that this systemwas also the top performer among the systems wesubmitted in NLI evaluation, with a test set accuracyof 64.5%, and a 10-fold CV accuracy of 63.77% onthe training+development set folds specified by theorganizers.We will now describe in the following two sub-sections how our n-gram features and word networkfeatures performed on the training set.
All results re-ported here reflect best 10-fold cross-validation ac-curacy in the training set among different classifiers(SVM, Na?
?ve Bayes, 1-nearest-neighbor (1NN), J48decision tree, and AdaBoost).
SVM and Na?
?veBayes gave best results in our experiments, so onlythese two are shown in Tables 2 to 5.3.1 Performance of N-gram FeaturesRecall from Section 2.1 that we extracted 168 differ-ent n-gram feature vectors corresponding to the rawfrequency, normalized frequency, and binary pres-ence/absence indicator of top k n-grams (with k =100, 200, 500, 1000) in the training+developmentset.
Performance of these n-gram features is givenin Tables 2 to 4.
A general observation with Tables 2to 4 is that cross-validation performance improves ask increases, although there are a few exceptions.
Wemarked those exceptions with an asterisk (?*?
).It is interesting to note that top k word unigramswith punctuation were the top performers in mostof the cases.
Also interesting is the fact that SVMmostly gave best performance on n-gram featuresamong different classifiers.
Note that Na?
?ve Bayeswas best performer in a few cases (Table 4).
Per-formance of raw and normalized frequency featureswere mostly comparable (Tables 2 and 3), whereasbinary presence/absence indicator achieved worseaccuracy values in general than raw and normalizedfrequency features (Table 4).Among different n-grams, word unigrams per-formed better than bigrams and trigrams, POS bi-grams performed better than POS trigrams, andcharacter bigrams and character trigrams performedcomparably well (Tables 2 and 3).
Exceptions tothis observation are seen in Table 4, where charactertrigrams performed better than character bigrams,and word bigrams sometimes performed better thanword unigrams.
In general, word n-grams performedthe best, followed by POS and character n-grams.3.2 Performance of Word Network FeaturesWord networks and word network features were de-scribed in Section 2.2.
We extracted ten local fea-tures on four different representative sets of words- the top k most frequent words (k = 100, 200, 500,1000) on the training+development set, respectively.Performance of these features is given in Table 5.Note that in general, word network features per-254N-gram FeatureBest Cross-validation Accuracy (%) on Top k Most Frequent N-gramsk = 100 k = 200 k = 500 k = 1000Word unigramw/ punctuation 45.07 (SVM) 52.85 (SVM) 60.14 (SVM) 62.46 (SVM)w/o punctuation 41.63 (SVM) 50.15 (SVM) 58.33 (SVM) 60.85 (SVM)Word bigramw/ punctuation 39.54 (SVM) 44.75 (SVM) 51.70 (SVM) 56.06 (SVM)w/o punctuation 33.40 (SVM) 39.34 (SVM) 47.54 (SVM) 51.86 (SVM)Word trigramw/ punctuation 30.62 (SVM) 35.26 (SVM) 41.56 (SVM) 44.97 (SVM)w/o punctuation 26.67 (SVM) 30.14 (SVM) 36.68 (SVM) 41.22 (SVM)POS unigramw/ punctuation N/A N/A N/A N/Aw/o punctuation N/A N/A N/A N/APOS bigramw/ punctuation 41.79 (SVM) 45.87 (SVM) 48.11 (SVM) 47.49 (SVM)*w/o punctuation 35.95 (SVM) 39.23 (SVM) 41.23 (SVM) 39.58 (SVM)*POS trigramw/ punctuation 34.97 (SVM) 38.78 (SVM) 43.17 (SVM) 44.52 (SVM)w/o punctuation 29.73 (SVM) 34.31 (SVM) 37.58 (SVM) 38.40 (SVM)Character unigramw/ space N/A N/A N/A N/Aw/o space N/A N/A N/A N/ACharacter bigramw/ space 42.48 (SVM) 48.43 (SVM) 55.87 (SVM) 56.12 (SVM)w/o space 36.84 (SVM) 45.93 (SVM) 51.11 (SVM) 53.41 (SVM)Character trigramw/ space 41.65 (SVM) 48.68 (SVM) 54.54 (SVM) 57.77 (SVM)w/o space 36.64 (SVM) 43.44 (SVM) 51.46 (SVM) 55.52 (SVM)Table 2: Performance of raw frequency of n-gram features.
Stratified ten-fold cross-validation accuracy values onTOEFL11 training set are shown, along with the classifiers that achieved these accuracy values.
Best results in differentcolumns are boldfaced.
Table cells marked ?N/A?
are the ones that correspond to an n-gram dictionary size < k.N-gram FeatureBest Cross-validation Accuracy (%) on Top k Most Frequent N-gramsk = 100 k = 200 k = 500 k = 1000Word unigramw/ punctuation 44.65 (SVM) 52.21 (SVM) 59.81 (SVM) 62.35 (SVM)w/o punctuation 41.15 (SVM) 50.41 (SVM) 58.18 (SVM) 60.61 (SVM)Word bigramw/ punctuation 39.63 (SVM) 44.69 (SVM) 52.31 (SVM) 56.08 (SVM)w/o punctuation 33.44 (SVM) 39.11 (SVM) 47.61 (SVM) 52.56 (SVM)Word trigramw/ punctuation 30.42 (SVM) 34.97 (SVM) 41.89 (SVM) 45.68 (SVM)w/o punctuation 26.08 (SVM) 30.03 (SVM) 37.16 (SVM) 42.39 (SVM)POS unigramw/ punctuation N/A N/A N/A N/Aw/o punctuation N/A N/A N/A N/APOS bigramw/ punctuation 41.08 (SVM) 45.04 (SVM) 48.23 (SVM) 47.78 (SVM)*w/o punctuation 34.85 (SVM) 38.95 (SVM) 41.16 (SVM) 40.84 (SVM)*POS trigramw/ punctuation 34.74 (SVM) 38.38 (SVM) 42.89 (SVM) 44.86 (SVM)w/o punctuation 28.74 (SVM) 33.67 (SVM) 36.93 (SVM) 38.64 (SVM)Character unigramw/ space N/A N/A N/A N/Aw/o space N/A N/A N/A N/ACharacter bigramw/ space 41.93 (SVM) 47.79 (SVM) 56.31 (SVM) 56.22 (SVM)*w/o space 36.21 (SVM) 45.18 (SVM) 51.58 (SVM) 53.63 (SVM)Character trigramw/ space 40.70 (SVM) 47.90 (SVM) 54.40 (SVM) 57.36 (SVM)w/o space 35.84 (SVM) 42.79 (SVM) 50.94 (SVM) 55.71 (SVM)Table 3: Performance of normalized frequency of n-gram features.
Stratified ten-fold cross-validation accuracy valueson TOEFL11 training set are shown, along with the classifiers that achieved these accuracy values.
Best results indifferent columns are boldfaced.
Table cells marked ?N/A?
are the ones that correspond to an n-gram dictionary size< k.255N-gram FeatureBest Cross-validation Accuracy (%) on Top k Most Frequent N-gramsk = 100 k = 200 k = 500 k = 1000Word unigramw/ punctuation 33.42 (SVM) 42.49 (SVM) 50.63 (Na?
?ve Bayes) 56.95 (SVM)w/o punctuation 33.05 (SVM) 42.82 (SVM) 50.13 (SVM) 55.91 (SVM)Word bigramw/ punctuation 37.74 (SVM) 40.99 (SVM) 46.16 (SVM) 52.66 (SVM)w/o punctuation 32.02 (SVM) 37.24 (SVM) 42.29 (SVM) 48.36 (SVM)Word trigramw/ punctuation 29.87 (SVM) 33.79 (SVM) 38.48 (SVM) 42.00 (SVM)w/o punctuation 25.75 (SVM) 28.79 (SVM) 34.14 (SVM) 37.80 (SVM)POS unigramw/ punctuation N/A N/A N/A N/Aw/o punctuation N/A N/A N/A N/APOS bigramw/ punctuation 29.75 (SVM) 35.50 (SVM) 40.39 (Na?
?ve Bayes) 41.11 (Na?
?ve Bayes)w/o punctuation 25.47 (SVM) 31.41 (SVM) 33.33 (Na?
?ve Bayes) 33.78 (Na?
?ve Bayes)POS trigramw/ punctuation 29.20 (SVM) 33.28 (SVM) 38.98 (Na?
?ve Bayes) 43.74 (Na?
?ve Bayes)w/o punctuation 23.71 (SVM) 28.98 (SVM) 32.21 (SVM) 37.49 (Na?
?ve Bayes)Character unigramw/ space N/A N/A N/A N/Aw/o space N/A N/A N/A N/ACharacter bigramw/ space 15.26 (SVM) 23.69 (SVM) 40.07 (SVM) 41.76 (SVM)w/o space 15.73 (SVM) 25.27 (SVM) 37.05 (SVM) 41.52 (SVM)Character trigramw/ space 20.42 (SVM) 28.17 (SVM) 37.61 (SVM) 47.93 (SVM)w/o space 23.85 (SVM) 30.38 (SVM) 37.39 (SVM) 45.60 (SVM)Table 4: Performance of binary presence/absence indicator on n-gram features.
Stratified ten-fold cross-validationaccuracy values on TOEFL11 training set are shown, along with the classifiers that achieved these accuracy values.Best results in different columns are boldfaced.
Table cells marked ?N/A?
are the ones that correspond to an n-gramdictionary size < k.Word Network FeatureBest Cross-validation Accuracy (%) on Top k Most Frequent Wordsk = 100 k = 200 k = 500 k = 1000Clustering Coefficient 15.31 (SVM) 17.73 (SVM) 19.96 (SVM) 20.71 (SVM)In-degree 39.89 (SVM) 49.28 (SVM) 56.83 (SVM) 59.47 (SVM)Out-degree 40.66 (SVM) 49.67 (SVM) 57.16 (SVM) 59.62 (SVM)Degree 41.05 (SVM) 50.74 (SVM) 58.17 (SVM) 60.21 (SVM)In-coreness 32.52 (SVM) 42.44 (SVM) 51.09 (SVM) 55.50 (SVM)Out-coreness 32.41 (SVM) 43.15 (SVM) 51.34 (SVM) 55.39 (SVM)Coreness 35.32 (SVM) 45.84 (SVM) 53.54 (SVM) 57.18 (SVM)In-neighborhood Size40.54 (SVM) 50.08 (SVM) 56.92 (SVM) 59.69 (SVM)(order 1)Out-neighborhood Size41.09 (SVM) 50.09 (SVM) 57.71 (SVM) 59.73 (SVM)(order 1)Neighborhood Size41.83 (SVM) 50.68 (SVM) 57.40 (SVM) 60.41 (SVM)(order 1)Table 5: Performance of word network features.
Stratified ten-fold cross-validation accuracy values on TOEFL11training set are shown, along with the classifiers that achieved these accuracy values.
Best results in different columnsare boldfaced.256Rank Word Network Feature Information Gain1 Degree of the word a 0.10582 Neighborhood size of the word a 0.10543 Out-neighborhood size of the word a 0.10504 Outdegree of the word a 0.10495 In-neighborhood size of the word a 0.10176 Indegree of the word a 0.10167 Neighborhood size of the word however 0.09288 Degree of the word however 0.09289 Indegree of the word however 0.092810 In-neighborhood size of the word however 0.092811 Outdegree of the word however 0.091612 Out-neighborhood size of the word however 0.091613 Out-coreness of the word however 0.085114 Coreness of the word however 0.085115 In-coreness of the word however 0.085016 Outdegree of the word the 0.079317 Out-neighborhood size of the word the 0.079018 Degree of the word the 0.074019 Neighborhood size of the word the 0.074020 Coreness of the word a 0.0710Table 6: Ranking of word network features based on Information Gain, on TOEFL11 training set.
We took 1000 mostfrequent words on the training+development set, and collected all their word network features in a single file.
Thisranking reflects the top 20 features in that file, along with their information gain values.formed quite well, with the best result (60.41% CVaccuracy on the train set) being competitive against(but slightly worse than) the baseline n-gram fea-tures (62.46% CV accuracy on the train set).
Perfor-mance improved with increasing k, thereby corrob-orating our general observation from Tables 2 to 4.Clustering coefficient performed poorly, and seemsrather unsuitable for the NLI task.
But degree, core-ness, and neighborhood size performed good.
Herealso, SVM turned out to be the best classifier, givingbest CV accuracy in all cases.We experimented with the in-, out-, and over-all versions of degree, coreness and neighborhoodsize.
Their performance was mostly comparablewith each other (Table 5).
To investigate which wordnetwork features are the most discriminatory in thistask, we collected all ten word network features ofthe top 1000 words in a single file, and then rankedthose features on the training set based on Infor-mation Gain (IG).
The 20 top-ranking features areshown in Table 6, along with their correspondingIG values.
Note that the words a, the, and howeverwere among the most discriminatory, and differentversions of degree, neighborhood size and corenessappeared among the top, which is in line with ourearlier observation that clustering coefficients werenot very discriminatory at the native language clas-sification task.4 Conclusions and Future WorkIn this paper, we described experiments with the NLItask using a baseline set of n-gram features, and aset of novel features derived from a word networkrepresentation of text documents.
Useful and lessuseful n-gram features were identified, along withthe fact that SVM was the best classifier in mostof the cases.
We learned that when using raw ornormalized frequency, lower-order n-grams performat least as good as higher-order n-grams; moreover,Na?
?ve Bayes sometimes give good results when bi-nary presence/absence indicator variables are usedas features.We described the construction of our word net-works in detail, and discussed experiments withword network features.
These features are compet-itive against the baseline n-gram features, and weneed to fine-tune our classifiers to see if they canexceed the performance of the baseline.
Cluster-ing coefficients were found to be less useful for theNLI task, and feature ranking based on information257gain helped us identify the most important word net-work features in a collection of top 1000 words inthe training+development set.Future work consists of experimenting with com-bined word network features; mixed word networkfeatures and baseline n-gram features; and the one-vs-all classification scheme instead of the multiclassclassification scheme.ReferencesVladimir Batagelj and Matjaz Zaversnik.
2003.
AnO(m) Algorithm for Cores Decomposition of Net-works.
CoRR, cs.DS/0310049.Shane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric Analysis of Scientific Articles.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 327?337, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Yves Bestgen, Sylviane Granger, and Jennifer Thewis-sen. 2012.
Error Patterns and Automatic L1 Identifi-cation.
In Scott Jarvis and Scott A. Crossley, editors,Approaching Language Transfer through Text Classi-fication, pages 127?153.
Multilingual Matters.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Conferenceof Learner Corpus Research (LCR2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Julian Brooke and Graeme Hirst.
2012a.
MeasuringInterlanguage: Native Language Identification withL1-influence Metrics.
In Nicoletta Calzolari, KhalidChoukri, Thierry Declerck, Mehmet Ug?ur Dog?an,Bente Maegaard, Joseph Mariani, Jan Odijk, and Ste-lios Piperidis, editors, Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation (LREC-2012), pages 779?784, Istanbul,Turkey, May.
European Language Resources Associ-ation (ELRA).
ACL Anthology Identifier: L12-1016.Julian Brooke and Graeme Hirst.
2012b.
Robust, Lex-icalized Native Language Identification.
In Proceed-ings of COLING 2012, pages 391?408, Mumbai, In-dia, December.
The COLING 2012 Organizing Com-mittee.Serhiy Bykh and Detmar Meurers.
2012.
Native Lan-guage Identification using Recurring n-grams ?
In-vestigating Abstraction and Domain Dependence.
InProceedings of COLING 2012, pages 425?440, Mum-bai, India, December.
The COLING 2012 OrganizingCommittee.Scott A. Crossley and Danielle McNamara.
2012.
De-tecting the First Language of Second Language Writ-ers Using Automated Indices of Cohesion, LexicalSophistication, Syntactic Complexity and ConceptualKnowledge.
In Scott Jarvis and Scott A. Crossley,editors, Approaching Language Transfer through TextClassification, pages 106?126.
Multilingual Matters.Gabor Csardi and Tamas Nepusz.
2006.
The igraph soft-ware package for complex network research.
Inter-Journal, Complex Systems:1695.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007a.
Author pro-filing for English emails.
In Proceedings of the 10thConference of the Pacific Association for Computa-tional Linguistics, pages 263?272, Melbourne, Aus-tralia.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007b.
TAT: An Au-thor Profiling Tool with Application to Arabic Emails.In Proceedings of the Australasian Language Technol-ogy Workshop 2007, pages 21?30, Melbourne, Aus-tralia, December.Felix Golcher and Marc Reznicek.
2011.
Stylometry andthe interplay of topic and L1 in the different annotationlayers in the FALKO corpus.
QITL-4?Proceedings ofQuantitative Investigations in Theoretical Linguistics,4:29?34.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Ramon Ferrer i Cancho and Ricard V. Sole?.
2001.
TheSmall World of Human Language.
Proceedings: Bio-logical Sciences, 268(1482):pp.
2261?2265.Scott Jarvis and Scott A. Crossley, editors.
2012.
Ap-proaching Language Transfer Through Text Classifica-tion: Explorations in the Detection-based Approach,volume 64.
Multilingual Matters Limited, Bristol,UK.Scott Jarvis, Yves Bestgen, Scott A. Crossley, Syl-viane Granger, Magali Paquot, Jennifer Thewissen,and Danielle McNamara.
2012.
The Comparativeand Combined Contributions of n-Grams, Coh-MetrixIndices and Error Types in the L1 Classification ofLearner Texts.
In Scott Jarvis and Scott A. Crossley,editors, Approaching Language Transfer through TextClassification, pages 154?177.
Multilingual Matters.Patrick Juola.
2006.
Authorship Attribution.
Found.Trends Inf.
Retr., 1(3):233?334, December.258Vlado Kes?elj, Fuchun Peng, Nick Cercone, and CalvinThomas.
2003.
N-gram-based author profiles for au-thorship attribution.
In Proceedings of the ConferencePacific Association for Computational Linguistics, PA-CLING, volume 3, pages 255?264.Ekaterina Kochmar.
2011.
Identification of a writer?s na-tive language by error analysis.
Master?s thesis, Uni-versity of Cambridge.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, pages 624?628, Chicago, IL.ACM.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2009.
Computational methods in authorship attribu-tion.
J.
Am.
Soc.
Inf.
Sci.
Technol., 60(1):9?26, Jan-uary.Yutaka Matsuo, Yukio Ohsawa, and Mitsuru Ishizuka.2001.
A Document as a Small World.
In Proceedingsof the Joint JSAI 2001 Workshop on New Frontiers inArtificial Intelligence, pages 444?448, London, UK,UK.
Springer-Verlag.Rada Mihalcea and Paul Tarau.
2004.
TextRank: Bring-ing Order into Texts.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 404?411,Barcelona, Spain, July.
Association for ComputationalLinguistics.Xuan-Hieu Phan.
2006.
CRFTagger: CRF English POSTagger.Efstathios Stamatatos.
2009.
A survey of modern author-ship attribution methods.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60(3):538?556, March.Benjamin Swanson and Eugene Charniak.
2012.
Na-tive Language Detection with Tree Substitution Gram-mars.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 193?197, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A Report on the First Native Language IdentificationShared Task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA, June.
Association forComputational Linguistics.Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from?round here, are you?
: naive Bayes de-tection of non-native utterance text.
In Proceedings ofthe second meeting of the North American Chapter ofthe Association for Computational Linguistics on Lan-guage technologies, pages 1?8, Pittsburgh, PA. Asso-ciation for Computational Linguistics.Rosemary Torney, Peter Vamplew, and John Yearwood.2012.
Using psycholinguistic features for profilingfirst language of authors.
Journal of the Ameri-can Society for Information Science and Technology,63(6):1256?1269.Hans van Halteren and Nelleke Oostdijk.
2004.
Linguis-tic profiling of texts for the purpose of language ver-ification.
In Proceedings of Coling 2004, pages 966?972, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Sze-Meng Jojo Wong and Mark Dras.
2009.
ContrastiveAnalysis and Native Language Identification.
In Pro-ceedings of the Australasian Language Technology As-sociation Workshop 2009, pages 53?61, Sydney, Aus-tralia, December.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic Modeling for Native Language Identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124, Canberra, Australia, December.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 699?709, Jeju Island, Korea,July.
Association for Computational Linguistics.259
