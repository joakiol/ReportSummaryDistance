Proceedings of NAACL-HLT 2013, pages 715?720,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAtypical Prosodic Structure as an Indicator of Reading Leveland Text DifficultyJulie Medero and Mari OstendorfElectrical Engineering DepartmentUniversity of WashingtonSeattle, WA 98195 USA{jmedero,ostendor}@uw.eduAbstractAutomatic assessment of reading abilitybuilds on applying speech recognition toolsto oral reading, measuring words correct perminute.
This work looks at more fine-grainedanalysis that accounts for effects of prosodiccontext using a large corpus of read speechfrom a literacy study.
Experiments show thatlower-level readers tend to produce relativelymore lengthening on words that are not likelyto be final in a prosodic phrase, i.e.
in lessappropriate locations.
The results have impli-cations for automatic assessment of text dif-ficulty in that locations of atypical prosodiclengthening are indicative of difficult lexicalitems and syntactic constructions.1 IntroductionFluent reading is known to be a good indicator ofreading comprehension, especially for early readers(Rasinski, 2006), so oral reading is often used toevaluate a student?s reading level.
One method thatcan be automated with speech recognition technol-ogy is the number of words that a student can readcorrectly of a normed passage, or Words CorrectPer Minute (WCPM) (Downey et al 2011).
SinceWCPM depends on speaking rate as well as liter-acy, we are interested in identifying new measuresthat can be automatically computed for use in com-bination with WCPM to provide a better assessmentof reading level.
In particular, we investigate fine-grained measures that, if useful in identifying pointsof difficulty for readers, can lead to new approachesfor assessing text difficulty.The WCPM is reduced when a person repeats orincorrectly reads a word, but also when they in-troduce pauses and articulate words more slowly.Pauses and lengthened articulation can be an indi-cator of uncertainty for a low-level reader, but thesephenomena are also used by skilled readers to markprosodic phrase structure, facilitating comprehen-sion in listeners.
Since prosodic phrase boundariestend to occur in locations that coincide with certainsyntactic constituent boundaries, it is possible to au-tomatically predict prosodic phrase boundary loca-tions from part-of-speech labels and syntactic struc-ture with fairly high reliability for read news stories(Ananthakrishnan and Narayanan, 2008).
Thus, wehypothesize that we can more effectively leverageword-level articulation and pause information by fo-cusing on words that are less likely to be associ-ated with prosodic phrase boundaries.
By compar-ing average statistics of articulation rate and paus-ing for words at boundary vs. non-boundary loca-tions, we hope to obtain a measure that could aug-ment reading rate for evaluating reading ability.
Wealso hypothesize that the specific locations of hesita-tion phenomena (word lengthening and pausing) ob-served for multiple readers will be indicative of par-ticular points of difficulty in a text, either because aword is difficult or because a syntactic constructionis difficult.
Detecting these regions and analyzingthe associated lexical and syntactic correlates is po-tentially useful for automatically characterizing textdifficulty.Our study of hesitation phenomena involves em-pirical analysis of the oral reading data from the Flu-ency Addition to the National Assessment of Adult715Literacy (FAN), which collected oral readings fromroughly 12,000 adults, reading short (150-200 word)fourth- and eighth grade passages (Baer et al 2009).The participants in that study were chosen to re-flect the demographics of adults in the United States;thus, speakers of varying reading levels and non-native speakers were included.
For our study, wehad access to time alignments of automatic tran-scriptions, but not the original audio files.2 Related WorkFor low-level readers, reading rate and fluency aregood indicators of reading comprehension (Millerand Schwanenflugel, 2006; Spear-Swerling, 2006).Zhang and colleagues found that features of chil-dren?s oral readings, along with their interactionswith an automated tutor, could predict a single stu-dent?s comprehension question performance overthe course of a document (2007).
Using oral read-ings is appealing because it avoids the difficulty ofseparating question difficulty from passage difficulty(Ozuru et al 2008) and of questions that can be an-swered through world knowledge (Keenan and Bet-jemann, 2006).WCPM is generally used as a tool for assessingreading level by averaging across one or more pas-sages.
It is more noisy when comparing the read-ability of different texts, especially when the readinglevel is measured at a fine-grained (e.g.
word) level.If longer words take longer to read orally, it maybe merely a consequence of having more phonemes,and not of additional reading difficulty.
Further,for communication reasons, pauses and slow aver-age articulation rates tend to coincide with majorphrase boundaries.
In our work, we would like to ac-count for prosodic context in using articulation rateto identify difficult words and constructions.Much of the previous work on using automaticspeech recognition (ASR) output for reading levelor readability analysis has focused on assessing thereading level of children (Downey et al 2011;Duchateau et al 2007).
Similar success has beenseen in predicting fluency scores in oral readingtests for L2 learners of English (Balogh et al 2012;Bernstein et al 2011).
Project LISTEN has a read-ing tutor for children that gives real-time feedback,and has used orthographic and phonemic featuresof individual words to predict the likelihood of realword subsitutions (Mostow et al 2002).3 FAN Literacy ScoresTo examine the utility of word-level pause and ar-ticulation rate features for predicting reading levelwhen controlled for prosodic context, we use the Ba-sic Reading Skills (BRS) score available for eachreader in the FAN data.
The BRS score measuresan individual?s average reading rate in WCPM.
Eachparticipant read three word lists, three pseudo-wordlists, one easy text passage, and one harder text pas-sage, and the BRS is the average WCPM over theeight different readings.
Specifically, the WCPMfor each case is computed automatically using Ordi-nate?s VersaReader system to transcribe the speechgiven the target text (Balogh et al 2005).
The sys-tem output is then automatically aligned to the tar-get texts using the track-the-reader method of Ras-mussen et al(2011), which defines weights for re-gressions and skipped words and then identifies aleast-cost alignment between the ASR output and atext.
Automatic calculation of WCPM has high cor-relation (.96-1.0) with human judgment of WCPM(Balogh et al 2012), so it has the advantage of be-ing easy to automate.Word Error Rate (WER) for the the ASR compo-nent in Ordinate?s prototype reading tracker (Baloghet al 2012) may be estimated to be between 6% and10%.
In a sample of 960 passage readings, wherevarious sets of two passages were read by each of480 adults (160 native Spanish speakers, 160 nativeEnglish-speaking African Americans, and 160 othernative English speakers), the Ordinate ASR systemexhibited a 6.9% WER on the 595 passages that con-tained no spoken material that was unintelligible tohuman transcribers.
On the complete set of 960 pas-sages, the system exhibited a 9.9% WER, with eachunintelligible length of speech contributing one ormore errors to the word error count.The greatest problem with speech recognition er-rors is for very low-level readers (Balogh et al2012).
In order to have more reliable time align-ments and BRS scores, approximately 15% of theFAN participants were excluded from the currentanalysis.
This 15% were those participants whoseBRS score was labeled ?Below Basic?
in the NAAL716reading scale.
Additional participants were elimi-nated because of missing or incomplete (less than afew seconds) recordings.
With these exclusions, thenumber of speakers in our study was 7587.4 Prosodic Boundary PredictionWe trained a regression tree1 on hand-annotateddata from the Boston University Radio News Cor-pus (Ostendorf et al 1995) to predict the locationswhere we expect to see prosodic boundaries.
Eachword in the Radio News Corpus is labeled with aprosodic boundary score from 0 (clitic, no bound-ary) to 6 (sentence boundary).
For each word, weuse features based on parse depth and structure andPOS bigrams to predict the prosodic boundary value.For evaluation, the break labels are grouped into:0-2 (no intonational boundary marker), 3 (intermedi-ate phrase), and 4-6 (intonational phrase boundary).Words with 0-2 breaks are considered non-boundarywords; 4-6 are boundary words.
We expect that, forfluent readers, lengthening and possibly pausing willbe observed after boundary words but not after non-boundary words.
Since the intermediate boundariesare the most difficult to classify, and may be can-didates for both boundaries and non-boundaries forfluent readers, we omit them in our analyses.
Ourmodel achieves 87% accuracy in predicting ?
in-tonational phrase boundaries and 83% accuracy inpredicting ?
no intonational boundary, treating in-termediate phrase boundaries as negative instancesin both cases.Note that our 3-way prosodic boundary predic-tion is aimed at identifying locations where fluentreaders are likely to place boundaries (or not), i.e.,reliable locations for feature extraction, vs. accept-able locations for text-to-speech synthesis.
Becauseof this goal and because work on prosodic bound-ary prediction labels varies in its treatment of inter-mediate phrase boundaries, our results are not di-rectly comparable to prior studies.
However, per-formance is in the range reported in recent studiespredicting prosodic breaks from text features only.Treating intermediate phrase boundaries as positiveexamples, Ananthakrishnan and Narayanan (2008)1Our approach differs slightly from previous work in the useof a regression (vs. classification) model; this gave a small per-formance gain.achieve 88% accuracy.
Treating them as negativeexamples, Margolis and Ostendorf (2010) achievesimilar results.
Both report results on a single held-out test set, while our results are based on 10-foldcross validation.5 Experiments with Prosodic Context5.1 Word-level Rate FeaturesWe looked at two acoustic cues related to hesitationor uncertainty: pause duration and word lengthen-ing.
While pause duration is straightforward to ex-tract (and not typically normalized), various meth-ods have been used for word lengthening.
We ex-plore two measures of word lengthening: i) thelongest normalized vowel, and ii) the average nor-malized length of word-final phones (the last voweland all following consonants).
Word-final length-ening is known to be a correlate of fluent prosodicphrase boundaries (Wightman et al 1992), andwe hypothesized that the longest normalized vowelmight be useful for hesitations though it can also in-dicate prosodic prominence.For word-level measures of lengthening, it is stan-dard to normalize to account for inherent phonemedurations.
We use a z-score: measured duration mi-nus phone mean divided by phone standard devia-tion.
In addition, Wightman et al(1992) found ituseful to account for speaking rate in normalizingphone duration.
We adopt the same model, whichassumes that phone durations can be characterizedby a Gamma distribution and that speaker variabil-ity is characterized by a linear scaling of the phone-dependent mean parameters, where the scaling termis shared by all phones.
The linear scale factor ?
fora speaker is estimated as:?
=1NN?i=1di?p(i)(1)where di is the duration of the i-th phone which haslabel p(i) and where ?p is the speaker-independentmean of phone p. Here, we use a speaker-independent phone mean computed from the TIMITCorpus,2 which has hand-marked phonetic labelsand times.
We make use of the speaking rate model2Available from the Linguistic Data Consortium.717to adjust the speaker-independent TIMIT phone du-rations to the speakers in the FAN corpus by cal-culating the linear scale factor ?
for each speaker.Thus, the phone mean and standard deviation usedin the z-score normalization is ?
?pi and ?
?pi , re-spectively.From the many readings of the eight passages, weidentified roughly 777K spoken word instances atpredicted phrase boundaries and 2.0M spoken wordsat predicted non-boundaries.
For each uttered word,we calculated three features: the length of the fol-lowing pause, the length of the longest normalizedvowel, and the averaged normed length of all phonesfrom the last vowel to the end of the word, as de-scribed above.
The word-level features can be av-eraged across instances from a speaker for assessingreading level or across instances of a particular wordin a text uttered by many speakers to assess local textdifficulty.The phone and pause durations are based on rec-ognizer output, so they will be somewhat noisy.The fact that the recognizer is biased towards theintended word sequence and the omission of thelowest-level readers from this study together con-tribute to reducing the error rate (< 10%) and in-creasing the reliability of the features.
In addition,noise is reduced by averaging over multiple wordsor multiple speakers.5.2 Reading Level AnalysisTo assess the potential for prosodic context to im-prove the utility of word-level features for assessingreading difficulty, we looked at duration lengtheningand pauses at boundary and non-boundary locations,where the boundary labels are predicted using thetext-based algorithm and 3-class grouping describedin section 4.First, for each speaker, we averaged each fea-ture across all boundary words read by that personand across all non-boundary words read by that per-son.
We hypothesized that skilled readers wouldhave shorter averages for all three features at non-boundary words compared to at boundary words,while the differences for lower-level readers wouldbe smaller because of lengthening due to uncertaintyat non-boundary words.
The difference between theboundary and non-boudnary word averages for nor-malized duration of end-of-word phones is plotted inFigure 1: Mean end-of-word normalized phone duration(+/- standard deviation) as a function of BRS scoreFigure 1 as a function of reading level.
As expected,the difference increases with reading skill, as mea-sured by BRS.
A similar trend is observed for thelongest normalized vowel in the word.We also looked at pause duration, finding that theaverage pause duration decreases as reading skill in-creases for both boundary and non-boundary words.Since pauses are not always present at intonationalphrase boundaries, but are more likely at sentenceboundaries, we investigated dividing the cases bypunctuation rather than prosodic context.
Table 1shows that for both the top 20% of readers and thebottom 20% of readers, sentence boundaries hadmuch longer pauses on average, followed by commaboundaries, and unpunctuated word boundaries.
Thedrop in both pause frequency and average pause du-ration is much greater for the more skilled readers.Looking at all speakers, the unpunctuated wordshad an average pause duration that scaled with thespeaking rate estimate for that passage, with highcorrelation (0.94).
The correlation was much lowerfor sentence boundaries (0.44).
Thus, we concludethat the length of pauses at non-boundary locationsis related to the speaker?s reading ability.5.3 Identifying Difficult TextsInstead of averaging over multiple words in a pas-sage, we can average over multiple readings of aparticular word.
We identified difficult regions intexts by sorting all tokens by the average normalizedlength of their end-of-word phones for the lowest718Top 20% Bottom 20%Pause Rate Avg.
Pause Duration Pause Rate Avg.
Pause DurationSentence-final 81.0% 177 ms 84.7% 283 msComma 26.1% 94 ms 47.0% 168 msNo punctuation 4.6% 77 ms 16.6% 139 msTable 1: Frequency of occurrence and average duration of pauses at sentence boundaries, comma boundaries, andunpunctuated word boundaries for the top and bottom 20% of all readers, as sorted by BRS score20% of readers.
The examples suggest that length-ening may coincide with reading difficulty causedby syntactic ambiguity.
Two sentences, with thelengthened word in bold, illustrate representativeambiguities:?
She was there for me the whole time mygrandfather was in the hospital.?
Since dogs are gentler when raised by a fam-ily the dogs are given to children when the dogsare about fourteen months old.In the first example, ?me?
could be the end of thesentence, while in the second example, readers mayexpect ?gentler?
to be the end of the subordinateclause started by ?since?.
The lengthening on thesewords is much smaller for the top 20% of readers,suggesting that the extra lengthening is associatedwith points of difficulty for the less skilled readers.Similarly, we identified sentences with non-boundary locations where readers commonlypaused, with the word after the pause in bold:?
We have always been able to share our es-capades and humor with our friends.?
Check with your doctor first if you are a manover forty or a woman over fifty and you planto do vigorous activity instead of moderate ac-tivity.We observe a wider variety of potential difficultieshere.
Some are associated with difficult words, as inthe first example, while others involve syntactic am-biguities similar to the ones seen in the lengtheningcases.6 SummaryWe have shown that duration lengthening and pausecues align with expected prosodic structure (pre-dicted from syntactic features) more for skilled read-ers than for low-level readers, which we hope maylead to a richer assessment of individual reading dif-ficulties.
In addition, we have proposed a methodof characterizing text difficulty at a fine grain basedon these features using multiple oral readings.
In or-der to better understand the information provided bythe different features, we are conducting eye track-ing experiments on these passages, and future workwill include an analysis of readers?
gaze during read-ing of these constructions that have been categorizedin terms of their likely prosodic context.In this work, where the original recordings werenot available, the study was restricted to durationfeatures.
However, other work has suggested thatother prosodic cues, particularly pitch and energyfeatures, are useful for detecting speaker uncertainty(Litman et al 2009; Litman et al 2012; Pon-Barryand Shieber, 2011).
Incorporating these cues mayincrease the reliability of detecting points of read-ing difficulty and/or offer complementary informa-tion for characterizing text difficulties.AcknowledgmentsWe are grateful to the anonymous reviewers for theirfeedback, and to our colleagues at Pearson Knowl-edge Technologies for their insights and data pro-cessing assistance.
This material is based uponwork supported by the National Science Founda-tion Graduate Research Fellowship under Grant No.DGE-0718124 and by the National Science Founda-tion under Grant No.
IIS-0916951.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation.ReferencesS.
Ananthakrishnan and S.S. Narayanan.
2008.
Auto-matic Prosodic Event Detection Using Acoustic, Lex-ical, and Syntactic Evidence.
IEEE Trans.
Audio,Speech, and Language Processing, 16(1):216?228.719J.
Baer, M. Kutner, J. Sabatini, and S. White.
2009.
BasicReading Skills and the Literacy of Americas Least Lit-erate Adults: Results from the 2003 National Assess-ment of Adult Literacy (NAAL) Supplemental Stud-ies.
Technical report, NCES.J.
Balogh, J. Bernstein, J. Cheng, and B. Townshend.2005.
Final Report Ordinates Scoring of FAN NAALPhase III: Accuracy Analysis.
Technical report, Ordi-nate.J.
Balogh, J. Bernstein, J. Cheng, A.
Van Moere,B.
Townshend, and M. Suzuki.
2012.
Validation ofAutomated Scoring of Oral Reading.
Educational andPsychological Measurement, 72:435?452.J.
Bernstein, J. Cheng, and M. Suzuki.
2011.
FluencyChanges with General Progress in L2 Proficiency.
InProc.
Interspeech, number August, pages 877?880.R.
Downey, D. Rubin, J. Cheng, and J. Bernstein.2011.
Performance of Automated Scoring for Chil-dren?s Oral Reading.
Proc.
Workshop on InnovativeUse of NLP for Building Educational Applications,(June):46?55, June.J.
Duchateau, L. Cleuren, H. Van, and P. Ghesqui.
2007.Automatic Assessment of Childrens Reading Level.Proc.
Interspeech, pages 1210?1213.J.M.
Keenan and R. Betjemann.
2006.
Comprehendingthe Gray Oral Reading Test Without Reading It: WhyComprehension Tests Should Not Include Passage-Independent Items.
Scientific Studies of Reading,10(4):363?380.D.
Litman, M. Rotaru, and G. Nicholas.
2009.
Classi-fying turn-level uncertainty using word-level prosody.In Proc.
Interspeech.D.
Litman, H. Friedberg, and K. Forbes-Riley.
2012.Prosodic cues to disengagement and uncertainty inphysics tutorial dialogues.
In Proc.
Interspeech.A.
Margolis, M. Ostendorf, and K. Livescu.
2010.
Cross-genre training for automatic prosody classification.
InProc.
Speech Prosody Conference.J.
Miller and P.J.
Schwanenflugel.
2006.
Prosody of Syn-tactically Complex Sentences in the Oral Reading ofYoung Children.
Journal of Educational Psychology,98(4):839?843.J.
Mostow, J. Beck, S. Winter, S. Wang, and B. Tobin.2002.
Predicting Oral Reading Miscues.
In Proc.
IC-SLP.M.
Ostendorf, P.J.
Price, and S. Shattuck-Hufnagel.1995.
The Boston University Radio News Corpus.Technical report, Boston University, March.Y.
Ozuru, M. Rowe, T. O?Reilly, and D.S.
McNamara.2008.
Where?s the difficulty in standardized readingtests: the passage or the question?
Behavior ResearchMethods, 40(4):1001?1015.H.
Pon-Barry and S.M.
Shieber.
2011.
Recognizing un-certainty in speech.
CoRR, abs/1103.1898.T.
Rasinski.
2006.
Reading fluency instruction: Mov-ing beyond accuracy, automaticity, and prosody.
TheReading Teacher, 59(7):704?706, April.M.H.
Rasmussen, J. Mostow, Z. Tan, B. Lindberg, andY.
Li.
2011.
Evaluating Tracking Accuracy of an Au-tomatic Reading Tutor.
In Proc.
Speech and LanguageTechnology in Education Workshop.L.
Spear-Swerling.
2006.
Childrens Reading Com-prehension and Oral Reading Fluency in Easy Text.Reading and Writing, 19(2):199?220.C.W.
Wightman, S. Shattuck-Hufnagel, M. Ostendorf,and P.J.
Price.
1992.
Segmental durations in the vicin-ity of prosodic phrase boundaries.
The Journal of theAcoustical Society of America, 91(3):1707?-1717.X.N.
Zhang, J. Mostow, and J.E.
Beck.
2007.
Can aComputer Listen for Fluctuations in Reading Com-prehension?
Artificial Intelligence in Education,158:495?502.720
