Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 163?168,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsAutomated Scoring of a Summary Writing TaskDesigned to Measure Reading ComprehensionNitin Madnani, Jill Burstein, John Sabatini and Tenaha O?ReillyEducational Testing Service660 Rosedale Road, Princeton, NJ 08541, USA{nmadnani,jburstein,jsabatini,toreilly}@ets.orgAbstractWe introduce a cognitive framework for mea-suring reading comprehension that includesthe use of novel summary writing tasks.
Wederive NLP features from the holistic rubricused to score the summaries written by stu-dents for such tasks and use them to design apreliminary, automated scoring system.
Ourresults show that the automated approach per-forms well on summaries written by studentsfor two different passages.1 IntroductionIn this paper, we present our preliminary work onautomatic scoring of a summarization task that is de-signed to measure the reading comprehension skillsof students from grades 6 through 9.
We first intro-duce our underlying reading comprehension assess-ment framework (Sabatini and O?Reilly, In Press;Sabatini et al In Press) that motivates the task ofwriting summaries as a key component of such as-sessments in ?2.
We then describe the summariza-tion task in more detail in ?3.
In ?4, we describe ourapproach to automatically scoring summaries writ-ten by students for this task and compare the resultswe obtain using our system to those obtained by hu-man scoring.
Finally, we conclude in ?6 with a briefdiscussion and possible future work.2 Reading for Understanding (RfU)FrameworkWe claim that to read for understanding, readersshould acquire the knowledge, skills, strategies, anddispositions that will enable them to:?
learn and process the visual and typographicalelements and conventions of printed texts andprint world of literacy;?
learn and process the verbal elements of lan-guage including grammatical structures andword meanings;?
form coherent mental representations of texts,consistent with discourse, text structures, andgenres of print;?
model and reason about conceptual content;?
model and reason about social content.We also claim that the ability to form a coher-ent mental model of the text that is consistent withtext discourse is a key element of skilled reading.This mental model should be concise but also reflectthe most likely intended meaning of the source.
Wemake this claim since acquiring this ability:1. requires the reader to have knowledge ofrhetorical text structures and genres;2. requires the reader to model the propositionalcontent of a text within that rhetorical frame,both from an author?s or reader?s perspective;and3.
is dependent on a skilled reader having ac-quired mental models for a wide variety ofgenres, each embodying specific strategies formodeling the meaning of the text sources toachieve reading goals.In support of the framework, research has shownthat the ability to form a coherent mental model163is important for reading comprehension.
Kintsch(1998) showed that it is a key aspect in the process ofconstruction integration and essential to understand-ing the structure and organization of the text.
Sim-ilarly, Gernsbacher (1997) considers mental modelsessential to structure mapping and in bridging andmaking knowledge-based inferences.2.1 Assessing Mental ModelsGiven the importance of mental models for readingcomprehension, the natural question is how does oneassess whether a student has been able to build suchmodels after reading a text.
We believe that suchan assessment must encompass asking a reader to(a) sample big ideas by asking them to describe themain idea or theme of a text, (b) find specific detailsin the text using locate/retrieve types of questions,and (c) bridging gaps between different points in thetext using inference questions.
Although these ques-tions can be multiple-choice, existing research indi-cates that it is better to ask the reader to write a briefsummary of the text instead.
Yu (2003) states thata good summary can prove useful for assessment ofreading comprehension since it contains the relevantimportant ideas, distinguishes accurate informationfrom opinions, and reflects the structure of the textitself.
More specifically, having readers write sum-maries is a promising solution since:?
there is considerable empirical support that itboth measures and encourages reading compre-hension and is an effective instructional strat-egy to help students improve reading skills(Armbruster et al 1989; Bean and Steenwyk,1984; Duke and Pearson, 2002; Friend, 2001;Hill, 1991; Theide and Anderson, 2003);?
it is a promising technique for engaging stu-dents in building mental models of text; and?
it aligns with our framework and cognitive the-ory described earlier in this section.However, asking students to write summaries in-stead of answering multiple choice questions entailsthat the summaries must be scored.
Asking humanraters to score these summaries, however, can betime consuming as well as costly.
A more cost-effective and efficient solution would be to use anautomated scoring technique using machine learn-ing and natural language processing.
We describesuch a technique in the subsequent sections.During the Neolithic Age, humans developed agriculture-what wethink of as farming.
Agriculture meant that people stayed in oneplace to grow their crops.
They stopped moving from place toplace to follow herds of animals or to find new wild plants to eat.And because they were settling down, people built permanentshelters.
The caves they had found and lived in before could bereplaced by houses they built themselves.To build their houses, the people of this Age often stacked mudbricks together to make rectangular or round buildings.
At first,these houses had one big room.
Gradually, they changed toinclude several rooms that could be used for different purposes.People dug pits for cooking inside the houses.
They may havefilled the pits with water and dropped in hot stones to boil it.
Youcan think of these as the first kitchens.The emergence of permanent shelters had a dramatic effect onhumans.
They gave people more protection from the weather andfrom wild animals.
Along with the crops that provided more foodthan hunting and gathering, permanent housing allowed people tolive together in larger communities.Please write a summary.
The first sentence of your summaryshould be about the whole passage.
Then write 3 moresentences.
Each sentence should be about one of theparagraphs.PassageDirectionsFigure 1: An example passage for which students areasked to write a summary, and the summary-writing di-rections shown to the students.3 Summary Writing TaskBefore describing the automated scoring approach,we describe the details of the summary writing taskitself.
The summarization task is embedded withina larger reading comprehension assessment.
As partof the assessment, students read each passage andanswer a set of multiple choice questions and, in ad-dition, write a summary for one of the passages.
Anexample passage and the instructions can be seen inFigure 1.
Note the structured format of summarythat is asked for in the directions: the first sentenceof the summary must be about the whole passageand the next three should correspond to each of theparagraphs in the passage.
All summary tasks arestructured similarly in that the first sentence shouldidentify the ?global concept?
of the passage and the164next three sentences should identify ?local concepts?corresponding to main points of each subsequentparagraph.Each summary written by a student is scored ac-cording to a holistic rubric, i.e., based on holisticcriteria rather than criteria based on specific dimen-sions of summary writing.
The scores are assignedon a 5-point scale which are defined as:Grade 4: summary demonstrates excellent globalunderstanding and understanding of all 3 lo-cal concepts from the passage; does not includeverbatim text (3+ words) copied from the pas-sage; contains no inaccuracies.Grade 3: summary demonstrates good global un-derstanding and demonstrates understanding ofat least 2 local concepts; may or may not in-clude some verbatim text, contains no morethan 1 inaccuracy.Grade 2: summary demonstrates moderate localunderstanding only (2-3 local concepts but noglobal); with or without verbatim text, containsno more than 1 inaccuracy; OR good global un-derstanding only with no local conceptsGrade 1: summary demonstrates minimal localunderstanding (1 local concept only), with orwithout verbatim text; OR contains only verba-tim textGrade 0: summary is off topic, garbage, or demon-strates no understanding of the text; OR re-sponse is ?I don?t know?
or ?IDK?.Note that students had the passage in front of themwhen writing the summaries and were not penalizedfor poor spelling or grammar in their summaries.
Inthe next section, we describe a system to automati-cally score these summaries.4 Automated Scoring of StudentSummariesWe used a machine learning approach to build anautomated system for scoring summaries of the typedescribed in ?3.
To train and test our system, weused summaries written by more than 2600 studentsfrom the 6th, 7th and 9th grades about two differ-ent passages.
Specifically, there were a total of 2695summaries ?
1016 written about a passage describ-ing the evolution of permanent housing through his-tory (the passage shown in Figure 1) and 1679 writ-ten about a passage describing living conditions atthe South Pole.
The distribution of the grades forthe students who wrote the summaries for each pas-sage is shown in Table 1.Passage Grade CountSouth Pole6 5747 5219 584Perm.
Housing6 3877 3059 324Table 1: The grade distribution of the students who wrotesummaries for each of the two passages.All summaries were also scored by an experi-enced human rater in accordance with the 5-pointholistic rubric described previously.
Figure 2 showsthe distribution of the human scores for both sets ofsummaries.South Pole (N=1679)Permanent Housing (N=1016)0100200300400500600700800900Score0 1 2 3 4 Score0 1 2 3 4Figure 2: A histogram illustrating the human score distri-bution of the summaries written for the two passages.Our approach to automatically scoring these sum-maries is driven by features based on the rubric.Specifically, we use the following features:1.
BLEU: BLEU (BiLingual Evaluation Under-study) (Papineni et al 2002) is an automatedmetric used extensively in automatically scor-ing the output of machine translation systems.165It is a precision-based metric that computes n-gram overlap (n=1 .
.
.
4) between the summary(treated as a single sentence) against the pas-sage (treated as a single sentence).
We chose touse BLEU since it measures how many of thewords and phrases are borrowed directly fromthe passage.
Note that some amount of borrow-ing from the passage is essential for writing agood summary.2.
ROUGE: ROUGE (Recall-Oriented Under-study for Gisting Evaluation) (Lin and Hovy,2003) is an automated metric used for scoringsummaries produced by automated documentsummarization systems.
It is a recall-basedmetric that measures the lexical and phrasaloverlap between the summary under consider-ation and a set of ?model?
(or reference) sum-maries.
We used a single model summary forthe two passages by randomly selecting eachfrom the set of student summaries assigned ascore of 4 by the human rater.3.
CopiedSumm: Ratio of the sum of lengths ofall 3-word (or longer) sequences that are copiedfrom the passage to the length of the summary.4.
CopiedPassage: Same as CopiedSumm butwith the denominator being the length of thepassage.5.
MaxCopy: Length of the longest word se-quence in the summary copied from the pas-sage.6.
FirstSent: Number of passage sentences thatthe first sentence of the summary borrows 2-word (or longer) sequences from.7.
Length: Number of sentences in the summary.8.
Coherence: Token counts of commonly useddiscourse connector words in the summary.ROUGE computes the similarity between thesummary S under consideration and a high-scoringsummary - a high value of this similarity indicatesthat S should also receive a high score.
Copied-Summ, CopiedPassage, BLEU, and MaxCopycapture verbatim copying from the passage.
First-Sent directly captures the ?global understanding?concept for the first sentence, i.e., a large value forthis feature means that the first sentence capturesmore of the passage as expected.
Length capturesthe correspondence between the number of para-graphs in the passage and the number of sentencesin the summary.
Finally, Coherence captures howwell the student is able to connect the different ?lo-cal concepts?
present in the passage.
Note that:?
Although the rubric states that students not bepenalized for spelling errors, we did not spell-correct the summaries before scoring them.
Weplan to do this for future experiments.?
The students were not explicitly told to refrainfrom verbatim copying since the summary-writing instructions indicated this implicitly(?.
.
.
about the whole passage?
and ?.
.
.
aboutone of the paragraphs?).
However, for futureexperiments, we plan to include explicit in-structions regarding copying.All features were combined in a logistic regres-sion classifier that output a prediction on the same5-point scale as the holistic rubric.
We trained a sep-arate classifier for each of the two passage types.1The 5-fold cross-validation performance of this clas-sifier on our data is shown in Table 2.
We computeexact as well as adjacent agreement of our predic-tions against the human scores using the confusionmatrices from the two classifiers.
The exact agree-ment shows the rate at which the system and thehuman rater awarded the same score to a summary.Adjacent agreement shows the rate at which scoresgiven by the system and the human rater were nomore than one score point apart (e.g., the system as-signed a score of 4 and the human rater assigned ascore of 5 or 3).
For holistic scoring using 5-pointrubrics, typical exact agreement rates are in the samerange as our scores (Burstein, 2012; Burstein et al2013).
Therefore, our system performed reasonablywell on the summary scoring task.
For comparison,we also show the exact and adjacent agreement ofthe most-frequent-score baseline.It is important to investigate whether the variousfeatures correlated in an expected manner with thescore in order to ensure that the summary-writingconstruct is covered accurately.
We examined theweights assigned to the various features in the clas-sifier and found that this was indeed the case.
As ex-pected, the CopiedSumm, CopiedPassage, BLEU,1We used the Weka Toolkit (Hall et al 2009).166Method Passage Exact AdjacentBaselineSouth Pole .51 .90Perm.
Housing .32 .77LogisticSouth Pole .65 .97Perm.
Housing .52 .93Table 2: Exact and adjacent agreements of the most-frequent-score baseline and of the 5-fold cross-validationpredictions from the logistic regression classifier, for bothpassages.and MaxCopy features all correlate negatively withscore, and ROUGE, FirstSent and Coherence cor-relate positively.In addition to overall performance, we also exam-ined which features were most useful to the classi-fier in predicting summary scores.
Table 3 shows thevarious features ranked using the information-gainmetric for both logistic regression models.
Theserankings show that the features performed consis-tently for both models.South Pole Perm.
HousingBLEU (.375) BLEU (.450)CopiedSumm (.290) ROUGE (.400)ROUGE (.264) CopiedSumm (.347)Length (.257) Length (.340)CopiedPassage (.246) MaxCopy(.253)MaxCopy (.231) CopiedPassage (.206)FirstSent (.120) Coherence (.155)Coherence (.103) FirstSent (.058)Table 3: Classifier features for both passages ranked byaverage merit values obtained using information-gain.5 Related WorkThere has been previous work on scoring summariesas part of the automated document summarizationtask (Nenkova and McKeown, 2011).
In that task,automated systems produce summaries of multipledocuments on the same topic and those machine-generated summaries are then scored by either hu-man raters or by using automated metrics such asROUGE.
In our scenario, however, the summariesare produced by students?not automated systems?and the goal is to develop an automated system toassign scores to these human-generated summaries.Although work on automatically scoring studentessays (Burstein, 2012) and short answers (Lea-cock and Chodorow, 2003; Mohler et al 2011) ismarginally relevant to the work done here, we be-lieve it is different in significant aspects based onthe scoring rubric and on the basis of the underlyingRfU framework.
We believe that the work most di-rectly related to ours is the Summary Street system(Franzke et al 2005; Kintsch et al 2007) whichattempts to score summaries written for tasks notbased on the RfU framework and uses latent seman-tic analysis (LSA) rather than a feature-based classi-fication approach.6 Conclusion & Future WorkWe briefly introduced the Reading for Understand-ing cognitive framework and how it motivates theuse of a summary writing task in a reading compre-hension assessment.
Our motivation is that such atask is theoretically suitable for capturing the abil-ity of a reader to form coherent mental representa-tions of the text being read.
We then described apreliminary, feature-driven approach to scoring suchsummaries and showed that it performed quite wellfor scoring the summaries about two different pas-sages.
Obvious directions for future work include:(a) getting summaries double-scored to be able tocompare system-human agreement against human-human agreement (b) examining whether a singlemodel trained on all the data can perform as well aspassage-specific models, and (c) using more sophis-ticated features such as TERp (Snover et al 2010)which can capture and reward paraphrasing in ad-dition to exact matches, and features that can bettermodel the ?local concepts?
part of the scoring rubric.AcknowledgmentsThe research reported here was supported by the Instituteof Education Sciences, U.S. Department of Education,through Grant R305F100005 to the Educational TestingService as part of the Reading for Understanding Re-search Initiative.
The opinions expressed are those of theauthors and do not represent views of the Institute or theU.S.
Department of Education.
We would also like tothank Kelly Bruce, Kietha Biggers and the Strategic Ed-ucational Research Partnership.167ReferencesB.
B. Armbruster, T. H. Anderson, and J. Ostertag.
1989.Teaching Text Structure to Improve Reading and Writ-ing.
Educational Leadership, 46:26?28.T.
W. Bean and F. L. Steenwyk.
1984.
The Effect ofThree Forms of Summarization Instruction on Sixth-graders?
Summary Writing and Comprehension.
Jour-nal of Reading Behavior, 16(4):297?306.J.
Burstein, J. Tetreault, and N. Madnani.
2013.
The E-rater Automated Essay Scoring System.
In M.D.
Sher-mis and J. Burstein, editors, Handbook for AutomatedEssay Scoring.
Routledge.J.
Burstein.
2012.
Automated Essay Scoring and Evalu-ation.
In Carol Chapelle, editor, The Encyclopedia ofApplied Linguistics.
Wiley-Blackwell.N.
K. Duke and P. D. Pearson.
2002.
Effective Practicesfor Developing Reading Comprehension.
In A. E.Farstrup and S. J. Samuels, editors, What Research hasto Say about Reading Instruction, pages 205?242.
In-ternational Reading Association.M.
Franzke, E. Kintsch, D. Caccamise, N. Johnson, andS.
Dooley.
2005.
Summary Street: Computer sup-port for comprehension and writing.
Journal of Edu-cational Computing Research, 33:53?80.R.
Friend.
2001.
Effects of Strategy Instruction on Sum-mary Writing of College Students.
Contemporary Ed-ucational Psychology, 26(1):3?24.M.
A. Gernsbacher.
1997.
Two Decades of StructureBuilding.
Discourse Processes, 23:265?304.P.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H. Witten.
2009.
The WEKA DataMining Software: An Update.
SIGKDD Explorations,11(1).M.
Hill.
1991.
Writing Summaries Promotes Think-ing and Learning Across the Curriculum ?
But Whyare They So Difficult to Write?
Journal of Reading,34(7):536?639.E.
Kintsch, D. Caccamise, M. Franzke, N. Johnson, andS.
Dooley.
2007.
Summary Street: Computer-guidedsummary writing.
In T. K. Landauer, D. S. McNa-mara, S. Dennis, and W. Kintsch, editors, Handbookof latent semantic analysis.
Lawrence Erlbaum Asso-ciates Publishers.W.
Kintsch.
1998.
Comprehension: A Paradigm forCognition.
Cambridge University Press.C.
Leacock and M. Chodorow.
2003.
C-rater: Auto-mated Scoring of Short-Answer Questions.
Comput-ers and the Humanities, 37(4):389?405.C.-Y.
Lin and E. H. Hovy.
2003.
Automatic Evaluationof Summaries Using N-gram Co-occurrence Statistics.In Proceedings of HLT-NAACL, pages 71?78.M.
Mohler, R. Bunescu, and R. Mihalcea.
2011.
Learn-ing to Grade Short Answer Questions using Seman-tic Similarity Measures and Dependency Graph Align-ments.
In Proceedings of ACL, pages 752?762.A.
Nenkova and K. McKeown.
2011.
Automatic Sum-marization.
Foundations and Trends in InformationRetrieval, 5(2?3):103?233.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of ACL, pages 311?318.J.
Sabatini and T. O?Reilly.
In Press.
Rationale For aNew Generation of Reading Comprehension Assess-ments.
In B. Miller, L. Cutting, and P. McCardle,editors, Unraveling the Behavioral, Neurobiological,and Genetic Components of Reading Comprehension.Brookes Publishing, Inc.J.
Sabatini, T. O?Reilly, and P. Deane.
In Press.
Prelimi-nary Reading Literacy Assessment Framework: Foun-dation and Rationale for Assessment and System De-sign.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.
2010.TER-Plus: Paraphrase, Semantic, and Alignment En-hancements to Translation Edit Rate.
Machine Trans-lation, 23:117?127.K.
W. Theide and M. C. M. Anderson.
2003.
Summariz-ing Can Improve Metacomprehension Accuracy.
Edu-cational Psychology, 28(2):129?160.G.
Yu.
2003.
Reading for Summarization as ReadingComprehension Test Method: Promises and Problems.Language Testing Update, 32:44?47.168
