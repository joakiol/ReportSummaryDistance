Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 99?103,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsHow to Evaluate Opinionated Keyphrase Extraction?Ga?bor BerendUniversity of SzegedDepartment of InformaticsA?rpa?d te?r 2., Szeged, Hungaryberendg@inf.u-szeged.huVeronika VinczeHungarian Academy of SciencesResearch Group on Artificial IntelligenceTisza Lajos krt.
103., Szeged, Hungaryvinczev@inf.u-szeged.huAbstractEvaluation often denotes a key issue insemantics- or subjectivity-related tasks.
Herewe discuss the difficulties of evaluating opin-ionated keyphrase extraction.
We present ourmethod to reduce the subjectivity of the taskand to alleviate the evaluation process andwe also compare the results of human andmachine-based evaluation.1 IntroductionEvaluation is a key issue in natural language pro-cessing (NLP) tasks.
Although for more basic taskssuch as tokenization or morphological parsing, thelevel of ambiguity and subjectivity is essentiallylower than for higher-level tasks such as questionanswering or machine translation, it is still an openquestion to find a satisfactory solution for the (auto-matic) evaluation of certain tasks.
Here we presentthe difficulties of finding an appropriate way of eval-uating a highly semantics- and subjectivity-relatedtask, namely opinionated keyphrase extraction.There has been a growing interest in the NLPtreatment of subjectivity and sentiment analysis ?see e.g.
Balahur et al (2011) ?
on the one hand andon keyphrase extraction (Kim et al, 2010) on theother hand.
The tasks themselves are demanding forautomatic systems due to the variety of the linguis-tic ways people can express the same linguistic con-tent.
Here we focus on the evaluation of subjectiveinformation mining through the example of assign-ing opinionated keyphrases to product reviews andcompare the results of human- and machine-basedevaluation on finding opinionated keyphrases.2 Related WorkAs the task we aim at involves extracting keyphrasesthat are responsible for the author?s opinion towardthe product, aspects of both keyphrase extractionand opinion mining determine our methodology andevaluation procedure.
There are several sentimentanalysis approaches that make use of manually an-notated review datasets (Zhuang et al, 2006; Liet al, 2010; Jang and Shin, 2010) and Wei andGulla (2010) constructed a sentiment ontology treein which attributes of the product and sentimentswere paired.For evaluating scientific keyphrase extraction,several methods have traditionally been applied.
Inthe case of exact match, the gold standard key-words must be in perfect overlap with the ex-tracted keywords (Witten et al, 1999; Frank et al,1999) ?
also followed in the SemEval-2010 taskon keyphrase extraction (Kim et al, 2010), whilein other cases, approximate matches or semanti-cally similar keyphrases are also accepted (Zeschand Gurevych, 2009; Medelyan et al, 2009).
In thiswork we applied the former approach for the evalu-ation of opinion phrases and made a thorough com-parison with the human judgement.Here, we use the framework introduced in Berend(2011) and conducted further experiments based onit to point out the characteristics of the evaluationof opinionated keyphrase extraction.
Here we pin-point the severe differences in performance mea-sures when the output is evaluated by humans com-pared to strict exact match principles and also exam-ine the benefit of hand-annotated corpus as opposed99to an automatically crawled one.
In addition, theextent to which original author keyphrases resemblethose of independent readers?
is also investigated inthis paper.3 MethodologyIn our experiments, we used the methodology de-scribed in Berend (2011) to extract opinionatedkeyphrase candidates from the reviews.
The sys-tem treats it as a supervised classification task us-ing Maximum Entropy classifier, in which certainn-grams of the product reviews are treated as classi-fication instances and the task is to classify them asproper or improper ones.
It incorporates a rich fea-ture set, relying on the usage of SentiWordNet (Esuliet al, 2010) and further orthological, morphologicaland syntactic features.
Next, we present the diffi-culties of opinionated keyphrase extraction and offerour solutions to the emerging problems.3.1 Author keyphrasesIn order to find relevant keyphrases in the texts,first the reviews have to be segmented into ana-lyzable parts.
We made use of the dataset de-scribed in Berend (2011), which contains 2000 prod-uct reviews each from two quite different domains,i.e.
mobile phone and video film reviews from the re-view portal epinions.com.
In the free-text partsof the reviews, the author describes his subjectivefeelings and views towards the product, and in thesections Pros and cons and Bottomline he summa-rizes the advantages and disadvantages of the prod-uct, usually by providing some keyphrases or shortsentences.
However, these pros and cons are noisysince some authors entered full sentences while oth-ers just wrote phrases or keywords.
Furthermore,the segmentation also differs from review to reviewor even within the same review (comma, semicolon,ampersand etc.).
There are also non-informativecomments such as none among cons.
For the abovereasons, the identification of the appropriate goldstandard phrases is not unequivocal.We had to refine the pros and cons of the re-views so that we could have access to a less noisydatabase.
Refinement included segmenting prosand cons into keyphrase-like units and also bring-ing complex phrases into their semantically equiva-Auth.
Ann1 Ann2 Ann3Auth.
?
0.415 0.324 0.396Ann1 0.601 ?
0.679 0.708Ann2 0.454 0.702 ?
0.713Ann3 0.525 0.690 0.688 ?Table 1: Inter-annotator agreement among the author?sand annotators?
sets of opinion phrases.
Elements aboveand under the main diagonal refer to the agreement ratesin Dice coefficient for pro and con phrases, respectively.lent, yet much simpler forms, e.g.
instead of ?even Ifound the phones menus to be confusing?, we wouldlike to have ?confusing phones menus?.
Refinementwas carried out both automatically by using hand-crafted transformation rules (based on POS patternsand parse trees) and manual inspection.
The an-notation guidelines for the human refinement andvarious statistics on the dataset can be accessed athttp://rgai.inf.u-szeged.hu/proCon.3.2 Annotator keyphrasesThe second problem with regard to opinionatedkeyphrase extraction is the subjectivity of the task.Different people may have different opinions on thevery same product, which is often reflected in theirreviews.
On the other hand, people can gather dif-ferent information from the very same review dueto differences in interpretation, which again compli-cates the way of proper evaluation.In order to evaluate the difficulty of identifyingopinion-related keyphrases, we decided to apply thefollowing methodology.
We selected 25 reviews re-lated to the mobile phone Nokia 6610, which werealso collected from the website epinions.com.The task for three linguists was to write positiveand negative aspects of the product in the form ofkeyphrases, similar to the original pros and cons.
Inorder not to be influenced by the keyphrases givenby the author of the review, the annotators were onlygiven the free-text part of the review, i.e.
the origi-nal Pros and cons and Bottomline sections were re-moved.
In this way, three different pro and con an-notations were produced for each review, besides,those of the original author were also at hand.
Theinter-annotator agreement rate is in Table 1.Concerning the subjectivity of the task, pro andcon phrases provided by the three annotators and100Eval Ref Top-5 Top-10 Top-153Ann?
man 32.14 44.66 53.923Ann?
auto 27.68 38.17 45.78Merged?
man 28.52 41.09 52.18Merged?
auto 27.39 37.67 46.343Ann?
man 34.89 43.31 44.923Ann?
auto 29.96 34.34 35.54Merged?
man 24.75 26.12 22.22Merged?
auto 21.39 20.94 21.89Author man 27.14 33.5 35.24Author auto 20.61 22.34 25.03Table 2: F-scores of the human evaluation of the automat-ically extracted opinion phrases.
Columns Eval and Refshow the way gold standard phrases were obtained and ifthey were refined manually or automatically.the original author showed a great degree of varietyalthough they had access to the very same review.Sometimes it happened that one annotator did notgive any pro or con phrases for a review whereas theothers listed a bunch of them, which reflects that thevery same feature can be judged as still tolerable,neutral or absolutely negative for different people.Thus, as even human annotations may differ fromeach other to a great extent, it is not unequivocal todecide which human annotation should be regardedas the gold standard upon evaluation.3.3 Evaluation methodologySince the comparison of annotations highlightedthe subjectivity of the task, we voted for smooth-ing the divergences of annotations.
We wanted totake into account all the available annotations whichwere manually prepared and regarded as acceptable.Thus, an annotator formed the union and the inter-section of the pro and con features given by each an-notator either including or excluding those definedby the original author.
With this, we aimed at elim-inating subjectivity since in the case of union, everykeyphrase mentioned by at least one annotator wastaken into consideration while in the case of inter-section, it is possible to detect keyphrases that seemto be the most salient for the annotators as regardsthe given document.
Thus, four sets of pros and conswere finally yielded for each review depending onwhether the unions or intersections were determinedpurely on the phrases of the annotators excluding theoriginal phrases of the author or including them.
Thefollowing example illustrates the way new sets werecreated based on the input sets (in italics):Pro1 : radio, organizer, phone bookPro2 : radio, organizer, loudspeakerPro3 : radio, organizer, calendarUnion: radio, organizer, calendar, loud-speaker, phone bookIntersection: radio, organizerProauthor : clear, funMerged Union: radio, organizer, calen-dar, loudspeaker, phone book, clear, funMerged Intersection: ?The reason behind this methodology was that itmade it possible to evaluate our automatic meth-ods in two different ways.
Comparing the automatickeyphrases to the union of human annotations meansthat a bigger number of keyphrases is to be identi-fied, however, with a bigger number of gold standardkeywords it is more probable that the automatic key-words occur among them.
At the same time having alarger set of gold standard tags might affect the recallnegatively since there are more keyphrases to return.On the other hand, in the case of intersection it canbe measured whether the most important features(i.e.
those that every annotator felt relevant) can beextracted from the text.
Note that our strategy is sim-ilar to the one applied in the case of BLEU/ROUGEscore (Papineni et al, 2002; Lin, 2004) with respectto the fact that multiple good solutions are takeninto account whereas the application of union andintersection is determined by the nature of the task:different annotators may attach several outputs (inother words, different numbers of keyphrases) to thesame document in the case of keyphrase extraction,which is not realistic in the case of machine trans-lation or summarization (only one output is offeredfor each sentence / text).3.4 ResultsIn our experiments, we used the opinion phrase ex-traction system based on the paper of Berend (2011).Results vary whether the manually or the automat-ically refined set of the original sets of pros andcons were regarded as positive training examplesand also whether the evaluation was carried out101Mobiles MoviesA/A 9.95 9.55 8.61 7.58 7.1 6.24A/M 13.51 12.73 11.2 9.95 9.05 7.72M/A 10.15 9.7 8.69 7.52 6.92 5.97M/M 15.27 14.11 12.17 12.22 10.63 8.67Table 3: F-scores achieved with different keyphrase re-finement strategies.
A and M as the first (second) charac-ter indicate the fact that the training (testing) was basedon the automatically and manually defined sets of goldstandard expressions, respectively.against purely the original set of author-assignedkeyphrases or the intersection/union of the man-ual annotations including and excluding the author-assigned keyphrases on the 25 mobile phone re-views.
Results of the various combinations in theexperiments for the top 5, 10 and 15 keyphrasesare reported in Table 2 containing both cases whenhuman and automatic refinement of the gold stan-dard opinion phrases were carried out.
Automatickeyphrases were manually compared to the abovementioned sets of keyphrases, i.e.
human annotatorsjudged them as acceptable or not.
Human evaluationhad the advantage over automated ones, that theycould accept the extracted term ?MP3?
when therewas only its mistyped version ?MP+?
in the set ofgold standard phrases (as found in the dataset).Table 3 presents the results of our experiments onkeyphrase refinement on the mobiles and movies do-mains.
In these settings strict matches were requiredinstead of human evaluation.
Results differ with re-spect to the fact whether the automatically or manu-ally refined sets of the original author phrases wereutilized for training and during the strict evaluation.Having conducted these experiments, we could ex-amine the possibility of a fully automatic system thatneeds no manually inspected training data, but it cancreate it automatically as well.4 Discussion and conclusionsBoth human and automatic evaluation reveal thatthe results yielded when the system was trained onmanually refined keyphrases are better.
The usageof manually refined keyphrases as the training setleads to better results (the difference being 5.9 F-score on average), which argues for human annota-tion as opposed to automatic normalization of thegold standard opinion phrases.
Note, however, thateven though results obtained with the automatic re-finement of training instances tend to stay below theresults that are obtained with the manual refinementof gold standard phrases, they are still comparable,which implies that with more sophisticated rules,training data could be automatically generated.If the inter-annotator agreement rates are com-pared, it can be seen that the agreement rates be-tween the annotators are considerably higher thanthose between a linguist and the author of the prod-uct review.
This may be due to the fact that thelinguists were to conform to the annotation guide-lines whereas the keyphrases given by the authorsof the reviews were not limited in any way.
Still,it can be observed that among the author-annotatoragreement rates, the con phrases could reach higheragreement than the pro phrases.
This can be due topsychological reasons: people usually expect thingsto be good hence they do not list all the features thatare good (since they should be good by nature), incontrast, they list negative features because this iswhat deviates from the normal expectations.In this paper, we discussed the difficulties of eval-uating opinionated keyphrase extraction and alsoconducted experiments to investigate the extent ofoverlap between the keyphrases determined by theoriginal author of a review and those assigned byindependent readers.
To reduce the subjectivity ofthe task and to alleviate the evaluation process, wepresented our method that employs several indepen-dent annotators and we also compared the results ofhuman and machine-based evaluation.
Our resultsreveal that for now, human evaluation leads to bet-ter results, however, we believe that the proper treat-ment of polar expressions and ambiguous adjectivesmight improve automatic evaluation among others.Besides describing the difficulties of the auto-matic evaluation of opinionated keyphrase extrac-tion, the impact of training on automatically crawledgold standard opinionated phrases was investigated.Although not surprisingly they lag behind the onesobtained based on manually refined training data,the automatic creation of gold standard keyphrasescan be a much cheaper, yet feasible option to manu-ally refined opinion phrases.
In the future, we plan toreduce the gap between manual and automatic eval-uation of opinionated keyphrase extraction.102AcknowledgmentsThis work was supported in part by the NIH grant(project codename MASZEKER) of the Hungariangovernment.ReferencesAlexandra Balahur, Ester Boldrini, Andres Montoyo, andPatricio Martinez-Barco, editors.
2011.
Proceedingsof the 2nd Workshop on Computational Approaches toSubjectivity and Sentiment Analysis (WASSA 2.011).ACL, Portland, Oregon, June.Ga?bor Berend.
2011.
Opinion expression mining by ex-ploiting keyphrase extraction.
In Proceedings of 5thInternational Joint Conference on Natural LanguageProcessing, pages 1162?1170, Chiang Mai, Thailand,November.
Asian Federation of Natural Language Pro-cessing.Andrea Esuli, Stefano Baccianella, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ing of 16th International Joint Conference on Artifi-cial Intelligence, pages 668?673.
Morgan KaufmannPublishers.Hayeon Jang and Hyopil Shin.
2010.
Language-specificsentiment analysis in morphologically rich languages.In Coling 2010: Posters, pages 498?506, Beijing,China, August.
Coling 2010 Organizing Committee.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-othy Baldwin.
2010.
Semeval-2010 task 5: Auto-matic keyphrase extraction from scientific articles.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, SemEval ?10, pages 21?26, Mor-ristown, NJ, USA.
ACL.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010.Structure-aware review mining and summarization.
InProceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages 653?661, Beijing, China, August.
Coling 2010 OrganizingCommittee.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.
ACL.Olena Medelyan, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1318?1327, Singapore, Au-gust.
ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the ACL, pages 311?318, Philadel-phia, Pennsylvania, USA, July.
ACL.Wei Wei and Jon Atle Gulla.
2010.
Sentiment learn-ing on product reviews via sentiment ontology tree.
InProceedings of the 48th Annual Meeting of the ACL,pages 404?413, Uppsala, Sweden, July.
ACL.Ian H. Witten, Gordon W. Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning.
1999.
Kea:Practical automatic keyphrase extraction.
In ACM DL,pages 254?255.Torsten Zesch and Iryna Gurevych.
2009.
Approxi-mate Matching for Evaluating Keyphrase Extraction.In Proceedings of the 7th International Conferenceon Recent Advances in Natural Language Processing,pages 484?489, September.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Moviereview mining and summarization.
In Proceedings ofthe 15th ACM international conference on Informationand knowledge management, CIKM ?06, pages 43?50,New York, NY, USA.
ACM.103
