Automatically Identifying the Source Wordsof Lexical Blends in EnglishPaul Cook?University of TorontoSuzanne Stevenson?
?University of TorontoNewly coined words pose problems for natural language processing systems because they are notin a system?s lexicon, and therefore no lexical information is available for such words.
A commonway to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic andpharmaceutical.
We propose a statistical model for inferring a blend?s source words drawing onobserved linguistic properties of blends; these properties are largely based on the recognizabilityof the source words in a blend.
We annotate a set of 1,186 recently coined expressions whichincludes 515 blends, and evaluate our methods on a 324-item subset.
In this first study ofnovel blends we achieve an accuracy of 40% on the task of inferring a blend?s source words,which corresponds to a reduction in error rate of 39% over an informed baseline.
We alsogive preliminary results showing that our features for source word identification can be usedto distinguish blends from other kinds of novel words.1.
Lexical BlendsNeologisms?newly coined words or new senses of an existing word?are constantlybeing introduced into a language (Algeo 1980; Lehrer 2003), often for the purposeof naming a new concept.
Domains that are culturally prominent or that are rapidlyadvancing, such as electronic communication and the Internet, often contain many ne-ologisms, although novel words arise throughout a language (Ayto 1990, 2006; Knowlesand Elliott 1997).
Consequently, any natural language processing (NLP) system operat-ing on recently produced text will encounter new words.
Because lexical resources areoften a key component of an NLP system, performance of the entire system will likelysuffer due to missing lexical information for neologisms.
Ideally, an NLP system couldidentify neologisms as such, and then infer various aspects of their syntactic or seman-tic properties necessary for the computational task at hand.
Recent approaches to thiskind of lexical acquisition task typically infer the target lexical information from sta-tistical distributional properties of the terms.
However, this technique is generally not?
Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,Canada, E-mail: pcook@cs.toronto.edu.??
Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,Canada, E-mail: suzanne@cs.toronto.edu.Submission received: 4 November 2008; revised submission received: 23 May 2009; accepted for publication:24 June 2009.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 1applicable to neologisms, which are relatively infrequent due to their recent intro-duction into the language.
Fortunately, linguistic observations regarding neologisms?namely, that they are formed through specific word formation processes?can giveinsights for automatically learning their lexical properties.New words come about through a variety of means, including derivational mor-phology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983;Plag 2003).
Computational work on neologisms has largely focused on particular wordformation processes, and has exploited information about the formation process tolearn aspects of the semantic properties of words (Means 1988; Nadeau and Turney2005; Baker and Brew 2008, for example).
Subtractive word formations?words formedfrom partial orthographic or phonological content from existing words?have receiveda fair amount of attention recently in computational linguistics, particularly under theheading of inferring the long form of acronyms, especially in the bio-medical domain(e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou2006, for example).Lexical blends?the focus of this study?also known as blends, are another commontype of subtractive word formation.
Most blends are formed by combining a prefixof one source word with a suffix of another source word, as in brunch (breakfast andlunch).
There may be overlap in the contribution of the source words, as in fantabulous( fantastic and fabulous).
It is also possible that one or both source words are entirelypresent, for example, gaydar (gay radar) and jetiquette ( jet etiquette).
We refer to blendssuch as these as simple two-word sequential blends, and focus on this common typeof blend in this article.
Blends in which (part of) a word is inserted within another(e.g., entertoyment, a blend of entertainment and toy) and blends formed from more thantwo source words (e.g., nofriendo from no, friend, and Nintendo) are rare.
In Algeo?s(1991) study of new words, approximately 5% were blends.
However, in our analysisof 1,186 words taken from a popular neologisms Web site, approximately 43% wereblends.
Clearly, computational techniques are needed that can augment lexicons withknowledge of novel blends.The precise nature and intended use of a computational lexicon will determinethe degree of processing required of a novel blend.
In some cases it may suffice forthe lexical entry for a blend to simply consist of its source words.
For example, asystem that employs a measure of distributional similarity may benefit from replacingoccurrences of a blend?likely a recently coined and hence low frequency item?by itssource words, for which distributional information is likely available.
In other cases,further semantic reasoning about the blend and its source words may be required (e.g.,determining the semantic relationship between the source words as an approximationof the meaning of the blend).
However, any approach to handling blends will needto recognize that a novel word is a blend and identify its source words.
These twotasks are the focus of this article.
Specifically, we draw on linguistic knowledge of howblends are formed as the basis for automatically determining the source words of ablend.
Language users create blends that tend to be interpretable by others.
Tapping intoproperties of blends believed to contribute to the recognizability of their source words?and hence the interpretability of the resulting blend?we develop statistical measuresthat indicate whether a word pair is likely the source words for a given blend.
Moreover,the fact that a novel word is determined to have a ?good?
source word pair may beevidence that it is in fact a blend, because we are unlikely to find two words that are a?good?
source word pair for a non-blend.
Thus, the statistical measures we developfor source word identification may also be useful in recognizing a novel word as ablend.130Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in EnglishTo our knowledge, the only computational treatment of blends is our earlier workthat presents preliminary statistical methods and results for the two tasks of recognizingan unknown word as a blend and identifying its source words (Cook and Stevenson2007).
Here we extend that work in a number of important directions.
We expand thestatistical features to better capture co-occurrence patterns of the source words thatcan indicate the likelihood of their combination into a blend.
We present experimentalresults confirming that the extended features provide a substantial improvement overthe earlier work on source word identification.
We further propose a means, based onlinguistic factors of the source words, for pruning the number of word pairs that areconsidered for a blend.
This filtering heuristic greatly reduces the number of candidatesource words processed, while giving modest gains in performance, even though thismethod excludes the correct word pair from consideration for a number of blends.
Wethen consider the use of a much larger lexicon of candidate source words, which couldpotentially improve performance greatly as it contains the correct source word pair formany more blends than a smaller lexicon.We also make improvements to the earlier experimental data set and methods.In our earlier study, we use a data set consisting of a list of blends extracted froma dictionary.
In the current work, we annotate a set of 324 blends (with their sourcewords) from a recent database of neologisms, to enable a more legitimate testing ofour method, on truly novel blends.
Experiments on this new data set show that therecent blends differ from established blends in terms of their statistical properties,and emphasize the need for further resources of neologisms.
We also experimentwith a machine learning approach to combine the information from the statisticalfeatures in a more sophisticated manner than in our previous work.
Finally, we per-form more extensive experiments on distinguishing blends from other kinds of novelwords.2.
A Statistical Model of Lexical BlendsWe present statistical features that are used to automatically infer the source words ofa word known to be a lexical blend, and show that the same features can be used todistinguish blends from other types of neologisms.
First, given a blend, we generateall word pairs that could have formed the blend.
This set is termed the candidate set,and the word pairs it contains are referred to as candidate pairs (Section 2.1).
Next, weextract a number of linguistically motivated statistical features for each candidate pair,as well as filter from the candidate sets those pairs that are unlikely to be source wordsdue to their linguistic properties (Section 2.2).
Later, we explain how we use the featuresto rank the candidate pairs according to how likely they are the source words for thatblend.
Interestingly, the ?goodness?
of a candidate pair is also related to how likely theword is actually a blend.2.1 Candidate SetsTo create the candidate set for a blend, we first generate each prefix?suffix pair such thatthe blend is composed of the prefix followed by the suffix.
(In this work, prefix and suffixrefer to the beginning or ending of a string, regardless of whether those portions areaffixes.)
We restrict the prefixes and suffixes to be of length two or more.
This heuristicreduces the size of the candidate sets, yet generally does not exclude a blend?s source131Computational Linguistics Volume 36, Number 1words from its candidate set since it is uncommon for a source word to contribute lessthan two letters.
For example, for brunch (breakfast+lunch) we consider the followingprefix?suffix pairs: br, unch; bru, nch; brun, ch.
For each prefix?suffix pair, we then find ina lexicon all words beginning with the prefix and all words ending in the suffix, ignoringhyphens and whitespace, and take the Cartesian product of the prefix words and suffixwords to form a list of candidate word pairs.
The candidate set for the blend is the unionof the candidate word pairs for all its prefix?suffix pairs.
Note that in this example, thecandidate pair brute crunch would be included twice: once for the prefix?suffix pair br,unch; and once again for bru, nch.
Unlike in our previous study, we remove all suchduplicate pairs from the final candidate set.
A candidate set for architourist, a blend ofarchitecture and tourist, is given in Table 1.2.2 Statistical FeaturesOur statistical features are motivated by properties of blends observed in corpus-basedstudies, and by cognitive factors in human interpretation of blends, particularly relatingto how easily humans can recognize a blend?s source words.
All the features are formu-lated to give higher values for more likely candidate pairs.
We organize the features intofour groups?frequency; length, contribution, and phonology; semantics; and syllablestructure?and describe each feature group in the following subsections.2.2.1 Frequency.
Various frequency properties of the source words influence how easily alanguage user recognizes the words that form a blend.
Because blends are most usefullycoined when the source words can be readily deduced, we hypothesize that frequency-based features will be useful in identifying blends and their source words.
We proposeten features that draw on the frequency of candidate source words.Lehrer (2003) presents a study in which humans are asked to give the sourcewords for blends.
Among her findings are that frequent source words are moreeasily recognizable.
Our first two features?the frequency of each candidate word,freq(w1) and freq(w2)?reflect this finding.
Lehrer also finds that the recognizability ofa source word is further affected by both the number of words in its neighborhood?the set of words which begin/end with the prefix/suffix which that source wordTable 1A candidate set for architourist, a blend of architecture and tourist.archimandrite touristarchipelago touristarchitect behaviouristarchitect touristarchitectural behaviouristarchitectural touristarchitecturally behaviouristarchitecturally touristarchitecture behaviouristarchitecture touristarchives touristarchivist tourist132Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishcontributes?and the frequencies of those words.
(Gries [2006] reports a similar finding.
)Our next two features capture this insight:freq(w1)freq(prefix)freq(w2)freq(suffix)(1)where freq(prefix) is the sum of the frequency of all words beginning with prefix, andsimilarly for freq(suffix).These four features were used in our previous study; the following six features arenew in this study.Because we observe that blends are often formed from two words that co-occurin language use, our previous study (Cook and Stevenson 2007) included a feature,the pointwise mutual information of w1 and w2, to reflect this.
However, this featureprovides only a weak indication that there is a semantic relation between two wordssufficient to lead to them being blended.
Here we propose six new features that capturevarious co-occurrence frequencies as follows.A blend?s source words often correspond to a common sequence of words, forexample, camouflanguage is camouflaged language.
We therefore include two featuresbased on Dice?s co-efficient to capture the frequency with which the source words occurconsecutively:2 ?
freq(w1 w2)freq(w1)+ freq(w2)2 ?
freq(w2 w1)freq(w1)+ freq(w2)(2)Because many blends can be paraphrased by a conjunctive phrase?for example, brocco-flower is broccoli and cauliflower?we also use a feature that reflects how often the candi-date words are used in this way:2 ?
( freq(w1 and w2)+ freq(w2 and w1))freq(w1 and)+ freq(and w1)+ freq(w2 and)+ freq(and w2)(3)Furthermore, some blends can be paraphrased by a noun modified by a prepositionalphrase, for example, a nicotini is a martini with nicotine.
Lauer (1995) suggests eightprepositional paraphrases for identifying the semantic relationship between the modi-fier and head in a noun compound.
Using the same paraphrases, the following featuremeasures how often two candidate source words occur with any of the followingprepositions P between them: about, at, for, from, in, of, on, with:2 ?
( freq(w1 P w2)+ freq(w2 P w1))freq(w1 P)+ freq(P w1)+ freq(w2 P)+ freq(P w2)(4)where freq(w P v) is the sum of the frequency of w and v occurring with each of the eightprepositions between w and v, and freq(w P) is the sum of the frequency of w occurringwith each of the eight prepositions immediately following w.133Computational Linguistics Volume 36, Number 1Because the previous three features target the source words occurring in very spe-cific patterns, we also count the candidate source words occurring in any of the patternsin an effort to avoid data sparseness problems.2 ?
( freq(w1 w2)+ freq(w2 w1)+ freq(w1 and w2)+freq(w2 and w1)+ freq(w1 P w2)+ freq(w2 P w1))freq(w1)+ freq(w2)(5)Finally, because the above patterns are very specific, and do not capture general co-occurrence information which may also be useful in identifying a blend?s source words,we include the following feature which counts the candidate source words co-occurringwithin a five-word window.2 ?
freq(w1,w2 in a 5 word window)freq(w1)+ freq(w2)(6)2.2.2 Length, Contribution, and Phonology.
Ten features tap into properties of the ortho-graphic or phonetic composition of the source words and blend.
In our previous workon blends, we found such features unhelpful in source word identification.
Here, wepropose revised versions of our old features, and a few new ones.
Note that although weuse information about the phonological and/or syllabic structure of the source words,we do not assume such knowledge for the blend itself, since it is a neologism for whichsuch lexical information is typically unavailable.The first word in a conjunct tends to be shorter than the second, and this also seemsto be the case for the source words in blends (Kelly 1998; Gries 2004).
The first threefeatures therefore capture this tendency based on the graphemic, phonemic, and syllabiclength of w2 relative to w1, respectively:lengraphemes(w2)lengraphemes(w1)+ lengraphemes(w2)(7)lenphonemes(w2)lenphonemes(w1)+ lenphonemes(w2)(8)lensyllables(w2)lensyllables(w1)+ lensyllables(w2)(9)A blend and its second source word also tend to be similar in length, possibly because,similar to compounds, the second source word of a blend is often the head; thereforeit is this word that determines the overall phonological structure of the resulting blend(Kubozono 1990).
The following feature captures this property using graphemic length134Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishas an approximation to phonemic length, because as stated previously, we assume nophonological information about the blend.1 ?|lengraphemes(blend) ?
lengraphemes(w2)|max(lengraphemes(blend), lengraphemes(w2))(10)We hypothesize that a candidate source word is more likely if it contributes moregraphemes to a blend.
We use two ways to measure contribution in terms of graph-emes: contseq(w, b) is the length of the longest prefix/suffix of word w which blend bbegins/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w andb.
This yields four features:contseq(w1, b)lengraphemes(w1)contseq(w2, b)lengraphemes(w2)(11)contlcs(w1, b)lengraphemes(w1)contlcs(w2, b)lengraphemes(w2)(12)Note that for some blends, such as spamdex (spam index), contseq and contlcs will be equal;however, this is not the case in general, as in the blend tomacco (tomato and tobacco) inwhich tomato overlaps with the blend not only in its prefix toma, but also in the final o.In order to be recognizable in a blend, the shorter source word will tend to con-tribute more material, relative to its length, than the longer source word (Gries 2004).We formulate the following feature which is positive only when this is the case:(contseq(w1, b)lengraphemes(w1)?contseq(w2, b)lengraphemes(w2))?
(lengraphemes(w2) ?
lengraphemes(w1)lengraphemes(w1)+ lengraphemes(w2))(13)For this feature we don?t have strong motivation for choosing one measure of contribu-tion over the other, and therefore use contseq, the simpler version of contribution.Finally, the source words in a blend are often phonologically similar, as in sheeple(sheep people); the following feature captures this (Gries 2006):LCSphonemes(w1,w2) (14)2.2.3 Semantics.
We include two semantic features from our previous study that arebased on Lehrer?s (2003) observation that people can more easily identify the sourcewords of a blend when there is a semantic relation between them.As noted, blends are often composed of two semantically similar words, reflecting aconjunction of their concepts.
For example, a pug and a beagle are both a kind of dog, andcan be combined to form the blend puggle.
Similarly an exergame is a blend of exercise andgame, both of which are types of activity.
Our first semantic feature captures similarityusing an ontological similarity measure, which is calculated over an ontology populatedwith word frequencies from a corpus.The source words of some blends are not semantically similar (in the sense of theirrelative positions within an ontology), but are semantically related.
For example, thesource words of slanguist?slang and linguist?are related in that slang is a type of lan-guage and a linguist studies language.
Our second semantic feature is a measure of se-mantic relatedness using distributional similarity between word co-occurrence vectors.135Computational Linguistics Volume 36, Number 12.2.4 Syllable Structure.
Kubozono (1990) notes that the split of a source word?into theprefix/suffix it contributes to the blend and the remainder of the word?occurs at asyllable boundary or immediately after the onset of the syllable.
Because this syllablestructure property holds sufficiently often, we use it as a filter over candidate pairs(rather than as an additional statistical feature) in an effort to reduce the size of thecandidate sets.
Candidate sets can be very large, and we expect that our features willbe more successful at selecting the correct source word pair from a smaller candidateset.
In our subsequent results, we analyze the reduction in candidate set size using thissyllable structure heuristic, and its impact on performance.3.
Creating a Data Set of Recent BlendsThe data set used in our previous work on blends contains dictionary words whoseetymological entry indicates they were formed from a blend of two words.
Using adictionary in this way provides an objective method for selecting experimental expres-sions and indicating their gold standard source words.
However, it results in a data setof blends that are sufficiently established in the language to appear in a dictionary.
Trulynovel blends?neologisms which have been recently added to the language?may havediffering properties from fully established forms in a dictionary.
In particular, manyof our features are based on properties of the source words, both individually and inrelation to each other, that may not hold for expressions that entered the language sometime ago.
For example, although meld is a blend of melt and weld, the current frequencyof the phrase melt and weld may not be as common as the source word co-occurrencesfor newly coined expressions.
Thus, an important step to support further researchon blends is to develop a data set of recent neologisms that are judged to be lexicalblends.To develop a data set of recently coined blends we drew on www.wordspy.com, apopular Web site documenting English neologisms (and a small number of rare orspecialized terms) that have been recently used in a recordable medium such as anewspaper or book, and that (typically) are not found in currently available dictionaries.A (partial) sample entry from Wordspy is given in Table 2.
The words on this Website satisfy our goal of being new; however, they include many kinds of neologisms,not just blends.
We thus annotated the data set to identify the blends and their sourceTable 2The Wordspy definition, and first citation given, for the blend staycation.staycation n. A stay-at-home vacation.
Also: stay-cation.
?staycationer n.Example Citation:Amy and Adam Geurden of Hollandtown, Wis., had planned a long summer of short,fun getaways with their kids, Eric, 6, Holly, 3, and Jake, 2.
In the works were water-parkvisits, roller-coaster rides, hiking adventures and a whirlwind weekend in Chicago.Then Amy did the math: their Chevy Suburban gets 17 miles to the gallon and, with gasprices topping $4, the family would have spent about $320 on fill-ups alone.
They?vesince scrapped their plans in favor of a ?staycation?
around the backyard swimmingpool.
?Linda Stern, ?Try Freeloading Off Friends!,?
Newsweek, May 26, 2008136Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishwords.
(In cases where multiple source words were found to be equally acceptable, allsource words judged to be valid were included in the annotation.)
Most expressionsin Wordspy include both a definition and an example usage, making the task fairlystraightforward.As of 17 July 2008 Wordspy contained 1,186 single word entries.
One author anno-tated each of these words as a blend or not a blend, and indicated the source wordsfor each blend.
To ensure validity of the annotation task, the other author similarlyannotated 100 words randomly sampled from the 1,186.
On this subset of 100 words, ob-served agreement on both the blend/non-blend annotation and the component sourceword identification was 92%, with an unweighted Kappa score of .84.
On four blends,the judges gave different variants of the same source word; for example, fuzzy buzzwordand fuzz buzzword for the blend fuzzword.
These items were counted as agreements, andall variants were considered correct source words.Given the high level of agreement between the annotators, only one person anno-tated all 1,186 items.
A total of 515 words were judged to be blends, with 351 beingsimple two-word sequential blends whose source words are not proper nouns (thislatter type of blend being the focus of this study).
Table 3 shows the variety of blendsencountered in the Wordspy data, organized according to a categorization scheme wedevised.
Of the simple two-word sequential blends, we restrict our experimental dataset to the 324 items whose entries included a citation of their usage, as we have evidencethat they have in fact been used; moreover, such blends may be less likely to be nonceformations?expressions which are used once but do not become part of the language.The usage data in the citations can also be used in the future for semantic features basedon contextual information.
We refer to this new data set of 324 items as WORDSPLEND(a blend of Wordspy and blend).4.
Materials and Methods4.1 Experimental ExpressionsThe data set used in our previous study of blends consisted of expressions from theMacquarie Dictionary (Delbridge 1981) with an etymology entry indicating that theyare blends.
All of our statistical features were devised using the development portion ofthis data set, enabling us to use the full WORDSPLEND data set for testing.
To compareour results to those in our earlier study, we also perform experiments on a subset ofthe previous data set.
We are uncertain as to whether a number of the blends from theMacquarie Dictionary are in fact blends.
For example, it does not match our intuitionthat clash is a blend of clap and dash.
We created a second data set of confirmed blends,MAC-CONF, consisting of only those blends from Macquarie that are found in at leastone of two additional dictionaries with an etymology entry indicating that they areblends.
We report results on the 30 expressions in the unseen test portion of MAC-CONF.4.2 Experimental ResourcesWe generate candidate sets using two different lexicons: the CELEX lexicon (Baayen,Piepenbrock, and Gulikers 1995),1 and a wordlist created from the Web 1T 5-gram1 From CELEX, we use lemmas as potential source words, as it is uncommon for a source word to be aninflected form?there are no such examples in our development data.137Computational Linguistics Volume 36, Number 1Table 3Types of blends and their frequency in Wordspy data.Blend type Freq.
ExampleSimple two-word sequential blends 351 digifeiter(digital counterfeiter)Proper nouns 50 Japanimation( Japanese animation)Affixes 61 prevenge(pre-revenge)Common one-letter prefix 10 e-business(electronic business)Non-source word material 7 aireoke(air guitar karaoke)w2 contributes a prefix 10 theocon(theological conservative)Foreign word 4 sousveillance(French sous, meaning under, and Englishsurveillance)Non-sequential blends 6 entertoyment(entertainment blended with toy)w1 contributes a suffix 5 caponomics(salary cap economics)Multiple source words 6 MoSoSo(mobile social software)Other 5 CUV(car blended with initialism SUV)Corpus (Brants and Franz 2006).
These are discussed further herein.
The frequencyinformation needed to calculate the frequency features is extracted from the Web 1T 5-gram Corpus.
The length, contribution, and phonology features, as well as the syllablestructure filter, are calculated on the basis of the source words themselves, or are derivedfrom information in CELEX (when CELEX is the lexicon in use).2 We compute semanticsimilarity between the source words using Jiang and Conrath?s (1997) measure in theWordNet::Similarity package (Pedersen, Patwardhan, and Michelizzi 2004), and wecompute semantic relatedness of the pair using the cosine between word co-occurrencevectors using software provided by Mohammad and Hirst (2006).We conduct separate experiments with the two different lexicons for candidate setcreation.
We began by using CELEX, because it contains rich phonological informationthat some of our features draw on.
However, in our analysis of the results, we notedthat for many expressions the correct candidate pair is not in the candidate set.
Manyof the blends in WORDSPLEND are formed from words which are themselves newwords, often coined for concepts related to the Internet, such as download, for example;such words are not listed in CELEX.
This motivated us to create a lexicon from a2 Note that it would be possible to automatically infer the phonological and syllabic information requiredfor our features using automatic approaches for text-to-phoneme conversion and syllabification (Bartlett,Kondrak, and Cherry 2008, for example).
Although such techniques currently provide noisy information,phonological and syllabic information for the blend itself could also be inferred, allowing thedevelopment of features that exploit this information.
We leave exploring such possibilities for futurework.138Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishrecent data set (the Web 1T 5-gram Corpus) that would be expected to contain manyof these new coinages.
To form a lexicon from this corpus, we extract the 100K mostfrequent words, restricted to lowercase and all-alphabetic forms.
Using this lexicon weexpect the correct source word pair to be in the candidate set for more expressions.However, this comes at the expense of potentially larger candidate sets, due to thelarger lexicon size.
Furthermore, since this lexicon does not contain phonological orsyllabic representations of each word, we cannot extract three features: the feature forthe syllable heuristic, and the two features that capture the tendency for the secondsource word to be longer than the first in terms of phonemes and syllables.
(We do cal-culate the phonological similarity between the two candidate source words, in terms ofgraphemes.
)4.3 Experimental MethodsBecause each of our features is designed to have a high value for a correct sourceword pair and a low value otherwise, we can simply sum the features for each candi-date pair to get a score for each pair indicating its degree of goodness as a source wordpair for the blend under consideration.
However, because our various features havevalues falling on differing ranges, we first normalize the feature values by subtractingthe mean of that feature within that candidate set and dividing by the correspondingstandard deviation.
We also take the arctan of each resulting feature value to reduce theinfluence of outliers.
We then sum the feature values for each candidate pair, and orderthe pairs within each candidate set according to this sum.
This ranks the pairs in termsof decreasing degree of goodness as a source word pair.
We refer to this method as thefeature ranking approach.We also use a machine learning approach applied to the features in a trainingregimen.
Our task can be viewed as a classification problem in which each candidatepair is either a positive instance (the correct source word pair) or a negative instance(an incorrect source word pair).
However, a standard machine learning algorithm doesnot directly apply because of the structure of the problem space.
In classification,we typically look for a hyperplane that separates the positive and negative trainingexamples.
In the context of our problem, this corresponds to separating all the correctcandidate pairs (for all blends in our data set) from all the incorrect candidate pairs.However, such an approach is undesirable as it ignores the structure of the candidatesets; it is only necessary to separate the correct source word pair for a given blend fromthe corresponding incorrect candidate pairs (i.e., for the same blend).
This is also inline with the formulation of our features, which are designed to give relatively highervalues to correct candidate pairs than incorrect candidate pairs within the candidate setfor a given blend; it is not necessarily the case that the feature values for the correctcandidate pair for a given blend will be higher than those for an incorrect candidatepair for another blend.
In other words, the features are designed to give values that arerelative to the candidates for a particular blend.To address this issue, we use a version of the perceptron algorithm similar tothat proposed by Shen and Joshi (2005).
In this approach, the classifier is trainedby only adjusting the perceptron weight vector when the correct candidate pair isnot scored higher than the incorrect pairs for the target blend (not across all the can-didate pairs for all blends).
Furthermore, to accommodate for the large variation incandidate set size we use an uneven margin?in this case the distance between theweighted sum of the feature vector for a correct and incorrect candidate pair?of139Computational Linguistics Volume 36, Number 11#correct cand.
pairs ?
#incorrect cand.
pairs .
We therefore learn a single weight vector such that,within each candidate set, the correct candidate pairs are scored higher than the in-correct candidate pairs by a factor of this margin.
When updating the weight vector,we multiply the update that we add to the weight vector by a factor of this marginto prevent the classifier from being overly influenced by large candidate sets.
Duringtesting, each candidate pair is ranked according to the weighted sum of its featurevector.
To evaluate this approach, on each of WORDSPLEND and MAC-CONF we per-form 10-fold cross-validation with 10 random restarts.
In these experiments, we use oursyllable heuristic as a feature, rather than as a filter, to allow the learner to weight itappropriately.4.4 Evaluation MetricsWe evaluate our methods according to two measures: accuracy and mean reciprocalrank (MRR).
Under the accuracy measure, the system is scored as correct if it ranks oneof the correct source word pairs for a given blend first, and as incorrect otherwise.
TheMRR gives the mean of the rank of the highest ranked correct source word pair for eachblend.
Although accuracy is more stringent than MRR, we are interested in MRR to seewhere the system ranks the correct source word pair in the case that it is not rankedfirst.
We compare the accuracy of our system against a chance (random) baseline, andan informed baseline in which the feature ranking approach is applied using just two ofour features, the frequency of each candidate source word.5.
Experimental Results5.1 Candidate SetsWe begin by examining some properties of the candidate sets created using CELEX asthe lexicon, also referred to as the CELEX candidate set, in rows 2?4 of Table 4.
First,in the second row of this table, we observe that only 78?83% of expressions have bothsource words in CELEX.
For the other 17?22% of expressions, our system is alwaysincorrect, because the CELEX candidate set cannot contain the correct source words.Table 4Percent of expressions (% exps) with their source words in each lexical resource and candidateset (CS), and after applying the syllable heuristic filter on the CELEX CS, as well as median CSsize, for both the WORDSPLEND and MAC-CONF data sets.Lexical resource or CS WORDSPLEND MAC-CONF% exps Med.
CS size % exps Med.
CS sizeCELEX 78 - 83 -CELEX CS 76 117 83 121CELEX CS after syllable filter 71 71 77 92Web 1T lexicon 92 - - -Web 1T CS 89 442 - -140Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in EnglishThe percentages reported in this row thus serve as an upper bound on the task for eachdata set.The third row of Table 4 shows the percentage of expressions for which the CELEXcandidate set contains the correct source words.
Note that in most cases, if the sourcewords are in CELEX, they are also in the CELEX candidate set.
The only expressionsin WORDSPLEND for which that is not the case are those in which a source wordcontributes a single letter to the blend.
We could remove our restriction that each sourceword contribute at least two letters; however, this would cause the candidate sets to bemuch larger and likely reduce accuracy.We now look at the effect of filtering the CELEX candidate sets to include only thosecandidate pairs that are valid according to our syllable heuristic.
This process results ina 24?39% reduction in median candidate set size, but only excludes the source wordsfrom the candidate set for a relatively small number of expressions (5?6%), as shown inthe fourth row of Table 4.
We will further examine the effectiveness of this heuristic inthe following subsection.Now we examine the candidate sets created using the lexicon derived from theWeb 1T 5-gram Corpus.3 In the final two rows of Table 4 we see that, as expected, manymore expressions have their source words in the Web 1T lexicon than in CELEX, andfurthermore, more expressions have their source words in the candidate sets createdusing the Web 1T lexicon than in the candidate sets formed from CELEX.
This meansthat the upper bound for our task is much higher when using the Web 1T lexicon thanwhen using CELEX.
However, this comes at the cost of creating much larger candidatesets; we examine this trade-off more thoroughly herein.5.2 Source Word IdentificationIn the following subsections we present results using the feature ranking approach(Section 5.2.1), and analyze some of the errors the system makes in these experiments(Section 5.2.2).
We then consider results using the modified perceptron algorithm (Sec-tion 5.2.3), and finally we compare our results to our previous study and humanperformance (Section 5.2.4).5.2.1 Feature Ranking.
Table 5 gives the accuracy using the feature ranking approach forboth the random and informed baselines, each feature group, and the combination ofall features, on each data set, using both the CELEX and Web 1T lexicons in the case ofWORDSPLEND.
Feature groups and combinations marked with an asterisk are signifi-cantly better than the informed baseline at the 0.05 confidence level using McNemar?sTest.4We first note that the informed baseline is an improvement over the random base-line in all cases, which points to the importance of word frequency in blend formation.We also see that the informed baseline is quite a bit higher on WORDSPLEND than MAC-CONF.
Inspection of candidate sets?created from the CELEX lexicon?that include thecorrect source words reveals that the average source word frequency for WORDSPLEND3 Syllable structure information is not available for all words in the Web 1T lexicon; therefore, we do notapply the syllable heuristic filter to the pairs in these candidate sets (see Section 4.2).
We do not createcandidate sets for MAC-CONF using the Web 1T lexicon since this lexicon was constructed specifically inresponse to the kinds of new words found in WORDSPLEND.4 McNemar?s Test is a non-parametric test that can be applied to correlated, nominal data.141Computational Linguistics Volume 36, Number 1Table 5Percent accuracy on blends in WORDSPLEND and MAC-CONF using the feature rankingapproach.
The size of each data set is given in parentheses.
The lexicon employed (CELEX orWEB 1T) is indicated.
The best accuracy obtained using this approach for each data set andlexicon is shown in boldface.
* = results that are significantly better than the informed baseline.Features WORDSPLEND MAC-CONF(324) (30)CELEX WEB 1T CELEXRandom Baseline 6 3 1Informed Baseline 27 27 7Frequency 32* 32* 30*Len./Cont./Phono.
20 20 7Semantic 15 13 20All 38* 42* 37*All+Syllable 40* - 37*is much higher than for MAC-CONF (118M vs. 34M).
On the other hand, the average fornon-source words in the candidate sets is similar across these data sets (11M vs. 9M).Thus, although source words are more frequent than non-source words for both datasets, frequency is a much more reliable indicator of being a source word for truly novelblends than for established blends.
This finding emphasizes the need for a data set suchas WORDSPLEND to evaluate methods for processing neologisms.All of the individual feature groups outperform the random baseline.
We also seethat our frequency features are better than the informed baseline.
Although sourceword frequency (the informed baseline) clearly plays an important role in forming inter-pretable blends, this finding confirms that additional aspects of source word frequencybeyond their unigram counts also play an important role in blend formation.
Also notethat the semantic features are substantially better than the informed baseline?althoughnot significantly so?on MAC-CONF, but not on WORDSPLEND.
This result demon-strates the importance of testing on true neologisms to have an accurate assessmentof a method.
It also supports our future plan to explore alternative semantic features,such as those that draw on the context of usage of a blend (as provided in our newdata set).We expect using all the features to provide an improvement in performance overany individual feature group, because they tap into very different types of informationabout blends.
Indeed, the combination of all features (All) does perform better thanthe frequency features, supporting our hypothesis that the information provided by thedifferent feature groups is complementary.5Looking at the results on WORDSPLEND using the Web 1T lexicon, we see that asexpected, due to the larger candidate sets, the random baseline is lower than whenusing the CELEX lexicon.
However, the informed baseline, and each feature groupused on its own, give very similar results, with only a small difference observed forthe semantic features.
The combination of all features gives slightly higher performance5 This difference is significant (p < 0.01) according to McNemar?s test for the WORDSPLEND data set usingboth the CELEX and Web 1T lexicons.
The difference was not significant for MAC-CONF.142Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishusing the Web 1T lexicon than the CELEX lexicon, although again this difference israther small.Recall that we wanted to see if the use of our syllable heuristic filter to reducecandidate set size would have a negative impact on performance.
Table 5 shows thatthe accuracy on all features when we apply our syllable heuristic filter (All+Syllable) isat least as good as when we do not apply the filter (All).
This is the case even thoughthe syllable heuristic filter removes the correct source word pairs for 5?6% of the blends(see Table 4).
It seems that the words this heuristic excludes from consideration arenot those that the features rank highly, indicating that it is a reasonable method forpruning candidate sets.
Moreover, reducing candidate set size will enable future workto explore features that are more expensive to extract than those currently used.
Giventhe promising results using the Web1T lexicon, we also intend to examine ways toautomatically estimate the syllable filtering heuristic for words for which we do nothave syllable structure information.5.2.2 Error Analysis.
We now examine some cases where the system ranks an incorrectcandidate pair first, to try to determine why the system makes the errors it does.
Wefocus on the expressions in WORDSPLEND using the CELEX lexicon, as we are able toextract all of our features for this experimental setup.
First, we observe that when con-sidering feature groups individually, the frequency features perform best; however, inmany cases, they also contribute to errors.
This seems to be primarily due to (incorrect)candidate pairs that occur very frequently together.
For example, in the case of mathlete(math athlete), the candidate pair male and athlete co-occurs much more frequently thanthe correct source word pair, causing the system to rank the incorrect source wordpair first.
We observe a similar situation for cutensil (cute utensil), where the candidatepair cup and utensil often co-occur.
In both these cases, phonological information forthe blend itself could help as, for example, cute ([kjut]) contributes more phonemes tocutensil ([kjutEnsl"]) than cup ([k2p]).Turning to the length, contribution, and phonology features, we see that althoughmany blends exhibit the properties on which these features are based, there are alsomany blends which do not.
For example, our first feature in this group captures theproperty that the second source word tends to be longer than the first; however, thisis not the case for some blends, such as testilie (testify and lie).
Furthermore, even forblends for which the second source word is longer than the first, there may exist acandidate pair that has a higher value for this feature than the correct source wordpair.
In the case of banalysis?banal analysis?banal electrolysis is a better source wordpair according to this feature.
These observations, and similar issues with other length,contribution, and phonology features, likely contribute to the poor performance ofthis feature group.
Moreover, such findings motivate approaches such as our modifiedperceptron algorithm?discussed in the following subsection?that learn a weightingfor the features.Finally, for the semantic features, we find cases where a blend?s source words aresimilar and related, but there is another (incorrect) candidate pair which is more similarand related according to these features.
For example, puggle, a blend of pug and beagle,has the candidate source words push and struggle which are more semantically similarand related than the correct source word pair.
In this case, the part-of-speech of thecandidate source words, along with contextual knowledge indicating the part-of-speechof the blend, may be useful; blending pug and beagle would result in a noun, while ablend of push and struggle would likely be a verb.
Another example is camikini, a blend143Computational Linguistics Volume 36, Number 1of camisole and bikini.
Both of these source words are women?s garments, so we wouldexpect them to have a moderately high similarity.
However, the semantic similarityfeature assigns this candidate pair the lowest possible score, since these words do notoccur in the corpus from which this feature is estimated.5.2.3 Modified Perceptron.
Table 6 shows the average accuracy of the modified perceptronalgorithm for the informed baseline and the combination of all features plus the featurecorresponding to the syllable heuristic, on each data set, using both the CELEX and Web1T lexicons in the case of WORDSPLEND.
We don?t compare this method directly againstthe results using the feature ranking approach because our perceptron experiments areconducted using cross-validation, rather than a held-out test set methodology.
Examin-ing the results using the combination of All+Syllable, we see that for each data set andlexicon the mean accuracy over the 10-fold cross-validation is significantly higher thanthat obtained using the informed baseline, according to an unpaired t-test (p < 0.0001in each case).Interestingly, on WORDSPLEND using the combination of all features, we see higherperformance using the CELEX lexicon than the Web 1T lexicon.
We hypothesize thatthis is due to the training data in the latter case containing many more negative ex-amples (incorrect candidate pairs?due to the larger candidate sets).
It is worth notingthat, despite the differing experimental methodologies, the results are in fact not verydifferent from those obtained in the feature ranking approach.
One limitation of thisperceptron algorithm is that it assumes that the training data is linearly separable.In future work, we will try other machine learning techniques that do not make thisassumption.5.2.4 Discussion.
We now compare the feature ranking results on MAC-CONF here of37% accuracy, to our previous best results on this data set of 27% accuracy, also usingfeature ranking (Cook and Stevenson 2007).
To make this comparison, we should con-sider the differing baselines and upper bounds across the experiments.
The informedbaseline in our previous study on MAC-CONF is 13%, substantially higher than the7% in the current study.
Recall that the first row of Table 4 shows the upper boundusing the CELEX lexicon on this data set to be 83%.
By contrast, in our previouswork we only use blends whose source words appear in the lexicon we used there(Macquarie), so the upper bound for that study is 100%.
Taking these factors intoaccount, the best results in our previous study correspond to a reduction in error rate(RER) over the informed baseline of 16%, while the feature ranking method here usingTable 6Percent accuracy on blends in WORDSPLEND and MAC-CONF using the modified perceptronalgorithm.
The size of each data set is given in parentheses.
The lexicon employed (CELEX orWEB 1T) is indicated.
* = results that are significantly better than the informed baseline.Features WORDSPLEND MAC-CONF(324) (30)CELEX WEB 1T CELEXInformed Baseline 23 24 7All+Syllable 40* 37* 35*144Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishthe combination of all features and the syllable heuristic filter achieves a much higherRER of 39%.6Lehrer (2003) finds human performance for determining the source words of blendsto be 34% to 79%?depending on the blends considered?which indicates the difficultyof this task.7 Our best accuracy on each data set of 37%?42% is quite respectablein comparison.
These accuracies correspond to mean reciprocal ranks of 0.47?0.51,and the random baseline on WORDSPLEND and MAC-CONF in terms of this mea-sure is 0.03?0.07.
This indicates that even when our system is incorrect, the correctsource word pair is still ranked fairly high.
Such information about the best inter-pretations of a blend could be useful in semi-automated methods, such as computer-aided translation, where a human may not be familiar with a novel blend in the sourcetext.6.
Blend IdentificationThe statistical features we have developed may also be informative about whetheror not a word is in fact a blend?that is, we expect that if a novel word has ?good?candidate source words, then the word is more likely to be a blend than the result ofanother word formation process.
Because our features are designed to be high for ablend?s source words and low for other word pairs, we hypothesize that the highestscoring candidate pairs for blends will be higher than those of non-blends.To test this hypothesis, we first create a data set of non-blends from our earlierannotation, which found 671 non-blends out of the 1,186 Wordspy expressions (seeSection 3).
From these words, we eliminate all those beginning with a capital letter(to exclude words formed from proper nouns) or containing a non-letter character (toexclude acronyms and initialisms).
This results in 663 non-blends.We create candidate sets for the non-blends using the CELEX lexicon.
Using theCELEX lexicon allows us to extract?and consider the contribution of?all of our length,contribution, and phonology features, some of which are not available when using theWeb 1T lexicon.
The candidate sets resulting from using the CELEX lexicon were alsomuch smaller than when using the Web 1T lexicon.
We calculate the features for the non-blends as we did for the blends, and then order all expressions (both blends and non-blends) according to the sum of the features for their highest-scoring candidate sourceword pair.
We use the same feature groups and combinations presented in Table 5.Rather than set an arbitrary cut-off to distinguish blends from non-blends, we insteadgive receiver operating characteristic (ROC) curves for some of these experiments.ROC curves plot true positive rate versus false positive rate as the cut-off is varied(see Figure 1).
The top-left corner represents perfect classification, with points furthertowards the top-left from the diagonal (a random classifier) being ?better.?
We see thatthe informed baseline is a substantial improvement over a random classifier, and thecombination All+Syllable is a further improvement over the informed baseline.
The in-dividual feature groups (not shown in Figure 1) do not perform as well as All+Syllable.6 Reduction in error rate =accuracy?baselineupper bound?baseline .7 Note that the high level of interannotator agreement achieved in our annotation task (Section 3) mayseem surprising in the context of Lehrer?s results.
However, our task is much easier, because ourannotators were given a definition of the blend, whereas Lehrer?s subjects were not.145Computational Linguistics Volume 36, Number 1Figure 1ROC curves for blend identification.In future work, we plan to re-examine this task and develop methods specifically foridentifying blends and other types of neologism.7.
Related WorkAs discussed in Section 1, techniques generally used in the automatic acquisition ofsyntactic and semantic properties of words are not applicable here, because they usecorpus statistics that cannot be accurately estimated for low frequency items, such asthe novel lexical blends considered in this study (Hindle 1990; Lapata and Brew 2004;Joanis, Stevenson, and James 2008, for example).
Other work has used the contextin which an unknown word occurs, along with domain-specific knowledge, to inferaspects of its meaning and syntax (Granger 1977; Cardie 1993; Hastings and Lytinen1994, for example).
These studies have been able to learn properties of an unknownword from just one usage, or a small number of usages; however, the domain-specificresources that these studies rely on limit their applicability to general text.Techniques for inferring lexical properties of neologisms can make use of infor-mation that is typically not available in other lexical acquisition tasks?specifically,knowledge of the processes through which neologisms are formed.
Computationalwork on neologisms has tended to focus on tasks pertaining to a specific type ofneologism, such as identifying and inferring the long form of acronyms (Schwartz andHearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example),recognizing loanwords (Baker and Brew 2008; Alex 2008, for example), and identifyingand expanding clippings (Means 1988, for example).
This study focuses on the tasksof identifying, and inferring the source words of, lexical blends, a common type ofneologism, which have been previously unaddressed except for our preliminary workin Cook and Stevenson (2007).In addition to knowledge about a word?s formation process, for many types ofneologism, information about its phonological and orthographic content can be used to146Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in Englishinfer aspects of its syntactic and semantic properties.
This is the case for neologisms thatare composed of existing words or affixes (e.g., compounds and derivations) or partialorthographic or phonological material from existing words or affixes (e.g., acronyms,clippings, and blends).
For example, in the case of part-of-speech tagging, informationabout the suffix of an unknown word can be used to determine its part-of-speech(Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example).
For the task of inferring thelong form of an acronym, the letters which compose a given acronym can be used todetermine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney2005; Okazaki and Ananiadou 2006, for example).The latter approach to acronyms is somewhat similar to the way in which we useknowledge of the letters that make up a blend to form candidate sets and determinethe most likely source words.
However, in the case of acronyms, each word in a longform typically contributes only one letter to the acronym, while for blends, a sourceword usually contributes more than one letter.
At first glance, it may appear that thismakes the task of source word identification easier for blends, since there is more sourceword material available to work with.
However, acronyms have two properties thathelp in their identification.
First, there is less uncertainty in the ?split?
of an acronym,because each letter is usually contributed by a separate word.
By contrast, due to thelarge variation in the amount of material contributed by the source words in blends,one of the challenges in blend identification is to determine which material in the blendbelongs to each source word.
Second, and more importantly, acronyms are typicallyintroduced in regular patterns (e.g., the long form followed by the acronym capitalizedand in parentheses) which can be exploited in acronym identification and long forminference; in the case of blends there is no counterpart for this information.8.
ConclusionsWe propose a statistical model for inferring the source words of lexical blends?a veryfrequent class of new words?based largely on properties related to the recognizabilityof their source words.
We also introduce a method based on syllable structure for re-ducing the number of words that are considered as possible source words.
We evaluateour methods on two data sets, one consisting of novel blends, the other containingestablished blends; in both cases our features significantly outperform an informedbaseline.
Moreover, the results in this study are substantially better than those reportedpreviously (Cook and Stevenson 2007).
We further show that our methods for sourceword identification can also be used to distinguish blends from other word types.
Inaddition, we annotate a data set of newly coined expressions which will support futureresearch not only on lexical blends, but on neologisms in general.Our future plans include expanding our techniques for identifying blends to ad-dress the more general problem of determining the formation process of a novel word.We further intend to apply our source word identification methods to other types ofneologisms formed from material from existing words, such as clippings (e.g., lab forlaboratory).AcknowledgmentsThis article is an extended and updatedversion of a paper that appeared in theProceedings of the Tenth Conference of thePacific Association for ComputationalLinguistics (PACLING-2007).
We thank theanonymous reviewers of this article for theircomments, which have helped us to improvethe quality of this work.
We also thank themembers of the computational linguisticsgroup at the University of Toronto for theircomments and feedback on our research.This work was financially supported by theNational Sciences and Engineering Research147Computational Linguistics Volume 36, Number 1Council of Canada, the Ontario GraduateScholarship program, and the Universityof Toronto.ReferencesAlex, Beatrice.
2008.
Comparingcorpus-based to Web-based lookuptechniques for automatic Englishinclusion detection.
In Proceedings ofthe Sixth International LanguageResources and Evaluation Conference(LREC?08), pages 2693?2697,Marrakech.Algeo, John.
1980.
Where do all the newwords come from?
American Speech,55(4):264?277.Algeo, John, editor.
1991.
Fifty Years Amongthe New Words.
Cambridge UniversityPress, Cambridge.Ayto, John, editor.
1990.
The Longman Registerof New Words, volume 2.
Longman,London.Ayto, John.
2006.
Movers and Shakers: AChronology of Words that Shaped our Age.Oxford University Press, Oxford.Baayen, R. Harald, Richard Piepenbrock, andLeon Gulikers.
1995.
The CELEX LexicalDatabase (release 2) [CD-ROM].Philadelphia, PA: Linguistic DataConsortium, University ofPennsylvania [distributor].Baker, Kirk and Chris Brew.
2008.Statistical identification of Englishloanwords in Korean using automaticallygenerated training data.
In Proceedingsof the Sixth International LanguageResources and Evaluation Conference(LREC?08), pages 1159?1163, Marrakech.Bartlett, Susan, Grzegorz Kondrak, andColin Cherry.
2008.
Automaticsyllabification with structured SVMsfor letter-to-phoneme conversion.
InProceedings of the 46th Annual Meetingof the Association for ComputationalLinguistics (ACL-08): Human LanguageTechnologies, pages 568?576,Columbus, OH.Bauer, Laurie.
1983.
English Word-formation.Cambridge University Press, Cambridge.Brants, Thorsten and Alex Franz.
2006.
Web1T 5-gram Corpus version 1.1.
LinguisticData Consortium, Philadelphia, PA.Brill, Eric.
1994.
Some advances intransformation-based part of speechtagging.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence,pages 722?727, Seattle, WA.Cardie, Claire.
1993.
A case-based approachto knowledge acquisition fordomain-specific sentence analysis.
InProceedings of the Eleventh NationalConference on Artificial Intelligence,pages 798?803, Washington, DC.Cook, Paul and Suzanne Stevenson.
2007.Automagically inferring the source wordsof lexical blends.
In Proceedings of the TenthConference of the Pacific Association forComputational Linguistics (PACLING-2007),pages 289?297, Melbourne.Delbridge, Arthur, editor.
1981.
The MacquarieDictionary.
Macquarie Library, Sydney.Granger, Richard H. 1977.
FOUL-UP: Aprogram that figures out the meanings ofwords from context.
In Proceedings of theFifth International Joint Conference onArtificial Intelligence, pages 172?178,Cambridge, MA.Gries, Stefan Th.
2004.
Shouldn?t it bebreakfunch?
A quantitative analysis of thestructure of blends.
Linguistics,42(3):639?667.Gries, Stefan Th.
2006.
Cognitivedeterminants of subtractiveword-formation processes: A corpus-basedperspective.
Cognitive Linguistics,17(4):535?558.Hastings, Peter M. and Steven L. Lytinen.1994.
The ups and downs of lexicalacquisition.
In Proceedings of the TwelfthNational Conference on ArtificialIntelligence, pages 754?759, Seattle, WA.Hindle, Donald.
1990.
Noun classificationfrom predicate-argument structures.
InProceedings of the 28th Annual Meetingof the Association for ComputationalLinguistics, pages 268?275, Pittsburgh, PA.Jiang, Jay J. and David W. Conrath.
1997.Semantic similarity based on corpusstatistics and lexical taxonomy.
InProceedings of the International Conferenceon Research in Computational Linguistics(ROCLING X), pages 19?33, Taiwan.Joanis, Eric, Suzanne Stevenson, andDavid James.
2008.
A general feature spacefor automatic verb classification.
NaturalLanguage Engineering, 14(3):337?367.Kelly, Michael H. 1998.
To ?brunch?
or to?brench?
: Some aspects of blendstructure.
Linguistics, 36(3):579?590.Knowles, Elizabeth and Julia Elliott, editors.1997.
The Oxford Dictionary of New Words.Oxford University Press, New York.Kubozono, Haruo.
1990.
Phonologicalconstraints on blending in English as acase for phonology-morphology interface.Yearbook of Morphology, 3:1?20.148Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in EnglishLapata, Mirella and Chris Brew.
2004.
Verbclass disambiguation using informativepriors.
Computational Linguistics,30(1):45?73.Lauer, Mark.
1995.
Designing StatisticalLanguage Learners: Experiments on NounCompounds.
Ph.D. thesis, MacquarieUniversity, Sydney.Lehrer, Adrienne.
2003.
Understandingtrendy neologisms.
Italian Journal ofLinguistics, 15(2):369?382.Means, Linda G. 1988.
Cn yur cmputr raedths?
In Proceedings of the Second Conferenceon Applied Natural Language Processing,pages 93?100, Austin, TX.Mikheev, Andrei.
1997.
Automatic ruleinduction for unknown-word guessing.Computational Linguistics, 23(3):405?423.Mohammad, Saif and Graeme Hirst.
2006.Distributional measures ofconcept-distance: A task-orientedevaluation.
In Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2006),pages 35?43, Sydney.Nadeau, David and Peter D. Turney.
2005.
Asupervised learning approach to acronymidentification.
In Proceedings of theEighteenth Canadian Conference on ArtificialIntelligence (AI?2005), pages 319?329,Victoria.Okazaki, Naoaki and Sophia Ananiadou.2006.
A term recognition approach toacronym recognition.
In Proceedings of the21st International Conference onComputational Linguistics and the 44thAnnual Meeting of the Association forComputational Linguistics (Coling-ACL2006), pages 643?650, Sydney.Pedersen, Ted, Siddharth Patwardhan,and Jason Michelizzi.
2004.Wordnet::Similarity?Measuring therelatedness of concepts.
In DemonstrationPapers at the Human Language TechnologyConference of the North American Chapterof the Association for ComputationalLinguistics (HLT-NAACL 2004),pages 38?41, Boston, MA.Plag, Ingo.
2003.
Word-formation inEnglish.
Cambridge University Press,Cambridge.Ratnaparkhi, Adwait.
1996.
A maximumentropy model for part-of-speechtagging.
In Proceedings of the Conferenceon Empirical Methods in NaturalLanguage Processing, pages 133?142,Philadelphia, PA.Schwartz, Ariel S. and Marti A. Hearst.2003.
A simple algorithm for identifyingabbreviation definitions in biomedicaltexts.
In Proceedings of the PacificSymposium on Biocomputing (PSB 2003),pages 451?462, Lihue, HI.Shen, Libin and Aravind K. Joshi.
2005.Ranking and reranking with perceptron.Machine Learning, 60(1):73?96.149
