Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 27?33,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA New Yardstick and Tool for Personalized Vocabulary BuildingThomas K Landauer Kirill KireyevPearson Education,Knowledge TechnologiesCharles Panaccione{tom.landauer,kirill.kireyev,charles.panaccione}@pearson.comAbstractThe goal of this research is to increase thevalue of each individual student's vocabularyby finding words that the student doesn?tknow, needs to, and is ready to learn.
To helpidentify such words, a better model of howwell any given word is expected to be knownwas created.
This is accomplished by using asemantic language model, LSA, to track howevery word changes with the addition of moreand more text from an appropriate corpus.
Wedefine the ?maturity?
of a word as the degreeto which it has become similar to that aftertraining on the entire corpus.An individual student?s average vocabu-lary level can then be placed on the word-maturity scale by an adaptive test.
Finally, thewords that the student did or did not know onthe test can be used to predict what otherwords the same student knows by using mul-tiple maturity models trained on random sam-ples of typical educational readings.
Thisdetailed information can be used to generatehighly customized vocabulary teaching andtesting exercises, such as Cloze tests.1 Introduction1.1 Why ?Vocabulary First?There are many arguments for the importanceof more effective teaching of vocabulary.
Here aresome examples:(1) Baker, Simmons, & Kame'enui (1997)found that children who enter school with limitedvocabulary knowledge grow much more discrepantover time from their peers who have rich vocabu-lary knowledge.(2.)
Anderson & Freebody (1981) found thatthe number of words in student?s meaning vocabu-laries was the best predictor of how well theycomprehend text.
(3) An unpublished 1966 study of the correla-tion between entering scores of Stanford Studentson the SAT found the vocabulary component to bethe best predictor of grades in every subject, in-cluding science.
(4) The number of words students learn variesgreatly, from 0.2 to 8 words per day and from 50 toover 3,000 per year.
(Anderson & Freebody,1981)(5) Printed materials in grades 3 to 9 on averagecontain almost 90,000, distinct word families andnearly 500,000 word forms (including propernames.)
(Nagy & Anderson, 1984).
(6) Nagy and Anderson (1984) found that onaverage not knowing more than one word in a sen-tence prevented its tested understanding, and thatthe probability of learning the meaning of a newword by one encounter on average was less thanone in ten.
(7) John B. Carroll?s (1993) meta-analysis offactor analyses of measured cognitive ability foundthe best predictor to be tests of vocabulary.
(8) Hart and Risley?s large randomized obser-vational study of the language used in householdswith young children found that the number ofwords spoken within hearing of a child was associ-ated with a three-fold difference in vocabulary byschool entry.1.2 The ChallengeSeveral published sources and inspection of thenumber of words taught in recent literacy text-books and online tools suggest that less than 400words per year are directly tutored in Americanschools.
Thus, the vast majority of vocabularymust be acquired from language exposure, espe-cially from print because the oral vocabulary ofdaily living is usually estimated to be about 20,00027words, of which most are known by early schoolyears.
But it would obviously be of great value tofind a way to make the explicit teaching of vocabu-lary more effective, and to make it multiply theeffects of reading.
These are the goals of the newmethodologies reported here.It is also clear that words are not learned in iso-lation: learning the meaning of a new word re-quires prior knowledge of many other words, andby most estimates it takes a (widely variable) aver-age of ten encounters in different and separatedcontexts.
(This, by the way, is what is required tomatch human adult competence in the computa-tional language model used here.
Given a text cor-pus highly similar to that experienced by alanguage learner, the model learns at very close tothe same rate as an average child, and it learns newwords as much as four times faster the more oldwords it knows (Landauer & Dumais, 1997).
)An important aside here concerns a widely cir-culated inference from the Nagy and Anderson(1984) result that teaching words by presentingthem in context doesn?t produce enough vocabu-lary growth to be the answer.
The problem is thatthe experiments actually show only that the in-serted target word itself is usually not learned wellenough to pass a test.
But in the simulations, wordsare learned a little at a time; exposure to a sentenceincreases the knowledge of many other words, bothones in the sentence and not.
Every encounter withany word in context percolates meaning throughthe whole current and future vocabulary.
Indeed, inthe simulator, indirect learning is three to fivetimes as much as direct, and is what accounts forits ability to match human vocabulary growth andpassage similarity.
Put differently, the helpful thingthat happens on encountering an unknown word isnot guessing its meaning but its contribution tounderlying understanding of language.However, a vicious negative feedback looplurks in this process.
Learning from reading re-quires vocabulary knowledge.
So the vocabulary-rich get richer and the vocabulary-poor get rela-tively poorer.
Fortunately, however, in absoluteterms there is a positive feedback loop: the morewords you know, the faster you can learn newones, generating exponential positive growth.
Thusthe problem and solution may boil down to in-creasing the growth parameter for a given studentenough to make natural reading do its magic better.Nonetheless, importantly, it is patently obviousthat it matters greatly what words are taught how,when and to which students.The hypothesis, then, is that a set of tools thatcould determine what particular words an individ-ual student knows and doesn?t, and which oneslearned (and sentences understood) would mosthelp other words to be learned by that studentmight have a large multiplying effect.
It is such atoolbox that we are endeavoring to create by usinga computational language model with demon-strated ability to simulate human vocabularygrowth to a reasonably close approximation.
Theprincipal foci are better selection and ?personaliza-tion?
of what is taught and teaching more quicklyand with more permanence by application of opti-mal spacing of tests and practice?into which wewill not go here.1.3 Measuring vocabulary knowledgeCurrently there are three main methods formeasuring learner vocabulary, all of which are in-adequate for the goal.
They are:1.
Corpus Frequency.
Collect a large sampleof words used in the domain of interest, for exam-ple a collection of textbooks and readers used inclassrooms, text from popular newspapers, a largedictionary or the Internet.
Rank the words by fre-quency of occurrence.
Test students on a randomsubset of, say, the 1,000, 2,000 and 5,000 mostfrequent words, compute the proportion known ateach ?level?
and interpolate and extrapolate.
Thisis a reasonable method, because frequently en-countered words are the ones most frequentlyneeded to be understood.2.
Educational Materials.
Sample vocabularylessons and readings over classrooms at differentschool grades.3.
Expert Judgments.
Obtain informed expertopinions about what words are important to knowby what age for what purposes.Some estimates combine two or more of theseapproaches, and they vary in psychometric sophis-tication.
For example, one of the most sophisti-cated, the Lexile Framework, uses Rasch scaling(Rasch, 1980) of a large sample of student vocabu-lary test scores (probability right on a test, holdingstudent ability constant) to create a difficultymeasure for sentences and then infers the difficultyof words, in essence, from the average difficulty ofthe sentences in which they appear.28The problem addressed in the present projectgoal is that all of these methods measure only theproportion of tested words known at one or morefrequency ranges, in chosen school grades or forparticular subsets of vocabulary (e.g.
?academic?words), and for a very small subset?those tested -some of the words that the majority of a classknows.
What they don?t measure is exactly whichwords in the whole corpus a given student knowsand to what extent, or which words would be mostimportant for that student to learn.A lovely analog of the problem comes fromErnst Rothkopf?s (1970) metaphor that everyonepasses through highly different ?word swarms?each day on their way to their (still highly differen-tiated) adult literacy.2 A new metric: Word MaturityThe new metric first applies Latent SemanticAnalysis (LSA) to model how representation ofindividual words changes and grows toward theiradult meaning as more and more language is en-countered.
Once the simulation has been created,an adaptive testing method can be applied to placeindividual words on separate growth curves - char-acteristic functions in psychometric terminology.Finally, correlations between growth curves atgiven levels can be used to estimate the achievedgrowth of other words.2.1 How it works in more detail: LSA.A short review of how LSA works will be use-ful here because it is often misunderstood and acorrect interpretation is important in what follows.LSA models how words combine into meaningfulpassages, the aspect of verbal meaning we take tobe most critical to the role of words in literacy.
Itdoes this by assuming that the ?meaning?
(pleasebear with the nickname) of a meaningful passage isthe sum of the meanings of its words:Meaning of passage ={meaning of first wd} +{meaning of second word} + ?
+{meaning of last word}A very large and representative corpus of thelanguage to be modeled is first collected and repre-sented as a term-by-document matrix.
A powerfulmatrix algebra method called Singular Value De-composition is then used to make every paragraphin the corpus conform to the above objective func-tion?word representations sum to passage repre-sentations - up to a best least-squaresapproximation.
A dimensionality-reduction step isperformed, resulting in each word and passagemeanings represented as a (typically) 300 elementreal number vector.
Note that the property of a vec-tor standing for a word form in this representationis the effect that it has on the vector standing forthe passage.
(In particular, it is only indirectly areflection of how similar two words are to eachother or how frequently they have occurred in thesame passages.)
In the result, the vector for a wordis the average of the vectors for all the passages inwhich it occurs, and the vector for a passage is, ofcourse, the average all of its words.In many previous applications to education, in-cluding automatic scoring of essays, the model?ssimilarity to human judgments (e.g.
by mutual in-formation measures) has been found to be 80 to90% as high as that between two expert humans,and, as mentioned earlier, the rate at which itlearns the meaning of words as assessed by variousstandardized and textbook-based tests has beenfound to closely match that of students.
For moredetails, evaluations and previous educational appli-cations, see (Landauer et al, 2007).2.2 How it works in more detail: Word Ma-turity.Taking LSA to be a sufficiently good approxi-mation of human learning of the meanings con-veyed by printed word forms, we can use it to tracktheir gradual acquisition as a function of increasingexposure to text representative in size and contentof that which students at successive grade levelsread.Thus, to model the growth of meaning of indi-vidual words, a series of sequentially accumulatedLSA ?semantic spaces?
(the collection of vectorsfor all of the words and passages) are created.
Cu-mulative portions of the corpus thus emulate thegrowing total amount of text that has been read bya student.
At each step, a new LSA semantic spaceis created from a cumulatively larger subset of thefull adult corpus.Several different ways of choosing the succes-sive sets of passages to be added to the training sethave been tried, ranging from ones based on read-ability metrics (such as Lexiles or DRPs) to en-29tirely randomly selected subsets.
Here, the stepsare based on Lexiles to emulate their order of en-counter in typical school reading.This process results in a separate LSA model ofword meanings corresponding to each stage of lan-guage learning.
To determine how well a word orpassage is known at a given stage of learning?agiven number or proportion of passages from thecorpus?its vector in the LSA model correspond-ing to a particular stage is compared with the vec-tor of the full adult model (one that has beentrained on a corpus corresponding to a typicaladult?s amount of language exposure).
This is doneusing a linear transformation technique known asProcrustes Alignment to align the two spaces?those after a given step to those based on the fullcorpus, which we call its ?adult?
meaning.Word maturity is defined as the similarity of aword?s vector at a given stage of training and thatat its adult stage as measured by cosine.
It is scaledas values ranging between 0 (least mature) and 1(most mature).Figure 1 shows growth curves for an illustrativeset of words.
In this example, 17 successive cumu-lative steps were created, each containing ~5000additional passages.Word Meaning Maturity0.00.20.40.60.81.01 3 5 7 9 11 13 15 17Model LevelSimilaritydogelectoralprimateproductivityturkeyFigure 1.
An illustration of meaning maturity growth of sev-eral words as a function of language exposure.Some words (e.g.
?dog?)
are almost at theiradult meaning very early.
Others hardly get starteduntil later.
Some grow quickly, some slowly.
Somegrow smoothly, some in spurts.
Some, like ?tur-key,?
grow rapidly, plateau, then resume growingagain, presumably due to multiple senses(?Thanksgiving bird?
vs.
?country?)
learned at dif-ferent periods (in LSA, multiple ?senses?
are com-bined in a word representation approximately inproportion to their frequency.
)The maturity metric has several conceptual ad-vantages over existing measures of the status ofa word?s meaning, and in particular should be keptconceptually distinct from the ambiguous and oftenpoorly defined term ?difficulty?
and from whetheror not students in general or at some developmen-tal stage can properly use, define or understand itsmeaning.
It is a mathematical property of a wordthat may or may not be related to what particularpeople can do with it.What it does is provide a detailed view of thecourse of development of a word?s changing repre-sentation?its ?meaning?, reciprocally defined asits effect on the ?meaning?
of passages in which itoccurs,?as a function of the amount and nature ofthe attestedly meaningful passages in which it hasbeen encountered.
Its relation to ?difficulty?
ascommonly used would depend, among otherthings, on whether a human could use it for somepurpose at some stage of development of the word.Thus, its relation to a student?s use of a word re-quires a second step of aligning the student?s wordknowledge with the metric scaling.
This is analo-gous to describing a runner?s ?performance?
byaligning it with well-defined metrics for time anddistance.It is nevertheless worth noting that the wordmaturity metric is not based directly on corpus fre-quency as some other measures of word status are(although its average level over all maturities ismoderately highly correlated with total corpus fre-quency as it should be) or on other heuristics, suchas grade of first use or expert opinions of suitabil-ity.What is especially apparent in the graph aboveis that after a given amount of language exposure,analogous to age or school grade, there are largedifferences in the maturity of different words.
Infact the correlation between frequency of occur-rence in a particular one of the 17 intermediate cor-pora and word maturity is only 0.1, measured over20,000 random words.
According to the model--and surely common sense--words of the same fre-quency of encounter (or occurrence in a corpus)are far from equally well known.
Thus, all methodsfor ?leveling?
text and vocabulary instructionbased on word frequency must hide a great rangeof differences.To illustrate this in more detail, Table 1, showscomputed word maturities for a set of words thathave nearly the same frequency in the full corpus30(column four) when they have been added only50?5 times (column two).
The differences are solarge as to suggest the choice of words to teachstudents in a given school grade would profit muchfrom being based on something more discrimina-tive than either average word frequency or wordfrequency as found in the texts being read or in thesmall sample that can be humanly judged.
Evenbetter, it would appear, should be to base what istaught to a given student on what that student doesand doesn?t know but needs to locally and wouldmost profit from generally.Word Occurrencesin intermedi-ate corpus(level 5)Occurrencesin adultcorpusWordmaturity(at level5)marble 54 485 0.21sunshine 49 508 0.31drugs 53 532 0.42carpet 48 539 0.59twin 48 458 0.61earn 53 489 0.70beam 47 452 0.76Table 1 A sample of words with roughly the same number ofoccurrences in both intermediate (~50) and adult (~500) cor-pusThe word maturity metric appears to performwell when validated by some external methods.For example, it reliably discriminates betweenwords that were assigned to be taught in differentschool grades by (Biemiller, 2008), based on acombination of expert judgments and comprehen-sion tests (p < 0.03), as shown in Table 2.grade 2,knownby > 80%grade 2,known by40-80%grade 6,known by40-80%grade 6,knownby < 40%n=1034 n=606 n=1125 n=14114.4 6.5 8.8 9.5Table 2 Average level for each word to reach a 0.5 maturitythreshold, for words that are known at different levels by stu-dents of different grades (Biemiller, 2008).Median word maturity also tracks the differ-ences (p < 0.01) between essays written by stu-dents in different grades as shown in Figure 2.Percent of "adult" words in essay0%1%2%3%4%5%4 6 8 10 12Student grade levelFigure 2 Percentage of ?adult?
words used in essays writtenby students of different grade levels.
?Adult?
words are de-fined as words that reach a 0.5 word maturity threshold at orlater than the point where half of the words in the languagehave reached 0.5 threshold.2.3 Finding words to teach individual stu-dentsUsing the computed word maturity values, asigmoid characteristic curve is generated to ap-proximate the growth curve of every word in thecorpus.
A model similar to one used in item re-sponse theory (Rasch, 1980) can be constructedfrom the growth curve due to its similarity in shapeand function to an IRT characteristic curve; bothcurves represent the ability of a student.
The char-acteristic curve for the IRT is needed to properlyadminister adaptive testing, which greatly in-creases the precision and generalizeability of theexam.
Words to be tested are chosen from the cor-pus beginning at the average maturity of words atthe approximate grade level of the student.
Thirtyto fifty word tests are used to home in on the stu-dent?s average word maturity level.
In initial trials,a combination of yes/no and Cloze tests are beingused.
Because our model does not treat all wordsof a given frequency as equivalent, this alone sup-ports a more precise and personalized measure of astudent?s vocabulary.
In plan, the student level willbe updated by the results of additional tests admin-istered in school or by Internet delivery.The final step is to generalize from the assessedknowledge of words a particular student (let?s callher Alice) is tested on to other words in the corpus.This is accomplished by first generating a largenumber of simulated students (and their word ma-turity curves) using the method described above.Each simulated student is trained on one of many ~12 million word corpora, size and content ap-proximating the lifelong reading of a typical col-lege student, that have been randomly sampledfrom a representative corpus of more than half a31billion words.
Some of these simulated students?knowledge of the words being tested will be moresimilar to Alice than others.
We can then estimateAlice?s knowledge of any other word w in the cor-pus by averaging the levels of knowledge of w bysimulated students whose patterns of tested wordknowledge are most similar hers.
The method restson the assumption that there are sufficiently strongcorrelations between the words that a given studenthas learned at a given stage (e.g.
resulting fromRothkopf?s personal ?swarms?.)
While simulationsare promising, empirical evidence as to the powerof the approach with non-simulated students is yetto be determined.3 Applying the methodOn the assumption that learning words by theireffects on passage meanings as LSA does is good,initial applications use Cloze items to simultane-ously test and teach word meanings by presentingthem in a natural linguistic context.
Using thesimulator, the context words in an item are pre-dicted to be ones that the individual student alreadyknows at a chosen level.
The target words, wherethe wider pedagogy permits, are ones that are re-lated and important to the meaning of the sentenceor passage, as measured by LSA cosine similaritymetric, and, ipso facto, the context tends to contex-tually teach their meaning.
They can also be cho-sen to be those that are computationally estimatedto be the most important for a student to know inorder to comprehend assigned or student-chosenreadings?because their lack has the most effect onpassage meanings?and/or in the language in gen-eral.
Using a set of natural language processingalgorithms (such as n-gram models, POS-tagging,WordNet relations and LSA) the distracter itemsfor each Cloze are chosen in such a way that theyare appropriate grammatically, but not semanti-cally, as illustrated in the example below.In summary, Cloze-test generation involves thefollowing steps:1.
Determine the student?s overall knowledgelevel and individual word knowledge predictionsbased on previous interactions.2.
Find important words in a reading that areappropriate for a particular student (using metricsthat include word maturity).3.
For each word, find a sentence in a largecollection of natural text, such that the rest of thesentence semantically implies (is related to) thetarget word and is appropriate for student?s knowl-edge level.4.Find distracter words that are (a) level-appropriate, (b) are sufficiently related and (c) fitgrammatically, but (d) not semantically, into thesentence.All the living and nonliving things around an ___is its environment.A.
organism   B. oxygen   C. algaeFreshwater habitats can be classified according tothe characteristic species of fish found in them,indicating the strong ecological relationship be-tween an ___ and its environment.A.
adaptation   B. energy   C. organismTable 3 Examples of auto-generated Cloze tests for the sameword (organism) and two students of lower and higher ability,respectively.4 Summary and present statusA method based on computational model-ing of language, in particular one that makes therepresentation of the meaning of a word its effecton the meaning of a passage its objective, LSA,has been developed and used to simulate thegrowth of meaning of individual word representa-tions towards those of literate adults.
Basedthereon, a new metric for word meaning growthcalled ?Word Maturity?
is proposed.
The measureis then applied to adaptively measuring the averagelevel of an individual student?s vocabulary, pre-sumably with greater breadth and precision thanoffered by other methods, especially those basedon knowledge of words at different corpus fre-quency.
There are many other things the metricmay support, for example better personalizedmeasurement of text comprehensibility.However, it must be emphasized that themethod is very new and essentially untried exceptin simulation.
And it is worth noting that while theproposed method is based on LSA, many or all ofits functionalities could be obtained with someother computational language models, for examplethe Topics model.
Comparisons with other meth-ods will be of interest, and more and more rigorousevaluations are needed, as are trials with morevarious applications to assure robustness.325 ReferencesRichard C. Anderson, Peter Freebody.
1981.
Vo-cabulary Knowledge.
In J. T. Guthrie (Ed.
),Comprehension and teaching: Research reviews(pp.
77-117).
International Reading Association,Newark DE.Scott K. Baker,  Deborah C. Simmons, Edward J.Kameenui.
1997.
Vocabulary acquisition: Re-search bases.
In Simmons, D. C. & Kameenui,E.
J.
(Eds.
), What reading research tells usabout children with diverse learning needs:Bases and basics.
Erlbaum, Mahwah, NJ.Andrew Biemiller (2008).
Words Worth Teaching.Co-lumbus, OH:  SRA/McGraw-Hill.John B Carroll.
1993.
Cognitive Abilities: A surveyof factor-analytic studies.
Cambridge: Cam-bridge University Press, 1993.Betty Hart, Todd R. Risley.
1995.
Meaningful dif-ferences in the everyday experience of youngAmerican children.
Brookes Publishing, 1995.Melanie R. Kuhn, Steven A. Stahl.
1998.
Teachingchildren to learn word meanings from context:A synthesis and some questions.
Journal of Lit-eracy Research, 30(1) 119-138.Thomas K Landauer, Susan Dumais.
1997.
A solu-tion to Plato's problem: The Latent SemanticAnalysis theory of the Acquisition, Induction,and Representation of Knowledge.
Psychologi-cal Review, 104, pp 211-240.Thomas K Landauer, Danielle S. McNamara,Simon Dennis, and Walter Kintsch.
2007.
Hand-book of Latent Semantic Analysis.
LawrenceErlbaum.Cleborne D. Maddux (1999).
Peabody Picture Vo-cabulary Test III (PPVT-III).
Diagnostique, v24n1-4, p221-28, 1998-1999William E. Nagy, Richard C. Anderson.
1984.How many words are there in printed schoolEnglish?
Reading Research Quarterly, 19, 304-330.Ernst Z. Rothkopf, Ronald D. Thurner.
1970.
Ef-fects of written instructional material on the sta-tistical structure of test essays.
Journal ofEducacational Psychology, 61, 83-89.George Rasch.
(1980).
Probabilistic models forsome intelligence and attainment tests.
(Copen-hagen, Danish Institute for Educational Re-search), expanded edition (1980) with forewordand afterword by B.D.
Wright.
Chicago: TheUniversity of Chicago Press.33
