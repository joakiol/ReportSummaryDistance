An Efficient Statistical Speech Act Type Tagging System forSpeech Translation SystemsHidek i  Tanaka  and  Ak io  YokooATR Interpret ing Telecommunicat ions Research Laborator ies2-2, Hikaridai, Seika-cho, Soraku-gun,  Kyoto,  619-0288, Japan{t anakah I ayokoo}?itl, atr.
co. jpAbst rac tThis paper describes a new efficient speechact type tagging system.
This system cov-ers the tasks of (1) segmenting a turn intothe optimal number of speech act units(SA units), and (2) assigning a speech acttype tag (SA tag) to each SA unit.
Ourmethod is based on a theoretically clearstatistical model that integrates linguistic,acoustic and situational information.
Wereport tagging experiments on Japaneseand English dialogue corpora manually la-beled with SA tags.
We then discuss theperformance difference between the twolanguages.
We also report on some trans-lation experiments on positive responseexpressions using SA tags.1 I n t roduct ionThis paper describes a statistical speech act typetagging system that utilizes linguistic, acoustic andsituational features.
This work can be viewed as astudy on automatic "Discourse Tagging" whose ob-jective is to assign tags to discourse units in texts ordialogues.
Discourse tagging is studied mainly fromtwo different viewpoints, i.e., linguistic and engineer-ing viewpoints.
The work described here belongs tothe latter group.
More specifically, we are interestedin automatically recognizing the speech act types ofutterances and in applying them to speech transla-tion systems.Several studies on discourse tagging to date havebeen motivated by engineering applications.
Theearly studies by Nagata and Morimoto (1994) andReithinger and Maier (1995) showed the possibilityof predicting dialogue act tags for next utteranceswith statistical methods.
These studies, however,presupposed properly segmented utterances, whichis not a realistic assumption.
In contrast o thisassumption, automatic utterance segmentation (ordiscourse segmentation) is desired here.Discourse segmentation i linguistics, whethermanual or automatic, has also received keen atten-tion because such segmentation provides the founda-tion of higher discourse structures (Grosz and Sid-net, 1986).Discourse segmentation has also received keen at-tention from the engineering side because the nat-ural language processing systems that follow thespeech recognition system are designed to accept lin-guistically meaningful units (Stolcke and Shriberg,1996).
There has been a lot of research followingthis line such as (Stolcke and Shriberg, 1996) (Cet-tolo and Falavigna, 1998), to only mention a few.We can take advantage of these studies as a pre-process for tagging.
In this paper, however, we pro-pose a statistical tagging system that optimally per-forms segmentation a d tagging at the same time.Previous tudies like (Litman and Passonneau, 1995)have pointed out that the use of a multiple informa-tion source can contribute to better segmentationand tagging, and so our statistical model integrateslinguistic, acoustic and situational information.The problem can be formalized as a search prob-lem on a word graph, which can be efficiently han-dled by an extended ynamic programming algo-rithm.
Actually, we can efficiently find the optimalsolution without limiting the search space at all.The results of our tagging experiments involvingboth Japanese and English corpora indicated a highperformance for Japanese but a considerably owerperformance for the English corpora.
This workalso reports on the use of speech act type tags fortranslating Japanese and English positive responseexpressions.
Positive responses quite often appearin task-oriented dialogues like those in our tasks.They are often highly ambiguous and problematicin speech translation.
We will show that these ex-pressions can be effectively translated with the helpof dialogue information, which we call speech acttype tags.2 The  Prob lemsIn this section, we briefly explain our speech act typetags and the tagged data and then formally definethe tagging problem.3812.1 Data  and  TagsThe data used in this study is a collection of tran-scribed dialogues on a travel arrangement task be-tween Japanese and English speakers mediated byinterpreters (Morimoto et al, 1994).
The tran-scriptions were separated by language, i.e., En-glish and Japanese, and the resultant wo corporashare the same content.
Both transcriptions wentthrough morphological nalysis, which was manuallychecked.
The transcriptions have clear turn bound-aries (TB's).Some of the Japanese and English dialogue fileswere manually segmented into speech act units (SAunits) and assigned with speech act type tags (SAtags).
The SA tags represent a speaker's intentionin an utterance, and is more or less similar to thetraditional illocutionary force type (Searle, 1969).The SA tags for the Japanese language were basedon the set proposed by Seligman et al (1994) andhad 29 types.
The English SA tags were based onthe Japanese tags, but we redesigned and reducedthe size to 17 types.
We believed that an excessivelydetailed tag classification would decrease the inter-coder reliability and so pruned some detailed tags)The following lines show an example of the Englishtagged dialogues.
Two turns uttered by a hotel clerkand a customer were Segmented into SA units andassigned with SA tags.<clerk's turn>Hello, (expressive)New York City Hotel, ( in fo rm)may I help you ?
(of fer)<customer(interpreter)'s turn>Hello, (express ive)my name is Hiroko Tanaka ( in fo rm)and I would like to make a reservation fora room at your hotel.
(desire)The tagging work to the dialogue was conductedby experts who studied the tagging manual before-hand.
The manual described the tag definitionsand turn segmentation strategies and gave examples.The work involved three experts for the Japanesecorpus and two experts for the English corpus.
2The result was checked and corrected by one ex-pert for each language.
Therefore, since the workwas done by one expert, the inter-coder tagging in-stability was suppressed to a minimum.
As the re-sult of the tagging, we obtained 95 common dialoguefiles with SA tags for Japanese and English and usedthem in our experiments.1Japanese tags, for example, had four tags mainlyused for dialogue endings: thank, offer-follow-up, good-wishes, and farewell, most of which were reduced to ex-pressive in English.2They did not listen to the recorded sounds in eithercase.2.2 P rob lem Formulat ionOur tagging system assumes an input of a word se-quence for a dialogue produced by a speech recog-nition system.
The word sequence is accompaniedwith clear turn boundaries.
Here, the words do notcontain any punctuation marks.
The word sequencecan be viewed as a sequence of quadruples:"'" (Wi-1, l i-1, ai-1,  s i -1),  (wi, li, ai, 8 i ) .
.
.where wi represents a surface wordform, and eachvector represents the following additional informa-tion for wi.l i: canonical form and part of speech ofwi (linguistic feature)ai:  pause duration measured millisecondsafter wi (acoustic feature)si :  speaker's identification for wi such asclerk or customer (situational feature)Therefore, an utterance like Hel lo  I am John Ph i l l ipsand ... uttered by a cus lomer  is viewed as a sequencelike(Hello, (hello, INTER),  100, customer),(I,(i, PRON),0, customer)), (am, (be,BE), 0, customer) ....From here, we will denote a word sequence as W =wl ,  w2, .. ?
w i ,  ..
?, Wn for simplicity.
However, notethat W is a sequence of quadruples as describedabove.The task of speech act type tagging in this pa-per covers two tasks: (1) segmentation of a wordsequence into the optimal number of SA units, and(2) assignment of an SA tag to each SA unit.
Here,the input is a word sequence with clear TB's, andour tagger takes each turn as a process unit.
3In this paper, an SA unit is denoted as u and thesequence is denoted as U.
An SA tag is denoted ase represents t and the sequence is denoted as T. x sa sequence of x starting from s to e. Therefore,represents a tag sequence from 1 to j .The task is now formally addressed as follows:find the best SA unit sequence U and tag sequenceT for each turn when a word sequence W with clearTB's is given.
We will treat this problem with thestatistical model described in the next section.3 S ta t i s t i ca l  Mode lThe problem addressed in Section 2 can be formal-ized as a search problem in a word graph that holdsall possible combinations of SA units in a turn.
Wetake a probabilistie approach to this problem, whichformalizes it as finding a path (U ,T )  in the wordgraph that maximizes the probability P (U ,  T I W) .3Although we do not explicitly represent TB's in aword sequence in the following discussions, one mightassume virtual TB markers like @ in the word sequence.382This is formally represented in equation (1).
Thisprobability is naturally decomposed into the prod-uct of two terms as in equation (3).
The first prob-ability in equation (3) represents an arbitrary wordsequence constituting one SA unit ui, given hj (thehistory of SA units and tags from the beginning ofa dialogue, hj = u J - l , t  j - l )  and input W.  The sec-ond probability represents the current SA unit u ibearing a particular SA tag tj, given uj, hi, andW.
(U ,T )  = argmaxP(U,T  I w) ,  (1)U,TkP(uj,tj I hi, W), = argmax H (2)U,T j=lk_ -  argm x l \ ]  P(ui I hi, W)U,T j=lx P(tj I uj, hi, W).
(3)We call the first term "unit existence probability"Ps  and the second term "tagging probability" PT.Figure 1 shows a simplified image of the probabilitycalculation in a word graph, where we have finishedprocessing the word sequence of w~ -1Now, we estimate the probability for the word se-quence w~ +p-1 constituting an SA unit uj and hav-ing a particular SA tag tj.
Because of the problem ofsparse data, these probabilities are hard to directlyestimate from the training corpus.
We will use thefollowing approximation techniques.3.1 Un i t  Ex i s tence  Probab i l i tyThe probability of unit existence PE is actuallyequivalent to the probability that the word sequencew~,.
.
.
,  w,+p-1 exists as one SA unit given h i andW (Fig.
1).We then approximate PE byPE ~-- P(B~,_I,w, = l l hj, W)xP(B~.+,,_,,w,.,, = 1 I hi, W)s+p--2x H P(Bw,-,~+I = 0 I h i ,W),  (4)I T l :$where the random variable Bw=,,~=+l takes the bi-nary values 1 and 0.
A value of 1 corresponds to theexistence of an SA unit boundary between wx andw=+l, and a value of 0 to the non-existence of an SAunit boundary.
PE is approximated by the productof two types of probabilities: for a word sequencebreak at both ends of an SA unit and for a non-break inside the unit.
Notice that the probabilitiesof the former type adjust an unfairly high probabil-ity estimation for an SA unit that is made from ashort word sequence.The estimation of PE is now reduced to that ofP(Bw=,w~+l I hi, W).
This probability is estimatedby a probabilistic decision tree and we haveP(Bw=,Wx+, I hi, W) ~- P(Bw .
.
.
.
+1 I eE(hj, W)),where riPE is a decision tree that categorizes hj, Winto equivalent classes (Jelinek, 1997).
We modi-fied C4.5 (Quinlan, 1993) style algorithm to produceprobability and used it for this purpose.
The deci-sion tree is known to be effective for the data sparse-ness problem and can take different ypes of parame-ters such as discrete and continuous values, which isuseful since our word sequence contains both typesof features.Through preliminary experiments, we found thathj (the past history of tagging results) was not usefuland discarded it.
We also found that the probabilitywas well estimated by the information available in ashort range of r around w=, which is stored in W.Actually, the attributes used to develop the tree wereat~X-\]-7* in W'  = ~- r+ l "  *+r ?
surface wordforms for ~=-~+1,z+r and the pause duration parts of speech for wx_ +l,between wx and w=+l.
The word range r was setfrom 1 to 3 as we will report in sub-section 5.3.As a result, we obtained the final form of PE asPE ~-- P(Bw .
.
.
.
~, = 1 \ [~s(W' ) )x P(B~,+p_,,~,+p = 1 \[ ~s(W' ) )s+p-2?
H P(S~,,.w~,+ 1 = 01~E(W' ) ) (5 )m:$3.2 Tagg ing  Probab i l i tyThe tagging probability PT was estimated by thefollowing formula utilizing a decision tree eT- Twofunctions named f and g were also utilized to extractinformation from the word sequence in uj.PT ~-- P(tj J ff2T(f(uj),g(uj),tj_l,...,tj_m)) (6)As this formula indicates, we only used informationavailable with the uj and m histories of SA tags inhi .
The function f(uj) outputs the speaker's identi-fication of uj.
The function g(uj) extracts cue wordsfor the SA tags from uj using a cue word list.
Thecue word list was extracted from a training corpusthat was manually labeled with the SA tags.
Foreach SA tag, the 10 most dependent words were ex-tracted with a x2-test.
After converting these intocanonical forms, they were conjoined.To develop a statistical decision tree, we used aninput table whose attributes consisted of a cue wordlist, a speaker's identification, and m previous tags.The value for each cue word was a binary value,where 1 was set when the utterance uj containedthe word, or otherwise 0.
The effect of f(uj), g(uj),and length m for the tagging performance will bereported in sub-section 5.3.4 Search MethodA search in a word graph was conducted using theextended ynamic programming technique proposed383h j  history turn boundary current process fronto-----o o \] ~.~ Uj-l' (i-1 ~ uj, (\] - - - O<: :>IO .
.
.
.
C:) - C:>0 .
.
.
.
CDWl Ws-1 | Ws Ws+l Ws+p-1 |Ws+p WnW word sequence for a dialogueFigure 1: Probability calculation.by Nagata (1994).
This algorithm was originally de-veloped for a statistical Japanese morphological an-alyzer whose tasks are to determine boundaries in aninput character sequence having no separators andto give an appropriate part of speech tag to eachword, i.e., a character sequence unit.
This algorithmcan handle arbitrary lengths of histories of pos tagsand words and efficiently produce n-best results.We can see a high similarity between our task andJapanese morphological analysis.
Our task requiresthe segmentation of a word sequence instead of acharacter sequence and the assignment of an SA taginstead of a pos tag.The main difference is that a word dictionary isavailable with a morphological analyzer.
Thanks toits dictionary, a morphological analyzer can assumepossible morpheme boundaries.
4 Our tagger, onthe other hand, has to assume that any word se-quence in a turn can constitute an SA unit in thesearch.
This difference, however, does not requireany essential change in the search algorithm.5 Tagg ing  Exper iments5.1 Data  Prof i leWe have conducted several tagging experiments onboth the Japanese and English corpora described insub-section 2.1.
Table 1 shows a summary of the95 files used in the experiments.
In the experimentsdescribed below, we used morpheme sequences forinput instead of word sequences and showed the cor-responding counts.The average number of SA units per turn was2.68 for Japanese and 2.31 for English.
The aver-age number of boundary candidates per turn was18 for Japanese and 12.7 for English.
The numberof tag types, the average number of SA units, andthe average number of SA boundary candidates in-dicated that the Japanese data were more difficultto process.4Als0, the probability for the existence of a word canbe directly estimated from the corpus.Table 1: Counts in both corpora.Counts Japanese EnglishTurn 2,020 2,020SA unit 5,416 4,675Morpheme 38,418 27,639POS types 30 33SA tag type 29 175.2 Evaluat ion MethodsWe used "labeled bracket matching" for evalua-tion (Nagata, 1994).
The result of tagging can beviewed as a set of labeled brackets, where brack-ets correspond to turn segmentation and their labelscorrespond to SA tags.
With this in mind, the eval-uation was done in the following way.
We countedthe number of brackets in the correct answer, de-noted as R (reference).
We also counted the num-ber of brackets in the tagger's output, denoted asS (system).
Then the number of matching bracketswas counted and denoted as M (match).
Thus, wecould define the precision rate with M/S and therecall rate with M/R.The matching was judged in two ways.
One was"segmentation match": the positions of both start-ing and ending brackets (boundaries) were equal.The other was "segmentation+tagging match": thetags of both brackets were equal in addition to thesegmentation match.The proposed evaluation simultaneously con-firmed both the starting and ending positions of anSA unit and was more severe than methods that onlyevaluate one side of the boundary of an SA unit.Notice that the precision and recall for the segmen-tation+tagging match is bounded by those of thesegmentation match.5.3 Tagging Resu l tsThe total tagging performance is affected by the twoprobability terms PE and PT, both of which containthe parameters in Table 2.
To find the best param-384Table 2: Parameters in probability terms.PE PTx+rWx-r+lr: word rangef(uj): speaker of ujg(uj): cue words in ujtj-1 ... tj_,~ : previous SA tagsTable 4: T-scores for segmentation accuracies.Recall PrecisionA B C A B CB 2.84 - - B 1.25 - -C 2.71 0.12 - C 0.83 0.44 -D 2.57 0.28 0.17 D 0.74 0.39 0.01Table 3: Average accuracy for segmentation match.Parameter Recall rate % Precision rate %A 89.50 91.99B 91.89 92.92C 92.00 92.57D 92.20 92.58Table 5: Average accuracy for seg.+tag, match.Parameter Recall rate % Precision rate %E 72.25 72.70F 74.91 75.35G 74.83 75.29H 74.50 74.96eter set and see the effect of each parameter, weconducted the following two types of experiments.I Change the parameters for PE with fixed pa-rameters for PTThe effect of the parameters in PE was mea-sured by the segmentation match.II Change the parameters for PT with fixed pa-rameters for PEThe effect of the parameters in PT was mea-sured by the segmentation+tagging match.Now, we report the details with the Japanese set.5.3.1 Effects of DE with Japanese DataWe fixed the parameters for PT as f(uj), g(uj),tj-1, i.e., a speaker's identification, cue words in thecurrent SA unit, and the SA tag of the previous SAunit.
The unit existence probability was estimatedusing the following parameters.
(A): Surface wordforms and pos's ofw~ +1, i.e., wordrange r = 1(B): Surface wordforms and pos's of w x+2 i.e., word x- i ,range r ---- 2(C): (h) with a pause duration between wx, Wx+l(D): (U) with a pause duration between wx, wx+lUnder the above conditions, we conducted 10-foldcross-validation tests and measured the average re-call and precision rates in the segmentation match,which are listed in Table 3.We then conducted l-tests among these averagescores.
Table 4 shows the l-scores between differentparameter conditions.
In the following discussions,we will use the following l-scores: t~=0.0~5(18) --2.10 and t~=0.05(18) = 1.73.We can note the following features from Tables 3and 4.?
recall rate(B), (C), and (D) showed statistically signif-icant (two-sided significance level of 5%, i.e.,t > 2.10) improvement from (A).
(D) did notshow significant improvement from either (B)nor (C).?
precision rateAlthough (n) and (C) did not improve from(A) with a high statistical significance, we canobserve the tendency of improvement.
(D) didnot show a significant difference from (B) or(C).We can, therefore, say that (B) and (C) showedequally significant improvement from (A): expansionof the word range r from I to 2 and using pause infor-mation with word range 1.
The combination of wordrange 2 and pause (D), however, did not show anysignificant differences from (B) or (C).
We believethat the combination resulted in data sparseness.5.3.2 Effects of  PT with Japanese DataFor the Type II experiments, we set the parame-ters for PE as condition (C): surface wordforms andpos's of wx TM and a pause duration between w~ andw~+l.
Then, PT was estimated using the followingparameters.
(E): Cue words in utterance uj, i.e., g(uj)(F): (S) with t j _  1(G): (E) with t j_ l  and tj_2(H): (E) with tj-1 and a speaker's identificationf (u j )The recall and precision rates for the segmenta-tion?tagging match were evaluated in the same wayas in the previous experiments.
The results areshown in Table 5.
The l-scores among these param-eter setting are shown in Table 6.
We can observethe following features.?
recall rate(F) and (G) showed an improvement from (E)with a two-sided significance level of 10% (1 >385Table 6: T-scores for seg.+tag, accuracies.Recall PrecisionE F G E F GF 1.87 - - F 1.97 - -G 1.78 0.05 - G 1.90 0.04 -H 1.50 0.26 0.21 H 1.60 0.28 0.241.73).
However, (G) and (H) did not show sig-nificant improvements from (F).?
precision rateSame as recall rate.Here, we can say that tj-1 together with the cuewords (F) played the dominant role in the SA tagassignment, and the further addition of history t j -2(G) or the speaker's identification f(uj) (H) did notresult in significant improvements.5.3.3 Summary  of  Japanese TaggingExper imentsAs a concise summary, the best recall and preci-sion rates for the segmentation match were obtainedwith conditions (n) and (C): approximately 92%and 93%, respectively.
The best recall and preci-sion rates for the segmentation+tagging match were74.91% and 75.35 %, respectively (Table 5 (F)).
Weconsider these figures quite satisfactory consideringthe severeness of our evaluation scheme.5.3.4 English Tagging ExperimentWe will briefly discuss the experiments with En-glish data.
The English corpus experiments weresimilar to the Japanese ones.
For the SA unit seg-mentation, we changed the word range r from 1 to3 while fixing the parameters for PT to (H), wherewe obtained the best results with word range r --- 2,i.e., (B).
The recall rate was 71.92% and the preci-sion rate was 78.10%.
5We conducted the exact same tagging experi-ments as the Japanese ones by fixing the parame-ter for PE to (B).
Experiments with condition (H)showed the best score: the recall rate was 53.17%and the precision rate was 57.75%.
We obtainedlower performance than that for Japanese.
This wassomewhat surprising since we thought English wouldbe easier to process.
The lower performance in seg-mentation affected the total tagging performance.We will further discuss the difference in section 7.6 App l i ca t ion  o f  SA  tags  to  speecht rans la t ionIn this section, we will briefly discuss an applicationof SA tags to a machine translation task.
This is one~Experiments with pause information were notconducted.of the motivations of the automatic tagging researchdescribed in the previous ections.
We actually dealtwith the translation problem of positive responsesappearing in both Japanese and English dialogues.Japanese positive responses like Hatand Soudesuka, and the English ones like Yes andI see appear quite often in our corpus.
Since our di-alogues were collected from the travel arrangementdomain, which can basically be viewed as a sequenceof a pair of questions and answers, they naturallycontain many of these expressions.These expressions are highly ambiguous in word-sense.
For example, Hai can mean Yes (accept), Uhhuh (acknowledgment), hello (greeting) and so on.Incorrect ranslation of the expression could confusethe dialogue participants.
These expressions, how-ever, are short and do not contain enough clues forproper translation in themselves, o some other con-textual information is inevitably required.We assume that SA tags can provide such neces-sary information since we can distinguish the trans-lations by the SA tags in the parentheses in theabove examples.We conducted a series of experiments to verifyif positive responses can be properly translated us-ing SA tags with other situational information.
Weassumed that SA tags are properly given to these ex-pressions and used the manually tagged corpus de-scribed in Table 1 for the experiments.We collected Japanese positive responses from theSA units in the corpus.
After assigning an En-glish translation to each expression, we categorizedthese expressions into several representative forms.For example, the surface Japanese expression Ee,Kekkou desu was categorized under the representa-tive form Kekkou.We also made such data for English positive re-sponses.
The size of the Japanese and English datain representative forms (equivalent to SA unit) isshown in Table 7.
Notice that 1,968 out of 5,416Japanese SA units are positive responses and 1,037out of 4,675 English SA units are positive responses.The Japanese data contained 16 types of Englishtranslations and the English data contained 12 typesof Japanese translations in total.We examined the effects of all possible combi-nations of the following four features on transla-tion accuracy.
We trained decision trees with theC4.5 (Quinlan, 1993) type algorithm while usingthese features (in all possible combinations) as at-tributes.
(I) Representative form of the positive response(J) SA tag for the positive response(K) SA tag for the SA unit previous to the positiveresponse(L) Speaker (Hotel/Clerk)386Table 7: Representation forms and the counts.Japanese freq.Kekkou 69Soudesu ka 192Hal 930Soudesu 120Moehiron 7Soudesu ne 16Shouchi 30Wakari-mashita 304Kashikomari-mashita 300English freq.I understand 6Great 5Okay 240I see 136All right 136Very well 13Certainly 27Yes 359Fine 52Right 10Sure 44Very good 9Total 1,968 Total 1,037Table 8: Accuracies with one feature.Feature J toE(%)  E to J  (%)I 54.83 46.96J 51.73 34.33K 73.02 55.35L 40.09 37.80We will show some of the results.
Table 8 showsthe accuracy when using one feature as the attribute.We can naturally assume that the use of feature (I)gives the baseline accuracy.The result gives us a strange impression in thatthe SA tags for the previous SA units (K) were farmore effective than the SA tags for the positive re-sponses themselves (J).
This phenomenon can beexplained by the variety of tag types given to theutterances.
A positive response xpressions of thesame representative form have at most a few SA tagtypes, say two, whereas the previous SA units canhave many SA tag types.
If a positive response x-pression possesses five translations, they cannot betranslated with two SA tags.Table 9 shows the best feature combinations ateach number of features from 1 to 4.
The best fea-ture combinations were exactly the same for bothtranslation directions, Japanese to English and viceversa.
The percentages are the average accuracy ob-tained by the 10-fold cross-validation, and the t -score in each row indicates the effect of adding onefeature from the upper row.
We again admit a t -score that is greater than 2.01 as significant (two-sided significance level of 5 %).The accuracy for Japanese translation was sat-urated with the two features (K) and (I).
Furtheraddition of any feature did not show any significantimprovement.
The SA tag for the positive responsesdid not work.The accuracy for English translation was satu-Table 9: Best performance for each number of fea-tures.Features J toE(%)  t E to J  (%) tK 73.02 - 55.35 -K,I 88.51 15.42 60.66 3.10K,I,L 88.92 0.51 65.58 2.49K,I,L,J 88.21 0.75 66.74 0.55rated with the three features (K), (I), and (L).
Thespeaker's identification proved to be effective, unlikeJapanese.
This is due to the necessity of controllingpoliteness in Japanese translations according to thespeaker.
The SA tag for the positive responses didnot work either.These results suggest hat the SA tag informa-tion for the previous SA unit and the speaker's in-formation should be kept in addition to representa-tive forms when we implement the positive responsetranslation system together with the SA tagging sys-tem.7 Re la ted  Works  and  D iscuss ionsWe discuss the tagging work in this section.
In sub-section 5.3, we showed that Japanese segmentationinto SA units was quite successful only with lexicalinformation, but English segmentation was not thatsuccessful.Although we do not know of any experiments di-rectly comparable to ours, a recent work reportedby Cettolo and Falavigna (1998) seems to be sim-ilar.
In that paper, they worked on finding se-mantic boundaries in Italian dialogues with the"appointment scheduling task."
Their semanticboundary nearly corresponds to our SA unit bound-ary.
Cettolo and Falavigna (1998) reported recalland precision rates of 62.8% and 71.8%, respec-tively, which were obtained with insertion and dele-tion of boundary markers.
These scores are clearlylower than our results with a Japanese segmentationmatch.Although we should not jump to a generalization,we are tempted to say the Japanese dialogues areeasier to segment than western languages.
With thisin mind, we would like to discuss our study.First of all, was the manual segmentation qualitythe same for both corpora?
As we explained in sub-section 2.1, both corpora were tagged by experts,and the entire result was checked by one of themfor each language.
Therefore, we believe that therewas not such a significant gap in quality that couldexplain the segmentation performance.Secondly, which lexical information yielded sucha performance gap?
We investigated the effects ofpart-of-speech and morphemes in the segmentation387of both languages.
We conducted the same 10-foldcross-validation tests as in sub-section 5.3 and ob-tained 82.29% (recall) and 86.16% (precision) forJapanese under condition (B'), which used only pos'sin " x+~ for the PE calculation.
English, in con- W -1trast, marked rates of 65.63% (recall) and 73.35%(precision) under the same condition.
These resultsindicated the outstanding effectiveness of Japanesepos's in segmentation.
Actually, we could see somepos's such as "ending particle (shu-jyoshi)" whichclearly indicate sentence ndings and we consideredthat they played important roles in the segmenta-tion.
English, on the other hand, did not seem tohave such strong segment indicating pos's.
Althoughlexical information is important in English segmen-tation (Stoleke and Shriberg, 1996), what other in-formation can help improve such segmentation?Hirschberg and Nakatani (1996) showed thatprosodic information helps human discourse segmen-tation.
Litman and Passonneau (1995) addressedthe usefulness of a "multiple knowledge source"in human and automatic discourse segmentation.Vendittiand Swerts (1996) stated that the into-national features for many Indo-European lan-guages help cue the structure of spoken dis-course.
Cettolo and Falavigna (1998) reported im-provements in Italian semantic boundary detectionwith acoustic information.
All of these works indi-cate that the use of acoustic or prosodic informationis useful, so this is surely one of our future directions.The use of higher syntacticM information is alsoone of our directions.
The SA unit should be a mean-ingful syntactic unit, although its degree of meaning-fulness may be less than that in written texts.
Thegoodness of this aspect can be easily incorporated inour probability term PE.8 Conc lus ionsWe have described a new efficient statistical speechact type tagging system based on a statistical modelused in Japanese morphological nalyzers.
This sys-tem integrates linguistic, acoustic, and situationalfeatures and efficiently performs optimal segmenta-tion of a turn and tagging.
From several taggingexperiments, we showed that the system segmentedturns and assigned speech act type tags at high ac-curacy rates when using Japanese data.
Compara-tively lower performance was obtained using Englishdata, and we discussed the performance difference.We Mso examined the effect of parameters in the sta-tistical models on tagging performance.
We finallyshowed that the SA tags in this paper are useful intranslating positive responses that often appear intask-oriented dialogues uch as those in ours.AcknowledgmentThe authors would like to thank Mr. YasuoTanida for the excellent programming works and Dr.Seiichi Yamamoto for stimulus discussions.Re ferencesM.
Cettolo and D. Falavigna.
1998.
Automatic de-tection of semantic boundaries based on acousticand lexical knowledge.
In ICSLP '98, volume 4,pages 1551-1554.B.
J. Grosz and C. L. Sidner.
1986.
Atten-tion, intentions and the structure of discourse.Computational Linguistics, 12(3):175-204, July-September.J.
Hirschberg and C. H. Nakatani.
1996.
A prosodicanalysis of discourse segments in direction-givingmonologues.
In 34th Annual Meeting of the Asso-ciation for the Computational Linguistics, pages286-293.F.
Jelinek, 1997.
Statistical Methods for SpeechRecognition, chapter 10.
The MIT Press.D.
J. Litman and R. J. Passonneau.
1995.
Com-bining multiple knowledge sourses for discoursesegmentation.
I  33rd Annual Meeting of the As-sociation for the Computational Linguistics, pages108-115.T.
Morimoto, N. Uratani, T. Takezawa, O. Furuse,Y.
Sobashima, H. Iida, A. Nakamura, Y. Sagisaka,N.
Higuchi, and Y. Yamazaki.
1994.
A speech andlanguage database for speech translation research.In ICSLP '94, pages 1791-1794.M.
Nagata nd T. Morimoto.
1994.
An information-theoretic model of discourse for next utterancetype prediction.
Transactions of InformationProcessing Society of Japan, 35(6):1050-1061.M.
Nagata.
1994.
A stochastic Japanese morpholog-ical analyzer using a forward-DP and backward-A* N-best search algorithm.
In Proceedings ofColing94, pages 201-207.J.
R. Quinlan.
1993.
C~.5: Programs for MachineLearning.
Morgan Kaufmann.N.
Reithinger and E. Maier.
1995.
Utilizing statisti-cal dialogue act processing in verbmobil.
In 33rdAnnual Meeting of the Associations for Computa-tional Linguistics, pages 116-121.J.
R. Searle.
1969.
Speech Acts.
Cambridge Univer-sity Press.M.
Seligman, L. Fais, and M. Tomokiyo.
1994.A bilingual set of communicative act labels forspontaneous dialogues.
Technical Report TR-IT-0081, ATR-ITL.A.
Stolcke and E. Shriberg.
1996.
Automatic lin-guistic segmentation f conversational speech.
InICSLP '96, volume 2, pages 1005-1008.J.
Venditti and M. Swerts.
1996.
Intonational cuesto discourse structure in Japanese.
In ICSLP '96,volume 2, pages 725-728.388
