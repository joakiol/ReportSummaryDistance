Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 59?70, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsDetecting Subgroups in Online Discussions by Modeling Positive andNegative Relations among ParticipantsAhmed HassanMicrosoft ResearchRedmond, WAhassanam@microsoft.comAmjad Abu-JbaraUniversity of MichiganAnn Arbor, MIamjbara@umich.eduDragomir RadevUniversity of MichiganAnn Arbor, MIradev@umich.eduAbstractA mixture of positive (friendly) and nega-tive (antagonistic) relations exist among usersin most social media applications.
However,many such applications do not allow users toexplicitly express the polarity of their interac-tions.
As a result most research has either ig-nored negative links or was limited to the fewdomains where such relations are explicitlyexpressed (e.g.
Epinions trust/distrust).
Westudy text exchanged between users in onlinecommunities.
We find that the polarity of thelinks between users can be predicted with highaccuracy given the text they exchange.
Thisallows us to build a signed network represen-tation of discussions; where every edge hasa sign: positive to denote a friendly relation,or negative to denote an antagonistic relation.We also connect our analysis to social psy-chology theories of balance.
We show that theautomatically predicted networks are consis-tent with those theories.
Inspired by that, wepresent a technique for identifying subgroupsin discussions by partitioning singed networksrepresenting them.1 IntroductionMost online communities involve a mixture of pos-itive and negative relations between users.
Positiverelations may indicate friendship, agreement, or ap-proval.
Negative relations usually indicate antago-nism, opposition, or disagreement.Most of the research on relations in social mediaapplications has almost exclusively focused on pos-itive links between individuals (e.g.
friends, fans,followers, etc.).
We think that one of the main rea-sons, of why the interplay of positive and negativelinks did not receive enough attention, is the lack ofa notion for explicitly expressing negative interac-tions.
Recently, this problem has received increas-ing attention.
However, all studies have been limitedto a handful of datasets from applications that allowusers to explicitly label relations as either positive ornegative (e.g.
trust/distrust on Epinion (Leskovec etal., 2010b) and friends/foes on Slashdot (Kunegis etal., 2009)).Predicting positive/negative relations betweendiscussants is related to another well studied prob-lem, namely debate stance recognition.
The ob-jective of this problem is to identify which partic-ipants are supporting and which are opposing thetopic being discussed.
This line of work does notpay enough attention to the relations between par-ticipants, rather it focuses on participant?s stance to-ward the topic.
It also assumes that every partici-pant either supports or opposes the topic being dis-cussed.
This is a simplistic view that ignore thenature of complex topics that has many aspects in-volved which may result in more than two subgroupswith different opinions.In this work, we apply Natural Language Pro-cessing techniques to text correspondences ex-changed between individuals to identify the under-lying signed social structure in online communities.We present a method for identifying user attitudeand for automatically constructing a signed socialnetwork representation of discussions.
We applythe proposed methods to a large set of discussionposts.
We evaluate the performance using a manu-ally labeled dataset.
We also conduct a large scaleevaluation by showing that predicted links are con-sistent with the principals of social psychology the-ories, namely the Structural Balance Theory (Hei-der, 1946).
The balance theory has been shown tohold both theoretically (Heider, 1946) and empiri-cally (Leskovec et al2010c) for a variety of socialcommunity settings.
Finally, we present a methodfor identifying subgroups in online discussions byidentifying groups with high density of intra-grouppositive relations and high density of inter-groupnegative relations.
This method is capable of identi-fying subgroups even if the community splits intomore than two subgroups which is more generalthan stance recognition which assumes that only twogroups exist.59BCDA EF GI HPositive NegativeSource Target Sign Evidence from Text A E - I have to disagree with what you are saying.
G A - You are missing the entire point, he is putting lives at risk.
D I - and you manufacture lies for what reason?
E G + you have explained your position very well.
C H + I am neutral on this, but I agree with your assessment!Figure 1: An example showing a signed social networkalong with evidence from text that justifies edge signs.The input to our algorithm is a set of text corre-spondences exchanged between users (e.g.
posts orcomments).
The output is a signed network whereedges signify the existence of an interaction betweentwo users.
The resulting network has polarity asso-ciated with every edge.
Edge polarity is a means forindicating positive or negative affinity between twoindividuals.Figure 1 shows a signed network representationfor a subset of posts from a long discussion thread.The thread discussed the November 2010 Wikileakscable release.
We notice that participants split intotwo groups, one supporting and one opposing theleak.
We also notice that most negative edges arebetween groups, and most positive edges are withingroups.
It is worth mentioning that networks gen-erated from larger datasets (i.e.
with thousands ofposts) have much more noise compared to this ex-ample.The rest of the paper is structured as follows.
Insection 2, we review some of the related prior workon mining sentiment from text, mining online dis-cussions, extracting social networks from text, andanalyzing signed social networks.
We define ourproblem and explain our approach in Section 3.
Sec-tion 4 describes our dataset.
Results and discussionare presented in Section 5.
We present a method foridentifying subgroups in online discussions in Sec-tion 3.3.
We conclude in Section 6.2 Related WorkIn this section, we survey several lines of researchthat are related to our work.2.1 Mining Sentiment from TextOur general goal of mining attitude from one indi-vidual toward another makes our work related to ahuge body of work on sentiment analysis.
One suchline of research is the well-studied problem of iden-tifying the polarity of individual words (Hatzivas-siloglou and McKeown, 1997; Turney and Littman,2003; Kim and Hovy, 2004; Takamura et al2005).Subjectivity analysis is yet another research line thatis closely related to our general goal of mining at-titude.
The objective of subjectivity analysis is toidentify text that presents opinion as opposed to ob-jective text that presents factual information (Wiebe,2000; Hatzivassiloglou and Wiebe, 2000; Banea etal., 2008; Riloff and Wiebe, 2003).
Our work is dif-ferent from subjectivity analysis because we are notonly interested in discriminating between opinionsand facts.
Rather, we are interested in identifyingthe polarity of interactions between individuals.
Ourmethod is not restricted to phrases or words, rather itgeneralizes this to identifying the polarity of an in-teraction between two individuals based on severalposts they exchange.2.2 Stance ClassificationPerhaps the closest work to this paper is the work onstance classification.
We notice that most of thesemethods focus on the polarity of the written text as-suming that anyone using positive text belongs toone group and anyone using negative text belongsto another.
This works well for single-aspect topicsor entities like the ones used in (Tan et al2011)(e.g.
Obama, Sara Palin, Lakers, etc.).
In this sim-ple notion of topics, it is safe to assume that textpolarity is a good enough discriminator.
This unfor-tunately is not the case in online discussions aboutcomplex topics having many aspects (e.g.
abortion,health care, etc.).
In such complex topics, people usepositive and negative text targeting different aspectsof the topic, for example in the health care bill topic,discussants expressed their opinion regarding manyaspects including: the enlarged coverage, the insur-ance premiums, Obama, socialism, etc.
This showsthat simply looking at text polarity is not enough toidentify groups.Tan et al2011) studied how twitter following re-lations can be used to improve stance classification.Their main hypothesis is that connected users aremore likely to hold similar opinions.
This may becorrect for the twitter following relations, but it isnot necessarily correct for open discussions where60no such relations exist.
The only criterion that can beused to connect discussants is how often they replyto each other?s posts.
We will show later that whilemany people reply to people with similar opinions,many others reply to people with different opinionsas well.Thomas et al2006) address the same problemof determining support and opposition as applied tocongressional floor-debates.
They assess the agree-ment/disagreement between different speakers bytraining a text classifier and applying it to a win-dow surrounding the names of other speakers.
Theyconstruct their training data by assuming that if twospeaker have the same vote, then every referenceconnecting them is an agreement and vice versa.We believe this will result in a very noisy train-ing/testing set and hence we decided to recruit hu-man annotators to create a training set.
We foundout that many instances with references to otherdiscussants were labeled as neither agreement nordisagreement regardless of whether the discussantshave similar or opposing positions.
We will use thissystem as a baseline and will show that the exis-tence of positive/negative words close to a personname does not necessarily show agreement or dis-agreement with that person.Hassan et al2010) use a language model basedapproach for identifying agreement and disagree-ment sentences in discussions.
This work is limitedto sentences.
It does not consider the overall rela-tion between participants.
It also does not considersubgroup detection.
We will use this method as abaseline for one of our components and will showthat the proposed method outperforms it.Murakami and Raymond (2010) present anothermethod for stance recognition.
They use a smallnumber of hand crafted rules to identify agreementand disagreement interactions.
Hand crafted rulesusually result in systems with very low recall caus-ing them to miss many agreement/disagreement in-stances (they report 0.26 recall at the 0.56 preci-sion level).
We present a machine learning systemto solve this problem and achieve much better per-formance.
Park et al2011) propose a method forfinding news articles with different views on con-tentious issues.
Mohit et al2008) present a setof heuristics for including disagreement informa-tion in a minimum cut stance classification frame-work.
Galley et al2004) show the value of us-ing durational and structural features for identify-ing agreement and disagreement in spoken conver-sational speech.
They use features like duration ofspurts, speech rate, speaker overlap, etc.
which arenot applicable to written language.Our approach is different from agree-ment/disagreement identification because wenot only study sentiment at the local sentimentlevel but also at the global level that takes intoconsideration many posts exchanged betweenparticipants to build a signed network representationof the discussion.
Research on debate stancerecognition attempts to perform classification underthe ?supporting vs. opposing?
paradigm.
Howeversuch simple view might not always be accuratefor discussions on more complex topics withmany aspects.
After building the signed networkrepresentation of discussions, we present a methodthat can detect how the large group could split intomany subgroups (not necessarily two) with coherentopinions.2.3 Extracting Social Networks from TextLittle work has been done on the front of extractingsocial relations between individuals from text.
El-son et al2010) present a method for extracting so-cial networks from nineteenth-century British nov-els and serials.
They link two characters based onwhether they are in conversation or not.
McCal-lum et al2007) explored the use of structured datasuch as email headers for social network construc-tion.
Gruzd and Hyrthonthwaite (2008) explored theuse of post text in discussions to study interactionpatterns in e-learning communities.
Extracting so-cial power relations from natural language (i.e.
whoinfluences whom) has been studied in (Bramsen etal., 2011; Danescu-Niculescu-Mizil et al2011).Our work is related to this line of research becausewe employ natural language processing techniquesto reveal embedded social structures.
Despite sim-ilarities, our work is uniquely characterized by thefact that we extract signed social networks with bothpositive and negative links from text.2.4 Signed Social NetworksMost of the work on social networks analysis hasonly focused on positive interactions.
A few recentpapers have taken the signs of edges into account.Brzozowski et al2008) study the positive andnegative relationships between users of Essembly.Essembly is an ideological social network that dis-tinguishes between ideological allies and nemeses.Kunegis et al2009) analyze user relationships in61the Slashdot technology news site.
Slashdot allowsusers of the website to tag other users as friends orfoes, providing positive and negative endorsements.Leskovec et al2010b) study signed social networksgenerated from Slashdot, Epinions, and Wikipedia.They also connect their analysis to theories of signednetworks from social psychology.
A similar studyused the same datasets for predicting positive andnegative links given their context (Leskovec et al2010a).All this work has been limited to analyzing ahandful of datasets for which an explicit notion ofboth positive and negative relations exists.
Our workgoes beyond this limitation by leveraging the powerof natural language processing to automate the dis-covery of signed social networks using the text em-bedded in the network.The research presented in this paper extends thisprevious work in a number of ways: (i) we presenta method based on linguistic analysis that finds in-stances of showing positive or negative attitude be-tween participants (ii) we propose a technique forrepresenting discussions as signed networks where asign is associated with every edge to denote whetherthe relation is friendly or antagonistic (iii) we eval-uate the proposed methods using human annotateddata and also conduct a large scale evaluation basedon social psychology theories; (iv) finally we presenta method for identifying subgroups that globallysplits the community involved in the discussion byutilizing the dynamics of the local interactions be-tween participants.3 Approach3.1 Identifying Attitude from TextTo build a signed network representation of discus-sants, we start by trying to identify sentences thatshow positive or negative attitude from the writer tothe addressee.
The first step toward identifying at-titude is to identify words with positive/negative se-mantic orientation.
The semantic orientation or po-larity of a word indicates the direction the word devi-ates from the norm (Lehrer, 1974).
We use Opinion-Finder (Wilson et al2005a) to identify words withpositive or negative semantic orientation.
The polar-ity of a word is also affected by the context wherethe word appears.
For example, a positive word thatappears in a negated context should have a negativepolarity.
Other polarized words sometimes appear asneutral words in some contexts.
To identify contex-tual polarity of words, a large set of features is usedincluding words, sentences, structure, and other fea-tures similar to the method described in (Wilson etal., 2005b).Our overall objective is to find the direct attitudebetween participants.
Hence after identifying the se-mantic orientation of individual words, we move onto predicting which polarized expressions target theaddressee and which do not.Text polarity alone cannot be used to identify at-titude between participants.
Sentences that showan attitude are different from subjective sentences.Subjective sentences are sentences used to expressopinions, evaluations, and speculations (Riloff andWiebe, 2003).
While every sentence that shows anattitude is a subjective sentence, not every subjectivesentence shows an attitude toward the recipient.In this method, we address the problem of iden-tifying sentences with attitude as a relation detec-tion problem in a supervised learning setting.
Westudy sentences that has mentions to the addresseeand polarized expressions (negative/positive wordsor phrases).
Mentions could either be names of otherparticipants or second person pronouns (you, your,yours) used in text posted as a reply to another par-ticipant.
Reply structure (i.e.
who replies to whom)is readily available in many discussion forums.
Incases where reply structure is not available, we canuse a method like the one in (Lin et al2009) to re-cover it.We predict whether the mention is related to thepolarized expression or not.
We regard the mentionand the polarized expression as two entities and tryto learn a classifier that predicts whether the two en-tities are related or not.The text connecting the two entities offers a verycondensed representation of the information neededto assess whether they are related or not.
For ex-ample the two sentences ?you are completely un-qualified?
and ?you know what, he is unqualified ...?show two different ways the words ?you?, and ?un-qualified?
could appear in a sentence.
In the firstcase the polarized word ?unqualified?
refers to theword ?you?.
In the second case, the two words arenot related.
The information in the shortest pathbetween two entities in a dependency tree can beused to assert whether a relationship exists betweenthem (Bunescu and Mooney, 2005).The sequence of words connecting the two enti-ties is a very good predictor of whether they are re-lated or not.
However, these paths are completely62lexicalized and consequently their performance willbe limited by data sparseness.
To alleviate this prob-lem, we use higher levels of generalization to rep-resent the path connecting the two tokens.
Theserepresentations are the part-of-speech tags, and theshortest path in a dependency graph connecting thetwo tokens.
We represent every sentence with sev-eral representations at different levels of generaliza-tion.
For example, the sentence ?your ideas are veryinspiring?
will be represented using lexical, polar-ity, part-of-speech, and dependency information asfollows:LEX: ?YOUR ideas are very POS?POS: ?YOUR NNS VBP RB JJ POS?DEP: ?YOUR poss nsubj POS?The set of features we use are the set of unigrams,and bigrams representing the words, part-of-speechtags, and dependency relations connecting the twoentities.
For example the following features will beset for the previous example:YOUR ideas, YOUR NNS, YOUR poss,poss nsubj, ...., etc.We use Support Vector Machines (SVM) as alearning system because it is good with handlinghigh dimensional feature spaces.3.2 Extracting the Signed NetworkIn this subsection, we describe the procedure weused to build the signed network given the compo-nent we described in the previous subsection.
Thisprocedure consists of two main steps.
The first isbuilding the network without signs, and the secondis assigning signs to different edges.To build the network, we parse our data to identifydifferent threads, posts and senders.
Every sender isrepresented with a node in the network.
An edgeconnects two nodes if there exists an interaction be-tween the corresponding participants.
We add a di-rected edgeA?
B, ifA replies toB?s posts at leastn times in m different threads.
We set m, and n to2 in all of our experiments.
The interaction infor-mation (i.e.
who replies to whom) can be extracteddirectly from the thread structure.
Alternatively, asmentioned earlier, we can use a method similar tothe one presented in (Lin et al2009) to recover thereply structure if it is not readily available.Once we build the network, we move to the morechallenging task in which we associate a sign withParticipant FeaturesNumber of posts per month for A (B)Percentage of positive posts per month for A (B)Percentage of negative posts per month for A (B)genderInteraction FeaturesPercentage/number of positive (negative) sentences per postPercentage/number of positive (negative) posts per threadDiscussion Domain (e.g.
politics, science, etc.
)Table 1: Features used by the Interaction Sign Classifier.every edge.
We have shown in the previous sectionhow sentences with positive and negative attitudecan be extracted from text.
Unfortunately the signof an interaction cannot be trivially inferred from thepolarity of sentences.
For example, a single negativesentence written by A and directed to B does notmean that the interaction between A and B is neg-ative.
One way to solve this problem would be tocompare the number of negative sentences to posi-tive sentences in all posts betweenA andB and clas-sify the interaction according to the plurality value.We will show later, in our experiments section, thatsuch a simplistic method does not perform well inpredicting the sign of an interaction.As a result, we decided to pose the problem as aclassical supervised learning problem.
We came upwith a set of features that we think are good predic-tors of the interaction sign, and we trained a classi-fier using those features on a labeled dataset.
Ourfeatures include numbers and percentages of pos-itive/negative sentences per post, posts per thread,and so on.
A sentence is labeled as positive/negativeif a relation has been detected in this sentence be-tween a mention referring to the addressee and apositive/negative expression.
A post is consideredpositive/negative based on the majority of relationsdetected in it.
We use two sets of features.
The firstset is related to A only or B only.
The second setis related to the interactions between A and B. Thefeatures are summarized in Table 1.3.3 Sub-Group DetectionIn any discussion, different subgroups may emerge.Members of every subgroup usually have a commonfocus (positive or negative) toward the topic beingdiscussed.
Each member of a group is more likelyto show positive attitude to members of the samegroup, and negative attitude to members of opposinggroups.
The signed network representation couldprove to be very useful for identifying those sub-groups.
To detect subgroups in a discussion thread,63we would like to partition the corresponding signednetwork such that positive intra-group links and neg-ative inter-group links are dense.This problem is related to the constrained cluster-ing (Wagstaff et al2001) and the correlation clus-tering problem (Bansal et al2004).
In constrainedclustering, a pairwise similarity metric (which isnot available in our domain), and a set of must-link/cannot-link constraints are used with a standarddata clustering algorithm.
Correlation clustering op-erates in a scenario where given a signed graphG = (V,E) where the edge label indicates whethertwo nodes are similar (+) or different (-), the taskis to cluster the vertices so that similar objects aregrouped together.
Bansal et.
al (2004) proved NP-hardness and gave constant-factor approximation al-gorithms for the special case in which the graphis complete (full information) and every edge hasweight +1 or -1 which is not the case in our network.Alternatively, we can use a greedy optimization al-gorithm to find partitions.
A criterion function fora local optimization partitioning procedure is con-structed such that positive links are dense withingroups and negative links are dense between groups.For any potential partition C, we seek to optimizethe following function: P (C) = ?
?n +(1??
)?pwhere?n is the number of negative links betweennodes in the same subgroup,?p is the number ofpositive links between nodes in different subgroups,and ?
is a trade factor that represents the importanceof the two terms.
We set ?
to 0.5 in all our experi-ments.Clusters are selected such that: C?
=argminP (C).
A greedy optimization frameworkis used to minimize P (C).
Initially, nodes are ran-domly partitioned into t different clusters and thecriterion function P is evaluated for that cluster.
Ev-ery cluster has a set of neighbors in the cluster space.A neighbor cluster is obtained by moving one nodefrom one cluster to another, or by exchanging twonodes in two different clusters.
Neighbor partitionsare evaluated, and if one with a lower value for thecriterion function is found, it is set as the currentpartition.
This greedy procedure is repeated withrandom restarts until a minimal solution is found.To determine the number of subgroups t, we selectt that minimizes the optimization function P (C).
Inall experiments we used an upper limit of t = 5.This technique was able to identify the correct num-ber of subgroups in 77% of the times.
In the rest ofthe cases, the number was different from the correctnumber by at most 1 except for a single case whereit was 2.4 Data4.1 Signed Network ExtractionOur data consists of a large amount of discussionthreads collected from online discussion forums.
Wecollected around 41, 000 topics (threads) and 1.2Mposts from the period between the end of 2008 andthe end of 2010.
All threads were in English and had5 posts or more.
They covered 11 different domainsincluding: politics, religion, science, etc.
The aver-age number of participants per domain is 1320 andper topic is 52.
The data was tokenized, sentence-split, and part-of-speech tagged with the OpenNLPtoolkit.
It was parsed with the Stanford parser (Kleinand Manning, 2003).We randomly selected around 5300 posts (1000interactions), and asked human annotators to labelthem.
Our annotators were instructed to read all theposts exchanged between two participants and de-cide whether the interaction between them is posi-tive or negative.
We used Amazon Mechanical Turkfor annotations.
Following previous work (Callison-Burch, 2009; Akkaya et al2010), we took sev-eral precautions to maintain data integrity.
We re-stricted annotators to those based in the US to main-tain an acceptable level of English fluency.
We alsorestricted annotators to those who have more than95% approval rate for all previous work.
Moreover,we asked three different annotators to label every in-teraction.
The label was computed by taking the ma-jority vote among the three annotators.
We refer tothis data as the Interactions Dataset.We ran a different annotation task where we se-lected sentences including mentions referring to dis-cussants (names or pronouns) and polarized expres-sions.
Annotators were asked to select sentenceswhere the polarized attribute is referring to the men-tion and hence show a positive or negative attitudetoward other discussion participants.
This resultedin a set of 5000 manually annotated sentences.
Werefer to this data as the Sentences Dataset.We asked three different annotators to label ev-ery instance.
The kappa measure between the threegroups of annotations was 0.62 for the InteractionsDataset and 0.64 for the Sentences Dataset.
To bet-ter assess the quality of the annotations, we asked atrained annotator to label 10% of the data.
We mea-sured the agreement between the expert annotator64Logistic Reg.Class Pos.
Neg.
Weigh.
Avg.Precision 0.848 0.724 0.809Recall 0.884 0.657 0.812F-Measure 0.866 0.689 0.81Accuracy - - 0.812SVMPrecision 0.906 0.71 0.844Recall 0.847 0.809 0.835F-Measure 0.875 0.756 0.838Accuracy - - 0.835Table 2: Interaction sign classifier performance.Classifier Random Thresh-Num Thresh-Perc.
SVMAccuracy 65% 69% 71% 83.5%Table 3: A comparison of different sign interaction clas-sifiers.and the majority label from Mechanical Turk.
Thekappa measure was 0.69 for the Interactions Datasetand 0.67 for the Sentences Dataset.4.2 Sub-group DetectionWe used a dataset of more than 42 topics and ap-proximately 9000 posts collected from two politicalforums (Createdebate1 and Politicalforum2).
The fo-rum administrators ran a poll asking participants toselect their stance from a set of possible answersand hence the dataset was self-labeled with respectto groups.
We also used a set of discussions fromthe Wikipedia discussion section.
When a topic onWikipedia is disputed, the editors of that topic start adiscussion about it.
We collected 117 Wikipedia dis-cussion threads.
The threads contain a total of 1,867posts.
The discussions were annotated by an expertannotator (a professor in sociolinguistics, not an au-thor of the paper) who was instructed to read eachof the Wikipedia discussion threads in its entiretyand determine whether the discussants split into sub-groups, in which case he was asked to identify thesubgroup membership for each discussant.
In to-tal, we had 159 topics with an average of approxi-mately 500 posts, 60 participants and 2.7 subgroupsper topic.
Examples of the topics include: Arizonaimmigration law, airport security, oil spill, evolution,Ireland partitions, abortion and many others.5 Results and DiscussionWe performed experiments on the data describedin the previous section.
We trained and tested thesentence with the attitude detection classifiers de-scribed in Section 3.1 using the Sentences Dataset.1www.createdebate.com2www.politicalforum.comWe also trained and tested the interaction sign clas-sifier described in Section 3.2 using the InteractionsDataset.
We built one signed social network for ev-ery domain (e.g.
politics, economics, etc.).
We de-cided to build a network for every domain as op-posed to one single network because the relation be-tween any two individuals may vary across domains(e.g.
politics vs. science).
In the rest of this section,we will describe the experiments we did to assess theperformance of the sentences with attitude detectionand interaction sign prediction steps.In addition to classical evaluation, we evaluateour results using the structural balance theory whichhas been shown to hold both theoretically (Heider,1946) and empirically (Leskovec et al2010c).
Wevalidate our results by showing that the automati-cally extracted networks mostly agree with the the-ory.
We evaluated the approach using the structuralbalance theory because it presents a global (pertain-ing to relations between multiple edges) and large-scale (used millions of posts and thousands of users)evaluation of the results as opposed to traditionalevaluation which is local in nature (only considersone edge at a time) and smaller in scale (used thou-sands of posts).5.1 Identifying Sentences with AttitudeWe compare the proposed methods to two baselines.The first baseline is based on the work of (Thomaset al2006).
We used the speaker agreement com-ponent presented in (Thomas et al2006) as a base-line.
The speaker agreement component is one stepin their approach.
In this component, they usedan SVM classifier trained using a window of textsurrounding references to other speakers to predictagreement/disagreement between speakers.We build an SVM text classifier trained on thesentence at which the mention referring to the otherparticipant occurred.
We refer to this baseline asthe Text Classification approach.
The second base-lines adopts the language model approach presentedin (Hassan et al2010).
Two language modelsare trained using a stream of words, part-of-speechtags, and dependency relations, one for sentencesthat show an attitude and one for sentences that donot.
New sentences are classified based on gener-ation likelihoods.
We refer to this baseline as theLanguage Models approach.We tested this component using the SentencesDataset described in Section 4.
We compared theperformance of the proposed method and the two65Extracted Networks Random NetworksDomain (+++) (++?)
(+??)
(???)
(+++) (++?)
(+??)
(???
)abortion 51.67 26.31 18.92 0.48 35.39 43.92 18.16 2.52current-events 67.36 22.26 8.76 0.23 54.08 36.90 8.39 0.64off-topic-chat 65.28 23.54 9.45 0.25 58.07 34.59 6.88 0.46economics 72.68 18.30 7.77 0.00 66.50 29.09 4.22 0.20political opinions 60.60 24.24 12.81 0.43 45.97 40.79 12.06 1.19environment 47.46 32.54 17.26 0.30 37.38 43.61 16.89 2.12latest world news 58.29 22.41 16.33 0.62 42.26 42.20 13.98 1.56religion 47.17 25.89 22.56 1.42 39.68 42.94 15.51 1.87science-technology 57.53 26.03 14.33 0.00 50.14 38.93 10.05 0.87terrorism 64.96 23.36 9.46 0.73 41.54 42.42 14.36 1.68Table 4: Percentage of different types of triangles in the extracted networks vs. the random networks.Method Accuracy Precision Recall F1Text Classification 60.4 61.1 60.2 60.6Language Models 80.3 81.0 79.4 80.2Relation Extraction 82.3 82.3 82.3 82.3Table 5: Comparison of attitude identification methods.baselines.
Table 5 compares the precision, recall,F1, and accuracy for the three methods.
The textclassification based approach does much worse thanothers.
The reasons is that it ignores the structureand uses much less information (part-of-speech tagsand dependency trees are not used) compared to theother methods.
Additionally, the short length of thesentences compared to what is typical in text clas-sification may have had a bad effect on the perfor-mance.
Both other models try to learn the char-acteristics of the path connecting the mention andthe polarized expression.
We notice that optimizingthe weights for unigram and bigrams features usingSVM results in a better performance compared tolanguage models because it does not have the con-straints imposed by the former model on the learnedweights.We evaluated the importance of the feature types(i.e.
dependency vs. pos tags vs words) by measur-ing the chi-squared statistic for every feature withrespect to the class.
Dependency features were mosthelpful, but other types of features helped improvethe performance as well.5.2 Interaction Sign ClassifierWe used the relation detection classifier described inSection 3.1 to find sentences with positive and nega-tive attitude.
The output of this classifier was used tocompute the features described in Section 3.2, whichwere used to train a classifier that predicts the signof an interaction between any two individuals.We used both Support Vector Machines (SVM)and logistic regression to train the sign interactionclassifier.
We report several performance metrics forthem in Table 2.
We notice that the SVM classifierperforms better with an accuracy of 83.5% and anF-measure of 81%.
All results were computed using10 fold cross validation on the labeled data.
To bet-ter assess the performance of the proposed classifier,we compare it to a baseline that labels the relation asnegative if the percentage of negative sentences ex-ceeds a particular threshold, otherwise it is labeledas positive.
The thresholds were empirically esti-mated using a separate development set.
The accu-racy of this baseline is only 71%.To better assess the performance of the proposedclassifier, we compare it to three baselines.
The firstis a random baseline that predicts an interaction aspositive with probability p that equals the proportionof positive instances to all instances in the trainingset.
The second classifier (Thresh-Num) labels theedge as negative if the number of negative instancesexceeds a threshold Tn.
The third classifier (Thresh-Perc) labels the edge as negative if the percentage ofnegative instances to all instances exceeds a thresh-old Tp.
The cutoff thresholds were estimated usinga separate development set.The 3 baselines were tested using the entire la-beled dataset.
The SVM classifier was tested using10 fold cross validation.
The accuracy of the ran-dom classifier, the two based on a cut off numberand percentage , and the SVM classifier are shownin Table 3.
We notice that the random classifier per-forms worst, and the classifier based on percentagecutoff outperforms the one based on number cut-off.
The SVM classifier significantly outperforms allother classifiers.
We tried to train a classifier usingboth the number and percentage of negative and pos-itive posts.
The improvement over using the baselineusing the percentage of negative posts was not sta-tistically significant.We evaluated the importance of the features listed66in Table 1 by measuring the chi-squared statistic forevery feature with respect to the class.
We foundout that the features describing the interaction be-tween the two participants are more informative thanthe ones describing individuals characteristics.
Thelater features are still helpful though and they im-prove the performance by a statistically significantamount.
We also noticed that all features based onpercentages are more informative than those basedon counts.
The most informative features are: per-centage of negative posts per tread, percentage ofnegative sentences per post, percentage of positiveposts per thread, number of negative posts, and dis-cussion domain.5.3 Structural Balance TheoryThe structural balance theory is a psychological the-ory that tries to explain the dynamics of signed so-cial interactions.
It has been shown to hold both the-oretically (Heider, 1946) and empirically (Leskovecet al2010c).
In this section, we study the agree-ment between the theory and our automatically ex-tracted networks.
The theory has its origins in thework of Heider (1946).
It was then formalized ina graph theoretic form by (Cartwright and Harary,1956).
The theory is based on the principles that ?thefriend of my friend is my friend?, ?the enemy of myfriend is my enemy?, ?the friend of my enemy ismy enemy?, and variations on these.
The structuralbalance theory states that triangles that have an oddnumber of positive signs (+ + + and + - -) are bal-anced, while triangles that have an even number ofpositive signs (- - - and + + -) are not.In this section, we compare the predictions ofedge signs made by our system to the structural bal-ance theory by counting the frequencies of differ-ent types of triangles in the predicted network.
Ta-ble 4 shows the frequency of every type of trian-gle for 10 different domains.
To better understandthese numbers, we compare them to the frequenciesof triangles in a set of random networks.
We shuf-fle the signs for all edges on every network keepingthe fractions of positive and negative edges constant.We repeat shuffling for 1000 times and report the av-erage.We find that the all-positive triangle (+ + +) isoverrepresented in the generated network comparedto chance across all domains.
We also see that thetriangle with two positive edges (+ + ?
), and theall-negative triangle (?
?
?)
are underrepresentedcompared to chance across all domains.
The tri-angle with a single positive edge is slightly over-represented in most but not all of the topics com-pared to chance.
This shows that the predicted net-works mostly agree with the structural balance the-ory.
The slightly non standard behavior of the tri-angle with one positive edge could be explained inlight of the weak balance theory.
In this theory,Davis (1967) states that this triangle, which corre-sponds to the ?enemy of enemy is my friend?
propo-sition, holds only if the network can be partitionedinto exactly two subsets, but not when there are morethan two.
In general, the percentage of balanced tri-angles in the predicted networks is higher than inthe shuffled networks, and hence the balanced trian-gles are significantly overrepresented compared tochance showing that our automatically constructednetwork is similar to explicit signed networks in thatthey both mostly agree with the balance theory.5.4 Sub-Group DetectionWe compare the performance of the sub-group de-tection method to three baselines.
The first base-line uses graph clustering (GC) to partition a net-work based on the frequency of interaction betweenparticipants.
We build a graph where each noderepresents a participant.
Edges link participants ifthey exchange posts, and edge weights are based onthe number of posts exchanged.
The second base-line (TC) is based on the premise that participantswith similar text are more likely to belong to thesame subgroup.
We measure text similarity by com-puting the cosine similarity between the tf-idf rep-resentations of the text in a high dimensional vec-tor space.
We tried two methods for partitioningthose graphs: spectral partitioning (Luxburg, 2007)and a hierarchical agglomeration algorithm whichworks by greedily optimizing the modularity forgraphs (Clauset et al2004).
The third baseline isbased on stance classification approaches (e.g.
(Tanet al2011)).
In this baseline we put all the partic-ipants who use more positive text in one subgroupand the participants who use more negative text inanother subgroup.
Text polarity is identified usingthe method described in Section 3.1.Table 6 shows the average purity (Purity), entropy(Entropy), Normalizes Mutual Information (NMI),and Rand Index (RandIndex) values of the methodbased on signed networks and the baselines usingdifferent partitioning algorithms.
The differences inthe results shown in the table are statistically sig-nificant at the 0.05 level (as indicated by a 2-tailed67Figure 2: A signed network representing participants in a discussion about the ?Health Care Reform Bill?.
Blue (dark)nodes represent participants with the bill, Yellow (light) nodes represent participants against the bill, red (solid) edgesrepresent negative attitude, while green (dashed) edges represent positive attitude.Createdebate Politicalforum WikipediaMethod Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex Purity Entropy NMI RandIndexGC - Spectral 0.50 0.85 0.28 0.40 0.50 0.88 0.27 0.39 0.49 0.89 0.33 0.35GC - Hierarchical 0.48 0.86 0.30 0.41 0.47 0.89 0.31 0.40 0.49 0.87 0.38 0.39TC - Spectral 0.50 0.85 0.31 0.43 0.48 0.90 0.30 0.45 0.51 0.87 0.40 0.46TC - Hierarchical 0.49 0.90 0.35 0.46 0.48 0.91 0.33 0.49 0.53 0.80 0.40 0.49Text Polarity 0.55 0.80 0.38 0.49 0.54 0.91 0.31 0.38 0.34 0.95 0.30 0.40Signed Networks 0.64 0.74 0.46 0.59 0.58 0.80 0.43 0.55 0.65 0.54 0.51 0.60Table 6: Comparison of the sub-group detection method to baseline systemspaired t-test).We notice that partitioning the signed networkthat was automatically extracted from text results insignificantly better partitions on the three datasets asindicated by the higher Purity, NMI, and RandIndexand the lower Entropy values it achieves.
We believethat the first two baselines performed poorly becausethe interaction frequency and the text similarity arenot key factors in identifying subgroup structures.Many people would respond to people they disagreewith more, while others would mainly respond topeople they agree with most of the time.
Also, peo-ple in opposing subgroups tend to use very similartext when discussing the same topic and hence textclustering does not work as well.
The baseline thatclassifies the stance of discussants based on the po-larity of their text performed bad too because it over-looks the fact that most of the discussed topics in ourdatasets have multiple aspects and a discussant mayuse both positive and negative text targeting differ-ent aspects of the topic.
An example of a signed net-work and the corresponding subgtoups as extractedfrom real data is showm in Figure 2.6 ConclusionsIn this paper, we have shown that natural languageprocessing techniques can be reliably used to extractsigned social networks from text correspondences.We believe that this work brings us closer to un-derstanding the relation between language use andsocial interactions and opens the door to further re-search efforts that go beyond standard social net-work analysis by studying the interplay of positiveand negative connections.
We rigorously evaluatedthe proposed methods on labeled data and connectedour analysis to social psychology theories to showthat our predictions mostly agree with them.
Finally,we presented potential applications that benefit fromthe automatically extracted signed network.AcknowledgmentsThis research was funded in part by the Office of theDirector of National Intelligence, Intelligence Ad-vanced Research Projects Activity.
All statementsof fact, opinion or conclusions contained herein arethose of the authors and should not be construed asrepresenting the official views or policies of IARPA,the ODNI or the U.S. Government68ReferencesCem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon mechanical turk forsubjectivity word sense disambiguation.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechani-cal Turk, CSLDAMT ?10, pages 195?203.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.
InLREC?08.Nikhil Bansal, Avrim Blum, and Shuchi Chawla.
2004.Correlation Clustering.
Machine Learning, 56(1):89?113.Mohit Bansal, Claire Cardie, and Lillian Lee.
2008.The power of negative thinking: Exploiting label dis-agreement in the min-cut classification framework.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters.Philip Bramsen, Martha Escobar-Molano, Ami Patel, andRafael Alonso.
2011.
Extracting social power rela-tionships from natural language.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies -Volume 1, pages 773?782.Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.2008.
Friends and foes: ideological social network-ing.
In Proceeding of the twenty-sixth annual SIGCHIconference on Human factors in computing systems,pages 817?820, New York, NY, USA.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing, HLT ?05, pages 724?731,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Chris Callison-Burch.
2009.
Fast, cheap, and creative:evaluating translation quality using amazon?s mechan-ical turk.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing:Volume 1 - Volume 1, EMNLP ?09, pages 286?295.Dorwin Cartwright and Frank Harary.
1956.
Structurebalance: A generalization of heiders theory.
Psych.Rev., 63.Aaron Clauset, Mark E. J. Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Phys.
Rev.
E, 70:066111.Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,and Jon M. Kleinberg.
2011.
Echoes of power: Lan-guage effects and power differences in social interac-tion.
CoRR.J.
A. Davis.
1967.
Clustering and structural balance ingraphs.
Human Relations, 20:181?187.David Elson, Nicholas Dames, and Kathleen McKeown.2010.
Extracting social networks from literary fiction.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 138?147,Uppsala, Sweden, July.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech: useof bayesian networks to model pragmatic dependen-cies.
In Proceedings of the 42nd Annual Meeting onAssociation for Computational Linguistics, ACL ?04,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Anatoliy Gruzd and Caroline Haythornthwaite.
2008.Automated discovery and analysis of social networksfrom threaded discussions.
In Proceedings of the In-ternational Network of Social Network Analysis (IN-SNA), St. Pete Beach, Florida.Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.2010.
What?s with the attitude?
: identifying sentenceswith attitude in online discussions.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1245?1255.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In EACL?97, pages 174?181.Vasileios Hatzivassiloglou and Janyce Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sen-tence subjectivity.
In COLING, pages 299?305.Fritz Heider.
1946.
Attitudes and cognitive organization.Journal of Psychology, 21:107?112.Soo-Min Kim and Eduard Hovy.
2004.
Determining thesentiment of opinions.
In COLING, pages 1367?1373.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL?03, pages 423?430.Je?ro?me Kunegis, Andreas Lommatzsch, and ChristianBauckhage.
2009.
The slashdot zoo: mining a so-cial network with negative edges.
In Proceedings ofthe 18th international conference on World wide web,pages 741?750, New York, NY, USA.Adrienne Lehrer.
1974.
Semantic fields and lezical struc-ture.
North Holland, Amsterdam and New York.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010a.
Predicting positive and negative links in onlinesocial networks.
In Proceedings of the 19th interna-tional conference on World wide web, pages 641?650,New York, NY, USA.Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010b.
Signed networks in social media.
In Proceed-ings of the 28th international conference on Humanfactors in computing systems, pages 1361?1370, NewYork, NY, USA.69Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.2010c.
Signed networks in social media.
In CHI 2010,pages 1361?1370, New York, NY, USA.
ACM.Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,and Wei Wang.
2009.
Simultaneously modeling se-mantics and structure of threaded discussions: a sparsecoding approach and its applications.
In SIGIR ?09,pages 131?138.Ulrike Luxburg.
2007.
A tutorial on spectral clustering.Statistics and Computing, 17:395?416, December.Andrew McCallum, Xuerui Wang, and Andre?s Corrada-Emmanuel.
2007.
Topic and role discovery in so-cial networks with experiments on enron and academicemail.
J. Artif.
Int.
Res., 30:249?272, October.Akiko Murakami and Rudy Raymond.
2010.
Support oroppose?
: classifying positions in online debates fromreply activities and opinion expressions.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics: Posters, pages 869?875.Souneil Park, KyungSoon Lee, and Junehwa Song.
2011.Contrasting opposing views of news articles on con-tentious issues.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, pages340?349.Ellen Riloff and Janyce Wiebe.
2003.
Learningextraction patterns for subjective expressions.
InEMNLP?03, pages 105?112.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientations of words usingspin model.
In ACL?05, pages 133?140.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedingsof the 17th ACM SIGKDD international conferenceon Knowledge discovery and data mining, KDD ?11,pages 1397?1405.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Get outthe vote: Determining support or opposition from con-gressional floor-debate transcripts.
In In Proceedingsof EMNLP, pages 327?335.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on InformationSystems, 21:315?346.Kiri Wagstaff, Claire Cardie, Seth Rogers, and StefanSchro?dl.
2001.
Constrained k-means clustering withbackground knowledge.
In Proceedings of the Eigh-teenth International Conference on Machine Learning,pages 577?584.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of the SeventeenthNational Conference on Artificial Intelligence andTwelfth Conference on Innovative Applications of Ar-tificial Intelligence, pages 735?740.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patward-han.
2005a.
Opinionfinder: a system for subjectiv-ity analysis.
In Proceedings of HLT/EMNLP on Inter-active Demonstrations, HLT-Demo ?05, pages 34?35,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP?05, Vancou-ver, Canada.Bo Yang, William Cheung, and Jiming Liu.
2007.
Com-munity mining from signed social networks.
IEEETrans.
on Knowl.
and Data Eng., 19(10):1333?1348.70
