Using Corpus Statistics and WordNetRelations for Sense IdentificationClaudia Leacock*Educational Testing ServiceGeorge A. MillersPrinceton UniversityMartin Chodorow tHunter College of CUNYCorpus-based approaches to word sense identification have flexibility and generality but sufferfrom a knowledge acquisition bottleneck.
We show how knowledge-based techniques can be used toopen the bottleneck by automatically ocating training corpora.
We describe astatistical classifierthat combines topical context with local cues to ident~y a word sense.
The classifier is used todisambiguate a noun, a verb, and an adjective.
A knowledge base in the form of WordNet's lexicalrelations is used to automatically locate training examples in a general text corpus.
Test resultsare compared with those from manually tagged training examples.1.
IntroductionAn impressive array of statistical methods have been developed for word sense identi-fication.
They range from dictionary-based approaches that rely on definitions (V~ronisand Ide 1990; Wilks et al 1993) to corpus-based approaches that use only word co-occurrence frequencies extracted from large textual corpora (Sch~itze 1995; Dagan andItai 1994).
We have drawn on these two traditions, using corpus-based co-occurrenceand the lexical knowledge base that is embodied in the WordNet lexicon.The two traditions complement each other.
Corpus-based approaches have theadvantage of being generally applicable to new texts, domains, and corpora withoutneeding costly and perhaps error-prone parsing or semantic analysis.
They require onlytraining corpora in which the sense distinctions have been marked, but therein liestheir weakness.
Obtaining training materials for statistical methods is costly and time-consuming--it s a "knowledge acquisition bottleneck" (Gale, Church, and Yarowsky1992a).
To open this bottleneck, we use WordNet's lexical relations to locate unsuper-vised training examples.Section 2 describes a statistical classifier, TLC (Topical/Local Classifier), that usestopical context (the open-class words that co-occur with a particular sense), local con-text (the open- and closed-class items that occur within a small window around aword), or a combination of the two.
The results of combining the two types of contextto disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented.
Thefollowing questions are discussed: When is topical context superior to local context(and vice versa)?
Is their combination superior to either type alone?
Do the answers tothese questions depend on the size of the training?
Do they depend on the syntacticcategory of the target??
Division of Cognitive and Instructional Science, Princeton, NJ 08541; e-mail: cleacock@ets.org.
Thework reported here was done while the author was at Princeton University.t Department of Psychology, 695 Park Avenue, New York, NY 10021; e-mail: mschc@cunyvm.cuny.eduCognitive Science Laboratory, 221 Nassau Street, Princeton, NJ 08542; e-mail: geo@clarity.princeton.eduQ 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 1Manually tagged training materials were used in the development of TLC andthe experiments in Section 2.
The Cognitive Science Laboratory at Princeton Univer-sity, with support from NSF-ARPA, is producing textual corpora that can be used indeveloping and evaluating automatic methods for disambiguation.
Examples of thedifferent meanings of one thousand common, polysemous, open-class English wordsare being manually tagged.
The results of this effort will be a useful resource for train-ing statistical classifiers, but what about the next thousand polysemous words, andthe next?
In order to identify senses of these words, it will be necessary to learn howto harvest raining examples automatically.Section 3 describes WordNet's lexical relations and the role that monosemous"relatives" of polysemous words can play in creating unsupervised training materials.TLC is trained with automatically extracted examples, its performance is comparedwith that obtained from manually tagged training materials.2.
Corpus-based Statistical Sense IdentificationWork on automatic sense identification from the 1950s onward has been well summa-rized by Hirst (1987) and Dagan and Itai (1994).
The discussion below is limited towork that is closely related to our research.2.1 Some Recent WorkHearst (1991) represents local context with a shallow syntactic parse in which thecontext is segmented into prepositional phrases, noun phrases, and verb groups.
Thetarget noun is coded for the word it modifies, the word that modifies it, and theprepositions that precede and follow it.
Open-class items within ?3 phrase segmentsof the target are coded in terms of their relation to the target (modifier or head)or their role in a construct that is adjacent to the target.
Evidence is combined in amanner similar to that used by the local classifier component of TLC.
With supervisedtraining of up to 70 sentences per sense, performance on three homographs was quitegood (88-100% correct); with fewer training examples and semantically related senses,performance on two additional words was less satisfactory (73-77% correct).Gale, Church, and Yarowsky (1992a) developed a topical classifier based onBayesian decision theory.
The only information the classifier uses is an unorderedlist of words that co-occur with the target in training examples.
No other cues, suchas part-of-speech tags or word order, are used.
Leacock, Towell, and Voorhees (1993)compared this Bayesian classifier with a content vector classifier as used in informationretrieval and a neural network with backpropagation.
The classifiers were comparedusing different numbers of senses (two, three, or six manually tagged senses of line)and different amounts of training material (50, 100, and 200 examples).
On the six-sense task, the classifiers averaged 74% correct answers.
Leacock, Towell, and Voorhees(1993) found that the response patterns of the three classifiers converged, suggestingthat each of the classifiers was extracting as much data as is available in purely top-ical approaches that look only at word counts from training examples.
If this is thecase, any technique that uses only topical information will not be significantly moreaccurate than the three classifiers tested.Leacock, Towell, and Voorhees (1996) showed that performance of the contentvector topical classifier could be improved with the addition of local templates--specific word patterns that were recognized as being indicative of a particular sense--in an extension of an idea initially suggested by Weiss (1973).
Although the templatesproved to be highly reliable when they occurred, all too often, none were found.Yarowsky (1993) also found that template-like structures are very powerful indi-148Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relationscators of sense.
He located collocations by looking at adjacent words or at the firstword to the left or right in a given part of speech and found that, with binary ambi-guity, a word has only one sense in a given collocation with a probability of 90-99%.
1However, he had an average of only 29% recall (i.e., the collocations were found inonly 29% of the cases).
When local information occurred it was highly reliable, but alltoo often, it did not occur.Bruce and Wiebe (1994a, 1994b) have developed a classifier that represents localcontext by morphology (the inflection on the target word), the syntactic ategory ofwords within a window of ?2 words from the target, and collocation-specific itemsfound in the sentence.
The collocation-specific items are those determined to be themost informative, where an item is considered informative if the model for indepen-dence between it and a sense tag provided a poor fit to the training data.
The relativeprobabilities of senses, available from the training corpus, are used in the decisionprocess as prior probabilities.
For each test example, the evidence in its local contextis combined in a Bayesian-type model of the probability of each sense, and the mostprobable sense is selected.
Performance ranges from 77-84% correct on the test words,where a lower bound for performance based on always selecting the most frequentsense for the same words (i.e., the sense with the greatest prior probability) wouldyield 53-80% correct.Yarowsky (1994), building on his earlier work, designed a classifier that looks atwords within :kk positions from the target; lemma forms are obtained through mor-phological analysis; and a coarse part-of-speech assignment is performed by dictionarylookup.
Context is represented by collocations based on words or parts of speech atspecific positions within the window or, less specifically, in any position.
Also codedare some special classes of words, such as WEEKDAY, that might serve to distinguishamong word senses.
For each type of local-context evidence found in the corpus, alog-likelihood ratio is constructed, indicating the strength of the evidence for one formof the homograph versus the other.
These ratios are then arranged in a sorted decisionlist with the largest values (strongest evidence) first.
A decision is made for a testsentence by scanning down the decision list until a match is found.
Thus, only thesingle best piece of evidence is used.
The classifier was tested on disambiguating thehomographs that result from accent removal in Spanish and French (e.g., seria, serfa).In tests with the number of training examples ranging from a few hundred to severalthousand, overall accuracy was high, above 90%.Clearly, sense identification is an active area of research, and considerable ingenu-ity is apparent.
But despite the promising results reported in this literature, the realityis that there still are no large-scale, operational systems for tagging the senses of wordsin text.2.2 Topical/Local Classifier (TLC)The statistical classifier, TLC, uses topical context, local context, or a combination of thetwo, for word sense identification.
TLC's flexibility in using both forms is an importantasset for our investigations.A noun, a verb, and an adjective were tested in this study.
Table 1 provides asynonym or brief gloss for each of the senses used.
Training corpora and testingcorpora were collected as follows:1.
Examples for serve, hard, and line, in base or inflected form, were located inon-line corpora.
Examples containing line and serve were taken from the 1987-89 LDC1 Yarowsky does not use the idiomatic or noncompositional sense of collocation.
Instead, he meansco-occurrence of any words.149Computational Linguistics Volume 24, Number 1Table 1Word senses used in the experiment and their relative frequencies.serve (verb) hard (adj) line (noun)supply with food .41 not easy (difficult) .80 product .54hold an office .29 not soft (metaphoric) .12 phone .10function as something .20 not soft (physical) .08 text .10provide a service .10 cord .09division .09formation .08Wall Street Journal corpus and from the American Printing House for the Blind corpus.
2Examples for hard were taken from the LDC San Jose Mercury News (SJM) corpus.
Eachconsisted of the sentence containing the target and one sentence preceding it.
Theresulting strings had an average length of 49 items.2.
Examples where the target was the head of an unambiguous collocation wereremoved from the files.
Being unambiguous, they do not need to be disambiguated.These collocations, for example, product line and hard candy were found using WordNet.In Section 3, we consider how they can be used for unsupervised training.
Exampleswhere the target was part of a proper noun were also removed; for example, Japan AirLines was not taken as an example of line.3.
Each occurrence of the target word was manually tagged with a WordNet senseuntil a large number of examples was obtained for six senses of line, four sensesof serve, and three senses of hard.
In the process of collecting and manually taggingexamples, it was possible to determine the relative frequencies of the senses of eachword.
The less frequent senses, which do not appear in Table 1, occurred too rarelyfor us to collect the minimum number of examples needed to perform the experimentdescribed in the next section.4.
Three sets of materials were prepared by partitioning the examples for eachsense into training and test sets.
3The size of the training set was varied by taking thefirst 25, 50, 100, and 200 examples of the least frequent sense, and examples from theother senses in numbers that reflected their relative frequencies in the corpus.
As anillustration, in the smallest raining set for hard, there were 25 examples of the leastfrequent sense, 37 examples of the second most frequent sense, and 256 examples ofthe most frequent sense.
The test sets were of fixed size: each contained 150 of theleast frequent sense and examples of the other senses in numbers that reflected theirrelative frequencies.The operation of TLC consists of preprocessing, training, and testing.
During pre-processing, examples are tagged with a part-of-speech tagger (Brill 1994); special tagsare inserted at sentence breaks; and each open-class word found in WordNet is re-placed with its base form.
This step normalizes across morphological variants without2 This 25-million-word corpus is archived at IBM's T. J. Watson Research Center; it consists of stories andarticles from books and general circulation magazines.3 When the examples were collected from the corpus, it was often the case that more than one wasextracted from a given document.
To prevent he classifier from being trained and then tested onsentences from the same text, care was taken to insure that the training and test materials wereseparate.
Much of the corpus consisted of newspapers and periodicals, where it is common practice torepeat or paraphrase the same story on successive days.
To minimize possible overlap due to repeatedstories, the temporal order of the documents was preserved, and the test set was selected in such away that it was not contiguous with the training materials.150Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relationsresorting to the more drastic measure of stemming.
Morphological information is notlost, since the part-of-speech tag remains unchanged.Training consists of counting the frequencies of various contextual cues for eachsense.
Testing consists of taking a new example of the polysemous word and comput-ing the most probable sense, based on the cues present in the context of the new item.A comparison is made to the sense assigned by a human judge, and the classifier'sdecision is scored as correct or incorrect.TLC uses a Bayesian approach to find the sense si that is the most probable giventhe cues q contained in a context window of ?k positions around the polysemoustarget word.
For each si, the probability is computed with Bayes' rule:p(si I C-k .
.
.
.
.
Ck) ~- P(C-k .
.
.
.
"Ck \[ si)p(si)p(C-k,  .
.
.,Ck)As Golding (1995) points out, the term p(c-k, .
.
.
,  Ck I Si) is difficult to estimate becauseof the sparse data problem, but if we assume, as is often done, that the occurrence ofeach cue is independent of the others, then this term can be replaced with:kp(C-k .
.
.
.
'Ck l Si) = H p(Cj I si)j=-kIn TLC, we have made this assumption and have estimated p(cj I si) from the training.Of course, the sparse data problem affects these probabilities too, and so TLC uses theGood-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values ofp(cj I si), including providing probabilities for cues that did not occur in the training.TLC actually uses the mean of the Good-Turing value and the training-derivedvalue for p(cj I si).
When cues do not appear in training, it uses the mean of the Good-Turing value and the global probability of the cue p(cj), obtained from a large textcorpus.
This approach to smoothing has yielded consistently better performance thanrelying on the Good-Turing values alone.TLC uses: (1) topical cues consisting of open-class words found in a wide windowthat includes the sentence in which the target is located plus the preceding sentence;(2) local open-class words found in a narrow window around the target; (3) localclosed-class items; (4) local part-of-speech tags.
The procedures for estimating p(cj I si)and p(cj) differ somewhat for the various types of cue.1.
The counts for open-class words (nouns, verbs, adjectives, and adverbs) fromwhich the topical cue probabilities p(cj I si) and p(cj) are calculated are not sensitive toposition within the wide window (the "bag-of-words" method).
By contrast, the localcue probabilities do take into account position relative to the target.2.
For open-class words found in the three positions to the left of the target (i.e.,j = -3, -2, -1), p(cj I si) is the probability that word cj appears in any of these positions.This permits TLC to generalize over variations in the placement of premodifiers, forexample.
In a similar manner, there is generalization over the three positions to theright of the target.
The local window does not extend beyond a sentence boundary.
Awindow size of ?3 was chosen on empirical grounds; a preliminary study using partsof the Brown corpus that had been manually tagged with senses in WordNet (Landes,Leacock, and Tengi 1998) and a version of TLC that looked only at local open-classwords performed best with this width when tested on a large number of nouns, verbs,and adjectives.3.
Local closed-class items include those elements not assigned anoun, verb, adjec-tive, or adverb tag.
Among these are determiners, prepositions, pronouns, and punc-151Computational Linguistics Volume 24, Number 1tuation.
For this cue type, p(cj \] si) is the probability that item cj appears precisely atlocation j for sense si.
Positions j = -2, -1,1, 2 are used.
The global probabilities, forexample p(the_l), are based on counts of closed-class items found at these positionsrelative to the nouns in a large text corpus.
The local window width of ?2 was selectedafter pilot testing on the semantically tagged Brown corpus.
As in (2) above, the localwindow does not extend beyond a sentence boundary.4.
Part-of-speech tags in the positions j = -2 , -1 ,  0,1, 2 are also used as cues.The probabilities for these tags are computed for specific positions (e.g., p(DT_i \] si),p(DT_i)) in a manner similar to that described in (3) above.When TLC is configured to use only topical information, cue type (1) is employed.When it is configured for local information, cue types (2), (3), and (4) are used.
Finally,in combined mode, the set of cues contains all four types.2.3 ResultsFigures 1 to 3 show the accuracy of the classifier as a function of the size of thetraining set when using local context, topical context, and a combination of the two,averaged across three runs for each training set.
To the extent that the words usedare representative, some clear differences appear as a function of syntactic ategory.With the verb serve, local context was more reliable than topical context at all levels oftraining (78% versus 68% with 200 training examples for the least frequent sense).
Thecombination of local and topical context showed improvement (83%) over either formalone (see Figure 1).
With the adjective hard, local context was much more reliable asan indicator of sense than topical context for all training sizes (83% versus 60% with200 training examples) and the combined classifier's performance (at 83%) was thesame as for local (see Figure 2).
In the case of the noun line, topical was slightly betterthan local at all set sizes, but with 200 training examples, their combination yielded8.4% accuracy, greater than either topical (78%) or local (67%) alone (see Figure 3).To summarize, local context was more reliable than topical context as an indicatorof sense for this verb and this adjective, but slightly less reliable for this noun.
Thecombination of local and topical context showed improved or equal performance forall three words.
Performance for all of the classifiers improved with increased trainingsize.
All classifiers performed best with at least 200 training examples per sense, butthe learning curve tended to level off beyond a minimum 100 training examples.These results are consistent with those of Yarowsky (1993), based on his exper-iments with pseudowords, homophones, and homonyms (discussed below).
He ob-served that performance for verbs and adjectives dropped sharply as the windowh~creased, while distant context remained useful for nouns.
Thus one is tempted toconclude that nouns depend more on topic than do verbs and adjectives.
But sucha conclusion is probably an overgeneralization, i asmuch as some noun senses areclearly nontopical.
Thus, Leacock, Towell, and Voorhees (1993) found that some sensesof the noun line are not susceptible to disambiguation with topical context.
For exam-ple, the 'textual' sense of line can appear with any topic, whereas the 'product' sense ofline cannot.
When it happens that a nontopical sense accounts for a large proportionof occurrences (in our study, all senses of hard are nontopical), then adding topicalcontext o local will have little benefit and may even reduce accuracy.One should not conclude from these results that the topical classifiers and TLCare inferior to the classifiers reviewed in Section 2.
In our experiments, monosemouscollocations in WordNet that contain the target word were systematically removedfrom the training and testing materials.
This was done on the assumption that thesewords are not ambiguous.
Removing them undoubtedly made the task more difficultthan it would normally be.
How much more difficult?
An estimate is possible.
We152Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relations008c2(3_0O~0O0000CO00,1- -  - 0 - - -  - .
-0  - - top ica l- -  -0  - -  - -  -0  - - .
local= = combinedL ~-0  .
.
.
.dI I I25 50 1 O0 200t ra in ing  s i ze  o f  leas t  f requent  senseFigure 1Classifier performance on four senses of the verb serve.
Percentage accounted for by mostfrequent sense = 41%.searched through 7,000 sentences containing line and found 1,470 sentences containedline as the head of a monosemous collocation in WordNet, i.e., line could be correctlydisambiguated in some 21% of those 7,000 sentences simply on the basis of the Word-Net entries in which it occurred.
In other words, if these sentences had been includedin the experiment--and had been identified by automatic lookup--overall accuracywould have increased from 83% to 87%.Using topical context alone, TLC performs no worse than other topical classifiers.Leacock, Towell, and Voorhees (1993) report that the three topical classifiers testedaveraged 74% accuracy on six senses of the noun line.
With these same training andtesting data, TLC performed at 73% accuracy.
Similarly, when the content vector andneural network classifiers were run on manually tagged training and testing examplesof the verb serve, they averaged 74% accuracy--as did TLC using only topical context.When local context is combined with topical, TLC is superior to the topical classifierscompared in the Leacock, Towell, and Voorhees (1993) study.2.4 Improving the Precision of Sense IdentificationJust how useful is a sense classifier whose accuracy is 85% or less?
Probably notvery useful if it is part of a fully automated NLP application, but its performancemight be adequate in an interactive application (e.g., machine-assisted translation,on-line thesaurus functions in word processing, interactive information retrieval).
Infact, when recall does not have to be 100% (as when a human is in the loop) theprecision of the classifier can be improved considerably.
The classifier described abovealways selects the sense that has the highest probability.
We have observed that when153Computational Linguistics Volume 24, Number 10O0EP_OOO00O- - 0 - - - - - -0  - - t o p i c a l- -  -0  - -  - -  -0  - -  local# combined//?//25 50  1 O0 200t ra in ing  s i ze  of leas t  f requent  senseFigure 2Classifier performance on three senses of the adjective hard.
Percentage accounted for by mostfrequent sense = 80%.the difference between the probabil ity of this sense and that of the second highestis relatively small, the classifier's choice is often incorrect.
One way to improve theprecision of the classifier, though at the price of reduced recall, is to identify thesesituations and allow it to respond do not know rather than forcing a decision.What is needed is a measure of the difference in the probabilities of the two senses.Following the approach of Dagan and Itai (1994), we use the log of the ratio of theprobabilities ln(pl/p2) for this purpose.
Based on this value, a threshold O can be setto control when the classifier selects the most probable sense.
For example, if O = 2,then ln(pl/p2) must be 2 or greater for a decision to be made.
Dagan and Itai (1994)also describe a way to make the threshold ynamic so that it adjusts for the amount  ofevidence used to estimate pl and p2.
The basic idea is to create a one-tailed confidenceinterval so that we can state with probability 1 - o~ that the true value of the differencemeasure is greater than 0.
When the amount of evidence is small, the value of themeasure must be larger in order to insure that O is indeed exceeded.Table 2 shows precision and recall values for serve, hard, and line at eight differentsettings of @ using a 60% confidence interval.
TLC was first trained on 100 examples ofeach sense, and it was then tested on separate 100-example sets.
In all cases, precisionwas positively correlated with the square root of O (all r values > .97), and recall wasnegatively correlated with the square root of O (r values < -.96).
As cross-validation,the equations of the lines that fit the precision and recall results on the test samplewere used to predict the precision and recall at the various values of O on a secondtest sample.
They provided a good fit to the new data, accounting for an average of93% of the variance.
The standard errors of estimate for hard, serve, and line were .028,1154Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relations00~=~08EQ.1, - -0O~0O00000 _?00OJ- - 0 - - - ~ - - top ica lJ - .0  - -  - -  -0  - - .
l oca l= = combined///I I I I25 50 1 O0 200training size of least frequent senseF igure  3Classifier performance on six senses of the noun line.
Percentage accounted for by mostfrequent sense = 54%.Tab le  2Recall and precision at various levels of the threshold for one test sample, following trainingwith 200 examples of each sense.ThresholdValue serve hard linefor ln(pl/P2) Recall (%) Precision (%) Recall (%) Precision (%) Recall (%) Precision (%)0 100 78 100 77 100 76.25 99 78 97 78 97 78.5 97 79 95 79 92 801 94 80 90 80 88 822 88 82 79 82 77 864 72 89 62 88 61 918 45 94 39 98 38 9716 14 98 18 100 14 100MostFrequent Sense 41 80 54.030, and  .029 for precision, and  .053, .068, and .041 for recall.
This demonstrates  thatit is possible to produce accurate predict ions of precision and  recall as a funct ion of(9 for new test sets.When the threshold is set to a large value, precision approaches 100%.
The criterionthus provides a way to locate those cases that can be identi f ied automatical ly  with veryhigh accuracy.
When TLC uses a high criterion for ass igning senses, it can be usedto augment  he tra in ing examples by automatical ly  collecting new examples from thetest corpus.155Computational Linguistics Volume 24, Number 1In summary, the results obtained with TLC support the following preliminaryconclusions: (a) improvement with training levels off after about 100 training examplesfor the least frequent sense; (b) the high predictive power of local context for theverb and adjective indicate that the local parameters effectively capture syntacticallymediated relations, e.g., the subject and object or complement of verbs, or the nounthat an adjective modifies; (c) nouns may be more "topical" than verbs and adjectives,and therefore benefit more from the combination of topical and local context; (d) theprecision of TLC can be considerably improved at the price of recall, a trade-off thatmay be desirable in some interactive NLP applications.A final observation we can make is that when topical and local information iscombined, what we have called "nontopical senses" can reduce overall accuracy.
Forexample, the 'textual' sense of line is relatively topic-independent.
The results of theline experiment were not affected too adversely because the nontopical sense of lineaccounted for only 10% of the training examples.
The effects of nontopical senses willbe more serious when most senses are nontopical, as in the case of many adjectivesand verbs.The generality of these conclusions must, of course, be tested with additionalwords, which brings us to the problem of obtaining training and testing corpora.
Onone hand, it is surprising that a purely statistical classifier can "learn" how to identify asense of a polysemous word with as few as 100 example contexts.
On the other hand,anyone who has manually built such sets knows that even collecting 100 examplesof each sense is a long and tedious process.
The next section presents one way inwhich the lexical knowledge in WordNet can be used to extract training examplesautomatically.3.
Unsupervised TrainingCorpus-based word sense identifiers are data hungry--it akes them mere seconds todigest all of the information contained in training materials that take months to preparemanually.
So, although statistical classifiers are undeniably effective, they are not fea-sible until we can obtain reliable unsupervised training data.
In the Gale, Church, andYarowsky (1992a) study, training and testing materials were automatically acquiredusing an aligned French-English bilingual corpus by searching for English words thathave two different French translations.
For example, English tokens of sentence weretranslated as either peine or phrase.
They collected contexts of sentence translated as peineto build a corpus for the judicial sense, and collected contexts of sentence translated asphrase to build a corpus for the grammatical sense.
One problem with relying on bilin-gual corpora for data collection is that bilingual corpora are rare, and aligned bilingualcorpora are even rarer.
Another is that since French and English are so closely related,different senses of polysemous English words often translate to the same French word.For example, line is equally polysemous in French and English--and most senses ofline translate into French as ligne.Several artificial techniques have been used so that classifiers can be developedand tested without having to invest in manually tagging the data: Yarowsky (1993)and Sch/itze (1995) have acquired training and testing materials by creating pseudo-words from existing nonhomographic forms.
For example, a pseudoword was createdby combining abused~escorted.
Examples containing the string escorted were collected totrain on one sense of the pseudoword and examples containing the string abused werecollected to train on the other sense.
In addition, Yarowsky (1993) used homophones(e.g., cellar~seller) and Yarowsky (1994) created homographs by stripping accents fromFrench and Spanish words.
Although these latter techniques are useful in their own156Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relationsright (e.g., spoken language systems or corrupted transmissions), the resulting materi-als do not generalize to the acquisition of tagged training for real polysemous or evenhomographic words.
The results of disambiguation strategies reported for pseudo-words and the like are consistently above 95% overall accuracy, far higher than thosereported for disambiguating three or more senses of polysemous words (Wilks et al1993; Leacock, Towell, and Voorhees 1993).Yarowsky (1992) used a thesaurus to collect raining materials.
He tested the un-supervised training materials on 12 nouns with almost perfect results on homonyms(95-99%), 72% accuracy for four senses of interest, and 77% on three senses of cone.
Thetraining was collected in the following manner.
Take a Roget's category--his exampleswere TOOL and ANIMAL--and collect sentences from a corpus (in this case, Grolier'sEncyclopedia) using the words in each category.
Consider the noun crane, which appearsin both the Roget's categories TOOL and ANIMAL.
To represent the TOOL category,Yarowsky extracted contexts from Grolier's Encyclopedia.
For example, contexts with thewords adz, shovel, crane, sickle, and so on.
Similarly he collected sentences with namesof animals from the ANIMAL category.
In these samples, crane and drill appearedunder both categories.
Yarowsky points out that the resulting noise will be a prob-lem only when one of the spurious enses is salient, dominating the training set, andhe uses frequency-based weights to minimize these effects.
We propose to minimizespurious training by using monosemous words and collocations--on the assumptionthat, if a word has only one sense in WordNet, it is monosemous.Sch~itze (1995) developed a statistical topical approach to word sense identificationthat provides its own automatically extracted training examples.
For each occurrencet of a polysemous word in a corpus, a context vector is constructed by summing allthe vectors that represent the co-occurrence patterns of the open-class words in t'scontext (i.e., topical information is expressed as a kind of second-order co-occurrence).These context vectors are clustered, and the centroid of each cluster is used to repre-sent a "sense."
When given a new occurrence of the word, a vector of the words inits context is constructed, and this vector is compared to the sense representations tofind the closest match.
Schi~tze has used the method to disambiguate pseudowords,homographs, and polysemous words.
Performance varies depending, in part, on thenumber of clusters that are created to represent senses, and on the degree to whichthe distinctions correspond to different opics.
This approach performs very well, es-pecially with pseudowords and homographs.
However, there is no automatic meansto map the sense representations derived from the system onto the more conventionalword senses found in dictionaries.
Consequently, it does not provide disambiguatedexamples that can be used by other systems.Yarowsky (1995) has proposed automatically augmenting a small set of experi-menter-supplied seed collocations (e.g., manufacturing plant and plant life for two dif-ferent senses of the noun plant) into a much larger set of training materials.
He resolvedthe problem of the sparseness ofhis collocations by iteratively bootstrapping acquisi-tion of training materials from a few seed collocations for each sense of a homograph.He locates examples containing the seeds in the corpus and analyzes these to findnew predictive patterns in these sentences and retrieves examples containing thesepatterns.
He repeats this step iteratively.
Results for the 12 pairs of homographs re-ported are almost perfect.
In his paper, Yarowsky suggests WordNet as a source forthe seed collocations--a suggestion that we pursue in the next section.WordNet is particularly well suited to the task of locating sense-relevant contextbecause ach word sense is represented asa node in a rich semantic lexical networkwith synonymy, hyponymy, and meronymy links to other words, some of them poly-semous and others monosemous.
These lexical "relatives" provide a key to finding157Computational Linguistics Volume 24, Number 1relevant raining sentences in a corpus.
For example, the noun suit is polysemous,but one sense of it has business uit as a monosemous daughter and another has legalproceeding as a hypernym.
By collecting sentences containing the unambiguous nounsbusiness uit and legal proceeding we can build two corpora of contexts for the respectivesenses of the polysemous word.
All the systems described in Section 2.1 could benefitfrom the additional training materials that monosemous relatives can provide.3.1 WordNet: A Lexical Database for EnglishThe WordNet on-line lexical database (Miller 1990, 1995) has been developed at Prince-ton University over the past 10 years.
4 Like a standard dictionary, WordNet containsthe definitions of words.
It differs from a standard dictionary in that, instead of be-ing organized alphabetically, WordNet is organized conceptually.
The basic unit inWordNet is a synonym set, or synset, which represents a lexicalized concept.
For ex-ample, WordNet Version 1.5 distinguishes between two senses of the noun shot withthe synsets {shot, snapshot} and {shot, injection}.
In the context, "The photographertook a shot of Mary," the word snapshot can be substituted for one sense of shot.
In thecontext, "The nurse gave Mary a flu shot," the word injection can be substituted foranother sense of shot.Nouns, verbs, adjectives, and adverbs are each organized ifferently in WordNet.All are organized in synsets, but the semantic relations among the synsets differ de-pending on the grammatical category, as can be seen in Table 3.Nouns are organized in a hierarchical tree structure based on hypernymy/hypo-nymy.
The hyponym of a noun is its subordinate, and the relation between a hyponymand its hypernym is an is a kind off relation.
For example, maple is a hyponym of tree,which is to say that a maple is a kind of tree.
Hypernymy (supername) and its inverse,hyponymy (subname), are transitive semantic relations between synsets.
Meronymy(part-name), and its inverse holonymy (whole-name), are complex semantic relationsthat distinguish component parts, substantive parts, and member parts.The verbal hierarchy is based on troponymy, the is a manner of relation.
For exam-ple, stroll is a troponym of walk, which is to say that strolling is a manner of walking.Entailment relations between verbs are also coded in WordNet.The organization of attributive adjectives i based on the antonymy relation.
Wheredirect antonyms exist, adjective synsets point to antonym synsets.
A head adjective isone that has a direct antonym (e.g., hot versus cold or long versus short).
Many adjec-tives, like sultry, have no direct antonyms.
When an adjective has no direct antonym,its synset points to a head that is semantically similar to it.
Thus sultry and torrid aresimilar in meaning to hot, which has the direct antonym of cold.
So, although sultryhas no direct antonym, it has cold as its indirect antonym.Relational adjectives do not have antonyms; instead they point to nouns.
Considerthe difference between a nervous disorder and a nervous tudent.
In the former, nervouspertains to a noun, as in nervous ystem, whereas the latter is defined by its relation toother adjectives--its synonyms (e.g., edgy) and antonyms (e.g., relaxed).Adverbs have synonymy and antonymy relations.
When the adverb is morpho-logically related to an adjective (when an -ly suffix is added to an adjective) andsemantically related to the adjective as well, the adverb points to the adjective.We have had some success in exploiting WordNet's semantic relations for wordsense identification.
Since the main problem with classifiers that use local context is4 Available by anonymous ftp from clarity.princeton.edu c  pub/wordnet orhttp://www.cogsci.princeton.edu / ~wn/158Leacock, Chodorow, and Miller Corpus Statistics and WordNet RelationsTable 3Semantic relations in WordNet.Semantic SyntacticRelation Category ExamplesSynonymy Noun pipe, tube(similar) Verb rise, ascendAdj sad, unhappyAdv rapidly, speedilyAntonymy Adj wet, dry(opposite) Adv rapidly, slowlyNoun top, bottomVerb rise, fallHyponymy Noun sugar maple, maple(subordinate) maple, treetree, plantMeronymy Noun brim, hat(port) gin, martiniship, fleetTroponymy Verb march, walk(manner) whisper, speakEntailment Verb drive, ridedivorce, marryDerivation Adj magnetic, magnetismAdv simply, simplethe sparseness of the training data, Leacock and Chodorow (1998) used a proximitymeasure on the hypernym relation to replace the subject and complement of the verbserve in the testing examples with the subject and complement from training examplesthat were "closest" to them in the noun hierarchy.
For example, one of the test sen-tences was "Sauerbraten is usually served with dumplings," where neither sauerbratennor dumpling appeared in any training sentence.
The similarity measures on WordNetfound that sauerbraten was most similar to dinner in the training, and dumpling to bacon.These nouns were substituted for the novel ones in the test sets.
Thus the sentence"Dinner is usually served with bacon" was substituted for the original sentence.
Aug-mentation of the local context classifier with WordNet similarity measures howed asmall but consistent improvement in the classifier's performance.
The improvementwas greater with the smaller training sets.Resnik (1992) uses an information-based measure, the most informative class, onthe WordNet taxonomy.
A class consists of the synonyms found at a node and thesynonyms at all the nodes that it dominates (all of its hyponyms).
Based on verb/objectpairs collected from a corpus, Resnik found, for example, that the objects for the verbopen fall into two classes: receptacle and oral communication.
Conversely, the class ofa verb's object could be used to determine the appropriate sense of that verb.The experiments in the next section depend on a subset of the WordNet lexicalrelations, those involving monosemous relatives, so we were interested in determiningjust what proportion of word senses have such relatives.
We examined 8,500 polyse-mous nouns that appeared in a moderate-size, 25-million-word corpus.
In all, these8,500 nouns have more than 24,000 WordNet senses.
Restricting the relations to syn-159Computational Linguistics Volume 24, Number 1Table 4Training materials and their frequencies for five senses of line.product formation text cord phoneproduct line 95 picket line 46 headline 52 rope 28 hot linebusiness line 5 line of punch line 23 ropes 27 phone linesuccession 10 opening line 10 clothesline 12 private linebread line 7 tag line 7 fishing line 11 toll linesingle file 7 line of poetry 4 shoelaces 4conga line 7 newspaper twine 3reception line 3 headline 2 dental floss 2ticket line 3 gag line 2 high wire 2chow line 2 jump rope 2rivet line 1 lasso 2single files 1 lead line 1trap line 1 mooring line 1484822onyms, immediate hyponyms (i.e., daughters), and immediate hypernyms (parents),we found that about 64% (15,400) have monosemous relatives attested in the cor-pus.
With larger corpora (e.g., with text obtained by Web crawling) and more lexicalrelations (e.g., meronymy), this percentage can be expected to increase.3.2 Training on WordNet's Monosemous  RelativesThe approach we have used is related to that of Yarowsky (1992) in that training ma-terials are collected using a knowledge base, but it differs in other respects, notablyin the selection of training and testing materials, the choice of a knowledge base, anduse of both topical and local classifiers.
Yarowsky collects his training and testing ma-terials from a specialized corpus, Grolier's Encyclopedia.
It remains to be seen whethera statistical classifier trained on a topically organized corpus such as an encyclopediawill perform in the same way when tested on general unrestricted text, such as news-papers, periodicals, and books.
One of our goals is to determine whether automaticextraction of training examples is feasible using general corpora.
In his experiment,Yarowsky uses an updated on-line version of RogeFs Thesaurus that is not generallyavailable to the research community.
The only generally available version of Roget's isthe 1912 edition, which contains many lexical gaps.
We are using WordNet, which canbe obtained via anonymous ftp.
Yarowsky's classifier is purely topical, but we alsoexamine local context.
Finally, we hope to avoid inclusion of spurious senses by usingmonosemous relatives.In this experiment we collected monosemous relatives of senses of 14 nouns.
Train-ing sets are created in the following manner.
A program called AutoTrain retrievesfrom WordNet al of the monosemous relatives of a polysemous word sense, sam-ples and retrieves example sentences containing these monosemous relatives from a30-million-word corpus of the San Jose Mercury News, and formats them for TLC.
Thesampling process retrieves the "closest" relatives first.
For example, suppose that thesystem is asked to retrieve 100 examples for each sense of the noun court.
The systemfirst looks for the strongest or top-level relatives: for monosemous synonyms of thesense (e.g., tribunal) and for daughter collocations that contain the target word as thehead (e.g., superior court) and tallies the number of examples in the corpus for each.
Ifthe corpus has 100 or more examples for these top-level relatives, it retrieves a sam-pling of them and formats them for TLC.
If there are not enough top-level examples,160Leacock, Chodorow, and Miller Corpus Statistics and WordNet RelationsTable 5TLC's performance when training on (1) manually tagged ata and (2) monosemous relativesof polysemous words.Manually MonosemousTarget Word Sense and Priors (%) Tagged Training Relative Trainingbill 89.1% 88.5%legal 85 97 98invoice 15 56 35duty 94.3% 93.5%tariff 56 95 97obligation 44 99 91line 82.6% 74.7%product 67 97 86phone 10 67 51cord 9 79 74formation 8 49 26text 6 67 52rate 81% 79%monetary 65 90 80frequency 35 66 77shot 89.8% 89.1%sports 74 99 95gunshot 17 77 87opportunity 8 36 47work 75.3% 65.2%activity 55 81 81product 45 68 46the remainder of the target's monosemous relatives are inspected in the order: all otherdaughters; hyponym collocations that contain the target; all other hyponyms;  hyper-nyms; and, finally, sisters.
AutoTrain takes as broad a sampling as possible across thecorpus and never takes more than one example from an article.
The number  of ex-amples for each relative is based on the relative proport ion of its occurrences in thecorpus.
Table 4 shows the monosemous relatives that were used to train five senses ofthe noun l ine- - the monosemous relatives of the sixth sense in the original study, lineas an abstract division, are not attested in the SJM corpus.The purpose of the experiment was to see how well TLC performed using un-supervised training and, when possible, to compare this with its performance whentraining on the manual ly tagged materials being produced at Princeton's Cognitive Sci-ence Laboratory.
s When a sufficient number  of examples for two or more senses wereavailable, 100 examples of each sense were set aside to use in training.
The remainderwere used for testing.
Only the topical and local open-class cues were used, since pre-l iminary tests showed that performance declined when using local closed-class andpart-of-speech cues obtained from the monosemous relatives.
This is not surprising,as many of the relatives are collocations whose local syntax is quite different from that5 These materials are being produced under the direction of Shari Landes.
Monosemous collocations thatcontain the target as the head have not been removed from the materials reported here---except for thenoun line.161Computational Linguistics Volume 24, Number 1Table 6TLC's performance when training on monosemous relatives of polysemous words.
(Manuallytagged ata were used for scoring but there were not enough examples to train on.
)Target Sense and Monosemous Target Sense and MonosemousWord Priors (%) Relative Training Word Priors (%) Relative Trainingbank 92.8% security 67.4%institution 92 98 certificate 67 85land form 8 35 precaution 37 63company 86.9% stock 99.9%business 87 90 capital 95 100troupe 7 67 broth 5 98guests 6 70court 97.5% strike 80.8%tribunal 96 99 work stoppage 78 98sports 4 71 attack 23 21party 87.8% trade 80.2%political 77 96 commerce 81 92social 23 59 swap 19 31of the polysemous word in its typical usage.
For example, the 'formation' sense of lineis often followed by an 0f-phrase as in a line of children, but its relative, picket line, isnot.
Prior probabilities for the sense were taken from the manually tagged materials.Table 5 shows the results when TLC was trained on monosemous relatives and onmanually tagged training materials.
Baseline performance is when the classifier alwayschooses the most frequent sense.
Eight additional words had a sufficient number ofmanually tagged examples for testing but not for training TLC.
These are shown inTable 6.For four of the examples in Table 5, training with relatives produced results within1% or 2% of manually tagged training.
Line and work, however, showed a substantialdecrease in performance.
In the case of line, this might be due to overly specific trainingcontexts.
Almost half of the training examples for the 'formation' sense of line comefrom one relative, picket line.
In fact, all of the monosemous relatives, except for rivetline and trap line, are human formations.
This may have skewed training so that theclassifier performs poorly on other uses of line as formation.In order to compare our results with those reported in Yarowsky (1992), we trainedand tested on the same two senses of the noun duty that Yarowsky had tested ('obliga-tion' and 'tax').
He reported that his thesaurus-based approach yielded 96% precisionwith 100% recall.
TLC used training examples based on monosemous WordNet rela-tives and correctly identified the senses with 93.5% precision at 100% recall.Table 6 shows TLC's performance on the other eight words after training withmonosemous relatives and testing on manually tagged examples.
Performance is aboutthe same as, or only slightly better than, the highest prior probability.
In part, this isdue to the rather high probability of the most frequent sense for this set.The values in the table are based on decisions made on all test examples.
If athreshold is set for TLC (see Section 2.4), precision of the classifier can be increasedsubstantially, at the expense of recall.
Table 7 shows recall levels when TLC is trainedon monosemous relatives and the value of @ is set for 95% precision.
Operating inthis mode, the classifier can gather new training materials, automatically, and with162Leacock, Chodorow, and Miller Corpus Statistics and WordNet RelationsTable 7Percentage of recall when the precision is 95%.Word Recall at 95% Precision Word Recall at 95% Precisionbank 95% rate 36%bill 73% security 35%company 72% shot 77%duty 93% strike 37%line 45% trade 42%party 78% work 2%high precision.
This is a particularly good way to find clear cases of the most frequentsense.The results also show that not all words are well suited to this kind of operation.Little can be gained for a word like work, where the two senses, 'activity' and 'product,'are closely related and therefore difficult for the classifier to distinguish, due to a highdegree of overlap in the training contexts.
Problems of this sort can be detected evenbefore testing, by computing correlations between the vectors of open-class words forthe different senses.
The cosine correlation between the 'activity' and 'product' sensesof work is r = .49, indicating a high degree of overlap.
The mean correlation betweenpairs of senses for the other words in Table 7 is r = .31.4.
Conc lus ionOur evidence indicates that local context is superior to topical context as an indicatorof word sense when using a statistical classifier.
The benefits of adding topical to localcontext alone depend on syntactic ategory as well as on the characteristics of theindividual word.
The three words studied yielded three different patterns; a substantialbenefit for the noun line, slightly less for the verb serve, and none for the adjectivehard.
Some word senses are simply not limited to specific topics, and appear freelyin many different domains of discourse.
The existence of nontopical senses also limitsthe applicability of the "one sense per discourse" generalization of Gale, Church, andYarowsky (1992b), who observed that, within a document, a repeated word is almostalways used in the same sense.
Future work should be directed toward developingmethods for determining when a word has a nontopical sense.
One approach to thisproblem is to look for a word that appears in many more topical domains than itstotal number of senses.Because the supply of manually tagged training data will always be limited, wepropose a method to obtain training data automatically using commonly availablematerials: exploiting WordNet's lexical relations to harvest training examples fromLDC corpora or even the World Wide Web.
We found this method to be effective,although not as effective as using manually tagged training.
We have presented thecomponents of a system for acquiring unsupervised training materials that can beused with any statistical classifier.The components can be fit together in the following manner.
For a polysemousword, locate the monosemous relatives for each of its senses in WordNet and extractexamples containing these relatives from a large corpus.
Senses whose contexts greatlyoverlap can be identified with a simple cosine correlation.
Often, correlations are highbetween senses of a word that are systematically related, as we saw for the 'activity'163Computational Linguistics Volume 24, Number 1and 'product '  senses of work.
In some cases, the contexts for the two closely relatedsenses may be combined.Since the frequencies of the monosemous relatives do not correlate with the fre-quencies of the senses, prior probabilities must  be estimated for classifiers that usethem.
In the experiments of Section 3.2, these were estimated from the testing mate-rials.
They can also be estimated from a small manual ly  tagged sample, such as theparts of the Brown corpus that have been tagged with senses in WordNet.When the threshold is set to maximize precision, the results are highly reliable andcan be used to support an interactive application, such as machine-assisted translation,with the goal of reducing the amount  of interaction.Although we have looked at only a few examples, it is clear that, given WordNetand a large enough corpus, the methods outlined for training on monosemous relativescan be generalized to build training materials for thousands of polysemous words.AcknowledgmentsWe are indebted to the other members ofthe WordNet group who have providedadvice and technical support: ChristianeFellbaum, Shari Landes, and Randee Tengi.We are also grateful to Paul Bagyenda, BenJohnson-Laird and Joshua Schecter.
Wethank Scott Wayland, Tim Allison and JillHollifield for tagging the serve and hardcorpora.
Finally we are grateful to the threeanonymous CL reviewers for theircomments and advice.This material is based upon worksupported in part by the National ScienceFoundation under NSF Award No.
IRI95-28983 and by the Defense AdvancedResearch Projects Agency, Grant No.N00014-91-1634.ReferencesBrill, Eric.
1994.
Some advances inrule-based part of speech tagging.
InProceedings ofthe Twelfth National Conferenceon Artificial Intelligence, Seattle.
AAAI.Bruce, Rebecca nd Janyce Wiebe.
1994a.
Anew approach to word sensedisambiguation.
I  Proceedings ofthe ARPAWorkshop on Human Language Technology,San Francisco, CA, Morgan Kaufman.Bruce, Rebecca nd Janyce Wiebe.
1994b.Word-sense disambiguation usingdecomposable models.
In Proceedings ofthe32nd Annual Meeting, Las Cruces, NM.Association for ComputationalLinguistics.Chiang, T-H., Y-C. Lin, and K-Y Su.
1995.Robust learning, smoothing, andparameter tying on syntactic ambiguityresolution.
Computational Linguistics,21 (3):321-349.Dagan, Ido and Alon Itai.
1994.
Word sensedisambiguation using a second languagemonolingual corpus.
ComputationalLinguistics, 20(4).Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992a.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities, 26.Gale, William, Kenneth W. Church, andDavid Yarowsky.
1992b.
One sense perdiscourse.
In Proceedings ofthe Speech andNatural Language Workshop, San Francisco,CA, Morgan Kaufmann.Golding, Andrew.
1995.
A Bayesian hybridmethod for context-sensitive spellingcorrection.
In Proceedings ofthe ThirdWorkshop on Very Large Corpora,Cambridge, MA.
ACL.Good, I. E 1953.
The population frequenciesof species and the estimation ofpopulation parameters.
Biometrica,40:237-264.Hearst, Marti A.
1991.
Noun homographdisambiguation using local context inlarge text corpora.
In Proceedings oftheSeventh Annual Conference ofthe UW Centrefor the New OED and Text Research: UsingCorpora, pages 1-22, Oxford.Hirst, Graeme.
1987.
Semantic Interpretationand the Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge, MA.Landes, Shari, Claudia Leacock, and RandeeTengi.
1998.
Building semanticconcordances.
In Christiane Fellbaum,editor, WordNet: A Lexical Reference Systemand its Application.
MIT Press, Cambridge,MA.Leacock, Claudia and Martin Chodorow.1998.
Combining local context withWordNet similarity for word senseidentification.
In Christiane Fellbaum,editor, WordNet: A Lexical Reference Systemand its Application.
MIT Press, Cambridge,MA.Leacock, Claudia, Geoffrey Towell, andEllen M. Voorhees.
1993.
Corpus-basedstatistical sense resolution.
In Proceedings164Leacock, Chodorow, and Miller Corpus Statistics and WordNet Relationsof the ARPA Workshop on Human LanguageTechnology, San Francisco, CA, MorganKaufman.Leacock, Claudia, Geoffrey Towell, andEllen M. Voorhees.
1996.
Towardsbuilding contextual representations ofword senses using statistical models.
InBranimir Boguraev and JamesPustejovsky, editors, Corpus Processing forLexical Acquisition.
MIT Press, Cambridge,MA.Miller, George A., editor, 1990.
WordNet: AnOn-Line Lexical Database.
Volume 3(4) ofthe International Journal of Lexicography.Oxford University Press.Miller, George A.
1995.
WordNet: Anon-line lexical database.
Communications ofthe ACM, 38(11).Resnik, Philip.
1992.
WordNet anddistributional nalysis: A class-basedapproach to lexical discovery.
In Workshopon Statistically-BasedNatural-Language-Processing Techniques, SanJose, July.Schfitze, Hinrich.
1995.
Ambiguity andLanguage Learning: Computational ndCognitive Models.
Ph.D. thesis, StanfordUniversity.V~ronis, Jean and Nancy Ide.
1990.
Wordsense disambiguation with very largeneural networks extracted from machinereadable dictionaries.
In Proceedings ofCOLING-90.Weiss, Stephen.
1973.
Learning todisambiguate.
Information Storage andRetrieval, 9.Wilks, Yorick, Dan Fass, Cheng Ming Guo,James E. McDonald, Tony Plate, andBrian M. Slator.
1993.
Machine tractabledictionary tools.
In James Pustejovsky,editor, Semantics and the Lexicon.
Kluwer,Dordrecht.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofCOLING-92,Nantes, France.Yarowsky, David.
1993.
One sense percollocation.
In Proceedings ofthe ARPAWorkshop on Human Language Technology,San Francisco, CA, Morgan Kaufman.Yarowsky, David.
1994.
Decision lists forlexical ambiguity resolution: Applicationto accent restoration i  Spanish andFrench.
In Proceedings ofthe 32nd AnnualMeeting.
Las Cruces, NM.
Association forComputational Linguistics.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting, Cambridge, MA.
Association forComputational Linguistics.165
