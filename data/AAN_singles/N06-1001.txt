Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 1?8,New York, June 2006. c?2006 Association for Computational LinguisticsCapitalizing Machine TranslationWei Wang and Kevin Knight and Daniel MarcuLanguage Weaver, Inc.4640 Admiralty Way, Suite 1210Marina del Rey, CA, 90292{wwang, kknight, dmarcu}@languageweaver.comAbstractWe present a probabilistic bilingual capi-talization model for capitalizing machinetranslation outputs using conditional ran-dom fields.
Experiments carried out onthree language pairs and a variety of ex-periment conditions show that our modelsignificantly outperforms a strong mono-lingual capitalization model baseline, es-pecially when working with small datasetsand/or European language pairs.1 IntroductionCapitalization is the process of recovering case in-formation for texts in lowercase.
It is also calledtruecasing (Lita et al, 2003).
Usually, capitalizationitself tries to improve the legibility of texts.
It, how-ever, can affect the word choice or order when inter-acting with other models.
In natural language pro-cessing, a good capitalization model has been shownuseful for tasks like name entity recognition, auto-matic content extraction, speech recognition, mod-ern word processors, and machine translation (MT).Capitalization can be viewed as a sequence la-beling process.
The input to this process is a sen-tence in lowercase.
For each lowercased word inthe input sentence, we have several available cap-italization tags: initial capital (IU), all uppercase(AU), all lowercase (AL), mixed case (MX), andall having no case (AN).
The output of capital-ization is a capitalization tag sequence.
Associ-ating a tag in the output with the correspondinglowercased word in the input results in a surfaceform of the word.
For example, we can tag theinput sentence ?click ok to save your changes to/home/doc.?
into ?click IU ok AU to AL save ALyour AL changes AL to AL /home/doc MX .
AN?,getting the surface form ?Click OK to save yourchanges to /home/DOC .
?.A capitalizer is a tagger that recovers the capi-talization tag for each input lowercased word, out-putting a well-capitalized sentence.
Since each low-ercased word can have more than one tag, and as-sociating a tag with a lowercased word can resultin more than one surface form (e.g., /home/doc MXcan be either /home/DOC or /home/Doc), we need acapitalization model to solve the capitalization am-biguities.
For example, Lita et al (2003) use a tri-gram language model estimated from a corpus withcase information; Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) com-bining features involving words and their cases.Capitalization models presented in most previ-ous approaches are monolingual because the modelsare estimated only from monolingual texts.
How-ever, for capitalizing machine translation outputs,using only monolingual capitalization models is notenough.
For example, if the sentence ?click ok tosave your changes to /home/doc .?
in the aboveexample is the translation of the French sentence?CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFI-CATIONS DANS /HOME/DOC .
?, the correct capitaliza-tion result should probably be ?CLICK OK TO SAVEYOUR CHANGES TO /HOME/DOC .
?, where all wordsare in all upper-case.
Without looking into the case1of the MT input, we can hardly get the correct capi-talization result.Although monolingual capitalization models inprevious work can apply to MT output, a bilingualmodel is more desirable.
This is because MT out-puts usually strongly preserve case from the input,and because monolingual capitalization models donot always perform as well on badly translated textas on well-formed syntactic texts.In this paper, we present a bilingual capitalizationmodel for capitalizing machine translation outputsusing conditional random fields (CRFs) (Lafferty etal., 2001).
This model exploits case informationfrom both the input sentence (source) and the out-put sentence (target) of the MT system.
We define aseries of feature functions to incorporate capitaliza-tion knowledge into the model.Experimental results are shown in terms of BLEUscores of a phrase-based SMT system with the cap-italization model incorporated, and in terms of cap-italization precision.
Experiments are performedon both French and English targeted MT systemswith large-scale training data.
Our experimental re-sults show that the CRF-based bilingual capitaliza-tion model performs better than a strong baselinecapitalizer that uses a trigram language model.2 Related WorkA simple capitalizer is the 1-gram tagger: the case ofa word is always the most frequent one observed intraining data, with the exception that the sentence-initial word is always capitalized.
A 1-gram capital-izer is usually used as a baseline for capitalizationexperiments (Lita et al, 2003; Kim and Woodland,2004; Chelba and Acero, 2004).Lita et al (2003) view capitalization as a lexi-cal ambiguity resolution problem, where the lexi-cal choices for each lowercased word happen to beits different surface forms.
For a lowercased sen-tence e, a trigram language model is used to find thebest capitalization tag sequence T that maximizesp(T, e) = p(E), resulting in a case-sensitive sen-tence E. Besides local trigrams, sentence-levelcontexts like sentence-initial position are employedas well.Chelba and Acero (2004) frame capitalization asa sequence labeling problem, where, for each low-MT DecoderTrain MonolingualCapitalization ModelMonolingual Cap Model CapitalizationLower CaseLower CasefLower CaseeFinputEoutputTrainTranslation ModelTrainLanguage ModelTranslationModelLanguagelModel{F}{E}{f}{e}Figure 1: The monolingual capitalization scheme employedby most statistical MT systems.ercased sentence e, they find the label sequence Tthat maximizes p(T |e).
They use a maximum en-tropy Markov model (MEMM) to combine featuresof words, cases and context (i.e., tag transitions).Gale et al (1994) report good results on capital-izing 100 words.
Mikheev (1999) performs capital-ization using simple positional heuristics.3 Monolingual Capitalization SchemeTranslation and capitalization are usually performedin two successive steps because removing case infor-mation from the training of translation models sub-stantially reduces both the source and target vocabu-lary sizes.
Smaller vocabularies lead to a smallertranslation model with fewer parameters to learn.For example, if we do not remove the case informa-tion, we will have to deal with at least nine prob-abilities for the English-French word pair (click,cliquez).
This is because either ?click?
or ?cliquez?can have at least three tags (IU, AL, AU), and thusthree surface forms.
A smaller translation model re-quires less training data, and can be estimated moreaccurately than otherwise from the same amountof training data.
A smaller translation model alsomeans less memory usage.Most statistical MT systems employ the monolin-gual capitalization scheme as shown in Figure 1.
Inthis scheme, the translation model and the target lan-guage model are trained from the lowercased cor-pora.
The capitalization model is trained from thecase-sensitive target corpus.
In decoding, we firstturn input into lowercase, then use the decoder togenerate the lowercased translation, and finally ap-2HYDRAULIC HEADER TILT CYLINDER KITKit de ve?rin d?inclinaison hydraulique de la plate-formehaut-parleur avant droit +HAUT-PARLEUR AVANT DROIT +Seat Controls, StandardCOMMANDES DU SIGE, STANDARDloading a saved legendChargement d?une le?gende sauvegardeTable 1: Errors made by monolingual capitalization model.Each row contains a pair of MT input and MT output.MT DecoderCapitalizationBilingualCap ModelTrain BilingualCap ModelalignmentWord/Phrase AlignerfLower CaseeFinputEoutput{F}{E}Figure 2: A bilingual capitalization scheme.ply the capitalization model to recover the case ofthe decoding output.The monolingual capitalization scheme makesmany errors as shown in Table 1.
Each cell inthe table contains the MT-input and the MT-output.These errors are due to the capitalizer does not haveaccess to the source sentence.Regardless, estimating mixed-cased translationmodels, however, is a very interesting topic andworth future study.4 Bilingual Capitalization Model4.1 The ModelOur probabilistic bilingual capitalization model ex-ploits case information from both the input sentenceto the MT system and the output sentence from thesystem (see Figure 2).
An MT system translates acapitalized sentence F into a lowercased sentence e.A statistical MT system can also provide the align-ment A between the input F and the output e; forexample, a statistical phrase-based MT system couldprovide the phrase boundaries in F and e, and alsothe alignment between the phrases.11We shall explain our capitalization model within thephrase-based SMT framework, the model, however, could beOKClick OKCliquezEFE?iF?jFigure 3: Alignment graph.
Brackets mean phrase bound-aries.The bilingual capitalization algorithm recoversthe capitalized sentence E from e, according to theinput sentence F , and the alignment A. Formally,we look for the best capitalized sentence E?
suchthatE?
= arg maxE?GEN(e)p(E|F,A) (1)where GEN(e) is a function returning the set ofpossible capitalized sentences consistent with e. No-tice that e does not appear in p(E|F,A) because wecan uniquely obtain e from E. p(E|F,A) is the cap-italization model of concern in this paper.2To further decompose the capitalization modelp(E|F,A), we make some assumptions.
As shownin Figure 3, input sentence F , capitalized output E,and their alignment can be viewed as a graph.
Ver-tices of the graph correspond to words in F andE.
An edge connecting a word in F and a wordin E corresponds to a word alignment.
An edgebetween two words in E represents the dependencybetween them captured by monolingual n-gram lan-guage models.
We also assume that both E andF have phrase boundaries available (denoted by thesquare brackets), and that A is the phrase alignment.In Figure 3, F?j is the j-th phrase of F , E?i is the i-thphrase of E, and they align to each other.
We do notrequire a word alignment; instead we find it reason-able to think that a word in E?i can be aligned to anyadapted to syntax-based machine translation, too.
To this end,the translational correspondence is described within a transla-tion rule, i.e., (Galley et al, 2004) (or a synchronous produc-tion), rather than a translational phrase pair; and the trainingdata will be derivation forests, instead of the phrase-alignedbilingual corpus.2The capitalization model p(E|F, A) itself does not requirethe existence of e. This means that in principle this model canalso be viewed as a capitalized translation model that performstranslation and capitalization in an integrated step.
In our paper,however, we consider the case where the machine translationoutput e is given, which is reflected by the the fact that GEN(e)takes e as input in Formula 1.3word in F?j .
A probabilistic model defined on thisgraph is a Conditional Random Field.
Therefore,it is natural to formulate the bilingual capitalizationmodel using CRFs:3p?
(E|F, A) =1Z(F, A, ?
)expIXi=1?ifi(E, F, A)!
(2)whereZ(F, A, ?)
=XE?GEN(e)expIXi=1?ifi(E,F, A)!
(3)fi(E,F,A), i = 1...I are the I features, and?
= (?1, ..., ?I) is the feature weight vector.
Basedon this capitalization model, the decoder in the cap-italizer looks for the best E?
such thatE?
= arg maxE?GEN(e,F )I?i=1?ifi(E,F,A) (4)4.2 Parameter EstimationFollowing Roark et al (2004), Lafferty et al (2001)and Chen and Rosenfeld (1999), we are looking forthe set of feature weights ?
maximizing the regu-larized log-likelihood LLR(?)
of the training data{E(n), F (n), A(n), n = 1, ..., N}.LLR(?)
=NXn=1log p?E(n)|F (n), A(n)??
||?||22?2 (5)The second term at the right-hand side of For-mula 5 is a zero-mean Gaussian prior on the pa-rameters.
?
is the variance of the Gaussian priordictating the cost of feature weights moving awayfrom the mean ?
a smaller value of ?
keeps featureweights closer to the mean.
?
can be determinedby linear search on development data.4 The use ofthe Gaussian prior term in the objective function hasbeen found effective in avoiding overfitting, leadingto consistently better results.
The choice of LLR asan objective function can be justified as maximuma-posteriori (MAP) training within a Bayesian ap-proach (Roark et al, 2004).3We chose CRFs over other sequence labeling models (i.e.MEMM) because CRFs have no label bias and we do not needto compute the partition function during decoding.4In our experiment, we use an empirical value ?
= 0.5 as in(Roark et al, 2004).4.3 Feature FunctionsWe define features based on the alignment graphin Figure 3.
Each feature function is defined on aword.Monolingual language model feature.
Themonolingual LM feature of word Ei is the loga-rithm of the probability of the n-gram ending atEi:fLM(Ei, F,A) = log p(Ei|Ei?1, ..., Ei?n+1) (6)p should be appropriately smoothed such that itnever returns zero.Capitalized translation model feature.
Sup-pose E phrase ?Click OK?
is aligned to Fphrase ?Cliquez OK?.
The capitalized transla-tion model feature of ?Click?
is computed aslog p(Click|Cliquez)+log p(Click|OK).
?Click?
isassumed to be aligned to any word in the F phrase.The larger the probability that ?Click?
is translatedfrom an F word, i.e., ?Cliquez?, the more chancesthat ?Click?
preserves the case of ?Cliquez?.
For-mally, for word Ei, and an aligned phrase pair E?land F?m, where Ei ?
E?l, the capitalized translationmodel feature of Ei isfcap?t1(Ei, F,A) = log|F?m|?k=1p(Ei|F?m,k) (7)p(Ei|F?m,k) is the capitalized translation table.
Itneeds smoothing to avoid returning zero, and is esti-mated from a word-aligned bilingual corpus.Capitalization tag translation feature.
The fea-ture value of E word ?Click?
aligning to F phrase?Cliquez OK?
is log p(IU|IU)p(click|cliquez) +log p(IU|AU)p(click|ok).
We see that this featureis less specific than the capitalized translation modelfeature.
It is computed in terms of the tag transla-tion probability and the lowercased word translationprobability.
The lowercased word translation proba-bility, i.e., p(click|ok), is used to decide how muchof the tag translation probability, i.e., p(IU|AU),will contribute to the final decision.
The smaller theword translation probability, i.e., p(click|ok), is, thesmaller the chance that the surface form of ?click?4preserves case from that of ?ok?.
Formally, this fea-ture is defined asfcap?tag?t1(Ei, F,A) =log|f?m|?k=1p(ei|f?m,k) ?
p(?(Ei)|?
(F?m,k)) (8)p(ei|f?m,k) is the t-table over lowercased word pairs,which is the usual ?t-table?
in a SMT system.p(?(Ei)|?
(F?m,k)) is the probability of a target cap-italization tag given a source capitalization tag andcan be easily estimated from a word-aligned bilin-gual corpus.
This feature attempts to help whenfcap?t1 fails (i.e., the capitalized word pair is un-seen).
Smoothing is also applied to both p(ei|f?m,k)and p(?(Ei)|?
(F?m,k)) to handle unseen words (orword pairs).Upper-case translation feature.
Word Ei is inall upper case if all words in the corresponding Fphrase F?m are in upper case.
Although this fea-ture can also be captured by the capitalization tagtranslation feature in the case where an AU tag inthe input sentence is most probably preserved in theoutput sentence, we still define it to emphasize itseffect.
This feature aims, for example, to translate?ABC XYZ?
into ?UUU VVV?
even if all words areunseen.Initial capitalization feature.
An E word is ini-tially capitalized if it is the first word that containsletters in the E sentence.
For example, for sentence??
Please click the button?
that starts with a bul-let, the initial capitalization feature value of word?please?
is 1 because ???
does not contain a letter.Punctuation feature template.
An E word is ini-tially capitalized if it follows a punctuation mark.Non-sentence-ending punctuation marks like com-mas will usually get negative weights.As one can see, our features are ?coarse-grained?
(e.g., the language model feature).
In contrast, Kimand Woodland (2004) and Roark et al (2004) use?fine-grained?
features.
They treat each n-gram asa feature for, respectively, monolingual capitaliza-tion and language modeling.
Feature weights tunedat a fine granularity may lead to better accuracy,but they require much more training data, and re-sult in much slower training speed, especially forlarge-scale learning problems.
Coarse-grained fea-tures enable us to efficiently get the feature valuesfrom a very large training corpus, and quickly tunethe weights on small development sets.
For exam-ple, we can train a bilingual capitalization model ona 70 million-word corpus in several hours with thecoarse-grained features presented above, but in sev-eral days with fine-grained n-gram count features.4.4 The GEN FunctionFunction GEN generates the set of case-sensitivecandidates from a lowercased token.
For exam-ple GEN(mt) = {mt, mT, Mt, MT}.
The follow-ing heuristics can be used to reduce the range ofGEN.
The returned set of GEN on a lower-cased to-ken w is the union of: (i) {w,AU(w), IU(w)}, (ii){v|v is seen in training data and AL(v) = w},and (iii) {F?m,k|AL(F?m,k) = AL(w)}.
The heuris-tic (iii) is designed to provide more candidates forw when it is translated from a very strange inputword F?m,k in the F phrase F?m that is aligned to thephrase that w is in.
This heuristic creates good capi-talization candidates for the translation of URLs, filenames, and file paths.5 Generating Phrase-Aligned TrainingDataTraining the bilingual capitalization model requiresa bilingual corpus with phrase alignments, which areusually produced from a phrase aligner.
In practice,the task of phrase alignment can be quite computa-tionally expensive as it requires to translate the en-tire training corpus; also a phrase aligner is not al-ways available.
We therefore generate the trainingdata using a na?
?ve phrase aligner (NPA) instead ofresorting to a real one.The input to the NPA is a word-aligned bilingualcorpus.
The NPA stochastically chooses for eachsentence pair one segmentation and phrase align-ment that is consistent with the word alignment.
Analigned phrase pair is consistent with the word align-ment if neither phrase contains any word aligningto a word outside the other phrase (Och and Ney,2004).
The NPA chunks the source sentence intophrases according to a probabilistic distribution oversource phrase lengths.
This distribution can be ob-tained from the trace output of a phrase-based MT5Entire Corpus (#W) Test-BLEULanguages Training Dev Test-Prec.
(#sents)E?F (IT) 62M 13K 15K 763F?E (news) 144M 11K 22K 241C?E (news) 50M 8K 17K 919Table 2: Corpora used in experiments.decoder on a small development set.
The NPA hasto retry if the current source phrase cannot find anyconsistent target phrase.
Unaligned target words areattached to the left phrase.
Heuristics are employedto prevent the NPA from not coming to a solution.Obviously, the NPA is a special case of the phraseextractor in (Och and Ney, 2004) in that it considersonly one phrase alignment rather than all possibleones.Unlike a real phrase aligner, the NPA need notwait for the training of the translation model to fin-ish, making it possible for parallelization of transla-tion model training and capitalization model train-ing.
However, we believe that a real phrase alignermay make phrase alignment quality higher.6 Experiments6.1 SettingsWe conducted capitalization experiments on threelanguage pairs: English-to-French (E?F) with abilingual corpus from the Information Technology(IT) domain; French-to-English (F?E) with a bilin-gual corpus from the general news domain; andChinese-to-English (C?E) with a bilingual corpusfrom the general news domain as well.
Each lan-guage pair comes with a training corpus, a develop-ment corpus and two test sets (see Table 2).
Test-Precision is used to test the capitalization precisionof the capitalizer on well-formed sentences drawnfrom genres similar to those used for training.
Test-BLEU is used to assess the impact of our capitalizeron end-to-end translation performance; in this case,the capitalizer may operate on ungrammatical sen-tences.
We chose to work with these three languagepairs because we wanted to test our capitalizationmodel on both English and French target MT sys-tems and in cases where the source language has nocase information (such as in Chinese).We estimated the feature functions, such as thelog probabilities in the language model, from thetraining set.
Kneser-Ney smoothing (Kneser andNey, 1995) was applied to features fLM, fcap?t1,and fcap?tag?t1.
We trained the feature weights ofthe CRF-based bilingual capitalization model usingthe development set.
Since estimation of the featureweights requires the phrase alignment information,we efficiently applied the NPA on the developmentset.We employed two LM-based capitalizers as base-lines for performance comparison: a unigram-basedcapitalizer and a strong trigram-based one.
Theunigram-based capitalizer is the usual baseline forcapitalization experiments in previous work.
Thetrigram-based baseline is similar to the one in(Lita et al, 2003) except that we used Kneser-Neysmoothing instead of a mixture.A phrase-based SMT system (Marcu and Wong,2002) was trained on the bitext.
The capitalizerwas incorporated into the MT system as a post-processing module ?
it capitalizes the lowercasedMT output.
The phrase boundaries and alignmentsneeded by the capitalizer were automatically in-ferred as part of the decoding process.6.2 BLEU and PrecisionWe measured the impact of our capitalization modelin the context of an end-to-end MT system usingBLEU (Papineni et al, 2001).
In this context, thecapitalizer operates on potentially ill-formed, MT-produced outputs.To this end, we first integrated our bilingual capi-talizer into the phrase-based SMT system as a post-processing module.
The decoder of the MT sys-tem was modified to provide the capitalizer withthe case-preserved source sentence, the lowercasedtranslation, and the phrase boundaries and theiralignments.
Based on this information, our bilin-gual capitalizer recovers the case information of thelowercased translation, outputting a capitalized tar-get sentence.
The case-restored machine transla-tions were evaluated against the target test-BLEUset.
For comparison, BLEU scores were also com-puted for an MT system that used the two LM-basedbaselines.We also assessed the performance of our capital-izer on the task of recovering case information forwell-formed grammatical texts.
To this end, we usedthe precision metric that counted the number of cor-6rectly capitalized words produced by our capitalizeron well-formed, lowercased inputprecision = #correctly capitalized words#total words (9)To obtain the capitalization precision, we im-plemented the capitalizer as a standalone program.The inputs to the capitalizer were triples of a case-preserved source sentence, a lowercased target sen-tence, and phrase alignments between them.
Theoutput was the case-restored version of the targetsentence.
In this evaluation scenario, the capitalizeroutput and the reference differ only in case infor-mation ?
word choices and word orders betweenthem are the same.
Testing was conducted on Test-Precision.
We applied the NPA to the Test-Precisionset to obtain the phrases and their alignments be-cause they were needed to trigger the features intesting.
We used a Test-Precision set that was dif-ferent from the Test-BLEU set because word align-ments were by-products only of training of transla-tion models on the MT training data and we couldnot put the Test-BLEU set into the MT trainingdata.
Rather than implementing a standalone wordaligner, we randomly divided the MT training datainto three non-overlapping sets: Test-Precision set,CRF capitalizer training set and dev set.6.3 ResultsThe performance comparisons between our CRF-based capitalizer and the two LM-based baselinesare shown in Table 3 and Table 4.
Table 3 showsthe BLEU scores, and Table 4 shows the precision.The BLEU upper bounds indicate the ceilings that aperfect capitalizer can reach, and are computed byignoring the case information in both the capitalizeroutputs and the reference.
Obviously, the precisionupper bounds for all language pairs are 100%.The precision and end-to-end BLEU based com-parisons show that, for European language pairs, theCRF-based bilingual capitalization model outper-forms significantly the strong LM-based baseline.We got more than one BLEU point improvement onthe MT translation between English and French, a34% relative reduction in capitalization error rate forthe French-to-English language pair, and a 42% rel-ative error rate reduction for the English-to-Frenchlanguage pair.
These results show that source lan-guage information provides significant help for cap-italizing machine translation outputs.
The resultsalso show that when the source language does nothave case, as in Chinese, the bilingual model equalsa monolingual one.The BLEU difference between the CRF-basedcapitalizer and the trigram one were larger thanthe precision difference.
This indicates that theCRF-based capitalizer performs much better on non-grammatical texts that are generated from an MTsystem due to the bilingual feature of the CRF capi-talizer.6.4 Effect of Training Corpus SizeThe experiments above were carried out on largedata sets.
We also conducted experiments to exam-ine the effect of the training corpus size on capital-ization precision.
Figure 4 shows the effects.
Theexperiment was performed on the E?F corpus.
Thebilingual capitalizer performed significantly betterwhen the training corpus size was small (e.g., un-der 8 million words).
This is common in many do-mains: when the training corpus size increases, thedifference between the two capitalizers decreases.7 ConclusionsIn this paper, we have studied how to exploit bilin-gual information to improve capitalization perfor-mance on machine translation output, and evaluatedthe improvement over traditional methods that useonly monolingual language models.We first presented a probabilistic bilingual cap-italization model for capitalizing machine transla-tion outputs using conditional random fields.
Thismodel exploits bilingual capitalization knowledge aswell as monolingual information.
We defined a se-ries of feature functions to incorporate capitalizationknowledge into the model.We then evaluated our CRF-based bilingual capi-talization model both on well-formed texts in termsof capitalization precision, and on possibly ungram-matical end-to-end machine translation outputs interms of BLEU scores.
Experiments were per-formed on both French and English target MT sys-tems with large-scale training data.
Our experimen-tal results showed that the CRF-based bilingual cap-7BLEU ScoresTranslation UnigramCapitalizerTrigramCapitalizerCRF-basedCapitalizerUpperBoundF?E 24.96 26.73 27.92 28.85E?F 32.63 34.66 36.10 36.17C?E 23.81 25.92 25.89 -Table 3: Impact of CRF-based capitalizer on end-to-end translation performance compared with two LM-based baselines.Capitalization Precision (%)Translation UnigramcapitalizerTrigramcapitalizerCRF-basedcapitalizerF?E 94.03 98.79 99.20E?F 91.52 98.47 99.11C?E 90.77 96.40 96.76Table 4: Impact of CRF-based capitalizer on capitalization precision compared with two LM-based baselines.100999897969594939264.032.016.08.04.02.01.00.50.20.1Precision(x%)Training Corpus Size (MWs)CRF-based capitalizerLM-based capitalizerFigure 4: Capitalization precision with respect to size of train-ing corpus.
LM-based capitalizer refers to the trigram-basedone.
Results were on E?F corpus.italization model performs significantly better than astrong baseline, monolingual capitalizer that uses atrigram language model.In all experiments carried out at Language Weaverwith customer (or domain specific) data, MT sys-tems trained on lowercased data coupled with theCRF bilingual capitalizer described in this paperconsistently outperformed both MT systems trainedon lowercased data coupled with a strong monolin-gual capitalizer and MT systems trained on mixed-cased data.ReferencesCiprian Chelba and Alex Acero.
2004.
Adaptation of maxi-mum entroy capitalizer: Little data can help a lot.
In Pro-ceedings of the 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP), Barcelona, Spain.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussian priorfor smoothing Maximum Entropy models.
Technical ReportCMUCS-99-108, Carnegie Mellon University.William A. Gale, Kenneth W. Church, and David Yarowsky.1994.
Discrimination decisions for 100,000-dimensionalspaces.
In Current issues in computational linguistics.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a Translation Rule?
In Proceedings of the HumanLanguage Technology Conference and the North AmericanAssociation for Computational Linguistics (HLT-NAACL),Boston, Massachusetts.Ji-Hwan Kim and Philip C. Woodland.
2004.
Automatic capi-talization generation for speech input.
Computer Speech andLanguage, 18(1):67?90, January.Reinhard Kneser and Hermann Ney.
1995.
Improved backing-off for m-gram language modeling.
In Proceedings of the In-ternational Conference on Acoustics, Speech, and SignalProcessing (ICASSP) 1995, pages 181?184, Detroit, Michi-gan.
IEEE.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.Conditional random fields: Probabilistic models for segmen-tation and labeling sequence data.Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and NandaKambhatla.
2003. tRuEcasIng.
In Proceedings of the 40thAnnual Meeting of the Association for Computational Lin-guistics (ACL), Sapporo, Japan, July.Daniel Marcu and William Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.
In Pro-ceedings of the 2002 Conference on Empirical Methods inNatural Language Processing (EMNLP), Philadelphia, PA.A.
Mikheev.
1999.
A knowledge-free method fro capitalizedword disambiguation.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguistics(ACL), College Park, Maryland, June.Franz Och and Hermann Ney.
2004.
The alignment templateapproach to statistical machine translation.
ComputationalLinguistics, 30(4).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2001.
BLEU: A method for automatic evaluationof Machine Translation.
Technical Report RC22176, IBM,September.Brian Roark, Murat Saraclar, Michael Collins, and Mark John-son.
2004.
Discriminative language modeling with condi-tional random field and the perceptron algorithm.
In Pro-ceedings of the 42nd Annual Meeting of the Association forComputational Linguistics (ACL), Barcelona, Spain.8
