NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 75?84,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsA Hybrid System for Spanish Text SimplificationStefan BottUniversitat Pompeu FabraC/ Tanger, 122-140Barcelona, SpainHoracio SaggionUniversitat Pompeu FabraC/ Tanger, 122-140Barcelona, SpainDavid FigueroaAsi-softC/ Albasanz 76Madrid, SpainAbstractThis paper addresses the problem of automatictext simplification.
Automatic text simplifica-tions aims at reducing the reading difficultyfor people with cognitive disability, amongother target groups.
We describe an automatictext simplification system for Spanish whichcombines a rule based core module with a sta-tistical support module that controls the ap-plication of rules in the wrong contexts.
Oursystem is integrated in a service architecturewhich includes a web service and mobile ap-plications.1 IntroductionAccording to the Easy-to-Read Foundation at least5% of the world population is functional illiteratedue to disability or language deficiencies.
Easyaccess to digital content for the intellectual dis-abled community or people with difficulty in lan-guage comprehension constitutes a fundamental hu-man right (United Nations, 2007); however it is farfrom being a reality.
Nowadays there are severalmethodologies that are used to make texts easy toread in such ways that they enable their reading bya target group of people.
These adapted or simpli-fied texts are currently being created manually fol-lowing specific guidelines developed by organiza-tions, such as the Asociaci?n Facil Lectura,1 amongothers.
Conventional text simplification requires aheavy load of human resources, a fact that not onlylimits the number of simplified digital content ac-1http://www.lecturafacil.netcessible today but also makes practically impossi-ble easy access to already available (legacy) mate-rial.
This barrier is especially important in contextswhere information is generated in real time ?
news?
because it would be very expensive to manuallysimplify this type of ?ephemeral?
content.Some people have no problem reading compli-cated official documents, regulations, scientific lit-erature etc.
while others find it difficult to under-stand short texts in popular newspapers or maga-zines.
Even if the concept of "easy-to-read" is notuniversal, it is possible in a number of specific con-texts to write a text that will suit the abilities of mostpeople with literacy and comprehension problems.This easy-to-read material is generally characterizedby the following features:?
The text is usually shorter than a standard textand redundant content and details which do notcontribute to the general understanding of thetopic are eliminated.2 It is written in variedbut fairly short sentences, with ordinary words,without too many subordinate clauses.?
Previous knowledge is not taken for granted.Backgrounds, difficult words and context areexplained but in such a way that it does not dis-turb the flow of the text.?
Easy-to-read is always easier than standard lan-guage.
There are differences of level in differ-2Other providers, for example the Simple English Wikipedia(http://simple.wikipedia.org) explicitly oppose tocontent reduction.
The writing guidelines for the Simple En-glish Wikipedia include the lemma "Simple does not meanshort".75ent texts, all depending on the target group inmind.Access to information about culture, literature,laws, local and national policies, etc.
is ofparamount importance in order to take part in so-ciety, it is also a fundamental right.
The United Na-tions (2007) "Convention on the Rights of Personswith Disabilities" (Article 21) calls on governmentsto make all public information services and docu-mentation accessible for different groups of peoplewith disabilities and to encourage the media - tele-vision, radio, newspapers and the internet - to maketheir services easily available to everyone.
Only afew systematic efforts have been made to addressthis issue.
Some governments or organisations forpeople with cognitive disability have translated doc-uments into a language that is "easy to read", how-ever, in most countries little has been done and orga-nizations and people such as editors, writers, teach-ers and translators seldom have guidelines on how toproduce texts and summaries which are easy to readand understand.1.1 Automatic Text SimplificationAutomatic text simplification is the process bywhich a computer transforms a text for a particularreadership into an adapted version which is easier toread than the original.
It is a technology which canassist in the effort of making information more ac-cessible and at the same time reduce the cost associ-ated with the mass production of easy texts.
Our re-search is embedded within the broader context of theSimplext project (Saggion et al, 2011).3 It is con-cerned with the development of assistive text simpli-fication technology in Spanish and for people withcognitive disabilities.
The simplification system iscurrently under development.
Some of the compo-nents for text simplification are operational, whileother parts are in a development stage.
The sys-tem is integrated in a larger service hierarchy whichmakes it available to the users.
This paper concen-trates on syntactic simplification, as one specific as-pect, which is a central, but not the only aspect ofautomatic text simplification.
More concretely, wepresent a syntactic simplification module, which is3http://www.simplext.esbased on a hybrid technique: The core of the sys-tem is a hand-written computational grammar whichreduces syntactic complexity and the application ofthe rules in this grammar is controlled by a statisti-cal support system, which acts as a filter to preventthe grammar from manipulating wrong target struc-tures.
Section 2 describes related work, in the con-text of which our research has been carried out.
Sec-tion 3 justifies the hybrid approach we have takenand section 4 describes our syntactic simplificationmodule, including an evaluation of the grammar andthe statistical component.
Finally, in section 5 weshow how our simplification system is integrated ina larger architecture of applications and services.2 Related WorkAs it has happened with other NLP tasks, the firstattempts to tackle the problem of text simplifica-tion were rule-based (Chandrasekar et al, 1996;Siddharthan, 2002).
In the last decade the focushas been gradually shifting to more data driven ap-proaches (Petersen and Ostendorf, 2007) and hybridsolutions.
The PorSimples (Alu?sio et al, 2008;Gasperin et al, 2010) project used a methodologywhere a parallel corpus was created and this cor-pus was used to train a decision process for sim-plification based on linguistic features.
Siddharthan(2011) compares a rule-based simplification systemwith a simplification system based on a general pur-pose generator.Some approaches have concentrated on specificconstructions which are especially hard to under-stand for readers with disabilities (Carroll et al,1998; Canning et al, 2000), others focused on textsimplification as a help for other linguistic taskssuch as the simplification of patent texts (Mille andWanner, 2008; Bouayad-Agha et al, 2009).
Re-cently the availability of larger parallel or quasi-parallel corpora, most notably the combination ofthe English and the Simple English Wikipedia,has opened up new possibilities for the use ofmore purely data-driven approaches.
Zhu et al(2010), for example, use a tree-based simplificationmodel which uses techniques from statistical ma-chine translation (SMT) with this data set.A recent work, which is interesting because ofits purely data-driven setup, is Coster and Kauchak76(2011).
They use standard software from the fieldof statistical machine translation (SMT) and applythese to the problem of text simplification.
Theycomplement these with a deletion component whichwas created for the task.
They concentrate on fourtext simplification operations: deletion, rewording(lexical simplification), reordering and insertions.Text simplification is explicitly treated in a simi-lar way to sentence compression.
They use stan-dard SMT software, Moses (Koehn et al, 2007) andGIZA++ (Och and Ney, 2000), and define the prob-lem as translating from English (represented by theEnglish Wikipedia) to Simple English (representedby the Simple English Wikipedia).
The translationprocess can then imply any of the four mentionedoperations.
They compared their approach to var-ious other systems, including a dedicated sentencecompression system (Knight and Marcu, 2002) andshow that their system outperforms the others whenevaluated on automatic metrics which use humancreated reference text, including BLEU (Papineni etal., 2002).
Their problem setting does, however, notinclude sentence splitting (as we will describe be-low).
Another potential problem is that the met-rics they use for evaluation compare to human ref-erences, but they do not necessarily reflect humanacceptability or grammaticality.Woodsend and Lapata (2011) use quasi-synchronous grammars as a more sophisticatedformalism and integer programming to learn totranslate from English to Simple English.
Thissystem can handle sentence splitting operationsand the authors use both automatic and humanevaluation and show an improvement over theresults of Zhu et al (2010) on the same data set, butthey have to admit that learning from parallel bi-textis not as efficient as learning from revision historiesof the Wiki-pages.
Text simplification can also beseen as a type of paraphrasing problem.
There arevarious data-driven approaches to this NLP-task(Madnani and Dorr, 2010), but they usually focus onlexical paraphrases and do not address the problemof sentence splitting, either.Such data-driven methods are very attractive, es-pecially because they are in principle language in-dependent, but they do depend on a large amount ofdata, which are not available for the majority of lan-guages.3 A Hybrid Approach to TextSimplificationThere are several considerations which lead us totake a hybrid approach to text simplification.
Firstof all there is a lack of parallel data in the case ofSpanish.
Within our project we are preparing a cor-pus of Spanish news texts (from the domain of na-tional news, international news, society and culture),consisting of 200 news text and their manually sim-plified versions.
The manual simplification is timeconsuming and requires work from specially trainedexperts, so the resulting corpus is not very big, evenif the quality is controlled and the type of data is veryspecific for our needs.
It is also very hard to findlarge amounts of parallel text from other sources.
Inorder to use data driven techniques we would requireamounts of bi-text comparable to those used for sta-tistical machine translation (SMT) and this makes itnearly impossible to approach the problem from thisdirection, at least for the time being.But there are also theoretic considerations whichmake us believe that a rule based approach is a goodstarting point for automatic text simplification.
Weconsider that there are at least four separate NLPtasks which may be combined in a text simplifica-tion setting and which may help to reduce the read-ing difficulty of a text.
They all have a different na-ture and require different solutions.?
Lexical simplification: technical terms, for-eign words or infrequent lexical items make atext more difficult to understand and the taskconsists in substituting them with counterpartswhich are easier to understand.?
Reduction of syntactic complexity: long sen-tences, subordinate structure and especially re-cursive subordination make a text harder to un-derstand.
The task consists in splitting longsentences in a series of shorter ones.?
Content reduction: redundant informationmake a text harder to read.
The task consistsin identifying linguistic structures which can bedeleted without harming the text grammatical-ity and informativeness in general.
This task issimilar to the tasks of automatic summarizationand sentence compression.77?
Clarification: Explaining difficult concepts re-duces the difficulty of text understanding.
Thetask consists in identifying words which needfurther clarification, selecting an appropriateplace for the insertion of a clarification or adefinition and finding an appropriate text unitwhich actually clarifies the concept.There is at least one task of the mentioned whichdoes not fully correspond to an established machinelearning paradigm in NLP, namely the reduction ofsyntactic complexity.
Consider the example (1), anexample from our corpus; (2) is the simplificationwhich was produced by our system.
(1) Se trata de un proyecto novedoso y pioneroque coordina el trabajo de seis concejal?as,destacando las delegaciones municipales deEducaci?n y Seguridad .
.
.
"This is a new and pioneering project thatcoordinates the work of six councillors,highlighting the municipal delegations Ed-ucation and Safety .
.
.
"(2) Se trata de un proyecto novedoso y pionero ,destacando las delegaciones municipales deEducaci?n y Seguridad .
.
.Este proyecto coordina el trabajo de seisconcejal?as.
"This is a new and pioneering project, high-lighting the municipal delegations Educa-tion and Safety .
.
.This project coordinates the work of sixcouncillors.
"What we can observe here is a split operationwhich identifies a relative clause, cuts it out of thematrix clause and converts it into a sentence of itsown.
In the process the relative pronoun is deletedand a subject phrase (este proyecto / this project) hasbeen added, whose head noun is copied from the ma-trix clause.
It is tempting to think that converting asource sentence A in a series of simplified sentences{b1, .
.
.
, bn} is a sort of translation task, and a verytrivial one.
In part this is true: most words translateto a word which is identical in its form and they hap-pen to appear largely in the same order.
The difficultpart of the problem is that translation is usually anoperation from sentence to sentence, while here theproblem setting is explicitly one in which one inputunit produces several output units.
This also affectsword alignment: in order to find the alignment forthe word proyecto in (1) the alignment learner hasto identify the word proyecto in two sentences in(2).
The linear distance between the two instances ofthis noun is considerable and the sentences in whichtwo alignment targets occur are not even necessarilyadjacent.
In addition, there may be multiple occur-rences of the same word in the simplified text whichare not correct targets; the most apparent case arefunctional words, but even words which are gener-ally infrequent may be used repeatedly in a smallstretch of text if the topic requires it (in this para-graph, for example, the word translation occurs 4times and the word sentence 5 times).
While a ma-chine can probably learn the one-to-may translationswhich are needed here, a non-trivial extension of themachine-translation setting is needed and the learn-ing problem needs to be carefully reformulated.
Ap-plying standard SMT machinery does not seem totruly address the problem of syntactic simplification.In fact, some approaches to SMT try use text simpli-fication as a pre-process for translation; for exam-ple Poornima et al (2011) apply a sentence splittingmodule in order to improve translation quality.On the other hand, other sub-task mentionedabove can be treated with data driven methods.
Lex-ical simplification requires the measurement of lex-ical similarity, combined with word sense disam-biguation.
Content reduction is very similar to ex-tractive summarization or sentence compression andthe insertion of clarifications can be broken downinto three learnable steps: identification of difficultwords, finding an insertion site and choosing a suit-able definition for the target word.4 Syntactic SimplificationWe are developing a text simplification systemwhich will integrate different simplification mod-ules, such as syntactic simplification, lexical simpli-fication (Drndarevic and Saggion, 2012) and contentreduction.
At the moment the most advanced mod-ule of this system is the one for syntactic simplifica-tion.
In (Bott et al, 2012) we describe the function-ing of the simplification grammar in more detail.For the representation of syntactic structures we78use dependency trees.
The trees are produced by theMate-tools parser (Bohnet, 2009) and the syntacticsimplification rules are developed within the MATEframework (Bohnet et al, 2000).
MATE is a graphtransducer which uses hand written grammars.
Forgrammar development we used a development cor-pus of 282 sentences.The grammar mainly focuses on syntactic simpli-fication and, in particular, sentence splitting.
Thetypes of sentence splitting operations we treat at themoment are the following ones:?
Relative clauses: we distinguish between sim-ple relative clauses which are only introducedby a bare relative pronoun (e.g.
a questionwhich is hard to answer) and complex relativeclauses which are introduced by a prepositionand a relative pronoun (e.g.
a question to whichthere is no answer)?
Gerundive constructions and participle con-structions (e.g.
the elections scheduled for nextNovember)?
Coordinations of clauses (e.g.
[the problem isdifficult] and [there is probably no right an-swer]) and verb phrases (e.g.
The problem [isdifficult] and [has no easy solution]).?
Coordinations of objects clauses (e.g.
.
.
.
to getclose to [the fauna], [the plant life] and [theculture of this immense American jungle re-gion])We carried out a evaluation of this grammar,which is resumed in Table 1.
This evaluation lookedat the correctness of the output.
Many of the er-rors were due to wrong parse trees and and thegrammar produced an incorrect output because theparsed input was already faulty.
In the case of rel-ative clauses nearly 10% occurred because of thisand in the case of gerundive construction 37% ofthe errors belonged into that category.
We alsofound that many of the syntactic trees are ambigu-ous and cannot be disambiguated only on the basisof morphosyntactic information.
A particular caseof such ambiguity is the distinction between restric-tive and non-restrictive relative clauses.
Only non-restrictive clauses can be turned into separate sen-tences and the distinction between the two types isusually not marked by syntax in Spanish4.
Erroranalysis showed us that 57.58% of all the errors re-lated to relative clauses were due to this distinction.A further 18.18% of the error occurred because thegrammar wrongly identified complement clauses asrelative clauses (in part because of previous parsingerrors).For this reason, and according to our general phi-losophy to apply data-driven approaches wheneverpossible, we decided to apply a statistical filter inorder to filter out cases where the applications ofthe simplification rules lead to incorrect results.
Fig-ure 1 shows the general architecture of the automaticsimplification system, including the statistical filter.The nucleus of the system in its current state is thesyntactic simplification system, implemented as aMATE grammar, which consists of various layers.Original Text ParserMarking ofTargetStructureStatisitcalFilteringMate-ToolsSimplified TextApplication of Structural Changes MATEMate-ToolsFigure 1: The architecture of the simplification systemSyntactic simplification is carried out in threesteps: first a grammar looks for suitable target struc-tures which could be simplified.
Such structures arethen marked with an attribute that informs subse-quent levels of the grammar.
After that the statisticalfilter applies and classifies the marked target struc-tures according to whether they should be changedor not.
In a third step the syntactic manipulationsthemselves are carried out.
This can combine dele-tions, insertions and copying of syntactic nodes orsubtrees.4In English it is mandatory to place non-restrictive relativeclauses between commas, even if many writers do not respectthis rule, but in Spanish comma-placement is only a stylisticrecommendation.79Operation Precision Recall FrequencyRelative Clauses (all types) 39.34% 0.80% 20.65%Gerundive Constructions 63.64% 20.59% 2.48%Object coordination 42.03% 58.33% 7.79%VP and clause coordination 64.81% 50% 6.09%Table 1: Percentage of right rule application and frequency of application (percentage of sentences affected) per ruletype4.1 Statistical FilteringSince the training of such filters requires a certainamount of hand-annotated data, so far we only im-plemented filters for simple and complex relativeclauses.
These filters are implemented as binaryclassifiers.
For each structure which the grammarcould manipulate, the classifier decides if the sim-plification operation should be carried out or not.In this way, restrictive relative clauses, comple-ment clauses and other non-relative clause construc-tions should be retained by the filter and only non-restrictive relative clauses are allowed to pass.For the training of the filters we hand annotateda selection of sentences which contained the rele-vant type of relative clauses (150 cases for simpleand 116 for complex).
The training examples weretaken from news texts published in the on-line edi-tion of an established Spanish newspaper.
The stylein which these news were written was notably differ-ent from the news texts of the corpus we are devel-oping in within our project, in that they were muchmore complex and contained more cases of recursivesubordination.
The annotators reported that some ofthe sentences had to be re-read in order to fully un-derstand them; this is not uncommon in this type ofnews which may contain opinion columns and in-depth comments.In our classification framework we consider oneset of contextual features arising from tokens sur-rounding the target structure to be classified5 ?
therelative pronoun marked by the simplification iden-tification rules.
This set is composed of, among oth-ers, the position of the target structure in the sen-tence; the parts of speech tags of neighbour token;the depth of the target in a dependency tree; the de-pendency information to neighbour tokens, etc.Linguistic intuitions such as specific construc-5A 5 words window to the left and to the right.tions which, according to the Spanish grammar,could be considered as indicating that the simplifi-cation can or cannot take place.
These features arefor example: the presence of a definite or indefinitearticle; the presence of a comma in the vicinity ofthe pronoun; specific constructions such as ya que(since), como que (as), etc.
where que is not relativepronoun; context where que is used as a comparativesuch as in m?s....que (more... than); contexts whereque is introducing a subordinate complement as inquiero que (I want that ...); etc.
While some of thesefeatures should be implemented relying on syntacticanalysis we have relied for the experiments reportedhere on finite state approximations implementing allfeatures in regular grammars using the GATE JAPElanguage (Cunningham et al, 2000; Maynard et al,2002).
For other learning tasks such as decidingfor the splitting of coordinations or the separationof participle clauses we design and implement spe-cific features based on intuitions; contextual featuresremain the same for all problems.The classification framework is implemented inthe GATE system, using the machine learning li-braries it provides (Li et al, 2005).
In particular,we have used the Support Vector Machines learn-ing libraries (Li and Shawe-Taylor, 2003) whichhave given acceptable classification results in otherNLP tasks.
The framework allows us to run cross-validation experiments as well as training and test-ing.Table 2 shows the performance of the statisticalfilter in isolation, i.e.
the capacity of the filter aloneto distinguish between good and bad target struc-tures for simplification operations.
The in-domainperformance was obtained by a ten-fold cross clas-sification of the training data.
The out-of-domainevaluation was carried out over news texts from ourown corpus, the same collection we used for the80Figure 2: A simplified news text produced by the serviceon a tablet computer running Androidevaluation of the grammar and the combination ofthe grammar with the statistical filter.
The perfor-mance is given here as the overall classification re-sult.
Table 3 shows the performance of the grammarwith and without application of the filter.64.2 DiscussionWe can observe that the statistical filters have aquite different performance when they are appliedin-domain and out-of-domain (cf.
Table 2), espe-cially in the case of simple relative clauses.
Weattribute this to the fact that the style of the textswhich we used for training is much more compli-cated than the texts which we find in our own cor-pus.
The annotators commented that many relativeclauses could not turned into separate sentences be-cause of the overall complexity of the sentence.
Thisproblem seems to propagate into the performanceof the combination of the grammar with the filter(cf.
Table 3).
The precision improves with filter-ing, but the recall drops even more.
Again, we sus-pect that the filter is very restrictive because in thetraining data many relative clauses were not separa-ble, due to the overall sentence complexity which ismuch lesser in the corpus from which the test datawas taken.
For the near future we plan to repeat6The results here are not fully comparable to Table 1, be-cause in order to evaluate the filter, we did not consider parseerrors, as we did in the previous evaluation.Este mi?rcoles las personas con Sindrome de Down celebransi d?a mundial .
En Espa?a , hay m?s de 34 .000 personas conesta discapacidad .
esta discapacidad ocurre en uno de cada800 nacimientos .El S?drome de Down es un trastorno gen?tico .
este trastornocausa la presencia de una copia extra del cromosoma 21 envez de los dos habituales ( trisom?a del par 21 ) .
Laconsecuencia es un grado variable de discapacidad cognitiva yunos r?sgos f?sicos particulares y reconocibles .Se trata de la causa m?s frecuente de discapacidad cognitivaps?quica cong?nita y debe su nombre a John Langdon HaydonDown .
este Landgdon fue el primero en describir estaalteraci?n gen?tica en 1866 .
Siegue sin conocerse conexactitud las causas .
estas causas provocan el excesocromos?mico , a?nque se relaciona estad?stica mente conmadres de m?s de 35 a?os .Table 4: The simplified text shown in figure2the experiment with annotated data which is moresimilar to the test set.
The performance in the caseof complex relative clauses is much better.
We at-tribute the difference between simple and complexrelative clauses to the fact that the complex construc-tions cannot be confounded with other, non-relative,constructions, while in the case of the simple typethis danger is considerable.
The somewhat unre-alistic value of 100% is a consequence of the factthat in the part of the corpus we annotated complexrelative clauses were not very frequent.
We tooksome additional cases from our corpus into consider-ation, evaluating more cases from the corpus wherethe corresponding rule was applied7 and the valuedropped to slightly over 90%.5 Integration of the Simplification Systemin ApplicationsAs we have mentioned in the introduction, our textsimplification system is integrated in a larger serviceand application setting.
Even if some modules of thesystem must still be integrated, we have an operativeprototype which includes a mobile application and aweb service.In the context of the Simplext project two mo-bile applications have been developed.
The first oneruns on iOS (developed by Apple Inc. for its de-vices: Iphone, Ipad and Ipod touch), and the otherone on Android (developed by Google, included inmany different devices).
These applications allow7For these cases we could not calculate recall because thiswould have implied a more extensive annotation of all the sen-tences of the part of the corpus from which they were taken.81Operation Precision Recall F-scoreSimple Relative Clauses (in domain) 85.41% 86.77% 86.06%Complex Relative Clauses (in domain) 70.88% 71.33% 71.10 %Simple Relative Clauses (out of domain) 76.35% 76.35% 76.35%Complex Relative Clauses (out of domain) 90.48% 85.71% 88.10%Table 2: The performance of the statistical filter in isolationOperation Precision Recall F-scoreSimple Relative Clauses (Grammar) 47.61% 95.24% 71.43%Complex Relative Clauses (Grammar) 62.50% 55.56% 59.02%Simple Relative Clauses (Grammar + Filter) 59.57% 66.67% 63.12%Complex Relative Clauses (Grammar + Filter) 100% 55.56% 77.78%Table 3: The performance of grammar and the statistical filter togetherto read news feeds (RSS / Atom) from differentsources through a proxy that provide the languagesimplification mechanism.
The mobile applicationsare basically RSS/Atom feed readers, with simpli-fication capabilities (provided by the service layer).Both applications work the same way and allow tothe user functionalities as keeping a list of favouritefeeds, adding and removing feeds, marking contentas favourite and showing the simplified and origi-nal versions of the content.
Also a web service wascreated, which works in a similar way for RSS andAtom feeds and allows to simplify the text portionof other publicly available websites.Figure 2 shows a screen capture of the mobile ap-plication running in a Android tablet, displaying asimplification example of a text taken from a newswebsite.
The display text of this image is reproducedin Table 4 for better readability.
The text itself is toolong for us to provide a translation, but it can be seenthat many sentences have been split.
Also a series ofminor problems can be seen, which we will resolvein the near future: The first word of a sentence is stillin lower case and the head noun of the named en-tity John Langdon Haydon Down was not correctlyidentified.6 ConclusionsAutomatic text simplification is an Assistive Tech-nology which help people with cognitive disabilitiesto gain access to textual information.
In this paperwe have presented a syntactic simplification moduleof a automatic text simplification system which isunder development.
We have presented argumentsfor the decision of using a hybrid strategy whichcombines a rule-based grammar with a statisticalsupport component, we have described the imple-mentation of this idea and have given a contrastiveevaluation of the grammar with and without statisti-cal support.
The simplification system we describedhere is integrated in a user-oriented service architec-ture with mobile applications and web services.
Infuture work we will further enhance the system andintegrate new components dedicated to other simpli-fication aspects, such as lexical simplification andcontent reduction.AcknowledgementsThe research described in this paper arises froma Spanish research project called Simplext: An au-tomatic system for text simplification (http://www.simplext.es).
Simplext is led by Tech-nosite and partially funded by the Ministry of In-dustry, Tourism and Trade of the Government ofSpain, by means of the National Plan of ScientificResearch, Development and Technological Innova-tion (I+D+i), within strategic Action of Telecom-munications and Information Society (Avanza Com-petitiveness, with file number TSI-020302-2010-84).
We are grateful to fellowship RYC-2009-04291from Programa Ram?n y Cajal 2009, Ministerio deEconom?a y Competitividad, Secretar?a de Estado deInvestigaci?n, Desarrollo e Innovaci?n, Spain.82ReferencesSandra M. Alu?sio, Lucia Specia, Thiago Alexan-dre Salgueiro Pardo, Erick Galani Maziero, and Re-nata Pontin de Mattos Fortes.
2008.
Towards brazil-ian portuguese automatic text simplification systems.In ACM Symposium on Document Engineering, pages240?248.Bernd Bohnet, Andreas Langjahr, and Leo Wanner.2000.
A development environment for MTT-basedsentence generators.
Revista de la Sociedad Espa?olapara el Procesamiento del Lenguaje Natural.Bernd Bohnet.
2009.
Efficient parsing of syntacticand semantic dependency structures.
In Proceed-ings of the Conference on Natural Language Learning(CoNLL), pages 67?72, Boulder, Colorado.
Associa-tion for Computational Linguistics.Stefan Bott, Horacio Saggion, and Simon Mille.
2012.Text simplification tools for spanish.
In Proceedingsof the LREC-2012, Estambul, Turkey.Nadjet Bouayad-Agha, Gerard Casamayor, Gabriela Fer-raro, and Leo Wanner.
2009.
Simplification of patentclaim sentences for their paraphrasing and summariza-tion.
In FLAIRS Conference.Yvonne Canning, John Tait, Jackie Archibald, and RosCrawley.
2000.
Cohesive generation of syntacticallysimplified newspaper text.
In TSD, pages 145?150.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof english newspaper text to assist aphasic readers.
InIn Proc.
of AAAI-98 Workshop on Integrating ArtificialIntelligence and Assistive Technology, pages 7?10.Raman Chandrasekar, Christine Doran, and BangaloreSrinivas.
1996.
Motivations and methods for text sim-plification.
In COLING, pages 1041?1044.William Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof Text-To-Text Generation, Portland, Oregon.
Associ-ation for Computational Linguistics.H.
Cunningham, D. Maynard, and V. Tablan.
2000.JAPE: a Java Annotation Patterns Engine (Second Edi-tion).
Research Memorandum CS?00?10, Departmentof Computer Science, University of Sheffield, Novem-ber.Biljana Drndarevic and Horacio Saggion.
2012.
Towardsautomatic lexical simplification in spanish: an empir-ical study.
In NAACL 2012 Workshop on Predictingand Improving Text Readability for Target Reader Pop-ulations, Montreal, Canada.Caroline Gasperin, Erick Galani Maziero, and Sandra M.Alu?sio.
2010.
Challenging choices for text simplifi-cation.
In PROPOR, pages 40?50.Kevin Knight and Daniel Marcu.
2002.
Summarizationbeyond sentence extraction: a probabilistic approachto sentence compression.
Artif.
Intell., 139(1):91?107,July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Y.
Li and J. Shawe-Taylor.
2003.
The SVM withUneven Margins and Chinese Document Categoriza-tion.
In Proceedings of The 17th Pacific Asia Con-ference on Language, Information and Computation(PACLIC17), Singapore, Oct.Yaoyong Li, Katalina Bontcheva, and Hamish Cunning-ham.
2005.
Using Uneven Margins SVM and Per-ceptron for Information Extraction.
In Proceedingsof Ninth Conference on Computational Natural Lan-guage Learning (CoNLL-2005).N.
Madnani and B.J.
Dorr.
2010.
Generating phrasal andsentential paraphrases: A survey of data-driven meth-ods.
Computational Linguistics, 36(3):341?387.Diana Maynard, Valentin Tablan, Hamish Cunningham,Cristian Ursu, Horacio Saggion, Katalina Bontcheva,and Yorik Wilks.
2002.
Architectural Elements ofLanguage Engineering Robustness.
Journal of Nat-ural Language Engineering ?
Special Issue on Ro-bust Methods in Analysis of Natural Language Data,8(2/3):257?274.Simon Mille and Leo Wanner.
2008.
Making text re-sources accessible to the reader: The case of patentclaims.
Marrakech (Marocco), 05/2008.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
pages 440?447, Hongkong, China, Oc-tober.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Sarah E. Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analysis.
InIn Proc.
of Workshop on Speech and Language Tech-nology for Education.C.
Poornima, V. Dhanalakshmi, K.M.
Anand, and KP So-man.
2011.
Rule based sentence simplification forenglish to tamil machine translation system.
Interna-tional Journal of Computer Applications, 25(8):38?42.83H.
Saggion, E. G?mez Mart?nez, E. Etayo, A. Anula, andL.
Bourg.
2011.
Text simplification in simplext.
mak-ing text more accessible.
Procesamiento de LenguajeNatural, 47(0):341?342.Advaith Siddharthan.
2002.
An architecture for a textsimplification system.
In In LREC?02: Proceedings ofthe Language Engineering Conference, pages 64?71.Advaith Siddharthan.
2011.
Text simplification usingtyped dependencies: A comparison of the robustnessof different generation strategies.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration (ENLG), pages 2?11, September.United Nations.
2007.
Convention on therights of persons with disabilities.
http://www2.ohchr.org/english/law/disabilities-convention.htm.Kristian Woodsend and Mirella Lapata.
2011.
Learningto simplify sentences with quasi-synchronous gram-mar and integer programming.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 409?420.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation model forsentence simplification.
In Proceedings of The 23rdInternational Conference on Computational Linguis-tics, pages 1353?1361, Beijing, China, Aug.84
