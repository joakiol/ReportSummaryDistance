Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 64?74,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsBecause Syntax Does Matter: Improving Predicate-Argument StructuresParsing with Syntactic FeaturesCorentin Ribeyre?
?Eric Villemonte de la Clergerie?Djam?
Seddah?
?Alpage, INRIA?Univ Paris Diderot, Sorbonne Paris Cit?Universit?
Paris Sorbonnefirstname.lastname@inria.frAbstractParsing full-fledged predicate-argument struc-tures in a deep syntax framework requiresgraphs to be predicted.
Using the DeepBank(Flickinger et al, 2012) and the Predicate-Argument Structure treebank (Miyao and Tsu-jii, 2005) as a test field, we show howtransition-based parsers, extended to handleconnected graphs, benefit from the use oftopologically different syntactic features suchas dependencies, tree fragments, spines orsyntactic paths, bringing a much needed con-text to the parsing models, improving notablyover long distance dependencies and elidedcoordinate structures.
By confirming this pos-itive impact on an accurate 2nd-order graph-based parser (Martins and Almeida, 2014), weestablish a new state-of-the-art on these datasets.1 IntroductionFor the majority of the state-of-the-art parsers thatroutinely reach ninety percent performance plateauin capturing tree structures, the question of what nextcrucially arises.
Indeed, it has long been thoughtthat the bottleneck preventing the advent of accu-rate syntax-to-semantic interfaces lies in the qual-ity of the preceding phase of analysis: the better theparse, the better the output.
The truth is that mostof the structures used to train current parsing mod-els are degraded versions of a more informative dataset: the Wall Street journal section of the Penn tree-bank (PTB, (Marcus et al, 1993)) which is oftenstripped of its richer set of annotations (i.e.
tracesand functional labels are removed), while, for rea-sons of efficiency and availability, projective depen-dency trees are often given preference over richergraph structures (Nivre and Nilsson, 2005; Sagaeand Tsujii, 2008).
This led to the emergence of sur-face syntax-based parsers (Charniak, 2000; Nivre,2003; Petrov et al, 2006) whose output cannot bythemselves be used to extract full-fledged predicate-argument structures.
For example, control verb con-structions, it-cleft structures, argument sharing in el-lipsis coordination, etc.
are among the phenomenarequiring a graph to be properly accounted for.
Thedichotomy between what can usually be parsed withhigh accuracy and what lies in the deeper syntac-tic description has initiated a line of research de-voted to closing the gap between surface syntax andricher structures.
For most of the previous decade,the term deep syntax was used for rich parsing mod-els built upon enriched versions of a constituencytreebank, either with added HPSG or LFG annota-tion or CCG (almost) full rewrites (Miyao and Tsu-jii, 2005; Cahill et al, 2004; Hockenmaier, 2003).Its use now spreads by misnomer to models that pro-vide more abstract structures, capable of generaliz-ing classical functional labels to more semantic (in alogical view) arguments, potentially capable of neu-tralizing diathesis distinctions and of providing ac-curate predicate-argument structures.
Although thebuilding of syntax-to-semantic interface seems inex-tricably linked to an efficient parsing stage, inspira-tional works on semantic role labelling (Toutanovaet al, 2005) and more recently on broad coveragesemantic parsing (Du et al, 2014) that provide state-of-the-art results without relying on surface syntax,lead us to question the usefulness of syntactic parsesfor predicate-argument structure parsing.In this study, we investigate the impact of syn-tactic features on a transition-based graph parserby testing on two treebanks.
We take advantageof the recent release for the SemEval 2014 sharedtask on semantic dependency parsing, by Oepen et64al.
(2014) of two semantic-based treebanks, derivedfrom two HPSG resources, the DeepBank (DM,(Flickinger et al, 2012)) and the Enju?s predicate ar-gument structure (PAS, (Miyao and Tsujii, 2005)),to investigate the impact of syntactic features ona transition-based graph parser.
Our results showthat surface syntactic features significantly improvethe parsing of predicate-argument structures.
Morespecifically, we show that adding syntactic contextimproves the recognition of long distance dependen-cies and elliptical constructions.
We finally discussthe usefulness of our approach, when applied on asecond-order model based on dual decomposition(Martins and Almeida, 2014), showing that our useof syntactic features enhances this model accuracyand provides state-of-the-art performance.2 Deep Syntax and UnderspecifiedSemantic CorporaDeepBank Corpus Semantic dependency graphsin the DM Corpus are the result of a two-step simpli-fication of the underspecified logical-form meaningrepresentations, based on Minimal Recursion Se-mantic (MRS, (Copestake et al, 1995; Copestakeet al, 2005)), derived from the manually annotatedDeepBank treebank (Flickinger et al, 2012).
First,Oepen and L?nning (2006) define a conversion fromoriginal MRS formulae to variable-free ElementaryDependency Structures (EDS), which (a) maps eachpredication in the MRS logical-form meaning rep-resentation to a node in a dependency graph and (b)transforms argument relations represented by sharedlogical variables into directed dependency links be-tween graph nodes.
Then, in a second conversionstep, the EDS graphs are further reduced into strictbi-lexical form, i.e.
a set of directed, binary depen-dency relations holding exclusively between lexicalunits (Ivanova et al, 2012).
Even though both con-version steps are, by design, lossy, DM semantic de-pendency graphs present a true subset of the infor-mation encoded in the full, original MRS data set.Predicate-Argument Structure Corpus EnjuPredicate-Argument Structures (PAS Corpus) arederived from the automatic HPSG-style annotationof the Penn Treebank (Miyao and Tsujii, 2004)that was primarily used for the development of theEnju parsing system (Miyao and Tsujii, 2005).
ThePAS data set is an extraction of predicate-argumentstructures from the Enju HPSG treebank and con-tains word-to-word semantic dependencies.
Eachdependency type is made of two elements: a coarsepart-of-speech of the head predicate dependent (e.g.verb and adjective), and the argument (e.g.
ARG1and ARG2).Although both are derived from HSPG resources(a hand-crafted grammar for DM, a treebank-basedone for PAS), they differ in their core linguisticchoices (functional heads vs lexical heads, coordi-nation scheme, etc.)
leading to different views ofthe predicate argument structure for the same sen-tence (Ivanova et al, 2012).
Thus, even though bothcorpora may appear to contain a similar number ofdependency labels, as shown in Table 1, their anno-tation schemes depict a deeply divergent linguisticreality exposed by two very different distributions.In DM, 9 labels account for almost 95% of all de-pendencies whereas a label set twice as large cov-ers the same percentage for PAS, as shown in Ta-ble 2.
Furthermore, semantically empty elementsare widespread in the DeepBank (around 21.5%),compared to a low rate of 4.3% in PAS.
In otherwords, the latter is somewhat more dense and con-sequently more syntactic.
This is due to the fact thatPAS integrates markers for infinitives, auxiliaries,and most punctuation marks into its graphs, whereasDM considers them as semantically void.
DM cor-pus is clearly heading toward more semantic analy-sis while the PAS corpus aims at providing a moreabstract deep syntax analysis than regular surfacesyntax trees.
Both treebanks are used in their bi-lexical dependency formats.DM CORPUS PAS CORPUSTRAIN DEV TRAIN DEV# SENTENCES 32,389 1,614 32,389 1,614# TOKENS 742,736 36,810 742,736 36,810% VOID TOKENS 21.63 21.58 4.30 4.25# PLANAR GRAPHS 18,855 972 17,477 953# NON PLANAR 13,534 642 14,912 661# EDGES 559,975 27,779 723,445 35,573% CROSSING EDGES 4.24 4.05 5.69 4.46LABEL SET 52 36 43 40Table 1: DM and PAS treebank properties65DM LABELS % PAS LABELS %ARG1 37.89 adj_ARG1 13.46ARG2 23.08 noun_ARG1 9.54compound 11.01 prep_ARG2 9.51BV 10.39 prep_ARG1 9.37root 5.77 verb_ARG2 9.34poss 2.23 verb_ARG1 9.23-and-c 2.02 det_ARG1 9.13loc 1.38 punct_ARG1 5.23ARG3 1.21 root 4.48times 0.87 aux-ARG2 3.06mwe 0.85 aux-ARG1 3.05appos 0.72 coord-ARG2 2.35conj 0.57 coord-ARG1 2.35neg 0.47 comp-ARG1 1.85subord 0.43 conj-ARG1 1.20-or-c 0.31 poss-ARG2 0.89-but-c 0.20 poss-ARG1 0.85total 94.98 total 94.89Table 2: Breakdown of Label Statistics.Cell values in italics not counted in the DM total.3 Transition-based Graphs Parsing(?,wi|?,A) ` (?|wi, ?, A) (shift)(?|wj|wi, ?, A) ` (?|wi, ?, A ?
(wi, r, wj)) (lR)(?|wj|wi, ?, A) ` (?|wj, ?, A ?
(wj, r, wi)) (rR)(?|wj|wi, ?, A) ` (?|wj|wi, ?, A ?
(wi, r, wj)) (lA)(?|wj|wi, ?, A) ` (?|wj, wi|?,A ?
(wj, r, wi) (rA)(?|wi, ?, A) ` (?, ?,A) (pop0)Figure 1: Set of transitions for dependency graphs.Shift-reduce transition-based parsers essentiallyrely on configurations formed of a stack and a buffer,with stack transitions used to move from a configu-ration to the next one, until reaching a final config-uration.
Following K?bler et al (2009), we definea configuration by c = (?, ?,A) where ?
denotes astack of words wi, ?
a buffer of words, and A a setof dependency arcs of the form (wi, r, wj), with withe head, wjthe dependent, and r a label in someset R. As shown in Figure 1, besides the usual shiftand reduce transitions (lR & rR) of the arc-standardstrategy, we introduced the new left and right attach(lA & rA) transitions for adding new dependencies(while keeping the dependent on the stack) and apop0 transition to remove a word from the stack af-ter attachment of its dependents.
All the transitionsthat add an edge must also satisfy the condition thatthe newly created edge does not introduce a cycle orWord?1,?2,?3Lemma?1,?2,?3POS?1,?2,?3Word?1,?2Lemma?1,?2POS?1,?2,?3leftPOS?1,?2rightPOS?1,?2leftLabel?1,?2rightLabel?1,?2a d12d?11Table 3: Baseline features for the parser.X?i, .
.
.
, ?jstands for X?i, .
.
.
, X?j.multiple edges between the same pair of nodes.
It isto be noted that the pop0 action may also be used toremove words with no heads.We base our work on the the DAG parser ofSagae and Tsujii (2008) (henceforth S&T) whichwe extended with the set of actions displayed above(Figure 1) to cope with partially connected planargraphs, and we gave it the ability to take advantageof an extended set of features.
Finally, for efficiencyreasons (memory consumption and speed), we re-placed the original Maxent model with an averagedstructured perceptron (Freund and Schapire, 1999;Collins, 2002).4 Feature Design4.1 Baseline FeaturesWe define Word?i(resp.
Lemma?iand POS?i) asthe word (resp.
lemma and part-of-speech) at posi-tion i in the queue.
The same goes for ?i, whichis the position i in the stack.
Let di,jbe the dis-tance between Word?iand Word?j.
We also defined?i,j, the distance between Word?iand Word?j.
Inaddition, we define leftPOS?i(resp.
leftLabel?i) thepart-of-speech (resp.
the label if any) of the wordimmediately to the left of ?i, and the same goes forrightPOS?i(resp.
rightLabel?i).
Finally, a is theprevious action predicted by the parser.
Table 3 listsour baseline features.
X?i, ?j, ?kmeans that we useX?i, X?j, X?kas unigram features as well as bi-gram and trigram features.4.2 Syntactic FeaturesWe combined the previous features with differenttypes of syntactic features (constituents and depen-dencies), our intuition being that syntax and se-mantic are interdependent, and that syntactic fea-tures should therefore help predicate-argument pars-ing.
In fact, we considered that the low densityof syntactic information (compared to regular de-pendency treebanks) would be counterbalanced by66adding more context.
We considered the followingpieces of information in particular.SNPDThe1Ncat2VPVsat3PPPon4NPDthe5Nmat6Head Path(w = 4)Spinedet detnsubjpreppobj?
= ?1?
= ?1?
= ?1?
= 1?
= 2Figure 2: Schema of Syntactic FeaturesConstituent Tree Fragments These consist offragments of syntactic trees predicted by the Petrovet al (2006) parser in a 10-way jackknife setting.They can be used as enhanced POS or as features.Spinal Elementary Trees A full set of parseswas reconstructed from the tree fragments using aslightly tweaked version of the CONLL 2009 sharedtask processing tools (Haji?c et al, 2009).
We thenextracted a spine grammar (Seddah, 2010) using thehead percolation table of the Bikel (2002) parser,slightly modified to avoid certain determiners beingmarked as heads in certain configurations.
The re-sulting spines were assigned in a deterministic way(red part in Figure 2).Predicted MATE Dependency Labels These con-sist of the dependency labels predicted by the MATEparser (Bohnet, 2010), trained on a Stanford surfacedependency version of the Penn Treebank.
We com-bined the labels with a distance ?
= t ?
h where tis the token position and h the head position (brownlabels and ?
in Figure 2).
In addition, we expandedthese features with the part-of-speech of the head ofa given token (HPOS).
The idea is to evaluate theinformativeness of more abstract syntactic featuressince a <LABEL,HPOS> pair can be seen as general-izing many constituent subtrees.Constituent Head Paths.
Inspired by Bj?rkelundet al (2013), we used MATE dependencies to ex-tract the shortest path between a token and its lex-ical head and included the path length w (in termsof traversed nodes) as a feature (blue part in Fig-ure 2).
The global idea is to use the phrase-basedfeatures to provide different kinds of syntactic con-text and the dependency-based features to providegeneralisations over the functional label governinga token.
The spines are seen as deterministic su-pertags, bringing a vertical context.We report, in Table 4, the counts for each syntac-tic feature on each set.TREE FRAG.
MATE LABELS+?
SPINES TREES HEAD PATHSTRAIN 648 1305 637 27,670DEV 272 742 265 3,320TEST 273 731 268 2,389Table 4: Syntactic features statistics (Counts).5 ExperimentsExperimental Setup Both DM and PAS tree-banks consist of texts from the PTB and which wereeither automatically derived from the original anno-tations or annotated with a hand-crafted grammar(see above).
We use them in their bi-lexical depen-dency format, aligned at the token level as providedby Oepen et al (2014)1.
The following split is used:sections 00-19 for training, 20 for the dev.
set and 21for test2.
All predicted parses are evaluated againstthe gold standard with labeled precision, recall andf-measure metrics.Results Our experiments are based on the evalua-tion of the combinations of the 4 main types of syn-tactic features described in section 4: tree fragments(BKY), predicted mate dependencies (BN) and theirextension with POS heads (BN(HPOS)), spinal ele-mentary trees (SPINES) and head paths (PATHS).The results are shown in Tables 5 and 6.
All im-provements from the baseline are significant with ap-value p < 0.05.
There was no significant differ-ence of the same p value between our two best mod-1This alignment entailed the removal of all unparsed sen-tences.2We used the same unusual split as in (Oepen et al, 2014)to be able to conduct meaningful comparisons with others.67els for each of the treebanks.3As expected from the rapid overview of ourdatasets exposed earlier in section 2, the use of eachsingle feature alone increases the performance overthe baseline by 0.5 points for the BN feature in DMto 1.44 for PATHS, and by 1.10 for the SPINES to1.85 for the PATHS features in PAS.
Looking at theconjunction of two classes in the DM table, it seemsthat dependency-based features benefit from the ex-tra context brought by constituents features, reach-ing an increase of 2.21 points for BKY+BN(HPOS).Interestingly, the maximum gain is brought by theaddition of topologically different phrase-based fea-tures such as SPINES (+2.80, inherently vertical) orBKY (+2.76, often wider) to the previous best.
Re-garding PAS, similar trends can be observed, al-though the gains are more distributed.
As opposedto DM where the conjunction of more features ledto inferior results, here using a four-features classprovides the second best improvement (ALL(HPOS)= BKY+BN(HPOS)+SPINES+PATHS), +2.82) whileremoving the SPINES slightly increases the score(+2.92).
In fact, adding too many features to themodel slightly degrades our scores, at least with re-gard to DM which has a larger label set than PAS.Results show that syntactic information improvesour parser performances.
As each feature representsone unique piece of information, they benefit frombeing combined in order to provide more structuralinformation.6 Results AnalysisFollowing Mcdonald and Nivre (2007), we con-ducted an error analysis based on the two best mod-els and the baseline for each corpus.
As shown insection 5, syntactic features greatly improve seman-tic parsing.
However, it is interesting to exploremore precisely what kind of syntactic informationboosts or penalizes our predictions.
We consider,among other factors, the impact in terms of distancebetween the head and the dependent (edge length)and the labels.
We also explore several linguisticphenomena well known to be difficult to recover.3We tested the statistical significance between our bestmodels and the baseline with the paired bootstrap test (Berg-Kirkpatrick et al, 2012).DM Corpus (dev.
set) LP LR LFBASELINE 83.66 80.33 81.97BN 84.12 80.91 82.48 +0.51BKY 85.10 81.70 83.36 +1.39SPINES 84.72 81.31 82.98 +1.01PATHS 85.15 81.74 83.41 +1.44BN(HPOS) 85.63 82.19 83.88 +1.91BKY+SPINES 85.41 81.88 83.61 +1.64SPINES+PATHS 85.49 82.01 83.71 +1.74BKY+BN 85.47 82.08 83.74 +1.77BKY+PATHS 85.70 82.22 83.92 +1.95BN(HPOS)+SPINES 85.94 82.48 84.17 +2.20BKY+BN(HPOS) 85.96 82.46 84.18 +2.21BN(HPOS)+PATHS 85.97 82.59 84.25 +2.28BN+SPINES 86.05 82.55 84.26 +2.29BN+PATHS 86.05 82.64 84.31 +2.34BKY+SPINES+PATHS 85.64 82.23 83.90 +1.93BKY+BN+SPINES 85.88 82.50 84.16 +2.19BKY+BN(HPOS)+SPINES 86.38 82.81 84.56 +2.59BN(HPOS)+SPINES+PATHS 86.28 82.91 84.56 +2.59BKY+BN(HPOS)+PATHS 86.49 82.94 84.68 +2.71BKY+BN+PATHS 86.55 82.98 84.73 +2.76BN+SPINES+PATHS 86.59 83.02 84.77 +2.80ALL 85.73 82.27 83.96 +1.99ALL(HPOS) 86.13 82.64 84.35 +2.38Table 5: Best results and gains on DM corpus.PAS Corpus (dev.
set) LP LR LFBASELINE 86.95 83.45 85.17SPINES 88.15 84.47 86.27 +1.10BN 88.21 84.77 86.46 +1.29BN(HPOS) 88.55 85.00 86.74 +1.57BKY 88.63 84.97 86.76 +1.59PATHS 88.85 85.24 87.01 +1.84BKY+SPINES 88.84 85.20 86.98 +1.81SPINES+PATHS 89.04 85.45 87.21 +2.04BN(HPOS)+SPINES 89.18 85.49 87.30 +2.13BN(HPOS)+PATHS 89.17 85.62 87.36 +2.19BN+PATHS 89.32 85.74 87.49 +2.32BKY+PATHS 89.44 85.72 87.54 +2.37BKY+BN 89.30 85.87 87.55 +2.38BN+SPINES 89.48 85.81 87.60 +2.43BKY+BN(HPOS) 89.49 85.80 87.61 +2.44BKY+SPINES+PATHS 89.35 85.54 87.40 +2.23BKY+BN+SPINES 89.56 86.02 87.75 +2.58BN(HPOS)+SPINES+PATHS 89.76 86.15 87.92 +2.75BN+SPINES+PATHS 89.88 86.13 87.96 +2.79BKY+BN+PATHS 89.82 86.20 87.97 +2.80BKY+BN(HPOS)+PATHS 89.93 86.32 88.09 +2.92ALL 89.70 86.11 87.87 +2.70ALL(HPOS) 89.91 86.14 87.99 +2.82Table 6: Best results and gains on PAS.6.1 Breakdown by LabelsIn Figures 3(a) and 4(a), we detail the scores for thefive most frequent labels.6860 70 80 90 100ARG1ARG2compoundpossBVDM F1-score (%)BN+SPINES+PATHSBKY+BN+PATHSBASELINE(a) Most frequent labels.30 40 50 60 70 80 90 100conjARG3_and_clocapposDM F1-score (%)BN+SPINES+PATHSBKY+BN+PATHSBASELINE(b) Best improved labels.10 20 30 40 50 50+70727476788082848688Sent.
Length (bins of size 10)F1-score(%)BN+SPINES+PATHSBKY+BN+PATHSBASELINE(c) Sentence length.5-10 11-15 16-20 21-252530354045505560Edges Length (bins of size 5)F1-score(%)BN+SPINES+PATHSBKY+BN+PATHSBASELINE(d) Long-distance dependen-cies.Figure 3: Error analysis on DM (dev.
set).As observed in the charts, the scores are higher forthe most frequent labels on both corpora, especiallywhen dealing with verbal arguments.
There are alsotwo interesting cases for DM: the predictions of_and_c and ARG3 edges show an improvement byat least 5 points (Figures 3(b) & 4(b)), showing thatthe recovery of coordination structures and the dis-ambiguation of less frequent or more distant argu-ments is achieved by adding non-local features.6.2 Length FactorLonger sentences are notoriously difficult to parsefor most parsing models.
Figures 3(c) and 4(c) showthe F1-measure of our models with respect to sen-tence length (in bins of size 10: 1-10, 11-20, etc.
)for the DM and PAS corpora.It is worth noting that we greatly improve thescores for longer sentences.
The use of paths andof the output of a graph-based parser (Bohnet, 2010)favors the capture of complex dependencies and en-hances the learning of these constructions for ourlocal transition-based parser.
However, we also ob-serve that the features are not able to completely stopthe loss of F1-score for longer sentences.
The slopesof the curves in the different charts show the sametrend: the longer the sentence, the lower the score.6.3 Linguistic FactorsWe now center our analysis on long-distance depen-dencies (LDDs), by focusing our attention on edgeslength, i.e.
the distance between two words linkedby an edge.
We will then concentrate on subject el-lipsis, in a treatment of LDDs more similar to thelinguistic definition of Cahill et al (2004).Long-distance Dependencies (LDDs) For manysystems, LDDs are difficult to recover because theyare generally under-represented in the training cor-pus and the constructions involved in LDDs often re-quire deep linguistic knowledge to be recovered.
InFigure 7, we report the distribution of long-distancedependencies by bins of size 5 up to 40.
They onlyaccount for 15% of all the dependencies in bothcorpora.
The longest dependencies consist of thefirst and second arguments of the verb as well ascoordination links.
In the case of elided coordina-tion structures, we have long-distance dependencieswhen two coordinated verbs share the same first orsecond argument, which explains the distribution oflengths.BINS 5-10 11-15 16-20 21-25 26-40DM 2907 734 329 141 92PAS 3705 1007 408 175 127Table 7: Number of LDDs edges (dev.
set).As outlined in Figures 3(d) and 4(d), we can seethat without structural information such as spines,surfacic dependencies or paths, the longest depen-dencies have low F1-scores.
When using these fea-tures, our models tend to perform better, with a gainof up to 25 points for high-dependency lengths (binsbetween 16-20 and 21-25).In Table 8, we show the global improvement whenconsidering edge lengths between 5 and 40.
Forboth corpora, the improvement is the same (around9 points), showing that structural information is thekey to better predictions.
Looking into this im-provement more closely, we found that PATHS com-bined with BN tend to be crucial, whereas SPINES6960 70 80 90 100prep_ARG1verb_ARG2adj_ARG1prep_ARG2noun_ARG1PAS F1-score (%)BKY+BN(H)+PATHSALL(HPOS)BASELINE(a) Most frequent labels.30 40 50 60 70 80 90 100verb_ARG3coord_ARG1conj_ARG2coord_ARG2verb_ARG1PAS F1-score (%)BKY+BN(H)+PATHSALL(HPOS)BASELINE(b) Best improved labels.10 20 30 40 50 50+74767880828486889092Sent.
Length (bins of size 10)F1-score(%)BKY+BN(H)+PATHSALL(HPOS)BASELINE(c) Sentence length.5-10 11-15 16-20 21-25404550556065Edges Length (bins of size 5)F1-score(%)BKY+BN(H)+PATHSALL(HPOS)BASELINE(d) Long-distance dependen-cies.Figure 4: Error analysis on PAS (dev.
set).BKY+BN(H)+PATHS stands for BKY+BN(HPOS)+PATHS.may sometimes penalize the models.
Even though,BN+SPINES+PATHS is the best model for DM, aspine is only a partial projection which lacks attach-ment information.
Spines alone only therefore pro-vide a local context and are unable to cope well withLDDs.Coordination Structures We now focus on struc-tures with subject ellipsis.
We extracted them byusing a simple graph pattern, i.e.
two verbs with ashared ARG1 and a coordination dependency.Our best models?
scores are displayed in Tables 9.Once again, our models improve the F1score, butnot in the same proportion.
DM considers the con-junction as a semantically empty word and attachesan edge _and_c between the two verbs to mark thecoordination.
Consequently this edge is more dif-ficult to predict, because it is less informative, ourbaseline model relying on tokens, lemmas and POS.We note that the difference in the number of eval-uated dependencies in both corpora comes from anannotation scheme divergence between PAS andDM regarding subject ellipsis.
DM opts for coordi-nate structures with a chain of dependencies rootedat the first conjunct, the coordinating conjunctionsbeing therefore semantically empty.
In PAS, the fi-nal coordinating conjunction and each coordinatingconjunction is a two-place predicate, taking left andright conjuncts as its arguments.The gain of 6.30 points for DM (Table 9(a),resp.
+3 for PAS) indicates that, when an annota-tion scheme is designed to have many semanticallyempty words, using syntactic information tends toenhance the parser accuracy.
This gives a clear in-sight into what type of information is required toparse semantic graphs: the greater the distance be-tween the head and the dependent, the larger the con-text needed to disambiguate the attachments.6.4 Ruling out the Structural Factor BiasPAS DMOverlap +2.87 +2.67Rest +2.70 +2.74It may argued that theimprovement we no-ticed could stem froma potentially strongoverlap between sur-face trees and predicate-argument structures, bothin terms of edges and labels.
In fact, the conversionfrom surfacic parses into predicate-argument struc-tures requires a large amount of edges relabeling(for instance, when nsubj is relabeled to ARG1).We tested this hypothesis by computing the numberof common edges between MATE predictions andDM and PAS.
The overlap corresponds to about22% of all edges in PAS and 27% in DM.
Althoughimportant, it does not represent the majority ofdependencies in our corpora, because most of edgesare not present in surface predictions.
We evaluatedthe improvement of the overlap as well as for therest.
Results show that our best models performroughly the same on both sets.
Interestingly, asopposed to PAS?s model, DM?s model performsbetter on the non-overlap part.
This suggests that theuse of PTB-based features is somehow not optimalwhen applied on a none PTB-based treebank, suchas DM which comes from a handcrafted grammar.7 DiscussionOur point was to prove that providing more syntac-tic context, in the form of phrased-based tree frag-ments and surface dependencies, helps transition-70LP LR LFBASELINE 54.95 42.53 47.95BN+SPINES+PATHS 64.23 50.55 56.57 +8.62BKY+BN+PATHS 64.88 50.90 57.05 +9.10(a) DM Corpus (dev.
set).LP LR LFBASELINE 66.62 50.17 57.23ALL(HPOS) 74.03 57.58 64.78 +7.55BKY+BN(HPOS)+PATHS 74.62 58.95 65.86 +8.73(b) PAS Corpus (dev.
set).Table 8: Long-distance dependencies eval.
(dev sets).LP LR LFBASELINE 90.00 48.57 63.09BN+SPINES+PATHS 96.02 53.65 68.84 +5.85BKY+BN+PATHS 96.07 54.29 69.37 +6.28(a) on DM (dev.
set, 315 dependencies).LP LR LFBASELINE 97.51 61.48 75.41ALL(HPOS) 97.86 64.78 77.96 +2.55BKY+BN(HPOS)+PATHS 98.57 65.09 78.41 +3.00(b) on PAS (dev.
set, 636 dependencies).Table 9: Shared subjects coordinations eval.
(devsets).based parsers to predict predicate-argument struc-tures, especially for LDDs.
Yet, compared to state-of-the-art systems, our results built on the S&Tparser score lower than the top performers (Table10).However, we are currently extending a more ad-vanced lattice-aware transition-based parser (DSR)with beams (Villemonte De La Clergerie, 2013)that takes advantage of cutting-edge techniques (dy-namic programming, averaged perceptron with earlyupdates, etc.
following (Goldberg et al, 2013;Huang et al, 2012))4, which proves effective byreaching the state-of-the-art on PAS, outperformingThomson et al (2014) and second to the model ofMartins and Almeida (2014).5The point here is that using the same syntactic fea-tures as our base system exhibits the same improve-ment over a now much stronger baseline.
We canconjecture that the ambiguities added by the relativescarcity of the deep annotations is efficiently han-dled by a more complete exploration of the searchspace, made possible by beam optimization.We can also wonder whether the lower improve-ment brought to DM parsing by the PTB-based syn-tactic features does not come from the fact that theDM corpus and the PTB have divergent annotation4It uses a different set of transitions, notably pop actions in-stead of left and right reduce, and a swap that allow limitedamount of non-planarity.
Such a set raises issues with beams(several paths leading to a same item, final items reached withpaths of various lengths, .
.
.
), overcome by adding a ?noop?
ac-tion only applied on final items to balance path lengths.5Leaving aside the multiple (19) ensemble models of Du etal.
(2014), because of the impracticability of the approach.schemes.
In that aspect, PTB syntactic features mayadd some noise to the learning process, because theygive more weight to conflicting decisions that led tocorrect structures in one but not in the other scheme.By using features which, to a certain extent, (i)extend the domain of locality available at a givennode and (ii) generalize some structural and func-tional contexts otherwise unavailable, we tried toovercome the main issue of transition-based parsers:they remain local in the sense that they lack a globalview of the whole sentence.Impact Beyond Transition-based Parser Ofcourse, it can be argued that improving over a some-what weak baseline is of limited interest.
Our pointwas to investigate how the direct parsing of rela-tively sparse graph structures would benefit fromthe inclusion of more context via the use of topo-logically different syntactic pieces of information.However in that work, we mostly focused on tran-sition based-parsing, which raises the question ofthe impact of our feature-set on a much more pow-erful and state-of-the-art model such as the TUR-BOSEMANTICPARSER developed by Martins andAlmeida (2014).To this end, we extended the T.PARSER so that itcould cope with our syntactic features and studiedthe interaction of our best feature set with secondorder features (i.e.
grand-parents and co-parents).Results in Table 11 show that the gain brought byadding syntactic features (+2.14 on DM over thebaseline) is higher than the sole use of second or-der ones (+1.09).
Furthermore, the gain brought by71PAS DM(T.PARSER+features, this paper) 92.11 89.70(Du et al, 2014) 92.04 89.40(Martins and Almeida, 2014) 91.76 89.16(DSR, this paper) 90.13 85.66(Thomson et al, 2014) 89.63 83.97(S&T, this paper) 87.5 83.84(DSR, this paper, no feat) 87.02 83.91(S&T, this paper, no feat) 84.18 81.17Table 10: Comparison with the State-of-the-Art.the second-order features is reduced by half whenused jointly with our feature set (+1.09 vs +0.57 withthem).
However, although we could assess that theneed of second order models is thus alleviated, theconjunction of both types of features still improvesthe parser performance by an overall gain of 1.62points on DM (1.18 on PAS), suggesting that bothfeature sets contribute to different types of ?struc-tures?.
In short, the use of syntactic features is alsorelevant with a strong baseline, as they provide aglobal view to graph-based models, establishing anew state-of-the-art on these corpora.-SYNT.
FEAT.
+SYNT.
FEAT.
?DM, baseline 86.99 89.13 +2.14+grandparent 87.66 89.43 +1.77+co-parents 88.08 89.7 +1.62PAS, baseline 89.73 91.68 +1.95+grandparent 90.15 91.92 +1.77+co-parents 90.93 92.11 +1.18Table 11: LF Results for T.PARSER (test set).Baseline = arc-factored + siblingsRelated Work A growing interest for semanticparsing has emerged over the past few years, withthe availability of resources such as PropBank andNomBank (Palmer et al, 2005; Meyers et al, 2004)built on top of the Penn Treebank.
The shal-low semantic annotations they provide were amongthe targets of successful shared tasks on seman-tic role labeling (Surdeanu et al, 2008; Carrerasand M?rquez, 2005).
Actually, the conjoint use ofsuch annotations with surface syntax dependenciesbears some resemblance with predicate-argumentstructure parsing like we presented here.
However,they diverge in that Propbank/Nombank annotationsdo not form connected graphs by themselves, asthey only cover argument identification and nominalpredicates.
The range of phenomena they describe isalso limited, compared to a full predicate-argumentanalysis as provided by DM and PAS (Oepen et al,2014).
More importantly, as pointed out by Yi et al(2007), being verb-specific, Propbank?s roles do notgeneralize well beyond the ARG0 argument (i.e.
thesubject/agent role) leading to inconsistencies.However, the advent of such semantic-based re-sources have ignited a fruitful line of research, ofwhich the use of heterogeneous sources of infor-mation to boost parsing performance has been in-vestigated over the past decade (Chen and Rambow,2003; Tsuruoka et al, 2004) with a strong regain ofinterest raised by the work of Moschitti et al (2008),Henderson et al (2008), Sagae (2009).8 ConclusionWe described the use and combination of severalkinds of syntactic features to improve predicate-argument parsing.
To do so, we tested our ap-proach of injecting surface-syntax features by tho-roughly evaluating their impact on one transition-based graph parser, then validating on two more ef-ficient parsers, over two deep syntax and semantictreebanks.
Results of the syntax-enhanced semanticparsers exhibit a constant improvement, regardlessof the annotation scheme and the parser used.The question is now to establish whether will this beverified in other semantic data sets?
From the pars-ing of deep syntax treebanks a la Meaning Text The-ory (Ballesteros et al, 2014), to Framenet semanticparsing (Das et al, 2014) or data-driven approachescloser to ours (Flanigan et al, 2014), it is difficult toknow which models will predominate from this bub-bling field and what kind of semantic data sets willbenefit the most from syntax.AcknowledgementsWe would like to thank Kenji Sagae and Andr?
F.T.
Martins for making their parsers available andfor kindly answering our questions.
We also thankour anonymous reviewers for their comments.
Thiswork was partly funded by the Program "Investisse-ments d?avenir" managed by Agence Nationale de laRecherche ANR-10-LABX-0083 (Labex EFL).72ReferencesMiguel Ballesteros, Bernd Bohnet, Simon Mille, and LeoWanner.
2014.
Deep-syntactic parsing.
In In Proc.
ofCOLING, Dublin, Ireland.Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.2012.
An Empirical Investigation of Statistical Signif-icance in NLP.
In Proc.
of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 995?1005.Daniel M. Bikel.
2002.
Design of a multi-lingual,parallel-processing statistical parsing engine.
In Proc.of the second international conference on Human Lan-guage Technology Research, pages 178?182.
MorganKaufmann Publishers Inc. San Francisco, CA, USA.Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,Thomas Mueller, and Wolfgang Seeker.
2013.
(re)ranking meets morphosyntax: State-of-the-art re-sults from the SPMRL 2013 shared task.
In Proc.of the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 135?145,October.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proc.
ofthe 23rd International Conference on ComputationalLinguistics, pages 89?97.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations.In Proc.
of ACL, pages 320?327.Xavier Carreras and Llu?s M?rquez.
2005.
Introductionto the conll-2005 shared task: Semantic role labeling.In Proc.
of the Ninth Conference on ComputationalNatural Language Learning, pages 152?164.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
of the 1st Annual Meeting of the NorthAmerican Chapter of the ACL (NAACL), Seattle.John Chen and Owen Rambow.
2003.
Use of deep lin-guistic features for the recognition and labeling of se-mantic arguments.
In Proc.
of the 2003 conferenceon Empirical methods in natural language processing,pages 41?48.
Association for Computational Linguis-tics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proc.
of theACL-02 Conference on Empirical Methods in NaturalLanguage Processing - Volume 10, pages 1?8.Ann Copestake, Dan Flickinger, Rob Malouf, SusanneRiehemann, and Ivan Sag.
1995.
Translation usingminimal recursion semantics.
In Proc.
of the SixthInternational Conference on Theoretical and Method-ological Issues in Machine Translation, pages 15?32.Citeseer.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan ASag.
2005.
Minimal recursion semantics: An in-troduction.
Research on Language and Computation,3(2-3):281?332.Dipanjan Das, Desai Chen, Andr?
FT Martins, NathanSchneider, and Noah A Smith.
2014.
Frame-semanticparsing.
Computational Linguistics, 40(1):9?56.Yantao Du, Fan Zhang, Weiwei Sun, and Xiaojun Wan.2014.
Peking: Profiling syntactic tree parsing tech-niques for semantic graph parsing.
In Proc.
of the8th International Workshop on Semantic Evaluation,pages 459?464.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, ChrisDyer, and Noah A. Smith.
2014.
A discriminativegraph-based parser for the abstract meaning represen-tation.
In in Proc.
of ACL, Baltimore, US.Daniel Flickinger, Yi Zhang, and Valia Kordoni.
2012.DeepBank: a dynamically annotated treebank of thewall street journal.
In Proc.
of the Eleventh Interna-tional Workshop on Treebanks and Linguistic Theo-ries, pages 85?96.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine learning, 37(3):277?296.Yoav Goldberg, Kai Zhao, and Liang Huang.
2013.Efficient implementation of beam-search incremen-tal parsers.
In Proc.
of the 51st Annual Meeting ofthe Association for Computational Linguistics (ACL),Sophia, Bulgaria.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant?nia Mart?, Llu?sM?rquez, Adam Meyers, Joakim Nivre, SebastianPad?, Jan ?t?ep?nek, et al 2009.
The conll-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proc.
of the Thirteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 1?18.James Henderson, Paola Merlo, Gabriele Musillo, andIvan Titov.
2008.
A latent variable model of syn-chronous parsing for syntactic and semantic dependen-cies.
In Proc.
of the Twelfth Conference on Computa-tional Natural Language Learning, pages 178?182.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.of HLT-NAACL 2012, pages 142?151.Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, andDan Flickinger.
2012. Who did what to whom?
: A73contrastive study of syntacto-semantic dependencies.In Proc.
of the sixth linguistic annotation workshop,pages 2?11.Sandra K?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan and ClaypoolPublishers.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.T.
Andr?
F. Martins and C. Mariana S. Almeida.
2014.Priberam: A turbo semantic parser with second orderfeatures.
In Proc.
of the 8th International Workshopon Semantic Evaluation, pages 471?476.Ryan Mcdonald and Joakim Nivre.
2007.
Characterizingthe errors of data-driven dependency parsing models.In Proc.
of the Conference on Empirical Methods inNatural Language Processing and Natural LanguageLearning.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
Annotating noun argument structurefor nombank.
In LREC, volume 4, pages 803?806.Yusuke Miyao and Jun?ichi Tsujii.
2004.
Deep Lin-guistic Analysis for the Accurate Identification ofPredicate-Argument Relations.
In Proc.
of the 18th In-ternational Conference on Computational Linguistics,pages 1392?1397, Geneva, Switzerland.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSG pars-ing.
In Proc.
of ACL 2005, pages 83?90.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proc.
of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 99?106.
Association for ComputationalLinguistics.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proc.
of the 8th Interna-tional Workshop on Parsing Technologies (IWPT.
Cite-seer.Stephan Oepen and Jan Tore L?nning.
2006.Discriminant-based mrs banking.
In Proc.
of the 5thinternational conference on language resources andevaluation (lrec 2006).Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Dan Flickinger, Jan Hajic, AngelinaIvanova, and Yi Zhang.
2014.
Semeval 2014 task8: Broad-coverage semantic dependency parsing.
InProc.
of the 8th International Workshop on SemanticEvaluation, pages 63?72.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reduce de-pendency DAG parsing.
In Proc.
of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 753?760.Kenji Sagae.
2009.
Analysis of discourse structure withsyntactic dependencies and data-driven shift-reduceparsing.
In Proc.
of the 11th International Conferenceon Parsing Technologies, pages 81?84.
Association forComputational Linguistics.Djam?
Seddah.
2010.
Exploring the spinal-stig modelfor parsing french.
In Proc.
of the Seventh conferenceon International Language Resources and Evaluation(LREC?10).Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
The conll-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proc.
of the Twelfth Confer-ence on Computational Natural Language Learning,pages 159?177.
Association for Computational Lin-guistics.Sam Thomson, Brendan O?Connor, Jeffrey Flanigan,David Bamman, Jesse Dodge, Swabha Swayamdipta,Nathan Schneider, Chris Dyer, and A. Noah Smith.2014.
CMU: Arc-Factored, Discriminative SemanticDependency Parsing.
In Proc.
of the 8th InternationalWorkshop on Semantic Evaluation, pages 176?180.Kristina Toutanova, Aria Haghighi, and Christopher DManning.
2005.
Joint learning improves semantic rolelabeling.
In Proc.
of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, pages 589?596.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsujii.2004.
Towards efficient probabilistic hpsg parsing: in-tegrating semantic and syntactic preference to guidethe parsing.
In Proc.
of the IJCNLP-04 Workshop onBeyond Shallow Analyses.
Citeseer.
?ric Villemonte De La Clergerie.
2013.
Exploring beam-based shift-reduce dependency parsing with DyALog:Results from the SPMRL 2013 shared task.
In 4thWorkshop on Statistical Parsing of MorphologicallyRich Languages (SPMRL?2013).Szu-Ting Yi, Edward Loper, and Martha Palmer.
2007.Can semantic roles generalize across genres?
In HLT-NAACL, pages 548?555.74
