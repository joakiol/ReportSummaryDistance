Proceedings of the 6th Workshop on Statistical Machine Translation, pages 217?226,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsAgreement Constraints for Statistical Machine Translation into GermanPhilip Williams and Philipp KoehnSchool of InformaticsUniversity of Edinburgh10 Crichton StreetEH8 9AB, UKp.j.williams-2@sms.ed.ac.ukpkoehn@inf.ed.ac.ukAbstractLanguages with rich inflectional morphologypose a difficult challenge for statistical ma-chine translation.
To address the problem ofmorphologically inconsistent output, we addunification-based constraints to the target-sideof a string-to-tree model.
By integrating con-straint evaluation into the decoding process,implausible hypotheses can be penalised orfiltered out during search.
We use a sim-ple heuristic process to extract agreement con-straints for German and test our approach onan English-German system trained on WMTdata, achieving a small improvement in trans-lation accuracy as measured by BLEU.1 IntroductionHistorically, most work in statistical machine trans-lation (SMT) has focused on translation into En-glish.
Languages with richer inflectional mor-phologies pose additional challenges for translationand conventional SMT approaches tend to performpoorly when either source or target language has richmorphology (Koehn, 2005).For complex source inflection, a successful ap-proach has been to cluster inflectional variants intoequivalence classes.
This removes information thatis redundant for translation and can be performed asa preprocessing step for input to a conventional sur-face form based translation model (Nie?en and Ney,2001; Goldwater and McClosky, 2005; Talbot andOsborne, 2006).For complex target inflection,Minkov et al (2007) investigate how post-processing can be used to generate inflection for asystem that produces uninflected output.
Their ap-proach is successfully applied to English-Arabic andEnglish-Russian systems by Toutanova et al (2008).Another promising line of research involves thedirect integration of linguistic information into SMTmodels.
Koehn and Hoang (2007) generalise thephrase-based model?s representation of the wordfrom a string to a vector, allowing additional featuressuch as part-of-speech and morphology to be asso-ciated with, or even to replace, surface forms dur-ing search.
Luong et al (2010) decompose wordsinto morphemes and use this extended represen-tation throughout the training, tuning, and testingpipeline.Departing further from traditional SMT mod-els, the transfer-based systems of Riezler andMaxwell (2006), Bojar and Hajic?
(2008), and Gra-ham et al (2009) employ rich feature structurerepresentations for linguistic attributes, but haveso far been limited by their dependence on non-stochastic parsers with limited coverage.
The Stat-XFER transfer-based framework (Lavie, 2008) isneutral with regard to the rule acquisition methodand the author describes a manually developedHebrew-English transfer grammar, which includes asmall number of constraints between agreement fea-tures.
In Hanneman et al (2009) the framework isused with a large automatically-extracted grammar,though this does not use feature constraints.In this paper we propose a model that retains theuse of surface forms during decoding whilst alsochecking linguistic constraints defined over asso-ciated feature structures.
Specifically, we extenda string-to-tree model by adding unification-based217constraints to the target-side of the synchronousgrammar.
We suggest that such a constraint systemcan:?
improve the model by enforcing inflectionalconsistency in combinations unseen by the lan-guage model?
improve search by allowing the early elimina-tion of morphologically-inconsistent hypothe-sesTo evaluate the approach, we develop a system forEnglish-German with constraints to enforce intra-NP/PP and subject-verb agreement, and with a sim-ple probabilistic model for NP case.2 PreliminariesThere is an extensive literature on constraint-basedapproaches to grammar, employing a rich varietyof terminology and linguistic devices.
We use onlya few of the core ideas, which we briefly describein this section.
We borrow the terminology andnotation of PATR-II (Shieber, 1984), a minimalconstraint-based formalism that extends context-freegrammar.Central to our model are the concepts of featurestructures and unification.
Feature structures are oftwo kinds:?
atomic feature structures are untyped, indivisi-ble values, such as NP, nom, or sg?
complex feature structures are partial functionsmapping features to values, the values them-selves being feature structures.Complex feature structures are conventionally writ-ten as attribute-value matrices.
For example, the fol-lowing might represent lexical entries for the Ger-man definite article, die, and the German noun,Katze, meaning cat:die ?
???????
?POS ARTAGR????
?CASE accDECL weakGENDER femNUMBER sg????????????
?Katze ?
????
?POS NNAGR??
?CASE accGENDER femNUMBER sg???????
?An equivalent representation, and the one we usefor implementation, is that of a rooted, labelled, di-rected acyclic graph.A value belonging to a complex feature structurecan be specified using a path notation that describesthe chain of features in enclosing feature structures.In the examples above, the path ?
AGR GENDER ?specifies the atomic value fem.Informally, unification is a merging operation thatgiven two feature structures, yields the minimal fea-ture structure containing all information from bothinputs.
A unification failure results if the inputfeature structures have mutually-conflicting values.The subject of unification, both in the context of nat-ural language processing and more generally, is sur-veyed in Knight (1989).
In this work, we use de-structive graph-based unification, which results inthe source feature structures sharing values uponunification.For example, the result of unifying the agreementvalues for the feature structures above would be:die ?
???????
?POS ARTAGR 1????
?CASE accDECL weakGENDER femNUMBER sg????????????
?Katze ?
[POS NNAGR 1]The index boxes are used to indicate that a value isshared.3 GrammarIn this section we describe the synchronous gram-mar used in our string-to-tree model.
Rule extractionis similar to the syntax-augmented model of Zoll-mann and Venugopal (2006), though we do not useextended categories in this work.
We then describehow we extend the grammar with target-side con-straints.3.1 Synchronous GrammarOur translation model is based on a synchronouscontext-free grammar (SCFG) learned from a par-allel corpus.
Rule extraction follows the hierarchi-cal phrase-based algorithm of Chiang (2005; 2007).Source non-terminals are given the undistinguishedlabel X, whereas the target non-terminals are givenpart-of-speech and constituent labels obtained from218a parse of the target-side of the parallel corpus.Rules in which the target span is not covered by aparse tree constituent are discarded.Compared with the hierarchical phrase-basedmodel, the restriction to constituent target phrasesreduces the total grammar size and the addition oflinguistic labels reduces the problem of spurious am-biguity.
We therefore relax Chiang?s (2007) rule fil-tering in the following ways:1.
Up to seven source-side terminal / non-terminalelements are allowed.2.
Rules with scope greater than three are filteredout (Hopkins and Langmead, 2010).3.
Consecutive source non-terminals are permit-ted.4.
Single-word lexical phrases are allowed for hi-erarchical subphrase subtraction.3.2 Constraint GrammarWe extend the synchronous grammar by adding con-straints to the target-side.
A constraint is an identitybetween either:i) feature structure values belonging to two ruleelements,ii) a feature structure value belonging to a rule el-ement and a constant value, oriii) a feature structure value belonging to a rule ele-ment and a random variable with an associatedprobability functionFor example, the following synchronous rule:NP-SB ?
the X1 cat | die AP1 Katzemight have the target constraint rule shown in Fig-ure 1.The first three constraints ensure that any AP hasagreement values consistent with the lexical itemsdie and Katze.
The next provides a probability basedon the resulting case value.
The final two are used todisambiguate between possible parts-of-speech.Constraints are evaluated by attempting to unifythe specified feature structures.
A rule element mayhave more than one associated feature structure, soNP-SB ?
die AP Katze?
NP-SB AGR?
= ?
die AGR??
NP-SB AGR?
= ?
AP AGR??
NP-SB AGR?
= ?
Katze AGR??
NP-SB AGR CASE?
= C?
die POS?
= ART?
Katze POS?
= NNP (C = c) =??????
?0.990, c = NOM0.005, c = DAT0.004, c = GEN0.001, c = ACCFigure 1: Example target constraint ruleunification is attempted between all combinations.
Ifno combination can be successfully unified then theconstraint fails.Ultimately, all feature structures originate in thelexicon, which maps a surface form word to a set ofzero or more complex feature structures.3.3 Some Constraints for GermanWe now describe the German constraints that we usein this paper.
Whilst the constraint model describedabove is language-independent, the actual form ofthe constraints will largely be language- and corpus-specific.In this work, the linguistic annotation is obtainedfrom a statistical parser and a morphological anal-yser.
We use the BitPar parser (Schmid, 2004)trained on the TIGER treebank (Brants et al, 2002)and the Morphisto morphological analyser (Zielin-ski and Simon, 2009).
We find that we can extractuseful constraints for German based on a minimalset of simple manually-developed heuristics.Base NP/PP AgreementGerman determiners and adjectives are inflectedto agree in gender and number with the nouns thatthey modify.
As in English, a distinction is made be-tween singular and plural number, with most nounshaving separate forms for each.
Grammatical genderhas three values: masculine, feminine, and neuter.A noun phrase?s case is usually determined by its219{ADJA, ART, NN, PDAT,PIAT, PPOSAT, PWAT}?
{NP, PP}{APPR, APPRART} ?
{PP}{ADJA} ?
{AP, CAP}{AP} ?
{CAP}{AP, CAP} ?
{NP, PP}Figure 2: Propagation rules used to capture NP/PP agree-ment relationsrole in the clause.
For example, nominative caseusually indicates the subject of a verb.
The case ofa prepositional phrase is usually determined by thechoice of preposition.We model these grammatical properties by i) as-sociating, via the lexicon, a set of possible agree-ment values with each preposition, determiner, ad-jective, and noun, and ii) enforcing agreement rela-tions through pairwise identities between rule ele-ments (as in the example in Figure 1).For constraint extraction, we first group parse treenodes into agreement relations.
We use the parsetree labels to determine whether a parent sharesagreement information with a child.
Figure 2 showsthe rules that we used in experiments.
These shouldbe read as saying that if a child node has a label thatappears on the left-hand side of a rule, r, and its par-ent node has a label that appears on the right-handside of r then the parent and child share agreementinformation.These rules are applied bottom-up from thepreterminal nodes of the training data trees.
Agree-ment relations are merged if they share a commonparent.
Finally, relations are extended to includechild words.
Figure 3 shows a sentence pair in whichthe target-side tree has been annotated to show twoNP agreement relations found according to the rulesof Figure 2.Of course, this process is not perfect and findsmany spurious relations.
We guard against the mostfrequent errors by:i) Filtering out relations based on label-patternsfound during error analysis (for example, rela-tions containing multiple NN nodes)ii) Attempting to unify the agreement featurestructures of the words and rejecting relationsfor which this failsHaving annotated the training data trees withagreement relations, rule extraction is extended toaccept annotated trees and to generate constraintrules of the form shown in Figure 1.
Constraints areproduced where any two target-side rule elementsbelong to a common agreement relation.
The result-ing constraints are grouped by relation into distinctconstraint sets.Subject-Verb AgreementWe add limited subject-verb agreement in a sim-ilar manner.
The additional propagation rules aregiven in Figure 4.
To determine the subject we relyupon the TIGER treebank?s grammatical functionlabels, which the parser affixes to constituent labels.These are otherwise ignored in all propagation rules.Probabilistic Constraints for NP CaseWe make further use of the treebank?s grammat-ical function labels in order to define probabilisticconstraints for noun phrase case.
Many of the func-tion labels are strongly biased towards a particu-lar case (NP-TOP uses nominative case in 91.5% ofunambiguous occurrences, for example).
We esti-mate probabilities by evaluating NP agreement rela-tions in the training data and counting case-label co-occurrences.
Ambiguous case values are ignored.The training data uses only 23 distinct NP labels,most of which occur very frequently, so no smooth-ing is applied.
Table 1 shows the 10 most commonlabels and their case frequencies.4 ModelAs is standard, we frame the decoding problem as asearch for the most probable target language tree t?given a source language string s:t?
= argmaxt p(t|s)The function p(t|s) is modelled by a log-linearsum of weighted feature functions:p(t|s) = 1Zn?i=1?ihi(s, t)220TOPS-TOPNP-SBPIATbeideNNVersa?umnisseVAFINhabenVP-OCNP-OAADJAterroristischeNNGruppenPP-MNRAPPRinNEPakistanVVPPgesta?rktPUNC..both failures have strengthened domestic terrorist groups .Figure 3: Sentence pair from training data.
The two NP agreement relations used for constraint extraction are indicatedby the rectangular and elliptical node borders.
{VAFIN, VMFIN, VVFIN} ?
{S}{NP-SB} ?
{S}Figure 4: Propagation rules used to capture subject-verbagreement relationsLabel Nom Acc Gen Dat FreqAG 0.1 0.0 99.9 0.0 308156CJ 10.9 10.3 32.4 46.4 77198OA 1.6 91.5 0.7 6.2 67686SB 99.0 0.1 0.4 0.5 60245DA 1.9 0.2 1.4 96.5 41624PD 98.2 0.2 1.4 0.3 19736APP 39.4 7.3 8.7 44.6 7739MO 18.6 17.3 56.9 7.2 7591PNC 30.6 0.0 47.4 22.0 4888OG 0.1 0.0 97.9 2.0 2060Table 1: The 10 most freqently occurring NP labels withtheir case frequencies (shown as percentages)4.1 String-to-Tree FeaturesOur feature functions include the n-gram languagemodel probability of t?s yield, a count of the wordsin t?s yield, and various scores for the synchronousderivation.
We score grammar rules according to thefollowing functions:?
p(RHSs|RHSt,LHS), the noisy-channel trans-lation probability.?
p(RHSt|RHSs,LHS), the direct translationprobability, which we further condition on theroot label of the target tree fragment.?
plex (RHSt|RHSs) and plex (RHSs|RHSt), thedirect and indirect lexical weights (Koehn et al,2003).?
ppcfg(FRAGt), the monolingual PCFG proba-bility of the tree fragment from which the rulewas extracted.
This is defined as?ni=1 p(ri),where r1 .
.
.
rn are the constituent CFG rulesof the fragment.
The PCFG parameters are esti-mated from the parse of the target-side trainingdata.
All lexical rules are given the probabil-ity 1.
This is similar to the pcfg feature used inMarcu et al (2006) and is intended to encour-age the production of syntactically well-formedderivations.?
exp(1), a rule penalty.2214.2 Constraint Model FeaturesIn addition to the string-to-tree features, we add twofeatures related to constraint evaluation:?
exp(f), where f is the derivation?s constraintset failure count.
This serves as a penalty fea-ture in a soft constraint variant of the model:for each constraint set in which a unificationfailure occurs, this count is increased and anempty feature structure is produced, permittingdecoding to continue.?
?n pcase(cn), the product of the derivation?scase model probabilities.
Where the case valueis ambiguous we take the highest possible prob-ability.5 DecodingWe use the Moses (Koehn et al, 2007) decoder, abottom-up synchronous parser that implements theCYK+ algorithm (Chappelier and Rajman, 1998)with cube pruning (Chiang, 2007).The constraint model requires some changes todecoding, which we briefly describe here:5.1 Hypothesis StateBottom-up constraint evaluation requires a featurestructure set for every rule element that participatesin a constraint.
For lexical rule elements these areobtained from the lexicon.
For non-lexical rule ele-ments these are obtained from predecessor hypothe-ses.
After constraint evaluation, each hypothesistherefore stores the resulting, possibly empty, set offeature structures corresponding to its root rule ele-ment.Hypothesis recombination must take these featurestructure states into account.
We take the simplestapproach of requiring sets to be equal for recombi-nation.5.2 Cube PruningAt each chart cell, the decoder determines whichrules can be applied to the span and which com-binations of subspans they can cover (the applica-tion contexts).
An n-dimensional cube is created foreach application context of a rule, where n?1 is therank of the rule.
Each cube has one dimension persubspan and one for target-side translation options.Cube pruning begins with these cubes being placedinto a priority queue ordered according to the modelscore of their corner hypotheses.With the introduction of the constraint model, thecube pruning algorithm must also allow for con-straint failure.
For the hard constraint model, wemake the following modifications:1.
Since the corner hypothesis might fail the con-straint check, rule cube ordering is based onthe score of the nearest hypothesis to the cornerthat satisifies its constraints (if any exists).
Thishypothesis is found by exploring neighbours inorder of estimated score (that is, without calcu-lating the full language model score) starting atthe corner.2.
When a hypothesis is popped from a cube andits neighbours created, constraint-failing neigh-bours are added to a ?bad neighbours?
queue.3.
If a cube cannot produce a new hypothesis be-cause all of the neighbours fail constraints, itstarts exploring neighbours of the bad neigh-bours.We place an arbitrary limit of 10 on the numberof consecutive constraint-failing hypotheses to con-sider before discarding the cube.We anticipate that decoding for a highly in-flected target language will result in a less mono-tonic search space due to the increased formation ofinflectionally-inconsistent combinations.6 Experiments6.1 Baseline SetupWe trained a baseline system using the English-German Europarl and News Commentary data fromthe ACL 2010 Joint Fifth Workshop on StatisticalMachine Translation and Metrics MATR1.The German-side of the parallel corpus wasparsed using the BitPar2 parser.
Where a parse failedthe pair was discarded, leaving a total of 1,516,961sentence pairs.
These were aligned using GIZA++1http://www.statmt.org/wmt10/translation-task.html2http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html222and SCFG rules were extracted as described in sec-tion 3.1 using the Moses toolkit.
The resulting gram-mar contained just under 140 million synchronousrules.We used all of the available monolingual Ger-man data to train three 5-gram language models (oneeach for the Europarl, News Commentary, and Newsdata sets).
These were interpolated using weightsoptimised against the development set and the re-sulting language model was used in experiments.We used the SRILM toolkit (Stolcke, 2002) withKneser-Ney smoothing (Chen and Goodman, 1998).The baseline system?s feature weights were tunedon the news-test2008 dev set (2,051 sentence pairs)using minimum error rate training (Och, 2003).6.2 Constraint Model SetupA feature structure lexicon was generated by run-ning the Morphisto3 morphological analyser overthe training vocabulary and then extracting featurevalues from the output.The constraint rules were extracted using theagreement relation identification and filtering meth-ods described in section 3.3.We tested two constraint model systems, one us-ing the rules as hard constraints and the other as softconstraints.
The former discarded all hypothesesthat failed constraints and used the modified cubepruning search algorithm.
The latter allowed con-straint failure but used the failure count feature as apenalty.
Both systems used the NP case probabil-ity feature.
The weights for these two features wereoptimised using MERT (with all baseline weightsfixed).
The systems were otherwise identical to thebaseline.6.3 EvaluationThe systems were evaluated against constrained ver-sions of the newstest2009, newstest2010, and new-stest2011 test sets.
We used a maximum rule spanof 20 tokens for decoding.
In order that the inputcould be covered without the use of glue rules (ex-cept for unknown words), we used sentences of 20or fewer tokens, giving test sets of 1,025, 1,054, and1,317 sentences, respectively.
We evaluated transla-tion quality using case-sensitive BLEU-4 (Papineni3http://code.google.com/p/morphisto/(NP-AG der (ADJA regelma?
?igen) (ADJA ta?glichen) (NN Handel))(PP-MO nach Angaben der (ADJA o?rtlichen) (NN Index))(NP-CJ die (ADJA amerikanischen) (NN Blutbad))(PP-MNR fu?r die (ADJA asiatischen) (NN Handel))(TOP (NP-SB der (NN Vorsprung) des (NN razor))(VVFIN ka?mpfen)(CNP-OA : (NN MP3-Player) (KON und) (NN Mobiltelefone)).
)Figure 5: Tree fragments containing the first five con-straint failures found on the baseline 1-best outputet al, 2002) with a single reference.Table 2 shows the results for the three constrainedtest tests.
The p-values were calculated using pairedbootstrap resampling (Koehn, 2004).
We suspectthat the substantially lower baseline scores on thenewstest2011 test set are largely due to recency ef-fects (since we use 2010 data for training).To gauge the frequency of agreement violationsin the baseline output we matched constraint rulesto the 1-best baseline derivations and performed abottom-up evaluation for each target-side tree.
Forthe three constrained test sets, newstest2009, new-stest2010, and newstest2011, we found that 15.5%,14.4%, and 15.6% of sentences, respectively, con-tained one or more constraint failures.
Figure 5shows the tree fragments for the first five failuresfound in newstest2009.In order to explore the interaction of the constraintmodel with search we then repeated the experimentsfor varying cube pruning pop limits.
Figure 6 showshow the mean test set BLEU score varies against poplimit.
Except at very low pop limits, the soft con-straint system outperforms the hard constraint sys-tem.
Together with the high p-values for the hardconstraint system, this suggests that, despite filter-ing, our simple constraint extraction heuristics maybe introducing significant numbers of spurious con-straints.
Alternatively, enforcing the hard constraintmay eliminate too many hypotheses that cannot besatisifactorily substituted ?
constraint-satisfying al-ternatives frequently differ in more than just inflec-tion.
Either way, the soft constraint model is able toovercome some of these deficiencies by permittingsome constraint failures in the 1-best output.223newstest2009-20 newstest2010-20 newstest2011-20Experiment BLEU p-value BLEU p-value BLEU p-valuebaseline 15.34 - 15.65 - 12.90 -hard constraint 15.49 0.164 15.95 0.065 12.87 0.318soft constraint 15.67 0.006 15.98 0.009 13.11 0.053Table 2: BLEU scores and p-values for the three test sets14.414.514.614.714.814.9150 500 1000 1500 2000 2500AvgBLEUPop limitbaselinesoft constrainthard constraintFigure 6: Cube pruning pop limit vs average BLEU score7 ConclusionIn this paper we have presented an SMT model thatallows the addition of linguistic constraints to thetarget-side of a conventional string-to-tree model.We have developed a simple heuristic method to ex-tract constraints for German and demonstrated theapproach on a constrained translation task, achiev-ing a small improvement in translation accuracy.In future work we intend to investigate the de-velopment of constraint models for target languageswith more complex inflection.
Besides the require-ment for suitable language processing tools, this re-quires the development of reliable language-specificconstraint extraction techniques.We also plan to investigate how the model couldbe extended to generate inflection during decoding:a complementary constraint system could curb theovergeneration of surface form combinations thathas limited previous approaches.AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
Thiswork was supported by the EuroMatrixPlus projectfunded by the European Commission (7th Frame-work Programme) and made use of the resourcesprovided by the Edinburgh Compute and Data Facil-ity.4 The ECDF is partially supported by the eDIKTinitiative.5 This work was also supported in part un-der the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-C-0022.
The first author was supported by anEPSRC Studentship.ReferencesOndr?ej Bojar and Jan Hajic?.
2008.
Phrase-basedand deep syntactic english-to-czech statistical machinetranslation.
In StatMT ?08: Proceedings of the ThirdWorkshop on Statistical Machine Translation, pages143?146, Morristown, NJ, USA.
Association for Com-putational Linguistics.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the workshop on treebanksand linguistic theories, pages 24?41.J.-C. Chappelier and M. Rajman.
1998.
A generalizedcyk algorithm for parsing stochastic cfg.
In Proceed-ings of the First Workshop on Tabulation in Parsingand Deduction, pages 133?137.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical report, Harvard University.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.4http://www.ecdf.ed.ac.uk5http://www.edikt.org.uk224David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Comput.
Linguist., 33(2):201?228.Sharon Goldwater and David McClosky.
2005.
Improv-ing statistical mt through morphological analysis.
InHLT ?05: Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing, pages 676?683, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Yvette Graham, Anton Bryl, and Josef van Genabith.2009.
F-structure transfer-based statistical machinetranslation.
In In Proceedings of Lexical FunctionalGrammar Conference 2009.Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,Alok Parlikar, and Alon Lavie.
2009.
An im-proved statistical transfer system for french?englishmachine translation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, StatMT?09, pages 140?144, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Mark Hopkins and Greg Langmead.
2010.
SCFG decod-ing without binarization.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing, pages 646?655, Cambridge, MA,October.
Association for Computational Linguistics.Kevin Knight.
1989.
Unification: a multidisciplinarysurvey.
ACM Comput.
Surv., 21(1):93?124.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In In Proceedings of EMNLP, 2007.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL?03: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 48?54, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Morristown, NJ, USA.
Association forComputational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit 2005.Alon Lavie.
2008.
Stat-xfer: a general search-basedsyntax-driven framework for machine translation.
InProceedings of the 9th international conference onComputational linguistics and intelligent text process-ing, CICLing?08, pages 362?375, Berlin, Heidelberg.Springer-Verlag.Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.2010.
A hybrid morpheme-word representationfor machine translation of morphologically rich lan-guages.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 148?157, Cambridge, MA, October.
Associa-tion for Computational Linguistics.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: statistical machine trans-lation with syntactified target language phrases.
InEMNLP ?06: Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing,pages 44?52, Morristown, NJ, USA.
Association forComputational Linguistics.Einat Minkov, Kristina Toutanova, and Suzuki Hisami.2007.
Generating complex morphology for machinetranslation.
In Proceedings of the ACL.Sonja Nie?en and Hermann Ney.
2001.
Toward hier-archical models for statistical machine translation ofinflected languages.
In Proceedings of the workshopon Data-driven methods in machine translation, pages1?8, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Stefan Riezler and John T. Maxwell, III.
2006.
Gram-matical machine translation.
In Proceedings of themain conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, pages 248?255, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProceedings of the 20th international conference onComputational Linguistics, COLING ?04, Strouds-225burg, PA, USA.
Association for Computational Lin-guistics.Stuart M. Shieber.
1984.
The design of a computer lan-guage for linguistic information.
In Proceedings of the10th international conference on Computational lin-guistics, COLING ?84, pages 362?366, Stroudsburg,PA, USA.
Association for Computational Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken Language Pro-cessing, Denver, Colorado, September 2002.David Talbot and Miles Osborne.
2006.
Modelling lex-ical redundancy for machine translation.
In ACL-44:Proceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 969?976, Morristown, NJ, USA.
Association forComputational Linguistics.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proceedings of ACL, Associ-ation for Computational Linguistics, June 2008.Andrea Zielinski and Christian Simon.
2009.
Mor-phisto ?an open source morphological analyzer forgerman.
In Proceeding of the 2009 conference onFinite-State Methods and Natural Language Process-ing: Post-proceedings of the 7th International Work-shop FSMNLP 2008, pages 224?231, Amsterdam, TheNetherlands, The Netherlands.
IOS Press.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InStatMT ?06: Proceedings of the Workshop on Statisti-cal Machine Translation, pages 138?141, Morristown,NJ, USA.
Association for Computational Linguistics.226
