LANGUAGE IDENTIFICATIONINUNKNOWN SIGNALSContact author: John Elliott, jre@scs.leeds.ac.ukCo-authors: Eric Atwell, eric@scs.leeds.ac.ukBill Whyte, billw@scs.leeds.ac.ukOrganisation: Centre for Computer Analysis of Language and Speech,School of Computer Studies, University of Leeds, Leeds, Yorkshire, LS2 9JT EnglandAbstractThis paper describes algorithms and software developed to characterise and detect genericintelligent language-like features iu an input signal, using Natural Language Learningtechniques: looking for characteristic statistical "language-signatures" in test corpora.
As afirst step towards such species-independent language-detection, we present a suite ofprograms to analyse digital representations of a range of data, and use the results toextrapolate whether or not there are language-like structures which distiuguish this data fromother sources, such as nmsic, images, and white noise.
We assume that generic species-independent commuuication can be detected by concentrating on localised patterns andrhythms, identifying segments at the level of characters, words and phrases, withoutnecessarily having to "understand" the content.We assume that a language-like signal will be encoded symbolically, i.e.
some kind ofcharacter-stream.
Our language-detection algorithm for symbolic input uses a number ofstatistical clues: data compression ratio, "chunking" to find character bit-length andboundaries, and matching against a Zipfian type-token distribution for "letters" and "words".We do not claim extensive (let alne exhaustive) empirical evidence that our language-detection clues are "correct"; the only real test will come when the Search for Extra-Terrestrial Intelligence finds true alien signals.
If and when true SETI signals are found, thefirst step to interpretation is to identify the language-like f atures, using techniques like theabove.
Our current research goal is to apply Natural Language Learning techniques to theidentification of "higher-level" grammatical nd semantic structure in a linguistic signal.IntroductionA useful thought experiment is to imagineeavesdropping on a signal from outerspace.
How can you decide that it is amessage between intelligent life forms,without dialogue with the source?
What isspecial about the language signal thatseparates it fiom non-language?
Whatspecial 'zone' in the signal universe doeslanguage occupy?
Is it, indeed, separablefrom other senti-structured sources, suchas DNA and music (fig 1).Solving this problem might not only beuseful in the event of detecting suchsignals fiom space, but also, bydeliberately ignoring preconceptions based011 human texts, may provide us with somebetter understanding of what languagereally is.The Signal Universettowever, we ueed to start somewhere, andour initial investigations - which this papersummarises make some basicassumptions (which we would hope torelax in later research).
Namely, thatidentifiable script will be a serial string,possessing a hierarchy of elements broadlyequivalent to 'characters,' 'words', and1021'spaces', and possess something akin tohuman grammar .Identifying the 'Character Set'In 'real' decoding of unknown scripts it isaccepted that identifying the correct set ofdiscrete symbols is no mean feat(Chadwick 1967).
To make life simple forourselves we assume a digital signal with afixed number of bits per character.
Verydifferent techniques are required to dealwith audio or analogue equivalentwaveforms (Elliott & Atweli 99, 00).
Wehave reason to believe that the followingmethod can be modified to relax thisconstraint, but this needs to be testedfurther.The task then reduces to trying to identifythe number of bits per character.Suppose the probability of a bit is P~.
Thenthe message ntropy of a string of lengthN will be given by:E = SUM \[PI In Pi\]; i =I,NIf the signal contains merely a set ofrandom digits, the expected wflue of thisfnnctiou will rise monotonically as Nincreases.
However, if the string contains aset of symbols of fixed length representinga character set used for communication, itis likely to show some decrease in entropywhen analysed in blocks of this length,because the signal is 'less random' whenthus blocked.
Of course, we need toanalyse blocks that begin and end atcharacter boundaries.
We simply carry outthe measurements in sliding windowsalong the data.
In figure 2 below, we seewhat happeus when we.
apply this tosamples of 8-bit ASCII text:iEnlropy Figure 2Lallgtlage{ II4 5 6 7 8 9Bit LengthEntropy profile as an indicalor of character bit-lengthWe notice a clear drop, as predicted, for abit length of 8.Modest progress though itmay be, it is not unreasonable to assumethat the first piece of evidence for thepresence of language-like sO'ucture,would be the identification of a low-entropy, character set within the signal.Identifying 'Words'Again, work by crytopaleologists suggeststhat, once the character set has been found,the separation into word-like units, is nottrivial and again we cheat, slightly: weassume that the language possessessomething akin to a 'space' character.Taking our entropy measurementdescribed above as a way of separatingcharacters, we now try to identify the one,which represents 'space'.
It is notunreasonable to believe that, in a word-based language, it is likely to be one of themost frequently used characters.Using a uumber of texts in a variety oflanguages, we first identified the top threemost used characters.
For each of these wehypothesised in turn that it represented'space'.
This then allowed us to segmentthe signal into words-like units ('words't'o1" simplicity).
We coukl then compute thefrequency distribution of words as afunction of word length, for each of thethree candidate 'space' characters (fig 3).Figure 3: Candidate word-lcnglh dishibulionsusing the 3 most frequent characters.40035Oo ~ 3oo: ~ 250g 200i , ,  ~ 150i 1 O0l 50I 0IIIt can be seen that  one 'separator'candidate (unsurprisingly, in fact, the mostfrequent character of all) results in a veryvaried distribution of word lengths.
This isan interesting distribution, which, on theright hand side of the peak, approximatelyfollows the well-known 'law' according toZipf (Zipf, 1949), which predicts this1022behaviour o i l  the grounds of minimumefl'ort in a communication act.To ascertain whether the word-lengthfrequency distribution holds for hmguagein general, nmltiple salnples from 20different hmguages fi'om Indo-European,Bantu, Semitic, Finno-Ugrian and Malayo-Polynesian groups were analysed (fig 4).Word lenglh dislribulions in mulliplc samples fl'omlndo-Eurol~ean, Semitic, l:inno-Ugrian, and Malayo-I'olynesian language groups20.00 :'15.oo10.00o5.00i:: ::y-: .
.0 .00  :v  '1 ' ,~ , '  i - :  'i ' '~' -r ..... ,?
- "~  I'~ 0 03 r,D 03 C',l LOOd Cxl Figure 4 Word lengthUsing statistical measures of signil'icance,it was found that most groups fell wellwithin 5% limits - only two individualhmguages were near exceeding theselimits -- of the proposed Human languageword-length profile shown in fig 5.Figure 5: Itulnan language word-lengfl~frequency distribution profile10.00t~ 5.00 f r l\[H .
.
.
.
.
.
.
.
.
.
i 0.00  ~ , ~ ~ ,Word l ength  liZipf's law is a strong indication oflanguage-like behaviour.
It can be usedto segment the signal ptvvided a 'space'character exists.However, we shotdd not assume Zipf to bean infifllible language detector.
Othernatural phenomena such as moleculardistribution in yeast DNA possesscharacteristics of power laws.
Analyses ofprotein length distributions also displayPoisson distributions where the number ofproteins is plotted against the lengths ofamino acids (Jenson 1998).Identifying 'Phrases'Although alien brains may be more oi lesspowerlhl than ours (Norris 1999), it isreasonable to assume that all intelligentDoblem solvers are subject to the sameultimate constraints of computatioualpower and storage and their symbolsystems will reflect his.Thus, language must use small sets ofrules to generate a vast world ofimplications and consequences.
Perhapsits most ilnportant single device is the useof embedded clauses and phrases (Minsky1984), with which to represent anexpression or description, howevercomplex, as a single component of anotherdescription.In serial languages, this appears to beachieved by clustering words into 'chunks'(phrases, sentences) of information, whichare more-or-less consistent and self-contained elements of thought.lVurthermore, in human language at least,these 'chunks' tend to consist of contelzlterms, which describe what the chunk is'about' and .fimctional terms, whichattribute references aud context by whichtile content erms convey their informationunambiguously.
'King' is usually acontent erm; 'of' and 'the' are functional.We use 'term' rather than word, becausemany languages make far less use of fullwords for l'unctional operations than doesEnglish: in Latin the transformation 'rex'('king') to 'regis' (of the king) is one suchexample.Functional terms in a language tend to beshort, probably attributable to the principleof least effort, as they are used frequently.A further distinguishing characteristic offunctional and content terms is thatdifferent texts will often wtry in theircontent but tend to share a commonlinguistic structure and therefore makesimilar use of functional terms.
That is, theprobability distribution of content termswill vary from text to text, but thedistribution of ftmction terms will not.Using English text, which had beenenciphered using a simple substitutioncipher (to avoid cheating), we identified1023across a variety of texts, the most commonwords, with least inter-text variation.These we call 'candidate function words'.Now, suppose these words occurred atrandom in the signal: we would expect osee the spacing between them to be merelya ftmction of their individual probabilitiesof occurrence.
Analysing this statistically(as a Poisson distribution) or simplysimulate it practically, we find that thereare a non-insignificant number of caseswherein there are very large gaps (of theorder of several tens of words) betweensuccessive occurrences.
Compare this withthe results from our analysis (fig 6).I\[ll,,.3 4 5 6 7 8 9Figure 6FunctionWordseparationinEnglish.Number of words between candidate flmclional wordsinitial findings show that the frequencydistribution of these lengths of text - ourcandidate phrases - follow a Zipfiandistribution curve and rarely exceedlengths of more than eight.We might conclude from this, that ourbrains tend to "chunk' linguisticinformation into phrase-like structures ofthe order of seven or so word units long.Interestingly enough, this fits in well withhuman cognition theory (Ally & Bacon1991), which states that out: short-termmental capacity operates well only up to 7(+ or -  2) pieces of information, but anycausal connection between this and ourresults must be considered highlyspeculative at this stage!Directions for Future ResearchWe are familiar with parts of speech(commonly, 'nouns', verbs' etc) inlanguage.
Identification of patternsindicative of these would be furtherevidence of language-like characteristicsand, by allowing us to group together thenumerous word tokens in any languageinto smaller, more manageable collectionswould facilitate statistical analysis.
Someattempts have been made in the past to usen-gram probabilities in order to defineword classes or 'parts of speech'(Charniak 1993).In our own work we have begun thedevelopment of tools that measure thecorrelation profile between pairs of words,as a precursor to deducing generalprinciples for 'typing' and clustering intosyntactico-semantic classes.Correlation profile for word pairP(wl,w2 ) P(w2,wl)The figure 7 above shows the results forthe relationship between a pair ofunknown (because of tile substitutioncipher approach) content and functionalwords, so identified by looking at theircross-corpus statistics as described above.It can be seen that the functional word hasa very high probability of preceding thecontent word but has 11o instance ofdirectly following it.
At leastmetaphorically, the graph can beconsidered to show the 'binding force'between the two words varying with theirseparation.
We are looking at how thismetaphor might be used in order todescribe language as a molecular structure,whose 'inter-molecular forces' can berelated to part-of-speech interaction andthe development of potential semanticcategories for the unknown language.So far we have mainly been working withEnglish, but we have begun to look at1024languages which represent their flmctionalrelationships by internal changes to wordsor by the addition of prefixes or suffixes.Although the process for separating intofunctional and content terms is morecomplex, we believe the fundamentalresults should be consistent.
This will beone test of the theories presented above.In general, we realise that testing ourhmguage detection algorithms will be asignificant issue.
We do not haveexamples that we know to be definitelyfrom non-hulnan, but intelligent origins,and we need to look extensively at signalsof non-intelligent origin which may mimicsome of the language characteristicsdescribed above.
This will form asignificant part of our fllture work and wewelcome discussion and suggestions.ConclusionLanguage in its written format has provedto be a rich source for a variety ot'statistical analyses - some more conclusivethan others - which when combined, give acomprehensive algorithm for identifyingthe presence of language-like systems.Analysis stages include compression,entropy profile, type-token distribution,word-length Zipfian analysis, finding afiequency distribution signature bysuccessive chunking, stemming, cohesionanalysis, phrase-length fiequencydistribution and pattern comparison acrosssmnples.REFERENCESAlly & Bacon, Cognitive Psychology,(third edition), Solso, Massachusetts,USA, 1991.Baldi, P., & Brunak, S., Bioinformatics -The Machine Learning Approach, MITpress, Cmnbridge Massaclmsetts, 1998.Chadwick, J. Tim Decipherment of LinearB, Cmnbridge University Press, 1967.Charniak E., Statistical language learningBradford/MIT Press, Cambridge.
1993.Elliott, J & Atwell, E, Language insignals: the detection of generic species-independent intelligent language featuresin symbolic and oral communications,Proceedings of the 50 th InternationalAstronautical Congress, paper IAA-99-IAA.9.1.08, International AstronauticalFederation, Paris, 1999.Elliott, J & Atweli, E., Is anybody outthere?
: the detection of intelligent andgeneric language-like f atures, Journal ofthe British Interplanetary Society, Vo153No 1 &2.Elliott, J, Decoding the MartianChronicles, MSc project report, School ofComlmter Studies, University of Leeds1999.Hughes J & Atwell E., The automatedewtluation of inferred word classificationsin Proceedings of the EuropeanConference on Artificial Intelligence(ECAI'94), pp550-554, John Wiley,Chichester.
1994.Jenson, H. Sell' Organised Criticality,Cambridge University Press, 1998.Minsky, M., Why Intelligent Aliens willbe Intelligible, Cambridge UniversityPress, 1984.Norris, R,.
How old is ET?, Proceedings of50th International Ashonautical Congress,paper 1AA-99-IAA.9.1.04, InternationalAstronautical Federation, Paris.
1999.Zipf, G. K., Human Behaviour and ThePrinciple of Least Effort, Addison WesleyPress, New York, 1949 (1965 reprint)1025
