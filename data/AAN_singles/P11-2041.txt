Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 236?241,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsSemi-Supervised Modeling for Prenominal Modifier OrderingMargaret MitchellUniversity of AberdeenAberdeen, Scotland, U.K.m.mitchell@abdn.ac.ukAaron DunlopOregon Health & Science UniversityPortland, ORdunlopa@cslu.ogi.eduBrian RoarkOregon Health & Science UniversityPortland, ORroark@cslu.ogi.eduAbstractIn this paper, we argue that ordering prenom-inal modifiers ?
typically pursued as a su-pervised modeling task ?
is particularly well-suited to semi-supervised approaches.
Byrelying on automatic parses to extract nounphrases, we can scale up the training databy orders of magnitude.
This minimizesthe predominant issue of data sparsity thathas informed most previous approaches.
Wecompare several recent approaches, and findimprovements from additional training dataacross the board; however, none outperforma simple n-gram model.1 IntroductionIn any given noun phrase (NP), an arbitrary num-ber of nominal modifiers may be used.
The order ofthese modifiers affects how natural or fluent a phrasesounds.
Determining a natural ordering is a key taskin the surface realization stage of a natural languagegeneration (NLG) system, where the adjectives andother modifiers chosen to identify a referent must beordered before a final string is produced.
For ex-ample, consider the alternation between the phrases?big red ball?
and ?red big ball?.
The phrase ?bigred ball?
provides a basic ordering of the words bigand red.
The reverse ordering, in ?red big ball?,sounds strange, a phrase that would only occur inmarked situations.
There is no consensus on the ex-act qualities that affect a modifier?s position, but it isclear that some modifier orderings sound more natu-ral than others, even if all are strictly speaking gram-matical.Determining methods for ordering modifiersprenominally and investigating the factors underly-ing modifier ordering have been areas of consider-able research, including work in natural languageprocessing (Shaw and Hatzivassiloglou, 1999; Mal-ouf, 2000; Mitchell, 2009; Dunlop et al, 2010), lin-guistics (Whorf, 1945; Vendler, 1968), and psychol-ogy (Martin, 1969; Danks and Glucksberg, 1971).A central issue in work on modifier ordering is howto order modifiers that are unobserved during sys-tem development.
English has upwards of 200,000words, with over 50,000 words in the vocabulary ofan educated adult (Aitchison, 2003).
Up to a quar-ter of these words may be adjectives, which poses asignificant problem for any system that attempts tocategorize English adjectives in ways that are usefulfor an ordering task.
Extensive in-context observa-tion of adjectives and other modifiers is required toadequately characterize their behavior.Developers of automatic modifier ordering sys-tems have thus spent considerable effort attemptingto make reliable predictions despite sparse data, andhave largely limited their systems to order modifierpairs instead of full modifier strings.
Conventionalwisdom has been that direct evidence methods suchas simple n-gram modeling are insufficient for cap-turing such a complex and productive process.Recent approaches have therefore utilized in-creasingly sophisticated data-driven approaches.Most recently, Dunlop et al (2010) used both dis-criminative and generative methods for estimat-ing class-based language models with multiple-sequence alignments (MSA).
Training on manuallycurated syntactic corpora, they showed excellent in-domain performance relative to prior systems, anddecent cross-domain generalization.However, following a purely supervised trainingapproach for this task is unduly limiting and leadsto conventional assumptions that are not borne outin practice, such as the inapplicability of simple n-236gram models.
NP segmentation is one of the mostreliable annotations that automatic parsers can nowproduce, and may be applied to essentially arbitraryamounts of unlabeled data.
This yields orders-of-magnitude larger training sets, so that methods thatare sensitive to sparse data and/or are domain spe-cific can be trained on sufficient data.In this paper, we compare an n-gram languagemodel and a hidden Markov model (HMM) con-structed using expectation maximization (EM) withseveral recent ordering approaches, and demonstratesuperior performance of the n-gram model acrossdifferent domains, particularly as the training datasize is scaled up.
This paper presents two importantresults: 1) N-gram modeling performs better thanpreviously believed for this task, and in fact sur-passes current class-based systems.1 2) Automaticparsers can effectively provide essentially unlimitedtraining data for learning modifier ordering prefer-ences.
Our results point the way to larger scale data-driven approaches to this and related tasks.2 Related WorkIn one of the earliest automatic prenominal mod-ifier ordering systems, Shaw and Hatzivassiloglou(1999) ordered pairs of modifiers, including adjec-tives, nouns (?baseball field?
); gerunds, (?runningman?
); and participles (?heated debate?).
Theydescribed a direct evidence method, a transitivitymethod, and a clustering method for ordering thesedifferent kinds of modifiers, with the transitivitytechnique returning the highest accuracy of 90.67%on a medical text.
However, when testing acrossdomains, their accuracy dropped to 56%, not muchhigher than random guessing.Malouf (2000) continued this work, orderingprenominal adjective pairs in the BNC.
He aban-doned a bigram model, finding it achieved only75.57% prediction accuracy, and instead pursuedstatistical and machine learning techniques that aremore robust to data sparsity.
Malouf achieved anaccuracy of 91.85% by combining three systems.However, it is not clear whether the proposed or-dering approaches extend to other kinds of modi-fiers, such as gerund verbs and nouns, and he didnot present analysis of cross-domain generalization.1But note that these approaches may still be useful, e.g.,when the goal is to construct general modifier classes.Dataset 2 mods 3 mods 4 modsWSJ 02-21 auto 10,070 1,333 129WSJ 02-21 manu 9,976 1,311 129NYT 1,616,497 191,787 18,183Table 1: Multi-modifier noun phrases in training dataDataset 2 mods 3 mods 4 modsWSJ 22-24 1,366 152 20SWBD 1,376 143 19Brown 1,428 101 9Table 2: Multi-modifier noun phrases in testing dataLater, Mitchell (2009) focused on creating a class-based model for modifier ordering.
Her systemmapped each modifier to a class based on the fre-quency with which it occurs in different prenominalpositions, and ordered unseen sequences based onthese classes.
Dunlop et al (2010) used a MultipleSequence Alignment (MSA) approach to order mod-ifiers, achieving the highest accuracy to date acrossdifferent domains.
In contrast to earlier work, bothsystems order full modifier strings.Below, we evaluate these most recent systems,scaling up the training data by several orders of mag-nitude.
Our results indicate that an n-gram modeloutperforms previous systems, and generalizes quitewell across different domains.3 CorporaFollowing Dunlop et al (2010), we use the Wall St.Journal (WSJ), Switchboard (SWBD) and Browncorpus sections of the Penn Treebank (Marcus et al,1993) as our supervised training and testing base-lines.
For semi-supervised training, we automati-cally parse sections 02-21 of the WSJ treebank usingcross-validation methods, and scale up the amountof data used by parsing the New York Times (NYT)section of the Gigaword (Graff and Cieri, 2003) cor-pus using the Berkeley Parser (Petrov and Klein,2007; Petrov, 2010).Table 1 lists the NP length distributions for eachtraining corpus.
The WSJ training corpus yields justunder 5,100 distinct modifier types (without normal-izing for capitalization), while the NYT data yields105,364.
Note that the number of NPs extractedfrom the manual and automatic parses of the WSJare quite close.
We find that the overlap between thetwo groups is well over 90%, suggesting that extract-237ing NPs from a large, automatically parsed corpuswill provide phrases comparable to manually anno-tated NPs.We evaluate across a variety of domains, includ-ing (1) the WSJ sections 22-24, and sections com-mensurate in size of (2) the SWBD corpus and (3)the Brown corpus.
Table 2 lists the NP length distri-butions for each test corpus.4 MethodsIn this section, we present two novel prenominalmodifier ordering approaches: a 5-gram model andan EM-trained HMM.
In both systems, modifiersthat occur only once in the training data are given theBerkeley parser OOV class labels (Petrov, 2010).In Section 5, we compare these approaches to theone-class system described in Mitchell (2010) andthe discriminative MSA described in Dunlop et al(2010).
We refer the interested reader to those pa-pers for the details of their learning algorithms.4.1 N-Gram ModelingWe used the SRILM toolkit (Stolcke, 2002) to buildunpruned 5-gram models using interpolated mod-ified Kneser-Ney smoothing (Chen and Goodman,1998).
In the testing phase, each possible permuta-tion is assigned a probability by the model, and thehighest probability sequence is chosen.We explored building n-gram models based onentire observed sequences (sentences) and on ex-tracted multiple modifier NPs.
As shown in Table3, we found a very large (12% absolute) accuracyimprovement in a model trained with just NP se-quences.
This is likely due to several factors, in-cluding the role of the begin string symbol <s>,which helps to capture word preferences for occur-ring first in a modifier sequence; also the behav-ior of modifiers when they occur in NPs may dif-fer from how they behave in other contexts.
Notethat the full-sentence n-gram model performs sim-ilarly to Malouf?s bigram model; although the re-sults are not directly comparable, this may explainthe common impression that n-gram modeling is noteffective for modifier ordering.
We find that syntac-tic annotations are critical for this task; all n-gramresults presented in the rest of the paper are trainedon extracted NPs.Training data for n-gram model AccuracyFull sentences 75.9Extracted multi-modifier NPs 88.1Table 3: Modifier ordering accuracy on WSJ sections 22-24, trained on sections 2-214.2 Hidden Markov ModelMitchell?s single-class system and Dunlop et.
al?sMSA approach both group tokens into position clus-ters.
The success of these systems suggests that aposition-specific class-based HMM might performwell on this task.
We use EM (Dempster et al, 1977)to learn the parameterizations of such an HMM.The model is defined in terms of state transitionprobabilities P(c?
| c), i.e., the probability of transi-tioning from a state labeled c to a state labeled c?
;and state observation probabilities P(w | c), i.e.,the probability of emitting word w from a particu-lar class c. Since the classes are predicting an or-dering, we include hard constraints on class tran-sitions.
Specifically, we forbid a transition from aclass closer to the head noun to one farther away.More formally, if the subscript of a class indicatesits distance from the head, then for any i, j, P(ci |cj) = 0 if i ?
j; i.e., ci is stipulated to never occurcloser to the head than cj .We established 8 classes and an HMM Markovorder of 1 (along with start and end states) basedon performance on a held-out set (section 00 of theWSJ treebank).
We initialize the model with a uni-form distribution over allowed transition and emis-sion probabilities, and use add-?
regularization inthe M-step of EM at each iteration.
We empiricallydetermined ?
smoothing values of 0.1 for emissionsand 500 for transitions.
Rather than training to fullconvergence of the corpus likelihood, we stop train-ing when there is no improvement in ordering accu-racy on the held-out dataset for five iterations, andoutput the best scoring model.Because of the constraints on transition probabil-ities, straightforward application of EM leads to thetransition probabilities strongly skewing the learn-ing of emission probabilities.
We thus followed ageneralized EM procedure (Neal and Hinton, 1998),updating only emission probabilities until no moreimprovement is achieved, and then training bothemission and transition probabilities.
Often, we238WSJ Accuracy SWBD Accuracy Brown AccuracyTraining data Ngr 1-cl HMM MSA Ngr 1-cl HMM MSA Ngr 1-cl HMM MSAWSJ manual 88.1 65.7 87.1 87.1 72.9 44.7 71.3 71.8 67.1 31.9 69.2 71.5auto 87.8 64.6 86.7 87.2 72.5 41.6 71.5 71.9 67.4 31.3 69.4 70.6NYT 10% 90.3 75.3 87.4 88.2 84.2 71.1 81.8 83.2 81.7 62.1 79.5 80.420% 91.8 77.2 87.9 89.3 85.2 72.2 80.9 83.1 82.2 65.9 78.9 82.150% 92.3 78.9 89.7 90.7 86.3 73.5 82.2 83.9 83.1 67.8 80.2 81.6all 92.4 80.2 89.3 92.1 86.4 74.5 81.4 83.4 82.3 69.3 79.3 82.0NYT+WSJ auto 93.7 81.1 89.7 92.2 86.3 74.5 81.3 83.4 82.3 69.3 79.3 81.8Table 4: Results on WSJ sections 22-24, Switchboard test set, and Brown test set for n-gram model (Ngr), Mitchell?ssingle-class system (1-cl), HMM and MSA systems, under various training conditions.find no improvement with the inclusion of transitionprobabilities, and they are left uniform.
In this case,test ordering is determined by the class label alone.5 Empirical resultsSeveral measures have been used to evaluate theaccuracy of a system?s modifier ordering, includ-ing both type/token accuracy, pairwise accuracy, andfull string accuracy.
We evaluate full string orderingaccuracy over all tokens in the evaluation set.
Forevery NP, if the model?s highest-scoring ordering isidentical to the actual observed order, it is correct;otherwise, it is incorrect.
We report the percentageof orders correctly predicted.We evaluate under a variety of training conditions,on WSJ sections 22-24, as well as the testing sec-tions from the Switchboard and Brown corpus por-tions of the Penn Treebank.
We perform no domain-specific tuning, so the results on the Switchboardand Brown corpora demonstrate cross-domain appli-cability of the approaches.5.1 Manual parses versus automatic parsesWe begin by comparing the NPs extracted frommanual parses to those extracted from automaticparses.
We parsed Wall Street Journal sections 02through 21 using cross-validation to ensure that theparses are as errorful as when sentences have neverbeen observed by training.Table 4 compares models trained on these twotraining corpora, as evaluated on the manually-annotated test set.
No system?s accuracy degradesgreatly when using automatic parses, indicating thatwe can likely derive useful training data by automat-ically parsing a large, unlabeled training corpus.5.2 Semi-supervised modelsWe now evaluate performance of the models on thescaled up training data.
Using the Berkeley parser,we parsed 169 million words of NYT text from theEnglish Gigaword corpus (Graff and Cieri, 2003),extracted the multiple modifier NPs, and trained ourvarious models on this data.
Rows 3-6 of Table4 show the accuracy on WSJ sections 22-24 aftertraining on 10%, 20%, 50% and 100% of this data.Note that this represents approximately 150 timesthe amount of training data as the original treebanktraining data.
Even with just 10% of this data (a15-fold increase in the training data), we see acrossthe board improvements.
Using all of the NYT dataresults in approximately 5% absolute performanceincrease for the n-gram and MSA models, yieldingroughly commensurate performance, over 92% ac-curacy.
Although we do not have space to presentthe results in this paper, we found further improve-ments (over 1% absolute, statistically significant) bycombining the four models, indicating a continuedbenefit of the other models, even if none of thembest the n-gram individually.Based on these results, this task is clearlyamenable to semi-supervised learning approaches.All systems show large accuracy improvements.Further, contrary to conventional wisdom, n-grammodels are very competitive with recent high-accuracy frameworks.
Additionally, n-gram modelsappear to be domain sensitive, as evidenced by thelast row of Table 4, which presents results when the1.8 million NPs in the NYT corpus are augmentedwith just 11 thousand NPs from the WSJ (auto) col-lection.
The n-gram model still outperforms theother systems, but improves by well over a percent,while the class-based HMM and MSA approaches239are relatively static.
(The single-class system showssome domain sensitivity, improving nearly a point.
)5.3 Cross-domain evaluationWith respect to cross-domain applicability, we seethat, as with the WSJ evaluation, the MSA and n-gram approaches are roughly commensurate on theBrown corpus; but the n-gram model shows a greateradvantage on the Switchboard test set when trainedon the NYT data.
Perhaps this is due to higher re-liance on conventionalized collocations in the spo-ken language of Switchboard.
Finally, it is clearthat the addition of the WSJ data to the NYT datayields improvements only for the specific newswiredomain ?
none of the results change much for thesetwo new domains when the WSJ data is included(last row of the table).We note that the improvements observed whenscaling the training corpus with in-domain data per-sist when applied to very diverse domains.
Interest-ingly, n-gram models, which may have been consid-ered unlikely to generalize well to other domains,maintain their superior performance in each trial.6 DiscussionIn this paper, we demonstrated the efficacy of scal-ing up training data for prenominal modifier or-dering using automatic parses.
We presented twonovel systems for ordering prenominal modifiers,and demonstrated that with sufficient data, a simplen-gram model outperforms position-specific models,such as an EM-trained HMM and the MSA approachof Dunlop et al (2010).
The accuracy achieved bythe n-gram model is particularly interesting, sincesuch models have previously been considered inef-fective for this task.
This does not obviate the needfor a class based model ?
modifier classes may in-form linguistic research, and system combinationstill yields large improvements ?
but points to newdata-rich methods for learning such models.AcknowledgmentsThis research was supported in part by NSF Grant#IIS-0811745 and DARPA grant #HR0011-09-1-0041.
Any opinions, findings, conclusions or recom-mendations expressed in this publication are those ofthe authors and do not necessarily reflect the viewsof the NSF or DARPA.ReferencesJean Aitchison.
2003.
Words in the mind: an intro-duction to the mental lexicon.
Blackwell Publishing,Cornwall, United Kindgom, third edition.
p. 7.Stanley Chen and Joshua Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report, TR-10-98, Harvard University.Joseph H. Danks and Sam Glucksberg.
1971.
Psycho-logical scaling of adjective order.
Journal of VerbalLearning and Verbal Behavior, 10(1):63?67.Arthur Dempster, Nan Laird, and Donald Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society: Se-ries B, 39(1):1?38.Aaron Dunlop, Margaret Mitchell, and Brian Roark.2010.
Prenominal modier ordering via multiple se-quence alignment.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the ACL (HLT-NAACL 2010), pages 600?608, Los Angeles, CA, USA.
Association for Compu-tational Linguistics.David Graff and Christopher Cieri.
2003.
English Giga-word.
Linguistic Data Consortium, Philadelphia, PA,USA.Robert Malouf.
2000.
The order of prenominal adjec-tives in natural language generation.
In Proceedings ofthe 38th ACL (ACL 2000), pages 85?92, Hong Kong.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.J.
E. Martin.
1969.
Semantic determinants of preferredadjective order.
Journal of Verbal Learning and VerbalBehavior, 8(6):697?704.Margaret Mitchell.
2009.
Class-based ordering ofprenominal modifiers.
In Proceedings of the 12th Eu-ropean Workshop on Natural Language Generation(ENLG 2009), pages 50?57, Athens, Greece.
Associa-tion for Computational Linguistics.Margaret Mitchell.
2010.
A flexible approach to class-based ordering of prenominal modifiers.
In E. Krah-mer and M. Theune, editors, Empirical Methods inNatural Language Generation, volume 5980 of Lec-ture Notes in Computer Science.
Springer, Berlin /Heidelberg.Radford M. Neal and Geoffrey E. Hinton.
1998.
A viewof the EM algorithm that justifies incremental, sparse,and other variants.
In Michael I. Jordan, editor, Learn-ing in Graphical Models.
Kluwer Academic Publish-ers, Dordrecht.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North American240Chapter of the ACL (HLT-NAACL 2007), pages 404?411, Rochester, NY, USA.
Association for Computa-tional Linguistics.Slav Petrov.
2010.
Berkeley parser.
GNU General Pub-lic License v.2.James Shaw and Vasileios Hatzivassiloglou.
1999.
Or-dering among premodifiers.
In Proceedings of the 37thACL (ACL 1999), pages 135?143, College Park, Mary-land.
Association for Computational Linguistics.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing (ICSLP2002), volume 2, pages 901?904.Zeno Vendler.
1968.
Adjectives and Nominalizations.Mouton, The Netherlands.Benjamin Lee Whorf.
1945.
Grammatical categories.Language, 21(1):1?11.241
