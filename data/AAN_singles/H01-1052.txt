Mitigating the Paucity-of-Data Problem: Exploring theEffect of Training Corpus Size on Classifier Performancefor Natural Language ProcessingMichele Banko and Eric BrillMicrosoft Research1 Microsoft WayRedmond, WA 98052 USA{mbanko, brill}@microsoft.comABSTRACTIn this paper, we discuss experiments applying machine learningtechniques to the task of confusion set disambiguation, using threeorders of magnitude more training data than has previously beenused for any disambiguation-in-string-context problem.
In anattempt to determine when current learning methods will cease tobenefit from additional training data, we analyze residual errorsmade by learners when issues of sparse data have beensignificantly mitigated.
Finally, in the context of our results, wediscuss possible directions for the empirical natural languageresearch community.KeywordsLearning curves, data scaling, very large corpora, natural languagedisambiguation.1.
INTRODUCTIONA significant amount of work in empirical natural languageprocessing involves developing and refining machine learningtechniques to automatically extract linguistic knowledge from on-line text corpora.
While the number of learning variants forvarious problems has been increasing, the size of training setssuch learning algorithms use has remained essentially unchanged.For instance, for the much-studied problems of part of speechtagging, base noun phrase labeling and parsing, the PennTreebank, first released in 1992, remains the de facto trainingcorpus.
The average training corpus size reported in paperspublished in the ACL-sponsored Workshop on Very LargeCorpora was essentially unchanged from the 1995 proceedings tothe 2000 proceedings.
While the amount of available on-line texthas been growing at an amazing rate over the last five years (bysome estimations, there are currently over 500 billion readilyaccessible words on the web), the size of training corpora used byour field has remained static.Confusable word set disambiguation, the problem of choosing thecorrect use of a word given a set of words with which it iscommonly confused, (e.g.
{to, too, two}, {your, you?re}), is aprototypical problem in NLP.
At some level, this task is identicalto many other natural language problems, including word sensedisambiguation, determining lexical features such as pronoun caseand determiner number for machine translation, part of speechtagging, named entity labeling, spelling correction, and someformulations of skeletal parsing.
All of these problems involvedisambiguating from a relatively small set of tokens based upon astring context.
Of these disambiguation problems, lexicalconfusables possess the fortunate property that supervised trainingdata is free, since the differences between members of a confusionset are surface-apparent within a set of well-written text.To date, all of the papers published on the topic of confusion setdisambiguation have used training sets for supervised learning ofless than one million words.
The same is true for most if not all ofthe other disambiguation-in-string-context problems.
In thispaper we explore what happens when significantly larger trainingcorpora are used.
Our results suggest that it may make sense forthe field to concentrate considerably more effort into enlargingour training corpora and addressing scalability issues, rather thancontinuing to explore different learning methods applied to therelatively small extant training corpora.2.
PREVIOUS WORK2.1 Confusion Set DisambiguationSeveral methods have been presented for confusion setdisambiguation.
The more recent set of techniques includesmultiplicative weight-update algorithms [4], latent semanticanalysis [7], transformation-based learning [8], differentialgrammars [10], decision lists [12], and a variety of Bayesianclassifiers [2,3,5].
In all of these papers, the problem isformulated as follows: Given a specific confusion set (e.g.
{to,two, too}), all occurrences of confusion set members in the testset are replaced by some marker.
Then everywhere the systemsees this marker, it must decide which member of the confusionset to choose.
Most learners that have been applied to thisproblem use as features the words and part of speech tagsappearing within a fixed window, as well as collocationssurrounding the ambiguity site; these are essentially the samefeatures as those used for the other disambiguation-in-string-context problems.2.2 Learning Curves for NLPA number of learning curve studies have been carried out fordifferent natural language tasks.
Ratnaparkhi [12] shows alearning curve for maximum-entropy parsing, for up to roughlyone million words of training data; performance appears to beasymptoting when most of the training set is used.
Henderson [6]showed similar results across a collection of parsers.Figure 1 shows a learning curve we generated for our task ofword-confusable disambiguation, in which we plot testclassification accuracy as a function of training corpus size usinga version of winnow, the best-performing learner reported to datefor this well-studied task [4].
This curve was generated by trainingon successive portions of the 1-million word Brown corpus andthen testing on 1-million words of Wall Street Journal text forperformance averaged over 10 confusion sets.
The curve mightlead one to believe that only minor gains are to be had byincreasing the size of training corpora past 1 million words.While all of these studies indicate that there is likely some (butperhaps limited) performance benefit to be obtained fromincreasing training set size, they have been carried out only onrelatively small training corpora.
The potential impact to be felt byincreasing the amount of training data by any signifcant order hasyet to be studied.0.700.720.740.760.780.800.82100,000 400,000 700,000 1,000,000Training Corpus Size (words)TestAccuracyFigure 1: An Initial Learning Curve for ConfusableDisambiguation3.
EXPERIMENTSThis work attempts to address two questions ?
at what point willlearners cease to benefit from additional data, and what is thenature of the errors which remain at that point.
The first questionimpacts how best to devote resources in order to improve naturallanguage technology.
If there is still much to be gained fromadditional data, we should think hard about ways to effectivelyincrease the available training data for problems of interest.
Thesecond question allows us to study failures due to inherentweaknesses in learning methods and features rather than failuresdue to insufficient data.Since annotated training data is essentially free for the problem ofconfusion set disambiguation, we decided to explore learningcurves for this problem for various machine learning algorithms,and then analyze residual errors when the learners are trained onall available data.
The learners we used were memory-basedlearning, winnow, perceptron,1 transformation-based learning, anddecision trees.
All learners used identical features2 and were usedout-of-the-box, with no parameter tuning.
Since our point is notto compare learners we have refrained from identifying thelearners in the results below.We collected a 1-billion-word training corpus from a variety ofEnglish texts, including news articles, scientific abstracts,government transcripts, literature and other varied forms of prose.Using this collection, which is three orders of magnitude greaterthan the largest training corpus previously used for this task, wetrained the five learners and tested on a set of 1 million words ofWall Street Journal text.3In Figure 2 we show learning curves for each learner, for up toone billion words of training data.4 Each point in the graphreflects the average performance of a learner over ten differentconfusion sets which are listed in Table 1.
Interestingly, even outto a billion words, the curves appear to be log-linear.
Note thatthe worst learner trained on approximately 20 million wordsoutperforms the best learner trained on 1 million words.
We seethat for the problem of confusable disambiguation, none of ourlearners is close to asymptoting in performance when trained onthe one million word training corpus commonly employed withinthe field.Table 1: Confusion Sets{accept, except} {principal, principle}{affect, effect} {then, than}{among, between} {their, there}{its, it?s} {weather, whether}{peace, piece} {your, you?re}The graph in Figure 2 demonstrates that for word confusables, wecan build a system that considerably outperforms the current bestresults using an incredibly simplistic learner with just slightlymore training data.
In the graph, Learner 1 corresponds to atrivial memory-based learner.
This learner simply keeps track ofall <wi-1, wi+1>, < wi-1> and <wi+1> counts for all occurrences ofthe confusables in the training set.
Given a test set instance, thelearner will first check if it has seen <wi-1,wi+1> in the training set.If so, it chooses the confusable word most frequently observedwith this tuple.
Otherwise, the learner backs off to check for thefrequency of <wi-1>; if this also was not seen then it will back offto <wi+1>, and lastly, to the most frequently observed confusion-1 Thanks to Dan Roth for making both Winnow and Perceptronavailable.2 We used the standard feature set for this problem.
For detailssee [4].3 The training set contained no text from WSJ.4 Learner 5 could not be run on more than 100 million words oftraining data.set member as computed from the training corpus.
Note that with10 million words of training data, this simple learner outperformsall other learners trained on 1 million words.Many papers in empirical natural language processing involveshowing that a particular system (only slightly) outperformsothers on one of the popular standard tasks.
These comparisonsare made from very small training corpora, typically less than amillion words.
We have no reason to believe that anycomparative conclusions drawn on one million words will holdwhen we finally scale up to larger training corpora.
For instance,our simple memory based learner, which appears to be among thebest performers at a million words, is the worst performer at abillion.
The learner that performs the worst on a million words oftraining data significantly improves with more data.Of course, we are fortunate in that labeled training data is easy tolocate for confusion set disambiguation.
For many naturallanguage tasks, clearly this will not be the case.
This reality hassparked interest in methods for combining supervised andunsupervised learning as a way to utilize the relatively smallamount of available annotated data along with much largercollections of unannotated data [1,9].
However, it is as yetunclear whether these methods are effective other than in caseswhere we have relatively small amounts of annotated dataavailable.4.
RESIDUAL ERRORSAfter eliminating errors arising from sparse data and examiningthe residual errors the learners make when trained on a billionwords, we can begin to understand inherent weaknesses inourlearning algorithms and feature sets.
Sparse data problems canalways be reduced by buying additional data; the remainingproblems truly require technological advances to resolve them.We manually examined a sample of errors classifiers made whentrained on one billion words and classified them into one of fourcategories: strongly misleading features, ambiguous context,sparse context and corpus error.
In the paragraphs that follow, wedefine the various error types, and discuss what problems remaineven after a substantial decrease in the number of errors attributedto the problem of sparse data.Strongly Misleading FeaturesErrors arising from strongly misleading features occur whenfeatures which are strongly associated with one class appear in thecontext of another.
For instance, in attempting to characterize thefeature set of weather (vs. its commonly-confused set memberwhether), according to the canonical feature space used for thisproblem we typically expect terms associated with atmosphericconditions, temperature or natural phenomena to favor use ofweather as opposed to whether.
Below is an example whichillustrates that such strong cues are not always sufficient toaccurately disambiguate between these confusables.
In such cases,a method for better weighing features based upon their syntacticcontext, as opposed to using a simple bag-of-words model, maybe needed.Example: On a sunny day whether she swims or not depends onthe temperature of the water.0.750.800.850.900.951.001 10 100 1000Sizeof TrainingCorpus (Millions of Words)Test AccuracyLearner 1Learner 2Learner 3Learner 4Learner 5Figure 2.
Learning Curves for Confusable DisambiguationAmbiguous ContextErrors can also arise from ambiguous contexts.
Such errors aremade when feature sets derived from shallow local contexts arenot sufficient to disambiguate among members of a confusableset.
Long-range, complex dependencies, deep semanticunderstanding or pragmatics may be required in order to draw adistinction among classes.
Included in this class of problems areso-called ?garden-path?
sentences, in which ambiguity causes anincorrect parse of the sentence to be internally constructed by thereader until a certain indicator forces a revision of the sentencestructure.Example 1: It's like you're king of the hill.Example 2: The transportation and distribution departmentsevaluate weather reports at least four times a day to determine ifdelivery schedules should be modified.Sparse ContextErrors can also be a result of sparse contexts.
In such cases, aninformative term appears, but the term was not seen in the trainingcorpus.
Sparse contexts differ from ambiguous contexts in thatwith more data, such cases are potentially solvable using thecurrent feature set.
Sparse context problems may also be lessenedby attributing informative lexical features to a word via clusteringor other analysis.Example: It's baseball's only team-owned spring training site.Corpus ErrorCorpus errors are attributed to cases in which the test corpuscontains an incorrect use of a confusable word, resulting inincorrectly evaluating the classification made by a learner.
In awell-edited test corpus such as the Wall Street Journal, errors ofthis nature will be minimal.Example: If they don't find oil, its going to be quite a letdown.Table 2 shows the distribution of error types found after learningwith a 1-billion-word corpus.
Specifically, the sample of errorsstudied included instances that one particular learner, winnow,incorrectly classified when trained on one billion words.
It isinteresting that more than half of the errors were attributed tosparse context.
Such errors could potentially be corrected werethe learner to be trained on an even larger training corpus, or ifother methods such as clustering were used.The ambiguous context errors are cases in which the feature spacecurrently utilized by the learners is not sufficient fordisambiguation; hence, simply adding more data will not help.Table 2: Distribution of Error TypesError Type Percent ObservedAmbiguous Context 42%Sparse Context 57%Misleading Features 0%Corpus Error 1%5.
A BILLION-WORD TREEBANK?Our experiments demonstrate that for confusion setdisambiguation, system performance improves with more data, upto at least one billion words.
Is it feasible to think of ever havinga billion-word Treebank to use as training material for tagging,parsing, named entity recognition, and other applications?Perhaps not, but let us run through some numbers.To be concrete, assume we want a billion words annotated withpart of speech tags at the same level of accuracy as the originalmillion word corpus.5 If we train a tagger on the existing corpus,the na?ve approach would be to have a person look at every singletag in the corpus, decide whether it is correct, and make a changeif it is not.
In the extreme, this means somebody has to look atone billion tags.
Assume our automatic tagger has an accuracy of95% and that with reasonable tools, a person can verify at the rateof 5 seconds per tag and correct at the rate of 15 seconds per tag.This works out to an average of 5*.95 + 15*.05 = 5.5 secondsspent per tag, for a total of 1.5 million hours to tag a billionwords.
Assuming the human tagger incurs a cost of $10/hour, andassuming the annotation takes place after startup costs due todevelopment of an annotation system have been accounted for, weare faced with $15 million in labor costs.
Given the cost and laborrequirements, this clearly is not feasible.
But now assume that wecould do perfect error identification, using sample selectiontechniques.
In other words, we could first run a tagger over thebillion-word corpus and using sample selection, identify all andonly the errors made by the tagger.
If the tagger is 95% accurate,we now only have to examine 5% of the corpus, at a correctioncost of 15 seconds per tag.
This would reduce the labor cost to $2million for tagging a billion words.
Next, assume we had a wayof clustering errors such that correcting one tag on average hadthe effect of correcting 10.
This reduces the total labor cost to$200k to annotate a billion words, or $20k to annotate 100million.
Suppose we are off by an order of magnitude; then withthe proper technology in place it might cost $200k in labor toannotate 100 million additional words.As a result of the hypothetical analysis above, it is not absolutelyinfeasible to think about manually annotating significantly largercorpora.
Given the clear benefit of additional annotated data, weshould think seriously about developing tools and algorithms thatwould allow us to efficiently annotate orders of magnitude moredata than what is currently available.6.
CONCLUSIONSWe have presented learning curves for a particular naturallanguage disambiguation problem, confusion set disambiguation,training with more than a thousand times more data than hadpreviously been used for this problem.
We were able significantlyreduce the error rate, compared to the best system trained on thestandard training set size, simply by adding more training data.5 We assume an annotated corpus such as the Penn Treebankalready exists, and our task is to significantly grow it.Therefore, we are only taking into account the marginal cost ofadditional annotated data, not start-up costs such as stylemanual design.We see that even out to a billion words the learners continue tobenefit from additional training data.It is worth exploring next whether emphasizing the acquisition oflarger training corpora might be the easiest route to improvedperformance for other natural language problems as well.7.
REFERENCES[1] Brill, E. Unsupervised Learning of Disambiguation Rulesfor Part of Speech Tagging.
In Natural Language ProcessingUsing Very Large Corpora, 1999.
[2] Gale, W. A., Church, K. W., and Yarowsky, D. (1993).
Amethod for disambiguating word senses in a large corpus.Computers and the Humanities, 26:415--439.
[3] Golding, A. R. (1995).
A Bayesian hybrid method forcontext-sensitive spelling correction.
In Proc.
3rd Workshopon Very Large Corpora, Boston, MA.
[4] Golding, A. R. and Roth, D. (1999), A Winnow-BasedApproach to Context-Sensitive Spelling Correction.
MachineLearning, 34:107--130.
[5] Golding, A. R. and Schabes, Y.
(1996).
Combining trigram-based and feature-based methods for context-sensitivespelling correction.
In Proc.
34th Annual Meeting of theAssociation for Computational Linguistics, Santa Cruz, CA.
[6] Henderson, J. Exploiting Diversity for Natural LanguageParsing.
PhD thesis, Johns Hopkins University, August 1999.
[7] Jones, M. P. and Martin, J. H. (1997).
Contextual spellingcorrection using latent semantic analysis.
In Proc.
5thConference on Applied Natural Language Processing,Washington, DC.
[8] Mangu, L. and Brill, E. (1997).
Automatic rule acquisitionfor spelling correction.
In Proc.
14th InternationalConference on Machine Learning.
Morgan Kaufmann.
[9] Nigam, K, McCallum, A, Thrun, S and Mitchell, T. TextClassification from Labeled and Unlabeled Documents usingEM.
Machine Learning.
39(2/3).
pp.
103-134.
2000.
[10] Powers, D. (1997).
Learning and application of differentialgrammars.
In Proc.
Meeting of the ACL Special InterestGroup in Natural Language Learning, Madrid.
[11] Ratnaparkhi, Adwait.
(1999) Learning to Parse NaturalLanguage with Maximum Entropy Models.
MachineLearning, 34, 151-175.
[12] Yarowsky, D. (1994).
Decision lists for lexical ambiguityresolution: Application to accent restoration in Spanish andFrench.
In Proc.
32nd Annual Meeting of the Association forComputational Linguistics, Las Cruces, NM.
