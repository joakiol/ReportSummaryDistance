Toward Connectionist SemanticsGarrison W. Cottrell 1University of California at San DiegoIntreductleaMuch of the study of language has centered around the study of syntax, to the detrimentof semantics and pragmatics.
Part of the reason for this may be akin to the motivation of thebesotted gentleman on his hands and knees beneath a streetlamp, who, when queried as to whyhe is looking on the sidewalk for the keys he lost in the alley, replies: "Because the light isbetter here!"
I believe it is time to start mucking about in the alley; the keys are there.
I alsothink we have a new flashlight: Parallel Distributed Processing 2.
PDP mechanisms allow us tobuild machines whose fundamental operations include best fit search, constraint relaxation andautomatic generalization.
These are useful properties for processing language.
I think theapplication of these models to NLP will change our view of what constitutes "semantics'.
Iwill argue that in order to deal with meaning seriously, we have to move beyond the folk-psychological level of symbols, and represent the microstructure of symbols.
This is more thana granularity issue.
It also has to do with the grounding of meaning in perception.
It is on thelevel of microfeatures that I believe this grounding occurs, and PDP gives us a way to expressthis interface between language and perception.My discussion of these issues will take the following course 3.
First I describe my previouswork on word sense disambiguation i a PDP framework as a springboard for the rest of thediscussion, and to give a sense of how lexical semantics might fit into an overall parsing model.Next I motivate a new model of word meanings through an example.
I try to show that PDPhas a natural way of expressing these meanings, and I give a sketch of how connectionistsemantics could be learned.
Finally, I briefly discuss metaphor.Word sense dlsamblguatlonOne of the fundamental problems of natural language processing is word sensedisambiguation.
Determining the correct sense of a word for a particular use involves theinteraction of many sources of knowledge: syntactic, semantic and pragmatic (i.e., "everythingelse').
In previous work (Cottrell, 1985) I have shown how word sense disambiguation can bemodeled as a constraint relaxation process between competing hypotheses instantiated as nodesin a network representing linguistic knowledge.
The representation is one that I havefancifully called proclarative: disambiguation happens as the result of activation spreadingthrough a knowledge base where constraints between hypotheses are represented by positiveand negative links between them.
Figure 1 shows the bas/c structure of the model.
The modeloperates as follows: First, words activate all of their lexical entries.
These, in turn, activatesyntactic and semantic (case) structures, which represent relations between word senses.
It isfeedback from these developing representations that provides upport for the correct meaningsand syntactic lasses of the words.
At the same time, bindings of constituents o roles in bothsyntax and semantics are mutually constraining one another to decide such things asprepositional phrase attachment.
Thus parsing into a case structure is modeled as a three wayconstraint relaxation between the lexical entries of the words, the possible syntacticrepresentations, and the possible semantic relations.
Syntactic and semantic information areaccessed in parallel, and operate s/multancously to determine the correct parse.
This wasI would like to thank Mike Mozer, Harold Pashler, and Dave Rumelhart for helpful comments on this paper.Any haziness that remains is mine.2\[ will resume familiarity with the coonectionist, or PDP paradigm.
The best introduction is Rumelhart andMcCleUand (1986).3I will restrict myself here to lexical semantics.
The generalization to logical form ia left as an ~erciae for thereader.65Figure 1.
Sources of knowledge and constraint paths for disambiguation.shown to be a useful model of the human disambiguation process, as evidenced by explanationsof various psycholinguistic and neurolinguistic results.One of the major weaknesses of that model was the representation f "meaning'.
Eachmeaning of a word is represented by a unit with an "awkward lexeme" (WilLs, 1976) as a label.Certainly, the label on a node is not important; it is the way the node connects up to othernodes that determine its relationship to other "meanings'.
But I think this is a general failingof almost all NLP programs currently in existence: the meaning of a word is best representednot  as a symbol, but as an aggregate of connected mlcrofeatures.
I will next try to show why.What  Is mean ing?
(A  thought experiment)It has been said that all words are polys~nous to a degree.
Let's take a fairly saleexample: truck.
This seems hardly polysemous, but it turns out we can bend the meaning, atleast the image formed, in fairly continuous ways.
Consider Billy picked up the truck.
If youare like me, you get a picture of a small, probably pl~tic truck.
In a symbolic system we mighthave a rule that if a usually large object is the obiect of a picking up action, then we should"toy-iCy" i t ,  either looking up the entry for "toy truck" or by apply/ng a "toy/fication"transformation to the repres~tation we had already retrieved: it weighs less, it is much smaller,it is composed of plastic.
Of course, in S,,perman picked up the truck, we have an exception tothe rule.
And in Bobby picked up the roy gun, the application of the toy-ify/ng rule would needto be modified so that the size is not reduced.
One can imagine that the list of rules and theirapplication criteria might get a bit unwieldy.One answer to this is, "Yes, the world is compL(cated."
The problem is that this is not anisolated phenomenon.
Rather, it pervades our conceptual landscape.
The concepts thatpeople use are not fixed entities, nor are they entities that vary discretely along a small numberof dimensions.
They covary in a continuous way.
In Tommy lugged the truck up dze hill, weimagine a heavier toy truck than the one Billy picked up, but a lighter one than Superman did.It might even be the same truck - Billy picked up the t ruck  and handed it to Tommy.
Tommylugged it up the hill."
In this case it is Tommy that we imagine is smaller than Billy!
Thus theinterpretation we derive of the words in a sentence is the result of constraints between themeanings of the individual words, as well as the usual list: the structure of the sentence, thecontext in which it is spoken, the relationship between the speaker and the hearer, the sharedknowledge, etc.
People are very good at tasks like this that involve the application of multiple,simultaneous constraints.
I claim that the "rules" that I attempted to describe above canemerge from the regularities of interaction among the internal structures of the conceptsthemselvea, rather than an application of explicit rules to atomic concepts 4.
There is no reasonthat this could not be implemented in a "symbolic" system that has a constraint propagationmechanism, and continuous-valued l vels of properties.
The problem is that the modificationwould alter it so radically that we might as well have started with a conneetionist model s.4I am not claiming these arc simply first order interactions; relations b?twccn fcatu~ ?
lu~m also need to becaptured.5Another reason for starting with a conncctionist model is the existence of powerful learning algorithms thatcan derive constraints between features, as we will son bc/ow.66A modest proposalIn this section I will draw on previous work of others to lay out how a connectionlstmodel can represent the kind of meanings that I think our experiment with truck point to.The basic idea is that meanings arc connectionist schemata.
These are assumed to beembedded in a system like the one I described above for word sense disamblguation - that is,they arc getting input from other schemata concerned with syntax and larger semantic (case)structures.Conneetionlst 5chernma.
Rumelhart et al (1986) have demonstrated how a connectionlstmodel of a schema can do something no implementation has done before: represent smoothlyvarying constraints between the slot fillers.
The demonstration model represents theinformation we have about rooms.
Each unit of the model represents one of forty possibledescriptors and contents of a room: size, walis, ceiling, bathtub, stove, etc.
The connectionstrengths between the units of the schema model were derived from people's reports of whatthey expected to find in each kind of room.
(The weights were set according to theconditions\] probability that one item was reported given another item was reported.)
Thingsthat occurred together often were given a strong positive weight, things that never occurredtogether were given a negative weight.
For exzmple, every room has walls and a ceiling.
Thesehave a strong positive connection between them because they always co-occur.
Probing themodel consists of "clamping on" some units, which then activate positively connected units,and inhibit ones negatively associated with them.
The office schema, for example, can beaccessed by probing the model with "desk" (and "ceiling', to simulate the context is "room')(se?
Figure 2).
The "prototype" rooms are shown to be peaks in a "goodness surface" in thespace of unit activations that reflects the number of constraints satisfied between units of themodel.
The activation of the units travels up the goodness surface to the corner where theelements of the office schema become activated.
This type of pattern completion is a typicalway to access information in connectionist models.An interesting variation on.
this is when two items arc probed together that do notnormally co-occur.
For example, if the model is probed with "bed" and "sofa" what rcsults is alarge bedroom with a fireplace.
The goodness pac.c has been warped by these two inputs toform a new stable peak, where the filler of one of the slots, "size-of-room', has constra/nedwhat wiU be in the contents ?
of the room in a way that is intuitively pleasing.. .
.
.
.
.
.
.
.
oven~Lg.
.
.
L .g .L~OI"~I  I - , -C~ 1 I computer?
* * .
.
.
.
.
.
.
.
.
.
.
.
GoOt--hOnget?
stole~,Olletball, tubtetevlslon.
.
.
.
.
.
dreSSe  r? "
coffee--potCuDDaordtoasterrefrigerators inkStovetirades" flr e--pi<3ce?
?
?
a a a 0 {3C3~,"~r '~m~'~"  ~ osn--tray" Q Q O O r3 C \ ]C~ ~ ~."
'?
- " - " .~1~ car fee--cupcosy--chairsofafloor --lamp?
?
OQ~DC~'  q ~f ,, +~L l l  l{ : .
.
.
.  '
J pictureclock~\[~ '  d il H tU  = o = = o 0 C~L-_---~,__--~--I books?
_ = ~ =  coroettypewriterbed.
.
.
.
very--small.
.
.
.
.
.
.
.
.
.
.
.
.
.
=moll.
.
.
.
.
.
.
.
.
.
.
.
.
medium?
-  O OO{'~l .
IJ I1 J * .
l ~  large.
.
.
.
.
very--largewindowwaf tsceitlngFigure 2.
Probing the mode\[ with "desk" and "ceiling'.
The size of the square indicates activa-tion value of the unit, and time moves from left to light.
(From McClelland & Rumeihart,1986, reprinted by permission).67. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
oven.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
computer.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
coot -hanger.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
s?ole.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
toilet.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
bott~tub?
?
OmOOOOOOfl-YmO000000 television~ n n ~ 1 2 1 ~ O ~ r ~ Q  dresser.
.
.
.
.
.
.
.
.
.
.
cof fee -pot.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
cupOoord.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
toaster.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
refr igerator.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sink.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
stove?
OOOi=lO00O~O00000OO drapes?
O 0 0 0 0 O O O C t m o \ [ ' 3 0 ~ .
~ O 0 0  tire-ptace.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o s h - t r ~ /.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
co l  fee--~qpaOOOOOOOOOOOmO0000 ?~sy-chmr00000000000"100000000 solo- .
O0000000000OO0000 ftoor-lamp?
?
onnoononr'~O0000000 picture?
.
+  + ?
o o o o o o o o o O O n O 0 0  clock.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
desk--choir?
0000000000200000000 books?
o00ncloOOr-~O0000000 corpet 0 f'lO000 Or-iO f"rlOn rli'30 Ore0 bookshelf.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
tyep~writ erO 0 0 m m o m m o o o o o o m o o n r n o. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
telephone.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
desk.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
very--stool!.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
smal l. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
medium?
.
@O00r'IC~OfT301~O1"lorlr'lO IorcJevery-lor(je : 0060060066066600o60 +~.~o.O000000000nno0000000 door?
o00rno0000000000000o wo.sO000OOOC1120r-lmO000001~\]O ?e i* ino JFigure 3.
Probing the model with "bed" and "sofa'.
(From McCIclland & Rumclhart, 1986, re-printed by permission).It is possible to train a connectionist model to exhibit this blending of meanings, and todo so at the more micro-level I am advocating for word s,~ses.
McClelland & Kawamoto(1986) trained a network to assign cam= roles to nouns presented in a matrix as VERB-SUB.l-OB J-MODIFIER.
The representation of the input was a set of features for each syntactic slotthat were linked to output feature schemata for each case role.
The model was trained on aset of sentences in this format, and then tested on novel sentences.
When given the novelsentence the doll "moved, the model interpreted the doll as animate, because of the sharedfeatures between doll and humans and a tendency to assign animaey to agents.
Thus, themodel adjustccl the meaning to fit the situation.
The point is, distributed connectionistrepresentations that represent symbols such as "doll" as a set of features and constraints canrelax those constraints depending on external constraints - inputs from combinations offeatures in the schemata of the other words in the sentence.These models assumed the elements of the schemata - the micro-features - were chosen bythe modeler.
The next section deals with how the features themselves might be learned, andhow they might be grounded in perceptual processes.Learning.
A problem with any representation of meaning in terms of features is theinfinite regression of fcatur?~ defined in terms of features.
What is the basis clause of theinductive process of building a semantic reprepreseatation?
I believe that semantics mustfundamentalIy be based in perception of and interaction with the environment.
Powerful newalgorithms have been discovered that allow connectionist networks to develop thdr owninternal representations of their environment.
Surprisingly, a rather useful network is one thatdoes an identity mapping (Figure 4).
The network has an input and output layer connotedthrough a smaller layer of hidden units.
By forcing the network to reproduce the input on theoutput through this narrow channel, it has to learn a, efficient encoding of the input at thehidden unit layer.
Such networks are self-organizing systems that learn to represent theimportant features of their environment.These systems have bean used to encode natural images and speech signals (CottreU,Munro & Zipscr, to appear; Eiman & Zipser, 1986).
The internal representations devised byth?~ two systems (auditory and visual) can then be the "environment" to a third system whichwould take into account covariances between the two of them in a unified abstract =acoding of68?
?
?
~ N Jnpu!
Unil~Figure 4.
A network that develops an efficient encoding of its environment.sound and light (see Figure 5) ~.
Now it will only take one of the input modalities to evoke theother.
The input of an image would activate the image encoding, which in turn would partiallyactivate the unif.,'xl encoding of associated sounds and images.
This can be filled out bypattern completion, enabling the unified encoding to feed back and activate the encoding ofthe word associated with the image.
That is, an image will evoke a word and a word an image.WhiIe this is an oversimplified sketch, the important point is that conneetionist systems use auniform representation medium for both modalities, and thus afford the modeler an ease ofcommunication between visual, proprloceptive and auditory inputs.
Thus, this approachpromises a computationally viable way to ground the infinite regress of meaning in associationsbetween speech sounds with other perceptual representations generated from interactions withthe environment While this is just the base case of the induction, it has not been addressedby other approaches.Metaphor.
A second problem for a model of meaning is the question of metaphor.
Howcould a connectionist system learn the metaphorical mappings that are such a big part oflanguage?
Connectionist schemata that have many stable states reflecting related meanings mayaccount for much of what we call "metaphor".
But how might new meanings be learned thatare more radical transformations of old ones?
For example, how might we learn that I feel uptoday means one's mood is devated?
Our identity mapping networks can be put to good usehere in the folIowing way 7.
Suppose we div/de up the input pattern in Figure 4 into portionscorresponding to a function, an input and an output.
So the triple (1: a b) represents F(a) =b, and given (F a b) the network produces (F a b).
It' we add a pattern completion etwork~ua/CodingEncoding00?
?
dVisuml Input  "Word I~pucFigure 5.
Automatically learned inter-modal encoding.A similar idea has bern independently proposed by Chauv/n (198.T).
"/The following network is implemented, -, McClelland wou/d say, in "hopcwarc'.69on the output layer, we can now give the network (F a *) (where " represents no input) and itwill produce F(a,b), computing that F(a) equals b.
In fact, within resource limitations, we cangive it F(*,b) or even "(a,b) and have it invert the mapping or induce the relationship betweenthe arguments.
In ambiguous cases it will produce blends of the possible answers.Now, assume that we have enough units in the argument positions that we can representanything we want, and that we have trained it with functions and arguments from severaldisparate domains.
Suppose we now give the network a function F with an argument c that isnot in the domain of F. One characteristic of these networks is that they map similar inputsto similar outputs.
The degree of overlap between the features of c and the features ofelements of the domain of F will determine the coherency of the mapping.
If c is sufficientlysimilar to a previously learned input, it will map c to an output similar to the previous one.
Itis able to do this because the mapping reflects constraints it has learned between the featuresof the inputs and outputs of F. If c is sufficiently different from other inputs it has learned inthe domain of F, the result will be uninterpretable.
Somewhere between these two ismetaphor s .ConclusionI have attempted to show in this paper that word meanings are more of a moving targetthan we would like to think, and that they covary depending on constraints between them.The connectionist approach to semantics has a natural way to capture these smoothly varyingconstraints and meanings.
I also have sketched how these meanings can be grounded inperceptual encoding; and how some aspects of metaphor might be captured in this framework.ReferencesChanvin, Yves.
(1986) Flypecmnesia, back-propagation, categorization, and semantics.
Preprintsof the Connectionist Models Summer School, Carnegie-Mellon University, Pittsburgh, Pa.,June 21-29, 1986.CottreIl, G.W.
(1985) A connectionist approach to word sense disambignation.
(PhD thesis)Available as TR 154, University of Rochester Computer Science Department.Cottrell, G.W., Munro, P. & Zipser, D. (to appear) Image compression by back-propagation.Technical Report, Institute for Cognitive Science, UCSD.Elman, J.L.
& Zipser, D. (1986) Discovering the structure of speech.
Paper presented at the112th meeting of the Acoustical Society of America December 1986, Anaheim, Ca.McClelland, J.L.
& Kawamoto, A.H. (1986) Mechanisms of sentence processing: Assigning rolesto constituents.
In J.L.
McCIelland and D.E.
Rumelhart (Eds.)
Parallel Distr\[b~ed Processing:Explorations in the microstructure of cognition.
Cambridge, MA:Bradford.Rumelhart, D.E.
and McCIelland, J.L.
(1986) Parallel Distributed Processing: Explorations in themicrostructvle of cognition.
Cambridge, MA:Bradford.Rumelhart, D.E., Smolensky, P.E., McCIelland, J.L.
& Hinton, G.E.
(1986) Schemata andsequential thought processes in PDP models.
In J.L.
McClelland & D.E.
Rumelhart (Eds.
)Parallel Distributed Processing: Explorations in the microstructure of cognition, Vol.
2.Cambridge, MA:Bradford.Wilks, Y.
(I976) Parsing English lI.
In Charniak and Wilks (Eds.
), Computational Semantics.North-Holland, pp.
155-184.SThis represents a light generalization f an idea of Dave Rumdhart's: his model did not include the functionin the mapping.70
