Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 21?24,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsMorfessor 2.0: Toolkit for statistical morphological segmentationPeter Smit1peter.smit@aalto.fiSami Virpioja2sami.virpioja@aalto.fiStig-Arne Gr?onroos1stig-arne.gronroos@aalto.fiMikko Kurimo1mikko.kurimo@aalto.fi1Department of Signal Processing and Acoustics, Aalto University2Department of Information and Computer Science, Aalto UniversityAbstractMorfessor is a family of probabilistic ma-chine learning methods for finding themorphological segmentation from raw textdata.
Recent developments include the de-velopment of semi-supervised methods forutilizing annotated data.
Morfessor 2.0is a rewrite of the original, widely-usedMorfessor 1.0 software, with well docu-mented command-line tools and library in-terface.
It includes new features such assemi-supervised learning, online training,and integrated evaluation code.1 IntroductionIn the morphological segmentation task, the goalis to segment words into morphemes, the small-est meaning-carrying units.
Morfessor is a familyof methods for unsupervised morphological seg-mentation.
The first version of Morfessor, calledMorfessor Baseline, was developed by Creutz andLagus (2002) its software implementation, Mor-fessor 1.0, released by Creutz and Lagus (2005b).A number of Morfessor variants have been devel-oped later, including Morfessor Categories-MAP(Creutz and Lagus, 2005a) and Allomorfessor(Virpioja et al., 2010).
Even though these algo-rithms improve Morfessor Baseline in some areas,the Baseline version has stayed popular as a gener-ally applicable morphological analyzer (Spiegleret al., 2008; Monson et al., 2010).Over the past years, Morfessor has been usedfor a wide range of languages and applications.The applications include large vocabulary contin-uous speech recognition (e.g.
Hirsim?aki et al.,2006), machine translation (e.g.
Virpioja et al.,2007), and speech retrieval (e.g.
Arisoy et al.,2009).
Morfessor is well-suited for languages withconcatenative morphology, and the tested lan-guages include Finnish and Estonian (Hirsim?akiet al., 2009), German (El-Desoky Mousa et al.,2010), and Turkish (Arisoy et al., 2009).Morfessor 2.0 is a new implementation of theMorfessor Baseline algorithm.1It has been writ-ten in a modular manner and released as an opensource project with a permissive license to encour-age extensions.
This paper includes a summary ofthe Morfessor 2.0 software and a description of thedemonstrations that will be held.
An extensive de-scription of the features in Morfessor 2.0, includ-ing experiments, is available in the report by Vir-pioja et al.
(2013).2 Morfessor model and algorithmsModels of the Morfessor family are generativeprobabilistic models that predict compounds andtheir analyses (segmentations) given the model pa-rameters.
We provide a brief overview of themethodology; Virpioja et al.
(2013) should be re-ferred to for the complete formulas and descriptionof the model and its training algorithms.Unlike older Morfessor implementations, Mor-fessor 2.0 is agnostic in regard to the actual databeing segmented.
In addition to morphologicalsegmentation, it can handle, for example, sentencechunking.
To reflect this we use the followinggeneric terms: The smallest unit that can be splitwill be an atom (letter).
A compound (word) is asequence of atoms.
A construction (morph) is asequence of atoms contained inside a compound.2.1 Model and cost functionThe cost function of Morfessor Baseline is derivedusing maximum a posteriori estimation.
That is,the goal is to find the most likely parameters ?1Morfessor 2.0 can be downloaded from the Mor-pho project website (http://www.cis.hut.fi/projects/morpho/) or GitHub repository (https://github.com/aalto-speech/morfessor).21given the observed training dataDW:?MAP= argmax?p(?)p(DW|?)
(1)Thus we are maximizing the product of the modelprior p(?)
and the data likelihood p(DW|?).
Asusual, the cost function to minimize is set as theminus logarithm of the product:L(?,DW) = ?
log p(?)?
log p(DW|?).
(2)During training, the data likelihood is calcu-lated using a hidden variable that contains the cur-rent chosen analyses.
Secondly, it is assumed thatthe constructions in a compound occur indepen-dently.
This simplifies the data likelihood to theproduct of all construction probabilities in the cho-sen analyses.
Unlike previous versions, Morfes-sor 2.0 includes also the probabilities of the com-pound boundaries in the data likelihood.For prior probability, Morfessor Baseline de-fines a distribution over the lexicon of the model.The prior assigns higher probability to lexiconsthat store fewer and shorter constructions.
Thelexicon prior consists of to parts, a product overthe form probabilities and a product over the usageprobabilities.
The former includes the probabilityof a sequence of atoms and the latter the maxi-mum likelihood estimates of the constructions.
Incontrast to Morfessor 1.0, Morfessor 2.0 currentlysupports only an implicit exponential length priorfor the constructions.2.2 Training and decoding algorithmsA Morfessor model can be trained in multipleways.
The standard batch training uses a localsearch utilizing recursive splitting.
The model isinitialized with the compounds and the full modelcost is calculated.
The data structures are designedin such way that the cost is efficient compute dur-ing the training.In one epoch of the algorithm, all compoundsin the training data are processed.
For each com-pound, all possible two-part segmentations aretested.
If one of the segmentations yields the low-est cost, it is selected and the segmentation is triedrecursively on the resulting segments.
In each stepof the algorithm, the cost can only decrease or staythe same, thus guaranteeing convergence.
The al-gorithm is stopped when the cost decreases lessthan a configurable threshold value in one epoch.An extension of the Viterbi algorithm is usedfor decoding, that is, finding the optimal segmen-tations for new compound forms without changingthe model parameters.3 New features in Morfessor 2.03.1 Semi-supervised extensionsOne important feature that has been implementedin Morfessor 2.0 are the semi-supervised exten-sions as introduced by Kohonen et al.
(2010)Morfessor Baseline tends to undersegmentwhen the model is trained for morphological seg-mentation using a large corpus (Creutz and Lagus,2005b).
Oversegmentation or undersegmentationof the method are easy to control heuristicallyby including a weight parameter ?
for the likeli-hood in the cost function.
A low ?
increases thepriors influence, favoring small construction lexi-cons, while a high value increases the data likeli-hood influence, favoring longer constructions.In semi-supervised Morfessor, the likelihood ofan annotated data set is added to the cost function.As the amount of annotated data is typically muchlower than the amount of unannotated data, its ef-fect on the cost function may be very small com-pared to the likelihood of the unannotated data.To control the effect of the annotations, a sepa-rate weight parameter ?
can be included for theannotated data likelihood.If separate development data set is available forautomatic evaluation of the model, the likelihoodsweights can be optimized to give the best out-put.
This can be done by brute force using a gridsearch.
However, Morfessor 2.0 implementationincludes a simple heuristic for automatically tun-ing the value of ?
during the training, trying tobalance precision and recall.
A simple heuristic,which gives an equivalent contribution to the an-notated data, is used for ?.3.2 On-line trainingIn addition to the batch training mode, Morfes-sor 2.0 supports on-line training mode, in whichunannotated text is processed one compound at atime.
This makes it simple to, for example, adaptpre-trained models for new type of data.
As fre-quent compounds are encountered many times inrunning text, Morfessor 2.0 includes an option forrandomly skipping compounds and constructionsthat have been recently analyzed.
The random22Figure 1: Screenshot from the Morfessor 2.0 demo.skips can also be used to speed up the batch train-ing.3.3 Integrated evaluation codeOne common method for evaluating the perfor-mance of a Morfessor model is to compare itagainst a gold standard segmentation using seg-mentation boundary precision and recall.
To makethe evaluation easy, the necessary tools for calcu-lating the BPR metric by (Virpioja et al., 2011)are included in Morfessor 2.0.
For significancetesting when comparing multiple models, we haveincluded the Wilcoxon signed-rank test.
Both theevaluation code and statistical testing code are ac-cessible from both the command line and the li-brary interface.3.4 N-best segmentationIn order to generate multiple segmentations for asingle compound, Morfessor 2.0 includes a n-bestViterbi algorithm.
It allows extraction of all possi-ble segmentations for a compound and the proba-bilities of the segmentations.4 DemonstrationIn the demonstration session, multiple featuresand usages of Morfessor will be shown.4.1 Web-based demonstrationA live demonstration will be given of segmentingtext with Morfessor 2.0 for different language andtraining data options.
In a web interface, the usercan choose a language, select the size of the train-ing corpus and other options.
After that a wordcan be given which will be segmented using n-bestViterbi, showing the 5 best results.A list of planned languages can be found in Ta-ble 1.
A screen shot of the demo interface is shownin Figure 1.Languages # Words # Word formsEnglish 62M 384.903Estonian 212M 3.908.820Finnish 36M 2.206.719German 46M 1.266.159Swedish 1M 92237Turkish 12M 617.298Table 1: List of available languages for Morfessor2.0 demonstration.4.2 Command line interfaceThe new command line interface will be demon-strated to train and evaluate Morfessor modelsfrom texts in different languages.
A diagram ofthe tools is shown in Figure 24.3 Library interfaceInterfacing with the Morfessor 2.0 Python librarywill be demonstrated for building own scientificexperiments, as well as integrating Morfessor in23Training dataAnnotation datamorfessor-trainMorfessormodelCorpusGold standardmorfessor-segmentmorfessor-evaluateSegmented corpusBPR-scoresFigure 2: The standard workflow for Morfessorcommand line toolsbigger project.
Also the code of the Web baseddemonstration will be shown as an example.AcknowledgementsThe authors have received funding from the EC?s7th Framework Programme (FP7/2007?2013) un-der grant agreement n?287678 and the Academyof Finland under the Finnish Centre of Excel-lence Program 2012?2017 (grant n?251170) andthe LASTU Programme (grants n?256887 and259934).
The experiments were performed us-ing computer resources within the Aalto Univer-sity School of Science ?Science-IT?
project.ReferencesE.
Arisoy, D. Can, S. Parlak, H. Sak, and M. Saraclar.2009.
Turkish broadcast news transcription and re-trieval.
Audio, Speech, and Language Processing,IEEE Transactions on, 17(5):874?883.M.
Creutz and K. Lagus.
2002.
Unsupervised discov-ery of morphemes.
In Mike Maxwell, editor, Pro-ceedings of the ACL-02 Workshop on Morphologicaland Phonological Learning, pages 21?30.
Associa-tion for Computational Linguistics, July.M.
Creutz and K. Lagus.
2005a.
Inducing the mor-phological lexicon of a natural language from unan-notated text.
In Proceedings of AKRR?05, Interna-tional and Interdisciplinary Conference on AdaptiveKnowledge Representation and Reasoning, pages106?113, Espoo, Finland, June.
Helsinki Universityof Technology.M.
Creutz and K. Lagus.
2005b.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using Morfessor 1.0.
TechnicalReport A81, Publications in Computer and Informa-tion Science, Helsinki University of Technology.A.
El-Desoky Mousa, M. Ali Basha Shaik, R. Schluter,and H. Ney.
2010.
Sub-lexical language models forGerman LVCSR.
In Spoken Language TechnologyWorkshop (SLT), 2010 IEEE, pages 171?176.
IEEE.T.
Hirsim?aki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-pioja, and J. Pylkk?onen.
2006.
Unlimited vocabu-lary speech recognition with morph language mod-els applied to Finnish.
Computer Speech & Lan-guage, 20(4):515?541.T.
Hirsim?aki, J. Pylkk?onen, and M. Kurimo.
2009.Importance of high-order n-gram models in morph-based speech recognition.
Audio, Speech, andLanguage Processing, IEEE Transactions on,17(4):724?732.O.
Kohonen, S. Virpioja, and K. Lagus.
2010.
Semi-supervised learning of concatenative morphology.In Proceedings of the 11th Meeting of the ACL Spe-cial Interest Group on Computational Morphologyand Phonology, pages 78?86, Uppsala, Sweden,July.
Association for Computational Linguistics.C.
Monson, K. Hollingshead, and B. Roark.
2010.Simulating morphological analyzers with stochastictaggers for confidence estimation.
In MultilingualInformation Access Evaluation I.
Text Retrieval Ex-periments, pages 649?657.
Springer.S.
Spiegler, B. Gol?enia, K. Shalonova, P. Flach, andR.
Tucker.
2008.
Learning the morphology of zuluwith different degrees of supervision.
In SpokenLanguage Technology Workshop, 2008.
SLT 2008.IEEE, pages 9?12.
IEEE.S.
Virpioja, J. V?ayrynen, M. Creutz, and M. Sadeniemi.2007.
Morphology-aware statistical machine trans-lation based on morphs induced in an unsupervisedmanner.
In Proceedings of the Machine TranslationSummit XI, pages 491?498, Copenhagen, Denmark,September.S.
Virpioja, O. Kohonen, and K. Lagus.
2010.
Unsu-pervised morpheme analysis with Allomorfessor.
InMultilingual Information Access Evaluation I. TextRetrieval Experiments, volume 6241 of LNCS, pages609?616.
Springer Berlin / Heidelberg.S.
Virpioja, V. Turunen, S. Spiegler, O. Kohonen, andM.
Kurimo.
2011.
Empirical comparison of evalua-tion methods for unsupervised learning of morphol-ogy.
TAL, 52(2):45?90.S.
Virpioja, P. Smit, S. Gr?onroos, and M. Kurimo.2013.
Morfessor 2.0: Python implementation andextensions for Morfessor Baseline.
Report 25/2013in Aalto University publication series SCIENCE +TECHNOLOGY, Aalto University, Finland.24
