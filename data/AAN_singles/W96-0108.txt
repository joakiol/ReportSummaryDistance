A Statistical Approach to Automatic OCR ErrorCorrection in ContextXiang Tong and  Dav id  A. EvansLaboratory  for Computat iona l  L inguist icsCarnegie  Mel lon Univers i tyP i t tsburgh,  PA 15213U.S.A.
{tong, dae } @lcl.cmu.eduAbstractThis paper describes an automatic, ontext-sensitive, word-error correction system basedon statistical language modeling (SLM) as applied to optical character recognition (OCR) post-processing.
The system exploits information from multiple sources, including letter n-grams,character confusion probabilities, and word-bigram probabilities.
Letter n-grams are used toindex the words in the lexicon.
Given a sentence to be corrected, the system decomposes achstring in the sentence into letter n-grams and retrieves word candidates from the lexicon bycomparing string n-grams with lexicon-entry n-grams.
The retrieved candidates are ranked bythe conditional probability of matches with the string, given character confusion probabilities.Finally, the wordobigram odel and Viterbi algorithm are used to determine the best scoringword sequence for the sentence.
The system can correct non-word errors as well as real-worderrors and achieves a 60.2% error reduction rate for real OCR text.
In addition, the system canlearn the character confusion probabilities for a specific OCR environment and use them inself-calibration toachieve better performance.1 IntroductionWord errors present problems for various text- or speech-based applications such as optical char-acter recognition (OCR) and voice-input computer interfaces.
In particular, though current OCRtechnology is quite refined and robust, sources uch as old books, poor-quality (nth-generation)photocopies, and faxes can still be difficult o process and may cause many OCR errors.
For OCR tobe truly useful in a wide range of applications, uch as office automation and information retrievalsystems, OCR reliability must be improved.
A method for the automatic correction of OCR errorswould be clearly beneficial.Essentially, there are two types of word errors: non-word errors and real-word errors.
A non-word error occurs when a word in a source text is interpreted (under OCR) as a string that doesnot correspond to any valid word in a given word list or dictionary.
A real-word error occurswhen a source-text word is interpreted as a string that actually does occur in the dictionary, but isnot identical with the source-text word.
For example, if the source text "John found the man" isrendered as "John fornd he man" by an OCR device, then "fornd" is a non-word error and "he" isa real-word error.
In general, non-word errors will never correspond to any dictionary entries and88will include wildly incorrect strings (such as "#--&&') as well as misrecognized alpha-numericsequences (such as "BN234" for "8N234").
However, some non-word errors might become real-word errors if the size of the word list or dictionary increases.
(For example, the word "ruel "~might count as a non-word error for the source-text word "rut" if a small dictionary is used forreference, but count as a real-word error if an unabridged ictionary is used.)
While non-worderrors might be corrected without considering the context in which the error occurs, a real-worderror can only be corrected by taking context into account.The problems of word-error detection and correction have been studied for several decades.A good survey in this area can be found in \[Kukich 1992\].
Most traditional word-correctiontechniques concentrate on non-word error correction and do not consider the context in which theerror appears.Recently, statistical language models (SLMs) and feature-based methods have been used forcontext-sensitive spelling-error correction.
For example, Atwell and Elliittm \[1987\] have used apart-of-speech (POS) tagging method to detect he real-word errors in text.
Mays and colleagues\[1991\] have exploited word trigrams to detect and correct both the non-word and real-word errorsthat were artificially generated from 100 sentences.
Church and Gale \[1991\] have used a Bayesianclassifier method to improve the performance for non-word error correction.
Golding \[1995\] hasapplied a hybrid Bayesian method for real-word error correction and Golding and Schabes \[1996\]have combined a POS trigram and Bayesian methods for the same purpose.The goal of the work described here is to investigate the effectiveness and efficiency of SLM-based methods applied to the problem of OCR error correction.
Since POS-based methods are noteffective in distinguishing among candidates with the same POS tags and since methods based onword-trigram odels involve extensive training data and require that huge word-trigram tablesbe available at run time, we used a word-bigram SLM as the first step in our investigation.In this paper, we describe a system that uses a word-bigram SLM technique to correct OCRerrors.
The system takes advantage of information from multiple sources, including letter n-grams, character confusion probabilities, and word bigram probabilities, to effect context-basedword error correction.
It can correct non-word as well as real-word errors.
In addition, the systemcan learn the character confusion probability table for a specific OCR environment and use it toachieve better performance.2 The Approach2.1 Problem StatementThe problem of context-based OCR word-error correction can be stated as follows:Let L = {wl, w~, ..., win} be the set of all the words in a given lexicon.
For an input sentence,S = sl, ..., sn, produced as the output of an OCR device, where sl, ...,s,~ are character stringsseparated by spaces, find the best word sequence, ~?g = wl, w2, ..., w,,  for wi E L, that maximizesthe probability pr (W\[ S):I?V = argmaxw (pr( WI S ) ) (1)"Ruel" is an obscure French-derivative word meaning the space between a bed and the wall.89Using Bayes' formula, we can rewrite 1 as:argmaxw (Pr( W I S ) )= argmaxw( pr(W)* pr(SlW))S )= argmaxw(pr(W ) , pr(S\[W)) (2)The probability pr(W) is given by the language model and can be decomposed as:npr(W) = I I  pr(wilwl.i_l) (3)i=1where pr(wilw~ .i- ~) is the probability that the word wi appears given that wl, w2, ?
?., wi_ ~ appearedpreviously.In a word-bigram language model, we assume that the probability that a word w~ will appearis affected only by the immediately preceding word.
Thus,andpr(w, = p (w, lw,_l) (4)pr(W) = I~ Pr(w*lw'-~) (5)i=1The conditional probability, pr(SIW ), reflects the channel (processing) characteristics of theOCR environment.
If we assume that strings produced under OCR are independent of oneanother, we have the following formula:Pr(SIW) = r I  Pr(S~lw~) (6)i=1So,= argmaxw(Pr(W ) , pr(SlW))n= argmaxw(l~ Ipr(wilwi_l) ?
pr(silwi)) (7)i=1Thus, the problem of calculating W is reduced to estimating the word-bigram probability, pr (wil w~_ 1),and the word confusion probability, pr(silw~).
The word-bigram probability, pr(wi\[wi_ ~), can beestimated by a maximum likelihood estimator (MLE):prML(WiiW,_i ) = C(Wi-1, Wi)where c(wi_x) is the number of times that wi-1 occurs in the text and c(wi_~, w~) is the number oftimes that the word bigram (Wi_l, wi) occurs in the text.However, the estimatation of unseen bigrams is a problem.
We use a back-off model similar tothat described in \[Dagan & Pereira 1994\] to estimate the word-bigram probabilities in our system.If we already have estimates of the probabilities pr(wilwi_l) and pr(si\[wi), the Viterbi algo-rithm \[Charniak 1993\] could be used to determine the best word sequence for the given sentence.Details of the back-off model and Viterbi algorithm can be found in \[Dagan & Pereira 1994\] and\[Charniak 1993\].902.2 Estimate of Channel Probabilit ies and Learning of Character Confusion TableThe probability pr(slw)--the conditional probability that, given a word w, it is recognized by theOCR software as the string s---can be estimated by the confusion probabilities of the characters ins if we assume that character recognition in OCR is an independent process.We assume that an OCR string is generated from the original word by one or more of thefollowing operations: (a) delete a character; (b) insert a character; or (c) substitute one characterfor another.
Under such circumstances, a dynamic programming method can be used to determinethe operations that maximize the conditional probability when transforming the original word tothe OCR string, given a character confusion probability table.Let tl, t~ ...  ti be the first i characters of the string that is produced by the OCR process fora source word s and let s l , s2 .
.
.
s j  be the first j actual characters of ~.
Define pr(il j ) to be theconditional probability that the substring sl,j is recognized as substring tl.i by the OCR process,i.e., pr(tl,ilSl.j).
The dynamic programming recurrence is given as follows:pr(i - l l j )  * pr(ins(ti))pr(i l j  ) = max pr(il j  - 1) ?
pr(del(sj)lsj) (8)pr ( i -  l \ [ j -  1)*pr(t, lsj)where pr(ins(y)) is the probability that letter y is inserted.pr(del(y)ly ) is the probability that letter y is deleted.pr(xly) is the probability that letter y is replaced by letter x.For example, suppose that source word "flag" is recognized as "flo" by an OCR device.
For-mula 8 may determine that a sequence of four operations--(1) substitute "f" for "f';  (2) substitute"T' for "l'; (3) substitute "a" for "o", and (4) delete "g"--maximizes the conditional probabilitypr("flo"l"flag").
Then the probability of "flag" being rendered as "flo" can be estimated as:pr("flo"l"flag') = pr("f"l"f" ) * pr("l"l"l" ) * pr("o'l"a") ?
pr(del("g")l"g")This method is similar to what was described in \[Wagner 1974\] where the minimum edit distancebetween two strings was computed.
The minimum edit distance is the minimum number of oper-ations that transform the source string to the target string.
Note that to effect spelling correction,we could include character transposition probabilities.If we have no information about the character confusion probabilities, we can estimate themas:pr(ylx) = { ~7 i fy=x- -  otherwise (9)1- -~pr(dd(x)lx) = pr(ins(x))= i (10)where N is the total number of printable characters.The estimator a can be regarded as the probability that a given character iscorrectly recognized.Our experiments show that system performance is very sensitive to the value of a, especially forreal-word error correction.
For example, if a is very high, then the probability pr(sls ) will be toohigh to be affected by subsequent processing and will not be changed.
On the other hand if a isvery low, some correct words may be detected as real-word errors and will be changed.If we have both the original text and the corresponding OCR output and if we assume that theerrors made by a particular OCR system are not random (but semi-deterministic), we can count the91cases of substitution, deletion, and insertion using a method similar to computing the minimumedit distance between strings \[Wagner 1974\] and we can estimate the probabilities using formulassimilar to those in \[Church & Gale 1991\]:pr(ylx ) = num(sub(x,y))/num(x) (11)pr(del(x)) = num(del(x))/num(x) (12)pr(ins(y)) = num(ins(y))/num( Kall letters> ) (13)Obviously, in practice, we typically do not have the original text to compare to the OCR text orto use for correction.
Moreover, as noted in \[Liu et al 1991\], the character confusion characteristicsare heavily dependent on the OCR environment, encompassing everything from the performancebiases of the specific OCR software to the size of characters inthe source text, fonts used, individualcharacter types, and print quality of the text being processed.
It is not feasible to train on texts toacquire character confusion probabilities for each OCR environment.The current system employs an iterative learning-from-correcting technique that treats the cor-rected OCR text as an approximation of the original text.
The system starts by assuming allcharacters are equally likely to be misrecognized (with some uniform, small probability) andlearns the character confusion probabilities by comparing the OCR text to the corrected OCR textafter each pass.
Then the learned character confusion probabilities are used for the next passprocessing (feedback processing).
This method proves to be quite effective in improving systemperformance.2.3 Generation of Word Candidates for a Given StringIdeally, each word, w, in the lexicon should be compared to a given OCR string, s, to compute theconditional probability, pr(wls ).
However, this approach would be computationally too expensive.Instead, the system operates in two steps, first to generate the candidates and then to specify themaximal number of candidates, N, to be considered for the correction of an OCR string.In step 1, the system retrieves a large list of word candidates for a given string.
To nominatecandidates, we use a vector space information retrieval technique \[Salton 1989\]: all the words inthe lexicon are indexed by letter n-grams and the (OCR) string, also parsed into letter n-grams, istreated as a query over the database of lexicon entries.
In particular, all words (or OCR strings)are indexed by their letter trigrams, including the 'beginning' and 'end' spaces urrounding thestring.
Words of four or fewer characters are also indexed by their letter bigrams.
For example:"the" ~ {#th, the, he#, #t, th, he, e#}"example" ~ {#ex, exa, xam, mpl, ple, le#}A given OCR string to be corrected is represented by a vector containing its letter n-grams.Using the vector as the query, the lexicon words that are similar to the word error are retrieved,giving a large list of candidate correct forms.
Candidates must share at least some features withthe input string (query).
A ranked list can be generated by scoring matches using a simple termfrequency (TF) count--the number of matches between the query vector and the n-gram vector ofa candidate word.
For example, given the string:"exanple" ~ {#exanple#}{#ex, exa, xan, anp, npl, ple, le#}the word "example" is a candidate:92"example" ~ {#example#}{#ex, exa, xam, amp, mpl, pie, le#}Since the two items share four letter n-grams--"#ex', "exa', "ple", and "le#'--the TF score of thecandidate word "example" for the input string "exanple" is four.
Note also that the TF score canbe used to establish a threshold or cutoff score to limit the number of candidates to consider.In step 2, the system re-ranks the words in the candidate list using channel probabilities asdescribed above.On average, the system generates several hundred candidates for a given string.
Only the firstN candidates are retained for context-based word-error correction.2.4 The Word Correction System for OCR Post-ProcessingThe architecture of the word correction system for OCR post-processing is given in Figure 1.I OCR Text lCandidate Generation='l Candidate Retrieval ....Candidate RankingI Lexicon 1Character Confusion 1TableI Word BigramTable:IraMaximum LikelihoodWord Sequence FindingFeedbackI Corrected OCR TextFigure 1: System ArchitectureThe lexicon is generated from the training text; it includes all the words in the training setwith frequency greater than the preset hreshold.
The words in the lexicon are indexed by lettern-grams as described in the previous ection.93The overall process for correcting a sentence is as follows:1.
Read a sentence from the input OCR text.2.
Retrieve up to M candidates from the lexicon for each possible errorJ Rerank the Mcandidates by their conditional probabilities to the error.
Keep only the top N candidates forthe next processing step.
(In the current system, M is 10,000 and N is 10.)3.
Use the Viterbi algorithm to get the best word sequence for the strings in the sentence.Figure 2 illustrates the alternative choices and the optimal path found during the processing(correcting) of the sentence "john fornd he man".Original Sentence: John found the man.Input Sentence: john fornd he man.Corrected Sentence: John found the man.john forned he manfound'~.._._._.._ .
r lae.~,~__._ .~.- - .
- '~ Joh~___.~___~ ~ ~- .
-~manjoin ~.
.
.
.~~-~ fond ~ ~  the~, ~- - -~-~ anCohn ~,~, .
.~N ford ~~- ' "~ be ~- .~~-" ' - - - .
.
.~  mayK ohn ~,~~-~a~ for ~ ~  ~ He ~ .
.
.
~  can~ " ~  The ~ x ~ ~  Jan Sohn ~ ~ a ,  form ,~forms ~\  ~ ~ ,  mane \\\.q ~X,~ ~,N\~ menjoint ~NN ~ food ~ her "~ San-~job ~'N~ force sheformed De ManJohns "X~ sound Le vanKahnBest Word Sequence: John found the man.Figure 2: Process of Correcting a SentenceThe system requires everal passes to correct an OCR text.
In the first pass, the system hasno information on the character confusion probabilities, so it will assume a prior belief o~ as theprobability that a character iscorrectly recognized.
The system distributes the rest of the probabilityuniformly among other events.
(Cf.
Formula 9.)
In each feedback step, the system first generatesa character confusion probability table by comparing the OCR text to the corrected OCR text fromthe last pass.
It uses the new confusion table for the next-pass correction of the OCR text.Sin its non-word error mode of operation, the system treats every word that does not match a lexicon entry as apossible rror.
In its non-word and real-word error mode, the system treats every word as though it were a possibleerror.943 Experiments and ResultsTo test our OCR-error-correction process, we used a set of electronic documents from the Ziff-Davis(ZIFF) news wire?
The documents in the corpus are business articles in the domain of computerscience and computer engineering.
We used 90% of the collection for training and the remaining10% for testing.The system created a lexicon and collected word-bigram sequences and statistics from thetraining data.
Words or word-bigrams with frequency less than three were discarded.
Theresulting lexicon contained about 100,000 words; these were indexed using 34,847 letter n-grams.The resulting word-bigram table had about 1,000,000 entries.Seventy pages of ZIFF data in the test set were printed in 7-point Times font.
We degradedthe print quality of the documents by photocopying them on a "light' setting.
The photocopieswere then scanned by a Fujitsu 3097E scanner and the resulting images were processed by XeroxTextbridge OCR software.The set of documents contained 55,699 strings and the overall word error rate after OCRprocessing was 22.9% (12,760).
For literal words in the source (only letter sequences, not alpha-numeric ones), the error rate was lower, 14.7% (8,198).
Table I gives the number of real-word andnon-word errors for literal words in the OCR data.Non-Word ErrorsNumber 6,506% 79.4Real-Word Errors Total Errorsi,692 8,19820.6 100Table 1: OCR Errors Originating from Literal WordsWe conducted three experiments:1.
Isolated-Word Error Correction: The system used only channel probabilities without consid-ering context information, i.e., it always selected the candidate with the highest rank in thecandidates list to correct a given OCR string.2.
Context-Dependent Non-Word Error Correction: The system used context o correct strings thatdid match valid lexicon words.3.
Context-Dependent Non- and Real-Word Error Correction: The system treated all input stringsas possible errors and tried to correct hem by taking into account he contexts in which thestrings appeared.In each experiment, he system conducted four correction passes: one initial pass with priorprobability c~ = 0.99 and three feedback passes.Results are given in Tables 2, 3, and 4.
In all cases, we considered only those strings whosecorrect forms are literal words (not alpha-numerics).
Note that errors can be introduced by thesystem when it incorrectly changes a correct word in the OCR text into another word.
In fact,we distinguish two types of errors introduced by the system: errors caused by changing correct3The ZIFF collection is distributed as part of the data used in the Text Retrieval Conference (TREC) evaluations.
Thecorpus contains about 33 million words.95unknown words and errors caused by changing correct lexicon words.
The error reduction ratewas calculated by subtracting total errors from 8,198 and dividing by 8,198.The system, running unoptimized code on a 128MHz DECalpha processor, processed the testcorpus at a rate of about 200 words (strings) per second for experiments 1 and 2; and 30 words(strings) per second for experiment 3.PassF~stFeedback-1Feedback-2Feedback-3Non-Word ErrorsRemain Corrected3,049 3,4572,816 3,6902,791 3,7152,784 3,722Real-Word ErrorsRemain Corrected1,692 01,692 01,692 : 01,692 !
0Introduced ErrorsUnknown Wds Lex Wds182 0182 0182 0182 0Table 2: Results from Isolated-Word Error CorrectionTotal ErrorErrors Reduction (%)4923 39.94,690 42.84,665 43.14,658 43.2Non-Word Errors !
Real-Word Errors Introduced ErrorsPass Unknown Wds Lex WdsFirst 182 0Feedback-1 182 0Feedback-2Feedback-3Remain Corrected2,684 3,8221,972 4,3541943 4,5631948 4,558Remain Corrected1,692 01,692 01,692 01,692 0182 0182 0Total ErrorErrors Reduction (%)4,558 44.43,846 53.13,817 53.43,822 53.4Table 3: Results from Context-Dependent Non-Word Error CorrectionPassFirstFeedback-1Feedback-2Feedback-3Non-Word ErrorsRemain Corrected2,529 3,9771,978 4,5281,935 4,5711,926 4,580Real-Word ErrorsRemain Corrected1,225 4671,031 6611,008 6841,015 677Introduced Errors Total ErrorUnknown Wds Lex Wds Errors Reduction (%)182 54 3,990 51.3182 119 3,310 59.6182 141 3,266 60.2182 , 147 3,270 60.1Table 4: Results from Context-Dependent Real- and Non-Word Error Correction4 AnalysisBased on the results, we can see that the predominant, positive ffect in correction occurs in the firstpass.
Performance also improves ignificantly in the first feedback process, as the system learnsthe character confusion probabilities by correcting the OCR text.
The second and third feedbacksteps have only slight effect on the error reduction rates.
Indeed, in experiment 3, the resultfrom the third feedback pass is actually worse than that from the second feedback pass.
Theseresults indicate that an initial pass followed by two feedback passes may optimize the method.
Inthe following discussion, we compare the three experiments using the results obtained from thesecond feedback step (Feedback-2).As we might expect, the results from the context-based experiments are much better thanthose from the isolated-word experiment.
The error reduction rates in experiments 2 and 3 are,08respectively, 10.3% and 17.1% higher than the rate in experiment 1.
This indicates that even amodest (e.g., bigram-based) representation f context is useful in selecting the best candidates forword-error correction.In all three experiments, the system introduced 182 new errors due to false corrections of wordsthat were not in the lexicon.
(Recall that the system lexicon is based on the words derived from thetraining corpus; some words may be present in the test corpus that are not in the training corpus.
)Whenever the system encounters an unknown word, it treats it as a non-word error and attemptsto correct it.
In such cases, the system replaces the presumed non-word error with a word from itslexicon.
Thus, for example, if the system encounters the word "MobileData" (a correct name) inthe OCR output, but does not have "MobileData" in its lexicon, it might change "MobileData" to"MobileComm" (a word that does exist in the training corpus lexicon).
Of course, such problemsin processing unknown words are not unique to OCR error correction; they represent a generalproblem for all natural-language processing tasks.As shown by experiment 3, when the system uses context-based non- and real-word errorcorrection, it achieves a total error reduction rate of 60.2%.
This is 6.8% higher than the rateachieved in the context-based non-word experiment.
The improvement in performance is gainedprincipally from the reduction of the real-word errors.
Although the system introduces additionalerrors--since all the strings in the OCR text are treated as possible rrors and subject o change--thenumber of corrected real-word errors far exceeds the number of real-word errors introduced.
Inthe second feedback pass, for example, the system introduced 141 new errors by changing correctlexicon words into other lexicon words.
On the other hand, the system properly corrected 684 realerrors--32.1% of all the real errors.
The corrected OCR text, therefore, has 543 fewer real-worderrors than the original OCR text.Certain types of errors in the source or OCR-output ext present systematic problems for ourapproach, highlighting the limitations of the system.
In particular, because the process is basedon the structural definition of a word (viz., a character sequence 'between white space')--not amorphological one--any errors that obscure word boundaries will defy correction.
For example,run-on errors (e.g., "of the"/"ofthe") and split-word errors ("training" /"train ng')  cannot becorrected.
In addition, the use of a vector-space querying to find candidate lexical entries--including our special approach to word decomposition and scoring--can present problems whenprocessing some OCR errors, especially short strings.
For example, if "both" (in the source) isrendered as "hotn" (in the OCR text), it is not possible for the system to generate "both" as one ofthe high-ranked candidates--they share only one feature, the bigram "ot"- -  despite the fact thatthe conditional probability pr("hotn"l"both" ) might be high.
Finally, the system suffers from thecommon limitation of word bigram or trigram models in that it cannot capture discourse propertiesof context, such as topic and tense, which are sometimes required to select he correct word.5 ConclusionThe system we have created uses information from a variety of sources--qetter n-grams, charac-ter confusion probabilities, and word-bigram probabilities---to realize context-based, automatic,word-error correction.
It can correct non-word errors as well as real-word errors.
The system canalso learn character confusion probability tables by correcting OCR text and use such informationto achieve better performance.
Overall, for complete (real- and non-word) error correction, itachieved a 60.2% rate of error reduction.07The techniques we have used are subject to certain systematic problems.
However, we believethey will prove to be useful not only in improving the quality of OCR processing, but also inenhancing a variety of information retrieval applications.In future work, we plan to explore different heuristics to deal with word boundary problemsand to incorporate other models of context representation, i cluding both SLM approaches, suchas word trigram models, and simple discourse structures.AcknowledgementsWe thank Nata~a Mili4-Frayling and an anonymous reviewer for their excellent comments onan earlier version of this paper.
Naturally, the authors alone are responsible for any errors oromissions in the current version.References\[Atwell & Elliittm 1987\] Atwell, E., and Elliittm S. 1987.
Dealing with ill-formed English text(Chapter 10).
In Garaside, R., Leach, G., and Sampson, G. (eds), The Computational Analysis ofEnglish: A Corpus-Based Approach.
New York: Longman, Inc.\[Charniak 1993\] Charniak, E. 1993.
Statistical Language Learning.
MIT Press.\[Church & Gale 1991\] Church, K.W., and Gale, W.A.
1991.
Probability scoring for spelling correc-tion.
Stat.
Comput., 1, 93-103.\[Dagan & Pereira 1994\] Dagan, I., and Pereira, F. 1994.
Similarity-based stimation of word co-occurrence probabilities.
Proceedings of the 32nd Annual Meeting of the ACL, New Mexico StateUniversity.\[Golding 1995\] Golding, R.A. 1995.
A Bayesian hybrid method for context-sensitive spelling cor-rection.
Proceedings ofthe Third Workshop on Very Large Corpora, Cambridge, MA.
39-53.\[Golding & Schabes 1996\] Golding, R.A., and Schabes, Y.
1996.
Combining trigram-based andfeature-based methods for context-sensitive spelling correction.
Proceedings off the 34th AnnualMeeting of the ACL, Santa Cruz, CA.
(To appear)\[Jelinek 1988\] Jelinek, F. 1988.
Self-organized language modeling for speech recognition.
InWaibel,A., and Lee, K.-F. (eds), Readings in Speech Recognition.
Morgan Kaufmann Publishers.
450-506.\[Kukich 1992\] Kukich, K. 1992.
Techniques for automatically correcting words in text.
Comput.Surv., 24, 4, 377-439.\[Liu et al 1991\] Liu, L.M., Babad, Y.M., Sun, W., Chan, K.K.
1991.
Adaptive post-processing ofOCR text via knowledge acquisition.
1991 ACM Computer Science Conference.
Preparing for the21st Century.
558-569.\[Maysetal.
1991\] Mays, E.,Damerau, F.J., and Mercer, R.L.1991.Contextbasedspellingcorrection.Information Processing and Management, 27, 5, 517-522.\[Salton 1989\] Salton, G. 1989.
Automatic Text Processing.
Addison-Wesley Publishing Company.\[Wagner 1974\] Wagner, R.A. 1974.
The string-to-string correction problem.
J  ACM, 21,1, Jan. 1974,168-173.08A Example of OCR CorrectionOriginal TextPower-supply IC controls both PWM and power-factor correction.Designers are focusing more on power-factor correction when creating integrated circuits, due tolimited energy supplies, new standards and the type of office electrical loads found in offices.Micro Linear Corp's ML4819 makes the designer's job easier by including bothpower-faction-correction and PWM control on one chip.
This integrated circuit aids in increasinga supply's power factor with fewer components than other implementations.
The ML4819 isavailable in 20-pin DIPs for $3.95 for 100 units.
Applications for the product include powersupplies for microcomputers in the 150 to 400W range, computer peripherals, instruments,plotters, printers and other off-line power supplies.OCR Text:tN-wer-supp(y IC conimls both PWM and power-factor correciiiifl.t)esigners are focusing more on power-factor correction when creating integrated circuits, due tolimited energy supplies, new -andards and the type of office electrical loacs found in iffices.Micro Linear Corp's ML4819 makes the designer's job easier by including bothpower-faction.correction and PWM control in one chip.
This integrated circuit aids in increasinga supply's power factor with Thwer components than other implementations.
The ML4819 isavailable in 20-pm DIPs for 53.95 for ((1) units.
Applications for the product include powersupplies for microcomputers in the 150 to 41)0W range, computer penpoerals, instirirnents.plotters, pnnters and other off-line power supplies.Corrected OCR Text from First-Pass Correction:Note: the correction for a given string is in brackets.tN-wer\[Newer\] - supp(y\[supply\] IC conimls\[coils\] both PWM and power - factorcorreciiiifl\[correction\].t)esigners\[Designers\] are focusing more on power - factor correction when creating integratedcircuits, due to limited energy supplies, new -andards\[standards\] and the type of officeelectrical\[electrical\] lo cs\[loads\] found in iffices\[offices\].
Micro Linear Corp' s ML4819\[XL19\]makes the designer' s job easier by including both power - faction\[action\], correction and PWMcontrol in one chip.
This integrated circuit aids in increasing a supply' spower factor withThwer\[fewer\] components han other implementations.
The ML4819\[XL19\] is available in 20 -pm\[ppm\] DIPs for 53.95 for ( ( 1 ) units.
Applications for the product include power supplies formicrocomputers in the 150 to 41 )0W\[NEW\] range, computer penpoerals\[peripherals\].instirirnents\[instruments\] .
plotters, pnnters\[punters\] and other off - line power supplies.00Corrected OCR Text from Feedback Correction:Note: the correction for a given string is in brackets.tN-wer\[Power\] - supp(y\[supply\] IC conimls\[controls\] both PWM and power - factorcorreciiiifl\[correction\].t)esigners\[Designers\] a e focusing more on power - factor correction when creating integratedcircuits, due to limited energy supplies, new "andards\[Standards\] and the type of officeelectrical\[electrical\] loacs\[loads\] found in iffices\[offices\].
Micro Linear Corp'  s ML4819 makes thedesigner'  s job easier by including both power - faction\[factor\], correction and PWM control inone chip.
This integrated circuit aids in increasing a supply'  s power factor with Thwer\[Tower\]components han other implementations .The ML4819 is available in 20 - pm\[pin\] DIPs for 53.95for ( ( 1 ) units.
Applications for the product include power supplies for microcomputers in the150 to 41 )0W\[ROW\] range, computer penpoerals\[peripherals\], in tirirnents\[instruments\] .plotters, pnnters\[printers\] andother off - line power supplies.100
