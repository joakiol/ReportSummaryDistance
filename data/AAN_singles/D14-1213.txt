Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1986?1996,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSelf-disclosure topic model for classifying and analyzing TwitterconversationsJinYeong Bak?Department of Computer ScienceKAISTDaejeon, South Koreajy.bak@kaist.ac.krChin-Yew LinMicrosoft ResearchBeijing 100080, P.R.
Chinacyl@microsoft.comAlice OhDepartment of Computer ScienceKAISTDaejeon, South Koreaalice.oh@kaist.eduAbstractSelf-disclosure, the act of revealing one-self to others, is an important social be-havior that strengthens interpersonal rela-tionships and increases social support.
Al-though there are many social science stud-ies of self-disclosure, they are based onmanual coding of small datasets and ques-tionnaires.
We conduct a computationalanalysis of self-disclosure with a largedataset of naturally-occurring conversa-tions, a semi-supervised machine learningalgorithm, and a computational analysisof the effects of self-disclosure on subse-quent conversations.
We use a longitu-dinal dataset of 17 million tweets, all ofwhich occurred in conversations that con-sist of five or more tweets directly reply-ing to the previous tweet, and from dyadswith twenty of more conversations each.We develop self-disclosure topic model(SDTM), a variant of latent Dirichlet al-location (LDA) for automatically classi-fying the level of self-disclosure for eachtweet.
We take the results of SDTM andanalyze the effects of self-disclosure onsubsequent conversations.
Our model sig-nificantly outperforms several comparablemethods on classifying the level of self-disclosure, and the analysis of the longitu-dinal data using SDTM uncovers signifi-cant and positive correlation between self-disclosure and conversation frequency andlength.1 IntroductionSelf-disclosure is an important and pervasive so-cial behavior.
People disclose personal informa-tion about themselves to improve and maintain?This work was done when JinYeong Bak was a visitingstudent at Microsoft Research, Beijing, China.relationships (Jourard, 1971; Joinson and Paine,2007).
A common instance of self-disclosure isthe start of a conversation with an exchange ofnames and additional self-introductions.
Anotherexample of self-disclosure, shown in Figure 1c,where the information disclosed about a familymember?s serious illness, is much more personalthan the exchange of names.
In this paper, we seekto understand this important social behavior usinga large-scale Twitter conversation data, automati-cally classifying the level of self-disclosure usingmachine learning and correlating the patterns withconversational behaviors which can serve as prox-ies for measuring intimacy between two conversa-tional partners.Twitter conversation data, explained in moredetail in section 4.1, enable an extremely largescale study of naturally-occurring self-disclosurebehavior, compared to traditional social sciencestudies.
One challenge of such large scale study,though, remains in the lack of labeled ground-truth data of self-disclosure level.
That is,naturally-occurring Twitter conversations do notcome tagged with the level of self-disclosure ineach conversation.
To overcome that challenge,we propose a semi-supervised machine learningapproach using probabilistic topic modeling.
Ourself-disclosure topic model (SDTM) assumes thatself-disclosure behavior can be modeled using acombination of simple linguistic features (e.g.,pronouns) with automatically discovered seman-tic themes (i.e., topics).
For instance, an utterance?I am finally through with this disastrous relation-ship?
uses a first-person pronoun and contains atopic about personal relationships.In comparison with various other models,SDTM shows the highest accuracy, and the result-ing conversation frequency and length patterns onself-disclosure are shown different over time.
Ourcontributions to the research community includethe following:1986?
We present key features and prior knowl-edge for identifying self-disclosure level, andshow relevance of it with experiment results(Sec.
2).?
We present a topic model that explicitly in-cludes the level of self-disclosure in a conver-sation using linguistic features and the latentsemantic topics (Sec.
3).?
We collect a large dataset of Twitter conver-sations over three years and annotate a smallsubset with self-disclosure level (Sec.
4).?
We compare the classification accuracy ofSDTM with other models and show that itperforms the best (Sec.
5).?
We correlate the self-disclosure patterns andconversation behaviors to show that there issignificant relationship over time (Sec.
6).2 Self-DisclosureIn this section, we look at social science literaturefor definition of the levels of self-disclosure.
Us-ing that definition, we devise an approach to au-tomatically identify the levels of self-disclosurein a large corpus of OSN conversations.
We dis-cuss three approaches, first, using first-person pro-noun features, and second, extracting seed wordsand phrases from the Twitter conversation cor-pus, and third, extracting seed words and phrasesfrom an external corpus of anonymously postedsecrets, and we demonstrate the efficacy of thoseapproaches with an annotated corpus.2.1 Self-disclosure (SD) levelTo analyze self-disclosure, researchers categorizeself-disclosure language into three levels: G (gen-eral) for no disclosure, M for medium disclosure,and H for high disclosure (Vondracek and Von-dracek, 1971; Barak and Gluck-Ofri, 2007).
Ut-terances that contain general (non-sensitive) in-formation about the self or someone close (e.g.,a family member) are categorized as M. Exam-ples are personal events, past history, or futureplans.
Utterances about age, occupation and hob-bies are also included.
Utterances that containsensitive information about the self or someoneclose are categorized as H. Sensitive informationincludes personal characteristics, problematic be-haviors, physical appearance and wishful ideas.Generally, these are thoughts and information that(a) A G level Twitter conversation(b) A M level Twitter conversation(c) A H level Twitter conversationFigure 1: An example of a Twitter conversation(from annotated dataset) with G, M and H level ofself-disclosure.one would keep as secrets to himself.
All otherutterances, those that do not contain informationabout the self or someone close are categorizedas G. Examples include gossip about celebritiesor factual discourse about current events.
Figure1 shows Twitter conversation examples with G,M and H levels from annotated dataset (see Sec-tion 4.2 for a detailed description of the annotateddataset).2.2 G Level of Self-DisclosureAn obvious clue of self-disclosure is the use offirst-person pronouns.
For example, phrases suchas ?I live?
or ?My name is?
indicate that the ut-terance contains personal information.
In pre-vious research, the simple method of countingfirst-person pronouns was used to measure the de-gree of self-disclosure (Joinson, 2001; Barak andGluck-Ofri, 2007).
Consequently, the absence of afirst-person pronoun signals that the utterance be-longs in the G level of self-disclosure.
We ver-ify this pattern with a dataset of Tweets annotatedwith G, M, and H levels.
We divide the annotatedTweets into two classes, G and M/H.
Then we com-pute mutual information of each unigram, bigram,or trigram feature to see which features are mostdiscriminative.
As Table 1 shows, 18 out of 301987Category Words/ExpressionsUnigram my, I, I?m, I?ll, but, was, I?ve, love, dad, haveBigram I love, I was, I have, my dad, go to, my mom,with my, have to, to go, my mumTrigram I have a, is going to, to go to, want to go, and Iwas, going to miss, I love him, I think I, I waslike, I wish ITable 1: High ranked words and expressions bymutual information between G and M/H level inannotated conversations.most highly ranked discriminative features containa first-person pronoun.2.3 M Level of Self-DisclosureUtterances with M level include two types: 1)information related with past events and futureplans, and 2) general information about self(Barak and Gluck-Ofri, 2007).
For the former, weadd as seed trigrams ?I have been?
and ?I will?.For the latter, we use seven types of informationgenerally accepted to be personally identifiable in-formation (McCallister, 2010), as listed in the leftcolumn of Table 2.
To find the appropriate tri-grams for those, we take Twitter conversation data(described in Section 4.1) and look for trigramsthat begin with ?I?
and ?my?
and occur more than200 times.
We then check each one to see whetherit is related with any of the seven types listed inthe table.
As a result, we find 57 seed trigrams forM level.
Table 2 shows several examples.Type TrigramName My name is, My last nameBirthday My birthday is, My birthday partyLocation I live in, I lived in, I live onContact My email address, My phone numberOccupation My job is, My new jobEducation My high school, My college isFamily My dad is, My mom is, My family isTable 2: Example seed trigrams for identifying Mlevel of SD.
There are 51 of these used in SDTM.2.4 H Level of Self-DisclosureUtterances with H level express secretive wishesor sensitive information that exposes self or some-one close (Barak and Gluck-Ofri, 2007).
Theseare generally kept as secrets.
With this intuition,we crawled 26,523 posts from Six Billion Secrets1site where users post secrets anonymously2.
We1http://www.sixbillionsecrets.com2This site is regularly monitored for spam.Category Words - SECRET Words - Annotatedphysicalappear-anceacne, hair, overweight,stomach, chest, hand,scar, thighs, chubbyankle, face, toe,skinmental/physicalconditionaddicted, bulimia, doc-tor, illness, alcoholic,disease, drugs, pillsache, epilepsy,pain, chiropractor,codeineTable 3: Example words for identifying H level ofSD from secret posts (2nd column) and annotateddata (3rd column).
Categories are hand-labeled.call this external dataset SECRET.
Unlike G and Mlevels, evidence of H level of self-disclosure tendsto be topical, such as physical appearance, mentaland physical illnesses, and family problems, so wetake an approach of fitting a topic model driven byseed words.
A similar approach has been success-ful in sentiment classification (Jo and Oh, 2011;Kim et al., 2013).A critical component of this approach is the setof seed words with which to drive the discoveryof topics that are most indicative of H level self-disclosure.
To extract the seed words that expresssecretive personal information, we compute mu-tual information (Manning et al., 2008) with SE-CRET and 24,610 randomly selected tweets.
Weselect 1,000 words with high mutual informationand filter out stop words.
Table 3 shows some ofthese words.
To extract seed trigrams of secretivewishes, we again look for trigrams that start with?I?
or ?my?, occur more than 200 times, and selecttrigrams of wishful thinking, such as ?I want to?,and ?I wish I?.
In total, there are 88 seed wordsand 8 seed trigrams for H.Since SECRET is quite different from Twitter,we must show that posts in SECRET are seman-tically similar to the H level Tweets.
Rather thandirectly comparing SECRET posts and Tweets, weuse the same method of extracting discriminativeword features from the annotated H level Tweets(see Section 4.2).
Table 3 shows the seed wordsextracted from SECRET as well as the annotatedTweets.
Because the annotated dataset consists ofonly 200 conversations, the coverage of the topicsseems narrower than the much larger SECRETS,but both datasets show similarities in the topics.This, combined with the results of the model withthe two sets of seed words (see Section 5 for theresults), shows that SECRETS is an effective andsimple-to-obtain substitute for an annotated cor-pus of H level of self-disclosure.1988??????CTN??????
3??????
3Figure 2: Graphical model of SDTMNotation DescriptionG; M ; H {general; medium; high} SD levelC; T ; N Number of conversations; tweets;wordsKG;KM;KHNumber of topics for {G; M; H}c; ct Conversation; tweet in conversation cyctSD level of tweet ct, G or M/HrctSD level of tweet ct, M or HzctTopic of tweet ctwctnnthword in tweet ct?
Learned Maximum entropy parametersxctFirst-person pronouns features?ctDistribution over SD level of tweet ctpicSD level proportion of conversation c?Gc;?Mc;?HcTopic proportion of {G; M; H} in con-versation c?G;?M ;?H Word distribution of {G; M; H}?
; ?
Dirichlet prior for ?
; pi?G,?M ;?H Dirichlet prior for ?G;?M ;?HnclNumber of tweets assigned SD level lin conversation cnlckNumber of tweets assigned SD level land topic k in conversation cnlkvNumber of instances of word v assignedSD level l and topic kmctkvNumber of instances of word v assignedtopic k in tweet ctTable 4: Summary of notations used in SDTM3 Self-Disclosure Topic ModelThis section describes our model, the self-disclosure topic model (SDTM), for classifyingself-disclosure level and discovering topics foreach self-disclosure level.3.1 ModelIn section 2, we discussed different approachesto identifying each level of self-disclosure, basedon social science literature, annotated and unan-notated Tweets, and an external corpus of se-cret posts.
In this section, we describe ourself-disclosure topic model, based on the widelyused latent Dirichlet allocation (Blei et al., 2003),which incorporates those approaches.Figure 2 illustrates the graphical model of1.
For each level l ?
{G, M, H}:For each topic k ?
{1, .
.
.
,Kl}:Draw ?lk ?
Dir(?l)2.
For each conversation c ?
{1, .
.
.
, C}:(a) Draw ?Gc ?
Dir(?
)(b) Draw ?Mc ?
Dir(?
)(c) Draw ?Hc ?
Dir(?
)(d) Draw pic ?
Dir(?
)(e) For each message t ?
{1, .
.
.
, T}:i.
Observe first-person pronouns features xctii.
Draw ?ct ?MaxEnt(xct,?)iii.
Draw yct ?
Bernoulli(?ct)iv.
If yct = 0 which is G level:A.
Draw zct ?Mult(?Gc )B.
For each word n ?
{1, .
.
.
, N}:Draw word wctn ?Mult(?Gzct)Else which can be M or H level:A.
Draw rct ?Mult(pic)B.
Draw zct ?Mult(?rctc )C. For each word n ?
{1, .
.
.
, N}:Draw word wctn ?Mult(?rctzct)Figure 3: Generative process of SDTM.SDTM and how those approaches are embodiedin it.
The first approach based on the first-personpronouns is implemented by the observed vari-able xctand the parameters ?
from a maximumentropy classifier for G vs. M/H level.
The ap-proach of seed words and phrases for levels M andH is implemented by the three separate word-topicprobability vectors for the three levels of SD: ?lwhich has a Bayesian informative prior ?lwherel ?
{G,M,H}, the three levels of self-disclosure.Table 4 lists the notations used in the model andthe generative process, and Figure 3 describes thegenerative process.3.2 Classifying G vs M/H levelsClassifying the SD level for each tweet is done intwo parts, and the first part classifies G vs. M/Hlevels with first-person pronouns (I, my, me).
Inthe graphical model, y is the latent variable thatrepresents this classification, and ?
is the distri-bution over y. x is the observation of the first-person pronoun in the tweets, and?
are the param-eters learned from the maximum entropy classifier.With the annotated Twitter conversation dataset(described in Section 4.2), we experimented withseveral classifiers (Decision tree, Naive Bayes)and chose the maximum entropy classifier becauseit performed the best, similar to other joint topicmodels (Zhao et al., 2010; Mukherjee et al., 2013).19893.3 Classifying M vs H levelsThe second part of the classification, the M and theH level, is driven by informative priors with seedwords and seed trigrams.
In the graphical model,r is the latent variable that represents this classi-fication, and pi is the distribution over r. ?
is anon-informative prior for pi, and ?lis an informa-tive prior for each SD level by seed words.
Forexample, we assign a high value for the seed word?acne?
for ?H, and a low value for ?My name is?.This approach is the same as joint models of topicand sentiment (Jo and Oh, 2011; Kim et al., 2013).3.4 InferenceFor posterior inference of SDTM, we use col-lapsed Gibbs sampling which integrates out la-tent random variables ?,pi,?, and ?.
Then weonly need to compute y, r and z for each tweet.We compute full conditional distribution p(yct=j?, rct= l?, zct= k?|y?ct, r?ct, z?ct,w,x) fortweet ct as follows:p(yct= 0, zct= k?|y?ct, r?ct, z?ct,w,x)?exp(?0?
xct)?1j=0exp(?j?
xct)g(c, t, l?, k?
),p(yct= 1, rct= l?, zct= k?|y?ct, r?ct, z?ct,w,x)?exp(?1?
xct)?1j=0exp(?j?
xct)(?l?+ n(?ct)cl?)
g(c, t, l?, k?
),where z?ct, r?ct,y?ctare z, r,y without tweetct, mctk?(?
)is the marginalized sum over word v ofmctk?vand the function g(c, t, l?, k?)
as follows:g(c, t, l?, k?)
=?
(?Vv=1?l?v+ nl??(ct)k?v)?
(?Vv=1?l?v+ nl??(ct)k?v+mctk?(?
))(?k?+ nl?(?ct)ck?
?Kk=1?k+ nl?ck)V?v=1?
(?l?v+ nl??(ct)k?v+mctk?v)?
(?l?v+ nl??
(ct)k?v).4 Data Collection and AnnotationTo test our self-disclosure topic model, we use alarge dataset of conversations consisting of Tweetsover three years such that we can analyze the re-lationship between self-disclosure behavior andconversation frequency and length over time.
Wechose to crawl Twitter because it offers a prac-tical and large source of conversations (Ritter etal., 2010).
Others have also analyzed Twitter con-versations for natural language and social mediaUsers Dyads Conv?s Tweets101,686 61,451 1,956,993 17,178,638Table 5: Dataset of Twitter conversations.
Wechose conversations consisting of five or moretweets each.
We chose dyads with twenty or moreconversations.research (boyd et al., 2010; Danescu-Niculescu-Mizil et al., 2011), but we collect conversationsfrom the same set of dyads over several months fora unique longitudinal dataset.
We also make surethat each conversation is at least five tweets, andthat each dyad has at least twenty conversations.4.1 Collecting Twitter conversationsWe define a Twitter conversation as a chain oftweets where two users are consecutively reply-ing to each other?s tweets using the Twitter replybutton.
We initialize the set of users by randomlysampling thirteen users who reply to other usersin English from the Twitter public streams3.
Thenwe crawl each user?s public tweets, and look atusers who are mentioned in those tweets.
It isa breadth-first search in the network defined byusers as nodes and edges as conversations.
Werun this search for dyads until the depth of four,and filter out users who tweet in a non-Englishlanguage.
We use an open source tool for de-tecting English tweets4.
To protect users?
privacy,we replace Twitter userid, usernames and url intweets with random strings.
This dataset consistsof 101,686 users, 61,451 dyads, 1,956,993 conver-sations and 17,178,638 tweets which were postedbetween August 2007 to July 2013.
Table 5 sum-marizes the dataset.4.2 Annotating self-disclosure levelTo measure the accuracy of our model, we ran-domly sample 301 conversations, each with ten orfewer tweets, and ask three judges, fluent in En-glish and graduate students/researchers, to anno-tate each tweet with the level of self-disclosure.Judges first read and discussed the definitions andexamples of self-disclosure level shown in (Barakand Gluck-Ofri, 2007), then they worked sepa-rately on a Web-based platform.As a result of annotation, there are 122 G levelconverstaions, 147 M level and 32 H level con-3https://dev.twitter.com/docs/api/streaming4https://github.com/shuyo/ldig1990Figure 4: Screenshot of annotation web-basedplatform.
Annotators read a Twitter conversationand annotate self-disclosure level to each tweet.versations, and inter-rater agreement using Fleisskappa (Fleiss, 1971) is 0.68, which is substantialagreement result (Landis and Koch, 1977).5 Classification of Self-Disclosure LevelThis section describes experiments and results ofSDTM as well as several other methods for classi-fication of self-disclosure level.We first start with the annotated dataset in sec-tion 4.2 in which each tweet is annotated with SDlevel.
We then aggregate all of the tweets of aconversation, and we compute the proportions oftweets in each SD level.
When the proportion oftweets at M or H level is equal to or greater than0.2, we take the level of the larger proportion andassign that level to the conversation.
When theproportions of tweets at M or H level are both lessthan 0.2, we assign G to the SD level.
The reasonfor setting 0.2 as the threshold is that a conversa-tion containing tweets with H or M level of self-disclosure usually starts with a greeting or a gen-eral comment, and contains one or more questionsor comments before or after the self-disclosuretweet.We compare SDTM with the following methodsfor classifying conversations for SD level:?
LDA (Blei et al., 2003): A Bayesian topicmodel.
Each conversation is treated as a doc-ument.
Used in previous work (Bak et al.,2012).?
MedLDA (Zhu et al., 2012): A super-vised topic model for document classifica-tion.
Each conversation is treated as a doc-ument and response variable can be mappedto a SD level.?
LIWC (Tausczik and Pennebaker, 2010):Word counts of particular categories5.
Usedin previous work (Houghton and Joinson,2012).?
Bag of Words + Bigrams + Trigrams(BOW+): A bag of words, bigram and tri-gram features.
We exclude features that ap-pear only once or twice.?
Seed words and trigrams (SEED): Occur-rences of seed words/trigrams from SECRETwhich are described in section 3.3.?
SDTM with seed words from annotatedTweets (SDTM?
): To compare with SDTMbelow using seed words from SECRET, thisuses seed words from the annotated data de-scribed in section 2.4.?
ASUM (Jo and Oh, 2011): A joint modelof sentiments and topics.
We map each SDlevel to one sentiment and use the same seedwords/trigrams from SECRET as in SDTMbelow.
Used in previous work (Bak et al.,2012).?
First-person pronouns (FirstP): Occurrenceof first-person pronouns which are describedin section 3.2.
To identify first-person pro-nouns, we tagged parts of speech in eachtweet with the Twitter POS tagger (Owoputiet al., 2013).?
First-person pronouns + Seed words/trigrams(FP+SE1): First-person pronouns and seedwords/trigrams from SECRET.?
Two stage classifier with First-person pro-nouns + Seed words/trigrams (FP+SE2): A5personal pronouns, 3rd person singular words, familywords, human words, sexual words, etc1991Method Acc G F1M F1H F1Avg F1LDA 49.2 0.00 0.65 0.05 0.23MedLDA 43.3 0.41 0.52 0.09 0.34LIWC 49.2 0.34 0.61 0.18 0.38BOW+ 54.1 0.50 0.59 0.15 0.41SEED 54.4 0.52 0.60 0.14 0.42ASUM 56.6 0.32 0.70 0.38 0.47SDTM?
60.4 0.57 0.70 0.14 0.47FirstP 63.2 0.63 0.69 0.10 0.47FP+SE1 61.0 0.61 0.67 0.16 0.48FP+SE2 60.4 0.64 0.69 0.17 0.50SDTM 64.5 0.61 0.71 0.43 0.58Table 6: SD level classification accuracies and F-measures using annotated data.
Acc is accuracy,and G F1is F-measure for classifying the G level.Avg F1is the macroaveraged value of G F1, M F1and H F1.
SDTM outperforms all other methodscompared.
The difference between SDTM andFirstP is statistically significant (p-value < 0.05for accuracy, < 0.0001 for Avg F1).two stage classifier with first-person pro-nouns and seed words/trigrams from SE-CRET.
In the first stage, the classifier identi-fies G with first-person pronouns.
Then in thesecond stage, the classifier uses seed wordsand trigrams to identify M and H levels.?
SDTM: Our model with first-person pro-nouns and seed words/trigrams from SE-CRET.SEED, LIWC, LDA and FirstP cannot be useddirectly for classification, so we use Maximum en-tropy model with outputs of each of those modelsas features6.
BOW+ uses SVM with a radial ba-sis kernel which performs better than all other set-tings tried including maximum entropy.
We splitthe data randomly into 80/20 for train/test.
We runMedLDA, ASUM and SDTM 20 times each andcompute the average accuracies and F-measure foreach level.
We run LDA and MedLDA with var-ious number of topics from 80 to 140, and 120topics shows best outputs.
So we set 120 topicsfor LDA, MedLDA and ASUM, 60; 40; 40 topicsfor SDTM KG,KMand KHrespectively whichis best perform from 40; 40; 40 to 60; 60; 60 top-ics.
We assume that a conversation has few topics6It performs better than other classifiers (C4.5, Naive-Bayes, SVM with linear kernel, polynomial kernel and radialbasis)and self-disclosure levels, so we set ?
= ?
= 0.1(Tang et al., 2014).
To incorporate the seed wordsand trigrams into ASUM and SDTM, we initial-ize ?G,?Mand ?Hdifferently.
We assign a highvalue of 2.0 for each seed word and trigram forthat level, and a low value of 10?6for each wordthat is a seed word for another level, and a defaultvalue of 0.01 for all other words.
This approachis the same as previous papers (Jo and Oh, 2011;Kim et al., 2013).As Table 6 shows, SDTM performs better thanthe other methods for accuracy as well as F-measure.
LDA and MedLDA generally showthe lowest performance, which is not surprisinggiven these models are quite general and not tunedspecifically for this type of semi-supervised clas-sification task.
BOW which is simple word fea-tures also does not perform well, showing espe-cially low F-measure for the H level.
LIWC andSEED perform better than LDA, but these havequite low F-measure for G and H levels.
ASUMshows better performance for classifying H levelthan others, confirming the effectiveness of a topicmodeling approach to this difficult task, but not aswell as SDTM.
FirstP shows good F-measure forthe G level, but the H level F-measure is quite low,even lower than SEED.
Combining first-personpronouns and seed words and trigrams (FP+SE1)shows better than each feature alone, and the twostage classifier (FP+SE2) which is a similar ap-proach taken in SDTM shows better results.
Fi-nally, SDTM classifies G and M level at a similaraccuracy with FirstP, FP+SE1 and FP+SE2, butit significantly improves accuracy for the H levelcompared to all other methods.6 Relations of Self-Disclosure andConversation BehaviorsIn this section, we investigate whether there isa relationship between self-disclosure and con-versation behaviors over time.
Self-disclosure isone way to maintain and improve relationships(Jourard, 1971; Joinson and Paine, 2007).
Sotwo people?s intimacy changes over time has rela-tionship with self-disclosure in their conversation.However, it is hard to identify intimacy betweenusers in large scale online social network.
So wechoose conversation behaviors such as conversa-tion frequency and length which can be treated asproxies for measuring intimacy between two peo-ple (Emmers-Sommer, 2004; Bak et al., 2012).1992With SDTM, we can automatically classify theSD level of a large number of conversations, sowe investigate whether there is a similar relation-ship between self-disclosure in conversations andsubsequent conversation behaviors with the samepartner on Twitter.For comparing conversation behaviors overtime, we divided the conversations into two setsfor each dyad.
For the initial period, we includeconversations from the dyad?s first conversation to20 days later.
And for the subsequent period,we include conversations during the subsequent 10days.
We compute proportions of conversation foreach SD level for each dyad in the initial andsubsequent periods.More specifically, we ask the following threequestions:1.
If a dyad shows high conversation frequencyat a particular time period, would they dis-play higher SD in their subsequent conver-sations?2.
If a dyad displays high SD level in their con-versations at a particular time period, wouldtheir subsequent conversations be longer?3.
If a dyad displays high overall SD level,would their conversations increase in lengthover time more than dyads with lower overallSD level?6.1 Experiment SetupWe first run SDTM with all of our Twitter con-versation data with 150; 120; 120 topics forSDTM KG,KMand KHrespectively.
Thehyper-parameters are the same as in section 5.
Tohandle a large dataset, we employ a distributed al-gorithm (Newman et al., 2009), and run with 28threads.Table 7 shows some of the topics that wereprominent in each SD level by KL-divergence.
Asexpected, G level includes general topics such asfood, celebrity, soccer and IT devices, M level in-cludes personal communication and birthday, andfinally, H level includes sickness and profanity.We define a new measurement, SD level scorefor a dyad in the period, which is a weighted sumof each conversation with SD levels mapped to 1,2, and 3, for the levels G, M, and H, respectively.0 5 10 15 20 25 30 35Initial conversation frequency2.002.022.042.062.082.102.122.14Subsequent SDlevelFigure 5: Relationship between initial conversa-tion frequency and subsequent SD level.
Thesolid line is the linear regression line, and the co-efficient is 0.0020 with p < 0.0001, which showsa significant positive relationship.6.2 Does high frequency of conversation leadto more self-disclosure?We investigate whether the initial conversationfrequency is correlated with the SD level in thesubsequent period.
We run linear regression withthe initial conversation frequency as the indepen-dent variable, and SD level in the subsequent pe-riod as the dependent variable.The regression coefficient is 0.0020 with low p-value (p < 0.0001).
Figure 5 shows the scatterplot.
We can see that the slope of the regressionline is positive.6.3 Does high self-disclosure lead to longerconversations?Now we investigate the effect of the self-disclosure level to conversation length.
We runlinear regression with the intial SD level score asthe independent variable, and the rate of changein conversation length between initial periodand subsequent period as the dependent variable.Conversation length is measured by the number oftweets in a conversation.The result of regression is that the independentvariable?s coefficient is 0.048 with a low p-value(p < 0.0001).
Figure 6 shows the scatter plot withthe regression line, and we can see that the slopeof regression line is positive.1993G level M level H level101 184 176 36 104 82 113 33 19chocolate obama league send twitter going ass better lipsbutter he?s win email follow party bitch sick kissesgood romney game i?ll tumblr weekend fuck feel lovecake vote season sent tweet day yo throat smilespeanut right team dm following night shit cold softlymilk president cup address account dinner fucking hope handsugar people city know fb birthday lmao pain eyescream good arsenal check followers tomorrow shut good neckmake going chelsea link facebook come dick cough armslove time liverpool need followed i?ll kick bad headyum party won message omg family face i?ve smirkshot election football let right fun hoe need slowlycookies gop united sure saw friends lmfao sore hairbanana paul final thanks page tonight nigga flu facebread way away my email timeline plans bi today chestTable 7: High ranked topics in each level by comparing KL-divergence with other level?s topics1.0 1.5 2.0 2.5 3.0Initial SD level0.100.050.000.050.100.15# Tweets inconversationchanges proportion overtimeFigure 6: Relationship between initial SD leveland conversation length changes over time.
Thesolid line is the linear regression line, and the co-efficient is 0.048 with p < 0.0001, which shows asignificant positive relationship.6.4 Is there a difference in conversationlength patterns over time depending onoverall SD level?Now we investigate the conversation lengthchanges over time with three groups, low,medium, and high, by overall SD level.
Thenwe investigate changes in conversation length overtime.Figure 7 shows the results of this investigation.First, conversations are generally lengthier whenSD level is high.
This phenomenon is also ob-0 5 10 15 20 25 30 35 40Conversation order8.08.59.09.510.010.5# Tweets in conversationhigh mid lowFigure 7: Changes in conversation length overtime.
We divide dyads into three groups by SDlevel score as low, medium, and high.
Conversa-tion length noticeably increases over time in themedium and high groups, but only slight in the lowgroup.served in figure 6, but here we can see it as along-term persistent pattern.
Second, conversationlength increases consistently and significantly forthe high and medium groups, but for the low SDgroup, there is not a significant increase of conver-sation length over time.7 Related WorkPrior work on quantitatively analyzing self-disclosure has relied on user surveys (Ledbetter et1994al., 2011; Trepte and Reinecke, 2013) or humanannotation (Barak and Gluck-Ofri, 2007; Court-ney Walton and Rice, 2013).
These methods con-sume much time and effort, so they are not suit-able for large-scale studies.
In prior work clos-est to ours, Bak et al.
(2012) showed that a topicmodel can be used to identify self-disclosure, butthat work applies a two-step process in which abasic topic model is first applied to find the top-ics, and then the topics are post-processed for bi-nary classification of self-disclosure.
We improveupon this work by applying a single unified modelof topics and self-disclosure for high accuracy inclassifying the three levels of self-disclosure.Subjectivity which is aspect of expressing opin-ions (Pang and Lee, 2008; Wiebe et al., 2004) isrelated with self-disclosure, but they are differentdimensions of linguistic behavior.
Because thereindeed are many high self-disclosure tweets thatare subjective, but there are also counter examplesin annotated dataset.
The tweet ?England manageris Roy Hodgson.?
is low self-disclosure and lowsubjectivity, ?I have barely any hair left.?
is highself-disclosure but low subjectivity, and ?Senatorstop lying!?
is low self-disclosure but high subjec-tivity.8 Conclusion and Future WorkIn this paper, we have presented the self-disclosuretopic model (SDTM) for discovering topics andclassifying SD levels from Twitter conversationdata.
We devised a set of effective seed wordsand trigrams, mined from a dataset of secrets.
Wealso annotated Twitter conversations to make aground-truth dataset for SD level.
With anno-tated data, we showed that SDTM outperformsprevious methods in classification accuracy and F-measure.
We publish the source code of SDTMand the dataset include annotated Twitter conver-sations and SECRET publicly7.We also analyzed the relationship between SDlevel and conversation behaviors over time.
Wefound that there is a positive correlation be-tween initial SD level and subsequent conversa-tion length.
Also, dyads show higher level ofSD if they initially display high conversation fre-quency.
Finally, dyads with overall medium andhigh SD level will have longer conversations overtime.
These results support previous results in so-7http://uilab.kaist.ac.kr/research/EMNLP2014cial psychology research with more robust resultsfrom a large-scale dataset, and show the effective-ness of computationally analyzing at SD behavior.There are several future directions for this re-search.
First, we can improve our modeling forhigher accuracy and better interpretability.
Forinstance, SDTM only considers first-person pro-nouns and topics.
Naturally, there are other lin-guistic patterns that can be identified by humansbut not captured by pronouns and topics.
Sec-ond, the number of topics for each level is varied,and so we can explore nonparametric topic mod-els (Teh et al., 2006) which infer the number oftopics from the data.
Third, we can look at therelationship between self-disclosure behavior andgeneral online social network usage beyond con-versations.
We will explore these directions in ourfuture work.AcknowledgmentsWe would like to thank Jing Liu and Wayne XinZhao for inspiring discussions, and the anony-mous reviewers for helpful comments.
Alice Ohis supported by ICT R&D program of MSIP/IITP[10041313, UX-oriented Mobile SW Platform].ReferencesJinYeong Bak, Suin Kim, and Alice Oh.
2012.
Self-disclosure and relationship strength in twitter con-versations.
In Proceedings of ACL.Azy Barak and Orit Gluck-Ofri.
2007.
Degree andreciprocity of self-disclosure in online forums.
Cy-berPsychology & Behavior, 10(3):407?417.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet allocation.
Journal of Ma-chine Learning Research, 3:993?1022.danah boyd, Scott Golder, and Gilad Lotan.
2010.Tweet, tweet, retweet: Conversational aspects ofretweeting on twitter.
In Proceedings of HICSS.S Courtney Walton and Ronald E Rice.
2013.
Medi-ated disclosure on twitter: The roles of gender andidentity in boundary impermeability, valence, dis-closure, and stage.
Computers in Human Behavior,29(4):1465?1474.Cristian Danescu-Niculescu-Mizil, Michael Gamon,and Susan Dumais.
2011.
Mark my words!
: Lin-guistic style accommodation in social media.
InProceedings of WWW.Tara M Emmers-Sommer.
2004.
The effect of com-munication quality and quantity indicators on inti-macy and relational satisfaction.
Journal of Socialand Personal Relationships, 21(3):399?411.1995Joseph L Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological bul-letin, 76(5):378.David J Houghton and Adam N Joinson.
2012.Linguistic markers of secrets and sensitive self-disclosure in twitter.
In Proceedings of HICSS.Yohan Jo and Alice H Oh.
2011.
Aspect and senti-ment unification model for online review analysis.In Proceedings of WSDM.Adam N Joinson and Carina B Paine.
2007.
Self-disclosure, privacy and the internet.
The Oxfordhandbook of Internet psychology, pages 237?252.Adam N Joinson.
2001.
Self-disclosure incomputer-mediated communication: The role ofself-awareness and visual anonymity.
EuropeanJournal of Social Psychology, 31(2):177?192.Sidney M Jourard.
1971.
Self-disclosure: An experi-mental analysis of the transparent self.Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, andShixia Liu.
2013.
A hierarchical aspect-sentimentmodel for online reviews.
In Proceedings of AAAI.J Richard Landis and Gary G Koch.
1977.
The mea-surement of observer agreement for categorical data.biometrics, pages 159?174.Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-root, Kevin R Meyer, Yuping Mao, and Brian Swaf-ford.
2011.
Attitudes toward online social con-nection and self-disclosure as predictors of facebookcommunication and relational closeness.
Communi-cation Research, 38(1):27?53.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informationretrieval, volume 1.
Cambridge University PressCambridge.Erika McCallister.
2010.
Guide to protecting the confi-dentiality of personally identifiable information.
DI-ANE Publishing.Arjun Mukherjee, Vivek Venkataraman, Bing Liu, andSharon Meraz.
2013.
Public dialogue: Analysis oftolerance in online discussions.
In Proceedings ofACL.David Newman, Arthur Asuncion, Padhraic Smyth,and Max Welling.
2009.
Distributed algorithmsfor topic models.
Journal of Machine Learning Re-search, 10:1801?1828.Olutobi Owoputi, Brendan OConnor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of HLT-NAACL.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In Pro-ceedings of HLT-NAACL.Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, QiaozhuMei, and Ming Zhang.
2014.
Understanding thelimiting factors of topic modeling via posterior con-traction analysis.
In Proceedings of The 31st In-ternational Conference on Machine Learning, pages190?198.Yla R Tausczik and James W Pennebaker.
2010.
Thepsychological meaning of words: Liwc and comput-erized text analysis methods.
Journal of Languageand Social Psychology.Yee Whye Teh, Michael I Jordan, Matthew J Beal, andDavid M Blei.
2006.
Hierarchical dirichlet pro-cesses.
Journal of the american statistical associ-ation, 101(476).Sabine Trepte and Leonard Reinecke.
2013.
The re-ciprocal effects of social network site use and thedisposition for self-disclosure: A longitudinal study.Computers in Human Behavior, 29(3):1102 ?
1112.Sarah I Vondracek and Fred W Vondracek.
1971.
Themanipulation and measurement of self-disclosure inpreadolescents.
Merrill-Palmer Quarterly of Behav-ior and Development, 17(1):51?58.Janyce Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational linguistics,30(3):277?308.Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly modeling aspects and opin-ions with a maxent-lda hybrid.
In Proceedings ofEMNLP.Jun Zhu, Amr Ahmed, and Eric P Xing.
2012.
Medlda:maximum margin supervised topic models.
Journalof Machine Learning Research, 13:2237?2278.1996
