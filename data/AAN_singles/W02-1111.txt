Fine-Grained Proper Noun Ontologies for Question AnsweringGideon S. MannDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, Maryland 21218gsm@cs.jhu.eduAbstractThe WordNet lexical ontology, which isprimarily composed of common nouns,has been widely used in retrieval tasks.Here, we explore the notion of a fine-grained proper noun ontology and arguefor the utility of such an ontology in re-trieval tasks.
To support this claim, webuild a fine-grained proper noun ontol-ogy from unrestricted news text and usethis ontology to improve performance ona question answering task.1 IntroductionThe WordNet lexical ontology (Miller, 1990) con-tains more than 100,000 unique noun forms.
Most ofthese noun forms are common nouns (nouns describ-ing non-specific members of a general class, e.g.?detective?).
Only a small percentage1 of the nounsin WordNet are proper nouns (nouns describing spe-cific instances, e.g.
?
[the detective] Columbo?
).The WordNet ontology has been widely useful,with applications in information retrieval (Sussna,1993), text classification (Scott and Matwin, 1998),and question answering (Pasca and Harabagiu,2001).
These successes have shown that commonnoun ontologies have wide applicability and utility.There exists no ontology with similar coverageand detail for proper nouns.
Prior work in propernoun identification has focused on ?named entity?1A random 100 synset sample was composed of 9% propernouns.recognition (Chinchor et al, 1999), stemming fromthe MUC evaluations.
In this task, each proper nounis categorized, for example, as a PERSON, a LOCA-TION, or an ORGANIZATION.These coarse categorizations are useful, but morefinely grained classification might have additionaladvantages.
While Bill Clinton is appropriatelyidentified as a PERSON, this neglects his identity asa president, a southerner, and a saxophone player.If an information request identifies the object of thesearch not merely as a PERSON, but as a typedproper noun (e.g.
?a southern president?
), this pref-erence should be used to improve the search.Unfortunately, building a proper noun ontologyis more difficult than building a common noun on-tology, since the set of proper nouns grows morerapidly.
New people are born.
As people change,their classification must change as well.
A broad-coverage proper noun ontology must be constantlyupdated.
Therefore, to propose a viable system, amethod, however limited, must be presented to builda proper noun ontology.In this paper, we explore the idea of a fine-grainedproper noun ontology and its use in question answer-ing.
We build a proper noun ontology from unre-stricted text using simple textual co-occurrence pat-terns (Section3).
This automatically constructed on-tology is then used on a question answering task togive preliminary results on the utility of this infor-mation (Section 4).2 Ontologies for Question AnsweringModern question answering systems rely heavily onthe fact that questions contain strong preferences forThe 1974 film ?That?s Entertainment!?
was made from film clips from what Hollywood studio?What king of Babylonia reorganized the empire under the Code that bears his name?What rock ?n?
roll musician was born Richard Penniman on Christmas Day?What is the oldest car company which still exists today?What was the name of the female Disco singer who scored with the tune ?Dim All the Lights?
in 1979?What was the name of the first Russian astronaut to do a spacewalk?What was the name of the US helicopter pilot shot down over North Korea?Which astronaut did Tom Hanks play in ?Apollo 13?
?Which former Klu Klux Klan member won an elected office in the U.S.?Who?s the lead singer of the Led Zeppelin band?Who is the Greek goddess of retribution or vengeance?Who is the prophet of the religion of Islam?Who is the author of the book, ?The Iron Lady: A Biography of Margaret Thatcher?
?Who was the lead actress in the movie ?Sleepless in Seattle?
?Table 1: Questions Indicating a Typed Proper Noun Preference (Trivia and Trec-8/9 Questions)the types of answers they expect.
Kupiec (1993) ob-serves that the WH word itself provides preferences(e.g.
?Who?
questions prefer PERSON answers).He further observes that questions also include typepreferences in other parts of the question.
Some-times these preferences occur within the WH phrase(?what color?
), and sometimes they are embeddedelsewhere within the question (?what is the color...?).
In both, the question indicates a preference forcolors as answers.Current question answering systems use ontolo-gies when these type preferences are detected.
Onesimple method is as follows: when a type preferenceis recognized, the preference is located within theWordNet ontology, and children of that synset aretreated as potential answers.
Given the question ?Inpool, what color is the eight ball?
?, and the ontol-ogy excerpt shown in Figure 1, the system can nar-row down the range of choices.
This approach hashigh precision: if the type preference can be located,and a candidate answer is found in a child node (in asuitable corpus context), then the candidate is likelyto be the answer.Harabagiu et al (2000) proposes another methodfor using an ontology: WordNet subtrees are linkedto types recognized by a named entity recognizer.Their system works as follows: given the question?What is the wingspan of a condor?
?, it locates?wingspan?
in the WordNet ontology.
It then detectsthat ?wingspan?
falls into the MAGNITUDE subtreewhich is linked to the QUANTITY type.
This linkswords in the MAGNITUDE subtree to numbers.While the WordNet ontology is primarily com-posed of common nouns, it contains some propernouns, typically those least likely to be ephemeral(e.g.
countries, cities, and famous figures in his-tory).
These can be used as any other commonnouns are used.
Given the question ?Which com-poser wrote ?The Marriage of Figaro??
?, the Word-Net ontology will provide the fact that ?WolfgangAmadeus Mozart?
is a composer.Table 1 lists sample questions where a propernoun ontology would be useful.
Some of the propernoun types are relatively static (Greek gods, kingsof Babylonia).
Other categories are more ephemeral(lead singers, British actresses).
WordNet enumer-ates 70 Greek gods and 80 kings, but no lead singersand no British actresses.Ravichandran and Hovy (2002) present an alter-native ontology for type preference and describe amethod for using this alternative ontology to extractparticular answers using surface text patterns.
Theirproposed ontology is orders of magnitude smallerthan WordNet and ontologies considered here, hav-ing less than 200 nodes.3 Building a Proper Noun OntologyIn order to better answer the questions in Table 1, webuilt a proper noun ontology from approximately 1gigabyte of AP news wire text.
To do so, we tok-Answer whiteWordNetblack greychromatic colorachromatic colorred blue pinkcolorPreferenceTypeFigure 1: Using WordNet to Directly Provide TypePreferencesdistance, lengthlight time altitudequantityAnswer 7wingspanNamed Entity RecognizerWordNetsizemagnitudeamountPreferenceTypeFigure 2: Linking WordNet subtrees to a Named En-tity Recognizerenized and part-of-speech tagged the text, and thensearched for instances of a common noun followedimmediately by a proper noun.
This pattern de-tects phrases of the form ?
[the] automaker MercedesBenz?, and is ideally suited for proper nouns.
In APnews wire text this is a productive and high preci-sion pattern, generating nearly 200,000 unique de-scriptions, with 113,000 different proper nouns and20,000 different descriptions.
In comparison, the?such as?
pattern (Section 5) occurs less than 50,000times in the same size corpora.
Table 2 shows thedescriptions generated for a few proper nouns usingthis simple pattern.To assess the precision of the extractions, we tooka sample of 100 patterns extracted from the AP-newstext.
From these 100, 79 of the items classified asnamed entities were in fact named entities, and outof those, 60 (75%) had legitimate descriptions.SingerFolk SingerEmmanuel CharlemagneBurl IvesHou DejianJoan BaezJohn DenverLead SingerAxel RoseMarjo LeinonenJohn FogertyJim MorrisonBonoFigure 3: Subset of ?singer?
subtree in the InducedProper Noun OntologyTo build the complete ontology, first each descrip-tion and proper noun forms its own synset.
Then,links are added from description to each proper nounit appears with.
Further links are put between de-scriptions ?X Y?
and ?Y?
(noun compounds andtheir heads).
Clearly, this method is problematic inthe cases of polysemous words or complex noun-noun constructions (?slalom king?)
and integratingthis ontology with the WordNet ontology requiresfurther study.This proper noun ontology fills many of the holesin WordNet?s world knowledge.
While WordNet hasno lead singer synset, the induced proper noun on-tology detects 13 distinct lead singers (Figure 3).WordNet has 2 folk singers; the proper noun ontol-ogy has 20.
In total, WordNet lists 53 proper nounsas singers, while the induced proper noun ontologyhas more than 900.
While the induced ontology isnot complete, it is more complete than what was pre-viously available.As can be seen from the list of descriptions gener-ated by this pattern, people are described in a varietyof different ways, and this pattern detects many ofthem.
Table 3 shows the descriptions generated fora common proper noun (?Bill Gates?).
When thedescriptions are grouped by WordNet synsets andsenses manually resolved, the variety of descriptionsdecreases dramatically (Figure 4).
?Bill Gates?
canbe described by a few distinct roles, and a distribu-tion over these descriptions provide an informativeunderstanding: leader (.48), businessperson (.27),worker (.05), originator (.05), expert (.05), and richbillionairerich person expertwhizhead bossleadermogul entrepreneurcreatorcapitalist workerorginatorskilled workerbusinesspersonofficialchairman executive founderofficerpresiding officerpersonpioneerFigure 4: Descriptions of Bill Gates Organized into WordNet, observed descriptions boxedProper Noun Count DescriptionAxel Rose 3 singer2 lead singer2 vocalistEmma Thompson 3 actressMercedes-Benz 4 Luxury car maker4 car maker3 automaker2 family2 luxury1 gold1 service1 subsidiaryTable 2: Proper Noun Descriptions Extracted fromNews Corporaperson (.02).
Steve Jobs, who has a career path sim-ilar to Bill Gates, has a similar but distinct signature:originator (.6), expert (.4).One immediate observation is that some of thedescriptions may be more relevant than others.
IsGates?
role as an ?office worker?
as important as hisrole as a ?billionaire??
The current system makes nodecision and treats all descriptions as equally rele-vant and stores all of them.
There is no need to re-ject descriptions since there is no human cost in su-perfluous or distracting descriptions (unlike in sum-marization tasks).
It is important that no invalid de-scriptions are added.The previous examples have focused on propernouns which are people?s names.
However, thismethod works for many organizations as well, asProper Noun Count DescriptionBill Gates 15 chairman9 mogul, tycoon,magnate2 officer2 whiz, genius1 pioneer1 head1 founder1 executive1 entrepreneur1 boss1 billionaireTable 3: Bill Gates Descriptions in AP Newswire,grouped by WordNet synsetthe data in Table 2 show.
However, while descrip-tion extraction for people is high quality (84% cor-rect descriptions in a 100 example sample), for non-people proper names, the quality of extraction ispoorer (47% correct descriptions).
This is a trendwhich requires further study.4 Using a Proper Noun Ontology in aQuestion Answering TaskWe generated the above ontology and used it in asentence comprehension task: given a question anda sentence which answers the question, extract theminimal short answer to the question from the sen-tence.
The task is motivated by the observation thatextracting short answers is more difficult than ex-tracting full sentence or passage length ones.
Fur-Ontology Correct Total PrecisionAnsweredWordNet 127 169 75.1IPNO 46 67 68.6WN + IPNO 145 194 74.7Table 4: Performance on a Test Corpus when an In-duced Proper Noun Ontology (IPNO) is combinedwith Wordnetthermore, retrieving answers from smaller documentspaces may be more difficult than retrieving answersfrom larger ones, if smaller spaces have less redun-dant coverage of potential answers.
In this sen-tence comprehension task, there is virtually no re-dundancy.
To generate data for this task, we tooktrivia games, which, along with the question, had afull sentence explanation (Mann, 2002).Baseline experiments used the WordNet ontologyalone.
From a semantic type preference stated inthe question, a word was selected from the sentenceas an answer if was a child of the type preference.?Black?
would be picked as an answer for a ?color?type preference (Figure 1).To utilize the induced proper noun ontology, wetook the raw data and selected the trailing noun foreach proper noun and for each description.
Thus,for an extraction of the form ?computer mogul BillGates?, we added a pattern of the form ?Gatesmogul?.
We created an ontology from these in-stances completely separate from the WordNet on-tology.We put this induced proper noun ontology intothe pipeline as follows: if WordNet failed to find amatch, we used the induced proper noun ontology.
Ifthat ontology failed to find a match, we ignored thequestion.
In a full system, a named entity recognizermight be added to resolve the other questions.We selected 1000 trivia game questions at randomto test out the new two-ontology system.
Table 4shows the results of the experiments.
The boost isclear: improved recall at slightly decreased preci-sion.
Gains made by inducing an ontology from anunrestricted text corpus (newstext) and applying it toa unmatched test set (trivia games), suggests that abroad-coverage general proper noun ontology maybe useful.It is further surprising that this improvementcomes at such a small cost.
The proper noun on-tology wasn?t trimmed or filtered.
The only disad-vantage of this method is simply that its coverageis small.
Coverage may be increased by using everlarger corpora.
Alternatively, different patterns (forexample, appositives) may increase the number ofwords which have descriptions.
A rough error anal-ysis suggests that most of the errors come from mis-tagging, while few come from correct relationshipsin the ontology.
This suggests that attempts at noisereduction might be able to lead to larger gains in per-formance.Another potential method for improving coverageis by bootstrapping descriptions.
Our test corpuscontained a question whose answer was ?Mercedes-Benz?, and whose type preference was ?car com-pany?.
While our proper noun ontology containeda related link (Mercedes-Benz automaker), it didnot contain the exact link (Mercedes-Benz car com-pany).
However, elsewhere there existed the links(Opel automaker) and (Opel car company).
Poten-tially these descriptions could be combined to infer(Mercedes-Benz car company).
Formally :(B Y) and (A Y) and (A Z)   (B Z)(Mercedes-Benz automaker) and (Opelautomaker) and (Opel car company)  (Mercedes-Benz car company)Expanding descriptions using a technique like thismay improve coverage.
Still, care must be takento ensure that proper inferences are made since thisrule is not always appropriate.
Bill Gates is a ten-billionaire; Steve Jobs isn?t.5 Prior Work in Building OntologiesThere has been considerable work in the pastdecade on building ontologies from unrestrictedtext.
Hearst (1992) used textual patterns (e.g.
?suchas?)
to identify common class members.
Cara-ballo and Charniak (1999) and Caraballo (1999)augmented these lexical patterns with more gen-eral lexical co-occurrence statistics (such as rel-ative entropy).
Berland and Charniak (1999) useHearst style techniques to learn meronym relation-ships (part-whole) from corpora.
There has alsobeen work in building ontologies from structuredCorrect Answer Question(Debbie) Reynolds What actress once held the title of ?Miss Burbank??
(Jim) Lovell Which astronaut did Tom Hanks play in ?Apollo 13?
?Xerxes Which Persian king moved an invasion force across theHellespont on a bridge of ships?
(Donna) Summer What was the name of the female Disco singerwho scored with the tune ?Dim All the Lights?
in 1979?MGM The 1974 film ?That?s Entertainment!?
was made from filmclips from what Hollywood studio?Table 5: Successes of the Proper Noun Ontology for the Question Answering tasktext, notably in the AQUILEX project (e.g.
Copes-take, 90) which builds ontologies from machinereadable dictionaries.The most closely related work is (Girju, 2001),which describes a method for inducing a domain-specific ontology using some of the techniques de-scribed in the previous paragraph.
This induced on-tology is then potential useful for a matched ques-tion domain.
Our paper differs in that it targetsproper nouns, in particular people, which are over-looked in prior work, have broad applicability, andcan be used in a cross-domain fashion.
Furthermore,we present initial results which attempt to gaugecoverage improvement as a result of the induced on-tology.Another related line of work is word clustering.In these experiments, the attempt is made to clustersimilar nouns, without regard to forming a hierarchy.Pereira et al (1993) presented initial work, cluster-ing nouns using their noun-verb co-occurrence in-formation.
Riloff and Lehnert (1993) build seman-tic lexicons using extraction pattern co-occurrence.Lin and Pantel (2001) extend these methods by us-ing many different types of relations and exploitingcorpora of tremendous size.The important difference for this work betweenthe hierarchical methods and the clustering meth-ods is that clusters are unlabelled.
The hierarchi-cal methods can identify that a ?Jeep Cherokee?
is atype of car.
In contrast, the clustering methods grouptogether related nouns, but exactly what the connec-tion is may be difficult to distinguish (e.g.
the clus-ter ?Sierra Club?, ?Environmental Defense Fund?,?Natural Resources Defense Council?, ?Public Cit-izen?, ?National Wildlife Federation?).
Generatinglabels for proper noun clusters may be another wayto build a proper noun ontology.The method we use to build the fine-grainedproper name ontology also resembles some of thework done in coarse-grained named entity recogni-tion.
In particular, Collins and Singer (1999) presenta sophisticated method for using bootstrapping tech-niques to learn the coarse-classification for a givenproper noun.
Riloff and Jones (1999) also present amethod to use bootstrapping to create semantic lexi-cons of proper nouns.
These methods may be appli-cable for use in fine-grained proper noun ontologyconstruction as well.Schiffman et al (2001) describe work on produc-ing biographical summaries.
This work attempts tosynthesize one description of a person from multi-ple mentions.
This summary is an end in itself, asopposed to general knowledge collected.
These de-scriptions also attempt to be parsimonious in con-trast to the rather free associations extracted by themethod presented above.6 ConclusionsIn this paper we have motivated the use of a propernoun ontology for question answering.
We de-scribed a method for inducing pieces of this on-tology, and then showed preliminary methods canbe useful.
Prior work on proper nouns has fo-cused on classifying them into very coarse cate-gories (e.g.
PERSON, LOCATION).
As this paperhas shown, these coarse classifications can be re-fined fortuitously, especially for the PERSON type.This paper demonstrates that inducing a gen-eral ontology improves question answering perfor-mance.
Previous work examined ontology inductionfor a specialized domain.
It is somewhat surprisingthat an ontology built from unrestricted text can leadto improvement on unmatched questions.The experiments we performed demonstrated thatthough the precision of the ontology is high, the cru-cial problem is increasing coverage.
Tackling thisproblem is an important area of future work.
Fi-nally, this work opens up a potential new avenue forwork on inducing proper noun ontologies.
There aredoubtlessly many more ways to extract descriptionsand to improve coverage.ReferencesMatthew Berland and Eugene Charniak.
1999.
Findingparts in very large corpora.
In Proceedings of the 37thAnnual Meeting of the Association for ComputationalLinguistics, pages 57?64.S.
Caraballo and E. Charniak.
1999.
Determining thespecificity of nouns from text.Sharon Caraballo.
1999.
Automatic acquisition of ahypernym-labeled noun hierarchy from text.
In Pro-ceedings of the 37th Annual Meeting of the Associationfor Computational Linguistics.N.
Chinchor, E. Brown, L. Ferro, and P. Robinson.
1999.1999 named entity recognition task definition.
TechReport.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing.Ann Copestake.
1990.
An approach to building the hi-erarchical element of a lexical knowledge base froma machine readable dictionary.
In First InternationalWorkshop on Inheritance in NLP.Roxana Girju.
2001.
Answer fusion with on-line on-tology development.
In Student Research WorkshopProceedings at The 2nd Meeting of the North Ameri-can Chapter of the Association for Computational Lin-guistics.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdea nu, R. Bunescu, R. Girju, V. Rus, andP.
Mor.
2000.
Falcon : Boosting knowledge for an-swer engines.
Proc.
of TREC-9.Marti Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
Proceedings of the FourteenthInternational Conference on Computational Linguis-tics (COLING-92).J.
Kupiec.
1993.
Murax: A robust linguistic approachfor question answering using an on-line encyclopedia.In ACM-SIGIR?93.Dekang Lin and Patrick Pantel.
2001.
Induction of se-mantic classes from natural language text.
In Proceed-ings of ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining, pages 317?322.Gideon S. Mann.
2002.
Learning how to answer ques-tions using trivia games.
In Proceedings of the Nine-teenth International Conference on ComputationalLinguistics (COLING 2002).G.
Miller.
1990.
Wordnet: An On-line Lexical Database.International Journal of Lexicography, 3(4):235?312.Marius Pasca and Sanda Harabagiu.
2001.
The informa-tive role of wordnet in open-domain question answer-ing.
In Proceedings of the NAACL 2001 Workshop onWordNet and Other Lexical Resources: Applications,Extensions and Customizations, pages 138?143.
Asso-ciation for Computational Linguistics.Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InMeeting of the Association for Computational Linguis-tics, pages 183?190.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics.Ellen Riloff and Rosie Jones.
1999.
Learning dictionar-ies for information extraction by multi-level bootstrap-ping.
In Proceedings of the Sixteenth National COn-ference on Artificial Intelligence, pages 1044?1049.E.
Riloff and W. Lehnert.
1993.
Automated DictionaryConstruction for Information Extraction from Text.
InProceedings of the Ninth IEEE Conference on Artifi-cial Intelligence for Applications, pages 93?99, LosAlamitos, CA.
IEEE Computer Society Press.Barry Schiffman, Inderjeet Mani, and Kristian J. Concep-cion.
2001.
Producing biographical summaries: Com-bining linguistic knowledge with corpus statistics.
InProceedings of the 39th Annual Meeting of the Associ-ation for Computational Linguistics.Sam Scott and Stan Matwin.
1998.
Text classificationusing WordNet hypernyms.
In Sanda Harabagiu, ed-itor, Use of WordNet in Natural Language ProcessingSystems: Proceedings of the Conference, pages 38?44.Association for Computational Linguistics, Somerset,New Jersey.M.
Sussna.
1993.
Word sense disambiguation for free-text indexing using a massive semantic network.
InProceedings of CIKM ?93.
