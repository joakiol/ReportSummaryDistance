Proceedings of NAACL-HLT 2015, pages 111?115,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsQuestion Answering System using Multiple Information Source andOpen Type Answer MergeSeonyeong Park, Soonchoul Kwon,  Byungsoo Kim, Sangdo Han,Hyosup Shim, Gary Geunbae LeePohang University of Science and Technology, Pohang, Republic of Korea{sypark322, theincluder, bsmail90, hansd, hyosupshim, gblee} @postech.ac.krAbstractThis paper presents a multi-strategy and multi-source question answering (QA) system thatcan use multiple strategies to both answer natu-ral language (NL) questions and respond tokeywords.
We use multiple informationsources including curated knowledge base, rawtext, auto-generated triples, and NL processingresults.
We develop open semantic answer typedetector for answer merging and improve pre-vious developed single QA modules such asknowledge base based QA, information re-trieval based QA.1 IntroductionSeveral massive knowledge bases such as DBpedia(Auer et al, 2007) and Freebase (Bollacker et al,2008) have been released.
To utilize these re-sources, various approaches to question answering(QA) on linked data have been proposed (He et al,2014; Berant et al, 2013).
QA on linked data or ona knowledge base (KB) can give very high preci-sion, but because KBs consist of fragmentaryknowledge with no contextual information and ispowered by community effort, they cannot coverall information needs of users.
Furthermore, QAsystems achieve low precision when disambiguat-ing question sentences in to KB concepts; this flawreduces QAs?
performance (Yih et al, 2014).A QA system can understand a natural language(NL) question and return the answer.
In someways, perfection of QA systems is the final goal ofinformation retrieval (IR).
Early QA systems wereIR-based QAs (IRQAs).
However, as large KBssuch as DBpedia and Freebase have been con-structed, KB-based QA (KBQA) has become in-creasingly important (Lehmann et al, 2015; Ungeret al, 2012).These two kinds of QA systems use heterogene-ous data; IRQA systems search raw text, whereasKBQA systems search KB.
KBQA systems giveaccurate answers because they search from KBscurated by humans.
However, they cannot utilizeany contextual information of the answers.
Theanswers of IRQA are relatively less accurate thanthose of KBQA, but IRQA systems utilize the con-textual information of the answers.We assert that a successful QA system will re-quire appropriate cooperation between a KBQAand an IRQA.
We propose a method to merge theKBQA and the IRQA systems and to exploit theinformation in KB ontology-based open semanticanswer type to merge the answers from the twosystems, unlike previous systems that use a pre-determined answer type.
We improve our previoussystem (Park et al, 2015).Also we can answer not only complete NL sen-tence questions, and questions composed of onlykeywords, which are frequently asked in real life.We suggest strategies and methods (Figure 1) tointegrate KBQA, IRQA, and keyword QA.2 System Architecture2.1 KB-based QAA KBQA system takes an NL question sentence asthe input and retrieves its answers from the KBs.Because the KBs (i.e., the information sources),are highly structured, the KBQA system can pro-duce very pin-pointed answer sets.111We combined two approaches to make this sys-tem possible.
The first approach is based on se-mantic parsing (Berant et al, 2013), and thesecond is based on lexico-semantic pattern match-ing (Shim et al, 2014).In the semantic parsing approach, the systemfirst generates candidate segments of the questionsentence and tries to match KB vocabularies to thesegments by combining use of string-similaritybased methods and an automatically generated dic-tionary that consists of pairs of NL phrase and KBpredicate (Berant et al, 2013).
Finally the systemgenerates query candidates by applying the seg-ments to a small set of hand-crafted grammar rulesto generate a single formal meaning representation(Berant et al, 2013).In the lexico-semantic pattern approach, we usesimple patterns paired with a formal query tem-plate.
The patterns consist of regular expressionpattern that describes lexical, part-of-speech (PoS),and chunk-type patterns of a question sentence(Shim et al, 2014).
Then the templates paired withthese patterns are equipped with methods to extractinformation from the sentence and to fill the in-formation into the template.KBQA can assess the answers even when it haslittle or no additional contextual information,whereas other systems like IRQA systems can relyon the context from which it is retrieved (Schlaeferet al, 2007).
Instead, type information and its hier-archy defined in the KB are good sources of con-textual information that the KBQA can exploit.However, not all the entities defined in the KBhave specific type information; therefore, relyingonly on the type information can reduce precision(Krishnamurthy and Mitchell, 2014).When KBQA systems fail, it is usually due toincorrect disambiguation of entities, or to incorrectdisambiguation of predicate.
Both types of failuresresult in production of answers of the wrong types.For example, for a question sentence "What sportdoes the Toronto Maple Leafs play?"
evoke an-swers about the arena in which the team plays, in-stead of the sport that the team plays, when theKBQA system fails in disambiguation.2.2 IR-based QAThe system uses a multi-source tagged text data-base which is a combination of raw text, auto-generated triples, co-reference results, named enti-ty disambiguation results, the types of named enti-ties, and syntactic and semantic NLP resultsincluding semantic role label, dependency parserresults, PoS tag.
The system uses clearNLP1  forsyntactic and semantic NLP, Stanford Co-referencetool2 for co-reference tagging, Spotlight (Mendeset al, 2011) for disambiguated named entity tag-ging, and SPARQL queries (e.g.
?SELECT1 http://clearnlp.wikispaces.com/2 http://nlp.stanford.edu/Figure 1.
Proposed System Architecture112DISTINCT ?uri WHERE { res:Nicole_Kidmanrdf:type ?uri.
}?)
for tagging DBpedia ontologyclass types that correspond to entities, and triplesthat correspond to the sentence.
As a result, from asentence ?Kim was born in 1990 in Bucheon,Gyeonggi, and moved to Gunpo when she was sixyears old?, the system tags several triples such as< Kim; was born in; 1990 >, < Kim; was born in;Bucheon >, < Kim; was born in; Gyeonggi >, and< Kim; moved to; Gunpo >.Our IRQA system consists of five parts similarto the architecture of our previous system (Park etal., 2015): the first part detects the semantic answertype of the question and analyzes the question; thesecond part generates the queries; the third partretrieves passages related to the user question; thefourth part extracts answer candidates using typechecking and semantic similarity; and the last partranks the answer candidates.
The system analyzesquestions from diverse aspects: PoS tagger, de-pendency parser, semantic role labeler, our pro-posed open Information extractor, and oursemantic answer type detector.
The system ex-pands query using resources such as Wordnet3 anddictionary.The system uses Lucene4 to generate an indexand search from multi-source tagged text database.This is an efficient method to search triples andtheir corresponding sentences, instead of searchingthe raw text.
Using Lucene, the system searchesraw sentences and the auto-generated triples at thesame time, but may find different sentences due toinformation loss during extraction of triples.
Thesesentences are scored by measuring semantic simi-larity to the user query.
From these sentences, thesystem extracts the named entities and comparesthe semantic answer type of the question to thetypes of these named entities (Figure 2.).
Along-side the answer type, the system uses contextualinformation of the corresponding sentences of theanswer candidates.
By combining these two meth-ods, the system selects answer candidates.2.3 Keyword QAKeyword QA takes a keyword sequence as the in-put and returns a NL report as the answer.
The sys-tem extracts answer triples from the KB from theuser input keyword sequences.
The system uses3 http://sourceforge.net/projects/jwordnet/4 http://lucene.apache.org/core/previously generated NL templates to generate anNL report (Han et al, 2015).2.4   Open Information ExtractionDespite their enormous data capacity, KBs havelimitation in the amount of knowledge compared tothe information on the web.
To remedy this defi-ciency, we construct a repository of triples extract-ed from the web text.
We apply the technique tothe English Wikipedia5 for the demo, but the tech-nique is scalable to a web corpus such as Clue-Web 6 .
Each triple is composed of the form <argument1; relation; argument2 >, where the ar-guments are entities in the input sentence and therelation represents the relationship between thearguments.The system integrates both dependency parsetree pattern and semantic role labeler (SRL) resultsof each input sentence when extracting the triples.The dependency parse tree patterns are used togeneralize NL sentences to abstract sentence struc-tures because the system can find unimportantword tokens can be ignored in the input sentence.We define how triples should be extracted for eachdependency pattern.
If a certain dependency pat-tern is satisfied, the word tokens in the pattern con-stitute the head word of each relation and arguments in the triple.
We call these patterns ?extractiontemplates?.
Since manual construction of extractiontemplates costs too much, we automatically con-struct them by bootstrapping with seed triples ex-tracted from simple PoS tag patterns.For each sentence, the SRL annotates the predi-cate and the arguments of the predicate with theirspecific roles in the sentence.
The predicate is re-garded as relation and the arguments are regardedas argument1 and argument2, according to theirroles.
We manually define conversion rules foreach SRL result.3 Methods for Integration3.1 Detecting Keywords and SentenceOur system disambiguates whether the user inputquery is a sentence or a keyword sequence.
To dis-ambiguate a sentence, the system uses bi-gram PoS5 http://dumps.wikimedia.org/backup-index.html6 http://www.lemurproject.org/clueweb12.php/113tag features and a maximum entropy algorithm.Our dataset includes 670 keyword sequences and4521 sentences.
Based on five-fold cross validation,our system correctly detected 96.27 % of the key-word sequences and 98.12 % of the sentences.When the user query is a sentence, the query issent to the KBQA/IRQA system.
Otherwise thequery is sent to the keyword QA system.3.2 Open Semantic Answer Type detectorThe proposed system integrates the answers fromthe KB and the multi-source tagged text databaseincluding the auto-generated triple database.Therefore the KBQA and the IRQA must share ananswer-type taxonomy.
A previous answer typeclassification task used UIUC answer type includ-ing six coarse-grained answer types and 50 fine-grained answer types (Li et al, 2002).
Instead, weuse the DBpedia class type hierarchy as the opensemantic answer type set.
The proposed semanticanswer type detector involves three steps.1.
Feature Extraction: The proposed system usesthe dependency parser and PoS tagger to ex-tract the main verb and the focus.
If the ques-tion is ?Who invented Macintosh computer?,?the main verb is ?invented?
and the focus is?who?.
The answer sentence is constructed byreplacing the focus with the answer candidateand changing to declarative sentence with pe-riod, when the focus is substituted with the an-swer.
The system can detect also whether thefocus is the subject or the object of the mainverb.2.
Mapping property: The system measures thesemantic similarity between ?invented?
andDBpedia properties.
The system determinesthat the most similar DBpedia property to ?in-vented?
is ?patent?.3.
Finding semantic answer type: The system canget the type of the subject and the object of theDBpedia property ?patent?.
If the focus isthe object of the property, the semantic answertype is the type of the object of the property;otherwise it is the type of the subject of theproperty.If the system cannot find the answer type by thesesteps, the system uses an answer type classifier asin Ephyra (Schlaefer et al 2007) and uses a trans-formation table to map their answer type classes inthe UIUC answer type (Li et al, 2002) taxonomyto DBpedia class ontology.3.3 Answer Merging and Re-rankingThis integrated system gets the answer candidatesfrom both the KBQA and the IRQA.
The systemFigure 2.
Semantic answer type detector used to merging answer candidates114extracts n-best sentences including the answer can-didates from the KBQA and the keywords from theuser query.The DBpedia types of the answer candidatesfrom both the KBQA and the IRQA can be detect-ed and compared to the semantic answer type (Fig-ure 2.
).Finally, the system selects the final answer listby checking the answer types of the user query andthe semantic relatedness among the answer sen-tence substituted focus with the answer candidates,and the retrieved sentences.4 ConclusionWe have presented a QA system that uses multiplestrategies and multiple sources.
The system cananswer both complete sentences and sequences ofkeywords.
To find answers, we used both a KBand multi-source tagged text data.
This is our base-line system; we are currently using textual entail-ment technology to improve merging accuracy.AcknowledgmentsThis work was supported by the ICT R&D pro-gram of MSIP/IITP [R0101-15-0176, Develop-ment of Core Technology for Human-like Self-taught Learning based on a Symbolic Approach].ReferencesS?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
Dbpedia: A nucleus for a web of open data.Proceedings of the Sixth international The semanticweb and Second Asian conference on Asian semanticweb conference (pp.
722-735).Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic Parsing on Freebase fromQuestion-Answer Pairs.
Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing.
1533-1544.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring hu-man knowledge.
Proceedings of the 2008 SIGMODinternational conference on Management of data.1247-1250.Sangdo Han, Hyosup Shim, Byungsoo Kim, SeonyeongPark, Seonghan Ryu, and Gary Geunbae Lee.
2015.Keyword Question Answering System with ReportGeneration for Linked Data.
Proceedings of the Sec-ond International Conference on Big Data and SmartComputing.Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu, andJun Zhao.
2014.
Question Answering over LinkedData Using First-order Logic.
Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing.
1092-1103.Jayant Krishnamurthy and Tom M. Mitchell.
2014.Joint Syntactic and Semantic Parsing with Combina-tory Categorial Grammar.
Proceedings of52nd Annual Meeting of the Association for Compu-tational Linguistics.
1188-1198.Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,Dimitris Kontokostas, Pablo N. Mendes, SebastianHellmann, Mohamed Morsey, Patrick van Kleef,S?ren Auer, and Christian Bizer.
2015.
DBpedia ?
Alarge-scale, multilingual knowledge base extractedfrom Wikipedia.
Semantic Web: 6(2).
167-195.Xin Li, Dan Roth, Learning question classifiers.
2002.Proceedings of the 19th international conference onComputational linguistics-Volume 1.
1-7.Pablo N. Mendes, Max Jakob, Andr?s Garc?a-Silva ,and Christian Bizer.
2011.
DBpedia Spotlight: Shed-ding Light on the Web of Documents.
Proceedings ofthe 7th International Conference on Semantic Sys-tems.
1-8.Seonyeong Park, Hyosup Shim, Sangdo Han, ByungsooKim, and Gary Geunbae Lee.
2015.
Multi-source hy-brid Question Answering system.
Proceeding of TheSixth International Workshop on Spoken Dialog Sys-temNico Schlaefer, Jeongwoo Ko, Justin Betteridge, GuidoSautter, Manas Pathak, and Eric Nyberg.
2007.
Se-mantic Extensions of the Ephyra QA System forTREC 2007.
Proceedings of the Sixteenth Text RE-trieval Conference.Hyosup Shim, Seonyeong Park, and Gary Geunbae Lee.2014.
Assisting semantic parsing-based QA systemwith lexico-semantic pattern query template.
Pro-ceedings of Human and Cognitive Language Tech-nology.
255-258.Christina Unger, Lorenz B?hmann, Jens Lehmann, Ax-el-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-based question an-swering over RDF data.
Proceedings of the 21st in-ternational conference on World Wide Web.
639-648.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation questionanswering.
Proceedings of the 52nd Annual Meetingof the Association for Computational Linguistics.643-648.115
