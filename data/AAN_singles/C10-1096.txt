Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851?859,Beijing, August 2010Simple and Efficient Algorithmfor Approximate Dictionary MatchingNaoaki OkazakiUniversity of Tokyookazaki@is.s.u-tokyo.ac.jpJun?ichi TsujiiUniversity of TokyoUniversity of ManchesterNational Centre for Text Miningtsujii@is.s.u-tokyo.ac.jpAbstractThis paper presents a simple and effi-cient algorithm for approximate dictio-nary matching designed for similaritymeasures such as cosine, Dice, Jaccard,and overlap coefficients.
We propose thisalgorithm, called CPMerge, for the ?
-overlap join of inverted lists.
First weshow that this task is solvable exactly bya ?
-overlap join.
Given inverted lists re-trieved for a query, the algorithm collectsfewer candidate strings and prunes un-likely candidates to efficiently find stringsthat satisfy the constraint of the ?
-overlapjoin.
We conducted experiments of ap-proximate dictionary matching on threelarge-scale datasets that include personnames, biomedical names, and generalEnglish words.
The algorithm exhib-ited scalable performance on the datasets.For example, it retrieved strings in 1.1ms from the string collection of GoogleWeb1T unigrams (with cosine similarityand threshold 0.7).1 IntroductionLanguages are sufficiently flexible to be able toexpress the same meaning through different dic-tion.
At the same time, inconsistency of surfaceexpressions has persisted as a serious problem innatural language processing.
For example, in thebiomedical domain, cardiovascular disorder canbe described using various expressions: cardio-vascular diseases, cardiovascular system disor-der, and disorder of the cardiovascular system.
Itis a nontrivial task to find the entry from these sur-face expressions appearing in text.This paper addresses approximate dictionarymatching, which consists of finding all strings ina string collection V such that they have similar-ity that is no smaller than a threshold ?
with aquery string x.
This task has a broad range of ap-plications, including spelling correction, flexibledictionary look-up, record linkage, and duplicatedetection (Henzinger, 2006; Manku et al, 2007).Formally, the task obtains a subset Yx,?
?
V ,Yx,?
= {y ?
V??
sim(x, y) ?
?
}, (1)where sim(x, y) presents the similarity between xand y.
A na?
?ve solution to this task is to com-pute similarity values |V | times, i.e., between xand every string y ?
V .
However, this solutionis impractical when the number of strings |V | ishuge (e.g., more than one million).In this paper, we present a simple and effi-cient algorithm for approximate dictionary match-ing designed for similarity measures such as co-sine, Dice, Jaccard, and overlap coefficients.
Ourmain contributions are twofold.1.
We show that the problem of approximatedictionary matching is solved exactly by a?
-overlap join (Sarawagi and Kirpal, 2004)of inverted lists.
Then we present CPMerge,which is a simple and efficient algorithm forthe ?
-overlap join.
In addition, the algorithmis easily implemented.2.
We demonstrate the efficiency of the al-gorithm on three large-scale datasets withperson names, biomedical concept names,851and general English words.
We com-pare the algorithm with state-of-the-art al-gorithms, including Locality Sensitive Hash-ing (Ravichandran et al, 2005; Andoni andIndyk, 2008) and DivideSkip (Li et al,2008).
The proposed algorithm retrievesstrings the most rapidly, e.g., in 1.1 ms fromGoogle Web1T unigrams (with cosine simi-larity and threshold 0.7).2 Proposed Method2.1 Necessary and sufficient conditionsIn this paper, we assume that the features of astring are represented arbitrarily by a set.
Al-though it is important to design a string represen-tation for an accurate similarity measure, we donot address this problem: our emphasis is not ondesigning a better representation for string simi-larity but on establishing an efficient algorithm.The most popular representation is given by n-grams: all substrings of size n in a string.
Weuse trigrams throughout this paper as an exampleof string representation.
For example, the string?methyl sulphone?
is expressed by 17 elementsof letter trigrams1, {?$$m?, ?$me?, ?met?,?eth?, ?thy?, ?hyl?, ?yl ?, ?l s?, ?
su?,?sul?, ?ulp?, ?lph?, ?pho?, ?hon?,?one?, ?ne$?, ?e$$?}.
We insert two $s be-fore and after the string to denote the start or endof the string.
In general, a string x consisting of|X| letters yields (|x| + n ?
1) elements of n-grams.
We call |x| and |X| the length and size,respectively, of the string x.Let X and Y denote the feature sets of thestrings x and y, respectively.
The cosine similaritybetween the two strings x and y is,cosine(X,Y ) = |X ?
Y |?|X||Y |.
(2)By integrating this definition with Equation 1, weobtain the necessary and sufficient condition for1In practice, we attach ordinal numbers to n-grams to rep-resent multiple occurrences of n-grams in a string (Chaud-huri et al, 2006).
For example, the string ?prepress?, whichcontains two occurrences of the trigram ?pre?, yieldsthe set {?$$p?#1, ?$pr?#1, ?pre?#1, ?rep?#1,?epr?#1, ?pre?#2, ?res?#1, ?ess?#1, ?ss$?#1,?s$$?#1}.Table 1: Conditions for each similarity measureMeasure min |Y | max |Y | ?
(= min |X ?
Y |)Dice ?2??
|X| 2???
|X| 12?
(|X|+ |Y |)Jaccard ?|X| |X|/?
?
(|X|+|Y |)1+?Cosine ?2|X| |X|/?2 ?
?|X||Y |Overlap ?
?
?min{|X|, |Y |}approximate dictionary matching,??
?|X||Y |??
|X ?
Y | ?
min{|X|, |Y |}.
(3)This inequality states that two strings x and y musthave at least ?
=??
?|X||Y |?features in com-mon.
When ignoring |X?Y | in the inequality, wehave an inequality about |X| and |Y |,??2|X|??
|Y | ??
|X|?2?
(4)This inequality presents the search range for re-trieving similar strings; that is, we can ignorestrings whose feature size is out of this range.Other derivations are also applicable to similar-ity measures, including Dice, Jaccard, and overlapcoefficients.
Table 1 summarizes the conditionsfor these similarity measures.We explain one usage of these conditions.
Letquery string x = ?methyl sulphone?
and thresh-old for approximate dictionary matching ?
= 0.7with cosine similarity.
Representing the stringswith letter trigrams, we have the size of x, |X| =17.
The inequality 4 gives the search range of |Y |of the retrieved strings, 9 ?
|Y | ?
34.
Presum-ing that we are searching for strings of |Y | = 16,we obtain the necessary and sufficient conditionfor the approximate dictionary matching from theinequality 3, ?
= 12 ?
|X ?
Y |.
Thus, we needto search for strings that have at least 12 letter tri-grams that overlap with X .
When considering astring y = ?methyl sulfone?, which is a spellingvariant of y (ph ?
f), we confirm that the stringis a solution for approximate dictionary matchingbecause |X?Y | = 13 (?
?
).
Here, the actual sim-ilarity is cosine(X,Y ) = 13/?17?
16 = 0.788(?
?
).2.2 Data structure and algorithmAlgorithm 1 presents the pseudocode of the ap-proximate dictionary matching based on Table 1.852Input: V : collection of stringsInput: x: query stringInput: ?
: threshold for the similarityOutput: Y: list of strings similar to the queryX ?
string to feature(x);1Y ?
[];2for l?
min y(|X|, ?)
to max y(|X|, ?)
do3?
?
min overlap(|X|, l, ?);4R?
overlapjoin(X , ?
, V , l);5foreach r ?
R do append r to Y;6end7return Y;8Algorithm 1: Approximate dictionarymatching.Given a query string x, a collection of strings V ,and a similarity threshold ?, the algorithm com-putes the size range (line 3) given by Table 1.For each size l in the range, the algorithm com-putes the minimum number of overlaps ?
(line 4).The function overlapjoin (line 5) finds sim-ilar strings by solving the following problem (?
-overlap join): given a list of features of the querystring X and the minimum number of overlaps ?
,enumerate strings of size l in the collection V suchthat they have at least ?
feature overlaps with X .To solve this problem efficiently, we build aninverted index that stores a mapping from the fea-tures to their originating strings.
Then, we canperform the ?
-overlap join by finding strings thatappear at least ?
times in the inverted lists re-trieved for the query features X .Algorithm 2 portrays a na?
?ve solution for the?
-overlap join (AllScan algorithm).
In this algo-rithm, function get(V , l, q) returns the invertedlist of strings (of size l) for the feature q. Inshort, this algorithm scans strings in the invertedlists retrieved for the query features X , counts thefrequency of occurrences of every string in theinverted lists, and returns the strings whose fre-quency of occurrences is no smaller than ?
.This algorithm is inefficient in that it scansall strings in the inverted lists.
The number ofscanned strings is large, especially when somequery features appear frequently in the strings,e.g., ?s$$?
(words ending with ?s?)
and ?pre?
(words with substring ?pre?).
To make mattersworse, such features are too common for charac-terizing string similarity.
The AllScan algorithmInput: X: array of features of the query stringInput: ?
: minimum number of overlapsInput: V : collection of stringsInput: l: size of target stringsOutput: R: list of strings similar to the queryM ?
{};1R?
[];2foreach q ?
X do3foreach i ?
get(V , l, q) do4M [i]?M [i] + 1;5if ?
?M [i] then6append i to R;7end8end9end10return R;11Algorithm 2: AllScan algorithm.is able to maintain numerous candidate strings inM , but most candidates are not likely to qualifiedbecause they have few overlaps with X .To reduce the number of the candidate strings,we refer to signature-based algorithms (Arasu etal., 2006; Chaudhuri et al, 2006):Property 1 Let there be a set (of size h) X and aset (of any size) Y .
Consider any subset Z ?
X ofsize (h?
?
+1).
If |X ?Y | ?
?
, then Z ?Y 6= ?.We explain one usage of this property.
Let querystring x = ?methyl sulphone?
and its trigram setX be features (therefore, |X| = h = 17).
Pre-suming that we seek strings whose trigrams aresize 16 and have 12 overlaps withX , then string ymust have at least one overlap with any subset ofsize 6 (= 17 ?
12 + 1) of X .
We call the subsetsignatures.
The property leads to an algorithmicdesign by which we obtain a small set of candi-date strings from the inverted lists for signatures,(|X| ?
?
+ 1) features in X , and verify whethereach candidate string satisfies the ?
overlap withthe remaining (?
?
1) n-grams.Algorithm 3 presents the pseudocode employ-ing this idea.
In line 1, we arrange the features inX in ascending order of the number of strings intheir inverted lists.
We denote the k-th element inthe ordered features as Xk (k ?
{0, ..., |X| ?
1}),where the index number begins with 0.
Based onthis notation,X0 andX|X|?1 are the most uncom-mon and the most common features in X , respec-tively.In lines 2?7, we use (|X| ?
?
+ 1) features853Input: X: array of features of the query stringInput: ?
: minimum number of overlapsInput: V : collection of stringsInput: l: size of target stringsOutput: R: list of strings similar to the querysort elements in X by order of |get(V , l, Xk)|;1M ?
{};2for k ?
0 to (|X| ?
?)
do3foreach s ?
get(V , l, Xk) do4M [s]?M [s] + 1;5end6end7R?
[];8for k ?
(|X| ?
?
+ 1) to (|X| ?
1) do9foreach s ?M do10if bsearch(get(V , l, Xk), s) then11M [s]?M [s] + 1;12end13if ?
?M [s] then14append s to R;15remove s from M ;16else if M [s] + (|X| ?
k ?
1) < ?
then17remove s from M ;18end19end20end21return R;22Algorithm 3: CPMerge algorithm.X0, ..., X|X|??
to generate a compact set of can-didate strings.
The algorithm stores the occur-rence count of each string s in M [s].
In lines 9?21, we increment the occurrence counts if eachof X|X|?
?+1, ..., X|X|?1 inverted lists contain thecandidate strings.
For each string s in the candi-dates (line 10), we perform a binary search on theinverted list (line 11), and increment the overlapcount if the string s exists (line 12).
If the overlapcounter of the string reaches ?
(line 14), then weappend the string s to the result list R and removes from the candidate list (lines 15?16).
We prunea candidate string (lines 17?18) if the candidate isfound to be unreachable for ?
overlaps even if itappears in all of the unexamined inverted lists.3 ExperimentsWe report the experimental results of approximatedictionary matching on large-scale datasets withperson names, biomedical names, and general En-glish words.
We implemented various systems ofapproximate dictionary matching.?
Proposed: CPMerge algorithm.?
Naive: Na?
?ve algorithm that computes thecosine similarity |V | times for every query.?
AllScan: AllScan algorithm.?
Signature: CPMerge algorithm withoutpruning; this is equivalent to Algorithm 3without lines 17?18.?
DivideSkip: our implementation of the algo-rithm (Li et al, 2008)2.?
Locality Sensitive Hashing (LSH) (Andoniand Indyk, 2008): This baseline system fol-lows the design of previous work (Ravichan-dran et al, 2005).
This system approxi-mately solves Equation 1 by finding dictio-nary entries whose LSH values are withinthe (bit-wise) hamming distance of ?
fromthe LSH value of a query string.
To adaptthe method to approximate dictionary match-ing, we used a 64-bit LSH function com-puted with letter trigrams.
By design, thismethod does not find an exact solution toEquation 1; in other words, the method canmiss dictionary entries that are actually sim-ilar to the query strings.
This system hasthree parameters, ?, q (number of bit permu-tations), and B (search width), to control thetradeoff between retrieval speed and recall3.Generally speaking, increasing these param-eters improves the recall, but slows down thespeed.
We determined ?
= 24 and q = 24experimentally4, and measured the perfor-mance when B ?
{16, 32, 64}.The systems, excluding LSH, share the sameimplementation of Algorithm 1 so that we canspecifically examine the differences of the algo-rithms for ?
-overlap join.
The C++ source code ofthe system used for this experiment is available5.We ran all experiments on an application serverrunning Debian GNU/Linux 4.0 with Intel Xeon5140 CPU (2.33 GHz) and 8 GB main memory.2We tuned parameter values ?
?
{0.01, 0.02, 0.04, 0.1,0.2, 0.4, 1, 2, 4, 10, 20, 40, 100} for each dataset.
We se-lected the parameter with the fastest response.3We followed the notation of the original pa-per (Ravichandran et al, 2005) here.
Refer to the originalpaper for definitions of the parameters ?, q, and B.4q was set to 24 so that the arrays of shuffled hash valuesare stored in memory.
We chose ?
= 24 from {8, 16, 24} be-cause it showed a good balance between accuracy and speed.5http://www.chokkan.org/software/simstring/8543.1 DatasetsWe used three large datasets with person names(IMDB actors), general English words (GoogleWeb1T), and biomedical names (UMLS).?
IMDB actors: This dataset comprises actornames extracted from the IMDB database6.We used all actor names (1,098,022 strings;18 MB) from the file actors.list.gz.The average number of letter trigrams in thestrings is 17.2.
The total number of trigramsis 42,180.
The system generated index filesof 83 MB in 56.6 s.?
Google Web1T unigrams: This dataset con-sists of English word unigrams included inthe Google Web1T corpus (LDC2006T13).We used all word unigrams (13,588,391strings; 121 MB) in the corpus after remov-ing the frequency information.
The aver-age number of letter trigrams in the stringsis 10.3.
The total number of trigrams is301,459.
The system generated index filesof 601 MB in 551.7 s.?
UMLS: This dataset consists of Englishnames and descriptions of biomedical con-cepts included in the Unified Medical Lan-guage System (UMLS).
We extracted allEnglish concept names (5,216,323 strings;212 MB) from MRCONSO.RRF.aa.gz andMRCONSO.RRF.ab.gz in UMLS Release2009AA.
The average number of letter tri-grams in the strings is 43.6.
The total numberof trigrams is 171,596.
The system generatedindex files of 1.1 GB in 1216.8 s.For each dataset, we prepared 1,000 querystrings by sampling strings randomly from thedataset.
To simulate the situation where querystrings are not only identical but also similar todictionary entries, we introduced random noiseto the strings.
In this experiment, one-third ofthe query strings are unchanged from the original(sampled) strings, one-third of the query stringshave one letter changed, and one-third of thequery strings have two letters changed.
Whenchanging a letter, we randomly chose a letter po-sition from a uniform distribution, and replaced6ftp://ftp.fu-berlin.de/misc/movies/database/the letter at the position with an ASCII letter ran-domly chosen from a uniform distribution.3.2 ResultsTo examine the scalability of each system, wecontrolled the number of strings to be indexedfrom 10%?100%, and issued 1,000 queries.
Fig-ure 1 portrays the average response time for re-trieving strings whose cosine similarity values areno smaller than 0.7.
Although LSH (B=16) seemsto be the fastest in the graph, this system missedmany true positives7; the recall scores of approx-imate dictionary matching were 15.4% (IMDB),13.7% (Web1T), and 1.5% (UMLS).
Increasingthe parameterB improves the recall at the expenseof the response time.
LSH (B=64)8.
It not onlyran slower than the proposed method, but alsosuffered from low recall scores, 25.8% (IMDB),18.7% (Web1T), and 7.1% (UMLS).
LSH wasuseful only when we required a quick responsemuch more than recall.The other systems were guaranteed to findthe exact solution (100% recall).
The proposedalgorithm was the fastest of all exact systemson all datasets: the response times per query(100% index size) were 1.07 ms (IMDB), 1.10 ms(Web1T), and 20.37 ms (UMLS).
The responsetimes of the Na?
?ve algorithm were too slow, 32.8 s(IMDB), 236.5 s (Web1T), and 416.3 s (UMLS).The proposed algorithm achieved substantialimprovements over the AllScan algorithm: theproposed method was 65.3 times (IMDB), 227.5times (Web1T), and 13.7 times (UMLS) fasterthan the Na?
?ve algorithm.
We observed that theSignature algorithm, which is Algorithm 3 with-out lines 17?18, did not perform well: The Sig-nature algorithm was 1.8 times slower (IMDB),2.1 times faster (Web1T), and 135.0 times slower(UMLS) than the AllScan algorithm.
These re-sults indicate that it is imperative to minimize thenumber of candidates to reduce the number ofbinary-search operations.
The proposed algorithmwas 11.1?13.4 times faster than DivideSkip.Figure 2 presents the average response time7Solving Equation 1, all systems are expected to retrievethe exact set of strings retrieved by the Na?
?ve algorithm.8The response time of LSH (B=64) on the IMDB datasetwas 29.72 ms (100% index size).85505101520250 20 40 60 80 100Averageresponseperquery[ms]Number of indexed strings (%)ProposedAllScanSignatureDivideSkipLSH (B=16)LSH (B=32)010203040500 20 40 60 80 100Averageresponseperquery[ms]Number of indexed strings (%)ProposedAllScanSignatureDivideSkipLSH (B=16)LSH (B=32)LSH (B=64)01020304050600 20 40 60 80 100Averageresponseperquery[ms]Number of indexed strings (%)(a) IMDB actors (b) Google Web1T unigrams (c) UMLSProposedAllScanSignatureDivideSkipLSH (B=16)LSH (B=32)LSH (B=64)Figure 1: Average response time for processing a query (cosine similarity; ?
= 0.7).0510152025300.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Averageresponseperquery[ms]Similarity thresholdDiceJaccardCosineOverlap0102030405060700.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Averageresponseperquery[ms]Similarity thresholdDiceJaccardCosineOverlap0501001502002503003504000.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Averageresponseperquery[ms]Similarity thresholdDiceJaccardCosineOverlap(a) IMDB actors (b) Google Web1T unigram (c) UMLSFigure 2: Average response time for processing a query.of the proposed algorithm for different similaritymeasures and threshold values.
When the similar-ity threshold is lowered, the algorithm runs slowerbecause the number of retrieved strings |Y| in-creases exponentially.
The Dice coefficient andcosine similarity produced similar curves.Table 2 summarizes the run-time statistics ofthe proposed method for each dataset (with co-sine similarity and threshold 0.7).
Using theIMDB dataset, the proposed method searched forstrings whose size was between 8.74 and 34.06;it retrieved 4.63 strings per query string.
Theproposed algorithm scanned 279.7 strings in 4.6inverted lists to obtain 232.5 candidate strings.The algorithm performed a binary search on 4.3inverted lists containing 7,561.8 strings in all.In contrast, the AllScan algorithm had to scan16,155.1 strings in 17.7 inverted lists and con-sidered 9,788.7 candidate strings, and found only4.63 similar strings.This table clearly demonstrates three key con-tributions of the proposed algorithm for efficientapproximate dictionary matching.
First, the pro-posed algorithm scanned far fewer strings than didthe AllScan algorithm.
For example, to obtaincandidate strings in the IMDB dataset, the pro-posed algorithm scanned 279.7 strings, whereasthe AllScan algorithm scanned 16,155.1 strings.Therefore, the algorithm examined only 1.1%?3.5% of the strings in the entire inverted lists inthe three datasets.
Second, the proposed algo-rithm considered far fewer candidates than didthe AllScan algorithm: the number of candidatestrings considered by the algorithm was 1.2%?6.6% of those considered by the AllScan algo-rithm.
Finally, the proposed algorithm read fewerinverted lists than did the AllScan algorithm.
Theproposed algorithm actually read 8.9 (IMDB), 6.0(Web1T), and 31.7 (UMLS) inverted lists duringthe experiments9.
These values indicate that theproposed algorithm can solve ?
-overlap join prob-lems by checking only 50.3% (IMDB), 53.6%(Web1T), and 51.9% of the total inverted lists re-9These values are 4.6 + 4.3, 3.1 + 2.9, and 14.3 + 17.4.856Table 2: Run-time statistics of the proposed algorithm for each datasetAveraged item IMDB Web1T UMLS Descriptionmin |y| 8.74 5.35 21.87 minimum size of trigrams of target stringsmax |y| 34.06 20.46 88.48 maximum size of trigrams of target strings?
14.13 9.09 47.77 minimum number of overlaps required/sufficient per query|Y| 4.63 3.22 111.79 number of retrieved strings per queryTotal ?
averaged for each query and target size:# inverted lists 17.7 11.2 61.1 number of inverted lists retrieved for a query# strings 16 155.1 52 557.6 49 561.4 number of strings in the inverted list# unique strings 9 788.7 44 834.6 17 457.5 number of unique strings in the inverted listCandidate stage ?
averaged for each query and target size:# inverted lists 4.6 3.1 14.3 number of inverted lists scanned for generating candidates# strings 279.7 552.7 1 756.3 number of strings scanned for generating candidates# candidates 232.5 523.7 1 149.7 number of candidates generated for a queryValidation stage ?
averaged for each query and target size:# inverted lists 4.3 2.9 17.4 number of inverted lists examined by binary search for a query# strings 7 561.8 19 843.6 20 443.7 number of strings targeted by binary searchtrieved for queries.4 Related WorkNumerous studies have addressed approximatedictionary matching.
The most popular configu-ration uses n-grams as a string representation andthe edit distance as a similarity measure.
Gra-vano et al (1998; 2001) presented various filter-ing strategies, e.g., count filtering, position fil-tering, and length filtering, to reduce the num-ber of candidates.
Kim et al (2005) proposedtwo-level n-gram inverted indices (n-Gram/2L) toeliminate the redundancy of position informationin n-gram indices.
Li et al (2007) explored theuse of variable-length grams (VGRAMs) for im-proving the query performance.
Lee et al (2007)extended n-grams to include wild cards and de-veloped algorithms based on a replacement semi-lattice.
Xiao et al (2008) proposed the Ed-Joinalgorithm, which utilizes mismatching n-grams.Several studies addressed different paradigmsfor approximate dictionary matching.
Bocek etal.
(2007) presented the Fast Similarity Search(FastSS), an enhancement of the neighborhoodgeneration algorithms, in which multiple variantsof each string record are stored in a database.Wang et al (2009) further improved the techniqueof neighborhood generation by introducing parti-tioning and prefix pruning.
Huynh et al (2006)developed a solution to the k-mismatch problemin compressed suffix arrays.
Liu et al (2008)stored string records in a trie, and proposed aframework called TITAN.
These studies are spe-cialized for the edit distance measure.A few studies addressed approximate dictio-nary matching for similarity measures such ascosine and Jaccard similarities.
Chaudhuri etal.
(2006) proposed the SSJoin operator for sim-ilarity joins with several measures including theedit distance and Jaccard similarity.
This algo-rithm first generates signatures for strings, findsall pairs of strings whose signatures overlap,and finally outputs the subset of these candi-date pairs that satisfy the similarity predicate.Arasu et al (2006) addressed signature schemes,i.e., methodologies for obtaining signatures fromstrings.
They also presented an implementation ofthe SSJoin operator in SQL.
Although we did notimplement this algorithm in SQL, it is equivalentto the Signature algorithm in Section 3.Sarawagi and Kirpal (2004) proposed the Mer-geOpt algorithm for the ?
-overlap join to approx-imate string matching with overlap, Jaccard, andcosine measures.
This algorithm splits invertedlists for a given query A into two groups, S andL, maintains a heap to collect candidate strings onS, and performs a binary search on L to verify thecondition of the ?
-overlap join for each candidatestring.
Their subsequent work includes an effi-cient algorithm for the top-k search of the overlapjoin (Chandel et al, 2006).Li et al (2008) extended this algorithm to theSkipMerge and DivideSkip algorithms.
The Skip-Merge algorithm uses a heap to compute the ?
-overlap join on entire inverted lists A, but hasan additional mechanism to increment the fron-857tier pointers of inverted lists efficiently based onthe strings popped most recently from the heap.Consequently, SkipMerge can reduce the numberof strings that are pushed to the heap.
Similarlyto the MergeOpt algorithm, DivideSkip splits in-verted lists A into two groups S and L, but it ap-plies SkipMerge to S. In Section 3, we reportedthe performance of DivideSkip.Charikar (2002) presented the Locality Sen-sitive Hash (LSH) function (Andoni and Indyk,2008), which preserves the property of cosinesimilarity.
The essence of this function is to mapstrings into N -bit hash values where the bitwisehamming distance between the hash values of twostrings approximately corresponds to the angle ofthe two strings.
Ravichandran et al (2005) ap-plied LSH to the task of noun clustering.
Adaptingthis algorithm to approximate dictionary match-ing, we discussed its performance in Section 3.Several researchers have presented refined sim-ilarity measures for strings (Winkler, 1999; Cohenet al, 2003; Bergsma and Kondrak, 2007; Davis etal., 2007).
Although these studies are sometimesregarded as a research topic of approximate dic-tionary matching, they assume that two strings forthe target of similarity computation are given; inother words, it is out of their scope to find stringsin a large collection that are similar to a givenstring.
Thus, it is a reasonable approach for an ap-proximate dictionary matching to quickly collectcandidate strings with a loose similarity threshold,and for a refined similarity measure to scrutinizeeach candidate string for the target application.5 ConclusionsWe present a simple and efficient algorithm forapproximate dictionary matching with the co-sine, Dice, Jaccard, and overlap measures.
Weconducted experiments of approximate dictio-nary matching on large-scale datasets with personnames, biomedical names, and general Englishwords.
Even though the algorithm is very sim-ple, our experimental results showed that the pro-posed algorithm executed very quickly.
We alsoconfirmed that the proposed method drastically re-duced the number of candidate strings consideredduring approximate dictionary matching.
We be-lieve that this study will advance practical NLPapplications for which the execution time of ap-proximate dictionary matching is critical.An advantage of the proposed algorithm overexisting algorithms (e.g., MergeSkip) is that itdoes not need to read all the inverted lists retrievedby query n-grams.
We observed that the proposedalgorithm solved ?
-overlap joins by checking ap-proximately half of the inverted lists (with cosinesimilarity and threshold ?
= 0.7).
This charac-teristic is well suited to processing compressedinverted lists because the algorithm needs to de-compress only half of the inverted lists.
It is nat-ural to extend this study to compressing and de-compressing inverted lists for reducing disk spaceand for improving query performance (Behm etal., 2009).AcknowledgmentsThis work was partially supported by Grants-in-Aid for Scientific Research on Priority Areas(MEXT, Japan) and for Solution-Oriented Re-search for Science and Technology (JST, Japan).ReferencesAndoni, Alexandr and Piotr Indyk.
2008.
Near-optimal hashing algorithms for approximate nearestneighbor in high dimensions.
Communications ofthe ACM, 51(1):117?122.Arasu, Arvind, Venkatesh Ganti, and Raghav Kaushik.2006.
Efficient exact set-similarity joins.
In VLDB?06: Proceedings of the 32nd International Confer-ence on Very Large Data Bases, pages 918?929.Behm, Alexander, Shengyue Ji, Chen Li, and JiahengLu.
2009.
Space-constrained gram-based indexingfor efficient approximate string search.
In ICDE?09: Proceedings of the 2009 IEEE InternationalConference on Data Engineering, pages 604?615.Bergsma, Shane and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InACL ?07: Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 656?663.Bocek, Thomas, Ela Hunt, and Burkhard Stiller.
2007.Fast similarity search in large dictionaries.
Tech-nical Report ifi-2007.02, Department of Informatics(IFI), University of Zurich.858Chandel, Amit, P. C. Nagesh, and Sunita Sarawagi.2006.
Efficient batch top-k search for dictionary-based entity recognition.
In ICDE ?06: Proceed-ings of the 22nd International Conference on DataEngineering.Charikar, Moses S. 2002.
Similarity estimation tech-niques from rounding algorithms.
In STOC ?02:Proceedings of the thiry-fourth annual ACM sym-posium on Theory of computing, pages 380?388.Chaudhuri, Surajit, Venkatesh Ganti, and RaghavKaushik.
2006.
A primitive operator for similar-ity joins in data cleaning.
In ICDE ?06: Proceed-ings of the 22nd International Conference on DataEngineering.Cohen, William W., Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A comparison ofstring distance metrics for name-matching tasks.In Proceedings of the IJCAI-2003 Workshop onInformation Integration on the Web (IIWeb-03),pages 73?78.Davis, Jason V., Brian Kulis, Prateek Jain, Suvrit Sra,and Inderjit S. Dhillon.
2007.
Information-theoreticmetric learning.
In ICML ?07: Proceedings of the24th International Conference on Machine Learn-ing, pages 209?216.Gravano, Luis, Panagiotis G. Ipeirotis, H. V. Jagadish,Nick Koudas, S. Muthukrishnan, and Divesh Srivas-tava.
2001.
Approximate string joins in a database(almost) for free.
In VLDB ?01: Proceedings of the27th International Conference on Very Large DataBases, pages 491?500.Henzinger, Monika.
2006.
Finding near-duplicateweb pages: a large-scale evaluation of algorithms.In SIGIR ?06: Proceedings of the 29th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 284?291.Huynh, Trinh N. D., Wing-Kai Hon, Tak-Wah Lam,and Wing-Kin Sung.
2006.
Approximate stringmatching using compressed suffix arrays.
Theoreti-cal Computer Science, 352(1-3):240?249.Kim, Min-Soo, Kyu-Young Whang, Jae-Gil Lee, andMin-Jae Lee.
2005. n-Gram/2L: a space and timeefficient two-level n-gram inverted index structure.In VLDB ?05: Proceedings of the 31st InternationalConference on Very Large Data Bases, pages 325?336.Lee, Hongrae, Raymond T. Ng, and Kyuseok Shim.2007.
Extending q-grams to estimate selectivity ofstring matching with low edit distance.
In VLDB?07: Proceedings of the 33rd International Confer-ence on Very Large Data Bases, pages 195?206.Li, Chen, Bin Wang, and Xiaochun Yang.
2007.Vgram: improving performance of approximatequeries on string collections using variable-lengthgrams.
In VLDB ?07: Proceedings of the 33rd In-ternational Conference on Very Large Data Bases,pages 303?314.Li, Chen, Jiaheng Lu, and Yiming Lu.
2008.
Effi-cient merging and filtering algorithms for approx-imate string searches.
In ICDE ?08: Proceedingsof the 2008 IEEE 24th International Conference onData Engineering, pages 257?266.Liu, Xuhui, Guoliang Li, Jianhua Feng, and LizhuZhou.
2008.
Effective indices for efficient approxi-mate string search and similarity join.
In WAIM ?08:Proceedings of the 2008 The Ninth InternationalConference on Web-Age Information Management,pages 127?134.Manku, Gurmeet Singh, Arvind Jain, and AnishDas Sarma.
2007.
Detecting near-duplicates forweb crawling.
In WWW ?07: Proceedings of the16th International Conference on World Wide Web,pages 141?150.Navarro, Gonzalo and Ricardo Baeza-Yates.
1998.
Apractical q-gram index for text retrieval allowing er-rors.
CLEI Electronic Journal, 1(2).Ravichandran, Deepak, Patrick Pantel, and EduardHovy.
2005.
Randomized algorithms and nlp: us-ing locality sensitive hash function for high speednoun clustering.
In ACL ?05: Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 622?629.Sarawagi, Sunita and Alok Kirpal.
2004.
Efficientset joins on similarity predicates.
In SIGMOD ?04:Proceedings of the 2004 ACM SIGMOD interna-tional conference on Management of data, pages743?754.Wang, Wei, Chuan Xiao, Xuemin Lin, and ChengqiZhang.
2009.
Efficient approximate entity extrac-tion with edit distance constraints.
In SIGMOD?09: Proceedings of the 35th SIGMOD Interna-tional Conference on Management of Data, pages759?770.Winkler, William E. 1999.
The state of record link-age and current research problems.
Technical Re-port R99/04, Statistics of Income Division, InternalRevenue Service Publication.Xiao, Chuan, Wei Wang, and Xuemin Lin.
2008.
Ed-Join: an efficient algorithm for similarity joins withedit distance constraints.
In VLDB ?08: Proceed-ings of the 34th International Conference on VeryLarge Data Bases, pages 933?944.859
