Proceedings of ACL-08: HLT, pages 317?325,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproving Parsing and PP attachment Performance with Sense InformationEneko AgirreIXA NLP GroupUniversity of the Basque CountryDonostia, Basque Countrye.agirre@ehu.esTimothy BaldwinLT Group, CSSEUniversity of MelbourneVictoria 3010 Australiatim@csse.unimelb.edu.auDavid MartinezLT Group, CSSEUniversity of MelbourneVictoria 3010 Australiadavidm@csse.unimelb.edu.auAbstractTo date, parsers have made limited use of se-mantic information, but there is evidence tosuggest that semantic features can enhanceparse disambiguation.
This paper shows thatsemantic classes help to obtain significant im-provement in both parsing and PP attachmenttasks.
We devise a gold-standard sense- andparse tree-annotated dataset based on the in-tersection of the Penn Treebank and SemCor,and experiment with different approaches toboth semantic representation and disambigua-tion.
For the Bikel parser, we achieved amaximal error reduction rate over the base-line parser of 6.9% and 20.5%, for parsing andPP-attachment respectively, using an unsuper-vised WSD strategy.
This demonstrates thatword sense information can indeed enhancethe performance of syntactic disambiguation.1 IntroductionTraditionally, parse disambiguation has relied onstructural features extracted from syntactic parsetrees, and made only limited use of semantic in-formation.
There is both empirical evidence andlinguistic intuition to indicate that semantic fea-tures can enhance parse disambiguation perfor-mance, however.
For example, a number of differentparsers have been shown to benefit from lexicalisa-tion, that is, the conditioning of structural featureson the lexical head of the given constituent (Mager-man, 1995; Collins, 1996; Charniak, 1997; Char-niak, 2000; Collins, 2003).
As an example of lexi-calisation, we may observe in our training data thatknife often occurs as the manner adjunct of open inprepositional phrases headed by with (c.f.
open witha knife), which would provide strong evidence forwith (a) knife attaching to open and not box in openthe box with a knife.
It would not, however, pro-vide any insight into the correct attachment of withscissors in open the box with scissors, as the disam-biguation model would not be able to predict thatknife and scissors are semantically similar and thuslikely to have the same attachment preferences.In order to deal with this limitation, we propose tointegrate directly the semantic classes of words intothe process of training the parser.
This is done bysubstituting the original words with semantic codesthat reflect semantic classes.
For example, in theabove example we could substitute both knife andscissors with the semantic class TOOL, thus relatingthe training and test instances directly.
We exploreseveral models for semantic representation, basedaround WordNet (Fellbaum, 1998).Our approach to exploring the impact of lexicalsemantics on parsing performance is to take twostate-of-the-art statistical treebank parsers and pre-process the inputs variously.
This simple methodallows us to incorporate semantic information intothe parser without having to reimplement a full sta-tistical parser, and also allows for maximum compa-rability with existing results in the treebank parsingcommunity.
We test the parsers over both a PP at-tachment and full parsing task.In experimenting with different semantic repre-sentations, we require some strategy to disambiguatethe semantic class of polysemous words in context(e.g.
determining for each instance of crane whetherit refers to an animal or a lifting device).
We explorea number of disambiguation strategies, including theuse of hand-annotated (gold-standard) senses, the317use of the most frequent sense, and an unsupervisedword sense disambiguation (WSD) system.This paper shows that semantic classes help toobtain significant improvements for both PP attach-ment and parsing.
We attain a 20.5% error reductionfor PP attachment, and 6.9% for parsing.
These re-sults are achieved using most frequent sense infor-mation, which surprisingly outperforms both gold-standard senses and automatic WSD.The results are notable in demonstrating that verysimple preprocessing of the parser input facilitatessignificant improvements in parser performance.
Weprovide the first definitive results that word senseinformation can enhance Penn Treebank parser per-formance, building on earlier results of Bikel (2000)and Xiong et al (2005).
Given our simple procedurefor incorporating lexical semantics into the parsingprocess, our hope is that this research will open thedoor to further gains using more sophisticated pars-ing models and richer semantic options.2 BackgroundThis research is focused on applying lexical seman-tics in parsing and PP attachment tasks.
Below, weoutline these tasks.ParsingAs our baseline parsers, we use two state-of-the-art lexicalised parsing models, namely the Bikelparser (Bikel, 2004) and Charniak parser (Charniak,2000).
While a detailed description of the respectiveparsing models is beyond the scope of this paper, itis worth noting that both parsers induce a contextfree grammar as well as a generative parsing modelfrom a training set of parse trees, and use a devel-opment set to tune internal parameters.
Tradition-ally, the two parsers have been trained and evaluatedover the WSJ portion of the Penn Treebank (PTB:Marcus et al (1993)).
We diverge from this norm infocusing exclusively on a sense-annotated subset ofthe Brown Corpus portion of the Penn Treebank, inorder to investigate the upper bound performance ofthe models given gold-standard sense information.PP attachment in a parsing contextPrepositional phrase attachment (PP attachment)is the problem of determining the correct attachmentsite for a PP, conventionally in the form of the nounor verb in a V NP PP structure (Ratnaparkhi et al,1994; Mitchell, 2004).
For instance, in I ate a pizzawith anchovies, the PP with anchovies could attacheither to the verb (c.f.
ate with anchovies) or to thenoun (c.f.
pizza with anchovies), of which the nounis the correct attachment site.
With I ate a pizza withfriends, on the other hand, the verb is the correct at-tachment site.
PP attachment is a structural ambigu-ity problem, and as such, a subproblem of parsing.Traditionally the so-called RRR data (Ratna-parkhi et al, 1994) has been used to evaluate PPattachment algorithms.
RRR consists of 20,081training and 3,097 test quadruples of the form(v,n1,p,n2), where the attachment decision iseither v or n1.
The best published results over RRRare those of Stetina and Nagao (1997), who em-ploy WordNet sense predictions from an unsuper-vised WSD method within a decision tree classifier.Their work is particularly inspiring in that it signifi-cantly outperformed the plethora of lexicalised prob-abilistic models that had been proposed to that point,and has not been beaten in later attempts.In a recent paper, Atterer and Schu?tze (2007) crit-icised the RRR dataset because it assumes that anoracle parser provides the two hypothesised struc-tures to choose between.
This is needed to derive thefact that there are two possible attachment sites, aswell as information about the lexical phrases, whichare typically extracted heuristically from gold stan-dard parses.
Atterer and Schu?tze argue that the onlymeaningful setting for PP attachment is within aparser, and go on to demonstrate that in a parser set-ting, the Bikel parser is competitive with the best-performing dedicated PP attachment methods.
Anyimprovement in PP attachment performance over thebaseline Bikel parser thus represents an advance-ment in state-of-the-art performance.That we specifically present results for PP attach-ment in a parsing context is a combination of us sup-porting the new research direction for PP attachmentestablished by Atterer and Schu?tze, and us wishingto reinforce the findings of Stetina and Nagao thatword sense information significantly enhances PPattachment performance in this new setting.Lexical semantics in parsingThere have been a number of attempts to incorpo-rate word sense information into parsing tasks.
The318most closely related research is that of Bikel (2000),who merged the Brown portion of the Penn Tree-bank with SemCor (similarly to our approach in Sec-tion 4.1), and used this as the basis for evaluation ofa generative bilexical model for joint WSD and pars-ing.
He evaluated his proposed model in a parsingcontext both with and without WordNet-based senseinformation, and found that the introduction of senseinformation either had no impact or degraded parseperformance.The only successful applications of word sense in-formation to parsing that we are aware of are Xionget al (2005) and Fujita et al (2007).
Xiong et al(2005) experimented with first-sense and hypernymfeatures from HowNet and CiLin (both WordNetsfor Chinese) in a generative parse model appliedto the Chinese Penn Treebank.
The combinationof word sense and first-level hypernyms produceda significant improvement over their basic model.Fujita et al (2007) extended this work in imple-menting a discriminative parse selection model in-corporating word sense information mapped ontoupper-level ontologies of differing depths.
Basedon gold-standard sense information, they achievedlarge-scale improvements over a basic parse selec-tion model in the context of the Hinoki treebank.Other notable examples of the successful incorpo-ration of lexical semantics into parsing, not throughword sense information but indirectly via selectionalpreferences, are Dowding et al (1994) and Hektoen(1997).
For a broader review of WSD in NLP appli-cations, see Resnik (2006).3 Integrating Semantics into ParsingOur approach to providing the parsers with senseinformation is to make available the semantic de-notation of each word in the form of a semanticclass.
This is done simply by substituting the origi-nal words with semantic codes.
For example, in theearlier example of open with a knife we could sub-stitute both knife and scissors with the class TOOL,and thus directly facilitate semantic generalisationwithin the parser.
There are three main aspects thatwe have to consider in this process: (i) the seman-tic representation, (ii) semantic disambiguation, and(iii) morphology.There are many ways to represent semantic re-lationships between words.
In this research weopt for a class-based representation that will mapsemantically-related words into a common semanticcategory.
Our choice for this work was the WordNet2.1 lexical database, in which synonyms are groupedinto synsets, which are then linked via an IS-A hi-erarchy.
WordNet contains other types of relationssuch as meronymy, but we did not use them in thisresearch.
With any lexical semantic resource, wehave to be careful to choose the appropriate level ofgranularity for a given task: if we limit ourselves tosynsets we will not be able to capture broader gen-eralisations, such as the one between knife and scis-sors;1 on the other hand by grouping words related ata higher level in the hierarchy we could find that wemake overly coarse groupings (e.g.
mallet, squareand steel-wool pad are also descendants of TOOL inWordNet, none of which would conventionally beused as the manner adjunct of cut).
We will test dif-ferent levels of granularity in this work.The second problem we face is semantic disam-biguation.
The more fine-grained our semantic rep-resentation, the higher the average polysemy and thegreater the need to distinguish between these senses.For instance, if we find the word crane in a con-text such as demolish a house with the crane, theability to discern that this corresponds to the DE-VICE and not ANIMAL sense of word will allow usto avoid erroneous generalisations.
This problem ofidentifying the correct sense of a word in context isknown as word sense disambiguation (WSD: Agirreand Edmonds (2006)).
Disambiguating each wordrelative to its context of use becomes increasinglydifficult for fine-grained representations (Palmer etal., 2006).
We experiment with different ways oftackling WSD, using both gold-standard data andautomatic methods.Finally, when substituting words with semantictags we have to decide how to treat different wordforms of a given lemma.
In the case of English, thispertains most notably to verb inflection and nounnumber, a distinction which we lose if we opt tomap all word forms onto semantic classes.
For ourcurrent purposes we choose to substitute all word1In WordNet 2.1, knife and scissors are sister synsets, bothof which have TOOL as their 4th hypernym.
Only by mappingthem onto their 1st hypernym or higher would we be able tocapture the semantic generalisation alluded to above.319forms, but we plan to look at alternative represen-tations in the future.4 Experimental settingWe evaluate the performance of our approach in twosettings: (1) full parsing, and (2) PP attachmentwithin a full parsing context.
Below, we outline thedataset used in this research and the parser evalu-ation methodology, explain the methodology usedto perform PP attachment, present the different op-tions for semantic representation, and finally detailthe disambiguation methods.4.1 Dataset and parser evaluationOne of the main requirements for our dataset is theavailability of gold-standard sense and parse tree an-notations.
The gold-standard sense annotations al-low us to perform upper bound evaluation of the rel-ative impact of a given semantic representation onparsing and PP attachment performance, to contrastwith the performance in more realistic semantic dis-ambiguation settings.
The gold-standard parse treeannotations are required in order to carry out evalu-ation of parser and PP attachment performance.The only publicly-available resource with thesetwo characteristics at the time of this work was thesubset of the Brown Corpus that is included in bothSemCor (Landes et al, 1998) and the Penn Tree-bank (PTB).2 This provided the basis of our dataset.After sentence- and word-aligning the SemCor andPTB data (discarding sentences where there was adifference in tokenisation), we were left with a totalof 8,669 sentences containing 151,928 words.
Notethat this dataset is smaller than the one described byBikel (2000) in a similar exercise, the reason beingour simple and conservative approach taken whenmerging the resources.We relied on this dataset alne for all the exper-iments in this paper.
In order to maximise repro-ducibility and encourage further experimentation inthe direction pioneered in this research, we parti-tioned the data into 3 sets: 80% training, 10% devel-opment and 10% test data.
This dataset is availableon request to the research community.2OntoNotes (Hovy et al, 2006) includes large-scale tree-bank and (selective) sense data, which we plan to use for futureexperiments when it becomes fully available.We evaluate the parsers via labelled bracketing re-call (R), precision (P) and F-score (F1).
We useBikel?s randomized parsing evaluation comparator3(with p < 0.05 throughout) to test the statistical sig-nificance of the results using word sense informa-tion, relative to the respective baseline parser usingonly lexical features.4.2 PP attachment taskFollowing Atterer and Schu?tze (2007), we wrotea script that, given a parse tree, identifies in-stances of PP attachment ambiguity and outputs the(v,n1,p,n2) quadruple involved and the attach-ment decision.
This extraction system uses Collins?rules (based on TREEP (Chiang and Bikel, 2002))to locate the heads of phrases.
Over the combinedgold-standard parsing dataset, our script extracted atotal of 2,541 PP attachment quadruples.
As withthe parsing data, we partitioned the data into 3 sets:80% training, 10% development and 10% test data.Once again, this dataset and the script used to ex-tract the quadruples are available on request to theresearch community.In order to evaluate the PP attachment perfor-mance of a parser, we run our extraction script overthe parser output in the same manner as for the gold-standard data, and compare the extracted quadru-ples to the gold-standard ones.
Note that there isno guarantee of agreement in the quadruple mem-bership between the extraction script and the goldstandard, as the parser may have produced a parsewhich is incompatible with either attachment possi-bility.
A quadruple is deemed correct if: (1) it existsin the gold standard, and (2) the attachment deci-sion is correct.
Conversely, it is deemed incorrect if:(1) it exists in the gold standard, and (2) the attach-ment decision is incorrect.
Quadruples not found inthe gold standard are discarded.
Precision was mea-sured as the number of correct quadruples divided bythe total number of correct and incorrect quadruples(i.e.
all quadruples which are not discarded), and re-call as the number of correct quadruples divided bythe total number of gold-standard quadruples in thetest set.
This evaluation methodology coincides withthat of Atterer and Schu?tze (2007).Statistical significance was calculated based on3www.cis.upenn.edu/?dbikel/software.html320a modified version of the Bikel comparator (seeabove), once again with p < 0.05.4.3 Semantic representationWe experimented with a range of semantic represen-tations, all of which are based on WordNet 2.1.
Asmentioned above, words in WordNet are organisedinto sets of synonyms, called synsets.
Each synsetin turn belongs to a unique semantic file (SF).
Thereare a total of 45 SFs (1 for adverbs, 3 for adjectives,15 for verbs, and 26 for nouns), based on syntacticand semantic categories.
A selection of SFs is pre-sented in Table 1 for illustration purposes.We experiment with both full synsets and SFs asinstances of fine-grained and coarse-grained seman-tic representation, respectively.
As an example ofthe difference in these two representations, knife inits tool sense is in the EDGE TOOL USED AS A CUT-TING INSTRUMENT singleton synset, and also in theARTIFACT SF along with thousands of other wordsincluding cutter.
Note that these are the two ex-tremes of semantic granularity in WordNet, and weplan to experiment with intermediate representationlevels in future research (c.f.
Li and Abe (1998), Mc-Carthy and Carroll (2003), Xiong et al (2005), Fu-jita et al (2007)).As a hybrid representation, we tested the effectof merging words with their corresponding SF (e.g.knife+ARTIFACT ).
This is a form of semantic spe-cialisation rather than generalisation, and allows theparser to discriminate between the different sensesof each word, but not generalise across words.For each of these three semantic representations,we experimented with substituting each of: (1) allopen-class POSs (nouns, verbs, adjectives and ad-verbs), (2) nouns only, and (3) verbs only.
There arethus a total of 9 combinations of representation typeand target POS.4.4 Disambiguation methodsFor a given semantic representation, we need someform of WSD to determine the semantics of eachtoken occurrence of a target word.
We experimentedwith three options:1.
Gold-standard: Gold-standard annotationsfrom SemCor.
This gives us the upper boundperformance of the semantic representation.SF ID DEFINITIONadj.all all adjective clustersadj.pert relational adjectives (pertainyms)adj.ppl participial adjectivesadv.all all adverbsnoun.act nouns denoting acts or actionsnoun.animal nouns denoting animalsnoun.artifact nouns denoting man-made objects...verb.consumption verbs of eating and drinkingverb.emotion verbs of feelingverb.perception verbs of seeing, hearing, feeling...Table 1: A selection of WordNet SFs2.
First Sense (1ST): All token instances of agiven word are tagged with their most fre-quent sense in WordNet.4 Note that the firstsense predictions are based largely on the samedataset as we use in our evaluation, such thatthe predictions are tuned to our dataset and notfully unsupervised.3.
Automatic Sense Ranking (ASR): First sensetagging as for First Sense above, except that anunsupervised system is used to automaticallypredict the most frequent sense for each wordbased on an independent corpus.
The methodwe use to predict the first sense is that of Mc-Carthy et al (2004), which was obtained us-ing a thesaurus automatically created from theBritish National Corpus (BNC) applying themethod of Lin (1998), coupled with WordNet-based similarity measures.
This method is fullyunsupervised and completely unreliant on anyannotations from our dataset.In the case of SFs, we perform full synset WSDbased on one of the above options, and then map theprediction onto the corresponding (unique) SF.5 ResultsWe present the results for each disambiguation ap-proach in turn, analysing the results for parsing andPP attachment separately.4There are some differences with the most frequent sense inSemCor, due to extra corpora used in WordNet development,and also changes in WordNet from the original version used forthe SemCor tagging.321CHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .857 .808 .832 .837 .845 .841SF .855 .809 .831 .847?
.854?
.850?SFn .860 .808 .833 .847?
.853?
.850?SFv .861 .811 .835 .847?
.856?
.851?word + SF .865?
.814?
.839?
.837 .846 .842word + SFn .862 .809 .835 .841?
.850?
.846?word + SFv .862 .810 .835 .840 .851 .845Syn .863?
.812 .837 .845?
.853?
.849?Synn .860 .807 .832 .841 .849 .845Synv .863?
.813?
.837?
.843?
.851?
.847?Table 2: Parsing results with gold-standard senses (?
in-dicates that the recall or precision is significantly betterthan baseline; the best performing method in each col-umn is shown in bold)5.1 Gold standardWe disambiguated each token instance in our cor-pus according to the gold-standard sense data, andtrained both the Charniak and Bikel parsers overeach semantic representation.
We evaluated theparsers in full parsing and PP attachment contexts.The results for parsing are given in Table 2.
Therows represent the three semantic representations(including whether we substitute only nouns, onlyverbs or all POS).
We can see that in almost allcases the semantically-enriched representations im-prove over the baseline parsers.
These results arestatistically significant in some cases (as indicatedby ?).
The SFv representation produces the best re-sults for Bikel (F-score 0.010 above baseline), whilefor Charniak the best performance is obtained withword+SF (F-score 0.007 above baseline).
Compar-ing the two baseline parsers, Bikel achieves betterprecision and Charniak better recall.
Overall, Bikelobtains a superior F-score in all configurations.The results for the PP attachment experiments us-ing gold-standard senses are given in Table 3, bothfor the Charniak and Bikel parsers.
Again, the F-score for the semantic representations is better thanthe baseline in all cases.
We see that the improve-ment is significant for recall in most cases (particu-larly when using verbs), but not for precision (onlyCharniak over Synv and word+SFv for Bikel).
Forboth parsers the best results are achieved with SFv,which was also the best configuration for parsingwith Bikel.
The performance gain obtained here islarger than in parsing, which is in accordance withthe findings of Stetina and Nagao that lexical se-mantics has a considerable effect on PP attachmentCHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .667 .798 .727 .659 .820 .730SF .710 .808 .756 .714?
.809 .758SFn .671 .792 .726 .706 .818 .758SFv .729?
.823 .773?
.733?
.827 .778?word + SF .710?
.801 .753 .706?
.837 .766?word + SFn .698?
.813 .751 .706?
.829 .763?word + SFv .714?
.805 .757?
.706?
.837?
.766?Syn .722?
.814 .765?
.702?
.825 .758Synn .678 .805 .736 .690 .822 .751Synv .702?
.817?
.755?
.690?
.834 .755?Table 3: PP attachment results with gold-standard senses(?
indicates that the recall or precision is significantly bet-ter than baseline; the best performing method in each col-umn is shown in bold)performance.
As in full-parsing, Bikel outperformsCharniak, but in this case the difference in the base-lines is not statistically significant.5.2 First sense (1ST)For this experiment, we use the first sense data fromWordNet for disambiguation.
The results for fullparsing are given in Table 4.
Again, the perfor-mance is significantly better than baseline in mostcases, and surprisingly the results are even betterthan gold-standard in some cases.
We hypothesisethat this is due to the avoidance of excessive frag-mentation, as occurs with fine-grained senses.
Theresults are significantly better for nouns, with SFnperforming best.
Verbs seem to suffer from lack ofdisambiguation precision, especially for Bikel.
Hereagain, Charniak trails behind Bikel.The results for the PP attachment task are shownin Table 5.
The behaviour is slightly different here,with Charniak obtaining better results than Bikel inmost cases.
As was the case for parsing, the per-formance with 1ST reaches and in many instancessurpasses gold-standard levels, achieving statisticalsignificance over the baseline in places.
Compar-ing the semantic representations, the best results areachieved with SFv, as we saw in the gold-standardPP-attachment case.5.3 Automatic sense ranking (ASR)The final option for WSD is automatic sense rank-ing, which indicates how well our method performsin a completely unsupervised setting.The parsing results are given in Table 6.
We cansee that the scores are very similar to those from322CHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .857 .807 .832 .837 .845 .841SF .851 .804 .827 .843 .850 .846SFn .863?
.813 .837?
.850?
.854?
.852?SFv .857 .808 .832 .843 .853?
.848word + SF .859 .810 .834 .833 .841 .837word + SFn .862?
.811 .836 .844?
.851?
.848?word + SFv .857 .808 .832 .831 .839 .835Syn .857 .810 .833 .837 .844 .840Synn .863?
.812 .837?
.844?
.851?
.848?Synv .860 .810 .834 .836 .844 .840Table 4: Parsing results with 1ST (?
indicates that therecall or precision is significantly better than baseline; thebest performing method in each column is shown in bold)CHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .667 .798 .727 .659 .820 .730SF .710 .808 .756 .702 .806 .751SFn .671 .781 .722 .702 .829 .760SFv .737?
.836?
.783?
.718?
.821 .766?word + SF .706 .811 .755 .694 .823 .753word + SFn .690 .815 .747 .667 .810 .731word + SFv .714?
.805 .757?
.710?
.819 .761?Syn .725?
.833?
.776?
.698 .828 .757Synn .698 .828?
.757?
.667 .817 .734Synv .722?
.811 .763?
.706?
.818 .758?Table 5: PP attachment results with 1ST (?
indicates thatthe recall or precision is significantly better than baseline;the best performing method in each column is shown inbold)1ST, with improvements in some cases, particularlyfor Charniak.
Again, the results are better for nouns,except for the case of SFv with Bikel.
Bikel outper-forms Charniak in terms of F-score in all cases.The PP attachment results are given in Table 7.The results are similar to 1ST, with significant im-provements for verbs.
In this case, synsets slightlyoutperform SF.
Charniak performs better than Bikel,and the results for Synv are higher than the best ob-tained using gold-standard senses.6 DiscussionThe results of the previous section show that the im-provements in parsing results are small but signifi-cant, for all three word sense disambiguation strate-gies (gold-standard, 1ST and ASR).
Table 8 sum-marises the results, showing that the error reductionrate (ERR) over the parsing F-score is up to 6.9%,which is remarkable given the relatively superficialstrategy for incorporating sense information into theparser.
Note also that our baseline results for theCHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .857 .807 .832 .837 .845 .841SF .863 .815?
.838 .845?
.852 .849SFn .862 .810 .835 .845?
.850 .847?SFv .859 .810 .833 .846?
.856?
.851?word + SF .859 .810 .834 .836 .844 .840word + SFn .865?
.813?
.838?
.844?
.852?
.848?word + SFv .856 .806 .830 .832 .839 .836Syn .856 .807 .831 .840 .847 .843Synn .864?
.813?
.838?
.844?
.851?
.847?Synv .857 .806 .831 .837 .845 .841Table 6: Parsing results with ASR (?
indicates that therecall or precision is significantly better than baseline; thebest performing method in each column is shown in bold)CHARNIAK BIKELSYSTEM R P F1 R P F1Baseline .667 .798 .727 .659 .820 .730SF .733?
.824 .776?
.698 .805 .748SFn .682 .791 .733 .671 .807 .732SFv .733?
.813 .771?
.710?
.812 .757?word + SF .714?
.798 .754 .675 .800 .732word + SFn .690 .807 .744 .659 .804 .724word + SFv .706?
.800 .750 .702?
.814 .754?Syn .733?
.827 .778?
.694 .805 .745Synn .686 .810 .743 .667 .806 .730Synv .714?
.816 .762?
.714?
.816 .762?Table 7: PP attachment results with ASR (?
indicates thatthe recall or precision is significantly better than baseline;the best performance in each column is shown in bold)dataset are almost the same as previous work pars-ing the Brown corpus with similar models (Gildea,2001), which suggests that our dataset is representa-tive of this corpus.The improvement in PP attachment was larger(20.5% ERR), and also statistically significant.
Theresults for PP attachment are especially important,as we demonstrate that the sense information hashigh utility when embedded within a parser, wherethe parser needs to first identify the ambiguity andheads correctly.
Note that Atterer and Schu?tze(2007) have shown that the Bikel parser performs aswell as the state-of-the-art in PP attachment, whichsuggests our method improves over the current state-of-the-art.
The fact that the improvement is largerfor PP attachment than for full parsing is suggestiveof PP attachment being a parsing subtask where lex-ical semantic information is particularly important,supporting the findings of Stetina and Nagao (1997)over a standalone PP attachment task.
We also ob-served that while better PP-attachment usually im-proves parsing, there is some small variation.
This323WSD TASK PAR BASE SEM ERR BESTPars.C .832 .839?
4.2% word+SFGold- B .841 .851?
6.3% SFvstandardPPC .727 .773?
16.9% SFvB .730 .778?
17.8% SFvPars.C .832 .837?
3.0% SFn, Synn1STB .841 .852?
6.9% SFnPPC .727 .783?
20.5% SFvB .730 .766?
13.3% SFvPars.C .832 .838?
3.6% SF, word+SFn, SynnASRB .841 .851?
6.3% SFvPPC .727 .778?
18.7% SynB .730 .762?
11.9% SynvTable 8: Summary of F-score results with error reduc-tion rates and the best semantic representation(s) for eachsetting (C = Charniak, B = Bikel)means that the best configuration for PP-attachmentdoes not always produce the best results for parsingOne surprising finding was the strong perfor-mance of the automatic WSD systems, actuallyoutperforming the gold-standard annotation overall.Our interpretation of this result is that the approachof annotating all occurrences of the same word withthe same sense allows the model to avoid the datasparseness associated with the gold-standard distinc-tions, as well as supporting the merging of differ-ent words into single semantic classes.
While theresults for gold-standard senses were intended asan upper bound for WordNet-based sense informa-tion, in practice there was very little difference be-tween gold-standard senses and automatic WSD inall cases barring the Bikel parser and PP attachment.Comparing the two parsers, Charniak performsbetter than Bikel on PP attachment when automaticWSD is used, while Bikel performs better on parsingoverall.
Regarding the choice of WSD system, theresults for both approaches are very similar, show-ing that ASR performs well, even if it does not re-quire sense frequency information.The analysis of performance according to the se-mantic representation is not so clear cut.
Gener-alising only verbs to semantic files (SFv) was thebest option in most of the experiments, particularlyfor PP-attachment.
This could indicate that seman-tic generalisation is particularly important for verbs,more so than nouns.Our hope is that this paper serves as the bridge-head for a new line of research into the impact oflexical semantics on parsing.
Notably, more couldbe done to fine-tune the semantic representation be-tween the two extremes of full synsets and SFs.One could also imagine that the appropriate level ofgeneralisation differs across POS and even the rel-ative syntactic role, e.g.
finer-grained semantics areneeded for the objects than subjects of verbs.On the other hand, the parsing strategy is verysimple, as we just substitute words by their semanticclass and then train statistical parsers on the trans-formed input.
The semantic class should be an in-formation source that the parsers take into account inaddition to analysing the actual words used.
Tighterintegration of semantics into the parsing models,possibly in the form of discriminative rerankingmodels (Collins and Koo, 2005; Charniak and John-son, 2005; McClosky et al, 2006), is a promisingway forward in this regard.7 ConclusionsIn this work we have trained two state-of-the-artstatistical parsers on semantically-enriched input,where content words have been substituted withtheir semantic classes.
This simple method allowsus to incorporate lexical semantic information intothe parser, without having to reimplement a full sta-tistical parser.
We tested the two parsers in both afull parsing and a PP attachment context.This paper shows that semantic classes achievesignificant improvement both on full parsing andPP attachment tasks relative to the baseline parsers.PP attachment achieves a 20.5% ERR, and parsing6.9% without requiring hand-tagged data.The results are highly significant in demonstratingthat a simplistic approach to incorporating lexicalsemantics into a parser significantly improves parserperformance.
As far as we know, these are the firstresults over both WordNet and the Penn Treebank toshow that semantic processing helps parsing.AcknowledgementsWe wish to thank Diana McCarthy for providing uswith the sense rank for the target words.
This workwas partially funded by the Education Ministry (projectKNOW TIN2006-15049), the Basque Government (IT-397-07), and the Australian Research Council (grant no.DP0663879).
Eneko Agirre participated in this researchwhile visiting the University of Melbourne, based on jointfunding from the Basque Government and HCSNet.324ReferencesEneko Agirre and Philip Edmonds, editors.
2006.
Word SenseDisambiguation: Algorithms and Applications.
Springer,Dordrecht, Netherlands.Michaela Atterer and Hinrich Schu?tze.
2007.
Prepositionalphrase attachment without oracles.
Computational Linguis-tics, 33(4):469?476.Daniel M. Bikel.
2000.
A statistical model for parsing andword-sense disambiguation.
In Proc.
of the Joint SIGDATConference on Empirical Methods in Natural Language Pro-cessing and Very Large Corpora (EMNLP/VLC-2000), pages155?63, Hong Kong, China.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
In Proc.of the 43rd Annual Meeting of the ACL, pages 173?80, AnnArbor, USA.Eugene Charniak.
1997.
Statistical parsing with a context-freegrammar and word statistics.
In Proc.
of the 15th AnnualConference on Artificial Intelligence (AAAI-97), pages 598?603, Stanford, USA.Eugene Charniak.
2000.
A maximum entropy-based parser.In Proc.
of the 1st Annual Meeting of the North Ameri-can Chapter of Association for Computational Linguistics(NAACL2000), Seattle, USA.David Chiang and David M. Bikel.
2002.
Recovering latentinformation in treebanks.
In Proc.
of the 19th InternationalConference on Computational Linguistics (COLING 2002),pages 183?9, Taipei, Taiwan.Michael Collins and Terry Koo.
2005.
Discriminative rerank-ing for natural language parsing.
Computational Linguistics,31(1):25?69.Michael J. Collins.
1996.
A new statistical parser based onlexical dependencies.
In Proc.
of the 34th Annual Meetingof the ACL, pages 184?91, Santa Cruz, USA.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguistics,29(4):589?637.John Dowding, Robert Moore, Franc?ois Andry, and DouglasMoran.
1994.
Interleaving syntax and semantics in an effi-cient bottom-up parser.
In Proc.
of the 32nd Annual Meetingof the ACL, pages 110?6, Las Cruces, USA.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.Sanae Fujita, Francis Bond, Stephan Oepen, and TakaakiTanaka.
2007.
Exploiting semantic information for HPSGparse selection.
In Proc.
of the ACL 2007 Workshop on DeepLinguistic Processing, pages 25?32, Prague, Czech Repub-lic.Daniel Gildea.
2001.
Corpus variation and parser performance.In Proc.
of the 6th Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2001), pages 167?202,Pittsburgh, USA.Erik Hektoen.
1997.
Probabilistic parse selection basedon semantic cooccurrences.
In Proc.
of the 5th Inter-national Workshop on Parsing Technologies (IWPT-1997),pages 113?122, Boston, USA.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes: The90% solution.
In Proc.
of the Human Language Technol-ogy Conference of the NAACL, Companion Volume: ShortPapers, pages 57?60, New York City, USA.Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998.Building semantic concordances.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database.
MITPress, Cambridge, USA.Hang Li and Naoki Abe.
1998.
Generalising case frames usinga thesaurus and the MDL principle.
Computational Linguis-tics, 24(2):217?44.Dekang Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proc.
of the 36th Annual Meeting of theACL and 17th International Conference on ComputationalLinguistics: COLING/ACL-98, pages 768?774, Montreal,Canada.David M. Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In Proc.
of the 33rd Annual Meeting of the ACL,pages 276?83, Cambridge, USA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn treebank.
Computational Linguistics,19(2):313?30.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automatically ac-quired selectional preferences.
Computational Linguistics,29(4):639?654.Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.2004.
Finding predominant senses in untagged text.
InProc.
of the 42nd Annual Meeting of the ACL, pages 280?7, Barcelona, Spain.David McClosky, Eugene Charniak, and Mark Johnson.
2006.Effective self-training for parsing.
In Proc.
of the Hu-man Language Technology Conference of the NAACL(NAACL2006), pages 152?159, New York City, USA.Brian Mitchell.
2004.
Prepositional Phrase Attachment usingMachine Learning Algorithms.
Ph.D. thesis, University ofSheffield.Martha Palmer, Hoa Dang, and Christiane Fellbaum.
2006.Making fine-grained and coarse-grained sense distinctions,both manually and automatically.
Natural Language Engi-neering, 13(2):137?63.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.A maximum entropy model for prepositional phrase attach-ment.
In HLT ?94: Proceedings of the Workshop on HumanLanguage Technology, pages 250?255, Plainsboro, USA.Philip Resnik.
2006.
WSD in NLP applications.
In EnekoAgirre and Philip Edmonds, editors, Word Sense Disam-biguation: Algorithms and Applications, chapter 11, pages303?40.
Springer, Dordrecht, Netherlands.Jiri Stetina and Makoto Nagao.
1997.
Corpus based PP attach-ment ambiguity resolution with a semantic dictionary.
InProc.
of the 5th Annual Workshop on Very Large Corpora,pages 66?80, Hong Kong, China.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, andYueliang Qian.
2005.
Parsing the Penn Chinese Tree-bank with semantic knowledge.
In Proc.
of the 2nd Inter-national Joint Conference on Natural Language Processing(IJCNLP-05), pages 70?81, Jeju Island, Korea.325
