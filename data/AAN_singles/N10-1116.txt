Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 751?759,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsFrom Baby Steps to Leapfrog: How ?Less is More?in Unsupervised Dependency Parsing?Valentin I. SpitkovskyStanford University and Google Inc.valentin@cs.stanford.eduHiyan AlshawiGoogle Inc., Mountain View, CA, 94043hiyan@google.comDaniel JurafskyStanford University, Stanford, CA, 94305jurafsky@stanford.eduAbstractWe present three approaches for unsupervisedgrammar induction that are sensitive to datacomplexity and apply them to Klein and Man-ning?s Dependency Model with Valence.
Thefirst, Baby Steps, bootstraps itself via iteratedlearning of increasingly longer sentences andrequires no initialization.
This method sub-stantially exceeds Klein and Manning?s pub-lished scores and achieves 39.4% accuracy onSection 23 (all sentences) of the Wall StreetJournal corpus.
The second, Less is More,uses a low-complexity subset of the avail-able data: sentences up to length 15.
Focus-ing on fewer but simpler examples trades offquantity against ambiguity; it attains 44.1%accuracy, using the standard linguistically-informed prior and batch training, beatingstate-of-the-art.
Leapfrog, our third heuristic,combines Less is More with Baby Steps bymixing their models of shorter sentences, thenrapidly ramping up exposure to the full train-ing set, driving up accuracy to 45.0%.
Thesetrends generalize to the Brown corpus; aware-ness of data complexity may improve otherparsing models and unsupervised algorithms.1 IntroductionUnsupervised learning of hierarchical syntacticstructure from free-form natural language text is ahard problem whose eventual solution promises tobenefit applications ranging from question answer-ing to speech recognition and machine translation.A restricted version that targets dependencies and?Partially funded by NSF award IIS-0811974; first authorsupported by the Fannie & John Hertz Foundation Fellowship.assumes partial annotation, e.g., sentence bound-aries, tokenization and typically even part-of-speech(POS) tagging, has received much attention, elicit-ing a diverse array of techniques (Smith and Eis-ner, 2005; Seginer, 2007; Cohen et al, 2008).
Kleinand Manning?s (2004) Dependency Model with Va-lence (DMV) was the first to beat a simple parsingheuristic ?
the right-branching baseline.
Today?sstate-of-the-art systems (Headden et al, 2009; Co-hen and Smith, 2009) are still rooted in the DMV.Despite recent advances, unsupervised parsers lagfar behind their supervised counterparts.
Althoughlarge amounts of unlabeled data are known to im-prove semi-supervised parsing (Suzuki et al, 2009),the best unsupervised systems use less data than isavailable for supervised training, relying on complexmodels instead: Headden et al?s (2009) ExtendedValence Grammar (EVG) combats data sparsity withsmoothing alone, training on the same small subsetof the tree-bank as the classic implementation of theDMV; Cohen and Smith (2009) use more compli-cated algorithms (variational EM and MBR decod-ing) and stronger linguistic hints (tying related partsof speech and syntactically similar bilingual data).We explore what can be achieved through judi-cious use of data and simple, scalable techniques.Our first approach iterates over a series of trainingsets that gradually increase in size and complex-ity, forming an initialization-independent scaffold-ing for learning a grammar.
It works with Klein andManning?s simple model (the original DMV) andtraining algorithm (classic EM) but eliminates theircrucial dependence on manually-tuned priors.
Thesecond technique is consistent with the intuition thatlearning is most successful within a band of the size-complexity spectrum.
Both could be applied to more751intricate models and advanced learning algorithms.We combine them in a third, efficient hybrid method.2 IntuitionFocusing on simple examples helps guide unsuper-vised learning,1 as blindly added confusing data caneasily mislead training.
We suggest that unless it isincreased gradually, unbridled, complexity can over-whelm a system.
How to grade an example?s diffi-culty?
The cardinality of its solution space presentsa natural proxy.
In the case of parsing, the num-ber of possible syntactic trees grows exponentiallywith sentence length.
For longer sentences, the un-supervised optimization problem becomes severelyunder-constrained, whereas for shorter sentences,learning is tightly reined in by data.
In the extremecase of a single-word sentence, there is no choicebut to parse it correctly.
At two words, a raw 50%chance of telling the head from its dependent is stillhigh, but as length increases, the accuracy of eveneducated guessing rapidly plummets.
In model re-estimation, long sentences amplify ambiguity andpollute fractional counts with noise.
At times, batchsystems are better off using less data.Baby Steps: Global non-convex optimization ishard.
We propose a meta-heuristic that takes theguesswork out of initializing local search.
Begin-ning with an easy (convex) case, it slowly extends itto the fully complex target task by taking tiny stepsin the problem space, trying not to stray far fromthe relevant neighborhoods of the solution space.
Aseries of nested subsets of increasingly longer sen-tences that culminates in the complete data set offersa natural progression.
Its base case ?
sentences oflength one ?
has a trivial solution that requires nei-ther initialization nor search yet reveals somethingof sentence heads.
The next step ?
sentences oflength one and two ?
refines initial impressionsof heads, introduces dependents, and exposes theiridentities and relative positions.
Although not rep-resentative of the full grammar, short sentences cap-ture enough information to paint most of the pictureneeded by slightly longer sentences.
They set up aneasier, incremental subsequent learning task.
Stepk + 1 augments training input to include lengths1It mirrors the effect that boosting hard examples has forsupervised training (Freund and Schapire, 1997).1, 2, .
.
.
, k, k + 1 of the full data set and executeslocal search starting from the (smoothed) model es-timated by step k. This truly is grammar induction.Less is More: For standard batch training, just us-ing simple, short sentences is not enough.
They arerare and do not reveal the full grammar.
We find a?sweet spot?
?
sentence lengths that are neither toolong (excluding the truly daunting examples) nor toofew (supplying enough accessible information), us-ing Baby Steps?
learning curve as a guide.
We trainwhere it flattens out, since remaining sentences con-tribute little (incremental) educational value.2Leapfrog: As an alternative to discarding data, abetter use of resources is to combine the results ofbatch and iterative training up to the sweet spot datagradation, then iterate with a large step size.3 Related WorkTwo types of scaffolding for guiding language learn-ing debuted in Elman?s (1993) experiments with?starting small?
: data complexity (restricting input)and model complexity (restricting memory).
In bothcases, gradually increasing complexity allowed ar-tificial neural networks to master a pseudo-naturalgrammar they otherwise failed to learn.
Initially-limited capacity resembled maturational changes inworking memory and attention span that occur overtime in children (Kail, 1984), in line with the ?lessis more?
proposal (Newport, 1988; 1990).
AlthoughRohde and Plaut (1999) failed to replicate this3 re-sult with simple recurrent networks, other machinelearning techniques reliably benefit from scaffoldedmodel complexity on a variety of language tasks.In word-alignment, Brown et al (1993) used IBMModels 1-4 as ?stepping stones?
to training Model 5.Other prominent examples include ?coarse-to-fine?2This is akin to McClosky et al?s (2006) ?Goldilocks effect.
?3Worse, they found that limiting input hindered languageacquisition.
And making the grammar more English-like (byintroducing and strengthening semantic constraints), increasedthe already significant advantage for ?starting large!?
With it-erative training invoking the optimizer multiple times, creatingextra opportunities to converge, Rohde and Plaut (1999) sus-pected that Elman?s (1993) simulations simply did not allownetworks exposed exclusively to complex inputs sufficient train-ing time.
Our extremely generous, low termination thresholdfor EM (see ?5.1) addresses this concern.
However, given theDMV?s purely syntactic POS tag-based approach (see ?5), itwould be prudent to re-test Baby Steps with a lexicalized model.752approaches to parsing, translation and speech recog-nition (Charniak and Johnson, 2005; Charniak et al,2006; Petrov et al, 2008; Petrov, 2009), and re-cently unsupervised POS tagging (Ravi and Knight,2009).
Initial models tend to be particularly simple,4and each refinement towards a full model introducesonly limited complexity, supporting incrementality.Filtering complex data, the focus of our work,is unconventional in natural language processing.Such scaffolding qualifies as shaping ?
a methodof instruction (routinely exploited in animal train-ing) in which the teacher decomposes a completetask into sub-components, providing an easier pathto learning.
When Skinner (1938) coined the term,he described it as a ?method of successive approx-imations.?
Ideas that gradually make a task moredifficult have been explored in robotics (typically,for navigation), with reinforcement learning (Singh,1992; Sanger, 1994; Saksida et al, 1997; Dorigoand Colombetti, 1998; Savage, 1998; Savage, 2001).Recently, Krueger and Dayan (2009) showed thatshaping speeds up language acquisition and leadsto better generalization in abstract neural networks.Bengio et al (2009) confirmed this for deep de-terministic and stochastic networks, using simplemulti-stage curriculum strategies.
They conjecturedthat a well-chosen sequence of training criteria ?different sets of weights on the examples ?
couldact as a continuation method (Allgower and Georg,1990), helping find better local optima for non-convex objectives.
Elman?s learners constrained thepeaky solution space by focusing on just the rightdata (simple sentences that introduced basic repre-sentational categories) at just the right time (earlyon, when their plasticity was greatest).
Self-shaping,they simplified tasks through deliberate omission (ormisunderstanding).
Analogously, Baby Steps in-duces an early structural locality bias (Smith andEisner, 2006), then relaxes it, as if annealing (Smithand Eisner, 2004).
Its curriculum of binary weightsinitially discards complex examples responsible for?high-frequency noise,?
with earlier, ?smoothed?objectives revealing more of the global picture.There are important differences between our re-sults and prior work.
In contrast to Elman, we use a4Brown et al?s (1993) Model 1 (and, similarly, the first babystep) has a global optimum that can be computed exactly, so thatno initial or subsequent parameters depend on initialization.large data set (WSJ) of real English.
Unlike Bengioet al and Krueger and Dayan, we shape a parser, nota language model.
Baby Steps is similar, in spirit, toSmith and Eisner?s methods.
Deterministic anneal-ing (DA) shares nice properties with Baby Steps,but performs worse than EM for (constituent) pars-ing; Baby Steps handedly defeats standard training.Structural annealing works well, but requires a hand-tuned annealing schedule and direct manipulation ofthe objective function; Baby Steps works ?out of thebox,?
its locality biases a natural consequence of acomplexity/data-guided tour of optimization prob-lems.
Skewed DA incorporates a good initializerby interpolating between two probability distribu-tions, whereas our hybrid, Leapfrog, admits multi-ple initializers by mixing structures instead.
?Lessis More?
is novel and confirms the tacit consensusimplicit in training on small data sets (e.g., WSJ10).4 Data Sets and MetricsKlein and Manning (2004) both trained and testedthe DMV on the same customized subset (WSJ10)of Penn English Treebank?s Wall Street Journal por-tion (Marcus et al, 1993).
Its 49,208 annotatedparse trees were pruned5 down to 7,422 sentencesof at most 10 terminals, spanning 35 unique POStags.
Following standard practice, automatic ?head-percolation?
rules (Collins, 1999) were used to con-vert the remaining trees into dependencies.
Forcedto produce a single ?best?
parse, their algorithmwas judged on accuracy: its directed score was thefraction of correct dependencies; a more flattering6undirected score was also used.
We employ thesame metrics, emphasizing directed scores, and gen-eralize WSJk to be the subset of pre-processed sen-tences with at most k terminals.
Our experiments fo-cus on k ?
{1, .
.
.
, 45}, but we also test on WSJ100and Section 23 of WSJ?
(the entire WSJ), as well asthe held-out Brown100 (similarly derived from theBrown corpus (Francis and Kucera, 1979)).
See Fig-ure 1 for these corpora?s sentence and token counts.5Stripped of all empty sub-trees, punctuation, and terminals(tagged # and $) not pronounced where they appear, those sen-tences still containing more than ten tokens were thrown out.6Ignoring polarity of parent-child relations partially ob-scured effects of alternate analyses (systematic choices betweenmodals and main verbs for heads of sentences, determiners fornoun phrases, etc.)
and facilitated comparison with prior work.753Corpus Sentences POS Tokens Corpus Sentences POS TokensWSJ1 159 159 WSJ13 12,270 110,760WSJ2 499 839 WSJ14 14,095 136,310WSJ3 876 1,970 WSJ15 15,922 163,715WSJ4 1,394 4,042 WSJ20 25,523 336,555WSJ5 2,008 7,112 WSJ25 34,431 540,895WSJ6 2,745 11,534 WSJ30 41,227 730,099WSJ7 3,623 17,680 WSJ35 45,191 860,053WSJ8 4,730 26,536 WSJ40 47,385 942,801WSJ9 5,938 37,408 WSJ45 48,418 986,830WSJ10 7,422 52,248 WSJ100 49,206 1,028,054WSJ11 8,856 68,022 Section 23 2,353 48,201WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 4551015202530354045Thousandsof SentencesThousandsof Tokens 100200300400500600700800900WSJkFigure 1: Sizes of WSJ{1, .
.
.
, 45, 100}, Section 23 of WSJ?
and Brown100.NNS VBD IN NN ?Payrolls fell in September .P = (1 ?0z }| {PSTOP(?, L, T)) ?
PATTACH(?, L, VBD)?
(1 ?
PSTOP(VBD, L, T)) ?
PATTACH(VBD, L, NNS)?
(1 ?
PSTOP(VBD, R, T)) ?
PATTACH(VBD, R, IN)?
(1 ?
PSTOP(IN, R, T)) ?
PATTACH(IN, R, NN)?
PSTOP(VBD, L, F) ?
PSTOP(VBD, R, F)?
PSTOP(NNS, L, T) ?
PSTOP(NNS, R, T)?
PSTOP(IN, L, T) ?
PSTOP(IN, R, F)?
PSTOP(NN, L, T) ?
PSTOP(NN, R, T)?
PSTOP(?, L, F)| {z }1?
PSTOP(?, R, T)| {z }1.Figure 2: A simple dependency structure for a short sen-tence and its probability, as factored by the DMV.5 New Algorithms for the Classic ModelThe DMV (Klein and Manning, 2004) is a single-state head automata model (Alshawi, 1996) over lex-ical word classes {cw} ?
POS tags.
Its generativestory for a sub-tree rooted at a head (of class ch) restson three types of independent decisions: (i) initialdirection dir ?
{L, R} in which to attach children, viaprobability PORDER(ch); (ii) whether to seal dir, stop-ping with probability PSTOP(ch, dir, adj), conditionedon adj ?
{T, F} (true iff considering dir?s first, i.e.,adjacent, child); and (iii) attachments (of class ca),according to PATTACH(ch, dir, ca).
This produces onlyprojective trees.7 A root token ?
generates the headof a sentence as its left (and only) child.
Figure 2displays an example that ignores (sums out) PORDER.The DMV lends itself to unsupervised learn-7Unlike spanning tree algorithms (McDonald et al, 2005),DMV?s chart-based method disallows crossing dependencies.ing via inside-outside re-estimation (Baker, 1979).Klein and Manning did not use smoothing andstarted with an ?ad-hoc harmonic?
completion: aim-ing for balanced trees, non-root heads attached de-pendents in inverse proportion to (a constant plus)their distance; ?
generated heads uniformly at ran-dom.
This non-distributional heuristic created favor-able initial conditions that nudged EM towards typi-cal linguistic dependency structures.5.1 Algorithm #0: Ad-Hoc??
A Variation on Original Ad-Hoc InitializationSince some of the important implementation detailsare not available in the literature (Klein and Man-ning, 2004; Klein, 2005), we had to improvise ini-tialization and terminating conditions.
We suspectthat our choices throughout this section do not matchKlein and Manning?s actual training of the DMV.We use the following ad-hoc harmonic scores (forall tokens other than ?
): P?ORDER ?
1/2;P?STOP ?
(ds + ?s)?1 = (ds + 3)?1, ds ?
0;P?ATTACH ?
(da + ?a)?1 = (da + 2)?1, da ?
1.Integers d{s,a} are distances from heads to stoppingboundaries and dependents.8 We initialize train-ing by producing best-scoring parses of all inputsentences and converting them into proper proba-bility distributions PSTOP and PATTACH via maximum-likelihood estimation (a single step of Viterbi train-ing (Brown et al, 1993)).
Since left and right chil-dren are independent, we drop PORDER altogether, mak-8Constants ?
{s,a} come from personal communication.Note that ?s is one higher than is strictly necessary to avoid bothdivision by zero and determinism; ?a could have been safely ze-roed out, since we never compute 1 ?
PATTACH (see Figure 2).754ing ?headedness?
deterministic.
Our parser care-fully randomizes tie-breaking, so that all parse treeshaving the same score get an equal shot at beingselected (both during initialization and evaluation).We terminate EM when a successive change in over-all per-token cross-entropy drops below 2?20 bits.5.2 Algorithm #1: Baby Steps?
An Initialization-Independent ScaffoldingWe eliminate the need for initialization by first train-ing on a trivial subset of the data ?
WSJ1; thisworks, since there is only one (the correct) way toparse a single-token sentence.
We plug the resultingmodel into training on WSJ2 (sentences of up to twotokens), and so forth, building up to WSJ45.9 Thisalgorithm is otherwise identical to Ad-Hoc?, withthe exception that it re-estimates each model usingLaplace smoothing, so that earlier solutions couldbe passed to next levels, which sometimes containpreviously unseen dependent and head POS tags.5.3 Algorithm #2: Less is More?
Ad-Hoc?
where Baby Steps FlatlinesWe jettison long, complex sentences and deploy Ad-Hoc?
?s initializer and batch training at WSJk??
?
anestimate of the sweet spot data gradation.
To findit, we track Baby Steps?
successive models?
cross-entropies on the complete data set, WSJ45.
An ini-tial segment of rapid improvement is separated fromthe final region of convergence by a knee ?
pointsof maximum curvature (see Figure 3).
We use animproved10 L method (Salvador and Chan, 2004) toautomatically locate this area of diminishing returns.Specifically, we determine its end-points [k0, k?]
byminimizing squared error, estimating k?0 = 7 andk??
= 15.
Training at WSJ15 just misses the plateau.5.4 Algorithm #3: Leapfrog?
A Practical and Efficient Hybrid MixtureCherry-picking the best features of ?Less is More?and Baby Steps, we begin by combining their mod-9Its 48,418 sentences (see Figure 1) cover 94.4% of all sen-tences in WSJ; the longest of the missing 790 has length 171.10Instead of iteratively fitting a two-segment form and adap-tively discarding its tail, we use three line segments, applyingordinary least squares to the first two, but requiring the third tobe horizontal and tangent to a minimum.
The result is a batchoptimization routine that returns an interval for the knee, ratherthan a point estimate (see Figure 3 for details).5 10 15 20 25 30 35 40 453.03.54.04.55.0WSJkbptCross-entropy h (in bits per token) on WSJ45Knee[7, 15] Tight, Flat, Asymptotic Boundminb0,m0,b1,m12<k0<k?<458>>>>>><>>>>>>:k0?1Xk=1(hk ?
b0 ?
m0k)2 +k?Xk=k0(hk ?
b1 ?
m1k)2 +45Xk=k?+1?hk ?45minj=k?+1hj?2Figure 3: Cross-entropy on WSJ45 after each baby step, apiece-wise linear fit, and an estimated region for the knee.els at WSJk??.
Using one best parse from each,for every sentence in WSJk?
?, the base case re-estimates a new model from a mixture of twice thenormal number of trees; inductive steps leap over k?
?lengths, conveniently ending at WSJ45, and estimatetheir initial models by applying a previous solutionto a new input set.
Both follow up the single step ofViterbi training with at most five iterations of EM.Our hybrid makes use of two good (condition-ally) independent initialization strategies and exe-cutes many iterations of EM where that is cheap ?at shorter sentences (WSJ15 and below).
It then in-creases the step size, training just three more times(at WSJ{15, 30, 45}) and allowing only a few (moreexpensive) iterations of EM.
Early termination im-proves efficiency and regularizes these final models.5.5 Reference Algorithms?
Baselines, a Skyline and Published ArtWe carve out the problem space using two extremeinitialization strategies: (i) the uninformed uniformprior, which serves as a fair ?zero-knowledge?
base-line for comparing uninitialized models; and (ii) themaximum-likelihood ?oracle?
prior, computed fromreference parses, which yields a skyline (a reversebaseline) ?
how well any algorithm that stumbledon the true solution would fare at EM?s convergence.In addition to citing Klein and Manning?s (2004)results, we compare our accuracies on Section 23of WSJ?
to two state-of-the-art systems and pastbaselines (see Table 2).
Headden et al?s (2009)lexicalized EVG is the best on short sentences, but7555 10 15 20 25 30 35 402030405060708090OracleBaby StepsAd-HocUninformedWSJk(a) Directed Accuracy (%) on WSJk5 10 15 20 25 30 35 40 45(b) Undirected Accuracy (%) on WSJkOracleBaby StepsAd-HocUninformedFigure 4: Directed and undirected accuracy scores attained by the DMV, when trained and tested on the same gradationof WSJ, for several different initialization strategies.
Green circles mark Klein and Manning?s (2004) published scores;red, violet and blue curves represent the supervised (maximum-likelihood oracle) initialization, Baby Steps, and theuninformed uniform prior.
Dotted curves reflect starting performance, solid curves register performance at EM?sconvergence, and the arrows connecting them emphasize the impact of learning.5 10 15 20 25 30 35 40 452030405060WSJkOracleLeapfrogBaby StepsAd-Hoc?UninformedAd-HocDirected Accuracy (%) on WSJkFigure 5: Directed accuracies for Ad-Hoc?
(shown ingreen) and Leapfrog (in gold); all else as in Figure 4(a).its performance is unreported for longer sentences,for which Cohen and Smith?s (2009) seem to bethe highest published scores; we include their in-termediate results that preceded parameter-tying ?Bayesian models with Dirichlet and log-normal pri-ors, coupled with both Viterbi and minimum Bayes-risk (MBR) decoding (Cohen et al, 2008).6 Experimental ResultsWe packed thousands of empirical outcomes into thespace of several graphs (Figures 4, 5 and 6).
The col-ors (also in Tables 1 and 2) correspond to differentinitialization strategies ?
to a first approximation,the learning algorithm was held constant (see ?5).Figures 4 and 5 tell one part of our story.
As datasets increase in size, training algorithms gain accessto more information; however, since in this unsu-pervised setting training and test sets are the same,additional longer sentences make for substantiallymore challenging evaluation.
To control for thesedynamics, we applied Laplace smoothing to all (oth-erwise unsmoothed) models and re-plotted their per-formance, holding several test sets fixed, in Figure 6.We report undirected accuracies parenthetically.6.1 Result #1: Baby StepsFigure 4 traces out performance on the training set.Klein and Manning?s (2004) published scores ap-pear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%).Baby Steps achieves 53.0% (65.7%) by WSJ10;trained and tested on WSJ45, it gets 39.7% (54.3%).Uninformed, classic EM learns little about directeddependencies: it improves only slightly, e.g., from17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learn-ing some of the structure, as evidenced by its undi-rected scores), but degrades with shorter sentences,where its initial guessing rate is high.
In the caseof oracle training, we expected EM to walk awayfrom supervised solutions (Elworthy, 1994; Meri-7565 10 15 20 25 30 35 4020304050607080(a) Directed Accuracy (%) on WSJ10WSJkOracleLeapfrogBaby StepsLess is More| {z }Ad-Hoc?Ad-HocUninformed5 10 15 20 25 30 35 40 45(b) Directed Accuracy (%) on WSJ40OracleLeapfrogBaby StepsLess is More| {z }Ad-Hoc?UninformedFigure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then testedagainst fixed evaluation sets ?
WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40.aldo, 1994; Liang and Klein, 2008), but the ex-tent of its drops is alarming, e.g., from the super-vised 69.8% (72.2%) to the skyline?s 50.6% (59.5%)on WSJ45.
In contrast, Baby Steps?
scores usu-ally do not change much from one step to thenext, and where its impact of learning is big (atWSJ{4, 5, 14}), it is invariably positive.6.2 Result #2: Less is MoreAd-Hoc?
?s curve (see Figure 5) suggests how Kleinand Manning?s Ad-Hoc initializer may have scaledwith different gradations of WSJ.
Strangely, our im-plementation performs significantly above their re-ported numbers at WSJ10: 54.5% (68.3%) is evenslightly higher than Baby Steps; nevertheless, givenenough data (from WSJ22 onwards), Baby Stepsovertakes Ad-Hoc?, whose ability to learn takes a se-rious dive once the inputs become sufficiently com-plex (at WSJ23), and never recovers.
Note that Ad-Hoc?
?s biased prior peaks early (at WSJ6), eventu-ally falls below the guessing rate (by WSJ24), yetstill remains well-positioned to climb, outperform-ing uninformed learning.Figure 6 shows that Baby Steps scales better withmore (complex) data ?
its curves do not trenddownwards.
However, a good initializer induces asweet spot at WSJ15, where the DMV is learnedbest using Ad-Hoc?.
This mode is ?Less is More,?scoring 44.1% (58.9%) on WSJ45.
Curiously, evenoracle training exhibits a bump at WSJ15: once sen-tences get long enough (at WSJ36), its performancedegrades below that of oracle training with virtuallyno supervision (at the hardly representative WSJ3).6.3 Result #3: LeapfrogMixing Ad-Hoc?
with Baby Steps at WSJ15 yieldsa model whose performance initially falls betweenits two parents but surpasses both with a little train-ing (see Figure 5).
Leaping to WSJ45, via WSJ30,results in our strongest model: its 45.0% (58.4%) ac-curacy bridges half of the gap between Baby Stepsand the skyline, and at a tiny fraction of the cost.6.4 Result #4: GeneralizationOur models carry over to the larger WSJ100, Section23 of WSJ?, and the independent Brown100 (seeTable 1).
Baby Steps improves out of domain, con-firming that shaping generalizes well (Krueger andDayan, 2009; Bengio et al, 2009).
Leapfrog doesbest across the board but dips on Brown100, despiteits safe-guards against over-fitting.Section 23 (see Table 2) reveals, unexpectedly,that Baby Steps would have been state-of-the-art in2008, whereas ?Less is More?
outperforms all priorwork on longer sentences.
Baby Steps is competi-tive with log-normal families (Cohen et al, 2008),scoring slightly better on longer sentences againstViterbi decoding, though worse against MBR.
?Lessis More?
beats state-of-the-art on longer sentencesby close to 2%; Leapfrog gains another 1%.757Ad-Hoc?
Baby Steps Leapfrog Ad-Hoc?
Baby Steps LeapfrogSection 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4)WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1)Table 1: Directed and undirected accuracies on Section 23 of WSJ?, WSJ100 and Brown100 for Ad-Hoc?, BabySteps and Leapfrog, trained at WSJ15 and WSJ45.Decoding WSJ10 WSJ20 WSJ?Attach-Right (Klein and Manning, 2004) ?
38.4 33.4 31.7DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2Dirichlet (Cohen et al, 2008) Viterbi 45.9 39.4 34.9Ad-Hoc (Cohen et al, 2008) MBR 46.1 39.9 35.9Dirichlet (Cohen et al, 2008) MBR 46.1 40.6 36.9Log-Normal Families (Cohen et al, 2008) Viterbi 59.3 45.1 39.0Baby Steps (@15) Viterbi 55.5 44.3 39.2Baby Steps (@45) Viterbi 55.1 44.4 39.4Log-Normal Families (Cohen et al, 2008) MBR 59.4 45.9 40.5Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2Less is More (Ad-Hoc?
@15) Viterbi 56.2 48.2 44.1Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0EVG Smoothed (skip-val) (Headden et al, 2009) Viterbi 62.1Smoothed (skip-head) (Headden et al, 2009) Viterbi 65.0Smoothed (skip-head), Lexicalized (Headden et al, 2009) Viterbi 68.8Table 2: Directed accuracies on Section 23 of WSJ{10, 20,? }
for several baselines and recent state-of-the-art systems.7 ConclusionWe explored three simple ideas for unsupervised de-pendency parsing.
Pace Halevy et al (2009), wefind ?Less is More?
?
the paradoxical result thatbetter performance can be attained by training withless data, even when removing samples from the true(test) distribution.
Our small tweaks to Klein andManning?s approach of 2004 break through the 2009state-of-the-art on longer sentences, when trained atWSJ15 (the auto-detected sweet spot gradation).The second, Baby Steps, is an elegant meta-heuristic for optimizing non-convex training crite-ria.
It eliminates the need for linguistically-biasedmanually-tuned initializers, particularly if the loca-tion of the sweet spot is not known.
This tech-nique scales gracefully with more (complex) dataand should easily carry over to more powerful pars-ing models and learning algorithms.Finally, Leapfrog forgoes the elegance and metic-ulousness of Baby Steps in favor of pragmatism.Employing both good initialization strategies atits disposal, and spending CPU cycles wisely, itachieves better performance than both ?Less isMore?
and Baby Steps.Future work could explore unifying these tech-niques with other state-of-the-art approaches.
It maybe useful to scaffold on both data and model com-plexity, e.g., by increasing head automata?s numberof states (Alshawi and Douglas, 2000).
We see manyopportunities for improvement, considering the poorperformance of oracle training relative to the super-vised state-of-the-art, and in turn the poor perfor-mance of unsupervised state-of-the-art relative to theoracle models.11 To this end, it would be instructiveto understand both the linguistic and statistical na-ture of the sweet spot, and to test its universality.AcknowledgmentsWe thank Angel X. Chang, Pi-Chuan Chang, David L.W.
Hall,Christopher D. Manning, David McClosky, Daniel Ramage andthe anonymous reviewers for many helpful comments on draftversions of this paper.ReferencesE.
L. Allgower and K. Georg.
1990.
Numerical ContinuationMethods: An Introduction.
Springer-Verlag.11To facilitate future work, all of our models are publiclyavailable at http://cs.stanford.edu/?valentin/.758H.
Alshawi and S. Douglas.
2000.
Learning dependency trans-duction models from unannotated examples.
In Royal Soci-ety of London Philosophical Transactions Series A, volume358.H.
Alshawi.
1996.
Head automata for speech translation.
InProc.
of ICSLP.J.
K. Baker.
1979.
Trainable grammars for speech recognition.In Speech Communication Papers for the 97th Meeting of theAcoustical Society of America.Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.
2009.Curriculum learning.
In ICML.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-cer.
1993.
The mathematics of statistical machine transla-tion: Parameter estimation.
Computational Linguistics, 19.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best pars-ing and MaxEnt discriminative reranking.
In Proc.
of ACL.E.
Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,I.
Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, andT.
Vu.
2006.
Multilevel coarse-to-fine PCFG parsing.
InHLT-NAACL.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic normal dis-tributions for soft parameter tying in unsupervised grammarinduction.
In Proc.
of NAACL-HLT.S.
B. Cohen, K. Gimpel, and N. A. Smith.
2008.
Logistic nor-mal priors for unsupervised probabilistic grammar induction.In NIPS.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.M.
Dorigo and M. Colombetti.
1998.
Robot Shaping: AnExperiment in Behavior Engineering.
MIT Press/BradfordBooks.J.
L. Elman.
1993.
Learning and development in neural net-works: The importance of starting small.
Cognition, 48.D.
Elworthy.
1994.
Does Baum-Welch re-estimation help tag-gers?
In Proc.
of ANLP.W.
N. Francis and H. Kucera, 1979.
Manual of Information toAccompany a Standard Corpus of Present-Day Edited Amer-ican English, for use with Digital Computers.
Department ofLinguistic, Brown University.Y.
Freund and R. E. Schapire.
1997.
A decision-theoretic gen-eralization of on-line learning and an application to boosting.Journal of Computer and System Sciences, 55(1).A.
Halevy, P. Norvig, and F. Pereira.
2009.
The unreasonableeffectiveness of data.
IEEE Intelligent Systems, 24(2).W.
P. Headden, III, M. Johnson, and D. McClosky.
2009.
Im-proving unsupervised dependency parsing with richer con-texts and smoothing.
In Proc.
of NAACL-HLT.R.
Kail.
1984.
The development of memory in children.
W. H.Freeman and Company, 2nd edition.D.
Klein and C. D. Manning.
2004.
Corpus-based induction ofsyntactic structure: Models of dependency and constituency.In Proc.
of ACL.D.
Klein.
2005.
The Unsupervised Learning of Natural Lan-guage Structure.
Ph.D. thesis, Stanford University.K.
A. Krueger and P. Dayan.
2009.
Flexible shaping: Howlearning in small steps helps.
Cognition, 110.P.
Liang and D. Klein.
2008.
Analyzing the errors of unsuper-vised learning.
In Proc.
of HLT-ACL.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2).D.
McClosky, E. Charniak, and M. Johnson.
2006.
Effectiveself-training for parsing.
In Proc.
of NAACL-HLT.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005.Non-projective dependency parsing using spanning tree al-gorithms.
In Proc.
of HLT-EMNLP.B.
Merialdo.
1994.
Tagging English text with a probabilisticmodel.
Computational Linguistics, 20(2):155?172.E.
L. Newport.
1988.
Constraints on learning and their role inlanguage acquisition: Studies of the acquisition of AmericanSign Language.
Language Sciences, 10(1).E.
L. Newport.
1990.
Maturational constraints on languagelearning.
Cognitive Science, 14(1).S.
Petrov, A. Haghighi, and D. Klein.
2008.
Coarse-to-finesyntactic machine translation using language projections.
InProc.
of EMNLP.S.
O. Petrov.
2009.
Coarse-to-Fine Natural Language Process-ing.
Ph.D. thesis, University of California, Berkeley.S.
Ravi and K. Knight.
2009.
Minimized models for unsuper-vised part-of-speech tagging.
In Proc.
of ACL-IJCNLP.D.
L. T. Rohde and D. C. Plaut.
1999.
Language acquisition inthe absence of explicit negative evidence: How important isstarting small?
Cognition, 72(1).L.
M. Saksida, S. M. Raymond, and D. S. Touretzky.
1997.Shaping robot behavior using principles from instrumentalconditioning.
Robotics and Autonomous Systems, 22(3).S.
Salvador and P. Chan.
2004.
Determining the number ofclusters/segments in hierarchical clustering/segmentation al-gorithms.
In Proc.
of ICTAI.T.
D. Sanger.
1994.
Neural network learning control ofrobot manipulators using gradually increasing task difficulty.IEEE Trans.
on Robotics and Automation, 10.T.
Savage.
1998.
Shaping: The link between rats and robots.Connection Science, 10(3).T.
Savage.
2001.
Shaping: A multiple contingencies analysisand its relevance to behaviour-based robotics.
ConnectionScience, 13(3).Y.
Seginer.
2007.
Fast unsupervised incremental parsing.
InProc.
of ACL.S.
P. Singh.
1992.
Transfer of learning by composing solutionsof elemental squential tasks.
Machine Learning, 8.B.
F. Skinner.
1938.
The behavior of organisms: An experi-mental analysis.
Appleton-Century-Crofts.N.
A. Smith and J. Eisner.
2004.
Annealing techniques forunsupervised statistical language learning.
In Proc.
of ACL.N.
A. Smith and J. Eisner.
2005.
Guiding unsupervised gram-mar induction using contrastive estimation.
In Proc.
of theIJCAI Workshop on Grammatical Inference Applications.N.
A. Smith and J. Eisner.
2006.
Annealing structural biasin multilingual weighted grammar induction.
In Proc.
ofCOLING-ACL.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.
Anempirical study of semi-supervised structured conditionalmodels for dependency parsing.
In Proc.
of EMNLP.759
