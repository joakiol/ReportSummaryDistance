Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216?223,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Simple, Similarity-based Model for Selectional PreferencesKatrin ErkUniversity of Texas at Austinkatrin.erk@mail.utexas.eduAbstractWe propose a new, simple model for the auto-matic induction of selectional preferences, usingcorpus-based semantic similarity metrics.
Fo-cusing on the task of semantic role labeling,we compute selectional preferences for seman-tic roles.
In evaluations the similarity-basedmodel shows lower error rates than both Resnik?sWordNet-based model and the EM-based clus-tering model, but has coverage problems.1 IntroductionSelectional preferences, which characterize typ-ical arguments of predicates, are a very use-ful and versatile knowledge source.
They havebeen used for example for syntactic disambigua-tion (Hindle and Rooth, 1993), word sense dis-ambiguation (WSD) (McCarthy and Carroll,2003) and semantic role labeling (SRL) (Gildeaand Jurafsky, 2002).The corpus-based induction of selectionalpreferences was first proposed by Resnik (1996).All later approaches have followed the same two-step procedure, first collecting argument head-words from a corpus, then generalizing to other,similar words.
Some approaches have usedWordNet for the generalization step (Resnik,1996; Clark and Weir, 2001; Abe and Li, 1993),others EM-based clustering (Rooth et al, 1999).In this paper we propose a new, simple modelfor selectional preference induction that usescorpus-based semantic similarity metrics, suchas Cosine or Lin?s (1998) mutual information-based metric, for the generalization step.
Thismodel does not require any manually createdlexical resources.
In addition, the corpus forcomputing the similarity metrics can be freelychosen, allowing greater variation in the domainof generalization than a fixed lexical resource.We focus on one application of selectionalpreferences: semantic role labeling.
The ar-gument positions for which we compute selec-tional preferences will be semantic roles in theFrameNet (Baker et al, 1998) paradigm, andthe predicates we consider will be semanticclasses of words rather than individual words(which means that different preferences will belearned for different senses of a predicate word).In SRL, the two most pressing issues today are(1) the development of strong semantic featuresto complement the current mostly syntactically-based systems, and (2) the problem of the do-main dependence (Carreras and Marquez, 2005).In the CoNLL-05 shared task, participating sys-tems showed about 10 points F-score differencebetween in-domain and out-of-domain test data.Concerning (1), we focus on selectional prefer-ences as the strongest candidate for informativesemantic features.
Concerning (2), the corpus-based similarity metrics that we use for selec-tional preference induction open up interestingpossibilities of mixing domains.We evaluate the similarity-based modelagainst Resnik?s WordNet-based model as wellas the EM-based clustering approach.
In theevaluation, the similarity-model shows lower er-ror rates than both Resnik?s WordNet-basedmodel and the EM-based clustering model.However, the EM-based clustering model hashigher coverage than both other paradigms.Plan of the paper.
After discussing previ-216ous approaches to selectional preference induc-tion in Section 2, we introduce the similarity-based model in Section 3.
Section 4 describesthe data used for the experiments reported inSection 5, and Section 6 concludes.2 Related WorkSelectional restrictions and selectional prefer-ences that predicates impose on their argumentshave long been used in semantic theories, (seee.g.
(Katz and Fodor, 1963; Wilks, 1975)).
Theinduction of selectional preferences from corpusdata was pioneered by Resnik (1996).
All sub-sequent approaches have followed the same two-step procedure, first collecting argument head-words from a corpus, then generalizing over theseen headwords to similar words.
Resnik usesthe WordNet noun hierarchy for generalization.His information-theoretic approach models theselectional preference strength of an argumentposition1 rp of a predicate p asS(rp) =?cP (c|rp) logP (c|rp)P (c)where the c are WordNet synsets.
The prefer-ence that rp has for a given synset c0, the selec-tional association between the two, is then de-fined as the contribution of c0 to rp?s selectionalpreference strength:A(rp, c0) =P (c0|rp) logP (c0|rp)P (c0)S(rp)Further WordNet-based approaches to selec-tional preference induction include Clark andWeir (2001), and Abe and Li (1993).
Brock-mann and Lapata (2003) perform a comparisonof WordNet-based models.Rooth et al (1999) generalize over seen head-words using EM-based clustering rather thanWordNet.
They model the probability of a wordw occurring as the argument rp of a predicate pas being independently conditioned on a set ofclasses C:P (rp, w) =?c?CP (c, rp, w) =?c?CP (c)P (rp|c)P (w|c)1We write rp to indicate predicate-specific roles, like?the direct object of catch?, rather than just ?obj?.The parameters P (c), P (rp|c) and P (w|c) areestimated using the EM algorithm.While there have been no isolated compar-isons of the two generalization paradigms thatwe are aware of, Gildea and Jurafsky?s (2002)task-based evaluation has found clustering-based approaches to have better coverage thanWordNet generalization, that is, for a given rolethere are more words for which they can state apreference.3 ModelThe approach we are proposing makes use oftwo corpora, a primary corpus and a gener-alization corpus (which may, but need not, beidentical).
The primary corpus is used to extracttuples (p, rp, w) of a predicate, an argumentposition and a seen headword.
The general-ization corpus is used to compute a corpus-basedsemantic similarity metric.Let Seen(rp) be the set of seen headwords foran argument rp of a predicate p. Then we modelthe selectional preference S of rp for a possibleheadword w0 as a weighted sum of the similari-ties between w0 and the seen headwords:Srp(w0) =?w?Seen(rp)sim(w0, w) ?
wtrp(w)sim(w0, w) is the similarity between the seenand the potential headword, and wtrp(w) is theweight of seen headword w.Similarity sim(w0, w) will be computed onthe generalization corpus, again on the ba-sis of extracted tuples (p, rp, w).
We willbe using the similarity metrics shown in Ta-ble 1: Cosine, the Dice and Jaccard coefficients,and Hindle?s (1990) and Lin?s (1998) mutualinformation-based metrics.
We write f for fre-quency, I for mutual information, and R(w) forthe set of arguments rp for which w occurs as aheadword.In this paper we only study corpus-based met-rics.
The sim function can equally well be in-stantiated with a WordNet-based metric (foran overview see Budanitsky and Hirst (2006)),but we restrict our experiments to corpus-basedmetrics (a) in the interest of greatest possible217simcosine(w,w?)
=Prpf(w,rp)?f(w?,rp)qPrpf(w,rp)2?qPrpf(w?,rp)2simDice(w,w?)
=2?|R(w)?R(w?)||R(w)|+|R(w?)|simLin(w,w?)
=Prp?R(w)?R(w?
)I(w,r,p)I(w?,r,p)Prp?R(w)I(w,r,p)Prp?R(w)I(w?,r,p) simJaccard(w,w?)
= |R(w)?R(w?)||R(w)?R(w?)|simHindle(w,w?)
=?rp simHindle(w,w?, rp) wheresimHindle(w,w?, rp) =??
?min(I(w,rp),I(w?,rp) if I(w, rp) > 0 and I(w?, rp) > 0abs(max(I(w,rp),I(w?,rp))) if I(w, rp) < 0 and I(w?, rp) < 00 elseTable 1: Similarity measures usedresource-independence and (b) in order to beable to shape the similarity metric by the choiceof generalization corpus.For the headword weights wtrp(w), the sim-plest possibility is to assume a uniform weightdistribution, i.e.
wtrp(w) = 1.
In addition, wetest a frequency-based weight, i.e.
wtrp(w) =f(w, rp), and inverse document frequency, whichweighs a word according to its discriminativity:wtrp(w) = lognum.
wordsnum.
words to whose context w belongs .This similarity-based model of selectionalpreferences is a straightforward implementa-tion of the idea of generalization from seenheadwords to other, similar words.
Like theclustering-based model, it is not tied to theavailability of WordNet or any other manuallycreated resource.
The model uses two corpora,a primary corpus for the extraction of seen head-words and a generalization corpus for the com-putation of semantic similarity metrics.
Thisgives the model flexibility to influence the simi-larity metric through the choice of text domainof the generalization corpus.Instantiation used in this paper.
Our aimis to compute selectional preferences for seman-tic roles.
So we choose a particular instantia-tion of the similarity-based model that makesuse of the fact that the two-corpora approachallows us to use different notions of ?predicate?and ?argument?
in the primary and general-ization corpus.
Our primary corpus will con-sist of manually semantically annotated data,and we will use semantic verb classes as pred-icates and semantic roles as arguments.
Ex-amples of extracted (p, rp, w) tuples are (Moral-ity evaluation, Evaluee, gamblers) and (Placing,Goal, briefcase).
Semantic similarity, on theother hand, will be computed on automaticallysyntactically parsed corpus, where the predi-cates are words and the arguments are syntac-tic dependents.
Examples of extracted (p, rp, w)tuples from the generalization corpus include(catch, obj, frogs) and (intervene, in, deal).2This instantiation of the similarity-basedmodel allows us to compute word sense specificselectional preferences, generalizing over manu-ally semantically annotated data using automat-ically syntactically annotated data.4 DataWe use FrameNet (Baker et al, 1998), a se-mantic lexicon for English that groups wordsin semantic classes called frames and lists se-mantic roles for each frame.
The FrameNet1.3 annotated data comprises 139,439 sentencesfrom the British National Corpus (BNC).
Forour experiments, we chose 100 frame-specific se-mantic roles at random, 20 each from five fre-quency bands: 50-100 annotated occurrencesof the role, 100-200 occurrences, 200-500, 500-1000, and more than 1000 occurrences.
Theannotated data for these 100 roles comprised59,608 sentences, our primary corpus.
To deter-mine headwords of the semantic roles, the cor-pus was parsed using the Collins (1997) parser.Our generalization corpus is the BNC.
It wasparsed using Minipar (Lin, 1993), which is con-siderably faster than the Collins parser butfailed to parse about a third of all sentences.2For details about the syntactic and semantic analysesused, see Section 4.218Accordingly, the arguments r extracted fromthe generalization corpus are Minipar depen-dencies, except that paths through prepositionnodes were collapsed, using the preposition asthe dependency relation.
We obtained parses for5,941,811 sentences of the generalization corpus.The EM-based clustering model was com-puted with all of the FrameNet 1.3 data (139,439sentences) as input.
Resnik?s model was trainedon the primary corpus (59,608 sentences).5 ExperimentsIn this section we describe experiments com-paring the similarity-based model for selectionalpreferences to Resnik?s WordNet-based modeland to an EM-based clustering model3.
For thesimilarity-based model we test the five similar-ity metrics and three weighting schemes listedin section 3.Experimental designLike Rooth et al (1999) we evaluate selectionalpreference induction approaches in a pseudo-disambiguation task.
In a test set of pairs(rp, w), each headword w is paired with a con-founder w?
chosen randomly from the BNC ac-cording to its frequency4.
Noun headwords arepaired with noun confounders in order not todisadvantage Resnik?s model, which only workswith nouns.
The headword/confounder pairsare only computed once and reused in all cross-validation runs.
The task is to choose the morelikely role headword from the pair (w,w?
).In the main part of the experiment, we counta pair as covered if both w and w?
are assignedsome level of preference by a model (?full cover-age?).
We contrast this with another condition,where we count a pair as covered if at least oneof the two words w,w?
is assigned a level of pref-erence by a model (?half coverage?).
If only oneis assigned a preference, that word is counted aschosen.To test the performance difference betweenmodels for significance, we use Dietterich?s3We are grateful to Carsten Brockmann and DetlefPrescher for the use of their software.4We exclude potential confounders that occur lessthan 30 or more than 3,000 times.Error Rate CoverageCosine 0.2667 0.3284Dice 0.1951 0.3506Hindle 0.2059 0.3530Jaccard 0.1858 0.3506Lin 0.1635 0.2214EM 30/20 0.3115 0.5460EM 40/20 0.3470 0.9846Resnik 0.3953 0.3084Table 2: Error rate and coverage (micro-average), similarity-based models with uniformweights.5x2cv (Dietterich, 1998).
The test involvesfive 2-fold cross-validation runs.
Let di,j (i ?
{1, 2}, j ?
{1, .
.
.
, 5}) be the difference in errorrates between the two models when using spliti of cross-validation run j as training data.
Lets2j = (d1,j?
d?j)2+(d2,j?
d?j)2 be the variance forcross-validation run j, with d?j =d1,j+d2,j2 .
Thenthe 5x2cv t?
statistic is defined ast?
=d1,1?15?5j=1 s2jUnder the null hypothesis, the t?
statistic hasapproximately a t distribution with 5 degrees offreedom.5Results and discussionError rates.
Table 2 shows error rates andcoverage for the different selectional prefer-ence induction methods.
The first five mod-els are similarity-based, computed with uniformweights.
The name in the first column is thename of the similarity metric used.
Next comeEM-based clustering models, using 30 (40) clus-ters and 20 re-estimation steps6, and the lastrow lists the results for Resnik?s WordNet-basedmethod.
Results are micro-averaged.The table shows very low error rates for thesimilarity-based models, up to 15 points lowerthan the EM-based models.
The error rates5Since the 5x2cv test fails when the error rates varywildly, we excluded cases where error rates differ by 0.8or more across the 10 runs, using the threshold recom-mended by Dietterich.6The EM-based clustering software determines goodvalues for these two parameters through pseudo-disambiguation tests on the training data.219Cos Dic Hin Jac Lin EM 40/20 ResnikCos -16 (73) -12 (73) -18 (74) -22 (57) 11 (67) 11 (74)Dic 16 (73) 2 (74) -8 (85) -10 (64) 39 (47) 27 (62)Hin 12 (73) -2 (74) -8 (75) -11 (63) 33 (57) 16 (67)Jac 18 (74) 8 (85) 8 (75) -7 (68) 42 (45) 30 (62)Lin 22 (57) 10 (64) 11 (63) 7 ( 68) 29 (41) 28 (51)EM 40/20 -11 ( 67 ) -39 ( 47 ) -33 ( 57 ) -42 ( 45 ) -29 ( 41 ) 3 ( 72 )Resnik -11 (74) -27 (62) -16 (67) -30 (62) -28 (51) -3 (72)Table 3: Comparing similarity measures: number of wins minus losses (in brackets non-significantcases) using Dietterich?s 5x2cv; uniform weights; condition (1): both members of a pair must becovered00.050.10.150.20.250.30.350.40  100  200  300  400  500error_ratenumhwLearning curve: num.
headwords, sim_based-Jaccard-Plain, error_rate, allMon Apr 09 02:30:47 20071000-100-200500-1000200-50050-100Figure 1: Learning curve: seen headwords ver-sus error rate by frequency band, Jaccard, uni-form weights50-100 100-200 200-500 500-1000 1000-Cos 0.3167 0.3203 0.2700 0.2534 0.2606Jac 0.1802 0.2040 0.1761 0.1706 0.1927Table 4: Error rates for similarity-based mod-els, by semantic role frequency band.
Micro-averages, uniform weightsof Resnik?s model are considerably higher thanboth the EM-based and the similarity-basedmodels, which is unexpected.
While EM-basedmodels have been shown to work better in SRLtasks (Gildea and Jurafsky, 2002), this has beenattributed to the difference in coverage.In addition to the full coverage condition, wealso computed error rate and coverage for thehalf coverage case.
In this condition, the errorrates of the EM-based models are unchanged,while the error rates for all similarity-basedmodels as well as Resnik?s model rise to valuesbetween 0.4 and 0.6.
So the EM-based modeltends to have preferences only for the ?right?words.
Why this is so is not clear.
It may be agenuine property, or an artifact of the FrameNetdata, which only contains chosen, illustrativesentences for each frame.
It is possible thatthese sentences have fewer occurrences of highlyfrequent but semantically less informative roleheadwords like ?it?
or ?that?
exactly because oftheir illustrative purpose.Table 3 inspects differences between errorrates using Dietterich?s 5x2cv, basically confirm-ing Table 2.
Each cell shows the wins minuslosses for the method listed in the row whencompared against the method in the column.The number of cases that did not reach signifi-cance is given in brackets.Coverage.
The coverage rates of thesimilarity-based models, while comparableto Resnik?s model, are considerably lower thanfor EM-based clustering, which achieves goodcoverage with 30 and almost perfect coveragewith 40 clusters (Table 2).
While peculiaritiesof the FrameNet data may have influenced theresults in the EM-based model?s favor (see thediscussion of the half coverage condition above),the low coverage of the similarity-based modelsis still surprising.
After all, the generalizationcorpus of the similarity-based models is farlarger than the corpus used for clustering.Given the learning curve in Figure 1 it isunlikely that the reason for the lower cover-age is data sparseness.
However, EM-basedclustering is a soft clustering method, whichrelates every predicate and every headword toevery cluster, if only with a very low probabil-220ity.
In similarity-based models, on the otherhand, two words that have never been seen inthe same argument slot in the generalizationcorpus will have zero similarity.
That is, asimilarity-based model can assign a level ofpreference for an argument rp and word w0 onlyif R(w0) ?
R(Seen(rp)) is nonempty.
Since theflexibility of similarity-based models extends tothe vector space for computing similarities, oneobvious remedy to the coverage problem wouldbe the use of a less sparse vector space.
Giventhe low error rates of similarity-based models,it may even be advisable to use two vectorspaces, backing off to the denser one for wordsnot covered by the sparse but highly accuratespace used in this paper.Parameters of similarity-based models.Besides the similarity metric itself, which we dis-cuss below, parameters of the similarity-basedmodels include the number of seen headwords,the weighting scheme, and the number of similarwords for each headword.Table 4 breaks down error rates by semanticrole frequency band for two of the similarity-based models, micro-averaging over roles of thesame frequency band and over cross-validationruns.
As the table shows, there was some vari-ation across frequency bands, but not as muchas between models.The question of the number of seen headwordsnecessary to compute selectional preferences isfurther explored in Figure 1.
The figure chartsthe number of seen headwords against error ratefor a Jaccard similarity-based model (uniformweights).
As can be seen, error rates reach aplateau at about 25 seen headwords for Jaccard.For other similarity metrics the result is similar.The weighting schemes wtrp had surprisinglylittle influence on results.
For Jaccard similar-ity, the model had an error rate of 0.1858 foruniform weights, 0.1874 for frequency weight-ing, and 0.1806 for discriminativity.
For othersimilarity metrics the results were similar.A cutoff was used in the similarity-basedmodel: For each seen headword, only the 500most similar words (according to a given sim-ilarity measure) were included in the computa-Cos Dic Hin Jac Lin(a) Freq.
sim.
1889 3167 2959 3167 860(b) Freq.
wins 65% 73% 79% 72% 58%(c) Num.
sim.
81 60 67 60 66(d) Intersec.
7.3 2.3 7.2 2.1 0.5Table 5: Comparing sim.
metrics: (a) avg.
freq.of similar words; (b) % of times the more fre-quent word won; (c) number of distinct similarwords per seen headword; (d) avg.
size of inter-section between rolestion; for all others, a similarity of 0 was assumed.Experiments testing a range of values for thisparameter show that error rates stay stable forparameter values ?
200.So similarity-based models seem not overlysensitive to the weighting scheme used, the num-ber of seen headwords, or the number of similarwords per seen headword.
The difference be-tween similarity metrics, however, is striking.Differences between similarity metrics.As Table 2 shows, Lin and Jaccard worked best(though Lin has very low coverage), Dice andHindle not as good, and Cosine showed the worstperformance.
To determine possible reasons forthe difference, Table 5 explores properties of thefive similarity measures.Given a set S = Seen(rp) of seen headwordsfor some role rp, each similarity metric producesa set like(S) of words that have nonzero simi-larity to S, that is, to at least one word in S.Line (a) shows the average frequency of wordsin like(S).
The results confirm that the Linand Cosine metrics tend to propose less frequentwords as similar.Line (b) pursues the question of the frequencybias further, showing the percentage of head-word/confounder pairs for which the more fre-quent of the two words ?won?
in the pseudo-disambiguation task (using uniform weights).This it is an indirect estimate of the frequencybias of a similarity metric.
Note that the head-word actually was more frequent than the con-founder in only 36% of all pairs.These first two tests do not yield any expla-nation for the low performance of Cosine, as theresults they show do not separate Cosine from221Jaccard CosineRide vehicle:Vehicle truck 0.05 boat 0.05coach 0.04 van 0.04 ship 0.04 lorry 0.04 crea-ture 0.04 flight 0.04 guy 0.04 carriage 0.04 he-licopter 0.04 lad 0.04Ingest substance:Substance loaf 0.04 icecream 0.03 you 0.03 some 0.03 that 0.03 er0.03 photo 0.03 kind 0.03 he 0.03 type 0.03thing 0.03 milk 0.03Ride vehicle:Vehicle it 1.18 there 0.88 they0.43 that 0.34 i 0.23 ship 0.19 second one 0.19machine 0.19 e 0.19 other one 0.19 response0.19 second 0.19Ingest substance:Substance there 1.23that 0.50 object 0.27 argument 0.27 theme0.27 version 0.27 machine 0.26 result 0.26response 0.25 item 0.25 concept 0.25 s 0.24Table 6: Highest-ranked induced headwords (seen headwords omitted) for two semantic classes ofthe verb ?take?
: similarity-based models, Jaccard and Cosine, uniform weights.all other metrics.
Lines (c) and (d), however, dojust that.
Line (c) looks at the size of like(S).Since we are using a cutoff of 500 similar wordscomputed per word in S, the size of like(S) canonly vary if the same word is suggested as similarfor several seen headwords in S. This way, thesize of like(S) functions as an indicator of thedegree of uniformity or similarity that a sim-ilarity metric ?perceives?
among the membersof S. To facilitate comparison across frequencybands, line (c) normalizes by the size of S, show-ing |like(S)||S| micro-averaged over all roles.
Herewe see that Cosine seems to ?perceive?
consid-erably less similarity among the seen headwordsthan any of the other metrics.Line (d) looks at the sets s25(r) of the 25 mostpreferred potential headwords of roles r, show-ing the average size of the intersection s25(r) ?s25(r?)
between two roles (preferences computedwith uniform weights).
It indicates another pos-sible reason for Cosine?s problem: Cosine seemsto keep proposing the same words as similar fordifferent roles.
We will see this tendency also inthe sample results we discuss next.Sample results.
Table 6 shows samples ofheadwords induced by the similarity-basedmodel for two FrameNet senses of the verb?take?
: Ride vehicle (?take the bus?)
and In-gest substance (?take drugs?
), a semantic classthat is exclusively about ingesting controlledsubstances.
The semantic role Vehicle of theRide vehicle frame and the role Substance of In-gest substance are both typically realized as thedirect object of ?take?.
The table only showsnew induced headwords; seen headwords wereomitted from the list.The particular implementation of thesimilarity-based model we have chosen, usingframes and roles as predicates and argumentsin the primary corpus, should enable the modelto compute preferences specific to word senses.The sample in Table 6 shows that this is indeedthe case: The preferences differ considerablyfor the two senses (frames) of ?take?, at leastfor the Jaccard metric, which shows a clearpreference for vehicles for the Vehicle role.
TheSubstance role of Ingest substance is harder tocharacterize, with very diverse seen headwordssuch as ?crack?, ?lines?, ?fluid?, ?speed?.While the highest-ranked induced words forJaccard do include three food items, there isno word, with the possible exception of ?icecream?, that could be construed as a controlledsubstance.
The induced headwords for theCosine metric are considerably less pertinentfor both roles and show the above-mentionedtendency to repeat some high-frequency words.The inspection of ?take?
anecdotally con-firms that different selectional preferences arelearned for different senses.
This point (whichcomes down to the usability of selectional pref-erences for WSD) should be verified in an em-pirical evaluation, possibly in another pseudo-disambiguation task, choosing as confoundersseen headwords for other senses of a predicateword.6 ConclusionWe have introduced the similarity-based modelfor inducing selectional preferences.
Comput-ing selectional preference as a weighted sum ofsimilarities to seen headwords, it is a straight-222forward implementation of the idea of general-ization from seen headwords to other, similarwords.
The similarity-based model is particu-larly simple and easy to compute, and seems notvery sensitive to parameters.
Like the EM-basedclustering model, it is not dependent on lexicalresources.
It is, however, more flexible in that itinduces similarities from a separate generaliza-tion corpus, which allows us to control the simi-larities we compute by the choice of text domainfor the generalization corpus.
In this paper wehave used the model to compute sense-specificselectional preferences for semantic roles.In a pseudo-disambiguation task the simila-rity-based model showed error rates down to0.16, far lower than both EM-based clusteringand Resnik?s WordNet model.
However its cov-erage is considerably lower than that of EM-based clustering, comparable to Resnik?s model.The most probable reason for this is the spar-sity of the underlying vector space.
The choiceof similarity metric is critical in similarity-basedmodels, with Jaccard and Lin achieving the bestperformance, and Cosine surprisingly bringingup the rear.Next steps will be to test the similarity-basedmodel ?in vivo?, in an SRL task; to test themodel in a WSD task; to evaluate the model ona primary corpus that is not semantically ana-lyzed, for greater comparability to previous ap-proaches; to explore other vector spaces to ad-dress the coverage issue; and to experiment ondomain transfer, using an appropriate general-ization corpus to induce selectional preferencesfor a domain different from that of the primarycorpus.
This is especially relevant in view of thedomain-dependence problem that SRL faces.Acknowledgements Many thanks to JasonBaldridge, Razvan Bunescu, Stefan Evert, RayMooney, Ulrike and Sebastian Pado?, and SabineSchulte im Walde for helpful discussions.ReferencesN.
Abe and H. Li.
1993.
Learning word associationnorms using tree cut pair models.
In Proceedings ofICML 1993.C.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet project.
In Proceedings of COLING-ACL1998, Montreal, Canada.C.
Brockmann and M. Lapata.
2003.
Evaluating andcombining approaches to selectional preference acqui-sition.
In Proceedings of EACL 2003, Budapest.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of semantic distance.
ComputationalLinguistics, 32(1).X.
Carreras and L. Marquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL-05, Ann Arbor, MI.S.
Clark and D. Weir.
2001.
Class-based probabilityestimation using a semantic hierarchy.
In Proceedingsof NAACL 2001, Pittsburgh, PA.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of ACL 1997,Madrid, Spain.T.
Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning algo-rithms.
Neural Computation, 10:1895?1923.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245?288.D.
Hindle and M. Rooth.
1993.
Structural ambiguity andlexical relations.
Computational Linguistics, 19(1).D.
Hindle.
1990.
Noun classification from predicate-argument structures.
In Proceedings of ACL 1990,Pittsburg, Pennsylvania.J.
Katz and J. Fodor.
1963.
The structure of a semantictheory.
Language, 39(2).D.
Lin.
1993.
Principle-based parsing without overgen-eration.
In Proceedings of ACL 1993, Columbus, OH.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL 1998,Montreal, Canada.D.
McCarthy and J. Carroll.
2003.
Disambiguatingnouns, verbs and adjectives using automatically ac-quired selectional preferences.
Computatinal Linguis-tics, 29(4).P.
Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computational re-alization.
Cognition, 61:127?159.M.
Rooth, S. Riezler, D. Prescher, G. Carroll, and F. Beil.1999.
Inducing an semantically annotated lexicon viaEM-based clustering.
In Proceedings of ACL 1999,Maryland.Y.
Wilks.
1975.
Preference semantics.
In E. Keenan,editor, Formal Semantics of Natural Language.
Cam-bridge University Press.223
