Reinterpretation of an existing?NLG system in a Generic GenerationArchitectureL.
Cahill, C. Doran~ R. Evans, C. Meilish, D. Paiva,:M. Reape, D. Scott,, N. Tipper.Universities of Brighton and Edinburgh.Email rags@itri, brighton, ac.
ukAbstractThe RAGS project aims to define a reference ar-chitecture for Natural Language Generation (NLG)systems.
Currently the major part of this archi-tecture consists of a set of datatype definitions forspecifying the input and output formats for mod-ules within NLG systems.
In this paper we describeour efforts to reinterpret an existing NLG system interms of these definitions.
The system chosen wasthe Caption Generation System.2.
Which aspects of the RAGS repertoire would: .
.
.
.
.
.
.
.
- .... -,.= ., ~,~,aemaltybe'requireti~ftrr~strch~a-~reinterpretation;which would be unnecessary and which addi-tions to the RAGS repertoire would be moti-vated.1 IntroductionThe RAGS project ~ aims to define a reference ar-chitecture for natural anguage generation systems.Currently the major part of this architecture consistsof a set of datatype definitions for specifying theinput and output formats for modules within NLGsystems.
The intention is that such representationscan be used to assist in reusability of componentsof NLG systems.
System components that adhereto these representations, or use a format hat can betranslated into such representations relatively eas-ily, can then, in principle, be substituted into othersystems.
Also, individual components could be de-veloped without the need for a complete system ifdatasets, based on the representations, were madeavailable.In this paper we describe an attempt to reinterpretan existing NLG system in terms of the RAGS datadefinitions.
The point of this exercise was to lem-n:1.
Whether these data structures were sufficientto describe the input and output functionalityof an existing, independently developed, ap-3.
Whether studying the system would generategood ideas about possible reusable generationmodules that could be developed.In this exercise it was important o choose a sys-tem that had been developed by people outside theRAGS project.
Equally, it was important o havesufficient clear information about the system in theavailable literature, and/or by means of personalcontact with the developers.
The system chosen wasthe Caption Generation System (Mittal et al, 1995;Mittal et al, 1998) 3.
This system was chosen be-cause, as well as fulfilling the criteria above, it ap-peared to be a relatively simple pipeline, thus avoid-ing complex control issues, with individual modulesperforming the varied linguistic tasks that the RAGSdata structures had been designed to handle.The reinterpretation exercise took the form ofcoming up with an account of how the interfacesto the CGS modules corresponded to the RAGSmodel and reimplementing a working version ofeach module (apart from Text Planning and Realisa-tion) which was tested to ensure that, given appro-priate input, its output was correct (i.e.
conformingto the global account) on key examples.
Naturally,given the scope of this exercise, we had to gloss oversome interesting implementational issues.
The aimwas not to produce a complete system or a systemas good as CGS, but merely to demonstrate hat thebroad functionality of the system could be repro-plied 2 NLG system.?
Now at the MITRE Corporation, Bedford, MA, USA,cdoran.
?mitre, org.tThis work was supported by ESPRC grants GR/L77041(Edinburgh) and GR/L77102 (Brighton), RAGS: Reference Ar-chitecture for Generation Systems.-'See (Paiva, 1998) for a definition of applied in this specificcontext."
.
-ducedwithin:the RAGS .structures.In this paper we first describe the RAGS datastructures.
We then describe the CGS system3In addition to these published sources, we were greatlyhelped by the developers of the system who gave us the ben-efit of their own expertise as well as access to the original codeof the system and a technical report hat included implementa-tional details such as system traces.69followed by our reinterpretation of the system in Abstract Rhetorical Abstract Rhetorical Repre-RAGS terms.
Finally we discuss,, the :implications:.
:.
-._..sentations ,are--tree-structures with,rhetorical .rela-for RAGS of this exercise, tions at the internal nodes and Abstract Rhetorical2 The RAGS datatypesThe RAGS project initially set out to develop a ref-erence architecture based on the three-stage pipelinesuggested by Reiter (Reiter, 1994).
However, atrees or Abstract Semantic Representations at theleaves.Rhetorical Abstract Rhetorical Representationsare viewed as descriptions of sets of possibleRhetorical Representations.
Each one may be trans-detailed analysis of existing applied NLG systems formed into some subset of the possible Rhetori-(Cahill and Reape~_~ l:998}:suggested~,that~ttch.an~ ar -~: ~<.eaLReprese, ntations by,,means ~ofa,set..o_f~.petmittedchitecture was not specific enough and not closely transformations, e.g.
reversing the order of nucleusenough adhered to by the majority of the systemssurveyed for this to be used as the basis of the archi-tecture.The abstract functionality of a generation systemcan be specified without specific reference to pro-cessing.
The RAGS approach to this is to develop adata model, that is, to define the functional modulesentirely in terms of the datatypes they manipulateand the operations they can perform on them.
Ontop of such a model, more specific process modelscan be created in terms of constraints on the orderand level of instantiation of different ypes of data inthe data model.
A 'rational reconstnaction' of somepipeline model might then be produced, but otherprocess models would also be possible.The RAGS levels of representation are as fol-lows4:Conceptual The conceptual level of representa-tion is defined only indirectly through an API viawhich a knowledge base (providing the contentfrom which generation takes place) can be viewedas if it were defined in a simple KL-ONE (Brach-man and Schmolze, 1985) like system.Abstract Semantic Abstract semantic representa-tions are the first level at which semantic predicatesare associated with arguments.
At this level, seman-tic predicates and roles are those used in the API toquery the knowledge base and arguments are knowl-edge base entities.Semantic (Concrete) semantic representationsprovide a complete notation for "logical forms"where there is no longer any reference to ,the knowl-edge base.
The representations are based on sys-tems such as SPL (Kasper, 1989) and DRT (Kampand Reyle, 1993).4More details can be found in (Cahill etal., 1999) and at the RAGS project web site:ht tp  : / /www.
i t r i  .
b r ighton ,  ac.
uk/rags.and satellite or changing the rhetorical relation toone within a permitted set.Abstract Document Document structure definesthe linear ordering of the constituents of the Rhetor-ical Representation with a POSITION feature, aswell as two other features, TEXT-LEVEL, whichtakes values such as paragraph or sentence; andLAYOUT, which takes values such as wrapped-textand vertical list.
It takes the form of a tree, usu-ally, but not necessarily, isomorphic to the Rhetor-ical Representation a d linked to it, but with thesethree features at the nodes instead of rhetorical rela-tions.Abstract Syntactic Abstract Syntactic Represen-tations capture high-level aspects of syntactic struc-ture in terms of notions such as lexical head, speci-fiers, modifiers and complements.
This level of rep-resentation is compatible with approaches such asLFG f-structure, HPSG and Meteer's Text Structure.3 Partial and Mixed RepresentationsFor all of the RAGS levels partial representationsare possible.
Without this, it is not possible for amodule to pass any result to another until that re-sult is completely determined, and this would im-pose an unwanted bias towards simple pipeline ar-chitectures into the model.
There are many casesin NLG where a representation is built collabora-tively by several modules.
For instance, many sys-tems have a referring expression generation modulewhose task is to complete a semantic representationwhich lacks those structures which will be realisedas NPs.
Such a functionality cannot be describedunless partially complete semantic representationscan be communicated.In addition, mixed representations are possible,where (possibly partial) representations at severallevels are combined with explicit links between theelements.
Many NLG modules have to be sensi-70tive to a number of levels at once (consider, for.......... instance, -aggregatiomxeferring,expmssion.,genera-tion and lexicalisation, all of which need to takeinto account rhetorical, semantic and syntactic on-straints).
The input to most reusable realisation sys-tems is also best viewed as a mixture of semanticand abstract syntactic information.The extra flexibility of having partial and mixedrepresentations turned out to be vital in the recon-struction of the CGS system.
(Mellish et al, 2000).4 The CGS systemThe Caption Generation System (CGS) generatesexplanatory captions of graphical presentations (2-D charts and graphs).
Its architecture is a pipelinewith several modules, shown in the left hand part ofFigure 1.
An example of a diagram and its accom-panying text are given in Figure 2.
The propositionsare numbered for ease of reference throughout thepaper.The input to CGS is a picture representation(graphical elements and its mapping from the dataset) generated by SAGE plus its complexity metric.The text planning module (Moore and Paris (1993))plans an explanation i  terms of high level discoursegoals.
The output of the planner is a partially or-dered plan with speech-acts as leaves.The ordering module receives as input the dis-course plan with links specifying the ordering re-lations between sub-trees and specifies an order forthem based on heuristics uch as that the descriptionshould be done from left to right in the visual space.The aggregation module "only conjoins pairs ofcontiguous propositions about the same graphemetype 5 in the same space" (Mittai et al, 1999) andinserts cue phrases compatible with the propositionse o ( .=., "whereas" for contrastive ones).
The internalorder of the sentence constituents i determined bythe centering module using an extension of the cen-tering theory of Grosz and colleagues (Grosz et al,1995).The referring expression module uses Date andReiter's (Dale and Reiter, 1995) algorithm to con-struct the set of attributes that can uniquely identifya referent.
There are'two, situations where the textplanning module helps specifically in the generationof referring expressions: (1) when the complexityfor expressing a graphic demands an example and5"Graphemes are the basic building blocks for constructingpictures.
Marks, text, lines and bars are some of the differentgrapheme classes available in SAGE."
(IVlittal et al, 1999).CGS architectureSAGERAGS representationsI I I  I I I  IV  VI II HI  IV  V?
I I I  I I I  IV  " V .I I I  In  IV  vl -  ..........
I /111 It  I I I  IV Vl -  ..........I 11 11I IV V.......... III1I I I  HI  IV  V; "  ..........
I I I I IFUFFigure 1: A RAGS view of the CGS system.
Thelabels for the RAGS representations refer to the fol-lowing: I = conceptual; II = semantic; III = rhetori-cal; IV = document; V = syntactic.it signals this both to SAGE (for highlighting thecorresponding grapheme) and to the rest of the textgeneration modules; and (2) when in a specific sit-uation the referring algorithm would need severalinteractions for detecting that an entity is unique in?
a certain visual space and.the planning could detectit in the construction of the description of this space.When this occurs, the text planner "circumvents heproblem for the:.referring ,expression :module at theplanning stage itself, processing the speech-acts ap-propriately to avoid this situation completely".After lexicalisation, which adds lexeme and ma-jor category information, the resulting functionaldescriptions are passed to the FUF/SURGE realiserthat generates texts like the caption of Figure 2.71\ [ \ ]Ote lOIZlZS:3I21 ,:7-. ,,S .
.
.
.
; .
.
.
.
.'
?O ~ ~Ipc~ q~L~\]I\]=::::::;=a___.,____.__,_______~.
,  , : ,  ; .
.
,Figure 2: (1) These two charts present information about house sales from data-set ts-1740.
(2) In the twocharts, the y-axis indicates the houses.
(7) In the first chart, the left edge of the bar shows the house's ellingprice whereas (8) the right edge shows the asking price.
(3) The horizontal position of the mark shows theagency estimate.
(4) The color shows the neighbourhood and (5) shape shows the listing agency.
(6) Sizeshows the number of rooms.
(9) The second chart shows the number of days on the market.5 Reinterpretat ion f  CGS in RAGSOur reinterpretation f the CGS system defines theinterfaces between the modules of CGS in termsof the RAGS data structures discussed above.
Inthis section we discuss the input and output inter-faces for each CGS module in turn as well as anyproblems we encountered in mapping the structuresinto RAGS structures.
Figure 1 shows the incre-mental build-up of the RAGS data levels acrossthe pipeline.
Here we have collapsed the AbstractRhetorical and Rhetorical and the Abstract Seman-tic and Semantic.
It is-interesting to note that thebuild up of levels of representation does not tend tocorrespond exactly with module boundaries.One of the major issues we faced in' our reinter-pretation was where to produce representations (orpartial representations) whose emergence was notdefined clearly in the descriptions of CGS.
For in-stance, many decisions about document structureare made only implicitly by the system.
In mostcases we have opted to produce all types of repre-sentations at the earliest point where they can con-ceivably have any content.
This means, for instance,that our reimplementation assumes an (unimple-mented) text planner which produces an AbstractRhetorical Representation with Abstract Semanticleaves and an Abstract Document Representation.Text Planner The input to the Longbow text plan-ner discussed in section 4 above is a representationof a picture in SAGE format (which has been an-notated to indicate the types of complexity of eachgrapheme) together with a goal, which can typi-cally be interpreted as "describe".
It outputs an es-sentially fiat sequence of plan operators, each ofwhich corresponds in the output?
text .to .a.speechact.
In our reinterpretation, we have assumed thatthis fiat structure needs to be translated into an Ab-stract Rhetorical Representation with (at least) min-imal structure.
Such a structure is implicit in theplan steps, and our interpretation f the rhetoricalstructure for the example text corresponds closely tothat of the post-processing trace produced by CGS.72I .AYOI  FII" * 'upped tel l" IU  ,I.EVIZL.
p J t l~aph~ f ~ I O N :  2POSlllON II.AYOtr'I+: -~pped tellTEX"T.L~ VEL(1)POSITION: I POSITION: 2LAYOUT: *T~,pl~n.l teat"IEXT-LEVEL: +(2)Po$ : I POSITION: 1LAYOUT: -mtpFcd te~t.
TE.ICr-t.EVEL~ ?0OSFI-K~N.
2 PosmoN: iPOSIllON: I PosrnoN: I POsmoN.
~ FoSmON: 4 POSt'nON I PosrnoN: 2LAYOUT: ~pp~d lesl LAYOU'T.
~ppe,.f ~xt LAYO\[rF.
~apped lesl LAYOUT: ~+r~pS~d I?xt LAYOUT.
~'?~l~,Od ~est LAYOUT: ~Tappe~ textTEXT,LEVEL  7 "II~XT,LEVEI.
: ~ "II~XT-LEVEL ?
"I I~XT-LEVEL: ?
TEXT-LEVEL  "+ TIE~XT-L.EVI:I.: ?
(3) (4) (5) (6) (7) (8)Figure 3: Initial Document Structure.
.
.Z.,However, we are still not entirely sureexactly CGS creates this structure, soposed it at the very beginning, onto thetext planner.Already at this stage it is necessaryabout wherewe have im-output of theto make useof mixed RAGS representations.
As well as thisAbstract Rhetorical Representation, the text plannerhas to produce an Abstract Document Representa-tion, linked to the Abstract Rhetorical Representa-tion.
This is already partially ordered - although theexact value of POSITION features cannot be speci-fied at this stage, the document tree is constructedso that propositions are already grouped together.In addition, we make explicit certain default infor-mation that the CGS leaves implicit at this stage,namely, that the LAYOUT feature is always wrappedtext and that the TEXT-LEVEL feature of the topnode is always paragraph.Ordering The ordering module takes the AbstractDocument Representation a d the Abstract Rhetor-ical Representation as input and outputs an AbstractDocument Representation with the POSITION fea-ture 's  value filled,for all :the nodes, .That is, it fixes.
?the linear order of the final output of the speech acts.In our example, the ordering is changed so that steps7 and 8 are promoted to appear before 3, 4, 5 and 6.The resulting structure is shown in figure 36 .6In this and the.following diagrams, objects are representedby circles with (labelled) arrows indicating the relations be--Aggregation Although aggregation might seemlike a self-contained process within NLG, in prac-tice it can make changes at a number of levels ofrepresentation a d indeed it may be the last opera-tion that has an effect on several levels.
The aggre-gation module in our reinterpretation thus has the fi-nal responsibility to convert an Abstract RhetoricalRepresentation with Abstract Semantic Represen-tation leaves into a Rhetorical Representation withSemantic Representation leaves.
The new Rhetori-cal Representation may be different from before asa result of speech acts being aggregated but whetherdifferent or not, it can now be considered final asit will no longer be changed by the system.
Theresulting Semantic Representations are no longerAbstract because further structure may have beendetermined for arguments to predicates.
On theother hand, referring expressions have not yet beengenerated and so the (Concrete) Semantic Repre-sentations cannot be complete.
The reconstruc-,.tion createspartia.i Semantic Representations with"holes" where the referring expressions (SemanticRepresentations) will be inserted.
These "holes" arelinked back to the knowledge base entities tfiat theycorrespond to.Because Aggregation affects text levels, it also af-fects the Abstract Document Representation, whichhas its TEXT-LEVEL feature's values all filled at thistween them.
Dashed arrows indicate links between differentlevels of representation.73SemRepfun(Role,SemRep)DR preS  , .
mRep?
AbsSynRep ~ AbsSynRe~/ " \  Y^.Z("FVM (~ ~ ,un(Funs,~gS~c) (,~ ~M (~) lun(Funs.ArgSpec)0 ~ 0 0?
+Adjs.
.
- ; .
.Figure 4: Syntactic representations constructed by Centeringpoint.
It may also need to change the structureof the Abstract Document Representation, for in-stance, adding in a node for a sentence above two,now aggregated, clause nodes.Centering Because Centering comes before Re-ferring Expression generation and Realisation, all itcan do is establish constraints that must be heededby the later modules.
At one stage, it seemed as ifthis required communicating a kind of informationthat was not covered by the RAGS datatypes.
How-ever, the fact that an NP corresponds (or not) to acenter of some kind can be regarded as a kind ofabstract syntactic information.
The reconstructiontherefore has the centering module building a partial(unconnected) Abstract Syntactic representation foreach Semantic Representation that will be realisedas an NP, inserting a feature that specifies whetherit constitutes a forward- or backward-facing cen-ter, approximately following Grosz et al(Grosz etal., 1995).
This information is used to determinewhether active or passive voice will be used.
Anexample of such a partial Abstract Syntactic Repre-sentation is given in Figure 4.Referring Expression In our reconstruction ofthe CGS system, we have deviated from reproduc-ing the exact functionality for the referring expres-sion module and part of the lexical choice module.In the CGS system, the referring expression modulecomputes association lists which can be used by thelexical choice module to construct referring expres-sions suitable for realisation.
In our reconstruction,however, the referring expression module directlycomputes the Semantic Representations of referringexpressions.We believe that this is a good example of acase where developing a system with the RAGSdata structures in mind simplifies the task.
Thereare undoubtedly many different ways in which thesame results could be achieved, and there are many(linguistic, engineering etc.)
reasons for choosingone rather than another.
Our particular choice isdriven by the desire for conceptual simplicity, ratherthan any strictly linguistic or computational motiva-tions.
We considered for each module which RAGSlevel(s) it contributed to and then implemented it tomanipulate that (or those) level(s).
In this case, thatmeant a much more conceptually simple modulewhich just adds information to the Semantic Rep-resentations.Lexical Choice In CGS, this module performs arange of tasks, including what we might call thelater.stages of_referring expression generation andlexical choice, before converting the plan leavesinto FDs (Functional Descriptions), which serve asthe input to the FUF/SURGE module.
In the re-construction, on the other hand, referring expres-sions have already been computed and the Rhetor-ical Representation, with its now complete Seman-tic Representations, needs to be "lexicalised" and74' ,t ~1" .
setFigure 5: Combined Semantic and Abstract Syntactic Representationtranslated into FUF/SURGE format.
Lexicalisa-tion in our terms involves adding the lexeme andmajor category information to the Abstract Syntac-tic Representations for the semantic predicates ineach Semantic Representation.
The FUF/SURGEinput format was regarded as a combination of Se-mantic and Abstract Syntactic information, and thiscan easily be produced from the RAGS representa-tions.
The combined Semantic and Abstract Syn-tactic Representations for the plan step "These twocharts present information about house sales fromdata set ts-1740" is shown in Figure 5.
The boxesindicate suppressed subgraphs of the lexemes cor-responding to the word in the boxes and trianglesindicate suppressed subgraphs of the two adjuncts.6 ConclusionsThe reconstruction of CGS has taken the form ofworking out in detail the RAGS representationspassed between modules at each stage for a setof key examples and reimplementing the modules(apart from the Planner and Realiser) in a way thatcorrectly reproduces these representations.
The ac-tual implementation used an incrementally growingdata store for the RAGS representations which themodules accessed in turn, though the passing of datacould also have been achieved in other ways.The fact that the reconstruction has been success-ful indicates that the RAGS architecture is broadlyadequate to redescribe this NLG system:?
No changes to the existing levels of represen-tation were needed, though it was necessary tomake extensive use of partial and mixed repre-sentations.o No new levels of representation needed to beintroduced to capture the inter-module com-munication of the system.o All of the levels of representation_apart fromthe Conceptual level were used significantly inthe reconstruction.In some ways, i t  is unfortunate that none of theinter-module interfaces of CGS turned out to use asingle level of RAGS representation.
Given the mo-tivation for partial and mixed representations above,however, this did not really come as a surprise.
Itmay well be that any really useful reusable modulesfor NLG will have to have this complexity.75In spite of the successful testing of the RAGS datamodel, somedifficulties were encountered:* It was difficult to determine the exact natureof the representations produced by the Planner,though in the end we were able to develop asystem to automatically translate these into aformat we could deal with.o Although the theoretical model o f  CGS has asimple modular structure, in practice the mod-ules are very tightly inte-gr~ifed and making-the "exact interfaces explicit was not always easy.?
Referring expression generation requires fur-ther access to the "knowledge base" holdinginformation about he graphic to be produced.This knowledge was only available via interac-tions with SAGE, and so it was not possible todetermine whether the RAGS view of Concep-tual Representations was applicable.
Our ownimplementation f referring expression gener-ation had to work around this problem in a non-portable way.?
It became clear that there are many housekeep-ing tasks that an NLG system must performfollowing Lexical Choice in order for the finalSemantic and Abstract Syntactic Representa-tions to be appropriate for direct input to a re-alisation system such as FUF.o The fact that the system was drivingFUF/SURGE seems to have had a signif-icant effect on the internal representationsused by CGS.
The reconstruction echoed thisand as a result may not be as general as couldbe desired.?
Even though CGS only performs imple typesof Aggregation, it is clear that this is a criticalmodule for determining the final form of sev-eral levels of representation.The division of CGS into modules is different fromthat used in any NLG systems we have previouslyworked on and so has been a useful stimulus to thinkabout ways in which reusable modules can be de-signed.
We envisage reusmgat  least,the reimple-mentation of the Centering module in our furtherwork.ReferencesR.
Brachman and J. Schmolze.
1985.
An overview of the KL-ONE knowledge representation system.
Cognitive Science,9:171-216.Lynne Cahill and Mike Reape.
1998.
Component asksin applied NLG .systems .
.
.
.
Technical Report ITR!-99-05, ITRI, University of Brighton.
obtainable athttp:/lwww.itri.brighton.ac.uk/projects/rags/.Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish,Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper.1999.
In Search of a Reference Architecture for NLG Sys-tems.
In Proceedings of the 7th European Workshop on Nat-ural Language Generation, pages 77-85, Toulouse.Robert Dale and Ehud Reiter.
1995.
Computational interpre-tations of the Gricean maxims in the generation ofreferringexpressions.
Cognitive Science, 18:233-263.B J .
Grosz, A/K.J6shil-and S.Weinstein.
1995~ Centering: aframework for modelling the local coherence of discourse.Computational Linguistics, 21 (2):203-226.H.
Kamp and U. Reyle.
1993.
From discourse to logic: Intro-duction to model theoretic semantics of natural language,formal logic and discourse representation theory.
Kluwer,Dordrecht; London.R.
T. Kasper.
1989.
A flexible interface for linking applica-tions to penman's sentence generator.
In Proceedings of theDARPA Speech and Natural Language Workshop, Philadel-phia.C.
Mellish, R. Evans, L. Cahill, C. Doran, D. Paiva, M. Reape,D.
Scott, and N. Tipper.
2000.
A representation forcomplexand evolving data dependencies in generation.
In Proceed-ings of the Applied Natural Language Processing (ANLP-NAACL2000) Conference, Seattle.V.
O. Mittal, S. Roth, J. D. Moore, J. Mattis, and G. Carenini.1995.
Generating explanatory captions for informationgraphics.
In Proceedings of the 15th International JointConference on Artificial Intelligence (IJCAI'95), pages1276-1283, Montreal, Canada, August.V.
O. Mittal, J. D. Moore, G. Carenini, and S. Roth.
1998.Describing complex charts in natural anguage: A captiongeneration system.
Computational Linguistics, 24(3):431-468.Daniel Paiva.
1998.
A survey of applied natural lan-guage generation systems.
Technical Report ITRI-98-03, Information Technology Research Insti-tute (ITRI), University of Brighton.
Available athttp://www.itri.brighton.ac.uk/techreports.Ehud Reiter.
1994.
Has a consensus NL generation architec-ture appeared and is it psycholinguistically p ausible?
InProceedings of the Seventh International Workshop on Nat-ural Language Generation, pages 163-170, Kennebunkport,Maine.AcknowledgementsWe would like to thank the numerous people who havehelped us in this work.
The developers of CGS, especiallyGiuseppe Carenini and Vibhu Mittal; the RAGS consultantsand other colleagues at Brighton and Edinburgh, who have con-tributed greatly to our development ofthe representations; andfinally to the anonymous reviewers of this paper.76
