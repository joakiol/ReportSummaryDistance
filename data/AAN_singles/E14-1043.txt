Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405?413,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsPredicting and Characterising User Impact on TwitterVasileios Lampos1, Nikolaos Aletras2, Daniel Preot?iuc-Pietro2and Trevor Cohn31Department of Computer Science, University College London2Department of Computer Science, University of Sheffield3Computing and Information Systems, The University of Melbournev.lampos@ucl.ac.uk, {n.aletras,d.preotiuc}@dcs.shef.ac.uk, trevor.cohn@gmail.comAbstractThe open structure of online social net-works and their uncurated nature give riseto problems of user credibility and influ-ence.
In this paper, we address the task ofpredicting the impact of Twitter users basedonly on features under their direct control,such as usage statistics and the text postedin their tweets.
We approach the problem asregression and apply linear as well as non-linear learning methods to predict a userimpact score, estimated by combining thenumbers of the user?s followers, followeesand listings.
The experimental results pointout that a strong prediction performance isachieved, especially for models based onthe Gaussian Processes framework.
Hence,we can interpret various modelling com-ponents, transforming them into indirect?suggestions?
for impact boosting.1 IntroductionOnline social networks have become a wide spreadmedium for information dissemination and inter-action between millions of users (Huberman et al.,2009; Kwak et al., 2010), turning, at the sametime, into a popular subject for interdisciplinaryresearch, involving domains such as Computer Sci-ence (Sakaki et al., 2010), Health (Lampos andCristianini, 2012) and Psychology (Boyd et al.,2010).
Open access along with the property of struc-tured content retrieval for publicly posted data havebrought the microblogging platform of Twitter intothe spotlight.Vast quantities of human-generated text froma range of themes, including opinions, news andeveryday activities, spread over a social network.Naturally, issues arise, like user credibility (Castilloet al., 2011) and content attractiveness (Suh et al.,2010), and quite often trustful or appealing informa-tion transmitters are identified by an impact assess-ment.1Intuitively, it is expected that user impactcannot be defined by a single attribute, but dependson multiple user actions, such as posting frequencyand quality, interaction strategies, and the text ortopics of the written communications.In this paper, we start by predicting user impactas a statistical learning task (regression).
For thatpurpose, we firstly define an impact score functionfor Twitter users driven by basic account proper-ties.
Afterwards, from a set of accounts, we mea-sure several publicly available attributes, such asthe quantity of posts or interaction figures.
Textualattributes are also modelled either by word frequen-cies or, more generally, by clusters of related wordswhich quantify a topic-oriented participation.
Themain hypothesis being tested is whether textualand non textual attributes encapsulate patterns thataffect the impact of an account.To model this data, we present a method basedon nonlinear regression using Gaussian Processes,a Bayesian non-parametric class of methods (Ras-mussen and Williams, 2006), proven more effec-tive in capturing the multimodal user features.
Themodelling choice of excluding components thatare not under an account?s direct control (e.g.
re-ceived retweets) combined with a significant userimpact prediction performance (r = .78) enabledus to investigate further how specific aspects of auser?s behaviour relate to impact, by examining theparameters of the inferred model.Among our findings, we identify relevant fea-tures for this task and confirm that consistent ac-tivity and broad interaction are deciding impactfactors.
Informativeness, estimated by computinga joint user-topic entropy, contributes well to theseparation between low and high impact accounts.Use case scenarios based on combinations of fea-tures are also explored, leading to findings such asthat engaging about ?serious?
or more ?light?
topicsmay not register a differentiation in impact.1For example, the influence assessment metric of Klout ?http://www.klout.com.4052 DataFor the experimental process of this paper, weformed a Twitter data set (D1) of more than 48 mil-lion tweets produced by |U | = 38, 020 users geolo-cated in the UK in the period between 14/04/2011and 12/04/2012 (both dates included, ?t = 365days).
D1 is a temporal subset of the data set usedfor modelling UK voting intentions in (Lampos etal., 2013).
Geolocation of users was carried outby matching the location field in their profile withUK city names on DBpedia as well as by check-ing that the user?s timezone is set to G.M.T.
(Routet al., 2013).
The use of a common greater geo-graphical area (UK) was essential in order to derivea data set with language and topic homogeneity.A distinct Twitter data set (D2) consisting of ap-prox.
400 million tweets was formed for learningterm clusters (Section 4.2).
D2 was retrieved fromTwitter?s Gardenhose stream (a 10% sample of theentire stream) from 02/01 to 28/02/2011.
D1 andD2 were processed using TrendMiner?s pipeline(Preot?iuc-Pietro et al., 2012).3 User Impact DefinitionOn the microblogging platform of Twitter, user ?or, in general, account ?
popularity is usually quan-tified by the raw number of followers (?in?
0),i.e.
other users interested in this account.
Likewise,a user can follow others, which we denote as his setof followees (?out?
0).
It is expected that userswith high numbers of followers are also popularin the real world, being well-known artists, politi-cians, brands and so on.
However, non popularentities, the majority in the social network, can alsogain a great number of followers, by exploiting,for example, a follow-back strategy.2Therefore,using solely the number of followers to quantifyimpact may lead to inaccurate outcomes (Cha et al.,2010).
A natural alternative, the ratio of ?in/?outis not a reliable metric, as it is invariant to scal-ing, i.e.
it cannot differentiate accounts of the type{?in, ?out} = {m,n} and {?
?
m, ?
?
n}.
Weresolve this problem by squaring the number offollowers(?2in/?out); note that the previous expres-sion is equal to (?in?
?out)?
(?in/?out) +?inandthus, it incorporates the ratio as well as the differ-ence between followers and followees.An additional impact indicator is the number oftimes an account has been listed by others (???
0).Lists provide a way to curate content on Twitter;thus, users included in many lists are attractors of2An account follows other accounts randomly expectingthat they will follow back.
?5 0 5 10 15 20 25 3000.050.10.15Impact Score (S)Probability Density@guardian@David_Cameron@PaulMasonNews@lampos@nikaletras@spam?Figure 1: Histogram of the user impact scores inour data set.
The solid black line represents a gen-eralised extreme value probability distribution fit-ted in our data, and the dashed line denotes themean impact score (= 6.776).
User @spam?
is asample account with ?in= 10, ?out= 1000 and?
?= 0; @lampos is a very active account, whereas@nikaletras is a regular user.interest.
Indeed, Pearson?s correlation between ?inand ?
?for all the accounts in our data set is equalto .765 (p < .001); the two metrics are correlated,but not entirely and on those grounds, it would bereasonable to use both for quantifying impact.Consequently, we have chosen to represent userimpact (S) as a log function of the number of fol-lowers, followees and listings, given byS(?in, ?out, ??)
= ln((?
?+ ?)
(?in+ ?
)2?out+ ?
),(1)where ?
is a smoothing constant set equal to 1 sothat the natural logarithm is always applied on areal positive number.
Figure 1 shows the impactscore distribution for all the users in our sample,including some pointers to less or more popularTwitter accounts.
The depicted user impact scoresform the response variable in the regression modelspresented in the following sections.4 User Account FeaturesThis section presents the features used in the userimpact prediction task.
They are divided into twocategories: non-textual and text-based.
All featureshave the joint characteristic of being under theuser?s direct control, something essential for char-acterising impact based on the actions of a user.Attributes such as the number of received retweetsor @-mentions (of a user in the tweets of others)were not considered as they are not controlled bythe account itself.406a1# of tweetsa2proportion of retweetsa3proportion of non-duplicate tweetsa4proportion of tweets with hashtagsa5hashtag-tokens ratio in tweetsa6proportion of tweets with @-mentionsa7# of unique @-mentions in tweetsa8proportion of tweets with @-repliesa9links ratio in tweetsa10# of favourites the account madea11total # of tweets (entire history)a12using default profile background (binary)a13using default profile image (binary)a14enabled geolocation (binary)a15population of account?s locationa16account?s location latitudea17account?s location longitudea18proportion of days with nonzero tweetsTable 1: Non textual attributes for a Twitter accountused in the modelling process.
All attributes referto a set of 365 days (?t) with the exception of a11,the total number of tweets in the entire history of anaccount.
Attributes ai, i ?
{2?
6, 8, 9} are ratiosof a1, whereas attribute a18is a proportion of ?t.4.1 Non textual attributesThe non-textual attributes (a) are derived eitherfrom general user behaviour statistics or directlyfrom the account?s profile.
Table 1 presents the 18attributes we extracted and used in our models.4.2 Text featuresWe process the text in the tweets of D1 and com-pute daily unigram frequencies.
By discardingterms that appear less than 100 times, we forma vocabulary of size |V | = 71, 555.
We then forma user term-frequency matrix of size |U |?|V | withthe mean term frequencies per user during the timeinterval ?t.
All term frequencies are normalisedwith the total number of tweets posted by the user.Apart from single word frequencies, we are alsointerested in deriving a more abstract representa-tion for each user.
To achieve this, we learn wordclusters from a distinct reference corpus (D2) thatcould potentially represent specific domains ofdiscussion (or topics).
From a multitude of pro-posed techniques, we have chosen to apply spec-tral clustering (Shi and Malik, 2000; Ng et al.,2002), a hard-clustering method appropriate forhigh-dimensional data and non-convex clusters(von Luxburg, 2007).
Spectral clustering performsgraph partitioning on the word-by-word similar-ity matrix.
In our case, tweet-term similarity isreflected by the Normalised Pointwise Mutual In-formation (NPMI), an information theoretic mea-sure indicating which words co-occur in the samecontext (Bouma, 2009).
We use the random walkgraph Laplacian and only keep the largest compo-nent of the resulting graph, eliminating most stopwords in the process.
The number of clusters needsto be specified in advance and each cluster?s mostrepresentative words are identified by the followingmetric of centrality:Cw(c) =?v?cNPMI(w, v)|c| ?
1, (2)where w is the target word and c the cluster it be-longs (|c| denotes the cluster?s size).
Examples ofextracted word clusters are illustrated in Table 4.Other techniques were also applied, such as onlineLDA (Hoffman et al., 2010), but we found thatthe results were not satisfactory, perhaps due tothe short message length and the foreign terms co-occuring within a tweet.
After forming the clustersusing D2, we compute a topic score (? )
for eachuser-topic pair in D1, representing a normaliseduser-word frequency sum per topic.5 MethodsThis section presents the various modelling ap-proaches for the underlying inference task, the im-pact score (S) prediction of Twitter users based ona set of their actions.5.1 Learning functions for regressionWe formulate this problem as a regression task,i.e.
we infer a real numbered value based on a setof observed features.
As a simple baseline, we ap-ply Ridge Regression (RR) (Hoerl and Kennard,1970), a reguralised version of the ordinary leastsquares.
Most importantly, we focus on nonlinearmethods for the impact score prediction task giventhe multimodality of the feature space.
Recently, itwas shown by Cohn and Specia (2013) that Sup-port Vector Machines for Regression (SVR) (Vap-nik, 1998; Smola and Sch?olkopf, 2004), commonlyconsidered the state-of-the-art for NLP regressiontasks, can be outperformed by Gaussian Processes(GPs), a kernelised, probabilistic approach to learn-ing (Rasmussen and Williams, 2006).
Their settingis close to ours, in that they had few (17) featuresand were also aiming to predict a complex con-tinuous phenomenon (human post-editing time).The initial stages of our experimental process con-firmed that GPs performed better than SVR; thus,407we based our modelling around them, includingRR for comparison.In GP regression, for the inputs x ?
Rdwe wantto learn a function f : Rd?
R that is drawn froma GP priorf(x) ?
GP(m(x), k(x,x?
)), (3)where m(x) and k(x,x?)
denote the mean (set to0 in our experiments) and covariance (or kernel)functions respectively.
The GP kernel function rep-resents the covariance between pairs of input.
Wewish to limit f to smooth functions over the inputs,with different smoothness in each input dimension,assuming that some features are more useful thanothers.
This can be accommodated by a squared ex-ponential covariance function with Automatic Rele-vance Determination (ARD) (Neal, 1996; Williamsand Rasmussen, 1996):kard(x,x?)
= ?2exp[d?i?(xi?
x?i)22`2i], (4)where ?2denotes the overall variance and `iisthe length-scale parameter for feature xi; all hy-perparameters are learned from data during modelinference.
Parameter `iis inversely proportional tothe feature?s relevancy in the model, i.e.
high val-ues of `iindicate a low degree of relevance for thecorresponding xi.
By setting `i= ` in Eq.
4, welearn a common length-scale for all the dimensions?
this is known as the isotropic squared exponen-tial function (kiso) since it is based purely on thedifference |x ?
x?|.
kisois a preferred choice whenthe dimensionality of the input space is high.
Hav-ing set our covariance functions, predictions areconducted using Bayesian integrationP(y?|x?,O) =?fP(y?|x?, f)P(f |O), (5)where y?is the response variable,O a labelled train-ing set and x?the current observation.
We learn thehyperparameters of the model by maximising thelog marginal likelihood P(y|O) using gradient as-cent.
However, inference becomes intractable whenmany training instances (n) are present as the num-ber of computations needed is O(n3) (Qui?nonero-Candela and Rasmussen, 2005).
Since our trainingsamples are tens of thousands, we apply a sparseapproximation method (FITC), which bases param-eter learning on a few inducing points in the train-ing set (Qui?nonero-Candela and Rasmussen, 2005;Snelson and Ghahramani, 2006).5.2 ModelsFor predicting user impact on Twitter, we developthree regression models that build on each other.The first and simplest one (A) uses only the non-textual attributes as features; the performance of Ais tested using RR,3SVR as well as a GP model.For SVR we used an RBF kernel (equivalent tokiso), whereas for the GP we applied the followingcovariance functionk(a,a?)
= kard(a,a?)
+ knoise(a,a?)
+ ?, (6)where knoise(a,a?)
= ?2?
?(a,a?
), ?
is a Kro-necker delta function and ?
is the regression bias;this function consists of (|a| + 3) hyperparame-ters.
Note that the sum of covariance functions isalso a valid covariance function (Rasmussen andWilliams, 2006).The second model (AW) extends model A byadding word-frequencies as features.
The 500 mostfrequent terms in D1 are discarded as stop wordsand we use the following 2, 000 ones (denoted byw).
Setting x = {a,w}, the covariance functionbecomesk(x,x?)
= kard(a,a?)
+ kiso(w,w?
)+ knoise(x,x?)
+ ?,(7)where we apply kisoon the term-frequencies due totheir high dimensionality; the number of hyperpa-rameters is (|a|+ 5).
This is an intermediate modelaiming to evaluate whether the incorporation oftext improves prediction performance.Finally, in the third model (AC) instead of rely-ing on the high dimensional space of single words,we use topic-oriented collections of terms extractedby applying spectral clustering (see Section 4.2).By denoting the set of different clusters or topicsas ?
and the entire feature space as x = {a,?
}, thecovariance function now becomesk(x,x?)
= kard(x,x?)
+ knoise(x,x?)
+ ?.
(8)The number of hyperparameters is equal to (|a|+|?
|+ 3) and this model is applied for |?
| = 50 and100.6 ExperimentsHere we present the experimental results for theuser impact prediction task and then investigate thefactors that can affect it.6.1 Predictive AccuracyWe evaluated the performance of the proposedmodels via 10-fold cross-validation.
Results arepresented in Table 2; Root Mean Squared Error3Given that the representation of attributes a16and a17(latitude, longitude) is ambiguous in a linear model, they werenot included in the RR-based models.408Linear (RR) Nonlinear (GP)Model r RMSE r RMSEA .667 2.642 .759 2.298AW .712 2.529 .768 2.263AC, |?
| = 50 .703 2.518 .774 2.234AC, |?
| = 100 .714 2.480 .780 2.210Table 2: Average performance (RMSE and Pear-son?s r) derived from 10-fold cross-validation forthe task of user impact score prediction.Model Top relevant featuresA a13, a11, a7, a1, a9, a8, a18, a4, a6, a3AW a7, a1, a11, a13, a9, a8, a18, a4, a6, a15AC, ?
= 50 a13, a11, a7, ?
?1, a1, a9, a8, ?
?2, a6, ?
?3AC, ?
= 100 a13, a11, a7, a1, a9, ?1, ?2, ?3, a18, a8Table 3: The 10 most relevant features in descend-ing relevance order for all GP models.
?
?iand ?idenote word clusters (may vary in each model).6(RMSE) and Pearson?s correlation (r) between pre-dictions and responses were used as the perfor-mance metrics.
Overall, the best performance interms of both RMSE (2.21 impact points) and lin-ear correlation (r = .78, p < .001) is achievedby the GP model (AC) that combines non-textualattributes with a 100 topic clusters; the differencein performance with all other models is statisticallysignificant.4The linear baseline (RR) follows thesame pattern of improvement through the differ-ent models, but never manages to reach the perfor-mance of the nonlinear alternative.
As mentionedpreviously, we have also tried SVR with an RBFkernel for model A (parameters were optimised ona held-out development set) and the performance(RMSE: 2.33, r = .75, p < .001) was significantlyworse than the one achieved by the GP model.4Notice that when word-based features are intro-duced in model AW, performance improves.
Thiswas one of the motivations for including text in themodelling, apart from the notion that the postedcontent should also affect general impact.
Lastly,turning this problem from regression to classifi-cation by creating 3 impact score pseudo-classesbased on the .25 and the .9 quantiles of the re-sponse variable (4.3 and 11.4 impact score pointsrespectively) and by using the outputs of modelAC (?
= 100) in each phase of the 10-fold cross-validation, we achieve a 75.86% classification ac-curacy.54Indicated by performing a t-test (5% significance level).5Similar performance scores can be estimated for differentclass threshold settings.01000100010001000 10 20 3001000 10 20 30L HL HL HL HL HTweetszinzentirezhistoryz(?11)Uniquez@-mentionsz(?7)Linksz(?9)@-repliesz(?8)Dayszwithznonzeroztweetsz(?18)Figure 2: User impact distribution (x-axis: impactpoints, y-axis: # of user accounts) for users with alow (L) or a high (H) participation in a selectionof relevant non-textual attributes.
Dot-dashed linesdenote the respective mean impact score; the redline is the mean of the entire sample (= 6.776).6.2 Qualitative AnalysisGiven the model?s strong performance, we nowconduct a more thorough analysis to identify andcharacterise the properties that affect aspects ofthe user impact.
GP?s length-scale parameters (`i)?
which are inversely proportional to feature rele-vancy ?
are used for ranking feature importance.Note that since our data set consists of UK users,some results may be biased towards specific cul-tural properties.Non-textual attributes.
Table 3 lists the 10 mostrelevant attributes (or topics, where applicable) asextracted in each GP model.
Ranking is determinedby the mean value of the length-scale parameter foreach feature in the 10-fold cross-validation process.We do not show feature ranking derived from theRR models as we focus on the models with the bestperformance.
Despite this, it is worth mentioning6Length-scales are comparable for features of the samevariance (z-scored).
Binary features (denoted by ) are notz-scored, but for comparison purposes we have rescaled theirlength-scale using the feature?s variance.409Label ?(`)?
?
(`) Cluster?s words ranked by centrality |c|?1: Weather 3.73?
1.80 mph, humidity, barometer, gust, winds, hpa, temperature, kt, #weather [...] 309?2: HealthcareFinanceHousing5.44?
1.55 nursing, nurse, rn, registered, bedroom, clinical, #news, estate, #hospital,rent, healthcare, therapist, condo, investment, furnished, medical, #nyc,occupational, investors, #ny, litigation, tutors, spacious, foreclosure [...]1281?3: Politics 6.07?
2.86 senate, republican, gop, police, arrested, voters, robbery, democrats, presi-dential, elections, charged, election, charges, #religion, arrest, repeal, dems,#christian, reform, democratic, pleads, #jesus, #atheism [...]950?4: ShowbizMoviesTV7.36?
2.25 damon, potter, #tvd, harry, elena, kate, portman, pattinson, hermione, jen-nifer, kristen, stefan, robert, catholic, stewart, katherine, lois, jackson, vam-pire, natalie, #vampirediaries, tempah, tinie, weasley, turner, rowland [...]1943?5: Commerce 7.83?
2.77 chevrolet, inventory, coupon, toyota, mileage, sedan, nissan, adde, jeep, 4x4,2002, #coupon, enhanced, #deal, dodge, gmc, 20%, suv, 15%, 2005, 2003,2006, coupons, discount, hatchback, purchase, #ebay, 10% [...]608?6: TwitterHashtags8.22?
2.98 #teamfollowback, #500aday, #tfb, #instantfollowback, #ifollowback, #in-stantfollow, #followback, #teamautofollow, #autofollow, #mustfollow [...]194?7: SocialUnrest8.37?
5.52 #egypt, #tunisia, #iran, #israel, #palestine, tunisia, arab, #jan25, iran, israel,protests, egypt, #yemen, #iranelection, israeli, #jordan, regime, yemen,#gaza, protesters, #lebanon, #syria, egyptian, #protest, #iraq [...]321?8: Non English 8.45?
3.80 yg, nak, gw, gue, kalo, itu, aku, aja, ini, gak, klo, sih, tak, mau, buat [...] 469?9: HoroscopeGambling9.11?
3.07 horoscope, astrology, zodiac, aries, libra, aquarius, pisces, taurus, virgo,capricorn, horoscopes, sagitarius, comprehensive, lottery, jackpot [...]1354?10: ReligionSports10.29?
6.27 #jesustweeters, psalm, christ, #nhl, proverbs, unto, salvation, psalms, lord,kjv, righteousness, niv, bible, pastor, #mlb, romans, awards, nhl [...]1610Table 4: The 10 most relevant topics (for model AC, |?
| = 100) in the prediction of a user?s impact scoretogether with their most central words.
The topics are ranked by their mean length-scale, ?
(`), in the10-fold cross-validation process (?
(`) is the respective standard deviation).that RR?s outputs also followed similar ranking pat-terns, e.g.
the top 5 features in model A were a18,a7, a3, a11and a9.
Notice that across all models,among the strongest features are the total numberof posts either in the entire account?s history (a11)or within the 365-day interval of our experiment(a1) and the number of unique @-mentions (a7),good indicators of user activity and user interactionrespectively.
Feature a13is also a very good predic-tor, but is of limited utility for modelling our dataset because very few accounts maintain the defaultprofile photo (0.4%).
Less relevant attributes (notshown) are the ones related to the location of auser (a16, a17) signalling that the whereabouts of auser may not necessarily relate to impact.
Anotherlow relevance attribute is the number of favouritesthat an account did (a10), something reasonable, asthose weak endorsements are not affecting the mainstream of content updates in the social network.In Figure 2, we present the distribution of userimpact for accounts with low (left-side) and high(right-side) participation in a selection of non-textual attributes.
Low (L) and high (H) participa-tions are defined by selecting the 500 accounts withlowest and highest scores for this specific attribute.The means of (L) and (H) are compared with themean impact score in our sample.
As anticipated,accounts with low activity (a11) are likely to beassigned impact scores far below the mean, whilevery active accounts may follow a quite oppositepattern.
Avoiding mentioning (a7) or replying (a8)to others may not affect (on average) an impactscore positively or negatively; however, accountsthat do many unique @-mentions are distributedaround a clearly higher impact score.
On the otherhand, users that overdo @-replies are distributed be-low the mean impact score.
Furthermore, accountsthat post irregularly with gaps longer than a day(a18) or avoid using links in their tweets (a9) willprobably appear in the low impact score range.Topics.
Regarding prediction accuracy (Table 2),performance improves when topics are included.In turn, some of the topics replace non-textual at-tributes in the relevancy ranking (Table 3).
Table 4presents the 10 most relevant topic word-clustersbased on their mean length-scale ?
(`) in the 10-fold cross-validation process for the best perform-ing GP model (AC, |?
| = 100).
We see that clusterswith their most central words representing topicssuch as ?Weather?, ?Healthcare/Finance?, ?Politics?and ?Showbiz?
come up on top.Contrary to the non-textual attributes, accountswith low participation in a topic (for the vast major-ity of topics) were distributed along impact scorevalues lower than the mean.
Based on the fact thatword clusters are not small in size, this is a rationaloutcome indicating that accounts with small word-frequency sums (i.e.
the ones that do not tweetmuch) will more likely be users with small impact41001000 10 20 3001000 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30?1 ?2 ?3 ?4 ?5?6 ?7 ?8 ?9 ?10Figure 3: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for accounts with ahigh participation in the 10 most relevant topics.
Dot-dashed lines denote mean impact scores; the red lineis the mean of the entire sample (= 6.776).Number of AccountsImpact Score (S)0 10 20 30050100AllLow EntropyHigh EntropyFigure 4: User impact distribution for accounts withhigh (blue) and low (dark grey) topic entropy.
Linesdenote the respective mean impact scores.scores.
Hence, in Figure 3 we only show the userimpact distribution for the 500 accounts with thetop participation in each topic.
Informally, this is away to quantify the contribution of each domain ortopic of discussion in the impact score.
Notice thatthe topics which ?push?
users towards the highestimpact scores fall into the domains of ?Politics?
(?3)and ?Showbiz?
(?4).
An equally interesting observa-tion is that engaging a lot about a specific topic willmore likely result to a higher than average impact;the only exception is ?8which does not deviatefrom the mean, but ?8rather represents the use of anon-English language (Indonesian) and therefore,does not form an actual topic of discussion.To further understand how participation in the10 most relevant topics relates to impact, we alsocomputed the joint user-topic entropy defined byH(ui, ?)
= ?M?j=1P(ui, ?j)?
log2P(ui, ?j), (9)where uiis a user and M = 10 (Shannon, 2001).This is a measure of user pseudo-informativeness,meaning that users with high entropy are consid-ered as more informative (without assessing thequality of the information).
Figure 4 shows the im-pact score distributions for the 500 accounts withthe lowest and highest entropy.
Low and high en-tropies are separated, with the former being placedclearly below the mean user impact score and thelatter above.
This pictorial assessment suggests thata connection between informativeness and impactmay exist, at least in their extremes (their correla-tion in the entire sample is r = .35, p < .001).Use case scenarios.
Most of the previous analysisfocused on the properties of single features.
How-ever, the user impact prediction models we learndepend on feature combinations.
For that reason,it is of interest to investigate use case scenariosthat bring various attributes together.
To reducenotation in this paragraph, we use x+i(x is ei-ther a non-textual attribute a or a topic ? )
to ex-press xi> ?
(xi), the set of users for which thevalue of feature xiis above the mean; equivalentlyx?i: xi< ?(xi).
We also use ?
?Ato express themore complex set {?+A?
??j?
... ?
?
?z}, an inter-section of users that are active in one topic (?A),but not very active in the rest.
Figure 5 depicts theuser impact distributions for five use case scenarios.Scenario A compares interactive to non interac-tive users, represented by P(a+1, a+6, a+7, a+8) andP(a+1, a?6, a?7, a?8) respectively; interactivity, de-fined by an intersection of accounts that tweet regu-larly, do many @-mentions and @-replies, but also4110 10 20 300150300450600750900 IANIA0 10 20 300100200300400 IAIAC0 10 20 300100200300400500 LNL0 10 20 300100200300400500 TOTF0 10 20 30050100150200 LTSTA B C D EFigure 5: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for five Twitteruse scenarios based on subsets of the most relevant attributes and topics ?
IA: Interactive, IAC: CliqueInteractive, L: Using many links, TO: Topic-Overall, TF: Topic-Focused, LT: ?Light?
topics, ST: ?Serious?topics.
(N) denotes negation and lines the respective mean impact scores.mention many different users, seems to be rewardedon average with higher impact scores.
Interactiveusers gain more impact than clique-interactive ac-counts represented by P(a+1, a+6, a?7, a+8), i.e.
userswho interact, but do not mention many differ-ent accounts, possibly because they are conduct-ing discussions with a specific circle only (sce-nario B).
The use of links when writing aboutthe most prevalent topics (?Politics?
and ?Show-biz?)
appears to be an important impact-wise fac-tor (scenario C); the compared probability distri-butions in that case were P(a+1, (?+3?
?+4), a+9)against P(a+1, (?+3?
?+4), a?9).
Surprisingly, whenlinks were replaced by hashtags in the previousdistributions, a clear class separation was notachieved.
In scenario D, topic-focused accounts,i.e.
users that write about one topic consistently,represented by P(a+1, (??2?
??3?
??4?
??7?
?
?10)),have on average slightly worse impact scores whencompared to accounts tweeting about many top-ics, P(a+1, ?+2, ?+3, ?+4, ?+7, ?+10).
Finally, scenarioE shows thats users engaging about more ?seri-ous?
topics, P(a+1, ?
?4, ?
?5, ?
?9, (?+3?
?+7)), werenot differentiated from the ones posting about more?light?
topics, P(a+1, (?+4?
?+5?
?+9), ?
?3, ?
?7).7 Related WorkThe task of user-impact prediction based on a ma-chine learning approach that incorporates text fea-tures is novel, to the best of our knowledge.
De-spite this fact, our work is partly related to researchapproaches for quantifying and analysing user in-fluence in online social networks.
For example,Cha et al.
(2010) compared followers, retweetsand @-mentions received as measures of influ-ence.
Bakshy et al.
(2011) aggregated all posts byeach user, computed an individual-level influenceand then tried to predict it by modelling user at-tributes (# of followers, followees, tweets and dateof joining) together with past user influence.
Theirmethod, based on classification and regression trees(Breiman, 1984), achieved a modest performance(r = .34).
Furthermore, Romero et al.
(2011) pro-posed an algorithm for determining user influenceand passivity based on information-forwarding ac-tivity, and Luo et al.
(2013) exploited user attributesto predict retweet occurrences.
The primary differ-ence with all the works described above is that weaim to predict user impact by exploiting featuresunder the user?s direct control.
Hence, our findingscan be used as indirect insights for strategies that in-dividual users may follow to increase their impactscore.
In addition, we incorporate the actual textposted by the users in the entire modelling process.8 Conclusions and Future WorkWe have introduced the task of user impact pre-diction on the microblogging platform of Twitterbased on user-controlled textual and non-textualattributes.
Nonlinear methods, in particular Gaus-sian Processes, were more suitable than linear ap-proaches for this problem, providing a strong per-formance (r = .78).
That result motivated the anal-ysis of specific characteristics in the inferred modelto further define and understand the elements thataffect impact.
In a nutshell, activity, non clique-oriented interactivity and engagement on a diverseset of topics are among the most decisive impactfactors.
In future work, we plan to improve variousmodelling components and gain a deeper under-standing of the derived outcomes in collaborationwith domain experts.
For more general conclusions,the consideration of different cultures and mediasources is essential.AcknowledgmentsThis research was supported by EU-FP7-ICTproject n.287863 (?TrendMiner?).
Lampos also ac-knowledges the support from EPSRC IRC projectEP/K031953/1.412ReferencesEytan Bakshy, Jake M. Hofman, Winter A. Mason, and Dun-can J. Watts.
2011.
Everyone?s an influencer: quantifyinginfluence on Twitter.
In 4th International Conference onWeb Search and Data Mining, WSDM?11, pages 65?74.Gerlof Bouma.
2009.
Normalized (pointwise) mutual in-formation in collocation extraction.
In Biennial GSCLConference, pages 31?40.Danah Boyd, Scott Golder, and Gilad Lotan.
2010.
Tweet,Tweet, Retweet: Conversational Aspects of Retweeting onTwitter.
In System Sciences, HICSS?10, pages 1?10.Leo Breiman.
1984.
Classification and regression trees.Chapman & Hall.Carlos Castillo, Marcelo Mendoza, and Barbara Poblete.
2011.Information credibility on Twitter.
In 20th InternationalConference on World Wide Web, WWW?11, pages 675?684.Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, andKrishna P. Gummadi.
2010.
Measuring User Influence inTwitter: The Million Follower Fallacy.
In 4th InternationalConference on Weblogs and Social Media, ICWSM?10,pages 10?17.Trevor Cohn and Lucia Specia.
2013.
Modelling AnnotatorBias with Multi-task Gaussian Processes: An Applicationto Machine Translation Quality Estimation.
In 51st AnnualMeeting of the Association for Computational Linguistics,ACL?13, pages 32?42.Arthur E. Hoerl and Robert W. Kennard.
1970.
Ridge Re-gression: Biased Estimation for Nonorthogonal Problems.Technometrics, 12(1):55?67.Matthew Hoffman, David Blei, and Francis Bach.
2010.
On-line Learning for Latent Dirichlet Allocation.
In Advancesin Neural Information Processing Systems, NIPS?10, pages856?864.Bernardo A. Huberman, Daniel M. Romero, and Fang Wu.2009.
Social Networks that Matter: Twitter Under theMicroscope.
First Monday, 14(1).Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is Twitter, a social network or a newsmedia?
In 19th International Conference on World WideWeb, WWW?10, pages 591?600.Vasileios Lampos and Nello Cristianini.
2012.
Nowcast-ing Events from the Social Web with Statistical Learning.ACM Transactions on Intelligent Systems and Technology,3(4):72:1?72:22.Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor Cohn.2013.
A user-centric model of voting intention from SocialMedia.
In 51st Annual Meeting of the Association forComputational Linguistics, ACL?13, pages 993?1003.Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang.2013.
Who will retweet me?
: finding retweeters in Twit-ter.
In 36th International Conference on Research andDevelopment in Information Retrieval, SIGIR?13, pages869?872.Radford M. Neal.
1996.
Bayesian Learning for Neural Net-works.
Springer.Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2002.
Onspectral clustering: Analysis and an algorithm.
In Advancesin Neural Information Processing Systems, NIPS?02, pages849?856.Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,Nicholas Gibbins, and Mahesan Niranjan.
2012.
Trend-miner: An Architecture for Real Time Analysis of SocialMedia Text.
In 6th International Conference on Weblogsand Social Media, ICWSM?12, pages 38?42.Joaquin Qui?nonero-Candela and Carl E. Rasmussen.
2005.A unifying view of sparse approximate Gaussian Processregression.
Journal of Machine Learning Research, 6:1939?1959.Carl E. Rasmussen and Christopher K. I. Williams.
2006.Gaussian Processes for Machine Learning.
MIT Press.Daniel M. Romero, Wojciech Galuba, Sitaram Asur, andBernardo A. Huberman.
2011.
Influence and Passivityin Social Media.
In Machine Learning and KnowledgeDiscovery in Databases, volume 6913, pages 18?33.Dominic Rout, Daniel Preot?iuc-Pietro, Bontcheva Kalina, andTrevor Cohn.
2013.
Where?s @wally: A classificationapproach to geolocating users based on their social ties.
In24th Conference on Hypertext and Social Media, HT?13,pages 11?20.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010.Earthquake shakes Twitter users: real-time event detectionby social sensors.
In 19th International Conference onWorld Wide Web, WWW?10, pages 851?860.Claude E. Shannon.
2001.
A mathematical theory of com-munication.
SIGMOBILE Mob.
Comput.
Commun.
Rev.,5(1):3?55 (reprint with corrections).Jianbo Shi and Jitendra Malik.
2000.
Normalized cuts andimage segmentation.
Transactions on Pattern Analysis andMachine Intelligence, 22(8):888?905.Alex J. Smola and Bernhard Sch?olkopf.
2004.
A tutorialon support vector regression.
Statistics and Computing,14(3):199?222.Edward Snelson and Zoubin Ghahramani.
2006.
SparseGaussian Processes using Pseudo-inputs.
In Advances inNeural Information Processing Systems, NIPS?06, pages1257?1264.Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi.2010.
Want to be Retweeted?
Large Scale Analytics onFactors Impacting Retweet in Twitter Network.
In SocialComputing, SocialCom?10, pages 177?184.Vladimir N. Vapnik.
1998.
Statistical learning theory.
Wiley.Ulrike von Luxburg.
2007.
A tutorial on spectral clustering.Statistics and computing, 17(4):395?416.Christopher K. I. Williams and Carl E. Rasmussen.
1996.Gaussian Processes for Regression.
In Advances in NeuralInformation Processing Systems, NIPS?96, pages 514?520.413
