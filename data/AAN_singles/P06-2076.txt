Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 587?594,Sydney, July 2006. c?2006 Association for Computational LinguisticsMachine-Learning-Based Transformation of Passive Japanese Sentencesinto Active by Separating Training Data into Each Input ParticleMasaki MurataNational Institute of Informationand Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japanmurata@nict.go.jpTamotsu ShiradoNational Institute of Informationand Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japanshirado@nict.go.jpToshiyuki KanamaruNational Institute of Informationand Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japankanamaru@nict.go.jpHitoshi IsaharaNational Institute of Informationand Communications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japanisahara@nict.go.jpAbstractWe developed a new method of transform-ing Japanese case particles when trans-forming Japanese passive sentences intoactive sentences.
It separates training datainto each input particle and uses machinelearning for each particle.
We also usednumerous rich features for learning.
Ourmethod obtained a high rate of accuracy(94.30%).
In contrast, a method that didnot separate training data for any inputparticles obtained a lower rate of accu-racy (92.00%).
In addition, a methodthat did not have many rich features forlearning used in a previous study (Mu-rata and Isahara, 2003) obtained a muchlower accuracy rate (89.77%).
We con-firmed that these improvements were sig-nificant through a statistical test.
Wealso conducted experiments utilizing tra-ditional methods using verb dictionar-ies and manually prepared heuristic rulesand confirmed that our method obtainedmuch higher accuracy rates than tradi-tional methods.1 IntroductionThis paper describes how passive Japanese sen-tences can be automatically transformed into ac-tive.
There is an example of a passive Japanesesentence in Figure 1.
The Japanese suffix retafunctions as an auxiliary verb indicating the pas-sive voice.
There is a corresponding active-voicesentence in Figure 2.
When the sentence in Fig-ure 1 is transformed into an active sentence, (i) ni(by), which is a case postpositional particle withthe meaning of ?by?, is changed into ga, which isa case postpositional particle indicating the sub-jective case, and (ii) ga (subject), which is acase postpositional particle indicating the subjec-tive case, is changed into wo (object), which isa case postpositional particle indicating the objec-tive case.
In this paper, we discuss the transfor-mation of Japanese case particles (i.e., ni ?
ga)through machine learning.1The transformation of passive sentences into ac-tive is useful in many research areas includinggeneration, knowledge extraction from databaseswritten in natural languages, information extrac-tion, and answering questions.
For example, whenthe answer is in the passive voice and the ques-tion is in the active voice, a question-answeringsystem cannot match the answer with the questionbecause the sentence structures are different andit is thus difficult to find the answer to the ques-tion.
Methods of transforming passive sentencesinto active are important in natural language pro-cessing.The transformation of case particles in trans-forming passive sentences into active is not easybecause particles depend on verbs and their use.We developed a new method of transformingJapanese case particles when transforming pas-sive Japanese sentences into active in this study.Our method separates training data into each in-put particle and uses machine learning for each in-put particle.
We also used numerous rich featuresfor learning.
Our experiments confirmed that ourmethod was effective.1In this study, we did not handle the transformation ofauxiliary verbs and the inflection change of verbs becausethese can be transformed based on Japanese grammar.587inu ni watashi ga kama- reta.
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice(I was bitten by a dog.
)Figure 1: Passive sentenceinu ni watashi ga kama- reta.ga wo(dog) (by) (I) subjective-case postpositional particle (bite) passive voice(I was bitten by a dog.
)Figure 3: Example in corpusinu ga watashi wo kanda.
(dog) subject (I) object (bite)(Dog bit me.
)Figure 2: Active sentence2 Tagged corpus as supervised dataWe used the Kyoto University corpus (Kurohashiand Nagao, 1997) to construct a corpus tagged forthe transformation of case particles.
It has ap-proximately 20,000 sentences (16 editions of theMainichi Newspaper, from January 1st to 17th,1995).
We extracted case particles in passive-voice sentences from the Kyoto University cor-pus.
There were 3,576 particles.
We assigned acorresponding case particle for the active voice toeach case particle.
There is an example in Figure3.
The two underlined particles, ?ga?
and ?wo?that are given for ?ni?
and ?ga?
are tags for caseparticles in the active voice.
We called the givencase particles for the active voice target case par-ticles, and the original case particles in passive-voice sentences source case particles.
We createdtags for target case particles in the corpus.
If wecan determine the target case particles in a givensentence, we can transform the case particles inpassive-voice sentences into case particles for theactive voice.
Therefore, our goal was to determinethe target case particles.3 Machine learning method (supportvector machine)We used a support vector machine as the basisof our machine-learning method.
This is becausesupport vector machines are comparatively betterthan other methods in many research areas (Kudohand Matsumoto, 2000; Taira and Haruno, 2001;Small Margin Large MarginFigure 4: Maximizing marginMurata et al, 2002).Data consisting of two categories were classi-fied by using a hyperplane to divide a space withthe support vector machine.
When these two cat-egories were, positive and negative, for example,enlarging the margin between them in the train-ing data (see Figure 42), reduced the possibility ofincorrectly choosing categories in blind data (testdata).
A hyperplane that maximized the marginwas thus determined, and classification was doneusing that hyperplane.
Although the basics of thismethod are as described above, the region betweenthe margins through the training data can includea small number of examples in extended versions,and the linearity of the hyperplane can be changedto non-linear by using kernel functions.
Classi-fication in these extended versions is equivalentto classification using the following discernmentfunction, and the two categories can be classifiedon the basis of whether the value output by thefunction is positive or negative (Cristianini andShawe-Taylor, 2000; Kudoh, 2000):2The open circles in the figure indicate positive examplesand the black circles indicate negative.
The solid line indi-cates the hyperplane dividing the space, and the broken linesindicate the planes depicting margins.588f(x) = sgn(l?i=1?iyiK(xi,x) + b)(1)b =maxi,yi=?1bi + mini,yi=1bi2bi = ?l?j=1?jyjK(xj,xi),where x is the context (a set of features) of an in-put example, xi indicates the context of a trainingdatum, and yi (i = 1, ..., l, yi ?
{1,?1}) indicatesits category.
Function sgn is:sgn(x) = 1 (x ?
0), (2)?1 (otherwise).Each ?i (i = 1, 2...) is fixed as a value of ?i thatmaximizes the value of L(?)
in Eq.
(3) under theconditions set by Eqs.
(4) and (5).L(?)
=l?i=1?i?12l?i,j=1?i?jyiyjK(xi,xj) (3)0 ?
?i?
C (i = 1, ..., l) (4)l?i=1?iyi= 0 (5)Although function K is called a kernel functionand various functions are used as kernel functions,we have exclusively used the following polyno-mial function:K(x,y) = (x ?
y + 1)d (6)C and d are constants set by experimentation.
Forall experiments reported in this paper, C was fixedas 1 and d was fixed as 2.A set of xi that satisfies ?i > 0 is called a sup-port vector, (SVs)3, and the summation portion ofEq.
(1) is only calculated using examples that aresupport vectors.
Equation 1 is expressed as fol-lows by using support vectors.f(x) = sgn??
?i:xi?SVs?iyiK(xi,x) + b??
(7)b =bi:yi=?1,xi?SVs+ bi:yi=1,xi?SVs2bi = ?
?i:xi?SVs?jyjK(xj ,xi),3The circles on the broken lines in Figure 4 indicate sup-port vectors.Table 1: FeaturesF1 part of speech (POS) of PF2 main word of PF3 word of PF4 first 1, 2, 3, 4, 5, and 7 digits of category numberof P5F5 auxiliary verb attached to PF6 word of NF7 first 1, 2, 3, 4, 5, and 7 digits of category numberof NF8 case particles and words of nominals that have de-pendency relationship with P and are other thanNF9 first 1, 2, 3, 4, 5, and 7 digits of category num-ber of nominals that have dependency relationshipwith P and are other than NF10 case particles of nominals that have dependencyrelationship with P and are other than NF11 the words appearing in the same sentenceF12 first 3 and 5 digits of category number of wordsappearing in same sentenceF13 case particle taken by N (source case particle)F14 target case particle output by KNP (Kurohashi,1998)F15 target case particle output with Kondo?s method(Kondo et al, 2001)F16 case patterns defined in IPAL dictionary (IPAL)(IPA, 1987)F17 combination of predicate semantic primitives de-fined in IPALF18 predicate semantic primitives defined in IPALF19 combination of semantic primitives of N definedin IPALF20 semantic primitives of N defined in IPALF21 whether P is defined in IPAL or notF22 whether P can be in passive form defined inVDIC6F23 case particles of P defined in VDICF24 type of P defined in VDICF25 transformation rule used for P and N in Kondo?smethodF26 whether P is defined in VDIC or notF27 pattern of case particles of nominals that have de-pendency relationship with PF28 pair of case particles of nominals that have depen-dency relationship with PF29 case particles of nominals that have dependencyrelationship with P and appear before NF30 case particles of nominals that have dependencyrelationship with P and appear after NF31 case particles of nominals that have dependencyrelationship with P and appear just before NF32 case particles of nominals that have dependencyrelationship with P and appear just after N589Table 2: Frequently occurring target case particles in source case particlesSource case particle Occurrence rate Frequent target case Occurrence rateparticles in insource case particles source case particlesni (indirect object) 27.57% (493/1788) ni (indirect object) 70.79% (349/493)ga (subject) 27.38% (135/493)ga (subject) 26.96% (482/1788) wo (direct object) 96.47% (465/482)de (with) 17.17% (307/1788) ga (subject) 79.15% (243/307)de (with) 13.36% (41/307)to (with) 16.11% (288/1788) to (with) 99.31% (286/288)wo (direct object) 6.77% (121/1788) wo (direct object) 99.17% (120/121)kara (from) 4.53% ( 81/1788) ga (subject) 49.38% ( 40/ 81)kara (from) 44.44% ( 36/ 81)made (to) 0.78% ( 14/1788) made (to) 100.00% ( 14/ 14)he (to) 0.06% ( 1/1788) ga (subject) 100.00% ( 1/ 1)no (subject) 0.06% ( 1/1788) wo (direct object) 100.00% ( 1/ 1)Support vector machines are capable of han-dling data consisting of two categories.
Data con-sisting of more than two categories is generallyhandled using the pair-wise method (Kudoh andMatsumoto, 2000).Pairs of two different categories (N(N-1)/2pairs) are constructed for data consisting of N cat-egories with this method.
The best category is de-termined by using a two-category classifier (in thispaper, a support vector machine4 is used as thetwo-category classifier), and the correct categoryis finally determined on the basis of ?voting?
onthe N(N-1)/2 pairs that result from analysis withthe two-category classifier.The method discussed in this paper is in fact acombination of the support vector machine and thepair-wise method described above.4 Features (information used inclassification)The features we used in our study are listed in Ta-ble 1, where N is a noun phrase connected to the4We used Kudoh?s TinySVM software (Kudoh, 2000) asthe support vector machine.5The category number indicates a semantic class ofwords.
A Japanese thesaurus, the Bunrui Goi Hyou (NLRI,1964), was used to determine the category number of eachword.
This thesaurus is ?is-a?
hierarchical, in which eachword has a category number.
This is a 10-digit number thatindicates seven levels of ?is-a?
hierarchy.
The top five lev-els are expressed by the first five digits, the sixth level is ex-pressed by the next two digits, and the seventh level is ex-pressed by the last three digits.6Kondo et al constructed a rich dictionary for Japaneseverbs (Kondo et al, 2001).
It defined types and characteris-tics of verbs.
We will refer to it as VDIC.case particle being analyzed, and P is the phrase?spredicate.
We used the Japanese syntactic parser,KNP (Kurohashi, 1998), for identifying N, P, partsof speech and syntactic relations.In the experiments conducted in this study, weselected features.
We used the following proce-dure to select them.?
Feature selectionWe first used all the features for learning.
Wenext deleted only one feature from all the fea-tures for learning.
We did this for every fea-ture.
We decided to delete features that wouldmake the most improvement.
We repeatedthis until we could not improve the rate of ac-curacy.5 Method of separating training datainto each input particleWe developed a new method of separating train-ing data into each input (source) particle that usesmachine learning for each particle.
For example,when we identify a target particle where the sourceparticle is ni, we use only the training data wherethe source particle is ni.
When we identify a tar-get particle where the source particle is ga, we useonly the training data where the source particle isga.Frequently occurring target case particles arevery different in source case particles.
Frequentlyoccurring target case particles in all source caseparticles are listed in Table 2.
For example, whenni is a source case particle, frequently occurring590Table 3: Occurrence rates for target case particlesTarget case Occurrence rateparticle Closed Openwo (direct object) 33.05% 29.92%ni (indirect object) 19.69% 17.79%to (with) 16.00% 18.90%de (with) 13.65% 15.27%ga (subject) 11.07% 10.01%ga or de 2.40% 2.46%kara (from) 2.13% 3.47%Other 2.01% 1.79%target case particles are ni or ga.
In contrast, whenga is a source case particle, a frequently occurringtarget case particle is wo.In this case, it is better to separate training datainto each source particle and use machine learn-ing for each particle.
We therefore developed thismethod and confirmed that it was effective throughexperiments (Section 6).6 Experiments6.1 Basic experimentsWe used the corpus we constructed described inSection 2 as supervised data.
We divided the su-pervised data into closed and open data (Both theclosed data and open data had 1788 items each.
).The distribution of target case particles in the dataare listed in Table 3.
We used the closed data todetermine features that were deleted in feature se-lection and used the open data as test data (datafor evaluation).
We used 10-fold cross validationfor the experiments on closed data and we usedclosed data as the training data for the experimentson open data.
The target case particles were deter-mined by using the machine-learning method ex-plained in Section 3.
When multiple target parti-cles could have been answers in the training data,we used pairs of them as answers for machinelearning.The experimental results are listed in Tables 4and 5.
Baseline 1 outputs a source case particleas the target case particle.
Baseline 2 outputs themost frequent target case particle (wo (direct ob-ject)) in the closed data as the target case particlein every case.
Baseline 3 outputs the most fre-quent target case particle for each source targetcase particle in the closed data as the target caseparticle.
For example, ni (indirect object) is themost frequent target case particle when the sourcecase particle is ni, as listed in Table 2.
Baseline 3outputs ni when the source case particle is ni.
KNPindicates the results that the Japanese syntacticparser, KNP (Kurohashi, 1998), output.
Kondo in-dicates the results that Kondo?s method, (Kondo etal., 2001), output.
KNP and Kondo can only workwhen a target predicate is defined in the IPAL dic-tionary or the VDIC dictionary.
Otherwise, KNPand Kondo output nothing.
?KNP/Kondo + Base-line X?
indicates the use of outputs by BaselineX when KNP/Kondo have output nothing.
KNPand Kondo are traditional methods using verb dic-tionaries and manually prepared heuristic rules.These traditional methods were used in this studyto compare them with ours.
?Murata 2003?
indi-cates results using a method they developed in aprevious study (Murata and Isahara, 2003).
Thismethod uses F1, F2, F5, F6, F7, F10, and F13 asfeatures and does not have training data for anysource case particles.
?Division?
indicates sepa-rating training data into each source particle.
?No-division?
indicates not separating training data forany source particles.
?All features?
indicates theuse of all features with no features being selected.
?Feature selection?
indicates features are selected.We did two kinds of evaluations: ?Eval.
A?
and?Eval.
B?.
There are some cases where multipletarget case particles can be answers.
For example,ga and de can be answers.
We judged the result tobe correct in ?Eval.
A?
when ga and de could beanswers and the system output the pair of ga andde as answers.
We judged the result to be correctin ?Eval.
B?
when ga and de could be answers andthe system output ga, de, or the pair of ga and deas answers.Table 4 lists the results using all data.
Table 5lists the results where a target predicate is definedin the IPAL and VDIC dictionaries.
There were551 items in the closed data and 539 in the open.We found the following from the results.Although selection of features obtained higherrates of accuracy than use of all features in theclosed data, it did not obtain higher rates of accu-racy in the open data.
This indicates that featureselection was not effective and we should haveused all features in this study.Our method using all features in the open dataand separating training data into each source parti-cle obtained the highest rate of accuracy (94.30%in Eval.
B).
This indicates that our method is ef-591Table 4: Experimental resultsMethod Closed data Open dataEval.
A Eval.
B Eval.
A Eval.
BBaseline 1 58.67% 61.41% 62.02% 64.60%Baseline 2 33.05% 33.56% 29.92% 30.37%Baseline 3 84.17% 88.20% 84.17% 88.20%KNP 27.35% 28.69% 27.91% 29.14%KNP + Baseline 1 64.32% 67.06% 67.79% 70.36%KNP + Baseline 2 48.10% 48.99% 45.97% 46.48%KNP + Baseline 3 81.21% 84.84% 80.82% 84.45%Kondo 39.21% 40.88% 39.32% 41.00%Kondo + Baseline 1 65.27% 68.57% 67.34% 70.41%Kondo + Baseline 2 54.87% 56.54% 53.52% 55.26%Kondo + Baseline 3 78.08% 81.71% 78.30% 81.88%Murata 2003 86.86% 89.09% 87.86% 89.77%Our method, no-division + all features 89.99% 92.39% 90.04% 92.00%Our method, no-division + feature selection 91.28% 93.40% 90.10% 92.00%Our method, division + all features 91.22% 93.79% 92.28% 94.30%Our method, division + feature selection 92.06% 94.41% 91.89% 93.85%Table 5: Experimental results on data that can use IPAL and VDIC dictionariesMethod Closed data Open dataEval.
A Eval.
B Eval.
A Eval.
BBaseline 1 57.71% 58.98% 58.63% 58.81%Baseline 2 37.39% 37.39% 37.29% 37.29%Baseline 3 84.03% 86.57% 86.83% 88.31%KNP 74.59% 75.86% 75.88% 76.07%Kondo 76.04% 77.50% 78.66% 78.85%Our method, no-division + all features 94.19% 95.46% 94.81% 94.81%Our method, division + all features 95.83% 96.91% 97.03% 97.03%fective.Our method that used all the features and didnot separate training data for any source particlesobtained an accuracy rate of 92.00% in Eval.
B.The technique of separating training data into eachsource particles made an improvement of 2.30%.We confirmed that this improvement has a signifi-cance level of 0.01 by using a two-sided binomialtest (two-sided sign test).
This indicates that thetechnique of separating training data for all sourceparticles is effective.Murata 2003 who used only seven features anddid not separate training data for any source par-ticles obtained an accuracy rate of 89.77% withEval.
B.
The method (92.00%) of using all fea-tures (32) made an improvement of 2.23% againsttheirs.
We confirmed that this improvement hada significance level of 0.01 by using a two-sidedbinomial test (two-sided sign test).
This indicatesthat our increased features are effective.KNP and Kondo obtained low accuracy rates(29.14% and 41.00% in Eval.
B for the open data).We did the evaluation using data and proved thatthese methods could work well.
A target predicatein the data is defined in the IPAL and VDIC dictio-naries.
The results are listed in Table 5.
KNP andKondo obtained relatively higher accuracy rates(76.07% and 78.85% in Eval.
B for the open data).However, they were lower than that for Baseline 3.Baseline 3 obtained a relatively high accuracyrate (84.17% and 88.20% in Eval.
B for the opendata).
Baseline 3 is similar to our method in termsof separating the training data into source parti-cles.
Baseline 3 separates the training data into592Table 6: Deletion of featuresDeleted Closed data Open datafeatures Eval.
A Eval.
B Eval.
A Eval.
BAcc.
Diff.
Acc.
Diff.
Acc.
Diff.
Acc.
Diff.Not deleted 91.22% ?
93.79% ?
92.28% ?
94.30% ?F1 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%F2 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%F3 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%F4 91.50% 0.28% 94.13% 0.34% 91.72% -0.56% 93.68% -0.62%F5 91.22% 0.00% 93.62% -0.17% 91.95% -0.33% 93.96% -0.34%F6 91.00% -0.22% 93.51% -0.28% 92.23% -0.05% 94.24% -0.06%F7 90.66% -0.56% 93.18% -0.61% 91.78% -0.50% 93.90% -0.40%F8 91.22% 0.00% 93.79% 0.00% 92.39% 0.11% 94.24% -0.06%F9 91.28% 0.06% 93.62% -0.17% 92.45% 0.17% 94.07% -0.23%F10 91.33% 0.11% 93.85% 0.06% 92.00% -0.28% 94.07% -0.23%F11 91.50% 0.28% 93.74% -0.05% 92.06% -0.22% 93.79% -0.51%F12 91.28% 0.06% 93.62% -0.17% 92.56% 0.28% 94.35% 0.05%F13 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%F14 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.41% 0.11%F15 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%F16 91.39% 0.17% 93.90% 0.11% 92.34% 0.06% 94.30% 0.00%F17 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%F18 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.46% 0.16%F19 91.33% 0.11% 93.90% 0.11% 92.28% 0.00% 94.30% 0.00%F20 91.11% -0.11% 93.68% -0.11% 92.34% 0.06% 94.35% 0.05%F21 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%F22 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%F23 91.28% 0.06% 93.79% 0.00% 92.28% 0.00% 94.24% -0.06%F24 91.22% 0.00% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%F25 89.54% -1.68% 92.11% -1.68% 90.04% -2.24% 92.39% -1.91%F26 91.16% -0.06% 93.74% -0.05% 92.28% 0.00% 94.30% 0.00%F27 91.22% 0.00% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%F28 90.94% -0.28% 93.51% -0.28% 92.11% -0.17% 94.13% -0.17%F29 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.30% 0.00%F30 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%F31 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.24% -0.06%F32 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%source particles and uses the most frequent tar-get case particle.
Our method involves separatingthe training data into source particles and usingmachine learning for each particle.
The fact thatBaseline 3 obtained a relatively high accuracy ratesupports the effectiveness of our method separat-ing the training data into source particles.6.2 Experiments confirming importance offeaturesWe next conducted experiments where we con-firmed which features were effective.
The resultsare listed in Table 6.
We can see the accuracy ratefor deleting features and the accuracy rate for us-ing all features.
We can see that not using F25greatly decreased the accuracy rate (about 2%).This indicates that F25 is particularly effective.F25 is the transformation rule Kondo used for Pand N in his method.
The transformation rules inKondo?s method were made precisely for ni (indi-rect object), which is particularly difficult to han-dle.
F25 is thus effective.
We could also see notusing F7 decreased the accuracy rate (about 0.5%).F7 has the semantic features for N. We found thatthe semantic features for N were also effective.6.3 Experiments changing number oftraining dataWe finally did experiments changing the numberof training data.
The results are plotted in Figure5.
We used our two methods of all features ?Di-vision?
and ?Non-division?.
We only plotted the593Figure 5: Changing number of training dataaccuracy rates for Eval.
B in the open data in thefigure.
We plotted accuracy rates when 1, 1/2, 1/4,1/8, and 1/16 of the training data were used.
?Divi-sion?, which separates training data for all sourceparticles, obtained a high accuracy rate (88.36%)even when the number of training data was small.In contrast, ?Non-division?, which does not sepa-rate training data for any source particles, obtaineda low accuracy rate (75.57%), when the number oftraining data was small.
This indicates that ourmethod of separating training data for all sourceparticles is effective.7 ConclusionWe developed a new method of transform-ing Japanese case particles when transformingJapanese passive sentences into active sentences.Our method separates training data for all input(source) particles and uses machine learning foreach particle.
We also used numerous rich featuresfor learning.
Our method obtained a high rate ofaccuracy (94.30%).
In contrast, a method that didnot separate training data for all source particlesobtained a lower rate of accuracy (92.00%).
In ad-dition, a method that did not have many rich fea-tures for learning used in a previous study obtaineda much lower accuracy rate (89.77%).
We con-firmed that these improvements were significantthrough a statistical test.
We also undertook ex-periments utilizing traditional methods using verbdictionaries and manually prepared heuristic rulesand confirmed that our method obtained muchhigher accuracy rates than traditional methods.We also conducted experiments on which fea-tures were the most effective.
We found thatKondo?s transformation rule used as a feature inour system was particularly effective.
We alsofound that semantic features for nominal targetswere effective.We finally did experiments on changing thenumber of training data.
We found that ourmethod of separating training data for all sourceparticles could obtain high accuracy rates evenwhen there were few training data.
This indicatesthat our method of separating training data for allsource particles is effective.The transformation of passive sentences into ac-tive sentences is useful in many research areasincluding generation, knowledge extraction fromdatabases written in natural languages, informa-tion extraction, and answering questions.
In thefuture, we intend to use the results of our study forthese kinds of research projects.ReferencesNello Cristianini and John Shawe-Taylor.
2000.
An Introduc-tion to Support Vector Machines and Other Kernel-basedLearning Methods.
Cambridge University Press.IPA.
1987.
(Information?Technology Promotion Agency,Japan).
IPA Lexicon of the Japanese Language for Com-puters IPAL (Basic Verbs).
(in Japanese).Keiko Kondo, Satoshi Sato, and Manabu Okumura.
2001.Paraphrasing by case alternation.
Transactions of Infor-mation Processing Society of Japan, 42(3):465?477.
(inJapanese).Taku Kudoh and Yuji Matsumoto.
2000.
Use of support vec-tor learning for chunk identification.
CoNLL-2000, pages142?144.Taku Kudoh.
2000.
TinySVM: Support Vector Machines.http://cl.aist-nara.ac.jp/?taku-ku//software/TinySVM/index.html.Sadao Kurohashi and Makoto Nagao.
1997.
Kyoto Univer-sity text corpus project.
3rd Annual Meeting of the Asso-ciation for Natural Language Processing, pages 115?118.
(in Japanese).Sadao Kurohashi, 1998.
Japanese Dependency/Case Struc-ture Analyzer KNP version 2.0b6.
Department of Infor-matics, Kyoto University.
(in Japanese).Masaki Murata and Hitoshi Isahara, 2003.
Conversion ofJapanese Passive/Causative Sentences into Active Sen-tences Using Machine Learning, pages 115?125.
SpringerPublisher.Masaki Murata, Qing Ma, and Hitoshi Isahara.
2002.
Com-parison of three machine-learning methods for Thai part-of-speech tagging.
ACM Transactions on Asian LanguageInformation Processing, 1(2):145?158.NLRI.
1964.
Bunrui Goi Hyou.
Shuuei Publishing.Hirotoshi Taira and Masahiko Haruno.
2001.
Feature se-lection in svm text categorization.
In Proceedings ofAAAI2001, pages 480?486.594
