Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 39?46,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning Rules and Categorization Networks for LanguageStandardizationGerhard B van Huyssteen Marelie H DavelHuman Language Technology Group Human Language Technology GroupCouncil for Scientific and Industrial Research Council for Scientific and Industrial ResearchPretoria, South Africa Pretoria, South Africagvhuyssteen@csir.co.za  mdavel@csir.co.zaAbstractIn this research, we use machine learningtechniques to provide solutions for descriptivelinguists in the domain of language standardi-zation.
With regard to the personal name con-struction in Afrikaans, we perform functionlearning from word pairs using the De-fault&Refine algorithm.
We demonstrate howthe extracted rules can be used to identify ir-regularities in previously standardized con-structions and to predict new forms of unseenwords.
In addition, we define a generic, auto-mated process that allows us to extract con-structional schemas and present these visuallyas categorization networks, similar to what isoften being used in Cognitive Grammar.
Weconclude that computational modeling of con-structions can contribute to new descriptivelinguistic insights, and to practical languagesolutions.1 IntroductionIn the main, constructionist approaches to grammarfocus on discovering generalizations in languageby analyzing clusters of usage-based instances oflinguistic phenomena.
Similarly, computationallinguistic approaches to grammar learning aim todiscover these very same patterns, using automatedtechniques such as machine learning (ML).In this research, we use techniques from ML toanalyze and predict irregular phenomena with li-mited data available, and then represent these phe-nomena visually in a way that is compatible withthe Cognitive Grammar descriptive framework (asa constructionist approach to grammar; henceforthCG).
Our grand goal is to develop language tech-nology tools that could be used in descriptive lin-guistics.
Specifically, we aim to (1) develop apredictor that could suggest derivational forms fornovel base-forms; and (2) automatically extractcategorization networks (i.e.
constructional sche-mas and the relationships between them) from adataset, which could serve as a heuristic input todescriptive linguistics.2 ContextualizationThis research originates from a practical problemrelated to language standardization.
Similar tostandardization bodies for languages like Dutch,and German, the ?Afrikaanse Taalkommisie?
(TK)is the official body responsible for the descriptionand regulation of Afrikaans spelling.
The TK regu-larly publishes the official orthography of Afri-kaans in the form of the Afrikaanse Woordelys enSpelre?ls (?Afrikaans Wordlist and SpellingRules?
; AWS (Taalkommissie, 2009)).One of the challenges faced by the TK is tostandardize the spelling of foreign place names(including names of countries, cities, regions,provinces, etc.
), and their derived forms (i.e.
adjec-tives, such as Amerika?ans ?American?
; and per-sonal names, such as Amerika?ner ?person fromAmerica?).
In the absence of sufficient usage-based39evidence, many variant forms are often being ac-cepted, either related to spelling or derivation;compare for instance the variant spelling formsMaskat or Masqat or Muskat ?Muscat?, or the va-riant derivational forms Turkmenistan?i or Turkme-nistan?ner ?person from Turkmenistan?.
The TK istherefore challenged with the task to give guide-lines regarding spelling and derivation, while facedwith highly irregular and sparse data containingmany variants.We contribute to address this challenge by dis-covering the constructions in seemingly unsyste-matic and irregular data.
Based on our tools andoutputs, the TK could then revise existing irregu-larities and variants, or use these tools to guidefuture decisions.3 Related Work3.1 Constructional SchemasMorphological constructions can be defined ascomposite symbolic assemblies (i.e.
complexform-meaning pairings) smaller than phrases, con-sisting of component structures between whichvalence relations hold (Van Huyssteen, 2010; seealso Tuggy, 2005).
One of the main componentstructures in morphological constructions is themorpheme, which is simply defined as a simplexsymbolic unit in the language system (i.e.
it doesnot contain smaller symbolic units as subparts).More schematic symbolic assemblies (i.e.
less spe-cified in their characterization) are referred to asconstructional schemas.Constructional schemas can be represented as anetwork with relationships of categorization hold-ing between different constructional schemas;these categorization networks provide the structur-al description of a construction (Langacker, 2008:222).
In the representations used in CG, categori-zation relationships of elaboration (i.e.
full instan-tiations of a schema), extension (i.e.
partialinstantiations), and correspondence are specified.Entrenchment and ease of activation is indicatedby the thickness of boxes: the thicker the line of abox, the more prototypical that unit is (Langacker,2008: 226; see also Figure 5).The aim of descriptive linguistics is to postulatecategorization networks that describe a construc-tion in a language, based on usage data.
Our re-search contributes to this aim by automaticallycreating visual representations of such languagemodels.
For our current research, we are specifical-ly interested in the personal name construction inAfrikaans.3.2 Afrikaans Personal Name ConstructionFormation of personal names by means of a per-sonal name creating derivational suffix (NRPERS) isa productive process in many languages.
The spe-cific category that we are investigating in this re-search is personal names derived from placenames, such as Trinidad?ees ?person from Trini-dad?.In one of the standard works on derivation inAfrikaans, Kempen (1969) identifies a number ofNRPERS suffixes that are used in derivations fromplace names.
He finds that there is no obvious sys-tematicity in their distribution (based on a datasetof 132 instances), but concludes that, in derivationsof foreign place names, the -ees and -s morphemesare most frequently used, with some distributionalso over -i, -n (especially -aan) and -r. In additionto some of the morphemes mentioned by Kempen(1969), Combrink (1990) also mentions a few,while excluding others.
In as far as we know, noother description of this construction in Afrikaanshas been done, and based on the difference be-tween Combrink (1990) and Kempen (1969), wecan also deduct that there is no comprehensive un-derstanding of this construction.Personal names from place names can be formedin four basic ways in Afrikaans: (1) suffixation(Aruba?an ?Arubian?
); (2) zero derivation (Aber-deen ?person from Aberdeen?
); (3) clipping andback-formation (Turk<Turkye ?person from Tur-key?
; Armeen<Armeni?
?person from Armenia?
);and (4) lexicalization (Cornwallis>Korni?r ?personfrom Cornwallis?).
In a rather large number of cas-es (119 in our dataset of 1,034; see 5.1) none of theabove strategies can be applied, and then paraph-rasing is being used (e.g.
?
persoon van Akkra ?aperson from Accra?
).Variants of morphemes (i.e.
allomorphs) existfor phonological reasons, of which a linking ele-ment is the most prominent (Combrink, 1990).Compare for example -aar in Brussel?aar ?personfrom Brussels?
(where the base-form is polysyllab-ic) vs. -enaar in Delft?enaar ?person from Delft?
(where the base-form is monosyllabic; Delftenaarcould therefore also be analyzed as Delft?en?aar).40For our purposes, we consider -enaar as an allo-morph (i.e.
elaboration) of ?aar, and is identifiedas such in our categorization network (see Figure5).
Similarly, we classify morphemes as allo-morphs in cases where an allomorph exists due toidentical vowel deletion (e.g.
-an as a variant of -aan when it combines with a base-form ending onan -a, as in Afrika?an ?person from Africa?
), as wellas consonant doubling after a short, stressed sylla-ble in the auslaut (e.g.
-mer as a variant of -er, asin Amsterdam?mer ?person from Amsterdam?
).3.3 Automatic Extraction of ConstructionalSchemasComputational modeling of morphology is a vastsubfield in computational linguistics, gainingpopularity since the 1980s.
Pioneering work in thefield has been done within the two-level morphol-ogy framework, and elaborations on this frame-work can be considered the basis of state-of-the-artmorphological analyzers today.
However, sinceconstructing such analyzers manually is hugelyexpensive in terms of time and human effort, theapproach does not scale well for new languages.To overcome this obstacle, many computationallinguists have developed techniques towards theautomatic learning of morphology (e.g.
Goldsmith,2001).
A key goal is to be able to produce a mor-phological analysis of the words of a corpus whenonly provided with the unannotated corpus.We are interested in the related goal of functionlearning: given a base-form of a word, learn otherforms of the word.
Most typically, function learn-ing takes pairs of words (base-forms plus in-flected/derived forms) as input to discover patternsin the data.
This is also the paradigm used in thecurrent paper.Several ML techniques have been used to solvespecific function learning tasks (such as learningthe past tense form of the English verb).
Ap-proaches include the use of decision trees, neuralnetworks, inductive logic programming, and statis-tical approaches (Shalonova & Flach, 2007).We are not aware of any work related to the au-tomated learning of categorization networks spe-cifically.4 ApproachOur research has two complementary goals, dealtwith separately: (1) to develop a predictor that cansuggest potential derivational forms for novel base-forms (and alternative forms for existing base-forms with irregular forms); and (2) to automati-cally extract categorization networks that are easilyinterpretable by linguists.4.1 Prediction of Derivational FormsIn order to analyze existing and predict new deri-vational forms, we use the Default&Refine (D&R)algorithm (Davel & Barnard, 2004).
This algorithmextracts context-sensitive rules from discrete data,and is particularly effective when learning fromsmall training sets.
It has the additional advantagethat rules generated are interpretable by humans.When applied to the grapheme-to-phoneme predic-tion task, it has been shown to outperform compar-ative algorithms (Davel & Barnard, 2008).The D&R algorithm defines a set of templatesand then uses a greedy search to find the most gen-eral rule (matching the templates) that describesthe training data in question.
Examples that aresuccessfully explained by this rule are removedfrom the data set and the process repeated.
When-ever a new rule contradicts examples previouslydealt with successfully, these are again added tothe training data to be ?re-explained?
by a laterrule.
The rule set therefore captures hierarchicaldefault behavior: the last rule defines the defaultbehavior for a specific pattern, and acts as a back-off rule to the second-last (more refined) rule,which would capture deviations from default beha-vior.
The second-last rule would then act as back-off to the third-last rule, and so forth.
Rules aretherefore explicitly ordered according to the re-verse rule extraction order.
(The rule extracted firstis matched last.
)Once a set of rules have been generated, thesedescribe the training data completely.
In addition,by tracing each of the possible rules that may applyto a new pattern (in order), various alternative de-rivational forms are identified, along with the evi-dence supporting each option (as in Table 2).4.2 Extraction of Categorization NetworksWhile the D&R rules extracted in Section Error!Reference source not found.
provide a perspec-tive on the phenomena that occur, these rule setscould become extremely large and, accordingly,more difficult to interpret.
We therefore attempt toextract categorization networks (a la CG) as visual41representations in a fully automated fashion.
Thesenetworks are more easily interpretable, especiallyto humans.An iterative string matching process is used tostructure ?potential morphemes?
within a directedtree.
Our main assumptions are that:?
the only input to the process consists of aset of unannotated word pairs: base-form +derivational form;?
a morpheme is added as a suffix;?
allomorphs are either shorter than the mainmorpheme (i.e.
characters removed) orlonger (i.e.
characters added); and?
preference is given to larger strings thatoccur systematically in the training data.The following steps are followed:1.
Generate a list of initial transformation classesbased on the word pairs provided.
These arederived through a comparison based on thelongest common substring of the derivationalform and its respective base-form (see Table1).
The classes specify the character string tobe removed from the base-form (if any), andthe replacement string; note that ellipses indi-cates the base-form (or part of it), and curlybrackets indicate deletions (i.e.
in China, de-lete the -a, and then add -ees).
If a place nameand its personal name are identical, the classwill be ?0?.Table 1: Examples of transformation classesPlacenamePersonalnameClass (constructionalschema)Aberdeen Aberdeen [[x] [0]]Amerika Amerikaner [[?]
[ner]]China Chinees [[?
{a}] [ees]]2.
Create a list of all transformation classes and,per transformation class, a set of all deriva-tional forms (referred to as the transformationderivations set).3.
For each transformation derivations set, findthe largest end-of-word string common to allmembers of that set (the set best string).
Theset of all ?set best strings?
are referred to as thebest string list and can be interpreted as a setof candidate morphemes.4.
For each transformation derivations set, con-sider the elements in the best string list, anddetermine if any subsets of the current set existthat match a larger string currently in the beststring list.
If so, partition the set into subsetsaccordingly.
(Each subset is therefore identi-fied by both a transformation class and a beststring.
For example, three different sets, eachwith a different best string may be related to asingle transformation class.
This makes it poss-ible to identify situations where an allomorphis created in other ways than simply adding themorpheme as a suffix.)5.
For each subset, update the set best stringbased on the latest partition; update the beststring list to reflect new best strings created.6.
Repeat steps (4) and (5) until no furtherchanges are made.
The set of morphemes areconsidered stable, and it now remains to struc-ture these elements into a visual categorizationnetwork.7.
In order to create the categorization network,we start with an empty directed graph.
Foreach set best string, create a list of all the trans-formation classes that are applicable (as calcu-lated above) and add these transformationclasses from largest to smallest to a singlebranch of the tree.
(One branch is created foreach string in the best string list, and is a firstattempt at capturing a morpheme along with itsdifferent variations.)8.
Consider the nodes at each level (all nodes thathave the same node as parent) and whereverone node fully contains another, move the con-tained node to become the parent of the other(cutting the link between the original parentnode and the contained node).
This process en-sures that morpheme candidates that are ac-tually variations of other morphemes aresuppressed at each level of the tree.9.
Now combine any nodes that occur in differentplaces in the tree but have identical transfor-mation classes, by merging the lower nodewith the higher node.
Only identical transfor-mation classes are merged.10.
For each node in the final tree, considerwhether the left hand side of the transforma-tion class can be refined, specifically by add-ing additional matching characters based onthe final transformation derivations set.The result of this process is a set of final transfor-mation classes, each describing a constructionalschema, and the relationships among these con-structional schemas, displayed as a categorizationnetwork.42Figure 1: Number of words, rules and initial trans-formations for the various person-x data sets5 Experimental Setup and Results5.1 DataThe dataset that we use is the list of foreign placenames and their corresponding personal namesfrom the AWS (Taalkommissie, 2009).
For pur-poses of brevity, we only report on suffixation andback-formation, and exclude cases with variantmorphemes, zero derivation and clipping, as wellas all cases of paraphrasing.
732 instances are re-tained (from the original dataset of 1,034 in-stances).A supplementary dataset consisting of adjectivalderivations of place names was also taken from theAWS and treated in the same manner as the per-sonal names; this dataset is used in Section 6.3 toverify certain of the findings.
This set contains 786instances.5.2 Development of PredictorThe full dataset is highly irregular, containingmany transformation classes that occur only once.We are interested in these irregularities (in order toidentify words that may need further review), aswell as in more systematic phenomena that occurin the data.
We therefore create different data sets;in each set (referred to as person-x) we only retainthose instances that occur x or more times in thetransformations.
(The person-1 set therefore con-tains all training data, including all exceptions,while the person-6 set only contains transforma-tions supported by 6 or more instances.)
In FigureFigure 2: Cross-validated rule accuracy for the per-son-x and adjective-x data sets.1 the number of words and number of uniquetransformation classes are displayed for each per-son-x data set.In order to verify the accuracy of our extractedrules, we use 10-fold cross-validation to obtain amean accuracy per data set, as depicted in Figure 2(labeled ?person?).
We also generate a rule setfrom the training and test data combined: this larg-er set is used to extract categorization networks.When the rule set is structured as a graph (calleda rule network), the data can be interpreted as fol-lows: the root node indicates the default transfor-mation, which applies unless any child node ismatched by the base-form, which again only ap-plies unless a child of the child node matches thebase-form (and so forth), which indicates that amore refined rule should be applied.
A small partof a rule network is displayed in Figure 3, witheach node listing the end-of-word string of thebase-form that will trigger the rule, the transforma-tion rule that will be applied, and the number ofinstances of the rule in the training data.
The com-plete rule network is very large: 266 nodes for theperson-1 data set, as indicated in Figure 1.As was expected, a large number of exceptionalrules are generated, indicating much inconsistencyin how derivations are formed.
For the person-1data set, 217 exceptions are identified.
For each ofthese exceptions, alternatives are suggested in or-der of prototypicality by tracing the rule network,as illustrated for the base-form Smirna in Table 2.Automatically generated tables like these provide apractical tool for language standardization.43Figure 3: A small subsection of a rule networkTable 2: Alternative suggestions for the exception:Smirna -> SmirniootAlternative Instances ExamplesSmirna 1 Smirna>SmirniootSmirnees 1 Navarra>NavarreesSmirnaan 58 Sparta>SpartaanAstana>AstanaanSmirnaer 155 Hiroshima>HiroshimaerBreda>Bredaer5.3 Development of Categorization NetworksThe categorization network in Figure 5 was com-piled automatically, as described in 4.2.
Note thatthis specific categorization network is based onconstruction schemas with three or more support-ing examples per node; for the sake of brevity, wedo not include the full categorization network(based on all the examples) in this paper.The relative prototypicality of constructionalschemas (indicated by the thickness of lines inFigure 5) is determined post hoc by observing dis-tribution frequencies.
We obtain four natural clus-ters in this way: highly prototypical (hundred ormore instantiations), prototypical (forty or moreinstantiations), less prototypical (three or more in-stantiations), and unprototypical (less than threeinstantiations, therefore also including exceptions);the latter category is not included in Figure 5.Full instantiations of a schema (i.e.
relationshipsof elaboration) is indicated with solid arrows; thehighest node in our network represents the seman-tic pole, and is here simply indicated as [[PLACE X][NRPERS]].
For each node in the network, we alsoindicate the class frequency, and provide three ex-amples of the base-form.6 Discussion6.1 PredictorThe extracted rules immediately provide us with:?
An indication of the predictability of thedata (rule accuracy);?
A set of all exceptions (single instancesthat require an individual rule to describethat instance); and?
A predictor of new forms (applying therules to unseen words).From the accuracies depicted in Figure 2, it is clearthat the full data set, including all phenomena thatonly occur once, describes a difficult learning task,with an overall accuracy of only 63.2% achieved.When more systematic phenomena are investigated(i.e.
transformations with six or more instances),our classification accuracy quickly increases above80%, indicating that the predictor is in fact usable.An error analysis reveals that improvements maybe possible by taking pronunciation informationinto account (stress patterns, syllable information,consonant categories, etc.
).A standardization body such as the TK coulduse the automatically generated list of exceptions(similar to Table 2) to review prior standardizationdecisions.
In addition, the predictor can be used tosuggest derivational forms for novel base-forms,which could then be verified with usage data.6.2 Categorization NetworksFrom Figure 5, observe that we have identifiedseven basic morphemes (i.e.
nodes on the highestlevel), viz.
-aan, -aar, -ees, -er, -i, -iet and -?r;with the exception of the latter, all these corres-pond to the morphemes identified by Kempen(1969) and Combrink (1990).
Linguistically speak-ing, -?r is actually an extension of the [[?]
[er]]construction, since the e-trema is used in Afrikaansorthography as a variant of the letter ?e?
to signifya syllable with a null onset, preceded by a syllablewithout a coda.
However, our algorithm treated -erand -?r as two separate morphemes.We can also observe that the [[?]
[er]] con-structional schema can be considered the most pro-totypical schema (based on frequency).
Otherprototypical constructional schemas include [[?a][an]], [[?]
[ner]] and [[?]
[?r]] (with the lattertwo actually instantiations of [[?]
[er]]).
Within a44CG framework, it is assumed that these prototypi-cal constructional schemas are more likely to beactivated for the categorization of novel examples.This observation contradicts Kempen?s (1969)finding that there is no obvious systematicity in thedistribution of personal name forming suffixes, aswell as his finding that the -ees and -s morphemesare most frequently used.
Conversely, we did notfind in our data significant evidence for the promi-nence that Kempen (1969) and Combrink (1990)give to morphemes/allomorphs such as -der, -lees,-naar, -aner, -een, -ein/-yn or -ioot; that does notmean that these do not exist ?
they are just not asprominent as these previous descriptions mighthave made us believe.Furthermore, if we look at allomorphs due tolinking elements, we identified six, viz.
-nees,-enaar, -iaan, -ner, -ter and -i?r.
With the excep-tion of -nees, all these have also been identified byKempen (1969) and Combrink (1990).
If we lookclosely at the instantiations of [[?]
[nees]], we seethat all base-form examples end on the stressedsyllables [an] or [on], with the exception of Baliand Mali.
A standardization body could thereforeinvestigate whether these two examples could notbe classified better under the [[?]
[?r]] construc-tional schema, resulting in, for example, Bali?
?r, aswe also find in Dutch.
If this could be the case,then it would make sense why -nees has not beenidentified by other morphologists, since it wouldthen be a case of an allomorph due to consonantdoubling, and not due to a linking element.A similar closer look at -ees vs. -nees shows thatall instantiations of the base-forms of [[?]
[nees]]end on a stressed syllable, while those for [[?
][ees]] are unstressed.
In the data, there is only oneexception to the latter schema, viz.
Gaboen?ees?person from Gabon?.
Since Gaboen ends on astressed syllable, it would actually fit better underthe [[?]
[nees]] constructional schema.
Supportfor this hypothesis comes from Donaldson (1993),where he indicates that it should be spelled Ga-boen?nees.
In the absence of usage data, and basedon this categorization network, the TK could there-fore reconsider the spelling of Gaboen?ees.Several similar observations can be made re-garding inconsistencies in the data (e.g.
inconsis-tencies regarding base-forms ending on [stan]).
Inthis sense, categorization networks like these couldbe a helpful descriptive tool for a standardizationbody in finding systematicity in data and rules.6.3 Supplementary Data: Adjectival Deriva-tionsIn order to validate the generic process, the fullprocess (as described in 4.1 and 4.2) is repeatedusing the supplementary data set of adjectivalforms described in 5.1.
Results are positive: a simi-larly efficient learning curve is obtained (see Fig-ure 2) and the categorization network, althoughquite different, is similarly interpretable (Figure 4).Figure 4: Categorization network for the adjective-4data set7 Conclusion and Future WorkIn this paper, we presented a methodology to au-tomatically discover constructional schemas fromhighly irregular data, and to represent these in away that is both interpretable by computers (pre-dictive rule sets) and humans (categorization net-works).
The graphical representation is by andlarge compatible with one of the major Construc-tion Grammar theories, viz.
CG: we show proto-typical examples (based on frequency), and alsoindicate relationships of elaboration.
In futurework, these representations could be further re-fined, to also indicate relationships of extensionsand correspondences.
We have illustrated howthese representations could provide insight in ourknowledge of the morphology of Afrikaans, aswell as providing practical language solutions forlanguage standardization (such as the predictor andthe tables with alternative suggestions).Other future work will continue in two direc-tions: (1) refining the current tool for predictingderivational forms by taking additional features45into account, incorporating data that was left out inour current experiments (such as zero derivations),and benchmarking our results with regard to alter-native approaches; and (2) applying our algorithmto describe other morphological constructions.AcknowledgmentsVan Huyssteen is jointly affiliated with North-West University.
Support by NWU is hereby ac-knowledged.Part of this research was made possible througha research grant by the South African National Re-search Foundation (FA207041600015).We would like to extend our gratitude to Handr?Groenewald and Martin Puttkammer for their help.ReferencesCombrink, J.G.H.
1990.
Afrikaanse morfologie [Afri-kaans morphology].
Pretoria: Academica.Davel, M. & Barnard, E. 2004.
A default-and-refinement approach to pronunciation prediction.Proceedings of the 15th Annual Symposium of thePattern Recognition Association of South Africa.Grabouw, November 2004. pp 119-123.Davel, M. & Barnard, E. 2008.
Pronunciation Predictionwith Default & Refine.
Computer Speech and Lan-guage.
22: 374-393.Donaldson, B.C.
1993.
A Grammar of Afrikaans.
Berlin:Mouton de Gruyter.Goldsmith, J.
2001.
Unsupervised Learning of the Mor-phology of a Natural Language.
Computational Lin-guistics 27, pp.
153-198.Kempen, W. 1969.
Samestelling, afleiding en woord-soortelike meerfunksionaliteit in Afrikaans [Com-pounding, derivation and change of part-of-speechcategory in Afrikaans].
Kaapstad: Nasou.Langacker, R.W.
2008.
Cognitive Grammar: A BasicIntroduction.
Oxford: Oxford University Press.Shalonova, K. & Flach, P. 2007.
Morphology learningusing tree of aligned suffix rules.
ICML Workshop:Challenges and Applications of Grammar Induction.Taalkommissie.
(comp.).
2009.
Afrikaanse Woordelysen Spelre?ls [Afrikaans Wordlist and Spelling Rules].Tenth edition.
Kaapstad: Pharos Dictionaries.Tuggy, D. 2005.
Cognitive Approach to Word-Formation.
In: ?tekauer, P. & Lieber, R.
(eds.
).Handbook of Word-Formation.
Dordrecht: Springer.pp.
233-265.Van Huyssteen, GB.
2010.
(Re)defining ComponentStructures in Morphological Constructions: A Cogni-tive Grammar Perspective.
In: Michel, S & Onysko,A (eds.).
Cognitive Approaches to Word-Formation.Berlin: Mouton de Gruyter.
pp.
97-126.Figure 5: Categorization network for the person-4data set46
