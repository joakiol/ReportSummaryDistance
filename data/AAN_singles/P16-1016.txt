Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 161?171,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Transition-Based System for Joint Lexical and Syntactic AnalysisMatthieu ConstantUniversit?e Paris-Est, LIGM (UMR 8049)Alpage, INRIA, Universit?e Paris DiderotParis, FranceMatthieu.Constant@u-pem.frJoakim NivreUppsala UniversityDept.
of Linguistics and PhilologyUppsala, Swedenjoakim.nivre@lingfil.uu.seAbstractWe present a transition-based system thatjointly predicts the syntactic structure andlexical units of a sentence by buildingtwo structures over the input words: asyntactic dependency tree and a forest oflexical units including multiword expres-sions (MWEs).
This combined represen-tation allows us to capture both the syn-tactic and semantic structure of MWEs,which in turn enables deeper downstreamsemantic analysis, especially for semi-compositional MWEs.
The proposed sys-tem extends the arc-standard transitionsystem for dependency parsing with tran-sitions for building complex lexical units.Experiments on two different data setsshow that the approach significantly im-proves MWE identification accuracy (andsometimes syntactic accuracy) comparedto existing joint approaches.1 IntroductionMultiword expressions (MWEs) are sequencesof words that form non-compositional semanticunits.
Their identification is crucial for semanticanalysis, which is traditionally based on the prin-ciple of compositionality.
For instance, the mean-ing of cut the mustard cannot be compositionallyderived from the meaning of its elements and theexpression therefore has to be treated as a singleunit.
Since Sag et al (2002), MWEs have attractedgrowing attention in the NLP community.Identifying MWEs in running text is challeng-ing for several reasons (Baldwin and Kim, 2010;Seretan, 2011; Ramisch, 2015).
First, MWEs en-compass very diverse linguistic phenomena, suchas complex grammatical words (in spite of, be-cause of), nominal compounds (light house), non-canonical prepositional phrases (above board),verbal idiomatic expressions (burn the midnightoil), light verb constructions (have a bath), multi-word names (New York), and so on.
They can alsobe discontiguous in the sense that the sequence caninclude intervening elements (John pulled Mary?sleg).
They may also vary in their morphologi-cal forms (hot dog, hot dogs), in their lexical el-ements (lose one?s mind/head), and in their syn-tactic structure (he took a step, the step he took).The semantic processing of MWEs is furthercomplicated by the fact that there exists a contin-uum between entirely non-compositional expres-sions (piece of cake) and almost free expressions(traffic light).
Many MWEs are indeed semi-compositional.
For example, the compound whitewine denotes a type of wine, but the color of thewine is not white, so the expression is only par-tially transparent.
In the light verb constructiontake a nap, nap keeps its usual meaning but themeaning of the verb take is bleached.
In addition,the noun can be compositionally modified as intake a long nap.
Such cases show that MWEs maybe decomposable and partially analyzable, whichimplies the need for predicting their internal struc-ture in order to compute their meaning.From a syntactic point of view, MWEs oftenhave a regular structure and do not need specialsyntactic annotation.
Some MWEs have an irreg-ular structure, such as by and large which on thesurface is a coordination of a preposition and anadjective.
They are syntactically as well as seman-tically non-compositional and cannot be repre-sented with standard syntactic structures, as statedin Candito and Constant (2014).
Many of theseirregular MWEs are complex grammatical wordslike because of, in spite of and in order to ?
fixed(grammatical) MWEs in the sense of Sag et al(2002).
In some treebanks, these are annotated us-ing special structures and labels because they can-161not be modified or decomposed.
We hereafter usethe term fixed MWE to refer to either fixed or ir-regular MWEs.In this paper, we present a novel representationthat allows both regular and irregular MWEs to beadequately represented without compromising thesyntactic representation.
We then show how thisrepresentation can be processed using a transition-based system that is a mild extension of a standarddependency parser.
This system takes as input asentence consisting of a sequence of tokens andpredicts its syntactic dependency structure as wellas its lexical units (including MWEs).
The result-ing structure combines two factorized substruc-tures: (i) a standard tree representing the syntacticdependencies between the lexical elements of thesentence and (ii) a forest of lexical trees includingMWEs identified in the sentence.
Each MWE isrepresented by a constituency-like tree, which per-mits complex lexical units like MWE embeddings(for example, [[Los Angeles ] Lakers], I will [takea [rain check]]).
The syntactic and lexical struc-tures are factorized in the sense that they share lex-ical elements: both tokens and fixed MWEs.The proposed parsing model is an extension ofa classical arc-standard parser, integrating specifictransitions for MWE detection.
In order to dealwith the two linguistic dimensions separately, ituses two stacks (instead of one).
It is synchro-nized by using a single buffer, in order to handlethe factorization of the two structures.
It also in-cludes different hard constraints on the system inorder to reduce ambiguities artificially created bythe addition of new transitions.
To the best of ourknowledge, this system is the first transition-basedparser that includes a specific mechanism for han-dling MWEs in two dimensions.
Previous relatedresearch has usually proposed either pipeline ap-proaches with MWE identification performed ei-ther before or after dependency parsing (Kong etal., 2014; Vincze et al, 2013a) or workaroundjoint solutions using off-the-shelf parsers trainedon dependency treebanks where MWEs are an-notated by specific subtrees (Nivre and Nilsson,2004; Eryi?git et al, 2011; Vincze et al, 2013b;Candito and Constant, 2014; Nasr et al, 2015).2 Syntactic and Lexical RepresentationsA standard dependency tree represents syntacticstructure by establishing binary syntactic relationsbetween words.
This is an adequate representa-tion of both syntactic and lexical structure on theassumption that words and lexical units are in aone-to-one correspondence.
However, as arguedin the introduction, this assumption is broken bythe existence of MWEs, and we therefore need todistinguish lexical units as distinct from words.In the new representation, each lexical unit ?whether a single word or an MWE ?
is asso-ciated with a lexical node, which has linguisticattributes such as surface form, lemma, part-of-speech tag and morphological features.
With anobvious reuse of terminology from context-freegrammar, lexical nodes corresponding to MWEsare said to be non-terminal, because they haveother lexical nodes as children, while lexical nodescorresponding to single words are terminal (anddo not have any children).Some lexical nodes are also syntactic nodes,that is, nodes of the syntactic dependency tree.These nodes are either non-terminal nodes corre-sponding to (complete) fixed MWEs or terminalnodes corresponding to words that do not belongto a fixed MWE.
Syntactic nodes are connectedinto a tree structure by binary, asymmetric depen-dency relations pointing from a head node to a de-pendent node.Figure 1 shows the representation of the sen-tence the prime minister made a few good de-cisions.
It contains three non-terminal lexicalnodes: one fixed MWE (a few), one contigu-ous non-fixed MWE (prime minister) and onediscontiguous non-fixed MWE (made decisions).Of these, only the first is also a syntactic node.Note that, for reasons of clarity, we have sup-pressed the lexical children of the fixed MWE inFigure 1.
(The non-terminal node correspond-ing to a few has the lexical children a and few.
)For the same reason, we are not showing thelinguistic attributes of lexical nodes.
For ex-ample, the node made-decisions has the follow-ing set of features: surface-form=?made deci-sions?, lemma=?make decision?, POS=?V?.
Non-fixed MWEs have regular syntax and their compo-nents might have some autonomy.
For example,in the light verb construction made-decisions, thenoun decisions is modified by the adjective goodthat is not an element of the MWE.The proposed representation of fixed MWEs isan alternative to using special dependency labelsas has often been the case in the past (Nivre andNilsson, 2004; Eryi?git et al, 2011).
In addition162the prime minister made a few good decisionsdetmod subj modobjmodmade-decisionsprime-ministerFigure 1: Representation of syntactic and lexical structure.she took a rain checksubjobjdetmodtook-rain-checkrain-checkFigure 2: Lexical structure of embedded MWEs.to special labels, MWEs are then represented as aflat subtree of the syntactic tree.
The root of thesubtree is the left-most or right-most element ofthe MWE, and all the other elements are attachedto this root with dependencies having special la-bels.
Despite the special labels, these subtreeslook like ordinary dependency structures and mayconfuse a syntactic parser.
In our representation,fixed MWEs are instead represented by nodes thatare atomic with respect to syntactic structure (butcomplex with respect to lexical structure), whichmakes it easier to store linguistic attributes thatbelong to the fixed MWE and cannot be derivedfrom its components.
The new representation alsoallows us to represent the hierarchical structure ofembedded MWEs.
Figure 2 provides an analysisof she took a rain check that includes such an em-bedding.
The lexical node took-rain-check corre-sponds to a light verb construction where the ob-ject is a compound noun that keeps its semantic in-terpretation whereas the verb has a neutral value.One of its children is the lexical node rain-checkcorresponding to a compound noun.Let us now define the representation formally.Given a sentence x = x1, .
.
.
, xnconsisting of ntokens, the syntactic and lexical representation isa quadruple (V, F,N,A), where1.
V is the set of terminal nodes, correspondingone-to-one to the tokens x1, .
.
.
, xn,2.
F is a set of n-ary trees on V , with eachtree corresponding to a fixed MWE and theroot labeled with the part-of-speech tag forthe MWE,3.
N is a set of n-ary trees on F , with each treecorresponding to a non-fixed MWE and theroot labeled with the part-of-speech tag forthe MWE,4.
A is a set of labeled dependency arcs defininga tree over F .This is a generalization of the standard definitionof a dependency tree (see, for example, K?ubler etal.
(2009)), where the dependency structure is de-fined over an intermediate layer of lexical nodes(F ) instead of directly on the terminal nodes (V ),with an additional layer of non-fixed MWEs addedon top.
To exemplify the definition, here are theformal structures corresponding to the representa-tion visualized in Figure 1.V = {1, 2, 3, 4, 5, 6, 7, 8}F = {1, 2, 3, 4,A(5, 6), 7, 8}N = {1,N(2, 3),V(4, 8),A(5, 6), 7}A = {(3, det, 1), (3,mod, 2),(4, subj, 3), (4, obj, 8),(8,mod,A(5, 6)), (8,mod, 7)}Terminal nodes are represented by integers corre-sponding to token positions, while trees are repre-sented by n-ary terms t(c1, .
.
.
, cn), where t is apart-of-speech tag and c1, .
.
.
, cnare the subtreesimmediately dominated by the root of the tree.The total set of lexical nodes is L = V ?
F ?N ,where V contains the terminal and (F ?
N) ?
Vthe non-terminal lexical nodes.
The set of syntac-tic nodes is simply F .It is worth noting that the representation im-poses some limitations on what MWEs can be rep-resented.
In particular, we can only represent over-lapping MWEs if they are cases of embedding,that is, cases where one MWE is properly con-tained in the other.
For example, in an examplelike she took a walk then a bath, it might be ar-gued that took should be part of two lexical units:took-walk and took-bath.
This cannot currently berepresented.
By contrast, we can accommodatecases where two lexical units are interleaved, asin the French example il prend un cachet et demi,with the two units prend-cachet and un-et-demi,which occur in the crossed pattern A1 B1 A2 B2.However, while these cases can be represented in163principle, the parsing model we propose will notbe capable of processing them.Finally, it is worth noting that, although our rep-resentation in general allows lexical nodes with ar-bitrary branching factor for flat MWEs, it is oftenconvenient for parsing to assume that all trees arebinary (Crabb?e, 2014).
For the rest of the paper,we therefore assume that non-binary trees are al-ways transformed into equivalent binary trees us-ing either right or left binarization.
Such transfor-mations add intermediate temporary nodes that areonly used for internal processing.3 Transition-Based ModelA transition-based parser is based on three compo-nents: a transition system for mapping sentencesto their representation, a model for scoring differ-ent transition sequences (derivations), and a searchalgorithm for finding the highest scoring transitionsequence for a given input sentence.
FollowingNivre (2008), we define a transition system as aquadruple S = (C, T, cs, Ct) where:1.
C is a set of configurations,2.
T is a set of transitions, each of which is apartial function t : C ?
C,3.
csis an initialization function that maps eachinput sentence x to an initial configurationcs(x) ?
C,4.
Ct?
C is a set of terminal configurations.A transition sequence for a sentence x is a se-quence of configurations C0,m= c0, .
.
.
, cmsuchthat c0= cs(x), cm?
Ct, and for every ci(0 ?
i < m) there is some transition t ?
T suchthat t(ci) = ci+1.
Every transition sequence de-fines a representation for the input sentence.Training a transition-based parser means train-ing the model for scoring transition sequences.This requires an oracle that determines what isan optimal transition sequence given an input sen-tence and the correct output representation (asgiven by treebank).
Static oracles define a singleunique transition sequence for each input-outputpair.
Dynamic oracles allow more than one opti-mal transition sequence and can also score non-optimal sequences (Goldberg and Nivre, 2013).Once a scoring model has been trained, parsingis usually performed as best-first search under thismodel, using greedy search or beam search.3.1 Arc-Standard Dependency ParsingOur starting point is the arc-standard transitionsystem for dependency parsing first defined inNivre (2004) and represented schematically inFigure 3.
A configuration in this system consistsof a triple c = (?, ?,A), where ?
is a stack con-taining partially processed nodes, ?
is a buffercontaining remaining input nodes, andA is a set ofdependency arcs.
The initialization function mapsx = x1, .
.
.
, xnto cs(x) = ([ ], [1, .
.
.
, n], { }),and the set Ctof terminal configurations containsany configuration of the form c = ([i], [ ], A).
Thedependency tree defined by such a terminal con-figuration is ({1, .
.
.
, n}, A).
There are three pos-sible transitions:?
Shift takes the first node in the buffer andpushes it onto the stack.?
Right-Arc(k) adds a dependency arc (i, k, j)to A, where j is the first and i the second el-ement of the stack, and removes j from thestack.?
Left-Arc(k) adds a dependency arc (j, k, i)to A, where j is the first and i the second el-ement of the stack, and removes i from thestack.A transition sequence in the arc-standard systembuilds a projective dependency tree over the set ofterminal nodes in V .
The tree is built bottom-upby attaching dependents to their head and remov-ing them from the stack until only the root of thetree remains on the stack.3.2 Joint Syntactic and Lexical AnalysisTo perform joint syntactic and lexical analysis weneed to be able to build structure in two parallel di-mensions: the syntactic dimension, represented bya dependency tree, and the lexical dimension, rep-resented by a forest of (binary) trees.
The two di-mensions share the token-level representation, aswell as the level of fixed MWEs, but the syntactictree and the non-fixed MWEs are independent.We extend the parser configuration to use twostacks, one for each dimension, but only onebuffer.
In addition, we need not only a set of de-pendency arcs, but also a set of lexical units.
Aconfiguration in the new system therefore consistsof a quintuple c = (?1, ?2, ?, A, L), where ?1and ?2are stacks containing partially processednodes (which may now be complex MWEs), ?
isa buffer containing remaining input nodes (which164Initial: ([ ], [0, .
.
.
, n], { })Terminal: ([i], [ ], A)Shift: (?, i|?,A) ?
(?|i, ?, A)Right-Arc(k): (?|i|j, ?,A) ?
(?|i, ?, A ?
{(i, k, j)})Left-Arc(k): (?|i|j, ?,A) ?
(?|j, ?,A ?
{(j, k, i)})Figure 3: Arc-standard transition system.Initial: ([ ], [ ], [0, .
.
.
, n], { }, { })Terminal: ([x], [ ], [ ], A, L)Shift: (?1, ?2, i|?,A, L) ?
(?1|i, ?2|i, ?, A, L)Right-Arc(k): (?1|x|y, ?2, ?, A, L) ?
(?1|x, ?2, ?, A ?
{(x, k, y)}, L)Left-Arc(k): (?1|x|y, ?2, ?, A, L) ?
(?1|y, ?2, ?, A ?
{(y, k, x)}, L)MergeF(t): (?1|x|y, ?2|x|y, ?,A, L) ?
(?1|t(x, y), ?2|t(x, y), ?, A, L)MergeN(t): (?1, ?2|x|y, ?,A, L) ?
(?1, ?2|t(x, y), ?, A, L)Complete: (?1, ?2|x, ?,A, L) ?
(?1, ?2, ?, A, L ?
{x})Figure 4: Transition system for joint syntactic and lexical analysis.are always tokens), A is a set of dependency arcs,and L is a set of lexical units (tokens or MWEs).The initialization function maps x = x1, .
.
.
, xnto cs(x) = ([ ], [ ], [1, .
.
.
, n], { }, { }), and the setCtof terminal configurations contains any config-uration of the form c = ([x], [ ], [ ], A, L).
Thedependency tree defined by such a terminal con-figuration is (F,A), and the set of lexical units isV ?
L. Note that the set F of syntactic nodes isnot explicitly represented in the configuration butis implicitly defined by A.
Similarly, the set Lonly contains F ?N .The new transition system is shown in Figure 4.There are now six possible transitions:?
Shift takes the first node in the buffer andpushes it onto both stacks.
This guaranteesthat the two dimensions are synchronized atthe token level.?
Right-Arc(k) adds a dependency arc(x, k, y) to A, where y is the first and x thesecond element of the syntactic stack (?1),and removes y from this stack.
It does notaffect the lexical stack (?2).1?
Left-Arc(k) adds a dependency arc (y, k, x)toA, where y is the first and x the second ele-ment of the syntactic stack (?1), and removesx from this stack.
Like Right-Arc(k), it does1We use the variables x and y, instead of i and j, becausethe stack elements can now be complex lexical units as wellas simple tokens.not affect the lexical stack (?2).?
MergeF(t) applies in a configuration wherethe two top elements x and y are identical onboth stacks and combines these elements intoa tree t(x, y) representing a fixed MWE withpart-of-speech tag t. Since it operates on bothstacks, the new element will be a syntacticnode as well as a lexical node.?
MergeN(t) combines the two top elementsx and y on the lexical stack (?2) into a treet(x, y) representing a non-fixed MWE withpart-of-speech tag t. Since it only operateson the lexical stack, the new element will notbe a syntactic node.?
Complete moves the top element x on thelexical stack (?2) to L, making it a final lex-ical unit in the output representation.
Notethat x can be a simple token, a fixed MWE(created on both stacks), or a non-fixed MWE(created only on the lexical stack).A transition sequence in the new system derivesthe set of lexical nodes and simultaneously buildsa projective dependency tree over the set of syntac-tic nodes.
By way of example, Figure 5 shows thetransition sequence for the example in Figure 1.3.3 Implicit CompletionThe system presented above has one potentialdrawback: it needs a separate Complete transi-tion for every lexical unit, even in the default case165Transition Configuration([ ], [ ], [1, 2, 3, 4, 5, 6, 7, 8], A0= { }, L0= { })Shift ?
([1], [1], [2, 3, 4, 5, 6, 7, 8], A0, L0)Complete ?
([1], [ ], [2, 3, 4, 5, 6, 7, 8], A0, L1= L0?
{1})Shift ?
([1, 2], [2], [3, 4, 5, 6, 7, 8], A0, L1)Shift ?
([1, 2, 3], [2, 3], [4, 5, 6, 7, 8], A0, L1)MergeN(N) ?
([1, 2, 3], [N(2, 3)], [4, 5, 6, 7, 8], A1, L1)Complete ?
([1, 2, 3], [ ], [4, 5, 6, 7, 8], A0, L2= L1?
{N(2, 3)})Left-Arc(mod) ?
([1, 3], [ ], [4, 5, 6, 7, 8], A1= A0?
{(3,mod, 2)}, L2)Left-Arc(det) ?
([3], [ ], [4, 5, 6, 7, 8], A2= A1?
{(3, det, 1)}, L2)Shift ?
([3, 4], [4], [5, 6, 7, 8], A2, L2)Left-Arc(subj) ?
([4], [4], [5, 6, 7, 8], A3= A2?
{(4, subj, 3)}, L2)Shift ?
([4, 5], [4, 5], [6, 7, 8], A3, L2)Shift ?
([4, 5, 6], [4, 5, 6], [7, 8], A3, L2)MergeF(A) ?
([4,A(5, 6)], [4,A(5, 6)], [7, 8], A3, L2)Complete ?
([4,A(5, 6)], [4], [7, 8], A3, L3= L2?
{A(5, 6)})Shift ?
([4,A(5, 6), 7], [4, 7], [8], A3, L3)Complete ?
([4,A(5, 6), 7], [4], [8], A3, L4= L3?
{7})Shift ?
([4,A(5, 6), 7, 8], [4, 8], [ ], A3, L4)Left-Arc(mod) ?
([4,A(5, 6), 8], [4, 8], [ ], A4= A3?
{(8,mod, 7)}, L4)Left-Arc(mod) ?
([4, 8], [4, 8], [ ], A5= A4?
{(8,mod,A(5, 6))}, L4)MergeN(V) ?
([4, 8], [V(4, 8)], [ ], A5, L4)Complete ?
([4, 8], [ ], [ ], A5, L5= L4?
{V(4, 8)})Right-Arc(obj) ?
([4], [ ], [ ], A6= A5?
{(4, obj, 8)}, L5)Figure 5: Transition sequence for joint syntactic and lexical analysis.when a lexical unit is just a token.
This makessequences much longer and increases the inherentambiguity.
One way to deal with this problem isto make the Complete transition implicit and de-terministic, so that it is not scored by the model(or predicted by a classifier in the case of deter-ministic parsing) but is performed as a side effectof the Right-Arc and Left-Arc transitions.
Everytime we apply one of these transitions, we checkwhether the dependent x of the new arc is part ofa unit y on the lexical stack satisfying one of thefollowing conditions: (i) x = y; (ii) x is a lexi-cal child of y and every lexical node z in y eitherhas a syntactic head in A or is the root of the de-pendency tree.
If (i) or (ii) is satisfied, we move yfrom the lexical stack to the set L of lexical unitsas a side effect of the arc transition.4 ExperimentsThis section provides experimental results ob-tained with a simple implementation of our sys-tem using a greedy search parsing algorithm and alinear model trained with an averaged perceptronwith shuffled examples and a static oracle.
Moreprecisely, the static oracle is defined using the fol-lowing transition priorities: MergeF> MergeN>Complete> LeftArc> RightArc> Shift.
At eachstate of the training phase, the static oracle selectsthe valid transition that has the higher priority.We evaluated the two variants of the system,namely Explicit and Implicit, with explicit and im-plicit completion, respectively.
They were com-pared against the joint approach proposed in Can-dito and Constant (2014) that we applied to an arc-standard parser, instead of a graph-based parser.The parser is trained on a treebank where MWEstatus and grammatical function are concatenatedin arc labels.
We consider it as the Baseline.We used classical transition-based parsing fea-tures consisting of patterns combining linguisticattributes of nodes on the stacks and the buffer, aswell as processed subtrees and transition history.We can note that the joint systems do not containfeatures sharing elements of both stacks.
Prelimi-nary tuning experiments did not show gains whenusing such features.We also compared these systems against weakerones, obtained by disabling some transitions andusing one stack only.
Two systems, namelySyntactic-baseline and Syntactic only predict thesyntactic nodes and the dependency structure byusing respectively a baseline parser and our systemwhere neither the lexical stack nor the MergeNandComplete transitions are used.
The latter one is animplementation of the proposal in Nivre (2014).Two systems are devoted only to the lexical layer:Lexical only recognizes the lexical units (only thelexical stack and the MergeNand Complete transi-tions are activated); Fixed only identifies the fixedexpressions.166CorpusEWT FTBTrain Test Train Dev Test# sent.
3,312 500 14,759 1,235 2,541# tokens 48,408 7,171 443,113 38,820 75,216# MWEs 2,996 401 23,556 2,119 4,043# fixed - - 10,987 925 1,992Table 1: Dataset statistics.We also implemented pipeline systems where:(i) fixed MWEs are identified by applying only theFixed system; (ii) elements of predicted MWEsare merged into single tokens; (iii) the retokenizedtext is parsed using the Baseline or Implicit sys-tems trained on a dataset where fixed MWEs con-sist of single tokens.We carried out our experiments on two differ-ent datasets annotating both the syntactic struc-ture and the MWEs: the French Treebank [FTB](Abeill?e et al, 2003) and the STREUSLE corpus(Schneider et al, 2014b) combined with the En-glish Web Treebank [EWT] (Bies et al, 2012).They are commonly used for evaluating the mostrecent MWE-aware dependency parsers and su-pervised MWE identification systems.
Concern-ing the FTB, we used the dependency version de-veloped in Candito and Constant (2014) derivedfrom the SPMRL shared task version (Seddah etal., 2013).
Fixed and non-fixed MWEs are dis-tinguished, but are limited to contiguous onesonly.
The STREUSLE corpus (Schneider et al,2014b) corresponds to a subpart of the EnglishWeb Treebank (EWT).
It consists of reviews andis comprehensively annotated in contiguous anddiscontiguous MWEs.
Fixed and non-fixed ex-pressions are not distinguished though the distinc-tion between non-compositional and collocationalMWEs is made.
This implies that the MergeFtransition is not used on this dataset.
Practi-cally, we used the LTH converter (Johansson andNugues, 2007) to obtain the dependency versionof the EWT constituent version.
We also usedthe predicted linguistic attributes used in Constantand Le Roux (2015) and in Constant et al (2016).Both datasets include predicted POS tags, lem-mas and morphology, as well as features computedfrom compound dictionary lookup.
None of themis entirely satisfying with respect to our model, butthey allow us to evaluate the feasibility of the ap-proach.
Statistics on the two datasets are providedin Table 1.Results are provided in Table 2 for French andin Table 3 for English.
In order to evaluate the syn-tactic layer, we used classical UAS and LAS met-rics.
Before evaluation, merged units were auto-matically decomposed in the form of flat subtreesusing specific arcs as in Seddah et al (2013), so allsystems can be evaluated and compared at the to-ken level.
MWE identification is evaluated withthe F-score of the MWE segmentation, namelyMWE for all MWEs and FMWE for fixed MWEsonly.
An MWE segment corresponds to the setof its component positions in the input token se-quence.First, results show that our joint system consis-tently and significantly outperforms the baselinein terms of MWE identification on both datasets.The merge transitions play a key role.
In terms ofsyntax, the Explicit system does not have any pos-itive impact (on par or degraded scores), whereasthe Implicit system allows us to obtain slightly bet-ter results on French and a significant improve-ment on English.
The very good performanceson English might be explained by the fact thatit contains a non-negligeable set of discontiguousMWEs which complicates the prediction of ex-plicit Complete transitions.When compared with weaker systems, we cansee that the addition of the lexical layer helps im-prove the prediction of the syntactic layer, whichconfirms results on symbolic parsing (Wehrli,2014).
The syntactic layer does not seem to im-pact the lexical layer prediction: we observe com-parable results.
This might be due to the factthat syntax is helpful for long-distance disconti-guity only, which does not appear in our datasets(the English dataset contains MWEs with smallgaps).
Another explanation could also be that syn-tactic parsing accuracy is rather low due to theuse of a simple greedy algorithm.
Developingmore advanced transition-based parsing methodslike beam-search may help improve both syntacticparsing accuracy and MWE identification.
Whencomparing joint systems with pipeline ones, wecan see that preidentifying fixed MWEs seems tohelp MWE identification whereas syntactic pars-ing accuracy tends to be slightly lower.
One hy-pothesis could be that MergeFtransitions mayconfuse the prediction of MergeNtransitions.When compared with existing state-of-the-artsystems, we can see that the proposed systemsachieve MWE identification scores that are com-parable with the pipeline and joint approachesused in Candito and Constant (2014) with a graph-167DEV TESTSystem UAS LAS MWE FMWE UAS LAS MWE FMWEBaseline 86.28 83.67 77.2 83.2 84.85 82.67 75.5 81.9Explicit 86.36 83.77 79.7 86.0 84.98 82.79 79.3 84.8Implicit 86.61 84.10 80.0 86.2 85.04 82.93 78.4 84.3Syntactic only -Baseline 86.31 83.69 - 83.5 84.89 82.70 - 82.0Syntactic only 86.39 83.77 - 85.0 85.02 82.84 - 83.8Lexical only - - 80.0 - - - 79.5 -Fixed only - - - 85.7 - - - 85.7Pipeline (Fixed only ?
Baseline) 85.33 83.29 80.6 85.7 84.86 82.86 80.4 85.7Pipeline (Fixed only ?
Implicit) 85.49 83.50 81.8 85.7 84.84 82.89 81.1 85.7graph-based (Candito and Constant, 2014) 89.7 87.5 77.6 85.4 89.21 86.92 77.0 85.1CRF+graph-based (Candito and Constant, 2014) 89.8 87.4 79.0 85.0 86.97 89.24 78.6 86.3CRF (SPMRL) (Le Roux et al, 2014) - - 82.4 - - - 80.5 -Table 2: Results on the FTB.
To reduce bias due to training with shuffled examples, scores are averagesof 3 different training/parsing runs.TRAIN Cross-validation TESTSystem UAS LAS MWE UAS LAS MWEBaseline 86.16 81.76 49.6 86.31 82.02 46.8Explicit 86.25 82.09 52.9 86.05 81.68 53.4Implicit 86.81 82.68 55.0 87.05 83.14 51.6Syntactic only 86.35 82.23 - 86.41 82.20 -Lexical only - - 54.5 - - 53.6(Schneider et al, 2014a) - - - - - 53.85Table 3: Results on the reviews part of the English Web Treebank, via cross-validation on the trainingset with 8 splits, and simple validation on the test set.based parser for French, and the base sequencetagger using a perceptron model with rich MWE-dedicated features of Schneider et al (2014a) forEnglish.
It reaches lower scores than the best sim-ple CRF-based MWE tagging system of Le Rouxet al (2014).
These scores are obtained on theSPMRL shared task version, though they are notentirely comparable with our system as they do notdistinguish fixed from non-fixed MWEs.5 Related workThe present paper proposes a new representationfor lexical and syntactic analysis in the frameworkof syntactic dependency parsing.
Most existingMWE-aware dependency treebanks represent anMWE as a flat subtree of the syntactic tree withspecial labels, like in the UD treebanks (Nivre etal., 2016) or in the SPMRL shared task (Seddah etal., 2013), or in other individual treebanks (Nivreand Nilsson, 2004; Eryi?git et al, 2011).
Such rep-resentation enables MWE discontinuity, but the in-ternal syntactic structure is not annotated.
Can-dito and Constant (2014) proposed a representa-tion where the irregular and regular MWEs aredistinguished: irregular MWEs are integrated inthe syntactic tree as above; regular MWEs are an-notated in their component attributes while theirinternal structure is annotated in the syntactic tree.The Prague Dependency Treebank (Bej?cek et al,2013) has several interconnected annotation lay-ers: morphological (m-layer), syntactic (a-layer)and semantic (t-layer).
All these layers are treesthat are interconnected.
MWEs are annotated onthe t-layer and are linked to an MWE lexicon(Bej?cek and Stra?n?ak, 2010).
Constant and LeRoux (2015) proposed a dependency representa-tion of lexical segmentation allowing annotationsof deeper phenomena like MWE nesting.
Moredetails on MWE-aware treebanks (including con-stituent ones) can be found in Ros?en et al (2015).Statistical MWE-aware dependency parsing hasreceived a growing interest since Nivre and Nils-son (2004).
The main challenge resides in find-ing the best orchestration strategy.
Past researchhas explored either pipeline or joint approaches.Pipeline strategies consist in positioning the MWErecognition either before or after the parser it-self, as in Nivre and Nilsson (2004), Eryi?git etal.
(2011), Constant et al (2013), and Kong etal.
(2014) for pre-identification and as in Vinczeet al (2013a) for post-identification.
Joint strate-gies have mainly consisted in using off-the-shelf168parsers and integrating MWE annotation in thesyntactic structure, so that MWE identification isblind for the parser (Nivre and Nilsson, 2004;Eryi?git et al, 2011; Seddah et al, 2013; Vinczeet al, 2013b; Candito and Constant, 2014; Nasr etal., 2015).Our system includes a special treatment ofMWEs using specific transitions in a classicaltransition-based system, in line with the proposalof Nivre (2014).
Constant et al (2016) alsoproposed a two-dimensional representation in theform of dependency trees anchored by the samewords.
The annotation of fixed MWEs is redun-dant on both dimensions, while they are sharedin our representation.
They propose, along withthis representation, an adaptation of an easy-firstparser able to predict both dimensions.
Contraryto our system, there are no special mechanisms fortreating MWEs.The use of multiple stacks to capture partly in-dependent dimensions is inspired by the multipla-nar dependency parser of G?omez-Rodr?
?guez andNivre (2013).
Our parsing strategy for (hierar-chical) MWEs is very similar to the deterministicconstituency parsing method of Crabb?e (2014).6 ConclusionThis paper proposes a transition-based system thatextends a classical arc-standard parser to handleboth lexical and syntactic analysis.
It is based ona new representation having two linguistic layerssharing lexical nodes.
Experimental results showthat MWE identification is greatly improved withrespect to the mainstream joint approach.
This canbe a useful starting point for several lines of re-search: implementing more advanced transition-based techniques (beam search, dynamic oracles,deep learning); extending other classical transitionsystems like arc-eager and hybrid as well as han-dling non-projectivity.AcknowledgmentsThe authors would like to thank Marie Canditofor her fruitful inputs.
This work has been partlyfunded by the French Agence Nationale pour laRecherche, through the PARSEME-FR project(ANR-14-CERA-0001).
This work has also beensupported in part by the PARSEME EuropeanCOST Action (IC1207).ReferencesAnne Abeill?e, Lionel Cl?ement, and Franc?ois Toussenel.2003.
Building a treebank for French.
In AnneAbeill?e, editor, Treebanks.
Kluwer, Dordrecht.Timothy Baldwin and Su Nam Kim.
2010.
Multi-word Expressions.
In Nitin Indurkhya and Fred J.Damerau, editors, Handbook of Natural LanguageProcessing, pages 267?292.
CRC Press, Taylor andFrancis Group, Boca Raton, FL, USA, 2 edition.Eduard Bej?cek and Pavel Stra?n?ak.
2010.
Annota-tion of Multiword Expressions in the Prague Depen-dency Treebank.
Language Resources and Evalua-tion, 44(1-2):7?21.Eduard Bej?cek, Eva Haji?cov?a, Jan Haji?c, Pavl??naJ?
?nov?a, V?aclava Kettnerov?a, Veronika Kol?a?rov?a,Marie Mikulov?a, Ji?r??
M?
?rovsk?y, Anna Nedoluzhko,Jarmila Panevov?a, Lucie Pol?akov?a, Magda?Sev?c?
?kov?a, Jan?St?ep?anek, and?S?arka Zik?anov?a.2013.
Prague Dependency Treebank 3.0.Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.2012.
English web treebank.Marie Candito and Matthieu Constant.
2014.
Strate-gies for Contiguous Multiword Expression Analysisand Dependency Parsing.
In ACL 14 - The 52nd An-nual Meeting of the Association for ComputationalLinguistics, Baltimore, United States, June.
ACL.Matthieu Constant and Joseph Le Roux.
2015.
Depen-dency Representations for Lexical Segmentation.
In6th Workshop on Statistical Parsing of Morphologi-cally Rich Languages (SPMRL 2015), Bilbao, Spain,July.Matthieu Constant, Marie Candito, and Djam?e Sed-dah.
2013.
The LIGM-Alpage Architecture for theSPMRL 2013 Shared Task: Multiword ExpressionAnalysis and Dependency Parsing.
In Fourth Work-shop on Statistical Parsing of Morphologically RichLanguages, pages 46?52, Seattle, United States, Oc-tober.Matthieu Constant, Joseph Le Roux, and Nadi Tomeh.2016.
Deep lexical segmentation and syntactic pars-ing in the easy-first dependency framework.
InProceeedings of the 15th Annual Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies (NAACL HLT 2016).Beno?
?t Crabb?e.
2014.
An LR-inspired generalizedlexicalized phrase structure parser.
In COLING2014, 25th International Conference on Computa-tional Linguistics, Proceedings of the Conference:Technical Papers, August 23-29, 2014, Dublin, Ire-land, pages 541?552.G?uls?en Eryi?git, Tugay?Ilbay, and Ozan Arkan Can.2011.
Multiword Expressions in Statistical Depen-dency Parsing.
In Proceedings of the Second Work-shop on Statistical Parsing of Morphologically Rich169Languages, SPMRL ?11, pages 45?55, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Yoav Goldberg and Joakim Nivre.
2013.
Training De-terministic Parsers with Non-Deterministic Oracles.TACL, 1:403?414.Carlos G?omez-Rodr?
?guez and Joakim Nivre.
2013.Divisible Transition Systems and Multiplanar De-pendency Parsing.
Comput.
Linguist., 39(4):799?845, December.Richard Johansson and Pierre Nugues.
2007.
Ex-tended Constituent-to-dependency Conversion forEnglish.
In Proceedings of NODALIDA 2007, pages105?112, Tartu, Estonia, May 25-26.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
Dependency parsing fortweets.
In Proceedings of the Conference onEmpirical Methods in Natural Language Pro-cessing, Doha, Qatar, October.
Association forComputational Linguistics.Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan and Claypool.Joseph Le Roux, Antoine Rozenknop, and MatthieuConstant.
2014.
Syntactic Parsing and CompoundRecognition via Dual Decomposition: Applicationto French.
In COLING.Alexis Nasr, Carlos Ramisch, Jos?e Deulofeu, andAndr?e Valli.
2015.
Joint Dependency Parsing andMultiword Expression Tokenization.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing of the Asian Federation of Natural LanguageProcessing, ACL 2015, July 26-31, 2015, Beijing,China, Volume 1: Long Papers, pages 1116?1126.Joakim Nivre and Jens Nilsson.
2004.
Multiword unitsin syntactic parsing.
Proceedings of Methodologiesand Evaluation of Multiword Units in Real-WorldApplications (MEMURA).Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-ter, Yoav Goldberg, Jan Haji?c, Christopher D. Man-ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,Natalia Silveira, Reut Tsarfaty, and Dan Zeman.2016.
Universal Dependencies v1: A MultilingualTreebank Collection.
In Proceedings of the 10th In-ternational Conference on Language Resources andEvaluation.Joakim Nivre.
2004.
Incrementality in DeterministicDependency Parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineeringand Cognition Together (ACL), pages 50?57.Joakim Nivre.
2008.
Algorithms for Deterministic In-cremental Dependency Parsing.
Comput.
Linguist.,34(4):513?553, December.Joakim Nivre.
2014.
Transition-Based Parsing withMultiword Expressions.
In 2nd PARSEME GeneralMeeting, Athens, Greece.Carlos Ramisch.
2015.
Multiword Expressions Ac-quisition: A Generic and Open Framework, volumeXIV of Theory and Applications of Natural Lan-guage Processing.
Springer.Victoria Ros?en, Gyri Sm?rdal Losnegaard, Koen-raad De Smedt, Eduard Bej?cek, Agata Savary, AdamPrzepi?orkowski, Petya Osenova, and Verginica Mi-titelu.
2015.
A survey of multiword expressionsin treebanks.
In Proceedings of the 14th Interna-tional Workshop on Treebanks and Linguistic Theo-ries (TLT14).Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
MultiwordExpressions: A Pain in the Neck for NLP.
InAlexander Gelbukh, editor, Computational Linguis-tics and Intelligent Text Processing, volume 2276of Lecture Notes in Computer Science, pages 1?15.Springer Berlin Heidelberg.Nathan Schneider, Emily Danchik, Chris Dyer, andNoah A. Smith.
2014a.
Discriminative lexical se-mantic segmentation with gaps: running the MWEgamut.
Transactions of the Association for Compu-tational Linguistics (TACL).Nathan Schneider, Spencer Onuffer, Nora Kazour,Emily Danchik, Michael T. Mordowanec, HenriettaConrad, and Noah A. Smith.
2014b.
Comprehen-sive Annotation of Multiword Expressions in a So-cial Web Corpus.
In Nicoletta Calzolari, KhalidChoukri, Thierry Declerck, Hrafn Loftsson, BenteMaegaard, Joseph Mariani, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation, pages 455?461, Reykjav?
?k,Iceland, May.
ELRA.Djam?e Seddah, Reut Tsarfaty, Sandra K?
?ubler, MarieCandito, Jinho Choi, Rich?ard Farkas, Jennifer Fos-ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,Spence Green, Nizar Habash, Marco Kuhlmann,Wolfgang Maier, Joakim Nivre, Adam Przepi-orkowski, Ryan Roth, Wolfgang Seeker, YannickVersley, Veronika Vincze, Marcin Woli?nski, AlinaWr?oblewska, and Eric Villemonte de la Cl?ergerie.2013.
Overview of the SPMRL 2013 Shared Task:A Cross-Framework Evaluation of Parsing Morpho-logically Rich Languages.
In Proceedings of the 4thWorkshop on Statistical Parsing of MorphologicallyRich Languages, Seattle, WA.Violeta Seretan.
2011.
Syntax-Based Collocation Ex-traction.
Text, Speech and Language Technology.Springer.Veronika Vincze, Istv?an Nagy T., and Rich?ard Farkas.2013a.
Identifying english and hungarian light verbconstructions: A contrastive approach.
In Proceed-ings of the 51st Annual Meeting of the Association170for Computational Linguistics, ACL 2013, 4-9 Au-gust 2013, Sofia, Bulgaria, Volume 2: Short Papers,pages 255?261.Veronika Vincze, J?anos Zsibrita, and Istv`an Nagy T.2013b.
Dependency parsing for identifying hungar-ian light verb constructions.
In Proceedings of In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP 2013), Nagoya, Japan.Eric Wehrli.
2014.
The Relevance of Collocationsfor Parsing.
In Proceedings of the 10th Workshopon Multiword Expressions (MWE), pages 26?32,Gothenburg, Sweden, April.
Association for Com-putational Linguistics.171
