JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 15?24,Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCPKey-concept extraction from French articles with KXSara Tonelli1 Elena Cabrio2 Emanuele Pianta1(1) FBK, via Sommarive 18, Povo (Trento), Italy(2) INRIA, 2004 Route des Lucioles BP93, Sophia Antipolis cedex, Francesatonelli@fbk.eu, elena.cabrio@inria.fr, pianta@fbk.euR?SUM?Nous pr?sentons une adaptation du syst?me KX qui accomplit l?extraction non supervis?e etmultilingue des mots-cl?s, pour l?atelier d?
?valuation francophone en fouille de textes (DEFT2012).
KX s?lectionne une liste de mots-cl?s (avec leur poids) dans un document, en combinantdes annotations linguistiques de base avec des mesures statistiques.
Pour l?adapter ?
la languefran?aise, un analyseur morphologique pour le Fran?ais a ?t?
ajout?
au syst?me pour d?river lespatrons lexicaux.
De plus, des param?tres comme les seuils de fr?quence pour l?extraction decollocations, et les index de relevance des concepts-cl?s ont ?t?
calcul?s et fix?s sur le corpusd?apprentissage.
En concernant les pistes de DEFT 2012, KX a obtenu de bons r?sultats (Piste 1 -avec terminologie : 0.27 F1 ; Piste 2 : 0.19 F1) en demandant un effort r?duit pour l?adaptationdu domaine et du langage.ABSTRACTWe present an adaptation for the French text mining challenge (DEFT 2012) of the KX systemfor multilingual unsupervised key-concept extraction.
KX carries out the selection of a list ofweighted keywords from a document by combining basic linguistic annotations with simplestatistical measures.
In order to adapt it to the French language, a French morphologicalanalyzer (PoS-Tagger) has been added into the extraction pipeline, to derive lexical patterns.Moreover, parameters such as frequency thresholds for collocation extraction and indicators forkey-concepts relevance have been calculated and set on the training documents.
In the DEFT2012 tasks, KX achieved good results (i.e.
0.27 F1 for Task 1 - with terminological list, and 0.19F1 for Task 2) with a limited additional effort for domain and language adaptation.MOTS-CL?S : Extraction de mots-cl?s, patrons linguistiques, terminologie.KEYWORDS: Key-concept extraction, linguistic patterns, terminology.1 IntroductionKey-concepts are simple words or phrases that provide an approximate but useful characterizationof the content of a document, and offer a good basis for applying content-based similarityfunctions.
In general, key-concepts can be used in a number of interesting ways both for humanand automatic processing.
For instance, a quick topic search can be carried out over a number15of documents indexed according to their key-concepts, which is more precise and efficientthan full-text search.
Also, key-concepts can be used to calculate semantic similarity betweendocuments and to cluster the texts according to such similarity (Ricca et al, 2004).
Furthermore,key-concepts provide a sort of quick summary of a document, thus they can be used as anintermediate step in extractive summarization to identify the text segments reflecting the contentof a document.
(Jones et al, 2002), for example, exploit key-concepts to rank the sentences in adocument by relevance, counting the number of key-concept stems occurring in each sentence.In the light of the increasing importance of key-concepts in several applications, from searchengines to digital libraries, a recent task for the evaluation of key-concept extraction was alsoproposed at SemEval-2010 campaign (Kim et al, 2010)In this work, we present an adaptation of the KX system for multilingual key-concept extraction(Pianta et Tonelli, 2010) for the French text mining challenge (DEFT 2012) task.
A preliminaryversion of KX for French took part in the DEFT 2011 campaign on ?Abstract ?
article matching?
(Tonelli et Pianta, 2011), and achieved good performances in both tracks (0.990 and 0.964 F1respectively).Compared to the previous version of KX, we have now integrated into the extraction pipeline aFrench morphological analyzer (Chrupala et al, 2008).
This allows us to exploit morphologicalinformation while selecting candidate key-concepts, while in the version used at DEFT 2011 theselection was made using regular expressions and black lists.The paper is structured as follows : in Section 2 we detail the architecture of KX (i.e.
our key-concepts extraction tool), providing an insight into its parameters configuration.
In Section 3 wepresent the setting defined and adopted for the DEFT 2012 task, while in Section 4 we report thesystem performances on the training and on the test sets.
Finally, we draw some conclusions, anddiscuss future improvements of our approach in Section 5.2 Key-concept extraction with KXThis section describes in details the basic KX architecture for unsupervised key-concept extraction.KX can handle texts in several languages (i.e.
English, Italian, French, Finnish and Swedish), andit is distributed with the TextPro NLP Suite1 (Pianta et al, 2008).
KX architecture is the sameacross all languages, except for the module selecting multiword expressions, that is based on PoStags (this is the only language-dependent part of the system).
In order to perform this selection,a morphological analyzer/PoS tagger has been integrated for each of the five languages, andsome selection rules have been manually defined.
More details on the French rules are reportedin Section 2.2 and in Section 3.2.1 Pre-processing of the reference corpusIf a domain corpus is available, the extraction of key-concepts from a single document can bepreceded by a pre-processing step, during which key-concepts are extracted from the corpus andtheir inverse document frequency (IDF) at corpus level is computed by applying the standardformula :1http://textpro.fbk.eu/16I DFk = log NDFkwhere N is the number of documents in the corpus, and DFk is the number of documents in thecorpus that contain the key-concepts k. The I DF of a rare term tends to be high, while the I DFof a frequent one is likely to be low.
Therefore, I DF may be a good indicator for distinguishingbetween common, generic words and specific ones, which are good candidates for being akey-concept.
For DEFT 2012, we have used as a reference corpus all the documents contained inthe training and in the test sets (468 documents in total).2.2 Key-concept extractionFigure 1 shows KX work-flow for the key-concept extraction process : starting from a document,a list of key-concepts ranked by relevance is provided as the output of the system.
The samework-flow applies both to i) the extraction of key-concepts from a single document, and to ii) theextraction of different statistics including IDF from a reference corpus, which can be optionallyused as additional information when processing a single document.
For more information, seeabove and Section 2.3.FIG.
1 ?
Key-concept extraction workflow with KXAs a first step, the system takes a document in input and tokenize the text.
Then, all possiblen-grams composed by any token sequence are extracted, for instance ?
?clipse de soleil?, ?tous les?,?ou chacun?.
The user can set the max length of the selected n-grams : for DEFT 2012 we set suchlength to six.Then, from the n-gram list a sublist of multiword expressions (MWE) is derived, i.e.
combinationsof words expressing a unitary concept, for example ?proc?s de travail?
or ?
?conomie politique?.17In the selection step, the user can choose to rely only on local (document) evidence or to makeuse also of global (corpus) evidence.
As for the first case, a frequency threshold called MinDoccan be set, which corresponds to the minimum number of occurrences of n-grams in the currentdocument.
If a reference corpus is also available, another threshold can be added, MinCorpus,which corresponds to the minimum number of occurrences of an n-gram in the corpus.
KX marksan n-gram in a document as a multiword term if it occurs at least MinCorpus times in thecorpus or at least MinDoc times in the document.
The two parameters depend on the size of thereference corpus and the document respectively.
In our case, the corpus was the set of documentsused in the training and in the test set (see Section 2.1).A similar, frequency-based, strategy is used to solve ambiguities in how sequences of contiguousmultiwords should be segmented.
For instance, given the sequence ?retour des bonnes mani?res?we need to decide whether we recognize ?retour des bonnes?
or ?bonnes mani?res?.
To thispurpose, the strength of each alternative MWE is calculated as follows, and then the strongerone is selected.St reng thcol loc = docF requenc y ?
corpusF requenc yIn the next step, the single words and the MWEs are ranked by frequency to obtain a first listof key-concepts.
Thus, frequency is the baseline ranking parameter, based on the assumptionthat important concepts are mentioned more frequently than less important ones.
Frequency isnormalized by dividing the number of key-concept occurrences by the total number of tokens inthe current document.As shown in Figure 1, the first key-concepts list is obtained by applying black and white listsalmost at every step of the process.
A black list is applied to discard n-grams containing one ofthe language-specific stopwords defined by the user, for example ?avons?, ?peut?, ?puis?, ?parce?.Also single words corresponding to stopwords are discarded when the most frequent tokensare included into the first key-concept list.
For example, in French we may want to exclude allkey-concepts containing the words ?toi?, ?tr?s?, ?finalement?, etc.When deriving multiword expressions (MWEs) from the n-gram list, KX applies another selectionstrategy.
This selection is crucial because only MWEs are selected as candidate key-concepts, sincethey correspond to combinations of words expressing a unitary concept, for example ?r?gime dedespotisme familial?
or ?reproduction mat?rielle?.
The n-grams are analyzed with the Morfettemorphological analyzer (Chrupala et al, 2008) in order to select as multiword expressions onlythe n-grams that match certain lexical patterns (i.e.
part-of-speech).
This is the so-called linguisticfilter.
For example, one of the patterns admitted for 3-grams is the following :[SP]?
[O]?
[SP]This means that a 3-gram is a candidate multiword term if it is composed by a single or pluralnoun (S and P respectively), followed by a preposition (defined as O), followed by another noun.This is matched for example by the 3-gram ?proc?s [S] de [O] travail [S]?.Finally, black and white lists can be manually compiled also for key-concepts, to define expressionsthat should never be selected as relevant key-concepts, as well as terms that should always beincluded in the key-concept rank.
For example, the preposition ?de?
is very frequent in documents,so it can happen that it is selected as single-word key-concept.
In order to avoid this, ?de?
can beincluded in the key-concept black list.182.3 First key-concept rankingDifferent techniques are used to re-rank the frequency-based list of key-concepts obtained inthe previous step according to their relevance.
If a reference corpus is available, as in our case,additional information can be used to understand which key-concepts are more specific to adocument, and therefore are more likely to be relevant for such document.In order to find the best ranking mechanism, and to tailor it to the type of key-concepts we wantto extract, the following parameters can be set :Key-concept IDF : This parameter takes into account the fact that, given a data collection, aconcept that is mentioned in many documents is less relevant to our task than a concept oc-curring in few documents.
To activate it, a reference corpus must undergo a pre-processingstep in which the key-concepts are extracted from each document in the corpus, and thecorresponding inverse document frequency (IDF) is computed, as described in Section 2.1.When this parameter is activated, for each key-concept found in the current document, itsI DF computed over the reference corpus is retrieved and multiplied by the key-conceptfrequency at document level.Key-concept length : Number of tokens in a key-concept.
Concepts expressed by longer phrasesare expected to be more specific, and thus more informative.
When this parameter isactivated, the frequency is multiplied by the key-concept length.
For example, if ?expressionverbale?
has frequency 6 and ?expression verbale des ?motions?
has frequency 5, theactivation of the key-concept length parameter gives ?expression verbale?
= 6 * 2 = 12 and?expression verbale des ?motions?
= 5 * 4 = 20.
In this way, the 4-gram is assigned a higherranking than the 2-gram.Position of first occurrence : Important concepts are expected to be mentioned before lessrelevant ones.
If the parameter is activated, the frequency score will be multiplied by thePosFact factor computed as :PosFact =DistF romEndMax Index2where Max Index is the length of the current document, and DistF romEnd is Max Indexminus the position of the first key-concept occurrence in the text.A configuration file allows the user to independently activate such parameters.
The key-conceptrelevance is then calculated by multiplying the normalized frequency of a key-concept by thescore obtained by each active parameter.
We eventually obtain a ranking of key-concepts orderedby relevance.
The user can also set the number of top ranked key-concepts to consider as bestcandidates.2.4 Final key-concept rankingSection 2.3 described the first set of ranking strategies, that can be optionally followed by anotherset of operations to adjust the preliminary ranking.
Again, such operations can be independentlyactivated through a separate configuration file.
The parameters have been introduced to deal19with the so-called nested key-concepts (Frantzi et al, 2000), i.e.
those that appear within otherlonger candidate key-concepts.
After the first ranking, which is still influenced by the key-conceptfrequency, nested (shorter) key-concepts tend to have a higher ranking than the containing(longer) ones, because the former are usually more frequent than the latter.
However, in somesettings, for example in scientific articles, longer key-concepts are generally preferred overshorter ones because they are more informative and specific.
In such cases, the user may wantto adjust the ranking in order to give preference to longer key-concepts and to reduce or set tozero the score of nested key-concepts.
These operations are allowed by activating the followingparameters :Shorter concept subsumption : It happens that two concepts can occur in the key-concept list,such that one is a specification of the other.
Concept subsumption and boosting (see below)are used to merge or rerank such couples of concepts.
If a key-concept is (stringwise)included in a longer key-concept with a higher frequency-based score, the score of theshorter key-concept is transferred to the count of the longer one.
For example, if ?expressionverbale?
has frequency 4 and ?expression verbale des ?motions?
has frequency 6, by activa-ting this parameter the relevance of ?expression verbale des ?motions?
is 6 + 4 = 10, whilethe relevance of ?expression verbale?
is set to zero.
The idea behind this strategy is thatnested key-concepts can be deleted from the final key-concept list without losing relevantinformation, since their meaning is nevertheless contained in the longer key-concepts.Longer concept boosting : This parameter applies in case a key-concept is (stringwise) includedin a longer key-concept with a lower relevance.
Its activation should better balance theranking in order to take into account that longer n-grams are generally less frequent, butnot less relevant, than shorter ones.
The parameter is available in two different versions,having different criteria for computing such boosting.
With the first option, the averagescore between the two key-concepts relevance is computed.
Such score is assigned to theless frequent key-concepts and subtracted from the frequency score of the higher rankedone.
With the second option, the longer key-concepts is assigned the frequency of the shorterone.
In none of the two variants key-concepts are deleted from the relevance list, as ithappens by activating the Shorter concept subsumption parameter.For example, if ?expression verbale?
has score 6 and ?expression verbale des ?motions?
hasscore 4, by activating the first option of this parameter the relevance of ?expression verbale?becomes 6 - ((6 + 4) / 2) = 1, while the relevance of ?expression verbale des ?motions?
isset to 5, i.e.
(6 + 4) / 2 .With the second option, both the relevance of ?expression verbale des ?motions?
and of?expression verbale?
is set to 6.The examples above show that these parameters set by the user can change the output of theranking by deleting some entries and boosting some others.
After applying one cycle of subsump-tion/boosting, the order of the concepts can dramatically change, producing the conditions forfurther subsumption/boosting of concepts.
The user can set the number of iterations for theapplication of this re-ranking mechanism, and each cycle increases the impact of the re-rankingon the key-concept list.
The parameters can be activated together and in different combinations.If all parameters are set, the short concept subsumption procedure is applied first, then the longerconcept boosting is run on the output of the first re-ranking, so that the initial relevance-basedlist goes through two reordering steps.203 KX configuration for the DEFT 2012 taskAs introduced before (Section 2.2), to port KX to the French language and, in particular, to adaptit to the DEFT 2012 task, the Morfette morphological analyzer (Chrupala et al, 2008) has beenintegrated into the system, to select as multiword expressions only the n-grams matching certainlexical patterns (i.e.
part-of-speech).
Such lexical patterns are learned on the gold standard, andmanually formalized and added into the system as a linguistic filter.
In order to speed up thisprocess, we took advantage of the set of lexical patterns defined for Italian, and we checkedif they could be applied also for French.
Moreover, new patterns were added in compliancewith DEFT training data requirements.
For example, the following n-grams have been added asallowed patterns (i.e.
candidate multiword terms) :?
6-grams : [SP]-[O]-[SP]-[O]-[S]-[JK], where S and P correspond to singular or plural nouns,O to the prepositions (also in combination with the article), and J and K to singular or pluraladjectives (e.g.
?soul?vement [S] des [O] M?tis [S] dans l?
[O] Ouest [S] canadien [J]?)
;?
5-grams : [SP]-[O]-[SP]-[O]-[P], (e.g.
?gestion [S] des [O] troupeaux [P] de [O] rennes [P]?)
;?
4-grams : [SP]-[JK]-[0]-[S], (e.g.
?histoire [S] canonique [J] de la [0] traduction [S]?)
;?
3-grams : [S]-[SP]-[JK], (e.g.
?fran?ais [S] langue [S] premi?re [J]?
).We compiled black lists both for common French stopwords (containing e.g.
articles, prepositions,a few numbers, and functional verbs) and stopphrases (prepositional structures such as ?au sujetde?, ?en dehors de?, ?en face de?
), since we do not want them to be selected as key-concepts.As for the IDF value mentioned in Section 2.1, it has been computed for 86,419 key-conceptsextracted from DEFT 2012 training and test set.
Among the key-concepts with the highestIDF (i.e.
best candidates for final selection), we find ?inversions culturelles?, ?hypertextualit?
?,?am?nagement terminologique?.
These are key-concepts that occur only in one document of thereference corpus.
Among the key-concepts with a low IDF, instead, we find very common termsand expressions such as ?rapport?, ?partie?
and ?exemple?, which are likely to be discarded askey-concepts.The standard KX architecture has also been adapted to one of the two tracks of DEFT 2012,namely the one in which a terminological list was provided.
For that track, the set of documentsto be processed was accompanied by a list of domain terminology.
By comparing the gold key-concepts in the training set with this list, we observed that all terms in the terminology werealso gold key-concepts.
Therefore, we modified KX so that, in the final re-ranking, the candidatekey-concepts being present in the terminology list were significantly boosted.
This adaptationlead to an improvement of almost 0.8 P/R/F1 on the test set (see Section 4).4 EvaluationSince KX does not require supervision, we used the training set to identify the best parametersetting, which was then applied in the test phase.
The results obtained on the training and on thetest set are discussed in the following subsections.214.1 System evaluation on the training setWe report in Table 1 the best parameter setting on the training documents.
Note that the reportedevaluation measures have been computed using our own scorer, which counts as correct eachkey-concept exactly matching with the gold standard (case-insensitive).
The results reported forthe test set, instead, have been computed by the task organizers with another scorer, which mayapply a slightly different strategy.We extracted for each document the top k key-concepts, with k being the number of key-concepts assigned to each document in the training set (this number may vary from document todocument).
For this reason, Precision and Recall are the same.Task 1 : Task 2 :with terminology w/o terminologyKX Parameters1.
MinCorpus 8 82.
MinDoc 3 33.
Use corpusId f Yes Yes4.
Multiply relevance by key-concept length Yes Yes5.
Consider position of first occurrence No No6.
Shorter concept subsumption No No7.
Longer concept boosting No No8.
Boost key-concepts in terminology list Yes NoP/R/F1 on training set F1 0.18 F1 0.15TAB.
1 ?
Best parameter combination for training setThe results obtained on the training set suggest that the key-concepts required in this task shouldnot be too specific, since the parameters aimed at preferring specific (i.e.
longer) key-conceptsare not activated in the best performing setting (we refer to parameters n. 6 and 7 in the aboveTable).
Also the position of the first key-concept occurrence is not relevant, since the parametern.
5 is not part of the best setting.
This is in contrast with KX setting used for Semeval 2010(Pianta et Tonelli, 2010).
In that case, boosting the relevance of specific key-concepts, and ofthose occurring in the article abstract had a positive effect on the final performance.
Note alsothat the performance measured on French documents in DEFT is around 0.10 points lowerthan that achieved at Semeval on English scientific articles.
We believe that this is not due toa different system performance on the two languages, but rather on the evaluation strategy,because Semeval scorer required the key-concepts to be stemmed and took into account somesyntactic variations of the same key-concept (Kim et al, 2010).4.2 System evaluation on the test setFor each task, we submitted three system runs, testing different parameter combinations.
Specifi-cally, for Task 1 (with terminological list), the three runs had the following configurations :1.
Parameter setting reported in Section 4.1 (with boosting of key-concepts in terminology22list) ;2.
Parameter setting as in Section 4.1 but Consider position of first occurrence activated (withboosting of key-concepts in terminology list) ;3.
Parameter setting as in Section 4.1 but terminology list is not taken into account.As for Task 2 (without terminological list), the three runs had the following configurations :1.
Parameter setting reported in Section 4.1 ;2.
Parameter setting as in Section 4.1 but Consider position of first occurrence activated ;3.
Parameter setting reported in Section 4.1 but system run only on article abstracts.Task 1 : Task 2 :with terminology w/o terminologyKX Run 1 0.2682 0.1880KX Run 2 0.2737 0.1901KX Run 3 0.1976 0.1149TAB.
2 ?
KX performance on test setWe decided to activate the parameter Consider position of first occurrence, even if it was notpart of the best performing setting in the training phase, because it achieved good results inthe Semeval 2010 challenge on English.
The results confirm that, in both tasks, this yielded a(limited) improvement.In both tasks, the third run was used to exploit configurations that were not tested in the trainingphase.
In Task 1, the third run was obtained without taking into account the terminology list.The difference in performance between Run 1 and Run 3 confirms that this information is indeedvery relevant.
In Task 2, the third run concerned the extraction of key-concepts only from theabstracts, and not from the whole articles.
Also in this case, the initial hypothesis that the abstractmay contain all relevant key-concepts proved to be wrong.At DEFT 2012, 10 teams submitted at least one run in Task 1, and 9 teams in Task 2.
The bestperforming run of KX was ranked 6th out of 10 in Task 1 and 5th out of 9 in Task 2.
In Task 1the mean F1 for the best submission of each team was 0.3575, the median was 0.3321 and thestandard deviation 0.2985, with system performances ranging from 0.0428 (lowest performance)to 0.9488 (best run).
In Task 2 the mean F1 for the best submission of each team was 0.2045,the median was 0.1901 and the standard deviation 0.1522, with system performances rangingfrom 0.0785 (lowest performance) to 0.5874 (best run).These results show that the use of terminology significantly improves the overall system perfor-mance, as confirmed in Table 2.
However, KX seems to be more competitive in the second taskcompared to other systems.
This confirms that KX strength lies in its domain-independence andin the fact that is does not require any additional information to achieve a good performance.Furthermore, we believe that the second task is more realistic than the first one : in a realapplication scenario, it is unlikely that a terminological list, containing only the key-concepts tobe identified, is actually available.235 ConclusionsIn this paper, we presented the French version of the KX system, and we described the experimentswe carried out for our participation at DEFT 2012.
KX achieved good results with few adjustmentsof the parameter setting and a limited additional effort for domain and language adaptation.
Oursystem requires no supervision and its English and Italian versions are distributed as a standalonekey-concept extractor.
Its extension, which takes into account a reference terminological list,proved to be effective and achieved a moderate improvement in the first task of the evaluationchallenge.A limitation of our system is that it is not able to identify key-concepts that are not presentin the document.
This kind of concepts amounted to around 20% of the gold key-concepts inthe training set, and this feature strongly affected the outcome of our evaluation.
A strategy toexploit external knowledge sources to extract common subsumers of the given key-concepts maybe investigated in the future.AcknowledgementsThe development of KX has been partially funded by the European Commission under the contractnumber FP7-248594, PESCaDO project.R?f?rencesCHRUPALA, G., DINU, G. et van GENABITH, J.
(2008).
Learning Morphology with Morfette.
InProceedings of the 6th International Conference on Languages Resources and Evaluations (LREC2008), Marrakech, Morocco.FRANTZI, K., ANANIADOU, S. et MIMA, H. (2000).
Automatic recognition of multi-word terms :the C-value/NC-value.
Journal of Digital Libraries, 3(2):115?130.JONES, S., LUNDY, S. et PAYNTER, G. (2002).
Interactive Document Summarisation Using Auto-matically Extracted Keyphrases.
In Proceedings of the 35th Hawaii International Conference onSystem Sciences, Hawaii.KIM, S. N., MEDELYAN, O., KAN, M.-Y.
et BALDWIN, T. (2010).
SemEval-2010 Task 5 : Automatickeyphrase extraction from scientific articles.
In Proceedings of SemEval 2010, Task 5 : Keywordextraction from Scientific Articles, Uppsala, Sweden.PIANTA, E., GIRARDI, C. et ZANOLI, R. (2008).
The TextPro tool suite.
In Proceedings of the 6thLanguage Resources and Evaluation Conference (LREC), Marrakech, Morocco.PIANTA, E. et TONELLI, S. (2010).
KX : A flexible system for Keyphrase eXtraction.
In Proceedingsof SemEval 2010, Task 5 : Keyword extraction from Scientific Articles, Uppsala, Sweden.RICCA, F., TONELLA, P., GIRARDI, C. et PIANTA, E. (2004).
An empirical study on keyword-basedweb site clustering.
In Proceedings of the 12th IWPC, Bari, Italy.TONELLI, S. et PIANTA, E. (2011).
Matching documents and summaries using key-concepts.
InProceedings of DEFT 2011, Montpellier, France.24
