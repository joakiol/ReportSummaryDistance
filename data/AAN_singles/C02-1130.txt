Fine Grained Classification of Named EntitiesMichael Fleischman and Eduard HovyUSC Information Science Institute4676 Admiralty WayMarina del Rey, CA 90292-6695U.S.A.
{fleisch, hovy} @ISI.eduAbstractWhile Named Entity extraction is useful inmany natural language applications, thecoarse categories that most NE extractorswork with prove insufficient for complexapplications such as Question Answering andOntology generation.
We examine onecoarse category of named entities, persons,and describe a method for automaticallyclassifying person instances into eight finer-grained subcategories.
We present asupervised learning method that considers thelocal context surrounding the entity as well asmore global semantic information derivedfrom topic signatures and WordNet.
Wereinforce this method with an algorithm thattakes advantage of the presence of entities inmultiple contexts.1.
IntroductionThere has been much interest in the recent pastconcerning automated categorization of namedentities in text.
Recent advances have made somesystems (such as BBN?s IdentiFinder (Bikel,1999)) very successful when classifying namedentities into broad categories, such as person,organization, and location.
While the accurateclassification of general named entities is useful inmany areas of natural language research, more fine-grained categorizations would be of particularvalue in areas such as Question Answering,information retrieval, and the automatedconstruction of ontologies.The research presented here focuses on thesubcategorization of person names, which extendsresearch on the subcategorization of location names(Fleischman, 2001).
While locations can often beclassified based solely on the words that surroundthe instance, person names are often morechallenging because classification relies on muchdeeper semantic intuitions gained from thesurrounding text.
Further, unlike the case withlocation names, exhaustive lists of person names bycategory do not exist and cannot be relied upon fortraining and test set generation.
Finally, the domainof person names presents a challenge because thesame individual (e.g., ?Ronald Reagan?)
is oftenrepresented differently at different points in thesame text (e.g., ?Mr.
Reagan?, ?Reagan?, etc.
).The subcategorization of person names is not atrivial task for humans either, as the examplesbelow illustrate.
Here, names of persons have beenencrypted using a simple substitution cipher.
Thenames are of only three subtypes: politician,businessperson, and entertainer, yet proveremarkably difficult to classify based upon thecontext of the sentence.1.
Unfortunately, Mocpm_____ and his immediatefamily did not cooperate in the making of the film .2.
"The idea that they'd introduce Npn Fuasm______into that is amazing ,"he said.3.
"It's dangerous to be right when government iswrong ," Lrsyomh______ told reporters1.
Mocpm = Nixon: politician2.
Npn Fuasm = Bob Dylan: entertainer3.
Lrsyomh = Keating: businesspersonIn this work we examine how different featuresand learning algorithms can be employed toautomatically subcategorize person names in text.In doing this we address how to inject semanticinformation into the feature space, how toautomatically generate training sets for use withsupervised learning algorithms, and how to handleorthographic inconsistencies between instances ofthe same person.2.
Data Set GenerationA large corpus of person instances wascompiled from a TREC9 database consisting ofarticles from the Associated Press and the WallStreet Journal.
Data was word tokenized, stemmedusing the Porter stemming algorithm (Porter, 1980),part of speech tagged using Brill?s tagger (Brill,1994), and named entity tagged using BBN?sIdentiFinder (Bikel, 1999).
Person instances wereclassified into one of eight categories: athlete,politician/government, clergy, businessperson,entertainer/artist, lawyer, doctor/scientist, andpolice.
These eight categories were chosen becauseof their high frequency in the corpus and alsobecause of their usefulness in applications such asQuestion Answering.
A training set of roughly25,000 person instances was then created using apartially automated classification system.In generating the training data automatically wefirst attempted to use the simple tagging methoddescribed for location names in (Fleischman,2001).
This method involved collecting lists ofinstances of each category from the Internet andusing those lists to classify person names found byIdentiFinder.
Although robust with location names,this method proved inadequate with persons (in asample of 300, over 25% of the instances werefound to be incorrect).
This was due to the fact thatthe same name will often refer to multipleindividuals (e.g., ?Paul Simon?
refers to apolitician, an entertainer, and Belgian scientist).In order to avoid this problem we implementeda simple bootstrapping procedure in which a seeddata set of 100 instances of each of the eightcategories was hand tagged and used to generate adecision list classifier using the C4.5 algorithm(Quinlan, 1993) with the word frequency and topicsignature features described below.
This simpleclassifier was then run over a large corpus andclassifications with a confidence score above a90% threshold were collected.
These confidentinstances were then compared to the lists collectedfrom the Internet, and, only if there was agreementbetween the two sources, were the instancesincluded in the final training set.
This procedureproduced a large training set with very fewmisclassified instances (over 99% of the instancesin a sample of 300 were found to be correct).
Avalidation set of 1000 instances from this set wasthen hand tagged to assure proper classification.A consequence of using this method for datageneration is that the training set created is not arandom sample of person instances in the realworld.
Rather, the training set is highly skewed,including only those instances that are both easyenough to classify using a simple classifier andcommon enough to be included in lists found onthe Internet.
To examine the generalizability ofclassifiers trained on such data, a held out data setof 1300 instances, also from the AP and WSJ, wascollected and hand tagged.3.
Features3.1 Word Frequency FeaturesEach instance in the text is paired with a set offeatures that represents how often the wordssurrounding the target instance occur with aspecific sub-categorization in the training set.
Forexample, in example sentence 2 in the introduction,the word ?introduce?
occurs immediately beforethe person instance.
The feature set describing thisinstance would thus include eight different features;each denoting the frequency with which?introduce?
occurred in the training setimmediately preceding an instance of a politician, abusinessperson, an entertainer, etc.
The feature setincludes these eight different frequencies for 10distinct word positions (totaling 80 features perinstance).
The positions used include the threeindividual words before the occurrence of theinstance, the three individual words after theinstance, the two-word bigrams immediately beforeand after the instance, and the three-word trigramsimmediately before and after the instance (seeFigure 1).# Position N-gram Category Freq.1 previous unigram ?introduce?
politician 32 previous unigram ?introduce?
entertainer 433 following bigram ?into that?
politician 24 following bigram ?into that?
business 0Figure 1.
Subset of word frequency features for instance inexample 2, above.
Shows the frequency with which an n-gramappears in the training data in a specific position relative toinstances of a specific category.These word frequency features provideinformation similar to the binary word features thatare often used in text categorization (Yang, 1997)with only a fraction of the dimensionality.
Suchreduced dimensionality feature sets can bepreferable when classifying very small texts(Fleischman, in preparation).3.2 Topic Signature FeaturesInspection of the data made clear the need forsemantic information during classification.
Wetherefore created features that use topic signaturesfor each of the person subcategories.
A topicsignature, as described in (Lin and Hovy, 2000), isa list of terms that can be used to signal themembership of a text in the relevant topic orcategory.
Each term in a text is given a topicsignature score that indicates its ability to signalthat the text is in a relevant category (the higher thescore, the more that term is indicative of thatcategory).
The topic signatures are automaticallygenerated for each specific term by computing thelikelihood ratio (?-score) between two hypotheses(Dunning, 1993).
The first hypothesis (h1) is thatthe probability (p1) that the text is in the relevantcategory, given a specific term, is equivalent to theprobability (p2) that the text is in the relevantcategory, given any other term (h1: p1=p2).
Thesecond hypothesis (h2) is that these twoprobabilities are not equivalent, and that p1 is muchgreater than p2 (h2: p1>>p2).
The calculation ofthis likelihood ratio [-2logL(h1)/L(h2)] for eachfeature and for each category gives a list of all theterms in a document set with scores indicating howmuch the presence of that term in a specificdocument indicates that the document is in aspecific category.Politician EntertainerWord ?-score Word ?-scorecampaign 3457.049 Star 3283.872republican 1969.707 Actor 2478.675budget 140.292 Budget 17.312bigot 2.577 Sexist 3.874Figure 2.
Subset of topic signatures generated from training setfor two categories.In creating topic signature features for thesubcategorization of persons, we created a databaseof topic signatures generated from the training set(see Figure 2).1  Each sentence from the training setwas treated as a unique document, and theclassification of the instance contained in thatsentence was treated as the relevant topic.
Weimplemented the algorithm described in (Lin andHovy, 2000) with the addition of a cutoff, such thatthe topic signatures for a term are only included ifthe p1/p2 for that term is greater than the meanp1/p2 over all terms.
This modification was madeto ensure the assumption that p1 is much greaterthan p2.
A weighted sum was then computed foreach of the eight person subcategories according tothe formula below:1 To avoid noise, we used only those sentences in whicheach person instance was of the same category.Topic Sig ScoreType= ?N [ ?-score of wordn,Type/(distance from instance)2]where N is the number of words in the sentence,?-score of wordn,Type is the topic signature score ofword n for topic Type, and distance from instanceis the number of words away from the instancethat word n is.
These topic signature scores arecalculated for each of the eight subcategories.These eight topic signature features conveysemantic information about the overall contextin which each instance exists.
The topicsignature scores are weighted according to theinverse square of their distance under the (notalways true) assumption that the farther away aword is from an instance, the less information itbears on classification.
This weighting isparticularly important when instances ofdifferent categories occur in the same sentence(e.g., ?
?of those donating to Bush?s campaignwas actor Arnold Schwarzenegger??
).3.3 WordNet FeaturesA natural limitation of the topic signaturefeatures is their inability to give weight torelated and synonymous terms that do notappear in the training data.
To address thislimitation, we took advantage of the onlineresource WordNet (Fellbaum, 1998).
TheWordNet hypernym tree was expanded for eachword surrounding the instance and each word inthe tree was given a score based on the topicsignature database generated from the trainingdata.
The scores were then weighted by theinverse of their height in the tree and thensummed together, similarly to the procedure in(Resnik, 1993).
These sums are computed foreach word surrounding the instance, and aresummed according to the weighting processdescribed above.
This produces a distinctWordNet feature for each of the eight classesand is described by the equation below:WordNet Score Type=?N[?M ?-score of wordm,Type/(depth of wordm in WordNet)]/(distance from instance) 2where the variables are as above and M is thenumber of words in the WordNet hypernymtree.
These WordNet features supplement thecoverage of the topic signatures generated fromthe training data by including synonyms thatmay not have existed in that data set.
Further,the features include information gained from thehypernyms themselves (e.g., the hypernym of?Congress?
is ?legislature?).
These finalhypernym scores are weighted by the inverse oftheir height in the tree to reduce the effect ofconcepts that may be too general (e.g., at the topof the hypernym tree for ?Congress?
is?group?).
In order to avoid noise due toinappropriate word senses, we only used datafrom senses that matched the part of speech.These eight WordNet features add to the abovefeatures for a total of 96 features.4.
Methods4.1 Experiment 1: Held out dataTo examine the generalizability of classifierstrained on the automatically generated data, a C4.5decision tree classifier (Quinlan, 1993) was trainedand tested on the held out test set described above.Initial results revealed that, due to differingcontexts, instances of the same name in a singletext would often be classified into differentsubcategories.
To deal with this problem, weaugmented the classifier with another program,MemRun, which standardizes the subcategorizationof instances based on their most frequentclassification.
Developed and tested in(Fleischman, 2001), MemRun is based upon thehypothesis that by looking at all the classificationsan instance has received throughout the test set, an?average?
sub-categorization can be computed thatoffers a better guess than a low confidenceindividual classification.MemRun operates in two rounds.
In the firstround, each instance of the test set is evaluatedusing the decision tree, and a classificationhypothesis is generated.
If the confidence level ofthis hypothesis is above a certain threshold(THRESH 1), then the hypothesis is entered intothe temporary database (see Figure 3) along withthe degree of confidence of that hypothesis, and thenumber of times that hypothesis has been received.Because subsequent occurrences of personinstances frequently differ orthographically fromtheir initial occurrence (e.g., ?George Bush?followed by ?Bush?)
a simple algorithm wasdevised for surface reference disambiguation.
Thealgorithm keeps a record of initial full name usagesof all person instances in a text.
When partialreferences to the instance are later encountered inthe text, as determined by simple regularexpression matching, they are entered into theMemRun database as further occurrences of theoriginal instance.
This record of full namereferences is cleared after a text is examined toavoid possible instance confusions (e.g., ?GeorgeW.
Bush?
and ?George Bush Sr.?).
This simplealgorithm operates on the assumption that partialreferences to individuals with the same last name inthe same text will not occur due to human authors?desire to avoid any possible confusion.2  When allof the instances in the data set are examined, theround is complete.In MemRun?s second round, the data set isreexamined, and hypothesis classifications areagain produced.
If the confidence of one of thesehypotheses is below a second threshold (THRESH2), then the hypothesis is ignored and the databasevalue is used.3  In this experiment, the entries in thedatabase are compared and the most frequent entry(i.e., the max classification based on confidencelevel multiplied by the increment) is returned.When all instances have been again examined, theround is complete.Figure 3.
MemRun database for Decision Tree classifier4.2 Experiment 2: Learning AlgorithmsHaving examined the generalizability whenusing automatically generated training data, we turnto the question of appropriate learning algorithmsfor the task.
We chose to examine five differentlearning algorithms.
Along with C4.5, weexamined a feed-forward neural network with 50hidden units, a k-Nearest Neighborsimplementation (k=1) (Witten & Frank, 1999), aSupport Vector Machine implementation using alinear kernel (Witten & Frank, 1999), and a na?veBayes classifier using discretized attributes and2 This algorithm does not address definite descriptions andpronominal references because they are not classified byIdentiFinder as people names, and thus are not marked forfine-grained classification in the test set.3 The ability of the algorithm to ignore the database?ssuggestion in the second round allows instances with thesame name (e.g., ?Paul Simon?)
to receive differentclassifications in different contexts.Instance Class Confidence OccurGeorge Bush Politician 97.5% 4Business 83.4% 1Dana Carvey Entertainer 92.4% 7Politician 72.1% 2with feature subset selection (Kohavi &Sommerfield, 1996).
For each classifier,comparisons were based on results from thevalidation set (~1000 instances) described above.4.3 Experiment 3: Feature setsTo examine the effectiveness of the individualtypes of features, a C4.5 decision tree classifier(Quinlan, 1993) was trained on the 25,000 instancedata set described above using all possiblecombinations of the three feature sets.
Theperformance was ascertained on the validation setdescribed above.5.
Results5.1 Experiment 1: Held out data47.370.483.55760.970.14050607080%CorrectValidation Held outBaseline No MemRun MemRunFigure 4.
Results of classifier on validation set and heldout data.
Results compare baseline of always choosingmost probable class with C4.5 classifier both with andwithout MemRun.The results of the classifier on both thevalidation set and the held out test set can be seenin Figure 4.
The results are presented for aclassifier trained using the C4.5 algorithm bothwith and without MemRun (THRESH1=85,THRESH2=98).
Also shown is the baseline scorefor each test set computed by always choosing themost frequent classification (Politician for both).It is clear from the figure that the classifiers forboth test sets and for both conditions performedbetter than baseline.
Also clear is that the MemRunalgorithm significantly improves performance onboth the validation and held out test sets.Figure 4 further shows a large discrepancybetween the performance of the classifier on thetwo data sets.
Expectedly, the validation set isclassified more easily both with and withoutMemRun.
The size of the discrepancy is a functionof how different the distribution of the training setis from the true distribution of person instances inthe world.
While this discrepancy is undeniable, itis interesting to note how well the classifiergeneralizes given the very biased sample uponwhich it was trained.5.2 Experiment 2: Learning Algorithms47.357.76468.1 69.570.44045505560657075%CorrectBaseline k-NN Na?veBayesSVM Neural Net C4.5Figure 5.
Comparison of different learning algorithms ona validation set.
Learners include: k-Nearest Neighbors,Na?ve Bayes, support vector machine, neural network, andC4.5 decision tree.Figure 5 shows the results of comparingdifferent machine learning strategies.
It is clearfrom the figure that all the algorithms performbetter than the baseline score, while the C4.5algorithm performs the best.
This is notsurprising as decision trees combine powerfulaspects of non-linear separation and featureselection.Interestingly, however, there is no clearrelationship between performance and thetheoretical foundations of the classifier.Although the two top performers (decision treeand the neural network) are both non-linearclassifiers, the linear SVM outperforms the non-linear k-Nearest Neighbors.
This must,however, be taken with a grain of salt, as littlewas done to optimize either the k-NN or SVMimplementation.Another interesting finding in recent workis an apparent relationship between classifiertype and performance on held out data.
Whilethe non-parametric learners, i.e.
C4.5 and k-NN,are fairly robust to generalization, theparametric learners, i.e.
Na?ve Bayes and SVM,perform significantly worse on the newdistribution.
In future work, we intend toexamine further this possible relationship.5.3 Experiment 3: Feature setsThe results of the feature set experiment canbe seen in figure 6.
Results are shown for thevalidation set using all combinations of thethree feature sets.
A baseline measure ofalways classifying the most frequent category(Politician) is also displayed.It is clear that each of the single feature sets(frequency features, topic signature features,and WordNet features) is sufficient tooutperform the baseline.
Interestingly, topicsignature features outperform WordNetfeatures, even though they are similar in form.This suggests that the WordNet features arenoisy and may contain too much generality.
Itmay be more appropriate to use a cutoff, suchthat only the concepts two levels above the termare examined.
Another source of noise comesfrom words with multiple senses.
Although ourmethod uses only word senses of theappropriate part of speech, WordNet still oftenprovides many different possible senses.47.359.463.667.160.768 68.170.44045505560657075%CorrectBaseline Freq WN Sig Freq & WN Freq & Sig Sig & WN AllFigure 6.
Results of using different combinations offeature sets.
Results shown on validation set using C4.5classifier without MemRun.Also of interest is the effect of combiningany two feature sets.
While using topicsignatures and either word frequencies orWordNet features improves performance by asmall amount, combining frequency andWordNet scores results in performance worsethan WordNet alne.
This suggests over fittingof the training data and may be due to the noisein the WordNet features.It is clear, however, that the combination ofall three features provides considerableimprovement in performance over any of theindividual features.
In future work we willexamine how ensemble learning (Hastie, 2001)might be used to capitalize further on thesequalitatively different feature sets.6.
Related WorkWhile much research has gone into the coarsecategorization of named entities, we are not awareof much previous work using learning algorithmsto perform more fine-grained classification.Wacholder et al (1997) use hand-written rulesand knowledge bases to classify proper names intobroad categories.
They employ an aggregationmethod similar to MemRun, but do not usemultiple thresholds to increase accuracy.MacDonald (1993) also uses hand-written rulesfor coarse named entity categorization.
However,where Wacholder et al use evidence internal to theentity name, MacDonald employs local context toaid in classification.
Such hand-written heuristicrules resemble those we automatically generate.Bechet et al (2000) use a decision treealgorithm to classify unknown proper names intothe categories: first name, last name, country, town,and organization.
This is still a much coarserdistinction than that focused on in this research.Further, Bechet et al focused only on those propernames embedded in complex noun phrases (NPs),using only elements in the NP as its feature set.7.
ConclusionsThe results of these experiments, thoughpreliminary, are very promising.
Our researchmakes clear that positive results are possiblewith relatively simple statistical techniques.This research has shown that training dataconstruction is critical.
The failure of ourautomatic data generation algorithm to producea good sample of training data is evident in thelarge disparity between performances onvalidation and held out test sets.
There are atleast two reasons for the algorithm?s poorsampling.First, by using only high confidence guessesfrom the seed trained classifier, the training datamay have a disproportionate number ofinstances that are easy to classify.
This isevident in the number of partial names that arepresent in the held out test set versus thetraining set.
Partial names, such as ?Simon?instead of ?Paul Simon,?
usually occur withweaker evidence for classification than fullnames.
In the training set only 45.1% of theinstances are partial names, whereas in the morerealistic distribution of the held out set, 58.4%are partial names.The second reason for the poor samplingstems from the use of lists of person names.Because the training set is derived fromindividuals in these lists, the coverage ofindividuals included in the training set isinherently limited.
For example, in thebusinessperson category, lists of individualswere taken from such resources as Forbes?annual ranking of the nation?s wealthiestpeople, under the assumption that wealthypeople are often in the news.
However, the listfails to mention the countless vice presidentsand analysts that frequent the pages of the WallStreet Journal.
This failure to include suchlower level businesspersons means that a largespace of the classification domain is notcovered by the training set, which in turn leadsto poor results on the held out test set.The results of these experiments suggestthat better fine-grained classification of namedentities will require not only more sophisticatedfeature selection, but also a better datageneration procedure.
In future work, we willinvestigate more sophisticated bootstrappingmethods, as (Collins & Singer, 1999) as well asco-training and co-testing (Muslea et al, 2000).In future work we will also examineadapting the hierarchical decision list algorithmfrom (Yarowsky, 2000) to our task.
Treatingfine-grained classification of named entities as aword sense disambiguation problem (wherecategories are treated as different senses of ageneric ?person name?)
allows these methods tobe directly applicable.
The algorithm isparticularly relevant in that it provides anintuitive way to take advantage of thesimilarities of certain categories (e.g., Athleteand Entertainer).Of more theoretical concern are theproblems of miscellaneous classifications thatdo not fit easily into any category, as well as,instances that may fit into more than onecategory (e.g., Ronald Reagan can be either aPolitician or an Entertainer).
We plan toaddress these issues as well as problems thatmay arise with extending this system for usewith other classes, such as organizations.8.
ReferencesBechet, F., Nasr, A., Genet, F.  2000.
Tagging unknown propernames using decision trees.
Proc.
of ACL, Hong Kong.Bikel, D., Schwartz, R., Weischedel, R.  1999.
An algorithmthat learns what?s in a name.
Machine Learning: SpecialIssue on NL Learning, 34, 1-3.Brill E. 1994.
Some advances in rule based part of speechtagging.
Proc.
of AAAI, Los Angeles.Collins, M. and Singer, Y.
1999.
Unsupervised models fornamed entity classification.
Proc.
of the Joint SIGDATConference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora.Dunning, T., 1993.
Accurate methods for statistics of surpriseand coincidence.
Computational Linguistics, 19:61--74.Fellbaum, C.
(ed.
), 1998.
An electronic lexical database.Cambridge, MA: MIT Press.Fleischman, M. 2001.
Automated Subcategorization of NamedEntities.
Proc.
of the ACL Student Workshop.Hastie, T., Tibshirani, R., and J. Friedman.
2001.
TheElements of Statistical Learning.
Springer.Jurafsky, D. and Martin, J..  2000.
Speech and LanguageProcessing.
Upper Saddle River, NJ: Prentice Hall.Kohavi,, R., Sommerfield, D., and Dougherty, J, 1996.Data mining using MLC++ : A machine learning libraryin C ++.
Tools with Artificial Intelligence, pp.
234-245.Lin, C.-Y.
and E.H. Hovy, 2000.
The AutomatedAcquisition of Topic Signatures for TextSummarization.
Proc.
of the COLING Conference.Strasbourg, France.MacDonald D.D., 1993.
Internal and external evidence in theidentification and semantic categorization of proper names.In B.Boguraev and J. Pustejovsky, eds., Corpus Processingfor Lexical Acquisition, pp.
61-76, Cambridge: MIT Press.Muslea, I., Minton, S., Knoblock, C.  Selective samplingwith redundant views.
Proc.
of the 15th NationalConference on Artificial Intelligence, AAAI-2000.Porter, M. F. 1980.
An algorithm for suffix stripping.Program, 14 (no.
3), 130-137.Quinlin, J.R., 1993.
C4.5: Programs for Machine Learning.San Mateo, CA: Morgan Kaufmann Publishers.Resnik, P. 1993.
Selection and Information.
PhD thesis,University of Pennsylvania.Wacholder, N., Ravin, Y., Choi, M. 1997.
Disambiguation ofProper Names in Text.
Proc.
of the Fifth Conference onApplied Natural Language Processing, Washington, D.C.Witten, I.
& Frank, E. 1999.
Data Mining: PracticalMachine Learning Tools and Techniques with JAVAimplementations.
Morgan Kaufmann, October.Yarowsky D. 2000.
Hierarchical Decision List for WordSense Disambiguation.
Computers and the Humanities.34: 179-186.Yang, Y., Pedersen, J.O., 1997.
A Comparative Study onFeature Selection in Text Categorization, Proc.
of the14th International Conference on Machine LearningICML97, pp.
412-420
