Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 939?947,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUnsupervised Induction of Semantic RolesJoel Lang and Mirella LapataSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKJ.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractDatasets annotated with semantic roles arean important prerequisite to developing high-performance role labeling systems.
Unfortu-nately, the reliance on manual annotations,which are both difficult and highly expen-sive to produce, presents a major obstacle tothe widespread application of these systemsacross different languages and text genres.
Inthis paper we describe a method for induc-ing the semantic roles of verbal arguments di-rectly from unannotated text.
We formulatethe role induction problem as one of detectingalternations and finding a canonical syntacticform for them.
Both steps are implemented ina novel probabilistic model, a latent-variablevariant of the logistic classifier.
Our methodincreases the purity of the induced role clus-ters by a wide margin over a strong baseline.1 IntroductionSemantic role labeling (SRL, Gildea and Jurafsky2002) is the task of automatically classifying the ar-guments of a predicate with roles such as Agent, Pa-tient or Location.
These labels capture aspects of thesemantics of the relationship between the predicateand the argument while abstracting over surface syn-tactic configurations.
SRL has received much atten-tion in recent years (Surdeanu et al, 2008; Ma`rquezet al, 2008), partly because of its potential to im-prove applications that require broad coverage se-mantic processing.
Examples include informationextraction (Surdeanu et al, 2003), question answer-ing (Shen and Lapata, 2007), summarization (Melliet al, 2005), and machine translation (Wu and Fung,2009).Given sentences (1-a) and (1-b) as input, an SRLsystem would have to identify the verb predicate(shown in boldface), its arguments (Michael andsandwich) and label them with semantic roles (Agentand Patient).
(1) a.
[Michael]Agent eats [a sandwich]Patient.b.
[A sandwich]Patient is eaten [byMichael]Agent.Here, sentence (1-b) is an alternation of (1-a).The verbal arguments bear the same semantic role,even though they appear in different syntactic posi-tions: sandwich is the object of eat in sentence (1-a)and its subject in (1-b) but it is in both instances as-signed the role Patient.
The example illustrates thepassive alternation.
The latter is merely one typeof alternation, many others exist (Levin, 1993), andtheir computational treatment is one of the mainchallenges faced by semantic role labelers.Most SRL systems to date conceptualize semanticrole labeling as a supervised learning problem andrely on role-annotated data for model training.
Prop-Bank (Palmer et al, 2005) has been widely used forthe development of semantic role labelers as well asFrameNet (Fillmore et al, 2003).
Under the Prop-Bank annotation framework (which we will assumethroughout this paper) each predicate is associatedwith a set of core roles (named A0, A1, A2, and soon) whose interpretations are specific to that pred-icate1 and a set of adjunct roles (e.g., Location orTime) whose interpretation is common across predi-cates.
In addition to large amounts of role-annotateddata, SRL systems often make use of a parser to ob-tain syntactic analyses which subsequently serve asinput to a pipeline of components concerned with1More precisely, A0 and A1 have a common interpreta-tion across predicates as proto-agent and proto-patient (Dowty,1991).939identifying predicates and their arguments (argu-ment identification) and labeling them with semanticroles (argument classification).Supervised SRL methods deliver reasonably goodperformance (a system will recall around 81% of thearguments correctly and 95% of those will be as-signed a correct semantic role; see Ma`rquez et al2008 for details).
Unfortunately, the reliance on la-beled training data, which is both difficult and highlyexpensive to produce, presents a major obstacleto the widespread application of semantic role la-beling across different languages and text genres.And although corpora with semantic role annota-tions exist nowadays in other languages (e.g., Ger-man, Spanish, Catalan, Chinese, Korean), they tendto be smaller than their English equivalents and oflimited value for modeling purposes.
Moreover, theperformance of supervised systems degrades consid-erably (by 10%) on out-of-domain data even withinEnglish, a language for which two major annotatedcorpora are available.
Interestingly, Pradhan et al(2008) find that the main reason for this are errorsin the assignment of semantic roles, rather than theidentification of argument boundaries.
Therefore, amechanism for inducing the semantic roles observedin the data without additional manual effort wouldenhance the robustness of existing SRL systems andenable their portability to languages for which anno-tations are unavailable or sparse.In this paper we describe an unsupervised ap-proach to argument classification or role induction2that does not make use of role-annotated data.
Roleinduction can be naturally formalized as a cluster-ing problem where argument instances are assignedto clusters.
Ideally, each cluster should contain argu-ments corresponding to a specific semantic role andeach role should correspond to exactly one cluster.
Akey insight in our approach is that many predicatesare associated with a standard linking.
A linking isa deterministic mapping from semantic roles ontosyntactic functions such as subject, or object.
Mostpredicates will exhibit a standard linking, i.e., theywill be predominantly used with a specific map-ping.
Alternations occur when a different linkingis used.
In sentence (1-a) the predicate eat is usedwith its standard linking (the Agent role is mappedonto the subject function and the Patient onto theobject), whereas in sentence (1-b) eat is used with2We use the term role induction rather than argument clas-sification for the unsupervised setting.its passive-linking (the Patient is mapped onto sub-ject and the Agent appears as a prepositional phrase).When faced with such alternations, we will attemptto determine for each argument the syntactic func-tion it would have had, had the standard linking beenused.
We will refer to this function as the arguments?canonical function, and use the term canonicaliza-tion to describe the process of inferring these canon-ical functions in the case of alternations.
So, in sen-tence (1-b) the canonical functions of the argumentsby Michael and sandwich are subject and object, re-spectively.Since linkings are injective, i.e., no two seman-tic roles are mapped onto the same syntactic func-tion, the canonical function of an argument uniquelyreferences a specific semantic role.
We define aprobabilistic model for detecting non-standard link-ings and for canonicalization.
The model specifies adistribution p(F) over the possible canonical func-tions F of an argument.
We present an extension ofthe logistic classifier with the addition of latent vari-ables which crucially allow to learn generalizationsover varying syntactic configurations.
Rather thanusing manually labeled data, we train our model onobserved syntactic functions which can be obtainedautomatically from a parser.
These training instancesare admittedly noisy but readily available and aswe show experimentally a useful data source forinducing semantic roles.
Application of the modelto a benchmark dataset yields improvements over astrong baseline.2 Related WorkMuch previous work on SRL relies on supervisedlearning methods for both argument identificationand argument classification (see Ma`rquez et al 2008for an overview).
Most systems use manually anno-tated resources to train separate classifiers for dif-ferent SRL subtasks (e.g., Surdeanu et al 2008).A few approaches adopt semi-supervised learningmethods.
The idea here is to to alleviate the datarequirements for semantic role labeling by extend-ing existing resources through the use of unlabeleddata.
Swier and Stevenson (2004) induce role la-bels with a bootstrapping scheme in which the setof labeled instances is iteratively expanded usinga classifier trained on previously labeled instances.Pado?
and Lapata (2009) project role-semantic anno-tations from an annotated corpus in one languageonto an unannotated corpus in another language.And Fu?rstenau and Lapata (2009) propose a method940in which annotations are projected from a sourcecorpus onto a target corpus, however within thesame language.Unsupervised approaches to SRL have been fewand far between.
Early work on lexicon acquisitionfocuses on identifying verbal alternations rather thantheir linkings.
This is often done in conjunction withhand-crafted resources such as a taxonomy of possi-ble alternations (McCarthy and Korhonen, 1998) orWordNet (McCarthy, 2002).
Lapata (1999) proposesa corpus-based method that is less reliant on taxo-nomic resources, however focuses only on two spe-cific verb alternations.
Other work attempts to clus-ter verbs into semantic classes (e.g., Levin 1993) onthe basis of their alternation behavior (Schulte imWalde and Brew, 2002).More recently, Abend et al (2009) propose anunsupervised algorithm for argument identifica-tion that relies only on part-of-speech annotations,whereas Grenager and Manning (2006) focus onrole induction which they formalize as probabilis-tic inference in a Bayesian network.
Their modeldefines a joint probability distribution over the par-ticular linking used together with a verb instanceand for each verbal argument, its lemma, syntacticfunction as well as semantic role.
Parameters in thismodel are estimated using the EM algorithm as thetraining instances include latent variables, namelythe semantic roles and linkings.
To make inferencetractable they limit the set of linkings to a smallnumber and do not distinguish between differenttypes of adjuncts.
Our own work also focuses oninducing the semantic roles and the linkings usedby each verb.
Our approach is conceptually sim-pler and computationally more tractable.
Our modelis a straightforward extension of the logistic classi-fier with latent variables applied to all roles not justcoarse ones.3 Problem FormulationWe treat role induction as a clustering problem.The goal is to assign argument instances (i.e., spe-cific arguments, occurring in an input sentence) intoclusters such that each cluster contains instanceswith the same semantic role, and each semanticrole is found in exactly one cluster.
As we as-sume PropBank-style roles (Palmer et al, 2005),our model will allocate a separate set of clusters foreach predicate and assign the arguments of a specificpredicate to one of the clusters associated with it.As mentioned earlier (Section 1) a linking is a de-A0 A1 TMP MNRSBJ 54514 19684 15 7OBJ 3359 51730 93 54ADV 162 3506 976 2308TMP 5 60 15167 22PMOD 2466 4860 142 62OPRD 37 5554 1 36LOC 17 145 43 157DIR 0 178 15 6MNR 5 48 13 3312PRP 9 50 11 6LGS 2168 36 2 2PRD 413 830 31 38NMOD 422 388 25 59EXT 0 20 2 12DEP 18 150 25 65SUB 3 84 4 2CONJ 198 331 22 8ROOT 62 147 84 264517 88616 16803 6404Table 1: Contingency table between syntactic func-tion and semantic role for two core roles Agent (A0)and Patient (A1) and two adjunct roles, Time (TMP)and Manner (MNR).
Only syntactic functions occur-ring more than 1000 times are listed.
Counts wereobtained from the CoNLL 2008 training dataset us-ing gold standard parses (the marginals in the bottomrow also include counts of unlisted co-occurrences).terministic mapping from semantic roles onto syn-tactic functions.
Table 1 shows how frequently in-dividual semantic roles map onto certain syntacticfunctions.
The frequencies were obtained from theCoNLL 2008 dataset (see Surdeanu et al 2008 fordetails) and constitute an aggregate across predi-cates.
As can be seen, there is a clear tendency fora semantic role to be mapped onto a single syntac-tic function.
This is true across predicates and evenmore so for individual predicates.
For example, A0is commonly mapped onto subject (SBJ), whereasA1 is often realized as object (OBJ).
There are tworeasons for this.
Firstly, a predicate is often asso-ciated with a standard linking which is most fre-quently used.
Secondly, the alternate linkings of agiven predicate often differ from the standard link-ing only with respect to a few roles.
Importantly, wedo not assume that a single standard linking is valid941for all predicates.
Rather, each predicate has its ownstandard linking.
For example, in the standard link-ing for the predicate fall, A1 is mapped onto subjectposition, whereas in the standarad linking for eat,A1 is mapped onto object position.When an argument is attested with a non-standardlinking, we wish to determine the syntactic func-tion it would have had if the standard linking hadbeen used.
This canonical function of the argumentuniquely references a specific semantic role, i.e., thesemantic role that is mapped onto the function underthe standard linking.
We can now specify an indi-rect method for partitioning argument instances intoclusters:1.
Detect arguments that are linked in a non-standard way (detection).2.
Determine the canonical function of these argu-ments (canonicalization).
For arguments withstandard linkings, their syntactic function cor-responds directly to the canonical function.3.
Assign arguments to a cluster according to theircanonical function.We distinguish between detecting non-standard link-ings and canonicalization because in principle twoseparate models could be used.
In our probabilis-tic formulation, both detection and canonicaliza-tion rely on an estimate of the probability distribu-tion p(F) over the canonical function F of an ar-gument.
When the most likely canonical functiondiffers from the observed syntactic function this in-dicates that a non-standard linking has been used(detection).
This most likely canonical function canbe taken as the canonical function of the argument(canonicalization).Arguments are assigned to clusters based ontheir inferred canonical function.
Since we assumepredicate-specific roles, we induce a separate clus-ter for each predicate.
Given K clusters, we use thefollowing scheme for determining the mapping fromfunctions to clusters:1.
Order the functions by occurrence frequency.2.
For each of the K ?
1 most frequent functionsallocate a separate cluster.3.
Assign all remaining functions to the K-th clus-ter.4 ModelThe detection of non-standard linkings and canon-icalization both rely on a probabilistic model p(F)which specifies the distribution over the canonicalfunctions F of an argument.
As is the case with mostSRL approaches, we assume to be given a syntacticparse of the sentence from which we can extract la-beled dependencies, corresponding to the syntacticfunctions of arguments.
To train the model we ex-ploit the fact that most observed syntactic functionswill correspond to canonical functions.
This enablesus to use the parser?s output for training even thoughit does not contain semantic role annotations.Critically, the features used to determine thecanonical function must be restricted so that theygive no cues about possible alternations.
If theywould, the model could learn to predict alternations,and therefore produce output closer to the observedsyntactic rather than canonical function of an argu-ment.
To avoid this pitfall we only use features ator below the node representing the argument head inthe parse tree apart from the predicate lemma (seeSection 5 for details).Given these local argument features, a simple so-lution would be to use a standard classifier such asthe logistic classifier (Berger et al, 1996) to learnthe canonical function of arguments.
However, thisis problematic, because in our setting the trainingand application of the classifier happen on the samedataset.
The model will over-adapt to the observedtargets (i.e., the syntactic functions) and fail to learnappropriate canonical functions.
Lexical sparsity isa contributing factor: the parameters associated withsparse lexical features will be unavoidably adjustedso that they are highly indicative of the syntacticfunction they occur with.One way to improve generalization is to incor-porate a layer of latent variables into the logisticclassifier, which mediates between inputs (featuresdefined over parse trees) and target (the canonicalfunction).
As a result, inputs and target are no longerdirectly connected and the information conveyed bythe features about the target must be transferred viathe latent layer.
The model is shown in plate notationin Figure 1a.
Here, Xi represents the observed in-put features, Y the observed target, and Z j the latentvariables.
The number of latent variables influencesthe generalization properties of the model.
With toofew latent variables too little information will betransferred via the latent variables, whereas with toomany latent variables generalization will degrade.The model defines a probability distribution overthe target variable Y and the latent variables Z, con-942YZ jMXiNYZ1 Z2X2X1 X3(a) (b)Figure 1: The logistic classifier with latent variables(shaded nodes) illustrated as a graphical model using(a) plate notation and (b) in unrolled form for M = 2and N = 3.ditional on the input variables X :p(y,z|x,?)
=1P(x,?
)exp(?k?k?k(x,y,z))(1)We will assume that the latent variables Zi are bi-nary.
Each of the feature functions ?k is associatedwith a parameter ?k.
The partition function normal-izes the distribution:P(x,?)
=?y?zexp(?k?k?k(x,y,z))(2)Note that this model is a special case of a conditionalrandom field with latent variables (Sutton and Mc-Callum, 2007) and resembles a neural network withone hidden layer (Bishop, 2006).Let (c,d) denote a training set of inputs and corre-sponding targets.
The maximum-likelihood parame-ters can then be obtained by finding the ?
maximiz-ing:l(?)
= log p(d|c)= ?i log?z p(di,z|ci)= ?i log?z exp(?k ?k?k(ci,di,z))P(ci,?
)(3)And the gradient is given by:(?l)k = ??
?k l(?
)= ?i?z p(z|di,ci)?k(ci,di,z)?
?i?y,z p(y,z|ci)?k(ci,y,z)(4)where the first term is the conditional expected fea-ture count and the second term is the expected fea-ture count.Thus far, we have written the equations in ageneric form for arbitrary conditional random fieldswith latent variables (Sutton and McCallum, 2007).In our model we have two types of pairwise suffi-cient statistics: ?
(x,z) : R?{0,1}?
R, between asingle input variable and a single latent variable, and?
(y,z) : Y ?{0,1}?
R, between the target and a la-tent variable.
Then, we can more specifically writethe gradient component of a parameter associatedwith a sufficient statistic ?
(x j,zk) as:?i?zkp(zk|di,ci)?
(ci, j,zk)??i?zkp(zk|ci)?
(ci, j,zk) (5)And the gradient component of a parameter associ-ated with a sufficient statistic ?
(y,zk) is:?i?zkp(zk|di,ci)?(di,zk)??i?y,zkp(y,zk|ci)?
(y,zk) (6)To obtain maximum-a-posteriori parameter esti-mates we regularize the equations.
Like for the stan-dard logistic classifier this results in an additionalterm of the target function and each componentof the gradient (see Sutton and McCallum 2007).Computing the gradient requires computation of themarginals which can be performed efficiently usingbelief propagation (Yedidia et al, 2003).
Note thatdue to the fact, that there are no edges between thelatent variables, the inference graph is tree structuredand therefore inference yields exact results.
We usea stochastic gradient optimization method (Bottou,2004) to optimize the target.
Optimization is likelyto result in a local maximum, as the likelihood func-tion is not convex due to the latent variables.5 Experimental DesignIn this section we discuss the experimental designfor assessing the performance of the model de-scribed above.
We give details on the dataset, fea-tures and evaluation measures employed and presentthe baseline methods used for comparison with ourmodel.943Figure 2: Dependency graph (simplified) of a sample sentence from the corpus.Data Our experiments were carried out on theCoNLL 2008 (Surdeanu et al, 2008) training datasetwhich contains both verbal and nominal predicates.However, we focused solely on verbal predicates,following most previous work on semantic role la-beling (Ma`rquez et al, 2008).
The CoNLL datasetis taken form the Wall Street Journal portion ofthe Penn Treebank corpus (Marcus et al, 1993).Role semantic annotations are based on PropBankand have been converted from a constituent-basedto a dependency-based representation (see Surdeanuet al 2008).
For each argument of a predicate onlythe head word is annotated with the correspond-ing semantic role, rather than the whole constituent.In this paper we are only concerned with role in-duction, not argument identification.
Therefore, weidentify the arguments of each predicate by consult-ing the gold standard.The CoNLL dataset alo supplies an automaticdependency parse of each input sentence obtainedfrom the MaltParser (Nivre et al, 2007).
The targetand features used in our model are extracted fromthese parses.
Syntactic functions occurring morethan 1,000 times in the gold standard are shownin Table 1 (for more details we refer the interestedreader to Surdeanu et al 2008).
Syntactic func-tions were further modified to include prepositions ifspecified, resulting in a set of functions with whicharguments can be distinguished more precisely.
Thiswas often the case with functions such as ADV,TMP, LOC, etc.
Also, instead of using the prepo-sition itself as the argument head, we used the ac-tual content word modifying the preposition.
Wemade no attempt to treat split arguments, namely in-stances where the semantic argument of a predicatehas several syntactic heads.
These are infrequent inthe dataset, they make up for less than 1% of all ar-guments.Model Setup The specific instantiation of themodel used in our experiments has 10 latent vari-ables.
With 10 binary latent variables we can en-code 1024 different target values, which seems rea-sonable for our set of syntactic functions whichcomprises around 350 elements.Features representing argument instances wereextracted from dependency parses like the oneshown in Figure 2.
We used a relatively small featureset consisting of: the predicate lemma, the argumentlemma, the argument part-of-speech, the prepositioninvolved in dependency between predicate and argu-ment (if there is one), the lemma of left-most/right-most child of the argument, the part-of-speech ofleft-most/right-most child of argument, and a keyformed by concatenating all syntactic functions ofthe argument?s children.
The features for the argu-ment maker in Figure 2 are [sell, maker, NN, ?, the,auto, DT, NN, NMOD+NMOD].
The target for thisinstance (and observed syntactic function) is SBJ.Evaluation Evaluating the output of our modelis no different from other clustering problems.
Wecan therefore use well-known measures from theclustering literature to assess the quality of ourrole induction method.
We first created a set ofgold-standard role labeled argument instances whichwere obtained from the training partition of theCoNLL 2008 dataset (corresponding to sections02?21 of PropBank).
We used 10 clusters for eachpredicate and restricted the set of predicates to thoseattested with more than 20 instances.
This rules outsimple cases with only few instances relative to thenumber of clusters, which trivially yield high scores.We compared the output of our method againstthe gold-standard using the following common mea-sures.
Let K denote the number of clusters, ci the setof instances in the i-th cluster and g j the set of in-stances having the j-th gold standard semantic rolelabel.
Cluster purity (PU) is defined as:PU =1K ?imaxj|ci ?g j| (7)We also used cluster accuracy (CA, Equation 8),944PU CA CP CR CF1Mic Mac Mic Mac Mic Mac Mic Mac Mic MacSyntFunc 73.2 75.8 82.0 80.9 67.6 65.3 55.7 50.1 61.1 56.7LogLV 72.5 74.0 81.1 79.4 64.3 60.6 59.7 56.3 61.9 58.4UpperBndS 94.7 96.1 96.9 97.0 97.4 97.6 90.4 100 93.7 93.8UpperBndG 98.8 99.4 99.9 99.9 99.7 99.9 100 100 99.8 100Table 2: Clustering results using our model (LogLV) against the baseline (SyntFunc) and upper bounds(UpperBndS and UpperBndG).cluster precision (CP, Equation 9), and cluster recall(CR, Equation 9).
Cluster F1 (CF1) is the harmonicmean of precision and recall.CA =T P+T NT P+FP+T N +FN(8)CP =T PT P+FPCR =T PT P+FN(9)Here T P is the number of pairs of instances whichhave the same role and are in the same cluster, T N isthe number of pairs of instances which have differentroles and are in different clusters, FP is the numberof pairs of instances with different roles in the samecluster and FN the number of pairs of instances withthe same role in different clusters.Baselines and Upper Bound We compared ourmodel against a baseline that assigns arguments toclusters based on their syntactic function.
Here, noattempt is made to correct the roles of arguments innon-standard linkings.
We would also like to com-pare our model against a supervised system.
Unfor-tunately, this is not possible, as we are using the des-ignated CoNLL training set as our test set, and anysupervised system trained on this data would achieveunfairly high scores.
Therefore, we approximate theperformance of a supervised system by clustering in-stances according to their gold standard role afterintroducing some noise.
Specifically, we randomlyselected 5% of the gold standard roles and mappedthem to an erroneous role.
This roughly correspondsto the clustering which would be induced by a state-of-the-art supervised system with 95% precision.
Fi-nally, we also report the results of the true upperbound obtained by clustering the arguments, basedon their gold standard semantic role (again using 10clusters per verb).6 ResultsOur results are summarized in Table 2.
We reportcluster purity, accuracy, precision, recall, and F1 forour latent variable logistic classifier (LogLV) and abaseline that assigns arguments to clusters accord-ing to their syntactic function (SyntFunc).
The tablealso includes the gold standard upper bound (Up-perBndG) and its supervised proxy (UpperBndS).We report micro- and macro-average scores.3Model scores are quite similar to the baseline,which might suggest that the model is simply repli-cating the observed data.
However, this is not thecase: canonical functions differ from observed func-tions for approximately 27% of the argument in-stances.
If the baseline treated these instances cor-rectly, we would expect it to outperform our model.The fact that it does not, indicates that the baselineerror rate is higher precisely on these instances.
Inother words, the model can help in detecting alter-nate linkings and thus baseline errors.We further analyzed our model?s ability to de-tect alternate linkings.
Specifically, if we assume astandard linking where model and observation agreeand an alternate linking where they disagree, weobtain the following.
The number of true positives(correctly detected alternate linkings) is 27,606, thenumber of false positives (incorrectly marked al-ternations) is 32,031, the number of true negatives(cases where the model correctly did not detect analternate linking) is 132,556, and the number of falsenegatives (alternate linkings that the model shouldhave detected but did not) is 32,516.4.
The analysisshows that 46% of alternations (baseline errors) aredetected.3Micro-averages are computed over instances while macro-averages are computed over verbs.4Note that the true/false positives/negatives here refer to al-ternate linkings, not to be confused with the true/false positivesin equations (8) and (9).945PU CA CP CR CF1Mic Mac Mic Mac Mic Mac Mic Mac Mic MacSyntFunct 73.9 77.8 82.1 81.3 68.0 66.5 55.9 50.3 61.4 57.3LogLV 82.6 83.7 87.4 85.5 79.1 74.5 73.3 68.5 76.1 71.4Table 3: Clustering results using our model to detect alternate linkings (LogLV) against the baseline (Synt-Func).We can therefore increase cluster purity by clus-tering only those instances where the model doesnot indicate an alternation.
The results are shownin Table 3.
Using less instances while keeping thenumber of clusters the same will by itself tend toincrease performance.
To compensate for this, wealso report results for the baseline on a reduceddataset.
The latter was obtained from the origi-nal dataset by randomly removing the same num-ber of instances.5 By using the model to detect al-ternations, scores improve over the baseline acrossthe board.
We observe performance gains for pu-rity which increases by 8.7% (micro-average; com-pare Tables 2 and 3).
F1 also improves considerablyby 13% (micro-average).
These results are encour-aging indicating that detecting alternate linkings isan important first step towards more accurate roleinduction.We also conducted a more detailed error analysisto gain more insight into the behavior of our model.In most cases, alternate linkings where A1 occurs insubject position and A0 in object position are canon-icalized correctly (with 96% and 97% precision, re-spectively).
Half of the detected non-standard link-ings involve adjunct roles.
Here, the model has muchmore difficulty with canonicalization and is success-ful approximately 25% of the time.
For example, inthe phrase occur at dawn the model canonicalizesLOC to ADV, whereas TMP would be the correctfunction.
About 75% of all false negatives are due tocore roles and only 25% due to adjunct roles.
Manyfalse negatives are due to parser errors, which arereproduced by the model.
This indicates overfitting,and indeed many of the false negatives involve in-frequent lexical items (e.g., juxtapose or Odyssey).Finally, to put our evaluation results into context,we also wanted to compare against Grenager andManning?s (2006) related system.
A direct compar-ison is somewhat problematic due to the use of dif-5This was repeated several times to ensure that the resultsare stable across runs.ferent datasets and the fact that we induce labels forall roles whereas they collapse adjunct roles to a sin-gle role.
Nevertheless, we made a good-faith effortto evaluate our system using their evaluation setting.Specifically, we ran our system on the same test set,Section 23 of the Penn Treebank (annotated withPropBank roles), using gold standard parses with sixclusters for each verb type.
Our model achieves acluster purity score of 90.3% on this dataset com-pared to 89.7% reported in Grenager and Manning.7 ConclusionsIn this paper we have presented a novel frameworkfor unsupervised role induction.
We conceptualizedthe induction problem as one of detecting alternatelinkings and finding their canonical syntactic form,and formulated a novel probabilistic model that per-forms these tasks.
The model extends the logis-tic classifier with latent variables and is trained onparsed output which is used as a noisy target forlearning.
Experimental results show promise, alter-nations can be successfully detected and the qualityof the induced role clusters can be substantially en-hanced.We argue that the present model could be use-fully employed to enhance the performance of othermodels.
For example, it could be used in an activelearning context to identify argument instances thatare difficult to classify for a supervised or semi-supervised system and would presumably benefitfrom additional (manual) annotation.
Importantly,the framework can incorporate different probabilis-tic models for detection and canonicalization whichwe intend to explore in the future.
We also aim toembed and test our role induction method within afull SRL system that is also concerned with argu-ment identification.
Eventually, we also intend to re-place the treebank-trained parser with a chunker.946ReferencesAbend, O., R. Reichart, and A. Rappoport.
2009.
Un-supervised Argument Identification for Semantic RoleLabeling.
In Proceedings of ACL-IJCNLP.
Singapore,pages 28?36.Berger, A., S. Della Pietra, and V. Della Pietra.
1996.A Maximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics 22(1):39?71.Bishop, C. 2006.
Pattern Recognition and MachineLearning.
Springer.Bottou, L. 2004.
Stochastic Learning.
In Advanced Lec-tures on Machine Learning, Springer Verlag, LectureNotes in Artificial Intelligence, pages 146?168.Dowty, D. 1991.
Thematic Proto Roles and ArgumentSelection.
Language 67(3):547?619.Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck.2003.
Background to FrameNet.
International Journalof Lexicography 16:235?250.Fu?rstenau, H. and M. Lapata.
2009.
Graph Aligmentfor Semi-Supervised Semantic Role Labeling.
In Pro-ceedings of EMNLP.
Singapore, pages 11?20.Gildea, D. and D. Jurafsky.
2002.
Automatic Label-ing of Semantic Roles.
Computational Linguistics28(3):245?288.Grenager, T. and C. Manning.
2006.
Unsupervised Dis-covery of a Statistical Verb Lexicon.
In Proceedingsof EMNLP.
Sydney, Australia, pages 1?8.Lapata, M. 1999.
Acquiring Lexical Generalizationsfrom Corpora: A Case Study for Diathesis Alterna-tions.
In Proceedings of the 37th ACL.
pages 397?404.Levin, B.
1993.
English Verb Classes and Alternations: APreliminary Investigation.
The University of ChicagoPress.Marcus, M., B. Santorini, and M. Marcinkiewicz.
1993.Building a Large Annotated Corpus of English: thePenn Treebank.
Computational Linguistics 19(2):313?330.Ma`rquez, L., X. Carreras, K. Litkowski, and S. Steven-son.
2008.
Semantic Role Labeling: an Introduc-tion to the Special Issue.
Computational Linguistics34(2):145?159.McCarthy, D. 2002.
Using Semantic Preferences to Iden-tify Verbal Participation in Role Switching Alterna-tions.
In Proceedings of the 1st NAACL.
Seattle, WA,pages 256?263.McCarthy, D. and A. Korhonen.
1998.
Detecting VerbalParticipation in Diathesis Alternations.
In Proceed-ings of COLING/ACL.
Montre?al, Canada, pages 1493?1495.Melli, G., Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,B.
Gu, A. Sarkar, and F. Popowich.
2005.
Descrip-tion of SQUASH, the SFU Question Answering Sum-mary Handler for the DUC-2005 Summarization Task.In Proceedings of the HLT/EMNLP Document Under-standing Workshop.
Vancouver, Canada.Nivre, J., J.
Hall, J. Nilsson, G. Eryigit A. Chanev,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A Language-independent System for Data-driven Dependency Parsing.
Natural Language Engi-neering 13(2):95?135.Pado?, S. and M. Lapata.
2009.
Cross-lingual AnnotationProjection of Semantic Roles.
Journal of Artificial In-telligence Research 36:307?340.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics 31(1):71?106.Pradhan, S. S., W. Ward, and J. H. Martin.
2008.
TowardsRobust Semantic Role Labeling.
Computational Lin-guistics 34(2):289?310.Schulte im Walde, S. and C. Brew.
2002.
InducingGerman Semantic Verb Classes from Purely Syntac-tic Subcategorisation Information.
In Proceedings ofthe 40th ACL.
Philadelphia, PA, pages 223?230.Shen, D. and M. Lapata.
2007.
Using Semantic Roles toImprove Question Answering.
In Proceedings of theEMNLP-CoNLL.
Prague, Czech Republic, pages 12?21.Surdeanu, M., S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using Predicate-Argument Structures for Infor-mation Extraction.
In Proceedings of the 41st ACL.Sapporo, Japan, pages 8?15.Surdeanu, M., R. Johansson, A. Meyers, and L. Ma`rquez.2008.
The CoNLL-2008 Shared Task on Joint Parsingof Syntactic and Semantic Dependencies.
In Proceed-ings of the 12th CoNLL.
Manchester, England, pages159?177.Sutton, C. and A. McCallum.
2007.
An Introduction toConditional Random Fields for Relational Learning.In L. Getoor and B. Taskar, editors, Introduction toStatistical Relational Learning, MIT Press, pages 93?127.Swier, R. and S. Stevenson.
2004.
Unsupervised Se-mantic Role Labelling.
In Proceedings of EMNLP.Barcelona, Spain, pages 95?102.Wu, D. and P. Fung.
2009.
Semantic Roles for SMT:A Hybrid Two-Pass Model.
In Proceedings ofNAACL HLT 2009: Short Papers.
Boulder, Colorado,pages 13?16.Yedidia, J., W. Freeman, and Y. Weiss.
2003.
Understand-ing Belief Propagation and its Generalizations.
Mor-gan Kaufmann Publishers Inc., pages 239?269.947
