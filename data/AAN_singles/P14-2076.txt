Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 464?469,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCheap and easy entity evaluationBen Hachey Joel Nothman Will RadfordSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiaben.hachey@sydney.edu.au{joel,wradford}@it.usyd.edu.auAbstractThe AIDA-YAGO dataset is a popular tar-get for whole-document entity recogni-tion and disambiguation, despite lacking ashared evaluation tool.
We review eval-uation regimens in the literature whilecomparing the output of three approaches,and identify research opportunities.
Thisutilises our open, accessible evaluationtool.
We exemplify a new paradigm ofdistributed, shared evaluation, in whichevaluation software and standardised, ver-sioned system outputs are provided online.1 IntroductionModern entity annotation systems detect mentionsin text and disambiguate them to a knowledge base(KB).
Disambiguation typically returns the corre-sponding Wikipedia page or NIL if none exists.Named entity linking (NEL) work is driven bythe TAC shared tasks on query-driven knowledgebase population (Ji and Grishman, 2011).
Eval-uation focuses on disambiguating queried namesand clustering NIL mentions, but most systemsinternally perform whole-document named en-tity recognition, coreference, and disambiguation(Cucerzan and Sil, 2013; Pink et al, 2013; Chenget al, 2013; Fahrni et al, 2013).
Wikificationwork generally evaluates end-to-end entity anno-tation including KB-driven mention spotting anddisambiguation (Milne and Witten, 2008b; Kulka-rni et al, 2009; Ratinov et al, 2011; Ferragina andScaiella, 2010).
Despite important differences inmention handling, NEL and wikification work havefollowed a similar trajectory.
Yet to our knowl-edge, there are no comparative whole-documentevaluations of NEL and wikification systems.Public data sets have also driven researchin whole-document entity disambiguation(Cucerzan, 2007; Milne and Witten, 2008b;Kulkarni et al, 2009; Bentivogli et al, 2010; Hof-fart et al, 2011; Meij et al, 2012).
However, withmany task variants and evaluation methodologiesproposed, it is very difficult to synthesise a clearpicture of the state of the art.We present an evaluation suite for named entitylinking, leveraging and advocating for the AIDAdisambiguation annotations (Hoffart et al, 2011)over the large and widely used CoNLL NER data(Tjong Kim Sang and Meulder, 2003).
This buildson recent rationalisation and benchmarking work(Cornolti et al, 2013), adding an isolated evalua-tion of disambiguation.
Contributions include:?
a simple, open-source evaluation suite forend-to-end, whole-document NEL;?
disambiguation evaluation facilitated bygold-standard mentions;?
reference outputs from state-of-the-art NELand wikification systems published with thesuite for easy comparison;?
implementation of statistical significance anderror sub-type analysis, which are often lack-ing in entity linking evaluation;?
a venue for publishing benchmark resultscontinuously, complementing the annual cy-cle of shared tasks;?
a repository for versioned corrections toground truth annotation.We see this repository, at https://github.com/wikilinks/conll03_nel_eval, as amodel for the future of informal shared evaluation.We survey entity annotation tasks and evalua-tion, proposing a core suite of metrics for end-to-end linking and tagging, and settings that isolatemention detection and disambiguation.
A compar-ison of state-of-the-art NEL and wikification sys-tems illustrates how key differences in mentionhandling affect performance.
Analysis suggeststhat focusing evaluation too tightly on subtaskslike candidate ranking can lead to results that donot reflect end-to-end performance.4642 Tasks and metricsThe literature includes many variants of the en-tity annotation task and even more evaluation ap-proaches.
Systems can be invoked under two set-tings: given text with expressions to be linked(gold mentions); or given plain text only (systemmentions).
The former enables a diagnostic evalu-ation of disambiguation, while the latter simulatesa realistic end-to-end application setting.Within each setting, metrics may considerdifferent subsets of the gold (G) and system (S)annotations.
Given sets of (doc, token span, kbid)tuples, we define precision, recall and F1scorewith respect to some annotation filter f :Pf=|f(G) ?
f(S)||f(S)|, Rf=|f(G) ?
f(S)||f(G)|We advocate two core metrics, corresponding tothe major whole-document entity annotation tasks.Link annotation measures performance over everylinked mention.
Its filter fLmatches spans andlink targets, disregarding NILs.
This is particularlyapt when entity annotation is a step in an informa-tion extraction pipeline.
Tag annotation measuresperformance over document-level entity sets: fTdisregards span information and NILs.
This is ap-propriate when entity annotation is used, e.g., fordocument indexing or social media mining (Mi-halcea and Csomai, 2007; Meij et al, 2012).
Weproceed to ground these metrics and diagnosticvariants in the literature.2.1 End-to-end evaluationWe follow Cornolti et al (2013) in evaluating end-to-end entity annotation, including both mentiondetection and disambiguation.
In this context,fLequates to Cornolti et al?s strong annotationmatch; fTmeasures what they call entity match.2.2 Mention evaluationMention detection performance may be evaluatedregardless of linking decisions.
A filter fMdis-cards the link target (kbid).
Of the present metrics,only this considers NIL-linked system mentions asdifferent from non-mentions.
For comparabilitywith wikification, we consider an additional filterfMKBto NEL output that retains only linked men-tions.
fMand fMKBare equivalent to Cucerzan?s(2007) mention evaluation and Cornolti et al?sstrong mention match respectively.
fMis com-parable to the NER evaluation from the CoNLL2003 shared task (Tjong Kim Sang and Meulder,2003): span equivalence is handled the same way,but metrics here ignore mention types.2.3 Disambiguation evaluationMost NEL and wikification literature focuses ondisambiguation, evaluating the quality of link tar-get annotations in isolation from NER error.
Pro-viding systems with ground truth mentions makesfLequivalent to Mihalcea and Csomai?s (2007)sense disambiguation evaluation and Milne andWitten?s (2008b) disambiguation evaluation.
Itdiffers from Kulkarni et al?s (2009) metric in be-ing micro-averaged (equal weight to each men-tion), rather than macro-averaged across docu-ments.
fLrecall is comparable to TAC?s KB recall(Ji and Grishman, 2011).
It differs in that all men-tions are evaluated rather than specific queries.Related evaluations have also isolated disam-biguation performance by: considering the linksof only correctly identified mentions (Cucerzan,2007); or only true mentions where the correctentity appears among top candidates before dis-ambiguation (Ratinov et al, 2011; Hoffart et al,2011; Pilz and Paass, 2012).
We do not prefer thisapproach as it makes system comparison difficult.For comparability, we implement a filter fLHOFthat retains only Hoffart-linkable mentions havinga YAGO means relation to the correct entity.Tag annotation (fT) with ground truth men-tions is equivalent to Milne and Witten?s (2008b)link evaluation, Mihalcea and Csomai?s (2007)keyword extraction evaluation and Ratinov etal.
?s (2011) bag-of-titles evaluation.
It is compara-ble to Pilz and Paass?s (2012) bag-of-titles evalua-tion, but does not account for sequential order andkeeps all gold-standard links regardless of whetherthey are found by candidate generation.2.4 Further diagnostics and rank evaluationSeveral evaluations in the literature are beyond thescope of this paper but planned for future versionsof the code.
This includes further diagnostic sub-task evaluation, particularly candidate set recall(Hachey et al, 2013), NIL accuracy (Ji and Grish-man, 2011) and weak mention matching (Cornoltiet al, 2013).
With a score for each prediction, fur-ther metrics are possible: rank evaluation of tagannotation with r-precision, mean reciprocal rankand mean average precision (Meij et al, 2012);and rank evaluation of mentions for comparison toHoffart et al (2011) and Pilz and Paass (2012).4653 DataThe CoNLL-YAGO dataset (Hoffart et al, 2011)is an excellent target for end-to-end, whole-document entity annotation.
It is public, free andmuch larger than most entity annotation data sets.It is based on the widely used NER data fromthe CoNLL 2003 shared task (Tjong Kim Sangand Meulder, 2003), building disambiguation onground truth mentions.
It has standard trainingand development splits that are representative ofthe held-out test data, all being sourced from theReuters text categorisation corpus (Lewis et al,2004), which is provided free for research pur-poses.
Training and development comprise 1,162stories from 22-31 August 1996 and held-out testcomprises 231 stories from 6-7 December 1996.The layered annotation provides useful informa-tion for analysis including categorisation topics(e.g., general news, markets, sport) and NE typemarkup (PER, ORG, LOC, MISC).The primary drawback is that KB annotationsare currently present only if there is a YAGOmeans relation between the mention string andthe correct entity.
This means that there are anumber of CoNLL entity mentions referring toentities that exist in Wikipedia that are nonethe-less marked NIL in the ground truth (e.g.
?DSE?for ?Dhaka Stock Exchange?).
This may be ad-dressed by using a shared repository to adopt ver-sioned improvements to the ground truth.
Anno-tation over CoNLL tokenisation sometimes resultsin strange mentions (e.g., ?Washington-based?
in-stead of ?Washington?).
However, prescribed to-kenisation simplifies comparison and analysis.Another concern is that link annotation goesstale, since Wikipedia titles are only canonicalwith respect to a particular point in time.
This isbecause pages may be renamed or reorganised:?
to improve editorial structure, such as down-grading an entity from having a page of itsown, to a mere section in another page;?
to account for newly notable entities, such ascreating a disambiguation page for a title thatformerly had a single known referent; or?
because of changes in fact, such as corporatemergers and name changes.All systems compared provide Wikipedia titles aslabels, which are mapped to current titles for com-parison: for each entity title t linked in the golddata, we query the Wikipedia API to find t?s canon-ical form tcand retrieve titles of all redirects to tc.4 Reference systemsEven on public data sets, comparison to publishedresults can be very difficult and extremely costly(Fokkens et al, 2013).
We include reference sys-tem output in our repository for simple compar-ison.
Other researchers are welcome to add ref-erence output, providing a continuous benchmarkthat complements the annual cycle of large sharedtasks like TAC KBP.4.1 TagMeTagMe (Ferragina and Scaiella, 2010) is an end-to-end wikification system specialising in shorttexts.
TagMe performs best among publicly avail-able wikification systems (Cornolti et al, 2013).Mention detection uses a dictionary of anchortext from links between Wikipedia pages.
Candi-date ranking is based on entity relatedness (Milneand Witten, 2008a), followed by mention prun-ing.
We use thresholds on annotation scores sup-plied by Marco Cornolti (personal communica-tion) of 0.289 and 0.336 respectively for men-tion/link and tag evaluation.
TagMe annota-tions may not align with CoNLL token bound-aries, e.g., <annot title=?Oakland, New Jer-sey?>OAKLAND, N.J</annot>.
Before evalua-tion, we extend annotations to overlapping tokens.4.2 AIDAAIDA (Hoffart et al, 2011) is the system pre-sented with the CoNLL-YAGO dataset and placesemphasis on state-of-the-art ranking of candi-date entity sets.
Mentions are ground truth fromthe CoNLL data to isolate ranking performance,equivalent to applying the fLHOFfilter.
Ranking isinformed by a graph model of entity compatibility.4.3 SchwaSchwa (Radford et al, 2012) is a heuristic NELsystem based on a TAC 2012 shared task en-trant.
Mention detection uses a NER model trainedon news text followed by rule-based corefer-ence.
Disambiguation uses an unweighted com-bination of KB statistics, document compatibil-ity (Cucerzan, 2007), graph similarity and targetedtextual similarity.
Candidates that score belowa threshold learned from TAC data are linked toNIL.
The system is very competitive, performingat 93% and 97% respectively of the best accuracynumbers we know of on 2011 and 2012 TAC eval-uation data (Cucerzan and Sil, 2013).466System Mentions Filter P R F1Cucerzan System fM82.2 84.8 83.5Schwa System fM86.9 76.7 81.5TagMe System fMKB75.2 60.4 67.0Schwa System fMKB82.5 74.5 78.3Table 1: Mention detection results.
Cucerzan re-sults as reported (Cucerzan, 2007).5 ResultsWe briefly report results over the reference sys-tems to highlight characteristics of the evaluationmetrics and task settings.
Results hinge uponSchwa since we have obtained only its output in allsettings.
Except where noted, all differences aresignificant (p < 0.05) according to approximaterandomisation (Noreen, 1989), permuting annota-tions over whole documents.5.1 Mention evaluationTable 1 evaluates mentions with and without NILs.None of the systems reported use a CoNLL-trained NER tagger, for which top shared task par-ticipants approached 90% F1in a stricter evalu-ation than fM.
We note the impressive numbersreported by Cucerzan (2007) using a novel ap-proach to mention detection based on capitalisa-tion and corpus co-occurrence statistics, and thesimilar performance1to Schwa, whose NER com-ponent is trained on another news corpus.In wikification, NIL-linked mentions may notbe relevant, and it may suffice to identify onlythe most canonical forms of names, rather thanall mentions in a coreference chain.
With fMKB,Schwa has much higher recall than TagMe, thoughTagMe?s precision is understated because it gener-ates non-NE annotations that are not present in theCoNLL-YAGO ground truth (e.g., linking ?striker?to Forward (association football)).5.2 Disambiguation evaluationTable 2 contains results isolating disambiguationperformance.
AIDA ranking outperforms Schwaaccording to both the link (fLHOF) and tag metrics(fTHOF).
If we remove the Hoffart et al (2011)linkable constraint, we observe that Schwa disam-biguation performance loses about 8 points in pre-cision on the link metric (fL) and 2 points on thetag metric (fT).
This suggests that disambiguation1Significance cannot be tested since we do not have theCucerzan (2007) output.System Mentions Filter P R F1Schwa Gold fL67.5 78.3 72.5Schwa Gold fLHOF79.7 78.3 79.0AIDA Gold fLHOF83.2 83.2 83.2Schwa Gold fT77.8 77.7 77.7Schwa Gold fTHOF80.1 77.6 78.8AIDA Gold fTHOF87.7 84.2 85.9Table 2: Disambiguation results for mention-levellinking and document-level tagging.System Mentions Filter P R F1TagMe System fL63.2 50.7 56.3Schwa System fL67.6 61.0 64.2TagMe System fT65.0 65.4 65.2Schwa System fT71.2 62.6 66.6Table 3: End-to-end results for mention-level link-ing and document-level tagging.evaluation without the linkable constraint is im-portant, especially if the application requires de-tecting and disambiguating all mentions.The comparison here highlights a notable eval-uation intricacy.
The Schwa system disambiguatesall gold mentions rather than those with KB links,and the document compatibility approach meansthat evidence from a NIL mention may offer con-founding evidence when linking linkable men-tions.
Further, although using the same mentions,systems use search resources with different recallcharacteristics, so the Schwa system may not re-trieve the correct candidate to disambiguate.5.3 End-to-end evaluationFinally, Table 3 contains end-to-end entity anno-tation results.
Again, these results highlight keydifferences in mention handling between NEL andwikification.
Coreference modelling helps NELdetect and link ambiguous names (e.g., ?Presi-dent Bush?)
that refer to the same entity as unam-biguous names in the same text (e.g., ?George W.Bush?).
And restricting the the universe to namedentities is appropriate for the CoNLL-YAGO data.The advantage is marked in the mention-level linkevaluation (fL).
However, the systems are statis-tically indistinguishable in the document-level tagevaluation (fT).
Thus the extra NER and corefer-ence machinery may not be justified if the applica-tion is document indexing or social media mining(Meij et al, 2012), wherein a KB-driven mentiondetector may be favourable for other reasons.467ErrorfLHOFfLAIDA Schwa TagMe Schwawrong link 752 896 429 605link as nil - 79 - 111nil as link - - 183 337missing - - 1,780 1,031extra - - 1,663 927Table 4: fLHOFand fLerror profiles.0 20 40 60 80 100LOCORGPERMISCFigure 1: Schwa fLand fLHOFF1for NE types6 AnalysisWe analyse the types of error that a system makes.We also harness the multi-layered annotation toquantify the effect of NE type and document topic.By error type Table 4 shows error counts basedon the disambiguation link evaluation with thelinkable constraint (fLHOF) and the end-to-endlink evaluation (fL).
Errors are divided as follows:wrong link: mention linked to wrong KB entrylink as nil: KB-entity mention linked to NILnil as link: NIL mention linked to the KBmissing: true mention not detectedextra: mention detected spuriouslyAIDA outperforms Schwa under the linkable eval-uation, making fewer wrong link errors.
Schwaalso overgenerates NIL, which may reflect candi-date recall errors or a conservative disambiguationthreshold.
On the end-to-end evaluation, Schwamakes more linking errors (wrong link, link as nil,nil as link) than TagMe, but fewer in mention de-tection, leading to higher overall performance.By entity type Figure 1 evaluates only men-tions where the CoNLL 2003 corpus (Tjong KimSang and Meulder, 2003) marks a NE mention ofeach type.
This is based on the link evaluationof Schwa.
The left and right bars correspond toend-to-end (fL) and disambiguation (fLHOF) F1respectively.
In accord with TAC results (Ji andGrishman, 2011), high accuracy can be achievedon PER when a full name is given, while ORG issubstantially more challenging.
MISC entities aresomewhat difficult to disambiguate, with identifi-cation errors hampering end-to-end performance.0 20 40 60 80 100SportsDomestic PoliticsCorporate / IndustrialInternat?l RelationsMarketsWar / Civil WarCrime / Law Enforc?tFigure 2: Schwa fLand fLHOFF1for top topicsBy topical category The underlying ReutersCorpus documents are labelled with topic, countryand industry codes (Lewis et al, 2004).
Figure 2reports F1on test documents from each frequenttopic.
It highlights that much ambiguity remainsunresolved in Sports, while very high performancelinking is attainable in categories such as Marketsand Domestic Politics, only when given groundtruth linkable mentions.7 ConclusionWe surveyed entity annotation tasks and advocateda core set of metrics for mention, disambiguationand end-to-end evaluation.
This enabled a directcomparison of state-of-the-art NEL and wikifica-tion systems, highlighting the effect of key differ-ences.
In particular, NER and coreference mod-ules make NEL approaches suitable for applica-tions that require all mentions, including ambigu-ous names and entities that are not in the KB.
Forapplications where document-level entity tags areappropriate, the NEL and wikification approacheswe evaluate have similar performance.The big picture we wish to convey is a newapproach to community evaluation that makesbenchmarking and qualitative comparison cheapand easy.
In addition to the code being opensource, we use the repository to store referencesystem output, and ?
we hope ?
emendations tothe ground truth.
We encourage other researchersto contribute reference output and hope that thiswill provide a continuous benchmark to comple-ment the current cycle of shared tasks.AcknowledgementsMany thanks to Johannes Hoffart, Marco Cornolti,Xiao Ling and Edgar Meij for reference outputsand guidance.
Ben Hachey is the recipient ofan Australian Research Council Discovery EarlyCareer Researcher Award (DE120102900).
Theother authors were supported by the Capital Mar-kets CRC Computable News project.468ReferencesLuisa Bentivogli, Pamela Forner, Claudio Giu-liano, Alessandro Marchetti, Emanuele Pianta, andKateryna Tymoshenko.
2010.
Extending EnglishACE 2005 corpus annotation with ground-truth linksto Wikipedia.
In COLING Workshop on The Peo-ple?s Web Meets NLP: Collaboratively ConstructedSemantic Resources, pages 19?27.Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-Wei Chang, Zhiye Fei, Mark Sammons, John Wi-eting, Subhro Roy, Chizheng Wang, and Dan Roth.2013.
Illinois cognitive computation group UI-CCGTAC 2013 entity linking and slot filler validationsystems.
In Text Analysis Conference.Marco Cornolti, Paolo Ferragina, and MassimilianoCiaramita.
2013.
A framework for benchmark-ing entity-annotation systems.
In 22nd InternationalConference on the World Wide Web, pages 249?260.Silviu Cucerzan and Avirup Sil.
2013.
The MSR sys-tems for entity linking and temporal slot filling atTAC 2013.
In Text Analysis Conference.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 708?716.Angela Fahrni, Benjamin Heinzerling, Thierry G?ockel,and Michael Strube.
2013.
HITS?
monolingual andcross-lingual entity linking system at TAC 2013.
InText Analysis Conference.Paolo Ferragina and Ugo Scaiella.
2010.
TAGME:On-the-fly annotation of short text fragments (byWikipedia entities).
In 19th International Confer-ence on Information and Knowledge Management,pages 1625?1628.Antske Fokkens, Marieke van Erp, Marten Postma, TedPedersen, Piek Vossen, and Nuno Freire.
2013.
Off-spring from reproduction problems: What replica-tion failure teaches us.
In 51st Annual Meetingof the Association for Computational Linguistics,pages 1691?1701.Ben Hachey, Will Radford, Joel Nothman, MatthewHonnibal, and James R. Curran.
2013.
Evaluat-ing entity linking with Wikipedia.
Artificial Intel-ligence, 194:130?150.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-dino, Hagen F?urstenau, Manfred Pinkal, Marc Span-iol, Bilyana Taneva, Stefan Thater, and GerhardWeikum.
2011.
Robust disambiguation of namedentities in text.
In Conference on Empirical Methodsin Natural Language Processing, pages 782?792.Heng Ji and Ralph Grishman.
2011.
Knowledge basepopulation: Successful approaches and challenges.In 49th Annual Meeting of the Association for Com-putational Linguistics, pages 1148?1158.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collective annota-tion of Wikipedia entities in web text.
In 15th Inter-national Conference on Knowledge Discovery andData Mining, pages 457?466.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
RCV1: A new benchmark collectionfor text categorization research.
Journal of MachineLearning Research, 5:361?397.Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.2012.
Adding semantics to microblog posts.
In 5thInternational Conference on Web Search and DataMining, pages 563?572.Rada Mihalcea and Andras Csomai.
2007.
Wik-ify!
Linking documents to encyclopedic knowledge.In 16th Conference on Information and KnowledgeManagement, pages 233?242.David Milne and Ian H. Witten.
2008a.
An effec-tive, low-cost measure of semantic relatedness ob-tained from Wikipedia links.
In AAAI Workshop onWikipedia and Articial Intelligence, pages 25?30.David Milne and Ian H. Witten.
2008b.
Learningto link with Wikipedia.
In 17th Conference on In-formation and Knowledge Management, pages 509?518.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
John Wiley & Sons.Anja Pilz and Gerhard Paass.
2012.
Collectivesearch for concept disambiguation.
In 24th Inter-national Conference on Computational Linguistics,pages 2243?2258.Glen Pink, Will Radford, Will Cannings, AndrewNaoum, Joel Nothman, Daniel Tse, and James R.Curran.
2013.
SYDNEY CMCRC at TAC 2013.
InText Analysis Conference.Will Radford, Will Cannings, Andrew Naoum, JoelNothman, Glen Pink, Daniel Tse, and James R.Curran.
2012.
(Almost) Total Recall ?
SYD-NEY CMCRC at TAC 2012.
In Text Analysis Con-ference.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to Wikipedia.
In 49th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1375?1384.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 shared task:Language-independent named entity recognition.
InConference On Computational Natural LanguageLearning, pages 142?147.469
