Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 63?70,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPUsing Large-scale Parser Output to Guide Grammar DevelopmentAscander DostPowerset, a Microsoft companyadost@microsoft.comTracy Holloway KingPowerset, a Microsoft companyTracy.King@microsoft.comAbstractThis paper reports on guiding parser de-velopment by extracting information fromoutput of a large-scale parser applied toWikipedia documents.
Data-driven parserimprovement is especially important forapplications where the corpus may differfrom that originally used to develop thecore grammar and where efficiency con-cerns affect whether a new constructionshould be added, or existing analyses mod-ified.
The large size of the corpus in ques-tion also brings scalability concerns to theforeground.1 IntroductionInitial development of rule-based parsers1 is oftenguided by the grammar writer?s knowledge of thelanguage and test suites that cover the ?core?
lin-guistic phenomena of the language (Nerbonne etal., 1988; Cooper et al, 1996; Lehmann et al,1996).
Once the basic grammar is implemented,including an appropriate lexicon, the direction ofgrammar development becomes less clear.
Inte-gration of a grammar in a particular applicationand the use of a particular corpus can guide gram-mar development: the corpus and application willrequire the implementation of specific construc-tions and lexical items, as well as the reevalua-tion of existing analyses.
To streamline this sortof output-driven development, tools to examineparser output over large corpora are necessary, andas corpus size increases, the efficiency and scal-ability of those tools become crucial concerns.Some immediate relevant questions for the gram-mar writer include:1The techniques discussed here may also be relevant topurely machine-learned parsers and are certainly applicableto hybrid parsers.?
What constructions and lexical items need tobe added for the application and corpus inquestion??
For any potential new construction or lexicalitem, is it worth adding, or would it be betterto fall back to robust techniques??
For existing analyses, are they applying cor-rectly, or do they need to be restricted, or evenremoved?In the remainder of this section, we briefly dis-cuss some existing techniques for guiding large-scale grammar development and then introducethe grammar being developed and the tool we usein examining the grammar?s output.
The remain-der of the paper discusses development of lexicalresources and grammar rules, how overall progressis tracked, and how analysis of the grammar outputcan help development in other natural languagecomponents.1.1 Current TechniquesThere are several techniques currently being usedby grammar engineers to guide large-scale gram-mar development, including error mining to de-tect gaps in grammar coverage, querying tools forgold standard treebanks to determine frequencyof linguistic phenomena, and tools for queryingparser output to determine how linguistic phenom-ena were analyzed in practice.An error mining technique presented by vanNoord (2004) (henceforth: the van Noord Tool)can reveal gaps in grammar coverage by compar-ing the frequency of arbitrary n-grams of wordsin unsuccessfully parsed sentences with the samen-grams in unproblematic sentences, for largeunannotated corpora.2 A parser can be run overnew text, and a comparison of the in-domain and2The suffix array error mining software is available at:http://www.let.rug.nl/?vannoord/SuffixArrays.tgz63out-of-domain sentences can determine, for in-stance, that the grammar cannot parse adjective-noun hyphenation correctly (e.g.
an electrical-switch cover).
A different technique for errormining that uses discriminative treebanking is de-scribed in (Baldwin et al, 2005).
This tech-nique aims at determining issues with lexical cov-erage, grammatical (rule) coverage, ungrammati-cality within the corpus (e.g.
misspelled words),and extragrammaticality within the corpus (e.g.bulleted lists).A second approach involves querying gold-standard treebanks such as the Penn Treebank(Marcus et al, 1994) and Tiger Treebank (Brantset al, 2004) to determine the frequency of cer-tain phenomena.
For example, Tiger Search (Lez-ius, 2002) can be used to list and frequency-sort stacked prepositions (e.g.
up to the door) ortemporal noun/adverbs after prepositions (e.g.
bynow).
The search tools over these treebanks al-low for complex searches involving specificationof lexical items, parts of speech, and tree config-urations (see (M?
?rovsky?, 2008) for discussion ofquery requirements for searching tree and depen-dency banks).The third approach we discuss here differs fromquerying gold-standard treebanks in that corporaof actual parser output are queried to examinehow constructions are analyzed by the grammar.For example, Bouma and Kloosterman (2002) useXQuery (an XML query language) to mine parseresults stored as XML data.3 It is this sort of ex-amination of parser output that is the focus of thepresent paper, and specific examples of our expe-riences follow in Section 2.2.Use of such tools has proven vital to the devel-opment of large-scale grammars.
Based on ourexperiences with them, we began extensively us-ing a tool called Oceanography (Waterman, 2009)to search parser output for very large (approxi-mately 125 million sentence) parse runs stored ona distributed file system.
Oceanography queriesthe parser output and returns counts of specificconstructions or properties, as well as the exam-ple sentences they were extracted from.
In thesubsequent sections we discuss how this tool (inconjunction with existing ones like the van No-ord Tool and Tiger Search) has enhanced grammardevelopment for an English-language Lexical-3See also (Bouma and Kloosterman, 2007) for further dis-cussion of this technique.Functional Grammar used for a semantic searchapplication over Wikipedia.1.2 The Grammar and its RoleThe grammar being developed is a Lexical-Functional Grammar (LFG (Dalrymple, 2001))that is part of the ParGram parallel grammarproject (Butt et al, 1999; Butt et al, 2002).
It runson the XLE system (Crouch et al, 2009) and pro-duces c(onstituent)-structures which are trees andf(unctional)-structures which are attribute valuematrices recording grammatical functions andother syntactic features such as tense and number,as well as debugging features such as the sourceof lexical items (e.g.
from a named entity finder,the morphology, or the guesser).
There is a basegrammar which covers the constructions found instandard written English, as well as three overlaygrammars: one for parsing Wikipedia sentences,one for parsing Wikipedia headers, and one forparsing queries (sentential, phrasal, and keyword).The grammar is being used by Powerset (a Mi-crosoft company) in a semantic consumer-searchreference vertical which allows people to searchWikipedia using natural language queries as wellas traditional keyword queries.
The system uses apipeline architecture which includes: text extrac-tion, sentence breaking, named entity detection,parsing (tokenization, morphological analysis, c-structure, f-structure, ranking), semantic analysis,and indexing of selected semantic facts (see Fig-ure 1).
A similar pipeline is used on the queryside except that the resulting semantic analysis isturned into a query execution language which isused to query the index.text extraction scriptsentence breaker finite statenamed entity detection MaxEnt modelLFG grammarstokenizer finite statemorphology finite stategrammar XLE: parserranking MaxEnt modelsemantics XLE: XFRFigure 1: NL Pipeline ComponentsThe core idea behind using a deep parser in thepipeline in conjunction with the semantic rules isto localize role information as to who did what towhom (i.e.
undo long-distance dependencies and64locate heads of arguments), to abstract away fromchoice of particular lexical items (i.e.
lemmatiza-tion and detection of synonyms), and generallyprovide a more normalized representation of thenatural language string to improve both precisionand recall.1.3 OceanographyAs a byproduct of the indexing pipeline, all ofthe syntactic and semantic structures are storedfor later inspection as part of failure analysis.4The files containing these structures are distributedover several machines since ?125 million sen-tences are parsed for the analysis of Wikipedia.For any given syntactic or semantic structure,the XLE ordered rewrite system (XFR; (Crouch etal., 2009)) can be used to extract information thatis of interest to the grammar engineer, by way of?rules?
or statements in the XFR language.
As theXFR ordered rewrite system is also used for thesemantics rules that turn f-structures into seman-tic representations, the notation is familiar to thegrammar writers and is already designed for ma-nipulating the syntactic f-structures.However, the mechanics of accessing each fileon each machine and then assembling the results isprohibitively complicated without a tool that pro-vides a simple interface to the system.
Oceanogra-phy was designed to take a single specification filestating:?
which data to examine (which corpus ver-sion; full Wikipedia build or fixed 10,000document set);?
the XFR rules to be applied;?
what extracted data to count and report back.Many concrete examples of Oceanography runswill be discussed below.
The basic idea is touse the XFR rules to specify searches over lexi-cal items, features, and constructions in a way thatis similar to that of Tiger Search and other facili-ties.
The Oceanography machinery enables thesesearches over massive data and helps in compil-ing the results for the grammar engineer to inspect.We believe that similar approaches would be fea-sible to implement in other grammar developmentenvironments and, in fact, for some grammar out-puts and applications, existing tools such as Tiger4The index is self-contained and does not need to refer-ence the semantic, much less the earlier syntactic, structuresas part of the search application.Search would be sufficient.
By providing exam-ples where such searches have aided our grammardevelopment, we hope to encourage other gram-mar engineers to similarly extend their efforts touse easy access to massive data to drive their work.2 Grammar DevelopmentThe ParGram English LFG grammar has been de-veloped over many years.
However, the focus ofdevelopment was on newspaper text and technicalmanuals, although some adaptation was done fornew domains (King and Maxwell, 2007).
Whenmoving to the Wikipedia domain, many new con-structions and lexical items were encountered (see(Baldwin et al, 2005) for a similar experiencewith the BNC) and, at the same time, the require-ments on parsing efficiency increased.2.1 Lexical DevelopmentWhen first parsing a new corpus, the grammar en-counters new words that were previously unknownto the morphology.
The morphology falls back to aguesser that uses regular expressions to guess thepart of speech and other features associated withan unknown form.
For example, a novel word end-ing in s might be a plural noun.
The grammarrecords a feature LEX-SOURCE with the valueguesser for all guessed words.
Oceanography wasused to extract all guessed forms and their partsof speech.
In many cases, the guesser had cor-rectly identified the word?s part of speech.
How-ever, words that occurred frequently were added tothe morphology to avoid the possibility that theywould be incorrectly guessed as a different part ofspeech.
The fact that Oceanography was able toidentify not just the word, but its posited part ofspeech and frequency in the corpus greatly spedlexical development.Incorrect guessing of verbs was of particularconcern to the grammar writers, as misidentifica-tion of verbs was almost always accompanied bya bad parse.
In addition, subcategorization framesfor guessed verbs were guessed as either transi-tive or intransitive, which often proved to be in-correct.
As such, the guessed verbs extracted us-ing Oceanography were hand curated: true verbswere added to the morphology and their subcate-gorization frames to the lexicon.
Due to the highrate of error with guessed verbs, once the correctlyguessed verbs were added to the morphology, this65option was removed from the guesser.5Overall, ?4200 new stems were added to thealready substantial morphology, with correct in-flection.
Approximately ?1300 of these wereverbs.
The decision to eliminate verbs as possi-ble guessed parts of speech was directly motivatedby data extracted using Oceanography.Since the guesser works with regular expres-sions (e.g.
lowercase letters + s form pluralnouns), it is possible to encounter forms inthe corpus that neither the morphology nor theguesser recognize.
The grammar will fragment onthese sentences, creating well-formed f-structurechunks but no single spanning parse, and the un-recognized forms will be recorded as TOKENs(Riezler et al, 2002).
An Oceanography run ex-tracting all TOKENs resulted in the addition of sev-eral new patterns to the guesser as well as the addi-tion of some of the frequent forms to the morphol-ogy.
For example, sequences of all upper case let-ters followed by a hyphen and then by a sequenceof digits were added for forms like AK-47, F-22,and V-1.The guesser and TOKENs Oceanography runslook for general problems with the morphologyand lexicon, and can be run for every new cor-pus.
More specific jobs are run when evaluatingwhether to implement a new analysis, or whenevaluating whether a current analysis is function-ing properly.
For example, use of the van No-ord tool indicated that the grammar had problemswith certain less common multiword prepositions(e.g.
pursuant to, in contrast with).
Once thesemultiword prepositions were added, the questionthen arose as to whether more common preposi-tions should be multiwords when stacked (e.g.
upto, along with).
An Oceanography run was per-formed to extract all occurrences of stacked prepo-sitions from the corpus.
Their frequency was tal-lied in both the stacked formations and when usedas simple prepositions.
With this information, wedetermined which stacked configurations to add tothe lexicon as multiword prepositions, while main-taining preposition stacking for less common com-binations.2.2 Grammar Rule DevelopmentIn addition to using Oceanography to help developthe morphology and lexicon, it has also proven ex-5It is simple to turn the guessed verbs back on in order torun the same Oceanography experiment with a new corpus.tremely useful in grammar rule development.
Ingeneral, the issue is not in finding constructionswhich the grammar does not cover correctly: aquick investigation of sentences which fragmentcan provide these and issues are identified and re-ported by the semantics which uses the syntax out-put as its input.
Furthermore, the van Noord toolcan be used to effectively identify gaps in gram-mar rule coverage.Rather, the more pressing issues includewhether it is worthwhile adding a construction,which possible solution to pick (when it is worth-while), and whether an existing solution is ap-plying correctly and efficiently.
Being able tolook at the occurrence of a construction over largeamounts of data can help with all of these issues,especially when combined with searching overgold standard treebanks such as the Penn Tree-bank.Determining which constructions to examineusing Oceanography is often the result of failureanalysis findings on components outside the gram-mar itself, but that build on the grammar?s outputlater in the natural language processing pipeline.The point we wish to emphasize here is that thegrammar engineer?s effectiveness can greatly ben-efit from being able to take a set of problematicdata gathered from massive parser output and de-termine from it that a particular construction mer-its closer scrutiny.2.2.1 When relative/subordinate clausesAn observation that subordinate clauses contain-ing when (e.g.
Mary laughed when Ed tripped.
)were sometimes misanalyzed as relative clausesattaching to a noun (e.g.
the time when Ed tripped)prompted a more directed analysis of whetherwhen relative clauses should be allowed to at-tach to nouns that were not time-related expres-sions (e.g.
time, year, day).
An Oceanography runwas performed to extract all when relative clauses,the modified nominal head, and the sentence con-taining the construction.
A frequency-sorted listof nouns taking when relative clause modifiershelped to direct hand-examination ofwhen relativeclauses for accuracy of the analysis.
This yieldedsome correct analyses:(1) There are times [when a Bigfoot sighting orfootprint is a hoax].More importantly, however, the search revealedmany incorrect analyses of when subordinate66clauses as relative clauses:(2) He gets the last laugh [when he tows awayhis boss?
car as well as everyone else?s].By extracting all when relative clauses, and theirhead nouns, it was determined that the construc-tion was generally only correct for a small class oftime expression nominals.
Comparatively, whenrelative clause modification of other nominals wasrarely correct.
The grammar was modified to dis-prefer relative clause analyses of when clauses un-less the head noun was an expression of time.
Asa result, the overall quality of parses for all sen-tences containing when subordinate clauses wasimproved.2.2.2 Relative clauses modifying gerundsAnother example of an issue with the accuracy of agrammatical analysis concerns gerund nouns mod-ified by relative clauses without an overt relativepronoun (e.g.
the singing we liked).
It was ob-served that many strings were incorrectly analyzedas a gerund and reduced relative clause modifier:(3) She lost all of her powers, including [hersonic screams].Again, a frequency sorted list of gerunds modi-fied by reduced relative clauses helped to guidehand inspection of the instances of this construc-tion.
By extracting all of the gerunds with re-duced relative clause modifiers, it was possibleto see which gerunds were appearing in this con-struction (e.g.
including occurred alarmingly fre-quently) and how rarely the overall analysis wascorrect.
As a result of the data analysis, such rel-ative clause modifiers are now dispreferred in thegrammar and certain verbs (e.g.
include) are addi-tionally dispreferred as gerunds in general.
Notethat this type of failure analysis is not possiblewith a tool (such as the van Noord tool) that onlypoints out gaps in grammar coverage.2.2.3 Noun-noun compoundsAs part of the semantic search application,argument-relation triples are extracted from thecorpus and presented to the user as a form of sum-mary over what Wikipedia knows about a partic-ular entity.
These are referred to as Factz.
Forexample, a search on Noam Chomsky will findFactz triples as in Figure 2.
Such an applicationhighlights parse problems, since the predicate-argument relations displayed are ultimately ex-tracted from the syntactic parses themselves.One class of problem arises when forms whichare ambiguous between nominal and verbal analy-ses are erroneously analyzed as verbs and henceshow up as Factz relations.
This is particularlytroublesome when the putative verb is part of anoun-noun compound (e.g.
ice cream, hot dog)and the verb form is comparatively rare.
A listof potentially problematic noun-noun compoundswas extracted by using an independent part ofspeech tagger over the sentences that generated theFactz triples.
If the relation in the triple was taggedas a noun and was not a deverbal noun (e.g.
de-struction, writing), then the first argument of thetriple and the relation were tagged as potentiallyproblematic noun-noun compounds.
Oceanogra-phy was then used to determine the relative fre-quency of whether the word pairs were analyzed asnoun-noun compounds, verb-argument relations,or independent nouns and verbs.This distributional information, in conjunctionwith information about known noun-noun com-pounds in WordNet (Fellbaum, 1998), is beingused to extract a set of ?100,000 noun-noun com-pounds whose analysis is extremely strongly pre-ferred by the grammar.
Currently, these are con-strained via c-structure optimality marks6 but theymay eventually be allowed only as noun-nouncompounds if the list proves reliable enough.3 Tracking Grammar ProgressThe grammar is used as part of a larger applica-tion which is actively being developed and whichis regularly updated.
As such, new versions ofthe grammar are regularly released.
Each releaseincludes a detailed list of improvements and bugfixes, as well as requirements on other compo-nents of the system (e.g.
the grammar may requirea specific version of the XLE parser or of the mor-phology).
It is extremely important to be able toconfirm that the changes to the grammar are inplace and are functioning as expected when usedin the pipeline.
Some changes can be confirmedby browsing documents, finding a sentence likelyto contain the relevant lexical item or construction,and then inspecting the syntactic structures for that6See (Frank et al, 2001) and (Crouch et al, 2009) on theuse of Optimality Theory marks within XLE.
C-structure op-timality marks apply preferences to the context free backbonebefore any constraints supplied by the f-structure annotationsare applied.
This means that the noun-noun compounds willbe the only analysis possible if any tree can be constructedwith them.67Figure 2: Example Factzdocument.3.1 Confirming Grammar ChangesHowever, some changes are more complicated toconfirm either because it is hard to determine froma sentence whether the grammar change would ap-ply or because the change is more frequency re-lated.
For these types of changes, Oceanogra-phy runs can detect whether a rare change oc-curred at all, alleviating the need to search throughdocuments by hand.
For example, to determinewhether the currency symbols are being correctlytreated by the grammar, especially the ones thatare not standard ASCII (e.g.
the euro and yen sym-bols), two simple XFR rules can be written: onethat looks for the relevant c-structure leaf node andcounts up which symbols occur under this nodeand one that looks for the known list of currencysymbols in the f-structure and counts up what part-of-speech they were analyzed as.To detect whether frequency related changesto the grammar are behaving as expected, twoOceanography runs can be compared, one withthe older grammar and one with the newer one.For example, to determine whether relative clausesheaded by when were dispreferred relative tosubordinate clauses, the number of such relativeclauses and such subordinate clauses were countedin two successive runs; the relative occurrence ofthe types confirmed that the preference mecha-nism was working correctly.
In addition, a quickexamination of sentences containing each typeshowed that the change was not over-applying(e.g.
incorrectly analyzing when relative clauses assubordinate clauses).3.2 General Grammar CheckingIn addition to Oceanography runs done to checkon specific changes to the grammar, a core set ofXFR rules extracts all of the features from the f-structure and counts them.
The resulting statisticsof features and counts are computed for each ma-jor release and compared to that of the previousrelease.
This provides a list of new features whichsubsequent components must be alerted to (e.g.
afeature added to indicate what type of punctua-tion surrounded a parenthetical).
It also provides aquick check of whether some feature is no longeroccurring with the same frequency.
In some casesthis is expected; once many guessed forms wereadded to the lexicon, the feature indicating thatthe guesser had applied dropped sharply.
How-ever, unexpected steep variations from previousruns can be investigated to make sure that ruleswere not inadvertently removed from the gram-mar, and that rules added to the grammar are func-tioning correctly.4 Using Grammar Output to DevelopOther ComponentsIn addition to being used in development of thegrammar itself, examination of the grammar out-put can be useful for engineering efforts on othercomponents.
In addition to the examples citedabove concerning the development of the mor-phology used by the grammar, we discuss one sim-ple example here.
The sentence breaker used inthe pipeline is designed for high precision; it onlybreaks sentences when it is sure that there is a sen-tence break.
To make up for breaks that may havebeen missed, the grammar contains a rule that al-lows multiple sentences to be parsed as a singlestring.
The resulting f-structure has the final sen-tence?s f-structure as the value of a feature, LAST,and the remainder as the value of a feature, REST.The grammar iteratively parses multiple sentencesinto these LAST-REST structures.
Because the fea-ture LAST is only instantiated when parsing mul-tiple sentences, input strings whose parses con-tained a LAST component could be extracted todetermine whether the sentence breaker?s behaviorshould be changed.
An example of two sentenceswhich were not broken is:68(4) The current air staff includes former CNNHeadline News gal Holly Firfer in the morn-ings with co-host Orff.
Mid-days is MaraDavis, who does a theme lunch hour.The relatively short unknown wordOrff before theperiod makes it unclear whether this is an abbrevi-ation or not.
Based on the Oceanography analysis,the number of unbroken sentences which receivedanalyses was roughly halved and one bug concern-ing footnote markers was discovered and fixed.5 ConclusionLarge-scale grammars are increasingly being usedin applications.
In order to maximize their effec-tiveness in terms of coverage, accuracy, and effi-ciency for a given application, it is increasinglyimportant to examine the behavior of the grammaron the relevant corpus and in the relevant applica-tion.Having good tools makes the grammar engi-neer?s task of massive data driven grammar de-velopment significantly easier.
In this paper wehave discussed how such a tool, which can ap-ply search patterns over the syntactic (and seman-tic) representations of Wikipedia, is being used ina semantic search research vertical.
When usedin conjunction with existing tools for detectinggaps in parser coverage (e.g.
the van Noord tool),Oceanography greatly aids in the evaluation of ex-isting linguistic analyses from the parser.
In ad-dition, oceanography provides vital information todetermining whether or not to implement coveragefor a particular construction, based on efficiencyrequirements.
Thus, the grammar writer has asuite of tools available to address the questionsraised in the introduction of this paper: what gapsexist in parser coverage, how to best address thosegaps, and whether existing analyses are function-ing appropriately.
We hope that our experiencesencourage other grammar engineers to use similartechniques in their grammar development efforts.AcknowledgmentsWe would like to thank Scott Waterman for creat-ing Oceanography and adapting it to our needs.ReferencesTimothy Baldwin, John Beavers, Emily M. Bender,Dan Flickinger, Ara Kim, and Stephan Oepen.2005.
Beauty and the beast: What running abroad-coverage precision grammar over thee bnctaught us about the grammar ?
and the corpus.In Stephan Kepser and Marga Reis, editors, Lin-guistic Evidence: Empirical, Theoretical, and Com-putational Perspectives, pages 49?70.
Mouton deGruyter, Berlin.Gosse Bouma and Geert Kloosterman.
2002.
Query-ing dependency treebanks in XML.
In Proceedingsof the Third international conference on LanguageResources and Evaluation (LREC), Gran Canaria.Gosse Bouma and Geert Kloosterman.
2007.
Miningsyntactically annotated corpora using XQuery.
InProceedings of the Linguistic Annotation Workshop,Prague, June.
ACL.Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-via Hansen, Esther Ko?nig, Wolfgang Lezius, Chris-tian Rohrer, George Smith, and Hans Uszkoreit.2004.
TIGER: Linguistic interpretation of a Germancorpus.
Research on Language and Computation,2:597?620.Miriam Butt, Tracy Holloway King, Mar?
?a-EugeniaNin?o, and Fre?de?rique Segond.
1999.
A GrammarWriter?s Cookbook.
CSLI Publications.Miram Butt, Helge Dyvik, Tracy Holloway King, Hi-roshi Masuichi, and Christian Rohrer.
2002.
TheParallel Grammar Project.
In COLING2002 Work-shop on Grammar Engineering and Evaluation,pages 1?7.Robin Cooper, Dick Crouch, Jan van Eijck, ChrisFox, Josef van Genabith, Jan Jaspars, Hans Kamp,David Milward, Manfred Pinkal, Massimo Poesio,and Steve Pulman.
1996.
Using the framework.FraCas: A Framework for Computational Semantics(LRE 62-051).Dick Crouch, Mary Dalrymple, Ronald Kaplan,Tracy Holloway King, John T. Maxwell III, andPaula Newman.
2009.
XLE Documentation.
On-line.Mary Dalrymple.
2001.
Lexical Functional Grammar.Syntax and Semantics.
Academic Press.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
The MIT Press.Anette Frank, Tracy Holloway King, Jonas Kuhn, andJohn T. Maxwell III.
2001.
Optimality theory styleconstraint ranking in large-scale LFG grammars.
InPeter Sells, editor, Formal and Empirical Issues inOptimality Theoretic Syntax, pages 367?397.
CSLIPublications.Tracy Holloway King and John T. Maxwell, III.
2007.Overlay mechanisms for multi-level deep processingapplications.
In Proceedings of the Grammar En-gineering Across Frameworks (GEAF07) Workshop.CSLI Publications.69Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein,Kirsten Falkedal, Frederik Fouvry, Dominique Esti-val, Eva Dauphin, Herve?
Compagnion, Judith Baur,Lorna Balkan, and Doug Arnold.
1996.
TSNLP?
Test Suites for Natural Language Processing.
InProceedings of COLING 1996.Wolfgang Lezius.
2002.
Ein Suchwerkzeug fu?r syn-taktisch annotierte Textkorpora (in German).
Ph.D.thesis, IMS, University of Stuttgart Arbeitspapieredes Instituts fu?r Maschinelle Sprachverarbeitung(AIMS).
volume 8, number 4.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn treebank: Annotatingpredicate argument structure.
In ARPA HumanLanguage Technology Workshop.Jir???
M??rovsky?.
2008.
PDT 2.0 requirements on a querylanguage.
In Proceedings of ACL-08: HLT, pages37?45.
Association for Computational Linguistics.John Nerbonne, Dan Flickinger, and Tom Wasow.1988.
The HP Labs natural language evaluationtool.
In Proceedings of the Workshop on Evaluationof Natural Language Processing Systems.Stefan Riezler, Tracy Holloway King, Ronald Kaplan,Dick Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using alexical-functional grammar and discriminative esti-mation techniques.
In Proceedings of the ACL.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings ofACL.Scott A. Waterman.
2009.
Distributed parse mining.In Proceedings of the NAACL Workshop on Soft-ware Engineering, Testing, and Quality Assurancefor Natural Language Processing.70
