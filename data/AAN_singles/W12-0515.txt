Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 115?123,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsCombining Different Summarization Techniques for Legal TextFilippo Galgani Paul ComptonSchool of Computer Science and EngineeringThe University of New South WalesSydney, Australia{galganif,compton,achim}@cse.unsw.edu.auAchim HoffmannAbstractSummarization, like other natural languageprocessing tasks, is tackled with a rangeof different techniques - particularly ma-chine learning approaches, where humanintuition goes into attribute selection andthe choice and tuning of the learning algo-rithm.
Such techniques tend to apply dif-ferently in different contexts, so in this pa-per we describe a hybrid approach in whicha number of different summarization tech-niques are combined in a rule-based sys-tem using manual knowledge acquisition,where human intuition, supported by data,specifies not only attributes and algorithms,but the contexts where these are best used.We apply this approach to automatic sum-marization of legal case reports.
We showhow a preliminary knowledge base, com-posed of only 23 rules, already outperformscompetitive baselines.1 IntroductionAutomatic summarization tasks are often ad-dressed with statistical methods: a first type ofapproach, introduced by Kupiec et al(1995), in-volves using a set of features of different types todescribe sentences, and supervised learning algo-rithms to learn an empirical model of how thosefeatures interact to identify important sentences.This kind of approach has been very popular insummarization; however the difficulty of this taskoften requires more complex representations, anddifferent kinds of models to learn relevance intext have been proposed, such as discourse-based(Marcu, 1997) or network-based (Salton et al,1997) models and many others.
Domain knowl-edge usually is present in the choice of featuresand algorithms, but it is still an open issue howbest to capture the domain knowledge required toidentify what is relevant in the text; manual ap-proaches to build knowledge bases tend to be te-dious, while automatic approaches require largeamounts of training data and the result may stillbe inferior.In this paper we present our approach to sum-marize legal documents, using knowledge acqui-sition to combine different summarization tech-niques.
In summarization, different kinds of in-formation can be taken in account to locate impor-tant content, at the sentence level (e.g.
particularterms or patterns), at the document level (e.g.
fre-quency information, discourse information) andat the collection level (e.g.
document frequenciesor citation analysis); however, the way such at-tributes interact is likely to depend on the con-text of specific cases.
For this reason we havedeveloped a set of methods for identifying im-portant content, and we propose the creation ofa Knowledge Base (KB) that specifies which con-tent should be used in different contexts, and howthis should be combined.
We propose to use theRipple Down Rules (RDR) (Compton and Jansen,1990) methodology to build this knowledge base:RDR has already proven to be a very effectiveway of building KBs, had has been used success-fully in several NLP task (see Section 2).
Thiskind of approach differs from the dominant super-vised learning approach, in which we first anno-tate text to identify relevant fragments, and thenwe use supervised learning algorithms to learn amodel; one example in the legal domain being thework of Hachey and Grover (2006).
Our approacheliminates the need for separate manual annota-tion of text, as the rules are built by a human whojudges the relevance of text and directly createsthe set of rules as the one process, rather than an-notating the text and then separately tuning thelearning model.We apply this approach to the summarization oflegal case reports, a domain which has an increas-ing need for automatic text processing, to copewith the large body of documents that is case law.115Table 1: Examples of catchphrases list for two cases.COSTS - proper approach to admiralty and commercial litigation - goods transported under bill of lading incorporating Himalaya clause- shipper and consignee sued ship owner and stevedore for damage to cargo - stevedore successful in obtaining consent orders on motiondismissing proceedings against it based on Himalaya clause - stevedore not furnishing critical evidence or information until after motionfiled - whether stevedore should have its costs - importance of parties cooperating to identify the real issues in dispute - duty to resolveuncontentious issues at an early stage of litigation - stevedore awarded 75% of its costs of the proceedingsMIGRATION - partner visa - appellant sought to prove domestic violence by the provision of statutory declarations made under Statelegislation - ?statutory declaration?
defined by the Migration Regulations 1994 (Cth) to mean a declaration ?under?
the Statutory DeclarationsAct 1959 (Cth) in Div 1.5 - contrary intention in reg 1.21 as to the inclusion of State declarations under s 27 of the Acts Interpretation Act -statutory declaration made under State legislation is not a statutory declaration ?under?
the Commonwealth Act - appeal dismissedCountries with ?common law?
traditions, such asAustralia, the UK and the USA, rely heavily onthe concept of precedence: on how the courts haveinterpreted the law in individual cases, in a pro-cess that is known as stare decisis (Moens, 2007),so legal professionals: lawyers, judges and schol-ars, have to deal with large volumes of past courtdecisions.Automatic summarization can greatly enhanceaccess to legal repositories; however, legal cases,rather than summaries, often contain lists ofcatchphrases: phrases that present the importantlegal points of a case.
The presence of catch-phrases can aid research of case law, as they givea quick impression of what the case is about: ?thefunction of catchwords is to give a summary clas-sification of the matters dealt with in a case.
[...]Their purpose is to tell the researcher whetherthere is likely to be anything in the case relevant tothe research topic?
(Olsson, 1999).
For this rea-son, rather than constructing summaries, we aimat extracting catchphrases from the full text of acase report.
Examples of catchphrases from twocase reports are shown in Table 1.In this paper we present our approach towardsautomatic catchphrase extraction from legal casereports, using a knowledge acquisition approachaccording to which rules are manually createdto combine a range of diverse methods to locatecatchphrase candidates in the text.2 Related WorkDifferent kinds of language processing havebeen applied to the legal domain, for exam-ple, automatic summarization, retrieval (Moens,2001), machine translation (Farzindar and La-palme, 2009), and citation analysis (Zhang andKoppaka, 2007; Galgani and Hoffmann, 2010).Among these tasks, the most relevant to catch-phrase extraction is the work on automatic sum-marization, with the difference that catchphrasesusually cover many dimensions of one case, giv-ing a broader representation than summaries.
Ex-amples of automatic summarization systems de-veloped for the legal domain are the work ofHachey and Grover (Hachey and Grover, 2006)to summarize the UK House of Lords judge-ments, and PRODSUM (Yousfi-Monod et al,2010), a summarizer of case reports for the Can-LII database (Canadian Legal Information Insti-tute) (see also (Moens, 2007) for an overview).Both systems rely on supervised learning algo-rithms, using sentences tagged as important tolearn how to recognize important sentences in thetext: in this case the domain knowledge is incor-porated mainly in the choice of features.
Thiscontrasts with our approach where the human in-tuition goes also in the weights given to differentattributes in different contexts.Ripple Down RulesAs we propose to use rules manually created forspecifying how to identify relevant text, our ap-proach is based on incremental Knowledge Ac-quisition (KA).
A KA methodology which has al-ready been applied to language processing tasks isRipple Down Rules (RDR) (Compton and Jansen,1990).
In RDR, rules are created by domain ex-perts without a knowledge engineer, the knowl-edge base is built with incremental refinementsfrom scratch, while the system is in use; the do-main expert monitors the system and whenever itperforms incorrectly he or she flags the error andprovides a rule based on the case which gener-ated the error, which is added to the knowledgebase and corrects the error.
RDR is essentially anerror-driven KA approach, the incremental refine-ment of the KB is achieved by patching the errorsit makes, in the form of exception rule structure.The strength of RDR is easy maintenance: thepoint of failure is automatically identified, the ex-pert patches the knowledge only locally, consid-ering the case at hand, and new rules are placedby the system in the correct position and checkedfor consistency with all cases previously correctlyclassified, so that unwanted indirect effects of rule116interactions are avoided (Compton and Jansen,1990).
The manual creation of rules, in contrastwith machine learning, requires a smaller quantityof annotated data, as the human in the loop canidentify the important features in a single case,whereas learning techniques require multiple in-stances to identify important features.RDR have been used to tackle natural lan-guage processing tasks with the system KAFTIE(Pham and Hoffmann, 2004) (for summarizationin (Hoffmann and Pham, 2003)).
Knowledgebases built with RDR were shown to outperformsmachine learning in legal citation analysis (2010)and in open information extraction (Kim et al,2011); while Xu and Hoffmann (2010) showedhow a knowledge base automatically built fromdata can be improved using manual knowledgeacquisition from a domain expert with RDR.3 DatasetWe use as the source of our data the legal databaseAustLII1, the Australasian Legal Information In-stitute (Greenleaf et al, 1995), one of the largestsources of legal material on the net, which pro-vides free access to reports on court decisions inall major courts in Australia.We created an initial corpus of 2816 casesaccessing case reports from the Federal Courtof Australia, for the years 2007 to 2009, forwhich author-made catchphrases are given andextracted the full text and the catchphrases of ev-ery document.
Each document contains on aver-age 221 sentences and 8.3 catchphrases.
In totalwe collected 23230 catchphrases, of which 15359(92.7%) were unique, appearing only in one doc-ument in the corpus.
These catchphrases are usedto evaluate our extracts using Rouge, as describedin Section 4.To have a more complete representation ofthese cases, we also included citation informa-tion.
Citation analysis has proven to be very use-ful in automatic summarization (Mei and Zhai,2008; Qazvinian and Radev, 2008).
We down-loaded citation data from LawCite2.
It is a ser-vice provided by AustLII which, for a given case,lists cited cases and more recent cases that cite thecase.
We downloaded the full texts and the catch-phrases (where available) from AustLII, of bothcited (previous) cases and more recent cases thatcite the current one (citing cases).
Of the 2816cases, 1904 are cited at least by one other case1http://www.austlii.edu.au/2http://www.lawcite.org(on average by 4.82 other cases).
We collectedthe catchphrases of these citing cases, searchedthe full texts to extract the location where a ci-tation is explicitly made, and extracted the con-taining paragraph(s).
For each of the 1904 caseswe collected on average 21.17 citing sentences,and we extracted an average of 35.36 catchphrases(from one or more other documents).
From pre-vious cases referenced by the judge, we extractedon average 67.41 catchphrases for each case.We also extracted, using LawCite, references toany type of legislation made in the report.
We lo-cated in the full text the sentences where each sec-tion or Act is mentioned; then we accessed the fulltexts of the legislation on AustLII, and extractedthe title of the sections (for example, if section477 is mentioned in the text, we extract the cor-responding title: CORPORATIONS ACT 2001 -SECT 477 Powers of liquidator).Our dataset thus contains the initial 2816 caseswith given catchphrases, and all cases relatedto them by incoming or outgoing citations, withcatchphrases and citing sentences explicitly iden-tified, and the references to Acts and sections ofthe law.4 Evaluation methodAs it was not reasonable to involve legal expertsin this sort of exploratory study, we looked fora simple way to evaluate candidate catchphrasesautomatically by comparing them with the author-made catchphrases from our AustLII corpus (con-sidered as our ?gold standard?
), to quickly assessthe performances of various methods on a largenumber of documents.
As our system extractssentences from text as candidate catchphrases, wepropose an evaluation method which is based onRouge (Lin, 2004) scores between extracted sen-tences and given catchphrases.
This method wasused also in (Galgani et al, 2012).
Rouge in-cludes several measures to quantitatively comparesystem-generated summaries to human-generatedsummaries, counting the number of overlappingn-grams of various lengths, word pairs and wordsequences between two or more summaries.Somewhat different from the standard useof Rouge (which would involve comparing thewhole block of catchphrases to the whole block ofextracted sentences), we evaluated extracted sen-tences individually so that the utility of any onecatchphrase is minimally affected by the others,or by their particular order.
On the other handwe want to extract sentences that contain an en-tire individual catchphrase, while a sentence that117contains small pieces of different catchphrases isnot as useful.We therefore compare each extracted sentencewith each catchphrase individually, using Rouge.If the recall (on the catchphrase) is higher thana threshold, the catchphrase-sentence pair is con-sidered a match.
For example if we have a 10-word catchphrase, and a 15 words candidate sen-tence, if they have 6 words in common we con-sider this as a match using Rouge-1 with a thresh-old of 0.5, but not a match with a threshold of0.7 (requiring at least 7/10 words from the catch-phrase to appear in the sentence).
Using otherRouge scores (Rouge-SU or Rouge-W), the or-der and sequence of tokens are also consideredin defining a match.
In this way, once a match-ing criterion is defined, we can divide all the sen-tences in ?relevant?
sentences (those that matchat least one catchphrase) and ?not relevant?
sen-tences (those that do not match any catchphrase).Once the matches between single sentences andcatchphrases are defined for a single documentand a set of extracted (candidate) sentences, wecan compute precision and recall as:Recall =MatchedCatchphrasesTotalCatchphrasesPrecision =RelevantSentencesExtractedSentencesThe recall is the number of catchphrases matchedby at least one extracted sentence, divided by thetotal number of catchphrases; the precision is thenumber of sentences extracted which match atleast one catchphrase, divided by the number ofextracted sentences.
This evaluation method givesus a way to compare the performance of differ-ent extraction systems automatically, by giving asimple but reasonable measure of how many ofthe desired catchphrases are generated by the sys-tems, and how many of the sentences extracted areuseful.
This is different from the use of standardRouge overall scores, where precision and recalldo not relate to the number of catchphrases or sen-tences, but to the number of smaller units suchas n-grams, skip-bigrams or sequences, whichmakes it more difficult to interpret the results.5 Relevance IdentificationDifferent techniques can be used to extract im-portant fragments from text.
Approaches such as(Hoffmann and Pham, 2003; Galgani and Hoff-mann, 2010) used regular expressions to recog-nize patterns in the text, based on cue phrases orparticular terms/constructs.
However, when man-ually examining legal texts, we realised that torecognize important content, several aspects ofthe text need to be considered.
Looking at onesentence by itself is clearly not enough to decideits importance: we must consider also document-scale information to know what the present caseis about, and at the same time we need to lookat corpus-wide information to decide what is pe-culiar to the present case.
For this reason we de-veloped several ways of locating potential catch-phrases in legal text, based on different kinds ofattributes, which form the building blocks for ourrule system.Using the NLTK library3 (Bird et al, 2009), wecollected all the words in the corpus, and obtaineda list of stemmed terms (we used the Porter stem-mer).
Then for each term (stem) of each docu-ment, we computed the following numerical at-tributes:1.
Term frequency (Tf): the number of occur-rences of the term in this document.2.
AvgOcc: the average number of occurrencesof the term in the corpus.3.
Document frequency (Df): computed as thenumber of document in which the term ap-pear at least once divided by the total numberof documents.4.
TFIDF: computed as the rank of the term inthe document (i.e.
TFIDF(term)=10 meansthat the term has the 10 highest TFIDF valuefor this document).5.
CpOcc: how many times the term occurs inthe set of all the known catchphrases presentin the corpus.6.
The FcFound score: from (Galgani 2012),this uses the known catchphrases to computethe ratio between how many times (that is inhow many documents) the term appears bothin the catchphrases and in the text of the case,and how many times in the text 4 :FcFound(t) =NDocstext&catchp.
(t)NDocstext(t)3http://www.nltk.org/4Attributes 5 and 6 use information from the set of ex-isting catchphrases.
We consider this set as a general re-source and believe that the corpus of catchphrases comprisesmost of the relevant words and phrases, and as such can bedeemed a general resource and can be applied to new datawithout loss of performances, as it was shown in (Galgani etal., 2012).1187.
CitSen: how many times the term occurs inall the sentences (from other documents) thatcite the target case.8.
CitCp: how many times the term occurs inall the catcphrases of other documents thatcite or are cited by the target case.9.
CitLeg: how many times the term occurs inthe section titles of the legislation cited bythe target case.Three more non-numeric attributes were also usedfor each term:10.
The Part Of Speech (POS) tag of theterm (obtained using the NLTK default partof speech tagger, a classifier-based taggertrained on the PENN Treebank corpus).11.
We extracted a set of legal terms from (Ols-son, 1999), which lists a set of possible titlesand subtitles for judgements.
The existenceof a term in this set is used as an attribute(Legal).12.
If the term is a proper noun (PrpNoun), asindicated by the POS tagger.Furthermore, we also use four sentence-level at-tributes:13.
Specific words or phrases that must bepresent in the sentence, i.e.
?court?
or?whether?.14.
If the sentence contains a citation to anothercase (HasCitCase).15.
If the sentence contains a citation to an act ora section of the law (HasCitLaw).16.
A constraint on the length of the sentence(Length).When constructing our set of features, we in-cluded different kinds of information that can beused to recognize important content.
Each of thedifferent features can be used to locate potentialcatchphrases in a case.
In (Galgani et al, 2011)automatic extraction methods based on these at-tributes were compared to each other, and it wasshown that citation-based methods in general out-perform text-only methods.
However, we believethat different methods best apply to different con-texts (for different documents and sentences), andwe propose to combine them using manually cre-ated rules.6 Building a Knowledge BaseOur catchphrase extraction system is based oncreating a knowledge base of rules that specifywhich sentences should be extracted from the fulltext, as candidate catchphrases.
These rules areacquired and organized in a knowledge base ac-cording to the RDR methodology.As the rules are created looking at examples,we built a tool to facilitate the inspection of le-gal cases.
The user, for each document, can ex-plore the relevant sentences and see which onesare most similar to the (given) catchphrases ofthe case.
The interface also shows citation in-formation, the catchphrases, relevant sentences ofcited/citing cases, and which parts of the relevantlegislation are cited.
For a document the user cansee the ?best?
sentences: those that are more sim-ilar to the catchphrases, or those similar to oneparticular catchphrase.
For each sentence, fre-quency information is also shown, according tothe attributes described in Section 5.In order to make a rule, the user looks at oneexample of a relevant sentence, together with allthe frequency and citation information, the catch-phrases and other information about the docu-ment.
The user can then set different constraintsfor the attributes: attributes 1 to 12 refer to a sin-gle term, with attributes 1-9 being numeric (forthese the user can specify a maximum and/or min-imum value) while attributes 10-12 require an ex-act value (a POS tag or a True/False value).
Theuser specifies how many terms which satisfy thatconstraint, must be present in a single sentencefor it to be extracted (for example, there must beat least 3 terms with FcFound > 0.1).
It is alsopossible to insert proximity constraints, such as:the 3 terms must be no more than 5 tokens apart(they must be within a window of 5 tokens).
Wecall this set of constraints on terms, a condition.A rule is composed of a conjunction of condi-tions (for example: there must be 3 terms withFcFound > 0.1 and AvgOcc < 1 AND 2 termswith CpOcc > 20 and CitCp > 1).
There is nolimit on the number of conditions that form a rule.The conclusion of a rule is always ?the sentenceis relevant?.To acquire rules from the user, we follow theRDR approach, according to which the user looksat an instance that is currently misclassified andformulates a rule to correct the error.
In our case,the user is presented with a sentence that matchesat least one catchphrase (a relevant sentence), butis not currently selected by the knowledge base.119Looking at the sentence at hand, and at the at-tributes values for the different terms, the userspecifies a possible rule condition, and can thentest it on the entire dataset.
This gives an imme-diate idea on how useful the condition is, as theuser can see how many sentences would be se-lected by that condition and how many of thesesentences are relevant (similar enough to at leastone catchphrase, as defined in Section 4).
At thesame time the user can inspect manually othersentences matched by the condition, and refine thecondition accordingly.
When he/she is satisfiedwith one condition, they can add and test moreconditions for the rule, and see other examples, tonarrow down the number of cases matched by therule and improve the precision while at the sametime trying to include as many cases as possible.When looking at the number of sentencesmatched by adding a condition, we can also com-pute the probability that the improvement givenby the rule/condition is random.
As initially de-scribed in (Gaines and Compton, 1995), for a twoclass problem (sentence is relevant/not relevant),we can use a binomial test to calculate the proba-bility that such results could occur randomly.
Thatis, when a condition is added to an existing rule, oradded to an empty rule we compute the probabil-ity that the improvement is random.
The probabil-ity of selecting randomly n sentences and gettingx or more relevant sentences is:r =n?k=x(nk)pk(1?
p)n?k =n!px(1?
p)n?xx!(n?
x)!where p is the random probability, i.e.
the propor-tion of relevant sentences among all sentences se-lected by the current rule.
If we know how manyrelevant sentences the new condition select (x),we can calculate this probability which can guidethe user in creating a condition that minimize thevalue of r.As an example, the user may be presented withthe following sentence:As might have been expected,the bill of lading contains a?Himalaya?
clause in the widest termswhich is usual in such transactions.which we know to be relevant, being similar to agiven catchphrase:goods transported under bill of ladingincorporating Himalaya clauseLooking at the attributes the user proposes a con-dition, for example based on the term lading andHimalaya (that are peculiar of this document), apossible condition is:SENTENCE contains at least 2 termswith CpOcc > 1 and FcFound > 0.1and CitCp > 1 and TFIDF < 4 andAvgOcc < 1Testing the condition on the dataset we can seethat it matches 1392 sentences, of which 849are relevant (precision = 0.61), those sentencescover a total of 536 catchphrases (there are casesin which a number of sentences match the samecatchphrase).
The probability that a random con-dition would have this precision is also computed(10e-136).
To improve the precision we can lookat the two other terms that occurs in the catch-phrase (bill and clause) and add another condi-tion, for example:SENTENCE also contains at least2 terms with CpOcc > 20 andFcFound > 0.02 and CitCp > 1 andisLegal and TFIDF < 16The rule with two conditions now matches 429sentences of which 347 are relevant (preci-sion=0.81), covering 331 catchphrases.
The prob-ability that a random condition added to the firstone would bring this improvement is 10e-19.
Theuser can look at other matches of the rule, for ex-ample:That is to say, the Tribunal had to deter-mine whether the applicant was, by rea-son of his war-caused incapacity alone,prevented from continuing to undertakeremunerative work that he had been un-dertaking.remunerative and war-caused are matched by thefirst condition, and Tribunal and work by the sec-ond.
If the user is satisfied the rule is committedto the knowledge base.
In this way the creation,testing and integration of the rule in the system isdone at the same time.During knowledge acquisition this same inter-action is repeated: the user looks at examples,creates conditions, tests them on the dataset un-til he/she is satisfied, and then commits the ruleto the knowledge base, following the RDR ap-proach.
When creating a rule the user is guidedboth by particular examples shown by the system,and by statistics computed on the large dataset.Some rules of our KB are presented in Table 2.120Table 2: Examples of rules inserted in the Knowledge BaseSENTENCE contains at least 2 terms with Tf > 30 and CpOcc > 200 and AvgOcc < 2.5 and TFIDF < 10 within awindow of 2SENTENCE contains at least 2 terms with Tf > 5 and CpOcc > 20 and FcFound > 0.02 and CitCp > 1 andTFIDF < 15and contains at least 2 terms with Tf > 5 and CpOcc > 2 and FcFound > 0.11 and AvgOcc < 0.2 and TFIDF < 5SENTENCE contains at least 10 terms with CitCp > 10and contains at least 6 terms with CitCp > 20SENTENCE contains the term corporations with Tf > 15 and CitCp > 57 Preliminary Results and FutureDevelopmentAfter building the knowledge acquisition inter-face, we conducted a preliminary KA session toverify the feasibility of the approach, and the ap-propriateness of the rule language.
We conducteda KA session creating a total of 23 rules (whichtook on average 6.5 minutes for each to be spec-ified, tested and commited).
These 23 rules ex-tracted a total of 12082 sentences, of which 10565were actually relevant, i.e.
matched a least onecatchphrase, where we used Rouge-1 with a sim-ilarity threshold of 0.5 to define a match.
Thesesentences are distributed among 1455 differentdocuments.
The overall precision of the KB isthus is 87.44% and the total number of catch-phrases covered is 6765 (29.12% of the total).Table 3 shows the comparison of this Knowl-edge Base with four other methods: Randomis a random selection of sentences, Citations isa methods that use only citation information toselect sentences (described in (Galgani et al,2011)); in particular it selects those sentences thatare most similar to the catchphrases of cited andciting documents.
As a state-of-the-art generalpurpose summarizer, we used LexRank (Erkanand Radev, 2004), an automatic tool that firstbuilds a network in which nodes are sentencesand a weighted edge between two nodes showsthe lexical cosine similarity, and then performs arandom walk to find the most central nodes in thegraphs and takes them as the summary.
We down-loaded the Mead toolkit5 and applied LexRank toall the documents to rank the sentences.
For everymethod we extracted the 5 top ranked sentences.Finally, because our rules have matches in only1455 documents (out of a total of 2816), we useda mixed approach in which for each document, ifthere is any sentence(s) selected by the KB we se-lect those, otherwise we take the best 5 sentencesas given by the Citation method.
This method is5www.summarization.com/mead/Table 3: Performances measured using Rouge-1 withthreshold 0.5.
SpD is the average number of extractedsentences per document.Method SpD Precision Recall F-measureKB 4.29 0.874 0.291 0.437Citations 4.56 0.789 0.527 0.632KB+CIT 7.29 0.828 0.553 0.663LexRank 4.87 0.563 0.402 0.469Random 5.00 0.315 0.233 0.268Table 4: Performances measured using Rouge-1 withthreshold 0.7.
SpD is the average number of extractedsentences per document.Method SpD Precision Recall F-measureKB 4.29 0.690 0.161 0.261Citations 4.56 0.494 0.233 0.317KB+CIT 7.28 0.575 0.265 0.363LexRank 4.87 0.351 0.216 0.267Random 5.00 0.156 0.098 0.120called KB+Citations.
We can see from the Ta-ble that the Knowledge Base outperforms all othermethods in precision, followed by KB+Citations,while KB+Citations obtains higher recall.Note that we can vary the matching criterion (asdescribed in Section 4) and only consider morestrict matches, in this case only sentences moresimilar to catchphrases are considered relevant.We can see the results of setting a higher similar-ity threshold (0.7) in Table 4.
All the approachesgive lower precision and recall, but the margin ofthe knowledge base over the other methods in-creases, with a relative improvement of precisionof 40% over the citation method.While the precision level of the KB alone ishigher than any other method, the recall is lowwhen compared to other approaches.
We onlyconducted a preliminary KA session, which tookslightly more than 2 hours.
Figure 1 shows pre-cision and recall of the KB as new rules are in-121serted into the system.
We can assume that a morecomprehensive set of rules, capturing more sen-tences and addressing different types of contexts,should cover a greater number of catchphrases,while keeping the precision at a high value; how-ever, the rules constructe so far only fire for somecases, and many cases are not covered at all.Even with this limited KB, we can use the ci-tation method as fall-back to select sentences forthose cases that are not matched by the rules.
Us-ing this approach, as we can see from Tables 3 and4 (method KB+CIT), that obtain the highest recallwhile keeping the precision very close to the pre-cision of the KB alone.For future work we plan not only to expand theKB in general with more rules, in order to im-prove recall, but also to construct rules specifi-cally for those cases that are not already covered,applying those rules in a selective way, only forthese of documents (and not for those which al-ready have a sufficient number of catchphrasescandidates).
In doing this we will seek to gen-eralize our experience of applying the citation ap-proach to documents where the KB did not pro-duce catchphrases.
We also hypothesize that therecall level of the rules is low because they selectseveral sentences that are similar among them,and thus match the same catchphrases, so that forsome documents we have a set of relevant sen-tences which cover only some aspects of the case.Using a similarity-based re-ranker would allow usto discard sentences to similar to those already se-lected.In future developments we also plan to developfurther the structure of the knowledge base intoan RDR tree, writing exception rules (rule withconclusion ?not relevant?)
that can patch the ex-isting rules whenever an error is found.
The cur-rent knowledge base only consists of a list of ruleswhile the RDR methodology will let us organizethe rules so they are used in different situationsdepending on which previous rule has fired.8 ConclusionThis paper presents our hybrid approach to textsummarization, based on creating rules to com-bine different types of statistical informationabout text.
In contrast to supervised learning,where human intuition applies only to attributeand algorithm selection, here human intuition alsoapplies to the organization of features in rules, butstill guided by the available dataset.We have applied our approach to a particu-lar summarization problem: creating catchphrasesFigure 1: Precision, Recall and F-measure as the sizeof the KB increasesfor legal case reports.
Catchphrases are consid-ered to be a significant help to lawyers searchingthrough cases to identify relevant precedents andare routinely used when browsing documents.
Wecreated a large dataset of case reports, correspond-ing catchphrases and both incoming and outgoingcitations to cases and legislation.
We created aKnowledge Acquisition framework based on Rip-ple Down Rules, and defined a rich rule languagethat includes different aspects of the case underconsideration.
We developed a tool that facili-tates the inspection of the dataset and the cre-ation of rules by selecting and specifying fea-tures depending on the context of the present caseand using different information for different situ-ations.
A preliminary KA session shows the ef-fectiveness of the rule approach: with only 23rules we can obtain a significantly higher preci-sion (87.4%) than any automatic method tried.We are confident that a more extensive knowledgebase would further improve the performances andcover a larger portion of the cases, improving therecall.ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.P.
Compton and R. Jansen.
1990.
Knowledge in con-text: a strategy for expert system maintenance.
InAI ?88: Proceedings of the second Australian JointConference on Artificial Intelligence, pages 292?306, New York, NY, USA.
Springer-Verlag NewYork, Inc.G.
Erkan and D.R.
Radev.
2004.
LexRank: Graph-based lexical centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research,22(2004):457?479.122Atefeh Farzindar and Guy Lapalme.
2009.
Machinetranslation of legal information and its evaluation.Advances in Artificial Intelligence, pages 64?73.B.
R. Gaines and P. Compton.
1995.
Inductionof ripple-down rules applied to modeling largedatabases.
J. Intell.
Inf.
Syst., 5:211?228, Novem-ber.Filippo Galgani and Achim Hoffmann.
2010.
Lexa:Towards automatic legal citation classification.
InJiuyong Li, editor, AI 2010: Advances in ArtificialIntelligence, volume 6464 of Lecture Notes in Com-puter Science, pages 445 ?454.
Springer Berlin Hei-delberg.Filippo Galgani, Paul Compton, and Achim Hoff-mann.
2011.
Citation based summarization of legaltexts.
Technical Report 201202, School of Com-puter Science and Engineering, UNSW, Australia.Filippo Galgani, Paul Compton, and Achim Hoff-mann.
2012.
Towards automatic generation ofcatchphrases for legal case reports.
In AlexanderGelbukh, editor, the 13th International Conferenceon Intelligent Text Processing and ComputationalLinguistics, volume 7182 of Lecture Notes in Com-puter Science, pages 415?426, New Delhi, India.Springer Berlin / Heidelberg.G.
Greenleaf, A. Mowbray, G. King, and P. Van Dijk.1995.
Public Access to Law via Internet: The Aus-tralian Legal Information Institute.
Journal of Lawand Information Science, 6:49.Ben Hachey and Claire Grover.
2006.
Extractivesummarisation of legal texts.
Artif.
Intell.
Law,14(4):305?345.Achim Hoffmann and Son Bao Pham.
2003.
To-wards topic-based summarization for interactivedocument viewing.
In K-CAP ?03: Proceedingsof the 2nd international conference on Knowledgecapture, pages 28?35, New York, NY, USA.
ACM.Myung Hee Kim, Paul Compton, and Yang Sok Kim.2011.
Rdr-based open ie for the web document.In Proceedings of the sixth international conferenceon Knowledge capture, K-CAP ?11, pages 105?112,New York, NY, USA.
ACM.Julian Kupiec, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In SIGIR?95: Proceedings of the 18th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 68?73, NewYork, NY, USA.
ACM.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associ-ation for Computational Linguistics.Daniel Marcu.
1997.
From discourse structures to textsummaries.
In In Proceedings of the ACL Workshopon Intelligent Scalable Text Summarization, pages82?88.Q.
Mei and C.X.
Zhai.
2008.
Generating impact-based summaries for scientific literature.
Proceed-ings of ACL-08: HLT, pages 816?824.Marie-Francine Moens.
2001.
Innovative techniquesfor legal text retrieval.
Artificial Intelligence andLaw, 9(1):29?57, 03.Marie-Francine Moens.
2007.
Summarizing court de-cisions.
Inf.
Process.
Manage., 43(6):1748?1764.Justice Leslie Trevor Olsson.
1999.
Guide To Uni-form Production of Judgments.
Australian Instituteof Judicial Administration, Carlton South, Vic, 2ndedition.Son Bao Pham and Achim Hoffmann.
2004.
Incre-mental knowledge acquisition for building sophis-ticated information extraction systems with kaftie.In in 5th International Conference on Practical As-pects of Knowledge Management, pages 292?306.Springer-Verlag.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific Paper Summarization Using Citation Sum-mary Networks.
Proceedings of the 22nd Inter-national Conference on Computational Linguistics(Coling 2008), pages 689?696.Gerard Salton, Amit Singhal, Mandar Mitra, and ChrisBuckley.
1997.
Automatic text structuring andsummarization.
Inf.
Process.
Manage., 33(2):193?207.Han Xu and Achim Hoffmann.
2010.
Rdrce: Com-bining machine learning and knowledge acquisi-tion.
In Byeong-Ho Kang and Debbie Richards, ed-itors, Knowledge Management and Acquisition forSmart Systems and Services, volume 6232 of Lec-ture Notes in Computer Science, pages 165?179.Springer Berlin / Heidelberg.Mehdi Yousfi-Monod, Atefeh Farzindar, and Guy La-palme.
2010.
Supervised machine learning forsummarizing legal documents.
In Canadian Con-ference on Artificial Intelligence 2010, volume 6085of Lecture Notes in Artificial Intelligence, pages 51?62, Ottawa, Canada, may.
Springer.Paul Zhang and Lavanya Koppaka.
2007.
Semantics-based legal citation network.
In ICAIL ?07: Pro-ceedings of the 11th international conference on Ar-tificial intelligence and law, pages 123?130, NewYork, NY, USA.
ACM.123
