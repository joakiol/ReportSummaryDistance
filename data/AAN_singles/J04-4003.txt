c?
2004 Association for Computational LinguisticsFast Approximate Search in LargeDictionariesStoyan Mihov?
Klaus U. Schulz?Bulgarian Academy of Sciences University of MunichThe need to correct garbled strings arises in many areas of natural language processing.
If adictionary is available that covers all possible input tokens, a natural set of candidates for correctingan erroneous input P is the set of all words in the dictionary for which the Levenshtein distance to Pdoes not exceed a given (small) bound k. In this article we describe methods for efficiently selectingsuch candidate sets.
After introducing as a starting point a basic correction method based on theconcept of a ?universal Levenshtein automaton,?
we show how two filtering methods known fromthe field of approximate text search can be used to improve the basic procedure in a significantway.
The first method, which uses standard dictionaries plus dictionaries with reversed words,leads to very short correction times for most classes of input strings.
Our evaluation resultsdemonstrate that correction times for fixed-distance bounds depend on the expected number ofcorrection candidates, which decreases for longer input words.
Similarly the choice of an optimalfiltering method depends on the length of the input words.1.
IntroductionIn this article, we face a situation in which we receive some input in the form of stringsthat may be garbled.
A dictionary that is assumed to contain all possible correct inputstrings is at our disposal.
The dictionary is used to check whether a given input iscorrect.
If it is not, we would like to select the most plausible correction candidatesfrom the dictionary.
We are primarily interested in applications in the area of naturallanguage processing in which the background dictionary is very large and fast selectionof an appropriate set of correction candidates is important.
By a ?dictionary,?
wemean any regular (finite or infinite) set of strings.
Some possible concrete applicationscenarios are the following:?
The dictionary describes the set of words of a highly inflectional oragglutinating language (e.g., Russian, German, Turkish, Finnish,Hungarian) or a language with compound nouns (German).
Thedictionary is used by an automated or interactive spelling checker.?
The dictionary is multilingual and describes the set of all words of afamily of languages.
It is used in a system for postcorrection of results ofOCR in which scanned texts have a multilingual vocabulary.?
Linguistic Modelling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences,25A, Akad.
G. Bonchev Str., 1113 Sofia, Bulgaria.
E-mail: stoyan@lml.bas.bg?
Centrum fu?r Informations-und Sprachverarbeitung, Ludwig-Maximilians-Universita?t-Mu?nchen,Oettingenstr.
67, 80538 Munchen, Germany.
E-mail: schulz@cis.uni-muenchen.deSubmission received: 12 July 2003; Revised submission received: 28 February 2004; Accepted forpublication: 25 March 2004452Computational Linguistics Volume 30, Number 4?
The dictionary describes the set of all indexed words and phrases of anInternet search engine.
It is used to determine the plausibility that a newquery is correct and to suggest ?repaired?
queries when the answer setreturned is empty.?
The input is a query to some bibliographic search engine.
The dictionarycontains titles of articles, books, etc.The selection of an appropriate set of correction candidates for a garbled input P isoften based on two steps.
First, all entries W of the dictionary are selected for which thedistance between P and W does not exceed a given bound k. Popular distance measuresare the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabiand McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar,and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, andWillett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992,1994) Second, statistical data, such as frequency information, may be used to computea ranking of the correction candidates.
In this article, we ignore the ranking problemand concentrate on the first step.
For selection of correction candidates we use thestandard Levenshtein distance (Levenshtein 1966).
In most of the above-mentionedapplications, the number of correction candidates becomes huge for large values of k.Hence small bounds are more realistic.In light of this background, the algorithmic problem discussed in the article canbe described as follows:Given a pattern P, a dictionary D, and a small bound k, efficientlycompute the set of all entries W in D such that the Levenshtein distancebetween P and W does not exceed k.We describe a basic method and two refinements for solving this problem.
The basicmethod depends on the new concept of a universal deterministic Levenshtein au-tomaton of fixed degree k. The automaton of degree k may be used to decide, forarbitrary words U and V, whether the Levenshtein distance between U and V doesnot exceed k. The automaton is ?universal?
in the sense that it does not depend onU and V. The input of the automaton is a sequence of bitvectors computed from Uand V. Though universal Levenshtein automata have not been discussed previouslyin the literature, determining Levenshtein neighborhood using universal Levenshteinautomata is closely related to a more complex table-based method described by theauthors Schulz and Mihov (2002).
Hence the main advantage of the new notion is itsconceptual simplicity.
In order to use the automaton for solving the above problem,we assume that the dictionary is given as a determininistic finite-state automaton.
Thebasic method may then be described as a parallel backtracking traversal of the uni-versal Levenshtein automaton and the dictionary automaton.
Backtracking proceduresof this form are well-known and have been used previously: for example, by Oflazer(1996) and the authors Schulz and Mihov (2002).For the first refinement of the basic method, a filtering method used in the fieldof approximate text search is adapted to the problem of approximate search in a dic-tionary.
In this approach, an additional ?backwards?
dictionary D?R (representing theset of all reverses of the words of a given dictionary D) is used to reduce approximatesearch in D with a given bound k ?
1 to related search problems for smaller boundsk?
< k in D and D?R.
As for the basic method, universal Levenshtein automata are usedto control the search.
Ignoring very short input words and correction bound k = 1,453Mihov and Schulz Fast Approximate Search in Large Dictionariesthis approach leads to a drastic increase in speed.
Hence the ?backwards dictionarymethod?
can be considered the central contribution of this article.The second refinement, which is only interesting for bound k = 1 and short inputwords, also uses a filtering method from the field of approximate text search (Muthand Manber 1996; Mor and Fraenkel 1981).
In this approach, ?dictionaries with singledeletions?
are used to reduce approximate search in a dictionary D with bound k = 1to a conventional lookup technique for finite-state transducers.
Dictionaries with singledeletions are constructed by deleting the symbol at a fixed position n in all words ofa given dictionary.For the basic method and the two refinements, detailed evaluation results are givenfor three dictionaries that differ in terms of the number and average length of entries:a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 sym-bols), a dictionary of German with 3,871,605 entries (dominated by compound nouns,average length 18.74 symbols), and a dictionary representing a collection of 1,200,073book titles (average length 47.64 symbols).
Tests were restricted to distance boundsk = 1, 2, 3.
For the approach based on backwards dictionaries, the average correctiontime for a given input word?including the displaying of all correction suggestions?is between a few microseconds and a few milliseconds, depending on the dictionary,the length of the input word, and the bound k. Correction times over one millisecondoccur only in a few cases for bound k = 3 and short input words.
For bound k = 1,which is important for practical applications, average correction times did not exceed40 microseconds.As a matter of fact, correction times are a joint result of hardware improvementsand algorithmic solutions.
In order to judge the quality of the correction procedure inabsolute terms, we introduce an ?idealized?
correction algorithm in which any kindof blind search and superfluous backtracking is eliminated.
Based on an analysis ofthis algorithm, we believe that using purely algorithmic improvements, our correctiontimes can be improved only by a factor of 50?250, depending on the kind of dictionaryused.
This factor represents a theoretical limit in the sense that the idealized algorithmprobably cannot be realized in practice.This article is structured as follows.
In Section 2, we collect some formal prelimi-naries.
In Section 3, we briefly summarize some known techniques from approximatestring search in a text.
In Section 4, we introduce universal deterministic Levenshteinautomata of degree k and describe how the problem of deciding whether the Lev-enshtein distance between two strings P and W does not exceed k can be efficientlysolved using this automaton.
Since the method is closely related to a table-basedapproach introduced by the authors (Schulz and Mihov 2002), most of the formal de-tails have been omitted.
Sections 5, 6, and 7 describe, respectively, the basic method,the refined approach based on backwards dictionaries, and the approach based ondictionaries with single deletions.
Evaluation results are given for the three dictio-naries mentioned above.
In Section 8 we briefly comment on the difficulties thatwe encountered when trying to combine dictionary automata and similarity keys(Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand deBeuvron and Trigano 1995).
Theoretical bounds for correction times are discussed inSection 9.The problem considered in this article is well-studied.
Since the number of contri-butions is enormous, a complete review of related work cannot be given here.
Relevantreferences with an emphasis on spell-checking and OCR correction are Blair (1960),Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari,Hull, and Choudhari (1983), Srihari (1985), Takahashi et al (1990), Kukich (1992), Zobel454Computational Linguistics Volume 30, Number 4and Dart (1995), and Dengel et al (1997).
Exact or approximate search in a dictionaryis discussed, for example, in Wells et al (1990), Sinha (1990), Bunke (1993), Oflazer(1996), and Baeza-Yates and Navarro (1998).
Some relevant work from approximatesearch in texts is described in Section 3.2.
Formal PreliminariesWe assume that the reader is familiar with the basic notions of formal language theoryas described, for example, by Hopcroft and Ullman (1979) or Kozen (1997).
As usual,finite-state automata (FSA) are treated as tuples of the form A = ?
?, Q, q0, F,?
?, where?
is the input alphabet, Q is the set of states, q0 ?
Q is the initial state, F is the setof final states, and ?
?
Q ?
??
?
Q is the transition relation.
Here ?
denotes theempty string and ??
:= ??{?}.
The generalized transition relation ??
is defined as thesmallest subset of Q ?
??
?
Q with the following closure properties:?
For all q ?
Q we have (q, ?, q) ?
??.?
For all q1, q2, q3 ?
Q and W1, W2 ?
?
?, if (q1, W1, q2) ?
??
and(q2, W2, q3) ?
?, then also (q1, W1W2, q3) ?
?
?.We write L(A) for the language accepted by A.
We have L(A) = {W ?
??
| ?q ?F : (q0, W, q) ?
??}.
Given A as above, the set of active states for input W ?
??
is{q ?
Q | (q0, W, q) ?
??
}.A finite-state automaton A is deterministic if the transition relation is a function?
: Q ?
?
?
Q.
Let A = ?
?, Q, q0, F, ??
be a deterministic FSA, and let ??
: Q ?
??
?
Qdenote the generalized transition function, which is defined in the usual way.
Forq ?
Q, we write LA(q) := {U ?
??
| ??
(q, U) ?
F} for the language of all words thatlead from q to a final state.The length of a word W is denoted by |W|.
Regular languages over ?
are definedin the usual way.
With L1 ?
L2 we denote the concatenation of the languages L1 andL2.
It is well-known that for any regular language L, there exists a deterministic FSAAL such that L(A) = L and AL is minimal (with respect to number of states) amongall deterministic FSA accepting L. AL is unique up to renaming of states.A p-subsequential transducer is a tuple T = ?
?,?, Q, q0, F, ?,?,?
?, where?
?
?, Q, q0, F, ??
is a deterministic finite-state automaton;?
?
is a finite output alphabet;?
?
: Q ?
?
?
??
is a function called the transition output function;?
the final function ?
: F ?
2??
assigns to each f ?
F a set of strings over?, where |?
(f )| ?
p.The function ?
is extended to the domain Q ?
??
by the following definition of ??
:?q ?
Q (??
(q, ?)
= ?
)?q ?
Q ?U ?
??
?a ?
?
(??
(q, Ua) = ??
(q, U)?(??
(q, U), a))The input language of the transducer is L(T) := {U ?
??
| ??
(q0, U) ?
F}.
Thesubsequential transducer maps each word from the input language to a set of at most455Mihov and Schulz Fast Approximate Search in Large Dictionariesp output words.
The output function OT : L(T) ?
2?
?of the transducer is defined asfollows:?U ?
L(T) (OT(U) = ??
(q0, U) ??(??
(q0, U)))By a dictionary, we mean a regular (finite or infinite) set of strings over a givenalphabet ?.
Using the algorithm described by Daciuk et al (2000), the minimal deter-ministic FSA AD accepting a finite dictionary D can be effectively computed.By a dictionary with output sets, we mean a regular (finite or infinite) set ofinput strings over a given alphabet together with a function that maps each of theinput strings to a finite set of output strings.
Given a finite dictionary with outputsets, we can effectively compute, using the algorithm described by Mihov and Maurel(2001), the minimal subsequential transducer that maps each input string to its set ofoutput strings.3.
BackgroundIn this section, we describe some established work that is of help in understandingthe remainder of the article from a nontechnical, conceptual point of view.
After in-troducing the Levenshtein distance, we describe methods for computing the distance,for checking whether the distance between two words exceeds a given bound, and forapproximate search for a pattern in a text.
The similarities and differences describedbelow between approximate search in a text, on the one hand, and approximate searchin a dictionary, on the other hand, should help the reader understand the contents ofthe following sections from a broader perspective.3.1 Computation of Levenshtein DistanceThe most prominent metric for comparing strings is the Levenshtein distance, whichis based on the notion of a primitive edit operation.
In this article, we consider thestandard Levenshtein distance.
Here the primitive operations are the substitution ofone symbol for another symbol, the deletion of a symbol, and the insertion of asymbol.
Obviously, given two words W and V in the alphabet ?, it is always possibleto rewrite W into V using primitive edit operations.Definition 1Let P, W be words in the alphabet ?.
The (standard) Levenshtein distance betweenP and W, denoted dL(P, W), is the minimal number of primitive edit operations (sub-stitutions, deletions, insertions) that are needed to transform P into W.The Levenshtein distance between two words P and W can be computed using thefollowing simple dynamic programming scheme, described, for example, by Wagnerand Fischer (1974):dL(?, W) = |W|dL(P, ?)
= |P|dL(Pa, Wb) ={dL(P, W) if a = b1 + min(dL(P, W), dL(Pa, W), dL(P, Wb)) if a = bfor P, W ?
??
and a, b ?
?.
Given P = p1 .
.
.
pm and W = w1 .
.
.wn (m, n ?
0), a standardway to apply the scheme is as follows: Proceeding top-down and from left to right,the cells of an (m + 1)?
(n + 1) table TL(P, W) are filled, where entry (i, j) of T(P, W)is dL(p1 .
.
.
pi, w1 .
.
.wj) (0 ?
i ?
m, 0 ?
j ?
n) (Wagner and Fischer 1974).
The first two456Computational Linguistics Volume 30, Number 40 1 2 3 4 5 61 1 1 2 3 4 52 1 2 1 2 3 43 2 2 2 1 2 34 3 3 3 2 1 25 4 4 4 3 2 1h c h o l dcholdFigure 1Computation of the Levenshtein distance using dynamic programming and filling tableTL(chold, hchold).
Shaded regions represent diagonals in Ukkonen?s approach (cf.
Section 3.2).clauses above are used for initialization and yield, respectively, the first column andthe first row.
The third clause is used to compute the remaining entries.
The table forthe strings chold and hchold is shown in Figure 1.3.2 Testing Levenshtein NeighborhoodThe algorithm of Wagner and Fischer, which has time complexity O(m ?
n), has beenimproved and generalized in many aspects.
(See, for example, Stephen [1994] for asurvey).
We briefly sketch a more efficient variant that can be used for the restrictedproblem of deciding whether the Levenshtein distance between two words P and Wexceeds a fixed bound, k. Ukkonen (1985a) shows that in this case only the values of2k + 1 ?diagonals?
of TL(P, W) are essential for a test to make such a determination.Figure 1 illustrates the situation in which k = 2.
Ukkonen obtained an algorithmwith time complexity O(k ?
min(m, n)).
He used the test for determining whether theLevenshtein distance between two words exceeds a given bound to derive an algorithmfor computing the edit distance with complexity O(min(m, n) ?
dL(P, W)).3.3 Approximate Search for a Pattern in a TextA problem closely related to approximate search in a dictionary is approximate searchfor a pattern in a text (AST): Given two strings P and T (called, respectively, the patternand the text), find all occurrences T?
of substrings of T that are within a given distanceof P. Each occurrence T?
is called a hit.
In the following discussion, we consider thecase in which a fixed bound k for the Levenshtein distance between P and potentialhits is specified.3.3.1 Adapting the Dynamic Programming Scheme.
A simple adaptation of theWagner-Fischer algorithm may be used for approximate search for a pattern P =p1 ?
?
?
pm in a text T = t1 ?
?
?
tn.
As before, we compute an (m+1)?
(n+1) table TAST(P, T).Entry (i, j) of TAST(P, T) has value h if h is the minimal Levenshtein distance betweenp1 ?
?
?
pi and a substring of T with last symbol (position) tj (j).
From the definition, wesee that all cells in line 0 have to be initialized with 0.
The remaining computationproceeds as above.
For a given bound k, the output consists of all positions j such thatentry (m, j) does not exceed k. Note that the positions j in the output are the end pointsin T of approximate matches of P. Figure 2 illustrates a search employing dynamicprogramming.3.3.2 Automaton Approach.
Several more-efficient methods for approximate searchof a pattern P in a text T take as their starting point a simple nondeterministic finite-state automaton, AAST(P, k), which accepts the language of all words with Levenshtein457Mihov and Schulz Fast Approximate Search in Large Dictionaries0 0 0 0 0 0 01 1 1 1 1 1 02 2 1 2 2 2 13 3 2 2 3 3 24 4 3 3 3 4 35 5 4 4 4 4 4t h i s _ cchold010123h011123i012212l012321dFigure 2Approximate search of pattern chold in a text using dynamic programming.distance ?
k to some word in ??
?P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yatesand Navarro 1999).
The automaton for pattern chold and distance bound k = 2 is shownin Figure 3.
States are numbered in the form be.
The ?base number?
b determines theposition of the state in the pattern.
The ?exponent?
e indicates the error level, thatis, the number of edit errors that have been observed.
Horizontal transitions encode?normal?
transitions in which the text symbol matches the expected next symbol ofthe pattern.
Vertical transitions represent insertions, nonempty (respectively, empty)diagonal transitions represent substitutions (respectively, deletions).
In the exampleshown in Figure 3, final states are 50, 51, and 52.
It is obvious that when using a giventext T as input, we reach a final state of AAST(P, 2) exactly at those positions wherea substring T?
of T ends such that dL(P, T?)
?
2.
For other bounds k, we just have tovary the number of levels.
Note that a string can be accepted in AAST(P, k) at severalfinal states.
In order to determine the optimal distance between P and a substring T?of T ending at a certain position, it is necessary to determine the final state that canbe reached that has the smallest exponent.In the remainder of the article, the set of all states with base number i is calledthe ith column of AAST(P, k).Remark 1There is a direct relationship between the entries in column j of the dynamic program-ming table TAST(P, T) and the set of active states of AAST(P, k) that are reached withinput t1 ?
?
?
tj.
Entry (i, j) of TAST(P, T) has the value h ?
k iff h is the exponent of thebottom-most active state in the ith column of AAST(P, k).
For example, in Figure 3, theset of active states of AAST(chold, k) reached after reading the two symbols t and his highlighted.
The bottom-most elements 00, 11, 21, and 32 correspond to the entries0, 1, 1, and 2 shaded in the upper part of the third column of Figure 2.?52423222120251413121110100 10 20 30 40 50c h o l d???????????
??
??
??
??
??
??
??
??
??
?c h o l dc h o l d012?
?Figure 3Nondeterministic automaton AAST(chold, 2) for approximate search with pattern chold anddistance bound k = 2.
Active states after symbols t and h have been read are highlighted.458Computational Linguistics Volume 30, Number 4?52423222120251413121110100 10 20 30 40 50c h o l d???????????
??
??
??
??
??
??
??
??
?
?c h o l dc h o l d012?
?Figure 4Nondeterministic automaton A(chold, 2) for testing Levenshtein distance with bound k = 2 forpattern chold.
Triangular areas are highlighted.
Dark states are active after symbols h and chave been read.The direct use of the nondeterministic automaton AAST(P, k) for conducting ap-proximate searches is inefficient.
Furthermore, depending on the length m of the pat-tern and the error bound k, the explicit construction and storage of a deterministicversion of AAST(P, k) might be difficult or impossible.
In practice, simulation of deter-minism via bit-parallel computation of sets of active states gives rise to efficient andflexible algorithms.
See Navarro (2001) and Navarro and Raffinot (2002) for surveysof algorithms along this line.4.
Testing Levenshtein Neighborhood with Universal DeterministicLevenshtein AutomataIn our approach, approximate search of a pattern P in a dictionary D is traced backto the problem of deciding whether the Levenshtein distance between P and an entryW of D exceeds a given bound k. A well-known method for solving this problem isbased on a nondeterministic automaton A(P, k) similar to AAST(P, k).
A string W isaccepted by A(P, k) iff dL(P, W) ?
k. The automaton A(P, k) does not have the initial ?loop that is needed in AAST(P, k) to traverse the text.
The automaton for pattern choldand distance bound k = 2 is shown in Figure 4.
Columns of A(P, k) with numbers0, .
.
.
, m = |P| are defined as for AAST(P, k).
In A(P, k), we use as final states all statesq from which we can reach one of the states in column m using a (possibly empty)sequence of -transitions.
The reason for this modification?which obviously does notchange the set of accepted words?will become apparent later.We now show that for fixed small error bounds k, the explicit computation ofA(P, k), in a deterministic or nondeterministic variant, can be completely avoided.
Inour approach, pattern P and entry W = w1 ?
?
?wn are compared to produce a sequenceof n bitvectors ?1, .
.
.
, ?n.
This sequence is used as input for a fixed automaton A?
(k).The automaton A?
(k) is deterministic and ?universal?
in the sense that it does notdepend on a given pattern P. For each bound k, there is just one fixed automatonA?
(k), which is precomputed once and used for arbitrary patterns P and words W.A?
(k) accepts input ?1, .
.
.
, ?n iff dL(P, W) ?
k. The efficiency of this method relieson the fact that given A?
(k), we need only one operation for each transition of therecognition phase after the initial computation of the bitvectors ?1, .
.
.
, ?n.It is worth mentioning that the possibility of using a fixed universal automatonA?
(k) instead of a specific automaton A(P, k) for each pattern P is based on specialfeatures of the automata A(P, k) (cf.
Remark 2); a similar technique for AAST(P, k)459Mihov and Schulz Fast Approximate Search in Large Dictionariesappears to be impossible.
For defining states, input vectors and transitions of A?
(k),the following two definitions are essential:Definition 2The characteristic vector ?
(w, V) of a symbol w ?
?
in a word V = v1 ?
?
?
vn ?
??
isthe bitvector of length n where the ith bit is set to 1 iff w = vi.Definition 3Let P denote a pattern of length m. The triangular area of a state p of A(P, k) consists ofall states q of A(P, k) that can be reached from p using a (potentially empty) sequenceof u upward transitions and, in addition, h ?
u horizontal or reverse (i.e., leftward)horizontal transitions.
Let 0 ?
i ?
m. By triangular area i, we mean the triangular areaof state i0.
For j = 1, .
.
.
, k, by triangular area m+j, we mean the triangular area of thestate mj.For example, in Figure 4, triangular areas 0, .
.
.
, 7 of A(chold, 2) are shown.In Remark 1, we pointed to the relationship between the entries in column i of tableTAST(P, T) and the set of active states of AAST(P, k) that are reached with input w1 ?
?
?wi.A similar relationship holds between the entries in column i of table TL(P, T) and theset of active states of the automaton A(P, k) that are reached with input w1 ?
?
?wi.Triangular area i corresponds to the ith column of the subregion of TL(P, T) given bythe 2k+ 1 diagonals used in Ukkonen?s (1985a) approach.
The left-to-right orientationin A(P, k) corresponds to a top-down orientation in TL(P, T).
As an illustration, theactive states of A(chold, 2) after symbols h and c have been consumed are marked inFigure 4.
The exponents 2, 1, 2, and 2 of the bottom-most active states in columns 1,2, 3, and 4 are found in the shaded region of the third column of Figure 1.Remark 2It is simple to see that for any input string W = w1 .
.
.wn, the set of active states ofA(P, k) reached after reading the ith symbol wi of W is a subset of triangular area i(0 ?
i ?
min{n, m + k}).
For i > m + k, the set is empty.
Furthermore, the set of activestates that is reached after reading symbol wi depends only1.
on the previous set of active states (the set reached after readingw1 .
.
.wi?1, a subset of the triangular area i ?
1);2. on the characteristic vector ?
(wi, pl ?
?
?
pi ?
?
?
pr) where l = max{1, i ?
k}and r = min{m, i + k}.The following description of A?
(k) proceeds in three steps that introduce, in order,input vectors, states, and the transition function.
States and transition function aredescribed informally.1.
Input vectors.
Basically we want to use the vectors ?
(wi, pl ?
?
?
pi ?
?
?
pr), whichare of length ?
2k + 1, as input for A?(k).
For technical reasons, we introduce twomodifications.
First, in order to standardize the length of the characteristic vectors thatare obtained for the initial symbols w1, w2, .
.
., we define p0 = p?1 = .
.
.
= p?k+1 := $.In other words, we attach to P a new prefix with k symbols $.
Here $ is a new symbolthat does not occur in W. Second imagine that we get to triangular area i after readingthe ith letter wi (cf.
Remark 2).
As long as i ?
m ?
k ?
1, we know that we cannotreach a triangular area containing final states after reading wi.
In order to encode460Computational Linguistics Volume 30, Number 4this information in the input vectors, we enlarge the relevant subword of P for inputwi and consider one additional position i + k + 1 on the right-hand side (wheneveri + k + 1 ?
m).
This means that we use the vectors ?i := ?
(wi, pi?k ?
?
?
pi ?
?
?
pr), wherer = min{m, i+k+1}, as input for A?
(k), for 1 ?
i ?
min{n, m+k}.
Consequently, for 0 ?i ?
m?k?1, the length of ?i is 2k+2; for i = m?k (respectively, m?k+1, .
.
.
, m, .
.
.
, m+k),the length of ?i is 2k + 1 (respectively, 2k, .
.
.
, k + 1, .
.
.
, 1).Example 1Consider Figure 4 where P is chold and k = 2.
Input hchold is translated into the vectors?1 = ?
(h, $$chol) = 000100?2 = ?
(c, $chold) = 010000?3 = ?
(h, chold) = 01000?4 = ?
(o, hold) = 0100?5 = ?
(l, old) = 010?6 = ?
(d, ld) = 01The computation of the vectors ?i for input W = w1 .
.
.wn is based on a prelimi-nary step in which we compute for each ?
?
?
the vector ?(?)
:= ?
(?, $ .
.
.
$p1 .
.
.
pm)(using k copies of $).
The latter vectors are initialized in the form ?
(w) := 0k+m.
Wethen compute for i = 1, .
.
.
, n the value ?
(wi) := ?
(wi) | 0k+i?110m?i.
Here the sym-bol | denotes bitwise OR.
Once we have obtained the values ?
(wi), which are repre-sented as arrays, the vectors ?i := ?
(wi, pi?k .
.
.
pi .
.
.
pr) can be accessed in constanttime.2.
States.
Henceforth, states of automata A(P, k) will be called positions.
Recallthat a position is given by a base number and an exponent e, 0 ?
e ?
k represent-ing the error count.
By a symbolic triangular area, we mean a triangular area inwhich ?explicit?
base numbers (like 1, 2, .
.
.)
in positions are replaced by ?symbolic?base numbers of a form described below.
Two kinds of symbolic triangular areasare used.
A unique ?I-area?
represents all triangular areas of automata A(P, k) thatdo not contain final positions.
The ?integer variable?
I is used to abstract from pos-sible base numbers i, 0 ?
i ?
m ?
k ?
1.
Furthermore, k + 1 ?M-areas?
are usedto represent triangular areas of automata A(P, k) that contain final positions.
Vari-able M is meant to abstract from concrete values of m, which differ for distinct P.Symbolic base numbers are expressions of the form I, I + 1, I ?
1, I + 2, I ?
2 .
.
.
(I-areas) or M, M ?
1, M ?
2, .
.
.
(M-areas).
The elements of the symbolic areas, whichare called symbolic positions, are symbolic base numbers together with exponentsindicating an error count.
Details should become clear in Example 2.
The use of ex-pressions such as (I + 2)2 simply enables a convenient labeling of states of A?(k)(cf.
Figure 6).
Using this kind of labeling, it is easy to formulate a correspondence be-tween derivations in automata A(P, k) and in A?
(k) (cf.
properties C1 and C2 discussedbelow).Example 2The symbolic triangular areas for bound k = 1 are(I-area) {I0, (I ?
1)1, I1, (I + 1)1}(First M-area) {M0, (M ?
1)1, M1}(Second M-area) {(M ?
1)0, (M ?
2)1, (M ?
1)1, M1}461Mihov and Schulz Fast Approximate Search in Large DictionariesM2M0M1(M-1)0(M-1)1(M-1)2(M-2)2(M-2)1(M-2)0(M-3)1(M-3)2(M-4)2I2I0I1(I-1)1(I-1)2(I-2)2(I+1)1(I+1)2 (I+2)2Figure 5Symbolic triangular areas and symbolic final positions for bound k = 2 (cf.
Example 2).Symbolic final positions for k = 1 are M1, M0, and (M ?
1)0.
The symbolic triangularareas for k = 2 are indicated in Figure 5, in which the ellipses around symbolic finalpositions are rendered in boldface.States of A?
(k) are merely subsets of symbolic triangular areas.
Subsets containingsymbolic final positions are final states of A?
(k), and {I0} is the start state.
A specialtechnique is used to reduce the number of states.
Returning to automata of the formA(P, k), it is simple to see that triangular areas often contain positions p = ge andq = hf where p ?subsumes?
q in the following sense: If, for some fixed input rest U, itis possible to reach a final position of A(P, k) starting from q and consuming U, thenwe may also reach a final position starting from p using U.
A corresponding notion ofsubsumption can be defined for symbolic positions.
States of A?
(k) are then definedas subsets of symbolic triangular areas that are free of subsumption in the sense that asymbolic position of a state is never subsumed by another position of the same state.Example 3The states of automaton A?
(1) are shown in Figure 6.
As a result of the above reductiontechnique, the only state containing the symbolic position I0 is {I0}, the start state.
Eachof the symbolic positions (I ?
1)1, I1, (I + 1)1 is subsumed by I0.3.
Transition function.
It remains to define the transition function ??
of A?(k).
Wedescribe only the basic idea.
Imagine an automaton A(P, k), where the pattern P haslength m. Let W = w1 .
.
.wn denote an input word.
Let SPi denote the set of activepositions of A(P, k) that are reached after reading the ith symbol wi (1 ?
i ?
n).
Forsimplicity, we assume that in each set, all subsumed positions are erased.
In A?
(k) wehave a parallel acceptance procedure in which we reach, say, state S?i after reading?i := ?
(wi, pi?k ?
?
?
pi ?
?
?
pr), where r = min{m, i + k + 1}, as above, for 1 ?
i ?
n.Transitions are defined in such a way that C1 and C2 hold:C1.
For all parallel sets SPi and S?i of the two sequencesSP0 SP1 .
.
.
SPi .
.
.
SPnS?0 S?1 .
.
.
S?i .
.
.
S?nthe set SPi is obtained from S?i by instantiating the letter I by i wheneverS?i uses variable I and instantiating M by m in the other cases.C2.
Whenever SPi contains a final position, then S?i is final.Given properties C1 and C2, it follows immediately that A(P, k) accepts w1 .
.
.wn iffA?
(k) accepts ?1, .
.
.
, ?n.462Computational Linguistics Volume 30, Number 4I1,I+11I-11,I+11I0I1I-11,I1M-21,M-11,M1M-21,M1M1M-11,M1M0_1I-11I+11I-11,I1,I+11M-10_1__1111__11__00(_)1_(_)(_)11_(_)_1_(_)_ _1__011_1__1_ _111110111011__11111_1001011__01_110(_)100(_)010(_)101__01110(_)(_)0101_(_)1_0(_)_1_10_001_0_1__01__ _10_1_101001_1_0_0_1_Figure 6The universal deterministic Levenshtein automaton A?(1).
See Example 4 for notation.Example 4The universal deterministic automaton A?
(1) is shown in Figure 6.
(Some redundanttransitions departing from nonfinal states S = {I0} and using vectors of length ?
3have been omitted.)
The symbol stands for either 1 or 0.
Moreover, ?
( ) is shorthandfor ?
or ?.
In order to illustrate the use of A?
(1), consider the pattern P of the formchold.
Input child is translated into the sequence?1 = ?
(c, $cho) = 0100?2 = ?
(h, chol) = 0100?3 = ?
(i, hold) = 0000?4 = ?
(l, old) = 010?5 = ?
(d, ld) = 01Starting from state {I0}, we successively reach {I0}, {I0}, {(I?1)1, I1}, {I1}, {M1}.
Hencechild is accepted.
In a similar way the input word cold is translated into the sequence0100?0010?0010?001.
Starting from {I0}, we successively reach {I0}, {(I?1)1, I1, (I+1)1},{(I + 1)1}, {M1}.
Hence cold is also accepted.
Third, input hchold is translated intothe sequence 0010?1000?1000?100?10?1.
We reach successively {(I ?
1)1, I1, (I + 1)1},{(I ?
1)1}, {(I ?
1)1}, {(I ?
1)1}, {(I ?
1)1}, {M1}.
Hence hchold is accepted as well.For larger values of k, the number of states of A?
(k) grows rapidly.
A?
(2) has 50nonfinal states and 40 final states.
The automaton A?
(3) has 563 states.
When we tried463Mihov and Schulz Fast Approximate Search in Large Dictionariesto minimize the automata A?
(1), A?
(2), and A?
(3), we found that these three automataare already minimal.
However, we do not have a general proof that our constructionalways leads to minimal automata.5.
Approximate Search in Dictionaries Using Universal Levenshtein AutomataWe now describe how to use the universal deterministic Levenshtein automaton A?
(k)for approximate search for a pattern in a dictionary.5.1 Basic Correction AlgorithmLet D denote the background dictionary, and let P = p1 .
.
.
pm denote a given pattern.Recall that we want to compute for some fixed bound k the set of all entries W ?
Dsuch that dL(P, W) ?
k. We assume that D is implemented in the form of a determin-istic finite-state automaton AD = ?
?, QD, qD0 , FD, ?D?, the dictionary automaton.
HenceL(AD) represents the set of all correct words.Let A?
(k) = ?
?, Q?, q?0 , F?, ???
denote the universal deterministic Levenshtein au-tomaton for bound k. We assume that we can access, for each symbol ?
?
?
andeach index 1 ?
i ?
m + k, the characteristic vector ?
(?, pi?k ?
?
?
pi ?
?
?
pr), where r =min{m, i+ k+ 1}, in constant time (cf.
Section 4).
We traverse the two automata A?
(k)and AD in parallel, using a standard backtracking procedure.
At each step, a symbol?
read in AD representing the ith symbol of the current dictionary path is translatedinto the bitvector ?
(?, pi?k ?
?
?
pi ?
?
?
pr), r = min{m, i+ k+1}, which is used as input forA?
(k).push (<0,?, qD0 , q?0>);while not empty(stack) do beginpop (<i, W, qD, q?>);for ?
in ?
do begin?
:= ?
(?, pi?k ?
?
?
pi ?
?
?
pr);qD1 := ?D(qD,?
);q?1 := ??
(q?, ?
);if (qD1 <> NIL) and (q?1 <> NIL) then beginW1 := concat(W,?
);push(<i + 1, W1, qD1 , q?1>);if (qD1 ?
FD) and (q?1 ?
F?)
then output(W1);end;end;end;Starting with the pair of initial states ?qD0 , q?0 ?, position i = 0, and the empty word?, each step of the traversal adds a new symbol ?
?
?
to the actual word W andleads from a pair of states ?qD, q??
?
QD ?
Q?
to ??D(qD,?
), ??
(q?, ?)?.
We proceed aslong as both components are distinct from the empty failure state,1 NIL.
Whenevera final state is reached in both automata, the actual word W is added to the output.It is trivial to show that the list of all output words represents exactly the set of alldictionary entries W such that dL(W, P) ?
k.The computational cost of the above algorithm is bounded by the size of thedictionary automaton AD and depends on the bound k used.
If k reaches the length of1 A failure state is a state q whose language LA(q) is empty.464Computational Linguistics Volume 30, Number 4the longest word in the dictionary, then in general (e.g., for the empty input word), thealgorithm will result in a complete traversal of AD.
In practice, small bounds are used,and only a small portion of AD will be visited.
For bound 0, the algorithm validatesin time O(|P|) if the input pattern P is in the dictionary.5.2 Evaluation Results for Basic Correction AlgorithmExperimental results were obtained using a Bulgarian lexicon (BL) with 965, 339 wordentries (average length 10.23 symbols), a German dictionary (GL) with 3, 871, 605entries (dominated by compound nouns, average length 18.74 symbols), and a ?lex-icon?
(TL) containing 1, 200, 073 bibliographic titles from the Bavarian NationalLibrary (average length 47.64 symbols).
The German dictionary and the titledictionary are nonpublic.
They were provided to us by Franz Guenthner andthe Bavarian National Library, respectively, for the tests we conducted.
Thefollowing table summarizes the dictionary automaton statistics for the threedictionaries:BL GL TLNumber of words 956,339 3,871,605 1,200,073Automaton states 39,339 4,068,189 29,103,779Automaton transitions 102,585 6,954,377 30,252,173Size (bytes) 1,191,548 90,206,665 475,615,320The basic correction algorithm was implemented in C and tested on a 1.6 GHzPentium IV machine under Linux.5.2.1 A Baseline.
Before we present our evaluation results, we give a simplified base-line.
Let a garbled word W be given.
In order to find all words from the dictionarywithin Levenshtein distance k, we can use two simple methods:1.
For each dictionary word V, check whether dL(V, W) ?
k.2.
For each string V such that dL(V, W) ?
k, check whether V is in thedictionary.We consider input words W of length 10.
Visiting a state in an automaton takesabout 0.1 ?s.
Using Method 1, the time needed to check whether dL(V, W) ?
k for adictionary word V using the universal Levenshtein automaton can be estimated as 1?s (a crude approximation).
When using Method 2, we need about 1 ?s for the dic-tionary lookup of a word with 10 symbols.
Assume that the alphabet has 30 symbols.Given the input W, we have 639 strings within Levenshtein distance 1, about 400,000strings within distance 2, and about 260,000,000 strings within distance 3.
Assum-ing that the dictionary has 1,000,000 words, we get the following table of correctiontimes:Distance 1 Distance 2 Distance 3Method 1 1,000 ms 1,000 ms 1,000 msMethod 2 0.639 ms 400 ms 260,000 ms5.2.2 Correction with BL.
To test the basic correction algorithm with the Bulgarianlexicon, we used a Bulgarian word list containing randomly introduced errors.
In eachword, we introduced between zero and four randomly selected symbol substitutions,465Mihov and Schulz Fast Approximate Search in Large Dictionariesinsertions, or deletions.
The number of test words created for each length is shown inthe following table:Length 3 4 5 6 7 8words 3,563 11,066 27,196 53,763 90,202 128,620Length 9 10 11 12 13 14words 155,888 163,318 148,879 117,783 81,481 50,291Length 15 16 17 18 19 20words 28,481 15,079 8,048 4,350 2,526 1,422Table 1 lists the results of the basic correction algorithm using BL and standardLevenshtein distance with bounds k = 1, 2, 3.
Column 1 shows the length of the inputwords.
Column 2 (CT1) describes the average time needed for the parallel traversal ofthe dictionary automaton and the universal Levenshtein automaton using Levenshteindistance 1.
The time needed to output the correction candidates is always included;hence the column represents the total correction time.
Column 3 (NC1) shows theaverage number of correction candidates (dictionary words within the given distancebound) per input word.
(For k = 1, there are cases in which this number is below 1.This shows that for some of the test words, no candidates were returned: These wordswere too seriously corrupted for correction suggestions to be found within the givendistance bound.)
Similarly Columns 4 (CT2) and 6 (CT3) yield, respectively, the totalcorrection times per word (averages) for distance bounds 2 and 3, and Columns 5(NC2) and 7 (NC3) yield, respectively, the average number of correction candidatesper word for distance bounds 2 and 3.
Again, the time needed to output all correctionsis included.5.2.3 Correction with GL.
To test the correction times when using the German lexicon,we again created a word list with randomly introduced errors.
The number of testwords of each particular length is shown in the following table:Length 1?14 15?24 25?34 35?44 45?54 55?64words 100,000 100,000 100,000 9,776 995 514The average correction times and number of correction candidates for GL are sum-marized in Table 2, which has the same arrangement of columns (with correspondinginterpretations) as Table 1.5.2.4 Correction with TL.
To test the correction times when using the title ?lexicon,?we again created a word list with randomly introduced errors.
The number of testwords of each length is presented in the following table:Length 1?14 15?24 25?34 35?44 45?54 55?64words 91,767 244,449 215,094 163,425 121,665 80,765Table 3 lists the results for correction with TL and standard Levenshtein distancewith bounds k = 1, 2, 3.
The arrangement of columns is the same as for Table 1, withcorresponding interpretations.5.2.5 Summary.
For each of the three dictionaries, evaluation times strongly depend onthe tolerated number of edit operations.
When fixing a distance bound, the length ofthe input word does not have a significant influence.
In many cases, correction worksfaster for long input words, because the number of correction candidates decreases.The large number of entries in GL leads to increased correction times.466Computational Linguistics Volume 30, Number 4Table 1Evaluation results for the basic correction algorithm, Bulgarian dictionary, standardLevenshtein distance, and distance bounds k = 1, 2, 3.
Times in milliseconds.Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)3 0.107 12.03 0.974 285.4 4.589 2983.24 0.098 8.326 1.048 192.1 5.087 2426.65 0.085 5.187 1.086 105.0 5.424 1466.56 0.079 4.087 0.964 63.29 5.454 822.777 0.079 3.408 0.853 40.95 5.426 466.868 0.081 3.099 0.809 30.35 5.101 294.849 0.083 2.707 0.824 22.36 4.631 187.1210 0.088 2.330 0.794 16.83 4.410 121.7311 0.088 1.981 0.821 12.74 4.311 81.09012 0.088 1.633 0.831 9.252 4.277 51.59113 0.089 1.337 0.824 6.593 4.262 31.40514 0.089 1.129 0.844 4.824 4.251 19.18715 0.089 0.970 0.816 3.748 4.205 12.33716 0.087 0.848 0.829 3.094 4.191 9.175217 0.086 0.880 0.805 2.970 4.138 8.125018 0.087 0.809 0.786 2.717 4.117 7.170119 0.087 0.810 0.792 2.646 4.078 6.654420 0.091 0.765 0.795 2.364 4.107 5.7686Table 2Evaluation results for the basic correction algorithm, German dictionary, standard Levenshteindistance, and distance bounds k = 1, 2, 3.
Times in milliseconds.Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)1?14 0.225 0.201 4.140 0.686 23.59 2.34515?24 0.170 0.605 3.210 1.407 19.66 3.82425?34 0.249 0.492 4.334 0.938 24.58 1.55835?44 0.264 0.449 4.316 0.781 24.06 1.18745?54 0.241 0.518 3.577 0.969 20.18 1.56355?64 0.233 0.444 3.463 0.644 19.03 0.737Table 3Evaluation results for the basic correction algorithm, title ?lexicon,?
standard Levenshteindistance, and distance bounds k = 1, 2, 3.
Times in milliseconds.Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)1?14 0.294 0.537 3.885 2.731 19.31 24.6715?24 0.308 0.451 4.024 0.872 19.50 1.70325?34 0.321 0.416 4.160 0.644 19.98 0.88435?44 0.330 0.412 4.225 0.628 20.20 0.84445?54 0.338 0.414 4.300 0.636 20.44 0.85755?64 0.344 0.347 4.340 0.433 20.61 0.449467Mihov and Schulz Fast Approximate Search in Large Dictionaries6.
Using Backwards Dictionaries for FilteringIn the related area of pattern matching in strings, various filtering methods have beenintroduced that help to find portions of a given text in which an approximate match ofa given pattern P is not possible.
(See Navarro [2001] and Navarro and Raffinot [2002]for surveys).
In this section, we show how one general method of this form (Wuand Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and Baeza-Yates 1999) can be adapted to approximate search in a dictionary, improving the basiccorrection algorithm.For approximate text search, the crucial observation is the following: If the Leven-shtein distance between a pattern P and a portion of text T?
does not exceed a givenbound k, and if we cut P into k + 1 disjoint pieces P1, .
.
.
, Pk+1, then T?
must containat least one piece.
Hence the search in text T can be started with an exact multipat-tern search for {P1, .
.
.
, Pk+1}, which is much faster than approximate search for P.When finding one of the pieces Pi in the text, the full pattern P is searched for (return-ing now to approximate search) within a small neighborhood around the occurrence.Generalizations of this idea rely on the following lemma (Myers 1994; Baeza-Yates andNavarro 1999; Navarro and Raffinot 2002):Lemma 1Let T?
match P with ?
k errors.
Let P be represented as the concatenation of j wordsP1, .
.
.
, Pj.
Let a1, .
.
.
, aj denote arbitrary integers, and define A =?ji=1 ai.
Then, forsome i ?
{1, .
.
.
, j}, Pi matches a substring of T?
with ?
aik/A errors.2In our experiments, which were limited to distance bounds k = 1, 2, 3, we used thefollowing three instances of the general idea.
Let P denote an input pattern, and let Wdenote an entry of the dictionary D. Assume we cut P into two pieces, representing itin the form P = P1P2:1.
If dL(P, W) ?
3, then W can be represented in the form W = W1W2,where we have the following mutually exclusive cases:(a) dL(P1, W1) = 0 and dL(P2, W2) ?
3(b) 1 ?
dL(P1, W1) ?
3 and dL(P2, W2) = 0(c) dL(P1, W1) = 1 and 1 ?
dL(P2, W2) ?
2(d) dL(P1, W1) = 2 and dL(P2, W2) = 12.
If dL(P, W) ?
2, then W can be represented in the form W = W1W2,where we have the following mutually exclusive cases:(a) dL(P1, W1) = 0 and dL(P2, W2) ?
2(b) dL(P2, W2) = 0 and 1 ?
dL(P1, W1) ?
2(c) dL(P1, W1) = 1 = dL(P2, W2)3.
If dL(P, W) ?
1, then W can be represented in the form W = W1W2,where we have the following mutually exclusive cases:(a) dL(P1, W1) = 0 and dL(P2, W2) ?
1(b) dL(P1, W1) = 1 and dL(P2, W2) = 02 As usual, r denotes the largest integer ?
r.468Computational Linguistics Volume 30, Number 4In order to make use of these observations, we compute, given dictionary D, the back-wards dictionary D?R := {W?R | W ?
D}.3 Dictionary D and backwards dictionaryD?R are compiled into deterministic finite-state automata AD and AD?R , respectively.
Ifthe dictionary is infinite and directly given as a finite-state automaton AD, the automa-ton AD?R may be computed using standard techniques from formal language theory.Further steps depend on the bound k. We will describe only approximate search withbound k = 3; the methods for bounds k = 1, 2 are similar.Let P denote the given pattern.
P is cut into two pieces P1, P2 of approximately thesame length.
We compute P?R2 and P?R1 .
We then start four subsearches, correspondingto Cases (1a)?
(1d) specified above.For subsearch (1a), we first traverse AD using input P1.
Let q denote the state that isreached.
Starting from q and the initial state {I0} of A?
(3), we continue with a paralleltraversal of AD and A?(3).
Transition symbols in AD are translated into input bitvectorsfor A?
(3) by matching them against appropriate subwords of $$$P2, as described inSection 5.
The sequence of all transition labels of the actual paths in AD is stored asusual.
Whenever we reach a pair of final states, the current sequence?which includesthe prefix P1?is passed to the output.
Clearly, each output sequence has the formP1P?2, where dL(P2, P?2) ?
3.
Conversely, any dictionary word of this form is foundusing subsearch (1a).For subsearch (1b), we first traverse AD?R using P?R2 .
Let q denote the state thatis reached.
Starting from q and the initial state {I0} of A?
(3), we continue with aparallel traversal of AD?R and A?(3).
Transition symbols in AD?R are translated intoinput bitvectors for A?
(3) by matching them against appropriate subwords of $$$P?R1 .Whenever we reach a pair of final states, the inversed sequence is passed to the output.Clearly, each output sequence has the form P?1P2, where dL(P1, P?1) ?
3.
Conversely, anydictionary word of this form is found using a search of this form.
For a given output,a closer look at the final state S that is reached in A?
(3) may be used to exclude casesin which P1 = P?1.
(Simple details are omitted).For subsearch (1c), we start with a parallel traversal of AD and A?(1).
Transitionsymbols in AD are translated into input bitvectors for A?
(1) by matching them againstappropriate subwords of $P1.
For each pair of states (q, S) that are reached, where Srepresents a final state of A?
(1), we start a parallel traversal of AD and A?
(2), departingfrom q and the initial state {I0} of A?(2).
Transition symbols in AD are translatedinto input bitvectors for A?
(2) by matching them against appropriate subwords of$$P2.
Whenever we reach a pair of final states, the current sequence is passed to theoutput.
Clearly, each output sequence has the form P?1P?2, where dL(P1, P?1) ?
1 anddL(P2, P?2) ?
2.
Conversely, any dictionary word of this form is found using a searchof this form.
A closer look at the final states that are respectively reached in A?
(1)and A?
(2) may be used to exclude cases in which P1 = P?1 or P2 = P?2.
(Again, simpledetails are omitted).For subsearch (1d), we start with a parallel traversal of AD?R and A?(1).
Tran-sition symbols in AD?R are translated into input bitvectors for A?
(1) by matchingthem against appropriate subwords of $P?R2 .
For each pair of states (q, S) that arereached, where S represents a final state of A?
(1), we start a parallel traversal of AD?Rand A?
(2), departing from q and the initial state {I0} of A?(2).
Transition symbolsin AD?R are translated into input bitvectors for A?
(2) by matching them against ap-propriate subwords of $$P?R1 .
Whenever we reach a pair of final states, the inversedsequence is passed to the output.
Clearly, each output sequence has the form P?1P?2,3 W?R denotes the reverse of W.469Mihov and Schulz Fast Approximate Search in Large Dictionarieswhere dL(P1, P?1) ?
2 and dL(P2, P?2) ?
1.
Conversely, any word in the dictionary ofthis form is found using a search of this form.
A closer look at the final states thatare reached in A?
(1) and A?
(2) may be used to exclude cases where P2 = P?2 ordL(P1, P?1) ?
1, respectively.
(Again, simple details are omitted).It should be noted that the output sets obtained from the four subsearches (1a)?
(1d) are not necessarily disjoint, because a dictionary entry W may have more thanone partition W = W1W2 of the form described in cases (1a)?
(1d).6.1 Evaluation ResultsThe following table summarizes the statistics of the automata for the three backwardsdictionaries:BL GL TLNumber of words 956,339 3,871,605 1,200,073Automaton states 54,125 4,006,357 29,121,084Automaton transitions 183,956 7,351,973 30,287,053Size (bytes) 2,073,739 92,922,493 475,831,001Note that the size of the backwards-dictionary automata is approximately the sameas the size of the dictionary automata.Tables 4, 5, and 6 present the evaluation results for the backwards dictionaryfiltering method using dictionaries BL, GL, and TL, respectively.
We have constructedadditional automata for the backwards dictionaries.For the tests, we used the same lists of input words as in Section 5.2 in orderto allow a direct comparison to the basic correction method.
Dashes indicate that thecorrection times were too small to be measured with sufficient confidence in their levelof precision.
In columns 3, 5, and 7, we quantify the speedup factor, that is, the ratioof the time taken by the basic algorithm to that taken by the backwards-dictionaryfiltering method.6.2 Backwards-Dictionary Method for Levenshtein Distance with TranspositionsUniversal Levenshtein automata can also be constructed for the modified Levenshteindistance, in which character transpositions count as a primitive edit operation, alongwith insertions, deletions, and substitutions.
This kind of distance is preferable whencorrecting typing errors.
A generalization of the techniques presented by the authors(Schulz and Mihov 2002) for modified Levenshtein distances?using either transpo-sitions or merges and splits as additional edit operations?has been described inSchulz and Mihov (2001).
It is assumed that all edit operations are applied in par-allel, which implies, for example, that insertions between transposed letters are notpossible.If we want to apply the filtering method using backwards dictionaries for themodified Levenshtein distance d?L(P, W) with transpositions, we are faced with thefollowing problem: Assume that the pattern P = a1a2 .
.
.
amam+1 .
.
.
an is split into P1 =a1a2 .
.
.
am and P2 = am+1am+2 .
.
.
an.
When we apply the above procedure, the case inwhich am and am+1 are transposed is not covered.
In order to overcome this problem,we can draw on the following observation:If d?L(P, W) ?
3, then W can be represented in the form W = W1W2, where thereare seven alternatives, inlcuding the following four:1. d?L(P1, W1) = 0 and d?L(P2, W2) ?
32.
1 ?
d?L(P1, W1) ?
3 and d?L(P2, W2) = 0470Computational Linguistics Volume 30, Number 4Table 4Evaluation results using the backwards-dictionary filtering method, Bulgarian dictionary, anddistance bounds k = 1, 2, 3.
Times in milliseconds and speedup factors (ratio of times) withrespect to basic algorithm.Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 33 0.031 3.45 0.876 1.11 6.466 0.714 0.027 3.63 0.477 2.20 4.398 1.165 0.018 4.72 0.450 2.41 2.629 2.066 0.016 4.94 0.269 3.58 2.058 2.657 0.011 7.18 0.251 3.40 1.327 4.098 0.012 6.75 0.196 4.13 1.239 4.129 0.009 9.22 0.177 4.66 0.828 5.5910 0.010 8.80 0.159 4.99 0.827 5.3311 0.008 11.0 0.147 5.59 0.603 7.1512 0.008 11.0 0.142 5.85 0.658 6.5013 0.006 14.8 0.128 6.44 0.457 9.3314 0.006 14.8 0.123 6.86 0.458 9.2815 0.005 17.8 0.112 7.29 0.321 13.116 0.005 17.4 0.111 7.47 0.320 13.117 0.005 17.2 0.108 7.45 0.283 14.618 0.005 17.4 0.108 7.28 0.280 14.719 0.004 21.8 0.103 7.69 0.269 15.220 ?
?
0.105 7.57 0.274 15.0Table 5Evaluation results using the backwards-dictionary filtering method, German dictionary, anddistance bounds k = 1, 2, 3.
Times in milliseconds and speedup factors (ratio of times) withrespect to basic algorithm.Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 31?14 0.007 32.1 0.220 18.8 0.665 35.515?24 0.010 17.0 0.175 18.3 0.601 32.725?34 0.009 27.7 0.221 19.6 0.657 37.435?44 0.007 37.7 0.220 19.6 0.590 40.845?54 ?
?
0.201 17.8 0.452 44.655?64 ?
?
0.195 17.8 0.390 48.83. d?L(P1, W1) = 1 and 1 ?
d?L(P2, W2) ?
24. d?L(P1, W1) = 2 and d?L(P2, W2) = 1In the remaining three alternatives, W1 = W?1am+1 ends with the symbol am+1, andW2 = amW?2 starts with am.
For P?1 := a1a2 .
.
.
am?1 and P?2 := am+2am+3 .
.
.
an, we havethe following three alternatives:5. d?L(P?1, W?1) = 0 and dL(P?2, W?2) ?
26. d?L(P?1, W?1) = 1 and d?L(P?2, W?2) ?
17. d?L(P?1, W?1) = 2 and d?L(P?2, W?2) = 0The cases for distance bounds k = 1, 2 are solved using similar extensions of theoriginal subcase analysis.
In each case, it is straightforward to realize a search proce-dure with subsearches corresponding to the new subcase analysis, using an ordinarydictionary and a backwards dictionary, generalizing the above ideas.471Mihov and Schulz Fast Approximate Search in Large DictionariesTable 6Evaluation results using the backwards-dictionary filtering method, title ?lexicon,?
anddistance bounds k = 1, 2, 3.
Times in milliseconds and speedup factors (ratio of times) withrespect to basic algorithm.Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 31?14 0.032 9.19 0.391 9.94 1.543 12.515?24 0.019 16.2 0.247 16.3 0.636 30.725?34 0.028 11.5 0.260 16.0 0.660 30.335?44 0.029 11.4 0.295 14.3 0.704 28.745?54 0.037 9.14 0.332 13.0 0.759 26.955?64 0.038 9.05 0.343 12.7 0.814 25.3Table 7Evaluation results using the backwards-dictionary filtering method for the modifiedLevenshtein distance d?L with transpositions, for German dictionary and title ?lexicon,?distance bound k = 3.
Times in milliseconds and speedup factors (ratio of times) with respectto basic algorithm.Length (CT3 GL) Speedup GL (CT3 TL) Speedup TL1?14 1.154 23.0 2.822 7.715?24 1.021 21.3 1.235 17.525?34 1.148 23.7 1.261 17.735?44 1.096 24.3 1.283 17.645?54 0.874 25.5 1.326 17.355?64 0.817 25.7 1.332 17.4We have tested the new search procedure for the modified Levenshtein distanced?L.
In Table 7 we present the experimental results with the German dictionary and thetitle ?lexicon?
for distance bound k = 3.6.2.1 Summary.
The filtering method using backwards dictionaries drastically im-proves correction times.
The increase in speed depends both on the length of theinput word and on the error bound.
The method works particularly well for longinput words.
For GL, a drastic improvement can be observed for all subclasses.
Incontrast, for very short words of BL, only a modest improvement is obtained.
Whenusing BL and the modified Levenshtein distance d?L with transpositions, the backwards-dictionary method improved the basic search method only for words of length ?
9.For short words, a large number of repetitions of the same correction candidates wasobserved.
The analysis of this problem is a point of future work.Variants of the backwards-dictionary method also can be used for the Levenshteindistance d?
?L , in which insertions, deletions, substitutions, merges, and splits are treatedas primitive edit operations.
Here, the idea is to split the pattern at two neighboringpositions, which doubles the number of subsearches.
We did not evaluate this variant.7.
Using Dictionaries with Single Deletions for FilteringThe final technique that we describe here is again an adaptation of a filtering methodfrom pattern matching in strings (Muth and Manber 1996; Navarro and Raffinot 2002).When restricted to the error bound k = 1, this method is very efficient.
It can be usedonly for finite dictionaries.
Assume that the pattern P = p1 .
.
.
pm matches a portion oftext, T?, with one error.
Then m ?
1 letters of P are found in T?
in the correct order.472Computational Linguistics Volume 30, Number 4This fact can be used to compute m+1 derivatives of P that are compared with similarderivatives of a window T?
of length m that is slid over the text.
A derivative of a wordV can be V or a word that is obtained by deleting exactly one letter of V. Coincidencebetween derivatives of P and T?
can be used to detect approximate matches of P ofthe above form.
For details we refer to Navarro and Raffinot (2002).
In what followswe describe an adaptation of the method to approximate search of a pattern P in adictionary D.Let i be an integer.
With V[i] we denote the word that is obtained from a wordV by deleting the ith symbol of V. For |V| < i, we define V[i] = V. By a dictionarywith output sets, we mean a list of strings W, each of which is associated with a setof output strings O(W).
Each string W is called a key.
Starting from the conventionaldictionary D, we compute the following dictionaries with output sets Dall, D1, D2, .
.
.,Dn0 , where n0 is the maximal length of an entry in D:?
The set of keys of Dall is D ?
{V | ?i ?
1, W ?
D such that V = W[i]}.
Theoutput set for key V isOall(V) := {W ?
D | W = V ?
V = W[i] for some i ?
1}.?
The set of keys for Di is D ?
{V | ?W ?
D such that V = W[i]}.
Theoutput set for a key V is Oi(V) := {W ?
D | W = V ?
V = W[i]}.Lemma 2Let P denote a pattern, and let W ?
D. Then dL(P, W) ?
1 iff either W ?
Oall(P) orthere exists i, 1 ?
i ?
|P|, such that W ?
Oi(P[i]).The proof of the lemma is simple and has therefore been omitted.In our approach, the dictionaries with output sets Dall, D1, D2, .
.
., Dn0 are com-piled, respectively, into minimal subsequential transducers Aall, A1, A2, .
.
.
, An0 .
Given apattern P, we compute the union of the output sets Oall(P),O1(P[1]),.
.
.,O|P|(P[|P|]) us-ing these transducers.
It follows from Lemma 2 that we obtain as result the set of allentries W of D such that dL(P, W) ?
1.
It should be noted that the output sets are notnecessarily disjoint.
For example, if P itself is a dictionary entry, then P ?
Oi(P[i]) forall 1 ?
i ?
|P|.After we implemented the above procedure for approximate search, we found thata similar approach based on hashing had been described as early as 1981 in a technicalreport by Mor and Fraenkel (1981).7.1 Evaluation ResultsTable 8 presents the evaluation results for edit distance 1 using dictionaries with singledeletions obtained from BL.
The total size of the constructed single-deletion dictionaryautomata is 34.691 megabytes.
The word lists used for tests are those described inSection 5.2.
GL and TL are not considered here, since the complete system of subdic-tionaries needed turned out to be too large.
For a small range of input words of length3?6, filtering using dictionaries with single deletions behaves better than filtering usingthe backwards-dictionary method.8.
Similarity KeysA well-known technique for improving lexical search not mentioned so far is the useof similarity keys.
A similarity key is a mapping ?
that assigns to each word W a sim-plified representation ?(W).
Similarity keys are used to group dictionaries into classes473Mihov and Schulz Fast Approximate Search in Large Dictionariesof ?similar?
entries.
Many concrete notions of ?similarity?
have been considered, de-pending on the application domain.
Examples are phonetic similarity (e.g., SOUNDEXsystem; cf.
Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms ofword shape and geometric form (e.g., ?envelope representation?
[Sinha 1990; Anig-bogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, andWillett 1983; Owolabi and McGregor 1988).
In order to search for a pattern P in thedictionary, the ?code?
?
(P) is computed.
The dictionary is organized in such a waythat we may efficiently retrieve all regions containing entries with code (similar to)?(P).
As a result, only small parts of the dictionary must be visited, which speeds upsearch.
Many variants of this basic idea have been discussed in the literature (Kukich1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995).In our own experiments we first considered the following simple idea.
Given asimilarity key ?, each entry W of dictionary D is equipped with an additional pre-fix of the form ?(W)&.
Here & is a special symbol that marks the border betweencodes and original words.
The enhanced dictionary D?
with all entries of the form?
(W)&W is compiled into a deterministic finite-state automaton AD?.
Approximatesearch for pattern P in D is then reorganized in the following way.
The enhancedpattern ?
(P)&P is used for search in AD?.
We distinguish two phases in the backtrack-ing process.
In Phase 1, which ends when the special symbol & is read, we computean initial path of AD?
in which the corresponding sequence of transition labels rep-resents a code ?
such that dL(?(P),?)
?
k. All paths of this form are visited.
Eachlabel sequence ?& of the above form defines a unique state q of the automaton AD?such that LAD?
(q) = {W ?
D | ?
(W) = ?}.
In Phase 2, starting from q, we compute allentries W with code ?
(W) = ?
such that dL(W, P) ?
k. In both phases the automa-ton A?
(k) is used to control the search, and transition labels of AD?
are translated intocharacteristic vectors.
In order to guarantee completeness of the method, the distancebetween codes of a pair of words should not exceed the distance between the wordsthemselves.It is simple to see that in this method, the backtracking search is automaticallyrestricted to the subset of all dictionary entries V such that dL(?(V),?
(P)) ?
k. Unfor-tunately, despite this, the approach does not lead to reduced search times.
A closerlook at the structure of (conventional) dictionary automata AD for large dictionaries Dshows that there exists an enormous number of distinct initial paths of AD of length3?5.
During the controlled traversal of AD, most of the search time is spent visitingpaths of this initial ?wall.?
Clearly, most of these paths do not lead to any correctioncandidate.
Unfortunately, however, these ?blind?
paths are recognized too late.
Usingthe basic method described in Section 5, we have to overcome one single wall in AD forthe whole dictionary.
In contrast, when integrating similarity keys in the above form,we have to traverse a similar wall for the subdictionary D?
(W) := {V ?
D | ?
(V) =?
(W)} for each code ?
(W) found in Phase 1.
Even if the sets D?
(W) are usually muchsmaller than D, the larger number of walls that are visited leads to increased traversaltimes.As an alternative, we tested a method in which we attached to each entry W of D allprefixes of the form ?&, where ?
represents a possible code such that dL(?(W),?)
?
k.Using a procedure similar to the one described above, we have to traverse only onewall in Phase 2.
With this method, we obtained a reduction in search time.
However,with this approach, enhanced dictionaries D?
are typically much larger than originaldictionaries D. Hence the method can be used only if both dictionary D and boundk are not too large and if the key is not too fine.
Since the method is not more effi-cient than filtering using backwards dictionaries, evaluation results are not presentedhere.474Computational Linguistics Volume 30, Number 4Table 8Results for BL, using dictionaries with single deletions for filtering, distance bound k = 1.Times in milliseconds and speedup factors (ratio of times) with respect to basic algorithm.Length (CT1) Speedup 1 Length (CT1) Speedup 13 0.011 9.73 12 0.026 3.384 0.011 8.91 13 0.028 3.185 0.010 8.50 14 0.033 2.706 0.011 7.18 15 0.035 2.547 0.013 6.08 16 0.039 2.238 0.015 5.40 17 0.044 1.959 0.017 4.88 18 0.052 1.6710 0.020 4.40 19 0.055 1.5811 0.022 4.00 20 0.063 1.449.
Concluding RemarksIn this article, we have shown how filtering methods can be used to improve finite-state techniques for approximate search in large dictionaries.
As a central contributionwe introduced a new correction method, filtering based on backwards dictionariesand partitioned input patterns.
Though this method generally leads to very shortcorrection times, we believe that correction times could possibly be improved furtherusing refinements and variants of the method or introducing other filtering methods.There are, however, reasons to assume that we are not too far from a situation inwhich further algorithmic improvements become impossible for fundamental reasons.The following considerations show how an ?optimal?
correction time can be estimatedthat cannot be improved upon without altering the hardware or using faster accessmethods for automata.We used a simple backtracking procedure to realize a complete traversal of thedictionary automaton AD.
During a first traversal, we counted the total number ofvisits to any state.
Since AD is not a tree, states may be passed through several timesduring the complete traversal.
Each such event counts as one visit to a state.
The ratioof the number of visits to the total number of symbols in the list of words D gives theaverage number of visits per symbol, denoted v0.
In practice, the value of v0 dependson the compression rate that is achieved when compiling D into the automaton AD.
It issmaller than 1 because of numerous prefixes of dictionary words that are shared in AD.We then used a second traversal of AD?not counting visits to states?to compute thetotal traversal time.
The ratio of the total traversal time to the number of visits yieldsthe time t0 that is needed for a single visit.
For the three dictionaries, the followingvalues were obtained:BL GL TLAverage number v0 of visits per symbol 0.1433 0.3618 0.7335Average time t0 for one visit (in ?s) 0.0918 0.1078 0.0865Given an input V, we may consider the total number nV of symbols in the list ofcorrection candidates.
Then nV ?v0?t0 can be used to estimate the optimal correction timefor V. In fact, in order to achieve this correction time, we need an oracle that knowshow to avoid any kind of useless backtracking.
Each situation in which we proceededon a dictionary path that does not lead to a correction candidate for V would requiresome extra time that is not included in the above calculation.
From another point ofview, the above idealized algorithm essentially just copies the correction candidatesinto a resulting destination.
The time that is consumed is proportional to the sum ofthe length of the correction candidates.475Mihov and Schulz Fast Approximate Search in Large DictionariesFor each of the three dictionaries, we estimated the optimal correction time forone class of input words.
For BL we looked at input words of length 10.
The averagenumber of correction candidates for Levenshtein distance 3 is 121.73 (cf.
Table 1).Assuming that the average length of correction candidates is 10, we obtain a total of1, 217.3 symbols in the complete set of all correction candidates.
Hence the optimalcorrection time is approximately1217.3 ?
0.0000918 ms ?
0.1433 = 0.016 msThe actual correction time using filtering with the backwards-dictionary method is0.827 ms, which is 52 times slower.For GL, we considered input words of length 15?24 and distance bound 3.
We haveon average 3.824 correction candidates of length 20, that is, 76.48 symbols.
Hence theoptimal correction time is approximately76.48 ?
0.0001078 ms ?
0.3618 = 0.003 msThe actual correction time using filtering with the backwards-dictionary method is0.601 ms, which is 200 times slower.For TL, we used input sequences of length 45?54 and again distance bound 3.
Wehave on average 0.857 correction candidates of length 50, that is, 42.85 symbols.
Hencethe optimal correction time is approximately42.85 ?
0.0000865 ms ?
0.7335 = 0.003 msThe actual correction time using filtering with the backwards-dictionary method is0.759 ms, which is 253 times slower.These numbers coincide with our basic intuition that further algorithmic improve-ments are simpler for dictionaries with long entries.
For example, variants of thebackwards-dictionary method could be considered in which a finer subcase analysisis used to improve filtering.AcknowledgmentsThis work was funded by a grant fromVolkswagenStiftung.
The authors thank theanonymous referees for many suggestionsthat helped to improve the presentation.ReferencesAngell, Richard C., George E. Freund, andPeter Willett.
1983.
Automatic spellingcorrection using a trigram similaritymeasure.
Information Processing andManagement, 19:255?261.Anigbogu, Julain C. and Abdel Belaid.
1995.Hidden Markov models in textrecognition.
International Journal of PatternRecognition and Artificial Intelligence,9(6):925?958.Baeza-Yates, Ricardo A. and GonzaloNavarro.
1998.
Fast approximative stringmatching in a dictionary.
In R. Werner,editor, Proceedings SPIRE?98, pages 14?22.IEEE Computer Science.Baeza-Yates, Ricardo A. and GonzaloNavarro.
1999.
Faster approximate stringmatching.
Algorithmica, 23(2):127?158.Blair, Charles R. 1960.
A program forcorrecting spelling errors.
Information andControl, 3:60?67.Bunke, Horst.
1993.
A fast algorithm forfinding the nearest neighbor of a word ina dictionary.
In Proceedings of the SecondInternational Conference on DocumentAnalysis and Recognition (ICDAR?93), pages632?637, Tsukuba, Japan.Daciuk, Jan, Stoyan Mihov, Bruce W.Watson, and Richard E. Watson.
2000.Incremental construction of minimalacyclic finite state automata.Computational Linguistics, 26(1):3?16.Davidson, Leon.
1962.
Retrieval ofmisspelled names in an airline passengerrecord system.
Communications of the ACM,5(3):169?171.476Computational Linguistics Volume 30, Number 4de Bertrand de Beuvron, Francois andPhilippe Trigano.
1995.
Hierarchicallycoded lexicon with variants.
InternationalJournal of Pattern Recognition and ArtificialIntelligence, 9(1):145?165.Dengel, Andreas, Rainer Hoch, FrankHo?nes, Thorsten Ja?ger, Michael Malburg,and Achim Weigel.
1997.
Techniques forimproving OCR results.
In Horst Bunkeand Patrick S. P. Wang, editors, Handbookof Character Recognition and Document ImageAnalysis, 227?258.
World Scientific.Hopcroft, John E. and Jeffrey D. Ullman.1979.
Introduction to Automata Theory,Languages, and Computation.Addison-Wesley, Reading, MA.Kim, Jong Yong and John Shawe-Taylor.1992.
An approximate string-matchingalgorithm.
Theoretical Computer Science,92:107?117.Kim, Jong Yong and John Shawe-Taylor.1994.
Fast string matching using ann-gram algorithm.
Software?Practice andExperience, 94(1):79?88.Kozen, Dexter C. 1997.
Automata andComputability.
Springer.Kukich, Karen.
1992.
Techniques forautomatically correcting words in texts.ACM Computing Surveys, 24:377?439.Levenshtein, Vladimir I.
1966.
Binary codescapable of correcting deletions, insertions,and reversals.
Soviet Physics-Doklady,10:707?710.Mihov, Stoyan and Denis Maurel.
2001.Direct construction of minimal acyclicsubsequential transducers.
In S. Yu andA.
Pun, editors, Implementation andApplication of Automata: Fifth InternationalConference (CIAA?2000) (Lecture Notes inComputer Science no.
2088), pages217?229.
Springer.Mor, Moshe and Aviezri S. Fraenkel.
1981.A hash code method for detecting andcorrecting spelling errors.
TechnicalReport CS81-03, Department of AppliedMathematics, Weizmann Institute ofScience, Rehovot, Israel.Muth, Robert and Udi Manber.
1996.Approximate multiple string search.
InProceedings of the Seventh AnnualSymposium on Combinatorical PatternMatching (Lecture Notes in ComputerScience, no.
1075) pages 75?86.
Springer.Myers, Eugene W. 1994.
A sublinearalgorithm for approximate keywordsearching.
Algorithmica, 12(4/5):345?374.Navarro, Gonzalo.
2001.
A guided tour toapproximate string matching.
ACMComputing Surveys, 33(1):31?88.Navarro, Gonzalo and Ricardo A.Baeza-Yates.
1999.
Very fast and simpleapproximate string matching.
InformationProcessing Letters, 72:65?70.Navarro, Gonzalo and Mathieu Raffinot.2002.
Flexible Pattern Matching in Strings.Cambridge University Press,Cambridge.Odell, Margaret K. and Robert C. Russell.1918.
U.S. Patent Number 1, 261, 167.
U.S.Patent Office, Washington, DC.Odell, Margaret K. and Robert C. Russell.1992.
U.S. Patent Number 1,435,663.
U.S.Patent Office, Washington, DC.Oflazer, Kemal.
1996.
Error-tolerantfinite-state recognition with applicationsto morphological analysis and spellingcorrection.
Computational Linguistics,22(1):73?89.Oommen, B. John and Richard K. S. Loke.1997.
Pattern recognition of strings withsubstitutions, insertions, deletions, andgeneralized transpositions.
PatternRecognition, 30(5):789?800.Owolabi, Olumide and D. R. McGregor.1988.
Fast approximate string matching.Software?Practice and Experience,18(4):387?393.Riseman, Edward M. and Roger W. Ehrich.1971.
Contextual word recognition usingbinary digrams.
IEEE Transactions onComputers, C-20(4):397?403.Schulz, Klaus U. and Stoyan Mihov.
2001.Fast string correction withLevenshtein-automata.
Technical Report01-127, Centrum fu?r Informations= undsprachverarbeitung, University ofMunich.Schulz, Klaus U. and Stoyan Mihov.
2002.Fast string correction withLevenshtein-automata.
InternationalJournal of Document Analysis andRecognition, 5(1):67?85.Seni, Giovanni, V. Kripasundar, andRohini K. Srihari.
1996.
Generalizing editdistance to incorporate domaininformation: Handwritten text recognitionas a case study.
Pattern Recognition,29(3):405?414.Sinha, R. M. K. 1990.
On partitioning adictionary for visual text recognition.Pattern Recognition, 23(5):497?500.Srihari, Sargur N. 1985.
Computer TextRecognition and Error Correction.
Tutorial.IEEE Computer Society Press, SilverSpring, MD.Srihari, Sargur N., Jonathan J.
Hull, andRamesh Choudhari.
1983.
Integratingdiverse knowledge sources in textrecognition.
ACM Transactions on OfficeInformation Systems, 1(1):68?87.Stephen, Graham A.
1994.
String SearchingAlgorithms.
World Scientific, Singapore.477Mihov and Schulz Fast Approximate Search in Large DictionariesTakahashi, Hiroyasu, Nobuyasu Itoh, TomioAmano, and Akio Yamashita.
1990.
Aspelling correction method and itsapplication to an OCR system.
PatternRecognition, 23(3/4):363?377.Ukkonen, Esko.
1985a.
Algorithms forapproximate string matching.
Informationand Control, 64:100?118.Ukkonen, Esko.
1985b.
Finding approximatepatterns in strings.
Journal of Algorithms,6(1?3):132?137.Ukkonen, Esko.
1992.
Approximatestring-matching with q-grams andmaximal matches.
Theoretical ComputerScience, 92:191?211.Ullman, Jeffrey R. 1977.
A binary n-gramtechnique for automatic correction ofsubstitution, deletion, insertion andreversal errors.
Computer Journal,20(2):141?147.Wagner, Robert A. and Michael J. Fischer.1974.
The string-to-string correctionproblem.
Journal of the ACM, 21(1):168?173.Weigel, Achim, Stephan Baumann, andJ.
Rohrschneider.
1995.
Lexicalpostprocessing by heuristic search andautomatic determination of the edit costs.In Proceedings of the Third InternationalConference on Document Analysis andRecognition (ICDAR 95), pages 857?860.Wells, C. J., L. J. Evett, Paul E. Whitby, andR.-J.
Withrow.
1990.
Fast dictionarylook-up for contextual word recognition.Pattern Recognition, 23(5):501?508.Wu, Sun and Udi Manber.
1992.
Fast textsearching allowing errors.
Communicationsof the ACM, 35(10):83?91.Zobel, Justin and Philip Dart.
1995.
Findingapproximate matches in large lexicons.Software?Practice and Experience,25(3):331?345.
