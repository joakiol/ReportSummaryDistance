Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 33?37,Beijing, August 2010How to Expand Dictionaries with Web-Mining TechniquesNicolas B?chet LIRMM, UMR 5506, CNRS, Univ.
Montpellier 2bechet@lirmm.frMathieu Roche LIRMM, UMR 5506, CNRS, Univ.
Montpellier 2mroche@lirmm.frAbstractThis paper presents an approach to en-rich conceptual classes based on the Web.
To test our approach, we first build conceptual classes using syntactic and semantic information provided by a cor-pus.
The concepts can be the input of a dictionary.
Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle.
The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes.1 IntroductionConcepts have several definitions; one of the most general describes a concept ?as the mind?s representation of a thing or an item?
(Desrosiers-Sabbath, 1984).
In a domain such as ours, i.e.
ontology building, semantic webs, and computa-tional linguistics, it seems appropriate to stick to the Aristotelian approach to a concept, and con-sider it as a set of knowledge (gathered informa-tion) on common semantic features.
The choice of the features and how the knowledge is gath-ered depend on criteria we will explain below.
In this paper, we deal with the building of conceptual classes, which can be defined as gathering semantically close terms.
First, we suggest building specific conceptual classes by focusing on knowledge extracted from corpora.
Conceptual classes are shaped by the study of syntactic dependencies between corpus terms (as described in section 2).
Dependencies tackle re-lations such as Verb/Subject, Noun/Noun Phrase Complements, Verb/Object, Verb/Complements,and sometimes Sentence Head/Complements.
In this paper, we focus on the Verb/Object depend-ency because it is representative of a field.
For instance, in computer science, the verb ?to load?
takes as objects, nouns of the conceptual class software (L?Homme, 1998).
This feature also extends to ?download?
or ?upload?, which have the same verbal root.
Corpora are rich sources of terminological in-formation that can be mined.
A terminology ex-traction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Naza-renko et al, 2001; Weeds et al, 2005) for medi-cine).
After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3).
In section 4, experiments con-ducted on real data enable us to validate our ap-proach.2 Building Conceptual Classes2.1 PrincipleIn our approach, a class can be defined as a gathering of terms with a common field.
In this paper, we focus on objects of verbs judged to be semantically close by using a measure.
These objects are thus considered as instances of con-ceptual classes.
The first step in building concep-tual classes consists in extracting Verb/Object syntactic relations as explained in the following section.332.2 Mining for Verb/Object relationsOur corpora are in French since our team is mostly devoted to French-based NLP applica-tions.
However, the following method can be used for any other language, provided a reliable dependency parser is available.
In our case, we use the SYGFRAN parser developed by (Chauch?, 1984).
As an example, in the French sentence ?Thierry Dusautoir brandissant le dra-peau tricolore sur la pelouse de Cardiff apr?s la victoire.?
(translation: ?Thierry Dusautoir bran-dishing the three colored flag on Cardiff lawn after the victory?
), there is a verb-object syntac-tic relation: ?verb: brandir (to brandish), object: drapeau (flag)?, which is a good candidate for retrieval.
The second step of the building process corresponds to the gathering of common objects related to semantically close verbs.Figure 1: Common and complementary objects of the verbs ?to consume?
and ?to eat?
Assumption of Semantic Closeness.
The un-derlying linguistic hypothesis is the following: Verbs with a significant number of common ob-jects are semantically close.
To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1).
This type of work is similar to distri-butional analysis approaches such as that of (Bourigault and Lame, 2002).
As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (ob-jects).
Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects.
NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects).
NbO(qi) is the number of occurrences of qi objects of q verb.
The Asium measure is then:Where logAsium(x) is equal to:?
for x = 0, logAsium(x) = 0?
else logAsium(x) = log(x) + 1  Therefore, conceptual classes instances are the common objects of close verbs, according to the ASIUM proximity measure.
The following section describes the acquisi-tion of new terms starting with a list of terms/concepts obtained with the global process summarized in this section and detailed in (B?-chet et al, 2008).3 Expanding conceptual classes3.1 Acquisition of candidate termsThe aim of this approach is to provide new can-didates for a given concept.
It is based on enu-meration on the Web of terms that are semanti-cally close.
For instance, with a query (string) ?bicycle, car, and?, we can find other vehicles.
We propose to use the Web to acquire new can-didates.
This kind of method uses information regarding the ?popularity?
of the web and is in-dependent of a particular corpus.
Our method of acquisition is quite similar to that of (Nakov and Hearst, 2008).
These authors propose to query the Web using the Google search engine to characterize the semantic rela-tion between a pair of nouns.
The Google star operator among others, is used to that end.
(Na-kov and Hearst, 2008) refer to the study of (Lin and Pantel, 2001) who used a Web mining ap-proach to discover inference rules missed by humans.
To apply our method, we first consider the common objects of semantically close verbs, which are instances of reference concepts (e.g.
vehicle).
Let N concepts Ci?
{1, N} and their respec-tive instances Ij(Ci).
For each concept Ci, we submit to a search engine the following queries: ?IjA(Ci), IjB(Ci), and?
and ?IjA(Ci), IjB(Ci), or?
with jA and jB ?
{1, ..., NbInstanceCi} and jA ?
jB.34The search engine returns a set of results from which we extract new candidate instances of a concept.
For example, if we consider the query: ?bicycle, car, and?, one page returned by a search engine gives the following text:  Listen here for the Great Commuter Race (17/11/05) between bicycle, car and bus, as part of...  Having identified the relevant features in the result returned (in bold in our example), we add the term ?bus?
to the initial concept ?vehicle?.
In this way, we obtain new candidates for our con-cepts.
The process can be repeated.
In order to automatically determine which candidates are relevant, the candidates are filtered as shown in the following section.3.2 Filtering of candidatesThe quality of the extracted terms can be vali-dated by an expert, or automatically by using the Web to check if the extracted candidates (see section 3.1) are relevant.
The principle is to con-sider a relevant term if it is often present with the terms of the original conceptual class (kernel of words).
Thus, our aim is to validate a term ?in the context?.
From that point of view, our method is close to that of (Turney, 2001), which queries the Web via the AltaVista search engine to determine appropriate synonyms for a given term.
Like (Turney, 2001), we consider that in-formation concerning the number of pages re-turned by the queries can give an indication of the relevance of a term.
Thus, we submit to a search engine different strings (using citation marks).
A query consists of the new candidate and both terms of the con-cept.
Formally, our approach can be defined as follows.
Let N concepts Ci ?
{1, N}, their respec-tive instances Ij(Ci) and the new candidates for a concept Ci, Nik ?
{1, NbNI(Ci)}.
For each Ci, each new candidate Nik is sent as a query to a Web search engine.
In practice the three terms are separated either by a comma or the word ?or?
or  ?and?1.
For each query, the search engine returns a num-ber of results (i.e.
number of web pages).
Then, the sum of these results is calculated using all possible combinations of ?or?, ?and?, or of the three words (words of the kernel plus candidate1 Note that the commas are automatically removed by the search engines.word to enrich it).
Below is an example with the kernel words ?car?, ?bicycle?
and the candidate ?bus?
to test (using Yahoo):?
?car, bicycle, and bus?
: 71 pages re-turned?
?car, bicycle, or bus?
: 268 pages re-turned?
?bicycle, bus, and car?
: 208 pages re-turned?
and so forth  Global result: 71 + 268 + 208...
The filtering of candidates consists in select-ing the k first candidates by class (i.e.
with the highest sum), they are added as new instances of the initial concept.
We can reiterate the acquisi-tion approach by including these new terms.
The acquisition/filtering process can be repeated sev-eral times.
In the next section, we present experiments conducted to evaluate the quality of our ap-proach.4 Experiments4.1 Evaluation protocolWe used a French corpus from the Yahoo site (http://fr.news.yahoo.com/) composed of 8,948 news items (16.5 MB) from newspapers.
Ex-periments were performed on 60,000 syntactic relations (B?chet et al, 2008; B?chet et al, 2009) to build original conceptual classes.
We manually selected five concepts (see Figure 2).
Instances of these concepts are the common ob-jects of verbs defining the concept (see section 2.2).Figure 2: The five selected concepts and their instances.35For our experiments, we use an API of the search engine Yahoo!
to obtain new terms.
We apply the following post-treatments for each new candidate term.
They are initially lemmatized.
Therefore, we only keep the nouns, after apply-ing a PoS (Part of Speech) tagger, the TreeTag-ger (Schmid, 1995).
After these post-treatments, we manually validate the new terms using three experts.
We compute the precision of our approach to each expert.
The average is calculated to define the quality of the terms.
Precision is defined as fol-lows.
Precision = Number of relevant terms given by our system Number of terms given by our system  In the next section, we present the evaluation of our method.4.2 Experimental resultsTable 1 gives the results of the term acquisition method (i.e.
for each acquisition step, we apply our approach to filter candidate terms).
For each step, the table lists the degree of precision ob-tained after expertise:?
All candidates.
We calculate the preci-sion before the filtering step.?
Filtered candidates.
After applying the automatic filtering by selecting k terms per class, we calculate the precision ob-tained.
Note that the automatic filtering (see section 3.2) reduces the number of terms proposed, and thus reduces the re-call2.Table 1: Results obtained with k=4 (i.e.
auto-matic selection of the k first ranked terms by the filtering approach).2   The recall is not calculated because in an unsuper-vised context it is difficult to estimate.Finally Table 1 shows the number of terms generated by the acquisition system.
These results show that a significant number of terms can be generated (i.e.
103 words).
For example, for the concept ?feeling?, using the ini-tial terms given in figure 1, we obtained the fol-lowing eight French terms (in two steps): ?hor-reur (horror), satisfaction (satisfaction), d?prime (depression), faiblesse (weakness), tristesse (sadness), d?senchantement (disenchantment), folie (madness), fatalisme (fatalism)?.
This approach is appropriate to produce new relevant terms to enrich conceptual classes, in particular when we select the first terms (k = 4) returned by the filtering system.
In a future work, we plan to test other values of the auto-matic filtering.
The precision obtained in the first two steps was high (i.e.
0.69 to 0.83).
The third step returned lower scores; noise was introduced because we were too ?far?
from the initial kernel words.5 Conclusion and Future WorkThis paper describes an approach for conceptual enrichment classes based on the Web.
We apply the ?enumeration?
principle to find new terms using Web search engines.
This approach has the advantage of being less dependent on the corpus.
Note that as the use of the Web requires valida-tion of candidates, we propose an automatic fil-tering method to select relevant terms to add to the concept.
In a future work, we plan to use other statistical web measures (e.g.
Mutual In-formation, Dice measure, and so forth) to auto-matically validate terms.ReferencesB?chet, N., M. Roche, and J. Chauch?e.
2008.
How the ExpLSA approach impacts the document clas-sification tasks.
In Proceedings of the Interna-tional Conference on Digital Information Man-agement, ICDIM?08, pages 241?246, University of East London, London, United Kingdom.B?chet, N., M. Roche, and J. Chauch?.
2009.
To-wards the selection of induced syntactic relations.
In European Conference on Information Retrieval (ECIR), Poster, pages 786?790.Bourigault, D. and G. Lame.
2002.
Analyse distribu-tionnelle et structuration de terminologie.
Applica-tion ?
la construction d?une ontologie documen-taire du droit.
In TAL, pages 43?51.1 0.69 0.83 292 0.69 0.77 473 0.56 0.65 103Precision Terms numberSteps # All terms Filtered terms (without filter)36Chauch?, J.
1984.
Un outil multidimensionnel de l?analyse du discours.
In Proceedings of COLING, Standford University, California, pages 11?15.Desrosiers-Sabbath, R. 1984.
Comment enseigner les concepts.
Presses de l?Universit?
du Qu?bec.Faure, D. and C. Nedellec.
1999.
Knowledge acquisi-tion of predicate argument structures from techni-cal texts using machine learning: The system ASIUM.
In Proceedings of the 11th European Workshop, Knowledge Acquisition, Modelling and Management, number 1937 in LNAI, pages 329?334.Faure, D. 2000.
Conception de m?thode d?appren-tissage symbolique et automatique pour l?acquisi-tion de cadres de sous-cat?gorisation de verbes et de connaissances s?mantiques ?
partir de textes : le syst?me ASIUM.
Ph.D. thesis, Universit?
Paris-Sud, 20 D?cembre.Harris, Z.
1968.
Mathematical Structures of Lan-guage.
John Wiley & Sons, New-York.L?Homme, M. C. 1998.
Le statut du verbe en langue de sp?cialit?
et sa description lexicographique.
In Cahiers de Lexicologie 73, pages 61?84.Lin, Dekang and Patrick Pantel.
2001.
Discovery of inference rules for question answering.
Natural Language Engineering, 7:343?360.Nakov, Preslav and Marti A. Hearst.
2008.
Solving relational similarity problems using the web as a corpus.
In ACL, pages 452?460.Nazarenko, A., P. Zweigenbaum, B. Habert, and J. Bouaud.
2001.
Corpus-based extension of a termi-nological semantic lexicon.
In Recent Advances in Computational Terminology, pages 327?351.Schmid, H. 1995.
Improvements in part-of-speech tagging with an application to german.
In Proceed ngs of the ACL SIGDAT-Workshop, Dublin.Turney, P.D.
2001.
Mining the Web for synonyms: PMI?IR versus LSA on TOEFL.
In Proceedings of ECML?01, Lecture Notes in Computer Science, pages 491?502.Weeds, J., J. Dowdall, G. Schneider, B. Keller, and D. Weir.
2005.
Weir using distributional similarity to organise biomedical terminology.
In Proceed-ings of Terminology, volume 11, pages 107?141.37
