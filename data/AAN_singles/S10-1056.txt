Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 252?255,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsUTDMet: Combining WordNet and Corpus Data forArgument Coercion DetectionKirk Roberts and Sanda HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, Texas, USA{kirk,sanda}@hlt.utdallas.eduAbstractThis paper describes our system for theclassification of argument coercion forSemEval-2010 Task 7.
We present two ap-proaches to classifying an argument?s se-mantic class, which is then compared tothe predicate?s expected semantic class todetect coercions.
The first approach isbased on learning the members of an arbi-trary semantic class using WordNet?s hy-pernymy structure.
The second approachleverages automatically extracted seman-tic parse information from a large corpusto identify similar arguments by the pred-icates that select them.
We show the re-sults these approaches obtain on the taskas well as how they can improve a tradi-tional feature-based approach.1 IntroductionArgument coercion (a type of metonymy) occurswhen the expected semantic class (relative to thea predicate) is substituted for an object of a dif-ferent semantic class.
Metonymy is a pervasivephenomenon in language and the interpretation ofmetonymic expressions can impact tasks from se-mantic parsing (Scheffczyk et al, 2006) to ques-tion answering (Harabagiu et al, 2005).
A seminalexample in metonymy from (Lakoff and Johnson,1980) is:(1) The ham sandwich is waiting for his check.The ARG1 for the predicate wait is typically ananimate, but the ?ham sandwich?
is clearly not ananimate.
Rather, the argument is coerced to ful-fill the predicate?s typing requirement.
This coer-cion is allowed because an object that would nor-mally fulfill the typing requirement (the customer)can be uniquely identified by an attribute (the hamsandwich he ordered).SemEval-2010 Task 7 (?Argument Selectionand Coercion?)
(Pustejovsky and Rumshisky,2009) was designed to evaluate systems that de-tect such coercions and provide a ?compositionalhistory?
of argument selection relative to the pred-icate.
In order to accomplish this, an argument isannotated with both the semantic class to which itbelongs (the ?source?
type) as well as the class ex-pected by the predicate (the ?target?
type).
How-ever, in the data provided, the target type was un-ambiguous given the lemmatized predicate, so theremainder of this paper discusses source type clas-sification.
The detection of coercion is then sim-ply performed by checking if the classified sourcetype and target type are different.In our system, we explore two approaches withseparate underlying assumptions about how arbi-trary semantic classes can be learned.
In our firstapproach, we assume a semantic class can be de-fined a priori from a set of seed terms and thatWordNet is capable of defining the membershipof that semantic class.
We apply the PageRank al-gorithm in order to weight WordNet synsets givena set of seed concepts.
In our second approach,we assume that arguments in the same semanticclass will be selected by similar verbs.
We apply astatistical test to determine the most representativepredicates for an argument.
This approach benefitsfrom a large corpus from which we automaticallyextracted 200 million predicate-argument pairs.The remainder of this paper is organized as fol-lows.
Section 2 discusses our WordNet-based ap-proach.
Section 3 describes our corpus approach.Section 4 discusses our experiments and results.Section 5 provides a conclusion and direction forfuture work.
Due to space limitations, previouswork is discussed when relevant.2 PageRanking WordNet HypernymsOur first approach assumes that semantic classmembers can be defined and acquired a priori.252Given a set of seed concepts, we mine WordNetfor other concepts that may be in the same seman-tic class.
Clearly, this approach has both practicallimitations (WordNet does not contain every pos-sible concept) and linguistic limitations (conceptsmay belong to different semantic classes based ontheir context).
However, given the often vague na-ture of semantic classes (is a building an ARTI-FACT or a LOCATION?
), access to a weighted listof semantic class members can prove useful for ar-guments not seen in the train set.Using (Esuli and Sebastiani, 2007) as inspira-tion, we have implemented our own naive ver-sion of WordNet PageRank.
They use sense-disambiguated glosses provided by eXtendedWordNet (Harabagiu et al, 1999) to link synsetsby starting with positive (or negative) sentimentconcepts in order to find other concepts with pos-itive (or negative) sentiment values.
For ourtask, however, hypernymy relations are more ap-propriate for determining a given synset?s mem-bership in a semantic class.
Hypernymy de-fines an IS-A relationship between the parentclass (the hypernym) and one of its child classes(the hyponym).
Furthermore, while PageRank as-sumes directed edges (e.g., hyperlinks in a webpage), we use undirected edges.
In this way, ifHYPERNYMOF(A, B), then A?s membership in asemantic class strengthens B?s and vice versa.Briefly, the formula for PageRank is:a(k)= ?a(k?1)W+ (1 ?
?
)e (1)where a(k) is the weight vector containing weightsfor every synset in WordNet at time k; Wi,jis theinverse of the total number of hypernyms and hy-ponyms for synset i if synset j is a hypernym orhyponym of synset i; e is the initial score vector;and ?
is a tuning parameter.
In our implementa-tion, a(0) is initialized to all zeros; ?
is fixed at0.5; and ei= 1 if synset i is in the seed set S,and zero otherwise.
The process is then run untilconvergence, defined by |a(k)i?
a(k?1)i| < 0.0001for all i.The result of this PageRank is a weighted listcontaining every synset reachable by a hyper-nym/hyponym relation from a seed concept.
Weran the PageRank algorithm six times, once foreach semantic class, using the arguments in thetrain set as seeds.
For arguments that are polyse-mous, we make a first WordNet sense assumption.Representative examples of the concepts gener-ated from this approach are shown in Table 1.ARTIFACT DOCUMENTfunny wagon .377 white paper .342liquor .353 progress report .342iced tea .338 screenplay .324tartan .325 papyrus .313alpaca .325 pie chart .308EVENT LOCATIONrock concert .382 heliport .381rodeo .369 mukataa .380radium therapy .357 subway station .342seminar .347 dairy farm .326pub crawl .346 gateway .320PROPOSITION SOUNDdibs .363 whoosh .353white paper .322 squish .353tall tale .319 yodel .339commendation .310 theme song .320field theory .309 oldie .312Table 1: Some of the concepts (and scores) learnedfrom applying PageRank to WordNet hypernyms.3 Leveraging a Large Corpus ofSemantic Parse AnnotationsOur second approach assumes that semantic classmembers are arguments of similar predicates.
As(Pustejovsky and Rumshisky, 2009) elaborate,predicates select an argument from a specific se-mantic class, therefore terms that belong in thesame semantic class should be selected by simi-lar predicates.
However, this assumption is oftenviolated: type coercion allows predicates to havearguments outside their intended semantic class.Our solution to this problem, partially inspired by(Lapata and Lascarides, 2003), is to collect statis-tics from an enormous amount of data in order tostatistically filter out these coercions.The English Gigaword Forth Edition corpus1contains over 8.5 million documents of newswiretext collected over a 15 year period.
We processedthese documents with the SENNA2 (Collobert andWeston, 2009) suite of natural language tools,which includes a part-of-speech tagger, phrasechunker, named entity recognizer, and PropBanksemantic role labeler.
We chose SENNA due to itsspeed, yet it still performs comparably with manystate-of-the-art systems.
Of the 8.5 million doc-uments in English Gigaword, 8 million were suc-cessfully processed.
For each predicate-argumentpair in these documents, we gathered counts byargument type and argument head.
The head wasdetermined with simple heuristics from the chunkparse and parts-of-speech for each argument (ar-guments consisting of more than three phrasechunks were discarded).
When available, namedentity types (e.g., PERSON, ORGANIZATION, LO-1LDC2009T132http://ml.nec-labs.com/senna/253coffee book meeting station report voicedrink write hold own release hearsip read attend build publish raisebrew publish schedule open confirm giveserve title chair attack issue addspill sell convene close comment havesmell buy arrange operate submit silencesell balance call fill deny soundpour illustrate host shut file lendbuy research plan storm prepare crackrise review make set voice findTable 2: Top ten predicates for the most commonword in the train set for the six semantic classes.CATION) were substituted for heads.
This resultedin over 511 million predicate-argument pairs forargument types ARG0, ARG1, and ARG2.
For thistask, however, we chose only to use ARG1 argu-ments (direct objects), which resulted in 210 mil-lion pairs, 7.65 million of which were unique.
TheARG1 argument was chosen because most of thearguments in the data are direct objects 3.The ?best?
predicates for a given argument aredefined by a ranking based on Fisher?s exact test(Fisher, 1922):p =(a + b)!
(c + d)!
(a + c)!
(b + d)!n!a!b!c!d!
(2)where a is the number of times the given argumentwas used with the given predicate, b is the numberof times the argument was used with a differentpredicate, c is the number of times the predicatewas used with a different argument, d is the num-ber of times neither the given argument or predi-cate was used, and n = a+b+c+d.
The top ranked(lowest p) predicates for the most common argu-ments in the training data are shown in Table 2.4 ExperimentsWe have conducted several experiments to testthe performance of the approaches outlined inSections 2 and 3 along with additional featurescommonly found in information extraction liter-ature.
All experiments were conducted using theSVMmulticlass support vector machine library4.4.1 WordNet PageRankWe experimented with the output of our WordNetPageRank implementation along three separate di-mensions: (1) which sense to use (since we didnot incorporate a word sense disambiguation sys-tem), (2) whether to use the highest scoring se-3The notable exception to this, however, is arrive, wherethe data uses the destination argument.
In the PropBankscheme (Palmer et al, 2005), this would correspond to theARG4, which usually signifies an end state.4http://svmlight.joachims.org/svm multiclass.htmlmantic class or every class an argument belongedto, and (3) how to use the weight output by the al-gorithm.
The results of these experiments yieldeda single feature for each class that returns true ifthe argument is in that class, regardless of weight.This resulted in a micro-precision score of 75.6%.4.2 Gigaword PredicatesWe experimented with both (i) the number of pred-icates to use for an argument and (ii) the scorethreshold to use.
Ultimately, the Fisher score didnot prove nearly as useful as a classifier as it didas a ranker.
Since the distribution of predicatesfor each argument varied significantly, choosing ahigh number of predicates would yield good re-sults for some arguments but not others.
However,because of size of the training data, we were ableto choose the top 5 predicates for each argumentas features and still achieve a reasonable micro-precision score of 89.6%.4.3 Other FeaturesMany other features common in information ex-traction are well-suited for this task.
Given thatSVMs can support millions of features, we choseto add many features simpler than those previouslydescribed in order to improve the final perfor-mance of the classifier.
These include the lemmaof the argument (both the last word?s lemma andevery word?s lemma), the lemma of the predicate,the number of words in the argument, the casing ofthe argument, the part-of-speech of the argument?slast word, the WordNet synset and all (recursive)hypernyms of the argument.
Additionally, sincethe EVENT class is both the most common andthe most often confused, we introduced two fea-tures based on annotated resources.
The first fea-ture indicates the most common part-of-speech forthe un-lemmatized argument in the Treebank cor-pus.
This helped classify examples such as think-ing which was confused with a PROPOSITION forthe predicate deny.
Second, we introduced a fea-ture that indicated if the un-lemmatized argumentwas considered an event in the TimeBank cor-pus (Pustejovsky et al, 2003) at least five times.This helped to distinguish events such as meet-ing, which was confused with a LOCATION for thepredicate arrive.4.4 Ablation TestWe conducted an ablation test using combina-tions of five feature sets: (1) our WordNet PageR-254+WNSH +WNPR +GWPA +EVNTWORD 89.2 94.2 95.0 95.6 96.1EVNT 31.1 89.7 89.9 90.8GWPA 89.6 90.8 91.0WNPR 75.6 89.4WNSH 89.0Table 3: Ablation test of feature sets showingmicro-precision scores.Precision RecallSelection vs.
Coercion Macro 95.4 95.7Micro 96.3 96.3Source Type Macro 96.5 95.7Micro 96.1 96.1Target Type Macro 100.0 100.0Micro 100.0 100.0Joint Type Macro 85.5 95.2Micro 96.1 96.1Table 4: Results for UTDMET on SemEval-2010Task 7.ank feature (WNPR), (2) our Gigaword Predicatesfeature (GWPA), (3) word, lemma, and part-of-speech features (WORD), (4) WordNet synset andhypernym features (WNSH), and (5) Treebank andTimeBank features (EVNT).
Of these 25 ?
1 =31 tests, 15 are shown in Table 3.
The Giga-word Predicates (GWPA) was the best overall fea-ture, but each feature set ended up helping the fi-nal score.
WordNet PageRank (WNPR) even im-proved the score when combined WordNet hyper-nym features (WNSH) despite the fact that theyare heavily related.
Ultimately, WordNet PageR-ank had a greater precision, while the other Word-Net features had greater recall.4.5 Task 7 ResultsTable 4 shows the official results for UTDMET onthe Task 7 data.
The target type was unambigu-ous given the lemmatized predicate.
For classify-ing selection vs. coercion, we simply checked tosee if the classified source type was the same asthe target type.
If this was the case, we returnedselection, otherwise a coercion existed.5 ConclusionWe have presented two approaches for determin-ing the semantic class of a predicate?s argument.The two approaches capture different informationand combine well to classify the ?source?
type inSemEval-2010 Task 7.
We showed how this can beincorporated into a system to detect coercions aswell as the argument?s compositional history rel-ative to its predicate.
In future work we plan toextend this system to more complex tasks such aswhen the predicate may be polysemous or unseenpredicates may be encountered.AcknowledgmentsThe authors would like to thank Bryan Rink forseveral insights during the course of this work.ReferencesRonan Collobert and Jason Weston.
2009.
DeepLearning in Natural Language Processing.
Tutorialat NIPS.Andrea Esuli and Fabrizio Sebastiani.
2007.
PageR-anking WordNet Synsets: An Application to Opin-ion Mining.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Lin-guistics, pages 424?431.Ronald A. Fisher.
1922.
On the interpretation of ?2from contingency tables, and the calculation of p.85(1):87?94.Sanda Harabagiu, George Miller, and Dan Moldovan.1999.
WordNet 2 - A Morphologically and Se-mantically Enhanced Resource.
In Proceedings ofthe SIGLEX Workshop on Standardizing Lexical Re-sources, pages 1?7.Sanda Harabagiu, Andrew Hickl, John Lehmann, andDan Moldovan.
2005.
Experiments with Inter-active Question-Answering.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, pages 205?214.George Lakoff and Mark Johnson.
1980.
MetaphorsWe Live By.
University of Chicago Press.Maria Lapata and Alex Lascarides.
2003.
A Proba-bilistic Account of Logical Metonymy.
Computa-tional Linguistics, 21(2):261?315.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31(1):71?106.James Pustejovsky and Anna Rumshisky.
2009.SemEval-2010 Task 7: Argument Selection and Co-ercion.
In Proceedings of the NAACL HLT Work-shop on Semantic Evaluations: Recent Achieve-ments and Future Directions, pages 88?93.James Pustejovsky, Patrick Hanks, Roser Saur?
?, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, and Marcia Lazo.
2003.
The TIMEBANKCorpus.
In Proceedings of Corpus Linguistics,pages 647?656.Jan Scheffczyk, Adam Pease, and Michael Ellsworth.2006.
Linking FrameNet to the Suggested UpperMerged Ontology.
In Proceedings of Formal Ontol-ogy in Information Systems, pages 289?300.255
