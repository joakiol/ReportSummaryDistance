Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1047?1057,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsProbabilistic models of similarity in syntactic contextDiarmuid O?
Se?aghdhaComputer LaboratoryUniversity of CambridgeUnited Kingdomdo242@cl.cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of CambridgeUnited KingdomAnna.Korhonen@cl.cam.ac.ukAbstractThis paper investigates novel methods for in-corporating syntactic information in proba-bilistic latent variable models of lexical choiceand contextual similarity.
The resulting mod-els capture the effects of context on the inter-pretation of a word and in particular its effecton the appropriateness of replacing that wordwith a potentially related one.
Evaluating ourtechniques on two datasets, we report perfor-mance above the prior state of the art for esti-mating sentence similarity and ranking lexicalsubstitutes.1 IntroductionDistributional models of lexical semantics, whichassume that aspects of a word?s meaning can be re-lated to the contexts in which that word is typicallyused, have a long history in Natural Language Pro-cessing (Spa?rck Jones, 1964; Harper, 1965).
Suchmodels still constitute one of the most popular ap-proaches to lexical semantics, with many proven ap-plications.
Much work in distributional semanticstreats words as non-contextualised units; the modelsthat are constructed can answer questions such as?how similar are the words body and corpse??
butdo not capture the way the syntactic context in whicha word appears can affect its interpretation.
Re-cent developments (Mitchell and Lapata, 2008; Erkand Pado?, 2008; Thater et al, 2010; Grefenstette etal., 2011) have aimed to address compositionality ofmeaning in terms of distributional semantics, lead-ing to new kinds of questions such as ?how similarare the usages of the words body and corpse in thephrase the body/corpse deliberated the motion.
.
.
?
?and ?how similar are the phrases the body deliber-ated the motion and the corpse rotted??.
In this pa-per we focus on answering questions of the formertype and investigate models that describe the effectof syntactic context on the meaning of a single word.The work described in this paper uses probabilis-tic latent variable models to describe patterns of syn-tactic interaction, building on the selectional prefer-ence models of O?
Se?aghdha (2010) and Ritter et al(2010) and the lexical substitution models of Dinuand Lapata (2010).
We propose novel methods forincorporating information about syntactic context inmodels of lexical choice, yielding a probabilisticanalogue to dependency-based models of contextualsimilarity.
Our models attain state-of-the-art per-formance on two evaluation datasets: a set of sen-tence similarity judgements collected by Mitchelland Lapata (2008) and the dataset of the EnglishLexical Substitution Task (McCarthy and Navigli,2009).
In view of the well-established effectivenessof dependency-based distributional semantics and ofprobabilistic frameworks for semantic inference, weexpect that our approach will prove to be of value ina wide range of application settings.2 Related workThe literature on distributional semantics is vast; inthis section we focus on outlining the research that ismost directly related to capturing effects of contextand compositionality.1 Mitchell and Lapata (2008)1The interested reader is referred to Pado?
and Lapata (2007)and Turney and Pantel (2010) for a general overview.1047follow Kintsch (2001) in observing that most dis-tributional approaches to meaning at the phrase orsentence level assume that the contribution of syn-tactic structure can be ignored and the meaning of aphrase is simply the commutative sum of the mean-ings of its constituent words.
As Mitchell and Lap-ata argue, this assumption clearly leads to an impov-erished model of semantics.
Mitchell and Lapata in-vestigate a number of simple methods for combiningdistributional word vectors, concluding that point-wise multiplication best corresponds to the effectsof syntactic interaction.Erk and Pado?
(2008) introduce the concept of astructured vector space in which each word is as-sociated with a set of selectional preference vec-tors corresponding to different syntactic dependen-cies.
Thater et al (2010) develop this geometric ap-proach further using a space of second-order distri-butional vectors that represent the words typicallyco-occurring with the contexts in which a word typi-cally appears.
The primary concern of these authorsis to model the effect of context on word meaning;the work we present in this paper uses similar intu-itions in a probabilistic modelling framework.A parallel strand of research seeks to representthe meaning of larger compositional structures us-ing matrix and tensor algebra (Smolensky, 1990;Rudolph and Giesbrecht, 2010; Baroni and Zampar-elli, 2010; Grefenstette et al, 2011).
This nascentapproach holds the promise of providing a muchricher notion of context than is currently exploitedin semantic applications.Probabilistic latent variable frameworks for gen-eralising about contextual behaviour (in the formof verb-noun selectional preferences) were proposedby Pereira et al (1993) and Rooth et al (1999).
La-tent variable models are also conceptually similarto non-probabilistic dimensionality reduction tech-niques such as Latent Semantic Analysis (Landauerand Dumais, 1997).
More recently, O?
Se?aghdha(2010) and Ritter et al (2010) reformulated Rooth etal.
?s approach in a Bayesian framework using mod-els related to Latent Dirichlet Allocation (Blei et al,2003), demonstrating that this ?topic modelling?
ar-chitecture is a very good fit for capturing selectionalpreferences.
Reisinger and Mooney (2010) inves-tigate nonparametric Bayesian models for teasingapart the context distributions of polysemous words.As described in Section 3 below, Dinu and Lapata(2010) propose an LDA-based model for lexical sub-stitution; the techniques presented in this paper canbe viewed as a generalisation of theirs.
Topic modelshave also been applied to other classes of semantictask, for example word sense disambiguation (Li etal., 2010), word sense induction (Brody and Lapata,2009) and modelling human judgements of semanticassociation (Griffiths et al, 2007).3 Models3.1 Latent variable context modelsIn this paper we consider generative models of lex-ical choice that assign a probability to a particularword appearing in a given linguistic context.
In par-ticular, we follow recent work (Dinu and Lapata,2010; O?
Se?aghdha, 2010; Ritter et al, 2010) in as-suming a latent variable model that associates con-texts with distributions over a shared set of variablesand associates each variable with a distribution overthe vocabulary of word types:P (w|c) =?z?ZP (w|z)P (z|c) (1)The set of latent variables Z is typically muchsmaller than the vocabulary size; this induces a (soft)clustering of the vocabulary.
Latent Dirichlet Allo-cation (Blei et al, 2003) is a powerful method forlearning such models from a text corpus in an unsu-pervised way; LDA was originally applied to doc-ument modelling, but it has recently been shown tobe very effective at inducing models for a variety ofsemantic tasks (see Section 2).Given the latent variable framework in (1) we candevelop a generative model of paraphrasing a wordo with another word n in a particular context c:PC?T (n|o, c) =?zP (n|z)P (z|o, c) (2)P (z|o, c) = P (o|z)P (z|c)?z?
P (o|z?
)P (z?|c)(3)In words, the probability P (n|o, c) is the probabilitythat n would be generated given the latent variabledistribution associated with seeing o in context c;this latter distribution P (z|o, c) can be derived usingBayes?
rule and the assumption P (o|z, c) = P (o|z).1048Given a set of contexts C in which an instance o ap-pears (e.g., it may be both the subject of a verb andmodified by an adjective), (2) and (3) become:PC?T (n|o, C) =?zP (n|z)P (z|o, C) (4)P (z|o, C) = P (o|z)P (z|C)?z?
P (o|z?
)P (z?|C)(5)P (z|C) =?c?C P (z|c)?z?
?c?C P (z?|c)(6)Equation (6) can be viewed as defining a ?productof experts?
model (Hinton, 2002).
Dinu and Lapata(2010) also use a similar formulation to (5), exceptthat P (z|o, C) is factorised over P (z|o, C) ratherthan just P (z|C):PDL10(z|o, C) =?c?CP (o|z)P (z|c)?z?
P (o|z?
)P (z?|c)(7)In Section 5 below, we find that using (5) rather than(7) gives better results.The model described above (henceforth C ?
T )models the dependence of a target word on its con-text.
An alternative perspective is to model the de-pendence of a set of contexts on a target word, i.e.,we induce a modelP (c|w) =?zP (c|z)P (z|w) (8)Making certain assumptions, a formula for P (n|o, c)can be derived from (8):PT?C(n|o, c) =P (c|o, n)P (n|o)P (c|o) (9)P (c|o, n) =?zP (c|z)P (z|o, n)P (z|o, n) = P (z|o)P (z|n)?z?
P (z?|o)P (z?|n)(10)P (c|o) =?zP (c|z)P (z|o) (11)P (n|o) = 1/V (12)The assumption of a uniform prior P (n|o) on thechoice of a paraphrase n for o is clearly not appro-priate from a language modelling perspective (onecould imagine an alternative P (n) based on corpusfrequency), but in the context of measuring semanticsimilarity it serves well.
The T ?
C model for a setof contexts C is:PT?C(n|o, C) =P (C|o, n)P (n|o)P (C|o) (13)P (C|o, n) =?zP (z|o, n)?c?CP (c|z) (14)P (C|o) =?zP (z|o)?c?CP (c|z) (15)P (z|o, C) = P (z|o)P (C|o)?z?
P (z?|o)P (C|o)(16)With appropriate priors chosen for the distribu-tions over words and latent variables, P (n|o, C) isa fully generative model of lexical substitution.
Anon-generative alternative is one that estimates thesimilarity of the latent variable distributions associ-ated with seeing n and o in context C. The princi-ple that similarity between topic distributions corre-sponds to semantic similarity is well-known in doc-ument modelling and was proposed in the contextof lexical substitution by Dinu and Lapata (2010).In terms of the equations presented above, we couldcompare the distributions P (z|o, C) with P (z|n,C)using equations (5) or (16).
However, Thater etal.
(2010) and Dinu and Lapata (2010) both ob-serve that contextualising both o and n can degradeperformance; in view of this we actually compareP (z|o, C) with P (z|n) and make the further simpli-fying assumption that P (z|n) ?
P (n|z).
The sim-ilarity measure we adopt is the Bhattacharyya coef-ficient, which is a natural measure of similarity be-tween probability distributions and is closely relatedto the Hellinger distance used in previous work ontopic modelling (Blei and Lafferty, 2007):simbhatt(Px(z), Py(z)) =?z?Px(z)Py(z) (17)This measure takes values between 0 and 1.In this paper we train LDA models of P (w|c) andP (c|w).
In the former case, the analogy to documentmodelling is that each context type plays the role ofa ?document?
consisting of all the words observedin that context in a corpus; for P (c|w) the roles arereversed.
The models are trained by Gibbs samplingusing the efficient procedure of Yao et al (2009).The empirical estimates for distributions over wordsand latent variables are derived from the assignment1049of topics over the training corpus in a single sam-pling state.
For example, to model P (w|c) we cal-culate:P (w|z) = fzw + ?fz?
+N?
(18)P (z|c) = fzc + ?zf?c +?z?
?z?
(19)where fzw is the number of words of type w as-signed topic z, fzc is the number of times z is associ-ated with context c, fz?
and f?c are the marginal topicand context counts respectively, N is the number ofword types and ?
and ?
parameterise the Dirichletprior distributions over P (z|c) and P (w|z).
Follow-ing the recommendations of Wallach et al (2009)we use asymmetric ?
and symmetric ?
; rather thanusing fixed values for these hyperparameters we es-timate them from data in the course of LDA train-ing using an EM-like method.2 We use standard set-tings for the number of training iterations (1000), thelength of the burnin period before hyperparameterestimation begins (200 iterations) and the frequencyof hyperparameter estimation (50 iterations).3.2 Context typesWe have not yet defined what the contexts c looklike.
In vector space models of semantics it iscommon to distinguish between window-based anddependency-based models (Pado?
and Lapata, 2007);one can make the same distinction for probabilis-tic context models.
A broad generalisation is thatwindow-based models capture semantic association(e.g.
referee is associated with football), whiledependency models capture a finer-grained notionof similarity (referee is similar to umpire but notto football).
Dinu and Lapata (2010) propose awindow-based model of lexical substitution; the setof contexts in which a word appears is the set ofsurrounding words within a prespecified ?windowsize?.
In this paper we also investigate dependency-based context sets derived from syntactic structure.Given a sentence such as2We use the estimation methods provided by the MAL-LET toolkit, available from http://mallet.cs.umass.edu/.The:d executive:j body:nn:ncmod:jOO decided:vv:ncsubj:n .
.
.the set C of dependency contexts for the noun bodyis {executive:j:ncmod?1:n, decide:v:ncsubj:n},where ncmod?1 denotes that body stands in an in-verse non-clausal modifier relation to executive (weassume that nouns are the heads of their adjectivalmodifiers).4 Experiment 1: Similarity in context4.1 DataMitchell and Lapata (2008) collected human judge-ments of semantic similarity for pairs of short sen-tences, where the sentences in a pair share the samesubject but different verbs.
For example, the salesslumped and the sales declined should be judged asvery similar while the shoulders slumped and theshoulders declined should be judged as less similar.The resulting dataset (henceforth ML08) consists of120 such pairs using 15 verbs, balanced across highand low expected similarity.
60 subjects rated thedata using a scale of 1?7; Mitchell and Lapata cal-culate average interannotator correlation to be 0.40(using Spearman?s ?).
Both Mitchell and Lapataand Erk and Pado?
(2008) split the data into a devel-opment portion and a test portion, the developmentportion consisting of the judgements of six annota-tors; in order to compare our results with previousresearch we use the same data split.
To evaluate per-formance, the predictions made by a model are com-pared to the judgements of each annotator in turn(using ?)
and the resulting per-annotator ?
values areaveraged.4.2 ModelsAll models were trained on the written section of theBritish National Corpus (around 90 million words),parsed with RASP (Briscoe et al, 2006).
The BNCwas also used by Mitchell and Lapata (2008) andErk and Pado?
(2008); as the ML08 dataset was com-piled using words appearing more than 50 times inthe BNC, there are no coverage problems causedby data sparsity.
We trained LDA models for thegrammatical relations v:ncsubj:n and n:ncsubj?1:v1050Model PARA SIMNo optimisationC ?
T 0.24 0.34T ?
C 0.36 0.39T ?
C 0.33 0.39Optimised on devC ?
T 0.24 0.35T ?
C 0.41 0.41T ?
C 0.37 0.41Erk and Pado?
(2008) Mult 0.24SVS 0.27Table 1: Performance (average ?)
on the ML08 testsetand used these to create predictors of type C ?
Tand T ?
C, respectively.
For each predictor, wetrained five runs with 100 topics for 1000 iterationsand averaged the predictions produced from their fi-nal states.
We investigate both the generative para-phrasing model (PARA) and the method of compar-ing topic distributions (SIM).
For both PARA andSIM we present results using each predictor type onits own as well as a combination of both types (T ?C); for PARA the contributions of the types are mul-tiplied and for SIM they are averaged.3 One poten-tial complication is that the PARA model is trainedto predict P (n|c, o), which might not be comparableacross different combinations of subject c and verbo.
Using P (n|c, o) as a proxy for the desired jointdistribution P (n, c, o) is tantamount to assuming auniform distribution P (c, o), which can be defendedon the basis that the choice of subject noun and ref-erence verb is not directly relevant to the task.
Asshown by the results below, this assumption seemsto work reasonably well in practice.As well as reporting correlations for straightfor-ward averages of each set of five runs, we also inves-tigate whether the development data can be used toselect an optimal subset of runs.
This is done by sim-ply evaluating every possible subset of 1?5 runs onthe development data and picking the best-scoringsubset.4.3 ResultsTable 1 presents the results of the PARA and SIMpredictors on the ML08 dataset.
The best results3This configuration seems the most intuitive; averagingPARA predictors and multiplying SIM also give good results.previously reported for this dataset were given byErk and Pado?
(2008), who measured average ?
val-ues of 0.24 for a vector multiplication method and0.27 for their structured vector space (SVS) syn-tactic disambiguation method.
Even without usingthe development set to select models, performance iswell above the previous state of the art for all predic-tors except PARAC?T .
Model selection on the de-velopment data brings average ?
up to 0.41, which iscomparable to the human ?ceiling?
of 0.40 measuredby Mitchell and Lapata.
In all cases the T ?
C pre-dictors outperform C ?
T : models that associatetarget words with distributions over context clustersare superior to those that associate contexts with dis-tributions over target words.Figure 1 plots the beneficial effect of averagingover multiple runs; as the number of runs n is in-creased, the average performance over all combi-nations of n predictors chosen from the set of fiveT ?
C and five C ?
T runs is observed to in-crease monotonically.
Figure 1 also shows that themodel selection procedure is very effective at se-lecting the optimal combination of models; develop-ment set performance is a reliable indicator of testset performance.5 Experiment 2: Lexical substitution5.1 DataThe English Lexical Substitution task, run as partof the SemEval-1 competition, required participantsto propose good substitutes for a set of target wordsin various sentential contexts (McCarthy and Nav-igli, 2009).
Table 2 shows two example sentencesand the substitutes appearing in the gold standard,ranked by the number of human annotators who pro-posed each substitute.
The dataset contains a total of2,010 annotated sentences with 205 distinct targetwords across four parts of speech (noun, verb, ad-jective, adverb).
In line with previous work on con-textual disambiguation, we focus here on the subtaskof ranking attested substitutes rather than proposingthem from an unrestricted vocabulary.
To this end,a candidate set is constructed for each target wordfrom all the substitutes proposed for that word in allsentences in the dataset.The data contains a number of multiword para-phrases such as rush at; as our models (like most10511 2 3 4 50.10.20.30.40.5No.
of predictors?
(a) PARA: Target ?
Context1 2 3 4 50.10.20.30.40.5No.
of predictors?
(b) PARA: Context ?
Target2 3 4 5 6 7 8 9 100.10.20.30.40.5No.
of predictors?
(c) PARA: Target ?
Context1 2 3 4 50.10.20.30.40.5No.
of predictors?
(d) SIM: Target ?
Context1 2 3 4 50.10.20.30.40.5No.
of predictors?
(e) SIM: Context ?
Target2 3 4 5 6 7 8 9 100.10.20.30.40.5No.
of predictors?
(f) SIM: Target ?
ContextFigure 1: Performance on the ML08 test set with different predictor types and different numbers of LDAruns per predictor type; the solid line tracks the average performance, the dashed line shows the performanceof the predictor combination that scores best on the development set.Realizing immediately that strangers have come, attack (5), rush at (1)the animals charge them and the horses began to fight.Commission is the amount charged to execute a trade.
levy (2), impose (1), take (1), demand (1)Table 2: Examples for the verb charge from the English Lexical Substitution Taskcurrent models of distributional semantics) do notrepresent multiword expressions, we remove suchparaphrases and discard the 17 sentences which haveonly multiword substitutes in the gold standard.4There are also 7 sentences for which the gold stan-dard contains no substitutes.
This leaves a total of1986 sentences.
These sentences were lemmatisedand parsed with RASP.Previous authors have partitioned the dataset invarious ways.
Erk and Pado?
(2008) use only a sub-set of the data where the target is a noun headedby a verb or a verb heading a noun.
Thater et al4Thater et al (2010) and Dinu and Lapata (2010) similarlyremove multiword paraphrases (Georgiana Dinu, p.c.).
(2010) discard sentences which their parser cannotparse and paraphrases absent from their training cor-pus and then optimise the parameters of their modelthrough four-fold cross-validation.
Here we aim forcomplete coverage on the dataset and do not performany parameter tuning.
We use two measures to eval-uate performance: Generalised Averaged Precision(Kishida, 2005) and Kendall?s ?b rank correlationcoefficient, which were used for this task by Thateret al (2010) and Dinu and Lapata (2010), respec-tively.
Generalised Averaged Precision (GAP) isa precision-like measure for evaluating ranked pre-dictions against a gold standard.
?b is a variant ofKendall?s ?
that is appropriate for data containingtied ranks.
We do not use the ?precision out of ten?1052COORDINATION:Cats andc:conj:nOOc:conj:nOOdogs runv:ncsubj:n?
Cats and dogsOOn:and:nOO runv:ncsubj:nPREDICATION:The cat isv:ncsubj:nOOv:xcomp:jfierce ?
The catn:ncmod:jis fiercePREPOSITIONS:The catn:ncmod:iini:dobj:nOOthe hat ?
The catn:prep in:nin the hatTable 3: Dependency graph preprocessingmeasure that was used in the original Lexical Substi-tution Task; this measure assigns credit for the pro-portion of the first 10 proposed paraphrases that arepresent in the gold standard and in the context ofranking attested substitutes it is unclear how to ob-tain non-trivial results for target words with 10 orfewer possible substitutes.
We calculate statisticalsignificance of performance differences using strati-fied shuffling (Yeh, 2000).55.2 ModelsWe apply the models developed in Section 3.1 to theLexical Substitution Task dataset using dependency-and window-based context information.
Here weonly use the SIM predictor type.
PARA did not givesatisfactory results; in particular, it tended to rankcommon words highly in most contexts.6As before we compiled training data by extractingtarget-context cooccurrences from a text corpus.
Inaddition to the parsed BNC described above we useda corpus of Wikipedia text consisting of over 45 mil-lion sentences (almost 1 billion words) parsed usingthe fast Combinatory Categorial Grammar (CCG)parser described by Clark et al (2009).
The depen-5We use the software package available at http://www.nlpado.de/?sebastian/sigf.html.6Favouring more general words may indeed make sense insome paraphrasing tasks (Nulty and Costello, 2010).dency representation produced by this parser is inter-operable with the RASP dependency format.
In or-der to focus our models on semantically discrimina-tive information and make inference more tractablewe ignored all parts of speech other than nouns,verbs, adjectives, prepositions and adverbs.
Stop-words and words of fewer than three characters wereremoved.
We also removed the very frequent but se-mantically weak lemmas be and have.We compare two classes of context models: mod-els learned from window-based contexts and modelslearned from syntactic dependency contexts.
For thesyntactic models we extracted all dependencies andinverse dependencies between lemmas of the afore-mentioned POS types; in order to maximise the ex-traction yield, the dependency graph for each sen-tence was preprocessed using the transformationsshown in Table 3.
For the window-based contextmodel we follow Dinu and Lapata (2010) in treatingeach word within five words of a target as a memberof its context set.It proved necessary to subsample the corpora inorder to make LDA training tractable, especially forthe window-based model where the training set ofcontext-target counts is extremely dense (each in-stance of a word in the corpus contributes up to10 context instances).
For the window-based data,we divided each context-target count by a factor of5 and a factor of 70 for the BNC and Wikipediacorpora respectively, rounding fractional counts tothe closest integer.
The choice of 70 for scalingWikipedia counts is adopted from Dinu and Lap-ata (2010), who used the same factor for the com-parably sized English Gigaword corpus.
As the de-pendency data is an order of magnitude smaller wedownsampled the Wikipedia counts by 5 and left theBNC counts untouched.
Finally, we created a largercorpus by combining the counts from the BNC andWikipedia datasets.
Type and token counts for theBNC and combined corpora are given in Table 4.We trained three LDA predictors for each corpus:a window-based predictor (W5), a Context ?
Tar-get predictor (C ?
T ) and a Target ?
Contextpredictor (T ?
C).
For W5 the sets of types andcontexts should be symmetrical (in practice thereis some discrepancy due to preprocessing artefacts).ForC ?
T , individual models were trained for eachof the four target parts of speech; in each case the set1053BNC BNC+WikipediaTokens Types Contexts Tokens Types ContextsNouns 18723082 122999 316237 54145216 106448 514257Verbs 7893462 18494 57528 20082658 16673 82580Adjectives 4385788 73684 37163 11536424 88488 57531Adverbs 1976837 7124 14867 3017936 4056 18510Window5 28329238 88265 102792 42828094 139640 143443Table 4: Type and token counts for the BNC and downsampled BNC+Wikipedia corporaBNC BNC + WikipediaGAP ?b Coverage GAP ?b CoverageW5 44.5 0.17 100.0 44.8 0.17 100.0C ?
T 43.2 0.16 86.4 48.7 0.21 86.5T ?
C 47.2 0.21 86.4 49.3 0.22 86.5T ?
C 45.7 0.20 86.4 49.1 0.23 86.5W5 + C ?
T 46.0 0.18 100.0 48.7 0.21 100.0W5 + T ?
C 48.6 0.21 100.0 49.3 0.22 100.0W5 + T ?
C 48.1 0.20 100.0 49.5 0.23 100.0Table 5: Results on the English Lexical Substitution Task dataset; boldface denotes best performance at fullcoverage for each corpusof types is the vocabulary for that part of speech andthe set of contexts is the set of dependencies takingthose types as dependents.
For T ?
C we againtrain four models; the sets of types and contexts arereversed.
For the both corpora we trained modelswith Z = {600, 800, 1000, 1200} topics; for eachsetting of Z we ran five estimation runs.
Each in-dividual prediction of similarity between P (z|C, o)and P (z|n) is made by averaging over the predic-tions of all runs and over all settings of Z. Choosinga single setting of Z does not degrade performancesignificantly; however, averaging over settings is aconvenient way to avoid having to pick a specificvalue.We also investigate combinations of predictortypes, once again produced by averaging: we com-bine C ?
T with C ?
T (T ?
C) and combineeach of these three models with W5.5.3 ResultsTable 5 presents the results attained by our mod-els on the Lexical Substitution Task data.
Thedependency-based models have imperfect coverage(86% of the data); they can make no prediction whenno syntactic context is provided for a target, per-haps as a result of parsing error.
The window-basedmodels have perfect coverage, but score noticeablylower.
By combining dependency- and window-based models we can reach high performance withperfect coverage.
All combinations outperform thecorresponding W5 results to a statistically signifi-cant degree (p < 0.01).
Performance at full cov-erage is already very good (GAP= 48.6, ?b = 0.21)on the BNC corpus, but the best results are attainedby W5 + T ?
C trained on the combined corpus(GAP= 49.5, ?b = 0.23).
The results for the W5model trained on BNC data is comparable to thattrained on the combined corpus; however the syntac-tic models show a clear benefit from the less sparsedependency data in the combined training corpus.As remarked in Section 3.1, Dinu and Lap-ata (2010) use a slightly different formulation ofP (z|C, o).
Using the window-based context modelour formulation (5) outperforms (7) for both trainingcorpora; the Dinu and Lapata (2010) version scoresGAP = 41.5, ?b = 0.15 for the BNC corpus andGAP = 42.0, ?b = 0.15 for the combined corpus.The advantage of our formulation is statistically sig-nificant for all evaluation measures.1054Nouns Verbs Adjectives Adverbs OverallGAP ?b GAP ?b GAP ?b GAP ?b GAP ?bW5 46.0 0.16 38.9 0.14 44.0 0.18 54.0 0.22 44.8 0.17W5 + T ?
C 50.7 0.22 45.1 0.20 48.8 0.24 55.9 0.24 49.5 0.23Thater et al (2010) (Model 1) 46.4 ?
45.9 ?
39.4 ?
48.2 ?
44.6 ?Thater et al (2010) (Model 2) 42.5 ?
?
?
43.2 ?
51.4 ?
?
?Dinu and Lapata (2010) (LDA) ?
0.16 ?
0.14 ?
0.17 ?
0.21 ?
0.16Dinu and Lapata (2010) (NMF) ?
0.15 ?
0.14 ?
0.16 ?
0.26 ?
0.16Table 6: Performance by part of speechTable 6 gives a breakdown of performance by tar-get part of speech for the BNC+Wikipedia-trainedW5 and W5 + T ?
C models, as well as figuresprovided by previous researchers.7 W5 + T ?
Coutperforms W5 on all parts of speech using bothevaluation metrics.
As remarked above, previous re-searchers have used the corpus in slightly differentways; we believe that the results of Dinu and Lapata(2010) are fully comparable, while those of Thater etal.
(2010) were attained on a slightly smaller datasetwith parameters set through cross-validation.
Theresults for W5 + T ?
C outperform all of Dinuand Lapata?s per-POS and overall results except fora slightly superior score on adverbs attained by theirNMF model (?b = 0.26 compared to 0.24).
Turn-ing to Thater et al, we report higher scores for ev-ery POS with the exception of the verbs where theirModel 1 achieves 45.9 GAP compared to 45.1; theoverall average for W5 + T ?
C is substantiallyhigher at 49.5 compared to 44.6.
On balance, wesuggest that our models do have an advantage overthe current state of the art for lexical substitution.6 ConclusionIn this paper we have proposed novel methods formodelling the effect of context on lexical mean-ing, demonstrating that information about syntacticcontext and textual proximity can fruitfully be inte-grated to produce state-of-the-art models of lexicalchoice.
We have demonstrated the effectiveness ofour techniques on two datasets but they are poten-tially applicable to a range of applications where se-mantic disambiguation is required.
In future work,7The overall average GAP for Thater et al (2010) does notappear in their paper but can be calculated from the score andnumber of instances listed for each POS.we intend to adapt our approach for word sense dis-ambiguation as well as related domain-specific taskssuch as gene name normalisation (Morgan et al,2008).
A further, more speculative direction for fu-ture research is to investigate more richly structuredmodels of context, for example capturing correla-tions between words in a text within a frameworksimilar to the Correlated Topic Model of Blei andLafferty (2007) or more explicitly modelling poly-semy effects as in Reisinger and Mooney (2010).AcknowledgementsWe are grateful to the EMNLP reviewers for theirhelpful comments.
This research was supported byEPSRC grant EP/G051070/1.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-10), Cambridge, MA.David M. Blei and John D. Lafferty.
2007.
A correlatedtopic model of science.
The Annals of Applied Statis-tics, 1(1):17?35.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the ACL-06 Interactive Presentation Sessions,Sydney, Australia.Samuel Brody and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of EACL-09, Athens,Greece.1055Stephen Clark, Ann Copestake, James R. Curran, YueZhang, Aurelie Herbelot, James Haggerty, Byung-GyuAhn, Curt Van Wyk, Jessika Roesner, Jonathan Kum-merfeld, and Tim Dawborn.
2009.
Large-scale syn-tactic processing: Parsing the web.
Technical report,Final Report of the 2009 JHU CLSP Workshop.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-10), Cambridge,MA.Katrin Erk and Sebastian Pado?.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-08),Honolulu, HI.Edward Grefenstette, Mehrnoosh Sadrzadeh, StephenClark, Bob Coecke, and Stephen Pulman.
2011.
Con-crete sentence spaces for compositional distributionalmodels of meaning.
In Proceedings of the 9th In-ternational Conference on Computational Semantics(IWCS-11), Oxford, UK.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representation.Psychological Review, 114(2):211?244.Kenneth E. Harper.
1965.
Measurement of similarity be-tween nouns.
In Proceedings of the 1965 InternationalConference on Computational Linguistics (COLING-65), New York, NY.Geoffrey E. Hinton.
2002.
Training products of expertsby minimizing contrastive divergence.
Neural Com-putation, 14(8):1771?1800.Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.Kazuaki Kishida.
2005.
Property of average precisionand its generalisation: An examination of evaluationindicator for information retrieval experiments.
Tech-nical Report NII-2005-014E, National Institute of In-formatics, Tokyo, Japan.Thomas K Landauer and Susan T Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240.Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010.Topic models for word sense disambiguation andtoken-based idiom detection.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics (ACL-10), Uppsala, Sweden.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics (ACL-08), Columbus, OH.Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,Aaron M Cohen, Juliane Fluck, Patrick Ruch, AnnaDivoli, Katrin Fundel, Robert Leaman, Jo?rg Haken-berg, Chengjie Sun, Heng hui Liu, Rafael Torres,Michael Krauthammer, William W Lau, HongfangLiu, Chun-Nan Hsu, Martijn Schuemie, K. BretonnelCohen, and Lynette Hirschman.
2008.
Overview ofBioCreative II gene normalization.
Genome Biology,9(Suppl 2).Paul Nulty and Fintan Costello.
2010.
UCD-PN: Select-ing general paraphrases using conditional probability.In Proceedings of the 5th International Workshop onSemantic Evaluation (SemEval-2), Uppsala, Sweden.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL-10), Uppsala, Sweden.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 31st Annual Meeting of the Associationfor Computational Linguistics, Columbus, OH.Joseph Reisinger and Raymond Mooney.
2010.
A mix-ture model with sharing for lexical semantics.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-10),Cambridge,MA.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent Dirichlet alocation method for selectional prefer-ences.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL-10), Uppsala, Sweden.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In Pro-ceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-99), CollegePark, MD.Sebastian Rudolph and Eugenie Giesbrecht.
2010.
Com-positional matrix-space models of language.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-10), Uppsala,Sweden.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structures inconnectionist systems.
Artificial Intelligence, 46(1?2):159?216.Karen Spa?rck Jones.
1964.
Synonymy and SemanticClassification.
Ph.D. thesis, University of Cambridge.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-1056ing syntactically enriched vector models.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics (ACL-10), Uppsala, Swe-den.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.Hanna Wallach, David Mimno, and Andrew McCallum.2009.
Rethinking LDA: Why priors matter.
In Pro-ceedings of NIPS-09, Vancouver, BC.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.
In Proceedingsof the 15th ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining (KDD-09),Paris, France.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th Conference on Computational Linguistics(COLING-00), Saarbru?cken, Germany.1057
