PREL IMINARY EVALUATION OF THE VOYAGER SPOKEN LANGUAGE SYSTEM*Victor Zue, James Glass, David Goodine, Hong Leung,Michael Phillips, Joseph Polifroni, and Stephanie SeneffSpoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, Massachusetts 02139I -ABSTRACTVOYAGER is a speech understanding system currently under development a MIT.
It provides informationand navigational ssistance for a geographical rea within the city of Cambridge, Massachusetts.
Recently,we have completed the initial implementation f the system.
This paper describes the preliminary evaluationof VOYAGEi% using a spontaneous speech database that was also recently collected.INTRODUCTIONOne of the important factors that have contributed to the steady progress of the Strategic ComputingSpeech program has been the establishment of standardized performance evaluation procedures \[1\].
Withthe use of common databases and metrics, we have been able to objectively assess the relative merits ofdifferent approaches and systems.
These practices have also had a positive influence on the natural languagecommunity in that databases and rigorous evaluation procedures for natural language systems are beginningto emerge.
As we move towards combining speech recognition and natural anguage technology to achievespeech understanding, it is essential that the issue of performance evaluation again be addressed early on,so that progress can be monitored and documented.
Since the Spoken Language Systems program is in itsinfancy, we do not as yet have a clear idea of how spoken langauge systems hould be evaluated.
Naturally,we should be able to benefit from hands-on experience with applying some candidate performance measuresto working systems.
The purpose of this paper is to document our experience with the preliminary evaluationof the VOYAGEa system currently under development a MIT, so that we may contribute to the evolutionaryprocess of defining the appropriate evaluation measures.VOYAGER is a speech understanding system that can provide information and navigational ssistancefor a geographical rea within the city of Cambridge, Massachusetts.
The components of the system aredescribed in a companion paper \[2\].
To evaluate VOYAGER we made use of a spontaneous speech databasethat we have recently collected consisting of nearly 10,000 sentences from 100 speakers.
The database isdescribed in another companion paper \[3\].EVALUATION ISSUESWe believe that spoken language systems hould be evaluated along several dimensions.
First, theaccuracy of the system and its various modules hould be documented.
Thus, for example, one can measurea given system's phonetic, word, and sentence accuracy, as well as linguistic and task completion accuracy.Second, one must measure the coverage and habitability of the system.
This can be applied to the lexicon,the language model, and the application back-end.
Third, the system's flexibility must be established.
For*This research was supported by DARPA under Contract N00014-89-J-1332, monitored through the Office of Naval Research.160example, how easy is it to add new knowledge to the system?
How difficult is it to port the system to adifferent application?
Finally, the e~iciency of the system should be evaluated.
One such measure may bethe task completion time.Whether we want to evaluate the accuracy of a spoken language system in part or as a whole, we mustfirst establish what the reference should be.
For example, determining word accuracy for speech recdgnizersrequires that the reference string of words first be transcribed.
Similarly, assessing the appropriateness of asyntactic parse presupposes that we know what the correct parse is.
In some cases, establishing the referenceis relatively straightforward and can be done almost objectively.
In other cases, such as specifying the correctsystem response, the process can be highly subjective.
For example, should the correct answer to the query," Do you know of any Chinese restaurants?"
be simply, "Yes," or a list of the restaurants that the systemknows?It is important o point out that at no time is a human totally out of the evaluation loop.
Even forsomething as innocent as word accuracy, we rely on the judgement of the transcriber for ambiguous eventssuch as "where is," versus "where's," or "I am" versus "I'm."
Therefore, the issue is not whether the referenceis obtained objectively, but the degree to which the reference is tainted by subjectivity.The outputs of the system modules naturally become more general at the higher levels of the system sincethese outputs represent more abstract information.
Unfortunately, this makes an automatic omparison witha reference output more difficult, both because the correct response may become more ambiguous and becausethe output representation must become more flexible.
The added flexibility that is necessary to express moregeneral concepts also allows a given concept o be expressed in many ways, making the comparison with areference more difficult.To evaluate these higher levels of the system, we will either have to restrict he representation a d answersto be ones that are unambiguous enough to evaluate automatically, or adopt less objective valuation criteria.We feel it is important not to restrict the representations and capabilities of the system on account of aninflexible evaluation process.
Therefore, we have begun to explore the use of subjective valuations of thesystem where we feel they are appropriate.
For these evaluations, rather than automatically comparingthe system response to a reference output, we present he input and output to human subjects and givethem a set of categories for evaluating the response.
At some levels of the system (for example evaluatingthe appropriateness of the response of the overall system) we have used subjects who were not previouslyfamiliar with the system, since we are interested in a user's evaluation of the system.
For other componentsof the system, such as the translation from parse to action, we are interested in whether they performed asexpected by their developers, o we have evaluated the output of these parts using people familiar with theirfunction.In the following section, we present he results of applying various evaluation procedures to the VOYAGERsystem.
We don't profess to know the answers regarding how performance evaluation should be achieved.By simply plunging in, we hope to learn something from this exercise.PERFORMANCE EVALUATIONOur evaluation of the VOYAGER system is divided into four parts.
The SUMMIT speech recognitionsystem is independently evaluated for its word and sentence accuracy.
The TINA natural anguage systemis evaluated in terms of its coverage and perplexity.
The accuracy of the commands generated by the backend is determined.
Finally, the appropriateness of the overall system response is assessed by a panel of naivesubjects.
Unless otherwise specified, all evaluations were done on the designated test set \[3\], consisting of 485and 501 spontaneous and read sentences, respectively, spoken by 5 male and 5 female subjects.
The averagenumber of words per sentence is 7.7 and 7.6 for the spontaneous and read speech test sets, respectively.161SpontaneousConditionReadFigure 1: Word and sentence accuracy for the spontaneous and read speech test sets.SPEECI I  RECOGNIT ION PERFORMANCEThe SUMMIT speech recognition system that we evaluated is essentially the same as the one we describedduring the last workshop \[4\], with the exception of a new training procedure as described elsewhere \[2\].Since the speech recognition and natural anguage components are not as yet fully integrated, we currentlyuse a word-pair grammar to constrain the search space.
The vocabulary size is 570 words, and the test setperplexity and coverage are 22 and 65% respectively3 Figure 1 displays the word and sentence accuracy forSUMMIT on both the spontaneous and read speech test sets.
For word accuracy, substitutions, insertions anddeletions are all included.
For sentence accuracy, we count as correct sentences where all the words wererecognized correctly.
We have included only those sentences that pass the word-pair grammar, following thepractice of past Resource Management evaluations.
However, overall system results are reported on all thesentences.
For spontaneous speech, we broke down the results into three categories: sentences that containpartial words, sentences that contain filled pauses, and uncontaminated sentences.
These results are shownin Figure 2.
Since we do not explicitly model these spontaneous speech events, we expected the performanceof the system to degrade.
However, we were somewhat surprised at the fact that the read speech results werevery similar to the spontaneous speech ones (Figure 1).
One possible reason is that the speaking rate forthe read speech test set is very high, about 295 words/min compared to 180 words/rain for the spontaneousspeech and 210 words/rain for the Resource Management February-89 test set.
The read speech sentenceswere collected uring the last five minutes of the recording session.
Apparently, the subjects were anxiousto complete the task, and we did not explicitly ask them to slow down.NATURAL LANGUAGE PERFORMANCEFollowing data collection, TINA's arc probabilities were trained using the 3,312 sentences from the desig-nated training set \[5\].
The resulting coverage and perplexity for the designated evelopment set are shown1The vocabulary in this case is larger than that for the entire system.
The latter is the intersection of the recognitioncomponent's vocabulary with that of the natural language component.1621??
I6~41?
Word Accuracy\ [ \ ]  Sentence ACc, lr'~ev66.686.90.0Partial Words Filled Pause No Non-Speech(1.5 %) (4.5 %) (94 %)ConditionFigure 2: Breakdown of word and sentence accuracy for the spontaneous speech test sets, depending onwhether the sentences contain false starts or filled pauses.in the top row of Table 1.
The left column gives the perplexity when all words that could follow a given wordare considered equally likely.
The middle column takes into account he probabilities on arcs as establishedfrom the training sentences.
The right column gives overall coverage in terms of percentage of sentences thatparsed.Examination of the training sentences led to some expansions of the grammar and the vocabulary toinclude some of the more commonly occurring patterns/words that had originally been left out due tooversight.
These additions led to an improvement in coverage from 69% to 76%, as shown in Table 1,but with a corresponding increase in perplexity.
This table also shows the performance of the expandedsystem on the training set.
The fact that there is little difference between this result and the result on thedevelopment set suggests that the training process is capturing appropriate generalities.
The final row givesperplexity and coverage for the test set.
The coverage for this set was somewhat lower, but the perplexitieswere comparable.Note also that perplexity as computed here is an upper bound measurement on the actual constraintprovided.
In a parser many long-distance constraints are not detected until long after the word has beenincorporated into the perplexity count.
For instance, the sentence "What does the nearest restaurant serve?
"would license the existence of "does" as a competitor for "is" following the word "what."
However, if "does"is actually substituted for "is" incorrectly in the sentence "What is the nearest restaurant?"
the parse wouldfail at the end due to the absence of a predicate.
It is difficult to devise a scheme that could accuratelymeasure the gain realized in a parser due to long-distance memory that is not present in a word-pair grammar.The above results were all obtained irectly from the log file, as typed in by the experimenter.
We alsohave available the orthographic transcriptions for the utterances, which included false starts explicitly.
Weran a separate xperiment on the test set in which we used the orthographic transcription, after strippingaway all partial words and non-words.
We found a 2.5% reduction in coverage in this case, presumably dueto back ups after false starts.Of course, we have not yet taken advantage of the constraint provided by TINA, except in an accept/rejectmode for recognizer output.
We expect TINA'S low perplexity to become an important factor for search spacereduction and performance improvement once the system is fully integrated.163Initial SystemNo-Prob ~r\]bDevelopment Set: 20.6 ICoverage69%Expanded SystemNo-ProbDevelopment Set: 27.1Training Set: 25.8Test Set: 26.0Prob Coverage8.3 76%8.1 78%8.2 72.5%Table 1: Perplexity and coverage for TINA for a number of different conditions.SYSTEM PERFORMANCEVOYAGER'S overall performance was evaluated in several ways.
In some cases, we used automatic meansto measure performance.
In others, we used the expert opinion of system developers to judge the correctnessof intermediate r presentations.
Finally, we used a panel of naive users to judge the appropriateness of theresponses of the system as well as the queries made by the subjects.Automated  Eva luat ionVOYAGER'S responses to sentences can be divided into three categories.
For some sentences, no parse isproduced, either due to recognizer errors, unknown words, or unseen linguistic structures.
For others, noaction is generated ue to inadequacies of the back end.
Some action is generated for the remainder of thesentences.
Figure 3 show the results on the spontaneous speech test set.
The system failed to generate aparse for one reason or another on two-thirds of the sentences.
Of those, 26% were found to contain unknownwords.
VOYAGER almost never failed to provide a response once a parse had been generated.
This is a directresult of our conscious decision to constrain TINA according to the capabilities of the back end.For diagnostic purposes, we also examined VOYAGER's responses when orthography, rather than speech,was presented to the system, after partial words and non-words had been removed.
The results are alsoshown in Figure 3.
Comparing the two sets of numbers, we can conclude that 30% of the sentences wouldhave failed to parse even if recognized correctly, and an additional 36% of the sentences failed to generatean action due to recognition errors or the system's inability to deal with spontaneous speech phenomena.Even if a response was generated, it may not have been the correct response.
It is difficult to know howto diagnose the quality of the responses, but we felt it was possible to break up the analysis into two parts,one measuring the performance of the portion of the system that translates the sentence into functions andarguments and the other assessing the capabilities of the back end.
For the first part, we had two experts whowere well informed on the functionalities in the back end assess whether the function calls generated by theinterface were complete and appropriate.
The experts worked as a committee and examined all the sentencesin the test set for which an action had been generated.
They agreed that 97% of the functions generatedwere correct.
Most of the failures were actually due to inadequacies in the back end.
For example, the backend had no mechanism for handling the quantifier "other" as in "any other restaurants," and therefore thisword was ignored by the function generator, esulting in an incomplete command specification.Human EvaluationFor the other half of the back end evaluation, we decided to solicit judgments from naive subjects whohad had no previous experience with VOYAGER.
We decided to have the subjects categorize both system1648~?
Speech\ [ \ ]  Orthography.4 .2No Parse No Action ActionResponseFigure 3: A breakdown of system performance for speech and orthographic nput.responses and user queries as to their appropriateness.
System responses came in two forms, a direct responseto the question if the system thought it understood, or an admission of failure and an attempt o explainwhat went wrong.
Subjects were asked to judge answers as either "appropriate," "verbose," or "incorrect,"and to judge error messages as either "appropriate" or "ambiguous."
In addition, they were asked to judgequeries as "reasonable," "ambiguous," "ill-formed," or "out-of-domain."
Statistics were collected separatelyfor the two conditions, "speech input" and "orthographic input."
In both cases, we threw out sentencesthat had out-of-vocabulary words or no parse.
We had three subjects judge each sentence, in order to assessinter-subject agreement.Table 2 shows a breakdown (in percentage) of the results, averaged across three subjects.
The columnsrepresent the judgement categories for the system's responses, whereas the rows represent judgement cate-gories for the user queries.
A comparison of the last row of the two conditions reveals that the results arequite consistent, presumably because the majority of the incorrectly recognized sentences are rejected by theparser.
About 80% of the sentences were judged to have an appropriate response, with an additional 5%being verbose but otherwise correct.
Only about 4% of the sentences produced error messages, for whichthe system was judged to give an appropriate response about two thirds of the time.
The response wasjudged incorrect about 10% of the time.
The table also shows that the subjects judged about 87% of theuser queries to be reasonable.In order to assess the reliability of the results, we examined the agreement in the judgements providedby the subjects.
For this limited experiment, at least two out of three subjects agreed in their judgementsabout 95% of the time.SUMMARYIn this paper we presented some results on the preliminary evaluation of the VOYAGER system.
As wehave stated at the onset, we are entering into a new era of research, and we do not have a clear idea ofhow spoken language systems hould best be evaluated.
However, we have chosen to explore this issue alongseveral dimensions.
We have reached the conclusion that a totally objective measure of performance may not165answer answer error error responseappropriate verbose appropriate ambiguous incorrectambiguous 5.1 0.3 0.9 0.9ill-formed 2.4 0.3 0.9 1.8out of domain 0.6reasonable 69.9 4.5 0.9 1.8 9.6total 78.0 5.1 2.7 1.8 12.3(a) Speech Inputanswer answer error error responseappropriate verbose appropriate ambiguous incorrectambiguous 5.5 0.3 1.0 0.1 0.7ill-formed 2.2 0.1 0.1 1.4out of domain 0.6 0.1 0.1reasonable 72.1 5.0 1.4 1.0 8.0total 80.4 5.4 2.6 1.1 10.2(b) Orthographic Inputtotal7.25.40.686.7total7.63.80.887.5Table 2: Breakdown of subjective judgements on system responses and user queries for (a) speech input,and (b) orthographic input.be possible now that systems have become more complex.
While some objective criteria exist for individualcomponents, overall system performance should probably incorporate subjective judgements as well.Thus far, we have not addressed the issue of efficiency, mainly because we have not focussed our attentionon that issue.
When VOYAGER was first developed, it ran on a Symbolics Lisp machine, and took severalminutes to process a sentence.
More recently, we have started to use general signal processing boards toderive the auditory-based signal representation, and a Sun workstation to implement the remainder of theSUMMIT recognition system.
Currently, the system runs in about 12 times real-time.
The approximatebreakdown in timing is shown in Table 3.
Note that the natural language component and the back endrun in well under real-time.
Refined algorithms, along with the availability of faster workstations and morepowerful signal processing chips should enable the current VOYAGER implementation to run in real-time inthe future.
On the other hand, the computation is likely to increase dramatically when speech recognitionand natural anguage are fully integrated, since many linguistic hypotheses must be pursued in parallel.References\[1\] Pallett, D. "Benchmark Tests for DARPA Resource Management Database Performance Evaluation,"Proc.
ICASSP-89, pp.
536-539, Glasgow, Scottland, 1989.\[2\] Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, J., and Seneff, S., "The VOYAGERSpeech Understanding System: A Progress Report," These Proceedings.\[3\] Zue, V., Daly, N., Glass, J., Leung, H., Phillips, M., Polifroni, J., Seneff, S., and Soclof, M., "TheCollection and Preliminary Analysis of a Spontaneous Speech Database," These Proceedings.166Components Timing (x RT)Speech RecognitionSignal Representation 2.5Phonetic Recognition 4Lexical Access 5Natural Language .2Back End .2Table 3: Breakdown i  computation for VOYAGER components.\[4\] Zue, V., Glass, J., Phillips, M., and Seneff, S., "The MIT SUMMIT Speech Recognition System: AProgress Report," Proceedings of the First DARPA Speech and Natural Language Workshop, pp.
178-189, February, 1989.\[5\] Seneff, S., "TINA: A Probabilistic Syntactic Parser for Speech Understanding Systems," Proceedings ofthe First DARPA Speech and Natural Language Workshop, pp.
168-178, February, 1989.167
