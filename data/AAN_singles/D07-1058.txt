Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
551?560, Prague, June 2007. c?2007 Association for Computational LinguisticsParsimonious Data-Oriented ParsingWillem ZuidemaInstitute for Logic, Language andComputation, University of AmsterdamPlantage Muidergracht 241018 TV, Amsterdam, the Netherlandsjzuidema@science.uva.nlAbstractThis paper explores a parsimonious ap-proach to Data-Oriented Parsing.
While al-lowing, in principle, all possible subtreesof trees in the treebank to be productiveelements, our approach aims at finding amanageable subset of these trees that canaccurately describe empirical distributionsover phrase-structure trees.
The proposedalgorithm leads to computationally muchmore tracktable parsers, as well as linguis-tically more informative grammars.
Theparser is evaluated on the OVIS and WSJcorpora, and shows improvements on effi-ciency, parse accuracy and testset likelihood.1 Data-Oriented ParsingData-Oriented Parsing (DOP) is a framework forstatistical parsing and language modeling originallyproposed by Scha (1990).
Some of its innovations,although radical at the time, are now widely ac-cepted: the use of fragments from the trees in anannotated corpus as the symbolic grammar (nowknown as ?treebank grammars?, Charniak, 1996)and inclusion of all statistical dependencies betweennodes in the trees for disambiguation (the ?all-subtrees approach?, Collins & Duffy, 2002).The best known instantiations of the DOP-framework are due to Bod (1998; 2001; 2003),using the Probabilistic Tree Substitution Grammar(PTSG) formalism.
Bod has advocated a maximal-ist approach to DOP, inducing grammars that con-tain all subtrees of all parse trees in the treebank,and using them to parse unknown sentences whereall of these subtrees can potentially contribute to themost probable parse.
Although Bod?s empirical re-sults have been excellent, his maximalism poses im-portant computational challenges that, although notnecessarily unsolvable, threaten both the scalabilityto larger treebanks and the cognitive plausibility ofthe models.In this paper I explore a different approach toDOP, that I will call ?Parsimonious Data-OrientedParsing?
(P-DOP).
This approach remains true toScha?s original program, by allowing, in principle,all possible subtrees of trees in the treebank to bethe productive elements.
But unlike Bod?s approach,P-DOP aims at finding a succinct subset of such el-ementary trees, chosen such that it can still accu-rately describe observed distributions over phrase-structure trees.
I will demonstrate that P-DOP leadsto computationally more tracktable parsers, as wellas linguistically more informative grammars.
More-over, as P-DOP is formulated as an enrichmentof the treebank Probabilistic Context-free Grammar(PCFG), it allows for much easier comparison to al-ternative approaches to statistical parsing (Collins,1997; Charniak, 1997; Johnson, 1998; Klein andManning, 2003; Petrov et al, 2006).2 Independence Assumptions in PCFGsParsing with treebank PCFGs, in its simplest form,involves the following steps: (1) a treebank is cre-ated by extracting phrase-structure trees from an an-notated corpus, and split in a train- and a testset;(2) a PCFG is read off from all productions in thetrainset trees, with weights proportional to their fre-551quency in the treebank (the ?relative frequency esti-mate?
); (3) a standard PCFG parser is used to findfor each yield of the test-set trees the most probableparse; (4) these parses are compared to the test-settrees to count matching brackets, labels and trees.PCFGs incorporate a strong statistical indepen-dence assumption: that the expansion of a nonter-minal node is only dependent on the node?s label.All state-of-the-art wide-coverage parsers relax thisassumption in some way, for instance by (i) chang-ing the parser in step (3), such that the applicationof rules is conditioned on other steps in the deriva-tion process (Collins, 1997; Charniak, 1997), orby (ii) enriching the nonterminal labels in step (1)with context-information (Johnson, 1998; Klein andManning, 2003), along with suitable backtransformsin step (4).
These two approaches often turn out tobe equivalent, although for some conditionings it isnot trivial to work out the equivalent enrichment andvice versa, especially when combined with smooth-ing.
Interesting recent work has focused on the au-tomatic induction of enrichments (Matzuzaki et al,2005; Prescher, 2005), leading to extremely accurateparsers (Petrov et al, 2006).DOP relaxes the independence assumption bychanging the class of probabilistic grammars in-duced in step (2).
In DOP1 (Bod, 1998), a PTSGis induced, which consists, subject to some heuris-tic constraints, of all subtrees1 of the treebanktrees with a weight proportional to their frequency.PTSGs allow multiple derivations to yield the sameparse; in DOP1 the sum of their probabilities givesthe probability of the parse.
The relation betweenDOP and enrichment/conditioning models was clar-ified by Joshua Goodman, who devised an efficientPCFG transform of the DOP1 model (Goodman,1996).
The size of the PCFG resulting from thistransform is linear in the number of nonterminals to-kens in the corpus.
Goodman?s transform, in com-bination with a range of heuristics, allowed Bod(2003) to run the DOP model on the Penn TreebankWSJ benchmark and obtain some of the best resultsobtained with a generative model.The computational challenges for DOP are farfrom solved, however.
The difference with style1A subtree t?
of a parse tree t is a tree such that every nodei?
in t?
equals a node i in t, and i?
either has no daughters or thesame daughter nodes as i.
(ii) enrichment is that we derive many more rulesfrom every original tree than the number of CFG-productions it contains.
This is one reason why therelative frequency estimator for DOP is inconsistent(Johnson, 2002).
But worse, perhaps, the size of thegrammar remains gigantic2 , making it difficult formany in the field to replicate Bod?s results.In this paper, we develop a parsimonious ap-proach to DOP, that avoids many of the computa-tional problems of the maximalist approach but triesto maintain its excellent empirical performance.
Ourapproach starts, both conceptually and technically,with an analysis of where the PCFG independenceassumption breaks down when modeling empiricaldistributions.
In section 2 we derive equations forthe expected frequency of arbitrary subtrees under adistribution defined by a given PCFG, and use themto measure how much observed subtree-frequenciesdeviate from expectation.
In section 4 we generalizethis analysis to PTSGs.
In section 5 we discuss an al-gorithm for estimating PTSGs from a treebank, thatis based on minimizing the differences between ex-pected and observed subtree-frequencies.
We thenproceed with discussing PTSGs induced from var-ious treebanks, and in section 6 the use of thesePTSGs for parsing.3 Deviations from a PCFG distributionPCFGs can be viewed as PTSGs where the elemen-tary trees are restricted to depth 1; we therefore startby repeating the definition of PTSGs (Bod, 1998),and use notation appropriate for PTSGs throughout.An PTSG is a 5-tuple ?Vn, Vt, S, T, w?, where Vn isthe set of non-terminal symbols; Vt is the set of ter-minal symbols; S ?
Vn is the start symbol; T is a setof elementary trees, such that for every ?
?
T theunique root node r(?)
?
Vn, the (possibly empty)set of internal nodes i(?)
?
Vn and the set of leafnodes l(?)
?
Vn ?
Vt; finally, w : T ?
[0, 1] is aprobability (weight) distribution over the elementarytrees, such that for any ?
?
T , ??
??R(?)
w(?
?)
= 1,where R(?)
is the set of elementary trees with thesame root label as ?
.
It will prove useful to alsodefine the set of all possible trees ?
over the defined2Sections 2-21 of WSJ contain 1676821 productions.
Ofthese,106 are lexical productions, and 36151 top-productions,leaving approx.
640000 internal productions which yield about2.5 ?
106 rules in Goodman?s transform.552alphabets (with the same conditions on root, internaland leaf nodes as for T ), and the set of all possiblecomplete parse trees ?
(with r(t) = S and all leafnodes l(t) ?
Vt).
Obviously, T ?
?
and ?
?
?.The substitution operation ?
is defined if the left-most nonterminal leaf in ?1 is identical to the rootof ?2.
Performing substitution ?1 ?
?2 yields t3, if t3is identical to ?1 with the leftmost nonterminal leafreplaced by ?2.
A derivation is a sequence of ele-mentary trees, where the first tree ?
?
T has root-label S and every next tree combines through sub-stitution with the result of the substitutions beforeit.
In this paper, we are only concerned with gram-mars that define proper probability distributions overtrees, such that the probability of all derivations sumup to 1 and no probability mass gets lost in deriva-tions that never reach a terminal yield.
That is, werequire (if t(d) is the tree derived by derivation d):?d:t(d)?
?P (d) = 1.
(1)For simplicity, but without loss of generality, we as-sume there are no recursions on the start symbol.In this section, we restrict ourselves to PCFG dis-tributions, and thus to a T with only depth 1 trees.The probability of a PCFG rule (conditioned on itsleft-hand side) in the conventional notation, P (A 7???
.
.
.
?|A), now corresponds to the probability of adepth 1 tree (conditioned on its root nonterminal):P??A?
?
.
.
.
?|A?
?Of course, the probability of a (complete) deriva-tion is simply the product of the (conditional) prob-abilities of the rules in the derivation.
It is useful toconsider, for a given grammar G generating a cor-pus of N trees, the expected frequency of visitingnonterminal state X:EF (X) ={N if X = S??
EF (?
)C(X, l(?))
otherwise(2)where C(X, l(?))
gives the number of occurrencesof nonterminal X among the leaves of elementarytree ?
.
Furthermore, the expected usage frequencyof ?
is given byEF (?)
= EF (r(?
))P (?
|r(?
))= EF (r(?))w(?)
(3)Substituting eq (3) into (2) yields a system of|Vn| linear equations, that can be straightforwardlysolved using standard methods.We are interested in the empirical deviations fromthe distribution defined by a given grammar (for in-stance, the treebank PCFG), such that we can adjustthe grammar to better model the training data (whilstavoiding overfitting).
In line with the general DOPapproach, we would like to measure this deviationfor every possible subtree.
Of course, the condi-tional probability of an arbitrary subtree is simplythe product of the rule probabilities.
The expectedfrequency of a subtree is the expected frequency ofits root state, times the conditional probability:EF (t) = EF (r(t))P (t|r(t)) (4)Using these equations, we can measure for eachobserved subtree in the corpus, the difference be-tween observed frequency and expected frequency.This will give high values for overrepresented andfrequent constructions in the corpus, such as sub-trees corresponding to revenues rose CD % to $ CDmillion from $ CD million last year, details weren?tdisclosed, NP-SUBJ declined to comment and con-tracted and negated auxiliaries such as won?t, can?tand don?t.
The top-10 overrepresented subtrees inthe WSJ20-corpus are given in figure 1.VPVBD?SAID?SBARSTOPSCC?BUT?S@1SCC?BUT?S@1NP-SBJNNP?MR.
?NNPSBARIN?THAT?SNPNP PPIN NPPPIN?OF?NPPP-LOCIN?IN?NPVP@1RB?N?T?VPTOPSCC S@1NP-SBJ S@2Figure 1: Top-10 overrepresented subtrees (excluding subtreeswith punctuation) in sentences of length ?
20, including punc-tuation, in sections 2-21 of the WSJ-corpus (transformed toChomsky Normal Form, whereby newly created nonterminalsare marked with an @).
Measured are the deviations fromthe expected frequencies according to the treebank PCFG (ofthis selection), as in equation (4) but with EF (r(t)) replacedby the empirical frequency o(r(t)).
Observed frequencies are(deviations between brackets): 461 (+408.2), 554 (+363.8),556 (+361.7), 479 (+348.2), 332 (+314.3), 415 (+313.3), 460(+305.1), 389 (+283.0), 426 (+277.2), 295 (+266.1).Of course, there are also many subtrees that oc-cur much less frequently than the grammar predicts,such as for instance subtrees corresponding to in-frequent or non-occurring variations of the frequent553ones, e.g.
revenues rose CD from $ CD million from$ CD million.
Underrepresented subtrees found inthe WSJ20 corpus, include (VP (VBZ ?IS?)
NP)),which occurs only once, even though it is predicted152.7 times more often (in all other VP?s with ?IS?,the NP is labeled NP-PRD); and (PP (IN ?IN?)
NP)),which occurs 38 times but is expected 121.0 timesmore often (IN NP-constructions are usually labeledPP-LOC).Given such statistics, how do we improve thegrammar such that it better models the data?
PCFGenrichment models (Klein and Manning, 2003;Schmid, 2006) split (and merge) nonterminals;in automatic enrichment methods (Prescher, 2005;Petrov et al, 2006) these transformations are per-formed so as to maximize data likelihood (undersome constraints).
The treebank PCFG-distributionthereby changes, such that the deviations from fig-ure 1 mostly disappear.
For instance, the overrepre-sentation of ?but?
as the sentence-initial CC in thesecond and third subtree of that figure, is dealt within (Schmid, 2006) by splitting the CC-category intoCC/BUT and CC/AND.
However, also when a rangeof such transformations is applied, some subtrees arestill greatly overrepresented.
Figure 2 gives the top-10 overrepresented subtrees of the same treebank,enriched with Schmid?s enrichment program tmod.In DOP, larger subtrees can be explicitly repre-sented as units.
This is the approach we take inthis paper, which involves switching from PCFGsto PTSGs.
However, we cannot simply add over-represented trees to the treebank PCFG; as is clearfrom figure 2, many of the overrepresented subtreesare in fact spurious variations of the same construc-tions (e.g.
?$ CD million?, ?a JJ NN?).
To reach ourgoal of finding the minimal set of subtrees that ac-curately models the empirical distribution over trees,we will thus need to consider a series of PTSGs, findthe subtrees that are still overrepresented and adaptthe grammar accordingly.4 Deviations from an PTSG distribution4.1 Expected Frequencies: An ExampleOnce we allow T to contain elementary trees ofdepth larger than 1, the equations above becomemore difficult.
The reason is that now multiplederivations may give rise to the same parse tree, and,NP-SBJ/3S/BASENNP?MR.
?NNP@1QP/$$ QP/$@1CD CD@1?MILLION?NP-SBJ/BASENNP?MR.
?NNP@1]QP/$$?$?QP/$@1CD CD@1?MILLION?QP/$@1CD CD@1?MILLION?NP/BASEDT/A NP/BASE@1JJ NNVP/FINMD VP/FIN@1RB/NOT VP/INFNP/BASEDT/A?A?NP/BASE@1JJ NNTOPNP-SBJ/3S/BASENNP?MR.?NNP@1S/FIN/.
@1NP/BASENNP NP/BASE@1NNP@1 NP/BASE@2Figure 2: Top-10 overrepresented subtrees (excluding subtreeswith punctuation) in the WSJ20 corpus, enriched with the tmodprogram (Schmid, 2006).
Empirical frequencies are as fol-lows (deviations between brackets): 262 (+207.6), 235 (+158.4)207 (+156.4), 228 (+153.5), 237 (+141.0), 190 (+134.2), 153(+126.5), 166 (+117.8), 139 (+110.0), 111 (+103.8).as a corrolary, a specific subtree can emerge in manydifferent ways.
Consider an PTSG that consists ofall subtrees of the trees t1, t2 and t3 in figure 3, andthe expected frequency of the subtree t?.t1 =SBxAyt2 =SAxBCyDxt3 =SAyBCxDyt?
=BC DyFigure 3: Three example treebank trees and the focal subtreeIt is clear that t?
might arise in many differentways.
For instance, it emerges in the derivation withelementary trees ?1 ?
?4 ?
?5 from figure 4, but also inderivations ?2 ?
?4 and ?3 ?
?5.
Note that in none ofthese derivations elementary tree t?
itself was used.
?1 =SAxBC D?2 = SAyBC Dy?3 = SAxBCyD?4 = Cx?5 =Dy?6 =BC DFigure 4: Some elementary trees extracted from the trees in fig 34.2 Expected Frequency: Usage & OccurrenceHence, when using PTSGs, we need to distinguishbetween the expected usage frequency of an elemen-tary tree (written as Eu(?
)), and the expected occur-rence frequency (Eo(t)) of the corresponding sub-tree.
Moreover, not all nonterminal nodes in a de-554rived tree are necessarily ?visited?
substitution sites.The expected frequency of visiting a nonterminalstate X as substitution site depends on the usage fre-quencies:EF (X) ={N if X = S??
Eu(?
)C(X, l(?))
otherwise(5)Relating usage frequencies to weights is still sim-ple (compare equation 3):Eu(?)
= EF (r(?))w(?)
(6)And hence: w(?)
= Eu(?)/??
?:r(?)=r(?
?)
Eu(?
?
).The expected frequency of a complete tree is notsimply a product anymore, but the sum of the differ-ent derivation probabilities (where der(t) gives theset of derivations of t):Eo(t) =?d?der(t)???dw(?)
if t ?
?
(7)4.3 Expected Frequency of Arbitrary SubtreesMost complex is the expected occurrence frequencyof an arbitrary subtree t. From the example above itis clear that it is not necessary that the root of t is asubstitution site.
Analogous to equation (4), we needthe expected frequency of arriving at some state ?
inthe derivation process that is still consistent with ex-panding to something that contains t, and multiply itwith the probability that this expansion indeed hap-pens:Eo(t) =?
?EF (?
)P (t|?)
(8)To be able to define the states ?, we redefine theset of derivations der(t) of a subtree t, such that thederivations der(t?)
of our example tree from figure 3are the following: d1 = B ?
?6 ?
?5, d2 = ?6 ?
?5,d3 = B ?
t?
and d4 = t?.
Only if a derivation startswith a single nonterminal is the root node consid-ered a substitution site.
The states ?
correspond tothe first elements of each of these derivations, i.e.
?B, ?6, B, t?
?.As was clear from the example in section 4.1,we need to consider all supertrees of the trees inthe derivation of t for calculating the expected fre-quency of a state and the probability of expandingfrom that state to form t. It is useful to distin-guish, as do Bod & Kaplan (Bod, 1998, ch.
10) twotypes of supertree-subtree relations, depending onwhether nodes must be removed from the root down-ward, or from the leaves (?frontier?)
upward.
?Root-subtrees?
of t are those subtrees headed by any oft?s internal nodes and everything below.
?Frontier-subtrees?
are those subtrees headed by t?s root-node,pruned at any number (?
0) of internal nodes.
Using?
to indicate left-most substitution, we can write:?
t1 is a root-subtree of t1, and t1 is a root-subtreeof t2, if ?t3, such that t3 ?
t1 = t2;?
t1 is a frontier-subtree of t1, and t1 is a frontier-subtree of t2, if ?t3 .
.
.
tn, such that t1 ?
t3 .
.
.
?tn = t2.?
t?
is the x-frontier-subtree of t, t?
= fsx(t), ifx is a set of nodes in t, such that if t is prunedat each i ?
x it equals t?.We use the notation st(t) for the set of subtrees oft, rs(t) for the set of root-subtrees of t and fs(t) forthe set of frontier-subtrees of t. Thus defined, the setof all subtrees of t is the set of all frontier-subtreesof all root-subtrees of t: st(t) = {t?|?t??(t??
?rs(t) ?
t?
?
fs(t??)).
We further define the sets ofroot-supertrees, frontier-supertrees, and supertreesas follows: (i) f?
sx(t) = {t?|t = fsx(t?
)}, (ii)f?
s(t) = {t?|t ?
fs(t?)}
(iii) s?t(t) = {t?|t ?
st(t?
)}.If there are only terminals in the yield of t, the ex-pected frequency of a state ?
is now simply the sumof the expected usage frequencies of those elemen-tary trees that have ?
at their frontier (i.e.
that ?
is aroot-subtree of):EF (?)
=??
?:??rs(?
?)Eu(?
?)
if l(t) ?
Vt (9)If there are nonterminals in the yield of t, as in theexample, we need to also consider elementary treesthat have these nonterminals already expanded.
Tosee why, consider again the example of section 4.1and check that also elementary tree ?3 contributes tothe expected frequency of t?.
If we take this intoaccount, and write nt(t) for the nonterminal nodesin the yield of t, the final expression for the expectedfrequency of state ?
becomes:EF (?)
=???f?s(?)??
??
?rsnt(t)(?)Eu(?
?)
(10)555Finally, the probability of expanding a state ?such that t emerges is again simplest if t has no non-terminals as leaves.
Remember that a state ?
was thefirst element of a derivation of t; the probability ofexpanding to t is simply the product of the weightsof the remaining elementary trees in the derivation(if states are unique for a derivation):P (t|?)
=???rest(d)w(?)
if l(t) ?
Vt (11)If there are nonterminals among the leaves of t,however, we need again to sum over possible expan-sions at those nonterminal leaves:P (t|?)
=???rest(d)??
??
?fsx(t)(?)w(?
?)
(12)Substituting equations (9) and (12) into equa-tion (8) gives a general expression for the expectedoccurrence frequency of an arbitary subtree t:Eo(t) =?d?der(t)( ???rest(d)??
??
?fsx(t)(?)w(?
?)???r?s(first(d))??
??
?fsx(t)(?)Eu(?
?)).
(13)5 Minimizing deviations: estimationThe equations just derived can be used to learn anPTSG from a treebank, using an estimation proce-dure we call ?push-n-pull?
(pnp).
This procedurewas described in some detail elsewhere (Zuidema,2006b); here I only sketch the basic idea.
Givenan initial setting of the parameters (all depth 1 el-ementary trees at their empirical frequency), themethod calculates the expected frequency of allcomplete and incomplete trees.
If a tree t?s ex-pected frequency Eo(t) is higher than its observedfrequency o(t), the method subtracts the differencefrom the tree?s score, and distributes (?pushes?)
itover the elementary trees involved in all its deriva-tions (der(t)).
If it is lower, it ?pulls?
the differencefrom all its derivations.The ?score?
of an elementary tree ?
is the al-gorithm?s estimate of the usage frequency u(?
).The amounts of score that are pushed or pulled arecapped by the requirement that ??
u(?)
?
0; more-over, the learning rate parameter ?
determines thefraction of the expected-observed difference that isactually pushed or pulled.
Finally, the method in-cludes a bias (B) for moving probability mass tosmaller elementary trees, to avoid overfitting (its ef-fects become smaller as more data gets observed).Because smaller elementary trees will be involvedin other derivations as well, the push and pull opera-tions will shift probabilities between different parsetrees.
Suppose a given complete tree is the only treewith nonzero frequency of all trees that can be builtfrom the same components.
This tree will continueto ?pull?
until it has in fact reached its appropriatefrequency.
Similarly, if a given tree does have zeroobserved frequency, it will continue to leak score toother derivations with the same components.NP/BASEDT/A NP/BASE@1JJ NNNP-SBJ/3S/BASENNP?MR.
?NNP1NP/BASEDT/THE?THE?NP/BASE@1JJ NNNP-SBJ/BASENNP?MR.
?NNP1NP/GEN/BASENNP NP/GEN/BASE@1NNP1 POS?
?S?NP/PPNP/BASE PP/OF/NPIN/OF?OF?NP/BASE1NP/BASENNP NP/BASE@1NNP1 NNP2NP/BASEJJ NP/BASE@1JJ1 NNSNPQP/$$ QP/$@1CD CD1?MILLION?VP/FINMD VP/FIN@1ADVP/VRB/VVP/INFFigure 5: Top-10 elementary trees of depth>1, excluding thosewith punctuation, from running pnp on the enriched WSJ20.The output of the push-n-pull algorithm is anPTSG, with the same set of elementary trees as theDOP models of Bod (1998; 2001).
This set is verylarge.
However, unlike those existing DOP models,the score distribution over these elementary trees isextremely skewed: relatively few trees receive highscores, and there is a very long tail of trees with lowscores.
In Zuidema (2006b) we give a qualitativeanalysis of the subtrees with the highest scores as in-duced from the ATIS treebank, which include manyof its frequent constructions including show me NP,I would like NP, ights from NP to NP.
The top-10larger elementary trees that result from running pnpon a randomly selected trainset of about 8000 sen-tences of the Dutch OVIS treebank (Veldhuijzen vanZanten et al, 1999), can be glossed as: Yes, from NPto NP, No thank you very much, I want to VP-INF,556No thank you, I want PP to VP-INF, I want PP, Iwant Prep NP-LOC Prep NP-LOC, Yes please, AtCD o?clock.
In figure 5 we give the top-10 elemen-tary trees resulting from the WSJ20-corpus.Figure 6: Log-log curves of (i) subtree frequencies against rank(for 106 subtrees from WSJ20), (ii) pnp-scores against rank,and (iii) the same for the top-10000 depth>1-subtrees.Figure 6 shows some characteristics of this lastgrammar.
Shown are log-log plots, such as com-monly used to visualise the Zipf-distributions in nat-ural language.
The top curve plots log(frequency)against log(rank) for each subtree of the trees inthe corpus, which shows the approximate Zipfianbehavior.
The second curve from above plots thelog(score) against log(rank) for these same subtrees.As can be observed, the score-distribution followsthe frequency distribution only for the most frequentsubtrees (all of depth 1), but then deviates from itdownwards.
The bottom curve ?
an almost straightline in this log-log space ?
gives the log(score) vslog(rank) of subtrees with a depth>1.Figure 7: Subtree frequencies against pnp-scores, includingsubsets pnp1000 (dark/blue) and pnp10000 (light/green).Figure 7 further illustrates the difference betweenthe score- and the frequency-distributions, by plot-ting for each subtree, log-frequency (y-axis) againstlog-score (x-axis).
The subtrees clearly fall into twocategories: those where the scores correlate stronglywith frequency (the depth 1 subtrees) and the largersubtrees that vary greatly in how strong scores corre-late with frequency.
Only larger subtrees that receiverelatively high scores should be used in parsing.Weights are proportional to subtree-frequenciesin the DOP1 and related ?maximalist?
models.The differences between the frequency and score-distributions thus illustrate a very important differ-ence between maximalist and parsimonious DOP.The characteristics of the score distribution allowP-DOP to throw away most of the subtrees withoutsignificantly affecting the distribution over completeparse trees that the grammar defines.
This is the ap-proach we take for evaluating parsing performance:we take as our baseline the treebank PCFG, and thenadd the n larger elementary trees with the highestscores from our induced PTSG.6 Parsing ResultsFor our parsing results we use BitPar, a fastand freely available general PCFG parser (Schmid,2004).
In our first experiments we used the OVIScorpus, with semantic tags and punctuation re-moved, and all trees (train- and testset) binarized.As a baseline experiment, we read off the treebankPCFG as decribed in section 2.
The recall, precisionand complete match results are in table 1, labeled tb-pcfg.
For comparison, we also show the results ob-tained with two versions of the DOP model, DOP1(Bod, 1998) and DOP* (Zollmann and Sima?an,2005) on the same treebank.We ran the pnp program as described above onthe trainset, with parameters B = 1.0, ?
= 0.1 andd = 4.
This run yielded a single PTSG that was usedin 4 parsing runs.
For these experiments, we addedincreasingly many of the depth>1 elementary treesfrom the PTSG, with minimum scores of 7.0, 1.0,0.5, and 0.075.
The added elementary trees werefirst converted to PCFG rules, by labeling all inter-nal nodes with a unique address label and readingoff the CFG-productions.
Each rule received a scoreequal to the score of the elementary tree it derivedfrom.
A copy of each rule, with the label removed,was also added with a negative score, BitPar auto-557matically sums (and substracts) and normalizes thefrequency information provided with each rule.
Bit-Par was then run on the testset sentences, with theoption to output the n best parses with n = 10 bydefault.
These parses were then read in in a post-processing program, which removes address labels,sums probabilities of equivalent parses and outputsthe most probable parse for each sentence (this is thesame approximation of MPP, albeit with smaller n,as used in most of Bod?s DOP results).
The results ofthese experiments are also in table 1, labeled pnpN ,where N is the number of elementary trees added.model # rules LR LP CMtb-pcfg 3000 93.45 95.5 85.84DOP1 1.4 ?
106 (87.55)DOP* (< 50000) (87.7)pnp100 3000+100 93.63 95.65 86.55pnp763 3000+763 93.5 95.52 86.75pnp1517 3000+1517 93.78 95.83 87.36pnp11411 3000+11411 94.26 96.4 87.77Table 1: Results on the Dutch OVIS tree bank, with semantictags and punctuation removed.
Reported are evalb scores on arandom testset of 1000 sentences (a second testset of 1000 sen-tences is kept for later evaluations).
The trainset for both thetreebank grammar and the pnp program consists of the remain-ing 8049 trees.
Coverage in all cases in 989 sentences out of1000.
Results in brackets are from Zollman & Sima?an, 2005,using a different train-test set split.As these experiments show, adding larger elemen-tary trees from the induced PTSG, in order of theirassigned scores, monotonously increases the parseaccuracy of the treebank PCFG.
Although the finalgrammar is at least 5 times larger than the origi-nal treebank PCFG, and the parser therefore slower,the grammar is orders of magnitude smaller than thecorresponding maximalist DOP models and showscomparable parse accuracy.For a second set of parsing experiments, we usedthe WSJ portion of the Penn Tree Bank (Marcus etal., 1993) and Helmut Schmid?s enrichment programtmod (Schmid, 2006).
Schmid?s program enrichesnonterminal labels in the treebank, using features in-spired by (Klein and Manning, 2003).
After enrich-ment, Schmid obtained excellent parsing scores withthe treebank PCFG.
In table 2, as model tb-pcfg, wegive our baseline results.
These are slightly lowerthan Schmid?s, for two reasons: (i) our implemen-tation ignores the upper/lower case distinction, and(ii) we do not use Schmid?s word class automatonfor unknown words (the only smoothing used is thebuilt-in feature of the BitPar parser, which extractsan open-class-tag file from the lexicon file).
Becauseour interest here is in the principles of enrichmentwe have not attempted to adapt these techniques forour implementation.As before, we ran the pnp program on the train-set, the enriched sections 2-21 of the WSJ.
Forcomputational reasons, pnp is only run on treeswith a yield of length (including punctuation) ?20.
This run, which took several days on a ma-chine with 1.5Gb RAM, again produced a very largePTSG, from which we extracted the 1000 and 10000depth>1 elementary trees with the highest scoresfor parsing experiments.
Parsing and postprocess-ing is performed as before, with the MPP approxi-mated from the best n = 20 parses.
Results fromthese experiments are shown in table 2, as modelspnp1000 and pnp10000.
With a small number ofadded trees, we see a small drop in the parsing per-formance, which we interpret as evidence that ouradditions somewhat disturb the nicely tuned prob-ability distribution of the treebank PCFG withoutproviding many advantages, because the most fre-quent constructions have already been addressed inthe manual PCFG enrichment.
However, with 10000added subtrees we see an increase in parse accuracy,providing evidence that pnp has learned potentialenrichments that go beyond the manual enrichment.model LR LP F1 CMtb-pcfg 83.27 83.53 83.40 26.58pnp1000 83.20 83.47 83.33 26.70pnp10000 83.56 83.99 83.77 26.93Table 2: Results on the WSJ section of the Penn Tree Bank,where nonterminals are enriched with features using HelmutSchmid?s tmod program (Schmid, 2006).
Reported are evalbscores (ignoring punctuation) on 1699 sentences ?
100, includ-ing punctuation, from section 22.
Sections 02-21 were the trainset for the treebank PCFG; only trees with a yield (includingpunctuation) of length ?
20 were used for the pnp program.Coverage in all cases is 1691 (excluding failed parses givesF1 = 85.19 for the tb-pcfg-baseline, and 85.54 for pnp10000).In figure 8(left) we plot the difference in parseaccuracy between the treebank PCFG and our bestmodel per testset sentence.
To make the plot moreinformative, the sentences are ordered by increasing558difference.
Hence, on the left are sentences wherethe treebank PCFG scores better, and at the rightthe sentence where pnp10000 scores best.
As isclear from this graph, for most sentences there isno difference, but there are small and about equallysized subsets of sentences for which one or the othermodel scores better.
We have briefly analysed thesesentences, but not found a clear pattern.
In fig-ure 8(right) we plot in a similar way the differencein log-likelihood that the parsers assign to each sen-tence.
Here we see a clear pattern: only very fewsentences receive slightly higher likelihood underthe PCFG model.
For a good portion of the sen-tences, however, the pnp10000 model assigns themsomewhat and in some cases much higher likeli-hood.
The highest likelihood gains are due to a smallnumber of frequent multiword expressions, such as?New York Stock Exchange Composite Trading?,which P-DOP treats as a unit; all of the other gainsin likelihood are also due to the use of depth>1 ele-mentary trees, including some non-contiguous con-structions such as revenues rose CD % to $ CD mil-lion from $ CD million.-60-40-2002040600  200  400  600  800  1000  1200  1400  1600  1800F-score difference (pnp-tbg)-100102030405060700  200  400  600  800  1000  1200  1400  1600  1800Log-likelihood difference (pnp-tbg)Figure 8: Per sentence difference in f-score (left) and log-likelihood (right) of the sentences of WSJ section 22.
The x-axis gives the sentence-rank when sentences are ordered fromsmall to large on y-axis value.7 Discussion and ConclusionsWe set out to develop a parsimonious approach toData-Oriented Parsing, where all subtrees can poten-tially become units in a probabilistic grammar butonly if the statistics require it.
The grammars re-sulting from our algorithm are orders of magnitudesmaller than those used in Bod?s maximalist DOP.Although our parsing results are not yet at the levelof the best results obtained by Bod, our results in-dicate that we are getting closer and that we alreadyinduce linguistically more plausible grammars.Could P-DOP eventually not only be more effi-cient, but also more accurate than maximalist DOPmodels?
Bod has argued that the explanation forDOP?s excellent results is that it takes into accountall possible dependencies between productions ina tree, and not just those from an a-priori chosensubset (e.g.
lexical, head, parent features).
Non-head dependencies in non-contiguous natural lan-guage constructions, like more ... than, as in morefreight than cargo, are typically excluded in the en-richment/conditioning approaches discussed in sec-tion 2.
Bod wants to include any dependency a pri-ori, and then ?let the statistics decide?.Although the inclusion of all dependencies mustsomehow explain the performance difference be-tween Bod?s best generative model and manually en-riched PCFG models, this explanation is not entirelysatisfactory.
Zuidema (2006a) shows that also theestimator (Bod, 2003) uses is biased and inconsis-tent, and will, even in the limit of infinite data, notcorrectly identify many possible distributions overtrees.
This is not just a theoretical problem.
Forinstance, in the Penn Tree Bank the constructionwon?t VP is annotated as (VP (MD wo) (RB n?t) VP).There is a strong dependency between the two mor-phemes: wo doesn?t exist as an independent word,and strongly predicts n?t.
However, Bod?s estimatorwill continue to reserve probability mass for othercombinations with the same POS-tags such as wonot, even with an infinite data set only containingwill not and wo n?t.
Because in parsing the stringsare given, this particular example will not harm theparse accuracy results.
The example might be di-agnostic for other cases that do, however, and cer-tainly will have impact when DOP is used as lan-guage model.
P-DOP, in contrast, does converge togrammars that treat won?t as a single unit.The exact relation of P-DOP to other DOP mod-els, including S-DOP (Bod, 2003), Backoff-DOP(Sima?an and Buratto, 2003), DOP* (Zollmann andSima?an, 2005) and ML-DOP (Bod, 2006; based onExpectation Maximization) and not dissimilar au-tomatic enrichment models such as (Petrov et al,2006), remains a topic for future work.Acknowledgments Funded by the NetherlandsOrganisation for Scientific Research (EW), projectnr.
612.066.405; many thanks to Reut Tsarfaty,Remko Scha, Rens Bod and three anonymous re-viewers for their comments.559ReferencesRens Bod.
1998.
Beyond Grammar: An experience-based theory of language.
CSLI, Stanford, CA.Rens Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy?
In Proceed-ings ACL-2001.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings EACL?03.Rens Bod.
2006.
An all-subtrees approach to unsuper-vised parsing.
Proceedings ACL-COLING?06.Eugene Charniak.
1996.
Tree-bank grammars.
Techni-cal report, Department of Computer Science, BrownUniversityEugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the fourteenth national conference on artificialintelligence, Menlo Park.
AAAI Press/MIT Press.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings ACL 2002.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Philip R. Cohenand Wolfgang Wahlster, editors, Proceedings ACL?97,pages 16?23.Joshua Goodman.
1996.
Efficient algorithms for parsingthe DOP model.
In Proceedings EMNLP, pages 143?152.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Mark Johnson.
2002.
The DOP estimation method isbiased and inconsistent.
Computational Linguistics,28(1):71?76.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings ACL?03.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2).T.
Matzuzaki, Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic CFG with latent annotations.
In ProceedingsACL?05, pages 75?82.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings ACL-COLING?06, pages 443?440.Detlef Prescher.
2005.
Inducing head-driven PCFGswith latent heads: Refining a tree-bank grammar forparsing.
In Proceedings ECML?05.Remko Scha.
1990.
Taaltheorie en taaltechnolo-gie; competence en performance.
In R. de Kortand G.L.J.
Leerdam, editors, Computertoepassin-gen in de Neerlandistiek, pages 7?22.
LVVN,Almere, the Netherlands.
English translation athttp://iaaa.nl/rs/LeerdamE.html.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProceedings COLING 2004.Helmut Schmid.
2006.
Trace prediction and recoverywith unlexicalized PCFGs and slash features.
In Pro-ceedings of COLING-ACL 2006.Khalil Sima?an.
2002.
Computational complexity ofprobabilistic disambiguation.
Grammars, 5(2):125?151.Khalil Sima?an and Luciano Buratto.
2003.
Backoff pa-rameter estimation for the DOP model.
In Proceedingsof the 14th European Conference on Machine Learn-ing (ECML?03, Cavtat-Dubrovnik, Croatia, number2837 in Lecture Notes in Artificial Intelligence, pages373?384.
Springer Verlag, Berlin, Germany.Gert Veldhuijzen van Zanten, Gosse Bouma, KhalilSima?an, Gertjan van Noord, and Remko Bonnema.1999.
Evaluation of the NLP components of theOVIS2 spoken dialogue system.
In van Eynde, Schu-urman, and Schelkens, editors, Computational Lin-guistics in the Netherlands 1998, pages 213?229.Rodopi, Amsterdam.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics,10(2/3):367?388.Willem Zuidema.
2006a.
Theoretical evalua-tion of estimation methods for Data-OrientedParsing.
In Proceedings EACL 2006 (Con-ference Companion), pages 183?186.
Associa-tion for Computational Linguistics.
Erratum onhttp://staff.science.uva.nl/?jzuidema/research.Willem Zuidema.
2006b.
What are the productive unitsof natural language grammar?
A DOP approach to theautomatic identification of constructions.
In Proceed-ings of the 10th International Conference on Compu-tational Natural Language Learning (CONLL-X).560
