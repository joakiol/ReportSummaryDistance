CLASSIFYING TEXTS USING RELEVANCY SIGNATURESEllen Riloff and Wendy LehnertDepartment of  Computer ScienceUniversity of  MassachusettsAmherst, MA 01003ABSTRACTText processing for complex domains such as terrorism iscomplicated by the difficulty of being able to reliablydistinguish relevant and irrelevant texts.
We have discovered asimple and effective filter, the Relevancy SignaturesAlgorithm, and demonstrated itsperformance in the domain ofterrorist event descriptions.
The Relevancy SignaturesAlgorithm is based on the natural language processingtechnique of selective concept extraction, and relies on textrepresentations that reflect predictable patterns of linguisticcontext.This paper describes text classification experiments conductedin the domain of terrorism using the MUC-3 text corpus.
Acustomized ictionary of about 6,000 words provides thelexical knowledge base needed to discriminate relevant texts,and the CIRCUS sentence analyzer generates relevancysignatures as an effortless ide-effect of its normal sentenceanalysis.
Although we suspect that the training base availableto us from the MUC-3 corpus may not be large enough toprovide optimal training, we were nevertheless able to attainrelevancy discriminations for significant levels of recall(ranging from 11% to 47%) with 100% precision in half of ourtest runs.TEXT CLASS IF ICAT IONText classification is central to many information retrievalapplications, as well as being relevant to messageunderstanding applications in text analysis.
To appreciatethe importance and difficulty of this problem, consider therole that it played in the MUC-3 (The Third MessageUnderstanding Conference) performance evaluation.
Lastyear 15 text analysis systems attempted to extractinformation from news articles about errorism (Lehnert &Sundheim 1991; Sundheim 1991).
According to anextensive set of domain guidelines, roughly 50% of thetexts in the MUC-3 development corpus did not containlegitimate information about terrorist activities.
Articlesthat described rumours or lacked specific details wereThis research supported by the Office of Naval Research,under a University Research Initiative Grant, Contract#N00014-86-K-0764,  NSF Presidential  YoungInvestigators Award NSFIST-8351863, and the AdvancedResearch Projects Agency of the Department of Defensemonitored by the Air Force Office of Scientific Researchunder Contract No.
F49620-88-C-0058.designated as irrelevant, as well as descriptions of specificevents that targetted military personnel and installations (aterrorist event was defined to be one in which civilians orcivilian locations were the apparent or accidental targets inan intentional act of violence).
In order to achieve high-precision information extraction, the MUC-3 text analyzershad to differentiate r levant and irrelevant texts withouthuman assistance.
A system with a high rate of falsepositives would tend to generate output for irrelevant texts,and this behavior would show up in both the scores forovergeneration a d spurious event counts.
An analysis ofthe MUC-3 evaluation suggests that all of the MUC-3systems experienced significant difficulty with relevant textclassification (Krupka et al 1991).Although some texts will inevitably require in-depthnatural anguage understanding capabilities in order to becorrectly classified, we will demonstrate hat skimmingtechniques can be used to identify subsets of a corpus thatcan be classified with very high levels of precision.
Ouralgorithm automatically derives relevancy signatures from atraining corpus using selective concept extractiontechniques.
These signatures are then used to recognizerelevant texts with a high degree of accuracy.RELEVANCY D ISCRIMINAT IONSTerrorism is a complex domain, especially when it iscombined with a complicated set of domain relevancyguidelines.
Relevancy judgements in this domain are oftendifficult even for human readers.
Many news articles gobeyond the scope of the guidelines or fall into grey areas nomatter how carefully the guidelines are constructed.
Evenso, human readers can reliably identify some subset ofrelevant texts in the terrorism domain with 100%precision, and often without reading these texts in theirentirety.
Text skimming techniques are therefore apromising strategy for text classification as long as lowerlevels of recall 1 are acceptable.
Although it might be1Recall refers to the percentage of relevant exts that arecorrectly classified as relevant.
Precis ion is thepercentage of texts classified as relevant that actually arerelevant.
To illustrate the difference, imagine that youanswer 3 out of 4 questions correctly on a true-or-falseexam.
Your recall rate is then 75%.
Your precision,however, depends on how many of the questions you224unrealistic to try to classify all of the news articles in acorpus with a high degree of precision using anything lessthan a complete, in-depth .natural anguage processingsystem, it is realistic to try to identify a subset of textsthat can be accurately classified using relatively simpletechniques.
2Intuitively, certain phrases seem to be very strongindicators of relevance for the terrorism domain.
"X wasassassinated" is very likely to be a reference to a terroristevent in which a civilian (politician, government leader,etc.)
was killed.
"X died" is a much weaker indicator ofrelevance because people often die in many ways that havenothing to do with terrorism.
Linguistic expressions thatpredict relevance for a domain can be used to recognize andextract relevant exts from a large corpus.
Identifying areliable set of such expressions i an interesting problemand one that is addressed by relevancy feedback algorithmsin information retrieval (Salton 1989).SELECTIVE CONCEPT EXTRACTIONUSING CIRCUSSelective concept extraction is a sentence analysistechnique that simulates the human ability to skim text andextract information in a selective manner.
CIRCUS(Lehnert 1990) is a sentence analyzer designed to performselective concept extraction in a robust manner.
CIRCUSdoes not presume complete dictionary coverage for aparticular domain, and does not rely on the application of aformal grammar for syntactic analysis.
CIRCUS was theheart of the text analyzer underlying the UMass/MUC-3system (Lehnert at al.
1991a, 1991b, 1991c), and itprovided us with the sentence analysis capabilities used inthe experiments we are about o describe.
3The most important dictionary entries for CIRCUS arethose thin contain a concept node definition.
Concept nodesprovide the case frames that are used to structure CIRCUSoutput.
If a sentence contains no concept node triggers,CIRCUS will produce no output for that sentence.
One ofthe research goals stimulated by our participation i MUC-3 was to gain a better understanding of these concept nodesand the vocabulary items associated with them.actually answered.
If you only answered 3 of them, thenyour precision is 100%.
But if you answered all 4 then our precision is only 75%.Many information retrieval tasks and messageunderstanding applications are considered tobe successful iflow levels of recall are attained with high degrees ofprecision.3 The UMass/MUC-3 system posted the highest recallscore and the highest combined scores for recall andprecision of all the MUC-3 text analyzers (Sundheim1991).Our UMass/MUC-3 dictionary was hand-crafted specificallyfor MUC-3.
A preliminary analysis of our MUC-3dictionary indicated that we had roughly equal numbers ofverbs and nouns operating as concept node triggers (131verbs and 125 nouns).
Other parts of speech also acted asconcept node triggers, but to a lesser extent han verbs andnouns.
Out of roughly 6000 dictionary entries, a total of286 lexical items were associated with concept nodedefinitions.All concept node definitions contain a set of enablementconditions that must be met before the concept node can beconsidered valid.
For example, if the lexical item "kill" isencountered in a sentence, a case frame associated with thatitem may be valid only if this instance of "kill" isoperating as a verb in the sentence.
Expectations for anagent and object will be useful for the verb "to kill" butnot for a head noun as in "went in for the kill".Enablements are typically organized as conjunctions ofconditions, and there is no restriction on what types ofenablements can be used.The enablement conditions for concept nodes effectivelyoperate as filters that block further analysis when crucialsentence structures are not detected.
If a filter is too strong,relevant information may be missed.
If a filter is too weak,information may be extracted that is not valid.
Whensentence analysis fails due to poorly crafted enablementconditions, no other mechanisms can step in to override theconsequences of that failure.RELEVANCY SIGNATURESIt is often the case that a single phrase will make a textrelevant.
For instance, a single reference to a kidnappinganywhere in a text generally signals relevance in theterrorism domain regardless of what else is said in theremainder of the article.
4 One implication of this fact isthat it is not always necessary to analyze an entire text inorder to accurately assess relevance.
This property makesthe technique of selective concept extraction particularlywell-suited for text classification tasks.We claim that specific linguistic expressions are reliableindicators of relevance for a particular domain.
Theseexpressions must be general enough to have broadapplicability but specific enough to be consistently reliable4In fact, there can be exceptions to any statement of thistype.
For example, an event hat happened over 2 monthsago was not considered to be relevant for MUC-3.
Ourapproach assumes that these special cases are relativelyinfrequent and that key phrases can indicate relevance mostof the time.
Our technique will therefore produce weakerresults under elevancy guidelines that detail special casesand exceptions if those conditions appear frequently in thetarget exts.225over large numbers of texts.
For example, the word "dead"often appears in a variety of linguistic ontexts uch as "hewas found dead", "leaving him dead", "left him dead", "theycounted 15 dead", etc.
Some of these expressions mayprovide stronger relevancy cues than others.
For example,"<person> was found dead" is a strong relevancy cue sincethere is a good chance that the person was the victim of aterrorist crime, whereas "<number> dead" is a much weakercue since it is often used in articles describing militaryepisodes that are not terrorist in nature.
Similarly, the word"casualties" by itself is not a strong relevancy cue sincemany articles discuss casualties in the context of militaryacts.
But the expression "no casualties"/s highly correlatedwith relevance since it often refers to civilians.
We willrefer to linguistic expressions that are strong relevancy cuesas relevancy signatures.In our system, these linguistic expressions are representedby ordered pairs of lexical items and concept nodes wherethe lexical item acts as a trigger for the concept node.
Forexample, the pattern "was found dead" is represented bythepair ("dead", Sfound-dead-pass$) where dead is thekey word that triggers the concept node Sfound-dead-pass$ which in turn activates enabling conditions thatexpect he passive form of the verb "found" to precede theword dead.By taking advantage of the text corpus and answer keysused in MUC-3, we can automatically derive a set ofrelevancy signatures that will reliably predict he relevanceof new texts.
The following section describes the algorithmthat derives a set of relevancy signatures from a trainingcorpus and then uses those signatures to classify new texts.THE RELEVANCY S IGNATURESALGORITHMMUC-3 provided its participants with a corpus of 1300news articles for development purposes and two additionalsets of 100 texts each that were made available for test runs(the TST1 and TST2 texts).
All of the MUC-3 texts weresupplied by the Foreign Broadcast Information Service andthey were drawn from a variety of news sources includingwire stories, transcripts of speeches, radio broadcasts,terrorist communiques, and interviews.
The MUC-3 textcorpus was supplemented by hand-coded case frameinstanfiations (answer keys) for each text in the corpus.The MUC-3 text corpus and answer keys therefore gave usaccess to 1500 texts  and their correct relevancyclassifications.
For our experiments, we set aside a smallportion of this corpus for testing purposes and dedicated theremaining texts to the training set.
The training set wasthen used to derive a set of relevancy signatures.The Relevancy Signatures Algorithm is fairly simple.Given a set of training texts, we parse each text usingCIRCUS and save the concept nodes that are producedduring the parse along with the lexical items that triggeredthose concept nodes.
As we parse the training texts, weupdate two statistics for each word/concept node pair: \[1\]the number of times that the pair occurred in the trainingset (N), and \[2\] the number of times that it occurred in arelevant ext (NR).
The ratio of NR over N gives us a"reliability" measure.
For example, .75 means that 75% ofthe instances (for that pair) appeared in relevant texts.Using these statistics, we then extract a set of "reliable"lexical item/concept node pairs by choosing two values: areliability threshold (R) and a minimum number ofoccurrences (M).
The reliability threshold specifies theminimum reliability measure that is acceptable.
Forexample, R=90 dictates that a pair must have a reliabilitymeasure greater than 90% in order to be considered reliable.The minimum number of occurrences parameter specifies aminimum number of times that the pair must haveoccurred in the training set.
For example, M=4 dictatesthat there must be more than 4 occurrences of a pair for itto be considered reliable.
This parameter is used toeliminate pairs that may have a very high reliabilitymeasure but have dubious statistical merit because theyappeared only a few times in the entire training set.
Oncethese parameters have been selected, we then identify allpairs that meet the above criteria.
We will refer to thesereliable word/concept node pairs as our set of relevancysignatures.To illustrate, here are some relevancy signatures that werederived from the corpus using the parameter values, R=90and M=10 along with some text samples that arerecognized by these signatures:("injured",$injury-l$)the terrorists injured 5 people("located",$1ocation-pass-l$)the banks were located("occurred",$bomb-attack-2$)an explosion occurred("perpetrated",$perp-pass-l$)the attack was perpetrated by...("placed",$1oc-val- 15)the terrorists placed a bomb...("placed",$1oc-val-pass- 15)a bomb was placed by...("planted",$1oc-val-pass- 15)a bomb was planted by...To classify a new text, we parse the text and save theconcept nodes that are produced uring the parse, alongwith the lexical items that triggered them.
The text istherefore represented asa set of these lexical item/concept226node pairs.
We then consult our list of relevancysignatures to see if any of them are present in the currenttext If we find one, the text is deemed to be relevant Ifnot, then the text is deemed to be irrelevant I is importantto note that it only takes one relevancy signature toclassify a text as relevant.EXPERIMENTAL RESULTSTo judge the effectiveness of the Relevancy SignaturesAlgorithm, we performed a variety of experiments.
Sinceour algorithm derives relevancy signatures from a trainingset of texts, it is important that the training set be largeenough to produce significant statistics.
It is harder for agiven word/concept node pair to occur than it is for onlythe word to occur, so many potenually useful pairings maynot occur very often.
At the same time, it is also importantto have a large test set so we can feel confident that ourresults accurately represent he effectiveness of thealgorithm.
Because we were constrained by the relativelysmall size of the MUC-3 collection (1500 texts), balancingthese two requirements was something of a problem.Dividing the MUC-3 corpus into 15 blocks of 100 textseach, we ran 15 preliminary experiments with each blockusing 1400 texts for training and the remaining 100 fortesting.
The results showed that we could achieve highlevels of precision with non-trivial levels of recall.
Of the15 experiments, 7 test sets reached 80% precision with70% recall, 10 sets hit 80% precision with _> 40% recall,and 12 sets achieved 80% precision with ~ 25% recall.
Inaddition, 7 of the test runs produced precision scores of100% for recall evels > 10% and 5 test sets produced recalllevels > 50% with precision over 85%.Based on these experiments, we identified two blocks ofI(X) texts that gave us our best and our worst results.
Withthese 200 texts in hand, we then trained once again on theremaining 1300 in order to obtain a uniform training baseunder which the remaining two test sets could be compared.Figure 1 shows the performance of these two test setsbased on the training set of 1300 texts.
Each data pointrepresents the results of the Relevancy SignaturesAlgorithm for a different combination of parameter values.We tested the reliability threshold at 70%, 75%, 80%,85%, 90%, and 95% and varied the minimum number ofoccurrences from 0 to 19.
As the data demonstrates, theresults of the two test sets are clearly separated.
Our besttest results are associated with uniformly high levels ofprecision throughout (> 78%), while our worst test resultsranged from 47% to 67% precision.
These results indicatethe full range of our performance: average performancewould fall somewhere in between these two extremes.91009080706C40I I I" A I A. .
.
.
.
; -  : .
!1 , : : .
i - ' l302010o lo 2o ~ ~ so 6o ~o ~) ~ ~ooRecall~ OEV401-SOO (66 fel) ~ 0EV801-9~ {39 retl \[Figure 1: Relevancy Discriminations on Two SeparateTest SetsLow reliability and low M thresholds produce strong recall(but weaker precision) for relevant texts while highrefiabifity and high M thresholds produce strong precision(but weaker ecall) for the relevant texts being retrieved.
Ahigh reliability threshold ensures that the algorithm usesonly relevancy signatures that are very strongly correlatedwith relevant texts and a high minimum number ofoccurrences threshold ensures that it uses only relevancysignatures that have appeared with greater frequency.
Byadjusting these two parameter values, we can manipulate arecall/precision tradeoff.However, a clear recall/precision tradeoff is evident onlywhen the algorithm is retrieving statistically significantnumbers of texts.
We can see from the graph in Figure 1that precision fluctuates dramatically for our worst test setwhen recall values are under 50%.
At these lower recallvalues, the algorithm is retreiving such small numbers oftexts (less than 20 for our worst test se0 that gaining orlosing a single text can have a significant impact onprecision.
Since our test sets contain only 100 texts each,statistical significance may not be reached until weapproach fairly high recall values.
With larger test sets wecould expect o see somewhat more stable precision scoresat lower recall levels because the number of texts beingretrieved would be greater.The percentage of relevant exts in a test set alo plays arole in determining statistical significance.
Each of the testsets contains a different number of relevant exts.
Forexample, the best test set (represented by the data pointsnear the top of the Y-axis) contains 66 relevant exts,whereas the worst test set (represented by the data pointsnear the middle of the Y-axis) contains only 39 relevanttexts.
The total percentage of relevant exts in the test227corpus provides abaseline against which precision must beassessed.
A constant algorithm that classifies all texts asrelevant will always yield 100% recall with a precisionlevel determined by this baseline percentage.
If only 10%of the test corpus is relevant, the constant algorithm willshow a 10% rate Of pre~Sision.
If 90% of the test corpus isrelevant, the constant algorithm will achieve 90%precision.
If we look at the graph in Figure 1 with this inmind, we find that a constant algorithm would yield 66%precision for the first test set but only 39% for the secondtest set.
From this vantage point, we can see that theRelevancy Signatures Algorithm performs substantiallybetter than the constant algorithm on both test sets.It was interesting to see how much variance we got acrossthe different test sets.
Several other factors may have alsocontributed to this.
For one, the corpus is not a randomlyordered collection of texts.
The MUC-3 articles were oftenordered by date so it is not uncommon to find sequences ofarticles that describe the same event.
One block of textsmay contain several articles about a specific kidnappingevent while a different block will not contain any articlesabout kidnappings.
Second, the quality of the answer keysis not consistent across the corpus.
During the course ofMUC-3, each participating site was responsible forencoding the answer keys for different parts of the corpus.Although some cross-checking was done, the quality of theencoding is not consistent across the corpus.
5 The qualityof the answer keys can affect both training and testing.The relatively small size of our training set wasundoubtedly a limiting factor since many linguisticexpressions appeared only a few times throughout theentire corpus.
This has two ramifications for ouralgorithm: (1) many infrequent expressions are neverconsidered as relevancy signatures because the minimumnumber of occurrences parameter prohibits them, and (2)expressions that occur with low frequencies will yield lessreliable statistics.
Having run experiments with smallertraining sets, we have seen our results show markedimprovement asthe training set grows.
We expect hat thistrend would continue for training sets greater than 1400,but corpus limitations have restricted us in that regard.CONCLUSIONSThe Relevancy Signatures Algorithm was inspired by thefact that human readers are capable of scanning a collectionof texts, and reliably identifying a subset of those texts thatare relevant o a given domain.
More importantly, this5 During the course of this research, we found that about4% of the irrelevant exts in the MUC-3 developmentcorpus were miscategorized.
These errors were uncovered byspot checks: no systematic effort was made to review allthe irrelevant texts.
We therefore suspect hat the actualerror rate is probably much higher.classification can be accomplished by fast text skimming:the reader hits on a key sentence and a determination frelevancy is made.
This method is not adequate if one'sgoal is to identify all possible relevant exts, but textskimming can be very reliable when a proper subset ofrelevant exts is sufficient.
We designed the RelevancySignatures Algorithm in an effort to simulate this process.In fact, the Relevancy Signatures Algorithm has anadvantage over humans insofar as it can automaticallyderive domain specifications from a set of training texts.While humans rely on domain knowledge, xplicit domainguidelines, and general world knowledge to identify relevanttexts, the Relevancy Signatures Algorithm requires noexplicit domain specification.
Given a corpus of textstagged for domain relevancy, an appropriate dictionary, andsuitable natural anguage processing capabilities, reliablerelevancy indicators are extracted from the corpus as asimple side effect of natural language analysis.
Once thistraining base has been obtained, no additional capabilitiesare needed to classify a new text.It follows that the Relevancy Signatures Algorithm avoidsthe knowledge-engineering bottleneck associated with manytext analysis ystems.
As a result, this algorithm can beeasily ported to new domains and is trivial to scale-up.With large online text corpora becoming increasinglyavailable to natural language researchers, we have anopportunity to explore operational alternatives to hand-coded knowledge bases and rule bases.
As we havedemonstrated, natural language processing capabilities canproduce domain signatures for representative text corporathat support high-precision text classification.
We haveobtained high degrees of precision for limited levels ofrecall, in an effort to simulate human capabilities with adomain-independent discrimination technique.1.2.3.4.BIBLIOGRAPHYKrupka, G., Iwanska, L., Jacobs, P., and Rau, L.
1991.
"GE NLToolset: MUC-3 Test Results and Analysis" inProceedings of the Third Message UnderstandingConference.
San Mateo, CA.
Morgan Kaufmann.Lehnert, W.G.
1990.
"Symbolic/SubsymbolieSentence Analysis: Exploiting the Best of TwoWorlds" in Advances in Connectionist and NeuralComputation Theory.
(Eds: J. Pollack and J. Barnden).Ablex Publishing.
Norwood, NJ.
pp.
135-164.Lehnert, W.G., Cardie, C., Fisher, D., Riloff, E.,Williams, R. 1991a.
"University of Massachuseus:Description of the CIRCUS System as Used for MUC-3", in The Proceedings of the Third MessageUnderstanding Conference.
pp.
223-233.Lehnert, W.G., Cardie, C., Fisher, D., Riloff, E., andWilliams, R. 1991b.
"University of Massachusetts:MUC-3 Test Results and Analysis,", in The228.6.7.8.9.Proceedings of the Third Message UnderstandingConference.
pp.
116-199.Lehnert, W.G., Cardie, C., Fisher, D., Riloff, E., andWilliams, R. 1991c.
"The CIRCUS system as used inMUC-3", COINS Technical Report 91-59.
Departmentof Computer and Information Science, University ofMassachusetts at Amherst.Lehnert, W. and Riloff, E. 1992.
"Relevancy FeedbackUsing Selective Concept Extraction" submitted toACL-92.Lehnert, W.G.
and Sundheim, B.
1991.
"A PerformanceEvaluation of Text Analysis Technologies", A IMagazine, vol 12; no.3, pp.
81-94.Salton, G. 1989.
Automatic Text Processing: TheTransformation, Analysis, and Retrieval ofInformation by Computer.
Reading, MA.
Addison-Wesley Publishing Company, Inc.Sundheim, B.
1991.
(ed.)
Proceedings of the ThirdMessage Understanding Conference.
San Marco, CA.Morgan Kanfmann.229
