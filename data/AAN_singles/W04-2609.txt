Models for the Semantic Classification of Noun PhrasesDan Moldovan, Adriana Badulescu,Marta Tatu, and Daniel AntoheComputer Science DepartmentUniversity of Texas at DallasDallas, Texasmoldovan@utdallas.eduRoxana GirjuDepartment of Computer ScienceBaylor UniversityWaco, Texasgirju@cs.baylor.eduAbstractThis paper presents an approach for detectingsemantic relations in noun phrases.
A learningalgorithm, called semantic scattering, is usedto automatically label complex nominals, gen-itives and adjectival noun phrases with the cor-responding semantic relation.1 Problem descriptionThis paper is about the automatic labeling of semanticrelations in noun phrases (NPs).The semantic relations are the underlying relations be-tween two concepts expressed by words or phrases.
Wedistinguish here between semantic relations and semanticroles.
Semantic roles are always between verbs (or nounsderived from verbs) and other constituents (run quickly,went to the store, computer maker), whereas semanticrelations can occur between any constituents, for exam-ple in complex nominals (malaria mosquito (CAUSE)),genitives (girl?s mouth (PART-WHOLE)), prepositionalphrases attached to nouns (man at the store (LOCATIVE)),or discourse level (The bus was late.
As a result, I missedmy appointment (CAUSE)).
Thus, in a sense, semanticrelations are more general than semantic roles and manysemantic role types will appear on our list of semanticrelations.The following NP level constructions are consid-ered here (cf.
the classifications provided by (Quirket al1985) and (Semmelmeyer and Bolander 1992)):(1) Compound Nominals consisting of two consecutivenouns (eg night club - a TEMPORAL relation - indicat-ing that club functions at night), (2) Adjective Noun con-structions where the adjectival modifier is derived from anoun (eg musical clock - a MAKE/PRODUCE relation), (3)Genitives (eg the door of the car - a PART-WHOLE rela-tion), and (4) Adjective phrases (cf.
(Semmelmeyer andBolander 1992)) in which the modifier noun is expressedby a prepositional phrase which functions as an adjective(eg toy in the box - a LOCATION relation).Example: ?Saturday?s snowfall topped a one-day recordin Hartford, Connecticut, with the total of 12.5 inches,the weather service said.
The storm claimed its fatal-ity Thursday, when a car which was driven by a collegestudent skidded on an interstate overpass in the moun-tains of Virginia and hit a concrete barrier, police said?.
(www.cnn.com - ?Record-setting Northeast snowstormwinding down?, Sunday, December 7, 2003).There are several semantic relations at the noun phraselevel: (1) Saturday?s snowfall is a genitive encoding aTEMPORAL relation, (2) one-day record is a TOPIC nouncompound indicating that record is about one-day snow-ing - an ellipsis here, (3) record in Hartford is an adjectivephrase in a LOCATION relation, (4) total of 12.5 inchesis an of-genitive that expresses MEASURE, (5) weatherservice is a noun compound in a TOPIC relation, (6) carwhich was driven by a college student encodes a THEMEsemantic role in an adjectival clause, (7) college student isa compound nominal in a PART-WHOLE/MEMBER-OF re-lation, (8) interstate overpass is a LOCATION noun com-pound, (9) mountains of Virginia is an of-genitive show-ing a PART-WHOLE/PLACE-AREA and LOCATION rela-tion, (10) concrete barrier is a noun compound encodingPART-WHOLE/STUFF-OF.1.1 List of Semantic RelationsAfter many iterations over a period of time we identified aset of semantic relations that cover a large majority of textsemantics.
Table 1 lists these relations, their definitions,examples, and some references.
Most of the time, thesemantic relations are encoded by lexico-syntactic pat-terns that are highly ambiguous.
One pattern can expressa number of semantic relations, its disambiguation be-ing provided by the context or world knowledge.
Oftensemantic relations are not disjoint or mutually exclusive,two or more appearing in the same lexical construct.
Thisis called semantic blend (Quirk et al1985).
For example,the expression ?Texas city?
contains both a LOCATION aswell as a PART-WHOLE relation.Other researchers have identified other sets of seman-tic relations (Levi 1979), (Vanderwende 1994), (Sowa1994), (Baker, Fillmore, and Lowe 1998), (Rosario andHearst 2001), (Kingsbury, et al 2002), (Blaheta andCharniak 2000), (Gildea and Jurafsky 2002), (Gildeaand Palmer 2002).
Our list contains the most frequentlyused semantic relations we have observed on a large cor-pus.Besides the work on semantic roles, considerable in-terest has been shown in the automatic interpretation ofcomplex nominals, and especially of compound nomi-nals.
The focus here is to determine the semantic re-lations that hold between different concepts within thesame phrase, and to analyze the meaning of these com-pounds.
Several approaches have been proposed for em-pirical noun-compound interpretation, such as syntacticanalysis based on statistical techniques (Lauer and Dras1994), (Pustejovsky et al 1993).
Another popular ap-proach focuses on the interpretation of the underlying se-mantics.
Many researchers that followed this approachrelied mostly on hand-coded rules (Finin 1980), (Van-derwende 1994).
More recently, (Rosario and Hearst2001), (Rosario, Hearst, and Fillmore 2002), (Lapata2002) have proposed automatic methods that analyze anddetect noun compounds relations from text.
(Rosario andHearst 2001) focused on the medical domain making useof a lexical ontology and standard machine learning tech-niques.2 Approach2.1 Basic ApproachWe approach the problem top-down, namely identifyand study first the characteristics or feature vectors ofeach noun phrase linguistic pattern, then develop mod-els for their semantic classification.
This is in contrast toour prior approach ( (Girju, Badulescu, and Moldovan2003a)) when we studied one relation at a time, andlearned constraints to identify only that relation.
Westudy the distribution of the semantic relations across dif-ferent NP patterns and analyze the similarities and dif-ferences among resulting semantic spaces.
We define asemantic space as the set of semantic relations an NP con-struction can encode.
We aim at uncovering the generalaspects that govern the NP semantics, and thus delineatethe semantic space within clusters of semantic relations.This process has the advantage of reducing the annotationeffort, a time consuming activity.
Instead of manually an-notating a corpus for each semantic relation, we do it onlyfor each syntactic pattern and get a clear view of its se-mantic space.
This syntactico-semantic approach allowsus to explore various NP semantic classification modelsin a unified way.This approach stemmed from our desire to answerquestions such as:1.
What influences the semantic interpretation of variouslinguistic constructions?2.
Is there only one interpretation system/model thatworks best for all types of expressions at all syntactic lev-els?
and3.
What parameters govern the models capable of seman-tic interpretation of various syntactic constructions?2.2 Semantic Relations at NP levelIt is well understood and agreed in linguistics that con-cepts can be represented in many ways using various con-structions at different syntactic levels.
This is in part whywe decided to take the syntactico-semantic approach thatanalyzes semantic relations at different syntactic levelsof representation.
In this paper we focus only on the be-havior of semantic relations at NP level.
A thorough un-derstanding of the syntactic and semantic characteristicsof NPs provides valuable insights into defining the mostrepresentative feature vectors that ultimately drive thediscriminating learning models.Complex NominalsLevi (Levi 1979) defines complex nominals (CNs) as ex-pressions that have a head noun preceded by one or moremodifying nouns, or by adjectives derived from nouns(usually called denominal adjectives).
Most importantlyfor us, each sequence of nouns, or possibly adjectives andnouns, has a particular meaning as a whole carrying animplicit semantic relation; for example, ?spoon handle?
(PART-WHOLE) or ?musical clock?
(MAKE/PRODUCE).CNs have been studied intensively in linguistics,psycho-linguistics, philosophy, and computational lin-guistics for a long time.
The semantic interpretationof CNs proves to be very difficult for a number of rea-sons.
(1) Sometimes the meaning changes with thehead (eg ?musical clock?
MAKE/PRODUCE, ?musical cre-ation?
THEME), other times with the modifier (eg ?GMcar?
MAKE/PRODUCE, ?family car?
POSSESSION).
(2)CNs?
interpretation is knowledge intensive and can be id-iosyncratic.
For example, in order to interpret correctly?GM car?
we have to know that GM is a car-producingcompany.
(3) There can be many possible semantic re-lations between a given pair of word constituents.
Forexample, ?USA city?
can be regarded as a LOCATION aswell as a PART-WHOLE relation.
(4) Interpretation of CNscan be highly context-dependent.
For example, ?applejuice seat?
can be defined as ?seat with apple juice on thetable in front of it?
(cf.
(Downing 1977)).GenitivesThe semantic interpretation of genitive constructionsNo.
Semantic Definition / ExampleRelation1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car.
), (Vanderwende 1994)2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary?s daughter;my sister); (Levi 1979)3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful.
); (Levi 1979)ATTRIBUTE-HOLDER4 AGENT the doer or instigator of the action denoted by the predicate;(employee protest; parental approval; The king banished the general.
); (Baker, Fillmore, and Lowe 1998)5 TEMPORAL time associated with an event; (5-o?clock tea; winter training; the store opens at 9 am),includes DURATION (Navigli and Velardi 2003),6 DEPICTION- an event/action/entity depicting another event/action/entity; (A picture of my niece.
),DEPICTED7 PART-WHOLE an entity/event/state is part of another entity/event/state (door knob; door of the car),(MERONYMY) (Levi 1979), (Dolan et al 1993),8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft)(IS-A) (Levi 1979), (Dolan et al 1993)9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping)10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquakegenerated a Tsunami), (Levi 1979)11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear power plant; GM makes cars) (Levi 1979)12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979)13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse;street show; I left the keys in the car), (Levi 1979), (Dolan et al 1993)14 PURPOSE a state/action intended to result from a another state/event; (migraine drug; wine glass; rescue mission;He was quiet in order not to disturb her.)
(Navigli and Velardi 2003)15 SOURCE/FROM place where an entity comes from; (olive oil; I got it from China) (Levi 1979)16 TOPIC an object is a topic of another object; (weather report; construction plan; article about terrorism); (Rosario and Hearst 2001)17 MANNER a way in which an event is performed or takes place; (hard-working immigrants; enjoy immensely; he died ofcancer); (Blaheta and Charniak 2000)18 MEANS the means by which an event is performed or takes place; (bus service; I go to school by bus.)
(Quirk et al1985)19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al1985)20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state of panic.
); (Sowa 1994)21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994)22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa 1994)23 INFLUENCE an entity/event that affects other entity/event; (drug-affected families; The war has an impact on the economy.
);24 ASSOCIATED WITH an entity/event/state that is in an (undefined) relation with another entity/event/state; (Jazz-associated company;)25 MEASURE an entity expressing quantity of another entity/event; (cup of sugar;70-km distance; centennial rite; The jacket cost $60.
)26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept;(NAME) (Marry is called Minnie); (Sowa 1994)27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite of full); (Sowa 1994)28 PROBABILITY OF the quality/state of being probable; likelihoodEXISTENCE (There is little chance of rain tonight); (Sowa 1994)29 POSSIBILITY the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994)30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning);31 THEME an entity that is changed/involved by the action/event denoted by the predicate;(music lover; John opened the door.
); (Sowa 1994)32 RESULT the inanimate result of the action/event denoted by the predicate; includes EFFECT and PRODUCT.
(combustion gases; I finished the task completely.
); (Sowa 1994)33 STIMULUS stimulus of the action or event denoted by the predicate (We saw [the painting].I sensed [the eagerness] in him.
I can see [that you are feeling great].)
(Baker, Fillmore, and Lowe 1998)34 EXTENT the change of status on a scale (by a percentage or by a value) of some entity;(The price of oil increased [ten percent].
Oil?s price increased by [ten percent].
); (Blaheta and Charniak 2000)35 PREDICATE expresses the property associated with the subject or the object through the verb;(He feels [sleepy].
They elected him [treasurer]. )
(Blaheta and Charniak 2000)Table 1: A list of semantic relations at various syntactic levels (including NP level), their definitions, some examples,and references.is considered problematic by linguists because theyinvolve an implicit relation that seems to allow fora large variety of relational interpretations; for ex-ample: ?John?s car?-POSSESSOR-POSSESSEE, ?Mary?sbrother?-KINSHIP, ?last year?s exhibition?-TEMPORAL,?a picture of my nice?-DEPICTION-DEPICTED, and ?thedesert?s oasis?-PART-WHOLE/PLACE-AREA.
A charac-teristic of these constructions is that they are very pro-ductive, as the construction can be given various inter-pretations depending on the context.
One such exampleis ?Kate?s book?
that can mean the book Kate owns, thebook Kate wrote, or the book Kate is very fond of.Thus, the features that contribute to the semantic in-terpretation of genitives are: the nouns?
semantic classes,the type of genitives, discourse and pragmatic informa-tion.Adjective Phrases are prepositional phrases attached tonouns acting as adjectives (cf.
(Semmelmeyer andBolander 1992)).
Prepositions play an important roleboth syntactically and semantically.
Semantically speak-ing, prepositional constructions can encode various se-mantic relations, their interpretations being providedmost of the time by the underlying context.
For instance,the preposition ?with?
can encode different semantic re-lations: (1) It was the girl with blue eyes (MERONYMY),(2) The baby with the red ribbon is cute (POSSESSION),(3) The woman with triplets received a lot of attention(KINSHIP).The conclusion for us is that in addition to the nouns se-mantic classes, the preposition and the context play im-portant roles here.In order to focus our research, we will concentrate fornow only on noun - noun or adjective - noun composi-tional constructions at NP level, ie those whose mean-ing can be derived from the meaning of the constituentnouns (?door knob?, ?cup of wine?).
We don?t considermetaphorical names (eg, ?ladyfinger?
), metonymies (eg,?Vietnam veteran?
), proper names (eg, ?John Doe?
), andNPs with coordinate structures in which neither noun isthe head (eg, ?player-coach?).
However, we check ifthe constructions are non-compositional (lexicalized) (themeaning is a matter of convention; e.g., ?soap opera?,?sea lion?
), but only for statistical purposes.
Fortunately,some of these can be identified with the help of lexicons.2.3 Corpus Analysis at NP levelIn order to provide a unified approach for the detection ofsemantic relations at different NP levels, we analyzed thesyntactic and semantic behavior of these constructions ona large open-domain corpora of examples.
Our intentionis to answer questions like: (1) What are the semantic re-lations encoded by the NP-level constructions?, (2) Whatis their distribution on a large corpus?, (3) Is there a com-mon subset of semantic relations that can be fully para-phrased by all types of NP constructions?, (4) How manyNPs are lexicalized?The dataWe have assembled a corpus from two sources: WallStreet Journal articles from TREC-9, and eXtendedWordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).We used XWN 2.0 since all its glosses are syntacti-cally parsed and their words semantically disambiguatedwhich saved us considerable amount of time.
Table 2shows for each syntactic category the number of ran-domly selected sentences from each corpus, the num-ber of instances found in these sentences, and finally thenumber of instances that our group managed to annotateby hand.
The annotation of each example consisted ofspecifying its feature vector and the most appropriate se-mantic relation from those listed in Table 1.Inter-annotator AgreementThe annotators, four PhD students in Computational Se-mantics worked in groups of two, each group focusingon one half of the corpora to annotate.
Noun - noun(adjective - noun, respectively) sequences of words wereextracted using the Lauer heuristic (Lauer 1995) whichlooks for consecutive pairs of nouns that are neitherpreceded nor succeeded by a noun after each sentencewas syntactically parsed with Charniak parser (Charniak2001) (for XWN we used the gold parse trees).
More-over, they were provided with the sentence in which thepairs occurred along with their corresponding WordNetsenses.
Whenever the annotators found an example en-coding a semantic relation other than those provided orthey didn?t know what interpretation to give, they hadto tag it as ?OTHERS?.
Besides the type of relation, theannotators were asked to provide information about theorder of the modifier and the head nouns in the syntac-tic constructions if applicable.
For instance, in ?ownerof car?-POSSESSION the possessor owner is followed bythe possessee car, while in ?car of John?-POSSESSION/Rthe order is reversed.
On average, 30% of the trainingexamples had the nouns in reverse order.Most of the time, one instance was tagged with onesemantic relation, but there were also situations in whichan example could belong to more than one relation in thesame context.
For example, the genitive ?city of USA?was tagged as a PART-WHOLE/PLACE-AREA relation andas a LOCATION relation.
Overall, there were 608 suchcases in the training corpora.
Moreover, the annotatorswere asked to indicate if the instance was lexicalized ornot.
Also, the judges tagged the NP nouns in the trainingcorpus with their corresponding WordNet senses.The annotators?
agreement was measured using theKappa statistics, one of the most frequently used mea-sure of inter-annotator agreement for classification tasks: , where ffflfiffi is the proportion oftimes the raters agree and fffl !ffi is the probability ofagreement by chance.
The K coefficient is 1 if there isa total agreement among the annotators, and 0 if there isno agreement other than that expected to occur by chance.Table 3 shows the semantic relations inter-annotatoragreement on both training and test corpora for each NPconstruction.
For each construction, the corpus was splintinto 80/20 training/testing ratio after agreement.We computed the K coefficient only for those instancestagged with one of the 35 semantic relations.
For eachpattern, we also computed the number of pairs that weretagged with OTHERS by both annotators, over the numberof examples classified in this category by at least one ofthe judges, averaged by the number of patterns consid-ered.The K coefficient shows a fair to good level of agree-ment for the training and testing data on the set of 35 re-lations, taking into consideration the task difficulty.
Thiscan be explained by the instructions the annotators re-ceived prior to annotation and by their expertise in lexicalsemantics.
There were many heated discussions as well.Wall Street Journal eXtended WordNet 2.0CNs Genitives Adjective Complex NominalsNN AdjN ?s of Phrases NNNo.
of sentences 7067 5381 50291 27067 14582 51058No.
of instances 5557 500 2990 4185 3502 12412No.
of annotated instances 2315 383 1816 3404 1341 1651Table 2: Corpus statistics.Kappa Agreement ( 1 - 35 )Complex Nominals Genitives Adjective OTHERSNN Adj N ?s of Phrases0.55 0.68 0.66 0.65 0.67 0.69Table 3: The inter-annotator agreement on the semantic annotation of the NP constructions considered on both trainingand test corpora.
For the semantic blend examples, the agreement was done on one of the relations only.2.4 Distribution of Semantic RelationsEven noun phrase constructions are very productive al-lowing for a large number of possible interpretations, Ta-ble 4 shows that a relatively small set of 35 semantic re-lations covers a significant part of the semantic distribu-tion of these constructions on a large open-domain cor-pus.
Moreover, the distribution of these relations is de-pendent on the type of NP construction, each type en-coding a particular subset.
For example, in the case ofof-genitives, there were 21 relations found from the totalof 35 relations considered.
The most frequently occur-ring relations were PART-WHOLE, ATTRIBUTE-HOLDER,POSSESSION, LOCATION, SOURCE, TOPIC, and THEME.By comparing the subsets of semantic relations in eachcolumn we can notice that these semantic spaces are notidentical, proving our initial intuition that the NP con-structions cannot be alternative ways of packing the sameinformation.
Table 4 also shows that there is a subsetof semantic relations that can be fully encoded by alltypes of NP constructions.
The statistics about the lex-icalized examples are as follows: N-N (30.01%), Adj-N(0%), s-genitive (0%), of-genitive (0%), adjective phrase(1%).
From the 30.01% lexicalized noun compounds ,18% were proper names.This simple analysis leads to the important conclusionthat the NP constructions must be treated separately astheir semantic content is different.
This observation isalso partially consistent with other recent work in lin-guistics and computational linguistics on the grammaticalvariation of the English genitives, noun compounds, andadjective phrases.We can draw from here the following conclusions:1.
Not all semantic relations can be encoded by all NPsyntactic constructions.2.
There are semantic relations that have preferences overparticular syntactic constructions.2.5 Models2.5.1 Mathematical formulationGiven each NP syntactic construction considered, thegoal is to develop a procedure for the automatic label-ing of the semantic relations they encode.
The semanticrelation derives from the lexical, syntactic, semantic andcontextual features of each NP construction.Semantic classification of syntactic patterns in generalcan be formulated as a learning problem, and thus bene-fit from the theoretical foundation and experience gainedwith various learning paradigms.
This is a multi-classclassification problem since the output can be one of thesemantic relations in the set.
We cast this as a supervisedlearning problem where input/ output pairs are availableas training data.An important first step is to map the characteristics ofeach NP construction (usually not numerical) into featurevectors.
Let?s define with   the feature vector of an in-stanceand let  be the space of all instances; ie     .The multi-class classification is performed by a func-tion that maps the feature space  into a semantic space, , whereis the set of semantic relationsfrom Table 1, ie .Let  be the training set of examples or instances   ff fi ffi fl ffiwhere !
is thenumber of examples   each accompanied by its semanticrelation label  .
The problem is to decide which semanticrelation  to assign to a new, unseen example   #"  .
In or-der to classify a given set of examples (members of  ),one needs some kind of measure of the similarity (or thedifference) between any two given members of  .
Mostof the times it is difficult to explicitly define this func-tion, since  can contain features with numerical as wellas non-numerical values.Note that the features, thus space  , vary from an NPpattern to another and the classification function will bepattern dependent.
The novelty of this learning problemis the feature space  and the nature of the discriminatingNo.
Semantic Frequency    ExamplesRelations CNs Genitives AdjectiveNN AdjN ?s of Phrases1 POSSESSION 2.91 9.44 14.55 3.89 1.47 ?family estate?2 KINSHIP 0 0 7.94 3.23 0.20 ?woman with triplets?3 ATTRIBUTE-HOLDER 12.38 7.34 8.96 10.77 4.09 ?John?s coldness?4 AGENT 4.21 10.49 9.75 0.98 3.46 ?the investigation of the crew?5 TEMPORAL 0.82 0 11.96 0.53 7.97 ?last year?s exhibition?6 DEPICTION-DEPICTED 0 0 1.49 2.86 0.20 ?a picture of my nice?7 PART-WHOLE 19.68 10.84 27.38 31.70 5.56 ?the girl?s mouth?8 IS-A (HYPERNYMY) 3.95 1.05 0 0.04 1.15 ?city of Dallas?9 ENTAIL 0 0 0 0 010 CAUSE 0.08 0 0.23 0.57 1.04 ?malaria mosquitoes?11 MAKE/PRODUCE 1.56 2.09 0.86 0.98 0.63 ?shoe factory?12 INSTRUMENT 0.65 0 0.07 0.16 0.94 ?pump drainage?13 LOCATION/SPACE 7.51 20.28 1.57 3.89 18.04 ?university in Texas?14 PURPOSE 6.69 3.84 0.07 0.20 5.45 ?migraine drug?15 SOURCE 1.69 11.53 2.51 5.85 2.94 ?president of Bolivia?16 TOPIC 4.04 4.54 0.15 4.95 9.13 ?museum of art?17 MANNER 0.40 0 0 0 0.20 ?performance with style?18 MEANS 0.26 0 0 0 0.10 ?bus service?19 ACCOMPANIMENT 0 0 0 0 0.42 ?meeting with friends?20 EXPERIENCER 0.08 2.09 0.07 0.16 2.30 ?victim of lung disease?21 RECIPIENT 1.04 0 3.54 2.66 2.51 ?Josephine?s reward?22 FREQUENCY 0.04 7.00 0 0 0.52 ?bi-annual meeting?23 INFLUENCE 0.13 0.35 0 0 0.73 ?drug-affected families?24 ASSOCIATED WITH 1.00 0.35 0.15 0.16 2.51 ?John?s lawyer?25 MEASURE 1.47 0.35 0.07 13.88 1.36 ?hundred of dollars?26 SYNONYMY 0 0 0 0 027 ANTONYMY 0 0 0 0 028 PROBABILITY 0 0 0 0 029 POSSIBILITY 0 0 0 0 030 CERTAINTY 0 0 0 0 031 THEME 6.51 1.75 3.30 6.26 9.75 ?car salesman?32 RESULT 0.48 0 0.07 0.41 1.36 ?combustion gas?33 STIMULUS 0 0 0 0 034 EXTENT 0 0 0 0 035 PREDICATE 0.04 0 0 0 0.10OTHERS 23.19 6.64 5.19 5.77 15.73 ?airmail stamp?Total no.
of examples 100   (2302) 100   (286) 100   (1271) 100   (2441) 100   (953)Table 4: The distribution of the semantic relations on the annotated corpus after agreement.
The number in parenthesesrepresent number of examples for a particular pattern.functionderived for each syntactic pattern.2.5.2 Feature spaceAn essential aspect of our approach below is theword sense disambiguation (WSD) of the content words(nouns, verbs, adjectives and adverbs).
Using a state-of-the-art open-text WSD system, each word is mappedinto its corresponding WordNet 2.0 sense.
When dis-ambiguating each word, the WSD algorithm takes intoaccount the surrounding words, and this is one importantway through which context gets to play a role in the se-mantic classification of NPs.So far, we have identified and experimented with thefollowing NP features:1.
Semantic class of head noun specifiesthe WordNet sense (synset) of the head noun andimplicitly points to all its hypernyms.
It is extractedautomatically via a word sense disambiguation module.The NP semantics is influenced heavily by the meaningof the noun constituents.
Example: ?car manufacturer?is a kind of manufacturer that MAKES/PRODUCES cars.2.
Semantic class of modifier nounspecifies the WordNet synset of the modifier noun.
Incase the modifier is a denominal adjective, we take thesynset of the noun from which the adjective is derived.Example: ?musical clock?
- MAKE/PRODUCE, and?electric clock?- INSTRUMENT.2.5.3 Learning ModelsSeveral learning models can be used to provide the dis-criminating function.
So far we have experimentedwith three models: (1) semantic scattering, (2) decisiontrees, and (3) naive Bayes.
The first is described below,the other two are fairly well known from the machinelearning literature.Semantic Scattering.
This is a new model developedby us particularly useful for the classification of com-pound nominals     without nominalization.
The se-mantic relation in this case derives from the semantics ofthe two noun concepts participating in these constructionsas well as the surrounding context.Model Formulation.
Let us define withandthe sets of semantic class features (ie,WordNet synsets) of the NP modifiers and, respectivelyNP heads (ie features 2 and 1).
The compound nominalsemantics is distinctly specified by the feature pair,written shortly as.
Given feature pair, the proba-bility of a semantic relation r is    ffi  , de-fined as the ratio between the number of occurrences of arelation r in the presence of feature pairover the num-ber of occurrences of feature pairin the corpus.
Themost probable relation is   ffi    ffi    ffiSince the number of possible noun synsets combina-tions is large, it is difficult to measure the quantitiesff ffi and ff ffi on a training corpus to calculate   ffi .
One way of approximating the feature vectoris to perform a semantic generalization, by replacingthe synsets with their most general hypernyms, followedby a series of specializations for the purpose of eliminat-ing ambiguities in the training data.
There are 9 noun hi-erarchies, thus only 81 possible combinations at the mostgeneral level.
Table 5 shows a row of the probability ma-trix    ffi forfiffffiflfl "!fiffffiflfl.
Each entry, forwhich there is more than one relation, is scattered intoother subclasses through an iterative process till there isonly one semantic relation per line.
This can be achievedby specializing the feature pair?s semantic classes withtheir immediate WordNet hyponyms.
The iterative pro-cess stops when new training data does not bring any im-provements (see Table 6).2.5.4 Overview of the Preliminary ResultsThe f-measure results obtained so far are summarizedin Table 7.
Overall, these results are very encouraginggiven the complexity of the problem.2.5.5 Error AnalysisAn important way of improving the performance of asystem is to do a detailed error analysis of the results.We have analyzed the sources of errors in each case andfound out that most of them are due to (in decreasing or-der of importance): (1) errors in automatic sense disam-biguation, (2) missing combinations of features that occurin testing but not in the training data, (3) levels of special-ization are too high, (4) errors caused by metonymy, (6)errors in the modifier-head order, and others.
These er-rors could be substantially decreased with more researcheffort.A further analysis of the data led us to consider a differ-ent criterion of classification that splits the examples intonominalizations and non-nominalizations.
The reason isthat nominalization noun phrases seem to call for a differ-ent set of learning features than the non-nominalizationnoun phrases, taking advantage of the underlying verb-argument structure.
Details about this approach are pro-vided in (Girju et al 2004)).3 ApplicationsSemantic relations occur with high frequency in opentext, and thus, their discovery is paramount for many ap-plications.
One important application is Question An-swering.
A powerful method of answering more difficultquestions is to associate to each question the semantic re-lation that reflects the meaning of that question and thensearch for that semantic relation over the candidates ofsemantically tagged paragraphs.
Here is an example.Q.
Where have nuclear incidents occurred?
From thequestion stem word where, we know the question asksfor a LOCATION which is found in the complex nomi-nal ?Three Mile Island?-LOCATION of the sentence ?TheThree Mile Island nuclear incident caused a DOE policycrisis?, leading to the correct answer ?Three Mile Island?.Q.
What did the factory in Howell Michigan make?The verb make tells us to look for a MAKE/PRODUCErelation which is found in the complex nominal ?carfactory?-MAKE/PRODUCE of the text: ?The car factory inHowell Michigan closed on Dec 22, 1991?
which leads toanswer car.Another important application is building semanticallyrich ontologies.
Last but not least, the discovery oftext semantic relations can improve syntactic parsing andeven WSD which in turn affects directly the accuracy ofother NLP modules and applications.
We consider theseapplications for future work.ReferencesC.
Baker, C. Fillmore, and J. Lowe.
1998.
The BerkeleyFrameNet Project.
In Proceedings of COLLING/ACL,Canada.D.
Blaheta and E. Charniak.
2000.
Assigning functiontags to parsed text.
In Proceedings of the 1st AnnualMeeting of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL), Seattle,WA.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
In Proceedings of ACL, Toulouse, France.W.
Dolan, L. Vanderwende, and S. Richardson.
1993.Automatically deriving structured KBs from on-linedictionaries.
In Proceedings of the Pacific Associationfor Computational Linguistics Conference.P.
Downing.
1977.
On the creation and use of Englishcompound nouns.
Language, 53(4), 810-842.T.
Finin.
1980.
The Semantic Interpretation of Com-pound Nominals.
Ph.D dissertation, University of Illi-nois, Urbana, Illinois.D.
Gildea and D. Jurafsky.
2002.
Automatic Labeling ofSemantic Roles.
In Computational Linguistics, 28(3).Relation no.
1 2 3 6 7 11 13 15 16 21 25 the rest  		 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0Table 5: Sample row from the conditional probability table where the feature pair is entity-entity.
The numbers in thetop row identify the semantic relations (as in Table 4).Level Level 1 Level 2 Level 3 Level 4Number of modifier 9 52 70 122featuresNumber head 9 46 47 47featuresNo.
of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734Number of features 1 152 181 225with only one relationAverage number of 2.7692 1.291 1.1765 1.144non-zero relations per lineTable 6: Statistics for the semantic class features by level of specialization.Syntactic Semantic Decision NaivePattern Scattering Tree BayesComplex NN      fifffl     fiff ffi  Nominals AdjN  !fl fi"   fi fi   !# #fi#  Genitives ?S  # ff!   fi$        fl    Of "fi   ff   39.94% 34.72%Adjective Phrases !fl     fiff "fiff       #  Table 7: F-measure results for the semantic classification of NP patterns obtained with four learning models on acorpus with an 80/20 training/testing ratio.
?NA?
means not available.D.
Gildea and M. Palmer.
2002.
The Necessity of Parsingfor Predicate Argument Recognition.
In Proceedingsof the 40th Meeting of the Association for Computa-tional Linguistics (ACL 2002).R.
Girju, A. Badulescu, and D. Moldovan.
2003.
Learn-ing Semantic Constraints for the Automatic Discoveryof Part-Whole Relations.
In Proceedings of the HumanLanguage Technology Conference (HLT-03), Canada.R.
Girju, A.M. Giuglea, M. Olteanu, O. Fortu, and D.Moldovan.
2004.
Support Vector Machines Applied tothe Classification of Semantic Relations in Nominal-ized Noun Phrases.
In Proceedings of HLT/NAACL2004 - Computational Lexical Semantics workshop,Boston, MA.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.
AddingSemantic Annotation to the Penn TreeBank.
In Pro-ceedings of the Human Language Technology Confer-ence (HLT 2002), California.M.
Lapata.
2002.
The Disambiguation of Nominalisa-tions.
In Computational Linguistics 28:3, 357-388.M.
Lauer and M. Dras.
1994.
A probabilistic model ofcompound nouns.
In Proceedings of the 7th AustralianJoint Conference on AI.M.
Lauer.
1995.
Designing Statistical Language Learn-ers: Experiments on Compound Nouns.
In PhD thesis,Macquarie University, Sidney.Judith Levi.
1979.
The Syntax and Semantics of Com-plex Nominals.
New York: Academic Press.R.
Navigli and P. Velardi.
2003.
Ontology Learning andIts Application to Automated Terminology Transla-tion.
In IEEE Intelligent Systems.J.
Pustejovsky, S. Bergler, and P. Anick.
1993.
Lexicalsemantic techniques for corpus analysis.
In Computa-tional Linguistics, 19(2).R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A comprehensive grammar of english language, Long-man, Harlow.B.
Rosario and M. Hearst.
2001.
Classifying the Se-mantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy.
In the Proceedings of the2001 Conference on Empirical Methods in NaturalLanguage Processing, (EMNLP 2001), Pittsburgh, PA.B.
Rosario, M. Hearst, and C. Fillmore.
2002.
The De-scent of Hierarchy, and Selection in Relational Seman-tics.
In the Proceedings of the Association for Compu-tational Linguistics (ACL-02), University of Pennsyl-vania.M.
Semmelmeyer and D. Bolander.
1992.
The New Web-ster?s Grammar Guide.
Lexicon Publications, Inc.J.
F. Sowa.
1994.
Conceptual Structures: InformationProcessing in Mind and Machine.
Addison Wesley.L.
Vanderwende.
1994.
Algorithm for automatic in-terpretation of noun sequences.
In Proceedings ofCOLING-94, pg.
782-788.
