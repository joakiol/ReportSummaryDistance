Proceedings of the ACL Student Research Workshop, pages 150?157,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExploring Word Order Universals: a Probabilistic Graphical ModelApproachXia LuDepartment of LinguisticsUniversity at BuffaloBuffalo, NY USAxialu@buffalo.eduAbstractIn this paper we propose a probabilistic graph-ical model as an innovative framework forstudying typological universals.
We view lan-guage as a system and linguistic features as itscomponents whose relationships are encodedin a Directed Acyclic Graph (DAG).
Takingdiscovery of the word order universals as aknowledge discovery task we learn the graph-ical representation of a word order sub-systemwhich reveals a finer structure such as directand indirect dependencies among word orderfeatures.
Then probabilistic inference enablesus to see the strength of such relationships:given the observed value of one feature (orcombination of features), the probabilities ofvalues of other features can be calculated.
Ourmodel is not restricted to using only two val-ues of a feature.
Using imputation techniqueand EM algorithm it can handle missing val-ues well.
Model averaging technique solvesthe problem of limited data.
In addition the in-cremental and divide-and-conquer method ad-dresses the areal and genetic effects simulta-neously instead of separately as in Daum?
IIIand Campbell (2007).1 IntroductionEver since Greenberg (1963) proposed 45 uni-versals of language based on a sample of 30 lan-guages, typologists have been pursuing this topicactively for the past half century.
Since some ofthem do not agree with the term (or concept) of?universal?
they use other terminology such as?correlation?, ?co-occurrence?, ?dependency?,?interaction?
and ?implication?
to refer to therelationships between/among linguistic featurepairs most of which concern morpheme andword order.
Indeed the definition of ?universals?has never been clear until recently, when mosttypologists agreed that such universals should bestatistical universals which are ?statisticaltendencies?
discovered from data samples byusing statistical methods as used in any otherscience.
Only those tendencies that can be ex-trapolated to make general conclusions about thepopulation can be claimed to be ?universals?since they reflect the global preferences of valuedistribution of linguistic features across genea-logical hierarchy and geographical areas.Previous statistical methods in the research ofword order universals have yielded interestingresults but they have to make strong assumptionsand do a considerable amount of data prepro-cessing to make the data fit the statistical model(Greenberg, 1963; Hawkins, 1982; Dryer, 1989;Nichols, 1986; Justeson & Stephens, 1990).
Re-cent studies using probabilistic models are muchmore flexible and can handle noise and uncer-tainty better (Daum?
III & Campbell, 2007; Dunnet al 2011).
However these models still rely onstrong theoretic assumptions and heavy datatreatment, such as using only two values of wordorder pairs while discarding other values, pur-posefully selecting a subset of the languages tostudy, or selecting partial data with completevalues.
In this paper we introduce a novel ap-proach of using a probabilistic graphical modelto study word order universals.
Using this modelwe can have a graphic representation of thestructure of language as a complex system com-posed of linguistic features.
Then the relationshipamong these features can be quantified as proba-bilities.
Such a model does not rely on strongassumptions and has little constraint on data.The paper is organized as follows: in Section 2we discuss the rationale of using a probabilisticgraphic model to study word order universalsand introduce our two models; Section 3 is aboutlearning structures and parameters for the twomodels.
Section 4 discusses the quantitativeanalysis while Section 5 gives qualitative analy-sis of the results.
Section 6 is about inferencesuch as MAP query and in Section 6 we discussthe advantage of using PGM to study word orderuniversals.1502 A new approach: probabilistic graph-ical modeling2.1 Rationale for using PGM in word orderstudyThe probabilistic graphical model is the marriageof probabilistic theory and graph theory.
It com-bines a graphical representation with a complexdistribution over a high-dimensional space.There are two major types of graphical represen-tations of distributions.
One is a Directed AcyclicGraph (DAG) which is also known as a Bayesiannetwork with all edges having a source and atarget.
The other is an Undirected Acyclic Graph,which is also called a Markov network with alledges undirected.
A mixture of these two types isalso possible (Koller & Friedman, 2009).There are two advantages of using this model tostudy word order universals.
First the graphicalstructure can reveal much finer structure of lan-guage as a complex system.
Most studies onword order correlations examine the pairwiserelationship, for example, how the order of verband object correlates with the order of noun andadjective.
However linguists have also noticedother possible interactions among the word orderfeatures, like chains of overlapping implications:Prep ?
((NAdj ?
NGen) & (NGen ?
NRel))proposed by Hawkins (1983); multi-conditionalimplications (Daum?
III, 2007); correlationsamong six word order pairs and three-way inter-actions  (Justeson & Stephens, 1990); spuriousword order correlations  (Croft et al 2011);chains of associations, e.g.
if C predicts B and Bpredicts A, then C predicts A redundantly (Bick-el, 2010b).
These claims about the possible inter-actions among word order features imply com-plex relationships among the features.
The studyof word order correlations started with pairwisecomparison, probably because that was what ty-pologists could do given the limited resources ofstatistical methods.
However when we study theproperties of a language, by knowing just severalword orders such as order of verb and object,noun and adpositions, etc., we are unable to sayanything about the language as a whole.
Here wewant to introduce a new perspective of seeinglanguage as a complex system.
We assume thereis a meta-language that has the universal proper-ties of all languages in the world.
We want amodel that can represent this meta-language andmake inferences about linguistic properties ofnew languages.
This system is composed of mul-tiple sub-systems such as phonology, morpholo-gy, syntax, etc.
which correspond to the subfieldsin linguistics.
In this paper we focus on the sub-system of word order only.The other advantage of PGM is that it enablesus to quantify the relationships among word or-der features.
Justeson & Stephens (1990) men-tioned the notion of ?correlation strength?
whenthey found out that N/A order appears lessstrongly related to basic V/S/O order and/oradposition type than is N/G order.
This is thebest a log-linear model can do, to indicatewhether a correlation is ?strong?, ?less strong?,?weak?
or ?less weak?.
Dunn et al(2011) usedBayes factor value to quantify the relationshipsbetween the word order pairs but they mistookthe strength of evidence for an effect as thestrength of the effect itself (Levy & Daum?
III,2011).
A PGM model for a word order subsys-tem encodes a joint probabilistic distribution ofall word order feature pairs.
Using probability wecan describe the degree of confidence about theuncertain nature of word order correlations.
Forexample, if we set the specific value as evidence,then we can get the values of other features usingan inference method.
Such values can be seen asquantified strength of relationship between val-ues of features.2.2 Our modelIn our word order universal modeling we will useDAG structure since we think the direction ofinfluence matters when talking about the rela-tionship among features.
In Greenberg (1966a)most of the universals are unidirectional, such as?If a language has object-verb order, then it alsohas subject-verb order?
while few are bidirec-tional universals.
The term ?directionality?
doesnot capture the full nature of the different status-es word order features have in the complex lan-guage system.
We notice in all the word orderstudies the order of SOV or OV was given spe-cial attention.
In Dryer?s study VO order is thedominant one which determines the set of wordorder pairs correlated with it (or not).
We assumeword order features have different statuses in thelanguage system and such differences should bemanifested by directionality of relationships be-tween feature pairs.
Therefore we choose DAGstructure as our current model framework.Another issue is the sampling problem.
Sometypologists (Dryer 1989, Croft 2003) have ar-gued that the language samples in the WALSdatabase (Haspelmath et al 2005) are not inde-pendent and identically distributed (i.i.d.)
be-cause languages can share the same feature val-ues due to either genetic or areal effect.
While151others (Maslova, 2010) argue that languageswithin a family have developed into distinct onesthrough the long history.
We notice that even wecan control the areal and genetic factors there arestill many other factors that can influence thetypological data distribution, such as 1) languagespeakers: cognitive, physiological, social, andcommunicative factors; 2) data collection: diffi-culty in identifying features; political biases(some languages are well documented); 3) ran-dom noise such as historical accidents.
Here wedo not make any assumption about the i.i.d prop-erty of the language samples and propose twomodels: one is FLAT, which assumes samplesare independent and identically distributed (i.i.d.
);the other is UNIV, which takes care of the possi-ble dependencies among the samples.
By com-paring the predictive power of these two modelswe hope to find one that is closer to the real dis-tribution.3 LearningTo build our models we need to learn both struc-ture and parameters for the two models.
We usedMurphy (2001)?s Bayesian Network Toolbox(BNT) and Leray & Francois (2004)?s BNTStructure Learning Package (BNT_SLP) for thispurpose.3.1 DataAs we mentioned earlier we will restrict our at-tention to the domain of word order only in thispaper.
In the WALS database there are 56 fea-tures belonging to the ?Word Order?
category.Because some of the features are redundant, wechose 15 sets of word order features which are:S_O_V1 (order of subject, object and verb) [72],S_V (order of subject and verb) [3], O_V (orderof object and verb) [3], O_Obl_V (order of Ob-ject, Oblique, and Verb) [6], ADP_NP (order ofadposition and noun phrase) [5], G_N (order ofgenitive and noun) [3], A_N (order of adjectiveand noun) [4], Dem_N (order of demonstrativeand noun) [4], Num_N (order of numeral andnoun) [4], R_N (order of relative clause andnoun) [7], Deg_A (order of degree word and ad-jective) [3], PoQPar (position of polar questionparticles) [6], IntPhr (position of interrogativephrases in content questions) [3], AdSub_Cl (or-der of adverbial subordinator and clause) [5],1 The detailed descriptions of these word order features andvalues can be found at http://wals.info/.2 The number in the square brackets indicates the number ofvalues for that feature.Neg_V (order of negative morpheme and verb)[4].
We did some minimal treatment of data.
ForNeg_V which has 17 values we collapsed its val-ues 7-17 to 6 (?Mixed?).
For Dem_N and Neg_V,we treat word and suffix as the same and col-lapsed values 1 and 3 to 1, and values 2 and 4 to2.
After deleting those languages with no valuefor all 15 word order features we have 1646 dataentries.
This database is very sparse: in overallthe percentage of missing values is 31%.
Forseven features more than 50% of the languageshave values missing.3.2 Learning the FLAT modelThere are two big problems in learning DAGstructure for the FLAT model.
One is caused bylarge number of missing values.
Because EMmethod for structures from incomplete data takesvery long time to converge due to the large pa-rameter space of our model, we decided to useimputation method to handle the missing dataproblem (Singh, 1997).
The other difficulty iscaused by limited data.
To solve this problem weused model averaging by using bootstrap repli-cates (Friedman et al 1999).
We use GES(greedy search in the space of equivalent classes)algorithm in BNT_SLP to learn structure from abootstrap dataset because it uses CPDAGs torepresent Markov equivalent classes whichmakes graph fusion easier.
The algorithm is asfollows:1) Use nearest-neighbor method to impute missingvalues in the original dataset D and create a com-plete dataset ?
?.2) Create T=200 bootstrap resamples by resamplingthe same number of instances as the original da-taset with replacement from ??
.
Then for eachresample ???
learn the highest scoring structure ??
?.3) Fuse the 200 graphs into a single graph ??
usingthe ?Intergroup Undirected Networks Integration?method (Liu et al 2007).
Then usecpdag_to_dag.m in BNT_SLP to change  ??
into adirected graph ??
?.4) Compute the BIC scores of ???
using the 200resamples and choose the highest one.
If the con-vergence criterion (change of BIC is less than10??
compared with the previous iteration) is met,stop.
Otherwise go to Step 5.5) Learn 200 sets of parameters ???
for ???
using the200 resamples and take a weighted-average as thefinal parameters ???.
Also use EM algorithm anddataset D to learn parameters  ???
for ???.
Choosethe parameters ?
between  ???
and  ???
that givesthe highest BIC score.
Use MAP estimation to fillin the missing values in D and generate a completedataset ????.
Go to Step 2.152The structure for the FLAT model is shown inFigure 1.Figure 1.
DAG structure of the FLAT model3.3 Learning the UNIV modelAs discussed in Section 2.2, the possible depend-encies among language samples pose difficultyfor statistical methods using the WALS data.Daum?
III & Campbell (2007)?s hierarchicalmodels provided a good solution to this problem;however their two models LINGHIER and DIS-THIER dealt with genetic and areal influencesseparately and the two separate results still donot tell us what the ?true universals?
are.Instead of trying to control the areal and genet-ic and other factors, we propose a different per-spective here.
As we have mentioned, the kind ofuniversals we care about are the stable propertiesof language, which means they can be foundacross all subsets of languages.
Therefore tosolve the problem of dependence among the lan-guages we take an incremental and divide-and-conquer approach.
Using clustering algorithm weidentified five clusters in the WALS data.
Ineach cluster we picked 1/n of the data and com-bine them to make a subset.
In this way we canhave n subsets of data which have decreased de-gree of dependencies among the samples.
Welearn a structure for each subset and fuse the ngraphs into one single graph.
The algorithm is asfollows:1) Use nearest-neighbor method to impute missingvalues and create M complete datasets ??
(1 ??
?
?
).2) For each ??
divide the samples into n subsets.Then for each subset ???
learn the highest scoringstructure ???
.3) Fuse the n graphs into a single graph ??
using the?Intragroup Undirected Networks Integration?method (Liu et al 2007).4) Fuse the M graphs to make a single directed graph???
as in Step 3 in the previous section.5) Compute the BIC score of ???
using datasets ??
(1 ?
?
?
?)
and choose the highest score.
If theconvergence criterion (same as in the previous sec-tion) is met, stop.
Otherwise go to Step 6.6) Learn parameters ??
for ???
using datasets ??
(1 ?
?
?
?)
and take a weighted-average as thefinal parameters ???.
Also use EM algorithm andoriginal dataset to learn parameters  ???
for ??
?.Choose the parameters ?
among ???
and  ???
thatgives the highest BIC score.
Use MAP estimationto fill in the missing values in D and generate an-other M complete dataset.
Go to Step 2.The final structure for the UNIV model isshown in Figure 2.Figure 2.
DAG structure of the UNIV modelThe semantics of a DAG structure cannot besimply interpreted as causality (Koller & Fried-man, 2009).
From this graph we can see wordorder features are on different tiers in the hierar-chy.
The root S_O_V seems to ?dominate?
allthe other features; noun modifiers and noun arein the middle tier while O_Obl_V, AdSub_Cl,Deg_A, Num_N, R, Neg_V and PoQPar are theleaf nodes which might indicate their smallestcontribution to the word order properties of alanguage.
O_V seems to be an important nodesince most paths start from it indicating its influ-ence can flow to many other nodes.We can also see there are two types of connec-tions among the nodes: 1) direct connection: anytwo nodes connected with an arc directly haveinfluence on each other.
This construction induc-es a correlation between the two features regard-less of the evidence.
This type of dependencywas the one most explored in the previous litera-tures.
2) three cases of indirect connections: a.indirect causal effect: e.g.
O_V does not influ-ence G_N directly, but via ADP_NP; b. indirectevidential effect: knowing G_N will change ourbelief about O_V indirectly; c. common cause:e.g.
ADP_NP and O_Obl_V can influence eachother without O_V being observed.
Our modelreveals a much finer structure of the word order153sub-system by distinguishing different types ofdependencies that might have been categorizedsimply as ?correlation?
in the traditional statisti-cal methods.4 Quantitative Analysis of ResultsThe word order universal results are difficult toevaluate because we do not know the correct an-swers.
Nonetheless we did a quantitative evalua-tion following Daum?
III and Campbell (2007)?smethod.
The results are shown in Figure 3.Figure 3.
Results of Quantitative EvaluationAs we can see the predictive power of theUNIV model is much better than that of theFLAT model.
The accuracy of our both modelsis lower than those of Daum?
III and Campbell?s.But this does not mean our models are worseconsidering the complexity in model learning.Instead our UNIV model shows steady accurateprediction for the top ten universals and has morestable performance compared with other models.Using the UNIV model we can do many typesof computation.
Besides pairwise feature values,we can calculate the probability of any combina-tion of word order feature values.
If we want toknow how value ?GN?
of feature ?G_N?
is de-pendent on value ?POST?
of feature ?ADP_NP?we set POST to be evidence (probability=100%)and get the probability of having ?GN?.
Such aprobability can be taken as a measurement ofdependence strength between these two values.We need more evidence for setting a thresholdvalue to define a word order universal but fornow we just use 0.5.
We calculated the probabili-ties of all pairwise feature values in the UNIVmodel which can found athttp://www.acsu.buffalo.edu/~xialu/univ.html.5 Qualitative Analysis of ResultsWe also did qualitative evaluation through com-parison with the well-known findings in wordorder correlation studies.
We compared our re-sults with three major works: those of Green-berg?s, Dryer?s, and Daum?
III and Campbell?s.5.1 Evaluation: compare with Greenberg?sand Dryer?s workComparison with Greenberg?s work is shown inTable 1 (in Appendix A).
If the probability isabove 0.5 we say it is a universal and mark it red.We think values like 0.4-0.5 can also give ussome suggestive estimates therefore we markthese green.
For Universal 2, 3, 4, 5, 10, 18 and19, our results conform to Greenberg?s.
But forothers there are discrepancies of different de-grees.
For example, for U12 our results show that?VSO?
can predict ?Initial?
but not very stronglycompared with ?SOV?
predicting ?Not_Initial?.Table 2 (in Appendix A) shows our compari-son with Dryer (1992)?s work.
We noticed thereis an asymmetry in terms of V_O?s influence onother word order pairs, which was not discussedin previous work.
In the correlated pairs, onlyADP_NP and G_N show bidirectional correla-tion with O_V while PoQPar becomes a non-correlated pair.
In the non-correlated pairs,Dem_N becomes a correlated pair and otherpairs also show correlation of weak strength.Most of our results therefore do not confirmDryer?s findings.5.2 Evaluation: compare with Daum?
IIIand Campbell?s workWe compared the probabilities of single valuepairs of the top ten word order universals withDaum?
III and Campbell?s results, which areshown in the following figures.Figure 4.
Compare with Daum?
III and Campbell?sHIER model0.50.550.60.650.70.750.80.850.90.9511 2 3 4 5 6 7 8 9 10probabilitiesthe first ten universalsp(true) prob PGM0.50.550.60.650.70.750.80.850.90.9511 2 3 4 5 6 7 8 9 10probabilitiesthe first ten universalsp(true) prob PGM0.50.550.60.650.70.750.80.850.90.950 1 2 3 4 5 6 7 8 9 10Prediction AccuracyNumber of Universals (        )FLAT UNIV154Figure 5.
Compare with Daum?
III and Campbell?sDIST modelP(true) is the probability of having the particu-lar implication; prob is the probability calculatedin a different way which is not specified in Dau-m?
III and Campbell?s work.
PGM is our model.It can be seen that our model provides moderatenumbers which fall between the two probabilitiesin Daum?
III and Campbell?s results.
In Figure 4the two universals that have the biggest gaps are:9) Prepositions ->VO and 10) Adjective-Noun->Demonstrative-Noun.
In Figure 5 the three uni-versals that have the biggest gaps are: 3) Noun-Genitive->Initial subordinator word, 6) Noun-Genitive->Prepositions and 8) OV->SV.
It ishard to tell which model does a better job just bydoing comparison like this.
Daum?
III andCampbell?s model computes the probabilities of3442 feature pairs separately.
Their model withtwo values as nodes does not consider the morecomplex dependencies among more than twofeatures.
Our model provides a better solution bytrying to maximize the joint probabilities of allword order feature pairs.6 InferenceBesides discovering word order universals, ourmodel can reveal more properties of word ordersub-system through various inference queries.
Atpresent we use SamIam3 for inference because ithas an easy-to-use interface for probabilistic in-ference queries.
Figure 6 (in Appendix B) givesan example: when we know the language is sub-ject preceding verb and negative morpheme pre-ceding verb, then we know the probability forthis language to have postpositions is 0.5349, aswell as the probabilities for the values of all oth-er features.The other type of query is MAP which aims tofind the most likely assignments to all of the un-observed variables.
For example, when we onlyknow that language is VO, we can use MAP que-ry to find the combination of values which hasthe highest probability (0.0032 as shown in Table3 in Appendix C).One more useful function is to calculate thelikelihood of a language in terms of word orderproperties.
If all values of 13 features of a lan-guage are known, then the probability (likelihood)of having such a language can be calculated.
Wecalculated the likelihood of eight languages andgot the results as shown in Figure 7 (in Appendix3 SamIam is a tool for modeling and reasoning with Bayesi-an networks ( http://reasoning.cs.ucla.edu/samiam/).C).
As we can see, English has the highest likeli-hood to be a language while Hakka Chinese hasthe lowest.
German and French have similar like-lihood; Portuguese and Spanish are similar butare less than German and French.
In other wordsEnglish is a typical language regarding word or-der properties while Hakka Chinese is an atypi-cal one.7 DiscussionProbabilistic graphic modeling provides solu-tions to the problems we noticed in the previousstudies of word order universals.
By modelinglanguage as a complex system we shift our atten-tion to the language itself instead of just features.Using PGM we can infer properties about a lan-guage given the known values and we can alsoinfer the likelihood of a language given all thevalues.
In the future if we include other domains,such as phonology, morphology and syntax, wewill be able to discover more properties aboutlanguage as a whole complex system.Regarding the relationships among the fea-tures since PGM can give a finer structure we areable to see how the features are related directlyor indirectly.
By using probability theory weovercome the shortcomings of traditional statisti-cal methods based on NHST.
Probabilities cap-ture our uncertainty about word order correla-tions.
Instead of saying ?A is correlated with B?,we can say ?A is correlated with B to a certainextent?.
PGM enables us to quantify ourknowledge about the word order properties oflanguages.Regarding the data treatment, we did very lit-tle preprocessing of data, therefore reducing thepossibility of bringing in additional bias fromother processes such as family construction inDunn et als experiment.
In addition we did notremove most of the values so that we can makeinferences based on values such as ?no determi-nant order?
and ?both orders?.
In this way weretain the information in our data to the largestextent.We think PGM has the potential to become anew methodology for studying word order uni-versals.
It also opens up many new possibilitiesfor studying linguistic typology as well:?
It can include other domains to build a morecomplex network and to discover more typologi-cal properties of languages.?
It can be used in field work for linguists tomake predictions about properties of unknownlanguages.155ReferencesBickel, B.
2010a.
Absolute and statistical universals.In Hogan, P. C.
(ed.)
The Cambridge Encyclopediaof the Language Sciences, 77-79.
Cambridge:Cambridge University Press.Bickel, B.
2010b.
Capturing particulars and univer-sals in clause linkage: a multivariate analysis.
InBril, I.
(ed.)
Clause-hierarchy and clause-linking:the syntax and pragmatics interface, pp.
51 - 101.Amsterdam: Benjamins.Croft, William.
2003.
Typology and universals.
2ndedn.
Cambridge: Cambridge University Press.Daphne Koller and Nir Friedman.
2009.
ProbabilisticGraphical Models: Principles and Techniques(Adaptive Computation and Machine Learning se-ries).
MIT Press, Aug 31, 2009Daum?, H., & Campbell, L. (2007).
A Bayesian mod-el for discovering typological implications.In Annual Meeting ?Association For Computation-al Linguistics (Vol.
45, No.
1, p. 65).D.M.
Chickering, D. Heckerman, and C. Meek.
1997.A Bayesian approach to learning Bayesian net-works with local structure.
Proceeding UAI'97Proceedings of the Thirteenth conference on Un-certainty in artificial intelligence.Dryer, M. S. 1989.
Large linguistic areas and lan-guage sampling.
Studies in Language 13,  257 ?292.Dryer, Matthew S. & Martin Haspelmath (eds.).
2011.The world atlas of language structures online.M?nchen: Max Planck Digital Library.Dryer, Matthew S. 2011.
The evidence for word ordercorrelations.
Linguistic Typology 15.
335?380.Dunn, Michael, Simon J. Greenhill, Stephen C. Lev-inson & Russell D. Gray.
2011.
Evolved structureof language shows lineage-VSHFL?FWUHQGVLQZRUG-order universals.
Nature 473.
79?82.E.
T. Jaynes.
2003.
Probability Theory: The Logic ofScience.
Cambridge University Press, Apr 10, 2003.Friedman, N. (1998, July).
The Bayesian structuralEM algorithm.
In Proceedings of the Fourteenthconference on Uncertainty in artificial intelli-gence (pp.
129-138).
Morgan Kaufmann PublishersInc.Friedman, N., Nachman, I., & Pe?r, D. (1999, July).Learning bayesian network structure from massivedatasets: the ?sparse candidate?
algorithm.In Proceedings of the Fifteenth conference on Un-certainty in artificial intelligence (pp.
206-215).Morgan Kaufmann Publishers Inc.Greenberg, J. H. 1963.
Some universals of grammarwith particular reference to the order of meaning-ful elements.
In Universals of Language, J. H.Greenberg, Ed.
MIT Press, Cambridge, MA, 73-113.Greenberg, Joseph H. 1966.
Synchronic and diachron-ic universals in phonology.
Language 42.
508?517.Greenberg, J. H. (1969).
Some methods of dynamiccomparison in linguistics.
Substance and structureof language, 147-203.Hawkins, John A.
1983.
Word Order Universals.
Ac-ademic Press, 1983.Justeson, J. S., & Stephens, L. D. (1990).
Explana-tions for word order universals: a log-linear analy-sis.
In Proceedings of the XIV International Con-gress of Linguists (Vol.
3, pp.
2372-76).Leray, P., & Francois, O.
(2004).
BNT structurelearning package: Documentation and experiments.Levy, R., & Daum?
III, H. (2011).
Computationalmethods are invaluable for typology, but the mod-els must match the questions: Commentary onDunn et al2011).Linguistic Typology.
(To appear).Liu, F., Tian, F., & Zhu, Q.
(2007).
Bayesian networkstructure ensemble learning.
In Advanced DataMining and Applications (pp.
454-465).
SpringerBerlin Heidelberg.Maslova, Elena & Tatiana Nikitina.
2010.
Languageuniversals and stochastic regularity of languagechange: Evidence from cross-linguistic distribu-tions of case marking patterns.
Manuscript.Murphy, K. (2001).
The bayes net toolbox formatlab.
Computing science and statistics, 33(2),1024-1034.Perkins, Revere D. 1989.
Statistical techniques fordetermining language sample size.
Studies in Lan-guage 13.
293?315.Singh, M. (1997, July).
Learning Bayesian networksfrom incomplete data.
In Proceedings of the Na-tional conference on Artificial Intelligence (pp.534-539).
JOHN WILEY & SONS LTD.William Croft, Tanmoy Bhattacharya, Dave Klein-schmidt, D. Eric Smith and T. Florian Jaeger.
2011.Greenbergian universals, diachrony and statisticalanalyses [commentary on Dunn et al Evolvedstructure of language shows lineage-specific trendsin word order universals].
Linguistic Typology15.433-53.156AppendicesA.
Comparison with others?
workUniversals Dependencies UNIVU2: ADP_NP<=>N_G POST->GNPRE->NGGN->POSTNG->PRE83.5970.2978.4581.91U3: VSO->PRE VSO->PRE 74.41U4: SOV->POST SOV->POST 85.28U5: SOV&NG->NA SOV&NG->NA 68.95U9: PoQPar<=>ADP_NP Initial->PREFinal->POSTPRE->InitialPOST->Final41.8749.6715.8031.73U10: PoQPar<=> VSO all values ofPoQPar:VSO below 10%below10%U11: IntPhr->VS Initial->VS 24.12U12: VSO->IntPhr VSO->InitialSOV->InitialSOV->Not_Initial50.5428.5260.41U17: VSO->A_N VSO->A_N 24.86U18&19:A_N<=>Num_N<=>Dem_NAN->NumNAN->DemNNA->NNumNA->NDem68.8673.7461.7461.00U24: RN->POST (or AN) RN->POSTRN->AN65.7329.23Table 1.
Comparison with Greenberg?s workOV UNIV VO UNIVcorrelated pairsADP_NP(POST) 90.48 ADP_NP(PRE) 82.72G_N(GN) 79.38 G_N(NG) 61.49R_N(RN) 19.66 R_N(NR) 75.17PoQPar(Final) 31.89 PoQPar(Initial) 15.79AdSub_Cl (Final) 20.90 AdSub_Cl (Initial) 49.22IntPhr(Not_Initial) 58.74 IntPhr(Initial) 34.36non-correlated pairsA_N(AN) 29.48 A_N(NA) 65.00Dem_N(Dem_N) 52.27 Dem_N(N_Dem) 54.25Num_N(NumN) 41.6 Num_N(NNum) 49.25Deg_A(Deg_A) 43.48 Deg_A(A_Deg) 38.44Neg_V(NegV) 48.06 Neg_V(VNeg) 25.13Table 2.
Comparison with Dryer?s workB.
Probabilistic query example in SamIamFigure 6.
One query exampleC.
Inference examplesP(MAP,e)=0.0015052949102098631P(MAP|e)=0.003213814742532023Variable ValueA_N NAADP_NP PREAdSub_Cl InitialDeg_A Deg_ADem_N N_DemG_N NGIntPhr Not_InitialNeg_V NegVNum_N NNumO_Obl_V VOXPoQPar FinalR_N NRS_O_V SVOS_V SVTable 3.
MAP query exampleFigure 7.
Likelihood of eight languages in terms ofword order properties0.00E+005.00E-071.00E-061.50E-062.00E-062.50E-06157
