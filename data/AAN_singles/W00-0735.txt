In: Proceedings of CoNLL-2000 and LLL-2000, pages 157-159, Lisbon, Portugal, 2000.Single-C lass i f ier  Memory-Based  Phrase  Chunk ingJ o rn  Veenst ra  and Anta l  van  den  BoschILK / Computational Linguistics, Ti lburg University(veenst ra ,  antalb}@kub, n l1 In t roduct ionIn the shared task for CoNLL-2000, words andtags form the basic multi-valued features forpredicting a rich phrase segmentation code.While the tag features, containing WSJ paxt-of-speech tags (Marcus et al, 1993), have about45 values, the word features have more than10,000 values.
In our study we have looked athow memory-based learning, as implemented inthe TiMBL software system (Daelemans et al,2000), can handle such features.
We have lim-ited our search to single classifiers, thereby ex-plicitly ignoring the possibility to build a meta-learning classifier architecture that could be ex-pected to improve accuracy.
Given this restric-tion we have explored the following:1.
The generalization accuracy of TiMBLwith default settings (multi-valued fea-tures, overlap metric, feature weighting).2.
The usage of MVDM (Stanfill and Waltz,1986; Cost and Salzberg, 1993) (Section 2),which should work well on word value pairswith a medium or high frequency, but maywork badly on word value pairs with lowfrequency.3.
The straightforward unpacking of featurevalues into binary features.
On some taskswe have found that splitting multi-valuedfeatures into several binary features can en-hance performance of the classifier.4.
A heuristic search for complex features onthe basis of all unpacked feature values, andusing these complex features for the classi-fication task.2 Methods  and  DataThe data used for this shared task is compa-rable to the dataset used in (Buchholz et al,1999), who found an optimal windowing contextsize of five words and POS tags to the left, theword itself, and three words and POS tags tothe right.
We also used this window size, andhave applied TiMBL to the shared task datausing default TiMBL settings.
TiMBL and theabovementioned feature metrics are introducedin the following sections.IB I - IG  The default TiMBL setting, IBI-IG,(Daelemans et al, 1997) is a memory-basedlearning algorithm that builds a database ofinstances (the instance base) during learning.An instance consists of a fixed-length vector ofn feature-value pairs, and an information fieldcontaining the classification of that particularfeature-value vector.
After the instance baseis built, new (test) instances axe classified bymatching them to all instances in the instancebase, and by calculating with each match thedistance between the new instance X and thememory instance Y.The most basic metric for patterns with sym-bolic features is the Over lap metr ic  given inequation 1; where A(X, Y) is the distance be-tween patterns X and Y, represented by n fea-tures, wi is a weight for feature i, and 5 is thedistance per feature.
The k-NN algorithm withthis metric, and equal weighting for all featuresis called IB1 (Aha et al, 1991).
Usually k is setto 1.rt= wi (1)i=1where: 5(xi, Yi) = 0 i f  xi = Yi, else 1This distance metric simply counts the num-ber of (mis)matching feature values in both pat-157method k II AdjP AdvP ConjP Intj NP PP PRT SBAR VP II totIBI-IG 1 60.9 75.3 17.6 66.7 91.0 95.9 65.4 78.2 91.6 90.5IBI-IG 3 64.3 76.8 38.5 66.7 91.5 95.8 61.4 79.6 91.7 91.0IBI-IG 5 65.4 76.5 38.5 66.7 91.6 95.8 63.7 78.6 91.6 91.0IBI-IG 7 66.1 76.5 41.7 66.7 91.2 95.4 63.7 76.8 91.4 90.7MVDM-all 1 58.3 73.9 34.5 0 90.2 95.6 54.6 78.1 89.7 89.6MVDM-all 3 60.0 77.0 30.8 0 91.0 96.2 60.6 80.0 91.6 90.9MVDM-all 5 61.0 76.8 30.8 0 91.3 96.1 60.1 79.5 91.9 91.0MVDM-all 7 62.4 76.5 40.0 0 91.4 96.0 59.8 78.8 91.9 91.1MVDM-POS 1 59.3 76.6 12.5 50.0 89.6 96.0 69.5 78.3 91.1 89.8MVDM-POS 3 65.9 77.4 26.7 66.7 91.8 96.6 74.1 81.6 92.3 91.5MVDM-POS 5 63.7 76.6 37.0 66.7 92.1 96.4 71.4 79.8 92.1 91.5MVDM-POS 7 65.2 77.2 41.7 66.7 92.0 96.3 70.7 79.6 92.0 91.5Unpacked features 1 \]\[ 49.4 72.9 47.1 0 88.7 95.8 59.1 79.3 89.1 \[\[ 88.8 \[Complex features 1 \[\[ 58.8 75.8 0 66.7 91.0 94.7 74.6 87.8 94.3 1\[ 91.3 ITable 1: Results on the shared task dataset, in the top row the best performing metric is shown.terns.
In the absence of information aboutfeature relevance, this is a reasonable choice.However, Information Theory gives us a usefultool for measuring feature relevance (Quinlan,1986; Quinlan, 1993).
In fo rmat ion  Gain  (IG)weighting looks at each feature in isolation, andmeasures how much information it contributesto our knowledge of the correct class label.
TheInformation Gain of feature f is measured bycomputing the difference in uncertainty (i.e.
en-tropy) between the situations without and withknowledge of the value of that feature.
The re-sulting IG values can then be used as weights inequation 1.Mod i f ied  Va lue  D i f fe rence  Met r i c  TheModified Value Difference Metric (MVDM) (Costand Salzberg, 1993) estimates the distance be-tween two values of a feature by comparing theclass distribution of both features.
~?\[VDM cangive good estimates if there are enough occur-rences of the two values, but for low-frequentvalues unreliable values of MVDM can occur.
Forthis data we can expect that this sparseness ef-fect hinders the word features more than thePOS features.Unpack ing  Features  Unpacking featuresimplies that all feature values receive individ-ual weights.
(Van den Bosch and Zavrel, 2000)warn that this operation forces feature weightsto be based on less observations, which couldmake the weights unrealistic in view of testdata.
Moreover, the k nearest neighbors canbe expected to contain less instances with afixed k when unpacking features; this is usu-ally not beneficial for generalization accuracy(Van den Bosch and Zavrel, 2000).Complex  Features  In the previously dis-cussed versions of memory-based learning, fea-tures are treated as independent.
However,sometimes combinations of features or featurevalues may be very good predictors.
Since thereare many possible combinations, search strate-gies are needed to select the best.
Such strate-gies have been developed for rule induction algo-rithms (Clark and Niblett, 1989; Quinlan, 1993;Cohen, 1995), and they can be used to find com-plex features for memory-based learning as well.We followed the following procedure:1. apply Ripper (Cohen, 1995) to the trainingset, and collect the set of induced rules;2. recode the instances in the training and testset, by setting binary features denoting therules that apply to them;3. apply memory-based learning to the re-coded training set, and classify the recodedtest set.3 Exper iments  and  Resu l tsIn Table 2 we give an overview of the exper-iments with different metrics and settings.
Inthe first block of rows we give the results of thedefault setting with IBI- IG and with a varying kparameter (number of nearest neighbours).
Wecan see that a larger k improves performance toa certain extent.In the second series of experiments we haveused the MVDM metric.
Here, we also varied the158value of k. We found that a larger k yielded bet-ter results.
In a variant on this series we appliedMVDM only to the POS features.
As expectedthis variant gave slightly better esults.In the third series we unpacked the features.Compared to the previous experiment the re-sults were worse.
Apparently, sparseness resultsin bad feature weights.
This negative ffect ap-pears to have outweighted any positive effect ofinformative individual features.In the last experiments we used Ripper togenerate 390 complex features.
The results arecomparable to the best TiMBL settings.In Table 2 we give an overview of the preci-sion, recall and Ff~ = 1 of one of the best scoringsetting: IBi-IG with k -- 34 D iscuss ionWe found in the experiments that minor im-provements on the default settings of TiMBLcan be obtained by applying MVDM, particu-larly to the POS tags.
A larger k generallyimproved accuracy to a certain extent.
Un-packing the features did not give the expectedimprovement.
Complex features, however, did,and seem a promising alley to go.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37-66.S.
Buchholz, W. Daelemans, and J. Veenstra.
1999.Cascaded grammatical relation assignment.
InProceedings of EMNLP/VLC-99, pages 239-246,University of Maryland, USA.P.
Clark and T. Niblett.
1989.
The CN2 rule induc-tion algorithm.
Machine Learning, 3:261-284.W.
W. Cohen.
1995.
Fast effective rule induction.In Proc.
of the Twelfth International Conferenceon Machine Learning, Lake Tahoe, California.S.
Cost and S. Salzberg.
1993.
A weighted nearestneighbour algorithm for learning with symbolicfeatures.
Machine Learning, 10:57-78.W.
Daelemans, A.
Van den Bosch, and A. Weijters.1997.
IGTree: using trees for compression andclassification i  lazy learning algorithms.
Artifi-cial Intelligence Review, 11:407-423.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2000.
TiMBL: Tilburg Mem-ory Based Learner, version 3.0, reference manual.Tech.
Rep. ILK-0001, ILK, Tilburg University.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of en-test dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPprecision68.73%79.85%19.05%100.00%0.00%90.69%96.00%80.22%85.77%91.86%all 91.05% 92.03%recall F~=i63.24% 65.8775.06% 77.3844.44% 26.6750.00% 66.670.00% 0.0092.86% 91.7697.19% 96.5968.87% 74.1177.76% 81.5792.74% 92.3091.54Table 2: Overview of the precision, recall andF~ = 1 of IBi-IG with k -- 3 and MVDM.glish: The penn treebank.
Computational Lin-guistics, 19(2):313-330.J.R.
Quinlan.
1986.
Induction of Decision Trees.Machine Learning, 1:81-206.J.R.
Quinlan.
1993. c4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.C.
Stanfill and D. Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29(12):1213-1228, December.A.
Van den Bosch and J Zavrel.
2000.
Un-packing multi-valued features and classes inmemory-based language l arning.
In Proceedingsof ICML2000, Stanford University, CA, USA.159
