Proceedings of the SIGDIAL 2013 Conference, pages 366?368,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsThe Map Task Dialogue System:A Test-bed for Modelling Human-Like DialogueRaveesh Meena Gabriel Skantze Joakim GustafsonKTH Speech, Music and HearingStockholm, Swedenraveesh@csc.kth.se, gabriel@speech.kth.se, jocke@speech.kth.seAbstractThe demonstrator presents a test-bed forcollecting data on human?computer dia-logue: a fully automated dialogue systemthat can perform Map Task with a user.In a first step, we have used the test-bedto collect human?computer Map Task di-alogue data, and have trained various da-ta-driven models on it for detecting feed-back response locations in the user?sspeech.
One of the trained models hasbeen tested in user interactions and wasperceived better in comparison to a sys-tem using a random model.
The demon-strator will exhibit three versions of theMap Task dialogue system?each using adifferent trained data-driven model ofResponse Location Detection.1 IntroductionA common procedure in modelling human-likedialogue systems is to collect data on human?human dialogue and then train models that pre-dict the behaviour of the interlocutors.
However,we think that it might be problematic to use acorpus of human?human dialogue as a basis forimplementing dialogue system components.
Oneproblem is the interactive nature of the task.
Ifthe system produces a slightly different behav-iour than what was found in the original data,this would likely result in a different behaviourin the interlocutor.
Another problem is that hu-mans are likely to behave differently towards asystem as compared to another human (even if amore human-like behaviour is being modelled).Yet another problem is that much dialogue be-haviour is optional and therefore makes the actu-al behaviour hard to use as a gold standard.Figure 1: The Map Task system user interfaceTo improve current systems, we need both abetter understanding of the phenomena of humaninteraction, better computational models and bet-ter data to build these models.
An alternative ap-proach that has proven to be useful is to trainmodels on human?computer dialogue data col-lected through Wizard-of-Oz studies (Dahlb?cket al 1993).
However, the methodology mightbe hard to use when the issue under investigationis time-critical behaviour such as back-channels.A third alternative is to use a boot-strappingprocedure, where more and more advanced (orhuman-like) versions of the system are built iter-atively.
After each iteration, users interact withthe system and data is collected.
This data is thenused to train/improve data-driven models of in-teraction in the system.
A problem here, howev-er, is how to build the first iteration of the sys-tem, since many components, e.g., AutomaticSpeech Recognition (ASR), need some data to beuseful at all.In this demonstration we present a test-bed forcollecting data on time-critical human?computerdialogue phenomena: a fully automated dialoguesystem that can perform the Map Task with a366user (Skantze, 2012).
In a first step, followingthe boot-strapping procedure, we collected hu-man?computer Map Task dialogue data usingthis test-bed and then trained various data-drivenmodels on this data for detecting feedback re-sponse locations in user?s speech.
A trainedmodel has been implemented and evaluated ininteraction with users?in the same environmentused for collecting the data (Meena et al inpress).
The demonstrator will exhibit three ver-sions of the Map Task dialogue system?eachusing a different trained data-driven model ofResponse Location Detection (RLD).2 The Map Task Dialogue SystemMap Task is a common experimental paradigmfor studying human?human dialogue.
In our set-up, the user (the information giver) is given thetask of describing a route on a map to the system(the information follower).
The choice of MapTask is motivated partly because the system mayallow the user to keep the initiative during thewhole dialogue, and thus only produce responsesthat are not intended to take the initiative, mostoften some kind of feedback.
Thus, the systemmight be described as an attentive listener.The basic components of the system can beseen in Figure 2.
Dashed lines indicate compo-nents that were not part of the first iteration ofthe system (used for data collection), but whichhave been used in the second iteration of the sys-tem that uses a model trained on the collecteddata.
To make the human?computer Map Taskdialogue feasible without any full speech under-standing we have implemented a trick: the user ispresented with a map on a screen (see Figure 1)and instructed to move the mouse cursor alongthe route as it is being described.
The user is toldthat this is for logging purposes, but the real rea-son for this is that the system tracks the mouseposition and thus knows what the user is current-ly talking about.
It is thereby possible to producea coherent system behaviour without any speechrecognition at all, only basic speech detection.This often results in a very realistic interaction1.The system uses a simple energy-based speechdetector to chunk the user?s speech into inter-pausal units (IPUs), that is, periods of speech thatcontain no sequence of silence longer than 200ms.
Such a short threshold allows the system togive backchannels (seemingly) while the user is1 An example video can be seen athttp://www.youtube.com/watch?v=MzL-B9pVbOE.speaking or take the turn with barely any gap.The end of an IPU is a candidate for the RLDmodel to identify whether it is a Response Loca-tion (RL).ProsodicanalysisDialoguemanagerMapWindowSpeechdetectorResponseLocationDetectorContextualfeaturesProsodicfeaturesIPUs ResponseLocationMouse movementsSpeechsynthesizerResponseASR Syntactic featuresFigure 2: The basic components of the system.Each time the RLD detects a RL, the dialoguemanager produces a response, depending on thecurrent state of the dialogue and the position ofthe mouse cursor.
Table 1 shows an exampleMap Task dialogue illustrating the different typesof responses the system could produce.
The dia-logue manager always starts with an Introductionand ends with an Ending, once the mouse cursorhas reached the destination.
Between these, itselects from the other responses (Reprise Frag-ment, Clarification and Backchannel), partlyrandomly, but also depending on the length ofthe last user turn (longer turn triggers Repeat orRestart) and the current mouse location (e.g., aGuess if mouse hovers over the same area forsome time).Table 1: An example Map Task dialogue between auser (U) and system (S) [User utterances with ASRerror(s) are indicated with a *]S Introduction Could you help me to find my way tothe bus stop?U  start from the department storeS Backchannel yeahU  and ehS Guess Should I start by going west?U  yeah do thatU  then you will get to a meadow andwhen you get to the meadow*you get to a meadow until you getto the meadowS Repeat Eh, could you repeat that?U  you go straight and you see a mead-ow on your right sideS ClarificationRequestA green field?U  ehm yeah a fieldS Backchannel mhmU  pass the meadow and turn right soyou are going north*the meadow and turn right to yourgoing northS Backchannel okayU  you get to a restaurant367S RepriseFragmentA restaurantU  then you have to turn right and thatwill take you to*you have turned right and at willtake you turnS Restart Eh, I think I lost you at the restau-rant, could you take that again?
?U  at the junction go south and thenyou will get to the bus stopS Ending Okay, thanks a lot.3 Data-driven models of RLDUsing the system described in the previoussection a corpus of 50 human?computer MapTask dialogue was collected and used to train adata-driven model of RLD.
Since we didn?t havea sophisticated model of RLD during the firstiteration a na?ve model was used.
This modelwould wait for a random period between 0 and800 ms after an IPU ended.
If no new IPUs wereinitiated during this period, a RL was detected.Each IPU in the corpus was then manually la-belled as either Hold (a response would be inap-propriate) or Respond (a response is expected)type.
On this data various models were trainedon online extractable features?covering syntax,context and prosody.
Table 2 illustrates the per-formance of the various models.
Going a stepfurther, model #6 was deployed in the Map Taskdialogue system (with an ASR component) andevaluated in user interactions.
The result sug-gests that the trained model provide for smoothturn-transitions in contrast to the Random model(Meena et al in press).Table 2: Performance of various models of RLD[NB: Na?ve Bayes; SVM: Support Vector Machine;Models with * will be exhibited in the demonstration]# RLD model % accuracy (on ASR results)1* Random 50.79% majority class baseline2 Prosody 64.5% (SVM learner)3 Context 64.8% (SVM learner)4*Prosody+ Context69.1% (SVM learner)5 Syntax 81.1% (NB learner)6*Syntax+ Prosody+ Context82.0 % (NB learner)4 Future applicationsThe Map Task test-bed presented here has thepotential for modelling other human-like conver-sational behaviour in dialogue systems:Clarification strategies: by deploying explicit(did you mean turn right?)
and implicit (a reprisesuch as turn right) or elliptical (?right??)
clarifi-cation forms in the grounding process one couldinvestigate the efficiency and effectively of thesehuman-like clarification strategies.User utterance completion: It has been sug-gested that completion of user utterances by adialogue system would result in human-like con-versational interactions.
However, completinguser?s utterance at every opportunity may not bethe best strategy (DeVault et al 2009).
The pre-sented system could be used to explore when it isappropriate to do so.
We have observed in ourdata that the system dialogue acts Guess (cf.
Ta-ble 1) and Reprise often helped the dialogue pro-ceed further ?
by completing user utterances ?when the user had difficulty describing a land-mark on a route.Visual cues: the system could be integrated ina robotic head, such as Furhat (Al Moubayed etal., 2013), and visual cues from the user could beused for improving the current model of RLD.This could be used further to explore the use ofextra-linguistic system behaviours, such as headnods and facial gestures, as feedback responses.AcknowledgementThis work is supported by the Swedish researchcouncil (VR) project Incremental processing inmultimodal conversational systems (2011-6237)ReferencesAl Moubayed, S., Skantze, G., & Beskow, J.
(2013).The Furhat Back-Projected Humanoid Head - Lipreading, Gaze and Multiparty Interaction.
Interna-tional Journal of Humanoid Robotics, 10(1).Dahlb?ck, N., J?nsson, A., & Ahrenberg, L. (1993).Wizard of Oz studies ?
why and how.
In Proceed-ings from the 1993 International Workshop on In-telligent User Interfaces (pp.
193-200).DeVault, D., Sagae, K., & Traum, D. (2009).
Can IFinish?
Learning When to Respond to IncrementalInterpretation Results in Interactive Dialogue.
InProceedings of SIGdial (pp.
11-20).
London, UK.Meena, R., Skantze, G., & Gustafson, J.
(in press).
AData-driven Model for Timing Feedback in a MapTask Dialogue System.
To be published in 14thAnnual Meeting of the Special Interest Group onDiscourse and Dialogue - SIGdial.
Metz, France.Skantze, G. (2012).
A Testbed for Examining theTiming of Feedback using a Map Task.
In Pro-ceedings of the Interdisciplinary Workshop onFeedback Behaviors in Dialog.
Portland, OR.368
