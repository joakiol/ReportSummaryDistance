First Joint Conference on Lexical and Computational Semantics (*SEM), pages 356?364,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSemEval-2012 Task 2: Measuring Degrees of Relational SimilarityDavid A. JurgensDepartment of Computer ScienceUniversity of California, Los Angelesjurgens@cs.ucla.eduSaif M. MohammadEmerging TechnologiesNational Research Council Canadasaif.mohammad@nrc-cnrc.gc.caPeter D. TurneyEmerging TechnologiesNational Research Council Canadapeter.turney@nrc-cnrc.gc.caKeith J. HolyoakDepartment of PsychologyUniversity of California, Los Angelesholyoak@lifesci.ucla.eduAbstractUp to now, work on semantic relations has fo-cused on relation classification: recognizingwhether a given instance (a word pair such asvirus:flu) belongs to a specific relation class(such as CAUSE:EFFECT).
However, instancesof a single relation class may still have signif-icant variability in how characteristic they areof that class.
We present a new SemEval taskbased on identifying the degree of prototypi-cality for instances within a given class.
Asa part of the task, we have assembled the firstdataset of graded relational similarity ratingsacross 79 relation categories.
Three teamssubmitted six systems, which were evaluatedusing two methods.1 IntroductionRelational similarity measures the degree of corre-spondence between two relations, where instancepairs that have high relational similarity are said tobe analogous, i.e., to express the same relation (Tur-ney, 2006).
However, a class of analogous relationsmay still have significant variability in the degree ofrelational similarity of its members.
Consider thefour word pairs dog:bark, cat:meow, floor:squeak,and car:honk.
We could say that these four X:Ypairs are all instances of the semantic relation EN-TITY:SOUND; that is, X is an entity that character-istically makes the sound Y .
Within a class of anal-ogous pairs, certain pairs are more characteristic ofthe relation.
For example, many would agree thatdog:bark and cat:meow are better prototypes of theENTITY:SOUND relation than floor:squeak.
Our taskrequires automatic systems to quantify the degree ofprototypicality of a target pair by measuring the re-lational similarity between it and pairs that are givenas defining examples of a particular relation.So far, most work in semantic relations has fo-cused on differences between relation categories forclassifying new relation instances.
Past SemEvaltasks that use relations have focused largely on dis-crete classification (Girju et al, 2007; Hendrickx etal., 2010) and paraphrasing the relations connectingnoun compounds with a verb (Butnariu et al, 2010),which is also a form of discrete classification due tothe lack of continuous degrees.
However, there issome loss of information in any discrete classifica-tion of semantic relations.
Furthermore, while somediscrete classifiers provide a degree of confidence orprobability for a relation classification, there is noa priori reason that such values would correspondto human prototypicality judgments.
Our proposedtask is distinct from these past tasks in that we fo-cus on measuring the degree of relational similarity.1A graded measure of the degree of relational simi-larity would tell us that dog:bark is more similar tocat:meow than to floor:squeak.
The discrete classifi-cation ENTITY:SOUND drops this information.Systems that are successful at identifying degreesof relation similarity can have a significant impactwhere an application must choose between multi-ple instances of the same relation.
We illustratethis with two examples.
First, consider a rela-tional search task (Cafarella et al, 2006).
A userof a relational search engine might give the query,1Task details and data are available athttps://sites.google.com/site/semeval2012task2/ .356Subcategory Relation name Relation schema Paradigms Responses8(e) AGENT:GOAL ?Y is the goal of X?
pilgrim:shrine patient:healthassassin:death runner:finishclimber:peak astronaut:space5(e) OBJECT:TYPICAL ACTION ?an X will typically Y ?
glass:break ice:meltsoldier:fight lion:roarjuggernaut:crush knife:stab4(h) DEFECTIVE ?an X is is a defect in Y ?
fallacy:logic pimple:skinastigmatism:sight ignorance:learninglimp:walk tumor:bodyTable 1: Examples of the three manually selected paradigms and the corresponding pairs generated by Turkers.
?List all things that are part of a car.?
SemEval-2007 Task 4 proposed that a relational search enginewould use semantic relation classification to answerqueries like this one.
For this query, a classifier thatwas trained with the relation PART:WHOLE would beused.
However, a system for measuring degrees ofrelational similarity would be better suited to rela-tional search than a discrete classifier, because therelational search engine could then rank the outputlist in order of applicability.
For the same query, thesearch engine could rank each item X in descendingorder of the degree of relational similarity betweenX:car and a training set of prototypical examples ofthe relation PART:WHOLE.
This would be analogousto how standard search engines rank documents orweb pages in descending order of relevance to theuser?s query.As a second example, consider the role of rela-tional similarity in analogical transfer.
When facedwith a new situation, we look for an analogous sit-uation in our past experience, and we use analogi-cal inference to transfer information from the pastexperience (the source domain) to the new situation(the target domain) (Gentner, 1983; Holyoak, 2012).Analogy is based on relational similarity (Gentner,1983; Turney, 2008).
The degree of relational sim-ilarity in an analogy is indicative of the likelihoodthat transferred knowledge will be applicable in thetarget domain.
For example, past experience tells usthat a dog barks to send a signal to other creatures.
Ifwe transfer this knowledge to a new experience witha cat meowing, we can predict that the cat is sendinga signal, and we can act appropriately with that pre-diction.
If we transfer this knowledge to a new expe-rience with a floor squeaking, we might predict thatthe floor is sending a signal, which might lead us toact inappropriately.
If we have a choice among sev-eral source analogies, usually the source pair withthe highest degree of relational similarity to the tar-get pair will prove to be the most useful analogy inthe target domain, providing practical benefits be-yond discrete relational classification.2 Task DescriptionHere, we describe our task and the two-level hierar-chy of semantic relation classes used for the task.2.1 ObjectiveOur task is to rate word pairs by the degree towhich they are prototypical members of a given re-lation class.
The relation class is specified by afew paradigmatic (highly prototypical) examples ofword pairs that belong to the class and also by aschematic representation of the relation class.
Thetask requires comparing a word pair to the paradig-matic examples and/or the schematic representation.For example, suppose the relation class is REVERSE.We may specify this class by the paradigmatic ex-amples attack:defend, buy:sell, love:hate, and theschematic representation ?X is the reverse act ofY ?
or ?X may be undone by Y .?
Given a pairsuch as repair:break, we compare this pair to theparadigmatic examples and/or the schematic repre-sentation, in order to estimate its degree of prototyp-icality.
The challenges are (1) to infer the relationfrom the paradigmatic examples and identify whatrelational or featural attributes best characterize thatrelation, and (2) to identify the relation of the givenpair and rate how similar it is to that shared by theparadigmatic examples.3572.2 Relation CategoriesResearchers in psychology and linguistics have con-sidered many different categorizations of semanticrelations.
The particular relation categorization isoften driven by both the type of data and the in-tended application.
Nastase and Szpakowicz (2003)propose a two-level hierarchy for noun-modifier re-lations, which has been widely used (Nakov andHearst, 2008; Nastase et al, 2006; Turney andLittman, 2005; Turney, 2005).
Others have usedclassifications based on the requirements for a spe-cific task, such as Information Extraction (Panteland Pennacchiotti, 2006) or biomedical applications(Stephens et al, 2001).We adopt the relation classification scheme of Be-jar et al (1991), which includes ten high-level cat-egories (e.g., CAUSE-PURPOSE and SPACE-TIME).Each category has between five and ten more re-fined subcategories (e.g., CAUSE-PURPOSE includesCAUSE:EFFECT and ACTION:GOAL), for a total of79 distinct subcategories.
Although these cate-gories do not reflect all possible semantic rela-tions, they greatly expand the coverage of rela-tion types from those used in past relation-basedSemEval tasks (Girju et al, 2007; Hendrickx etal., 2010), which used only seven and nine re-lation types, respectively.
Furthermore, the clas-sification includes many of the fundamental rela-tions, e.g., TAXONOMIC and PART:WHOLE, whilealso including relations between a variety of partsof speech and less common relations, such as REF-ERENCE (e.g., SIGN:SIGNIFICANT) and NONAT-TRIBUTE (e.g., AGENT:ATYPICAL ACTION).
Usingsuch a large relation class inventory enables evalu-ating the generality of an approach, while still mea-suring performance on commonly used relations.3 Task DataWe constructed a new data set for the task, in whichword pairs are manually classified into relation cat-egories.
Word pairs within a category are manuallydistinguished according to how well they representthe category; that is, the degree to which they arerelationally similar to paradigmatic members of thegiven semantic relation class.
Paradigmatic mem-bers of a class were taken from examples providedby Bejar et al (1991).
Due to the large number ofQuestion 1: Consider the following word pairs: pil-grim:shrine, hunter:quarry, assassin:victim, climber:peak.What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?Question 2: Consider the following word pairs: pil-grim:shrine, hunter:quarry, assassin:victim, climber:peak.These X:Y pairs share a relation, ?X R Y ?.
Give four ad-ditional word pairs that illustrate the same relation, in thesame order (X on the left, Y on the right).
Please do notuse phrases composed of two or more words in your ex-amples (e.g., ?racing car?).
Please do not use names ofpeople, places, or things in your examples (e.g., ?Europe?,?Kleenex?).
(1) :(2) :(3) :(4) :Figure 1: An example of the two questions for Phase 1.annotations needed, we used Amazon MechanicalTurk (MTurk),2 which is a popular choice in com-putational linguistics for gathering large numbers ofhuman responses to linguistic questions (Snow et al,2008; Mohammad and Turney, 2010).
We refer tothe MTurk workers as Turkers.The data set was built in two phases.
In the firstphase, Turkers were given three paradigmatic exam-ples of a subcategory and asked to create new pairsthat instantiate the same relation as the paradigms.In the second phase, people were asked to distin-guish the new pairs from the first phase according tothe degree to which they are good representatives ofthe given subcategory.Phase 1 In the first phase, we built upon theparadigmatic examples of Bejar et al (1991), whoprovided one to ten examples for each subcategory.From these examples, we manually selected threeinstances to use as seeds for generating new exam-ples, adding examples when a subcategory had lessthan three.
The examples were selected to be bal-anced across topic domains so as not to bias theTurkers.
For each subcategory, we manually createda schematic representation of the relation for the ex-amples.
Table 1 gives three examples.2https://www.mturk.com/358To gather new examples of each subcategory,a two-part questionnaire was presented to Turk-ers (see Figure 1).
In the first part, Turkers wereshown the three paradigm word pairs for a sub-category along with a list of four relation descrip-tions (schematic representations of possible rela-tions).
One of the four schematic representationsaccurately described the three paradigm pairs andthe other three schematics were distractors (con-founding descriptions).
Turkers were asked to se-lect which of the four schematic representations bestmatched the paradigms.
The first part of the ques-tionnaire serves as quality control by ensuring thatthe Turker is capable of recognizing the relation.
Anincorrect answer to the question is used to recog-nize and eliminate confused or negligent responses,which were approximately 7% of the responses.In the second part of the Phase 1 questionnaire,Turkers were shown the three prototypes again andasked to generate four word pairs that expressed thesame relation.
Turkers were directed to be mindfulof the order of the words in each pair, as reversedorderings can have very different degrees of proto-typicality in the case of directional relations.The Turkers provided a total of 3160 additionalexamples for the 79 subcategories, 2905 of whichwere unique.
We applied minor manual correctionto remove spelling errors, which reduced the totalnumber of examples to 2823.
A median of 38 exam-ples were found per subcategory with a maximum of40 and minimum of 23.
We note that Phase 1 gathersboth high and low quality examples of the relation,which were all included to capture different degreesof prototypicality.We included an additional 395 pairs by randomlysampling five instances of each subcategory andcreating a new pair from the reversed arguments,i.e., adding pair Y :X to the subcategory contain-ing X:Y .
Adding reversals was inspired by an ob-servation during Phase 1 that reversed pairs wouldoccasionally be added by the Turkers themselves.We were curious to see what impact reversals wouldhave on Turker responses and on the output of au-tomatic systems.
Reversals should reveal order sen-sitivity with a strongly directional relation, such asPART:WHOLE, but also perhaps there is order sensi-tivity with more symmetric relations, such as SYN-ONYMY.
Phase 1 produced a total of 3218 pairs.Question 1: Consider the following word pairs: pil-grim:shrine, hunter:quarry, assassin:victim, climber:peak.What relation best describes these X:Y word pairs?
(1) ?X worships/reveres Y ?
(2) ?X seeks/desires/aims for Y ?
(3) ?X harms/destroys Y ?
(4) ?X uses/exploits/employs Y ?Question 2: Consider the following word pairs: pil-grim:shrine, hunter:quarry, assassin:victim, climber:peak.These X:Y pairs share a relation, ?X R Y ?.
Now considerthe following word pairs:(1) pig:mud(2) politician:votes(3) dog:bone(4) bird:wormWhich of the above numbered word pairs is the MOST illus-trative example of the same relation ?X R Y ?
?Which of the above numbered word pairs is the LEAST illus-trative example of the same relation ?X R Y ?
?Note: In some cases, a word pair might be in reverse order.For example, tree:forest is in reverse order for the relation?X is made from a collection of Y ?.
The correct order wouldbe forest:tree; a forest is made from a collection of trees.You should treat reversed pairs as BAD examples of the givenrelation.Figure 2: An example of the two questions for Phase 2.Phase 2 In the second phase, the response pairsfrom Phase 1 were ranked according to their pro-totypicality.
We opted to create a ranking usingMaxDiff questions (Louviere, 1991).
MaxDiff is achoice procedure consisting of a question about atarget concept and four or five alternatives.
A partic-ipant must choose both the best and worse answersfrom the given alternatives.MaxDiff is a strong alternative to creating a rank-ing from standard rating scales, such as the Likertscale, because it avoids scale biases.
FurthermoreMaxDiff is more efficient than other choice proce-dures such as pairwise comparison, because it doesnot require comparing all pairs.Like Phase 1, Phase 2 was performed using a two-part questionnaire.
The first question was identicalto that of Phase 1: four examples of the same re-lation subcategory generated in Phase 1 were pre-sented and the Turker was asked to select the cor-rect relation from a list of four options.
This firstquestion served as a quality control measure for en-suring the Turker could properly identify the rela-tion in question and it also served as a hint, guiding359the Turker toward the intended understanding of theshared relation underlying the three paradigms.
Inthe second part, the Turker selects the most and leastillustrative example of that relation from amongthe four examples of pairs generated by Turkers inPhase 1.We aimed for five Turker responses for eachMaxDiff question but averaged 4.73 responses foreach MaxDiff question in a subcategory, with aminimum of 3.45 responses per MaxDiff question.Turkers answered a total of 48,846 questions over aperiod of five months, of which 6,536 (13%) wererejected due to a missing answer or an incorrect re-sponse to the first question.3.1 Measuring PrototypicalityThe MaxDiff responses were converted into theprototypicality scores using a counting procedure(Orme, 2009).
For each word pair, the prototyp-icality is scored as the percentage of times it ischosen as most illustrative minus the percentage oftimes it is chosen as least illustrative (see Figure 2).While methods such as hierarchical Bayes modelscan be used to compute a numerical rank from theresponses, we found the counting method to producevery reasonable results.3.2 Data SetsThe 79 subcategories were divided into trainingand testing segments.
Ten subcategories were pro-vided as training with both the Turkers?
MaxDiffresponses and the computed prototypicality ratings.The ten training subcategories were randomly se-lected.
The remaining 69 subcategories were usedfor testing.
All data sets are now released on the taskwebsite under the Creative Commons 3.0 license.3Participants were given the list of all pairs gath-ered in Phase 1 and the Phase 2 responses for the 10training subcategories.
Phase 2 responses for the 69test categories were not made available.
Participantsalso had access to the set of questionnaire materialsprovided to the Turkers, the full list of paradigmaticexamples provided by Bejar et al (1991), and theconfounding schema relations from the initial ques-tions in Phase 1 and Phase 2, which might serve asnegative training examples.3http://creativecommons.org/licenses/by/3.0/4 EvaluationSystems are given examples of pairs from a singlecategory and asked to provide numeric ratings of thedegree of relational similarity for each pair relativeto the relation expressed in that category.4.1 ScoringSpearman?s rank correlation coefficient, ?, and aMaxDiff score were used to evaluate the systems.For Spearman?s ?, the prototypicality rating of eachpair is used to build a ranking of all pairs in a sub-category.
Spearman?s ?
is then computed betweenthe pair rankings of a system and the gold standardranking.
This evaluation abstracts away from com-paring the numeric values so that only their relativeordering in prototypicality is measured.In the second scoring procedure, we measure theaccuracy of a system at answering the same set ofMaxDiff questions as answered by the Turkers inPhase 2 (see Figure 2).
Given the four word pairs,the system selects the pair with the lowest numeri-cal rating as least illustrative and the pair with thehighest numerical rating as most illustrative.
Tiesin prototypicality are broken arbitrarily.
Accuracy ismeasured as the percentage of questions answeredcorrectly.
An answer is considered correct when itagrees with the majority of the Turkers.
In somecases, two answers may be considered correct.
Forexample, when five Turkers answer a given MaxD-iff question, two Turkers might choose one pair asthe most illustrative and two other Turkers mightchoose another pair as the most illustrative.
In thiscase, both pairs would count as correct choices forthe most illustrative pair.4.2 BaselinesWe consider two baselines for evaluation: Randomand PMI.
The Random baseline rates each pair in asubcategory randomly.
The expected Spearman cor-relation for Random ratings is zero.
The expectedMaxDiff score for Random ratings would be 25%(because there are four word pairs to choose fromin Phase 2) if there were always a unique majority,but it is actually about 31%, due to cases where twopairs both get two votes from the Turkers.Given a MaxDiff question, a Turker might selectthe pair whose words are most strongly associated360Team Members System DescriptionBeneme?ritaUniversidadAuto?noma dePuebla (Me?xico)(BUAP)Mireya T. Vidal,Darnes V. Ayala,Jose A.R.
Ortiz,Azucena M.Rendon,David Pinto, andSaul L. SilverioBUAP Each pair is represented as a vector over multiple features: lexical,intervening words, WordNet relations between the pair, and syntacticfeatures such as part of speech and morphology.
Prototypicality isbased on cosine similarity with the class?s pairs.University of Texas atDallas (UTD)Bryan Rink andSanda HarabagiuNB Unsupervised learning identifies intervening patterns between all wordpairs.
Each pattern is then ranked according to its subcategoryspecificity by learning a generative model from patterns to word pairs.Prototypicality ratings are based on confidence that the highest scoringpattern found for a pair belongs to the subcategory.SVM Intervening patterns are found using the same method as UTD-NB.Word pairs are then represented as feature vectors of matchingpatterns.
An SVM classifier is trained using a subcategory?s pairs aspositive training data and all other pairs as negative.
Prototypicalityratings are based on SVM confidence of class inclusion.University ofMinnesota, Duluth(Duluth)Ted Pedersen V0 WordNet is used to build the set of concepts connected by WordNetrelations to the pairs?
words.
Prototypicality is estimated using thevector similarity of the concatenated glosses.V1 Same procedure as V0, with one further expansion to related concepts.V2 Same procedure as V0, with two further expansions to related concepts.Table 2: Descriptions of the participating teams and systems.as the most illustrative and the least associated asthe least illustrative.
Therefore, we propose a sec-ond baseline where pairs are rated according to theirPointwise Mutual Information (PMI) (Church andHanks, 1990), which measures the statistical asso-ciation between two words.
For this baseline, theprototypicality rating given to a word pair is simplythe PMI score for the pair.
For two terms x and y,PMI(x, y) is defined as log2(p(x,y)p(x)p(y))where p(?
)denotes the probability of a term or pair of terms.The PMI score was calculated using the method ofTurney (2001) on a corpus of approximately 50 bil-lion tokens, indexed by the Wumpus search engine.4To calculate p(x, y), we recorded all co-occurrencesof both terms within a ten-word window.5 SystemsThree teams submitted six systems for evaluation.Table 2 summarizes the teams and systems.
Twoteams (BUAP and UTD) based their approaches ondiscovering relation-specific patterns for each cat-egory, while the third team (Duluth) used vectorspace comparisons of the glosses related to the pairs.4http://www.wumpus-search.org/No single system was able to achieve superior per-formance on all subcategories.
Table 3 reports theaverages across all subcategories for Spearman?s ?and MaxDiff accuracy.
Five systems were able toperform above the Random baseline, while only onesystem, UTD-NB, consistently performed above thePMI baseline.However, the average performance masks supe-rior performance on individual subcategories.
Ta-ble 3 also reports the number of subcategories inwhich a system obtained a statistically significantSpearman?s ?
with the gold standard ranking.
De-spite the low average performance, most modelswere able to obtain significant correlation in multi-ple subcategories.
Furthermore, the significant cor-relations for different systems were not always ob-tained in the same subcategories.
Across all subcat-egories, 43 had a significant correlation at p < 0.05and 27 at p < 0.01.
The broad coverage of signifi-cantly correlated subcategories spanned by the com-bination of all systems and the PMI baseline sug-gests that high performance on this task may be pos-sible, but that adapting to each of the specific rela-tion types may be very beneficial.361Team System Spearman?s ?
# of Subcategories MaxDiffp < 0.05 p < 0.01BUAP BUAP 0.014 2 0 31.7UTD NB 0.229 22 16 39.4SVM 0.116 11 5 34.7Duluth V0 0.050 9 3 32.4V1 0.039 10 4 31.5V2 0.038 7 3 31.1Baselines Random 0.018 4 0 31.2PMI 0.112 15 7 33.9Table 3: Average Spearman?s ?
and MaxDiff scores for all system across all 69 test subcategories.
Columns 4 and 5denote the number of subcategories with a Spearman?s ?
that is statistically significant at the noted level of confidence.Relation Class Random PMI BUAP UTD-NB UTD-SVM Duluth-V0 Duluth-V1 Duluth-V2Class-Inclusion 0.057 0.221 0.064 0.233 0.093 0.045 0.178 0.168Part-Whole 0.012 0.144 0.066 0.252 0.142 -0.061 -0.084 -0.054Similar 0.026 0.094 -0.036 0.214 0.131 0.183 0.208 0.198Contrast -0.049 0.032 0.000 0.206 0.162 0.142 0.120 0.051Attribute 0.037 -0.032 -0.095 0.158 0.052 0.044 -0.003 0.008Non-Attribute -0.070 0.191 0.009 0.098 0.094 0.079 0.066 0.074Case Relations 0.090 0.168 -0.037 0.241 0.187 -0.011 -0.068 -0.115Cause-Purpose -0.011 0.130 0.114 0.183 0.060 0.021 0.022 0.042Space-Time 0.013 0.084 0.035 0.375 0.139 0.055 -0.004 0.040Reference 0.142 0.125 -0.001 0.346 0.082 0.028 0.074 0.067Table 4: Average Spearman?s ?
correlation with the Turker rankings in each of the high-level relation categories, withthe highest average correlation for each subcategory shown in bold.6 DiscussionSensitivity to Pair Association The PMI base-line performed much better than anticipated, outper-forming all systems but UTD-NB on many of thesubcategories, despite treating all relations as direc-tionless.
Performance was highest in subcategorieswhere the X:Y pair might reasonably be expectedto occur together, e.g., FUNCTIONAL or CONTRA-DICTORY.
However, PMI benefits from the designof our task, which focuses on rating pairs within agiven subcategory.
In a different task that mixedpairs from a variety of subcategories, PMI wouldperform poorly, because it would assign high scoresto pairs of strongly associated words, regardless ofwhether they belong to a given subcategory.Difficulty of Specific Subcategories Performanceacross the high-level categories was highly variedbetween approaches.
The category-level summaryshown in Table 4 reveals high-level trends in diffi-culty across all submitted systems.
The submittedsystems performed best for subcategories under theSimilar category, while the systems performed worstfor Non-Attribute subcategories.As a further possibility of explaining performancedifferences between subcategories, we consideredthe hypothesis that the difficulty of a subcategory isinversely proportional to the range of prototypicalityscores, i.e., subcategories with restricted ranges aremore difficult.
However, we found that the difficultywas uncorrelated with both the size of the intervalspanned by prototypicality scores and the standarddeviation of the scores.Sensitivity to Argument Reversal The direction-ality of a relation can significantly impact the ratedprototypicality of a pair whose arguments have beenreversed.
As an approximate measure of the ef-fect on prototypicality when a pairs?
arguments arereversed, we calculated the expected drop in rank362Spearman?s ?Team System No Reversals With ReversalsBUAP BUAP -0.003 0.014UTD NB 0.190 0.229SVM 0.104 0.116Duluth V0 0.062 0.050V1 0.040 0.039V2 0.046 0.038Baselines Random 0.004 0.018PMI 0.143 0.112Table 5: Average pair ranking correlation for all subcate-gories when reversed pairs are included and excluded.between a pair and its reversed form.
Based onthe Turker rankings, the SEQUENCE (e.g., preg-nancy:birth) and FUNCTIONAL (e.g., weapon:knife)subcategories exhibited the strongest sensitivity toargument reversal, while ATTRIBUTE SIMILARITY(e.g., rake:fork) and CONTRARY (e.g., happy:sad)exhibited the least.The inclusion of reversed pairs potentially addsa small amount of noise to the relation identifica-tion process for subcategories with directional rela-tions.
Two teams, BUAP and UTD, accounted forrelation directionality, while Duluth did not, whichresulted in the Duluth systems ranking reversed pairsthe same.
Therefore, we conducted a post-hoc anal-ysis of the impact of reversals by removing the re-versed pairs from the computed prototypicality rank-ings.
Table 5 reports the resulting Spearman?s ?.With Spearman?s ?, we can easily evaluate the im-pact of the reversals, because we can delete a re-versed pair without affecting anything else.
For theMaxDiff questions, if there is one reversal in a groupof four choices, then we need to delete the wholeMaxDiff question.
Therefore we do not include theMaxDiff score in Table 5.Removing reversals decreased performance in thethree systems that were sensitive to pair order-ing (BUAP, UTD-NB, and UTD-SVM), while onlymarginally increasing performance in the three sys-tems that ignored the ordering.
The performance de-crease in systems that use ordering suggests that thereversed pairs are easily identified and ranked ap-propriately low.
As a further estimate of the models?ability to correctly order reversals, we compared thedifference in a reversal?s rank for both a system?sTeam System RMSEBUAP BUAP 256.07UT Dallas NB 257.15SVM 209.95Baseline Random 227.25Table 6: RMSE in estimating the difference in rank be-tween a pair and its reversal in the gold standard.ranking and the ranking computed from Turker Re-sponses.
Table 6 reports the Root Mean SquaredError (RMSE) in ranking difference for the threesystems that took argument order into account.
Al-though not the best performing system, Table 6 indi-cates that the UTD-SVM system was most able toappropriately weight reversals?
prototypicality.
Incontrast, the UTD-NB system often had many pairstied for the lowest rank, which either resulted in pairand its reversal being tied or having a much smallerrank difference, thereby increasing its RMSE.7 ConclusionsWe have introduced a new task focused on rating thedegrees of prototypicality for word pairs sharing thesame relation.
Participants first identify the relationshared between example pairs and then rate the de-gree to which each pair expresses that relation.
Asa part of the task, we constructed a dataset of proto-typicality ratings for 3218 word pairs in 79 differentrelation categories.Participating systems used combinations ofcorpus-based, syntactic, and WordNet features, withvarying degrees of success.
The task also included acompetitive baseline, PMI, which surpassed all butone system.
Several models obtained moderate per-formance in select relation subcategories, but no oneapproach succeeded in general, which introducesmuch opportunity for future improvement.
We alsohope that both the example pairs and their prototyp-icality ratings will be a valuable data set for futureresearch in Linguistics as well as Cognitive Psychol-ogy.
All data sets for this task have been made pub-licly available on the task website.AcknowledgementsThis research was supported by ONR grantN000140810186.363ReferencesIsaac I. Bejar, Roger Chaffin, and Susan E. Embretson.1991.
Cognitive and Psychometric Analysis of Ana-logical Problem Solving.
Springer-Verlag.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Dair-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2010.
Semeval-2010 task 9: The interpretation ofnoun compounds using paraphrasing verbs and prepo-sitions.
In Proceedings of the 5th International Work-shop on Semantic Evaluation (SemEval-2010), pages39?44.
Association for Computational Linguistics.Michael J. Cafarella, Michele Banko, and Oren Etzioni.2006.
Relational web search.
In WWW Conference.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational linguistics, 16(1):22?29.Dedre Gentner.
1983.
Structure-mapping: A theoreticalframework for analogy.
Cognitive science, 7(2):155?170.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 task 04: Classification of semantic re-lations between nominals.
In Proceedings of the4th International Workshop on Semantic Evaluation(SemEval-2007).Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid O?
Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2010.
SemEval-2010 task 8: Multi-way classi-fication of semantic relations between pairs of nom-inals.
In Proceedings of the 5th International Work-shop on Semantic Evaluation (SemEval-2010), pages33?38.
Association for Computational Linguistics.Keith J. Holyoak.
2012.
Analogy and relational reason-ing.
In Oxford handbook of thinking and reasoning,pages 234?259.
Oxford University Press.Jordan J. Louviere.
1991.
Best-worst scaling: A modelfor the largest difference judgments.
Working Paper.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: Usingmechanical turk to create an emotion lexicon.
In Pro-ceedings of the NAACL-HLT 2010 Workshop on Com-putational Approaches to Analysis and Generation ofEmotion in Text, pages 26?34.
Association for Com-putational Linguistics.Preslav Nakov and Marti A. Hearst.
2008.
Solving rela-tional similarity problems using the web as a corpus.In Proceedings of ACL, volume 8.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Fifth Interna-tional Workshop on Computational Semantics (IWCS-5), pages 285?301.
ACL Press Tilburg,, The Nether-lands.Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,and Stan Szpakowicz.
2006.
Learning noun-modifiersemantic relations with corpus-based and wordnet-based features.
In Proceedings of AAAI, volume 21,page 781.Bryan Orme.
2009.
Maxdiff analysis: Simple counting,individual-level logit, and hb.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 113?120.
Associa-tion for Computational Linguistics.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 254?263.
Association for Computational Lin-guistics.M.
Stephens, M. Palakal, S. Mukhopadhyay, R. Raje,J.
Mostafa, et al 2001.
Detecting gene relations frommedline abstracts.
In Pacific Symposium on Biocom-puting, volume 6, pages 483?495.
Citeseer.Peter D. Turney and Michael L Littman.
2005.
Corpus-based learning of analogies and semantic relations.Machine Learning, 60(1?3):251?278.Peter D. Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe Twelfth European Conference on Machine Learn-ing (ECML-2001), pages 491?502.Peter D. Turney.
2005.
Measuring semantic similarityby latent relational analysis.
In Proceedings of IJCAI,pages 1136?1141.Peter D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Peter D. Turney.
2008.
The latent relation mapping en-gine: Algorithm and experiments.
Journal of ArtificialIntelligence Research, 33(1):615?655.364
