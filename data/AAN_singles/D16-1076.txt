Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 795?804,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRationale-Augmented Convolutional Neural Networksfor Text ClassificationYe Zhang,1 Iain Marshall,2 Byron C. Wallace31Department of Computer Science, University of Texas at Austin2Department of Primary Care and Public Health Sciences, Kings College London3College of Computer and Information Science, Northeastern Universityyezhang@cs.utexas.edu, iain.marshall@kcl.ac.ukbyron@ccs.neu.eduAbstractWe present a new Convolutional Neural Net-work (CNN) model for text classification thatjointly exploits labels on documents and theirconstituent sentences.
Specifically, we con-sider scenarios in which annotators explic-itly mark sentences (or snippets) that sup-port their overall document categorization,i.e., they provide rationales.
Our model ex-ploits such supervision via a hierarchical ap-proach in which each document is representedby a linear combination of the vector repre-sentations of its component sentences.
Wepropose a sentence-level convolutional modelthat estimates the probability that a given sen-tence is a rationale, and we then scale thecontribution of each sentence to the aggre-gate document representation in proportion tothese estimates.
Experiments on five classifi-cation datasets that have document labels andassociated rationales demonstrate that our ap-proach consistently outperforms strong base-lines.
Moreover, our model naturally providesexplanations for its predictions.1 IntroductionNeural models that exploit word embeddings haverecently achieved impressive results on text classifi-cation tasks (Goldberg, 2015).
Feed-forward Con-volutional Neural Networks (CNNs), in particular,have emerged as a relatively simple yet powerfulclass of models for text classification (Kim, 2014).These neural text classification models havetended to assume a standard supervised learning set-ting in which instance labels are provided.
Here weconsider an alternative scenario in which we assumethat we are provided a set of rationales (Zaidan etal., 2007; Zaidan and Eisner, 2008; McDonnell etal., 2016) in addition to instance labels, i.e., sen-tences or snippets that support the correspondingdocument categorizations.
Providing such rationalesduring manual classification is a natural interactionfor annotators, and requires little additional effort(Settles, 2011; McDonnell et al, 2016).
Therefore,when training new classification systems, it is natu-ral to acquire supervision at both the document andsentence level, with the aim of inducing a better pre-dictive model, potentially with less effort.Learning algorithms must be designed to capital-ize on these two types of supervision.
Past work(Section 2) has introduced such methods, but thesehave relied on linear models such as Support VectorMachines (SVMs) (Joachims, 1998), operating oversparse representations of text.
We propose a novelCNN model for text classification that exploits bothdocument labels and associated rationales.Specific contributions of this work as follows.
(1)This is the first work to incorporate rationales intoneural models for text classification.
(2) Empiri-cally, we show that the proposed model uniformlyoutperforms relevant baseline approaches across fivedatasets, including previously proposed models thatcapitalize on rationales (Zaidan et al, 2007; Mar-shall et al, 2016) and multiple baseline CNN vari-ants, including a CNN equipped with an attentionmechanism.
We also report state-of-the-art resultson the important task of automatically assessing therisks of bias in the studies described in full-textbiomedical articles (Marshall et al, 2016).
(3) Ourmodel naturally provides explanations for its predic-795tions, providing interpretability.We have made available online both a Theano1and a Keras implementation2 of our model.2 Related Work2.1 Neural models for text classificationKim (2014) proposed the basic CNN model we de-scribe below and then build upon in this work.
Prop-erties of this model were explored empirically in(Zhang and Wallace, 2015).
We also note that Zhanget al (2016) extended this model to jointly accom-modate multiple sets of pre-trained word embed-dings.
Roughly concurrently to Kim, Johnson andZhang (2014) proposed a similar CNN architecture,although they swapped in one-hot vectors in placeof (pre-trained) word embeddings.
They later de-veloped a semi-supervised variant of this approach(Johnson and Zhang, 2015).In related recent work on Recurrent Neural Net-work (RNN) models for text, Tang et al (2015) pro-posed using a Long Short Term Memory (LSTM)layer to represent each sentence and then passinganother RNN variant over these.
And Yang et al(2016) proposed a hierarchical network with twolevels of attention mechanisms for document clas-sification.
We discuss this model specifically as wellas attention more generally and its relationship toour proposed approach in Section 4.3.2.2 Exploiting rationalesIn long documents the importance of sentencesvaries; some are more central than others.
Priorwork has investigated methods to measure the rel-ative importance sentences (Ko et al, 2002; Murataet al, 2000).
In this work we adopt a particular viewof sentence importance in the context of documentclassification.
In particular, we assume that docu-ments comprise sentences that directly support theircategorization.
We call such sentences rationales.The notion of rationales was first introduced byZaidan et al (2007).
To harness these for classifi-cation, they proposed modifying the Support Vec-tor Machine (SVM) objective function to encodea preference for parameter values that result in in-stances containing manually annotated rationales1https://github.com/yezhang-xiaofan/Rationale-CNN2https://github.com/bwallace/rationale-CNNbeing more confidently classified than ?pseudo?-instances from which these rationales had beenstripped.
This approach dramatically outperformedbaseline SVM variants that do not exploit such ra-tionales.
Yessenalina et al (2010) later developedan approach to generate rationales.Another line of related work concerns models thatcapitalize on dual supervision, i.e., labels on indi-vidual features.
This work has largely involved in-serting constraints into the learning process that fa-vor parameter values that align with a priori feature-label affinities or rankings (Druck et al, 2008; Mannand McCallum, 2010; Small et al, 2011; Settles,2011).
We do not discuss this line of work furtherhere, as our focus is on exploiting provided ratio-nales, rather than individual labeled features.3 Preliminaries: CNNs for textclassificationConvolutionlayerword embedding1 max poolingPatientswererandomizedtoplaceboorinterventionFour featuremapsSentence featurevector oSoftmax layerFigure 1: A toy example of a CNN for sentenceclassification.
Here there are four filters, two withheights 2 and two with heights 3, resulting in featuremaps with lengths 6 and 5 respectively.We first review the simple one-layer CNNfor sentence modeling proposed by Kim (2014).Given a sentence or document comprising n wordsw1, w2,...,wn, we replace each word with its d-dimensional pretrained embedding, and stack themrow-wise, generating an instance matrix A ?
Rn?d.796We then apply convolution operations on this ma-trix using multiple linear filters, these will have thesame width d but may vary in height.
Each filterthus effectively considers distinct n-gram features,where n corresponds to the filter height.
In practice,we introduce multiple, redundant features of eachheight; thus each filter height might have hundredsof corresponding instantiated filters.
Applying filteri parameterized by Wi ?
Rhi?d to the instance ma-trix induces a feature map fi ?
Rn?hi+1.
This pro-cess is performed by sliding the filter from the topof the matrix (the start of the document or sentence)to the bottom.
At each location, we apply element-wise multiplication between filter i and sub-matrixA[j : j + hi ?
1], and then sum up the resultantmatrix elements.
In this way, we induce a vector(feature map) for each filter.We next run the feature map through an element-wise non-linear transformation.
Specifically, we usethe Rectified Linear Unit, or ReLU (Krizhevsky etal., 2012).
We extract the maximum value oi fromeach feature map i (1-max pooling).Finally, we concatenate all of the features oi toform a vector representation o ?
R|F | for this in-stance, where |F | denotes the total number of filters.Classification is then performed on top of o, via asoftmax function.
Dropout (Srivastava et al, 2014)is often applied at this layer as a means of regular-ization.
We provide an illustrative schematic of thebasic CNN architecture just described in Figure 1.For more details, see (Zhang and Wallace, 2015).This model was originally proposed for sentenceclassification (Kim, 2014), but we can adapt it fordocument classification by simply treating the doc-ument as one long sentence.
We will refer to thisbasic CNN variant as CNN in the rest of the paper.Below we consider extensions that account for doc-ument structure.4 Rationale-Augmented CNN forDocument ClassificationWe now move to the main contribution of thiswork: a rationale-augmented CNN for text classi-fication.
We first introduce a simple variant of theabove CNN that models document structure (Section4.1) and then introduce a means of incorporatingrationale-level supervision into this model (Section4.2).
In Section 4.3 we discuss connections to atten-tion mechanisms and describe a baseline equippedwith one, inspired by Yang et al (2016).4.1 Modeling Document StructureRecall that rationales are snippets of text markedas having supported document-level categorizations.We aim to develop a model that can exploit these an-notations during training to improve classification.Here we achieve this by developing a hierarchicalmodel that estimates the probabilities of individualsentences being rationales and uses these estimatesto inform the document level classification.As a first step, we extend the CNN model aboveto explicitly account for document structure.
Specif-ically, we apply a CNN to each individual sentencein a document to obtain sentence vectors indepen-dently.
We then sum the respective sentence vectorsto create a document vector.3 As before, we add asoftmax layer on top of the document-level vectorto perform classification.
We perform regularizationby applying dropout both on the individual sentencevectors and the final document vector.
We will re-fer to this model as Doc-CNN.
Doc-CNN forms thebasis for our novel approach, described below.4.2 RA-CNNIn this section we present the Rationale-AugmentedCNN (RA-CNN).
Briefly, RA-CNN induces adocument-level vector representation by taking aweighted sum of its constituent sentence vectors.Each sentence weight is set to reflect the estimatedprobability that it is a rationale in support of the mostlikely class.
We provide a schematic of this modelin Figure 2.RA-CNN capitalizes on both sentence- anddocument-level supervision.
There are thus twosteps in the training phase: sentence level trainingand document level training.
For the former, we ap-ply a CNN to each sentence j in document i to obtainsentence vectors xijsen.
We then add a softmax layerparametrized by Wsen; this takes as input sentencevectors.
We fit this model to maximize the probabil-ities of the observed rationales:3We also experimented with taking the average of sentencevectors, but summing performed better in informal testing.797p(           )sentence vectorsFilms adapted fromcomic books......The film, however,is all good.Now onto from hell?sappearance: it?s............sentence model?......yidocyi0senp(           )Wsenxseni0xseni0exp(.
).xsenij= kkxsenilxseniyilsen= kNij=1Nip(           )yisen= kNixdociWdocxdociexp()kp(            )= kdocument iFilms adapted fromcomic books...Now onto from hell?sappearance: it?s.........The film, however, isall good.Nisentencespositive rationaleneutralneutral==document modelmax{                                            }yijsenpositiverationale=p()yijsennegativerationale=,p()document vectorFigure 2: A schematic of our proposed Rationale-Augmented Convolution Neural Network (RA-CNN).The sentences comprising a text are passed through a sentence model that outputs probabilities encoding thelikelihood that sentences are neutral or a (positive or negative) rationale.
Sentences likely to be rationalesare given higher weights in the global document vector, which is the input to the document model.p(yijsen = k;E,C,Wsen) = exp(W(k)Tsen xijsen)?Ksenk=1 exp(W(k)Tsen xijsen)(1)Where yijsen denotes the rationale label for sentence jin document i, Ksen denotes the number of possibleclasses for sentences, E denotes the word embed-ding matrix, C denotes the convolution layer param-eters, and Wsen is a matrix of weights (comprisingone weight vector per sentence class).In our setting, each sentence has three possiblelabels (Ksen = 3).
When a rationale sentence ap-pears in a positive document,4 it is a positive ratio-nale; when a rationale sentence appears in a negativedocument, it is a negative rationale.
All other sen-4All of the document classification tasks we consider hereare binary, although extension of our model to multi-class sce-narios is straight-forward.tences belong to a third, neutral class: these are non-rationales.
We also experimented with having onlytwo sentence classes: rationales and non-rationales,but this did not perform as well as explicitly main-taining separate classes for rationales of differentpolarities.We train an estimator using the provided ratio-nale annotations, optimizing over {E,C,Wsen} tominimize the categorical cross-entropy of sentencelabels.
Once trained, this sub-model can provideconditional probability estimates regarding whethera given sentence is a positive or a negative rationale,which we will denote by ppos and pneg, respectively.We next train the document-level classificationmodel.
The inputs to this are vector representationsof documents, induced by summing over constituentsentence vectors, as in Doc-CNN.
However, in theRA-CNN model this is a weighted sum.
Specifi-cally, weights are set to the estimated probabilities798that corresponding sentences are rationales in themost likely direction.
More precisely:xidoc =Ni?j=1xijsen ?max{pijpos, pijneg} (2)Where Ni is the number of sentences in the ith doc-ument.
The intuition is that sentences likely to be ra-tionales will have greater influence on the resultantdocument vector representation, while the contribu-tion of neutral sentences (which are less relevant tothe classification task) will be minimized.The final classification is performed by a softmaxlayer parameterized by Wdoc; the inputs to this layerare the document vectors.
The Wdoc parameters aretrained using the document-level labels, yidoc:p(yidoc = k;E,C,Wdoc) =exp(W(k)Tdoc xidoc)?Kdock=1 exp(W(k)Tdoc xidoc)(3)where Kdoc is the cardinality of the document labelset.
We optimize over parameters to minimize cross-entropy loss (w.r.t.
the document labels).We note that the sentence- and document-levelmodels share word embeddings E and convolutionlayer parameters C, but the document-level modelhas its own softmax parameters Wdoc.
When train-ing the document-level model, E, C and Wdoc arefit, but we hold Wsen fixed.The above two-step strategy can be equivalentlydescribed as follows.
We first estimate E, C andWsen, which parameterize our model for identifyingrationales in documents.
We then move to fitting ourdocument classification model.
For this we initializethe word embedding and convolution parameters tothe E and C estimates from the preceding step.
Wethen directly minimize the document level classifica-tion objective, tuning E and C and simultaneouslyfitting Wdoc.Note that this sequential training strategy differsfrom the alternating training approach commonlyused in multi-task learning (Collobert and Weston,2008).
We found that the latter approach does notwork well here, leading us to instead adopt thecascade-like feature learning approach (Collobertand Weston, 2008) just described.One nice property of our model is that it naturallyprovides explanations for its predictions: the modelidentifies rationales and then categorizes documentsinformed by these.
Thus if the model classifies a testinstance as positive, then by construction the sen-tences associated with the highest pijpos estimates arethose that the model relied on most in coming to thisdisposition.
These sentences can of course be out-put in conjunction with the prediction.
We provideconcrete examples of this in Section 7.2.4.3 Rationales as ?Supervised Attention?One may view RA-CNN as a supervised variant of amodel equipped with an attention mechanism (Bah-danau et al, 2014).
On this view, it is apparent thatrather than capitalizing on rationales directly, wecould attempt to let the model learn which sentencesare important, using only the document labels.
Wetherefore construct an additional baseline that doesjust this, thereby allowing us to assess the impact oflearning directly from rationale-level supervision.Following the recent work of Yang et al (2016),we first posit for each sentence vector a hidden rep-resentation uijsen.
We then define a sentence-levelcontext vector us, which we multiply with each uijsento induce a weight ?ij .
Finally, the document vec-tor is taken as a weighted sum over sentence vectors,where weights reflect ??s.
We have:uijsen = tanh(Wsxijsen + bs) (4)?ij =exp(uTs uijsen)?Nij exp(uTs uijsen)(5)xidoc =Ni?j?ijxijsen (6)where xidoc again denotes the document vector fedinto a softmax layer, and Ws, us and bs are learnedduring training.
We will refer to this attention-basedmethod as AT-CNN.5 DatasetsWe used five text classification datasets to evaluateour approach in total.
Four of these are biomedicaltext classification datasets (5.1) and the last is a col-lection of movie reviews (5.2).
These datasets sharethe property of having recorded rationales associated799with each document categorization.
We summarizeattributes of all datasets used in this work in Table 1.5.1 Risk of Bias (RoB) DatasetsWe used a collection Risk of Bias (RoB) text classifi-cation datasets, described at length elsewhere (Mar-shall et al, 2016).
Briefly, the task concerns as-sessing the reliability of the evidence presented infull-text biomedical journal articles that describe theconduct and results of randomized controlled trials(RCTs).
This involves, e.g., assessing whether ornot patients were properly blinded as to whether theywere receiving an active treatment or a comparator(such as a placebo).
If such blinding is not donecorrectly, it compromises the study by introducingstatistical bias into the treatment efficacy estimate(s)derived from the trial.A formal system for making bias assessments iscodified by the Cochrane Risk of Bias Tool (Hig-gins et al, 2011).
This tool defines multiple do-mains; the risk of bias may be assessed in each ofthese.
We consider four domains here.
(1) Randomsequence generation (RSG): were patients were as-signed to treatments in a truly random fashion?
(2)Allocation concealment (AC): were group assign-ments revealed to the person assigning patients togroups (so that she may have knowingly or unknow-ingly) influenced these assignments?
(3) Blindingof Participants and Personnel (BPP): were all trialparticipants and individuals involved in running thetrial blinded as to who was receiving which treat-ment?
(4) Blinding of outcome assessment (BOA):were the parties who measured the outcome(s) of in-terest blinded to the intervention group assignments?These assessments are somewhat subjective.
To in-crease transparency, researchers performing RoB as-sessment therefore record rationales (sentences fromarticles) supporting their assessments.5.2 Movie Review DatasetWe also ran experiments on a movie review (MR)dataset with accompanying rationales.
Pang and Lee(2004) developed and published the original ver-sion of this dataset, which comprises 1000 positiveand 1000 negative movie reviews from the InternetMovie Database (IMDB).5 Zaidan et al (2007) then5http://www.imdb.com/N #sen #token #ratRSG 8399 300 9.92 0.31AC 11512 297 9.87 0.15BPP 7997 296 9.95 0.21BOA 2706 309 9.92 0.2MR 1800 32.6 21.2 8.0Table 1: Dataset characteristics.
N is the number ofinstances, #sen is the average sentence count, #tokenis the average token per-sentence count and #rat isthe average number of rationales per document.augmented this dataset by adding rationales corre-sponding to the binary classifications for 1800 doc-uments, leaving the remaining 200 for testing.
Be-cause 200 documents is a modest test sample size,we ran 9-fold cross validation on the 1800 annotateddocuments (each fold comprising 200 documents).The rationales, as originally marked in this dataset,were sub-sentential snippets; for the purposes of ourmodel, we considered the entire sentences contain-ing the marked snippets as rationales.6 Experimental Setup6.1 BaselinesWe compare against several baselines to assess theadvantages of directly incorporating rationale-levelsupervision into the proposed CNN architecture.
Wedescribe these below.SVMs.
We evaluated a few variants of linear Sup-port Vector Machines (SVMs).
These rely on sparserepresentations of text.
We consider variants that ex-ploit uni- and bi-grams; we refer to these as uni-SVMand bi-SVM, respectively.
We also re-implementedthe rationale augmented SVM (RA-SVM) proposedby Zaidan et al (2007), described in Section 2.For the RoB dataset, we also compare to a re-cently proposed multi-task SVM (MT-SVM) modeldeveloped specifically for these RoB datasets (Mar-shall et al, 2015; Marshall et al, 2016).
This modelexploits the intuition that the risks of bias across thedomains codified in the aforementioned CochraneRoB tool will likely be correlated.
That is, if weknow that a study exhibits a high risk of bias for onedomain, then it seems reasonable to assume it is atan elevated risk for the remaining domains.
Further-more, Marshall et al (2016) include rationale-levelsupervision by first training a (multi-task) sentence-level model to identify sentences likely to support800RoB assessments in the respective domains.
Specialfeatures extracted from these predicted rationales arethen activated in the document-level model, inform-ing the final classification.
This model is the state-of-the-art on this task.CNNs.
We compare against several baseline CNNvariants to demonstrate the advantages of our ap-proach.
We emphasize that our focus in this workis not to explore how to induce generally ?better?document vector representations ?
this question hasbeen addressed at length elsewhere, e.g., (Le andMikolov, 2014; Jozefowicz et al, 2015; Tang et al,2015; Yang et al, 2016).Rather, the main contribution here is an augmen-tation of CNNs for text classification to capitalize onrationale-level supervision, thus improving perfor-mance and enhancing interpretability.
This informedour choice of baseline CNN variants: standard CNN(Kim, 2014), Doc-CNN (described above) and AT-CNN (also described above) that capitalizes on an(unsupervised) attention mechanism at the sentencelevel, described in Section 4.3.66.2 Implementation/Hyper-Parameter DetailsSentence splitting.
To split the documents from alldatasets into sentences for consumption by our Doc-CNN and RA-CNN models, we used the NaturalLanguage Toolkit (NLTK)7 sentence splitter.SVM-based models.
We kept the 50,000 mostfrequently occurring features in each dataset.
Forestimation we used SGD.
We tuned the C hyper-parameter using nested development sets.
For theRA-SVM, we additionally tuned the ?
and Ccontrastparameters, as per Zaidan et al (2007).CNN-based models.
For all models and datasetswe initialized word embeddings to pre-trained vec-tors fit via Word2Vec.
For the movie reviewsdataset these were 300-dimensional and trained onGoogle News.8 For the RoB datasets, these were200-dimensional and trained on biomedical texts inPubMed/PubMed Central (Pyysalo et al, 2013).96We also experimented briefly with LSTM and GRU (GatedRecurrent Unit) models, but found that simple CNN performedbetter than these.
Moreover, CNNs are relatively robust and lesssensitive to hyper-parameter selection.7http://www.nltk.org/api/nltk.tokenize.html8https://code.google.com/archive/p/word2vec/9http://bio.nlplab.org/Training proceeded as follows.
We first extractedall sentences from all documents in the trainingdata.
The distribution of sentence types is highlyimbalanced (nearly all are neutral).
Therefore, wedownsampled sentences before each epoch, so thatsentence classes were equally represented.
Aftertraining on sentence-level supervision, we moved todocument-level model fitting.
For this we initializedembedding and convolution layer parameters to theestimates from the preceding sentence-level trainingstep (though these were further tuned to optimize thedocument-level objective).For RA-CNN, we tuned the dropout rate (range:0-.9) applied at the sentence vector level on eachtraining fold (using a subset of the training data asa validation set) during the document level trainingphase.
Anecdotally, we found this has a greater ef-fect than the other model hyperparameters, whichwe thus set after a small informal process of exper-imentation on a subset of the data.
Specifically, wefixed the dropout rate at the document level to 0.5,and we used 3 different filter heights: 3, 4 and 5,following (Zhang and Wallace, 2015).
For each fil-ter height, we used 100 feature maps for the baselineCNN, and 20 for all the other CNN variants.For parameter estimation we used ADADELTA(Zeiler, 2012), mini-batches of size 50, and an earlystopping strategy (using a validation set).7 Results and Discussion7.1 Quantitative ResultsFor all CNN models, we replicated experiments 5times, where each replication constituted 5-fold and9-fold CV respectively the RoB and the moviesdatasets, respectively.
We report the mean and ob-served ranges in accuracy across these 5 replicationsfor these models, because attributes of the model(notably, dropout) and the estimation procedure ren-der model fitting stochastic (Zhang and Wallace,2015).
We do not report ranges for SVM-basedmodels because the variance inherent in the estima-tion procedure is much lower for these simpler, lin-ear models.Results on the RoB datasets and the moviesdataset are shown in Tables 2 and Table 3, respec-tively.
RA-CNN consistently outperforms all of thebaseline models, across all five datasets.
We also801Method RSG AC BPP BOAUni-SVM 72.16 72.81 72.80 65.85Bi-SVM 74.82 73.62 75.13 67.29RA-SVM 72.54 74.11 75.15 66.29MT-SVM 76.15 74.03 76.33 67.50CNN 72.50 (72.22, 72.65) 72.16 (71.49, 72.93) 75.03 (74.16, 75.44) 63.76 (63.12, 64.15)Doc-CNN 72.60 (72.43, 72.90) 72.92 (72.19, 73.48) 74.24 (74.03, 74.38) 63.64 (63.23, 64.37)AT-CNN 74.14 (73.40, 74.58) 73.66 (73.12, 73.92) 74.29 (74.09, 74.74) 63.34 (63.21, 63.49)RA-CNN 77.42 (77.33, 77.59) 76.14 (75.89, 76.29) 76.47 (76.15, 76.75) 69.67 (69.33, 69.93)Human 85.00 80.00 78.10 83.20Table 2: Accuracies on the four RoB datasets.
Uni-SVM: unigram SVM, Bi-SVM: Bigram SVM, RA-SVM:Rationale-augmented SVM (Zaidan et al, 2007), MT-SVM: a multi-task SVM model specifically designedfor the RoB task, which also exploits the available sentence supervision (Marshall et al, 2016).
We alsoreport an estimate of human-level performance, as calculated using subsets of the data for each domain thatwere assessed by two experts (one was arbitrarily assumed to be correct).
We report these numbers forreference; they are not directly comparable to the cross-fold estimates reported for the models.observe that CNN/Doc-CNN do not necessarily im-prove over the results achieved by SVM-based mod-els, which prove to be strong baselines for longerdocument classification.
This differs from previ-ous comparisons in the context of classifying shortertexts.
In particular, in previous work (Zhang andWallace, 2015) we observed that CNN outperformsSVM uniformly on sentence classification tasks (theaverage sentence-length in these datasets was about10).
In contrast, in the datasets we consider in thepresent paper, documents often comprise hundredsof sentences, each in turn containing multiple words.We believe that it is in these cases that explicitlymodeling which sentences are most important willresult in the greatest performance gains, and thisaligns with our empirical results.Another observation is that AT-CNN does of-ten improve performance over vanilla variants ofCNN (i.e., without attention), especially on the RoBdatasets, probably because these comprise longerdocuments.
However, as one might expect, RA-CNN clearly outperforms AT-CNN by exploitingrationale-level supervision directly.
And by exploit-ing rationale information directly, RA-CNN is ableto consistently perform better than baseline CNNand SVM model variants.
Indeed, we find that RA-CNN outperformed MT-SVM on all of the RoBdatasets, and this was accomplished without exploit-ing cross-domain correlations (i.e., without multi-task learning).Method AccuracyUni-SVM 86.44Bi-SVM 86.94RA-SVM 88.89CNN 85.59 (85.27, 86.17)Doc-CNN 87.14 (86.70, 87.60)AT-CNN 86.69 (86.28, 87.17)RA-CNN 90.43 (90.11, 91.00)Table 3: Accuracies on the movie review dataset.7.2 Qualitative Results: Illustrative RationalesIn addition to realizing superior classification perfor-mance, RA-CNN also provides explainable catego-rizations.
The model can provide the highest scoringrationales (ranked by max{ppos, pneg}) for any giventarget instance, which in turn ?
by construction ?
arethose that most influenced the final document classi-fication.For example, a sample positive rationale support-ing a correct designation of a study as being at lowrisk of bias with respect to blinding of outcomesassessment reads simply The study was performeddouble blind.
An example rationale extracted for astudy (correctly) deemed at high risk of bias, mean-while, reads as the present study is retrospective,there is a risk that the woman did not properly re-call how and what they experienced ....Turning to the movie reviews dataset, an exam-ple rationale extracted from a glowing review of?Goodfellas?
(correctly classified as positive) readsthis cinematic gem deserves its rightful place amongthe best films of 1990s.
While a rationale extracted802from an unfavorable review of ?The English Patient?asserts that the only redeeming qualities about thisfilm are the fine acting of Fiennes and Dafoe and thebeautiful desert cinematography.In each of these cases, the extracted rationalesdirectly support the respective classifications.
Thisprovides direct, meaningful insight into the auto-mated classifications, an important benefit for neuralmodels, which are often seen as opaque.8 ConclusionsWe developed a new model (RA-CNN) for text clas-sification that extends the CNN architecture to di-rectly exploit rationales when available.
We showedthat this model outperforms several strong, rele-vant baselines across five datasets, including vanillaand hierarchical CNN variants, and a CNN modelequipped with an attention mechanism.
Moreover,RA-CNN automatically provides explanations forclassifications made at test time, thus providing in-terpretability.Moving forward, we plan to explore additionalmechanisms for exploiting supervision at lower lev-els in neural architectures.
Furthermore, we believean alternative approach may be a hybrid of the AT-CNN and RA-CNN models, wherein an auxiliaryloss might be incurred when the attention mecha-nism output disagrees with the available direct su-pervision on sentences.AcknowledgmentsResearch reported in this article was supported bythe National Library of Medicine (NLM) of the Na-tional Institutes of Health (NIH) under award num-ber R01LM012086.
The content is solely the re-sponsibility of the authors and does not necessarilyrepresent the official views of the National Institutesof Health.
This work was also made possible bythe support of the Texas Advanced Computer Center(TACC) at UT Austin.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th international conference on Machine learn-ing, pages 160?167.
ACM.Gregory Druck, Gideon Mann, and Andrew McCallum.2008.
Learning from labeled features using gener-alized expectation criteria.
In Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 595?602.
ACM.Yoav Goldberg.
2015.
A primer on neural network mod-els for natural language processing.
arXiv preprintarXiv:1510.00726.Julian PT Higgins, Douglas G Altman, Peter C G?tzsche,Peter Ju?ni, David Moher, Andrew D Oxman, Je-lena Savovic?, Kenneth F Schulz, Laura Weeks, andJonathan AC Sterne.
2011.
The cochrane collabo-rations tool for assessing risk of bias in randomisedtrials.
Bmj, 343:d5928.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
Springer.Rie Johnson and Tong Zhang.
2014.
Effective use ofword order for text categorization with convolutionalneural networks.
arXiv preprint arXiv:1412.1058.Rie Johnson and Tong Zhang.
2015.
Semi-supervisedconvolutional neural networks for text categorizationvia region embedding.
In Advances in Neural Infor-mation Processing Systems (NIPs), pages 919?927.Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.2015.
An empirical exploration of recurrent networkarchitectures.
In Proceedings of the 32nd Interna-tional Conference on Machine Learning (ICML-15),pages 2342?2350.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
arXiv preprint arXiv:1408.5882.Youngjoong Ko, Jinwoo Park, and Jungyun Seo.
2002.Automatic text categorization using the importance ofsentences.
In Proceedings of the 19th internationalconference on Computational linguistics-Volume 1,pages 1?7.
Association for Computational Linguistics.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.2012.
Imagenet classification with deep convolutionalneural networks.
In Advances in neural informationprocessing systems, pages 1097?1105.Quoc V Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
arXiv preprintarXiv:1405.4053.Gideon S Mann and Andrew McCallum.
2010.
Gener-alized expectation criteria for semi-supervised learn-ing with weakly labeled data.
The Journal of MachineLearning Research, 11:955?984.803Iain J Marshall, Joe?l Kuiper, and Byron C Wallace.
2015.Automating risk of bias assessment for clinical trials.Biomedical and Health Informatics, IEEE Journal of,19(4):1406?1412.Iain J Marshall, Joe?l Kuiper, and Byron C Wallace.2016.
Robotreviewer: evaluation of a system forautomatically assessing bias in clinical trials.
Jour-nal of the American Medical Informatics Association,23(1):193?201.Tyler McDonnell, Matthew Lease, Tamer Elsayad, andMucahid Kutlu.
2016.
Why Is That Relevant?Collecting Annotator Rationales for Relevance Judg-ments.
In Proceedings of the 4th AAAI Conference onHuman Computation and Crowdsourcing (HCOMP).10 pages.Masaki Murata, Qing Ma, Kiyotaka Uchimoto, HiromiOzaku, Masao Utiyama, and Hitoshi Isahara.
2000.Japanese probabilistic information retrieval using lo-cation and category information.
In Proceedings ofthe fifth international workshop on on Information re-trieval with Asian languages, pages 81?88.
ACM.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndannual meeting on Association for Computational Lin-guistics, page 271.
Association for Computational Lin-guistics.Sampo Pyysalo, Filip Ginter, Hans Moen, TapioSalakoski, and Sophia Ananiadou.
2013.
Distribu-tional semantics resources for biomedical text pro-cessing.
Proceedings of Languages in Biology andMedicine.Burr Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 1467?1478.
Association for Computational Lin-guistics.Kevin Small, Byron Wallace, Thomas Trikalinos, andCarla E Brodley.
2011.
The constrained weight spacesvm: learning with ranked features.
In Proceedings ofthe 28th International Conference on Machine Learn-ing (ICML-11), pages 865?872.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Duyu Tang, Bing Qin, and Ting Liu.
2015.
Documentmodeling with gated recurrent neural network for sen-timent classification.
In Proceedings of the 2015 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1422?1432.Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, AlexSmola, and Eduard Hovy.
2016.
Hierarchical atten-tion networks for document classification.
In Proceed-ings of the 2016 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies.Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010.Automatically generating annotator rationales to im-prove sentiment classification.
In Proceedings of theACL 2010 Conference Short Papers, pages 336?341.Association for Computational Linguistics.Omar F Zaidan and Jason Eisner.
2008.
Modeling anno-tators: A generative approach to learning from anno-tator rationales.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 31?40.
Association for Computational Linguis-tics.Omar Zaidan, Jason Eisner, and Christine D Piatko.2007.
Using?
annotator rationales?
to improve ma-chine learning for text categorization.
In HLT-NAACL,pages 260?267.
Citeseer.Matthew D Zeiler.
2012.
Adadelta: an adaptive learningrate method.
arXiv preprint arXiv:1212.5701.Ye Zhang and Byron C. Wallace.
2015.
A sensitivityanalysis of (and practitioners?
guide to) convolutionalneural networks for sentence classification.
arXivpreprint arXiv:1510.03820.Ye Zhang, Stephen Roller, and Byron C. Wallace.
2016.Mgnc-cnn: A simple approach to exploiting multipleword embeddings for sentence classification.
In Pro-ceedings of the 2016 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 1522?1527, San Diego, California, June.
Association forComputational Linguistics.804
