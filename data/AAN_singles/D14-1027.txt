Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 214?220,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning to Differentiate Better from Worse TranslationsFrancisco Guzm?an Shafiq Joty Llu?
?s M`arquezAlessandro Moschitti Preslav Nakov Massimo NicosiaALT Research GroupQatar Computing Research Institute ?
Qatar Foundation{fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qaAbstractWe present a pairwise learning-to-rankapproach to machine translation evalua-tion that learns to differentiate better fromworse translations in the context of a givenreference.
We integrate several layersof linguistic information encapsulated intree-based structures, making use of boththe reference and the system output simul-taneously, thus bringing our ranking closerto how humans evaluate translations.
Mostimportantly, instead of deciding upfrontwhich types of features are important, weuse the learning framework of preferencere-ranking kernels to learn the features au-tomatically.
The evaluation results showthat learning in the proposed frameworkyields better correlation with humans thancomputing the direct similarity over thesame type of structures.
Also, we showour structural kernel learning (SKL) canbe a general framework for MT evaluation,in which syntactic and semantic informa-tion can be naturally incorporated.1 IntroductionWe have seen in recent years fast improvementin the overall quality of machine translation (MT)systems.
This was only possible because of theuse of automatic metrics for MT evaluation, suchas BLEU (Papineni et al., 2002), which is the de-facto standard; and more recently: TER (Snover etal., 2006) and METEOR (Lavie and Denkowski,2009), among other emerging MT evaluation met-rics.
These automatic metrics provide fast and in-expensive means to compare the output of differ-ent MT systems, without the need to ask for hu-man judgments each time the MT system has beenchanged.As a result, this has enabled rapid develop-ment in the field of statistical machine translation(SMT), by allowing to train and tune systems aswell as to track progress in a way that highly cor-relates with human judgments.Today, MT evaluation is an active field of re-search, and modern metrics perform analysis atvarious levels, e.g., lexical (Papineni et al., 2002;Snover et al., 2006), including synonymy andparaphrasing (Lavie and Denkowski, 2009); syn-tactic (Gim?enez and M`arquez, 2007; Popovi?cand Ney, 2007; Liu and Gildea, 2005); semantic(Gim?enez and M`arquez, 2007; Lo et al., 2012);and discourse (Comelles et al., 2010; Wong andKit, 2012; Guzm?an et al., 2014; Joty et al., 2014).Automatic MT evaluation metrics compare theoutput of a system to one or more human ref-erences in order to produce a similarity score.The quality of such a metric is typically judgedin terms of correlation of the scores it produceswith scores given by human judges.
As a result,some evaluation metrics have been trained to re-produce the scores assigned by humans as closelyas possible (Albrecht and Hwa, 2008).
Unfortu-nately, humans have a hard time assigning an ab-solute score to a translation.
Hence, direct hu-man evaluation scores such as adequacy and flu-ency, which were widely used in the past, arenow discontinued in favor of ranking-based eval-uations, where judges are asked to rank the out-put of 2 to 5 systems instead.
It has been shownthat using such ranking-based assessments yieldsmuch higher inter-annotator agreement (Callison-Burch et al., 2007).While evaluation metrics still produce numeri-cal scores, in part because MT evaluation sharedtasks at NIST and WMT ask for it, there has alsobeen work on a ranking formulation of the MTevaluation task for a given set of outputs.
Thiswas shown to yield higher correlation with humanjudgments (Duh, 2008; Song and Cohn, 2011).214Learning automatic metrics in a pairwise set-ting, i.e., learning to distinguish between two al-ternative translations and to decide which of thetwo is better (which is arguably one of the easiestways to produce a ranking), emulates closely howhuman judges perform evaluation assessments inreality.
Instead of learning a similarity functionbetween a translation and the reference, they learnhow to differentiate a better from a worse trans-lation given a corresponding reference.
While thepairwise setting does not provide an absolute qual-ity scoring metric, it is useful for most evaluationand MT development scenarios.In this paper, we propose a pairwise learningsetting similar to that of Duh (2008), but we extendit to a new level, both in terms of feature represen-tation and learning framework.
First, we integrateseveral layers of linguistic information encapsu-lated in tree-based structures; Duh (2008) onlyused lexical and POS matches as features.
Second,we use information about both the reference andtwo alternative translations simultaneously, thusbringing our ranking closer to how humans ranktranslations.
Finally, instead of deciding upfrontwhich types of features between hypotheses andreferences are important, we use a our structuralkernel learning (SKL) framework to generate andselect them automatically.The structural kernel learning (SKL) frameworkwe propose consists in: (i) designing a struc-tural representation, e.g., using syntactic and dis-course trees of translation hypotheses and a refer-ences; and (ii) applying structural kernels (Mos-chitti, 2006; Moschitti, 2008), to such representa-tions in order to automatically inject structural fea-tures in the preference re-ranking algorithm.
Weuse this method with translation-reference pairsto directly learn the features themselves, insteadof learning the importance of a predetermined setof features.
A similar learning framework hasbeen proven to be effective for question answer-ing (Moschitti et al., 2007), and textual entailmentrecognition (Zanzotto and Moschitti, 2006).Our goals are twofold: (i) in the short term, todemonstrate that structural kernel learning is suit-able for this task, and can effectively learn to rankhypotheses at the segment-level; and (ii) in thelong term, to show that this approach provides aunified framework that allows to integrate severallayers of linguistic analysis and information and toimprove over the state-of-the-art.Below we report the results of some initial ex-periments using syntactic and discourse structures.We show that learning in the proposed frameworkyields better correlation with humans than apply-ing the traditional translation?reference similaritymetrics using the same type of structures.
Wealso show that the contributions of syntax and dis-course information are cumulative.
Finally, de-spite the limited information we use, we achievecorrelation at the segment level that outperformsBLEU and other metrics at WMT12, e.g., our met-ric would have been ranked higher in terms of cor-relation with human judgments compared to TER,NIST, and BLEU in the WMT12 Metrics sharedtask (Callison-Burch et al., 2012).2 Kernel-based Learning from LinguisticStructuresIn our pairwise setting, each sentence s inthe source language is represented by a tuple?t1, t2, r?, where t1and t2are two alternativetranslations and r is a reference translation.
Ourgoal is to develop a classifier of such tuples thatdecides whether t1is a better translation than t2given the reference r.Engineering features for deciding whether t1isa better translation than t2is a difficult task.
Thus,we rely on the automatic feature extraction en-abled by the SKL framework, and our task is re-duced to choosing: (i) a meaningful structural rep-resentation for ?t1, t2, r?, and (ii) a feature func-tion ?mtthat maps such structures to substruc-tures, i.e., our feature space.
Since the designof ?mtis complex, we use tree kernels appliedto two simpler structural mappings ?M(t1, r) and?M(t2, r).
The latter generate the tree representa-tions for the translation-reference pairs (t1, r) and(t2, r).
The next section shows such mappings.2.1 RepresentationsTo represent a translation-reference pair (t, r), weadopt shallow syntactic trees combined with RST-style discourse trees.
Shallow trees have beensuccessfully used for question answering (Severynand Moschitti, 2012) and semantic textual sim-ilarity (Severyn et al., 2013b); while discourseinformation has proved useful in MT evaluation(Guzm?an et al., 2014; Joty et al., 2014).
Com-bined shallow syntax and discourse trees workedwell for concept segmentation and labeling (Salehet al., 2014a).215DIS:ELABORATIONEDU:NUCLEUS EDU:SATELLITE-RELVP NP-REL NP VP-REL o-REL o-RELRB TO-REL VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-RELnot to give them the time to think .
"VP NP-REL NP VP-REL o-REL o-RELTO-REL `` VB-REL PRP-REL DT NN-REL TO-REL VB-REL .-REL ''-RELto " give them no time to think .
"a) Hypothesisb) Reference DIS:ELABORATIONEDU:NUCLEUS EDU:SATELLITE-RELBag-of-words relationsrelationpropagation directionFigure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS.Figure 1 shows two example trees combiningdiscourse, shallow syntax and POS: one for atranslation hypothesis (top) and the other one forthe reference (bottom).
To build such structures,we used the Stanford POS tagger (Toutanova etal., 2003), the Illinois chunker (Punyakanok andRoth, 2001), and the discourse parser1of (Joty etal., 2012; Joty et al., 2013).The lexical items constitute the leaves of thetree.
The words are connected to their respec-tive POS tags, which are in turn grouped intochunks.
Then, the chunks are grouped into el-ementary discourse units (EDU), to which thenuclearity status is attached (i.e., NUCLEUS orSATELLITE).
Finally, EDUs and higher-order dis-course units are connected by discourse relations(e.g., DIS:ELABORATION).2.2 Kernels-based modelingIn the SKL framework, the learning objects arepairs of translations ?t1, t2?.
Our objective is toautomatically learn which pair features are impor-tant, independently of the source sentence.
Weachieve this by using kernel machines (KMs) overtwo learning objects ?t1, t2?, ?t?1, t?2?, along withan explicit and structural representation of thepairs (see Fig.
1).1The discourse parser can be downloaded fromhttp://alt.qcri.org/tools/More specifically, KMs carry out learning usingthe scalar productKmt(?t1, t2?, ?t?1, t?2?)
= ?mt(t1, t2) ?
?mt(t?1, t?2),where ?mtmaps pairs into the feature space.Considering that our task is to decide whethert1is better than t2, we can conveniently rep-resent the vector for the pair in terms of thedifference between the two translation vectors,i.e., ?mt(t1, t2) = ?K(t1) ?
?K(t2).
We canapproximate Kmtwith a preference kernel PK tocompute this difference in the kernel space K:PK(?t1, t2?, ?t?1, t?2?)
(1)= K(t1)?
?K(t2)) ?
(?K(t?1)?
?K(t?2))= K(t1, t?1) +K(t2, t?2)?K(t1, t?2)?K(t2, t?1)The advantage of this is that now K(ti, t?j) =?K(ti) ?
?K(t?j) is defined between two transla-tions only, and not between two pairs of transla-tions.
This simplification enables us to map trans-lations into simple trees, e.g., those in Figure 1,and then to apply them tree kernels, e.g., the Par-tial Tree Kernel (Moschitti, 2006), which carry outa scalar product in the subtree space.We can further enrich the representation ?K, ifwe consider all the information available to thehuman judges when they are ranking translations.That is, the two alternative translations along withtheir corresponding reference.216In particular, let r and r?be the references forthe pairs ?t1, t2?
and ?t?1, t?2?, we can redefine allthe members of Eq.
1, e.g., K(t1, t?1) becomesK(?t1, r?, ?t?1, r??)
= PTK(?M(t1, r), ?M(t?1, r?
))+ PTK(?M(r, t1), ?M(r?, t?1)),where ?Mmaps a pair of texts to a single tree.There are several options to produce the bitext-to-tree mapping for ?M.
A simple approach isto only use the tree corresponding to the first ar-gument of ?M.
This leads to the basic modelK(?t1, r?, ?t?1, r??)
= PTK(?M(t1), ?M(t?1)) +PTK(?M(r), ?M(r?
)), i.e., the sum of two treekernels applied to the trees constructed by ?M(wepreviously informally mentioned it).However, this simple mapping may be ineffec-tive since the trees within a pair, e.g., (t1, r), aretreated independently, and no meaningful featuresconnecting t1and r can be derived from theirtree fragments.
Therefore, we model ?M(r, t1) byusing word-matching relations between t1and r,such that connections between words and con-stituents of the two trees are established usingposition-independent word matching.
For exam-ple, in Figure 1, the thin dashed arrows show thelinks connecting the matching words between t1and r. The propagation of these relations worksfrom the bottom up.
Thus, if all children in a con-stituent have a link, their parent is also linked.The use of such connections is essential as it en-ables the comparison of the structural propertiesand relations between two translation-referencepairs.
For example, the tree fragment [ELABORA-TION [SATELLITE]] from the translation is con-nected to [ELABORATION [SATELLITE]] in thereference, indicating a link between two entire dis-course units (drawn with a thicker arrow), and pro-viding some reliability to the translation2.Note that the use of connections yields a graphrepresentation instead of a tree.
This is problem-atic as effective models for graph kernels, whichwould be a natural fit to this problem, are not cur-rently available for exploiting linguistic informa-tion.
Thus, we simply use K, as defined above,where the mapping ?M(t1, r) only produces a treefor t1annotated with the marker REL represent-ing the connections to r. This marker is placed onall node labels of the tree generated from t1thatmatch labels from the tree generated from r.2Note that a non-pairwise model, i.e., K(t1, r), couldalso be used to match the structural information above, butit would not learn to compare it to a second pair (t2, r).In other words, we only consider the trees en-riched by markers separately, and ignore the edgesconnecting both trees.3 Experiments and DiscussionWe experimented with datasets of segment-levelhuman rankings of system outputs from theWMT11 and the WMT12 Metrics shared tasks(Callison-Burch et al., 2011; Callison-Burch et al.,2012): we used the WMT11 dataset for trainingand the WMT12 dataset for testing.
We focusedon translating into English only, for which thedatasets can be split by source language: Czech(cs), German (de), Spanish (es), and French (fr).There were about 10,000 non-tied human judg-ments per language pair per dataset.
We scoredour pairwise system predictions with respect tothe WMT12 human judgments using the Kendall?sTau (?
), which was official at WMT12.Table 1 presents the ?
scores for all metric vari-ants introduced in this paper: for the individuallanguage pairs and overall.
The left-hand side ofthe table shows the results when using as sim-ilarity the direct kernel calculation between thecorresponding structures of the candidate transla-tion and the reference3, e.g., as in (Guzm?an et al.,2014; Joty et al., 2014).
The right-hand side con-tains the results for structured kernel learning.We can make the following observations:(i) The overall results for all SKL-trained metricsare higher than the ones when applying direct sim-ilarity, showing that learning tree structures is bet-ter than just calculating similarity.
(ii) Regarding the linguistic representation, we seethat, when learning tree structures, syntactic anddiscourse-based trees yield similar improvementswith a slight advantage for the former.
More in-terestingly, when both structures are put togetherin a combined tree, the improvement is cumula-tive and yields the best results by a sizable margin.This provides positive evidence towards our goalof a unified tree-based representation with multi-ple layers of linguistic information.
(iii) Comparing to the best evaluation metricsthat participated in the WMT12 Metrics sharedtask, we find that our approach is competitive andwould have been ranked among the top 3 partici-pants.3Applying tree kernels between the members of a pair togenerate one feature (for each different kernel function) hasbecome a standard practice in text similarity tasks (Severyn etal., 2013b) and in question answering (Severyn et al., 2013a).217Similarity Structured Kernel LearningStructure cs-en de-en es-en fr-en all cs-en de-en es-en fr-en all1 SYN 0.169 0.188 0.203 0.222 0.195 0.190 0.244 0.198 0.158 0.1982 DIS 0.130 0.174 0.188 0.169 0.165 0.176 0.235 0.166 0.160 0.1843 DIS+POS 0.135 0.186 0.190 0.178 0.172 0.167 0.232 0.202 0.133 0.1834 DIS+SYN 0.156 0.205 0.206 0.203 0.192 0.210 0.251 0.240 0.223 0.231Table 1: Kendall?s (? )
correlation with human judgements on WMT12 for each language pair.Furthermore, our result (0.237) is ahead of thecorrelation obtained by popular metrics such asTER (0.217), NIST (0.214) and BLEU (0.185) atWMT12.
This is very encouraging and shows thepotential of our new proposal.In this paper, we have presented only the firstexploratory results.
Our approach can be easilyextended with richer linguistic structures and fur-ther combined with some of the already existingstrong evaluation metrics.TestingTrain cs-en de-en es-en fr-en all1 cs-en 0.210 0.204 0.217 0.204 0.2092 de-en 0.196 0.251 0.203 0.202 0.2133 es-en 0.218 0.204 0.240 0.223 0.2214 fr-en 0.203 0.218 0.224 0.223 0.2175 all 0.231 0.258 0.226 0.232 0.237Table 2: Kendall?s (? )
on WMT12 for cross-language training with DIS+SYN.Note that the results in Table 1 were for train-ing on WMT11 and testing on WMT12 for eachlanguage pair in isolation.
Next, we study the im-pact of the choice of training language pair.
Ta-ble 2 shows cross-language evaluation results forDIS+SYN: lines 1-4 show results when training onWMT11 for one language pair, and then testing foreach language pair of WMT12.We can see that the overall differences in perfor-mance (see the last column: all) when training ondifferent source languages are rather small, rang-ing from 0.209 to 0.221, which suggests that ourapproach is quite independent of the source lan-guage used for training.
Still, looking at individ-ual test languages, we can see that for de-en andes-en, it is best to train on the same language; thisalso holds for fr-en, but there it is equally goodto train on es-en.
Interestingly, training on es-enimproves a bit for cs-en.These somewhat mixed results have motivatedus to try tuning on the full WMT11 dataset; as line5 shows, this yielded improvements for all lan-guage pairs except for es-en.
Comparing to line4 in Table 1, we see that the overall Tau improvedfrom 0.231 to 0.237.4 Conclusions and Future WorkWe have presented a pairwise learning-to-rank ap-proach to MT evaluation, which learns to differen-tiate good from bad translations in the context ofa given reference.
We have integrated several lay-ers of linguistic information (lexical, syntactic anddiscourse) in tree-based structures, and we haveused the structured kernel learning to identify rel-evant features and learn pairwise rankers.The evaluation results have shown that learningin the proposed SKL framework is possible, yield-ing better correlation (Kendall?s ? )
with humanjudgments than computing the direct kernel sim-ilarity between translation and reference, over thesame type of structures.
We have also shown thatthe contributions of syntax and discourse informa-tion are cumulative, indicating that this learningframework can be appropriate for the combinationof different sources of information.
Finally, de-spite the limited information we used, we achievedbetter correlation at the segment level than BLEUand other metrics in the WMT12 Metrics task.In the future, we plan to work towards our long-term goal, i.e., including more linguistic informa-tion in the SKL framework and showing that thiscan help.
This would also include more semanticinformation, e.g., in the form of Brown clusters orusing semantic similarity between the words com-posing the structure calculated with latent seman-tic analysis (Saleh et al., 2014b).We further want to show that the proposedframework is flexible and can include informationin the form of quality scores predicted by otherevaluation metrics, for which a vector of featureswould be combined with the structured kernel.AcknowledgmentsThis research is part of the Interactive sYstemsfor Answer Search (Iyas) project, conducted bythe Arabic Language Technologies (ALT) groupat Qatar Computing Research Institute (QCRI)within the Qatar Foundation.218ReferencesJoshua Albrecht and Rebecca Hwa.
2008.
Regressionfor machine translation evaluation at the sentencelevel.
Machine Translation, 22(1-2):1?27.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on StatisticalMachine Translation, WMT ?07, pages 136?158,Prague, Czech Republic.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, WMT ?11, pages 22?64, Edin-burgh, Scotland, UK.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, WMT?12, pages 10?51, Montr?eal, Canada.Elisabet Comelles, Jes?us Gim?enez, Llu?
?s M`arquez,Irene Castell?on, and Victoria Arranz.
2010.Document-level automatic MT evaluation based ondiscourse representations.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, WMT ?10, pages 333?338, Uppsala, Sweden.Kevin Duh.
2008.
Ranking vs. regression in machinetranslation evaluation.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation, WMT?08, pages 191?194, Columbus, Ohio, USA.Jes?us Gim?enez and Llu?
?s M`arquez.
2007.
Linguis-tic features for automatic evaluation of heterogenousMT systems.
In Proceedings of the Second Work-shop on Statistical Machine Translation, WMT ?07,pages 256?264, Prague, Czech Republic.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez, andPreslav Nakov.
2014.
Using discourse structureimproves machine translation evaluation.
In Pro-ceedings of 52nd Annual Meeting of the Associationfor Computational Linguistics, ACL ?14, pages 687?698, Baltimore, Maryland, USA.Shafiq Joty, Giuseppe Carenini, and Raymond Ng.2012.
A Novel Discriminative Framework forSentence-Level Discourse Analysis.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 904?915, Jeju Island, Korea.Shafiq Joty, Giuseppe Carenini, Raymond Ng, andYashar Mehdad.
2013.
Combining Intra- andMulti-sentential Rhetorical Parsing for Document-level Discourse Analysis.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, ACL ?13, pages 486?496, Sofia,Bulgaria.Shafiq Joty, Francisco Guzm?an, Llu?
?s M`arquez, andPreslav Nakov.
2014.
DiscoTK: Using discoursestructure for machine translation evaluation.
In Pro-ceedings of the Ninth Workshop on Statistical Ma-chine Translation, WMT ?14, pages 402?408, Balti-more, Maryland, USA.Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translation.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 25?32, Ann Ar-bor, Michigan, USA.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.2012.
Fully automatic semantic MT evaluation.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, WMT ?12, pages 243?252,Montr?eal, Canada.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for ques-tion answer classification.
In Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, ACL ?07, pages 776?783, Prague,Czech Republic.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Proceedings of 17th European Conference on Ma-chine Learning and the 10th European Conferenceon Principles and Practice of Knowledge Discoveryin Databases, ECML/PKDD ?06, pages 318?329,Berlin, Germany.Alessandro Moschitti.
2008.
Kernel methods, syn-tax and semantics for relational text categorization.In Proceedings of the 17th ACM Conference on In-formation and Knowledge Management, CIKM ?08,pages 253?262, Napa Valley, California, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meting of the Association for Com-putational Linguistics, ACL ?02, pages 311?318,Philadelphia, Pennsylvania, USA.Maja Popovi?c and Hermann Ney.
2007.
Word errorrates: Decomposition over POS classes and applica-tions for error analysis.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,WMT ?07, pages 48?55, Prague, Czech Republic.Vasin Punyakanok and Dan Roth.
2001.
The use ofclassifiers in sequential inference.
In Advances inNeural Information Processing Systems 14, NIPS?01, pages 995?1001, Vancouver, Canada.219Iman Saleh, Scott Cyphers, Jim Glass, Shafiq Joty,Llu?
?s M`arquez, Alessandro Moschitti, and PreslavNakov.
2014a.
A study of using syntactic and se-mantic structures for concept segmentation and la-beling.
In Proceedings of the 25th InternationalConference on Computational Linguistics, COLING?14, pages 193?202, Dublin, Ireland.Iman Saleh, Alessandro Moschitti, Preslav Nakov,Llu?
?s M`arquez, and Shafiq Joty.
2014b.
Semantickernels for semantic parsing.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?14, Doha, Qatar.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In Proceedings of the 35th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?12,pages 741?750, Portland, Oregon, USA.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Learning adaptable patterns forpassage reranking.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning, CoNLL ?13, pages 75?83, Sofia,Bulgaria.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning semantic textual sim-ilarity with structural representations.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), ACL ?13, pages 714?718, Sofia, Bulgaria.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Ameri-cas, AMTA ?06, Cambridge, Massachusetts, USA.Xingyi Song and Trevor Cohn.
2011.
Regression andranking based optimisation for sentence-level MTevaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, WMT ?11, pages123?129, Edinburgh, Scotland, UK.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, HLT-NAACL ?03, pages 173?180, Ed-monton, Canada.Billy Wong and Chunyu Kit.
2012.
Extending ma-chine translation evaluation metrics with lexical co-hesion to document level.
In Proceedings of the2012 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning, EMNLP-CoNLL ?12,pages 1060?1068, Jeju Island, Korea.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailmentswith cross-pair similarities.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics, COLING-ACL?06, pages 401?408, Sydney, Australia.220
