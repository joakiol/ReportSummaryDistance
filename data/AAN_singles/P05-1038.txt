Proceedings of the 43rd Annual Meeting of the ACL, pages 306?313,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsLexicalization in Crosslinguistic Probabilistic Parsing:The Case of FrenchAbhishek Arun and Frank KellerSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, UKa.arun@sms.ed.ac.uk, keller@inf.ed.ac.ukAbstractThis paper presents the first probabilisticparsing results for French, using the re-cently released French Treebank.
We startwith an unlexicalized PCFG as a base-line model, which is enriched to the levelof Collins?
Model 2 by adding lexical-ization and subcategorization.
The lexi-calized sister-head model and a bigrammodel are also tested, to deal with the flat-ness of the French Treebank.
The bigrammodel achieves the best performance:81% constituency F-score and 84% de-pendency accuracy.
All lexicalized mod-els outperform the unlexicalized baseline,consistent with probabilistic parsing re-sults for English, but contrary to resultsfor German, where lexicalization has onlya limited effect on parsing performance.1 IntroductionThis paper brings together two strands of researchthat have recently emerged in the field of probabilis-tic parsing: crosslinguistic parsing and lexicalizedparsing.
Interest in parsing models for languagesother than English has been growing, starting withwork on Czech (Collins et al, 1999) and Chinese(Bikel and Chiang, 2000; Levy and Manning, 2003).Probabilistic parsing for German has also been ex-plored by a range of authors (Dubey and Keller,2003; Schiehlen, 2004).
In general, these authorshave found that existing lexicalized parsing modelsfor English (e.g., Collins 1997) do not straightfor-wardly generalize to new languages; this typicallymanifests itself in a severe reduction in parsing per-formance compared to the results for English.A second recent strand in parsing research hasdealt with the role of lexicalization.
The conven-tional wisdom since Magerman (1995) has been thatlexicalization substantially improves performancecompared to an unlexicalized baseline model (e.g., aprobabilistic context-free grammar, PCFG).
How-ever, this has been challenged by Klein and Man-ning (2003), who demonstrate that an unlexicalizedmodel can achieve a performance close to the stateof the art for lexicalized models.
Furthermore, Bikel(2004) provides evidence that lexical information(in the form of bi-lexical dependencies) only makesa small contribution to the performance of parsingmodels such as Collins?s (1997).The only previous authors that have directly ad-dressed the role of lexicalization in crosslinguisticparsing are Dubey and Keller (2003).
They showthat standard lexicalized models fail to outperforman unlexicalized baseline (a vanilla PCFG) on Ne-gra, a German treebank (Skut et al, 1997).
Theyattribute this result to two facts: (a) The Negra an-notation assumes very flat trees, which means thatCollins-style head-lexicalization fails to pick up therelevant information from non-head nodes.
(b) Ger-man allows flexible word order, which means thatstandard parsing models based on context free gram-mars perform poorly, as they fail to generalize overdifferent positions of the same constituent.As it stands, Dubey and Keller?s (2003) work doesnot tell us whether treebank flatness or word orderflexibility is responsible for their results: for English,the annotation scheme is non-flat, and the word or-der is non-flexible; lexicalization improves perfor-mance.
For German, the annotation scheme is flatand the word order is flexible; lexicalization fails toimprove performance.
The present paper providesthe missing piece of evidence by applying proba-bilistic parsing models to French, a language withnon-flexible word order (like English), but with atreebank with a flat annotation scheme (like Ger-man).
Our results show that French patterns with En-glish: a large increase of parsing performance can beobtained by using a lexicalized model.
We concludethat the failure to find a sizable effect of lexicaliza-tion in German can be attributed to the word orderflexibility of that language, rather than to the flatnessof the annotation in the German treebank.The paper is organized as follows: In Section 2,we give an overview of the French Treebank we usefor our experiments.
Section 3 discusses its anno-tation scheme and introduces a set of tree transfor-mations that we apply.
Section 4 describes the pars-306<NP><w lemma="eux" ei="PROmp"ee="PRO-3mp" cat="PRO"subcat="3mp">eux</w></NP>Figure 1: Word-level annotation in the French Tree-bank: eux ?they?
(cat: POS tag, subcat: subcate-gory, ei, ee: inflection)ing models, followed by the results for the unlexi-calized baseline model in Section 6 and for a rangeof lexicalized models in Section 5.
Finally, Section 7provides a crosslinguistic comparison involving datasets of the same size extracted from the French, En-glish, and German treebanks.2 The French Treebank2.1 Annotation SchemeThe French Treebank (FTB; Abeille?
et al 2000) con-sists of 20,648 sentences extracted from the dailynewspaper Le Monde, covering a variety of authorsand domains (economy, literature, politics, etc.
).1The corpus is formatted in XML and has a rich mor-phosyntactic tagset that includes part-of-speech tag,?subcategorization?
(e.g., possessive or cardinal), in-flection (e.g., masculine singular), and lemma in-formation.
Compared to the Penn Treebank (PTB;Marcus et al 1993), the POS tagset of the FrenchTreebank is smaller (13 tags vs. 36 tags): all punc-tuation marks are represented as the single PONCTtag, there are no separate tags for modal verbs, wh-words, and possessives.
Also verbs, adverbs andprepositions are more coarsely defined.
On the otherhand, a separate clitic tag (CL) for weak pronouns isintroduced.
An example for the word-level annota-tion in the FTB is given in Figure 1The phrasal annotation of the FTB differs fromthat for the Penn Treebank in several aspects.
Thereis no verb phrase: only the verbal nucleus (VN) isannotated.
A VN comprises the verb and any clitics,auxiliaries, adverbs, and negation associated with it.This results in a flat syntactic structure, as in (1).
(1) (VN (V sont) (ADV syste?matiquement) (Varre?te?s)) ?are systematically arrested?The noun phrases (NPs) in the FTB are also flat; anoun is grouped together with any associated deter-miners and prenominal adjectives, as in example (2).Note that postnominal adjectives, however, are ad-joined to the NP in an adjectival phrase (AP).1The French Treebank was developed at Universite?
Paris 7.A license can be obtained by emailing Anne Abeille?
(abeille@linguist.jussieu.fr).<w compound="yes" lemma="d?entre"ei="P" ee="P" cat="P"><w catint="P">d?</w><w catint="P">entre</w></w>Figure 2: Annotation of compounds in the FrenchTreebank: d?entre ?between?
(catint: compound-internal POS tag)(2) (NP (D des) (A petits) (N mots) (AP (ADV tre`s)(A gentils))) ?small, very gentle words?Unlike the PTB, the FTB annotates coordinatedphrases with the syntactic tag COORD (see the leftpanel of Figure 3 for an example).The treatment of compounds is also different inthe FTB.
Compounds in French can comprise wordswhich do not exist otherwise (e.g., insu in the com-pound preposition a` l?insu de ?unbeknownst to?)
orcan exhibit sequences of tags otherwise ungrammat-ical (e.g., a` la va vite ?in a hurry?
: Prep + Det +finite verb + adverb).
To account for these proper-ties, compounds receive a two-level annotation inthe FTB: a subordinate level is added for the con-stituent parts of the compound (both levels use thesame POS tagset).
An example is given in Figure 2.Finally, the FTB differs from the PTB in that itdoes not use any empty categories.2.2 Data SetsThe version of the FTB made available to us (ver-sion 1.4, May 2004) contains numerous errors.
Twomain classes of inaccuracies were found in the data:(a) The word is present but morphosyntactic tagsare missing; 101 such cases exist.
(b) The tag in-formation for a word (or a part of a compound) ispresent but the word (or compound part) itself ismissing.
There were 16,490 instances of this errorin the dataset.Initially we attempted to correct the errors, butthis proved too time consuming, and we often foundthat the errors cannot be corrected without access tothe raw corpus, which we did not have.
We thereforedecided to remove all sentences with errors, whichlead to a reduced dataset of 10,552 sentences.The remaining data set (222,569 words at an av-erage sentence length of 21.1 words) was split intoa training set, a development set (used to test theparsing models and to tune their parameters), and atest set, unseen during development.
The training setconsisted of the first 8,552 sentences in the corpus,with the following 1000 sentences serving as the de-velopment set and the final 1000 sentences formingthe test set.
All results reported in this paper wereobtained on the test set, unless stated otherwise.3073 Tree TransformationsWe created a number of different datasets from theFTB, applying various tree transformation to dealwith the peculiarities of the FTB annotation scheme.As a first step, the XML formatted FTB data wasconverted to PTB-style bracketed expressions.
Onlythe POS tag was kept and the rest of the morphologi-cal information for each terminal was discarded.
Forexample, the NP in Figure 1 was transformed to:(3) (NP (PRO eux))In order to make our results comparable to re-sults from the literature, we also transformed theannotation of punctuation.
In the FTB, all punc-tuations is tagged uniformly as PONCT.
We re-assigned the POS for punctuation using the PTBtagset, which differentiates between commas, peri-ods, brackets, etc.Compounds have internal structure in the FTB(see Section 2.1).
We created two separate data setsby applying two alternative tree transformation tomake FTB compounds more similar to compoundsin other annotation schemes.
The first was collaps-ing the compound by concatenating the compoundparts using an underscore and picking up the catinformation supplied at the compound level.
For ex-ample, the compound in Figure 2 results in:(4) (P d?
entre)This approach is similar to the treatment of com-pounds in the German Negra treebank (used byDubey and Keller 2003), where compounds are notgiven any internal structure (compounds are mostlyspelled without spaces or apostrophes in German).The second approach is expanding the compound.Here, the compound parts are treated as individualwords with their own POS (from the catint tag),and the suffix Cmp is appended the POS of the com-pound, effectively expanding the tagset.2 Now Fig-ure 2 yields:(5) (PCmp (P d?)
(P entre)).This approach is similar to the treatment of com-pounds in the PTB (except hat the PTB does not usea separate tag for the mother category).
We foundthat in the FTB the POS tag of the compound partis sometimes missing (i.e., the value of catint isblank).
In cases like this, the missing catint wassubstituted with the cat tag of the compound.
Thisheuristic produces the correct POS for the subpartsof the compound most of the time.2An alternative would be to retain the cat tag of the com-pound.
The effect of this decision needs to be investigated infuture work.XPX COORDC XPXXPX C XPXXPX C XFigure 3: Coordination in the FTB: before (left) andafter transformation (middle); coordination in thePTB (right)As mentioned previously, coordinate structureshave their own constituent label COORD in theFTB annotation.
Existing parsing models (e.g., theCollins models) have coordination-specific rules,presupposing that coordination is marked up in PTBformat.
We therefore created additional datasetswhere a transformation is applied that raises coor-dination.
This is illustrated in Figure 3.
Note thatin the FTB annotation scheme, a coordinating con-junction is always followed by a syntactic category.Hence the resulting tree, though flatter, is still notfully compatible with the PTB treatment of coordi-nation.4 Probabilistic Parsing Models4.1 Probabilistic Context-Free GrammarsThe aim of this paper is to further explore thecrosslinguistic role of lexicalization by applying lex-icalized parsing models to the French Treebank pars-ing accuracy.
Following Dubey and Keller (2003),we use a standard unlexicalized PCFG as our base-line.
In such a model, each context-free rule RHS ?LHS is annotated with an expansion probabilityP(RHS|LHS).
The probabilities for all the rules withthe same left-hand side have to sum up to one andthe probability of a parse tree T is defined as theproduct of the probabilities of each rule applied inthe generation of T .4.2 Collins?
Head-Lexicalized ModelsA number of lexicalized models can then be appliedto the FTB, comparing their performance to the un-lexicalized baseline.
We start with Collins?
Model 1,which lexicalizes a PCFG by associating a word wand a POS tag t with each non-terminal X in thetree.
Thus, a non-terminal is written as X(x) wherex = ?w, t?
and X is constituent label.
Each rule nowhas the form:P(h) ?
Ln(ln) .
.
.L1(l1)H(h)R1(r1) .
.
.Rm(rm)(1)Here, H is the head-daughter of the phrase, whichinherits the head-word h from its parent P. L1 .
.
.Lnand R1 .
.
.Rn are the left and right sisters of H. Eithern or m may be zero, and n = m for unary rules.308The addition of lexical heads leads to an enor-mous number of potential rules, making direct esti-mation of P(RHS|LHS) infeasible because of sparsedata.
Therefore, the generation of the RHS of a rulegiven the LHS is decomposed into three steps: firstthe head is generated, then the left and right sistersare generated by independent 0th-order Markov pro-cesses.
The probability of a rule is thus defined as:P(RHS|LHS) =P(Ln(ln) .
.
.L1(l1)H(h),R1(r1) .
.
.Rm(rm)|P(h))= Ph(H|P,h)?
?m+1i=1 Pr(Ri(ri)|P,h,H,d(i))?
?n+1i=1 Pl(Li(li)|P,h,H,d(i))(2)Here, Ph is the probability of generating the head, Pland Pr are the probabilities of generating the left andright sister respectively.
Lm+1(lm+1) and Rm+1(rm+1)are defined as stop categories which indicate when tostop generating sisters.
d(i) is a distance measure, afunction of the length of the surface string betweenthe head and the previously generated sister.Collins?
Model 2 further refines the initial modelby incorporating the complement/adjunct distinctionand subcategorization frames.
The generative pro-cess is enhanced to include a probabilistic choice ofleft and right subcategorization frames.
The proba-bility of a rule is now:Ph(H|P,h)?Plc(LC|P,H,h)?Prc(RC|P,H,h)?
?m+1i=1 Pr(Ri(ri)|P,h,H,d(i),RC)?
?n+1i=1 Pl(Li(li)|P,h,H,d(i),LC)(3)Here, LC and RC are left and right subcat frames,multisets specifying the complements that the headrequires in its left or right sister.
The subcat re-quirements are added to the conditioning context.
Ascomplements are generated, they are removed fromthe appropriate subcat multiset.5 Experiment 1: Unlexicalized Model5.1 MethodThis experiment was designed to compare the per-formance of the unlexicalized baseline model onfour different datasets, created by the tree trans-formations described in Section 3: compoundsexpanded (Exp), compounds contracted (Cont),compounds expanded with coordination raised(Exp+CR), and compounds contracted with coordi-nation raised (Cont+CR).We used BitPar (Schmid, 2004) for our unlexi-calized experiments.
BitPar is a parser based on abit-vector implementation of the CKY algorithm.
Agrammar and lexicon were read off our training set,along with rule frequencies and frequencies for lex-ical items, based on which BitPar computes the ruleModel LR LP CBs 0CB ?2CB Tag CovExp 59.97 58.64 1.74 39.05 73.23 91.00 99.20Exp+CR 60.75 60.57 1.57 40.77 75.03 91.08 99.09Cont 64.19 64.61 1.50 46.74 76.80 93.30 98.48Cont+CR 66.11 65.55 1.39 46.99 78.95 93.22 97.94Table 1: Results for unlexicalized models (sentences?40 words); each model performed its own POStagging.probabilities using maximum likelihood estimation.A frequency distribution for POS tags was also readoff the training set; this distribution is used by BitParto tag unknown words in the test data.All models were evaluated using standard Par-seval measures of labeled recall (LR), labeled pre-cision (LP), average crossing brackets (CBs), zerocrossling brackets (0CB), and two or less crossingbrackets (?2CB).
We also report tagging accuracy(Tag), and coverage (Cov).5.2 ResultsThe results for the unlexicalized model are shown inTable 1 for sentences of length ?40 words.
We findthat contracting compounds increases parsing per-formance substantially compared to expanding com-pounds, raising labeled recall from around 60% toaround 64% and labeled precision from around 59%to around 65%.
The results show that raising co-ordination is also beneficial; it increases precisionand recall by 1?2%, both for expanded and for non-expanded compounds.Note that these results were obtained by uni-formly applying coordination raising during evalu-ation, so as to make all models comparable.
For theExp and Cont models, the parsed output and the goldstandard files were first converted by raising coordi-nation and then the evaluation was performed.5.3 DiscussionThe disappointing performance obtained for the ex-panded compound models can be partly attributedto the increase in the number of grammar rules(11,704 expanded vs. 10,299 contracted) and POStags (24 expanded vs. 11 contracted) associated withthat transformation.However, a more important point observation isthat the two compound models do not yield compa-rable results, since an expanded compound has morebrackets than a contracted one.
We attempted to ad-dress this problem by collapsing the compounds forevaluation purposes (as described in Section 3).
Forexample, (5) would be contracted to (4).
However,this approach only works if we are certain that themodel is tagging the right words as compounds.
Un-309fortunately, this is rarely the case.
For example, themodel outputs:(6) (NCmp (N jours) (N commerc?ants))But in the gold standard file, jours and commerc?antsare two distinct NPs.
Collapsing the compoundstherefore leads to length mismatches in the test data.This problem occurs frequently in the test set, so thatsuch an evaluation becomes pointless.6 Experiment 2: Lexicalized Models6.1 MethodParsing We now compare a series of lexicalizedparsing models against the unlexicalized baseline es-tablished in the previous experiment.
Our is was totest if French behaves like English in that lexicaliza-tion improves parsing performance, or like German,in that lexicalization has only a small effect on pars-ing performance.The lexicalized parsing experiments were run us-ing Dan Bikel?s probabilistic parsing engine (Bikel,2002) which in addition to replicating the modelsdescribed by Collins (1997) also provides a con-venient interface to develop corresponding parsingmodels for other languages.Lexicalization requires that each rule in a gram-mar has one of the categories on its right hand sideannotated as the head.
These head rules were con-structed based on the FTB annotation guidelines(provided along with the dataset), as well as by us-ing heuristics, and were optimized on the develop-ment set.
Collins?
Model 2 incorporates a comple-ment/adjunct distinction and probabilities over sub-categorization frames.
Complements were markedin the training phase based on argument identifica-tion rules, tuned on the development set.Part of speech tags are generated along withthe words in the models; parsing and tagging arefully integrated.
To achieve this, Bikel?s parserrequires a mapping of lexical items to ortho-graphic/morphological word feature vectors.
Thefeatures implemented (capitalization, hyphenation,inflection, derivation, and compound) were againoptimized on the development set.Like BitPar, Bikel?s parser implements a prob-abilistic version of the CKY algorithm.
As withnormal CKY, even though the model is defined ina top-down, generative manner, decoding proceedsbottom-up.
To speed up decoding, the algorithm im-plements beam search.
Collins uses a beam width of104, while we found that a width of 105 gave us thebest coverage vs. parsing speed trade-off.Label FTB PTB Negra Label FTB PTB NegraSENT 5.84 2.22 4.55 VPpart 2.51 ?
?Ssub 4.41 ?
?
VN 1.76 ?
?Sint 3.44 ?
?
PP 2.10 2.03 3.08Srel 3.92 ?
?
NP 2.45 2.20 3.08VP ?
2.32 2.59 AdvP 2.24 ?
2.08VPinf 3.07 ?
?
AP 1.34 ?
2.22Table 2: Average number of daughter nodes per con-stituents in three treebanksFlatness As already pointed out in Section 2.1,the FTB uses a flat annotation scheme.
This canbe quantified by computing the average number ofdaughters for each syntactic category in the FTB,and comparing them with the figures available forPTB and Negra (Dubey and Keller, 2003).
This isdone in Table 2.
The absence of sentence-internalVPs explains the very high level of flatness for thesentential category SENT (5.84 daughters), com-pared to the PTB (2.44), and even to Negra, which isalso very flat (4.55 daughters).
The other sententialcategories Ssub (subordinate clauses), Srel (relativeclause), and Sint (interrogative clause) are also veryflat.
Note that the FTB uses VP nodes only for non-finite subordinate clauses: VPinf (infinitival clause)and VPpart (participle clause); these categories areroughly comparable in flatness to the VP categoryin the PTB and Negra.
For NP, PPs, APs, and AdvPsthe FTB is roughly as flat as the PTB, and somewhatless flat than Negra.Sister-Head Model To cope with the flatness ofthe FTB, we implemented three additional parsingmodels.
First, we implemented Dubey and Keller?s(2003) sister-head model, which extends Collins?base NP model to all syntactic categories.
Thismeans that the probability function Pr in equation (2)is no longer conditioned on the head but instead onits previous sister, yielding the following definitionfor Pr (and by analogy Pl):Pr(Ri(ri)|P,Ri?1(ri?1),d(i))(4)Dubey and Keller (2003) argue that this implicitlyadds binary branching to the grammar, and thereforeprovides a way of dealing with flat annotation (inNegra and in the FTB, see Table 2).Bigram Model This model, inspired by the ap-proach of Collins et al (1999) for parsing the PragueDependency Treebank, builds on Collins?
Model 2by implementing a 1st order Markov assumption forthe generation of sister non-terminals.
The latter arenow conditioned, not only on their head, but also onthe previous sister.
The probability function for Pr(and by analogy Pl) is now:Pr(Ri(ri)|P,h,H,d(i),Ri?1,RC)(5)310Model LR LP CBs 0CB ?2CB Tag CovModel 1 80.35 79.99 0.78 65.22 89.46 96.86 99.68Model 2 80.49 79.98 0.77 64.85 90.10 96.83 99.68SisterHead 80.47 80.56 0.78 64.96 89.34 96.85 99.57Bigram 81.15 80.84 0.74 65.21 90.51 96.82 99.46BigramFlat 80.30 80.05 0.77 64.78 89.13 96.71 99.57Table 3: Results for lexicalized models (sentences?40 words); each model performed its own POStagging; all lexicalized models used the Cont+CRdata setThe intuition behind this approach is that the modelwill learn that the stop symbol is more likely to fol-low phrases with many sisters.
Finally, we also ex-perimented with a third model (BigramFlat) that ap-plies the bigram model only for categories with highdegrees of flatness (SENT, Srel, Ssub, Sint, VPinf,and VPpart).6.2 ResultsConstituency Evaluation The lexicalized modelswere tested on the Cont+CR data set, i.e., com-pounds were contracted and coordination was raised(this is the configuration that gave the best perfor-mance in Experiment 1).Table 3 shows that all lexicalized models achievea performance of around 80% recall and precision,i.e., they outperform the best unlexicalized model byat least 14% (see Table 1).
This is consistent withwhat has been reported for English on the PTB.Collins?
Model 2, which adds the comple-ment/adjunct distinction and subcategorizationframes achieved only a very small improvementover Collins?
Model 1, which was not statisticallysignificant using a ?2 test.
It might well be thatthe annotation scheme of the FTB does not lenditself particularly well to the demands of Model 2.Moreover, as Collins (1997) mentions, some ofthe benefits of Model 2 are already captured byinclusion of the distance measure.A further small improvement was achieved us-ing Dubey and Keller?s (2003) sister-head model;however, again the difference did not reach sta-tistical significance.
The bigram model, however,yielded a statistically significant improvement overCollins?
Model 1 (recall ?2 = 3.91, df = 1, p?
.048;precision ?2 = 3.97, df = 1, p ?
.046).
This is con-sistent with the findings of Collins et al (1999)for Czech, where the bigram model upped depen-dency accuracy by about 0.9%, as well as for En-glish where Charniak (2000) reports an increasein F-score of approximately 0.3%.
The BigramFlatmodel, which applies the bigram model to only thoselabels which have a high degree of flatness, performsModel LR LP CBs 0CB ?2CB Tag CovExp+CR 65.50 64.76 1.49 42.36 77.48 100.0 97.83Cont+CR 69.35 67.93 1.34 47.43 80.25 100.0 96.97Model1 81.51 81.43 0.78 64.60 89.25 98.54 99.78Model2 81.69 81.59 0.78 63.84 89.69 98.55 99.78SisterHead 81.08 81.56 0.79 64.35 89.57 98.51 99.57Bigram 81.78 81.91 0.78 64.96 89.12 98.81 99.67BigramFlat 81.14 81.19 0.81 63.37 88.80 98.80 99.67Table 4: Results for lexicalized and unlexical-ized models (sentences ?40 words) with correctPOS tags supplied; all lexicalized models used theCont+CR data setat roughly the same level as Model 1.The models in Tables 1 and 3 implemented theirown POS tagging.
Tagging accuracy was 91?93%for BitPar (unlexicalized models) and around 96%for the word-feature enhanced tagging model of theBikel parser (lexicalized models).
POS tags are animportant cue for parsing.
To gain an upper boundon the performance of the parsing models, we reranthe experiments by providing the correct POS tagfor the words in the test set.
While BitPar alwaysuses the tags provided, the Bikel parser only usesthem for words whose frequency is less than the un-known word threshold.
As Table 4 shows, perfecttagging increased parsing performance in the lexi-calized models by around 3%.
This shows that thepoor POS tagging performed by BitPar is one of thereasons of the poor performance of the lexicalizedmodels.
The impact of perfect tagging is less dras-tic on the lexicalized models (around 1% increase).However, our main finding, viz., that lexicalizedmodels outperform unlexicalized models consider-able on the FTB, remains valid, even with perfecttagging.3Dependency Evaluation We also evaluated ourmodels using dependency measures, which havebeen argued to be more annotation-neutral thanParseval.
Lin (1995) notes that labeled bracketingscores are more susceptible to cascading errors,where one incorrect attachment decision causes thescoring algorithm to count more than one error.The gold standard and parsed trees were con-verted into dependency trees using the algorithm de-scribed by Lin (1995).
Dependency accuracy is de-fined as the ratio of correct dependencies over the to-tal number of dependencies in a sentence.
(Note thatthis is an unlabeled dependency measure.)
Depen-dency accuracy and constituency F-score are shown3It is important to note that the Collins model has a rangeof other features that set it apart from a standard unlexicalizedPCFG (notably Markovization), as discussed in Section 4.2.
Itis therefore likely that the gain in performance is not attributableto lexicalization alone.311Model Dependency F-scoreCont+CR 73.09 65.83Model 2 83.96 80.23SisterHead 84.00 80.51Bigram 84.20 80.99Table 5: Dependency vs. constituency scores for lex-icalized and unlexicalized modelsin Table 5 for the most relevant FTB models.
(F-score is computed as the geometric mean of labeledrecall and precision.
)Numerically, dependency accuracies are higherthan constituency F-scores across the board.
How-ever, the effect of lexicalization is the same on bothmeasures: for the FTB, a gain of 11% in dependencyaccuracy is observed for the lexicalized model.7 Experiment 3: CrosslinguisticComparisonThe results reported in Experiments 1 and 2 shedsome light on the role of lexicalization for parsingFrench, but they are not strictly comparable to theresults that have been reported for other languages.This is because the treebanks available for differentlanguages typically vary considerably in size: ourFTB training set was about 8,500 sentences large,while the standard training set for the PTB is about40,000 sentences in size, and the Negra training setused by Dubey and Keller (2003) comprises about18,600 sentences.
This means that the differences inthe effect of lexicalization that we observe could besimply due to the size of the training set: lexicalizedmodels are more susceptible to data sparseness thanunlexicalized ones.We therefore conducted another experiment inwhich we applied Collins?
Model 2 to subsets ofthe PTB that were comparable in size to our FTBdata sets.
We combined sections 02?05 and 08 ofthe PTB (8,345 sentences in total) to form the train-ing set, and the first 1,000 sentences of section 23to form our test set.
As a baseline model, we alsorun an unlexicalized PCFG on the same data sets.For comparison with Negra, we also include the re-sults of Dubey and Keller (2003): they report theperformance of Collins?
Model 1 on a data set of9,301 sentences and a test set of 1,000 sentences,which are comparable in size to our FTB data sets.4The results of the crosslinguistic comparison areshown in Table 6.5 We conclude that the effect of4Dubey and Keller (2003) report only F-scores for the re-duced data set (see their Figure 1); the other scores were pro-vided by Amit Dubey.
No results for Model 2 are available.5For this experiments, the same POS tagging model was ap-plied to the PTB and the FTB data, which is why the FTB fig-Corpus Model LR LP CBs 0CB ?2CBFTB Cont+CR 66.11 65.55 1.39 46.99 78.95Model 2 79.20 78.58 0.83 63.33 89.23PTB Unlex 72.79 75.23 2.54 31.56 58.98Model 2 86.43 86.79 1.17 57.80 82.44Negra Unlex 69.64 67.27 1.12 54.21 82.84Model 1 68.33 67.32 0.83 60.43 88.78Table 6: The effect of lexicalization on different cor-pora for training sets of comparable size (sentences?40 words)lexicalization is stable even if the size of the train-ing set is held constant across languages: For theFTB we find that lexicalization increases F-score byaround 13%.
Also for the PTB, we find an effect oflexicalization of about 14%.
For the German Negratreebank, however, the performance of the lexical-ized and the unlexicalized model are almost indis-tinguishable.
(This is true for Collins?
Model 1; notethat Dubey and Keller (2003) do report a small im-provement for the lexicalized sister-head model.
)8 Related WorkWe are not aware of any previous attempts to builda probabilistic, treebank-trained parser for French.However, there is work on chunking for French.
Thegroup who built the French Treebank (Abeille?
et al,2000) used a rule-based chunker to automaticallyannotate the corpus with syntactic structures, whichwere then manually corrected.
They report an un-labeled recall/precision of 94.3/94.2% for openingbrackets and 92.2/91.4% for closing brackets, and alabel accuracy of 95.6%.
This result is not compara-ble to our results for full parsing.Giguet and Vergne (1997) present use a memory-based learner to predict chunks and dependenciesbetween chunks.
The system is evaluated on textsfrom Le Monde (different from the FTB texts).
Re-sults are only reported for verb-object dependencies,for which recall/precision is 94.04/96.39%.
Again,these results are not comparable to ours, which wereobtained using a different corpus, a different depen-dency scheme, and for a full set of dependencies.9 ConclusionsIn this paper, we provided the first probabilis-tic, treebank-trained parser for French.
In Exper-iment 1, we established an unlexicalized baselinemodel, which yielded a labeled precision and re-call of about 66%.
We experimented with a num-ber of tree transformation that take account of thepeculiarities of the annotation of the French Tree-ures are slightly lower than in Table 3.312bank; the best performance was obtained by rais-ing coordination and contracting compounds (whichhave internal structure in the FTB).
In Experiment 2,we explored a range of lexicalized parsing models,and found that lexicalization improved parsing per-formance by up to 15%: Collins?
Models 1 and 2performed at around 80% LR and LP.
No signifi-cant improvement could be achieved by switching toDubey and Keller?s (2003) sister-head model, whichhas been claimed to be particularly suitable for tree-banks with flat annotation, such as the FTB.
A smallbut significant improvement (to 81% LR and LP)was obtained by a bigram model that combines fea-tures of the sister-head model and Collins?
model.These results have important implications forcrosslinguistic parsing research, as they allow usto tease apart language-specific and annotation-specific effects.
Previous work for English (e.g.,Magerman, 1995; Collins, 1997) has shown that lex-icalization leads to a sizable improvement in pars-ing performance.
English is a language with non-flexible word order and with a treebank with a non-flat annotation scheme (see Table 2).
Research onGerman (Dubey and Keller, 2003) showed that lex-icalization leads to no sizable improvement in pars-ing performance for this language.
German has aflexible word order and a flat treebank annotation,both of which could be responsible for this counter-intuitive effect.
The results for French presented inthis paper provide the missing piece of evidence:they show that French behaves like English in thatit shows a large effect of lexicalization.
Like En-glish, French is a language with non-flexible wordorder, but like the German Treebank, the FrenchTreebank has a flat annotation.
We conclude thatDubey and Keller?s (2003) results for German can beattributed to a language-specific factor (viz., flexibleword order) rather than to an annotation-specific fac-tor (viz., flat annotation).
We confirmed this claim inExperiment 3 by showing that the effects of lexical-ization observed for English, French, and Germanare preserved if the size of the training set is keptconstant across languages.An interesting prediction follows from the claimthat word order flexibility, rather than flatness ofannotation, is crucial for lexicalization.
A languagewhich has a flexible word order (like German), buta non-flat treebank (like English) should show noeffect of lexicalization, i.e., lexicalized models arepredicted not to outperform unlexicalized ones.
Infuture work, we plan to test this prediction for Ko-rean, a flexible word order language whose treebank(Penn Korean Treebank) has a non-flat annotation.ReferencesAbeille?, Anne, Lionel Clement, and Alexandra Kinyon.
2000.Building a treebank for French.
In Proceedings of the 2nd In-ternational Conference on Language Resources and Evalu-ation.
Athens.Bikel, Daniel M. 2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedings of the2nd International Conference on Human Language Technol-ogy Research.
Morgan Kaufmann, San Francisco.Bikel, Daniel M. 2004.
A distributional analysis of a lexicalizedstatistical parsing model.
In Dekang Lin and Dekai Wu, ed-itors, Proceedings of the Conference on Empirical Methodsin Natural Language Processing.
Barcelona, pages 182?189.Bikel, Daniel M. and David Chiang.
2000.
Two statistical pars-ing models applied to the Chinese treebank.
In Proceedingsof the 2nd ACL Workshop on Chinese Language Processing.Hong Kong.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Conference of the North AmericanChapter of the Association for Computational Linguistics.Seattle, WA, pages 132?139.Collins, Michael.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Linguisticsand the 8th Conference of the European Chapter of the Asso-ciation for Computational Linguistics.
Madrid, pages 16?23.Collins, Michael, Jan Hajic?, Lance Ramshaw, and ChristophTillmann.
1999.
A statistical parser for Czech.
In Pro-ceedings of the 37th Annual Meeting of the Association forComputational Linguistics.
University of Maryland, CollegePark.Dubey, Amit and Frank Keller.
2003.
Probabilistic parsing forGerman using sister-head dependencies.
In Proceedings ofthe 41st Annual Meeting of the Association for Computa-tional Linguistics.
Sapporo, pages 96?103.Giguet, Emmanuel and Jacques Vergne.
1997.
From part-of-speech tagging to memory-based deep syntactic analysis.
InProceedings of the International Workshop on Parsing Tech-nologies.
Boston, pages 77?88.Klein, Dan and Christopher Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics.
Sapporo.Levy, Roger and Christopher Manning.
2003.
Is it harder toparse Chinese, or the Chinese treebank?
In Proceedings ofthe 41st Annual Meeting of the Association for Computa-tional Linguistics.
Sapporo.Lin, Dekang.
1995.
A dependency-based method for evaluatingbroad-coverage parsers.
In Proceedings of the InternationalJoint Conference on Artificial Intelligence.
Montreal, pages1420?1425.Magerman, David.
1995.
Statistical decision-tree models forparsing.
In Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics.
Cambridge, MA,pages 276?283.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn Treebank.
Computational Linguistics19(2):313?330.Schiehlen, Michael.
2004.
Annotation strategies for probabilis-tic parsing in German.
In Proceedings of the 20th Interna-tional Conference on Computational Linguistics.
Geneva.Schmid, Helmut.
2004.
Efficient parsing of highly ambiguouscontext-free grammars with bit vectors.
In Proceedings ofthe 20th International Conference on Computational Lin-guistics.
Geneva.Skut, Wojciech, Brigitte Krenn, Thorsten Brants, and HansUszkoreit.
1997.
An annotation scheme for free word orderlanguages.
In Proceedings of the 5th Conference on AppliedNatural Language Processing.
Washington, DC.313
