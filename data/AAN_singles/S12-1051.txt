First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385?393,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSemEval-2012 Task 6: A Pilot on Semantic Textual SimilarityEneko AgirreUniversity of the Basque CountryDonostia, 20018, Basque Countrye.agirre@ehu.esDaniel CerStanford UniversityStanford, CA 94305, USAdanielcer@stanford.eduMona DiabCenter for Computational Learning SystemsColumbia Universitymdiab@ccls.columbia.eduAitor Gonzalez-AgirreUniversity of the Basque CountryDonostia, 20018, Basque Countryagonzalez278@ikasle.ehu.esAbstractSemantic Textual Similarity (STS) measuresthe degree of semantic equivalence betweentwo texts.
This paper presents the results ofthe STS pilot task in Semeval.
The trainingdata contained 2000 sentence pairs from pre-viously existing paraphrase datasets and ma-chine translation evaluation resources.
Thetest data also comprised 2000 sentences pairsfor those datasets, plus two surprise datasetswith 400 pairs from a different machine trans-lation evaluation corpus and 750 pairs from alexical resource mapping exercise.
The sim-ilarity of pairs of sentences was rated on a0-5 scale (low to high similarity) by humanjudges using Amazon Mechanical Turk, withhigh Pearson correlation scores, around 90%.35 teams participated in the task, submitting88 runs.
The best results scored a Pearsoncorrelation>80%, well above a simple lexicalbaseline that only scored a 31% correlation.This pilot task opens an exciting way ahead,although there are still open issues, speciallythe evaluation metric.1 IntroductionSemantic Textual Similarity (STS) measures thedegree of semantic equivalence between two sen-tences.
STS is related to both Textual Entailment(TE) and Paraphrase (PARA).
STS is more directlyapplicable in a number of NLP tasks than TE andPARA such as Machine Translation and evaluation,Summarization, Machine Reading, Deep QuestionAnswering, etc.
STS differs from TE in as much asit assumes symmetric graded equivalence betweenthe pair of textual snippets.
In the case of TE theequivalence is directional, e.g.
a car is a vehicle, buta vehicle is not necessarily a car.
Additionally, STSdiffers from both TE and PARA in that, rather thanbeing a binary yes/no decision (e.g.
a vehicle is not acar), STS incorporates the notion of graded semanticsimilarity (e.g.
a vehicle and a car are more similarthan a wave and a car).STS provides a unified framework that allows foran extrinsic evaluation of multiple semantic compo-nents that otherwise have tended to be evaluated in-dependently and without broad characterization oftheir impact on NLP applications.
Such componentsinclude word sense disambiguation and induction,lexical substitution, semantic role labeling, multi-word expression detection and handling, anaphoraand coreference resolution, time and date resolution,named-entity handling, underspecification, hedging,semantic scoping and discourse analysis.
Thoughnot in the scope of the current pilot task, we plan toexplore building an open source toolkit for integrat-ing and applying diverse linguistic analysis modulesto the STS task.While the characterization of STS is still prelim-inary, we observed that there was no comparableexisting dataset extensively annotated for pairwisesemantic sentence similarity.
We approached theconstruction of the first STS dataset with the fol-lowing goals: (1) To set a definition of STS as agraded notion which can be easily communicated tonon-expert annotators beyond the likert-scale; (2) Togather a substantial amount of sentence pairs fromdiverse datasets, and to annotate them with highquality; (3) To explore evaluation measures for STS;(4) To explore the relation of STS to PARA and Ma-chine Translation Evaluation exercises.385In the next section we present the various sourcesof the STS data and the annotation procedure used.Section 4 investigates the evaluation of STS sys-tems.
Section 5 summarizes the resources and toolsused by participant systems.
Finally, Section 6draws the conclusions.2 Source DatasetsDatasets for STS are scarce.
Existing datasets in-clude (Li et al, 2006) and (Lee et al, 2005).
Thefirst dataset includes 65 sentence pairs which cor-respond to the dictionary definitions for the 65word pairs in Similarity(Rubenstein and Goode-nough, 1965).
The authors asked human informantsto assess the meaning of the sentence pairs on ascale from 0.0 (minimum similarity) to 4.0 (maxi-mum similarity).
While the dataset is very relevantto STS, it is too small to train, develop and test typ-ical machine learning based systems.
The seconddataset comprises 50 documents on news, rangingfrom 51 to 126 words.
Subjects were asked to judgethe similarity of document pairs on a five-point scale(with 1.0 indicating ?highly unrelated?
and 5.0 indi-cating ?highly related?).
This second dataset com-prises a larger number of document pairs, but it goesbeyond sentence similarity into textual similarity.When constructing our datasets, gathering natu-rally occurring pairs of sentences with different de-grees of semantic equivalence was a challenge in it-self.
If we took pairs of sentences at random, thevast majority of them would be totally unrelated, andonly a very small fragment would show some sort ofsemantic equivalence.
Accordingly, we investigatedreusing a collection of existing datasets from tasksthat are related to STS.We first studied the pairs of text from the Recog-nizing TE challenge.
The first editions of the chal-lenge included pairs of sentences as the following:T: The Christian Science Monitor named a USjournalist kidnapped in Iraq as freelancer JillCarroll.H: Jill Carroll was abducted in Iraq.The first sentence is the text, and the second isthe hypothesis.
The organizers of the challenge an-notated several pairs with a binary tag, indicatingwhether the hypothesis could be entailed from thetext.
Although these pairs of text are interesting wedecided to discard them from this pilot because thelength of the hypothesis was typically much shorterthan the text, and we did not want to bias the STStask in this respect.
We may, however, explore usingTE pairs for STS in the future.Microsoft Research (MSR) has pioneered the ac-quisition of paraphrases with two manually anno-tated datasets.
The first, called MSR Paraphrase(MSRpar for short) has been widely used to evaluatetext similarity algorithms.
It contains 5801 pairs ofsentences gleaned over a period of 18 months fromthousands of news sources on the web (Dolan etal., 2004).
67% of the pairs were tagged as para-phrases.
The inter annotator agreement is between82% and 84%.
Complete meaning equivalence isnot required, and the annotation guidelines allowedfor some relaxation.
The pairs which were anno-tated as not being paraphrases ranged from com-pletely unrelated semantically, to partially overlap-ping, to those that were almost-but-not-quite seman-tically equivalent.
In this sense our graded annota-tions enrich the dataset with more nuanced tags, aswe will see in the following section.
We followedthe original split of 70% for training and 30% fortesting.
A sample pair from the dataset follows:The Senate Select Committee on Intelligenceis preparing a blistering report on prewarintelligence on Iraq.American intelligence leading up to thewar on Iraq will be criticized by a powerfulUS Congressional committee due to reportsoon, officials said today.In order to construct a dataset which would reflecta uniform distribution of similarity ranges, we sam-pled the MSRpar dataset at certain ranks of stringsimilarity.
We used the implementation readily ac-cessible at CPAN1 of a well-known metric (Ukko-nen, 1985).
We sampled equal numbers of pairsfrom five bands of similarity in the [0.4 .. 0.8] rangeseparately from the paraphrase and non-paraphrasepairs.
We sampled 1500 pairs overall, which we split50% for training and 50% for testing.The second dataset from MSR is the MSR VideoParaphrase Corpus (MSRvid for short).
The authorsshowed brief video segments to Annotators fromAmazon Mechanical Turk (AMT) and were asked1http://search.cpan.org/?mlehmann/String-Similarity-1.04/Similarity.pm386Figure 1: Video and corresponding descriptions fromMSRvidFigure 2: Definition and instructions for annotationto provide a one-sentence description of the main ac-tion or event in the video (Chen and Dolan, 2011).Nearly 120 thousand sentences were collected for2000 videos.
The sentences can be taken to beroughly parallel descriptions, and they included sen-tences for many languages.
Figure 1 shows a videoand corresponding descriptions.The sampling procedure from this dataset is sim-ilar to that for MSRpar.
We construct two bags ofdata to draw samples.
The first includes all possiblepairs for the same video, and the second includespairs taken from different videos.
Note that not allsentences from the same video were equivalent, assome descriptions were contradictory or unrelated.Conversely, not all sentences coming from differentvideos were necessarily unrelated, as many videoswere on similar topics.
We took an equal number ofsamples from each of these two sets, in an attempt toprovide a balanced dataset between equivalent andnon-equivalent pairs.
The sampling was also doneaccording to string similarity, but in four bands in the[0.5 .. 0.8] range, as sentences from the same videohad a usually higher string similarity than those inthe MSRpar dataset.
We sampled 1500 pairs overall,which we split 50% for training and 50% for testing.Given the strong connection between STS sys-tems and Machine Translation evaluation metrics,we also sampled pairs of segments that had beenpart of human evaluation exercises.
Those pairs in-cluded a reference translation and a automatic Ma-chine Translation system submission, as follows:The only instance in which no tax is levied iswhen the supplier is in a non-EU country andthe recipient is in a Member State of the EU.The only case for which no tax is stillperceived ?is an example of supply in theEuropean Community from a third country.We selected pairs from the translation shared taskof the 2007 and 2008 ACL Workshops on StatisticalMachine Translation (WMT) (Callison-Burch et al,2007; Callison-Burch et al, 2008).
For consistency,we only used French to English system submissions.The training data includes all of the Europarl humanranked fr-en system submissions from WMT 2007,with each machine translation being paired with thecorrect reference translation.
This resulted in 729unique training pairs.
The test data is comprised ofall Europarl human evaluated fr-en pairs from WMT2008 that contain 16 white space delimited tokens orless.In addition, we selected two other datasets thatwere used as out-of-domain testing.
One of themcomprised of all the human ranked fr-en systemsubmissions from the WMT 2007 news conversa-tion test set, resulting in 351 unique system refer-ence pairs.2 The second set is radically different asit comprised 750 pairs of glosses from OntoNotes4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum,1998) senses.
The mapping of the senses of both re-sources comprised 110K sense pairs.
The similaritybetween the sense pairs was generated using simpleword overlap.
50% of the pairs were sampled fromsenses which were deemed as equivalent senses, therest from senses which did not map to one another.3 AnnotationIn this first dataset we defined a straightforward lik-ert scale ranging from 5 to 0, but we decided to pro-vide definitions for each value in the scale (cf.
Fig-ure 2).
We first did pilot annotations of 200 pairs se-2At the time of the shared task, this data set contained dupli-cates resulting in 399 sentence pairs.387lected at random from the three main datasets in thetraining set.
We did the annotation, and the pairwisePearson ranged from 84% to 87% among ourselves.The agreement of each annotator with the averagescores of the other was between 87% and 89%.In the future, we would like to explore whetherthe definitions improve the consistency of the tag-ging with respect to a likert scale without defini-tions.
Note also that in the assessment of the qual-ity and evaluation of the systems performances, wejust took the resulting SS scores and their averages.Using the qualitative descriptions for each score inanalysis and evaluation is left for future work.Given the good results of the pilot we decided todeploy the task in Amazon Mechanical Turk (AMT)in order to crowd source the annotation task.
Theturkers were required to have achieved a 95% of ap-proval rating in their previous HITs, and had to passa qualification task which included 6 example pairs.Each HIT included 5 pairs of sentences, and waspaid at 0.20$ each.
We collected 5 annotations perHIT.
In the latest data collection, each HIT required114.9 second for completion.In order to ensure the quality, we also performedpost-hoc validation.
Each HIT contained one pairfrom our pilot.
After the tagging was completedwe checked the correlation of each individual turkerwith our scores, and removed annotations of turkerswhich had low correlations (below 50%).
Given thehigh quality of the annotations among the turkers,we could alternatively use the correlation betweenthe turkers itself to detect poor quality annotators.4 Systems EvaluationGiven two sentences, s1 and s2, an STS systemwould need to return a similarity score.
Participantscan also provide a confidence score indicating theirconfidence level for the result returned for each pair,but this confidence is not used for the main results.The output of the systems performance is evaluatedusing the Pearson product-moment correlation co-efficient between the system scores and the humanscores, as customary in text similarity (Rubensteinand Goodenough, 1965).
We calculated Pearson foreach evaluation dataset separately.In order to have a single Pearson measure for eachsystem we concatenated the gold standard (and sys-tem outputs) for all 5 datasets into a single gold stan-dard file (and single system output).
The first ver-sion of the results were published using this method,but the overall score did not correspond well to theindividual scores in the datasets, and participantsproposed two additional evaluation metrics, both ofthem based on Pearson correlation.
The organizersof the task decided that it was more informative, andon the benefit of the community, to also adopt thoseevaluation metrics, and the idea of having a singlemain evaluation metric was dropped.
This decisionwas not without controversy, but the organizers gavemore priority to openness and inclusiveness and tothe involvement of participants.
The final result ta-ble thus included three evaluation metrics.
For thefuture we plan to analyze the evaluation metrics, in-cluding non-parametric metrics like Spearman.4.1 Evaluation metricsThe first evaluation metric is the Pearson correla-tion for the concatenation of all five datasets, as de-scribed above.
We will use overall Pearson or sim-ply ALL to refer to this measure.The second evaluation metric normalizes the out-put for each dataset separately, using the linear leastsquares method.
We concatenated the system resultsfor five datasets and then computed a single Pear-son correlation.
Given Y = {yi} and X = {xi}(the gold standard scores and the system scores,respectively), we transform the system scores intoX ?
= {x?i} in order to minimize the squared error?i (yi ?
x?i)2.
The linear transformation is given byx?i = xi ?
?1 + ?2, where ?1 and ?2 are found an-alytically.
We refer to this measure as NormalizedPearson or simply ALLnorm.
This metric was sug-gested by one of the participants, Sergio Jimenez.The third evaluation metric is the weighted meanof the Pearson correlations on individual datasets.The Pearson returned for each dataset is weightedaccording to the number of sentence pairs in thatdataset.
Given ri the five Pearson scores foreach dataset, and ni the number of pairs in eachdataset, the weighted mean is given as?i=1..5(ri ?ni)/?i=1..5 ni We refer to this measure as weightedmean of Pearson or Mean for short.4.2 Using confidence scoresParticipants were allowed to include a confidencescore between 1 and 100 for each of their scores.We used weighted Pearson to use those confidence388scores3.
Table 2 includes the list of systems whichprovided a non-uniform confidence.
The resultsshow that some systems were able to improve theircorrelation, showing promise for the usefulness ofconfidence in applications.4.3 The Baseline SystemWe produced scores using a simple word overlapbaseline system.
We tokenized the input sentencessplitting at white spaces, and then represented eachsentence as a vector in the multidimensional to-ken space.
Each dimension had 1 if the token waspresent in the sentence, 0 otherwise.
Similarity ofvectors was computed using cosine similarity.We also run a random baseline several times,yielding close to 0 correlations in all datasets, as ex-pected.
We will refer to the random baseline againin Section 4.5.4.4 ParticipationParticipants could send a maximum of three systemruns.
After downloading the test datasets, they hada maximum of 120 hours to upload the results.
35teams participated, submitting 88 system runs (cf.first column of Table 1).
Due to lack of space wecan?t detail the full names of authors and institutionsthat participated.
The interested reader can use thename of the runs to find the relevant paper in theseproceedings.There were several issues in the submissions.
Thesubmission software did not ensure that the nam-ing conventions were appropriately used, and thiscaused some submissions to be missed, and in twocases the results were wrongly assigned.
Some par-ticipants returned Not-a-Number as a score, and theorganizers had to request whether those where to betaken as a 0 or as a 5.Finally, one team submitted past the 120 hourdeadline and some teams sent missing files after thedeadline.
All those are explicitly marked in Table 1.The teams that included one of the organizers arealso explicitly marked.
We want to stress that inthese teams the organizers did not allow the devel-opers of the system to access any data or informa-tion which was not available for the rest of partic-ipants.
One exception is weiwei, as they generated3http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Calculating_a_weighted_correlationthe 110K OntoNotes-WordNet dataset from whichthe other organizers sampled the surprise data set.After the submission deadline expired, the orga-nizers published the gold standard in the task web-site, in order to ensure a transparent evaluation pro-cess.4.5 ResultsTable 1 shows the results for each run in alphabeticorder.
Each result is followed by the rank of the sys-tem according to the given evaluation measure.
Tothe right, the Pearson score for each dataset is given.In boldface, the three best results in each column.First of all we want to stress that the large majorityof the systems are well above the simple baseline,although the baseline would rank 70 on the Meanmeasure, improving over 19 runs.The correlation for the non-MT datasets were re-ally high: the highest correlation was obtained wasfor MSRvid (0.88 r), followed by MSRpar (0.73 r)and On-WN (0.73 r).
The results for the MT evalu-ation data are lower, (0.57 r) for SMT-eur and (0.61r) for SMT-News.
The simple token overlap base-line, on the contrary, obtained the highest resultsfor On-WN (0.59 r), with (0.43 r) on MSRpar and(0.40 r) on MSRvid.
The results for MT evaluationdata are also reversed, with (0.40 r) for SMT-eur and(0.45 r) for SMT-News.The ALLnorm measure yields the highest corre-lations.
This comes at no surprise, as it involves anormalization which transforms the system outputsusing the gold standard.
In fact, a random base-line which gets Pearson correlations close to 0 in alldatasets would attain Pearson of 0.58914.Although not included in the results table for lackof space, we also performed an analysis of confi-dence intervals.
For instance, the best run accordingto ALL (r = .8239) has a 95% confidence interval of[.8123,.8349] and the second a confidence intervalof [.8016,.8254], meaning that the differences arenot statistically different.5 Tools and resources usedThe organizers asked participants to submit a de-scription file, special emphasis on the tools and re-sources that they used.
Table 3 shows in a simpli-4We run the random baseline 10 times.
The mean is reportedhere.
The standard deviation is 0.0005389Run ALL Rank ALLnrm Rank Mean Rank MSRpar MSRvid SMT-eur On-WN SMT-news00-baseline/task6-baseline .3110 87 .6732 85 .4356 70 .4334 .2996 .4542 .5864 .3908aca08ls/task6-University Of Sheffield-Hybrid .6485 34 .8238 15 .6100 18 .5166 .8187 .4859 .6676 .4280aca08ls/task6-University Of Sheffield-Machine Learning .7241 17 .8169 18 .5750 38 .5166 .8187 .4859 .6390 .2089aca08ls/task6-University Of Sheffield-Vector Space .6054 48 .7946 44 .5943 27 .5460 .7241 .4858 .6676 .4280acaputo/task6-UNIBA-DEPRI .6141 46 .8027 38 .5891 31 .4542 .7673 .5126 .6593 .4636acaputo/task6-UNIBA-LSARI .6221 44 .8079 30 .5728 40 .3886 .7908 .4679 .6826 .4238acaputo/task6-UNIBA-RI .6285 41 .7951 43 .5651 45 .4128 .7612 .4531 .6306 .4887baer/task6-UKP-run1 .8117 4 .8559 4 .6708 4 .6821 .8708 .5118 .6649 .4672baer/task6-UKP-run2 plus postprocessing smt twsi .8239 1 .8579 2 .6773 1 .6830 .8739 .5280 .6641 .4937baer/task6-UKP-run3 plus random .7790 8 .8166 19 .4320 71 .6830 .8739 .5280 -.0620 -.0520croce/task6-UNITOR-1 REGRESSION BEST FEATURES .7474 13 .8292 12 .6316 10 .5695 .8217 .5168 .6591 .4713croce/task6-UNITOR-2 REGRESSION ALL FEATURES .7475 12 .8297 11 .6323 9 .5763 .8217 .5102 .6591 .4713croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS .6289 40 .8150 21 .5939 28 .4686 .8027 .4574 .6591 .4713csjxu/task6-PolyUCOMP-RUN1 .6528 31 .7642 59 .5492 51 .4728 .6593 .4835 .6196 .4290danielcer/stanford fsa?
.6354 38 .7212 70 .4848 66 .3795 .5350 .4377 .6052 .4164danielcer/stanford pdaAll?
.4229 77 .7160 72 .5044 62 .4409 .4698 .4558 .6468 .4769danielcer/stanford rte?
.5589 55 .7807 55 .4674 67 .4374 .8037 .3533 .3077 .3235davide buscaldi/task6-IRIT-pg1 .4280 76 .7379 65 .5009 63 .4295 .6125 .4952 .5387 .3614davide buscaldi/task6-IRIT-pg3 .4813 68 .7569 61 .5202 58 .4171 .6728 .5179 .5526 .3693davide buscaldi/task6-IRIT-wu .4064 81 .7287 69 .4898 65 .4326 .5833 .4856 .5317 .3480demetrios glinos/task6-ATA-BASE .3454 83 .6990 81 .2772 87 .1684 .6256 .2244 .1648 .0988demetrios glinos/task6-ATA-CHNK .4976 64 .7160 73 .3215 86 .2312 .6595 .1504 .2735 .1426demetrios glinos/task6-ATA-STAT .4165 79 .7129 75 .3312 85 .1887 .6482 .2769 .2950 .1336desouza/task6-FBK-run1 .5633 54 .7127 76 .3628 82 .2494 .6117 .1495 .4212 .2439desouza/task6-FBK-run2 .6438 35 .8080 29 .5888 32 .5128 .7807 .3796 .6228 .5474desouza/task6-FBK-run3 .6517 32 .8106 25 .6077 20 .5169 .7773 .4419 .6298 .6085dvilarinoayala/task6-BUAP-RUN-1 .4997 63 .7568 62 .5260 56 .4037 .6532 .4521 .6050 .4537dvilarinoayala/task6-BUAP-RUN-2 -.0260 89 .5933 89 .1016 89 .1109 .0057 .0348 .1788 .1964dvilarinoayala/task6-BUAP-RUN-3 .6630 25 .7474 64 .5105 59 .4018 .6378 .4758 .5691 .4057enrique/task6-UNED-H34measures .4381 75 .7518 63 .5577 48 .5328 .5788 .4785 .6692 .4465enrique/task6-UNED-HallMeasures .2791 88 .6694 87 .4286 72 .3861 .2570 .4086 .6006 .5305enrique/task6-UNED-SP INIST .4680 69 .7625 60 .5615 47 .5166 .6303 .4625 .6442 .4753georgiana dinu/task6-SAARLAND-ALIGN VSSIM .4952 65 .7871 50 .5065 60 .4043 .7718 .2686 .5721 .3505georgiana dinu/task6-SAARLAND-MIXT VSSIM .4548 71 .8258 13 .5662 43 .6310 .8312 .1391 .5966 .3806jan snajder/task6-takelab-simple .8133 3 .8635 1 .6753 2 .7343 .8803 .4771 .6797 .3989jan snajder/task6-takelab-syntax .8138 2 .8569 3 .6601 5 .6985 .8620 .3612 .7049 .4683janardhan/task6-janardhan-UNL matching .3431 84 .6878 84 .3481 83 .1936 .5504 .3755 .2888 .3387jhasneha/task6-Penn-ELReg .6622 27 .8048 34 .5654 44 .5480 .7844 .3513 .6040 .3607jhasneha/task6-Penn-ERReg .6573 28 .8083 28 .5755 37 .5610 .7857 .3568 .6214 .3732jhasneha/task6-Penn-LReg .6497 33 .8043 36 .5699 41 .5460 .7818 .3547 .5969 .4137jotacastillo/task6-SAGAN-RUN1 .5522 57 .7904 47 .5906 29 .5659 .7113 .4739 .6542 .4253jotacastillo/task6-SAGAN-RUN2 .6272 42 .8032 37 .5838 34 .5538 .7706 .4480 .6135 .3894jotacastillo/task6-SAGAN-RUN3 .6311 39 .7943 45 .5649 46 .5394 .7560 .4181 .5904 .3746Konstantin Z/task6-ABBYY-General .5636 53 .8052 33 .5759 36 .4797 .7821 .4576 .6488 .3682M Rios/task6-UOW-LEX PARA .6397 36 .7187 71 .3825 80 .3628 .6426 .3074 .2806 .2082M Rios/task6-UOW-LEX PARA SEM .5981 49 .6955 82 .3473 84 .3529 .5724 .3066 .2643 .1164M Rios/task6-UOW-SEM .5361 59 .6287 88 .2567 88 .2995 .2910 .1611 .2571 .2212mheilman/task6-ETS-PERP .7808 7 .8064 32 .6305 11 .6211 .7210 .4722 .7080 .5149mheilman/task6-ETS-PERPphrases .7834 6 .8089 27 .6399 7 .6397 .7200 .4850 .7124 .5312mheilman/task6-ETS-TERp .4477 73 .7291 68 .5253 57 .5049 .5217 .4748 .6169 .4566nitish aggarwal/task6-aggarwal-run1?
.5777 52 .8158 20 .5466 52 .3675 .8427 .3534 .6030 .4430nitish aggarwal/task6-aggarwal-run2?
.5833 51 .8183 17 .5683 42 .3720 .8330 .4238 .6513 .4499nitish aggarwal/task6-aggarwal-run3 .4911 67 .7696 57 .5377 53 .5320 .6874 .4514 .5827 .2818nmalandrakis/task6-DeepPurple-DeepPurple hierarchical .6228 43 .8100 26 .5979 23 .5984 .7717 .4292 .6480 .3702nmalandrakis/task6-DeepPurple-DeepPurple sigmoid .5540 56 .7997 41 .5558 50 .5960 .7616 .2628 .6016 .3446nmalandrakis/task6-DeepPurple-DeepPurple single .4918 66 .7646 58 .5061 61 .4989 .7092 .4437 .4879 .2441parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach?
.3880 82 .6706 86 .4111 76 .3427 .3549 .4271 .5298 .4034rada/task6-UNT-CombinedRegression .7418 14 .8406 7 .6159 14 .5032 .8695 .4797 .6715 .4033rada/task6-UNT-IndividualDecTree .7677 9 .8389 9 .5947 25 .5693 .8688 .4203 .6491 .2256rada/task6-UNT-IndividualRegression .7846 5 .8440 6 .6162 13 .5353 .8750 .4203 .6715 .4033sbdlrhmn/task6-sbdlrhmn-Run1 .6663 23 .7842 53 .5376 54 .5440 .7335 .3830 .5860 .2445sbdlrhmn/task6-sbdlrhmn-Run2 .4169 78 .7104 77 .4986 64 .4617 .4489 .4719 .6353 .4353sgjimenezv/task6-SOFT-CARDINALITY .7331 15 .8526 5 .6708 3 .6405 .8562 .5152 .7109 .4833sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION .7107 19 .8397 8 .6486 6 .6316 .8237 .4320 .7109 .4833siva/task6-DSS-alignheuristic .5253 60 .7962 42 .6030 21 .5735 .7123 .4781 .6984 .4177siva/task6-DSS-average .5490 58 .8047 35 .5943 26 .5020 .7645 .4875 .6677 .4324siva/task6-DSS-wordsim .5130 61 .7895 49 .5287 55 .3765 .7761 .4161 .5728 .3964skamler /task6-EHU-RUN1v2??
.3129 86 .6935 83 .3889 79 .3605 .5187 .2259 .4098 .3465sokolov/task6-LIMSI-cosprod .6392 37 .7344 67 .3940 78 .3948 .6597 .0143 .4157 .2889sokolov/task6-LIMSI-gradtree .6789 22 .7377 66 .4118 75 .4848 .6636 .0934 .3706 .2455sokolov/task6-LIMSI-sumdiff .6196 45 .7101 78 .4131 74 .4295 .5724 .2842 .3989 .2575spirin2/task6-UIUC-MLNLP-Blend .4592 70 .7800 56 .5782 35 .6523 .6691 .3566 .6117 .4603spirin2/task6-UIUC-MLNLP-CCM .7269 16 .8217 16 .6104 17 .5769 .8203 .4667 .5835 .4945spirin2/task6-UIUC-MLNLP-Puzzle .3216 85 .7857 51 .4376 69 .5635 .8056 .0630 .2774 .2409sranjans/task6-sranjans-1 .6529 30 .8018 39 .6249 12 .6124 .7240 .5581 .6703 .4533sranjans/task6-sranjans-2 .6651 24 .8128 22 .6366 8 .6254 .7538 .5328 .6649 .5036sranjans/task6-sranjans-3 .5045 62 .7846 52 .5905 30 .6167 .7061 .5666 .5664 .3968tiantianzhu7/task6-tiantianzhu7-1 .4533 72 .7134 74 .4192 73 .4184 .5630 .2083 .4822 .2745tiantianzhu7/task6-tiantianzhu7-2 .4157 80 .7099 79 .3960 77 .4260 .5628 .1546 .4552 .1923tiantianzhu7/task6-tiantianzhu7-3 .4446 74 .7097 80 .3740 81 .3411 .5946 .1868 .4029 .1823weiwei/task6-weiwei-run1??
.6946 20 .8303 10 .6081 19 .4106 .8351 .5128 .7273 .4383yeh/task6-SRIUBC-SYSTEM1?
.7513 11 .8017 40 .5997 22 .6084 .7458 .4688 .6315 .3994yeh/task6-SRIUBC-SYSTEM2?
.7562 10 .8111 24 .5858 33 .6050 .7939 .4294 .5871 .3366yeh/task6-SRIUBC-SYSTEM3?
.6876 21 .7812 54 .4668 68 .4791 .7901 .2159 .3843 .2801ygutierrez/task6-UMCC DLSI-MultiLex .6630 26 .7922 46 .5560 49 .6022 .7709 .4435 .4327 .4264ygutierrez/task6-UMCC DLSI-MultiSem .6529 29 .8115 23 .6116 16 .5269 .7756 .4688 .6539 .5470ygutierrez/task6-UMCC DLSI-MultiSemLex .7213 18 .8239 14 .6158 15 .6205 .8104 .4325 .6256 .4340yrkakde/task6-yrkakde-DiceWordnet .5977 50 .7902 48 .5742 39 .5294 .7470 .5531 .5698 .3659yrkakde/task6-yrkakde-JaccNERPenalty .6067 47 .8078 31 .5955 24 .5757 .7765 .4989 .6257 .3468Table 1: The first row corresponds to the baseline.
ALL for overall Pearson, ALLnorm for Pearson after normaliza-tion, and Mean for mean of Pearsons.
We also show the ranks for each measure.
Rightmost columns show Pearson foreach individual dataset.
Note: ?
system submitted past the 120 hour window, ?
post-deadline fixes, ?
team involvingone of the organizers.390Run ALL ALLw MSRpar MSRparw MSRvid MSRvidw SMT-eur SMT-eurw On-WN On-WNw SMT-news SMT-newswdavide buscaldi/task6-IRIT-pg1 .4280 .4946 .4295 .4082 .6125 .6593 .4952 .5273 .5387 .5574 .3614 .4674davide buscaldi/task6-IRIT-pg3 .4813 .5503 .4171 .4033 .6728 .7048 .5179 .5529 .5526 .5950 .3693 .4648davide buscaldi/task6-IRIT-wu .4064 .4682 .4326 .4035 .5833 .6253 .4856 .5138 .5317 .5189 .3480 .4482enrique/task6-UNED-H34measures .4381 .2615 .5328 .4494 .5788 .4913 .4785 .4660 .6692 .6440 .4465 .3632enrique/task6-UNED-HallMeasures .2791 .2002 .3861 .3802 .2570 .2343 .4086 .4212 .6006 .5947 .5305 .4858enrique/task6-UNED-SP INIST .4680 .3754 .5166 .5082 .6303 .5588 .4625 .4801 .6442 .5761 .4753 .4143parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach .3880 .3636 .3427 .3498 .3549 .3353 .4271 .3989 .5298 .4619 .4034 .3228tiantianzhu7/task6-tiantianzhu7-1 .4533 .5442 .4184 .4241 .5630 .5630 .2083 .4220 .4822 .5031 .2745 .3536tiantianzhu7/task6-tiantianzhu7-2 .4157 .5249 .4260 .4340 .5628 .5758 .1546 .4776 .4552 .4926 .1923 .3362tiantianzhu7/task6-tiantianzhu7-3 .4446 .5229 .3411 .3611 .5946 .5899 .1868 .4769 .4029 .4365 .1823 .4014Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongsidetheir scores.fied way the tools and resources used by those par-ticipants that did submit a valid description file.
Inthe last row, the totals show that WordNet was themost used resource, followed by monolingual cor-pora and Wikipedia.
Acronyms, dictionaries, mul-tilingual corpora, stopword lists and tables of para-phrases were also used.Generic NLP tools like lemmatization and PoStagging were widely used, and to a lesser extent,parsing, word sense disambiguation, semantic rolelabeling and time and date resolution (in this or-der).
Knowledge-based and distributional methodsgot used nearly equally, and to a lesser extent, align-ment and/or statistical machine translation software,lexical substitution, string similarity, textual entail-ment and machine translation evaluation software.Machine learning was widely used to combine andtune components.
Several less used tools were alsolisted but were used by three or less systems.The top scoring systems tended to use most ofthe resources and tools listed (UKP, Takelab), withsome notable exceptions like Sgjimenez which wasbased on string similarity.
For a more detailed anal-ysis, the reader is directed to the papers of the par-ticipants in this volume.6 Conclusions and Future WorkThis paper presents the SemEval 2012 pilot eval-uation exercise on Semantic Textual Similarity.
Asimple definition of STS beyond the likert-scale wasset up, and a wealth of annotated data was pro-duced.
The similarity of pairs of sentences wasrated on a 0-5 scale (low to high similarity) by hu-man judges using Amazon Mechanical Turk.
Thedataset includes 1500 sentence pairs from MSRparand MSRvid (each), ca.
1500 pairs from WMT,and 750 sentence pairs from a mapping betweenOntoNotes and WordNet senses.
The correlation be-tween non-expert annotators and annotations fromthe authors is very high, showing the high quality ofthe dataset.
The dataset was split 50% as train andtest, with the exception of the surprise test datasets:a subset of WMT from a different domain and theOntoNotes-WordNet mapping.
All datasets are pub-licly available.5The exercise was very successful in participationand results.
35 teams participated, submitting 88runs.
The best results scored a Pearson correlationover 80%, well beyond a simple lexical baselinewith 31% of correlation.
The metric for evaluationwas not completely satisfactory, and three evalua-tion metrics were finally published.
We discuss theshortcomings of those measures.There are several tasks ahead in order to makeSTS a mature field.
The first is to find a satisfac-tory evaluation metric.
The second is to analyze thedefinition of the task itself, with a thorough analysisof the definitions in the likert scale.We would also like to analyze the relation be-tween the STS scores and the paraphrase judgementsin MSR, as well as the human evaluations in WMT.Finally, we would also like to set up an open frame-work where NLP components and similarity algo-rithms can be combined by the community.
All inall, we would like this dataset to be the focus of thecommunity working on algorithmic approaches forsemantic processing and inference at large.AcknowledgementsWe would like to thank all participants, specially (in al-phabetic order) Yoan Gutierrez, Michael Heilman, Ser-gio Jimenez, Nitin Madnami, Diana McCarthy and Shru-tiranjan Satpathy for their contributions on evaluationmetrics.
Eneko Agirre was partially funded by the5http://www.cs.york.ac.uk/semeval-2012/task6/391AcronymsDictionariesDistributionalthesaurusMonolingualcorporaMultilingualcorporaStopwordsTablesofparaphrasesWikipediaWordNetAlignmentDistributionalsimilarityKBSimilarityLemmatizerLexicalSubstitutionMachineLearningMTevaluationMWENamedEntityrecognitionPOStaggerSemanticRoleLabelingSMTStringsimilaritySyntaxTextualentailmentTimeanddateresolutionWordSenseDisambiguationOtheraca08ls/task6-University Of Sheffield-Hybrid x x x x x x xaca08ls/task6-University Of Sheffield-Machine Learning x x x x x x xaca08ls/task6-University Of Sheffield-Vector Space x x x x xbaer/task6-UKP-run1 x x x x x x x x x x x x x xbaer/task6-UKP-run2 plus postprocessing smt twsi x x x x x x x x x x x x x xbaer/task6-UKP-run3 plus random x x x x x x x x x x x x x xcroce/task6-UNITOR-1 REGRESSION BEST FEATURES x x x x x xcroce/task6-UNITOR-2 REGRESSION ALL FEATURES x x x x x xcroce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS x x x x x xcsjxu/task6-PolyUCOMP-RUN x x x xdanielcer/stanford fsa x x x x x x xdanielcer/stanford pdaAll x x x x x x xdanielcer/stanford rte x x x x x x x xdavide buscaldi/task6-IRIT-pg1 x x x x xdavide buscaldi/task6-IRIT-pg3 x x x x xdavide buscaldi/task6-IRIT-wu x x x x xdemetrios glinos/task6-ATA-BASE x x x x x x xdemetrios glinos/task6-ATA-CHNK x x x x x x xdemetrios glinos/task6-ATA-STAT x x x x x x xdesouza/task6-FBK-run1 x x x x x x x x x x x x xdesouza/task6-FBK-run2 x x x x x x x xdesouza/task6-FBK-run3 x x x x x xdvilarinoayala/task6-BUAP-RUN-1 x xdvilarinoayala/task6-BUAP-RUN-2 xdvilarinoayala/task6-BUAP-RUN-3 x xjan snajder/task6-takelab-simple x x x x x x x x x x x x xjan snajder/task6-takelab-syntax x x x x x x x x xjanardhan/task6-janardhan-UNL matching x x x x x xjotacastillo/task6-SAGAN-RUN1 x x x x x x x xjotacastillo/task6-SAGAN-RUN2 x x x x x x x xjotacastillo/task6-SAGAN-RUN3 x x x x x x x xKonstantin Z/task6-ABBYY-GeneralM Rios/task6-UOW-LEX PARA x x x x x x x xM Rios/task6-UOW-LEX PARA SEM x x x x x x x xM Rios/task6-UOW-SEM x x x x x x xmheilman/task6-ETS-PERP x x x x x x xmheilman/task6-ETS-PERPphrases x x x x x x x x xmheilman/task6-ETS-TERp x x x x x x xparthapakray/task6-JU CSE NLP-Semantic Syntactic Approach x x x x x x x x x xrada/task6-UNT-CombinedRegression x x x x x x x x xrada/task6-UNT-IndividualDecTree x x x x x x x x xrada/task6-UNT-IndividualRegression x x x x x x x x xsgjimenezv/task6-SOFT-CARDINALITY x x xsgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION x x xskamler /task6-EHU-RUN1v2 x x x x xsokolov/task6-LIMSI-cosprod x x x xsokolov/task6-LIMSI-gradtree x x x xsokolov/task6-LIMSI-sumdiff x x x xspirin2/task6-UIUC-MLNLP-Blend x x x x x x x x x x xspirin2/task6-UIUC-MLNLP-CCM x x x x x x x x x x xspirin2/task6-UIUC-MLNLP-Puzzle x x x x x x x x x x xsranjans/task6-sranjans-1 x x x x x x x xsranjans/task6-sranjans-2 x x x x x x x x x x xsranjans/task6-sranjans-3 x x x x x x x x x x xtiantianzhu7/task6-tiantianzhu7-1 x x x xtiantianzhu7/task6-tiantianzhu7-2 x x xtiantianzhu7/task6-tiantianzhu7-3 x x x xweiwei/task6-weiwei-run1 x x x x x xyeh/task6-SRIUBC-SYSTEM1 x x x x x x xyeh/task6-SRIUBC-SYSTEM2 x x x x x x xyeh/task6-SRIUBC-SYSTEM3 x x x x x x xygutierrez/task6-UMCC DLSI-MultiLex x x x x x x xygutierrez/task6-UMCC DLSI-MultiSem x x x x x x xygutierrez/task6-UMCC DLSI-MultiSemLex x x x x x x x xyrkakde/task6-yrkakde-DiceWordnet x x xTotal 8 6 10 33 5 5 9 20 47 7 31 37 49 13 13 4 7 12 43 9 4 13 17 10 5 15 25Table 3: Resources and tools used by the systems that submitted a description file.
Leftmost columns correspond tothe resources, and rightmost to tools, in alphabetic order.392European Communitys Seventh Framework Programme(FP7/2007-2013) under grant agreement no.
270082(PATHS project) and the Ministry of Economy undergrant TIN2009-14715-C04-01 (KNOW2 project).
DanielCer gratefully acknowledges the support of the DefenseAdvanced Research Projects Agency (DARPA) MachineReading Program under Air Force Research Labora-tory (AFRL) prime contract no.
FA8750-09-C-0181 andthe support of the DARPA Broad Operational LanguageTranslation (BOLT) program through IBM.
The STS an-notations were funded by an extension to DARPA GALEsubcontract to IBM # W0853748 4911021461.0 to MonaDiab.
Any opinions, findings, and conclusion or recom-mendations expressed in this material are those of theauthor(s) and do not necessarily reflect the view of theDARPA, AFRL, or the US government.ReferencesChris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, StatMT ?07, pages 136?158.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, StatMT ?08, pages 70?106.David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meetings of the Asso-ciation for Computational Linguistics (ACL).B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In COLING04: Proceedings of the 20th international conferenceon Computational Linguistics, page 350.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL.Michael D. Lee, Brandon Pincombe, and Matthew Welsh.2005.
An empirical evaluation of models of text doc-ument similarity.
In Proceedings of the 27th AnnualConference of the Cognitive Science Society, pages1254?1259, Mahwah, NJ.Y.
Li, D. McLean, Z.
A. Bandar, J. D. O?Shea, andK.
Crockett.
2006.
Sentence similarity based on se-mantic nets and corpus statistics.
IEEE Transactionson Knowledge and Data Engineering, 18(8):1138?1150, August.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.E.
Ukkonen.
1985.
Algorithms for approximate stringmatching.
Information and Contro, 64:110?118.393
