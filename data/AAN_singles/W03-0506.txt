A Study for Documents Summarization based onPersonal AnnotationHaiqin ZhangUniversity of Science andTechnology of Chinaface@mail.ustc.edu.cnZheng Chen Wei-ying MaMicrosoft Research Asiazhengc@microsoft.comwyma@microsoft.comQingsheng CaiUniversity of Science andTechnology of Chinaqscai@ustc.edu.cnAbstractFor one document, current summarization sys-tems produce a uniform version of summaryfor all users.
Personalized summarizations arenecessary in order to represent users?
prefer-ences and interests.
Annotation is getting im-portant for document sharing andcollaborative filtering, which in fact recordusers?
dynamic behaviors compared to tradi-tional steady profiles.
In this paper we intro-duce a new summarization system based onusers?
annotations.
Annotations and their con-texts are extracted to represent features of sen-tences, which are given different weights forrepresentation of the document.
Our systemproduces two versions of summaries for eachdocument: generic summary without consider-ing annotations and annotation-based sum-mary.
Since annotation is a kind of personaldata, annotation-based summary is tailored touser?s interests to some extent.
We show byexperiments that annotations can help a lot inimproving summarization performance com-pared to no annotation consideration.
At thesame time, we make an extensive study on us-ers?
annotating behaviors and annotations dis-tribution, and propose a variety of techniquesto evaluate the relationships between annota-tions and summaries, such as how the numberof annotations affects the summarizing per-formance.
A study about collaborative filter-ing is also made to evaluate thesummarization based on annotations of similarusers.1 IntroductionAs information explodes in the Internet, it?s hard forusers to read through all the published materials poten-tially interesting.
Therefore, it is of great help to presentthem in a condensed way, i.e.
using extracts or abstractsthat generalize the content of an article.
Text summari-zation is the process of distilling the most importantinformation from a source (or sources) to produce anabridged version for a particular user (or users) and task(or tasks) (Mani and Maybury, 1999).
Summary canhelp the user quickly get a general idea about the articleand decide if it deserves more detailed attention.The problem with present summarization systems isthat they produce one uniform summary for a givendocument without considering users?
opinions on it,while different users may have different perspectives onthe same text, thus need a different summary.
A goodsummary should change corresponding to interests andpreferences of its reader.
We refer to the adaptation ofthe summarization process to a particular user as per-sonalized summarization.
We wish to extract the kind ofpersonalized summary in this paper.Annotating is a common behavior while reading,since many users would like to make some remarks orto highlight some important parts of the text.
So wethink that, to some extent, annotations represent users?interests by recording some viewpoints of users on thedocuments, such as which part is important or interest-ing, which is not.
With the rapidly technologies im-provements on tablet-pc, it is possible to record thesebehaviors automatically.
From another point, annota-tions can be seen as a kind of user feedback, but differ-ent from traditional explicit relevance feedback (Saltonand Buckley, 1990) in the sense of that annotation canbe collected automatically without user?s consciousness.Since annotation is a kind of personal data, it is ex-pected to help improve the personalization of present*This work was done when the first author visited Micro-soft Research Asia.document summarizers; also, intention of users can belearned through this interaction process.Since a user may make annotations freely, not all ofthem are useful for representing his real preferences.Therefore it is helpful to find how some of the annota-tions affect on summarization performance, based onthat, we make a study on evaluations about the numberof annotations effective.In spite of summarization based on the user?s anno-tation himself, we want to do more when his annotationon current document is not available.
Suppose he is newto the domain, but other users annotated on the docu-ment, we can resort to annotations of similar users tohelp his summarization.
Study in such situation isthought of as collaboratively filtering which filter theuseful information from similar users to help the sum-marization of current user.
To our knowledge, no workhas been done about such annotation studies.The remainder of the paper is organized as follows.Section 2 introduces some related work in text summa-rization.
Section 3 presents the new annotation basedtext summarization approach, including annotation, con-text and keywords extraction, and summary generation.Section 4 gives the metrics for evaluation of annotatedsummarization.
Section 5 discusses the detailed experi-ments and evaluation of summarization, annotations andcollaborative filtering.
Section 6 concludes the paperwith a simple summary and future work.2 Related workTo summarize, is to reduce complexity of documents,hence, in length, while retaining some of the importantinformation in the original documents.
Titles, keywords,tables-of-contents and abstracts might all be consideredas forms of summary; here, we consider summary as aset of sentences containing some of the essential infor-mation from the original document.A lot of approaches were proposed in text summari-zation, such as word frequency based method (Luhn,1958), cue phrase method (Edmundson, 1969), Position-based methods (Edmundson, 1969; Hovy and Lin, 1997;Teufel and Moens, 1997).At the same time, some machine learning methodswere used to integrate different clues in documents.Given a corpus and its predefined summaries as trainingset, it is to identify the relationships between documentsand their summaries, the sentences which satisfy therules are the ones to be extracted (Kupiec et al, 1995).Other machine learning methods perform sentence clus-tering based on a set of extracted features of sentences,and, choose a representative sentence from each cluster,and combine them into a summary according to theiroriginal order in the text (Nomoto and Matsumoto,2001).Most of the above techniques have the limitationswe have mentioned at the beginning, they failed to sup-ply a personalized summary which reflects the interestsand preferences of different users.There were some work based on annotation(Golovchinsky et al, 1999; Price et al, 1998), but theymainly focus on supplying an authoring tool whichgives instructions on how to do annotation; or annota-tion identification and extraction which is difficult sinceannotation may be done freely and randomly; or annota-tion based query which aims at query expansion basedon annotations.
But rarely people think of using annota-tion for summarization.
In fact, we only found one workabout summarization based on annotation (Nagao andHasida, 1998), but annotations there are defined on acomplex set of GDA (Global Document Annotation)tags, which is an XML-based tag set, and allows ma-chines to automatically infer underlying structures ofdocuments by parsing, and authors of WWW files canannotate their documents by those tags, but they are notstudied further about how to affect the summarization.Since annotations reflect user?s opinions, differentusers may have different annotations; thus summariza-tions based on annotations are tailored to users?
intereststo some extent.
Therefore we will integrate annotationsinto our summarization framework, which is expected tosupply personalized summaries for given users, anddifferent from traditional uniform summary.
Here wemake an assumption that what are annotated is interest-ing or important compared to other parts of document,which is reasonable since this is a common view aboutwhy users make annotations.In this paper, we mainly focus on the kind of annota-tions that are parts of the text in order to avoid complexmanuscript recognition.
Since we mainly consider theeffect of annotations on the performance of summariza-tion, annotations can only make sense when they arethought of as keywords.
However, past experimentsshow that keywords method has a lower performancewhen working with other methods (Edmundson, 1969;Teufel and Moens, 1997), so the main approach we usedhere is based on key words frequency.3 Annotation based summarizationAnnotation is defined as a body of words markedamong the text.
It may be any word, phrase or sentenceswhich readers may feel interesting or important.
Whenwe say ?annotation?, we mean its position and content,that is, where it is located and what texts it contains.Since users may annotate part of the important informa-tion, or the annotations may be incomplete, therefore, inspite of annotations themselves, we also need to con-sider ?context?
as a supplement to what users are inter-ested in.
For a particular annotation, context is definedthe surrounding text of the annotation.Since annotations contain a set of keywords, its sig-nificance can be identified when compared to otherwords of the text.
For a given document, we first extractuser?s annotations and their contexts, and construct anew keywords set together with original keywords inthe text, where annotations and contexts are givenhigher scores than others; Then we weight sentencesaccording to keywords they contained and do summari-zation by selecting high-weighted ones.3.1 Annotations &Context ExtractionA set of sentences is extracted from a given document.And for each annotation, we identify which sentence itis located at, as the context of the annotation.
Thenkeywords are extracted from annotations and their con-texts.
An annotation may span through several sentences,a sentence may include several annotations, and an an-notation may contain several keywords.
For each sen-tence, we simplify the keywords extraction problem asidentifying the annotations it contains.
Annotated sen-tences are defined as those who contain annotations.The keywords occurring in annotations are called anno-tated keywords (F1).
Keywords occurring in annotatedsentences are called context keywords (F2).
Frequen-cies f of repetitive keywords in F1 or F2 are accumu-lated.
It?s obvious that F1 is a subset of F2.
Thus we gettwo keywords vector set:Annotated keywords (F1):)1(1,11,1,11 FsizennifwF ii =?
?>=<Context keywords (F2):)2(2,21,2,22 FsizennifwF ii =?
?>=<3.2 Keywords ExtractionFrom the document, content words are stemmed fromPorter?s algorithm (Porter, 1980).
Content keywords arereferred to words whose frequencies are beyond a cer-tain threshold and not occurring in stopping wordlist.Word frequencies are calculated by tf*idf method (Sal-ton and Buckley, 1988).
After applying word occur-rences statistics to full text; we get the vectorset >< ii fw , .Text keywords are those occurred either in F2 ortheir frequency satisfies a given threshold ?
( ?>if ).Annotated words are considered superiorly whateverfrequency they occur originally, since users may be in-terested in some rare or ?unknown?
keywords in thedocuments, this kind of words should not be excludedbeyond text keywords.Text keywords (F0):)0(0,01,0,00 FsizennifwF ii =?
?>=<It is obvious that both F1 and F2 are subsets of F0.Next in order to apply the emphasis of annotationsand contexts on summarization, combination is per-formed to integrate F0, F1 and F2.
Since keywords indifferent sets may have different influences on summa-rization, some parameters are used to balance their ef-fects respectively.Final keywords (F): F = F0 + ?
F1 + ?
F2;)(,1,, FsizennifwF ii =??>=<?
is annotation weight (?
>=0), ?
is context weight (?>=0).
?=0 means considering no annotations.
?=0 meansconsidering no context.3.3 Sentence ExtractionSentences are weighted according to the keywords itcontains:???
?=SwiSwisiifSorfW 21|S| is the length of a sentence, which means the key-word count it contains.
Sentences are ranked by theirweights, and then top scored sentences are selected asimportant ones and used to compose into a summaryaccording to their original position.4 EvaluationThe problem of evaluating text summarization is a quitedeep one, and some problems remain concerning theappropriate methods and types of evaluation.
There area variety of possible bases for comparison of summari-zation performance, e.g., summary to source, system tomanual summary.
In general, methods for text summa-rization can be classified into two categories (Firminand B, 1998; Mani and Maybury, 1999).
The first isintrinsic evaluation, which judge the quality of thesummarization directly based on analysis of the sum-mary, including user judgments of fluency of the sum-mary, coverage of the ?key/essential ideas?, or similarlyto an ?ideal?
summary which is hard to establish.
Theother is extrinsic evaluation, which judge the quality ofthe summarization based on how it affects on the com-pletion of other tasks, such as question answering andcomprehension tasks.Here we use intrinsic evaluation for our summariza-tion performance.
It is to compare the system summarywith an ideal manual summary.
Since we need to collectannotations for experimented documents, which requirereading through the text, manual summaries can bemade consequently after the reading.The documents dataset to be evaluated are suppliedwith human annotations and summaries, which will bedescribed in detail in the next section.For one annotated document, our annotation basedsummarization (ABS) system produce two versions ofsummaries: generic summary without considering an-notations, and annotated summary considering annota-tions.
For evaluation, we made comparison betweenhuman-made summary and generic summary, and com-parisons between human-made summary and annotatedsummary.
There are a lot of measures to make the com-parisons (Firmin and B, 1998; Mani and Maybury,1999), such as precision, recall, some of which will beused for our evaluation.
Another measure we are inter-ested in is the cosine similarity for two summaries,which is defined on keywords, and reflects the generalsimilarity of two summaries in global distribution.
Forhuman-made summary S0 and summary Si generated byABS, summary similarity is as follows:)()(),(2,2,0,,000????????
?=ijjijSwjiSwjSSwjijoiffffSSSIMitemsretrievedofNumberretrieveditemscorrectofNumberecision =PritemscorrectofNumberretrieveditemscorrectofNumbercall =RePrecision and recall are generally applied to sen-tences; in fact they can be applied to keywords too,which reflects the percentage of keywords correctlyidentified.
Therefore, in spite of summary similarity, ourmeasures for evaluation also include sentences precision,sentences recall, keywords precision and keywords re-call.
For keywords evaluation, a keyword is correct onlyif it occurs in human-made summary.
For sentencesevaluation, a sentence in summary is correct if it has asmany possible keywords as in the corresponding sen-tence in the human-made summary, that is, their similar-ity (calculated same as summary similarity) is beyond acertain threshold.
We use two types of sentences matchin the experiments: one is perfect match, which means asentence in summary is correct only if it occurs in man-ual summary; the other is conditional match, whichmeans most concepts of the two sentences are correct, inthis case the match similarity threshold is less than 1.For a set of annotated documents, average values forthe above five measures are calculated to show the gen-eral performance of the comparison.5 ExperimentsSince our approaches are based on annotated documents,we need to collect users?
annotations for a set of docu-ments.
In the meantime, users are required to supply asummary consisting of several sentences that reflectsthe main ideas of the document.
We supply an annotat-ing tool called ?Annotation?
that can be used to high-light words, phrases, or sentences which users areinterested in, and to select important sentences intosummary.The original data we used is from Yahoo news,which is interesting to most users.
The data is composedof eight categories: business, sports, entertainment,world, science, technology, politics, and oddly, withtotally about 6000 documents.
Documents are preproc-essed and removed graphics and tags before experi-ments.
We hired five students 10 days to annotate thedocuments, each student were supplied 20 percent of thedocuments.
They are allowed to choose articles interest-ing to do the experiments.
Users are told to make anno-tations freely and summaries which reflect the mainideas of the text.
The process of annotating and summa-rizing for one user are independent, that is, they aredone at different time.
At last, we collect totally 1098different annotated documents, each of which consistsof a set of annotations and a human-made summary.The statistics for five persons is presented in table 11,which shows that the average summary length (sen-tences number) is 6.11, and the average annotationsnumber is 11.86.P1 P2 P3 P4 P5DN 192 198 312 199 197ASN 25.93 23.38 30.82 33.89 24.02ASL 6.75 3.05 6.15 7.67 6.97ANN 11.54 8.08 15.18 9.98 12.77Table 1.
A user study.5.1 Summarization EvaluationSince different users may have different annotation style,we separate the experiments for each individual?s data.In experiments, the keyword threshold ?
is set 2, whichis reasonable that most keywords?
frequency is at least 2.The threshold for summary similarity related to sen-tences precision, is 0.3 (which in fact means its squareroot 55% sentences are correct).
The summarizer pro-duce the same number of sentences as are in the corre-sponding manual summary, as in (Kupiec et al, 1995),therefore, precision and recall are the same for summa-ries sentences comparison.First, we make experiments with different annota-tions and context weights.
The results are presented inTable.2.
The first column contains different combina-tions of annotation weight ?
and context weight ?, asdescribed in Section 3.2.
See 2 for the meanings of othercolumns.
It is obvious that context can help to improvethe summarization performance than no context consid-eration, so in our later experiments, we set the contextweight ?=1, and annotation weight ?=1.1 For ease of representation in table 1, ?Pi?
is for differentusers; ?DN?
is document number; ?ASN?
is average sentencesnumber; ?ASL?
is average summary length; ?ANN?
is aver-age annotation number.2 For two compared summaries, ?SS?
means summary similar-ity; ?SP?
means sentences precision for conditional match;?PP?
means sentences precision for perfect match; ?KP?means keywords precision; and ?KR?
means keywords recall.?,?
SS(%) SP(%) PP(%) KP(%) KR(%)1, 0 73.79 60.35 52.88 59.25 75.231, 1 74.26 60.86 53.62 59.86 75.132, 0 74.04 60.67 53.21 59.70 74.902, 1 74.63 61.48 53.95 60.46 75.03Table 2.
Comparison of annotated summary withdifferent parameters.Next, we make formal experiments for genericsummarization and annotation based summarization.Table 3 presents the average performance results forfive users?
data3.
From the above table and figure, wefound that annotation-based summarization is muchbetter than generic summarization.
The improvementsare quite inspiring.
In the case of user P4, the cosinesimilarity is increased by 10.1%; the sentences precisionfor conditional match is increased by 13.57%; precisionfor perfect match is increased by 17.59%; keywordsprecision is increased by 11.6%; keywords recall is in-creased by 13.18%, which shows that annotations canhelp a lot to improve the summarization performance.SS(%) SP(%) PP(%) KP(%) KR(%)G 71.65 57.15 49.55 54.71 71.36P1 A 77.24 64.98 59.13 61.17 81.00G 53.05 43.26 37.80 43.83 56.65P2 A 63.10 52.90 50.98 54.01 67.50G 69.29 53.84 42.40 52.77 66.84P3 A 73.64 59.21 49.73 57.26 73.31G 67.73 49.21 36.81 52.17 63.94P4 A 77.74 62.88 54.40 63.77 77.12G 76.64 60.54 49.09 60.21 73.85P5 A 80.06 65.36 56.18 64.57 77.91Table 3.
Comparison of generic summary and an-notated summary for 5 users?
data.Figure 1 shows the average performance comparisonfor total 1098 documents.
Compared with generic sum-marization, cosine similarity of annotation based sum-marization is increased by 6.47%; sentences precisionfor conditional match is increased by 7.99%; precisionfor perfect match increased by 10.62%; keywords preci-sion increased by 7.14%; keywords recall increased by8.61%.
The most significant is that the improvement ofsentences precision for perfect match is higher than 10%,since perfect matches require two sentences totallymatched, it is very important to gain this point, showingthat in general annotations are able to contribute muchto the performance of summarization.
In Figure 1, ?BA?means ?first Best?
Annotated summary, which will beexplained in the next subsection.3 In Table 3 and later figures, ?G?
means comparison of man-ual summary and generic summary, and ?A?
means compari-son of manual summary and annotated summary.Figure 1.
Performance comparison of genericsummary, annotated summary and ?first best?summary.5.2 Annotation EvaluationIn our last experiments, we found that the average anno-tation number was 11.86, which was much higher thansummary length.
We wonder whether there are somerelations between the number of annotations and sum-marization performance.
Thus we make such annota-tions evaluations to study how the number ofannotations affects on summarization performance.The first experiment is to find the best summary byselecting the first k (k?n, n is total number of annota-tions) annotations, that is ?first Best Annotated sum-mary?.
In fact, from figure 1 we can see that when usingall of the annotations, the performance falls about in themiddle of generic summary and first Best Annotatedsummary (labeled by ?BA?).
However we found that, insome annotated documents, some of annotations arebeyond the scope of the manual summary.
This meansthat some of them are noisy to summarization; using allof them cannot reach the best performance; there musthave a changing process of performance as more anno-tations are considered, which confirms us to explore therelationship between the annotation number and thesummarization performance.So the next experiment is to observe how the aver-age summarization performance evolves as we selectany of the k annotations.
That is, for any annotationnumber k?n, to average the summarization performanceby enumerating all possible annotations combinations.Figure 2 presents such a plot for a document?sports_78.html?, which indicates how the number ofannotations affects summarization performance.
It hastotally 15 annotations, ?0?
stands for generic summaryperformance.
When considering only one annotation,the performance drops a bit down (We found in thisdocument some of the single annotations are far awayfrom corresponding summary), but as more annotationsconsidered, the performance begins to increase slowlyand reaches the best at annotation number 12, then againbegins to drop.
For other documents, we find similar0102030405060708090SS SP PP KP KRPerformance(%)G A BAsituations that at beginning the performance increasesalong the number of annotations, but after it reaches acertain degree, the performance will fluctuate and some-times drop slightly down.
These are all true for our 5evaluation measures, which are consistent and reason-able since too many annotations will introduce somedegree of noise, which would bias the summarizationprocess.
We conclude from this evolving process thatnot all of the annotations are valuable for summarization.0204060800 2 4 6 8 10 12 14Annotation numberPerformance(%)SS SP PPKP KRFigure 2.
Performance along with the annotationsnumber.In figure 2, the best and worst performance point canbe identified, for example, we get the best point at anno-tation number 12 and the worst point at annotationnumber 1.
For a subset of 10 documents, summary simi-larity comparisons with generic, best, worst, and allannotations-based summarization are shown in figure 3.0204060801001201 3 5 7 9 11Documens IDSimilarity(%)G B W AFigure 3.
Comparisons of summaries with generic,best, worst, and all annotations.Different documents have different annotations,which have different influences on summarization.
Inmost cases, best summaries are better than all annota-tions-based summaries, which are better than genericand worst summaries.
There is an exception in Figure 3at Document 5, we found in this document some of theannotations are irrelevant to the summary.
For example,in this document, percentage of summary annotatedsentences in annotated sentences is 28.57%; and per-centage of summary annotated keywords in summarykeywords is only 17.19%.
While for Document 6, thecorresponding values are 50.00% and 32.73%.
Thisindicates that the user?s interests drifted as he read thedocument.When annotations are consistent with users?
summa-ries, they help to improve the personalization of summa-rization, the more the better, otherwise, when theannotations are far from users?
summaries, the influenceof annotations may be negative, for example some an-notations subset make the performance worse.
But gen-erally the former has a much larger proportion than thelatter.
Along the documents in figure 3, the main trendis that annotation-based summaries are better thansummaries with no annotations consideration; the aver-age improvement for 12 documents is 13.49%.5.3 Collaborative FilteringAnother part of our experiments is about collaborativefiltering, which identifies how much effect one user?sannotation is on others?
summaries.
To do that, we addi-tionally invited the previous 5 users to annotate andsummarize 100 common documents.
After removingsome bad data, we got totally 91 common documents atlast.
For each user?s data, we will find whether it ishelpful when his annotations are applied on other users?summarizations.
Figure 4 presents the contributions ofuser P3?s annotation on the total five persons?
summari-zation.
Intuitively, P3?s annotations are most helpful toP3?s summaries; they also have some contributions toP2, P4 and P1?s according to importance, but makenegative effects on P5?s summarization.
This indicatesthat P3?s interest is most close to P2?s, but far from P5?s.This is possibly understandable since different usershave different preferences for a document, thus theirannotations style may have significant variances.Figure 4.
Contributions of P3?s annotations on all?ssummarization.For validation, we also make a reverse plot in figure5 which presents the contributions of all?s annotationson P3?s summarization.
We got similar results from this010203040506070SS SP PP KP KRPerformance(%)G P1 P2 P3 P4 P5 Avgfigure that P2?s annotations contributes most to P3?ssummarization among other four persons.
In fact wefound most of the annotations of P2 and P3 are similarfor most documents.
For example, in document ?busi-ness_642.txt?
whose title is ?Stocks Sag; EDS WarningsWhacks IBM Lower?, we found that, P2 and both P3annotated ?drooped, Computer makers, Homebuilders,Federal Reserve?, which occupies 27% of P2?s annota-tions and 36% of P3?s annotations.
While in anotherdocument, the corresponding values are 36% and 50%.In table 4, we calculate the annotations cosine similarityfor any two users, and found that averaged 37% of P3and P2?s annotations are consistent, but 29% for P3 andP5?s.
This confirms that P2 and P3 should fall into oneinterest group, while P5 belongs to another.Figure 5.
Contributions of all?s annotations on P3?ssummarization.P1(%) P2(%) P3(%) P4(%) P5(%)P1 100.00 17.90 13.14 10.52 19.83P2 17.90 100.00 36.87 32.92 29.23P3 13.14 36.87 100.00 36.02 25.28P4 10.52 32.92 36.02 100.00 19.40P5 19.83 29.23 25.28 19.40 100.00Table 4.
Users annotation similarities.6 ConclusionThis paper introduces a new document summarizationapproach based on users?
annotations.
Annotations areused to help generate personalized summary for a par-ticular document.
Contexts are also considered to makeup the incompleteness of annotations.
Two types ofsummaries are produced for each document: genericsummary without considering annotations and annota-tion-based summary.
Performance comparisons aremade between these two summaries.
We show by ex-periments that annotations are quite useful in improvingthe personalization of summarization compared to noannotations consideration.We made extensive evaluations on the relationshipsbetween annotations and summaries, and concluded thatannotation selection is necessary in order to get a bestsummarization.
We will try to identify how to choose anappropriate subset of annotations that make summariza-tion best, which is challenging due to the variety of us-ers?
interests in different documents.We also did collaborative filtering to find whetherone user?s annotation is helpful for other?s summariza-tion.
The answer is positive that summarization per-formance can be improved based on similar users?annotations.
For the next step, we think that ?similarusers?
needs more precise definition to improve theperformance of collaborative filtering.As an extension of collaborative filtering, morework will be done for multi-documents summarizationbased on annotations of similar ones.
They will helpuser to get a global personal view for a set of documents.ReferencesEdmundson, H.P.
1969.
New methods in automatic abstracting.Journal of the ACM, 16(2):264-285.Eduard Hovy and Chin-Yew Lin.
1997.
Automated text sum-marization in SUMMARIST.
In Proceedings of ACLWorkshop on Intelligent Scalable Text Summarization,Madrid, Spain, July 1997, 18-24.Firmin Hand, T. and B. Sundheim.
1998.
TIPSTERSUMMAC Summarization Evaluation.
Proceedings of theTIPSTER Text Phase III Workshop, Washington.Gene Golovchinsky, Morgan N. Price, and Bill N. Schilit.1999.
From reading to retrieval: Freeform ink annotationsas queries.
In Proceedings of SIGIR'99, ACM, New York,August 1999, 19-25.Gerard Salton and Chris Buckley.
1988.
Term-weighting ap-proaches in automatic text retrieval.
Information Process-ing and Management, 24(5):513-523.Gerard Salton and Chris Buckley.
1990.
Improving retrievalperformance by relevance feedback.
Journal of the Ameri-can Society for Information Science, 41(4):288-297.Julian M. Kupiec, Jan Pedersen, and Francine Chen.
1995.
Atrainable document summarizer.
In Proceedings of the 18thAnnual International ACM SIGIR Conference on Researchand Development in Information Retrieval, Washington,July 1995, 68-73.Katashi Nagao and Koiti Hasida.
1998.
Automatic text sum-marization based on the Global Document Annotation.
InProceedings of COLING-ACL, San Francisco.Luhn, H. P. 1958.
The automatic creation of literature ab-stracts.
IBM journal of Research and Development,2(2):159-165.Mani, Inderjeet and Mark T. Maybury.
1999.
Advances inAutomatic Text Summarization.
MIT Press, Cambridge,MA.010203040506070SS SP PP KP KRPerformance(%)G P1 P2 P3 P4 P5 AvgMorgan N. Price, Bill N. Schilit and Gene Golovchinsky.
1998.XLibris: The active reading machine.
In Proceedings ofCHI '98 Human Factors in Computing Systems, Los Ange-les, volume 2 of Demonstrations: Dynamic Documents, 22-23.Porter, M.F.
1980.
An algorithm for suffix stripping.
Program,14(3):130-137.Simone Teufel and Marc Moens.
1997.
Sentence extraction asa classification task.
In ACL/EACL-97 Workshop on Intel-ligent Scal-able Text Summarization, Madrid, Spain, July1997, 58-65.Tadashi Nomoto and Yutaka Shinagawa.
2001.
A new ap-proach to unsupervised text summarization.
In Proceedingsof SIGIR 2001, September 2001, New Orleans, LA, 26-34.
