Const ruct ion  of  a Hierarchica l  \ ]?ans la t ion  MemoryS.
Vogel, H. NeyLehrs tuh l  flit In tbrmat ik  VI, Computer  Science Depar tment1{1~7~111 Aach(',n Univers i ty  of T(~chnology1)-52056 Aachen,  Gerinm~yElnaih voge l@in format ik ,  rwth -aachen,  deAbst rac t_q}:anslation memories are t)ronfising devi('es forartt;omati(- translation.
Their main weakuess,however, is poor coverage, on llllSeell {;ex|;.
\]illthis t)at)er, l;he use of a hierarchical ;ransla-tion memory, (:onsisting of a ('as(:ade of finitesi;~d;e transducers, is t)rot)os(;d. A mmfl)er oftr~nsdu(:e, rs is al)l)\]ied to (;onverl; s(;ni;enee 1)airsfl:om a t)ilingual cortms into translat ion pat-terns, which are then used as a translat ion me, m-ory.
Pr(;l iminary results on the (\]erman EnglishV ERBMOIIIL ('orl)us a,re given.1 In t roduct ionIn reeenl; years, exa,int)le-1)ased t;l"ansl~l;i<)l~ hasbeen 1)rol)osed as an efli<:ient ~n(;t;llo<l for auto-m~d;i(: translation (Sal;o and Nag;to, 1990; Ki-tan(), 1993; Brown, \]99(i).
'lli'anslations aresl;()l;ed il l a t ra l l s la i ; i ( ) l l  l l le.
l l lory tloll(t llso, d t;o coi1-SI;YllCI; trauslations for new sealten(:e.s.
In its sin>1)lest version, examl)le-1)ased translat ion boilsdown to l l S i l lg  a (tat;fl)ase of SOllrce sell(;el l(;eswith their l;rmlslations.
For many translat;iontasks, esl)eeially in coml)ul;er assisl;cd |;ransla-tion, this at)l)roa(:h works with greal; success.For flflly aul;onlat;i(" l;ranslal;ion the main t ) ro t )  -\ ]em is t )oor  COVel'a~e oi1 l leW data.
To overco l I lethis weakness, it hierar(:hi(:al translation llleln-ory is prot)osed.
Al)plying a cascade of tinitesti%te |;ra, i ls(hl( 'ers~ a~ SOllrce Se l l te l l ce  is {;ralis-laW, d into the tin:get language.2 The  Transducers2.1 Overv iewA translat;ion lnemory is siml)ly a eolle(:|;ionof source-l;arge3; string i)airs.
As a tirst Stel) ~these translat ion examt)les (:all be (:onvertedinl;o translat ion 1)atl;(.q:ns t)y lilt;reducing cate-gory \]abels, e.g.
tbr prol)er nmnes or numbers.3.
'0 make the translat ion patterns even more use-ful, not only single words but comph;x phrasescan be replace.d by category labels.
Whichphrases t;o select for categorization depends onthe aplflication, l,br example, the corpus lls0.dfor this si;udy coal;alas many time and date ex-pressions.
Therefore, a specialized |;ransduce, rwas constructed to recognize and translal;e suche, xl)ressions.Each transducer is a se~ of quadrut)les of thetbrm:label # source pal;t;ern # l;arget; t)atl;ern # score,Som'ce l)al;terns and target patterns may con-tain category labels.
We call su(:h l)atterns~(:ompomldL ~.l.~:ansdueea's working only on theword level are (:ailed 'simple'.
\]if a la:ans-dll(:e,r coal;alas recursive p:tl;terns, e.g.
\])ATE #\])NPE lind \])ATE # I)AS?I'~ and \])ATI'3 # -3.0, it;has |;o be.
apl)lied re.cursively t;o t;he input;.The scores a,t|;a(:hed to the translation t)at -terns can be viewed ns translat ion scores.
Theyare llse, d to bi~ts towards 1;he selection of lollg(;rpart;eras and towards lliore likely translationsin I;hose cases where several targol; patterns areassociated with ()lie SOl l rce  t)a,l;i;ern.
'.l'he transducers can be applied in 1)oth di-rections, i.e.
for a given language pair, eachlanguage can be viewed as source language.Thcrel)y, bil ingual abeling is possilfle.
This canl)e applied to convert a bilingual corlms into aselection of translat ion l)atterns which are.
for-mulated in terms of words and ('ategory lal)els.2.2 Const ruct ion  o f  the  TransducersThe transducers should t)e selected in such away am to minimize l;he lle, ed tbr recursive ap-t)li(:al;ion in order l;o lint)rove efficiency.
There-tbre, |;11(' l)atl;erns to search tbr are l)artitioned toforln a ('as(:ade of t;ransducers.
Sonic trans(luc-ers analys(,' l)arts of the senten(:e and rel)la(:e it1131by a category label, which is then used at a laterstep by another transducer.
The labeling of thedays of the week or the names of the months isa prerequisite to apply more complex patternsfor date expressions.
The transducers currentlyused are listed in Table 1.
'Fable 1: List of transducers.1.
names (persons, towns, places, events, etc)2. spelling (e.g.
'D A double L')3. numbers (ordinal, cardinal, fractions, etc)4. time and date expressions5.
parts of speech (tbr certain word classes)6. grammar (noun phrases, verb phrases)Some transducers are general in scope, e.g.the transducers for numbers, part of speech tagsand grammar.
Others are costumized towardsthe domain tbr which the translation system isdeveloped.
In tile VERBMOBIL corpus, which isused for the experiments, time and date expres-sions are very prominent.
To recognize theseexpressions, a small grammar has been devel-oped and coded as finite state transducer.
Ac-tually, two transducers are used.
On the firstlevel, words are replaced by labels, like DAY-OFWEEK = { Montag, Dienstag, ...}.
On thesecond level, these labels are used to t'orm com-plex time and date expressions.
This secondtransducer works recursively, as simpler expres-sions are used to build more complex expres-sions.Finally, a small grammar based on POS (partof speech) tags has been crafted mamlally.
Thepurpose of this grammar is to recognize simplenoun phrases.
Extensions to handle the differ-ent word ordering in the verb phrases arc underdevelopment.2.3 ScoringThe scores attached to the translation patternscan be viewed as a kind of translation scores.In the current implementation a rather crudeheuristic together with some manual tuning inthe grammar transducer is applied.
The ideais to give preference to longer translation pat-terns as they take more context into accountand encode word reordering in an explicit man-ner.
Thus, fbr simple and compound translationpatterns the score is exponential to the lengthof the source pattern.
Tile scores are negativeby convention: not translating a word gives zerocost, translating it gives a benefit, i.e.
negativecosts.
In future, scoring will be refined by usingcorpus statistics to assign probabilities to thetranslation patterns.2.4 Bilingual LabelingThe sentence pairs ill the bilingual training cor-pus can be segmented into shorter segmentswith the help of an alignment progrmn (Och etal., 1999).
This collection of segments could beused directly as a translation memory.
However,to improve the coverage on unseen data, thesesegnmnts are labeled.
Applying the transducersas given in Table 1 transfbrms these segmentsinto compound t)hrases.The procedure is as follows:1.
For each transducer taken from the com-plete cascade - as given in Table 1 ap-lilY the transducer to both, the source andtlm target sentences of the bilingual train-ing cortms.2.
Find those sentence pairs which containequal number and types of category labelstbr both sentences.3.
For sentence pairs which do not match inmmflmr and type of the category labelskeep the original sentence pair.Table 2 shows examples of some translationpatterns which resulted flom bilingual abeling.3 Applying the TransducersThe working of the transducers i best describedas tile construction of a translation graph.
Thatis to say, the sentence to be translated is viewedas a graph which is traversed fi'om left to right.For each matching source pattern, as encodedin the transducers, a new edge is added to thegraph.
The edge is labeled with the category la-bel of the translation pattern.
The translationand the translation score are attached to theedge.
In this way a translation graph is con-structed.
In those cases, where a source patternhas several translations, one edge tbr each trans-lation is added to the graph.Tim left right search on the graph is orga-nized in such a way that all paths are traversed1132Table 2: Coml)ound translation t)atterns (CTP).CTP ~ DATE_DAY ginge es wiedcrCTP ~ SURNAME am A1)i)~ratCTP ~ NP dauert DATECTP @ nehmen PPER NP DATE@ DATE_DAY it is possible again :~ -4.6~/: this is SURNAME st)caking @ -3.3NP takes DATE :~ -3.3let PPER take NP DATE @ -4.6in parallel and tile patterns l;ored in the trans-ducer are matched synchronously.
For each~lo(te n and each edge e leading to n, all patternsin tile transducer starting with the label of e arcattached to n. This gives a mmlber of hypothe-ses describing partially matching patterns.
Al-ready started hypotheses are expanded with tilelal)el of the edge running ti'om the l)revious nodeto the current node.
This procedure is shown inl~'igul'e 1.
For a selection of t;rmmlation patternsfrom the siml)le , word-1)ased translation mem-ory the hyt)otheses tbr 1)artially matching pat-terns generated uring the left--right traversalare shown as well as the resulting new edges.The result of applying all transducers is agraph where each path is a (partial) transla-tion of the source sentence.
The 1)ath with thebest overall score is used to construct the fi-nal translation.
For good result;s, not; only thescores from t;he transducers houl(l 1)e used inselecting the best t)ath, but a language modelof the target language should l)e inchlde(l.1 llIIl # al, on, at the9 Montag# Monday17 waere  es so  moeglich # would that  be possilflc18 wic ist cs bel lhncn # how about you19 wie waerc es # how al)out20 wie wacrc cs denn # how about21 wie waere es denn am Montag # how about Monday22 wie wacrc es am Montag # Imw about MondayFigure 1: Ext)ansion of Pattern Hypotheses3.1 Error Tolerant MatchTo improve tile coverage on unseen test data,it may be avantageous to allow tbr approxima-tivc matching.
The idea is, to apply longer seg-ments tbr syntactically better translations with-out loosing to much as far as tile content of thesentences i concerned.We us(; weighted edit distance, i.e.
each er-ror (insertion, deletion, substitution) is assici-ated with an individual score.
Thereby, thedeletion or insertion of typical filler words canbe allowed, whereas the deletion or insertion ofcontent words is avoided.3.2 Translation on Word Lat t i cesThe approach described so far can be used fora tight integration of speech recognition andtranslation.
Speech recognition systems typi-cally 1)ro(luce wor(l lattices which encode themost likely word sequences in an e.flicient lllall-net.
A direct translation on the lattice has,compared to transforming the lattice, into an n-best list;, translating each word sequence, mMselecting the overall best translation, a nulnberof advantages:?
all the paths can be covered, whereas inan n-best approach typically only a smallfraction of tile paths is considered;?
partial translation hypotheses are reused;?
acoustic scores can be taken into accountwhen calculating an overall score for eachtranslation hypothesis.4 Exper iments  and Resu l tsIn this section, we will report on first expert-ments and results obtained with the cascadedtransducer approach.
Experiments were per-tbrmed on the VERBMOBIL corpus.
This cor-pus consists of spontaneously spoken dialogs inthe appointment scheduling domain (Wahlster,1993).
The vocabulary comprises 7335 German1133words and 4382 English words.
A test corI)usof 147 sentences with a total of 1 968 words wasused to test the coverage of tile transducers andto run preliminary translation experiments.In Table 3 the sizes of the transducers aregiven.Table 3: Number of translation t)atterns of tiletransducers.Transducer PatternsNalneSpellNumberDatePOS Tags~ralnnlar44260342334671.41244.1 CoverageIn a first series of experiments, the coverageof the cascaded transducers was tested.
TILesentences pairs Dora the training corpus weresegmented into shorter segments.
This resultedin 43609 bilingual phrases running from 1 wordup to 82 words in length.
The longest phraseswere discarded as it is very unlikely that theywill match other sentences.
Thus, for the ex-periments only 40000 sentence pairs were used,the longest sentences containing sixteen sourcewords.Starting fi'om those simple phrases, succes-sively more transducers were applied 1lt) to thefllll cascade.
In Table 4 the coverage for eachlevel is shown.
As expected, the coverage in-creases and nearly flfll coverage on the testsentences is reached.
In tile final step, thePOS transducer and the grammer transducerare both applied.The first cohnnn shows which transducershave been applied.
In each step, one additionaltransducer is applied tbr bilingual labeling andtbr translation.
Bilingual labeling reduces thenumber of distinct patterns in the translationmemory, whereas the immber of compound pat-terns increases.
The last column shows thenumber of words in the test sentences not cov-ered by the patterns ill tile translation mmory.As can be seen, the coverage increases whicheach step.
The large improvement in the finalTable 4: Efl'ect of selected transducers oi1 cov-erage on test corpus.%'ansdncers Patterns Coln- notpound coveredNOlleName+ Spell+ Number+ Date+ Gramnlar4000039624395083866936118355191.25914681118114684156822732542492382159step results froln applying tile POS-tag trans-ducer whidl coveres a large part of the vocabu-lary.4.2 Translat ionFirst experiments have been performed to testtile approach tbr translation.
So far, no lan-guage model tbr the target language is appliedto score the different ranslations.For the sentence 'Samstag und Februar sindgut, aber der siebzehnte ware besser' the bestt)ath through the resnlting translation graphgives a structure as shown in Figure 2.
IlL Ta-ble 5, some translation examples for test sen-tences not seen ill the training corpus arc given.Table 5: Three translations generated t'rom thehierarchical translation memory.Ich werde lnit dem Fhlgzeug kolnnmn.I will come with the plane.Ja, wunderbar.
Machen wir das so, unddann treflbn wir uns daim ill Hamburg.Vielen Dank und auf WiederhSren.Well, excellent.
Shall we fix this, andthen we will meet then in Hanfl)urg.Thank you very much goodbye.Das kann ich nicht einrichten.
Ich habeeine Chance ab dreimldzwanzigstenOktober.
Ist es da bei Ihnen m6glich?It can I not arrange.
I havea chance froln twenty-third ofOctober.
Is it as for you possible?1134I C_PHRASEthe fourth would be better-7.4~DATE \]Saturday and February-4,2DATE DATESaturday February-0.6 -0.6DAYWEEKSaturday-0.5Samstag IMONTHFebruary-0.5Feb  ruar  4I DATEthe fourth-4.1~ DATEDAY the fourth-4.0are   ood but-2 1 -0 1~a_ere ~ AFigure 2: \[\[~'m~slation example5 Sun'nnary  and  conc lus ionsIn this t)npcr a translation at)pronch 1)asexl oncascaded tin|re state l;ra,nsducers has l)een pre-sen|ext.
A mm~l\] mm~l)er of simple l;rmlsdut'-(;rs is handcrafted and then used to convert; nbilingual cortms in|;o a translation memory con-sisting of som:(:c l)al;tcrn target; i)a,l;l;(;rn p~tirs,which inchuh; category lnlmls.
Trmlslni;ion isthen lmrformcd by applying l;he comtflel;e cas-ca(le of l;rans(luce.rs.First (;xl)e.rim(mts ha,v(; shown l;lm \])ot,cnl;i;Jof this ai)l)ro~u:h for m~tchine l;ransla,tion.
Goodcoverag(~ on mlse,(m test data ('ould 1)e ol)l;aine(l.The.
main ditficulty in this nt)l)roach is to (te-l|he a (:onsistenl; scoring s('heme thr the (litt'e,r-ent transdu(:('rs.
Especially, ~ good l)M~m('e t)(;-tween the grammm: and th(', word-t)as(;d |,ransb>lion m('mory is n(',c(;ssary.
'Phis will t)e th(' mainfocus for futur(', work.As Mrea(ty mentioned, ;~ l~tngmtge modal forth(; tnrget l~mguag(; has to bc integrated intot;h(, scoring of the translation hyl)othes(,s. Fi-mflly, the l, rmmdu('er based al)t)roadl to transla-tion will 1)e tested on word lattice.s as i)rodu(:edby spee,(:h recognition systeans.Acknowledgement .
This work was partlySUl)t)orted l)y the German Fede.ral Ministry ofE(tuc~ttion, S(:ie.n(:e, ll.es(;m:ch mM 3b.
(:hnoh)gyunder the.
Contract Nulnl)er 01 IV 701 Td(vl m vonu,).ReferencesR.
1).
Brown.
i\[996.
Exmut)lc-1)ase, d machinetranslation in the pangloss system, l"rocc, cd-ings of the 16th, international Co~@rencc, onComputational Linguistics, 169-174, Copcn-tm,ge, n, l)emnark, August.It.
l(itmJo.
1993.
A COml)rehensive mM prn(>ti(-M model of memory-ha,seal machine trmls-la.tion, l~mcccdi,ng.~ of the 13th, hzl, c'r,natio'nalJoint Co'nfere, nce, o'n Art{/icial bl, tclligc,'n, ce,vohmm 2.
1276 1282.
Morgmt Ka.ufmmm.F..\].
Och, C. Tillmmm, mM H. Ney.
1999. lm-prove, d aligmnent models for statistical ma-chilw, I;ranslation.
Procceding,s of the JointSIGDAT Co~@rcncc on Empirical Meth, odsin Na, t,wral Language PTwccs.sin9 and VeryLarge, Corpora, 20 28, University of Mm:y~land, College Park, MD, USA, June.S.
Sato and M. Nagao.
1990.
Towmd memory-based tnmslation.
P'rocc, edings of the 13thInternational Cm@rcnce on ComputationalLingui,~tics, vol.
3, 24:7 ~252, Hclsinki, Fin-land.W.
Wahlster.
1993.
Vert)mobil: %'anslation oft'a(:c-to-fac(; dialogs.
Proceedings of th, e MTSummit IV, 1.27 135, Kol)e, Jal)mL1135
