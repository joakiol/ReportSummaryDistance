Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 36?42,Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational LinguisticsLanguage Technology for Agile Social Media ScienceSimon WibberleyDepartment of InformaticsUniversity of Sussexsw206@susx.ac.ukJeremy ReffinDepartment of InformaticsUniversity of Sussexj.p.reffin@susx.ac.ukDavid WeirDepartment of InformaticsUniversity of Sussexdavidw@susx.ac.ukAbstractWe present an extension of the DUALISTtool that enables social scientists to engagedirectly with large Twitter datasets.
Ourapproach supports collaborative construc-tion of classifiers and associated gold stan-dard data sets.
The tool can be used tobuild classifier cascades that decomposestweet streams, and provide analysis of tar-geted conversations.
A central concern isto provide an environment in which socialscience researchers can rapidly develop aninformed sense of what the datasets looklike.
The intent is that they develop, notonly an informed view as to how the datacould be fruitfully analysed, but also howfeasible it is to analyse it in that way.1 IntroductionIn recent years, automatic social media analysis(SMA) has emerged, not only as a major focusof attention within the academic NLP community,but as an area that is of increasing interest to a va-riety of business and public sectors organisations.Among the many social media platforms in use to-day, the one that has received the most attention isTwitter, the second most popular social media net-work in the world with over 400 million tweetssent each day.
The popularity of Twitter as a tar-get of SMA derives from both the public natureof tweets, and the availability of the Twitter APIwhich provides a variety of flexible methods forscraping tweets from the live Twitter stream.A plethora of social media monitoring plat-forms now exist, that are mostly concerned withproviding product marketing oriented services1.For example, brand monitoring services seek toprovide companies with an understanding of what1http://wiki.kenburbary.com/social-media-monitoring-wiki lists 230 Social Media Monitoring Solutionsis being said about their brands and products,with language processing technology being usedto capture relevant comments or conversations andapply some form of sentiment analysis (SA), in or-der to derive insights into what is being said.
Thispaper forms part of a growing body of work thatis attempting to broaden the scope of SMA be-yond the realm of product marketing, and into ar-eas of concern to social scientists (Carvalho et al2011; Diakopoulos and Shamma, 2010; Gonzalez-Bailon et al 2010; Marchetti-Bowick and Cham-bers, 2012; O?Connor et al 2010; Tumasjan et al2011; Tumasjan et al 2010).Social media presents an enormous opportunityfor the social science research community, consti-tuting a window into what large numbers of peopleare talking.
There are, however, significant obsta-cles facing social scientists interested in makinguse of big social media datasets, and it is importantfor the NLP research community to gain a betterunderstanding as to how language technology cansupport such explorations.A key requirement, and the focus of this paper,is agility: the social scientist needs to be able toengage with the data in a way that supports an it-erative process, homing in on a way of analysingthe data that is likely to produce valuable insight.Given what is typically a rather broad topic as astarting point, there is a need to see what issues re-lated to that topic are being discussed and to whatextent.
It can be important to get a feeling for thekind of language being used in these discussions,and there is a need to rapidly assess the accuracyof the automated decision making.
There is littlevalue in developing an analysis of the data on anapproach that relies on the technology making de-cisions that are so nuanced that the method beingused is highly unreliable.
As the answers to thesequestions are being exposed, insights emerge fromthe data, and it becomes possible for the social sci-entist to progressively refine the topics that are be-36ing targetted, and ultimately create a way of au-tomatically analysing the data that is likely to beinsightful.Supporting this agile methodology presents se-vere challenges from an NLP perspective, wherethe predominant approaches use classifiers thatinvolve supervised machine learning.
The needfor substantial quantities of training data, andthe detrimental impact on performance that re-sults when applying them to ?out-of-domain?
datamean that exisiting approaches cannot support theagility that is so important when social scientistsengage with big social media datasets.We describe a tool being developed in collab-oration with a team of social scientists to supportthis agile methodology.
We have built a frame-work based on DUALIST, an active learning toolfor building classifiers (Settles, 2011; Settles andZhu, 2012).
This framework provides a way fora group of social scientists to collaboratively en-gage with a stream of tweets, with a goal of con-structing a chain (or cascade) of automatic docu-ment classification layers that isolate and analysetargeted conversions on Twitter.
Section 4 dis-cusses ways in which the design of our frame-work is intended to support the agile methodol-ogy mentioned above, with particular emphasis onthe value of DUALIST?s active learning approach,and the crucial role of the collaborative gold stan-dard and model building activities.
Section 4.3discusses additional data processing step that havebeen introduced to increase the frameworks use-fulness, and section 5 introduces some projects towhich the framework is being applied.2 Related WorkWork that focuses on addressing sociologicalquestions with SMA broadly fall into one of threecategories.?
Approaches that employ automatic data analy-sis without tailoring the analysis to the specifics ofthe situation e.g.
(Tumasjan et al 2010; Tumas-jan et al 2011; O?Connor et al 2010; Gonzalez-Bailon et al 2010; Sang and Bos, 2012; Bollenet al 2011).
This body of research involves lit-tle or no manual inspection of the data.
An an-alytical technique is selected a-priori, applied tothe SM stream, and the results from that analy-sis are then aligned with a real-world phenomenonin order to draw predictive or correlative conclu-sions about social media.
A typical approach isto predict election outcomes by counting mentionsof political parties and/or politicians as ?votes?
invarious ways.
Further content analysis is thenoverlaid, such as sentiment or mood anlysis, inan attempt to improve performance.
However thegeneric language-analysis techniques that are ap-plied lead to little or no gain, often causing ad-justments to target question to something with lessstrict assessment criteria, such as poll trend insteadof election outcome (Tumasjan et al 2010; Tu-masjan et al 2011; O?Connor et al 2010; Sangand Bos, 2012).
This research has been criticisedfor applying out-of-domain techniques in a ?blackbox?
fashion, and questions have been raised asto how sensitive the results are to parameters cho-sen (Gayo-Avello, 2012; Jungherr et al 2012).?
Approaches that employ manual analysis ofthe data by researchers with a tailored analyti-cal approach (Bermingham and Smeaton, 2011;Castillo et al 2011).This approach reflects tra-ditional research methods in the social sciences.Through manual annotation effort, researchers en-gage closely with the data in a manual but in-teractive fashion, and this effort enables them touncover patterns in the data and make inferencesas to how SM was being used in the context ofthe sociocultural phenomena under investigation.This research suffers form either being restrictedto fairly small datasets.?
Approaches that employ tailored automaticdata analysis, using a supervised machine-learningapproach(Carvalho et al 2011; Papacharissi andde Fatima Oliveira, 2012; Meraz and Papacharissi,2013; Hopkins and King, 2010).
This research in-fers properties of the SM data using statistics fromtheir bespoke machine learning analysis.
Mannualannotation effort is required to train the classifiersand is typically applied in a batch process at thecommencement of the investigation.Our work aims to expand this last category, im-proving the quality of research by capturing moreof the insight-provoking engagement with the dataseen in more traditional research.3 DUALISTOur approach is built around DUALIST (Settles,2011; Settles and Zhu, 2012), an open-sourceproject designed to enable non-technical analyststo build machine-learning classifiers by annotat-ing documents with just a few minutes of effort.37In Section 4, we discuss various ways in whichwe have extended DUALIST, including function-ality allowing multiple annotators to work in par-allel; incorporating functionality to create ?gold-standard?
test sets and measure inter-annotatoragreement; and supporting on-going performanceevaluation against the gold standard during theprocess of building a classifier.
DUALIST pro-vides a graphical interface with which an annota-tor is able to build a Na?
?ve Bayes?
classifier givena collection of unlabelled documents.
During theprocess of building a classifier, the annotator ispresented with a selection of documents (in ourcase tweets) that he/she has an opportunity to la-bel (with one of the class labels), and, for eachclass, a selection of features (tokens) that the an-notator has an opportunity to mark as being strongfeatures for that class.Active learning is used to select both the docu-ments and the features being presented for annota-tion.
Documents are selected on the basis of thosethat the current model is most uncertain about(as measured by posterior class entropy), and fea-tures are selected for a given class on the basisof those with highest information gain occurringfrequently with that class.
After a batch of docu-ments and features have been annotated, a revisedmodel is built using both the labelled data and thecurrent model?s predictions for the remaining un-labelled data, through the use of the Expectation-Maximization algorithm.
This new model is thenused as the basis for selecting the set of documentsand features that will be presented to the annotatorfor the next iteration of the model building pro-cess.
Full details can be found in Settles (2011).The upshot of this is two-fold: not only can areasonable model be rapidly created, but the re-searcher is exposed to an interesting non-uniformsample of the training data.
Examples that are rel-atively easy for the model to classify, i.e.
thosewith low entropy, are ranked lower in the list ofunlabelled data awaiting annotation.
The effect ofthis is that the training process facilitates a form ofdata exploration that exposes the user to the hard-est border cases.4 Extending DUALIST for Social MediaScience ResearchThis section describes ways in which we have ex-tended DUALIST to provide an integrated data ex-ploration tool for social scientists.
As outlined inthe introduction, our vision is that a team of socialscientists will be able to use this tool to collabora-tively work towards the construction of a cascadeof automatic document classification layers thatcarve up an incoming Twitter data stream in orderto pick out one or more targeted ?conversations?,and provide an analysis of what is being discussedin each of these ?conversations?.
In what follows,we refer to the social scientists as the researchersand the activity during which the researchers areworking towards delivering a useful classifier cas-cade as data engagement.4.1 Facilitating data engagementWhen embarking on the process of building oneof the classifiers in the cascade, researchers bringpreconceptions as to the basis for the classifica-tion.
It is only when engaging with the data thatit becomes possible to develop an adequate clas-sification policy.
For example, when looking fortweets that express some attitude about a targetedissue, one needs a policy as to how a tweet thatshares a link to an opinion piece on that topicwithout any further comment should be classified.There are a number of ways in which we supportthe classification policy development process.?
One of the impacts of the active learning ap-proach adopted in DUALIST is that by presentingtweets that the current model is most unsure of,DUALIST will very rapidly expose issues aroundhow to make decisions on boundary cases.?
We have extended DUALIST to allow multi-ple researchers to build a classifier concurrently.In addition to reducing the time it takes to buildclassifiers, this fosters a collaborative approach toclassification policy development.?
We have added functionality that allows for thecollaborative construction of gold standard datasets.
Not only does this provide feedback dur-ing the model building process as to when perfor-mance begins to plateau, but, as a gold standardis being built, researchers are shown the currentinter-annotator agreement score, and are shownexamples of tweets where there is disagreementamong annotators.
This constitutes yet anotherway in which researchers are confronted with themost problematic examples.4.2 Building classifier cascadesHaving considered issues that relate to the con-struction of an individual classifier, we end this38section by briefly considering issues relating tothe classifier cascade.
The Twitter API providesbasic boolean search functionality that is used toscrape the Twitter stream, producing the input tothe cascade.
A typical strategy is to select queryterms for the boolean search with a view to achiev-ing a reasonably high recall of relevant tweets2.An effective choice of query terms that actuallyachieves this is one of the things that is not wellunderstood in advance, but which we expect toemerge during the data engagement phase.
Cap-turing an input stream that contains a sufficientlylarge proportion of interesting (relevant) tweets isusually achieved at the expense of precision (theproportion of tweets in the stream being scrapedthat are relevant).
As a result, the first task that istypically undertaken during the data engagementphase involves building a relevancy classifier, tobe deployed at the top of the classifier cascade,that is designed to filter out irrelevant tweets fromthe stream of tweets being scraped.When building the relevancy classifier, the re-searchers begin to see how well their preconcep-tions match the reality of the data stream.
It is onlythrough the process of building this classifier thatthe researchers begin to get a feel for the compo-sition of the relevant data stream.
This drives theresearcher?s conception as to how best to divideup the stream into useful sub-streams, and, as aresult, provides the first insights into an appropri-ate cascade architecture.
Our experience is that inmany cases, classifiers at upper levels of the cas-cade are involved in decomposing data streams inuseful ways, and classifiers that are lower downin the cascade are designed to measure some facet(e.g.
sentiment polarity) of the material on someparticular sub-stream.4.3 Tools for Data AnalysisAs social scientists are starting to engage withreal-world data using this framework, it hasemerged that certain patterns of downstream dataanalysis are of particular use.Time series analysis.
For many social phenom-ena, the timing and sequence of social media mes-sages are of critical importance, particularly for aplatform such as Twitter.
Our framework supportstweet volume analysis across any time frame, al-2In many cases it is very hard to estimate recall since thereis no way to estimate accurately the volume of relevant tweetsin the full Twitter stream.lowing researchers to review changes over timein any classifier?s input or output tweet flows(classes).
This extends the common approach ofsentiment tracking over time to tracking over timeany attitudinal (or other) response whose essen-tial features can be captured by a classifier of thiskind.
These class-volume-by-time-interval plotscan provide insight into how and when the streamchanges in response to external events.Link analysis.
It is becoming apparent that linksharing (attaching a URL to a tweet, typicallypointing to a media story) is an important aspect ofhow information propagates through social media,particularly on Twitter.
For example, the mean-ing of a tweet can sometimes only be discerned byinspecting the link to which it points.
We are in-troducing to the framework automatic expansionof shortened URLs and the ability to inspect linkURL contents, allowing researchers to interprettweets more rapidly and accurately.
A combina-tion of link analysis with time series analysis isalso providing researchers with insights into howmainstream media stories propagate through soci-ety and shape opinion in the social media age.Language use analysis.
Once a classifier hasbeen initially established, the framework analysesthe language employed in the input tweets usingan information gain (IG) measure.
High IG fea-tures are those that have occurrence distributionsthat closely align the document classification dis-tributions; essentially they are highly indicative ofthe class.
This information is proving useful to so-cial science researchers for three purposes.
First,it helps identify the words and phrases people em-ploy to convey a particular attitude or opinion inthe domain of interest.
Second, it can provide in-formation on how the language employed shiftsover time, for example as new topics are intro-duced or external events occur.
Third, it can beused to select candidate keywords with which toaugment the stream?s boolean scraper query.
Inthis last case, however, we need to augment theanalysis; many high IG terms make poor scraperterms because they are poorly selective in the moregeneral case (i.e.
outside of the context of the ex-isting query-selected sample).
We take a sampleusing the candidate term alone with the search APIand estimate the relevancy precision of the scrapedtweet sample by passing the tweets through thefirst-level relevancy classifier.
The precision of the39new candidate term can be compared to the preci-sion of existing terms and a decision made.5 Applications and ExtensionsThe framework?s flexibility enables it to be appliedto any task that can be broken down into a series ofclassification decisions, or indeed where this ap-proach materially assists the social scientist in ad-dressing the issue at hand.
In order to explore itsapplication, our framework is being applied to avariety of tasks:Identifying patterns of usage.
People use thesame language for different purposes; the frame-work is proving to be a valuable tool for eluci-dating these usage patterns and for isolating datasets that illustrate these patterns.
As an example,the authors (in collaboration with a team of so-cial scientists) are studying the differing ways inwhich people employ ethnically and racially sensi-tive language in conversations on-line.
The frame-work has helped to reveal and isolate a number ofdistinct patterns of usage.Tracking changes in opinion over time.
Sen-timent classifiers trained in one domain performpoorly when applied to another domain, evenwhen the domains are apparently closely related(Pang and Lee, 2008).
Traditionally, this hasforced a choice between building bespoke clas-sifiers (at significant cost), or using generic sen-timent classifiers (which sacrifice performance).The ability to rapidly construct sentiment classi-fiers that are specifically tuned to the precise do-main can significantly increase classifier perfor-mance without imposing major additional costs.Moving beyond sentiment, with these bespokeclassifiers it is in principle possible to track overtime any form of opinion that is reflected in lan-guage.
In a second study, the authors are (in col-laboration with a team of social scientists) build-ing cascades of bespoke classifiers to investigateshifts in citizens?
attitudes over time (as expressedin social media) to a range of political and socialissues arising across the European Union.Entity disambiguation.
References to individ-uals are often ambiguous.
In the general case,word sense disambiguation is most success-fully performed by supervised-learning classifiers(Ma`rquez et al 2006), and the low cost of pro-ducing classifiers using this framework makes thisapproach practical for situations where we requirerepeated high recall, high precision searches oflarge data sets for a specific entity.
As an example,this approach is being employed in the EU attitu-dinal survey study.Repeated complex search.
In situations wherea fixed but complex search needs to be performedrepeatedly over a relatively long period of time,then a supervised-learning classifier can be ex-pected both to produce the best results and to becost-effective in terms of the effort required totrain it.
The authors have employed this approachin a commercial environment (Lyra et al 2012),and the ability to train classifiers more quicklywith this framework reduces the cost still furtherand makes this a practical approach in a widerrange of circumstances.With regard to extension of the framework, wehave identified a number of avenues for expansionand improvement that will significantly increaseits usefulness and applicability to real-world sce-narios, and we have recently commenced an 18-month research programme to formalise and ex-tend the framework and its associated methodol-ogy for use in social science research3.Conclusions and Future WorkWe describe an agile analysis framework builtaround the DUALIST tool designed to support ef-fective exploration of large twitter data sets bysocial scientists.
The functionality of DUAL-IST has been extended to allow the scraping oftweets through access to the Twitter API, collab-orative construction of both gold standard datasets and Na?
?ve Bayes?
classifiers, an InformationGain-based method for automatic discovery ofnew search terms, and support for the constructionof classifier cascades.
Further extensions currentlyunder development include grouping tweets intothreads conversations, and automatic clustering ofrelevant tweets in order to discover subtopics un-der discussion.AcknowledgmentsWe are grateful to our collaborators at the Cen-tre for the Analysis of social media, Jamie Bartlettand Carl Miller for valuable contributions to thiswork.
We thank the anonymous reviewers for theirhelpful comments.
This work was partially sup-ported by the Open Society Foundation.3Towards a Social Media Science, funded by the UKESRC National Centre for Research Methods.40References[Bermingham and Smeaton2011] Adam Berminghamand Alan F Smeaton.
2011.
On using Twitter tomonitor political sentiment and predict electionresults.
In Proceedings of the Workshop on Senti-ment Analysis where AI meets Psychology (SAAIP),IJCNLP 2011, pages 2?10.
[Bollen et al011] Johan Bollen, Alberto Pepe, andHuina Mao.
2011.
Modeling public mood and emo-tion: Twitter sentiment and socio-economic phe-nomena.
In Proceedings of the Fifth InternationalAAAI Conference on Weblogs and Social Media,pages 450?453.
[Carvalho et al011] Paula Carvalho, Lu?
?s Sarmento,Jorge Teixeira, and Ma?rio J. Silva.
2011.
Liars andsaviors in a sentiment annotated corpus of commentsto political debates.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies: shortpapers - Volume 2, pages 564?568, Stroudsburg, PA,USA.
[Castillo et al011] Carlos Castillo, Marcelo Mendoza,and Barbara Poblete.
2011.
Information credibilityon Twitter.
In Proceedings of the 20th InternationalConference on World wide web, pages 675?684.
[Diakopoulos and Shamma2010] Nicholas A Di-akopoulos and David A Shamma.
2010.
Charac-terizing debate performance via aggregated Twittersentiment.
In Proceedings of the 28th internationalconference on Human factors in computing systems,pages 1195?1198.
[Gayo-Avello2012] Daniel Gayo-Avello.
2012.
Iwanted to predict elections with twitter and all igot was this lousy paper a balanced survey on elec-tion prediction using Twitter data.
arXiv preprintarXiv:1204.6441.
[Gonzalez-Bailon et al010] Sandra Gonzalez-Bailon,Rafael E Banchs, and Andreas Kaltenbrunner.
2010.Emotional reactions and the pulse of public opin-ion: Measuring the impact of political events onthe sentiment of online discussions.
arXiv preprintarXiv:1009.4019.
[Hopkins and King2010] Daniel J. Hopkins and GaryKing.
2010.
A method of automated nonparametriccontent analysis for social science.
American Jour-nal of Political Science, 54(1):229?247.
[Jungherr et al012] Andreas Jungherr, Pascal Ju?rgens,and Harald Schoen.
2012.
Why the Pirate Partywon the German election of 2009 or the troublewith predictions: A response to Tumasjan, Sprenger,Sander, & Welpe.
Social Science Computer Review,30(2):229?234.
[Lyra et al012] Matti Lyra, Daoud Clarke, HamishMorgan, Jeremy Reffin, and David Weir.
2012.Challenges in applying machine learning to mediamonitoring.
In Proceedings of Thirty-second SGAIInternational Conference on Artificial Intelligence(AI-2012).
[Marchetti-Bowick and Chambers2012] MicolMarchetti-Bowick and Nathanael Chambers.2012.
Learning for microblogs with distant su-pervision: political forecasting with Twitter.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 603?612.
[Ma`rquez et al006] Llu?
?s Ma`rquez, Gerard Escudero,David Mart?
?nez, and German Rigau.
2006.
Su-pervised corpus-based methods for wsd.
In EnekoAgirre and Philip Edmonds, editors, Word SenseDisambiguation, volume 33 of Text, Speech andLanguage Technology, pages 167?216.
SpringerNetherlands.
[Meraz and Papacharissi2013] Sharon Meraz and ZiziPapacharissi.
2013.
Networked gatekeeping andnetworked framing on #egypt.
The InternationalJournal of Press/Politics, 18(2):138?166.
[O?Connor et al010] Brendan O?Connor, RamnathBalasubramanyan, Bryan R Routledge, and Noah ASmith.
2010.
From tweets to polls: Linking textsentiment to public opinion time series.
In Proceed-ings of the International AAAI Conference on We-blogs and Social Media, pages 122?129.
[Pang and Lee2008] Bo Pang and Lillian Lee.
2008.Opinion mining and sentiment analysis.
Founda-tions and trends in Information Retrieval, 2(1-2):1?135.
[Papacharissi and de Fatima Oliveira2012] Zizi Pa-pacharissi and Maria de Fatima Oliveira.
2012.Affective news and networked publics: the rhythmsof news storytelling on #egypt.
Journal of Commu-nication, 62(2):266?282.
[Sang and Bos2012] Erik Tjong Kim Sang and JohanBos.
2012.
Predicting the 2011 dutch senate elec-tion results with Twitter.
Proceedings of the Euro-pean Chapter of the Association for ComputationalLinguistics 2012, page 53.
[Settles and Zhu2012] Burr Settles and Xiaojin Zhu.2012.
Behavioral factors in interactive training oftext classifiers.
In Proceedings of the 2012 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 563?567.
[Settles2011] Burr Settles.
2011.
Closing the loop:Fast, interactive semi-supervised annotation withqueries on features and instances.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 1467?1478.
[Tumasjan et al010] Andranik Tumasjan, Timm OSprenger, Philipp G Sandner, and Isabell M Welpe.2010.
Predicting elections with Twitter: What 140characters reveal about political sentiment.
In Pro-ceedings of the fourth international AAAI confer-ence on weblogs and social media, pages 178?185.41[Tumasjan et al011] Andranik Tumasjan, Timm OSprenger, Philipp G Sandner, and Isabell M Welpe.2011.
Election forecasts with Twitter how 140 char-acters reflect the political landscape.
Social ScienceComputer Review, 29(4):402?418.42
