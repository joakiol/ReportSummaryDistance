A flexible d ist r ibuted arch i tecture for NLP  systemdevelopment and useFreddy  Y. Y. Cho iArtificial Intelligence GroupUniversity of ManchesterManchester, U.K.choif@cs.man.ac.ukAbst rac tWe describe a distributed, modular architecturefor platform independent atural anguage sys-tems.
It features automatic interface genera-tion and self-organization.
Adaptive (and non-adaptive) voting mechanisms are used for inte-grating discrete modules.
The architecture issuitable for rapid prototyping and product de-livery.1 In t roduct ionThis article describes TEA 1, a flexible architec-ture for developing and delivering platform in-dependent text engineering (TE) systems.
TEAprovides a generalized framework for organizingand applying reusable TE components (e.g.
to-kenizer, stemmer).
Thus, developers are ableto focus on problem solving rather than imple-mentation.
For product delivery, the end userreceives an exact copy of the developer's edition.The visibility of configurable options (differentlevels of detail) is adjustable along a simple gra-dient via the automatically generated user inter-face (Edwards, Forthcoming).Our target application is telegraphic textcompression (Choi (1999b); of Roelofs (Forth-coming); Grefenstette (1998)).
We aim to im-prove the efficiency of screen readers for thevisually disabled by removing uninformativewords (e.g.
determiners) in text documents.This produces a stream of topic cues for rapidskimming.
The information value of each wordis to be estimated based on an unusually widerange of linguistic information.TEA was designed to be a development en-vironment for this work.
However, the targetapplication has led us to produce an interestingtTEA is an acronym for Text Engineering Architec-ture.architecture and techniques that are more gen-erally applicable, and it is these which we willfocus on in this paper.2 Arch i tec tureI System inputand outputI I L I IPlug*ins Shared knowledge System controls~ructureFigure 1: An overview of the TEA systemframework.The central component of TEA is a frame-based data model (F) (see Fig.2).
In this model,a document isa list of frames (Rich and Knight,1991) for recording the properties about eachtoken in the text (example in Fig.2).
A typicalTE system converts a document into F with aninput plug-in.
The information required at theoutput determines the set of process plug-ins toactivate.
These use the information in F to addannotations to F. Their dependencies are auto-matically resolved by TEA.
System behavior iscontrolled by adjusting the configurable param-eters.Frame 1: (:token An :pos art :begin_s 1)Frame 2: (:token example :pos n)Frame 3: (:token sentence :pos n)Frame 4: (:token .
:pos punc :end_s 1)Figure 2: "An example sentence."
in a frame-based data model615This type of architecture has been imple-mented, classically, as a 'blackboard' systemsuch as Hearsay-II (Erman, 1980), where inter-module communication takes place through ashared knowledge structure; or as a 'message-passing' system where the modules communi-cate directly.
Our architecture is similar toblackboard systems.
However, the purpose ofF (the shared knowledge structure in TEA) isto provide a single extendable data structure forannotating text.
It also defines a standard in-terface for inter-module communication, thus,improves system integration and ease of soft-ware reuse.2.1 Vot ing  mechan ismA feature that distinguishes TEA from similarsystems is its use of voting mechanisms for sys-tem integration.
Our approach as two distinctbut uniformly treated applications.
First, forany type of language analysis, different tech-niques ti will return successful results P(r) ondifferent subsets of the problem space.
Thuscombining the outputs P(rlti) from several tishould give a result more accurate than any onein isolation.
This has been demonstrated in sev-eral systems (e.g.
Choi (1999a); van Halterenet al (1998); Brill and Wu (1998); Veronis andIde (1991)).
Our architecture currently offerstwo types of voting mechanisms: weighted av-erage (Eq.1) and weighted maximum (Eq.2).
ABayesian classifier (Weiss and Kulikowski, 1991)based weight estimation algorithm (Eq.3) is in-cluded for constructing adaptive voting mecha-nisms.P(r )  = w P(rlti)i=1(1)P( r )  = max{WlP(r l tx) , .
.
.
,w, ,P(r l t , )}  (2)= P(r l t , ) )  (3)Second, different ypes of analysis a/ will pro-vide different information about a problem,hence, a solution is improved by combining sev-eral ai.
For telegraphic text compression, we es-timate E(w), the information value of a word,based on a wide range of different informationsources (Fig.2.1 shows a subset of our workingsystem).
The output of each ai are combined bya voting mechanism to form a single measure.Vo~ng mechanism 0Pmcoss 0I " ....... " Il I I !
Technique Ane~ysiscom~na~on ?om~n~onFigure 3: An example configuration of TEA fortelegraphic text compression.Thus, for example, if our system encoun-ters the phrase 'President Clinton', both lexicallookup and automatic tagging will agree that'President' is a noun.
Nouns are generally infor-mative, so should be retained in the compressedoutput text.
However, grammar-based syntac-tic analysis gives a lower weighting to the firstnoun of a noun-noun construction, and bigramanalysis tells us that 'President Clinton' is acommon word pair.
These two modules overrulethe simple POS value, and 'President Clinton'is reduced to 'Clinton'.3 Re la ted  workCurrent trends in the development of reusableTE tools are best represented by the Edinburghtools (LTGT) 2 (LTG, 1999) and GATE 3 (Cun-ningham et al, 1995).
Like TEA, both LTGTand GATE are frameworks for TE.LTGT adopts the pipeline architecture formodule integration.
For processing, a text doc-ument is converted into SGML format.
Pro-cessing modules are then applied to the SGMLfile sequentially.
Annotations are accumulatedas mark-up tags in the text.
The architecture issimple to understand, robust and future proof.The SGML/XML standard is well developedand supported by the community.
This im-proves the reusability of the tools.
However,2LTGT is an acronym for the Edinburgh LanguageTechnology Group Too lsaGATE is an acronym for General Architecture forText Engineering.616tile architecture ncourages tool developmentrather than reuse of existing TE components.GATE is based on an object-oriented datamodel (similar to the T IPSTER architecture(Grishman, 1997)).
Modules communicate byreading and writing information to and from acentral database.
Unlike LTGT, both GATEand TEA are designed to encourage softwarereuse.
Existing TE tools are easily incorporatedwith Tcl wrapper scripts and Java interfaces, re-spectively.Features that distinguish LTCT, GATE andTEA are the configuration methods, portabil-ity and motivation.
Users of LTGT write shellscripts to define a system (as a chain of LTGTcomponents).
With GATE, a system is con-structed manually by wiring TE components o-gether using the graphical interface.
TEA as-sumes the user knows nothing but the availableinput and required output.
The appropriate setof plug-ins are automatically activated.
Moduleselection can be manually configured by adjust-ing the parameters of the voting mechanisms.This ensures a TE system is accessible to com-plete novices ~,,-I yet has sufficient control fordevelopers.LTGT and GATE are both open-source C ap-plications.
They can be recompiled for manyplatforms.
TEA is a Java application.
It canrun directly (without compilation) on any Javasupported systems.
However, applications con-structed with the current release of GATE andTEA are less portable than those produced withLTGT.
GATE and TEA encourage reuse of ex-isting components, not all of which are platformindependent 4.
We believe this is a worth whiletrade off since it allows developers to constructprototypes with components hat are only avail-able as separate applications.
Native tools canbe developed incrementally.4 An  exampleOur application is telegraphic text compression.The examples were generated with a subset ofour working system using a section of the bookHAL's legacy (Stork, 1997) as test data.
First,we use different compression techniques to gen-erate the examples in Fig.4.
This was done bysimply adjusting a parameter of an output plug-4This is not  a problem for LTGT since the architec-ture does not encourage component reuse.in.
It is clear that the output is inadequate forrapid text skimming.
To improve the system,the three measures were combine with an un-weighted voting mechanism.
Fig.4 presents twolevels of compression using the new measure.1.
With science fiction films the more scienceyou understand the less you admire the film orrespect its makers2.
fiction films understand less admire respectmakers3.
fiction understand less admire respect makers4.
science fiction films science film makersFigure 4: Three measures of information value:(1) Original sentence, (2) Token frequency, (3)Stem frequency and (4) POS.1.
science fiction films understand less admirefilm respect makers2.
fiction makersFigure 5: Improving telegraphic text compres-sion by analysis combination.5 Conc lus ions  and  fu ture  d i rec t ionsWe have described an interesting architecture(TEA) for developing platform independenttext engineering applications.
Product delivery,configuration and development are made sim-ple by the self-organizing architecture and vari-able interface.
The use of voting mechanismsfor integrating discrete modules is original.
Itsmotivation is well supported.The current implementation f TEA is gearedtowards token analysis.
We plan to extendthe data model to cater for structural annota-tions.
The tool set for TEA is constantly be-ing extended, recent additions include a proto-type symbolic classifier, shallow parser (Choi,Forthcoming), sentence segmentation algorithm(Reynar and Ratnaparkhi, 1997) and a POStagger (Ratnaparkhi, 1996).
Other adaptivevoting mechanisms are to be investigated.
Fu-ture release of TEA will support concurrent ex-ecution (distributed processing) over a network.Finally, we plan to investigate means of im-proving system integration and module orga-nization, e.g.
annotation, module and tag setcompatibility.617ReferencesE.
Brill and J. Wu.
1998.
Classifier combina-tion for improved lexical disambiguation.
IProceedings of COLING-A CL '98, pages 191-195, Montreal, Canada, August.F.
Choi.
1999a.
An adaptive voting mechanismfor improving the reliability of natural an-guage processing systems.
Paper submittedto EACL'99, January.F.
Choi.
1999b.
Speed reading for thevisually disabled.
Paper submitted toSIGART/AAAI'99 Doctoral Consortium,February.F.
Choi.
Forthcoming.
A probabilistic ap-proach to learning shallow linguistic patterns.In ProCeedings of ECAI'99 (Student Session),Greece.H.
Cunningham, R.G.
Gaizauskas, andY.
Wilks.
1995.
A general architecture fortext engineering (gate) - a new approachto language ngineering research and de-velopment.
Technical Report CD-95-21,Department ofComputer Science, Universityof Sheffield.
http://xxx.lanl.gov/ps/cmp-lg/9601009.M.
Edwards.
Forthcoming.
An approach toautomatic interface generation.
Final yearproject report, Department of Computer Sci-ence, University of Manchester, Manchester,England.L.
Erman.
1980.
The hearsay-ii speech under-standing system: Integrating knowledge toresolve uncertainty.
In A CM Computer Sur-veys, volume 12.G.
Grefenstette.
1998.
Producing intelligenttelegraphic text reduction to provide an audioscanning service for the blind.
In AAAI'98Workshop on Intelligent Text Summariza-tion, San Francisco, March.R.
Grishman.
1997.
Tipster architecture de-sign document version 2.3.
Technical report,DARPA.
http://www.tipster.org.LTG.
1999.
Edinburgh univer-sity, hcrc, ltg software.
WWW.http://www.ltg.ed.ac.uk/software/index.html.H.
Rollfs of Roelofs.
Forthcoming.
Telegraph-ese: Converting text into telegram style.Master's thesis, Department of Computer Sci-ence, University of Manchester, Manchester,England.G.
M. P. O'Hare and N. R. Jennings, edi-tots.
1996.
Foundations of Distributed Ar-tificial Intelligence.
Sixth generation com-puter series.
Wiley Interscience Publishers,New York.
ISBN 0-471-00675.A.
Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceed-ings of the empirical methods in NLP confer-ence, University of Pennsylvania.J.
Reynar and A. Ratnaparkhi.
1997.
A max-imum entropy approach to identifying sen-tence boundaries.
In Proceedings of the fifthconference on Applied NLP, Washington D.C.E.
Rich and K. Knight.
1991.
Artificial Intel-ligence.
McGraw-Hill, Inc., second edition.ISBN 0-07-100894-2.D.
Stork, editor.
1997.
Hal's Legacy: 2001'sComputer in Dream and Reality.
MIT Press.http: / / mitpress.mit.edu\[ e-books /Hal /.H.
van Halteren, J. Zavrel, and W. Daelemans.1998.
Improving data driven wordclass tag-ging by system combination.
In Proceedingsof COLING-A CL'g8, volume 1.J.
Veronis and N. Ide.
1991.
An accessment ofsemantic nformation automatically extractedfrom machine readable dictionaries.
In Pro-ceedings of EA CL'91, pages 227-232, Berlin.S.
Weiss and C. Kulikowski.
1991.
ComputerSystems That Learn.
Morgan Kaufmann.618
