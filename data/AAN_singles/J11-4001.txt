ACL Lifetime Achievement AwardThe Brain as a Statistical InferenceEngine?and You Can Too?Eugene Charniak?Brown UniversityThere are several possible templates for award talks.1 The most common is an intellec-tual history?how I came to make all these wonderful discoveries.
However, I am nevercompletely happy with my work, as it seems a pale shadow of what I think it shouldhave been.
Thus I am picking a different model?things we all know but do not sayout loud because we have no evidence to support them; and besides, making such boldclaims sounds pretentious.Thus I do not expect to say anything too novel here.
I hope all my readers alreadyknow that the brain exploits statistics, and most of them suspect that we in statisticalcomputational linguistics have something to say about how this works out in the caseof language.
My goal is therefore not to say anything you do not believe, but to causeyou to believe it more passionately.1.
Evidence for StatisticsThere is a growing body of evidence that our brains do their work via statistics.
Here Ipresent two studies: one classic, one new.1.1 Lexical Acquisition in InfantsThe classic work is that of Saffran and Newport (Saffran, Aslin, and Newport 1996)(S&N) on eight-month-olds?
acquisition of lexical items.
As is well known, speech isheard as a mostly unsegmented stream, thus raising the question of how children learnto segment it into words.
What S&N show is that infants use statistical regularities inthe input.
When the stream is mid-word, there are fewer possible continuations thanbetween words because of the uncertainty in the next word.
More technically, the per-phoneme entropy is higher between words than within them.
S&N show that eight-month-olds are capable of detecting such differences.To do this S&N create an artificial ?language?
in which each ?word?
consists ofthree arbitrary phonemes, for example, bidukapupadotigolabubidaku.
.
.
.
So biduka andpupado are words, but the second two syllables of the first plus the first syllable of thesecond (dukapu) is not.
All the words are played with no emphasis on any syllable and?
With apologies to Stephen Colbert.?
Department of Computer Science, Brown University, Box 1910, Providence, RI 02912.E-mail: ec@cs.brown.edu.1 I have written this paper in the first person to reflect its origins as the Lifetime Achievement Award talkat ACL-2011.
It is not, however, based upon a transcript, since I can write better writing than I can speak.I also have included a few things I did not have time to say in the original version.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 4no difference in the spacing between syllables.
(It is pretty boring, but the child is onlysubjected to two minutes of it.)
After that the child is tested to see if he or she candistinguish real words from non-words.To test, either a word or a non-word is played from one of two speakers.
This is notdone until the child is already looking at that speaker and the word is replayed until thechild looks away.
The children are expected to gaze longer at the speaker that is playinga novel (non-) word than for words that they have already heard.Thus there are two testing conditions.
In the first, the non-words are completelynovel in the sense that the three-syllable combination did not occur in the two minutesof pretest training.
On average, the children focus on the speaker 0.88 seconds longerfor the novel words.The second condition is more interesting.
Here the non-words are made up of soundcombinations that have in fact occurred on the tape, but relatively infrequently becausethey consist of pieces of two different words.
Here the question is not a categoricalone (Have I heard this combination or not?)
but a statistical one: Is this a frequentcombination or is it rare?
Now the focus differential is 0.83 seconds.
The conclusionis that children are indeed sensitive to the statistical differences.1.2 What You See Where You Are Not LookingTry the following test.
Keep your gaze on the plus sign in the following example andtry to identify the letters to its left and its right.A + B R A C EThe ?A?
on the left is not too hard.
The letters on the right are much harder, a phe-nomenon called ?crowding.?
The work I am following here (Rosenholtz 2011) looksinto this and related phenomena.Obviously once we move our gaze around we have no problems with the letters.The center of the eye, the fovea, sends the brain a very detailed description.
But else-where the brain gets many fewer bits of information.
These bits have to ?summarize?that piece of the image.
The question that Rosenholtz (2011) looks at is what informationthese bits encode.Suppose we want a 1,000-bit summary of the top left image in Figure 1.
The otherthree images offer three possibilities.
The top-right image simply down-samples thepixels.
This is clearly not what the eye is doing.
Going back to the crowding exam-ple, we cannot make out the letters on the right, but we can be pretty sure they areletters.
Furthermore, we have little difficulty identifying the letter on the left, so it isnot just down-sampled.
The bottom-left image also down-samples, but on wavelets.It is little better, so we can ignore the fact that many of us don?t know what wave-lets are.The bottom-right image is the most interesting.
It assumes that the brain receivesa basket of statistics found useful by the statistical vision community for summarizingimages in general and textures in particular.
The image shown here is a sample from theposterior of the statistics for the original (leftmost) image.
Here it is reasonably clear thatwe are looking at letters, but we cannot be sure what they are, matching introspectionin the crowding example.If these are the statistics available to our brain, then looking foveally at such a recon-struction ought to be similar (equally difficult) to looking non-foveally at the original.644Charniak The Brain as a Statistical Inference EngineFigure 1Simulations of the information available on non-foveated portions of an image.The work in Rosenholtz (2011) shows that this seems to be true, again supporting theidea that brains manipulate statistical information.2.
Bayes?
LawOnce one accepts that the brain is manipulating probabilities, it seems inevitable thatQ2the overriding equation governing this process is Bayes?
Law:P(M | E) =P(M)P(E |M)P(E)where M is the learned model of the world and E is the relevant evidence (our per-ceptions).
We take Bayes?
Law as our guide in the rest of this talk because there is somuch to be learned from it.
More specifically, let us use it in the following form:P(M | E) ?
P(M)P(E |M)This is the case because P(E) acts as a linear scaling factor and thus can be ignored.First, Bayes?
Law divides responsibility between a prior over possible models(P(M)) and a term that depends on the evidence, its posterior given the model.
The warbetween rationalists and empiricists seems to have quieted of late, but it raged duringmuch of my academic career, with the rationalists (typically followers of Chomsky)insisting that the brain is pre-wired for language (and presumably much of the rest)and the empiricists believing that our model of the world is determined primarily bythe evidence of our senses.
Bayes?
Law calmly says that both play large roles, the exactbalance to be determined in the future.
The next section looks at the role of informativepriors in current models of learning from sensory input.Secondly, Bayes?
Law says that evidence is incorporated via a generative model ofthe world (P(E |M)).
This is, of course, as opposed to a discriminative model (P(M | E)).There is a lot to be said in favor of the latter.
When both are possibilities, the general645Computational Linguistics Volume 37, Number 4wisdom has it that discriminative models work better on average.
I certainly have thatimpression.
But children have only the evidence of their senses to go by.
Nobody readsthem the Penn Treebank or any other training data early in their career.
Thus generativemodels seem to be the only game in town.Furthermore, this generative model has to be a very large one?one of the entireworld insofar as the child knows it.
For example, we can describe visual experiences inlanguage, so it must include both vision and language, not to mention touch and smell.Thus it must be a joint model of all of these.
Taking this to heart, I look in Section 4 atsome work that uses joint modeling of multiple phenomena.Lastly, Bayes?
Law gives another clue, this time by what it omits?any notion of howany of this is done.
This suggests, to me at least, that the inference mechanism itself isnot learned, and if not, it must be innate.
Or to put it another way, Darwinian selectionhas already done the learning for us.
In the final section I will address what we can sayabout this mechanism.One last point before moving on.
What is the model that is to be learned?
Bayes?Law does not, in fact, tell us.
It only says that different models will be supported todiffering degrees given the evidence available.Various answers have been suggested.
One group tells us that ?true?
Bayesians donot adopt any model, they integrate over them.
That is, if you don?t know the correctmodel, you should plan accordingly and not commit.
This corresponds to the equationP(E) =?MP(M)P(E |M)Now integrating over all models makes a lot of sense, but to me it sounds a lot liketelling people to change all their passwords every month?it is sound advice, but whowill take the time?One possibility is to adopt the most likely one, for example,argmaxMP(M)P(E |M)At first glance this would seem to be a no-brainer, but some worry that we could havea probability distribution something like that in Figure 2.Here the most likely one, off on the right, has the very bad property that if we areonly a little bit wrong we end up with a very bad model of the world, thus negativelyimpacting the probability that we will survive to have progeny.
Better is to take theaverage model.
This will put us somewhere in the middle of the large block on the left,relatively safe from catastrophic results from small errors.My personal opinion is that this will turn out to be a non-problem.
Given the rightprior, the probability space over models will have one huge peak and not much else.Furthermore, as I discuss in the last section, our options on inference are going toFigure 2Projection on a line of models vs. probability for a bad case.646Charniak The Brain as a Statistical Inference Engineconstrain us quite a bit, with an integration over comparatively few models comingout on top.3.
Informative PriorsBayes?
Law shows us how priors on possible world models should be combined withthe evidence of our senses to guide our search for the correct one.
In this section I givethree examples of such priors, two in language, one in vision.3.1 Priors in Word SegmentationIn section 1 we looked at the work in Saffran, Aslin, and Newport (1996) on infants?ability to divide a speech stream into individual ?words.?
This problem has also beenattacked using computational approaches, albeit with simplifying assumptions.
Webase our discussion on the work of Goldwater and Griffiths (2007), as it dramaticallyshows the need for some informative prior.This, like most other computational linguistic work on word segmentation, consid-ers an abstract model of the problem.
A corpus of child-directed speech is translatedinto a simplified phoneme string by first transcribing the words, then for each wordwriting down its most common pronunciation.
All spaces between sounds inside anutterance are removed, but the boundaries between utterances are kept.
For exam-ple, you want to see the book would come out as yuwanttusiD6bUk.
The output is tobe the phoneme sequence with word boundaries restored.
Although we are reallydealing with phoneme sequences with spaces added, we will speak of dividing thespeech stream into words.A simple generative model for this task goes as follows:For each utterance:Repeat until the ?end of utterance?
symbol is chosenPick the next word according to P(w).It is assumed here that P(w) is over possible vocabulary items plus a special ?end-of-utterance?
symbol.If we have no prior on possible P(w)?s, then Bayes?
Law reduces to finding theM =P(w) that makes the data most likely, and this is easy to specify.
It is simply a distributionwith n ?words,?
each one a single utterance.
That is, it is a distribution that memorizesthe training data.
It is easy to see why this is so.
If the model generalized at all, it wouldassign probability to a sequence that is not in the input, and thus the probability ofobserved training data must be less than that assigned by the memorization model.In Goldwater and Griffiths (2007) the model was forced away from this solution byadopting a ?sparse prior?
?in this case a ?Dirichlet?
distribution.
This says, in effect,prefer M?s with as few different words as possible.
Thus if the number of words issignificantly smaller than the number of utterances, there is some pressure to prefera word-based distribution over one for each utterance.Unfortunately, the Dirichlet is a very weak prior, that is, the ?pressure?
is not verygreat.
So in one solution to the problem the child?s utterance comes out as youwant tosee thebook.
There is still a distinct tendency to merge words together.Why is Dirichlet so weak?
We want to evaluate the probability of our prior mul-tiplied by a generative posterior for a particular model M. In our case M is just a647Computational Linguistics Volume 37, Number 4probability distribution over possible words.
We assume for the sake of argument thatwe get this distribution from estimated integer counts over possible words.
(In Section 5we look at how Goldwater and Griffiths [2007] actually infers this distribution.)
So ourcurrent guess is that we have seen the ?word?
D6bUk two times, and so forth.
A simplemaximum likelihood distribution would assign a probability to this word of 2L , where Lis the total number of word tokens the model currently proposes in all of the utterances.Of course, the maximum likelihood distribution is called the maximum likelihooddistribution because for a given set of counts it assigns probabilities to make the prob-ability of the data (the ?likelihood?)
as high as possible.
Therefore it will lead us to thememorization result.The Dirichlet does something slightly different.
Imagine that before we do thedivision 2L we subtract12 from each nonzero word count.
Then D6bUk has a probability1.5L?0.5K , where K is the number of different word types (dictionary entries, as it were).This creates sparsity in the sense that entries that already have few counts tend to getstill fewer because they are impacted more than words with high counts.
So if we havetwo words, one with a count of 1 and one with a count of 1,000, and we have, say, a totalof 10,000 word tokens and 1,000 word types, the first will go from a probability of 10?5to approximately 0.53 ?
10?5 and the second will actually increase slightly (from 0.1 to0.105).
Thus words with small counts get crowded out, and the distribution becomesmore sparse.But the crowding is not all that strong, and in this model there are much strongercountervailing winds.
The model assumes that words appear at random, and this isvery far from the case.
In the first utterance the phrase the book occurs much more oftenthan the independent probabilities of the and book would suggest, so the model has anincentive to make them one word in order to capture a regularity it otherwise has noway to represent.
Including bigram probabilities in the model would do the trick, andindeed, performance is greatly enhanced by this change.
But this takes us beyond ourcurrent topic?priors.3.2 Priors in Image SegmentationWe turn now to work by Sudderth and Jordan (2008) on segmenting an image into piecesthat correspond to objects and background, as in Figure 3.
To put the goal another way,each pixel in the image should be assigned to exactly one of a small number of groups, sothat all the pixels in the same group exactly cover the places in the image correspondingto one type of object, be it people, trees, sky, and so on.Any one pixel can, a priori, be in any of the different groups.
But not all groupingsare equally plausible in an everyday image.
Consider Figure 4.
Each row correspondsto random samples from a prior distribution over possible images, where each colorcorresponds to one pixel set.
Although neither looks like scenes from the world aroundus, the upper row is more plausible than the lower one.
The lower one corresponds tothe so-called ?Potts?
prior: each pixel is a node in a Markov random field, and the Pottsprior encourages neighboring points to be from the same region.
Although these pixelarrays have such a property, there are all sorts of features that are not image-like.
Singlepixels are surrounded by other regions.
If we divide the array into, say, tenths, eachtenth probably has at least one pixel from all of the sets, and so forth.The upper, more plausible ?images?
come from the prior illustrated in Figure 5.
Theprocess of randomly selecting from the prior works as follows: First generate a three-dimensional surface by picking a small number of points and placing a two-dimensionalGaussian over them.
The surface point height corresponds to the sum of all the Gaussian648Charniak The Brain as a Statistical Inference EngineFigure 3Several scenes segmented (by people) into their basic parts.Figure 4Samples from two image priors.values at that point.
The topmost image on the left is an example.
Now locate a randomplane parallel to the bottom of the image.
All surface areas above the plane correspondto image region number 1.
The dark blue area in the image on the right corresponds toa plane through the top left Gaussian with the yellow and red areas above the selectedplane.
We assign region 1 to be the areas most in the foreground.We then repeat this process with new Gaussians and new planes (images 2 and 3 onthe left) until we get the desired number of regions.
The result is the random pixel arrayon the right.649Computational Linguistics Volume 37, Number 4Figure 5Image priors from sets of Gaussians.Naturally, in image segmentation one does not generate random selections fromthe prior.
But the depictions give the reader an idea of the biases placed on imagereconstruction.
A better way to think about the process would be to imagine that wetry different reconstructions of the kinds of texture, lines, and so forth, that correspondto a single image region, and then present each of these region combinations to the priorfor its ?take?
on things.
It will strongly prefer blobby regions, with nearer ones tendingto be convex.
And, as we would hope, region segmentation works better with the morerealistic prior.3.3 Priors in Part-of-Speech InductionPart-of-speech tagging is the process of labeling each word of a string with its correctpart of speech, a category such as ?plural noun?
(NNS) or preposition (PP), that to somedegree captures the syntactic properties of the word.
The problem is not completely welldefined insofar as there is no single ?correct?
set?part-of-speech sets for English rangein size between 12 and 20 ?
12.
Nevertheless, words do fall into such categories andeven as rough approximations they are useful as a first step in parsing.
In unsupervisedpart-of-speech induction we want our program to infer a set of parts of speech.
Becauseof their arbitrary properties, the size of this set is typically set in advance?for Englishtypically 45, corresponding to the Penn Treebank set.Since the early work of Merialdo (1994) we have known that this is a difficultproblem, and in retrospect it is easy to see why.
Most generative models work fromthe following generative story:For each word wiGenerate ti from ti?1 according to P(ti | ti?1)Generate wi from its tag according to P(wi | ti).650Charniak The Brain as a Statistical Inference Enginewhere ti is the ith (current) tag, ti?1 the previous, and wi is the current word.
With noprior over models, any such story is simply going to divide words into some numberof classes to best raise the probability of the words.
Merialdo uses expectation maxi-mization (EM) to find his classes.
Nothing in this set-up says anything about lookingfor syntactic properties, and it should therefore not be surprising that the classes foundare as much semantic or phonological as syntactic.
For example, a good way to raiseprobability is to split determiners into two categories, one including a, the secondan.
Similarly, for nouns create two classes, ?starting with vowel?
and ?starting withconsonant.?
This means that other less-used categories (foreign word or symbol) getomitted entirely.
This is the sort of thing EM does.
Words are assigned many tags(typically on the order of 10 to 20) and it evens out the tag assignments so most tagshave the same number of words assigned to them.There is, however, prior knowledge that is useful here.
I am following here the workof Clark (2003).
Clark first notes that, for the languages tried to date, most words haveeither just one tag or one tag that dominates all the others.
As just remarked, this is a farcry from what EM does.
So the first thing Clark (2003) does is to impose a one-tag-per-word-type constraint.
This more closely models the true distribution than does the EMoutcome.
In the paper he notes that giving this up inevitably yields poorer results.Secondly, he notes that, again contrary to EM?s proclivities, tags have very skewedword counts.
Some, typically content-word types like nouns, have very large numbersof word types, whereas grammatical types such as prepositions have very few.
Puttingthis fact into the prior also helps significantly.Note that using these facts (close to one tag per word and widely differing word-type counts per tag) in our prior amounts to positing linguistic universals.
I am not anexpert in world languages and thus anything I say about this is far from definitive, butas linguistic universals go, these look reasonable to me.4.
Joint Generative ModelingBayes?
Law tells us that the proper way to connect the prior on a world model to oursensory input is via P(M)P(E |M).
We now consider the second term of this product,the generative model of the environment.
Earlier we noted that this must be a jointmodel of vision and language because we can talk about what we see.
Although workis beginning on such modeling, vision is (currently) a much harder problem, so jointwork tends to help it and does little to help language researchers.
However, there arebenefits from combining different language problems into a single joint model, and inthis section we consider two such cases.4.1 Part-of-Speech InductionIn our discussion of priors for learning parts of speech we considered words to beatomic in the sense that we did not look at how they are built up from smaller parts(e.g., playing consists of play plus the progressive marker ing).
This is the study ofmorphology.English does not have a particularly rich morphology, especially compared to so-called free-word-order languages such as Russian, or Czech, but even in English thespellings of words can give big hints about their parts of speech.
Playing in the PennTreebank has the part of speech VBG, for progressive verb, and the s in plays indicatesa present-tense third-person-singular verb (VBZ).
Thus the tag induction program in651Computational Linguistics Volume 37, Number 4Clark (2003) actually computes the joint probability of the part of speech and themorphology.
To a rough approximation, it uses the following generative model:For each word wiGenerate ti from ti?1 according to P(ti | ti?1)Generate wi from its tag according to P(wi | ti).If we do not have a spelling for wi, generate a sequenceof characters ci, .
.
.
cn from the single-letter model Pti (c).This substantially improves the model.As noted in Clark (2003), however, this is not a true morphological model insofaras it does not model the relations among a word?s morphological forms.
For example, ifwe see ?playing?
and correctly identify it as a VBG, then the probability of ?plays?
as aVBZ should be higher than say ?doors?
(ignoring the probabilities of the other letters ofthe words).
Morphology, however, is still difficult for our models as it requires learningmulti-letter rule paradigms, and so far spelling by itself has worked quite well.4.2 Joint Named-Entity Recognition and Co-ReferenceNamed-entity recognition assigns objects in a discourse (those that have names) to asmall set of classes?prototypically, persons, places, and organizations.
In another caseof joint modeling, the Haghighi-Klein unsupervised co-reference model (Haghighi andKlein 2010) also learns named-entity recognition.
(It can to some degree also learn entitytypes that are not realized by names, but we ignore that here.)
Consider a simple co-reference example:Congressman Weiner said that his problems were all Facebook?s fault.
They shouldnever have given him an account.Suppose the program were able correctly to separate person names from those oforganizations.
This in turn would allow it to learn that people are referred to by he,she, and their variants, whereas organizations take it and they.
That, plus the fact thatentities with names are more likely to be antecedents of pronouns, would solve theexample.
(The program should be able to learn these facts about pronouns simply bylooking at which pronouns are most likely to follow people/organizations.
Naturally,there is no guarantee that the first pronoun following a person?s name refers back tothat individual, but the statistical signal is pretty strong.
)Conversely, suppose we have solved the co-reference problem.
We could then pickup facts such as Congressmen is a good descriptor of one kind of entity (e.g., person).
Orthat people and organizations, but not places, can say things.In practice the model distinguishes among entity types (e.g., person), entities (e.g.,the particular congressman), and mentions (the word sequence Congressman Weiner).All entities have an entity type that provides a prior on the properties of the entity.So suppose we see a new entity that is described by the mention CongressmanBoehner.
The program will make a guess about entity type.
If the program already?knows?
that congressman is a legitimate descriptor for people but not other types, thiswill bias the program towards person.Naturally, the program does not call these types ?person,?
?place,?
and so on.Rather, the program is told in advance to expect some small number of entity types,say ten.
If it works properly, we should see after the fact that the program has indeed652Charniak The Brain as a Statistical Inference Enginejointly learned not only co-reference, but the common types of individuals mentionedin the news articles.Of course, the program cannot even begin to approach the accuracy of discrim-inative named-entity tagging.
But from the perspective of this talk, that is besides thepoint.
We are taking a child?s view of the problem.
Furthermore, eventually we want notjust three or ten entity types, but a very detailed hierarchy (almost certainly an acyclicgraph) of them.
My guess is that such a graph can only be learned.5.
InferenceBayes?
Law says nothing about the mechanism used to infer a model of the world andfrom that I conclude that this mechanism is innate.
Nevertheless, any model, no matterhow abstracted from the particulars of neural implementation, must eventually confrontthe question of how these inferences are carried out.For many of us, optimizing generative models has been virtually synonymous withexpectation maximization.
It is relatively efficient, and although it can be prone to localmaxima, it often works well when started from uninformative initial states?the IBMModel 1 of machine-translation fame being an obvious case in point.However, it has very serious drawbacks: It does not make room for a prior overmodels, it requires storing the training data for multiple iterations, and as its sine quanon, it requires the storage of all possible expectations on the data, which in many casesof interest is simply not feasible.Consider again the problem of segmenting a sound stream into words.
The stringyuwanttusiD6bUk can be broken up into yu want tusiD6b Uk, yuwant tu siD6bUk, andso forth.
To a first approximation the number grows exponentially in the length of thestring and EM requires storing expectations counts for all of the possibilities.This is why Goldwater and Griffiths (2007) use Gibbs sampling rather than EM.
InGibbs sampling we do not store the probabilities of all possibilities, but rather repeat-edly go through data making somewhat random choices based upon the program?scurrent ?beliefs.?
For word segmentation this means we look at the point betweentwo phonemes and ask, ?Should we put a word boundary here??
We compute theprobability for the two cases (two words here or just one) based upon the counts wehave currently guessed.
So if the ?no-segment?
case produces a word we have seenreasonably often, its probability would be higher than the product of the probabilitiesfor the two-word case.
Suppose it comes out 60?40.
We then flip a 60?40 biased coin andput in a split if it is heads.
Note that at any one time we need to store only counts forthose words for which the program has at least one attestation, a number that growsonly linearly in string length.But Gibbs sampling, too, has serious drawbacks.
Like EM, it is an iterative algo-rithm.
As a psychological theory it would require the child to store all of her experiencesand continually revise her beliefs about all of them.
Another major problem withGibbs sampling is its requirement that the probability model is ?exchangeable.?
EachGibbs iteration requires visiting all the decisions and (possibly) revising them.
Beingexchangeable means that if we had made the same decisions but in a different order(picking the sentences in reverse order, or first picking even-numbered places betweenphonemes, then odd), the probability after visiting every one exactly once would comeout the same.
Amazingly, the word-segmentation model is exchangeable.But mathematical miracles do not grow on trees.
Although the Haghighi-Klein ref-erence model uses Gibbs sampling, their model is not in fact exchangeable, something653Computational Linguistics Volume 37, Number 4they had to work around.
More generally, this requirement is ever less likely to besatisfied as our probability models become more complicated.Or consider the options in inference mechanisms for statistical parsing.
If we restrictourselves to constituent parsing (as opposed to dependency parsing), the most usedsearch algorithm is Cocke-Kasami-Younger (CKY).
CKY takes a context-free grammarand uses the context-free property to reduce an exponential search problem to O(n3),where n is the length of the sentence.
CKY is bottom?up, so for a sentence like Dogs eatfood it would first find the noun phrases Dogs and food, then build up a verb phrase eatfood, and only after completely processing the entire sentence would it link the subjectDogs to the verb phrase eat food.But there are very strong reasons to believe that people do nothing like this.
Thestrongest is the observation that various tests (as well as introspection) indicate that weare well aware that Dog is the subject of eat before the sentence is complete.
(This isincreasingly obvious as we make the sentence longer.)
The second reason is more subtlebut to me equally important, the context-free requirement.
In a context-free grammar,saying that there is, say, a VP spanning positions 1 to 3 in our sentence is also saying thatno knowledge from anywhere else in the sentence has any effect on what goes on insidethis verb phrase, and vice versa.
Putting it another way, every symbol in a CKY chartis a knowledge barrier between its outside and inside.
We have managed to tame thissomewhat by ?state splitting,?
annotating the ?VP?
with other information, but to thedegree that we do this we are exactly losing the performance that CKY should deliver.We see a pattern here.
We want large joint models, and we want to take advantageof them by using as much as possible of our experiential history (e.g., previous words ofa sentence) to influence our expectations of what should come next and how our modelof the world should adapt.
But increasingly as we do this, various mathematical andalgorithm shortcuts cease to work.Actually, when you look at our requirements, there are not that many options.
Wewant a mechanism that uses probabilities to guide the search for a model, it has to beable to incorporate prior knowledge, and it cannot depend on storing very much of ourexperiential history.
Offhand this sounds like some sort of beam search with probabilityestimates as the search heuristic and this is, in fact, the solution that Brian Roark (Roarkand Johnson 1999) and I (Charniak 2010) both adopted for parsing models that seemreasonable as models of human performance.A standard example of such a search mechanism is particle filtering.
It is somewhatlike Gibbs sampling, except rather than repeatedly re-evaluating previous inputs it triesmultiple evaluations immediately.
In the limit, as the size of the beam goes to infinity,it is equivalent to Gibbs sampling but does not require exchangeability.
But as JohnMaynard Keynes almost said, in the limit we are all dead.
In practice we cannot affordthat many particles (beam size).
When discussing model selection, I commented that asfar as I can see the only option is to ?integrate?
over a very small number of possiblemodels.
I said this because this is essentially what particle filtering is doing.
Comparedto EM, or Gibbs sampling, or CKY, this is quite inefficient.
But it is not as though wehave a lot of options here.
It, or something quite like it, seems to be the best we canhope for.6.
ConclusionI have argued that the brain is a statistical information processor and as such we shouldsee it as operating according to Bayes?
Law.
On this basis we can see that learningdepends both on a prior probability over models (our innate biases about how the world654Charniak The Brain as a Statistical Inference Engineworks) and a joint generative model of the world.
I also noted that Bayes?
Law saysnothing about inference mechanisms, and hence I assume that this mechanism is notlearned?that it, too, is innate.
I suggested that particle filtering, a beam search methodbased upon a probability evaluation function, seems a good bet.Well, I said at the beginning that I would say little that most of my audience doesnot already believe.But I also have had a subtext.
I am not arguing that we as a field need to change ourfocus.
Just the opposite.
Our present focus has made statistical computational linguisticsone of the most vibrant intellectual endeavors of our time.
Rather, my argument is thatwe are already part-time cognitive scientists and together we are building a path to anew basis for the science of higher cognitive processes.
I intend to follow that path.
Youcan, too.ReferencesCharniak, Eugene.
2010.
Top-downnearly-context-sensitive parsing.In Proceedings of the 2010 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 674?683,Cambridge, MA.Clark, Alexander.
2003.
Combiningdistributional and morphologicalinformation for part of speech induction.In 10th Conference of the European Chapterof the Association for ComputationalLinguistics, pages 59?66, Budapest.Goldwater, Sharon and Tom Griffiths.2007.
A fully Bayesian approach tounsupervised part-of-speech tagging.In Proceedings of the 45th Annual Meetingof the Association of ComputationalLinguistics, pages 744?751, Prague.Haghighi, Aria and Dan Klein.
2010.Coreference resolution in a modular,entity-centered model.
In HumanLanguage Technologies: The 2010 AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,pages 385?393, Los Angeles, CA.Merialdo, Bernard.
1994.
Tagging Englishtext with a probabilistic model.Computational Linguistics, 20:155?171.Roark, Brian and Mark Johnson.
1999.Broad-coverage predictive parsing.Paper presented at the 12th AnnualCUNY Conference on Human SentenceProcessing, New York, NY.Rosenholtz, Ruth.
2011.
What your visualsystem sees where you are not looking.In Proceedings of SPIE: Human Vision andElectronic Imaging, pages 7865?7910,San Francisco, CA.Saffran, J., R. Aslin, and E. Newport.
1996.Statistical learning by 8-month-old infants.Science, 274:1926?1928.Sudderth, Eric B. and Michael I. Jordan.2008.
Shared segmentation of naturalscenes using dependent Pitman-Yorprocesses.
In Proceedings of Conference onNeural Information Processing Systems,pages 1585?1592, Vancouver.655
