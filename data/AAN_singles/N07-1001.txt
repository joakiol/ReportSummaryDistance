Proceedings of NAACL HLT 2007, pages 1?8,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsExploiting acoustic and syntactic features for prosody labeling ina maximum entropy frameworkVivek Rangarajan, Shrikanth NarayananSpeech Analysis and Interpretation LaboratoryUniversity of Southern CaliforniaViterbi School of Electrical Engineeringvrangara@usc.edu,shri@sipi.usc.eduSrinivas BangaloreAT&T Research Labs180 Park AvenueFlorham Park, NJ 07932, U.S.A.srini@research.att.comAbstractIn this paper we describe an automaticprosody labeling framework that exploitsboth language and speech information.We model the syntactic-prosodic informa-tion with a maximum entropy model thatachieves an accuracy of 85.2% and 91.5%for pitch accent and boundary tone la-beling on the Boston University RadioNews corpus.
We model the acoustic-prosodic stream with two different mod-els, one a maximum entropy model andthe other a traditional HMM.
We finallycouple the syntactic-prosodic and acoustic-prosodic components to achieve signifi-cantly improved pitch accent and bound-ary tone classification accuracies of 86.0%and 93.1% respectively.
Similar experimen-tal results are also reported on Boston Di-rections corpus.1 IntroductionProsody refers to intonation, rhythm and lexicalstress patterns of spoken language that convey lin-guistic and paralinguistic information such as em-phasis, intent, attitude and emotion of a speaker.Prosodic information associated with a unit ofspeech, say, syllable, word, phrase or clause, influ-ence all the segments of the unit in an utterance.
Inthis sense they are also referred to as suprasegmen-tals (Lehiste, 1970).
Prosody in general is highlydependent on individual speaker style, gender, di-alect and other phonological factors.
The difficulty inreliably characterizing suprasegmental informationpresent in speech has resulted in symbolic and para-meteric prosody labeling standards like ToBI (Tonesand Break Indices) (Silverman et al, 1992) and Tiltmodel (Taylor, 1998) respectively.Prosody in spoken language can be characterizedthrough acoustic features or lexical features or both.Acoustic correlates of duration, intensity and pitch,like syllable nuclei duration, short time energy andfundamental frequency (f0) are some acoustic fea-tures that are perceived to confer prosodic promi-nence or stress in English.
Lexical features like parts-of-speech, syllable nuclei identity, syllable stress ofneighboring words have also demonstrated high de-gree of discriminatory evidence in prosody detectiontasks.The interplay between acoustic and lexical fea-tures in characterizing prosodic events has been suc-cessfully exploited in text-to-speech synthesis (Bu-lyko and Ostendorf, 2001; Ma et al, 2003), speechrecognition (Hasegawa-Johnson et al, 2005) andspeech understanding (Wightman and Ostendorf,1994).
Text-to-speech synthesis relies on lexical fea-tures derived predominantly from the input text tosynthesize natural sounding speech with appropri-ate prosody.
In contrast, output of a typical auto-matic speech recognition (ASR) system is noisy andhence, the acoustic features are more useful in pre-dicting prosody than the hypothesized lexical tran-script which may be erroneous.
Speech understand-ing systems model both the lexical and acoustic fea-tures at the output of an ASR to improve naturallanguage understanding.
Another source of renewedinterest has come from spoken language translation(No?th et al, 2000; Agu?ero et al, 2006).
A pre-requisite for all these applications is accurate prosodydetection, the topic of the present work.In this paper, we describe our framework for build-ing an automatic prosody labeler for English.
Wereport results on the Boston University (BU) Ra-dio Speech Corpus (Ostendorf et al, 1995) andBoston Directions Corpus (BDC) (Hirschberg andNakatani, 1996), two publicly available speech cor-pora with manual ToBI annotations intended for ex-periments in automatic prosody labeling.
We con-dition prosody not only on word strings and theirparts-of-speech but also on richer syntactic informa-tion encapsulated in the form of Supertags (Banga-lore and Joshi, 1999).
We propose a maximum en-tropy modeling framework for the syntactic features.We model the acoustic-prosodic stream with two dif-ferent models, a maximum entropy model and a moretraditional hidden markov model (HMM).
In an au-tomatic prosody labeling task, one is essentially try-1ing to predict the correct prosody label sequence fora given utterance and a maximum entropy model of-fers an elegant solution to this learning problem.
Theframework is also robust in the selection of discrim-inative features for the classification problem.
So,given a word sequence W = {w1, ?
?
?
, wn} and a setof acoustic-prosodic features A = {o1, ?
?
?
, oT }, thebest prosodic label sequence L?
= {l1, l2, ?
?
?
, ln} isobtained as follows,L?
= argmaxLP (L|A,W ) (1)= argmaxLP (L|W ).P (A|L,W ) (2)?
argmaxLP (L|?
(W )).P (A|L,W ) (3)where ?
(W ) is the syntactic feature encoding of theword sequence W .
The first term in Equation (3)corresponds to the probability obtained through ourmaximum entropy syntactic model.
The second termin Equation (3), computed by an HMM correspondsto the probability of the acoustic data stream whichis assumed to be dependent only on the prosodic la-bel sequence.The paper is organized as follows.
In section 2we describe related work in automatic prosody la-beling followed by a description of the data used inour experiments in section 3.
We present prosodyprediction results from off-the-shelf synthesizers insection 4.
Section 5 details our proposed maximumentropy syntactic-prosodic model for prosody label-ing.
In section 6, we describe our acoustic-prosodicmodel and discuss our results in section 7.
We finallyconclude in section 8 with directions for future work.2 Related workAutomatic prosody labeling has been an active re-search topic for over a decade.
Wightman and Os-tendorf (Wightman and Ostendorf, 1994) developeda decision-tree algorithm for labeling prosodic pat-terns.
The algorithm detected phrasal prominenceand boundary tones at the syllable level.
Bulykoand Ostendorf (Bulyko and Ostendorf, 2001) useda prosody prediction module to synthesize naturalspeech with appropriate prosody.
Verbmobil (No?thet al, 2000) incorporated prosodic labeling into atranslation framework for improved linguistic analy-sis and speech understanding.Prosody has typically been represented either sym-bolically, e.g., ToBI (Silverman et al, 1992) orparametrically, e.g., Tilt Intonation Model (Tay-lor, 1998).
Parametric approaches either restrictthe variants of prosody by definition or automati-cally learn prosodic patterns from data (Agu?ero etal., 2006).
The BU corpus is a widely used cor-pus with symbolic representation of prosody.
Thehand-labeled ToBI annotations make this an attrac-tive corpus to perform prosody labeling experiments.The main drawback of this corpus is that it com-prises only read speech.
Prosody labeling on sponta-neous speech corpora like Boston Directions corpus(BDC), Switchboard (SWBD) has garnered atten-tion in (Hirschberg and Nakatani, 1996; Gregory andAltun, 2004).Automatic prosody labeling has been achievedthrough various machine learning techniques, suchas decision trees (Hirschberg, 1993; Wightman andOstendorf, 1994; Ma et al, 2003), rule-based sys-tems (Shimei and McKeown, 1999), bagging andboosting on CART (Sun, 2002), hidden markovmodels (Conkie et al, 1999), neural networks(Hasegawa-Johnson et al, 2005),maximum-entropymodels (Brenier et al, 2005) and conditional ran-dom fields (Gregory and Altun, 2004).Prosody labeling of the BU corpus has been re-ported in many studies (Hirschberg, 1993; Hasegawa-Johnson et al, 2005; Ananthakrishnan andNarayanan, 2005).
Hirschberg (Hirschberg, 1993)used a decision-tree based system that achieved82.4% speaker dependent accent labeling accuracyat the word level on the BU corpus using lexical fea-tures.
(Ross and Ostendorf, 1996) also used an ap-proach similar to (Wightman and Ostendorf, 1994)to predict prosody for a TTS system from lexical fea-tures.
Pitch accent accuracy at the word-level wasreported to be 82.5% and syllable-level accent accu-racy was 80.2%.
(Hasegawa-Johnson et al, 2005)proposed a neural network based syntactic-prosodicmodel and a gaussian mixture model based acoustic-prosodic model to predict accent and boundary toneson the BU corpus that achieved 84.2% accuracy inaccent prediction and 93.0% accuracy in intonationalboundary prediction.
With syntactic informationalone they achieved 82.7% and 90.1% for accent andboundary prediction, respectively.
(Ananthakrish-nan and Narayanan, 2005) modeled the acoustic-prosodic information using a coupled hidden markovmodel that modeled the asynchrony between theacoustic streams.
The pitch accent and boundarytone detection accuracy at the syllable level were75% and 88% respectively.
Our proposed maximumentropy syntactic model outperforms previous work.On the BU corpus, with syntactic information alonewe achieve pitch accent and boundary tone accuracyof 85.2% and 91.5% on the same training and testsets used in (Chen et al, 2004; Hasegawa-Johnsonet al, 2005).
Further, the coupled model with bothacoustic and syntactic information results in accura-cies of 86.0% and 93.1% respectively.
On the BDCcorpus, we achieve pitch accent and boundary toneaccuracies of 79.8% and 90.3%.3 DataThe BU corpus consists of broadcast news stories in-cluding original radio broadcasts and laboratory sim-2BU BDCCorpus statistics f2b f1a m1b m2b h1 h2 h3 h4# Utterances 165 69 72 51 10 9 9 9# words (w/o punc) 12608 3681 5058 3608 2234 4127 1456 3008# pitch accents 6874 2099 2706 2016 1006 1573 678 1333# boundary tones (w IP) 3916 1059 1282 1023 498 727 361 333# boundary tones (w/o IP) 2793 684 771 652 308 428 245 216Table 1: BU and BDC dataset used in experimentsulations recorded from seven FM radio announcers.The corpus is annotated with orthographic transcrip-tion, automatically generated and hand-correctedpart-of-speech tags and automatic phone alignments.A subset of the corpus is also hand annotated withToBI labels.
In particular, the experiments in thispaper are carried out on 4 speakers similar to (Chenet al, 2004), 2 male and 2 female referred to here-after asm1b, m2b, f1a and f2b.
The BDC corpus ismade up of elicited monologues produced by subjectswho were instructed to perform a series of direction-giving tasks.
Both spontaneous and read versions ofthe speech are available for four speakers h1, h2, h3and h4 with hand-annotated ToBI labels and auto-matic phone alignments, similar to the BU corpus.Table 1 shows some of the statistics of the speakersin the BU and BDC corpora.In Table 1, the pitch accent and boundary tonestatistics are obtained by decomposing the ToBI la-bels into binary classes using the mapping shown inTable 2.BU Labels Intermediate Mapping Coarse MappingH*,!H*L* Single Accent*,*?,X*?
accentH+!H*,L+H*,L+!H* Bitonal AccentL*+!H,L*+HL-L%,!H-L%,H-L%H-H% Final Boundary toneL-H%%?,X%?,%H btoneL-,H-,!H- Intermediate Phrase (IP) boundary-X?,-?<,>,no label none noneTable 2: ToBI label mapping used in experimentsIn all our prosody labeling experiments we adopta leave-one-out speaker validation similar to themethod in (Hasegawa-Johnson et al, 2005) for thefour speakers with data from one speaker for testingand from the other three for training.
For the BUcorpus, f2b speaker was always used in the trainingset since it contains the most data.
In addition toperforming experiments on all the utterances in BUcorpus, we also perform identical experiments on thetrain and test sets reported in (Chen et al, 2004)which is referred to as Hasegawa-Johnson et al set.4 Baseline ExperimentsWe present three baseline experiments.
One is sim-ply based on chance where the majority class label ispredicted.
The second is a baseline only for pitch ac-cents derived from the lexical stress obtained throughlook-up from a pronunciation lexicon labeled withstress.
Finally, the third and more concrete base-line is obtained through prosody detection in currentspeech synthesis systems.4.1 Prosody labels derived from lexicalstressPitch accents are usually carried by the stressed syl-lable in a particular word.
Lexicons with phonetictranscription and lexical stress are available in manylanguages.
Hence, one can use these lexical stressmarkers within the syllables and evaluate the corre-lation with pitch accents.
Eventhough the lexiconhas a closed vocabulary, letter-to-sound rules can bederived from it for unseen words.
For each word car-rying a pitch accent, we find the particular syllablewhere the pitch accent occurs from the manual anno-tation.
For the same syllable, we predict pitch accentbased on the presence or absence of a lexical stressmarker in the phonetic transcription.
The results arepresented in Table 3.4.2 Prosody labeling with Festival andAT&T Natural Voices R?
SpeechSynthesizerFestival (Black et al, 1998) and AT&T NaturalVoices R?
(NV) speech synthesizer (att, ) are twopublicly available speech synthesizers that have aprosody prediction module available.
We performedautomatic prosody labeling using the two synthesiz-ers to get a baseline.4.2.1 AT&T Natural Voices R?
SpeechSynthesizerThe AT&T NV R?
speech synthesizer is a halfphone speech synthesizer.
The toolkit acceptsan input text utterance and predicts appropriateToBI pitch accent and boundary tones for each of3Pitch accent Boundary toneCorpus Speaker Set Prediction Module Chance Accuracy Chance AccuracyLexical stress 54.33 72.64 - -Entire Set AT&T Natural Voices 54.33 81.51 81.14 89.10Festival 54.33 69.55 81.14 89.54Lexical stress 56.53 74.10 - -BU Hasegawa-Johnson et al set AT&T Natural Voices 56.53 81.73 82.88 89.67Festival 56.53 68.65 82.88 90.21Lexical stress 57.60 67.42 - -BDC Entire Set AT&T Natural Voices 57.60 68.49 88.90 84.90Festival 57.60 64.94 88.90 85.17Table 3: Classification results of pitch accents and boundary tones (in %) using Festival and AT&T NV R?
synthesizerthe selected units (in this case, a pair of phones)from the database.
We reverse mapped the se-lected half phone units to words, thus obtainingthe ToBI labels for each word in the input utter-ance.
The toolkit uses a rule-based procedure topredict the ToBI labels from lexical information.The pitch accent labels predicted by the toolkit areLaccent  {H?,L?,none} and the boundary tonesare Lbtone  {L-L%,H-H%,L-H%,none}.4.2.2 Festival Speech SynthesizerFestival (Black et al, 1998) is an open-source unitselection speech synthesizer.
The toolkit includesa CART-based prediction system that can predictToBI pitch accents and boundary tones for the inputtext utterance.
The pitch accent labels predicted bythe toolkit are Laccent  {H?,L+H?, !H?,none}and the boundary tones areLbtone  {L-L%,H-H%,L-H%,none}.
Theprosody labeling results obtained through both thespeech synthesis engines are presented in Table3.
The chance column in Table 3 is obtained bypredicting the most frequent label in the data set.In the next sections, we describe our proposedmaximum entropy based syntactic model and HMMbased acoustic-prosodic model for automatic prosodylabeling.5 Syntactic-prosodic ModelWe propose a maximum entropy approach to modelthe words, syntactic information and the prosodiclabels as a sequence.
We model the prediction prob-lem as a classification task as follows: given a se-quence of words wi in a sentence W = {w1, ?
?
?
, wn}and a prosodic label vocabulary (li  L), we needto predict the best prosodic label sequence L?
={l1, l2, ?
?
?
, ln}.
We approximate the conditionalprobability to be within a bounded n-gram context.Thus,L?
= argmaxLP (L|W,T, S) (4)?
argmaxLn?ip(li|wi+ki?k, ti+ki?k, si+ki?k) (5)where W = {w1, ?
?
?
, wn} is the word sequence andT = {t1, ?
?
?
, tn}, S = {s1, ?
?
?
, sn} are the corre-sponding part-of-speech and additional syntactic in-formation sequences.
The variable k controls thecontext.The BU corpus is automatically labeled (andhand-corrected) with part-of-speech (POS) tags.The POS inventory is the same as the Penn treebankwhich includes 47 POS tags: 22 open class categories,14 closed class categories and 11 punctuation labels.We also automatically tagged the utterances usingthe AT&T POS tagger.
The POS tags were mappedto function and content word categories 1 which wasadded as a discrete feature.
In addition to the POStags, we also annotate the utterance with Supertags(Bangalore and Joshi, 1999).
Supertags encapsulatepredicate-argument information in a local structure.They are composed with each other using substi-tution and adjunction operations of Tree-AdjoiningGrammars (TAGs) to derive a dependency analysisof an utterance and its predicate-argument structure.Even though there is a potential to exploit the de-pendency structure between supertags and prosodylabels as demonstrated in (Hirschberg and Rambow,2001), for this paper we use only the supertag labels.Finally, we generate one feature vector (?)
foreach word in the data set (with local contextual fea-tures).
The best prosodic label sequence is then,L?
= argmaxLn?iP (li|?)
(6)To estimate the conditional distribution P (li|?)
weuse the general technique of choosing the maximumentropy (maxent) distribution that estimates the av-erage of each feature over the training data (Bergeret al, 1996).
This can be written in terms of Gibbsdistribution parameterized with weights ?, where Vis the size of the prosodic label set.
Hence,P (li|?)
=e?li .?
?Vl=1 e?li .?
(7)1function and content word features were obtainedthrough a look-up table based on POS4k=3Corpus Speaker Set Syntactic features accent btonecorrect POS tags 84.75 91.39Entire Set AT&T POS + supertags 84.59 91.34BU Joint Model (w AT&T POS + supertags) 84.60 91.36correct POS tags 85.22 91.33Hasegawa-Johnson et al set AT&T POS + supertags 84.95 91.21Joint Model (w AT&T POS + supertags) 84.78 91.54BDC Entire Set AT&T POS + supertags 79.81 90.28Joint Model (w AT&T POS + supertags) 79.57 89.76Table 4: Classification results (%) of pitch accents and boundary tones for different syntactic representation (k = 3)We use the machine learning toolkit LLAMA(Haffner, 2006) to estimate the conditional distribu-tion using maxent.
LLAMA encodes multiclass max-ent as binary maxent to increase the training speedand to scale the method to large data sets.
Each ofthe V classes in the label set L is encoded as a bitvector such that, in the vector for class i, the ith bitis one and all other bits are zero.
Finally, V one-versus-other binary classifiers are used as follows.P (y|?)
= 1?
P (y?|?)
=e?y.?e?y.?
+ e?y?.?
(8)where ?y?
is the parameter vector for the anti-label y?.To compute P (li|?
), we use the class independenceassumption and require that yi = 1 and for all j 6=i, yj = 0.P (li|?)
= P (yi|?
)V?j 6=iP (yj |?)
(9)5.1 Joint Modeling of Accents andBoundary TonesProsodic prominence and phrasing can also beviewed as joint events occurring simultaneously.
Pre-vious work by (Wightman and Ostendorf, 1994) sug-gests that a joint labeling approach may be morebeneficial in prosody labeling.
In this scenario,we treat each word to have one of the four labelsli  L = {accent-btone, accent-none, none-btone, none-none}.
We trained the classifier onthe joint labels and then computed the error rates forindividual classes.
The results of prosody predictionusing the set of syntactic-prosodic features for k = 3is shown in Table 4.
The joint modeling approachprovides a marginal improvement in the boundarytone prediction but is slightly worse for pitch accentprediction.5.2 Supertagger performance onIntermediate Phrase boundariesPerceptual experiments have indicated that inter-annotator agreement for ToBI intermediate phraseboundaries is very low compared to full-intonationalboundaries (Syrdal and McGory, 2000).
Interme-diate phrasing is important in TTS applications tosynthesize appropriate short pauses to make the ut-terance sound natural.
The significance of syntacticfeatures in the boundary tone prediction promptedus to examine the effect of predicting intermediatephrase boundaries in isolation.
It is intuitive to ex-pect supertags to perform well in this task as theyessentially form a local dependency analysis on anutterance and provide an encoding of the syntacticphrasal information.
We performed this task as athree way classification where li  L = {btone, ip,none}.
The results of the classifier on IPs is shownin Table 5.Model Syntactic features IP accuracycorrect POS tags 83.25k=2 (bigram context) AT&T POS tags 83.32supertags 83.37correct POS tags 83.30k=3 (trigram context) AT&T POS tags 83.46supertags 83.74Table 5: Accuracy (in %) obtained by leave-one outspeaker validation using IPs as a separate class onentire speaker set6 Acoustic-prosodic modelWe propose two approaches to modeling theacoustic-prosodic features for prosody prediction.First, we propose a maximum entropy frameworksimilar to the syntactic model where we quantizethe acoustic features and model them as discretesequences.
Second, we use a more traditional ap-proach where we train continuous observation den-sity HMMs to represent pitch accents and bound-ary tones.
We first describe the features used in theacoustic modeling followed by a more detailed de-scription of the acoustic-prosodic model.6.1 Acoustic-prosodic featuresThe BU corpus contains the corresponding acoustic-prosodic feature file for each utterance.
The f0, RMSenergy (e) of the utterance along with features for5Pitch accent Boundary toneCorpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntaxEntire Set Maxent acoustic model 80.09 84.53 84.10 91.56HMM acoustic model 70.58 85.13 71.28 92.91BU Hasegawa-Johnson et al set Maxent acoustic model 80.12 84.84 82.70 91.76HMM acoustic model 71.42 86.01 73.43 93.09BDC Entire Set Maxent acoustic model 74.51 78.64 83.53 90.49Table 6: Classification results of pitch accents and boundary tones (in %) with acoustics only and acoustics+syntaxusing both our modelsdistinction between voiced/unvoiced segment, cross-correlation values at estimated f0 value and ratio offirst two cross correlation values are computed over10 msec frame intervals.
In our experiments, we usethese values rather than computing them explicitlywhich is straightforward with most audio toolkits.Both the energy and the f0 levels were normalizedwith speaker specific means and variances.
Deltaand acceleration coefficients were also computed foreach frame.
The final feature vector is 6-dimensionalcomprising of f0, ?f0, ?2f0, e, ?e, ?2e per frame.6.2 Maximum Entropy acoustic-prosodicmodelWe propose a maximum entropy modeling frame-work to model the continuous acoustic-prosodic ob-servation sequence as a discrete sequence throughthe means of quantization.
The quantized acousticstream is then used as a feature vector and the condi-tional probabilities are approximated by an n-grammodel.
This is equivalent to reducing the vocabu-lary of the acoustic-prosodic features and hence of-fers better estimates of the conditional probabilities.Such an n-gram model of quantized continuous fea-tures is similar to representing the set of featureswith a linear fit as done in the tilt intonational model(Taylor, 1998).The quantized acoustic-prosodic feature stream ismodeled with a maxent acoustic-prosodic model sim-ilar to the one described in section 5.
Finally, we ap-pend the syntactic and acoustic features to model thecombined stream with the maxent acoustic-syntacticmodel, where the objective criterion for maximiza-tion is Equation (1).
The pitch accent and bound-ary tone prediction accuracies for quantization per-formed by considering only the first decimal placeis reported in Table 6.
As expected, we found theclassification accuracy to drop with increasing num-ber of bins used in the quantization due to the smallamount of training data.6.3 HMM acoustic-prosodic modelWe also investigated the traditional HMM approachto model the high variability exhibited by theacoustic-prosodic features.
First, we trained sepa-rate context independent single state Gaussian mix-ture density HMMs for pitch accents and boundarytones in a generative framework.
The label sequencewas decoded using the viterbi algorithm.
Next, wetrained HMMs with 3 state left-to-right topologywith uniform segmentation.
The segmentations needto be uniform due to lack of an acoustic-prosodicmodel trained on the features pertinent to our taskto obtain forced segmentation.The final label sequence using the maximum en-tropy syntactic-prosodic model and the HMM basedacoustic-prosodic model was obtained by combin-ing the syntactic and acoustic probabilities shown inEquation (3).
The syntactic-prosodic maxent modeloutputs a posterior probability for each class perword.
We formed a lattice out of this structure andcomposed it with the lattice generated by the HMMacoustic-prosodic model.
The best path was chosenfrom the composed lattice through a Viterbi search.The acoustic-prosodic probability P (A|L,W ) wasraised by a power of ?
to adjust the weighting be-tween the acoustic and syntactic model.
The value of?
was chosen as 0.008 and 0.015 for pitch accent andboundary tone respectively, by tuning on the train-ing set.
The results of the acoustic-prosodic modeland the coupled model are shown in Table 6.7 DiscussionThe baseline experiment with lexical stress obtainedfrom a pronunciation lexicon for prediction of pitchaccent yields substantially higher accuracy thanchance.
This could be particularly useful in resource-limited languages where prosody labels are usuallynot available but one has access to a reasonable lex-icon with lexical stress markers.
Off-the-shelf speechsynthesizers like Festival and AT&T speech synthe-sizer perform reasonably well in pitch accent andboundary tone prediction.
AT&T speech synthesizerperforms better than Festival in pitch accent predic-tion and the latter performs better in boundary toneprediction.
This can be attributed to better rulesin the AT&T synthesizer for pitch accent prediction.Boundary tones are usually highly correlated withpunctuation and Festival seems to capture this well.However, both these synthesizers generate a high de-6gree of false alarms.Our syntactic-prosodic maximum entropy modelproposed in section 5 outperforms previously re-ported results on pitch accent and boundary toneclassification.
Much of the gain comes from the ro-bustness of the maximum entropy modeling in cap-turing the uncertainty in the classification task.
Con-sidering the inter-annotator agreement for ToBI la-bels is only about 81% for pitch accents and 93% forboundary tones, the maximum entropy framework isable to capture the uncertainty present in manual an-notation.
The supertag feature offers additional dis-criminative information over the part-of-speech tags(also as shown by (Hirschberg and Rambow, 2001).The maximum entropy acoustic-prosodic modeldiscussed in section 6.2 performs reasonably well inisolation.
This is a simple method and the quantiza-tion resolution can be adjusted based on the amountof data available for training.
However, the modeldoes not perform as well when combined with thesyntactic features.
We conjecture that the gener-alization provided by the acoustic HMM model iscomplementary to that provided by the maximumentropy model, resulting in better accuracy whencombined together as compared to that of a maxent-based acoustic and syntactic model.The weighted maximum entropy syntactic-prosodic model and HMM acoustic-prosodic modelperforms the best in pitch accent and boundary toneclassification.
The classification accuracies are asgood as the inter-annotator agreement for the ToBIlabels.
Our HMM acoustic-prosodic model is a gen-erative model and does not assume the knowledgeof word boundaries in predicting the prosodic labelsas in most approaches (Hirschberg, 1993; Wightmanand Ostendorf, 1994; Hasegawa-Johnson et al,2005).
This makes it possible to have true parallelprosody prediction during speech recognition.
Theweighted approach also offers flexibility in prosodylabeling for either speech synthesis or speech recog-nition.
While the syntactic-prosodic model wouldbe more discriminative for speech synthesis, theacoustic-prosodic model is more appropriate forspeech recognition.8 Conclusions and Future WorkIn this paper, we described a maximum entropymodeling framework for automatic prosody label-ing.
We presented two schemes for prosody label-ing that utilize the acoustic and syntactic informa-tion from the input utterance, a maximum entropymodel that models the acoustic-syntactic informa-tion as a sequence and the other that combines themaximum entropy syntactic-prosodic model and aHMM based acoustic-prosodic model.
We also usedenriched syntactic information in the form of su-pertags in addition to POS tags.
The supertagsprovide an improvement in both the pitch accentand boundary tone classification.
Especially, in thecase where the input utterance is automatically POStagged (and not hand-corrected), supertags providea marginal but definite improvement in prosody la-beling.
The maximum entropy syntactic-prosodicmodel alone resulted in pitch accent and bound-ary tone accuracies of 85.2% and 91.5% on trainingand test sets identical to (Chen et al, 2004).
Asfar as we know, these are the best results on theBU corpus using syntactic information alone and atrain-test split that does not contain the same speak-ers.
The acoustic-syntactic maximum entropy modelperforms better than its syntactic-prosodic counter-part for the boundary tone case but is slightly worsefor pitch accent scenario partly due to the approx-imation involved in quantization.
But these resultsare still better than the baseline results from out-of-the-box speech synthesizers.
Finally, our com-bined maximum entropy syntactic-prosodic modeland HMM acoustic-prosodic model performs the bestwith pitch accent and boundary tone labeling accu-racies of 86.0% and 93.1% respectively.As a continuation of our work, we are incorpo-rating our automatic prosody labeler in a speech-to-speech translation framework.
Typically, state-of-the-art speech translation systems have a sourcelanguage recognizer followed by a machine transla-tion system.
The translated text is then synthesizedin the target language with prosody predicted fromtext.
In this process, some of the critical prosodicinformation present in the source data is lost duringtranslation.
With reliable prosody labeling in thesource language, one can transfer the prosody to thetarget language (this is feasible for languages withphrase level correspondence).
The prosody labels bythemselves may or may not improve the translationaccuracy but they provide a framework where onecan obtain prosody labels in the target language fromthe speech signal rather than depending on a lexicalprosody prediction module in the target language.AcknowledgementsWe would like to thank Vincent Goffin, StephanKanthak, Patrick Haffner, Enrico Bocchieri for theirsupport with acoustic modeling tools.
We are alsothankful to Alistair Conkie, Yeon-Jun Kim, AnnSyrdal and Julia Hirschberg for their help and guid-ance with the synthesis components and ToBI label-ing standard.ReferencesP.
D. Agu?ero, J. Adell, and A. Bonafonte.
2006.Prosody generation for speech-to-speech transla-7tion.
In Proceedings of ICASSP, Toulouse, France,May.S.
Ananthakrishnan and S. Narayanan.
2005.
An au-tomatic prosody recognizer using a coupled multi-stream acoustic model and a syntactic-prosodiclanguage model.
In In Proceedings of ICASSP,Philadelphia, PA, March.AT&T Natural Voices speech synthesizer.http://www.naturalvoices.att.com.S.
Bangalore and A. K. Joshi.
1999.
Supertagging:An approach to almost parsing.
ComputationalLinguistics, 25(2), June.A.
Berger, S. D. Pietra, and V. D. Pietra.
1996.
Amaximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1):39?71.A.
W. Black, P. Taylor, and R. Caley.1998.
The Festival speech synthesis system.http://festvox.org/festival.J.
M. Brenier, D. Cer, and D. Jurafsky.
2005.
Thedetection of emphatic words using acoustic andlexical features.
In In Proceedings of Eurospeech.I.
Bulyko and M. Ostendorf.
2001.
Joint prosodyprediction and unit selection for concatenativespeech synthesis.
In Proc.
of ICASSP.K.
Chen, M. Hasegawa-Johnson, and A. Cohen.2004.
An automatic prosody labeling system usingANN-based syntactic-prosodic model and GMM-based acoustic-prosodic model.
In Proceedings ofICASSP.A.
Conkie, G. Riccardi, and R. C. Rose.
1999.Prosody recognition from speech utterances usingacoustic and linguistic based models of prosodicevents.
In Proc.
Eurospeech, pages 523?526, Bu-dapest, Hungary.M.
Gregory and Y. Altun.
2004.
Using conditionalrandom fields to predict pitch accent in conver-sational speech.
In 42nd Annual Meeting of theAssociation for Computational Linguistics (ACL).P.
Haffner.
2006.
Scaling large margin classifiers forspoken language understanding.
Speech Commu-nication, 48(iv):239?261.M.
Hasegawa-Johnson, K. Chen, J. Cole, S. Borys,S.
Kim, A. Cohen, T. Zhang, J. Choi, H. Kim,T.
Yoon, and S. Chavara.
2005.
Simultaneousrecognition of words and prosody in the bostonuniversity radio speech corpus.
Speech Communi-cation, 46:418?439.J.
Hirschberg and C. Nakatani.
1996.
A prosodicanalysis of discourse segments in direction-givingmonologues.
In Proceedings of the 34th confer-ence on Association for Computational Linguis-tics, pages 286?293.J.
Hirschberg and O. Rambow.
2001.
Learningprosodic features using a tree representation.
InProceedings of Eurospeech, pages 1175?1180, Aal-borg.J.
Hirschberg.
1993.
Pitch accent in context: Pre-dicting intonational prominence from text.
Artifi-cial Intelligence, 63(1-2).I.
Lehiste.
1970.
Suprasegmentals.
MIT Press, Cam-bridge, MA.X.
Ma, W. Zhang, Q. Shi, W. Zhu, and L. Shen.2003.
Automatic prosody labeling using bothtext and acoustic information.
In Proceedings ofICASSP, volume 1, pages 516?519, April.E.
No?th, A. Batliner, A. Kie?ling, R. Kompe, andH.
Niemann.
2000.
VERBMOBIL: The use ofprosody in the linguistic components of a speechunderstanding system.
IEEE Transactions onSpeech and Audio processing, 8(5):519?532.M.
Ostendorf, P. J.
Price, and S. Shattuck-Hufnagel.1995.
The Boston University Radio News Corpus.Technical Report ECS-95-001, Boston University,March.K.
Ross and M. Ostendorf.
1996.
Prediction of ab-stract prosodic labels for speech synthesis.
Com-puter Speech and Language, 10:155?185, Oct.P.
Shimei and K. McKeown.
1999.
Word infor-mativeness and automatic pitch accent modeling.In In Proceedings of EMNLP/VLC, College Park,Maryland.K.
Silverman, M. Beckman, J. Pitrelli, M. Osten-dorf, C. Wightman, P. Price, J. Pierrehumbert,and J. Hirschberg.
1992.
ToBI: A standard for la-beling English prosody.
In Proceedings of ICSLP,pages 867?870.X.
Sun.
2002.
Pitch accent prediction using ensem-ble machine learning.
In Proc.
of ICSLP.A.
K. Syrdal and J. McGory.
2000.
Inter-transcriberreliability of tobi prosodic labeling.
In Proc.
IC-SLP, pages 235?238, Beijing, China.P.
Taylor.
1998.
The tilt intonation model.
In Proc.ICSLP, volume 4, pages 1383?1386.C.
W. Wightman and M. Ostendorf.
1994.
Auto-matic labeling of prosodic patterns.
IEEE Trans-actions on Speech and Audio Processing, 2(3):469?481.8
