Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 759?764,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsEM Decipherment for Large VocabulariesMalte Nuhn and Hermann NeyHuman Language Technology and Pattern RecognitionComputer Science Department, RWTH Aachen University, Aachen, Germany<surname>@cs.rwth-aachen.deAbstractThis paper addresses the problem of EM-based decipherment for large vocabular-ies.
Here, decipherment is essentiallya tagging problem: Every cipher tokenis tagged with some plaintext type.
Aswith other tagging problems, this one canbe treated as a Hidden Markov Model(HMM), only here, the vocabularies arelarge, so the usual O(NV2) exact EM ap-proach is infeasible.
When faced withthis situation, many people turn to sam-pling.
However, we propose to use a typeof approximate EM and show that it workswell.
The basic idea is to collect fractionalcounts only over a small subset of linksin the forward-backward lattice.
The sub-set is different for each iteration of EM.One option is to use beam search to do thesubsetting.
The second method restrictsthe successor words that are looked at, foreach hypothesis.
It does this by consultingpre-computed tables of likely n-grams andlikely substitutions.1 IntroductionThe decipherment of probabilistic substitution ci-phers (ciphers in which each plaintext token canbe substituted by any cipher token, following adistribution p(f |e), cf.
Table 2) can be seen asan important step towards decipherment for MT.This problem has not been studied explicitly be-fore.
Scaling to larger vocabularies for proba-bilistic substitution ciphers decipherment is a dif-ficult problem: The algorithms for 1:1 or homo-phonic substitution ciphers are not applicable, andstandard algorithms like EM training become in-tractable when vocabulary sizes go beyond a fewhundred words.
In this paper we present an effi-cient EM based training procedure for probabilis-tic substitution ciphers which provides high deci-pherment accuracies while having low computa-tional requirements.
The proposed approach al-lows using high order n-gram language models,and is scalable to large vocabulary sizes.
We showimprovements in decipherment accuracy in a va-riety of experiments (including MT) while beingcomputationally more efficient than previous pub-lished work on EM-based decipherment.2 Related WorkSeveral methods exist for deciphering 1:1 substi-tution ciphers: Ravi and Knight (2008) solve 1:1substitution ciphers by formulating the decipher-ment problem as an integer linear program.
Cor-lett and Penn (2010) solve the same problem us-ing A?search.
Nuhn et al (2013) present a beamsearch approach that scales to large vocabularyand high order language models.
Even though be-ing successful, these algorithms are not applicableto probabilistic substitution ciphers, or any of itsextensions as they occur in decipherment for ma-chine translation.EM training for probabilistic ciphers was firstcovered in Ravi and Knight (2011).
Nuhn et al(2012) have given an approximation to exact EMtraining using context vectors, allowing to train-ing models even for larger vocabulary sizes.
Ravi(2013) report results on the OPUS subtitle corpususing an elaborate hash sampling technique, basedon n-gram language models and context vectors,that is computationally very efficient.Conventional beam search is a well studiedtopic: Huang et al (1992) present beam search forautomatic speech recognition, using fine-grainedpruning procedures.
Similarly, Young and Young(1994) present an HMM toolkit, including prunedforward-backward EM training.
Pal et al (2006)use beam search for training of CRFs.759Method Publications ComplexityEM Full (Knight et al, 2006), (Ravi and Knight, 2011) O(NVn)EM Fixed Candidates (Nuhn et al, 2012) O(N)EM Beam This Work O(NV )EM Lookahead This Work O(N)Table 1: Different approximations to exact EM training for decipherment.
N is the cipher sequencelength, V the size of the target vocabulary, and n the order of the language model.The main contribution of this work is the pre-selection beam search that?to the best of ourknowledge?was not known in literature before,and serves as an important step to applying EMtraining to the large vocabulary deciphermentproblem.
Table 1 gives an overview of the EMbased methods.
More details are given in Sec-tion 3.2.3 Probabilistic Substitution CiphersWe define probabilistic substitutions ciphers us-ing the following generative story for ciphertextsequences fN1:1.
Stochastically generate a plaintext sequenceeN1according to a bigram1language model.2.
For each plaintext token enchoose a substi-tution fnwith probability P (fn|en, ?
).This generative story corresponds to the modelp(eN1, fN1, ?)
= p(eN1) ?
p(fN1|eN1, ?)
, (1)with the zero-order membership modelp(fN1|eN1, ?)
=N?n=1plex(fn|en, ?)
(2)with parameters p(f |e, ?)
?
?f |eand normaliza-tion constraints ?e?f?f |e= 1, and first-orderplaintext sequence modelP (eN1) =N?n=1pLM(en|en?1) .
(3)Thus, the probabilistic substitution cipher can beseen as a Hidden Markov Model.
Table 2 gives anoverview over the model.
We want to find thoseparameters ?
that maximize the marginal distribu-tion p(fN1|?):?
= argmax??????
[eN1]p(fN1, eN1|??)???
(4)1This can be generalized to n-gram language models.After we obtained the parameters ?
wecan obtain eN1as the Viterbi decodingargmaxeN1{p(eN1|fN1, ?
)}.3.1 Exact EM trainingIn the decipherment setting, we are given the ob-served ciphertext fN1and the model p(fN1|eN1, ?
)that explains how the observed ciphertext has beengenerated given a latent plaintext eN1.
Marginaliz-ing the unknown eN1, we would like to obtain themaximum likelihood estimate of ?
as specified inEquation 4.
We iteratively compute the maximumlikelihood estimate by applying the EM algorithm(Dempster et al, 1977):?
?f |e=?n:fn=fpn(e|fN1, ?
)?f?n:fn=fpn(e|fN1, ?
)(5)withpn(e|fN1, ?)
=?
[eN1:en=e]p(eN1|fN1, ?)
(6)being the posterior probability of observing theplaintext symbol e at position n given the cipher-text sequence fN1and the current parameters ?.pn(e|fN1, ?)
can be efficiently computed using theforward-backward algorithm.3.2 Approximations to EM-TrainingThe computational complexity of EM trainingstems from the sum?
[eN1:en=e]contained in theposterior pn(e|fN1, ?).
However, we can approx-imate this sum (and hope that the EM trainingprocedure is still working) by only evaluating thedominating terms, i.e.
we only evaluate the sumfor sequences eN1that have the largest contribu-tions to?[eN1:en=e].
Note that due to this approxi-mation, the new parameter estimates in Equation 5can become zero.
This is a critical issue, sincepairs (e, f) with p(f |e) = 0 cannot recover from760Sequence of cipher tokens : fN1= f1, .
.
.
, fNSequence of plaintext tokens : eN1= e1, .
.
.
, eNJoint probability : p(fN1, eN1|?)
= p(eN1) ?
p(fN1|eN1, ?
)Language model : p(eN1) =N?n=1pLM(en|en?1)Membership probabilities : p(fN1|eN1, ?)
=N?n=1plex(fn|en, ?
)Paramater Set : ?
= {?f |e}, p(f |e, ?)
= ?f |eNormalization : ?e :?f?f |e= 1Probability of cipher sequence : p(fN1|?)
=?
[eN1]p(fN1, eN1|?
)Table 2: Definition of the probabilistic substitution cipher model.
In contrast to simple or homophonicsubstitution ciphers, each plaintext token can be substituted by multiple cipher text tokens.
The parameter?f |erepresents the probability of substituting token e with token f .acquiring zero probability in some early iteration.In order to allow the lexicon to recover from thesezeros, we use a smoothed lexicon ?plex(f |e) =?plex(f |e) + (1 ?
?
)/|Vf| with ?
= 0.9 whenconducting the E-Step.3.2.1 Beam SearchInstead of evaluating the sum for terms with theexact largest contributions, we restrict ourselves toterms that are likely to have a large contribution tothe sum, dropping any guarantees about the actualcontribution of these terms.Beam search is a well known algorithm relatedto this idea: We build up sequences ec1with grow-ing cardinality c. For each cardinality, only a setof the B most promising hypotheses is kept.
Thenfor each active hypothesis of cardinality c, all pos-sible extensions with substitutions fc+1?
ec+1are explored.
Then in turn only the best B out ofthe resulting B ?
Vemany hypotheses are kept andthe algorithm continues with the next cardinality.Reaching the full cardinality N , the algorithm ex-plored B ?N ?
Vemany hypotheses, resulting in acomplexity of O(BNVe).Even though EM training using beam searchworks well, it still suffers from exploring all Vepossible extensions for each active hypothesis, andthus scaling linearly with the vocabulary size.
Dueto that, standard beam search EM training is tooslow to be used in the decipherment setting.3.2.2 Preselection SearchInstead of evaluating all substitutions fc+1?ec+1?
Ve, this algorithm only expands a fixednumber of candidates: For a hypothesis ending ina language model state ?, we only look at BLMmany successor words ec+1with the highest LMprobability pLM(ec+1|?)
and at Blexmany suc-cessor words ec+1with the highest lexical prob-ability plex(fc+1|ec+1).
Altogether, for each hy-pothesis we only look at (BLM+Blex) many suc-cessor states.
Then, just like in the standard beamsearch approach, we prune all explored new hy-potheses and continue with the pruned set of Bmany hypotheses.
Thus, for a cipher of length Nwe only explore N ?
B ?
(BLM+ Blex) many hy-potheses.2Intuitively speaking, our approach solves theEM training problem for decipherment using largevocabularies by focusing only on those substitu-tions that either seem likely due to the languagemodel (?What word is likely to follow the cur-rent partial decipherment??)
or due to the lexiconmodel (?Based on my knowledge about the cur-rent cipher token, what is the most likely substitu-tion??
).In order to efficiently find the maximizing e forpLM(e|?)
and plex(f |e), we build a lookup ta-ble that contains for each language model state ?the BLMbest successor words e, and a separatelookup table that contains for each source word fthe Blexhighest scoring tokens e. The languagemodel lookup table remains constant during all it-erations, while the lexicon lookup table needs tobe updated between each iteration.Note that the size of the LM lookup table scaleslinearly with the number of language model states.Thus the memory requirements for the lookup ta-2We always use B = 100, Blex= 5, and BLM= 50.761f1 f2 f3 f4 f5e5e4e3e2e1Beam Search Preselection SearchFull Searchf6...startVocabSentenceFigure 1: Illustration of the search space explored by full search, beam search, and preselection search.Full search keeps all possible hypotheses at cardinality c and explores all possible substitutions at (c+1).Beam search only keeps the B most promising hypotheses and then selects the best new hypotheses forcardinality (c+ 1) from all possible substitutions.
Preselection search keeps only the B best hypothesesfor every cardinality c and only looks at the (Blex+ BLM) most promising substitutions for cardinality(c+ 1) based on the current lexicon (Blexdashed lines) and language model (BLMsolid lines).Name Lang.
Sent.
Words Voc.VERBMOBIL English 27,862 294,902 3,723OPUSSpanish 13,181 39,185 562English 19,770 61,835 411Table 3: Statistics of the copora used in this pa-per: The VERBMOBIL corpus is used to conductexperiments on simple substitution ciphers, whilethe OPUS corpus is used in our Machine Transla-tion experiments.ble do not form a practical problem of our ap-proach.
Figure 1 illustrates full search, beamsearch, and our proposed method.4 Experimental EvaluationWe first show experiments for data in which theunderlying model is an actual 1:1 substitution ci-pher.
In this case, we report the word accuracyof the final decipherment.
We then show experi-ments for a simple machine translation task.
Herewe report translation quality in BLEU.
The cor-pora used in this paper are shown in Table 3.4.1 Simple Substitution CiphersIn this set of experiments, we compare the exactEM training to the approximations presented inthis paper.
We use the English side of the German-English VERBMOBIL corpus (Wahlster, 2000) toconstruct a word substitution cipher, by substitut-ing every word type with a unique number.
In or-der to have a non-parallel setup, we train languageVocab LM Method Acc.
[%] Time[h]200 2 exact 97.19 224.88200 2 beam 98.87 9.04200 2 presel.
98.50 4.14500 2 beam 92.12 24.27500 2 presel.
92.16 4.703 661 3 beam 91.16 302.813 661 3 presel.
90.92 19.683 661 4 presel.
92.14 23.72Table 4: Results for simple substitution ciphersbased on the VERBMOBIL corpus using exact,beam, and preselection EM.
Exact EM is nottractable for vocabulary sizes above 200.models of order 2, 3 and 4 on the first half of thecorpus and use the second half as ciphertext.
Ta-ble 4 shows the results of our experiments.Since exact EM is not tractable for vocabularysizes beyond 200 words, we train word classes onthe whole corpus and map the words to classes(consistent along the first and second half of thecorpus).
By doing this, we create new simple sub-stitution ciphers with smaller vocabularies of size200 and 500.
For the smallest setup, we can di-rectly compare all three EM variants.
We also in-clude experiments on the original corpus with vo-cabulary size of 3661.
When comparing exact EMtraining with beam- and preselection EM training,the first thing we notice is that it takes about 20times longer to run the exact EM training thantraining with beam EM, and about 50 times longerthan the preselection EM training.
Interestingly,762Model Method BLEU [%] Runtime2-gram Exact EM(Ravi and Knight, 2011) 15.3 850.0hwhole segment lm Exact EM(Ravi and Knight, 2011) 19.3 850.0h2-gram Preselection EM (This work) 15.7 1.8h3-gram Preselection EM (This work) 19.5 1.9hTable 5: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) onthe Spanish/English OPUS corpus using only non-parallel corpora for training.the accuracy of the approximations to exact EMtraining is better than that of the exact EM train-ing.
Even though this needs further investigation,it is clear that the pruned versions of EM trainingfind sparser distributions plex(f |e): This is desir-able in this set of experiments, and could be thereason for improved performance.For larger vocabularies, exact EM training is nottractable anymore.
We thus constrain ourselves torunning experiments with beam and preselectionEM training only.
Here we can see that the runtimeof the preselection search is roughly the same aswhen running on a smaller vocabulary, while thebeam search runtime scales almost linearly withthe vocabulary size.
For the full vocabulary of3661 words, preselection EM using a 4-gram LMneeds less than 7% of the time of beam EM with a3-gram LM and performs by 1% better in symbolaccuracy.To summarize: Beam search EM is an or-der of magnitude faster than exact EM trainingwhile even increasing decipherment accuracy.
Ournew preselection search method is in turn or-ders of magnitudes faster than beam search EMwhile even being able to outperform exact EM andbeam EM by using higher order language mod-els.
We were thus able to scale the EM deci-pherment to larger vocabularies of several thou-sand words.
The runtime behavior is also consis-tent with the computational complexity discussedin Section 3.2.4.2 Machine TranslationWe show that our algorithm is directly applicableto the decipherment problem for machine transla-tion.
We use the same simplified translation modelas presented by Ravi and Knight (2011).
Becausethis translation model allows insertions and dele-tions, hypotheses of different cardinalities coex-ist during search.
We extend our search approachsuch that pruning is done for each cardinality sep-arately.
Other than that, we use the same pres-election search procedure as used for the simplesubstitution cipher task.We run experiments on the opus corpus as pre-sented in (Tiedemann, 2009).
Table 5 shows pre-viously published results using EM together withthe results of our new method:(Ravi and Knight, 2011) is the only publicationthat reports results using exact EM training andonly n-gram language models on the target side:It has an estimated runtime of 850h.
All otherpublished results (using EM training and Bayesianinference) use context vectors as an additionalsource of information: This might be an explana-tion why Nuhn et al (2012) and Ravi (2013) areable to outperform exact EM training as reportedby Ravi and Knight (2011).
(Ravi, 2013) reportsthe most efficient method so far: It only consumesabout 3h of computation time.
However, as men-tioned before, those results are not directly compa-rable to our work, since they use additional contextinformation on the target side.Our algorithm clearly outperforms the exactEM training in run time, and even slighlty im-proves performance in BLEU.
Similar to the sim-ple substitution case, the improved performancemight be caused by inferring a sparser distributionplex(f |e).
However, this requires further investi-gation.5 ConclusionWe have shown a conceptually consistent and easyto implement EM based training method for deci-pherment that outperforms exact and beam searchEM training for simple substitution ciphers anddecipherment for machine translation, while re-ducing training time to a fraction of exact andbeam EM.
We also point out that the preselectionmethod presented in this paper is not restricted toword based translation models and can also be ap-plied to phrase based translation models.763ReferencesEric Corlett and Gerald Penn.
2010.
An exact A*method for deciphering letter-substitution ciphers.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 1040?1047, Uppsala, Sweden, July.
The As-sociation for Computer Linguistics.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the Royal Sta-tistical Society, B, 39.Xuedong Huang, Fileno Alleva, Hsiao wuen Hon, Meiyuh Hwang, and Ronald Rosenfeld.
1992.
Thesphinx-ii speech recognition system: An overview.Computer, Speech and Language, 7:137?148.Kevin Knight, Anish Nair, Nishit Rathod, and KenjiYamada.
2006.
Unsupervised Analysis for De-cipherment Problems.
In Proceedings of theCOLING/ACL on Main conference poster sessions,COLING-ACL ?06, pages 499?506, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Malte Nuhn, Arne Mauser, and Hermann Ney.
2012.Deciphering foreign language by combining lan-guage models and context vectors.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (ACL), pages 156?164,Jeju, Republic of Korea, July.
Association for Com-putational Linguistics.Malte Nuhn, Julian Schamper, and Hermann Ney.2013.
Beam search for solving substitution ciphers.In Annual Meeting of the Assoc.
for ComputationalLinguistics, pages 1569?1576, Sofia, Bulgaria, Au-gust.Chris Pal, Charles Sutton, and Andrew McCallum.2006.
Sparse forward-backward using minimum di-vergence beams for fast training of conditional ran-dom fields.
In International Conference on Acous-tics, Speech, and Signal Processing (ICASSP).Sujith Ravi and Kevin Knight.
2008.
Attacking de-cipherment problems optimally with low-order n-gram models.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 812?819, Honolulu, Hawaii.
Asso-ciation for Computational Linguistics.Sujith Ravi and Kevin Knight.
2011.
Decipheringforeign language.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies (ACL-HLT), pages 12?21, Portland, Oregon, USA, June.Association for Computational Linguistics.Sujith Ravi.
2013.
Scalable decipherment for ma-chine translation via hash sampling.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics (ACL), pages 362?371,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.J?org Tiedemann.
2009.
News from OPUS - A col-lection of multilingual parallel corpora with toolsand interfaces.
In N. Nicolov, K. Bontcheva,G.
Angelova, and R. Mitkov, editors, RecentAdvances in Natural Language Processing, vol-ume V, pages 237?248.
John Benjamins, Amster-dam/Philadelphia, Borovets, Bulgaria.Wolfgang Wahlster, editor.
2000.
Verbmobil: Foun-dations of speech-to-speech translations.
Springer-Verlag, Berlin.S.J.
Young and Sj Young.
1994.
The htk hiddenmarkov model toolkit: Design and philosophy.
En-tropic Cambridge Research Laboratory, Ltd, 2:2?44.764
