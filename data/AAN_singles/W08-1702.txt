Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 9?16Manchester, August 2008Making Speech Look Like Textin the Regulus Development EnvironmentElisabeth Kron3 St Margarets Road, Cambridge CB3 0LT, Englandelisabethkron@yahoo.co.ukManny Rayner, Marianne Santaholma, Pierrette Bouillon, Agnes LisowskaUniversity of Geneva, TIM/ISSCO, 40 bvd du Pont-d?ArveCH-1211 Geneva 4, SwitzerlandEmmanuel.Rayner@issco.unige.chMarianne.Santaholma@eti.unige.chPierrette.Bouillon@issco.unige.chAgnes.Lisowska@issco.unige.chAbstractWe present an overview of the de-velopment environment for Regulus, anOpen Source platform for construction ofgrammar-based speech-enabled systems,focussing on recent work whose goal hasbeen to introduce uniformity between textand speech views of Regulus-based appli-cations.
We argue the advantages of be-ing able to switch quickly between text andspeech modalities in interactive and offlinetesting, and describe how the new func-tionalities enable rapid prototyping of spo-ken dialogue systems and speech transla-tors.1 IntroductionSex is not love, as Madonna points out at the be-ginning of her 1992 book Sex, and love is notsex.
None the less, even people who agree withMadonna often find it convenient to pretend thatthese two concepts are synonymous, or at leastclosely related.
Similarly, although text is notspeech, and speech is not text, it is often conve-nient to pretend that they are both just different as-pects of the same thing.In this paper, we will explore the similarities anddifferences between text and speech, in the con-crete setting of Regulus, a development environ-ment for grammar based spoken dialogue systems.Our basic goal will be to make text and speechprocessing as similar as possible from the pointof view of the developer.
Specifically, we arrangec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.things so that the developer is able to develop hersystem using a text view; she will write text-basedrules, and initially test the system using text in-put and output.
At any point, she will be able toswitch to a speech view, compiling the text-basedprocessing rules into corresponding speech-basedversions, and test the resulting speech-based sys-tem using speech input and output.Paradoxically, the reason why it is so importantto be able to switch seamlessly between text andspeech viewpoints is that text and speech are infact not the same.
For example, a pervasive prob-lem in speech recognition is that of easily confus-able pairs of words.
This type of problem is of-ten apparent after just a few minutes when runningthe system in speech mode (the recogniser keepsrecognising one word as the other), but is invis-ible in text mode.
More subtly, some grammarproblems can be obvious in text mode, but hardto see in speech mode.
For instance, articles like?the?
and ?a?
are short, and usually pronouncedunstressed, which means that recognisers can bereasonably forgiving about whether or not to hy-pothesise them when they are required or not re-quired by the recognition grammar.
In text mode, itwill immediately be clear if the grammar requiresan article in a given NP context: incorrect vari-ants will fail to parse.
In speech mode, the symp-toms are far less obvious, and typically amount tono more than a degradation in recognition perfor-mance.The rest of the paper is structured as follows.Sections 2 and 3 provide background on the Reg-ulus platform and development cycle respectively.Section 4 describes speech and text support in theinteractive development environment, and 5 de-scribes how the framework simplifies the task of9switching between modalities in regression testing.Section 6 concludes.2 The Regulus platformThe Regulus platform is a comprehensive toolkitfor developing grammar-based speech-enabledsystems that can be run on the commercially avail-able Nuance recognition environment.
The plat-form has been developed by an Open Source con-sortium, the main partners of which have beenNASA Ames Research Center and Geneva Univer-sity, and is freely available for download from theSourceForge website1.
In terms of ideas (thoughnot code), Regulus is a descendent of SRI Inter-national?s CLE and Gemini platforms (Alshawi,1992; Dowding et al, 1993); other related systemsare LKB (Copestake, 2002), XLE (Crouch et al,2008) and UNIANCE (Bos, 2002).Regulus has already been used to build sev-eral large applications.
Prominent examplesare Geneva University?s MedSLT medical speechtranslator (Bouillon et al, 2005), NASA?s Clarissaprocedure browser (Rayner et al, 2005) and FordResearch?s experimental SDS in-car spoken dia-logue system, which was awarded first prize atthe 2007 Ford internal demo fair.
Regulus is de-scribed at length in (Rayner et al, 2006), the firsthalf of which consists of an extended tutorial in-troduction.
The release includes a command-linedevelopment environment, extensive online docu-mentation, and several example applications.The core functionality offered by Regulus iscompilation of typed unification grammars intoparsers, generators, and Nuance-formatted CFGlanguage models, and hence also into Nuancerecognition packages.
These recognition packagesproduced by Regulus can be invoked through theRegulus SpeechServer (?Regserver?
), which pro-vides an interface to the underlying Nuance recog-nition engine.
The value added by the Regserveris to provide a view of the recognition processbased on the Regulus unification grammar frame-work.
In particular, recognition results, originallyproduced in the Nuance recognition platform?s in-ternal format, are reformatted into the semantic no-tation used by the Regulus grammar formalism.There is extensive support within the Regulustoolkit for development of both speech translationand spoken dialogue applications.
Spoken dia-1http://sourceforge.net/projects/regulus/logue applications (Rayner et al, 2006, Chapter 5)use a rule-based side-effect free state update modelsimilar in spirit to that described in (Larsson andTraum, 2000).
Very briefly, there are three types ofrules: state update rules, input management rules,and output management rules.
State update rulestake as input the current state, and a ?dialoguemove?
; they produce as output a new state, and an?abstract action?.
Dialogue moves are abstract rep-resentations of system inputs; these inputs can ei-ther be logical forms produced by the grammar, ornon-speech inputs (for example, mouse-clicks in aGUI).
Similarly, abstract actions are, as the namesuggests, abstract representations of the concreteactions the dialogue system will perform, for ex-ample speaking or updating a visual display.
Inputmanagement rules map system inputs to dialoguemoves; output management rules map abstract ac-tions to system outputs.Speech translation applications are also rule-based, using an interlingua model (Rayner et al,2006, Chapter 6).
The developer writes a secondgrammar for the target language, using Regulustools to compile it into a generator; mappings fromsource representation to interlingua, and from in-terlingua to target representation, are defined bysets of translation rules.
The interlingua itself isspecified using a third Regulus grammar (Bouillonet al, 2008).To summarise, the core of a Regulus applicationconsists of several different linguistically orientedrule-sets, some of which can be interpreted in ei-ther a text or a speech modality, and all of whichneed to interact correctly together.
In the next sec-tion, we describe how this determines the nature ofthe Regulus development cycle.3 The Regulus development cycleSmall unification grammars can be compiled di-rectly into executable forms.
The central ideaof Regulus, however, is to base as much ofthe development work as possible on large,domain-independent, linguistically motivated re-source grammars.
A resource grammar for En-glish is available from the Regulus website; similargrammars for several other languages have beendeveloped under the MedSLT project at GenevaUniversity, and can be downloaded from the Med-SLT SourceForge website2.
Regulus contains2http://sourceforge.net/projects/medslt10an extensive set of tools that permit specialiseddomain-specific grammars to be extracted from thelarger resource grammars, using example-basedmethods driven by small corpora (Rayner et al,2006, Chapter 7).
At the beginning of a project,these corpora can consist of just a few dozen exam-ples; for a mature application, they will typicallyhave grown to something between a few hundredand a couple of thousand sentences.
Specialisedgrammars can be compiled by Regulus into effi-cient recognisers and generators.As should be apparent from the preceding de-scription, the Regulus architecture is designed toempower linguists to the maximum possible ex-tent, in terms of increasing their ability directlyto build speech enabled systems; the greater partof the core development teams in the large Reg-ulus projects mentioned in Section 1 have indeedcome from linguistics backgrounds.
Experiencewith Regulus has however shown that linguists arenot quite as autonomous as they are meant to be,and in particular are reluctant to work directly withthe speech view of the application.
There are sev-eral reasons.First, non-toy Regulus projects require a rangeof competences, including both software engineer-ing and linguistics.
In practice, linguist rule-writers have not been able to test their rules inthe speech view without writing glue code, scripts,and other infrastructure required to tie together thevarious generated components.
These are not nec-essarily things that they want to spend their timedoing.
The consequence can easily be that the lin-guists end up working exclusively in the text view,and over-refine the text versions of the rule-sets.From a project management viewpoint, this resultsin bad prioritisation decisions, since there are morepressing issues to address in the speech view.A second reason why linguist rule-writers havebeen unhappy working in the speech view is thelack of reproducibility associated with speech in-put.
One can type ?John loves Mary?
into a text-processing system any number of times, and ex-pect to get the same result.
It is much less reason-able to expect to get the same result each time ifone says ?John loves Mary?
to a speech recogniser.Often, anomalous results occur, but cannot be de-bugged in a systematic fashion, leading to generalfrustration.
The result, once again, is that linguistshave preferred to stick with the text view, wherethey feel at home.Yet another reason why rule-writers tend tolimit themselves to the text view is simply thelarge number of top-level commands and inter-mediate compilation results.
The current Reguluscommand-line environment includes over 110 dif-ferent commands, and compilation from the initialresource grammar to the final Nuance recognitionpackage involves creating a sequence of five com-pilation steps, each of which requires the outputcreated by the preceding one.
This makes it diffi-cult for novice users to get their bearings, and in-creases their cognitive load.
Additionally, once thecommands for the text view have been mastered,there is a certain temptation to consider that theseare enough, since the text and speech views canreasonably be perceived as fairly similar.In the next two sections, we describe an en-hanced development environment for Regulus,which addresses the key problems we have justsketched.
From the point of view of the linguistrule-writer, we want speech-based development tofeel more like text-based development.4 Speech and text in the onlinedevelopment environmentThe Regulus GUI (Kron et al, 2007) is intendedas a complete redesign of the development envi-ronment, which simultaneously attacks all of thecentral issues.
Commands are organised in a struc-tured set of functionality-based windows, each ofwhich has an appropriate set of drop-down menus.Following normal GUI design practice (Dix et al,1998, Chapters 3 and 4); (Jacko and Sears, 2003,Chapter 13), only currently meaningful commandsare executable in each menu, with the others showngreyed out.Both compile-time and run-time speech-relatedfunctionality can be invoked directly from thecommand menus, with no need for external scripts,Makefiles or glue code.
Focussing for the momenton the specific case of developing a speech transla-tion application, the rule-writer will initially writeand debug her rules in text mode.
She will be ableto manipulate grammar rules and derivation treesusing the Stepper window (Figure 1; cf.
also (Kronet al, 2007)), and load and test translation rulesin the Translate window (Figure 2).
As soon asthe grammar is consistent, it can at any point becompiled into a Nuance recognition package us-ing the command menus.
The resulting recogniser,together with other speech resources (license man-11Figure 1: Using the Stepper window to browse trees in the Toy1 grammar from (Rayner et al, 2006,Chapter 4).
The upper left window shows the analysis tree for ?switch on the light in the kitchen?
; thelower left window shows one of the subtrees created by cutting the first tree at the higher NP node.
Cutsubtrees can be recombined for debugging purposes (Kron et al, 2007).Figure 2: Using the Translate window to test the toy English ?
French translation application from(Rayner et al, 2006, Chapter 6).
The to- and from-interlingua rules used in the example are shown in thetwo pop-up windows at the top of the figure.12regulus_config(regulus_grammar,[toy1_grammars(toy1_declarations),toy1_grammars(toy1_rules),toy1_grammars(toy1_lexicon)]).regulus_config(top_level_cat, ?.MAIN?
).regulus_config(nuance_grammar, toy1_runtime(recogniser)).regulus_config(to_interlingua_rules,toy1_prolog(?eng_to_interlingua.pl?)).regulus_config(from_interlingua_rules,toy1_prolog(?interlingua_to_fre.pl?
)).regulus_config(generation_rules, toy1_runtime(?generator.pl?)).regulus_config(nuance_language_pack,?English.America?
).regulus_config(nuance_compile_params, [?-auto_pron?, ?-dont_flatten?
]).regulus_config(translation_rec_params,[package=toy1_runtime(recogniser), grammar=?.MAIN?
]).regulus_config(tts_command,?vocalizer -num_channels 1 -voice juliedeschamps -voices_from_disk?
).Figure 3: Config file for a toy English ?
French speech translation application, showing items relevantto the speech view.
Some declarations have been omitted for expositional reasons.ager, TTS engine etc), can then be started using asingle menu command.In accordance with the usual Regulus designphilosophy of declaring all the resources associ-ated with a given application in its config file, thespeech resources are also specified here.
Figure 3shows part of the config file for a toy translationapplication, in particular listing all the declara-tions relevant to the speech view.
If we needed tochange the speech resources, this would be donejust by modifying the last four lines.
For example,the config file as shown specifies construction ofa recogniser using acoustic models appropriate toAmerican English.
We could change this to BritishEnglish by replacing the entryregulus_config(nuance_language_pack,?English.America?).withregulus_config(nuance_language_pack,?English.UK?
).When the speech resources have been loaded,the Translate window can take input equally easilyin text or speech mode; the Translate button pro-cesses written text from the input pane, while theRecognise button asks for spoken input.
In eachcase, the input is passed through the same process-ing stages of source-to-interlingua and interlingua-to-target translation, followed by target-languagegeneration.
If a TTS engine or a set of recordedtarget language wavfiles is specified, they are usedto realise the final result in spoken form (Figure 4).Every spoken utterance submitted to recognitionis logged as a SPHERE-headed wavfile, in a time-stamped directory started at the beginning of thecurrent session; this directory also contains a meta-data file, which associates each recorded wavfilewith the recognition result it produced.
The Trans-late window?s History menu is constructed usingthe meta-data file, and allows the user to select anyrecorded utterance, and re-run it through the sys-tem as though it were a new speech input.
Theconsequence is that speech input becomes just asreproducible as text, with corresponding gains forinteractive debugging in speech mode.5 Speech and text in regression testingIn earlier versions of the Regulus developmentenvironment (Rayner et al, 2006, ?6.6), regres-sion testing in speech mode was all based onNuance?s batchrec utility, which permits of-fline recognition of a set of recorded wavfiles.A test suite for spoken regression testing conse-quently consisted of a list of wavfiles.
Thesewere first passed through batchrec; outputswere then post-processed into Regulus form, andfinally passed through Regulus speech understand-ing modules, such as translation or dialogue man-agement.As Regulus applications grow in complexity,this model has become increasingly inadequate,since system input is very frequently not just alist of monolingual speech events.
In a multi-modal dialogue system, input can consist of eitherspeech or screen events (text/mouse-clicks); con-text is generally important, and the events have tobe processed in the order in which they occurred.Dialogue systems which control real or simulatedrobots, like the Wheelchair application of (Hockey13Figure 4: Speech to speech translation from the GUI, using a Japanese to Arabic translator built fromMedSLT components (Bouillon et al, 2008).
The user presses the Recognise button (top right), speaks inJapanese, and receives a spoken translation in Arabic together with screen display of various processingresults.
The application is defined by a config file which combines a Japanese recogniser and analy-sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generationgrammar, and recorded Arabic wavfiles used to construct a spoken result.and Miller, 2007) will also receive asynchronousinputs from the robot control and monitoring pro-cess; once again, all inputs have to be processed inthe appropriate temporal order.
A third example iscontextual bidirectional speech translation (Bouil-lon et al, 2007).
Here, the problem is slightlydifferent ?
we have only speech inputs, but theyare for two different languages.
The basic issue,however, remains the same, since inputs have to beprocessed in the right order to maintain the correctcontext at each point.With examples like these in mind, we have alsoeffected a complete redesign of the Regulus envi-ronment?s regression testing facilities.
A test suiteis now allowed to consist of a list of items of anytype ?
text, wavfile, or non-speech input ?
in anyorder.
Instead of trying to fit processing into theconstraints imposed by the batchrec utility, of-fline processing now starts up speech resources inthe same way as the interactive environment, andsubmits each item for appropriate processing in theorder in which it occurs.
By adhering to the prin-ciple that text and speech should be treated uni-formly, we arrive at a framework which is simpler,less error-prone (the underlying code is less frag-ile) and above all much more flexible.6 Summary and conclusionsThe new functionality offered by the redesignedRegulus top-level is not strikingly deep.
In thecontext of any given application, it could allhave been duplicated by reasonably simple scripts,which linked together existing Regulus compo-nents.
Indeed, much of this new functionality isimplemented using code derived precisely fromsuch scripts.
Our observation, however, has beenthat few developers have actually taken the timeto write these scripts, and that when they havebeen developed inside one project they have usu-ally not migrated to other ones.
One of the thingswe have done, essentially, is to generalise previ-ously ad hoc application-dependent functionality,and make it part of the top-level development en-vironment.
The other main achievements of thenew Regulus top-level are to organise the existingfunctionality in a more systematic way, so that it iseasier to find commands, and to package it all as anormal-looking Swing-based GUI.Although none of these items sound dramatic,they make a large difference to the platform?s over-14all usability, and to the development cycle it sup-ports.
In effect, the Regulus top-level becomesa generic speech-enabled application, into whichdevelopers can plug their grammars, rule-sets andderived components.
Applications can be tested inthe speech view much earlier, giving a correspond-ingly better chance of catching bad design deci-sions before they become entrenched.
The mecha-nisms used to enable this functionality do not de-pend on any special properties of Regulus, andcould readily be implemented in other grammar-based development platforms, such as Gemini andUNIANCE, which support compilation of featuregrammars into grammar-based language models.At risk of stating the obvious, it is also worthpointing out that many users, particularly youngerones who have grown up using Windows and Macenvironments, expect as a matter of course that de-velopment platforms will be GUI-based rather thancommand-line.
Addressing this issue, and sim-plifying the transition between text- and speech-based, views has the pleasant consequence of im-proving Regulus as a vehicle for introducing lin-guistics students to speech technology.
An initialRegulus-based course at the University of SantaCruz, focussing on spoken dialogue systems, is de-scribed in (Hockey and Christian, 2008); a similarone, but oriented towards speech translation andusing the new top-level described here, is currentlyunder way at the University of Geneva.
We expectto present this in detail in a later paper.ReferencesAlshawi, H., editor.
1992.
The Core Language Engine.MIT Press, Cambridge, Massachusetts.Bos, J.
2002.
Compilation of unification grammarswith compositional semantics to speech recognitionpackages.
In Proceedings of the 19th InternationalConference on Computational Linguistics, Taipei,Taiwan.Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.Hockey, M. Santaholma, M. Starlander, Y. Nakao,K.
Kanzaki, and H. Isahara.
2005.
A generic multi-lingual open source platform for limited-domainmedical speech translation.
In Proceedings of the10th Conference of the European Association forMachine Translation (EAMT), pages 50?58, Bu-dapest, Hungary.Bouillon, P., G. Flores, M. Starlander,N.
Chatzichrisafis, M. Santaholma, N. Tsourakis,M.
Rayner, and B.A.
Hockey.
2007.
A bidirectionalgrammar-based medical speech translator.
In Pro-ceedings of the ACL Workshop on Grammar-basedApproaches to Spoken Language Processing, pages41?48, Prague, Czech Republic.Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-hara, N. Tsourakis, M. Starlander, B.A.
Hockey, andM.
Rayner.
2008.
Developing non-european trans-lation pairs in a medium-vocabulary medical speechtranslation system.
In Proceedings of LREC 2008,Marrakesh, Morocco.Copestake, A.
2002.
Implementing Typed FeatureStructure Grammars.
CSLI Press, Chicago.Crouch, R., M. Dalrymple, R. Kaplan, T. King,J.
Maxwell, and P. Newman, 2008.
XLE Documenta-tion.
http://www2.parc.com/isl/groups/nltt/xle/doc.As of 29 Apr 2008.Dix, A., J.E.
Finlay, G.D. Abowd, and R. Beale, edi-tors.
1998.
Human Computer Interaction.
Seconded.
Prentice Hall, England.Dowding, J., M. Gawron, D. Appelt, L. Cherny,R.
Moore, and D. Moran.
1993.
Gemini: A naturallanguage system for spoken language understanding.In Proceedings of the Thirty-First Annual Meeting ofthe Association for Computational Linguistics.Hockey, B.A.
and G. Christian.
2008.
Zero to spokendialogue system in one quarter: Teaching computa-tional linguistics to linguists using regulus.
In Pro-ceedings of the Third ACL Workshop on TeachingComputational Linguistics (TeachCL-08), Colum-bus, OH.Hockey, B.A.
and D. Miller.
2007.
A demonstration ofa conversationally guided smart wheelchair.
In Pro-ceedings of the 9th international ACM SIGACCESSconference on Computers and accessibility, pages243?244, Denver, CO.Jacko, J.A.
and A. Sears, editors.
2003.
Thehuman-computer interaction handbook: Fundamen-tals, evolving technologies and emerging applica-tions.
Lawerence Erlbaum Associates, Mahwah,New Jersey.Kron, E., M. Rayner, P. Bouillon, and M. Santa-holma.
2007.
A development environment for build-ing grammar-based speech-enabled applications.
InProceedings of the ACL Workshop on Grammar-based Approaches to Spoken Language Processing,pages 49?52, Prague, Czech Republic.Larsson, S. and D. Traum.
2000.
Information state anddialogue management in the TRINDI dialogue moveengine toolkit.
Natural Language Engineering, Spe-cial Issue on Best Practice in Spoken Language Di-alogue Systems Engineering, pages 323?340.Rayner, M., B.A.
Hockey, J.M.
Renders,N.
Chatzichrisafis, and K. Farrell.
2005.
Avoice enabled procedure browser for the interna-tional space station.
In Proceedings of the 43rd15Annual Meeting of the Association for Compu-tational Linguistics (interactive poster and demotrack), Ann Arbor, MI.Rayner, M., B.A.
Hockey, and P. Bouillon.
2006.Putting Linguistics into Speech Recognition: TheRegulus Grammar Compiler.
CSLI Press, Chicago.16
