Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813?821,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPUnbounded Dependency Recovery for Parser EvaluationLaura Rimell and Stephen ClarkUniversity of CambridgeComputer Laboratorylaura.rimell@cl.cam.ac.ukstephen.clark@cl.cam.ac.ukMark SteedmanUniversity of EdinburghSchool of Informaticssteedman@inf.ed.ac.ukAbstractThis paper introduces a new parser eval-uation corpus containing around 700 sen-tences annotated with unbounded depen-dencies, from seven different grammaticalconstructions.
We run a series of off-the-shelf parsers on the corpus to evaluate howwell state-of-the-art parsing technology isable to recover such dependencies.
Theoverall results range from 25% accuracyto 59%.
These low scores call into ques-tion the validity of using Parseval scoresas a general measure of parsing capability.We discuss the importance of parsers be-ing able to recover unbounded dependen-cies, given their relatively low frequencyin corpora.
We also analyse the various er-rors made on these constructions by one ofthe more successful parsers.1 IntroductionStatistical parsers are now obtaining Parsevalscores of over 90% on the WSJ section of the PennTreebank (Bod, 2003; Petrov and Klein, 2007;Huang, 2008; Carreras et al, 2008).
McClosky etal.
(2006) report an F-score of 92.1% using self-training applied to the reranker of Charniak andJohnson (2005).
Such scores, in isolation, maysuggest that statistical parsing is close to becom-ing a solved problem, and that further incrementalimprovements will lead to parsers becoming as ac-curate as POS taggers.A single score in isolation can be misleading,however, for a number of reasons.
First, the singlescore is an aggregate over a highly skewed distri-bution of all constituent types; evaluations whichlook at individual constituent or dependency typesshow that the accuracies on some, semanticallyimportant, constructions, such as coordination andPP-attachment, are much lower (Collins, 1999).Second, it is well known that the accuracy ofparsers trained on the Penn Treebank degradeswhen they are applied to different genres and do-mains (Gildea, 2001).
Finally, some researchershave argued that the Parseval metrics (Black et al,1991) are too forgiving with respect to certain er-rors and that an evaluation based on syntactic de-pendencies, for which scores are typically lower,is a better test of parser performance (Lin, 1995;Carroll et al, 1998).In this paper we focus on the first issue, that theperformance of parsers on some constructions ismuch lower than the overall score.
The construc-tions that we focus on are various unbounded de-pendency constructions.
These are interesting forparser evaluation for the following reasons: one,they provide a strong test of the parser?s knowl-edge of the grammar of the language, since manyinstances of unbounded dependencies are diffi-cult to recover using shallow techniques in whichthe grammar is only superficially represented; andtwo, recovering these dependencies is necessaryto completely represent the underlying predicate-argument structure of a sentence, useful for appli-cations such as Question Answering and Informa-tion Extraction.To give an example of the sorts of constructionswe are considering, and the (in)ability of parsersto recover the corresponding unbounded depen-dencies, none of the parsers that we have testedwere able to recover the dependencies shown inbold from the following sentences:We have also developed techniques for recognizing andlocating underground nuclear tests through the waves in theground which they generate.By Monday , they hope to have a sheaf of documents bothsides can trust.By means of charts showing wave-travel times and depthsin the ocean at various locations , it is possible to estimatethe rate of approach and probable time of arrival at Hawaiiof a tsunami getting under way at any spot in the Pacific .813The contributions of this paper are as follows.First, we present the first set of results for therecovery of a variety of unbounded dependen-cies, for a range of existing parsers.
Second, wedescribe the creation of a publicly available un-bounded dependency test suite, and give statisticssummarising properties of these dependencies innaturally occurring text.
Third, we demonstratethat performing the evaluation is surprisingly dif-ficult, because of different conventions across theparsers as to how the underlying grammar is rep-resented.
Fourth, we show that current parsingtechnology is very poor at representing some im-portant elements of the argument structure of sen-tences, and argue for a more focused construction-based parser evaluation as a complement to exist-ing grammatical relation-based evaluations.
Wealso perform an error-analysis for one of the moresuccessful parsers.There has been some prior work on evaluatingparsers on long-range dependencies, but no workwe are aware of that has the scope and focus ofthis paper.
Clark et al (2004) evaluated a CCGparser on a small corpus of object extraction cases.Johnson (2002) began the body of work on insert-ing traces into the output of Penn Treebank (PTB)parsers, followed by Levy and Manning (2004),among others.
This PTB work focused heavilyon the representation in the Treebank, evaluat-ing against patterns in the trace annotation.
Inthis paper we have tried to be more ?formalism-independent?
and construction focused.2 Unbounded Dependency Corpus2.1 The constructionsAn unbounded dependency construction containsa word or phrase which appears to have beenmoved, while being interpreted in the positionof the resulting ?gap?.
An unlimited numberof clause boundaries may intervene between themoved element and the gap (hence ?unbounded?
).The seven constructions in our corpus were cho-sen for being relatively frequent in text, comparedto other unbounded dependency types, and rela-tively easy to identify.
An example of each con-struction, along with its associated dependencies,is shown in Table 1.
Here we give a brief descrip-tion of each construction.Object extraction from a relative clause ischaracterised by a relative pronoun (a wh-word orthat) introducing a clause from which an argumentin object position has apparently been extracted:the paper which I wrote.
Our corpus includescases where the extracted word is (semantically)the object of a preposition in the verb phrase: theagency that I applied to.Object extraction from a reduced relativeclause is essentially the same, except that there isno overt relative pronoun: the paper I wrote; theagency I applied to.
We did not include participialreduced relatives such as the paper written by theprofessor.Subject extraction from a relative clause ischaracterised by the apparent extraction of an ar-gument from subject position: the instrument thatmeasures depth.
A relative pronoun is obligatoryin this construction.
Our corpus includes passivesubjects: the instrument which was used by theprofessor.Free relatives contain relative pronouns with-out antecedents: I heard what she said, wherewhat does not refer to any other noun in the sen-tence.
Free relatives can usually be paraphrased bynoun phrases such as the thing she said (a standarddiagnostic for distinguishing them from embeddedinterrogatives like I wonder what she said).
Themajority of sentences in our corpus are object freerelatives, but we also included some adverbial freerelatives: She told us how to do it.Objectwh-questions are questions in which thewh-word is the semantic object of the verb: Whatdid you eat?.
Objects of prepositions are included:What city does she live in?.
Also included are afew cases where the wh-word is arguably adver-bial, but is selected for by the verb: Where is thepark located?.Right node raising (RNR) is characterised bycoordinated phrases from which a shared elementapparently moves to the right: Mary saw and Su-san bought the book.
This construction is uniquewithin our corpus in that the ?raised?
element canhave a wide variety of grammatical functions.
Ex-amples include: noun phrase object of verb, nounphrase object of preposition (material about ormessages from the communicator), a combinationof the two (applied for and won approval), prepo-sitional phrase modifier (president and chief exec-utive of the company), infinitival modifier (the willand the capacity to prevent the event), and modi-fied noun (a good or a bad decision).Subject extraction from an embedded clauseis characterised by a semantic subject which is ap-814Object extraction from a relative clauseEach must match Wisman?s ?pie?
with the fragment that he carries with him.dobj(carries, fragment)Object extraction from a reduced relative clausePut another way, the decline in the yield suggests stocks have gotten pretty rich in price relative to thedividends they pay, some market analysts say.dobj(pay, dividends)Subject extraction from a relative clauseIt consists of a series of pipes and a pressure-measuring chamber which record the rise and fall of thewater surface.nsubj(record, series)nsubj(record, chamber)Free relativeHe tried to ignore what his own common sense told him, but it wasn?t possible; her motives were tooblatant.dobj(told, what)Object wh-questionWhat city does the Tour de France end in?pobj(in, city)Right node raisingFor the third year in a row, consumers voted Bill Cosby first and James Garner second in persuasivenessas spokesmen in TV commercials, according to Video Storyboard Tests, New York.prep(first, in)prep(second, in)Subject extraction from an embedded clauseIn assigning to God the responsibility which he learned could not rest with his doctors, Eisenhowergave evidence of that weakening of the moral intuition which was to characterize his administrationin the years to follow.nsubj(rest, responsibility)Table 1: Examples of the seven constructions in the unbounded dependency corpus.parently extracted across two clause boundaries,as shown in the following bracketing (where ?marks the origin of the extracted element): theresponsibility which [the government said [?
laywith the voters]].
Our corpus includes sentenceswhere the embedded clause is a so-called smallclause, i.e.
one with a null copula verb: the planthat she considered foolish, where plan is the se-mantic subject of foolish.2.2 The dataThe corpus consists of approximately 100 sen-tences for each of the seven constructions; 80 ofthese were reserved for each construction for test-ing, giving a test set of 560 sentences in total, andthe remainder were used for initial experimenta-tion (for example to ensure that default settings forthe various parsers were appropriate for this data).We did not annotate the full sentences, since weare only interested in the unbounded dependenciesand full annotation of such a corpus would be ex-tremely time-consuming.With the exception of the question construc-tion, all sentences were taken from the PTB, withroughly half from the WSJ sections (excluding2-21 which provided the training data for many815of the parsers in our set) and half from Brown(roughly balanced across the different sections).The questions were taken from the question datain Rimell and Clark (2008), which was obtainedfrom various years of the TREC QA track.
Wechose to use the PTB as the main source becausethe use of traces in the PTB annotation provides astarting point for the identification of unboundeddependencies.Sentences were selected for the corpus by acombination of automatic and manual processes.A regular expression applied to PTB trees, search-ing for appropriate traces for a particular con-struction, was first used to extract a set of can-didate sentences.
All candidates were manuallyreviewed and, if selected, annotated with one ormore grammatical relations representing the rel-evant unbounded dependencies in the sentence.Some of the annotation in the treebank makesidentification of some constructions straightfor-ward; for example right node raising is explicitlyrepresented as RNR.
Indeed it may have been pos-sible to fully automate this process with use ofthe tgrep search tool.
However, in order to ob-tain reliable statistics regarding frequency of oc-currence, and to ensure a high-quality resource,we used fairly broad regular expressions to iden-tify the original set followed by manual review.We chose to represent the dependencies asgrammatical relations (GRs) since this formatseemed best suited to represent the kind of seman-tic relationship we are interested in.
GRs are head-based dependencies that have been suggested as amore appropriate representation for general parserevaluation than phrase-structure trees (Carroll etal., 1998).
Table 1 gives examples of how GRsare used to represent the relevant dependencies.The particular GR scheme we used was based onthe Stanford scheme (de Marneffe et al, 2006);however, the specific GR scheme is not too crucialsince the whole sentence is not being representedin the corpus, only the unbounded dependencies.3 ExperimentsThe five parsers described in Section 3.2 were usedto parse the test sentences in the corpus, and thepercentage of dependencies in the test set recov-ered by each parser for each construction was cal-culated.
The details of how the parsers were runand how the parser output was matched againstthe gold standard are given in Section 3.3.
ThisConstruction WSJ Brown OverallObj rel clause 2.3 1.1 1.4Obj reduced rel 2.7 2.8 2.8Sbj rel clause 10.1 5.7 7.4Free rel 2.6 0.9 1.3RNR 2.2 0.9 1.2Sbj embedded 2.0 0.3 0.4Table 2: Frequency of constructions in the PTB(percentage of sentences).is essentially a recall evaluation, and so is opento abuse; for example, a program which returns allthe possible word pairs in a sentence, together withall possible labels, would score 100%.
However,this is easily guarded against: we simply assumethat each parser is being run in a ?standard?
mode,and that each parser has already been evaluated ona full corpus of GRs in order to measure precisionand recall across all dependency types.
(Calculat-ing precision for the unbounded dependency eval-uation would be difficult since that would requireus to know howmany incorrect unbounded depen-dencies were returned by each parser.
)3.1 Statistics relating to the constructionsTable 2 shows the percentage of sentences in thePTB, from those sections that were examined,which contain an example of each type of un-bounded dependency.
Perhaps not surprisingly,root subject extractions from relative clauses areby far the most common, with the remaining con-structions occurring in roughly between 1 and 2%of sentences.
Note that, although examples ofeach individual construction are relatively rare, thecombined total is over 10% (assuming that eachconstruction occurs independently).
Section 6contains a discussion regarding the frequency ofoccurrence of these events and the consequencesof this for parser performance.Table 3 shows the average and maximum dis-tance between head and dependent for each con-struction, as measured by the difference betweenword indices.
This is a fairly crude measure ofdistance but gives some indication of how ?long-range?
the dependencies are for each construc-tion.
The cases of object extraction from a relativeclause and subject extraction from an embeddedclause provide the longest dependencies, on aver-age.
The following sentence gives an example ofhow far apart the head and dependent can be in a816Construction Avg Dist Max DistObj rel clause 6.8 21Obj reduced rel 3.4 8Sbj rel clause 4.4 18Free rel 3.4 16Obj wh-question 4.8 9RNR 4.8 23Sbj embedded 7.0 21Table 3: Distance between head and dependent.subject embedded construction:the same stump which had impaled the car ofmany a guest in the past thirty years and which herefused to have removed.3.2 The parsersThe parsers that we chose to evaluate are the C&CCCG parser (Clark and Curran, 2007), the EnjuHPSG parser (Miyao and Tsujii, 2005), the RASPparser (Briscoe et al, 2006), the Stanford parser(Klein and Manning, 2003), and the DCU post-processor of PTB parsers (Cahill et al, 2004),based on LFG and applied to the output of theCharniak and Johnson reranking parser.
Of coursewe were unable to evaluate every publicly avail-able parser, but we believe these are representativeof current wide-coverage robust parsing technol-ogy.1The C&C parser is based on CCGbank (Hock-enmaier and Steedman, 2007), a CCG version ofthe Penn Treebank.
It is ideally suited for this eval-uation because CCG was designed to capture theunbounded dependencies being considered.
TheEnju parser was designed with a similar motiva-tion to C&C, and is also based on an automat-ically extracted grammar derived from the PTB,but the grammar formalism is HPSG rather thanCCG.
Both parsers produce head-word dependen-cies reflecting the underlying predicate-argumentstructure of a sentence, and so in theory should bestraightforward to evaluate.The RASP parser is based on a manually con-structed POS tag-sequence grammar, with a sta-tistical parse selection component and a robust1One obvious omission is any form of dependency parser(McDonald et al, 2005; Nivre and Scholz, 2004).
However,the dependencies returned by these parsers are local, and itwould be non-trivial to infer from a series of links whether along-range dependency had been correctly represented.
Also,dependency parsers are not significantly better at recoveringhead-based dependencies than constituent parsers based onthe PTB (McDonald et al, 2005).partial-parsing technique which allows it to re-turn output for sentences which do not obtain afull spanning analysis according to the grammar.RASP has not been designed to capture many of thedependencies in our corpus; for example, the tag-sequence grammar has no explicit representationof verb subcategorisation, and so may not knowthat there is a missing object in the case of extrac-tion from a relative clause (though it does recoversome of these dependencies).
However, RASP isa popular parser used in a number of applications,and it returns dependencies in a suitable format forevaluation, and so we considered it to be an appro-priate and useful member of our parser set.The Stanford parser is representative of a largenumber of PTB parsers, exemplified by Collins(1997) and Charniak (2000).
The Parseval scoresreported for the Stanford parser are not the highestin the literature, but are competitive enough for ourpurposes.
The advantage of the Stanford parser isthat it returns dependencies in a suitable format forour evaluation.
The dependencies are obtained bya set of manually defined rules operating over thephrase-structure trees returned by the parser (deMarneffe et al, 2006).
Like RASP, the Stanfordparser has not been designed to capture unboundeddependencies; in particular it does not make use ofany of the trace information in the PTB.
However,we wanted to include a ?standard?
PTB parser inour set to see which of the unbounded dependencyconstructions it is able to deal with.Finally, there is a body of work on insertingtrace information into the output of PTB parsers(Johnson, 2002; Levy and Manning, 2004), whichis the annotation used in the PTB for representingunbounded dependencies.
The work which dealswith the PTB representation directly, such as John-son (2002), is difficult for us to evaluate because itdoes not produce explicit dependencies.
However,the DCU post-processor is ideal because it doesproduce dependencies in a GR format.
It has alsoobtained competitive scores on general GR evalu-ation corpora (Cahill et al, 2004).3.3 Parser evaluationThe parsers were run essentially out-of-the-boxwhen parsing the test sentences.
The one excep-tion was C&C, which required some minor adjust-ing of parameters, as described in the parser doc-umentation, to obtain close to full coverage on thedata.
In addition, the C&C parser comes with a817Obj RC Obj Red Sbj RC Free Obj Q RNR Sbj Embed TotalC&C 59.3 62.6 80.0 72.6 (81.2) 27.5 49.4 22.4 (59.7) 53.6Enju 47.3 65.9 82.1 76.2 32.5 47.1 32.9 54.4DCU 23.1 41.8 56.8 46.4 27.5 40.8 5.9 35.7Rasp 16.5 1.1 53.7 17.9 27.5 34.5 15.3 25.3Stanford 22.0 1.1 74.7 64.3 41.2 45.4 10.6 38.1Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each constructionis in bold; the figures in brackets for C&C derive from the use of a separate question model.specially designed question model, and so we ap-plied both this and the standard model to the objectwh-question cases.The parser output was evaluated against eachdependency in the corpus.
Due to the various GRschemes used by the parsers, an exact match on thedependency label could not always be expected.We considered a correctly recovered dependencyto be one where the gold-standard head and depen-dent were correctly identified, and the label wasan ?acceptable match?
to the gold-standard label.To be an acceptable match, the label had to indi-cate the grammatical function of the extracted el-ement at least to the level of distinguishing activesubjects, passive subjects, objects, and adjuncts.For example, we allowed an obj (object) relationas a close enough match for dobj (direct object)in the corpus, even though obj does not distin-guish different kinds of objects, but we did not al-low generic ?relative pronoun?
relations that areunderspecified for the grammatical role of the ex-tracted element.The differences in GR schemes were such thatwe ended up performing a time-consuming largelymanual evaluation.
We list here some of the keydifferences that made the evaluation difficult.In some cases, the parser?s set of labels was lessfine-grained than the gold standard.
For example,RASP represents the direct objects of both verbsand prepositions as dobj (direct object), whereasthe gold-standard uses pobj for the prepositioncase.
We counted the RASP output as correctlymatching the gold standard.In other cases, the label on the dependencycontaining the gold-standard head and depen-dent was too underspecified to be acceptable byitself.
For example, where the gold-standardrelation was dobj(placed,buckets), DCUproduced relmod(buckets,placed) witha generic ?relative modifier?
label.
However,the correct label could be recovered from else-where in the parser output, specifically a com-bination of relpro(buckets,which) andobj(placed,which).
In this case we countedthe DCU output as correctly matching the goldstandard.In some constructions the Stanford scheme,upon which the gold-standard was based, makesdifferent choices about heads than other schemes.For example, in the the phrase Honolulu, which isthe center of the warning system, the corpus con-tains a subject dependency with center as the head:nsubj(center,Honolulu).
Other schemes,however, treat the auxiliary verb is as the head ofthe dependency, rather than the predicate nominalcenter.
As long as the difference in head selec-tion was due solely to the idiosyncracies of the GRschemes involved, we counted the relation as cor-rect.Finally, the different GR schemes treat coordi-nation differently.
In the corpus, coordinated ele-ments are always represented with two dependen-cies.
Thus the phrase they may half see and halfimagine the old splendor has two gold-standarddependencies: dobj(see,splendor) anddobj(imagine,splendor).
If a parser pro-duced only the former dependency, but appearedto have the coordination correct, then we awardedtwo marks, even though the second dependencywas not explicitly represented.4 ResultsAccuracies for the various parsers are shown in Ta-ble 4, with the highest score for each constructionin bold.
Enju and C&C are the top performers,operating at roughly the same level of accuracyacross most of the constructions.
Use of the C&Cquestion model made a huge difference for the wh-object construction (81.2% vs. 27.5%), showingthat adaptation techniques specific to a particular818construction can be successful (Rimell and Clark,2008).In order to learn more from these results, in Sec-tion 5 we analyse the various errors made by theC&C parser on each construction.
The conclusionsthat we arrive at for the C&C parser we would alsoexpect to apply to Enju, on the whole, since the de-sign of the two parsers is so similar.
In fact, someof the recommendations for improvement on thiscorpus, such as the need for a better parsing modelto make better attachment decisions, are parser in-dependent.The poor performance of RASP on this corpusis clearly related to a lack of subcategorisation in-formation, since this is crucial for recovering ex-tracted arguments.
For Stanford, incorporating thetrace information from the PTB into the statisticalmodel in some way is likely to help.
The C&C andEnju parsers do this through their respective gram-mar formalisms.
Our informal impression of theDCU post-processor is that it has much of the ma-chinery available to recover the dependencies thatthe Enju and C&C parsers do, but for some reasonwhich is unclear to us it performs much worse.5 Analysis of the C&C ParserWe categorised the errors made by the C&C parseron the development data for each construction.
Wechose the C&C parser for the analysis because itwas one of the top performers and we have moreknowledge of its workings than those of Enju.The C&C parser first uses a supertagger to as-sign a small number of CCG lexical categories (es-sentially subcategorisation frames) to each word inthe sentence.
These categories are then combinedusing a set of combinatory rules to build a CCGderivation.
The parser uses a log-linear probabil-ity model to select the highest-scoring derivation(Clark and Curran, 2007).
In general, errors in de-pendency recovery may occur if the correct lexicalcategory is not assigned by the supertagger for oneor more of the words in a sentence, or if an incor-rect derivation is chosen by the parsing model.For unbounded dependency recovery, onesource of errors (labeled type 1 in Table 5) is thewrong lexical category being assigned to the word(normally a verb or preposition) governing the ex-traction site.
In these testaments that I would sub-mit here, if submit is assigned a category for anintransitive rather than transitive verb, the verb-object relation will not be recovered.1a 1b 1c 1d 2 3 Errs TotObjRC 6 5 2 13 20ObjRed 2 1 1 1 3 8 23SbjRC 8 1 9 43Free 1 1 2 22ObjQ 2 2 4 25RNR 2 1 7 3 13 28SbjEmb 3 2 1 4 10 13Subtotal 6 2 12 4Total 24 21 14 59 174Table 5: Error analysis for C&C.
Errs is the to-tal number of errors for a construction, Tot is thenumber of dependencies of that type in the devel-opment data.There are a number of reasons why the wrongcategory may be assigned.
First, the lexicon maynot contain enough information about possiblecategories for the word (1a), or the necessary cat-egory may not exist in the parser?s grammar at all(1b).
Even if the grammar contains the correct cat-egory and the lexicon makes it available, the pars-ing model may not choose it (1c).
Finally, a POS-tagging error on the word may mislead the parserinto assigning the wrong category (1d).2A second source of errors (type 2) is attach-ment decisions that the parser makes indepen-dently of the unbounded dependency.
In Morgan.
.
.
carried in several buckets of water from thespring which he poured into the copper boiler, theparser assigns the correct categories for the rela-tive pronoun and verb, but chooses spring ratherthan buckets as the head of the relativized NP (i.e.the object of pour).
Most attachment errors in-volve prepositional phrases (PPs) and coordina-tion, which have long been known to be areaswhere parsers need improvement.Finally, errors in unbounded dependency recov-ery may be due to complex errors in the surround-ing parse context (type 3).
We will not commentmore on these cases since they do not tell us muchabout unbounded dependencies in particular.Table 5 shows the distribution of error typesacross constructions for the C&C parser.
Subjectrelative clauses, for example, did not have any er-rors of type 1, because a verb with an extracted2We considered an error to be type 1 only when the cate-gory error occurred on the word governing the extraction site,except in the subject embedded sentences, where we also in-cluded the embedding verb, since the category of this verb iskey to dependency recovery.819subject does not require a special lexical category.Most of the errors here are of type 2.
For exam-ple, in a series of pipes and a pressure-measuringchamber which record the rise and fall of the wa-ter surface, the parser attaches the relative clauseto chamber but not to series.Subject embedded sentences show a differentpattern.
Many of the errors can be attributed toproblems with the lexicon and grammar (1a and1b).
For example, in shadows that they imaginedwere Apaches, the word imagined never appears inthe training data with the correct category, and sothe required entry is missing from the lexicon.Object extraction from a relative clause hada higher number of errors involving the parsingmodel (1c).
In the first carefree, dreamless sleepthat she had known, the transitive category isavailable for known, but not selected by the model.The majority of the errors made by the parserare due to insufficient grammar coverage or weak-ness in the parsing model due to sparsity of headdependency data, the same fundamental problemsthat have dogged automatic parsing since its in-ception.
Hence one view of statistical parsing isthat it has allowed us to solve the easy problems,but we are still no closer to a general solution forthe recovery of the ?difficult?
dependencies.
Onepossibility is to create more training data target-ing these constructions ?
effectively ?active learn-ing by construction?
?
in the way that Rimell andClark (2008) were able to build a question parser.We leave this idea for future work.6 DiscussionUnbounded dependencies are rare events, out inthe Zipfian ?long tail?.
They will always consti-tute a fraction of a percent of the overall total ofhead-dependencies in any corpus, a proportion toosmall to make a significant impact on global mea-sures of parser accuracy, when expressive parsersare compared to those that merely approximatehuman grammar using finite-state or context-freecovers.
This will remain the case even when suchmeasures are based on dependencies, rather thanon parse trees.Nevertheless, unbounded dependencies remainhighly significant in a much more important sense.They support the constructions that are central tothose applications of parsing technology for whichprecision is as important as recall, such as open-domain question-answering.
As low-power ap-proximate parsing methods improve (as they mustif they are ever to be usable at all for such tasks),we predict that the impact of the constructions weexamine here will become evident.
No matter howinfrequent object questions like ?What do frogseat??
are, if they are answered as if they were sub-ject questions (?Herons?
), users will rightly rejectany excuse in terms of the overall statistical distri-bution of related bags of words.Whether such improvements in parsers comefrom the availability of more human-labeled data,or from a breakthrough in unsupervised machinelearning, we predict an imminent ?Uncanny Val-ley?
in parsing applications, due to the inability ofparsers to recover certain semantically importantdependencies, of the kind familiar from humanoidrobotics and photorealistic animation.
In such ap-plications, the closer the superficial resemblanceto human behavior gets, the more disturbing sub-tle departures become, and the more they inducemistrust and revulsion in the user.7 ConclusionIn this paper we have demonstrated that currentparsing technology is poor at recovering someof the unbounded dependencies which are crucialfor fully representing the underlying predicate-argument structure of a sentence.
We have alsoargued that correct recovery of such dependen-cies will become more important as parsing tech-nology improves, despite the relatively low fre-quency of occurrence of the corresponding gram-matical constructions.
We also see this more fo-cused parser evaluation methodology ?
in thiscase construction-focused ?
as a way of improv-ing parsing technology, as an alternative to theexclusive focus on incremental improvements inoverall accuracy measures such as Parseval.AcknowledgmentsLaura Rimell and Stephen Clark were supportedby EPSRC grant EP/E035698/1.
Mark Steed-man was supported by EU IST Cognitive Systemsgrant IP FP6-2004-IST-4-27657 (PACO-PLUS).We would like to thank Aoife Cahill for produc-ing the DCU data.820ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cov-erage of English grammars.
In HLT ?91: Proceed-ings of the Workshop on Speech and Natural Lan-guage, pages 306?311.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings of the 10th Meeting ofthe EACL, pages 19?26, Budapest, Hungary.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
InProceedings of the Interactive Demo Session ofCOLING/ACL-06, Sydney, Australia.A.
Cahill, M. Burke, R. O?Donovan, J. van Genabith,and A.
Way.
2004.
Long-distance dependencyresolution in automatically acquired wide-coveragePCFG-based LFG approximations.
In Proceedingsof the 42nd Meeting of the ACL, pages 320?327,Barcelona, Spain.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Dynamic programming and the perceptron forefficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Natural Language Learning(CoNLL-08), pages 9?16, Manchester, UK.John Carroll, Ted Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new pro-posal.
In Proceedings of the 1st LREC Conference,pages 447?454, Granada, Spain.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Meeting ofthe ACL, pages 173?180, Michigan, Ann Arbor.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Meetingof the NAACL, pages 132?139, Seattle, WA.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark, Mark Steedman, and James R. Curran.2004.
Object-extraction and question-parsing usingCCG.
In Proceedings of the EMNLP Conference,pages 111?118, Barcelona, Spain.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe 35th Meeting of the ACL, pages 16?23, Madrid,Spain.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th LREC Conference, Genoa,Italy.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proceedings of the 2001 EMNLP Con-ference, Pittsburgh, PA.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proceed-ings of the 46th Meeting of the ACL, pages 586?594,Columbus, Ohio.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Meeting of theACL, pages 136?143, Philadelphia, PA.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of Association for Computa-tional Linguistics, pages 423?430, Sapporo, Japan.Roger Levy and Christopher Manning.
2004.
Deep de-pendencies from context-free statistical parsers: cor-recting the surface dependency approximation.
InProceedings of the 42nd Meeting of the ACL, pages328?335, Barcelona, Spain.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof IJCAI-95, pages 1420?1425, Montreal, Canada.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the North American Chapter of theAssociation for Computational Linguistics Confer-ence, pages 152?159, Brooklyn, NY.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd Meet-ing of the ACL, pages 91?98, Michigan, Ann Arbor.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilis-tic disambiguation models for wide-coverage HPSGparsing.
In Proceedings of the 43rd Meeting of theACL, pages 83?90, Michigan, Ann Arbor.J.
Nivre and M. Scholz.
2004.
Deterministic depen-dency parsing of English text.
In Proceedings ofCOLING-04, pages 64?70, Geneva, Switzerland.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofthe HLT/NAACL Conference, Rochester, NY.Laura Rimell and Stephen Clark.
2008.
Adapting alexicalized-grammar parser to contrasting domains.In Proceedings of the 2008 EMNLP Conference,pages 475?484, Honolulu, Hawai?i.821
