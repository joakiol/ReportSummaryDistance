Language-Independent Context Aware Query Translation using WikipediaRohit Bharadwaj GSearch and Information Extraction LabLTRCIIIT Hyderabad, Indiabharadwaj@research.iiit.ac.inVasudeva VarmaSearch and Information Extraction LabLTRCIIIT Hyderabad, Indiavv@iiit.ac.inAbstractCross lingual information access (CLIA) sys-tems are required to access the large amountsof multilingual content generated on the worldwide web in the form of blogs, news articlesand documents.
In this paper, we discuss ourapproach to query formation for CLIA sys-tems where language resources are replacedby Wikipedia.
We claim that Wikipedia,with its rich multilingual content and struc-ture, forms an ideal platform to build a CLIAsystem.
Our approach is particularly usefulfor under-resourced languages, as all the lan-guages don?t have the resources(tools) withsufficient accuracies.
We propose a contextaware language-independent query formationmethod which, with the help of bilingual dic-tionaries, forms queries in the target language.Results are encouraging with a precision of69.75% and thus endorse our claim on usingWikipedia for building CLIA systems.1 INTRODUCTIONCross lingual information access (CLIA) systemsenable users to access the rich multilingual contentthat is created on the web daily.
Such systems arevital to bridge the gap between information avail-able and languages known to the user.
Considerableamount of research has been done on building suchsystems but most of them rely heavily on the lan-guage resources and tools developed.
With a con-stant increase in the number of languages around theworld with their content on the web, CLIA systemsare in need.
Language independent approach is par-ticularly useful for languages that fall into the cat-egory of under-resourced (African, few Asian lan-guages), that doesn?t have sufficient resources.
Inour approach towards language-independent CLIAsystem, we have developed context aware querytranslation using Wikipedia.
Due to voluntary con-tribution of millions of users, Wikipedia gathers verysignificant amount of updated knowledge and pro-vides a structured way to access it.Figure 1: Number of Wikipedia pages(Y-axis) with andwithout Inter language link (ILL) to English in each lan-guage (X-axis)The statistics in the Figure 1 show that it hasrich multilingual content and is growing indepen-dent of the presence of English counter part.
Withits structurally rich content, it provides an ideal plat-form to perform cross lingual research.
We harnessWikipedia and its structure to replace the languagespecific resources required for CLIA.Our work is different from existing approaches in145Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 145?150,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticsterms of?
No language resource has been used at anystage of query translation.?
Wikipedia structure has been fully utilized forachieving CLIA between English and Hindi,unlike the existing approaches, especially forquery formation.We have constructed a bilingual dictionary us-ing cross lingual links present across the articles ofsame topic in different languages.
As each word inthe dictionary can have several translations based onvarious attributes like context, sense etc, we needa mechanism to identify the target word accuratelybased on the context of the query.
To identifythe context of a query, ?Content Words?, that arebuilt for each Wikipedia article, are used.
?ContentWords?
of the article are similar to the tags of the ar-ticle, that reflects the context of the article in a moredetailed way.In this paper, we detail our approach in formingthis ?Content Words?
and using them to form thequery.
Since our approach is language-independentand context-aware, we used a metric proposedby (Bharadwaj and Varma, 2011) to evaluate alongwith a dicationary-based metric.
The system is builtbetween languages English and Hindi.
Hindi is se-lected as target language because of the availabil-ity of resources for evaluation.
As our approachis language-independent, it can be used to trans-late queries between any pair of languages presentin Wikipedia.
The remainder of paper is organizedas follows.
Section 2 shows the related work.
Pro-posed method is discussed in Section 3.
Results andDiscussion are in Section 4.
We finally conclude inSection 5.2 RELATED WORKWe discuss the related work of the two stages are in-volved in our system of language-independent con-text aware query translation,?
Resource building/ collection (Dictionaries inour case)?
Query formationDictionary building can be broadly classified intotwo approaches, manual and automatic.
At initialstages, various projects like (Breen, 2004) try tobuild dictionaries manually, taking lot of time andeffort.
Though manual approaches perform well,they lag behind when recent vocabulary is consid-ered.
To reduce the effort involved, automatic ex-traction of dictionaries has been envisioned.
Theapproach followed by (Kay and Roscheisen, 1999)and (Brown et al, 1990) were towards statistical ma-chine translation, that can also be applied to dic-tionary building.
The major requirement for us-ing statistical methods is the availability of bilin-gual parallel corpora, that again is limited for under-resourced languages.
Factors like sentence struc-ture, grammatical differences, availability of lan-guage resources and the amount of parallel corpusavailable further hamper the recall and coverage ofthe dictionaries extracted.After parallel corpora, attempts have been madeto construct bilingual dictionaries using varioustypes of corpora like comparable corpus (Sadatet al, 2003) and noisy parallel corpus (Fung andMcKeown, 1997).
Though there exist various ap-proaches, most of them make use of the languageresources.
Wikipedia has also been used to minedictionaries.
(Tyers and Pienaar, 2008), (Erdmann etal., 2008), (Erdmann et al, 2009) have built bilin-gual dictionaries using Wikipedia and language re-sources.
We have mined our dictionaries similarlyconsidering the cross lingual links present.
Our ap-proach to dictionary building is detailed in section 3.Wikipedia has been used for CLIA at variousstages including query formation.
Most recently,Wikipedia structure has been exploited in (Gaillardet al, 2010) for query translation and disambigua-tion.
In (Scho?nhofen et al, 2008), Wikipedia hasbeen exploited at all the stages of building a CLIAsystem.
We tread the same path of (Scho?nhofenet al, 2008) in harnessing Wikipedia for dictionarybuilding and query formation.
Similar to them weextract concept words for each Wikipedia article anduse them to disambiguate and form the query.For evaluation purposes, we adapted evaluationmeasures based on Wikipedia and existing dictio-naries (Bharadwaj and Varma, 2011).
The authorshave proposed a classification based technique, us-ing Wikipedia article and the inter-language links146present between them to classify the sentences asparallel or non-parallel based on the context of thesentences rather than at the syntactic level.
We adopta similar classification based technique and buildfeature vectors for classification using Support Vec-tor Machines (SVM 1) for evaluation.3 PROPOSED METHODThe architecture of the system is given in the Fig-ure 2.Figure 2: Architecture of the systemThe following subsections describe each modulein detail.3.1 Dictionary BuildingBilingual dictionaries (English-Hindi) are built fromWikipedia by mining parallel/ near-parallel textfrom each structural information like title, infobox,category and abstract (initial paragraph) of the En-glish(En) and Hindi(Hi) articles that are connectedwith Inter language link (ILL, arrows between EnWikipedia articles and Hi Wikipedia articles in Fig-ure 2).
The motivation for considering the otherstructural information of the Wikipedia article is toincrease vocabulary of the dictionary both in termsof the number of words and categories of words.
Ti-tles, Infobox and Categories of the article consideronly named entities that are used in the language.1http://www.cs.cornell.edu/People/tj/svm_light/To increase the coverage of the dictionary and alsoto include other categories of words (like negations,quantifiers etc), abstract of the article is considered.Also the Inter language links between the articlesare assumed to be bi-directional even if they are uni-directional.
An approach similar to (Tyers and Pien-aar, 2008) is followed to construct dictionaries.
Thedictionary is constructed iteratively by using the pre-viously constructed dictionaries from each structure.The structural aspects of the article used are?
Title: Titles of the articles linked.?
Infobox: Infobox of the articles that are linked.?
Category: Categories of the articles linked.?
Abstract: The inital paragraph of the articleslinked are considered as the article abstractsand are used for dictionary building.A dictionary consists of word and its several pos-sible translations, scored according to their align-ment scores.
Each structural information is used toenhance the dictionary built previously.
Dictionarybuilt from titles are used as starting point.
As eachEnglish word is mapped to several Hindi words, fil-tering of words or re-ranking of the words at queryformation is vital.
The scoring function used for thewords while building the dictionary isscore(wiE , wjH) =W iE?W jHW iE(1)Where wiE is the ith word in English word list; wjHis the jth word in Hindi word list; W iE?W jH is thecount of co-occurrence of wiE and wjH in the parallelcorpus and; W iE is the count of occurrences of theword wiE in the corpus.3.2 Building Content wordsThe context of each English Wikipedia article Ai isextracted from the following structural informationof the article.?
Title : Title of the article?
Redirect title : Redirect title of the article, ifpresent.147?
Category : Categories of the article that are pre-defined.?
Subsections : Titles of the different sub-sections of the article.?
In-links : Meta data present in the links to thisarticle from other articles in same language.?
Out-links : Meta data of the links that linkthe current article to other articles in same lan-guage.As these structural attributes are spread across thearticle, they help to identify the context (orienta-tion) of the article in depth when compared withthe Categories of the article.
Each structural as-pect described above have unique content that willhelp to identify the context of the article.
?ContentWords?
are formed from each of these structural as-pects.
Word count of the words present in each ofthe above mentioned attributes are calculated andare filtered by a threshold to form the context wordsof the article.
The threshold for filtering has beencalculated by manual tagging with the help of lan-guage annotators.
?Content Words?
for the Hindiarticles are also formed similarly.
The formation of?Content Words?
is similar to tagging but is not astricly tagging mechanism as we have no constrainton the number of tags.
Category alone can help toget the context but considering in-links, out-links,subsections will increase the depth of context wordsand will reduce the information lost by tagging thewords.3.3 Query formationQuery formation of our system depends on the con-text words built.
For an English query (qE) that con-tains the words wiE (i: 0 to n),?
Build WH of size m, that contains the words re-turned by the dictionary for each of the words.?
For all words in (qE), extract all the articles aki(k: 0 to n) with wiE as one of its context word.?
Form the corresponding Hindi set of articles Ahusing the cross lingual link, if present in the En-glish article set constructed in the above step.?
For each Hindi word wjH (j: 0 to m), add it toHindi query (qH) if at least one of the articlesai (with wjH as its context word) is present inAh.This approach helps to identify the context of thequery as each query is represented by a set of articlesinstead of query words, that forms the concepts thatthe query can be interpreted to limited to Wikipediadomain.
Queries are translated based on the archi-tecture described in Figure 2.4 Results and Discussion4.1 Evaluation, Dataset and ResultsA classification based approach and a dictionarybased approach are employed to calculate the accu-racy of the queries translated.
400 sentences withtheir corresponding translations (English-Hindi)have been used as test set to evaluate the perfor-mance of the query formation.
The sentence pairsare provided by FIRE2.
These sentences contain alltypes of words (Named entities, Verbs etc) and willbe referred to as samples.
The English language sen-tences are used as queries and are translated to Hindiusing the approach described.
Before forming thequery, stop words are removed from the English sen-tence.
The query lengths after removing stop wordsvary from 2 words to 8 words.
The dictionary usedfor evaluation is an existing one, Shabdanjali3.
Inthe following sections, we describe our two evalu-ation strategies and the performance of our systemusing them.4.1.1 Dictionary based evaluationShabdanjali dictionary has been used to evaluatethe translated queries.
The evaluation metric is wordoverlap, though it is relaxed further.
The formula2http://www.isical.ac.in/ clia/3Shabdanjali is an open source bilingual dictionary thatis most used between English and Hindi.
It is availableat http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict_Frame.html148used for calculating the precision isprecision = No.ofCorrectSamplesTotalNumberofSamples (2)A sample is said to be correct if its overLapScoreis greater than threshold instead of complete over-lap.
The overLapScore of each sample is mea-sured using Formula 3.
Threshold is the averageoverLapScore of the positive training set used fortraining the classifier (Training dataset is discussedin Section 4.1.2).overLapScore = No.ofWordOverlapTotalNumberofWords (3)The number of word overlaps are measured bothmanually and automatically to avoid inconsistent re-sults due to varios syntactic representation of thesame word in Wikipedia.The precision for the test dataset using this ap-proach is 42.8%.4.1.2 Classification based evaluationAs described in Section 2, we have used a clas-sification based technique for identifying whetherthe translated queries contain the same informa-tion or not.
We have collected 1600 pairs of sen-tences where 800 sentences are parallel to eachother (positive samples, exact translations) whilethe other half have word overlaps, but not paral-lel, (not exact translations but have similar content)form the negative samples.
Various statistics areextracted from Wikipedia for each sentence pair toconstruct feature vector as described in (Bharad-waj and Varma, 2011).
Each English and Hindisentences are queried as bag-of-words query to cor-responding Wikipedia articles and statistics are ex-tracted based on the articles retrieved.
The classifierused is SVM and is trained on the feature vectorsgenerated for 1600 samples.
The precision in thisapproach is the accuracy of the classifier.
The for-mula used for calculating the accuracy isaccuracy = No.ofSamplesCorrectlyClassifiedTotalNumberofSamples(4)The correctness of the sample is the prediction of theclassifier.
The precision for the test set is 69.75%.4.2 DiscussionThe precision achieved by classification based eval-uation is higher than that of existing dictionary(Shabdanjali) primarily due to?
Dictionary (Shabdanjali) doesn?t contain wordsof the query.
(Coverage is less).?
Word forms present in the dictionary are differ-ent to that of words present in translated query.
(Ex: spelling, tense etc).To negate the effect of above factors, classificationbased evaluation ( 4.1.2) has been considered.
Clas-sification based evaluation shows that the results arebetter when the entire sentence and its context isconsidered.
As there are no existing systems thattranslate queries based on the context and languageindependent, our results are encouraging to work inthis direction.
Since no language resources wereused, our approach is scalable and can be applied toany pair of languages present in Wikipedia.
The rel-atively low coverage of the dictionaries built usingWikipedia structure also affects the process of querytranslation.
In future, the coverage of dictionariescan also be increased by considering other structuralproperties of Wikipedia.5 ConclusionIn this paper, we have described our approachtowards building a language-independent contextaware query translation, replacing the language re-sources with the rich multilingual content provider,Wikipedia.
Its structural aspects have been exploitedto build the dictionary and its articles are used toform queries and also to evaluate them.
Further ex-ploitation of Wikipedia and its structure to increasethe coverage of the dictionaries built will increasethe overall precision.
Though queries are translatedin a language-independent way, using language re-sources of English, as it is a richly resourced lan-guage, for query formation is also envisioned.ReferencesRohit G. Bharadwaj and Vasudeva Varma.
2011.
Lan-guage independent identification of parallel sentences149using wikipedia.
In Proceedings of the 20th inter-national conference companion on World wide web,WWW ?11, pages 11?12, New York, NY, USA.
ACM.J.
W. Breen.
2004.
JMdict:A Japanese-MultilingualDictionary.
In COLING Multilingual Linguistic Re-sources Workshop, pages 71?78.P.F.
Brown, J. Cocke, S.A.D.
Pietra, V.J.D.
Pietra, F. Je-linek, J.D.
Lafferty, R.L.
Mercer, and P.S.
Roossin.1990.
A statistical approach to machine translation.Computational linguistics, 16(2):85.M.
Erdmann, K. Nakayama, T. Hara, and S. Nishio.2008.
An approach for extracting bilingual terminol-ogy from wikipedia.
In Database Systems for Ad-vanced Applications, pages 380?392.
Springer.M.
Erdmann, K. Nakayama, T. Hara, and S. Nishio.2009.
Improving the extraction of bilingual termi-nology from Wikipedia.
ACM Transactions on Multi-media Computing, Communications, and Applications(TOMCCAP), 5(4):1?17.P.
Fung and K. McKeown.
1997.
A technical word-andterm-translation aid using noisy parallel corpora acrosslanguage groups.
Machine Translation, 12(1):53?87.B.
Gaillard, M. Boualem, and O. Collin.
2010.
QueryTranslation using Wikipedia-based resources for anal-ysis and disambiguation.M.
Kay and M. Roscheisen.
1999.
Text-translationAlignment.
In Computational Linguistics, volume 19,pages 604?632.F.
Sadat, M. Yoshikawa, and S. Uemura.
2003.
Bilin-gual terminology acquisition from comparable corporaand phrasal translation to cross-language informationretrieval.
In Proceedings of the 41st Annual Meetingon Association for Computational Linguistics-Volume2, pages 141?144.
Association for Computational Lin-guistics.P.
Scho?nhofen, A. Benczu?r, I.
B?
?ro?, and K. Csaloga?ny.2008.
Cross-language retrieval with wikipedia.
Ad-vances in Multilingual and Multimodal InformationRetrieval, pages 72?79.F.M.
Tyers and J.A.
Pienaar.
2008.
Extracting bilingualword pairs from Wikipedia.
Collaboration: interop-erability between people in the creation of languageresources for less-resourced languages, page 19.150
