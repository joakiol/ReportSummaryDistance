Proceedings of the 43rd Annual Meeting of the ACL, pages 247?254,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsTowards Finding and Fixing Fragments: Using ML to IdentifyNon-Sentential Utterances and their Antecedents in Multi-Party DialogueDavid SchlangenDepartment of LinguisticsUniversity of PotsdamP.O.
Box 601553D-14415 Potsdam ?
Germanydas@ling.uni-potsdam.deAbstractNon-sentential utterances (e.g., short-answers as in ?Who came to the party????Peter.?)
are pervasive in dialogue.
Aswith other forms of ellipsis, the elided ma-terial is typically present in the context(e.g., the question that a short answer an-swers).
We present a machine learningapproach to the novel task of identifyingfragments and their antecedents in multi-party dialogue.
We compare the perfor-mance of several learning algorithms, us-ing a mixture of structural and lexical fea-tures, and show that the task of identifyingantecedents given a fragment can be learntsuccessfully (f(0.5) = .76); we discusswhy the task of identifying fragments isharder (f(0.5) = .41) and finally reporton a combined task (f(0.5) = .38).1 IntroductionNon-sentential utterances (NSUs) as in (1) are per-vasive in dialogue: recent studies put the proportionof such utterances at around 10% across differenttypes of dialogue (Ferna?ndez and Ginzburg, 2002;Schlangen and Lascarides, 2003).
(1) a.
A: Who came to the party?B: Peter.
(= Peter came to the party.)b.
A: I talked to Peter.B: Peter Miller?
(= Was it Peter Milleryou talked to?)c.
A: Who was this?
Peter Miller?
(= Wasthis Peter Miller?Such utterances pose an obvious problem for naturallanguage processing applications, namely that theintended information (in (1-a)-B a proposition) hasto be recovered from the uttered information (here,an NP meaning) with the help of information fromthe context.While some systems that automatically resolvesuch fragments have recently been developed(Schlangen and Lascarides, 2002; Ferna?ndez et al,2004a), they have the drawback that they require?deep?
linguistic processing (full parses, and also in-formation about discourse structure) and hence arenot very robust.
We have defined a well-definedsubtask of this problem, namely identifying frag-ments (certain kinds of NSUs, see below) and theirantecedents (in multi-party dialogue, in our case),and present a novel machine learning approach to it,which we hypothesise will be useful for tasks suchas automatic meeting summarisation.1The remainder of this paper is structured as fol-lows.
In the next section we further specify the taskand different possible approaches to it.
We then de-scribe the corpus we used, some of its characteris-tics with respect to fragments, and the features weextracted from it for machine learning.
Section 4describes our experimental settings and reports theresults.
After a comparison to related work in Sec-tion 5, we close with a conclusion and some further1(Zechner and Lavie, 2001) describe a related task, linkingquestions and answers, and evaluate its usefulness in the contextof automatic summarisation; see Section 5.247work that is planned.2 The TasksAs we said in the introduction, the main task wewant to tackle is to align (certain kinds of) NSUsand their antecedents.
Now, what characterises thiskind of NSU, and what are their antecedents?In the examples from the introduction, the NSUscan be resolved simply by looking at the previousutterance, which provides the material that is elidedin them.
In reality, however, the situation is not thatsimple, for three reasons: First, it is of course notalways the previous utterance that provides this ma-terial (as illustrated by (2), where utterance 7 is re-solved by utterance 1); in our data the average dis-tance in fact is 2.5 utterances (see below).
(2) 1 B: [.
.
. ]
What else should be done ?2 C: More intelligence .3 More good intelligence .4 Right .5 D: Intelligent intelligence .6 B: Better application of face and voicerecognition .7 C: More [.
.
. ]
intermingling of theagencies , you know .
[ from NSI 20011115 ]Second, it?s not even necessarily a single utter-ance that does this?it might very well be a spanof utterances, or something that has to be inferredfrom such spans (parallel to the situation with pro-nouns, as discussed empirically e.g.
in (Strube andMu?ller, 2003)).
(3) shows an example where a newtopic is broached by using an NSU.
It is possible toanalyse this as an answer to the question under dis-cussion ?what shall we organise for the party?
?, as(Ferna?ndez et al, 2004a) would do; a question, how-ever, which is only implicitly posed by the previousdiscourse, and hence this is an example of an NSUthat does not have an overt antecedent.
(3) [after discussing a number of different topics]1 D: So, equipment.2 I can bring [.
.
.
][ from NSI 20011211 ]Lastly, not all NSUs should be analysed as being theresult of ellipsis: backchannels for example (like the?Right?
in utterance 4 in (2) above) seem to directlyfulfil their discourse function without any need forreconstruction.2To keep matters simple, we concentrate in this pa-per on NSUs of a certain kind, namely those that a)do not predominantly have a discourse-managementfunction (like for example backchannels), but ratherconvey messages (i.e., propositions, questions orrequests)?this is what distinguishes fragments fromother NSUs?and b) have individual utterances asantecedents.
In the terminology of (Schlangen andLascarides, 2003), fragments of the latter type areresolution-via-identity-fragments, where the elidedinformation can be identified in the context andneed not be inferred (as opposed to resolution-via-inference-fragments).
Choosing only this specialkind of NSUs poses the question whether this sub-group is distinguished from the general group offragments by criteria that can be learnt; we will re-turn to this below when we analyse the errors madeby the classifier.We have defined two approaches to this task.
Oneis to split the task into two sub-tasks: identifyingfragments in a corpus, and identifying antecedentsfor fragments.
These steps are naturally performedsequentially to handle our main task, but they alsoallow the fragment classification decision to comefrom another source?a language-model used in anautomatic speech recognition system, for example?and to use only the antecedent-classifier.
The otherapproach is to do both at the same time, i.e.
to clas-sify pairs of utterances into those that combine afragment and its antecedent and those that don?t.
Wereport the results of our experiments with these tasksbelow, after describing the data we used.3 Corpus, Features, and Data Creation3.1 CorpusAs material we have used six transcripts from the?NIST Meeting Room Pilot Corpus?
(Garofolo et al,2004), a corpus of recordings and transcriptions ofmulti-party meetings.3 Those six transcripts con-2The boundaries are fuzzy here, however, as backchan-nels can also be fragmental repetitions of previous material,and sometimes it is not clear how to classify a given utter-ance.
A similar problem of classifying fragments is discussedin (Schlangen, 2003) and we will not go further into this here.3We have chosen a multi-party setting because we are ulti-mately interested in automatic summarisation of meetings.
Inthis paper here, however, we view our task as a ?stand-alonetask?.
Some of the problems resulting in the presence of many248average distance ?
?
?
(utterances): 2.5?
declarative 159 (52%)?
interrogative 140 (46%)?
unclassfd.
8 (2%)?
declarative 235 (76%)?
interrogative (23%)?
unclassfd.
2 (0.7%)?
being last in their turn 142 (46%)?
being first in their turn 159 (52%)Table 1: Some distributional characteristics.
(?
de-notes antecedent, ?
fragment.
)sist of 5,999 utterances, among which we identified307 fragment?antecedent pairs.4,5 With 5.1% this isa lower rate than that reported for NSUs in other cor-pora (see above); but note that as explained above,we are actually only looking at a sub-class of allNSUs here.For these pairs we also annotated some more at-tributes, which are summarised in Table 1.
Notethat the average distance is slightly higher than thatreported in (Schlangen and Lascarides, 2003) for(2-party) dialogue (1.8); this is presumably due tothe presence of more speakers who are able to re-ply to an utterance.
Finally, we automatically an-notated all utterances with part-of-speech tags, us-ing TreeTagger (Schmid, 1994), which we?vetrained on the switchboard corpus of spoken lan-guage (Godfrey et al, 1992), because it contains,just like our corpus, speech disfluencies.6We now describe the creation of the data we usedfor training.
We first describe the data-sets for thedifferent tasks, and then the features used to repre-sent the events that are to be classified.3.2 Data SetsData creation for the fragment-identification task(henceforth simply fragment-task) was straightfor-speakers are discussed below.4We have used the MMAX tool (Mu?ller and Strube, 2001))for the annotation.5To test the reliability of the annotation scheme, we had asubset of the data annotated by two annotators and found a sat-isfactory ?-agreement (Carletta, 1996) of ?
= 0.81.6The tagger is available free for academic research fromhttp://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html.ward: for each utterance, a number of features wasderived automatically (see next section) and the cor-rect class (fragment / other) was added.
(Notethat none of the manually annotated attributes wereused.)
This resulted in a file with 5,999 data pointsfor classification.
Given that there were 307 frag-ments, this means that in this data-set there is a ratiopositives (fragments) vs. negatives (non-fragments)for the classifier of 1:20.
To address this imbalance,we also ran the experiments with balanced data-setswith a ratio of 1:5.The other tasks, antecedent-identification(antecedent-task) and antecedent-fragment-identification (combined-task) required the creationof data-sets containing pairs.
For this we createdan ?accessibility window?
going back from eachutterance.
Specifically, we included for eachutterance a) all previous utterances of the samespeaker from the same turn; and b) the three lastutterances of every speaker, but only until onespeaker took the turn again and up to a maximumof 6 previous utterances.
To illustrate this method,given example (2) it would form pairs with utterance7 as fragment-candidate and all of utterances 6?2,but not 1, because that violates condition b) (it is thesecond turn of speaker B).In the case of (2), this exclusion would be a wrongdecision, since 1 is in fact the antecedent for 7.
Ingeneral, however, this dynamic method proved goodat capturing as many antecedents as possible whilekeeping the number of data points manageable.
Itcaptured 269 antecedent-fragment pairs, which hadan average distance of 1.84 utterances.
The remain-ing 38 pairs which it missed had an average distanceof 7.27 utterances, which means that to capture thosewe would have had to widen the window consid-erably.
E.g., considering all previous 8 utteranceswould capture an additional 25 pairs, but at the costof doubling the number of data points.
We hencechose the approach described here, being aware ofthe introduction of a certain bias.As we have said, we are trying to link utterances,one a fragment, the other its antecedent.
The no-tion of utterance is however less well-defined thanone might expect, and the segmentation of contin-uous speech into utterances is a veritable researchproblem on its own (see e.g.
(Traum and Heeman,1997)).
Often it is arguable whether a prepositional249Structural featuresdis distance ?
?
?, in utterancessspk same speaker yes/nonspk number speaker changes (= # turns)iqu number of intervening questionsalt ?
last utterance in its turn?bft ?
first utterance in its turn?Lexical / Utterance-based featuresbvb (tensed) verb present in ?
?bds disfluency present in ?
?aqm ?
contains question markawh ?
contains wh wordbpr ratio of polar particles (yes, no, maybe, etc..)/ other in ?apr ratio of polar particles in ?lal length of ?lbe length of ?nra ratio nouns / non-nouns in ?nra ratio nouns / non-nouns in ?rab ratio nouns in ?
that also occur in ?rap ratio words in ?
that also occur in ?god google similarity (see text)Table 2: The Featuresphrase for example should be analysed as an adjunct(and hence as not being an utterance on its own) oras a fragment.
In our experiments, we have followedthe decision made by the transcribers of the origi-nal corpus, since they had information (e.g.
aboutpauses) which was not available to us.For the antecedent-task, we include only pairswhere ?
(the second utterance in the pair) is afragment?since the task is to identify an antecedentfor already identified fragments.
This results in adata-set with 1318 data points (i.e., we created onaverage 4 pairs per fragment).
This data-set is suf-ficiently balanced between positives and negatives,and so we did not create another version of it.
Thedata for the combined-task, however, is much big-ger, as it contains pairs for all utterances.
It consistsof 26,340 pairs, i.e.
a ratio of roughly 1:90.
For thisreason we also used balanced data-sets for training,where the ratio was adjusted to 1:25.3.3 FeaturesTable 2 lists the features we have used to representthe utterances.
(In this table, and in this section, wedenote the candidate for being a fragment with ?
andthe candidate for being ?
?s antecedent with ?.
)We have defined a number of structural fea-tures, which give information about the (discourse-)structural relation between ?
and ?.
The rationalebehind choosing them should be clear; iqu for ex-ample indicates in a weak way whether there mighthave been a topic change, and high nspk shouldpresumably make an antecedent relation between ?and ?
less likely.We have also used some lexical or utterance-based features, which describe lexical properties ofthe individual utterances and lexical relations be-tween them which could be relevant for the tasks.For example, the presence of a verb in ?
is presum-ably predictive for its being a fragment or not, asis the length.
To capture a possible semantic rela-tionship between the utterances, we defined two fea-tures.
The more direct one, rab, looks at verbatimre-occurrences of nouns from ?
in ?, which occurfor example in check-questions as in (4) below.
(4) A: I saw Peter.B: Peter?
(= Who is this Peter you saw?
)Less direct semantic relations are intended to becaptured by god, the second semantic feature weuse.7 It is computed as follows: for each pair (x, y)of nouns from ?
and ?, Google is called (via theGoogle API) with a query for x, for y, and for x andy together.
The similarity then is the average ratio ofpair vs. individual term:Google Similarity(x, y) = (hits(x, y)hits(x) +hits(x, y)hits(y) )?12We now describe the experiments we performedand their results.4 Experiments and Results4.1 Experimental SetupFor the learning experiments, we used three classi-fiers on all data-sets for the the three tasks:?
SLIPPER (Simple Learner with Iterative Prun-ing to Produce Error Reduction), (Cohen and Singer,1999), which is a rule learner which combinesthe separate-and-conquer approach with confidence-rated boosting.
It is unique among the classifiers that7The name is short for google distance, which indicates itsrelatedness to the feature used by (Poesio et al, 2004); it is how-ever a measure of similarity, not distance, as described above.250we have used in that it can make use of ?set-valued?features, e.g.
strings; we have run this learner bothwith only the features listed above and with the ut-terances (and POS-tags) as an additional feature.?
TIMBL (Tilburg Memory-Based Learner),(Daelemans et al, 2003), which implements amemory-based learning algorithm (IB1) which pre-dicts the class of a test data point by looking at itsdistance to all examples from the training data, us-ing some distance metric.
In our experiments, wehave used the weighted-overlap method, which as-signs weights to all features.?
MAXENT, Zhang Le?s C++ implementation8 ofmaximum entropy modelling (Berger et al, 1996).In our experiments, we used L-BFGS parameter es-timation.We also implemented a na?
?ve bayes classifier andran it on the fragment-task, with a data-set consistingonly of the strings and POS-tags.To determine the contribution of all features, weused an iterative process similar to the one describedin (Kohavi and John, 1997; Strube and Mu?ller,2003): we start with training a model using a base-line set of features, and then add each remainingfeature individually, recording the gain (w.r.t.
the f-measure (f(0.5), to be precise)), and choosing thebest-performing feature, incrementally until no fur-ther gain is recorded.
All individual training- andevaluation-steps are performed using 8-fold cross-validation (given the small number of positive in-stances, more folds would have made the number ofinstances in the test set set too small).The baselines were as follows: for the fragment-task, we used bvb and lbe as baseline, i.e.
we letthe classifier know the length of the candidate andwhether the candidate contains a verb or not.
Forthe antecedent-task we tested a very simple baseline,containing only of one feature, the distance between?
and ?
(dis).
The baseline for the combined-task, finally, was a combination of those two base-lines, i.e.
bvb+lbe+dis.
The full feature-set forthe fragment-task was lbe, bvb, bpr, nrb,bft, bds (since for this task there was no ?
tocompute features of), for the two other tasks it wasthe complete set shown in Table 2.8Available from http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.4.2 ResultsThe Tables 3?5 show the results of the experiments.The entries are roughly sorted by performance of theclassifier used; for most of the classifiers and data-sets for each task we show the performance for base-line, intermediate feature set(s), and full feature-set,for the rest we only show the best-performing set-ting.
We also indicate whether a balanced or unbal-anced data set was used.
I.e., the first three linesin Table 3 report on MaxEnt on a balanced data setfor the fragment-task, giving results for the baseline,baseline+nrb+bft, and the full feature-set.We begin with discussing the fragment task.
AsTable 3 shows, the three main classifiers performroughly equivalently.
Re-balancing the data, as ex-pected, boosts recall at the cost of precision.
For allsettings (i.e., combinations of data-sets, feature-setsand classifier), except re-balanced maxent, the base-line (verb in ?
yes/no, and length of ?)
already hassome success in identifying fragments, but addingthe remaining features still boosts the performance.Having available the string (condition s.s; slipperwith set valued features) interestingly does not helpSLIPPER much.Overall the performance on this task is not great.Why is that?
An analysis of the errors made showstwo problems.
Among the false negatives, there is ahigh number of fragments like ?yeah?
and ?mhm?,which in their particular context were answers toquestions, but that however occur much more of-ten as backchannels (true negatives).
The classifier,without having information about the context, can ofcourse not distinguish between these cases, and goesfor the majority decision.
Among the false positives,we find utterances that are indeed non-sentential,but for which no antecedent was marked (as in (3)above), i.e., which are not fragments in our narrowsense.
It seems, thus, that the required distinctionsare not ones that can be reliably learnt from lookingat the fragments alone.The antecedent-task was handled more satisfac-torily, as Table 4 shows.
For this task, a na?
?ve base-line (?always take previous utterance?)
preforms rel-atively well already; however, all classifiers wereable to improve on this, with a slight advantage forthe maxent model (f(0.5) = 0.76).
As the entryfor MaxEnt shows, adding to the baseline-features251Data Set Cl.
Recall Precision f(0.5) f(1.0) f(2.0)B; bl m 0.00 0.00 0.00 0.00 0.00B; bl+nrb+bft m 36.39 31.16 0.31 0.33 0.35B; all m 40.61 44.10 0.43 0.42 0.41UB; all m 22.13 65.06 0.47 0.33 0.25B; bl t 31.77 21.20 0.22 0.24 0.28B; bl+nrb+bpr+bds t 42.18 41.26 0.41 0.42 0.42B; all t 44.54 32.74 0.34 0.37 0.41UB; bl+nrb t 26.22 59.05 0.47 0.36 0.29B; bl s 21.07 16.95 0.17 0.18 0.20B; bl+nrb+bft+bds s 36.37 49.28 0.46 0.41 0.38B; all s 36.67 43.31 0.42 0.40 0.38UB; bl+nrb s 28.28 57.88 0.48 0.38 0.31B s.s 32.57 42.96 0.40 0.36 0.34B b 55.62 19.75 0.23 0.29 0.41UB b 66.50 20.00 0.23 0.31 0.45Table 3: Results for the fragment task.
(Cl.
= classifier used, where s = slipper, s.s = slipper + set-valuedfeatures, t = timbl, m = maxent, b = naive bayes; UB/B = (un)balanced training data.
)Data Set Cl.
Recall Precision f(0.5) f(1.0) f(2.0)dis=1 - 44.95 44.81 0.45 0.45 0.45UB; bl m 0 0 0.0 0.0 0.0UB; bl+awh m 43.21 52.90 0.50 0.47 0.45UB; bl+awh+god m 36.98 75.31 0.62 0.50 0.41UB; bl+awh+god+lbe+lal+iqu+nra+buh m 64.26 80.39 0.76 0.71 0.67UB; all m 58.16 73.57 0.69 0.64 0.60UB; bl s 0.00 0.00 0.00 0.00 0.00UB; bl+aqm s 36.65 78.44 0.63 0.49 0.41UB; bl+aqm+rab+iqu+lal s 49.72 79.75 0.71 0.61 0.54UB; all s 49.43 72.57 0.66 0.58 0.52UB; bl t 0 0 0.0 0.0 0.0UB; bl+aqm t 36.98 73.58 0.61 0.49 0.41UB; bl+aqm+awh+rab+iqu t 46.41 77.65 0.68 0.58 0.50UB; all t 60.57 58.74 0.59 0.60 0.60Table 4: Results for the antecedent task.Data Set Cl.
Recall Precision f(0.5) f(1.0) f(2.0)B; bl m 0.00 0.00 0.00 0.00 0.00B; bl+rap m 5.83 40.91 0.18 0.10 0.07B; bl+rap+god m 7.95 55.83 0.25 0.14 0.10B; bl+rap+god+nspk m 11.70 49.15 0.30 0.19 0.14B; bl+rap+god+nspk+alt+awh+nra+lal m 20.27 50.02 0.38 0.28 0.23B; all m 23.29 43.79 0.36 0.30 0.25UB; bl+rap+god+nspk+iqu+nra+bds+rab+awh m 13.01 54.87 0.33 0.21 0.15B; bl s 0.00 0.00 0.00 0.00 0.00B; bl+god s 11.80 35.60 0.25 0.17 0.13B; bl+god+bds s 14.44 46.98 0.32 0.22 0.17B; all s 17.78 41.96 0.32 0.24 0.20UB; bl+alt+bds+god+sspk+rap s 11.37 56.34 0.31 0.19 0.13B; bl t 0.00 0.00 0.00 0.00 0.00B; bl+god t 17.20 29.09 0.25 0.21 0.19B; all t 17.87 19.97 0.19 0.19 0.18UB; bl+god+iqu+rab t 14.24 41.63 0.29 0.21 0.16B; bl+rab+buh s.s 8.63 54.20 0.26 0.15 0.10Table 5: Results for the combined task.252information about whether ?
is a question or not al-ready boost the performance considerably.
An anal-ysis of the predictions of this model then indeedshows that it already captures cases of question andanswer pairs quite well.
Adding the similarity fea-ture god then gives the model information aboutsemantic relatedness, which, as hypothesised, cap-tures elaboration-type relations (as in (1-b) and (1-c)above).
Structural information (iqu) further im-proves the model; however, the remaining featuresonly seem to add interfering information, for perfor-mance using the full feature-set is worse.If one of the problems of the fragment-task wasthat information about the context is required to dis-tinguish fragments and backchannels, then the hopecould be that in the combined-task the classifierwould able to capture these cases.
However, the per-formance of all classifiers on this task is not satis-factory, as Table 5 shows; in fact, it is even slightlyworse than the performance on the fragment taskalone.
We speculate that instead of of cancelling outmistakes in the other part of the task, the two goals(let ?
be a fragment, and ?
a typical antecedent) in-terfere during optimisation of the rules.To summarise, we have shown that the task ofidentifying the antecedent of a given fragment islearnable, using a feature-set that combines struc-tural and lexical features; in particular, the inclusionof a measure of semantic relatedness, which wascomputed via queries to an internet search engine,proved helpful.
The task of identifying (resolution-via-identity) fragments, however, is hindered by thehigh number of non-sentential utterances which canbe confused with the kinds of fragments we are in-terested in.
Here it could be helpful to have a methodthat identifies and filters out backchannels, presum-ably using a much more local mechanism (as for ex-ample proposed in (Traum, 1994)).
Similarly, theperformance on the combined task is low, also dueto a high number of confusions of backchannels andfragments.
We discuss an alternative set-up below.5 Related WorkTo our knowledge, the tasks presented here have sofar not been studied with a machine learning ap-proach.
The closest to our problem is (Ferna?ndez etal., 2004b), which discusses classifying certain typesof fragments, namely questions of the type ?Who??,?When?
?, etc.
(sluices).
However, that paper doesnot address the task of identifying those in a cor-pus (which in any case should be easier than ourfragment-task, since those fragments cannot be con-fused with backchannels).Overlapping from another direction is the workpresented in (Zechner and Lavie, 2001), where thetask of aligning questions and answers is tackled.This subsumes the task of identifying question-antecedents for short-answers, but again is presum-ably somewhat simpler than our general task, be-cause questions are easier to identify.
The authorsalso evaluate the use of the alignment of questionsand answers in a summarisation system, and reportan increase in summary fluency, without a compro-mise in informativeness.
This is something we hopeto be able to show for our tasks as well.There are also similarities, especially of the an-tecedent task, to the pronoun resolution task (seee.g.
(Strube and Mu?ller, 2003; Poesio et al, 2004)).Interestingly, our results for the antecedent task areclose to those reported for that task.
The problem ofidentifying the units in need of an antecedent, how-ever, is harder for us, due to the problem of therebeing a large number of non-sentential utterancesthat cannot be linked to a single utterance as an-tecedent.
In general, this seems to be the main differ-ence between our task and the ones mentioned here,which concentrate on more easily identified mark-ables (questions, sluices, and pronouns).6 Conclusions and Further WorkWe have presented a machine learning approachto the task of identifying fragments and their an-tecedents in multi-party dialogue.
This represents awell-defined subtask of computing discourse struc-ture, which to our knowledge has not been studied sofar.
We have shown that the task of identifying theantecedent of a given fragment is learnable, usingfeatures that provide information about the structureof the discourse between antecedent and fragment,and about semantic closeness.The other tasks, identifying fragments and thecombined tasks, however, did not perform as well,mainly because of a high rate of confusions be-tween general non-sentential utterances and frag-253ments (in our sense).
In future work, we will trya modified approach, where the detection of frag-ments is integrated with a classification of utterancesas backchannels, fragments, or full sentences, andwhere the antecedent task only ranks pairs, leavingopen the possibility of excluding a supposed frag-ment by using contextual information.
Lastly, weare planning to integrate our classifier into a pro-cessing pipeline after the pronoun resolution step,to see whether this would improve both our perfor-mance and the quality of automatic meeting sum-marisations.9ReferencesAdam L. Berger, Stephen Della Pietra, and Vincent J. DellaPietra.
1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22(1):39?71.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Linguistics,22(2):249?254.William Cohen and Yoram Singer.
1999.
A simple, fast, andeffective rule learner.
In Proceedings of the Sixteenth Na-tional Conference on Artificial Intelligence (AAAI-99), Or-lando, Florida, July.
AAAI.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2003.
TiMBL: Tilburgmemory based learner, version 5.0, reference guide.ILC Technical Report 03-10, Induction of Linguis-tic Knowledge; Tilburg University.
Available fromhttp://ilk.uvt.nl/downloads/pub/...papers/ilk0310.pdf.Raquel Ferna?ndez and Jonathan Ginzburg.
2002.
Non-sentential utterances in dialogue: A corpus-based study.
InKristiina Jokinen and Susan McRoy, editors, Proceedingsof the Third SIGdial Workshop on Discourse and Dialogue,pages 15?26, Philadelphia, USA, July.
ACL Special InterestGroup on Dialog.Raquel Ferna?ndez, Jonathan Ginzburg, Howard Gregory, andShalom Lappin.
2004a.
Shards: Fragment resolution indialogue.
In H. Bunt and R. Muskens, editors, ComputingMeaning, volume 3.
Kluwer.Raquel Ferna?ndez, Jonathan Ginzburg, and Shalom Lappin.2004b.
Classifying ellipsis in dialogue: A machine learn-ing approach.
In Proceedings of COLING 2004, Geneva,Switzerland, August.John S. Garofolo, Christophe D. Laprun, Martial Michel, Vin-cent M. Stanford, and Elham Tabassi.
2004.
The NITS9Acknowledgements: We would like to acknowledge help-ful discussions with Jason Baldridge and Michael Strube duringthe early stages of the project, and helpful comments from theanonymous reviewers.meeting room pilot corpus.
In Proceedings of the Interna-tional Language Resources Conference (LREC04), Lisbon,Portugal, May.J.J.
Godfrey, E. C. Holliman, and J. McDaniel.
1992.
SWITCH-BOARD: Telephone speech corpus for research and devlop-ment.
In Proceedings of the IEEE Conference on Acoustics,Speech, and Signal Processing, pages 517?520, San Fran-cisco, USA, March.Ron Kohavi and George H. John.
1997.
Wrappers for featureselection.
Artificial Intelligence Journal, 97(1?2):273?324.Christoph Mu?ller and Michael Strube.
2001.
MMAX: A Toolfor the Annotation of Multi-modal Corpora.
In Proceedingsof the 2nd IJCAI Workshop on Knowledge and Reasoningin Practical Dialogue Systems, pages 45?50, Seattle, USA,August.Massimo Poesio, Rahul Mehta, Axel Maroudas, and JanetHitzeman.
2004.
Learning to resolve bridging refer-ences.
In Proceedings of the 42nd annual meeting of theAssociation for Computational Linguistics, pages 144?151,Barcelona, Spain, July.David Schlangen and Alex Lascarides.
2002.
Resolvingfragments using discourse information.
In Johan Bos,Mary Ellen Foster, and Colin Matheson, editors, Proceed-ings of the 6th International Workshop on Formal Semanticsand Pragmatics of Dialogue (EDILOG 2002), pages 161?168, Edinburgh, September.David Schlangen and Alex Lascarides.
2003.
The interpreta-tion of non-sentential utterances in dialogue.
In AlexanderRudnicky, editor, Proceedings of the 4th SIGdial workshopon Discourse and Dialogue, Sapporo, Japan, July.David Schlangen.
2003.
A Coherence-Based Approach tothe Interpretation of Non-Sentential Utterances in Dialogue.Ph.D.
thesis, School of Informatics, University of Edin-burgh, Edinburgh, UK.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging us-ing decision trees.
In Proceedings of the International Con-ference on New Methods in Language Processing, Manch-ester, UK.Michael Strube and Christoph Mu?ller.
2003.
A machine learn-ing approach to pronoun resolution in spoken dialogue.
InProceedings of the 41st Annual Meeting of the Associationfor Computational Lingustics, Sapporo, Japan.D.
Traum and P. Heeman.
1997.
Utterance units in spokendialogue.
In E. Maier, M. Mast, and S. LuperFoy, editors,Dialogue Processing in Spoken Language Systems, LectureNotes in Artificial Intelligence.
Springer-Verlag.David R. Traum.
1994.
A Computational Theory of Groundingin Natural Language Conversation.
Ph.D. thesis, ComputerScience, University of Rochester, Rochester, USA, Decem-ber.Klaus Zechner and Anton Lavie.
2001.
Increasing the coher-ence of spoken dialogue summaries by cross-speaker infor-mation linking.
In Proceedings of the NAAACL Workshopon Automatic Summarisation, Pittsburgh, USA, June.254
