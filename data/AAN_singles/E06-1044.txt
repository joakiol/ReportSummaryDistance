Modelling Semantic Role Plausibility in Human Sentence ProcessingUlrike Pad?
and Matthew CrockerComputational LinguisticsSaarland University66041 Saarbr?ckenGermany{ulrike,crocker}@coli.uni-sb.deFrank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKkeller@inf.ed.ac.ukAbstractWe present the psycholinguistically moti-vated task of predicting human plausibilityjudgements for verb-role-argument triplesand introduce a probabilistic model thatsolves it.
We also evaluate our model onthe related role-labelling task, and com-pare it with a standard role labeller.
Forboth tasks, our model benefits from class-based smoothing, which allows it to makecorrect argument-specific predictions de-spite a severe sparse data problem.
Thestandard labeller suffers from sparse dataand a strong reliance on syntactic cues, es-pecially in the prediction task.1 IntroductionComputational psycholinguistics is concernedwith modelling human language processing.Much work has gone into the exploration of sen-tence comprehension.
Syntactic preferences thatunfold during the course of the sentence have beensuccessfully modelled using incremental proba-bilistic context-free parsing models (e.g., Jurafsky,1996; Crocker and Brants, 2000).
These modelsassume that humans prefer the most likely struc-tural alternative at each point in the sentence.
Ifthe preferred structure changes during processing,such models correctly predict processing difficultyfor a range of experimentally investigated con-structions.
They do not, however, incorporate anexplicit notion of semantic processing, while thereare many phenomena in human sentence process-ing that demonstrate a non-trivial interaction ofsyntactic preferences and semantic plausibility.Consider, for example, the well-studied case ofreduced relative clause constructions.
When incre-mentally processing the sentence The deer shot bythe hunter was used as a trophy, there is a localambiguity at shot between continuation as a mainclause (as in The deer shot the hunter) or as a re-duced relative clause modifying deer (equivalentto The deer which was shot .
.
.
).
The main clausecontinuation is syntactically more likely.However, there is a second, semantic clue pro-vided by the high plausibility of deer being shotand the low plausibility of them shooting.
Thisinfluences readers to choose the syntactically dis-preferred reduced relative reading which interpretsthe deer as an object of shot (McRae et al, 1998).Plausibility has overridden the syntactic default.On the other hand, for a sentence like The huntershot by the teenager was only 30 years old, se-mantic plausibility initially reinforces the syntac-tic main clause preference and readers show diffi-culty accommodating the subsequent disambigua-tion towards the reduced relative.In order to model effects like these, we needto extend existing models of sentence process-ing by introducing a semantic dimension.
Pos-sible ways of integrating different sources of in-formation have been presented e.g.
by McRaeet al (1998) and Narayanan and Jurafsky (2002).Our aim is to formulate a model that reliably pre-dicts human plausibility judgements from corpusresources, in parallel to the standard practice ofbasing the syntax component of psycholinguisticmodels on corpus probabilities or even probabilis-tic treebank grammars.
We can then use both thesyntactic likelihood and the semantic plausibilityscore to predict the preferred syntactic alterna-tive, thus accounting for the effects shown e.g.
byMcRae et al (1998).Independent of a syntactic model, we want anysemantic model we define to satisfy two criteria:First, it needs to be able to make predictions in-345crementally, in parallel with the syntactic model.This entails dealing with incomplete or unspeci-fied (syntactic) information.
Second, we want toextend to semantics the assumption made in syn-tactic models that the most probable alternative isthe one preferred by humans.
The model thereforemust be probabilistic.We present such a probabilistic model that canassign roles incrementally as soon as a predicate-argument pair is seen.
It uses the likelihood of the-matic role assignments to model human interpre-tation of verb-argument relations.
Thematic rolesare a description of the link between verb and ar-gument at the interface between syntax and se-mantics.
Thus, they provide a shallow level ofsentence semantics which can be learnt from an-notated corpora.We evaluate our model by verifying that it in-deed correctly predicts human judgements, and bycomparing its performance with that of a standardrole labeller in terms of both judgement predictionand role assignment.
Our model has two advan-tages over the standard labeller: It does not relyon syntactic features (which can be hard to comeby in an incremental task) and our smoothing ap-proach allows it to make argument-specific rolepredictions in spite of extremely sparse trainingdata.
We conclude that (a) our model solves thetask we set, and (b) our model is better equippedfor our task than a standard role labeller.The outline of the paper is as follows: Afterdefining the prediction task more concretely (Sec-tion 2), we present our simple probabilistic modelthat is tailoured to the task (Section 3).
We in-troduce our test and training data in Section 4.
Itbecomes evident immediately that we face a se-vere sparse data problem, which we tackle on twolevels: By smoothing the distribution and by ac-quiring additional counts for sparse cases.
Thesmoothed model succeeds on the prediction task(Section 5).
Finally, in Section 6, we compare ourmodel to a standard role labeller.2 The Judgement Prediction TaskWe can measure our intuitions about the plau-sibility of hunters shooting and deer being shotin terms of plausibility judgements for verb-role-argument triples.
Two example items from McRaeet al (1998) are presented in Table 1.
The judge-ments were gathered by asking raters to assign avalue on a scale from 1 (not plausible) to 7 (veryVerb Noun Role Ratingshoot hunter agent 6.9shoot hunter patient 2.8shoot deer agent 1.0shoot deer patient 6.4Table 1: Test items: Verb-noun pairs with ratingson a 7 point scale from McRae et al (1998).plausible) to questions like How common is it fora hunter to shoot something?
(subject reading:hunter must be agent) or How common is it for ahunter to be shot?
(object reading: hunter must bepatient).
The number of ratings available in eachof our three sets of ratings is given in Table 2 (seealso Section 4).The task for our model is to correctly predict theplausibility of each verb-role-argument triple.
Weevaluate this by correlating the model?s predictedvalues and the judgements.
The judgement datais not normally distributed, so we correlate usingSpearman?s ?
(a non-parametric rank-order test).The ?
value ranges between 0 and 1 and indicatesthe strength of association between the two vari-ables.
A significant positive value indicates thatthe model?s predictions are accurate.3 A Model of Human PlausibilityJudgementsWe can formulate a model to solve the predictiontask if we equate the plausibility of a role assign-ment to a verb-argument pair with its probability,as suggested above.
This value is influenced aswell by the verb?s semantic class and the grammat-ical function of the argument.
The plausibility fora verb-role-argument triple can thus be estimatedas the joint probability of the argument head a, therole r, the verb v, the verb?s semantic class c andthe grammatical function g f of a:Plausibilityv,r,a = P(r,a,v,c,g f )This joint probability cannot be easily estimatedfrom co-occurrence counts due to lack of data.But we can decompose this term into a numberof subterms that approximate intuitively impor-tant information such as syntactic subcategorisa-tion (P(g f |v,c)), the syntactic realisation of a se-mantic role (P(r|v,c,g f )) and selectional prefer-ences (P(a|v,c,g f ,r)):Plausibilityv,r,a = P(r,a,v,c,g f ) =P(v) ?P(c|v) ?P(g f |v,c) ?P(r|v,c,g f ) ?P(a|v,c,g f ,r)346shoot.02: [The hunter Arg0] shot [the deer Arg1].Killing: [The hunter Killer] shot [the deer Victim].Figure 1: Example annotation: PropBank (above)and FrameNet (below).Each of these subterms can be estimated more eas-ily from the semantically annotated training datasimply using the maximum likelihood estimate.However, we still need to smooth our estimates,especially as the P(a|v,c,g f ,r) term remains verysparse.
We describe our use of two complemen-tary smoothing methods in Section 5.Our model fulfils the requirements we havespecified: It is probabilistic, able to work incre-mentally as soon as a single verb-argument pairis available, and can make predictions even if theinput information is incomplete.
The model gen-erates the missing values if, e.g., the grammaticalfunction or the verb?s semantic class are not spec-ified.
This means that we can immediately evalu-ate on the judgement data without needing furtherverb sense or syntactic information.4 Test and Training dataTraining Data To date, there are two mainannotation efforts that have produced semanti-cally annotated corpora: PropBank (PB) andFrameNet (FN).
Their approaches to annotationdiffer enough to warrant a comparison of the cor-pora as training resources.
Figure 1 gives an exam-ple sentence annotated in PropBank and FrameNetstyle.
The PropBank corpus (c. 120,000 propo-sitions, c. 3,000 verbs) adds semantic annotationto the Wall Street Journal part of the Penn Tree-bank.
Arguments and adjuncts are annotated forevery verbal proposition in the corpus.
A commonset of argument labels Arg0 to Arg5 and ArgM(adjuncts) is interpreted in a verb-specific way.Some consistency in mapping has been achieved,so that Arg0 generally denotes agents and Arg1patients/themes.The FrameNet corpus (c. 58,000 verbal propo-sitions, c. 1,500 verbs in release 1.1) groups verbswith similar meanings together into frames (i.e.descriptions of situations) with a set of frame-specific roles for participants and items involved(e.g.
a killer, instrument and victim in the Killingframe).
Both the definition of frames as semanticverb classes and the semantic characterisation offrame-specific roles introduces a level of informa-tion that is not present in PropBank.
Since corpusannotation is frame-driven, only some senses of averb may be present and word frequencies may notbe representative of English.Test Data Our main data set consists of 160 datapoints from McRae et al (1998) that were splitrandomly into a 60 data point development set anda 100 data point test set.
The data is made up oftwo arguments per verb and two ratings for eachverb-argument pair, one for the subject and onefor the object reading of the argument (see Section2).
Each argument is highly plausible in one ofthe readings, but implausible in the other (recallTable 1).
Human ratings are on a 7-point scale.In order to further test the coverage of ourmodel, we also include 76 items from Trueswellet al (1994) with one highly plausible object perverb and a rating each for the subject and objectreading of the argument.
The data were gath-ered in the same rating study as the McRae etal.
data, so we can assume consistency of the rat-ings.
However, in comparison to the McRae dataset, the data is impoverished as it lacks ratings forplausible agents (in terms of the example in Ta-ble 1, this means there are no ratings for hunter).Lastly, we use 180 items from Keller and Lapata(2003).
In contrast with the previous two studies,the verbs and nouns for these data were not hand-selected for the plausibility of their combination.Rather, they were extracted from the BNC corpusby frequency criteria: Half the verb-noun combi-nations are seen in the BNC with high, mediumand low frequency, half are unseen combinationsof the verb set with nouns from the BNC.
Thedata consists of ratings for 30 verbs and 6 argu-ments each, interpreted as objects.
The humanratings were gathered using the Magnitude Esti-mation technique (Bard et al, 1996).
This dataset alows us to test on items that were not hand-selected for a psycholinguistic study, even thoughthe data lacks agenthood ratings and the items arepoorly covered by the FrameNet corpus.All test pairs were hand-annotated withFrameNet and PropBank roles following thespecifications in the FrameNet on-line databaseand the PropBank frames files.1The judgement prediction task is very hard tosolve if the verb is unseen during training.
Back-ing off to syntactic information or a frequency1Although a single annotator assigned the roles, the anno-tation should be reliable as roles were mostly unambiguousand the annotated corpora were used for reference.347Total RevisedSource FN PBMcRae et al (1998) 100 64 (64%) 92 (92%)Trueswell et al (1994) 76 52 (68.4%) 72 (94.7%)Keller and Lapata (2003) 180 ?
162 (90%)Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seenverbs (% of total ratings).
?
: Coverage too low (26.7%).baseline only works if the role set is small and syn-tactically motivated, which is the case for Prop-Bank, but not FrameNet.
We present results bothfor the complete test sets and and for revised setscontaining only items with seen verbs.
Exclud-ing unseen verbs seems justified for FrameNet andhas little effect for the PropBank corpus, since itscoverage is generally much better.
Table 2 showsthe total number of ratings for each test set andthe sizes of the revised test sets containing onlyitems with seen verbs.
FrameNet alays has sub-stantially lower coverage.
Since only 27% of theverbs in the Keller & Lapata items are covered inFrameNet, we do not test this combination.5 Experiment 1: Smoothing MethodsWe now turn to evaluating our model.
It is im-mediately clear that we have a severe sparse dataproblem.
Even if all the verbs are seen, the com-binations of verbs and arguments are still mostlyunseen in training for all data sets.We describe two complementary approachesto smoothing sparse training data.
One, Good-Turing smoothing, approaches the problem of un-seen data points by assigning them a small proba-bility.
The other, class-based smoothing, attemptsto arrive at semantic generalisations for words.These serve to identify equivalent verb-argumentpairs that furnish additional counts for the estima-tion of P(a|v,c,g f ,r).5.1 Good-Turing Smoothing and LinearInterpolationWe first tackle the sparse data problem by smooth-ing the distribution of co-occurrence counts.
Weuse the Good-Turing re-estimate on zero and onecounts to assign a small probability to unseenevents.
This method relies on re-estimating theprobability of seen and unseen events based onknowledge about more frequent events.Adding Linear Interpolation We also exper-imented with the linear interpolation method,which is typically used for smoothing n-grammodels.
It re-estimates the probability of the n-gram in question as a weighted combination of then-gram, the n-1-gram and the n-2-gram.
For ex-ample, P(a|v,c,g f ,r) is interpolated asP(a|v,c,g f ,r) = ?1P(a|v,c,g f ,r)+?2P(a|v,c,r)+?3P(a|v,c)The ?
values were estimated on the trainingdata, separately for each of the model?s four con-ditional probability terms, by maximising five-foldcross-validation likelihood to avoid overfitting.We smoothed all model terms using the Good-Turing method and then interpolated the smoothedterms.
Table 3 lists the test results for both train-ing corpora and all test sets when Good-Turingsmoothing (GT) is used alone and with linear in-terpolation (GT/LI).
We also give the unsmoothedcoverage and correlation.
The need for smoothingis obvious: Coverage is so low that we can onlycompute correlations in two cases, and even forthose, less than 20% of the data are covered.GT smoothing alone always outperforms thecombination of GT and LI smoothing, especiallyfor the FrameNet training set.
Maximising thedata likelihood during ?
estimation does not ap-proximate our final task well enough: The loglikelihood of the test data is duly improved from?797.1 to ?772.2 for the PropBank data and from?501.9 to ?446.3 for the FrameNet data.
How-ever, especially for the FrameNet training data,performance on the correlation task diminishes asdata probability rises.
A better solution might beto use the correlation task directly as a ?
estima-tion criterion, but this is much more complex, re-quiring us to estimate all ?
terms simultaneously.Also, the main problem seems to be that the ?
in-terpolation smoothes by de-emphasising the mostspecific (and sparsest) term, so that, on our finaltask, the all-important argument-specific informa-tion is not used efficiently when it is available.
Wetherefore restrict ourselves to GT smoothing.348Smoothed UnsmoothedTrain Smoothing Test Coverage ?
Coverage ?PBGTMcRae 93.5% (86%) 0.112, ns 2% (2%) ?Trueswell 100% (94.7%) 0.454, ** 17% (16%) nsKeller&Lapata 100% (90%) 0.285, ** 5% (4%) 0.727, *GT/LIMcRae 93.5% (86%) 0.110, ns 2% (2%) ?Trueswell 100% (94.7%) 0.404, ** 17% (16%) nsKeller&Lapata 100% (90%) 0.284, ** 5% (4%) 0.727, *FNGT McRae 87.5% (56%) 0.164, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.046, ns 6% (4%) ?GT/LI McRae 87.5% (56%) 0.042, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.009, ns 6% (4%) ?Table 3: Experiment 1, GT and Interpolation smoothing.
Coverage on seen verbs (and on all items) andcorrelation strength (Spearman?s ?
for PB and FN data on all test sets.
?
: too few data points, ns: notsignificant, *: p < 0.05, **: p < 0.01.Model Performance Both versions of thesmoothed model make predictions for all seenverbs; the remaining uncovered data points arethose where the correct role is not accounted forin the training data (the verb may be very sparseor only seen in a different FrameNet frame).
Forthe FrameNet training data, there are no significantcorrelations, but for the PropBank data, we seecorrelations for the Trueswell and Keller&Lapatasets.
One reason for the good performance ofthe PB-Trueswell and PB-Keller&Lapata combi-nations is that in the PropBank training data, theobject role generally seems to be the most likelyone.
If the most specific probability term is sparseand expresses no role preference (which is the casefor most items: see Unsmoothed Coverage), ourmodel is biased towards the most likely role giventhe verb, semantic class and grammatical function.Recall that the Trueswell and Keller&Lapata datacontain ratings for (plausible) objects only, so thatpreferring the patient role is a good strategy.
Thisalso explains why the model performs worse forthe McRae et al data, which also has ratings forgood agents (and bad patients).
On FrameNet, thispreference for ?patient?
roles is not as marked, sothe FN-Trueswell case does not behave like thePB-Trueswell case.5.2 Class-Based SmoothingIn addition to smoothing the training distribution,we also attempt to acquire more counts to es-timate each P(a|v,c,g f ,r) by generalising fromtokens to word classes.
The term we estimatebecomes P(classa|classv,g f ,r).
This allows usto make argument-specific predictions as we donot rely on a uniform smoothed term for unseenP(a|v,c,g f ,r) terms.
We use lexicographic nounclasses from WordNet and verb classes inducedby soft unsupervised clustering, which outperformlexicographic verb classes.Noun Classes We tested both the coarsest andthe finest noun classification available in Word-Net, namely the top-level ontology and the nounsynsets which contain only synonyms of the targetword.2 The top-level ontology proved to overgen-erate alternative nouns, which raises coverage butdoes not produce meaningful role predictions.
Wetherefore use the noun synsets below.Verb Classes Verbs are clustered according tolinguistic context information, namely argumenthead lemmas, the syntactic configuration of verband argument, the verb?s semantic class, the goldrole information and a combined feature of goldrole and syntactic configuration.
The evaluation ofthe clustering task itself is task-based: We choosethe clustering configuration that produces optimalresults in the the prediction task on the McRae de-velopment set.
The base corpus for clustering wasalways used for frequency estimation.We used an implementation of two soft clus-tering algorithms derived from information the-ory (Marx, 2004): the Information Distortion (ID)(Gedeon et al, 2003) and Information Bottleneck(IB) (Tishby et al, 1999) methods.
Soft cluster-ing allows us to take verb polysemy into accountthat is often characterised by different patterns ofsyntactic behaviour for each verb meaning.2For ambiguous nouns, we chose the sense that led to thehighest probability for the current role assignment.349A number of parameters were set on the devel-opment set, namely the clustering algorithm, thesmoothing method within the algorithms and thenumber of clusters within each run.
For our task,the IB algorithm generally yielded better results.We decided which clustering parametrisationsshould be tried on the test sets based on the notionof stability: Both algorithms increase the numberof clusters by one at each iteration.
Thus, eachparametrisation yields a series of cluster configu-rations as the number of iterations increases.
Wechose those parametrisations where a series of atleast three consecutive cluster configurations re-turned significant correlations on the developmentset.
This should be an indication of a generalisablesuccess, rather than a fluke caused by peculiaritiesof the data.
On the test sets, results are reportedfor the configuration (characterised by the itera-tion number) that returned the first significant re-sult in such a series on the development set, as thisis the most general grouping.5.3 Combining the Smoothing MethodsWe now present results for combining the GTand class-based smoothing methods.
We use in-duced verb classes and WordNet noun synsets forclass-based smoothing of P(a|v,c,g f ,r), and relyon GT smoothing if the counts for this term arestill sparse.
All other model terms are alwayssmoothed using the GT method.
Table 4 containsresults for three clustering configurations each forthe PropBank and FrameNet data that have provenstable on the development set.
We characterisethem by the clustering algorithm (IB or ID) andnumber of clusters.
Note that the upper bound forour ?
values, human agreement or inter-rater cor-relation, is below 1 (as indicated by a correlationof Pearson?s r = .640 for the seen pairs from theKeller and Lapata (2003) data).For the FrameNet data, there is a marked in-crease in performance for both test sets.
The hu-man judgements are now reliably predicted withgood coverage in five out of six cases.
Clearly,equivalent verb-argument counts have furnishedaccurate item-specific estimates.
On the PropBankdata set, class-based smoothing is less helpful: ?values generally drop slightly.
Apparently, theFrameNet style of annotation allows us to induceinformative verb classes, whereas the PropBankclasses introduce noise at most.6 Experiment 2: Role LabellingWe have shown that our model performs well onits intended task of predicting plausibility judge-ments, once we have proper smoothing methodsin place.
But since this task has some similarityto role labelling, we can also compare the modelto a standard role labeller on both the predictionand role labelling tasks.
The questions are: Howwell do we do labelling, and does a standard rolelabeller also predict human judgements?Beginning with work by Gildea and Jurafsky(2002), there has been a large interest in se-mantic role labelling, as evidenced by its adop-tion as a shared task in the Senseval-III compe-tition (FrameNet data, Litkowski, 2004) and atthe CoNLL-2004 and 2005 conference (PropBankdata, Carreras and M?rquez, 2005).
As our modelcurrently focuses on noun phrase arguments only,we do not adopt these test sets but continue to useours, defining the correct role label to be the onewith the higher probability judgement.
We evalu-ate the model on the McRae test set (recall that theother sets only contain good patients/themes andare therefore susceptible to labeller biases).We formulate frequency baselines for our train-ing data.
For PropBank, always assigning Arg1results in F = 45.7 (43.8 on the full test set).
ForFrameNet, we assign the most frequent role giventhe verb, so the baseline is F = 34.4 (26.8).We base our standard role labelling system onthe SVM labeller described in Giuglea and Mos-chitti (2004), although without integrating infor-mation from PropBank and VerbNet for FrameNetclassification as presented in their paper.
Thus, weare left with a set of fairly standard features, suchas phrase type, voice, governing category or paththrough parse tree from predicate.
These are usedto train two classifiers, one which decides whichphrases should be considered arguments and onewhich assigns role labels to these arguments.
TheSVM labeller?s F score on an unseen test set isF = 80.5 for FrameNet data when using gold ar-gument boundaries.
We also trained the labelleron the PropBank data, resulting in an F score ofF = 98.6 on Section 23, again on gold boundaries.We also evaluate the SVM labeller on the cor-relation task by normalising the scores that the la-beller assigns to each role and then correlating thenormalised scores to the human ratings.In order to extract features for the SVM labeller,we had to present the verb-noun pairs in full sen-350Train Test Verb Clusters Coverage ?PBMcRaeID 4 93.5% (86%) 0.097, nsIB 10 93.5% (86%) 0.104, nsIB 5 93.5% (86%) 0.107, nsTrueswellID 4 100% (94.7%) 0.419, **IB 10 100% (94.7%) 0.366, **IB 5 100% (94.7%) 0.439, **Keller&LapataID 4 100% (90%) 0.300, **IB 10 100% (90%) 0.255, **IB 5 100% (90%) 0.297, **FNMcRaeID 4 87.5% (56%) 0.304, *IB 9 87.5% (56%) 0.275, *IB 10 87.5% (56%) 0.267, *TrueswellID 4 76.9% (52.6%) 0.256, nsIB 9 76.9% (52.6%) 0.342, *IB 10 76.9% (52.6%) 0.365, *Table 4: Experiment 1: Combining the smoothing methods.
Coverage on seen verbs (and on all items)and correlation strength (Spearman?s ?)
for PB and FN data.
WN synsets as noun classes.
Verb classes:IB/ID: smoothing algorithm, followed by number of clusters.
ns: not significant, *: p<0.05, **: p<0.01tences, as the labeller relies on a number of fea-tures from parse trees.
We used the experimentalitems from the McRae et al study, which are alldisambiguated towards a reduced relative reading(object interpretation: The hunter shot by the ...)of the argument.
In doing this, we are potentiallybiasing the SVM labeller towards one label, de-pending on the influence of syntactic features onrole assignment.
We therefore also created a mainclause reading of the verb-argument pairs (sub-ject interpretation: The hunter shot the ...) andpresent the results for comparison.
For our model,we have previously not specified the grammaticalfunction of the argument, but in order to put bothmodels on a level playing field, we now supply thegrammatical function of Ext (external argument),which applies for both formulations of the items.Table 5 shows that for the labelling task, ourmodel outperforms the labelling baseline and theSVM labeller on the FrameNet data by at least16 points F score while the correlation with hu-man data remains significant.
For the PropBankdata, labelling performance is on baseline level,below the better of the two SVM labeller condi-tions.
This result underscores the usefulness ofargument-specific plausibility estimates furnishedby class-based smoothing for the FrameNet data.For the PropBank data, our model essentially as-signs the most frequent role for the verb.The performance of the SVM labeller suggestsa strong influence of syntactic features: On thePropBank data set, it always assigns the Arg0 la-bel if the argument was presented as a subject(this is correct in 50% of cases) and mostly theappropriate ArgN label if the argument was pre-sented as an object.
On FrameNet, performanceagain is above baseline only for the subject condi-tion, where there is also a clear trend for assign-ing agent-style roles.
(The object condition is lessclear-cut.)
This strong reliance on syntactic cues,which may be misleading for our data, makes thelabeller perform much worse than on the standardtest sets.
For both training corpora, it does nottake word-specific plausibility into account due todata sparseness and usually assigns the same roleto both arguments of a verb.
This precludes a sig-nificant correlation with the human ratings.Comparing the training corpora, we find thatboth models perform better on the FrameNet dataeven though there are many more role labels inFrameNet, and the SVM labeller does not profitfrom the greater smoothing power of FrameNetverb clusters.
Overall, FrameNet has proven moreuseful to us, despite its smaller size.In sum, our model does about as well (PB data)or better (FN data) on the labelling task as theSVM labeller, while the labeller does not solvethe prediction task.
The success of our model, es-pecially on the prediction task, stems partly fromthe absence of global syntactic features that biasthe standard labeller strongly.
This also makes ourmodel suited for an incremental task.
Instead of351Train Model Coverage ?
Labelling F Labelling Cov.PBBaseline ?
?
45.7 (43.8%) 100%SVM Labeller (subj) 100% (92%) ns 50 (47.9%) 100%SVM Labeller (obj) 100% (92%) ns 45.7 (43.8%) 100%IB 5 (subj/obj) 93.5% (86%) ns 45.7 (43.8%) 100%FNBaseline ?
?
34.4 (26.8%) 100%SVM Labeller (subj) 87.5% (56%) ns 40.6 (31.7%) 100%SVM Labeller (obj) 87.5% (56%) ns 34.4 (26.8%) 100%ID 4 (subj/obj) 87.5% (56%) 0.271, * 56.3 (43.9%) 100%Table 5: Experiment 2: Standard SVM labeller vs our model.
Coverage on seen verbs (and on all items),correlation strength (Spearman?s ?
), labelling F score and labelling coverage on seen verbs (and on allitems, if different) for PB and FN data on the McRae test set.
ns: not significant, *: p<0.05.syntactic cues, we successfully rely on argument-specific plausibility estimates furnished by class-based smoothing.
Our joint probability model hasthe further advantage of being conceptually muchsimpler than the SVM labeller, which relies on asophisticated machine learning paradigm.
Also,we need to compute only about one-fifth of thenumber of SVM features.7 ConclusionsWe have defined the psycholinguistically moti-vated task of predicting human plausibility ratingsfor verb-role-argument triples.
To solve it, wehave presented an incremental probabilistic modelof human plausibility judgements.
When we em-ploy two complementary smoothing methods, themodel achieves both good coverage and reliablecorrelations with human data.
Our model per-forms as well as or better than a standard role la-beller on the task of assigning the preferred role toeach item in our test set.
Further, the standard la-beller does not succeed on the prediction task, as itcannot overcome the extreme sparse data problem.Acknowledgements Ulrike Pad?
acknowledgesa DFG studentship in the International Post-Graduate College ?Language Technology andCognitive Systems?.
We thank Ana-Maria Giu-glea, Alessandro Moschitti and Zvika Marx formaking their software available and are grateful toAmit Dubey, Katrin Erk, Mirella Lapata and Se-bastian Pad?
for comments and discussions.ReferencesBard, E. G., Robertson, D., and Sorace, A.
(1996).
Magnitudeestimation of linguistic acceptability.
Language, 72(1),32?68.Carreras, X. and M?rquez, L. (2005).
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL-2005.Crocker, M. and Brants, T. (2000).
Wide-coverage proba-bilistic sentence processing.
Journal of PsycholinguisticResearch, 29(6), 647?669.Gedeon, T., Parker, A., and Dimitrov, A.
(2003).
Informationdistortion and neural coding.
Canadian Applied Mathe-matics Quarterly, 10(1), 33?70.Gildea, D. and Jurafsky, D. (2002).
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3), 245?288.Giuglea, A.-M. and Moschitti, A.
(2004).
Knowledge discov-ery using FrameNet, VerbNet and PropBank.
In Proceed-ings of the Workshop on Ontology and Knowledge Discov-ering at ECML 2004.Jurafsky, D. (1996).
A probabilistic model of lexical and syn-tactic access and disambiguation.
Cognitive Science, 20,137?194.Keller, F. and Lapata, M. (2003).
Using the web to obtain fre-quencies for unseen bigrams.
Computational Linguistics,29(3), 459?484.Litkowski, K. (2004).
Senseval-3 task: Automatic labeling ofsemantic roles.
In Proceedings of Senseval-3: The ThirdInternational Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.Marx, Z.
(2004).
Structure-Based computational aspects ofsimilarity and analogy in natural language.
Ph.D. thesis,Hebrew University, Jerusalem.McRae, K., Spivey-Knowlton, M., and Tanenhaus, M.(1998).
Modeling the influence of thematic fit (and otherconstraints) in on-line sentence comprehension.
Journalof Memory and Language, 38, 283?312.Narayanan, S. and Jurafsky, D. (2002).
A Bayesian modelpredicts human parse preference and reading time in sen-tence processing.
In S. B. T. G. Dietterich and Z. Ghahra-mani, editors, Advances in Neural Information ProcessingSystems 14, pages 59?65.
MIT Press.Tishby, N., Pereira, F. C., and Bialek, W. (1999).
The in-formation bottleneck method.
In Proc.
of the 37-th An-nual Allerton Conference on Communication, Control andComputing, pages 368?377.Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994).
Seman-tic influences on parsing: Use of thematic role informationin syntactic ambiguity resolution.
Journal of Memory andLanguage, 33, 285?318.352
