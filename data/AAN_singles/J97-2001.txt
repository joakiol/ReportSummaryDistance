Floating Constraints in Lexical ChoiceMichael Elhadad*Ben Gurion University in the NegevJacques Robin~Universidade Federal de PernambucoKathleen McKeown tColumbia UniversityLexical choice is a computationally complex task, requiring a generation system to considera potentially large number of mappings between concepts and words.
Constraints that aid indetermining which word is best come from a wide variety of sources, including syntax, semantics,pragmatics, the lexicon, and the underlying domain.
Furthermore, in some situations, differentconstraints come into play early on, while in others, they apply much later.
This makes it difficultto determine a systematic ordering in which to apply constraints.
In this paper, we present ageneral approach to lexical choice that can handle multiple, interacting constraints.
We focus onthe problem of floating constraints, emantic or pragmatic onstraints that float, appearing at avariety of different syntactic ranks, often merged with other semantic constraints.
This means thatmultiple content units can be realized by a single surface element, and conversely, that a singlecontent unit can be realized by a variety of surface elements.
Our approach uses the FunctionalUnification Formalism (Fu~) to represent a generation lexicon, allowing for declarative andcompositional representation findividual constraints.1.
IntroductionGiven a request o communicate, a language generator typically must select infor-mation from an underlying domain representation a d determine how to order thisinformation, ultimately realizing the representation as sentences by selecting wordsand linearly ordering them under the syntactic onstraints of the language.
The prob-lem of determining what words to use for the concepts in the domain representationis termed lexical choice.
In an effort to make domain representations i dependent oflanguage, there may be a variety of different words that can be used to express anyconcept in the domain, and a language generator must choose which one is most ap-propriate in the current context.
A one-to-one mapping between each domain conceptand a word of the language would imply that concepts are represented by words,clearly an undesirable situation.
Just as there is no reason to assume that a conceptuniquely determines a word, there is no reason to assume that a single concept mustmap to a single word; a domain concept may be expressed by multiple words, orconversely, a single word may express a combination of concepts (Talmy 1985; Zock1988).Avoiding encoding any assumptions about the mapping between domain andlanguage has the benefit of portability; the architecture and some knowledge sourcesof the generator can be reused for a variety of different applications in quite different* Mathematics and Computer Science Department, Beer Sheva, 84105 Israel.
E-mail: elhadad@cs.bgu.ac.ilt Computer Science Department, New York, NY 10027 USA.
E-mail: kathy@cs.columbia.edu:~ Departamento de Informatica, Recife, PE 50740-540 Brazil.
E-mail: jr@di.ufpe.br(~) 1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 2domains.
However, it means the task of lexical choice is computationally complex,requiring consideration of a potentially large number of mappings between conceptsand words.
This is complicated by the fact that constraints on lexical choice come froma wide variety of sources:?
Syntax (the choice of a particular verb influences the syntactic forms thatcan be used to realize its arguments, which in turn constrains the wordsused to lexical)ze these arguments).
For example, if the main verb toallow is selected, then the object must be either a clause (allow one to select)or a noun-group (allow the selection).
1?
Semantics (the concept itself and how it is taxonomized in the domaininfluence which word should be used).
For example, when discussingbasketball, the words rebound and point realize distinct concepts underthe generic concept of a player "performance."?
The lexicon (the choice of one word can constrain the choice of otherwords in a sentence).
For example, the selection of rebound as objectnoun would entail preferring to grab over to score as main verb, while theselection of point would entail the opposite verb choice, since to grabrebounds and to score points are lexical collocations, whereas ?to scorerebounds and ?to grab points are not.?
The domain (the same words are used to refer to different concepts indifferent domain sublanguages).
For example, rebound means differentthings in the basketball domain and in the stock-market domain (IBMrebounded from a 3 day loss vs. Magic grabbed 20 rebounds).?
Pragmatics (information about speaker intent, hearer background, orprevious discourse plays a role).
This may lead to the decision to refer tothe same situation as a glass half ull or half empty.Furthermore, interaction between constraints i multidirectional, making it difficultto determine a systematic ordering in which constraints should be taken into account.In fact, earlier work on lexical choice (Danlos 1986) implied that a new ordering ofconstraints, and thus a new architecture for lexical choice, must be developed for eachnew domain.In this paper, we present a general approach to lexical choice that can handlemultiple, interacting constraints.
Our architecture positions the lexical choice modulebetween a language generator's content planner and surface sentence generator, inorder to take into account conceptual, pragmatic, and linguistic constraints on wordchoice.
We show how the Functional Unification Formalism (FUF) (Elhadad 1993a),originally developed for representing syntactic grammars (Kay 1979), can be used torepresent a generation lexicon, allowing for declarative and compositional represen-tation of independent constraints.
The order of constraint application is determineddynamically through unification, allowing for different orderings as required.
Sinceany approach must deal with a combinatorial explosion of possible mappings andordering of constraints, computational efficiency is in general an issue.
We show con-trol techniques we have developed within FUF to reduce overall search.
In this paper,1 The options are different in French for example, where the corresponding verb governs a VP permet desdectioner.196Elhadad, McKeown, and Robin Floating Constraints in Lexical Choicewe illustrate our model for lexical choice as it has been implemented in ADVISOR-II (Elhadad 1993c), a system that can advise students about course selection, butwe also draw on examples from two other systems based on the same model butwithin different generation architectures: STREAK, a system for generating basketballgame summaries (Robin 1994a; Robin and McKeown 1996) and COOK (Smadja andMcKeown 1991), a system that generates tock market reports.
2We have used thissame model for lexical choice in other systems we have developed, such as COMET(McKeown et al 1990), a multimedia explanation system for equipment maintenanceand repair, and PLANDOC (Kukich et al 1994), an automated ocumentation systemunder collaborative development with Bellcore.We focus on the problem of floating constraints, constraints that cannot be mappedin a systematic way from an input conceptual representation to the linguistic structure.Instead, such constraints float, appearing at a variety of different levels in the resultinglinguistic structure, depending on other constraints in the input.
Such constraints poseproblems (see discussion in Elhadad and Robin \[1992\]) for the top-down recursivebuilding of the linguistic structure used by most generation algorithms (Meteer et al1987; Shieber et al 1990); these algorithms typically only handle structural constraints,constraints that are consistently expressed at a given linguistic rank (e.g., the sentence,clause, group, or word rank) (Halliday 1985) in the application domain sublanguage.We consider two different ypes of floating constraints:?
Interlexical constraints, which arise from restrictions on lexicalco-occurrences such as collocations (Smadja 1991) (they are orthogonal tothe mapping from input content units onto output linguistic form sincethey both originate from the lexicon and act upon the lexicon).?
Cross-ranking constraints, which arise from the fact that an inputnetwork of content units is not isomorphic with the resulting linguisticstructure, allowing a single content unit to be realized by surfaceelements of various linguistic ranks (cross-ranking proper), or multiplecontent units to be realized by the same surface element (merging).Sentences (1) and (2) below, generated by COOK, illustrate cross-ranking con-straints.
They show how time and manner can be mapped to two different surfaceelements of different syntactic rank in the sentence, among many other possibilities.Sentences (3) and (4), generated by STREAK, show how game result and manner canbe realized as two separate surface elements or can be merged into a single element,the verb.
(1) Wall Street Indexes opened strongly.
(time in verb, manner as adverb)(2) Stock indexes surged at the start of the trading day.
(time as PP, manner inverb)(3) The Denver Nuggets beat the Boston Celtics with a narrow margin,102-101.
(game result in verb, manner as PP).
(4) The Denver Nuggets edged out the Boston Celtics 102-101.
(game resultand manner in verb)2 The differences between the system architectures of these three systems are discussed inSection 6.1.2.197Computational Linguistics Volume 23, Number 2In these examples, the input conceptual constraints (time and manner) float, ap-pearing at a variety of different syntactic ranks (here, verb and circumstantial), andare sometimes merged with other semantic onstraints.Which content units are floating and which are structural depends on the domainand the particular target sublanguage.
Our corpus analysis of the basketball domain,for example, indicates that historical knowledge is floating, whereas game result in-formation is structural.
Similarly, in the student advising domain, we found courseevaluation (e.g., how difficult or interesting a course is) to be floating, whereas thedescription of the assignments required (e.g., how many there are or whether theyinvolve writing essays, software, or proofs) in a course is structural.Floating constraints have not been addressed in a general way in previous work;most systems implicitly hardwire the choices or permit only one or two of many pos-sibilities.
In contrast, our model for lexical choice accommodates floating constraints,resulting in a system with a high degree of paraphrasing power.In the following sections, we first present our general model for lexical choice,illustrating it with a relatively simple example.
We then discuss different ypes of con-straints and the problems they pose, presenting the techniques we have developedwithin FUF to address these issues, turning from structural constraints, to pragmaticcross-ranking constraints, and to interlexical constraints.
Finally, we compare our ap-proach with other work on lexical choice, closing with a summary of our contributions.2.
An Architecture for Lexical ChoiceThe place of lexical choice in the overall architecture of generation systems has variedfrom project o project.
Due to the varied nature of the constraints on lexical choice,exactly how lexical choice is done often depends on the type of constraints a systemdesign accounts for.
For example, if syntactic and lexical constraints are the researchfocus, it may make sense to delay lexical choice until late in the generation process,during syntactic realization.
If only conceptual constraints are accounted for, lexicalchoice may be done early on, for example, during content planning by associatingconcepts with the words or phrases that can realize them.In this section, we describe a general model for lexical choice as part of an overallgeneration system architecture.
Due to the wide variety of constraints on word selec-tion that we consider, lexicalization is positioned after the content of the generated texthas been determined and before syntactic realization takes place.
We detail the natureof input and output to the lexical choice module, thus specifying the tasks the lexicalchoice module performs and the tasks that are expected to be done elsewhere in thesystem.
We illustrate, through a relatively simple example that depends on a singletype of constraint, how FUF and unification are used for lexicalization.
Our criteriafor a model for lexical choice are fourfold:..3..It must be able to use the full variety of constraints whether pragmatic,semantic, lexical, or syntactic.It must be able to apply constraints in a flexible order.It must avoid encoding assumptions about the mapping betweendomain concepts and lexical structure.It must be able to handle floating constraints.198Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceCommunication RequestContent SpecificationLexical~' L 1 ChooserI Surface R alization~l .
(  Lexical ~ .l C Lexica-----~T I r :m: ionl I i?
1 I 1Figure 1Possible placements of lexical choice within a generator's architecture.Text ~t2.1 Lexical Choice within a Generation System ArchitectureGeneration systems perform two types of tasks: one conceptual, determining the con-tent of the text to be generated, and one linguistic, determining the form of thattext (McDonald 1983; McKeown 1985).
Typically, a generator has two modules, eachcorresponding to one of these two tasks, a content planner and a linguistic realizer.While many systems allow for interaction across these components, there is generalconsensus that these two components can be separated (Reiter 1994).
Furthermore,within the linguistic component, here appears to be further consensus that the taskof syntactic realization can be isolated.
As evidence, note that a number of dedicatedsyntactic realization components have been developed such as SURGE (Elhadad andRobin 1996), NICEL (Matthiessen 1991), MUMBLE (Meteer et al 1987), and TAGs (Yang,McCoy, and Vijay-Shanker 1991; Harbusch 1994).
Such components expect as input aspecification of the thematic structure of the sentence to generate, with the syntacticcategory and open-class words of each thematic role.
3 Thematic structure involves rolessuch as agent, pat ient,  instrument, etc.
It is opposed to surface syntactic structurewhich involves roles such as subject,  object,  adjunct, etc.
Due to general syntac-tic alternations (Levin 1993) such as passive, dative, it-extraposition, or clefting, themapping from thematic roles onto surface syntactic roles is one-to-many.
The role ofthe syntactic grammar is to (1) map the thematic structure onto a surface syntacticone, (2) enforce syntactic rules such as agreement, (3) choose the closed-class words,(4) inflect the open-class ones, and (5) linearize the surface syntactic tree into a naturallanguage string.
These tasks indicate the kind of information the syntactic grammarneeds as input.
For example, unless the system is to choose randomly, it needs enoughinformation to choose between different syntactic options available in the grammar.Furthermore, input must either specify all words, or provide enough features o thatthe syntactic grammar can lexicalize any words that are syntactically determined.Lexical choice could be carried out at any number of places within this standardarchitecture.
Figure I shows the typical anguage generation architecture used in manysystems, indicating the different places for lexical choice to occur.
One option wouldbe to position lexical choice as part of syntactic realization, as just a very specific typeof syntactic decision (i.e., option 3 in Figure 1).
Researchers who work on reversible3 Words are traditionally divided into (a) open-class words such as nouns, verbs, adjectives and adverbsand (b) closed-class words (also called fimction words) such as articles, pronouns, and conjunctions.Open classes are large and constantly expanding while closed classes are small and stable.Distinguishing elements in an open class requires emantics while in a closed class, it can be done onsyntactic grounds only.199Computational Linguistics Volume 23, Number 2grammar formalisms, using the same grammar to both parse and generate language,take this approach (Van Noord 1990; Shieber and Shabes 1991; Strzalkowski 1994).The systemic grammar paradigm also takes this approach, where lexical choice is themost "delicate" of decisions, occurring as a by-product of many high-level syntacticchoices.
However, in computational implementations of the systemic paradigm, suchas NIGEL (Mann and Matthiessen 1983), only the syntactic onstraints on lexical choiceare handled during syntactic realization.
The semantic onstraints on lexical choiceare in effect taken into account in the input knowledge representation (i.e., option 1in Figure 1).There are two problems with option 3 (during syntactic realization).
First, the rangeof constraints on lexical choice covered in this line of work is quite restricted and wehave some question about whether it could be extended to include the pragmaticconstraints considered here.
Furthermore, since words are selected only once the fullsyntactic tree is constructed, it would be quite difficult, if not impossible, to account forfloating constraints.
Such constraints cannot be considered solely from local positionswithin a constructed tree, but require some global knowledge of interaction betweensemantic units.If lexical choice is not part of the syntactic realization component, hen all decisionsregarding open-class word selection must be made before the grammar is invoked#They then must occur either as part of content planning or after all content has beendetermined and expressed in a language-independent manner.
While some researchershave directly associated words with each concept in the domain-knowledge base (e.g.,Reiter 1991; Swartout 1983), this approach does not allow for consideration of syntac-tic and lexical constraints unless a phrasal exicon is used (e.g., Kukich 1983b; Danlos1986; Jacobs 1985; Hovy 1988).
Using a phrasal lexicon, however, means hand-encodingthe many mappings of multiple constraints onto multiword phrasings.
It thus doesnot allow for compositional lexical representation (Pustejovsky and Boguraev 1993),and entails a combinatorial explosion in the number of entries to cover the variationsof phrases that are possible in different contexts.
This approach thus does not allowfor scaling up paraphrasing power (see Robin and McKeown \[1996\] for a quantita-tive evaluation of the scalability gains resulting from the compositional word-basedapproach).By waiting until content planning is complete, lexical and syntactic onstraints canbe represented explicitly and independently ofone another, instead of being embeddedinto full phrases, allowing for a more economical and flexible word-based lexicon thatincorporates phrasal constraints.The only remaining option is to position the lexical choice module between thecontent planner and the syntactic realization module.
Note that some high-level de-cisions about sentence structure must be made early on with this architecture (i.e.,before syntactic realization), since, for example, selecting the verb imposes syntacticconstraints on how its arguments can be realized.
This is desirable since it allows asystem to take into account only those syntactic onstraints on lexical choice that arerelevant.
In fact, in the eight domains for which we have implemented generators,we have never found a case where other syntactic decisions made during realizationforce the lexical chooser to undo an earlier decision.
This experience strongly supportsmodularization between lexical choice and syntactic realization.
54 In fact, most portable syntactic components mentioned arlier, such as SURGE, MUMBLE, and TAGs,expect as input a fully lexicalized specification, thus supporting this approach.5 The only argument for option 3 is that it allows for an integrated account of the influence of surfacestructure on lexical choice within a single component.
However, our experience (corroborated by200Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceAs we will show, this architecture allows us to convey several aspects of thesemantic ontent using the same word and at the same time, allows us to realize thesame semantic oncept at a variety of syntactic ranks depending on the situation.
Inparticular, by selecting words before a syntactic tree has been constructed, the lexicalsyntactic features associated with alternate l xical choices can constrain the high-levelstructure of the final tree, which is a key feature to handling floating constraints.2.2 Input and OutputGiven this organization, input to the lexical choice module will be structures from theapplication domain representation selected uring content planning, possibly aug-mented with discourse relations.
Following our criteria for generation, the structureof the conceptual input will not be committed to any linguistic structure, in orderto avoid encoding assumptions about realization into the domain application, andin order to free the content planner from reasoning about linguistic features whendetermining what information should be included.
Thus, it should be possible for aconceptual structure to be realized by a clause, a nominalization, a noun-noun mod-ifier, a predicative adjective, or a prepositional phrase.
Similarly, the mapping neednot be one-to-one.
Several conceptual e ements may be realized by the same linguisticconstituent, and conversely, several linguistic onstituents may be needed to realize asingle conceptual element.For example, consider the conceptual input to ADVISOR-II's Lexical Chooser whosegraph is outlined at the top tier of Figure 2.
The domain from which concepts areselected is an expert system rule base, which uses gradual rules of inference (e.g., themore assignments in a class, the harder the class).
The input is a set of three relations,each of which is represented similarly by a set of attribute-value pairs in the featurestructure form shown in the central tier of Figure 2, except for cardinality, whichreduces to an integer.
This content representation does not indicate which relationsshould appear as the head element in the linguistic structure and which should appearas dependents.
Nor does it indicate which syntactic relations hould be used.As a result, many different paraphrases of this content can be generate d, as il-lustrated by the five given at the bottom tier of Figure 2.
Note that while in (1) therelation assignt-type surfaces as the main element in the syntactic structure, in (2)-(5)it appears as a dependent element.
Note also the syntactic variety of these dependentelements.
This example illustrates that, in contrast o much previous work in gen-eration (but see Meteer \[1990\]), we do not assume that relations will be realized asverbs and objects as their arguments.
Instead, the Lexical Chooser must reason abouthow different domain entities can be realized.
The ability to realize relations by com-pact constituents such as predicative adjectives or noun-noun modifiers allows for thefluency of the sentences of Figure 2.
Realizing all relations in the Figure 2 input asclauses would result in rather cumbersome s ntences such as: Programming is the kindof assignments ofthe class whose topic is AI and the number of these assignments is six.Note that in order to choose between these sentences, the Lexical Chooser needsinformation other than just content encoded in the domain representation.
I  general,the Lexical Chooser needs information about discourse and about speaker intent.
Forthis particular example, it needs information about the speaker's focus and her per-spective, at this point in the discourse.
Such information must be part of the input tothe Lexical Chooser and can typically be provided by a content planner (McKeownReiter's \[Reiter 1994\]) shows that he influences ofsyntax on lexical choice can be accounted for beforesyntactic realization.201Computational Linguistics Volume 23, Number 2AI AI-assignt "~, programming/~ber6i semrcat set \]assignments \[1\] cardinality 6generic_elt \[cat assignment \]class \[2\] name ai\[ cat assignment_activity \] activity \[3\] name programming7 \]\] class \[2\] relation1 args assignt \[1\]assignt \[11 relation2 args activity \[3\]...4..The six AI assignments require programming.
(relation assignt_type as mainclause)AI has six assignments which involve programming.
(relation assignt_type asrelative clause)AI has six assignments of programming nature.
(relation assignt_type as PP)AI has six programming assignments.
(relation assignt_type as predicativeadjective)AI has six implementation assignments.
(relation assignt_type asnoun-noun modifier)Figure 2An example of an input conceptual network with paraphrases that can be generated from it.1985; Polgu~re 1990; McCoy and Cheng 1991; Carcagno and Iordanskaja 1993), whichmust keep track of how focus shifts as it plans the discourse, or text.
Similarly, anygoals of the speaker must be provided as input to the Lexical Chooser.
In the studentadvising domain, argumentative intent, or the desire of the speaker to cause the hearerto evaluate the information provided in a particular light, plays an important role.
Forexample, whether the six programming assignments should be viewed as a plus of AIor a minus will depend both on hearer 6goals and on what action the speaker 7 thinksthe hearer should pursue (i.e., take AI or not).
Such goals, or argumentative intent, areused by the content planner in reasoning about what information to include.
Again,6 In our  case, the sys tem user.7 In our  case, the system.202Elhadad, McKeown, and Robin Floating Constraints in Lexical Choicesince such goals are available to the content planner, they can easily be provided asinput to the Lexical Chooser.2.3 Two Tasks for Lexical ChoiceGiven the input and output specified above, this leaves two tasks for the LexicalChooser:?
Syntagmatic decisions: choosing among the many possible mappingsfrom the fiat conceptual network it receives as input onto the thematictree structure it must produce as output, (e.g., the choice of expressingthe assignt_type relation in the network of Figure 2 as the main verband the cardinal relation as a noun modifier in paraphrase \[1\]).?
Paradigmatic decisions: choosing among alternative lexicalizationsinside a particular thematic structure, (e.g., the choice of the verb torequire to express the assignt_type relation in paraphrase \[1\] instead ofto involve in \[2\]).While syntagmafic decisions may seem to be more syntactic in nature, they aredirectly intertwined with various lexical choices.
Selecting the verb, or a higher-levelrelation such as a connective between two clauses, automatically determines overallthematic structure, while selecting which concept in the input will serve as head ofthe sentence directly influences choice of words.
Minimally, syntagmatic decisions in-dude determining the main process, which constrains the set of possible verbs, s Forexample, in paraphrase (4) of Figure 2, this means choosing:?
To map the relation class_assignt as the main process of the sentence.?
A possessive thematic structure for that main process.?
To map the arguments class and assignt of class_assignt ontorespectively the possessor and possessed roles of the possessiveprocess.Further syntagmatic choices determine which concepts will function as modifiersof any of these roles, ultimately surfacing as relative clauses, prepositional phrases,or adjectival describers.
This mapping of conceptual structure to linguistic structure iscarried out first in the Lexical Chooser.
We call this initial stage involving syntagmaticdecisions phrase planning.
Then, the Lexical Chooser selects the actual words thatare used to realize each role.
We call this subsequent stage involving paradigmaticdecisions lexicalization proper.Floating constraints are handled in both of these stages: for example merging twocontent units in a single linguistic unit is a phrase planning decision, whereas pickingthe appropriate collocate of an already chosen word is a paradigmatic decision.2.4 An Implementation Based on the FUF/SURGE PackageThe implementation of ADVlSOR-II builds on a software nvironment dedicated to thedevelopment of language generation systems: the FUF/SURGE package (Elhadad 1993a,8 Here we use the word "process" inthe systemic sense, see Section 2.4.2.203Computational Linguistics Volume 23, Number 21993c).
FUF (Functional Unification Formalism) is a programming language based onfunctional unification (Kay 1979).
9 Both the input and the output of a FUF programare feature structures called Functional Descriptions (FDs).
The program itself, calleda Functional Unification Grammar (FUG), is also a feature structure, but one whichcontains disjunctions and control annotations.
The output FD results from the unifi-cation of this FUG with the input FD.
The disjunctions in the FUG make unificationnondeterministic.Functional unification has traditionally been used to represent syntactic grammarsfor sentence generation (e.g., Appelt 1983; McKeown 1985; Paris 1987) and FUF comesas a package with SURGE, a grammar of English implemented in FUF.
SURGE is usableas a portable front-end for syntactic processing.
FUF is the formalism part of the package,a language in which to encode the various knowledge sources needed by a generator.SURGE is the data part of the package, an encoded knowledge source usable by anygenerator.
Using the FUF/SURGE package, implementing a generation system thus con-sists of decomposing nonsyntactic processing into subprocesses and encoding in FUFthe knowledge sources for each of these subprocesses.
Each such knowledge source isrepresented as a FUG.
1?
In the case of ADVISOR-II, the focus of nonsyntactic processingis lexical choice.
ADVISOR-II thUS essentially consists of a pipeline of two FUGs: a lex-ical FUG encoding the domain-specific lexical chooser and the domain-independentsyntactic FUG SURGE.
Lexical choice is performed by unifying the conceptual inputwith the lexical FUG or lexicon.
The resulting lexicalized thematic tree is then unifiedwith SURGE, which produces the final sentence.We now briefly overview the FUF language and then the SURGE syntactic grammarbefore explaining in detail how unification is used to perform lexical choice.2.4.1 FUF: A Functional Unification Formalism.
Functional unification is based on twoprinciples: information is encoded in simple and regular structures called functionaldescriptions (FDs) and FDs can be manipulated through the operation of unification.
11A Functional Unifier takes as input two FDs and produces a new FD if unificationsucceeds and failure otherwise.
Note that contrary to structural unification (SU, as usedin Prolog for example), functional unification (FU) is not based on order and length(see Shieber \[1992\] and Carpenter \[1992\] for recent and comprehensive d scriptions offeature structures).An important property of FDs is their ability to describe structure sharing (or reen-trancy): an FD can describe an identity equation between two attributes.
For example,subject-verb agreement in a sentence is described by the FD:subject \[ number \[1\]\]verb \[ number \[1\] \]In this FD, the notation \[1\] indicates that the value of the attribute number under subjectmust be identical to that of the attribute number under verb, whatever it may be.
InFUF, tags like \[1\] are encoded with the path notation such as {verb number}.
A pathis best understood as a pointer within the FD.
Because paths can be used with noconstraints, the graph encoding an FD can include loops and does not need to be atree.9 FUF is currently implemented in Common Lisp.10 To avoid overloading the word "grammar," we will use "FUG" to refer to the common representationformalism and "syntactic grammar" to refer to syntactic data encoded in SURCE using this formalism.11 FDs are often called feature structures or attribute value matrices in the literature.204Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceThe a l t  keyword expresses disjunction in FUF.
The value of the a l t  keywordis a list of FDs, each one called a branch.
When unifying an input FD with such adisjunction, the unifier nondeterministically selects one branch that is compatible withthe input.
Disjunctions encode the available choice points of a system and introducebacktracking in the unification process.
In this paper, alts are represented using thefollowing standard notation for disjunctions in feature structures:ALT name-of-alt }branch1branch2A disjunction can be embedded in another one if necessary.
In addition, disjunctionscan be named using the de f -a l t  notation, and referred to in other places usingthe notation (:!
name).
This notation allows for a modular notation of large gram-mars written in FUF.
Other FUF constructs are introduced as needed in the rest of thepaper.2.4.2 SURGE: A Wide-coverage Syntactic Front-End For Generation.
SURGE is awide-coverage syntactic grammar of English implemented in FUF and usable as asyntactic front-end portable across domains.
It has been progressively developed overthe last seven years and extensively tested for the generation of texts as varied asmultimedia explanations (McKeown et al 1990), stock market reports (Smadja andMcKeown 1991), student advisory sessions (Elhadad 1993c), telephone planning engi-neer activity reports (Kukich et al 1994; McKeown, Kukich, and Shaw 1994), taxationcorrespondence, visual scene descriptions (Abella 1994), didactic biology term deft-nitions (Lester 1994), basketball game summaries (Robin 1994a), workflow diagramdescriptions (Passoneau et al 1996), news article summaries (McKeown and Radev1995), intensive care patient summaries (Dalai et al 1996), and web-page access de-mographics.SURGE represents our own synthesis, within a single working system and compu-tational framework, of the descriptive work of several (noncomputational) linguists.Our main source of inspiration were: Halliday (1985) and Winograd (1983) for theoverall organization of the grammar and the core of the clause and nominal subgram-mars, Fawcett (1987) and Lyons (1977) for the semantic aspects of the clause, Pollardand Sag (1994) for the treatment of long-distance dependencies and Quirk et al (1985)for the many linguistic phenomena not mentioned in other works, yet encounteredin many generation application domains.
Since many of these sources belong to thesystemic linguistic school, SURGE is mostly a functional unification implementationof systemic grammar ules.
In particular, the type of FD it accepts as input specifiesa "process" in the systemic sense, i.e., any type of situation involving a given set ofparticipants (or thematic roles).
This situation can be an event, a relation, or state inaddition to a process in its most common, aspectually restricted sense.
In this broadersystemic sense, a process is thus a very general concept, simply denoting a semanticclass of verbs sharing the same thematic roles.
However, SURGE also includes aspectsof lexical grammars, such as subcategorization.Furthermore, while SURGE is essentially systemic in terms of the type of thematicstructure it expects as input, it differs from a purely systemic grammar implementationsuch as NIGEL (Mann and Matthiessen 1983) in terms of control.
Because it is based onfunctional unification, SURGE is driven by both the structure of the grammar and that205Computational Linguistics Volume 23, Number 2of the input, working in tandem.
In contrast, NIGEL is driven solely by the structure ofthe grammar, as encoded in its system networks.2.4.3 Lexical Choice by Functional Unification.
We apply FUF to lexical choice byrepresenting the lexicon as a FUG whose branches pecify both constraints on lexicalchoice (as tests) and the lexical features selected as a result of the different tests.
Lexicalchoice is performed automatically by unifying the lexicon, or lexical FUG, with theconceptual input.
During unification, the tests probe both the input conceptual networkand the linguistic tree under construction.FUF is particularly suited for the representation f lexical constraints for a va-riety of reasons, some of which have been discussed elsewhere (e.g., McKeown andElhadad 1990; McKeown et al 1990).
First, FUF allows the representation f constraintson lexical choice in a declarative and compositional manner.
This means that each con-straint can be represented separately and the ordering on how the constraints apply isdetermined dynamically through unification.
Because the constraints are representedseparately, lexical features are added as each constraint apphes, thus compositionallyconstructing the set of features that define the final choice.
Finally, because unificationis the governing process, constraints are bidirectional.Given the wide variety of constraints on lexical choice (Robin 1990) and the unpre-dictable manner in which they interact, hese features of FUF are particularly desirable.Since in different contexts, different constraints play more or less of a role, unificationcan determine dynamically which constraints are triggered and in what order.
This isa benefit in several different scenarios.
For example, sometimes a constraint is statedin the input while at other times it may be derived from the choice of another wordin the sentence.
If the constraint is available, it can influence the choice of that word;if not, then if the word is selected based on other constraints, it will trigger the con-straint, which may in turn trigger selection of other words.
With lexical constraintsthat hold between two or more words, it is not critical which word is chosen first.Examples of such patterns of interaction are given in the following sections.2.5 A Simple ExampleTo see how lexicalization works for our simple example sentence AI has six assign-ments, we will only consider semantic onstraints.
The input specification received bythe Lexical Chooser is shown in Figure 3.
The conceptual representation in the in-put is encoded as an FD under the semr (SEMantic Representation) attribute.
In thisexample, it is a simple encoding in FUF notation of a binary relation CLASS_ASSIGNTholding between two entities: the individual AI and the set ASSIGNT_SET1.
The linkbetween the arguments of the relation and its fillers is indicated by path values (of\[1\] and \[2\] respectively).
In the matrix notation used here, we use a number in brack-ets (In\]) to both label a value and subsequently represent the path to that value.
Asthe Lexical Chooser proceeds, it adds features to this feature structure, representingthe syntactic elements of the clause that is to be produced.
The output is shown inFigure 4.
It consists of the input semr attribute nriched by a syntactic structure ofcategory clause (cf.
note 1) with lexical items specified for each linguistic onstituent(cf.
note 3 and the features next to notes 4 and 5 of the figure).
This output FD is thenfed to the SURGE syntactic realization component, to eventually produce the expectedsentence.In the architecture we are describing, the Lexical Chooser must meet the require-ments of the underlying application, which feeds it its input, on the one hand, andon the other hand it must produce an output acceptable by the syntactic realizationcomponent.
The particular semantic features (name of the relations and of the argu-206Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoicentAI AI-assigntsemrassignmentsclassrelation1cat set \]name assignt_setl\[1\] cardinality 6gen.ic l, \[co, as.gnm.., \]\[2\] name aiFigure 3Conceptual input to the Lexical Chooser in FD format.ments) are specific to the ADVISOR-II domain.
The particular thematic roles found inthe output are characteristic of SURCE.
The mapping process is generic.The Lexical Chooser first traverses the input conceptual structure (which appearsunder semr) to decide what syntactic ategory will be chosen to realize it.
In this case,the Lexical Chooser decides to map a semantic relation to a simple clause.
This decisionis done during phrase planning, a process we detail in Section 3.
As the clause is beingconstructed, the feature (cat clause) is added (cf.
Note 1 in Figure 4) and the syntacticstructure of a clause is constructed.
A clause skeleton is added to the top level of theFD (cf.
notes 2, 4, and 5).Since the main verb has not yet been selected, the Lexical Chooser cannot proceedfurther and determine which participants (or verb arguments) will be inserted in theclause, and how they will map to the arguments of the input semantic relation.
But thephrase planning component has already determined to use the main verb to realize theinput relation.
To represent this decision, the Lexical Chooser copies information fromthe top-level semantic representation in the semr feature under process (cf.
note 2), thusindicating that the main verb maps to the semantic relation CLASS_ASSIGNT.
This is ageneral feature of the technique: the Lexical Chooser incrementally builds a syntacticstructure, and each time a new linguistic constituent is introduced, a subconstituentfrom the semr is copied under the semr of the syntactic subconstituent, representing themapping between semantic and syntactic onstituents.
This process is the generationcounterpart of a compositional semantic interpretation.The next task of the Lexical Chooser is to select a word or phrase to realize therelation CLASS_ASSIGNT.
The verb is selected by recursively unifying the process de-scription (including its newly assigned semr feature, cf.
note 2) with a disjunctionof the verbs stored in the lexicon.
The relevant fragment of the lexicon is shown inFigure 5.The selected entry contains two types of features: syntagmatic (constraints ondaughter nodes in the syntactic tree) and paradigmatic (choice of alternative lexicalentries for the same node in the tree).
12 First the verb to have is selected (cf.
note 312 A generation lexicon is indexed by concepts instead of words.
Each of its entries groups the alternativewords to express a given concept.207Computational Linguistics Volume 23, Number 2semrcatprocesslex_cset% Testsassignmentsclass\[1\]... %cf.Fig.3\[2\]... %cf.Fig.3% Relation1 is of type class-assignt.% It is mapped to the process of the main clause.\[ name \[3\] class-assignt \]relationl argl \[1\]arg2 \[2\]% Enrichment: semr is mapped to a clause% of process type possessiveclausecat semr namecat verb-grouptype possessivelex ,have Itclass-relation\[3\]% Indicates the syntactic onstituents to be recursively lexicalized{.1 I l}participants% Clause complements'% Pointer to the semr subconstituentsemr \[1\]possessor \[4\]% Linguistic features from the lexicon| cat properL lex "AI"\[2\]np\[ cat noun \]lex "assignment"possessed nopluralpluralyesyes\[value 6\ ]semrcathead\[5\] definitenumberref _numberquantitativeexactcardinalFigure 4Output from the Lexical Chooser.in Figure 4) as the default option for possessive verbs---English uses a possessivemetaphor to refer to the link existing between a class and its assignments (a class"owns" the assignments).
13 As discussed below, this is a domain-specific decision thatonly applies to the particular elation CLASS_ASSIGNT.Once the verb class is known, the transitivity of the clause is determined, andthe clause skeleton can be extended by specifying the verb's complements.
In SURGE,possessive clauses expect two participants, named POSSESSOR and POSSESSED.
Thesecond part of the lexical entry therefore determines how the semr  features of the twosyntactic participants are to be linked to the semantic arguments of the input semanticrelation (in our case, this is done by the FUF pointers next to notes 4 and 5 in Figure 4).This mapping is domain-specific, and is completely contained in the lexicon en-13 Under different conditions, the Lexical Chooser could select one of the other verbs represented in theentry, such as to require or a construction such as in c lass ,  there is assignment.208Elhadad, McKeown, and Robin Floating Constraints in Lexical Choice% First-level index: relation type is indexed by type of first argument.DEF-ALT RELATIONS: index (process emr cat) \[ \[s r \[cot c,os.e,otion \]\] \]| % This extends into the named alt| % class-relations belowL :!
CLASS-RELATIONS\[ process \[ semr \[ cat student_relation \] \] \]\[ :!
STUDENT-RELATIONS?
.
.
Other types of relations .
.
.% Second-level index: relations by name.DEF-ALT CLASS_RELATIONS: index (process emr name)% For each type of class_relation, determine possible% lexicalizafion and its cat and identify sub-constituents\[class \ [1 \ ] \ ]  semr assignments \[2\]oc ss \[s r \[nome css ssignt \]\]ALT ASSIGNMENrI~: bk-class ao% Lexicalizations: C requires A, C has A% there is A in C, in C they do A% Branch 1: C requires A% Branch 2: C has A\[ type possessive \] process lex "have"% lex_cset declaration to handle recursionlcx_cset ( \ [3 \ ]  \[4\] )possessee \[4\]\[sem~ \[2\]\]% Branch 3: In C there is Aprocess \[ type existential \]lex_cset ( \[5\] \[61 )participants \[located \[5\] \[ semr \ [2 \ ] \ ] \ ]circumstances \[location \[6\] \[ semr \[1\] \] \].
.
.
Other types of classmelafions .
.
.Figure 5Fragment of the lexicon for Verb selection.try for the domain relation CLASS_ASSIGNT.
In contrast to previous lexical choice ap-proaches, such as Bateman et al (1990), we make no claims that the linguistic relation ofpossession used in this case is more general than the domain relation CLASS_ASSIGNT.That is, we do not attempt to fit each domain relation under a general ontology basedon linguistic generalizations.
Such fixed categorization of domain relations in effectprevents a generator from realizing the same domain relation at various linguistic209Computational Linguistics Volume 23, Number 2ranks and thus drastically reduces its paraphrasing power.
14 The only information thismapping encodes is that one option to lexicalize the domain relation CLASS_ASSlGNTin English is a possessive clause.At this point, the top-level unification of the input with the Lexical Chooser iscompleted.
The intermediate FD corresponds to the features next to notes 1, 2, 4, and5 in Figure 4.
The verb has been selected and the clause structure has been built.In order to continue the lexicalization of the arguments, the Lexical Chooser mustspecify which constituents in this FD require further processing.
This is accomplishedby adding a lex_cset (LEXical Constituent SET) declaration to the (lexical) FUG.
Thevalue of lex_cset is a list of pointers to the constituents of the FD as shown in Figure 5.Constituents bring structure to functional descriptions.
15 To handle constituents, thecomplete unification procedure implemented in FUF is:..3.Unify top-level input with the lexicon (i.e., the single unification stepdescribed just above).Identify constituents in result (i.e., read the lex_cset attribute).Recursively unify each constituent with the lexicon.The lexical entry for an argument-bearing word (usually a verb) includes a lex_csetdeclaration.
In the example, it specifies the two complements, POSSESSOR and POS-SESSED, each of which will eventually be realized as an NP.
When this is the case,the head noun of the NP realizes the argument of the conceptual relation.
When thisargument is shared by other relations in the input conceptual network, those otherrelations are realized as nominal modifiers.
Lexicalizing such complex NPs requiresdetermining:?
Which relations in the complex NP will appear as premodifiers andwhich as postmodifiers.?
Which postmodifiers will be realized as prepositional phrases and whichas relative clauses.?
Selecting the features the grammar needs in order to select a determiner,if any.Details on how the linguistic features appearing under the NP constituents areselected are given in Elhadad (1993b, 1996).In summary, the Lexical Chooser proceeds as follows:..A stage of phrase planning first processes the semantic input anddetermines to which syntactic ategory it is to be mapped.A skeletal FD for the selected category enriches the semantic input.14 Note that it is not the very idea of using an ontological upper-model that we criticize here (with all itsadvantages in terms of knowledge inheritance and reuse) but the use of the most common linguisticrealization of each concept as the main criteria for classification.15 SURaE also uses the special feature cset to encode the surface syntactic onstituents of the sentence,following Kay (1979).
Thus, two cross-cutting constituent s ructures, thematic and syntactic, can berepresented in the same FD.
SURCE ignores the lex_cset features and recurses according to the csetdeclarations.210Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceFigure 6IO(~ winner ~//team ~kBulls Jazz Jazz-stmak~(~ number3~ t  pr?gram~ mingy6Networks of semantic relations with shared arguments....(a)(b)The head word for the linguistic onstituent is selected by looking up thesemantic feature in (i.e., unifying the semr feature with) the lexicon.The lexical entry for the head word is responsible for:Providing a lexical item with its lexical features.Mapping semantic subconstituents to the complements of thehead word.The lex_cset attribute triggers recursion on the immediate descendants ofthe linguistic head.
The linguistic structure is therefore incrementallyexpanded as each head is lexicalized in turn.
The FUF default controlregime develops this structure in breadth-first order.3.
Phrase Planning in Lexical ChoiceWhen the input semantic network contains more than one relation, the Lexical Choosermust decide how to articulate the different predicates into a coherent linguistic struc-ture.
We refer to this stage of processing as "phrase planning" because of its closerelationship to paragraph planning.
In a simplistic generation system, all semanticrelations would be mapped to clauses, while entity and set descriptions would bemapped to noun phrases.
This strategy is not felicitous when dealing with multiplerelations, as illustrated by the two examples whose inputs and corresponding alterna-tive outputs are shown in Figure 6 and Figure 7, respectively.If every relation is to be realized as a clause, then the only option for lexicalizingthe relations 1 and 2 in Example 1 of Figure 7 is to generate two separate sentencesas in (1), or to embed one of the relations as a relative clause modifier of the sharedargument as in (2) or (3).
Our corpus analysis (Robin and McKeown 1993), however,has shown that sentences in the basketball report domain tend to be more syntacticallycomplex than sentences (1) to (3).
Sentences (4) and (5) illustrate the type of complexitywe found: the two semantic relations are merged into a single sentence, but the secondrelation is realized as a prepositional djunct of different ypes.
Example 2, in theADVI$OR-II domain, shows other options for realizing attached relations: as noun-noun modifiers (AI assignments) or premodifier (programming assignments).
To accountfor this observed syntactic omplexity, the Lexical Chooser must be able to accept asinput networks of several semantic relations, sharing certain arguments.
The semanticnetworks corresponding to Examples 1, 2, and 3 are shown graphically in Figure 6.211Computational Linguistics Volume 23, Number 2Example 1 (left network of Figure 6):(1) Two sentences:The Jazz defeated the Bulls.They extended their winning streak to three games.
(2) One sentence - beat as head - No lexical optimization:The Jazz who extended thei r winning streak to three games, defeated the Bulls.
(3) One sentence - s t reak  as head - No lexical optimization:The Jazz who defeated the Bulls, extended their winning streak to three games.
(4) One sentence - beat as head - With lexical optimization:The Jazz defeated the Bulls for their third straight win.
(5) One sentence - s t reak  as head - With lexical optimization:The Jazz extended their winning streak to three games with a victory over the Bulls.Example 2 (right network of Figure 6, perspective alternation with fixed focus):(6) AI has six programming assignments.
(perspective c lass_ass ignt ,  focus aI)(7) The six AI assignments require programming.
(perspective ass ignt_type,  focus AI)Example 3 (right network of Figure 6, focus alternation with fixed perspective):(8) AI requires ix programming assignments.
(perspective c lass_ass ignt ,  focus AI)(9) Six programming assignments are required in AI.
(perspective c lass_ass ignt ,  focusassignt_set 1)Figure 7Alternative outputs for the input of Figure 6.3.1 Selecting the Head Relation and Building its Argument StructureThe Lexical Chooser must first decide which relation to map to the main clause, andwhich one to embed as a modifier.
16 We refer to this decision as perspective selec-tion.
The notion of perspective is related to the notion of focus as used, for example,in McKeown (1985).
However, the perspective is a relation in the conceptual networkwhereas the focus is an entity.
Once the perspective is chosen, focus can shift betweenthe participants of a relation, by switching the order of the complements, as in sen-tences (8) and (9) of Figure 7.
This is in contrast o sentences (6) and (7) in the samefigure, where perspective switches from c lass_ass ignt  to ass ignt_type (with the fo-cus being the same).
We have not investigated further which pragmatic factors affectthe selection of perspective.
Our research as focused on building into the LexicalChooser the ability to realize any choice of perspective on the structures producedby the content planner.
We thus assume that perspective is given in the input to theLexical Chooser.
Figure 2 shows in FD form the input the Lexical Chooser receivesfor the example that produces entences (6) to (9) (the network form for this inputis shown on the right in Figure 6), depending on the values of the additional inputfeatures perspective and focus omitted in Figure 6.The ADVISOR-II system expects in its input up to four semantic relations, thehighest number of relations that we observed expressed by a single sentence in our16 Note that while an object cannot in general serve as a verb, a relation can serve as clause, noun, and avariety of different modifiers.
Thus, while we are restricted to selecting a relation as a main clause, weare not restricted inhow we do the mapping of other input relations to syntactic constituents.212Elhadad, McKeown, and Robin Floating Constraints in Lexical Choicemodel corpus of advising dialogues.
The clause planning process has two compo-nents: one, domain specific, maps from the domain relations to a clause structure, andone, generic, maps the clause structure to the appropriate types of syntactic modifiers(relative clause, prepositional djunct, adjectival premodifier, or noun-noun modifier).To explain this process, we will go through the clause planning of the exampleinput shown in Figure 2 step-by-step.
Assume that the content planner has decidedthe focus is on aI, and the perspective is on the c lass -ass ignt  relation.
First, the headconstituent of the linguistic structure is built from the description of the c lass -ass igntrelation.This mapping is shown in the top half of Figure 8.
The left-hand side shows theconceptual structure that is the input to the Lexical Chooser.
The right-hand side showsthe linguistic structure that is constructed.
At this stage, the conceptual relation thatwill be realized as the head has been selected (shown by the dotted line pointing tothe node c lass-ass ignt)  and the Lexical Chooser has decided to use a process (i.e.,ultimately a verb) to realize it.
In addition, the ro les feature is added as a genericargument structure for the clause.
The roles point to the appropriate arguments of theclass-assignt (again, note the dotted lines to the nodes AI and AI-assignt).
Note,however, that during this stage of phrase planning, neither the syntactic categorynor the lexical head of the constituent are yet chosen.
The input conceptual graphis merely transformed into a semantic tree.
It is only during the subsequent stage oflexicalization (proper), when the specific verb (to have in this example), is selected.
17In the implementation, generic roles (role1, role2) are used to point to the argumentsof a process as long as the specific verb is not selected.
They are mapped to clausalparticipants (e.g., CARRIER, ATTRIBUTE) only once the verb is chosen.3.2 Attaching the Remaining Relations as ModifiersThe second step of the mapping is to map the second relation, assignt_type, ontomodifiers of the arguments of the head clause.
The assignt_type is mapped ontothe modifier slot of the a t t r ibute  role of the head clause, when it is found that theassignt_type and class_assignt share an argument.This mapping is illustrated in the bottom half of Figure 8 for the example sentence:(5) AI has programming assignments.The modifier description has the same format as a clause in the linguistic structure.The process of the clause is mapped to the relation ass ignt - type and the processroles to the arguments of assignt-type.
This does not mean that the modifier willnecessarily be realized by a clause as in the following sentence:(6) AI has assignments which involve programming.It can also be realized by an adjective or a noun.
But these modifiers are analyzed asbeing derived from the relative clause construction using only linguistic derivations,following Levi (1978).
Thus, sentence (5) above is analyzed as being derived from(6) by deletion of the predicate involve and migration of its object, programming, to apremodifier of the head assignments.Another option to attach a second relation is to add it as a separate clause to avoiddeeply embedded structures.
For example, the clause combination, sentence (7) below,17 The same process generalizes to the treatment of nominal heads.213Computational Linguistics Volume 23, Number 2AI ~classAl'assignt ~ -a-'ssig~n: ,programming ( ~  activity 1II\[ lex = "hale'__\]..~rrier 2~te\[lex="Al"\] ( == head X~lfler\[ lex = "assignment" \] / ~  2 identifierP7 roles -~deleted 1 ~) t  ifiedconceptualFigure 8Construction of a clause structure.\[ lex = "programming" \]linguisticis preferred over the embedded combination, sentence (8) below, because in the latterthe relative clause is twice embedded:(7) Intro to AI has many assignments which consist of writing essays.You do not have experience in writing essays.
(8) Intro to AI has many assignments which consist of writing essays in which you donot have experience.In summary, the first step of the mapping from conceptual network to clause is(1) to select a perspective among the conceptual relations of the network, which deter-mines a head clause, and (2) to attach the remaining relations as either embedded orsubordinate modifiers of the head clause.
The perspective is selected using focus con-straints; the choice between embedding or subordination is based on simple stylisticcriteria.
The output of this stage is a hierarchical structure where heads correspond tolinguistic constituents of a given category (clause or NP), but where the lexical headsare not yet selected.These two operations constitute clause planning, similar to text planning at theparagraph level.
A similar process for NP planning is described in Elhadad (1996).Once the head clause structure has been built, it is passed to the rest of the LexicalChooser, which determines which syntactic forms can be selected for each modifier,when appropriate l xical resources are found.These operations of phrase planning are possible in this approach because theconceptual input is not already linguistically structured.
Such planning is a majorsource of paraphrasing power, and since it is controlled by pragmatic factors (as ex-plained in Section 4) it also increases the sensitivity of the generator to the situationof enunciation.214Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceTAKE(S,C)S enjoy CINTEREST(C,S) /ASSIGNMENT(C,A)~____________~C ire A requDIFFICULT(A,S)TAKE(S,C) ~ WIN(X,Y)DIFFICULT(C,S) / .
.
- -W"  S struggle with C LARGE(WIN) / " " "W~Figure 9Merging two semantic units onto a single lexical item.X crush Y4.
Cross-ranking and Merged RealizationsThe two structures that the Lexical Chooser has to match--a network of semanticunits and a syntactic structure--are in general not isomorphic.
This can be explainedby two factors: a combination of semantic elements can be expressed by a singlesurface element, or a single semantic element by a combination of surface elements(Talmy 1985, 57).
This non-isomorphism between syntactic and semantic structures ia pervasive phenomenon, asillustrated by Talmy's extensive cross-linguistic analysisof constructions expressing motion and causation (Talmy 1976, 1983).This discrepancy between the structures of the input and output of the LexicalChooser imposes two constraints: ince several semantic units can be realized by thesame lexical item, the Lexical Chooser must be able to merge semantic units, and sincethe same semantic unit can be realized at different syntactic levels, the Lexical Choosermust be able to handle cross-ranking realization--that is, to dispatch a semantic unitfrom a given level in the semantic network onto several different ranks of the syntacticstructure.An example of merging is provided by verbs that convey an evaluative connota-tion, as illustrated in Figure 9.
TM Here, the verb enjoy, used in the student enjoyed the AIclass, conveys two semantic units:The student took the AI class (a binary relation between the AI class andthe student)2 9The student found the AI class interesting (an argumentative evaluation).By choosing a verb with connotations such as enjoy, the Lexical Chooser satisfies twoinput constraints at once.An example of cross-ranking realization is shown in Figure 10.
All four sentencesin this example convey similar information and satisfy the same argumentative intent:they evaluate the AI class as high on the scale of difficulty.
The difference is that thisevaluation is realized at four distinct syntactic ranks.In (1), the evaluation is realized by selecting the judgment determiner many andrelying on the commonsense inference rule "the more assignments in a class, the moredifficult it is."
Here the Lexical Chooser decided to use the marked evaluative xpres-sion many instead of six to refer to the number of assignments.
Judgment determinersinclude many, few, a great number of, etc.
In (2), the use of a scalar adjective directly18 In which student, class, and assignments are abbreviated S, C, and A, respectively.19 We do not address the issue of deciding whether presupposed content units will be conveyed in theoutput.215Computational Linguistics Volume 23, Number 2.2..4.Judgment determiner: "\[AI has many assignments.\]"Predicative scalar adjective: "\[AI, which is difficult, has sevenassignments.\]"Connotative verb: "\[AI requires even assignments.\]"Argumentative connective: "AI is interesting but \[it has six assignments\].
"Figure 10Cross-ranking ofargumentative evaluation.realizes the evaluative intention of the speaker.
Scalar adjectives include difficult, inter-esting, important, etc.
In (3), the choice of the connotative main verb require can also berelated to the speaker's intention to evaluate AI as a difficult class.
The verb to requiremerges the expression of the relation between the class and the assignments with theconnotation that AI is difficult.
In contrast, the verb to have only expresses the firstsemantic unit and does not have any connotation with respect o difficulty.Finally, in (4), the argumentative connective but projects an evaluation on theclauses it connects in an indirect way.
The clause AI has seven assignments i not eval-uative taken alone; but when it is contrasted with the first evaluation AI is interestingby way of a but, it becomes an argument that must be opposed to interesting, andthe whole sentence supports this second argument.
In this case, the speaker elieson two commonsense rules that predict hat (a) the more a course is interesting, themore a student wants to take it and (b) the more a course is difficult, the less astudent wants to take it.
Such common sense relations are called topoi by Ducrot(Anscombre and Ducrot 1983).
The modeling of argumentative evaluation using topoifor text generation is described in detail in Elhadad (1995).In summary, the same content unit--the valuation of the class of AI on the scale ofdifficulty--can be realized by very different linguistic devices: connective, main verb,noun modifier, and determiner sequence.
We use the term floating constraints to de-scribe input constraints such as evaluations, which can be realized at different syntacticlevels.
Figures 11 and 12 show how these floating constraints are distinguished fromstructural constraints such as semantic predications.
Structural constraints require thepresence of syntactic onstituents at a given linguistic rank in the output and thusguide the structural mapping process from the conceptual network to the thematictree.
For example, when the input relation STUDENT-CLASS is mapped to a clause, thepredicate to take determines how the arguments of the relation are mapped onto thethematic roles of the clause: STUDENT to AGENT and CLASS to RANGE.
In contrast, float-ing constraints are conveyed by either semantically richer wordings for the obligatoryconstituents introduced by the structural constraints (e.g., in Figure 12 changing theverb to have into to require or the determiner six into many to add the evaluation of AIas a difficult class) or by optional constituents (e.g., in Figure 12 the connective but orthe qualifier difficult).Both merging of semantic units in a single site and cross-ranking ofa single seman-tic unit to different syntactic sites, are found in other domains as well.
For example, inthe basketball domain, the semantic unit describing a game result (victory or defeat)can be merged with an evaluation of the ease or the predictability of the result inverbs like to outlast, to crush, to surprise, etc.
as shown in Figure 9.
Similarly, the phe-nomenon of cross-ranking is not restricted to evaluative connotations.
For example, in216Elhadad, McKeown, and Robin Floating Constraints in Lexical Choice\[Relation = student-dass\] \[cat = clause\]C- -~ ~N~-- - - - -  : - - -~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ ' (~ participantsclass/ ~studen, "take" /~/ ~ agent/Figure 11Structural constraint.\[cat complex-clause\]connec~ve~~rectiveI .
.
.
. "
" proc 7 ~cipants.
- " / ' "  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
> 0 ( 3-<~2 llex="require"\] / ~:: .
.
.
.
7 T-.. \[lex = "difficult"\] \[/ex = "many"\] [lex ="~lgm~ent~"l".
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
)~Figure 12Floating constraint.the following sentences taken from stock market reports (Kukich 1983b; Smadja 1991),the semantic unit expressing the time appears as a floating unit at different syntacticlevels:?
Stock prices got off to a strong start.
\[time in both (prepositional) verband object\]?
Wall Street Indexes opened strongly.
\[time in verb only\]?
Stock indexes surged at the start of the trading day.
\[time in adjunct\]Thus, this phenomenon is a pervasive aspect of lexicalization.
The need to performcross-ranking realization and to deal with floating constraints requires that the inputto the generator be neutral to linguistic form.
This is in sharp contrast with previousgenerators (Meteer et al 1987; Bateman et al 1990), whose input already determineslinguistic structure (e.g., semantic relations are always realized as clauses, and individ-uals always as noun phrases).
The distinction between the structure of the conceptualinput and the linguistic structure used to realize it implies that the Lexical Choosermust not only perform paradigmatic hoices (select among substitutable items, e.g.,between require and necessitate), but also syntagmatic choices (determine the linguistic217Computational Linguistics Volume 23, Number 2structure corresponding to a given input specification, e.g., select between a clauseand a nominalization, determine which construction will realize an evaluation).
Inprevious work, these structural decisions were made imphcitly at the content deter-mination stage, when building the input to a text generator, and usually were donethrough hand-coding.
In this section, we show how these decisions can be made bythe Lexical Chooser in an efficient way.4.1 Merging of a Conceptual Unit with an Argumentative EvaluationWe first illustrate the implementation f floating constraints in a Lexical Chooser takingas example the merging of a conceptual unit with an argumentative evaluation.Consider the task of mapping an argumentative evaluation onto a simple clause.In this simple example, the input is made up of two semantic units: a conceptualrelation, e.g., c lass_ass ignt  (ass ignt_set l ,  a i )  and an argumentative evaluation,e.g., eva l (a i ,  d i f f i cu l ty ) .
For this example, we will restrict the available sites inthe syntactic lause capable of realizing the evaluation to be: the main verb, modifiersof the NP realizing the class (i.e., premodifying adjective or relative clause), and thedeterminer sequence.
2?The input description for this configuration of semantic units is shown in Fig-ure 13 both graphically and as an FD matrix.
In the graphical representation, argu-mentative valuations are represented as wavy lines, and semantic predications asstraight lines.
The dotted line indicates that the two evaluations (many assignmentsand difficulty) are part of an argumentative rule--a topos--which reads: the more aclass has assignments he more difficult it is.
In the FD matrix, the evaluations and theconceptual relations are represented under two separate top-level features of the inputsemr ( f loat ing  and s t ructura l  respectively).
This representation does not commit heLexical Chooser to map the evaluations to any particular site a priori--highlightingthe floating nature of evaluations.Therefore, one of the tasks of the Lexical Chooser, is to decide to which node inthe linguistic tree the argumentative evaluations will be attached.
In the overall flowof control followed by FUF (discussed in Section 2.5), semantic relations are mappedonto a linguistic tree, which is expanded top-down, breadth-first.
So the decision toattach an evaluation at a given node in the linguistic structure must also be madetop-down.
This order of decision making is shown in Figure 14 and is followed as theclause structure is constructed.At each stage of this traversal, the Lexical Chooser checks whether there is lexicalmaterial available in the lexicon to realize the evaluation at the specified syntacticlevel.
For example, at the verb level, the Lexical Chooser checks whether there exists averb that expresses the c lass_ass ignt  relation and has for connotation that the c lassargument is difficult.
In this case, the verb require can be selected.
If, however, theevaluation was on the scale of interest, no appropriate verb could be found (thereis no verb expressing both that a course is interesting and that it involves certainassignments).
At the NP level, the Lexical Chooser would then check whether thereexists a scalar adjective that can realize the evaluation.20 In the implementation f ADVISOR-II, the Lexical Chooser also considers as potential sites connectivesand indirect argumentative evaluations--i.e., relying on a topos relation of the form "the more a class hasassignments, he more difficult it is," one can realize the evaluation of a class as difficult by evaluatingas large the set of assignments i  requires.
In that case, a modifier of the NP assignments realizes theevaluation on the entity class.218Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceN Class-assig nt+ .
.
.
.
.
.
.
.
.
+cardinality difficultysemrfloatingstructural% Evaluative Relationcat toposscaleleft orientation valueevaluatedscaleright orientation valueevaluated6cardinality-I- \]\[11difficulty++ \]\[2\]% Objective relation: AI has 6 assignmentscat set \]assignments \[1\] cardinality 6generic_elt \[cat assignment \]class \[2\] name aiI assignt \[11 relationl args class \[2\]Figure 13Lexical Chooser input for one semantic unit and one floating constraint.
Example ofcorresponding output sentence: AI has many assignments, soit is difficult.can the evaluation be ~/ /~?
Is there an appropriate connotative verb?expressed at the clause level?Is there an appropriate adverbial?Is there an appropriate pre-modifyingcan the evaluation be ~ adjective?expressed at the NP level?I Can an evaluative relative clause be added?I can the evaluation be Is a judgment determiner appropriate?expressed at the determiner level?Figure 14Order of decisions during top-down traversal for mapping argumentative evaluations.The way  these decisions are imp lemented  in FUF is shown in Figures 15 and 16.The overal l  process is shown in Figure 15: the Lexical Chooser  first tries to attach the219Computational Linguistics Volume 23, Number 2Input semantic net:\[ \[structural I0 = semr floatinghas been packaged as follows by the phrase planning branch:... %cf.Fig.13 .%cf.Fig.13 \] \]11 =process \[ semr ... \]roles \[ 1 \[ semr \[1\] %Understructural \] \]2 \[ semr \[2\] % Under structural \]ao \[ sem r \[3\] % Under floating \]CONJ LEX-CLAUSEcatprocessALT MAP-AO-CLAUSE:bk-class ao% At the top-level of the clause, AO can be mapped to:% (1) Verb with connotation% (2) An adverbial% If no lexical resource is found at this level:% Try to attach AO at another level% Branch 1: No AO specified in input - nothing to do\[ao NONE \]% Branch 2: Attach AO down to verb\[ ao \[4\]GIVEN \]process \[ao .
\[conveyed proc s \]\]% Branch 3: Attach AO to adverbao \[5\] GIVEN \]adverb \[ ao \[5\]\[ conveyed adverb \] \]% Branch 4: Delegate AO to other constituents\[ ao GIVEN \]ao \[conveyed ANY \]clause\[cat verb_group \]:!
rolesFigure 15Lexicon fragments mapping a floating constraint at the clause level.element expressing argumentative evaluation 21(A0 attribute in the figure) to an appro-priate constituent (in the map-ao-clause alt).
Then the regular structural constraintsare dealt with (in the ro les  alt).
The map-ao-clause alt implements the followingalgorithm: if no AO is specified in the input, nothing needs to be done (first branch).Since unification is bidirectional, a feature \[a v\] in a grammar branch can be eithera test for the presence of the feature or the instruction to enrich the input FD withthat feature if it is not present.
The FUF keyword given is used to specify that thefeature is intended only as a test.
If an AO is specified, then it can be attached either21 More often known as speaker intention or goal.220Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceIn the context of I1 in Fig.
15 for the reference to \[1\] and \[2\]:DEF-ALT ASSIGNMENTS: bk_class ao% Branch 1: C requires A - Argumentative evaluationattributive"require"GIVEN % Only if ao is specified in inputprocesstypelexaoevaluatedscaleao conveyedorientation( .I >carrier \[41\[ semr\[5\] \[ semr attributelex_csetparticipants\[i\]name "difficulty"\]\]verb_level+.1\]\]\[21 \]% Branch 2: C has A - no AO connotation\[ type possessive \]process lex "have II% lex_cset declaration to handle recursionlex_cset ( \ [6 \ ]  \[7\] >\[ possessor \[6\]\[ semr \[1\] \] \]participantspossessed \[7\]\[ semr \ [2\] \ ]% Branch 3: In C there is A - no AO connotationprocess \[ type ex/stential \]tex~set ( \[SJ \[91 )participants \[ located \[8\]\[ scmr \[2\] \] \]circumstances \[location \[9\]\[ semr \[1\] \] \]Figure 16Three alternative lexical entries satisfying an AO constraint.on the process (branch 2) of the clause, or as an adverbial (branch 3).
In last recourse,it can be "delegated" to another lower-level constituent (brafich 4).
Note that at thislevel of processing, the Lexical Chooser does not yet know whether appropriate lexicalmaterial will be found to satisfy the constraint of expressing the AO connotation.
Forexample, when deciding to try branch 2, we still do not know whether an appropriateverb will be found.
So we just try to attach the AO at a certain level, and wait untilthe Lexical Chooser arrives at the level of the verb in its traversal of the constituenttree to verify whether our decision was justified.
This is the equivalent of a predictionstep in a top-down parsing algorithm.In the end, we must find an appropriate lexical element that will satisfy the re-quirement to express AO.
Such an element is shown in Figure 16.
It is the lexicalentry for the semantic relation assignments, which holds between a class and its as-signments.
Three options are listed to realize this relation: c lass  require assignment,c lass  have assignment and in c lass  there is assignment.
Only the verb to require con-veys an argumentative valuation (that the class is difficult).
When it is selected, thefeature \[\[ao \[\[conveyed verb - level\]\]\]\] is added--which signals that the AO is effectivelyexpressed at the verb level.In summary, the process develops as follows: at the clause level, choose a site forthe AO.
Here we committed AO to the verb by copying the feature AO from the clause221Computational Linguistics Volume 23, Number 2level under the process constituent, with the feature conflation:ao \[4\]GIVEN \]process \[ ao \[4\]\]Then proceed with regular structural traversal of the constituent tree: process, argu-ments, then under process, verb.
At the verb level, we look up the lexicon for an appro-priate entry to realize the process.
In this case, the process is already enriched with acopy of the AO annotation.
So we select he verb to require, which satisfies this request.The AO constraint is thus satisfied, and the \[ao conveyed\] feature is instantiated--whichmeans that the AO constraint is satisfied.Now, imagine that no lexical entries were found to express the AO constraint,neither at the verb level, nor at the adverbial level.
In this case, the AO constraint mustbe mapped own to an NP participant of the clause.
The mapping inside the NPs isnot described in the map-ao-clause alt, because, at this level, the Lexical Chooser hasno knowledge of which NPs will be added to the syntactic structure.
Instead, the AOconstraint is simply delegated to another constituent.
When a new NP constituent isconstructed by the Lexical Chooser, the alt map-ao-np will perform a function similarto map-ao-clause but within the NP.At the clause level, the delegation is encoded using the feature:\[ ao \[ conveyed ANY\] \]The aNY feature is a powerful construct of FUF that imposes that a feature be instan-tiated at the end of the unification process.
Because unification is order-independent, aNYconstraints can only be checked after the entire grammar has been traversed at all thelevels.
In our example,\[ ao \[ conveyed ANY\] \]declares that AO must eventually be instantiated toa ground value---that is, that somelexical element has finally taken the responsibility of expressing the AO.If we look back at the flowchart in Figure 14, we see that the algorithm justdescribed eterministically maps the AO to the first slot found in the constituent treethat can account for it.
We discuss in the next section how to add variety to thisdecision and allow a nondeterministic selection of the AO realization site.4.2 BK-CLASS: Smart Backtracking for Efficient Cross-ranking RealizationThe process just described is a search process where the Lexical Chooser tries to findan appropriate site for the realization of a floating constraint.
This search is not tightlyconstrained, since, for example, at the clause level, the Lexical Chooser maps the AOfeature to different sites without knowing in advance whether the sites are capableof handling it.
For example, the AO element is first tentatively mapped to the verb,without knowing whether an appropriate connotative verb will eventually be found.On the other hand, the construction of the linguistic structure depends on thismapping; for example, if the argumentative evaluation is mapped to a qualifier, an-other type of semantic element will not be mapped to the same site.
22 Thus, it is notpossible to first build the whole linguistic structure ignoring the AO constraint andthen subsequently examine which sites are available for the AO.22 Here, we assume a simplistic stylistic onstraint of limitating the number of postmodifiers to one.222Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceIn summary, the problem of finding an appropriate site to attach a floating con-straint is inherently a hard search problem, which cannot be handled efficiently bystraightforward search techniques.
Additional knowledge must be added to controlthe search and make it efficient.
In this section we introduce bk-class, a control toolimplemented in FUF, which reduces the search space in a significant way and makesthe implementation f such lexicalization tasks practical.A version of dependency-directed backtracking (de Kleer et al 1979), bk-class isspecialized to the case of FUF.
The bk-class construct relies on the fact that in FUF,a failure always occurs because there is a conflict between two values for a certainattribute at a leaf location in the input FD, as already enriched by some featuresfrom the FUG at the point of failure within the recursive unification process.
In ourexample, backtracking is triggered by the requirement that the value of the subattributeconveyed of the attribute ao be instantiated, when the actual subattribute is not.
Thepath {A0 conveyed} defines the address of the failure, i.e., the only decision pointsin the backtracking stack that could have caused the failure.
Identifying the addressof a failure requires additional control knowledge that must be declared in the FUG.More precisely, we allow the FUG writer to declare certain paths in the FUG to be ofa certain bk-class.
We then require the explicit declaration i  the FUG of the choicepoints that correspond to this bk-class.
In such cases, the writer must realize thatthe decisions within the class are interdependent, but she need not have knowledgeof which one is most likely to succeed.For example, the statement: (def ine-bk-class A0 conveyed) specifies that allpaths ending with the attribute conveyed are of class AO.
In addition, we tag in theFUF program all alts that have an influence on the handling of the AO constraint witha declaration ( :bk-class A0) as shown in Figures 15 and 16.To illustrate how bk-class helps in dealing with floating constraints, consideragain the require example discussed in the previous ubsection.
In the lexicon frag-ment of Figure 15, costly backtracking is avoided by forcing a fixed order for theconsideration of the possible sites to express the AO: first verb, then adverb, and fi-nally argument NP.
Also the alternative option of a separate clause is not considered.To overcome the limitation of this deterministic site selection for the AO attach-ment and allow a variety of possible outputs, we can force a random selection in thea l t  map-A0-clause of Figure 15.
This is achieved by adding an (:order :random)annotation to the a l t  map-A0-clause, which becomes:(def -a l t  map-ao-clause (:bk-class A0) (:order :random) .
.
.
)With this annotation, FUF nondeterministically chooses one of the four branches ofthe disjunction.
In this manner, the algorithm shown in Figure 14 is not performed ina fixed order.
As a consequence, the Lexical Chooser can map the AO evaluation to anoun modifier, even though it was possible to merge it into the verb--producing eitherAI requires six assignments or AI, which is difficult, has seven assignments.
In the LexicalChooser of ADVISOR-II, the AO mapping mechanism also interacts with the phraseplanning component and maps the AO to a dedicated evaluative clause--producingin our example the complex clause AI has seven assignments; therefore it is difficult.
Arandom selection among these options adds significantly to the variety of possibleoutputs.This randomization can also lead to computationally expensive dead-ends.
TheAO mapping grammar describes three possible attachment sites for AO, leading tothe sentences:?
AI requires ix assignments.223Computational Linguistics Volume 23, Number 2?
AI, which  is difficult, has six assignments.?
AI has six assignments; therefore it is difficult.If one of these sites is not available, another one must be chosen.
Unfortunately, itcan take time to realize that a given site is not available.
The possibilities are outlinedbelow.?
Case 1: all sites available: all three sentences can be generated.?
Case 2: the NP site is not avai lable because another modifier must beattached to the same NP.
This would occur, for example, in sentenceslike AL which is taught by Smith, has six assignments.?
Case 3: the verb site is not avai lable because there is no connotativeverb that simultaneously conveys the structural relation and the AOevaluation.
This would occur, for example, in sentences like AL which isdifficult, deals with some mathematical topics, since there is no English verbthat expresses the relation "deal with" together with a connotation ofdifficulty in the domain sublanguage of ADVISOR-II.?
Case 4: both the verb site and the NP site are not  avai lable (thecombination of case 2 and 3 above): only a complex clause can begenerated.The problem is that at the time the map-ao-clause disjunction is traversed, we donot yet know whether the corresponding sites are available.
The following scenariosillustrate how the late detection of availability can lead to a prohibitive search time:?
Case 1: All sites available.
The order in which map-ao-clause istraversed etermines which sentence is generated.
No price is paidcomputationally, since all choices lead directly to an acceptableconfiguration.?
Case 2: NP site not available---Verb site tried first in map-ao-clause.
Inthis case again, there is no time wasted, the Lexical Chooser "guesses"right the first time.?
Case 3: NP site not avai lable - NP site tried first.
In this case, theLexical Chooser goes the wrong way and must backtrack.
Backtrackingis triggered only when the\[ ao \[ conveyed ANY\ ]  \]constraint is checked, which is at the end of the unification process.
Thismeans that full lexicalization of the input is performed--ignoring the AOconstraint--and at the end, a failure is discovered.
Using a simplechronological backtracking control, such a late failure is the worstpossible scenario.
All possible options starting at the wrong guess inmap-ao-clause are systematically explored, until finally, theraap-ao-clause choice point is tried again, leading to an acceptablesentence.224Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceDeciSiOnMakingI Lexicalize ASSIGNMENT relation IFigure 17Backtracking and BK-CLASS.Map AO constraintI Choose verb site - Delay \[ANY\] 1I Map args to complements 1Processall (subconstituents> <Check ANY:AO IS NOT CONVEYEDBacktrackingCase 4: Two sites (verb and NP) not ava i lab lemboth  tried before thecomplex clause option.
The scenario is the same, except hat whenbacktracking reaches the map-ao-clause, it starts once more with awrong guess, and the same wasteful search is triggered.The bk-c lass construct solves this problem efficiently by jumping back directly tothe map-ao-clause when the ANY failure is detected at address {ao conveyed}.
This ispossible only if the grammar writer has declared the map-ao-clause as a choice pointof class a0.
Figure 17 illustrates the search process, and how bk-c lass affects it.
Whenthe unifier fails at a location of class A0, it directly backtracks to the last choice pointof class a0, ignoring all intermediate decisions.In our example, when the ANY constraint fails, we directly backtrack to the lastAO choice point encountered, which was in the map-ao-np disjunction.
If no otheroption is left in the NP, we backtrack up to the alt map-ao-clause of Figure 16.
Wetherefore use the knowledge that only the verb, the NP, or an additional clause cansatisfy the AO constraint in a clause to drastically reduce the search space.
Thanksto the bk-c lass construct, this knowledge is locally expressed at each relevant choicepoint, retaining the possibility of independently expressing each constraint in the FUFprogram.To evaluate the practical effect of bk-class, we have measured the number ofbacktracking points required to lexicalize different clauses illustrating the scenariodiscussed above.
Table 1 summarizes these measurements, performed on the lexicalchooser for ADVISOR-II.225Computational Linguistics Volume 23, Number 2Table 1Measuring the effect of bk-class.Backtrack pointsInput Output w/o bk w/  bkAll sites available AI requires ix assignt.
46 46AL which is hard, has six assignt.
59 59AI has six assignt., therefore it is hard.
189 189Verb unavailableNP site tried firsttwo clauses tried firstVerb site tried firstNP unavailableNP site tried firstNP and verb unavailabletwo clauses tried lastAL which is hard, deals with math.AI deals with math, therefore it is hard.AI, which is hard, deals with math.58 5859 598,379 86AL .
.
.
.
requires ix assignt.
10,546 124AI, .
.
.
.
deals with math, therefore it is hard.
15,580 128AL .
.
.
.
has six assignt., therefore it is hard.
22,719 189The number of backtracking points required to lexicalize ach example clause islisted with and without bk-class.
The first group, with all sites available, indicatesthe size of the lexicon.
The numbers can be interpreted as the number of decisions theLexical Chooser makes to lexicalize abasic clause for which practically no backtrackingis required.
It roughly corresponds to the number of unretracted decisions made bythe Lexical Chooser and is the optimal number of backtracking points that a searchcontrol regime can obtain for the given input.
Without bk-class, the wide variation innumber of backtracking points among the examples indicates the exponential natureof the blind search that floating constraints impose on the standard control regime.In contrast, with bk-class, the variation in number of backtracking points remainswithin a factor of three among all the examples.The dependency-directed mechanism implemented in FUF with bk-class thereforecomplements a general top-down control regime to make the processing of floatingconstraints efficient.
The performance penalty imposed by a floating constraint de-pends on the number of sites in the syntactic structure where it can be realized.
Forexample, the AO constraint can be realized at three levels and it may require the uni-fier to retraverse the same FUG branches three times until it finds a site to convey theAO constraint.
Each floating constraint can be characterized by its range of possibleattachment odes.The bk-class mechanism improves the efficiency of functional unification whilepreserving its desirable properties--declarativeness and bidirectional constraint satis-faction.
It is a declarative statement ofdependency between a decision in the FUG anda class of constraints in the input.
Using bk-class, however, is not always easy forthe FUG writer since it requires thinking about he control strategy of the unifier--thesame drawback as for PROLOG'S cut  mechanism.
We are currently investigating theuse of abstract interpretation techniques (Cousot and Cousot 1977) to automaticallydetermine where bk-class annotations are necessary and thus ease the task of theprogrammer.5.
Interlexical ConstraintsInterlexical constraints occur when a pair of content units is realizable by alternativesets of collocations (Smadja nd McKeown 1991).
This is the case, for example, in the226Elhadad, McKeown, and Robin Floating Constraints in Lexical Choicefollowing situation:?
A domain relation R is realizable by two verbs V1 and V2?
A domain entity E is realizable by two nouns N1 and N2.?
<V1,NI> and (V2,N2) are verb-object collocations.?
(V1,N2) and <V2,N1) are not verb-object ollocations.We observed the influence of interlexical constraints in our corpus of newswirebasketball reports used in developing STREAK.
The act of performing strictly betterthan ever before can alternatively be lexicalized by the collocations to break a recordor to post a high.
However, even though the elements of the collocation are dependenton different portions of the input network, they are not interchangeable.
One cansay neither ?to break a high nor ?to post a record.
For other relations, the words areinterchangeable; forexample, one can say either to equal a high or to equal a record.The input for these collocations i shown in Figure 18.
It includes two relations:performance(player, statistic) and improve(performance, maximum), which in-dicate that a player's performance improves some previously set maximum.
The verbof the collocation realizes these two semantic relations, such as to break or to post simul-taneously indicating (I) a relation between a player and one of its game statistics and(2) that this statistic is strictly higher than some previous maximum.
If, in contrast, thestatistic was even with the previous maximum, then the verb equal or match would beselected instead.
Whether the object is high or record depends on the type of maximum.In this domain, a maximum can occur for a variety of different reference sets.
It canbe a maximum for a player, for a team, or for a league.
The noun high is used whenthe performance is a maximum of past performances by the same player while recordis used when it is maximum over past performances of other players as well.These semantic onstraints on individual word choice are orthogonal to the inter-lexical constraints between verb and object.
It is possible, therefore, to encode interlex-ical constraints in various locations in the Lexical Chooser.
One possibility is to encodethe verb-object ollocation with the lexicalization options for the predicate.
This meansthat both the lexicalization of the predicate and that of its collocationally constrainedargument are chosen all at once.
Such simultaneous choices hinder the modularity ofthe Lexical Chooser and declarative r presentation f individual constraints.
With thisorganization, the choice of verb must be indexed by the semantic onstraints from thearguments even though these constraints do not directly influence predicate choice.Furthermore, if this constraint is encoded in the lexical entry for the predicate, it willhave to be repeated for the other domain relations that can take the same conceptas an argument, such as equal.
This is undesirable since the verbs for lexicalizingthis relation are not collocationally restricted, as indicated by the validity of all thefollowing forms: to equal a high, to match a high, to equal a record, to match a record.A second possibility is to represent the semantic onstraints on verb and nounchoice separately; where there are several possible verbs for a given semantic on-straint, the verb is chosen randomly.
The collocational constraints between verb andnoun are represented with the noun in this scheme.
In this case, modularity in therepresentation f individual constraints i preserved.
Semantic onstraints on the re-spective lexicalizations of the predicate and its argument are independently encoded.However, the orthogonal interlexical constraint can trigger backtracking: for example,if the verb to break is first randomly chosen for the relation improve and then the nounhigh gets selected because the input indicates the player as the reference set for themaximum concept, the Lexical Chooser must backtrack to select he verb post instead.227Computational Linguistics Volume 23, Number 2semrstructuralfloating% Relation 1 marked as structural by the content planner% Stockton scored 45 pointsname performance\[cat p layer \ ]actor \[1\] name Stocktonrelation1 surname Johnperf \[2\] value 45unit points% Relation 2 marked as floating by the content planner% This performance improves Stockton's maximumrelation2 args past_perf \[3\]\[\]% Past performance that the new performance \[2\] improves:% Maximum of the set of all scoring+ performances scored by Stocktoncat maximumcat setref _set generic_elf actor \[1\] cat statisticperf unit pointsFigure 18Test input for the sentence John Stockton posted ahigh with 45 points.In order to both preserve modularity and avoid backtracking, the solution is to de-lay the choice of one collocate until the other one is chosen.
We have implemented thisin FUF through a control mechanism termed :wait, which allows explicit representa-tion of the interlexical constraints along with modular epresentation f the individualsemantic onstraints on word choice.
The :wait mechanism is used to indicate thata particular choice should be delayed until some other specific choice point in thegrammar.
Again, the grammar writer must know that the two decisions are interde-pendent, but does not need to know which one will take priority.
Here, the choiceof verb is delayed until the head of the object is selected.
Figure 19 shows how the:wait annotation of FUF implements such a delay.
In this case, the choice of verb issuspended until its object noun collocates have been chosen.
Therefore, no backtrack-ing occurs even though the respective semantic onstraints on the lexicalization ofthe predicate and its argument are kept separate.
The :wait annotation is a generalcontrol facility that allows optimizing a FUG whenever it is known that two decisionclasses are dependent on one another.
The case illustrated in this paper, where thesetwo decisions classes are verb choice and object head noun choice, is only one of themany types of optimizations made possible by the use of :wait (see Elhadad \[1993c\]for other types).6.
Other Approaches to Lexical Choice6.1 FUF-based SystemsIn this section, we present four applications, developed at Columbia, which use FUFfor lexical choice.
The first two use a very similar approach to ADVISOR-II, while thelast two incorporate the same model within distinct system architectures.228Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceDEF-ALT LEX_SCORE% Branch 1: .
.
.% Branch 2: Verbs merging a PERFORMANCE structural relation% and an IMPROVE floating relation% Unifies with result of clause planning on input of Fig.18process \[semr\[nameperformance\]\]roles perf \[1\] GIVENfloating GIVEN\[ Process i semr \[ name improve \] \] i \]floating new_perf \[1\]roles oldqoer f \[cat maximum \]:!
IMPROVE-MAX_VERBSDEF-ALT IMPROVE_MAX_VERBS% Choice of verb delayed until object is lexicalized : order randomwait ( participants affected head lex \ /% Purely lexical test since done when object noun already chosen% Branch 1: break a recordprocess \[ lex "break" \]affected head \[ lex "record"\]\]% Recurse on both argumentslex_cset < \[\]  \[ \]  >% Branch 2: postprocessparticipantslex-cseta high\[ lex "post" \]agent \[3\]affected \[4\]\[ head \[ lex% Recurse on both arguments (L31 t4\]>"high"\]\] \]"DEF-CONJ LEX-MAXsemr \[cat maximum\]cat npALT SEMANTIC-CONSTRAINT-ON-NOUN% Choose noun for max.
with no reference to embedding verb% Branch 1: record\[ semr \[ ref.set \[ generic_elt \[ actor \[cat{teamleague} \] \] \] \]head \[ lex "record" \]% Branch 2: high\[ semr \[ ref~set \[ generic_elt \[ actor \[catplayer\] \ \] \] \]head \[ lex "high"\]Figure 19Use of :wait to preserve modularity and avoid backtracking.229Computational Linguistics Volume 23, Number 26.1.1 Systems with Similar Architectures.
Two applications developed at Columbiause FUF for lexical choice in much the same way as ADVI$OR-II: COMET (COordinatedMultimedia Explanation Testbed) (Feiner and McKeown 1990; Elhadad et al 1989)and PLANDOC (McKeown, Robin, and Kukich 1995).
These systems are more limited,however, in that they do not handle floating constraints, and thus lexical choice consistsof a one-to-one mapping between conceptual nd linguistic structures.
This mappingfocuses on the paradigmatic decisions and can be efficiently performed using thesimple default op-down regime of recursive unification.
But in effect, this shifts theburden of performing most syntagmatic decisions to the Content Planner, which mustthen hand to the Lexical Chooser a tree-structured input that already prefigures the(lexicalized) thematic tree that the Lexical Chooser in turn passes on to SURGE.In COMET, a system which generates multimedia explanations for equipment main-tenance and repair, the major research focus is the co-ordination of multiple mediaand some word choices are influenced by decisions made by the graphics component.For example, to determine how to refer to an object in an accompanying illustration,the Lexical Chooser takes into account how the object is depicted (McKeown et al1992).
COMET also considers constraints from the user and from previous discoursein selecting words.
This case has some similarities with floating constraints; if a wordis unknown to the user, an alternative word that can simply be substituted in thesentence may not exist.
Instead, COMET must replan the entire sentence when an al-ternative word is not available, reinvoking its content planner (McKeown, Robin, andTanenblatt 1993).PLANDOC is an automated ocumentation system under joint development byColumbia and Bellcore.
It produces one to two page reports documenting the activi-ties of telephone planners.
While PLANDOC does include a wide variety of paraphrases,some of which involve different word choices, and handles interactions between dif-ferent choices, lexical choice was not the primary issue in this system.
Instead, thefocus was on the systematic use of conjunction with ellipsis and its interaction withsyntagmatic paraphrases, in order to produce concise, yet fluent, summaries.
FUF wasused as a tool for developing the Lexical Chooser, but with less-novel results on thetopic of lexical choice.6.1.2 Systems with Different Architectures.
Two other applications developed atColumbia also use FUF for lexical choice, but within a different system architecture:COOK (Smadja nd McKeown 1991) and STREAK (Robin 1994b).
Some of the examplesdiscussed in this paper within the framework defined by the architecture of ADVISOR-II originated from the respective domains for which these two other systems wereimplemented.The focus in COOK, a system for generating stock market reports, was on interlexi-cal constraints such as those presented in Section 5.
By representing in FUF collocationsthat were automatically derived from a corpus of stock market reports, COOK couldmerge phrasal and single word constraints in the same lexicon as well as handle inter-actions between collocations.
The way lexical choice is performed in COOK differs fromthe way it is performed in all the other FUF-based systems in that it is not driven by atop-down traversal of the input conceptual structure.
Instead, lexical entries (individ-ual words or collocations) are chosen in afixed order in terms of the syntactic functionthey will ultimately occupy in the output sentence: first the verb arguments, then themain verb, finally the adjuncts.
This bottom-up regime is less efficient han the top-down regime for handling the structural constraints described in Section 4.
However,it allows indexing the lexicon by <lexical item, syntactic function> pairs rather thanby concepts, a property that is handy for using syntactically marked collocations auto-230Elhadad, McKeown, and Robin Floating Constraints in Lexical Choicematically produced by a domain-independent toolwith no semantic knowledge suchas  XTRACT (Smadja 1991).The focus in STREAK was the definition of a draft-and-revision architecture forgenerating the type of very complex sentences used in newswire reports to summarizequantitative data about a basketball game and its historical background.
STREAK relieson revision rules drawn from a corpus analysis of such reports (Robin and McKeown1993) to incrementally generate such complex sentences.
While the draft-and-revisionapproach of STREAK sets it apart from all the other FUF-based generators, the LexicalChooser of STREAK is akin to the Lexical Chooser of ADVISOR-II in that:?
It handles floating constraints.?
Its input is a linguistically unbiased fiat semantic network.?
It is based on top-down recursive functional unification.There is, however, an important difference in the architectures of the two systems.STREAK handles the structural constraints and the floating constraints in two sepa-rate passes.
Structural constraints are handled during an initial draft-building passand floating constraints during a subsequent revision pass.
This is in contrast withADVISOR-II, which handles both types of constraints simultaneously.
In STREAK, theLexical Chooser is thus called first to build the draft and then again at each revisionincrement.
The need for revision in STREAK primarily stems from the necessity in writ-ten summaries to concisely pack many facts into very complex sentences, as observedin newswire stories.
For example, in the corpus of basketball reports that served asmodel output for STREAK, some sentences conveyed up to 12 conceptual relations andcontained up to 46 words.
In contrast, in the corpus of student advising sessions thatserved as model output for ADVISOR-II, no sentences conveyed more than four con-ceptual relations or contained more than 25 words.
For the sentences ofmore ordinarycomplexity found in dialogues, revision is not needed and simultaneously combiningsemantic units into a single word can be achieved far more efficiently in a single pass.6.2 Other SystemsIn this section we overview approaches tolexical choice that do not rely on a FuF-basedlexicon.
We have classified them according to where they position the task of lexicalchoice in the overall generation architecture.
For each class of systems, we describethe constraints on lexical choice that were considered.6.2.1 After Content Planning and Before Syntactic Realization.
This approach is themost common among text generation systems.
The surface realization component isseparated into two successive modules: lexical choice followed by syntactic realization.The lexical choice module builds the linguistic structure and chooses all open-classwords.
The syntactic realization component deals with agreements, ordering of con-stituents, choice of closed-class words, and in most systems can also perform syntactictransformations on the structure provided by the Lexical Chooser (such as passiviza-tion, dative movement, here-insertion, or it-extraposition).
This is the approach usedin the systems TEXT (McKeown 1985), GENARO-MUMBLE (Conklin 1983; Meteer et al1987), PENMAN (The Penman NLG 1989), SPOKESMAN (Meteer 1990), EPICURE (Dale1992), KALIPSOS (Nogier 1990) and in the generators based on the Meaning-Text The-ory (MTT) (Mel'Suk and Pertsov 1987) such as: FOC (Bourbeau et al 1990), GossIP(Carcagno and Iordanskaja 1993) and LFS (Iordanskaja et al 1994).231Computational Linguistics Volume 23, Number 21.
Main Verb with Adjunct: Prices increased by 20%.2.
Object Noun with Postmodifier: Prices showed an increase of 20%.3.
Subject Noun with Premodifier: A 20% increase has been reported.Figure 20Cross-ranking paraphrasing in the Meaning-Text Theory.These systems primarily use semantic and syntactic onstraints in selecting thewords to use.
Typically, each semantic oncept has an entry in the lexicon.
The lexicalentry for the semantic head (usually the verb, but indicated by either the semanticrepresentation r the underlying application) is accessed first.
The word chosen candepend on the semantic features of the arguments o the head (thus, this scheme basi-cally follows Goldman's \[1975\] use of discrimination networks).
It is at this point thatthe overall syntactic structure is determined.
This lexical entry then usually invokesthe lexical entries for the arguments to the head and this control is followed untilall input arguments have been lexicalized.
In these systems, pragmatic onstraintsare not consistently accounted for and when interlexical constraints are accounted forthey are encoded as phrasal entries.
Perspective on the input conceptual structure isfixed; thus, different entry points into this input structure cannot determine what thesyntactic head of the sentence will be (except in SPOKESMAN \[Meteer 1990\]).
In otherwords, the conceptual structure determines linguistic structure.
Lexical choice was nota major research issue in any of these systems (unlike the work presented here), withtwo exceptions: KALIPSOS and MTT-based generators.Research for KALIPSOS focused on mapping multiple conceptual elements to thesame word using matching of conceptual graphs.
It also allowed for some decisionmaking in determining the perspective of the clause, selecting the verb that covers thelargest portion of the input network.
Unlike our work, it handled primarily semanticand syntactic constraints on choice and thus, for example, does not dispatch conceptualnodes to different syntactic ranks.Paraphrasing power using a word-based lexicon is also a central issue in MTT, ageneration-oriented an  highly stratificational linguistic theory in which sentences arerepresented atmultiple layers, five of which are relevant to the task of lexical choice(Polgu6re 1990).ADVISOR-II and MTT-based systems are similar in that lexical choice in both startsfrom a flat conceptual network (the Conceptual Communicative R presentation (CCR)layer in the case of MTT-based systems), they perform choice of perspective as wellas syntagmatic choices, and they take into account interlexical constraints.There are also three main differences:The MTT approach is more stratified.
In GOSSIP and LFS, lexical choice isdecomposed into a pipeline of four mappings between the five layers ofMTT representations.
In ADVISOR-II, the input flat conceptual network(corresponding to the CCR in MTT) is directly mapped onto the outputlexicalized syntactic tree.
2323 It is interesting to note that he most applied among MTr systems, FOG, directly maps the CCR onto aDSyntR.
It may be an indication that all the intermediary representations defined in the MTr are notnecessary in all domains.232Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceMTT makes the different ypes of lexical choices in a fixed order: first,paradigmatic choices that depend only on semantic onstraints, thenperspective choice, followed by syntagmatic choices, and finallyparadigmatic choices that depend on interlexical constraints.
ADVISOR-IIstarts with perspective choice and then interleaves syntagmatic andparadigmatic choices.MTT does not handle pragmatic onstraints that float across the whole spectrum oflinguistic ranks (i.e., all the way from connectives linking several clausesor sentences down to determiners) uch as the argumentative evaluationsgenerated by ADVISOR-II.
MTT also does not explicitly distinguishbetween structural and floating constraints, which makes considering theconnective or determiner as alternative sites to clausal or nominal sitesproblematic.
This is significant, since using a connective or a determinerinstead of a semantically rich verb or a noun modifier allows for a moreimplicit expression of floating constraints (at least in the case ofargumentative evaluations).
MTT does, however, allow cross-rankingparaphrasing limited to the clause and NP ranks, as illustrated by theexamples in Figure 20 (taken from Boyer and Lapalme \[1985\]).6.2.2 During Surface Realization, Interleaved with Syntactic Realization.
In this ap-proach, lexical choice is considered as one linguistic decision like any other one, and,therefore, it is not isolated in a dedicated component.
Instead, the syntactic grammarcontains very specific rules for lexical insertion.This approach is generally associated with the use of either:?
A phrasal exicon such as in the generation systems ANA (Kukich 1983a),PI-IRF.D (Jacobs 1985) and PAULINE (Hovy 1988).?
A lexicalist reversible grammar (Strzalkowski 1994), such as a synchronousTAG (Shieber and Shabes 1991), especially in conjunction with thesemantic-head-driven algorithm to generation (Shieber et al 1990).In a phrasal lexicon entry, all the syntactic onstraints between the elements of thetemplate are already preselected, so there is no need for much syntactic realization:constituents are already ordered, their syntactic ategory is fixed in the phrasal tem-plate, closed-class words are already selected.
Alternations like passivization or dativemovement are encoded by defining, for each domain concept, several different em-plates, one for each combination of these orthogonal options (and thus failing to cap-ture the generality of these syntactic transformations independently of specific wordsand concepts).
Interlexical constraints pose no problem since mutually constrainedwords are not chosen individually but all at once by accessing a phrasal template (itis just a matter of encoding only the entries corresponding tovalid collocations).
Sim-ilarly, floating constraints are less of a problem for the phrasal approach, since mostalternative locations for their expression remain for the most part within the scope ofa phrasal template.
However, the price to pay for this simplicity is high: generatorsrelying on a phrasal exicon simply do not scale up.
Extending their paraphrasingpower and semantic overage requires hand-coding a combinatorially explosive num-ber of phrasal templates.
With a more compositional pproach, such scaling up canbe achieved by adding only few words along with the constraints that bind some ofthem (see Robin and McKeown \[1996\] for a corpus-based quantitative evaluation ofthe scalability gains achievable through compositionality).233Computational Linguistics Volume 23, Number 2In most approaches based on a lexicalist grammar, the semantic structure is tra-versed and a semantic head is selected; after the head is selected, all the syntacticdecisions that depend on this head are performed; then the dependent constituentsare identified and lexicalized in turn.
Lexicalization and syntactic realization are, there-fore, interleaved as the linguistic structure is being built.
With this approach, only arestricted range of constraints on lexical choice can easily be handled: in most casesonly the semantic and syntactic onstraints coming from the local constituent influencewhat word is selected.
Interlexical and pragmatic onstraints are not discussed and wehave some doubts about how easy it would be to incorporate them in this approach.Since words are selected only as the full syntactic tree is constructed, it would bedifficult to take floating constraints into account, which collapse different portions ofthe semantic input into different portions of the syntactic tree.6.2.3 During Content Planning and Before Syntactic Realization.
Another class ofgeneration systems positions the task of lexical choice somewhere during the process ofdeciding what to say, before the surface generator is invoked.
This positioning allowslexical choice to influence content and to drive syntactic hoice.
Danlos (1986) choosesthis ordering of decisions for her domain.
In her system, a lexicon grammar first selectslexical items for the predicative lements of the conceptual input.
A discourse grammaris then used to combine clauses into valid discourse organization patterns, which alsodetermine syntactic realization (choice passive/active for example).
Danlos's systemthus performs content determination and lexical choice before discourse organizationand syntactic realization (a very original position).
Since all the possible combinationsof alternative discourse organizations and alternative syntactic forms need to be hand-coded in the discourse grammar, this approach suffers from the same drawback as thephrasal exicon approach: it is not compositional nd thus does not scale up.Rubinoff's (1992) IGEN also addresses the problem of the interaction between con-tent decisions and linguistic decisions and implements an architecture that relies onexplicit negotiation between the two components: the content planner equests a set ofoptions from the Lexical Chooser to realize a certain conceptual element, and the Lex-ical Chooser eplies with annotated options.
The content planner selects among theseannotations as it proceeds and combines the preferred options into an English mes-sage.
Rubinoff designed a language of annotations that maintains a good separationof knowledge between the conceptual and linguistic components.Other researchers advocate folding the lexicon into the knowledge representation.In this approach, as soon as a concept is selected for the text, the words associated withit in the knowledge base would automatically be selected as well.
This is the approachin KING (Jacobs 1987) and FN (Reiter 1991).
In this approach, interlexical and syntacticconstraints on lexical choice are not addressed, thus restricting the coverage of thedomain sublanguage of the generator to sentences where these constraints do notcome into play.
McDonald's (1983) use of realization classes allows for the advantagesof this approach while nonetheless allowing for syntactic onstraints to play a role.He makes use of inheritance within a knowledge base (McDonald 1981) to associategeneric concepts uch as 0BJECT directly with a phrase category such as NP, butallows for individuations of the concept o have exceptions.
Options are expressedin realization classes.
While this does allow for a given input semantic element obe mapped to different syntactic onstructions, this approach does not address theproblem of merging semantic oncepts in a single word, nor does it address interlexicalconstraints.234Elhadad, McKeown, and Robin Floating Constraints in Lexical Choice7.
ConclusionIn this paper, we have presented a general model for lexical choice, which allows ageneration system to take into account awide range of constraints on word selection ina compositional, flexible, and yet efficient fashion.
By positioning the Lexical Chooserafter content selection and before syntactic realization, both conceptual and linguisticconstraints can influence word choice.
Critically, our work has focused on the problemof floating constraints, providing a mechanism that allows the choice of a single wordto realize several semantic oncepts and conversely allows a single semantic onceptto be realized by multiple words at distinct linguistic ranks.
Our technique for lexicalchoice is characterized by the following features:?
It is capable of handling a wide variety of constraints on lexical selection(co-occurrence r strictions, connotation of lexical items, syntactic andconceptual properties, and pragmatic effects).?
Interaction among these constraints i easily handled through the use ofa unification formalism to describe lexical entries.?
Floating constraints can be attached at different levels of the generatedsyntactic structure, and several conceptual elements can be merged ontoa single linguistic item, providing for more compact and lexically richeroutput.?
Syntagmatic decisions are also made by the Lexical Chooser, so that aninput conceptual structure can be mapped to a variety of linguisticstructures.
For example, the selection of "perspective" in a clause isguided by the available lexical resources (which verbs exist to express agiven conceptual relation) and by pragmatic onsiderations (whichrelation is highlighted as the topic of the discourse).Our implementation of the Lexical Chooser as a functional unification grammarallows the separate, declarative representation f different constraints, with unificationallowing for interaction between constraints.
Furthermore, within this framework wehave developed an algorithm for lexical selection, and the consequent building ofsyntactic structure, such that at each choice point in the structure, the Lexical Chooserconsiders all semantic oncepts that can be realized, choosing a word that conveys asmany as possible or discharging a concept o dependent linguistic sites, at the endchecking that all concepts have been covered.
In this framework, floating constraintscan be merged with other semantic onstraints at any linguistic rank.Finally, handling floating constraints requires that the same semantic structure ismapped to different syntactic structures in different contexts.
Thus, a Lexical Choosermust also consider syntagmatic choice.
We have presented an approach that allowsperspective to be chosen depending on the discourse focus.
Different relations may bechosen as the head of a syntactic structure, depending on the focus.
This means thatthe generation system encodes no a priori assumptions about which domain conceptswill be realized as which syntactic onstituents.Any lexical selection algorithm that handles uch a wide range of constraints mustdeal with the issue of computational complexity.
To ensure an efficient Lexical Chooser,we have developed a collection of techniques in the FUF programming environmentto limit the cost of operations required during lexical choice.
We have described inthis paper lazy evaluation (with the : wait annotation) and dependency-directed back-tracking (with the :bk-c3.ass annotation) and shown how it reduces the cost of lexical235Computational Linguistics Volume 23, Number 2choice.
With the help of these tools, we have implemented sophisticated lexical opti-mizations in ~uF.Our work results in an interpreter for an efficient Lexical Chooser, which allowshandling of a wide variety of interacting constraints.
In future work, we plan to in-vestigate the construction of domain- independent lexicon modules and methods fororganizing large scale lexica, topics that we did not address in this work.
This willrequire extending our work to provide tools for representing content of the lexicon inaddition to the tools for manipulating and representing content hat we have detailedhere.AcknowledgmentsThis research was partially supported by ajoint grant from the Office of NavalResearch and the Advanced ResearchProjects Agency under contractN00014-89-J-1782, by National ScienceFoundation Grants IRT-84-51438 andGER-90-2406, and by the New York StateScience and Technology Foundation underthe auspices of the Columbia UniversityCAT in High Performance Computing andCommunications in Health Care, a NewYork State Center for Advanced Technology.We would like to thank the anonymousreviewers for their helpful comments.ReferencesAbella, A.
1994.
From pictures to words:Generating locative descriptions of objectsin an image.
In Proceedings ofthe ImageUnderstanding Workshop, Monterey, CA,November.Anscombre, J. C. and O. Ducrot.
1983.L'argumentation dans la langue.
Philosophieet langage.
Pierre Mardaga, Bruxelles.Appelt, D. 1983.
Telegram: A grammarformalism for language planning.
InProceedings ofthe 8th International JointConference on Artificial Intelligence.
IJCAI.Bateman, J.
A., R. T. Kasper, J. D. Moore,and R. A. Whitney.
1990.
A generalorganization of knowledge for naturallanguage processing: The Penmanupper-model.
Technical Report, ISI,Marina del Rey, CA.Bourbeau, L., D. Carcagno, E. Goldberg,R.
Kittredge, and A. Polgu~re.
1990.Bilingual generation of weather forecastsin an operations environment.
InProceedings ofthe 13th InternationalConference on Computational Linguistics,Helsinki University, Finland.
COLING.Boyer, M. and G. Lapalme.
1985.
Generatingparaphrases from meaning-text semanticnetworks.
Computational Intelligence, pages103-117, August-November.Carcagno, D. and L. Iordanskaja.
1993.Content determination a d textstructuring: Two interrelated processes.
InH.
Horacek and M. Zock, editors, NewConcepts in Natural Language Generation:Planning, Realization and Systems.
FrancesPinter, London and New York.Carpenter, B.
1992.
The Logic of Typed FeatureStructures with Applications to UnificationGrammars, Logic Programs and ConstraintResolution, Cambridge Tracts inTheoretical Computer Science, volume 32.Cambridge University Press, Cambridge.Conklin, E. 1983.
Data-driven I deliblePlanning of Discourse Generation UsingSalience.
Ph.D. thesis, University ofMassachusetts, Amherst, MA.Cousot, P. and R. Cousot.
1977.
Abstractinterpretation: A unified lattice model forstatic analysis of programs byconstruction or approximation offixpoints.
In Proceedings ofthe Fourth ACMSymposium on the Principles of ProgrammingLanguages, pages 238--252, Los Angeles,CA.Dalai, M., S. K. Feiner, K. R. McKeown,D.
Jordan, B. Allen, and Y. alSafadi.
1996.Magic: An experimental system forgenerating multimedia briefings aboutpost-bypass patient status.
In Proceedingsof the American Medical InformaticsAssociation, Washington, D.C., October.Dale, R. 1992.
Generating ReferringExpressions.
ACL-M1T Press Series inNatural Language Processing, Cambridge,MA.Danlos, L. 1986.
The Linguistic Basis of TextGeneration.
Studies in Natural LanguageProcessing.
Cambridge University Press,Cambridge.de Kleer, J., J. Doyle, G. L. Steele, andG.
J. Sussman.
1979.
Explicit control ofreasoning.
In Winston P.J.
and R.H.Brown, editors, Artificial Intelligence: AnMIT Perspective.
MIT Press, Cambridge,MA, pages 93-116.Elhadad, M. 1993a.
Fuf: The universalunifier--user manual, version 5.2.Technical Report CUCS-038-91, ColumbiaUniversity, New York.236Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceElhadad, M. 1993b.
Generatingargumentative judgment determiners.
InProceedings ofthe 11th National Conference onArtificial Intelligence, pages 344-349.
AAAI.Elhadad, M. 1993c.
Using Argumentation toControl Lexical Choice: A Unification-basedImplementation.
Ph.D. thesis, ComputerScience Department, Columbia University,New York.Elhadad, M. 1995.
Using Argumentation itext generation.
Journal of Pragmatics,24:189-220.Elhadad, M. 1996.
Lexical choice forcomplex noun phrases.
MachineTranslation.Elhadad, M. and J. Robin.
1992.
Controllingcontent realization with functionalunification grammars.
In R. Dale,H.
Hovy, D. Roesner, and O. Stock,editors, Aspects of Automated NaturalLanguage Generation.
Springer Verlag,pages 89-104.Elhadad, M. and J. Robin.
1996.
Anoverview of surge: A reusablecomprehensive syntactic realizationcomponent.
Technical Report 96-03, BenGurion University, Dept of ComputerScience, Beer Sheva, Israel, April.Elhadad, M., D. Seligmann, S. Feiner, andK.
McKeown.
1989.
A common intentiondescription language for interactivemulti-media systems.
In A New Generationof Intelligent Interfaces: Proceedings ofI\]CAI89 Workshop on Intelligent Interfaces,pages 46-52, Detroit, MI, August 22.Fawcett, R. P. 1987.
The semantics of clauseand verb for relational processes inEnglish.
In M. A. K. Halliday andR.
P. Fawcett, editors, New Developments inSystemic Linguistics.
Frances Pinter,London and New York.Feiner, S. and K. McKeown.
1990.Generating coordinated multimediaexplanations.
In Proceedings ofthe IEEEConference on AI Applications, SantaBarbara, CA, March.Goldman, N. 1975.
Conceptual generation.In Roger Schank, editor, ConceptualInformation Processing.
North-Holland,Amsterdam, pages 289-374.Halliday, M. A. K. 1985.
An Introduction toFunctional Grammar.
Edward Arnold,London.Harbusch, K. 1994.
Towards an integratedgeneration approach with tree-adjoininggrammars.
Computational Intelligence,10(4):579-590.Hovy, E. 1988.
Generating Natural Languageunder Pragmatic Constraints.
L. ErlbaumAssociates, Hillsdale, N.J.Iordanskaja, L., M. Kim, R. Kittredge,B.
Lavoie, and A. PolguSre.
1994.Generation of extended bilingualstatistical reports.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics.
COLING.Jacobs, P. 1985.
Phred: a generator fornatural language interfaces.
ComputationalLinguistics, 11(4):219-242.Jacobs, P. S. 1987.
Knowledge-intensivenatural language generation.
ArtificialIntelligence, 33:325-378.Kay, M. 1979.
Functional grammar.
InProceedings ofthe 5th Annual Meeting of theBerkeley Linguistic Society.Kukich, K. 1983a.
The design of aknowledge-based report generator.
InProceedings ofthe 21st Annual Meeting.Association for ComputationalLinguistics.Kukich, K. 1983b.
Knowledge-based ReportGeneration: A Knowledge EngineeringApproach to Natural Language ReportGeneration.
Ph.D. thesis, University ofPittsburgh.Kukich, K., K. McKeown, J. Shaw, J. Robin,N.
Morgan, and J. Phillips.
1994.User-needs analysis and designmethodology for an automated documentgenerator.
In A. Zampolli, N. Calzolari,and M. Palmer, editors, Current Issues inComputational Linguistics: In Honour of DonWalker.
Kluwer Academic Publishers,Boston.Lester, J. C. 1994.
Generating NaturalLanguage Explanations from Large-ScaleKnowledge Bases.
Ph.D. thesis, ComputerScience Department, Universtity of Texasat Austin, New York, NY.Levi, J.
1978.
The Syntax and Semantics ofComplex Nominals.
Academic Press.Levin, B.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Lyons, J.
1977.
Semantics.
CambridgeUniversity Press, Cambridge.Mann, W. C. and C. M. Matthiessen.
1983.Nigel: a systemic grammar for textgeneration.
Technical ReportISI/RR-83-105, ISI, Marina del Rey, CA.Matthiessen, C. M. 1991.
Lexicogrammaticalchoice in text generation.
In C. Paris,W.
Swartout, and W. C. Mann, editors,Natural Language Generation i ArtificialIntelligence and Computational Linguistics.Kluwer Academic Publishers, Boston.McCoy, K. F. and J. Cheng.
1991.
Focus ofattention: Constraining what can be saidnext.
In C. Paris, W. Swartout, andW.
C. Mann, editors, Natural LanguageGeneration i Artificial Intelligence and237Computational Linguistics Volume 23, Number 2Computational Linguistics.
KluwerAcademic Publishers, pages 103-124.McDonald, D. 1981.
Language production:The source of the dictionary.
InProceedings ofthe 19th Annual Meeting.Association for ComputationalLinguistics.McDonald, D. 1983.
Description directedcontrol: Its implications for naturallanguage generation.
In KarenSparck-Jones, Barbara Grosz, andBonnie-Lynn Webber, editors, Readings inNatural Language Processing.
MorganKaufmann Publishers.McKeown, K. 1985.
Using Discourse Strategiesand Focus Constraints to Generate NaturalLanguage Text.
Studies in NaturalLanguage Processing.
CambridgeUniversity Press, Cambridge.McKeown, K. and M. Elhadad.
1990.
Acontrastive evaluation of functionalunification grammar for surface languagegenerators: A case study in choice ofconnectives.
In Natural LanguageGeneration i  Artificial Intelligence andComputational Linguistics.
KluwerAcademic Publishers, Boston.
(Also,Columbia Technical Report CUCS-407-88).McKeown, K. R., M. Elhadad, Y. Fukumoto,J.
G. Lim, C. Lombardi, J. Robin, andE A. Smadja.
1990.
Text generation icomet.
In R. Dale, C. S. Mellish, andM.
Zock, editors, Current Research inNatural Language Generation.
AcademicPress, pages 103-140.McKeown, K. R., S. Feiner, J. Robin,Seligmann D. D., and M. Tanenblatt.
1992.Generating cross-references formultimedia explanation.
In Proceedings ofthe lOth Annual Conference on ArtificialIntelligence, pages 9-16.
AAAI.McKeown, K., K. Kukich, and J. Shaw.
1994.Practical issues in automaticdocumentation generation.
In Proceedingsof the ACL Applied Natural LanguageConference, Stuttgart, Germany, October.McKeown, K. and D. Radev.
1995.Generating summaries of multiple newsarticles.
In Proceedings ofSIGIR, Seattle,WA, July.McKeown, K., J. Robin, and K. Kukich.1995.
Generating concise natural languagesummaries.
Information Processing andManagement.
31(5):703-733, September.Special Issue on Summarization.McKeown, K., J. Robin, and M. Tanenblatt.1993.
Tailoring lexical choice to the user'svocabulary in multimedia explanationgeneration.
In Proceedings ofthe 31stAnnual Meeting.
Association forComputational Linguistics.Mel'~uk, I.
A. and N. V. Pertsov.
1987.Surface-syntax of English, A Formal Model inthe Meaning-Text Theory.
Benjamins,Amsterdam and Philadelphia.Meteer, M. W. 1990.
The Generation Gap: TheProblem of Expressibility in Text Planning.Ph.D.
thesis, University of Massachusettsat Amherst.
Also available as BBNTechnical Report No.
7347.Meteer, M. W., D. D. McDonald,S.
D. Anderson, D. Forster, L. S. Gay,A.
K. Huettner, and P. Sibun.
1987.Mumble-86: Design and implementation.Technical Report COINS 87-87, Universityof Massachusetts at Amherst, Amherst,MA.Nogier, J. F. 1990.
Un systeme de production delanguage fonde sur le modele des graphesconceptuels.
Ph.D. thesis, Universit~ deParis VII.Paris, C. L. 1987.
The use of explicit user modelsin text generation: Tailoring to a user's level ofexpertise.
Ph.D. thesis, ColumbiaUniversity.
Also available as TechnicalReport CUCS-309-87.Passoneau, R., K. Kukich, J. Robin,V.
Hatzivassiloglou, L. Lefkowitz, andH.
Jing.
1996.
Generating summaries ofworkflow diagrams.
In Proceedings oftheInternational Conference on Natural LanguageProcessing and Industrial Applications(NLP-IA'96), Moncton, New Brunswick,Canada.The Penman NLG.
1989.
The Penman userguide.
Technical report, InformationScience Institute, Marina Del Rey, CA.Polgu~re, A.
1990.
Structuration et mise en jeuproc~durale d'un modele linguistique ddclaratifdans un cadre de gdndration de texte.
Ph.D.thesis, Universit~ de Montreal, Quebec,Canada.Pollard, C. and I.
A.
Sag.
1994.
Head DrivenPhrase Structure Grammar.
University ofChicago Press, Chicago.Pustejovsky, J. and B. Boguraev.
1993.Lexical knowledge representation a dnatural language processing.
ArtificialIntelligence, 63(1-2):193-223.Quirk, R., S. Greenbaum, G. Leech, andJ.
Svartvik.
1985.
A ComprehensiveGrammar of the English Language.
Longman.Reiter, E. B.
1991.
A new model for lexicalchoice for open-class words.Computational Intelligence, December.Reiter, E. B.
1994.
Has a consensus naturallanguage generation architectureappeared and is it psycholinguisticallyplausible?
In Proceedings ofthe 7thInternational Workshop on Natural LanguageGeneration, pages 163-170, June.238Elhadad, McKeown, and Robin Floating Constraints in Lexical ChoiceRobin, J.
1990.
Lexical choice in naturallanguage generation.
Technical ReportCUCS-040-90, Columbia University.Robin, J.
1994a.
Automatic generation andrevision of natural language summariesproviding historical background.
InProceedings ofthe 11th Brazilian Symposiumon Artificial Intelligence, Fortaleza, CE,October.Robin, J.
1994b.
Revision-based generationof natural anguage summaries providinghistorical background: Corpus-basedanalysis, design, implementation a devaluation.
Technical ReportCU-CS-034-94, Computer ScienceDepartment, Columbia University, NewYork.
Ph.D. Thesis.Robin, J. and K. McKeown.
1993.
Corpusanalysis for revision-based generation ofcomplex sentences.
In Proceedings ofthe11th National Conference on ArtificialIntelligence, pages 365-372.
AAAI.Robin, J. and K. McKeown.
1996.Empirically designing and evaluating anew revision-based model for summarygeneration.
Artificial Intelligence, 85,August.
Special Issue on EmpiricalMethods.Rubinoff, R. 1992.
Negotiation, Feedback andPerspective within Natural LanguageGeneration.
Ph.D. thesis, Computer ScienceDepartment, University of Pennsylvania.Shieber, S. 1992.
Constraint-based GrammarFormalism: Parsing and Type Inference forNatural and Computer Languages.
MITPress, Cambridge, MA.Shieber, S. M. and Y. Shabes.
1991.Generation and synchronoustree-adjoining grammars.
ComputationalIntelligence, 7(4):220-228, December.Shieber, S. M., G. Van Noord, R. M. Moore,and Pereira E C. P. 1990.
Semantichead-driven generation.
ComputationalLinguistics, 16(1):30--42.Smadja, E 1991.
Retrieving CollocationalKnowledge from Textual Corpora.
AnApplication: Language Generation.
Ph.D.thesis, Computer Science Department,Columbia University, New York.Smadja, E A. and K. McKeown.
1991.
Usingcollocations for language generation.Computational Intelligence, 7(4):229-239,December.Strzalkowski, T., editor.
1994.
ReversibleGrammars in Natural Language Processing.Kluwer Academic Publishers, Boston.Swartout, W. 1983.
The gist behaviorexplainer.
In Proceedings ofthe NationalConference on Artificial Intelligence, pages402-407, Washington D.C. AAAI.Talmy, L. 1976.
Semantic ausative types.
InM.
Shibatani, editor, The grammar ofcausative constructions.
Syntax andsemantics, volume 6.
Academic Press,London.Talmy, L. 1983.
How language structuresspace.
In H. L. Pick and L. P. Acredolo,editors, Spatial Orientation: Theory, Researchand Application.
Plenum Press, New Yorkand London.Talmy, L. 1985.
Lexicalization patterns:Semantic structure in lexical form.
InT.
Shopen, editor, Grammatical Categoriesand the Lexicon, Language typology andsyntactic description, volume 3.Cambridge University Press, Cambridge.Van Noord, G. 1990.
An overview ofhead-driven bottom-up generation.
InR.
Dale, C. Mellish, and M. Zock, editors,Current Research in Natural LanguageGeneration.
Academic Press, pages141-166.Winograd, T. 1983.
Language as a CognitiveProcess.
Addison-Wesley.Yang, G., K. McCoy, and K. Vijay-Shanker.1991.
From functional specification tosyntactic structure: Systemic grammarand tree adjoining rammars.Computational Intelligence, 7(4), December.Zock, M. 1988.
Natural anguages areflexible tools, that's what makes themhard to explain, to learn and to use.
InM.
Zock and G. Sabah, editors, Advancesin Natural Language Generation: AnInterdisciplinary Perspective.
Pinter andAblex.239
