Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 113?118,Prague, June 2007. c?2007 Association for Computational LinguisticsSVO triple based Latent Semantic Analysis for recognising textualentailmentGaston Burek Christian PietschCentre for Research in ComputingThe Open UniversityWalton Hall, Milton Keynes, MK7 6AA, UK{g.g.burek,c.pietsch,a.deroeck}@open.ac.ukAnne De RoeckAbstractLatent Semantic Analysis has only recentlybeen applied to textual entailment recogni-tion.
However, these efforts have sufferedfrom inadequate bag of words vector repre-sentations.
Our prototype implementationfor the Third Recognising Textual Entail-ment Challenge (RTE-3) improves the ap-proach by applying it to vector represen-tations that contain semi-structured repre-sentations of words.
It uses variable sizen-grams of word stems to model indepen-dently verbs, subjects and objects displayedin textual statements.
The system perfor-mance shows positive results and providesinsights about how to improve them further.1 IntroductionThe Third Recognising Textual Entailment Chal-lenge (RTE-3) task consists in developing a systemfor automatically determining whether or not a hy-pothesis (H) can be inferred from a text (T), whichcould be up to a paragraph long.Our entry to the RTE-3 challenge is a systemthat takes advantage of Latent Semantic Analysis(LSA) (Landauer and Dumais, 1997).
This numer-ical method for reducing noise generated by wordchoices within texts is extensively used for docu-ment indexing and word sense disambiguation.
Re-cently, there have also been efforts to use techniquesfrom LSA to recognise textual entailment (Clarke,2006; de Marneffe et al, 2006).
However, we arguethat these efforts (like most LSA approaches in thepast) suffer from an inadequate vector representationfor textual contexts as bags of words.
In contrast,we have applied LSA to vector representations ofsemi-structured text.
Our representation takes intoaccount the grammatical role (i.e.
subject, verb orobject) a word occurs in.Within this system report, we describe and dis-cuss our methodology in section 2, our current im-plementation in section 3, and system results in sec-tion 4.
We conclude in section 5 with a discussionof the results obtained and with the presentation ofpossible steps to improve our system?s performance.2 Methodology for detecting TextualEntailment2.1 Textual entailment formalisationOur approach addresses the problem of the semanticgap that exists between low level linguistic entities(words) and concepts.
Concepts can be describedby means of predicate-argument structures or by aset of alternative natural language realisations.
Inthis work we use terminology co-occurrence infor-mation to identify when different spans of text havecommon semantic content even if they do not sharevocabulary.
To achieve this, we use variable size n-grams to independently model subject, verb and ob-ject, and capture semantics derived from grammati-cal structure.
In order to detect textual entailment wemeasure the semantic similarity between n-grams ineach T?H pair.2.2 Using n-grams to align SVOsTo align subjects, verbs and objects within H and T,we build the set of all n-grams for T, and do the same113for H. Section 3.5 describes this process in more de-tail.2.3 Deriving word senses with Latent SemanticAnalysisOur approach is based on the assumption that aword sense can be derived from the textual contextsin which that words occurs.
This assumption wasformalised in the Distributional Hypothesis (Harris,1954).We implemented a vector space model (Salton etal., 1975) to capture word semantics from linguis-tic (i.e.
grammatical role) and contextual (i.e.
fre-quency) information about each word.
To avoid highmatrix sparsity our vector space model uses secondorder co-occurrence (Widdows, 2004, p. 174).We assumed that the corpus we generated the vec-tor space model from has a probabilistic word distri-bution that is characterised by a number of seman-tic dimensions.
The LSA literature seems to agreethat optimal number of dimensions is somewherebetween two hundred and one thousand dependingon corpus and domain.
As specified by LSA we ap-plied Singular Value Decomposition (SVD) (Berry,1992) to identify the characteristic semantic dimen-sions.The resulting model is a lower dimensional pro-jection of the original model that captures indi-rect associations between the vectors in the originalmodel.
SVD reduces the noise in word categori-sations by producing the best approximation to theoriginal vector space model.3 Implementation3.1 Development data setThe development data set consists of eight hundredT?H pairs, half of them positive.
By positive pairwe mean a T?H pair in which T entails H. All otherpairs we call negative.
Each T?H pair belongs to aparticular sub-task.
Those sub-tasks are InformationExtraction (IE), Information Retrieval (IR), Ques-tion Answering (QA) and Summarisation (SUM).
Inthe current prototype, we ignored annotations aboutsub-tasks.3.2 Corpus analysis3.2.1 Corpora usedThe only knowledge source we used in our imple-mentation was a parsed newswire corpus (ReutersNews Corpus) (Lewis et al, 2004).
To derive con-textual information about the meaning of words con-stituting the SVOs, we analysed the Reuters corpusas explained below.3.2.2 SVO triple extractionFor parsing the corpus, we used Minipar1 becauseof its speed and because its simple dependency tripleoutput format (-t option) contains word stems andthe grammatical relations between them.
A simpleAWK script was used to convert the parse results intoProlog facts, one file for each sentence.
A straight-forward Prolog program then identified SVOs ineach of these fact files, appending them to one bigstructured text file.Our algorithm currently recognises intransitive,transitive, ditransitive, and predicative clauses.
In-transitive clauses are encoded as SVOs with anempty object slot.
Transitive clauses result in a fullyinstantiated SVOs.
Ditransitive clauses are encodedas two different SVOs: the first containing subject,verb and direct object; the second triple containingthe subject again, an empty verb slot, and the indi-rect object.
Predicatives (e.g.
?somebody is some-thing?)
are encoded just like transitive clauses.In this first prototype, we only used one word(which could be a multi-word expression) for sub-ject, verb and object slot respectively.
We realisethat this approach ignores much information, butgiven a large corpus, it might not be detrimental tobe selective.3.2.3 SVO Stemming and labelingTo reduce the dimensionality of our vector spacemodel we stem the SVOs using Snowball2.
Then,we calculate how many times stems co-occur as sub-ject, verb or object with another stem within thesame SVO instance.1Minipar can be downloaded from http://www.cs.ualberta.ca/?lindek/minipar.htm.
It is based on Principar, which is de-scribed in Lin (1994).2Snowball is freely available from http://snowball.tartarus.org/.
The English version is based on the original Porter Stem-mer (Porter, 1980).114To keep track of the grammatical role (i.e.
subject,verb, object) of the words we stem them and labelthe stems with the corresponding role.3.3 Building vector spaces to represent stemsemanticsFrom the corpus, we built a model (S,V,O) of theEnglish (news) language consisting of three matri-ces: S for subjects, V for verbs, and O for objects.We built the three stem-to-stem matrices fromlabeled stem co-occurrences within the extractedtriples.
The entries to the matrices are the frequen-cies of the co-occurrence of each labeled stem withitself or with another labeled stem.
In our currentprototype, due to technical restrictions explained inSection 3.4, each matrix has 1000 rows and 5000columns.Columns of matrix S contain entries for stems la-beled as subject, columns of matrix V contain en-tries for stems labeled as verb, and columns of ma-trix O contain entries for stems labeled as object.The frequency entries of each matrix correspond tothe set of identically labeled stems with the highestfrequency.Rows of the three matrices contain entries corre-sponding to the same set of labeled stems.
Those la-beled stems are the ones with the highest frequencyin the set of all labeled stems.
Of these, 347 stemsare labeled as subject, 356 are labeled as verb, and297 are labeled as object.
Each row entry is the fre-quency of co-occurrence of two labeled stems withinthe same triple.Finally, each column entry is divided by the num-ber of times the labeled stem associated with thatcolumn occurs within all triples.3.4 Calculating the singular valuedecompositionWe calculated the Singular Value Decompositions(SVDs) for S, V and O.
Each SVD of a matrix A isdefined as a product of three matrices:A = U ?
S ?
V ?
(1)SVD is a standard matrix operation which is sup-ported by many programming libraries and com-puter algebra applications.
The problem is that onlyvery few can handle the large matrices required forreal-world LSA.
It is easy to see that the memory re-quired for representing a full matrix of 64 bit float-ing point values can easily exceed what current hard-ware offers.
Fortunately, our matrices are sparse, soa library with sparse matrix support should be ableto cope.
Unfortunately, these are hard to find out-side the Fortran world.
We failed to find any Java li-brary that can perform SVD on sparse matrices.3 Wefinally decided to use SVDLIBC, a modernised ver-sion of SVDPACKC using only the LAS2 algorithm.In pre-tests with a matrix derived from a differenttext corpus (18371 rows ?
3469 columns, density0.73%), it completed the SVD task within ten min-utes on typical current hardware.
However, whenwe try to use it for this task on a matrix S of dimen-sion 5000 ?
5000 (density 1.4%), SVDLIBC did notterminate4.
In theory, there is a Singular Value De-composition for every given matrix, so we assumethis is an implementation flaw in either SVDLIBC orGCC.
With no time left to try Fortran alternatives,we resorted to reducing the size of our three matri-ces to 1000 ?
5000, thus losing much informationin our language model.3.5 Looking for evidence of H in T usingvariable size n-grams3.5.1 Building variable size n-gramsOurMinipar triple extraction algorithm is not ableto handle SVOs that are embedded within otherSVOs (as e.g.
in ?Our children play a new game thatinvolves three teams competing for a ball.?).
There-fore, in order to determine if SVOs displayed in Hare semantically similar to any of those displayed inT, we generate all n-grams of all lengths for each Tand H: one set for subjects, one for verbs and anotherone for objects.Example: ?The boy played tennis.
?Derived n-grams: the; the boy; the boy played; theboy played tennis; boy; boy played; boy played3The popular JAMA library and the related Jampack libraryhave no sparse matrix support at all.
MTJ and Colt do supportsparse matrices but cannot perform SVD on them without firstconverting them to full matrices.4We tried various hardware / operating system / compilercombinations.
On Linux systems, SVDLIBC would abort afterabout 15 minutes with an error message ?imtqlb failed to con-verge?.
On Solaris and Mac OS X systems, the process wouldnot terminate within several days.115tennis; played; played tennis; tennis.We use n-grams to model subjects, verbs and ob-jects of SVOs within T and H.3.5.2 How to compare n-gramsWe generate three vector representations for eachn-gram.
To do this, we add up columns from theReuters Corpus derived matrices.
To build the firstvector representation, we use the S matrix, to buildthe second vector we use the V matrix, and to buildthe third vector we use the O matrix.
Each ofthe three representations is the result of adding thecolumns corresponding to each stem within the n-gram.To calculate the semantic similarity between n-grams, we fold the three vector representations ofeach n-gram into one of the dimensionally reducedmatrices S200, V200 or O200.
Vector representationoriginating from the S matrix are folded into S200.We proceed analogously for vector representationsoriginating from V200 and O200.
We apply equation2 to fold vectors from Gr where r ?
{S,V,O}.
Gis a matrix which consists of all the vector represen-tations of all the n-grams modeling T or H. Sr200 andU r200 are SVD results reduced to 200 dimensions.Gr?
?
U r200 ?
(Sr200)?1 = Gr200 (2)For each T?H pair we calculate the dot productbetween the G matrices for H and T as expressed inequation 3textGr200 ?hypothesis Gr?200 = Or (3)The resulting matrix Or contains the dot productsimilarity between all pairs of n-grams within thesame set.
Finally, for each T?H pair we obtain threesimilarity values s, v, o by selecting the entry of Orwith the highest value.3.5.3 ScoringNow we have calculated almost everything weneed to venture a guess about textual entailment.For each T?H pair, we have three scores s, v, ofor for subject, verb and object slot respectively.
Theremaining task is to combine them in a meaningfulway in order to make a decision.
This requires someamount of training which in the current prototypeis as simple as computing six average values: s?p,s?
v?
o?positive 0.244 5.05 ?
10?7 0.323negative 0.196 4.76 ?
10?7 0.277Table 1: Values computed for s?p, v?p, o?p, s?n, v?n, o?n0 0.10.2 0.30.4 0.50.6 0.70.8 0.90  100  200  300  400  500  600  700  800dotproductsimilarityn-gramsFigure 1: Subject similarities s = maxOS for allH?T pairsv?p, o?p are the average scores of subject, verb andobject slots over those T?H pairs for which textualentailment is known to hold.
Conversely, s?n, v?n, o?nare the averages for those pairs that do not stand ina textual entailment relation.
The (rounded) valueswere determined are shown in table 1.Note that the average values for non-entailmentare always lower than the average values for entail-ment, which indicates that our system indeed tendsto discriminate correctly between these cases.The very low values for the verb similarities (fig-ure 3) compared to subject similarities (figure 1) andobject similarities (figure 2) remind us that beforewe can combine slot scores, they should be scaledto a comparable level.
This is achieved by divid-ing each slot score by its corresponding average.
Ig-noring the difference between positive and negativepairs for a moment, the basic idea of our scoring al-gorithm is to use the following threshold:ss?+vv?+oo?= 3 (4)0 0.10.2 0.30.4 0.50.6 0.70.80  100  200  300  400  500  600  700  800dotproductsimilarityn-gramsFigure 2: Object similarities o = maxOO for allH?T pairs1160 2e-074e-07 6e-078e-07 1e-061.2e-06 1.4e-061.6e-060  100  200  300  400  500  600  700  800dotproductsimilarityn-gramsFigure 3: Verb similarities v = maxOV for all H?TpairsAt this point we observed that scaling the verbsimilarities so much seemed to make results worse.It seems to be necessary to introduce weights:?ss?+ ?vv?+ ?oo?= ?
+ ?+ ?
(5)Without loss of generality, we may assume ?
= 1:?ss?+vv?+ ?oo?= ?
+ 1 + ?
(6)The complete scoring formula with both positiveand negative scores is shown below.
We assumedthat the weights ?
and ?
are the same in the positiveand in the negative case, so ?
= ?p = ?n and ?
=?p = ?n.
?ss?p+vv?p+?oo?p+?ss?n+vv?n+?oo?n= 2(?+1+?
)(7)At this point, some machine learning over the de-velopment data set should be performed in order todetermine optimal values for ?
and ?.
For lack oftime, we simply performed a dozen or so of test runsand finally set ?
= ?
= 3.Our entailment threshold is thus simplified:3ss?p+vv?p+ 3oo?p+ 3ss?n+vv?n+ 3oo?n= q (8)If q > 14, our prototype predicts textual entail-ment.
Otherwise, it predicts non-entailment.4 ResultsUsing the scoring function described in section3.5.3, our system achieved an overall accuracy of0.5638 on the development dataset.
Table 2 showsresults for the system run on the test dataset.
On thisunseen dataset, the overall accuracy decreased onlyall IE IR QA SUMaccuracy 0.5500 0.4950 0.5750 0.5550 0.5750av.
prec.
0.5514 0.4929 0.5108 0.5842 0.6104Table 2: Results on the test setslightly to 0.5500.
We take this as a strong indica-tion that the thresholds we derived from the develop-ment dataset work well on other comparable input.Results show that our system has performed signifi-cantly above the 0.5 baseline that would result froma random decision.As shown in section 3.5.3, the values in the threesimilarity plots (see figures 1, 2 and 3) obtained withthe development set seem to be scattered around themeans.
Therefore it seems that the threshold valuesused to the decide whether or not T entails H do notfully reflect the semantics underlying textual entail-ment.The nature of the SVD calculations do not allowus directly to observe the performance of the vari-able size n-grams in independently aligning subject,verb and objects from T and from H. Neverthelesswe can infer from figures 1, 2 and 3 that many ofthe values shown seem to be repeated.
These valueconfigurations can be observed in the three horizon-tal lines.
These lines better visible in figures 2 and3 are the effect of (a) many empty vectors resultingfrom the rather low number of stems represented bycolumns in our Reuters-derived matrices S, V andO, and (b) the effect of folding the n-gram vectorrepresentations into reduced matrices with two hun-dred dimensions.5 ConclusionEven though our system was developed from scratchin a very short period of time, it has already out-performed other LSA-based approaches to recognis-ing textual entailment (Clarke, 2006), showing thatit is both feasible and desirable to move away froma bag-of-words semantic representation to a semi-structured (here, SVO) semantic representation evenwhen using LSA techniques.Our system displays several shortcomings andlimitations owing to its immature implementationstate.
These will be addressed in future work, andwe are confident that without changing its theoret-ical basis, this will improve performance dramati-117cally.
Envisaged changes include:?
using larger matrices as input to SVD?
using the complete Reuters corpus, and addingWikinews texts?
performing corpus look-up for unknown words?
extracting larger chunks from S and O slots?
using advanced data analysis and machinelearning techniques to improve our scoringfunctionIn addition, our approach currently does not takeinto consideration the directionality of the entail-ment relationship between the two text fragments.
Incases where T1 entails T2 but T2 does not entail T1,our approach will treat (T1, T2) and (T2, T1) as thesame pair.
We expect to correct this misrepresenta-tion by evaluating the degree of specificity of wordscomposing the SVOs in asymmetric entailment rela-tionships where the first text fragment is more gen-eral than the second one.
For that purpose, one canuse term frequencies as an indicator of specificity(Spa?rck Jones, 1972).Obviously, system performance could be furtherimproved by taking a hybrid approach as e.g.
in deMarneffe et al (2006), but we find it more instruc-tive to take our pure LSA approach to its limits first.6 AcknowledgementsWe are grateful to Prof. Donia Scott, head of the Nat-ural Language Generation group within the Centrefor Research in Computing of the Open University,who made us aware of the RTE-3 Challenge and en-couraged us to participate.References[Berry1992] M. W. Berry.
1992.
Large-scale sparse sin-gular value computations.
The International Journalof Supercomputer Applications, 6(1):13?49, Spring.
[Clarke2006] Daoud Clarke.
2006.
Meaning as contextand subsequence analysis for entailment.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.
[de Marneffe et al2006] Marie-Catherine de Marneffe,Bill MacCartney, Trond Grenager, Daniel Cer, AnnaRafferty, and Christopher D. Manning.
2006.
Learn-ing to distinguish valid textual entailments.
In Pro-ceedings of the Second PASCAL Challenges Workshopon Recognising Textual Entailment, Venice, Italy.
[Harris1954] Zelig S. Harris.
1954.
Distributional struc-ture.
WORD, 10:146?162.
Reprinted in J. Fodor and J.Katz, The structure of language: Readings in the phi-losophy of language, pp.
33?49, Prentice-Hall, 1964.
[Landauer and Dumais1997] T. K. Landauer and S. T. Du-mais.
1997.
A solution to Plato?s Problem.
The LatentSemantic Analysis theory of the acquisition, inductionand representation of knowledge.
Psychological Re-view, 104(2):211?240.
[Lewis et al2004] D. D. Lewis, Y. Yang, T. Rose, andF.
Li.
2004.
RCV1: A new benchmark collectionfor text categorization research.
Journal of MachineLearning Research, 5:361?397.
[Lin1994] Dekang Lin.
1994.
PRINCIPAR ?
an effi-cient, broad-coverage, principle-based parser.
In Proc.COLING-94, pages 42?488, Kyoto, Japan.
[Porter1980] M. F. Porter.
1980.
An algorithm for suffixstripping.
Program, 14(3):130?137.
[Salton et al1975] G. Salton, A. Wong, and C. S. Yang.1975.
A vector space model for automatic indexing.Commun.
ACM, 18(11):613?620.
[Spa?rck Jones1972] Karen Spa?rck Jones.
1972.
A statis-tical interpretation of term specificity and its applica-tion in retrieval.
Journal of Documentation, 28(1):11?21.
Reprinted 2004 in 60(5):493?502 and in Spa?rckJones (1988).
[Spa?rck Jones1988] Karen Spa?rck Jones.
1988.
A statis-tical interpretation of term specificity and its applica-tion in retrieval.
Document retrieval systems, pages132?142.
[Widdows2004] DominicWiddows.
2004.
Geometry andMeaning.
Number 172 in CSLI Lecture Notes.
Uni-versity of Chicago Press.118
