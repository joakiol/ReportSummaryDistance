Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 415?425,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUnsupervised Relation Extraction with General Domain KnowledgeOier Lopez de Lacalle1,2 and Mirella Lapata11Institute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB2IKERBASQUE, Basque Foundation for Science, Bilbao, Spainoier.lopezdelacalle@ehu.es, mlap@inf.ed.ac.ukAbstractIn this paper we present an unsupervised ap-proach to relational information extraction.Our model partitions tuples representing anobserved syntactic relationship between twonamed entities (e.g., ?X was born in Y?and ?X is from Y?)
into clusters correspond-ing to underlying semantic relation types(e.g., BornIn, Located).
Our approach incor-porates general domain knowledge which weencode as First Order Logic rules and auto-matically combine with a topic model devel-oped specifically for the relation extractiontask.
Evaluation results on the ACE 2007English Relation Detection and Categoriza-tion (RDC) task show that our model outper-forms competitive unsupervised approachesby a wide margin and is able to produce clus-ters shaped by both the data and the rules.1 IntroductionInformation extraction (IE) is becoming increas-ingly useful as a form of shallow semantic analy-sis.
Learning relational facts from text is one of thecore tasks of IE and has applications in a varietyof fields including summarization, question answer-ing, and information retrieval.
Previous work (Sur-deanu and Ciaramita, 2007; Culotta and Sorensen,2004; Zhou et al 2007) has traditionally relied onextensive human involvement (e.g., hand-annotatedtraining instances, manual pattern extraction rules,hand-picked seeds).
Standard supervised techniquescan yield high performance when large amountsof hand-labeled data are available for a fixed in-ventory of relation types (e.g., Employment, Lo-cated), however, extraction systems do not easilygeneralize beyond their training domains and oftenmust be re-engineered for each application.
Un-supervised approaches offer a promising alternativewhich could lead to significant resource savings andmore portable extraction systems.It therefore comes as no surprise that latent topicanalysis methods have been used for a variety ofIE tasks.
Yao et al(2011), for example, proposea series of topic models which perform relationdiscovery by clustering tuples representing an ob-served syntactic relationship between two named en-tities (e.g., ?X was born in Y?
and ?X is from Y?
).The clusters correspond to semantic relations whosenumber or type is not known in advance.
Their mod-els depart from standard Latent Dirichlet Allocation(Blei et al 2003) in that a document consists of re-lation tuples rather than individual words; moreover,tuples have features each of which is generated in-dependently from a hidden relation (e.g., the wordscorresponding to the first and second entities, thetype and order of the named entities).
Since thesefeatures are local, they cannot capture more globalconstraints pertaining to the relation extraction task.Such constraints may take the form of restrictionson which tuples should be clustered together ornot.
For instance, different types of named entitiesmay be indicative of different relations (ORG-LOCentities often express a Location relation whereasPER-PER entities express Business or Family rela-tions) and thus tuples bearing these entities shouldnot be grouped together.
Another example are tupleswith identical or similar features which intuitivelyshould be clustered together.In this paper, we propose an unsupervised ap-proach to relation extraction which does not re-415quire any relation-specific training data and allowsto incorporate global constraints general express-ing domain knowledge.
We encode domain knowl-edge as First Order Logic (FOL) rules and automati-cally integrate them with a topic model to produceclusters shaped by the data and the constraints athand.
Specifically, we extend the Fold-all (First-Order Logic latent Dirichlet Allocation) framework(Andrzejewski et al 2011) to the relation extractiontask, explain how to incorporate meaningful con-straints, and develop a scalable inference technique.In the presence of multiple candidate relation de-compositions for a given corpus, domain knowledgecan steer the model towards relations which are bestaligned with user and task modeling goals.
We alsoargue that a general mechanism for encoding addi-tional modeling assumptions and side informationcan lessen the need for ?custom?
relation extractionmodel variants.
Experimental results on the ACE-2007 Relation Detection and Categorization (RDC)dataset show that our model outperforms competi-tive unsupervised approaches by a wide margin andis able to uncover meaningful relations with onlytwo general rule types.Our contributions in this work are three-fold: anew model that modifies the Fold-all framework andextends it to the relation extraction task; a new for-malization of the logic rules applicable to topic mod-els defined over a rich set of features; and a proposalfor mining the logic rules automatically from a cor-pus contrary to Andrzejewski et al(2011) who em-ploy manually crafted seeds.2 Related WorkA variety of learning paradigms have been appliedto relation extraction.
As mentioned earlier, super-vised methods have been shown to perform well inthis task.
The reliance on manual annotation, whichis expensive to produce and thus limited in quantity,has provided the impetus for semi-supervised andpurely unsupervised approaches.
Semi-supervisedmethods use a small number of seed instances orpatterns (per relation) to launch an iterative train-ing process (Riloff and Jones, 1999; Agichtein andGravano, 2000; Bunescu and Mooney, 2007; Pan-tel and Pennacchiotti, 2006).
The seeds are usedto extract a new set of patterns from a large cor-pus, which are then used to extract more instances,and so on.
Unsupervised relation extraction meth-ods are not limited to a predefined set of targetrelations, but discover all types of relations foundin the text.
The relations represent clusters overstrings of words (Banko et al 2007; Hasegawa etal., 2004), syntactic patterns between entities (Yaoet al 2011; Shinyama and Sekine, 2006), or logicalexpressions (Poon and Domingos, 2009).
Anotherlearning paradigm is distant supervision which doesnot require labeled data but instead access to a rela-tional database such as Freebase (Mintz et al 2009).The idea is to take entities that appear in some rela-tion in the database, find the sentences that expressthe relation in an unlabeled corpus, and use them totrain a relation classifier.Our own work adds an additional approach intothe mix.
We use a topic model to infer an arbi-trary number of relations between named entities.Although we do not have access to relation-specificinformation (either as a relational database or manu-ally annotated data), we impose task-specific con-straints which inject domain knowledge into thelearning algorithm.
We thus alleviate known prob-lems with the interpretability of the clusters obtainedfrom topic models and are able to guide our modeltowards reasonable relations.
Andrzejewski et al(2011) show how to integrate First-Order Logic withvanilla LDA.
We extend their formulation to relationtuples rather than individual words.
Our model gen-erates a corpus of entity tuples which are in turn rep-resented by features and uses automatically acquiredFOL rules.
The idea of integrating topic modelingwith FOL builds on research in probabilistic logicmodeling such as Markov Logic Networks (Richard-son and Domingos, 2006).
Schoenmackers et al(2010) learn Horn clauses from web-scale text withaim of finding answers to a user?s query.
Our workis complementary to theirs.
We could make use oftheir rules to discover more accurate relations.The general goal of assisting the learner in re-covering the ?correct?
clustering by supplying ad-ditional domain knowledge is not new.
Gondek andHofmann (2004) supply a known clustering they donot want the learner to return, whereas Wagstaffet al(2001) use pairwise labels for items indicat-ing whether they belong in the same cluster.
Thesemethods combine domain knowledge with statisticallearning in order to improve performance with re-416spect to the true target clustering.
Although, the tar-get labels are not available in our case, we are able toshow that the inclusion of domain knowledge yieldsclustering improvements.3 Learning SettingOur relation extraction task broadly adheres to theACE specification guidelines.
Our aim is to detectand characterize the semantic relations betweentwo named entities.
The input to our model is acorpus of documents, where each document is abag of relation tuples which can be obtained fromthe output of any dependency parser.
Each tuplerepresents a syntactic relationship between twonamed entity (NE) mentions, and consists of threecomponents: the dependency path between thetwo mentions, the source NE, and the target NE.
Adependency path is the concatenation of dependencyedges and nodes along a path in the dependencytree.
For example, the sentence ?George Bushtraveled to France on Thursday for a summit.
?would yield the tuple [SOURCE:George Bush(PER),PATH:?nsubj?traveled?prep?to?pobj?,DES:France(LOC)].
The tuple here expresses therelation Located, however our model does notobserve any relation labels during training.
Themodel assigns tuples to clusters, corresponding toan underlying relation type.
Each tuple instance canbe then labeled with an identifier corresponding tothe cluster (aka relation) it has been assigned to.4 Modeling FrameworkOur model builds on the work of Yao et al(2011)who develop a series of generative probabilisticmodels for relation extraction.
Specifically, we ex-tend their relational LDA model by interfacing itwith FOL-rules.
In the following, we first describetheir approach in more detail and then present ourextensions and modifications.4.1 Relational LDARelational LDA is an extension to LDA with a sim-ilar generative story.
LDA models each documentas a mixture of topics, which are in turn character-ized as distributions over words.
In relational LDA,each document is a mixture of relations over tuplesrepresenting syntactic relations between two namedentities.
The relation tuples are in turn generated aby set of features drawn independently from the un-derlying relation distribution.More technically, a multinomial distribution overrelations ?di is drawn from a Dirichlet prior(?
?
Dir(?))
at the document level.
Relation tuplesare generated from a multinomial distribution ?di(zi|?di ?
Mult(?di)) and are represented with k fea-tures.
Each feature is drawn (independently) froma multinomial distribution selected by the relationassigned to tuple i (fik|zi, ?zi ?
Mult(?zi)).
Rela-tions are drawn from a Dirichlet prior (?
?
Dir(?
)).In other words, each tuple in a document is assigneda hidden relation (z = z1...zN ); each relation isrepresented by a multinomial distribution over fea-tures ?r (Dirichlet prior ?).
?r is a vector with Fdimensions each corresponding to a feature.
Fi-nally, documents (j = 1...D) are associated with amultinomial distribution ?j over relations (Dirichletprior ?).
?j is a vector with R dimensions, one foreach relation.Figure 1 represents relational LDA model as a anundirected graphical model or factor graph (Bishop,2006), ignoring for the moment the factor whichconnects the d, z, f1...k and o variables.
Directedgraphical models can be converted into undirectedones by adding edges between co-parents (Kollerand Friedman, 2009).
Each clique in the graph de-fines a potential function which replaces the condi-tional probabilities in the directed graph.
Each max-imal clique is associated with a special factor node(the black squares) and clique members are con-nected to that factor.
The probability of any specificconfiguration is calculated by multiplying the poten-tial functions and normalizing them.
We adopt thefactor graph representation as is it convenient for in-troducing logic rules into the model.
The joint prob-ability of the model given the priors and the docu-ments (P (p, z, ?, ?|?, ?,d)) is equivalent to:R?rp(?r|?
)D?jp(?j |?
)N?i?di(zi)?k?pi?zi(fk) (1)where ?di(zi) is the zi-th element in the vector ?diand ?zi(fk) is fk-th feature in the ?zi vector.
Vari-able pi is the i-th tuple containing k features.
Theparameters of the latent variables (e.g., ?, ?)
aretypically estimated using an approximate inferencealgorithm such as Gibbs Sampling (Griffiths andSteyvers, 2004).417Figure 1: Relational LDA as a factor graph.
Filledcircles represent observed variables, empty circles areassociated with latent variables or model hyperparame-ters, and plates indicate repeating structures.
The blacksquares are the factor nodes and are associated with thepotential functions corresponding to conditional indepen-dence among the variables.
The model observes D doc-uments (d) consisting of N tuples (p), each representedby a set of features f1,f2 .
.
.
fk.
z represents the relationtype assignment to a tuple, ?
is the relation type propor-tion for a given document, and ?
the relation type dis-tribution over the features.
The logic factor (indicatedwith the arrow) connects the KB with the relational LDAmodel.
Variable o is an observed variable which containsthe side information expressed in FOL.As shown in Figure 1, the observed variables arerepresented by filled circles.
In our case, our modelsees the corpus (p, d), where d is the variable rep-resenting the document and the tuples (p) are repre-sented by a set of features f1,f2 .
.
.
fk in the graph.Empty circles are associated with latent variables tobe estimated: z represents the relation type assign-ment to the tuple, ?
is the relation type proportionfor the given document, and ?
is the relation typedistribution over the features.The features representing the tuples tap onto se-mantic information expressed by different surfaceforms and are an important part of the model.
Weuse a subset of the features proposed in Yao et al(2011) which we briefly describe below:SOURCE This feature corresponds to the first en-tity mention of the tuple.
In the sentence GeorgeBush traveled to France on Thursday for a summit.,the value of this feature would be George Bush .Value Predicate Descriptionzi = r Z(i, r) Latent relation typefk = v F(k, v) feature of relation tuplepi = i P(i, fk) tuple i contains feature fkdi = j D(i, j) observed documentTable 1: Logical variables for Relational LDA.
The vari-able i ranges over tuples in the corpus (i = [1 .
.
.
N ]),and k over features in the corpus (k = [1 .
.
.
F ]).DEST The feature corresponds to the second entitymention and its value would be France in the previ-ous example.NEPAIR The feature indicates the type and orderof two entity mentions in the tuple.
This wouldbe PER-ORG in our example.PATH This feature refers to the dependencypath between two entity mentions.
In oursentence, the value of the feature would bePATH:?nsubj?traveled?prep?to?pobj?.TRIGGER Finally, trigger features are contentwords occurring in the dependency path.
The pathPATH:?nsubj?traveled?prep?to?pobj?
con-tains only one trigger word, namely traveled.
Theintuition behind this feature is that paths sharing thesame set of trigger words should be grouped in thesame cluster.4.2 First Order Logic and Relational LDAWe next couple relational LDA with global con-straints, which we express using FOL rules.
Webegin by representing relational LDA as a MarkovLogic Network (Richardson and Domingos, 2006).We define a logical predicate for each model vari-able.
For example, assigned relation variable(Z(i, r)) is true if zi = r and false otherwise.
Table 1shows the mapping of model variables onto logicalpredicates.
Logical rules are encoded in the form ofa weighted FOL knowledge base (KB) which is thenconverted into Conjunctive Normal form:KB = {(?1, ?1), ..., (?L, ?L)} (2)The KB consists of L pairs, where each ?l rep-resents a FOL rule and ?l ?
0 its weight.
Rulesare soft preferences rather than hard constraints;the weights represent the importance of ?l and are418set manually by the domain expert.
The KB istied to the probabilistic model via its groundingsin the corpus.
For each FOL rule ?l, let G(?l) bethe set of groundings, each mapping the free vari-ables in ?l to a specific value.
For example, in therule ?i, j, p : F(i, Obama) ?
F(j,WhiteHouse) ?P(p, i) ?
P(p, j) ?
Z(p, r)1, G consists of all therules where the free variables i, j and p are instanti-ated.
At grounding time, we parse the corpus search-ing for the tuples that satisfy the logic rules and storethe indices of the tuples that ground the rule.
Thestored indices are used to set ?l to a specific value.For the (Obama, White House) example above, Gconsists of F propositional rules for each observedfeature, where i ?
[1 .
.
.
F ].
For each grounding(g ?
G(?l)) we define an indicator function:1g(z,p,d,o) =????
?1, if g is true underz and p,d,o0, otherwise(3)where z are relation assignments to tuples, p is theset of features in tuples, d are documents, and othe side information encoded in FOL.
Contrary toAndrzejewski et al(2011), we need to ground therules while taking into account if the feature speci-fied in the rule is expressed by any tuple or the spe-cific given tuple, since we are assigning relations totuples, and not directly to words.Next, we define a Markov Random Field (MRF)which combines relational LDA with the FOLknowledge base.
The MRF is defined over latentrelation tuple assignments z, relation feature multi-nomials ?, and relation document multinomials ?
(the feature set, document, and external informa-tion o are observed).
Under this model the con-ditional probability P (z, ?, ?|?, ?,p,d,o,KB) isproportional to:exp??L?l?g?G(?l)?l1g(z,p,d,o)???R?rp(?r|?
)D?jp(?j |?
)N?i?di(zi)?k?pi?zi(fk)(4)The first term in Equation (4) corresponds to thelogic factor in Figure 1 that groups variables d, z,1This rule translates as ?every tuple containing Obama andWhite House as features should be in relation cluster r?.f1, f2, .
.
.
fk and o.
The remaining terms in Equa-tion (4) refer to relational LDA.
The goal of themodel is to estimate the most likely ?
and ?
for thegiven observed state.
As z can not be marginalizedout, we proceed with MAP estimation of (z, ?, ?
),maximizing the log of the probability as in Andrze-jewski et al(2011):argmaxz,?,?L?l?g?G(?l)?l1g(z,p,d,o)+R?rlog p(?r|?
)+N?ilog ?di(zi)?k?pi?zi(fk)(5)Once the parameters of the model are estimated(see Section 4.3 for details), we use the ?
proba-bility distribution to assign a relation to a new testtuple.
We select the relation that maximizes theprobability argmaxr?ki P (fi|?r) where f1 .
.
.
fkare features representing the tuple and r the relationindex.4.3 InferenceExact inference is intractable for both relationalLDA and MLN models.
In order to infer the mostlikely multinomial parameters ?
and ?, we appliedthe Alternating Optimization with Mirror Descentalgorithm introduced in Andrzejewski et al(2011).The algorithm alternates between optimizing themultinomial parameters (?, ?
), whilst holding the re-lation assignments (z) fixed, and vice-versa.
At eachiteration, the algorithm first finds the optimal (?, ?
)for a fixed z as the MAP estimate of the Dirichletposterior:?r(f) ?
nrf + ?
?
1 (6)?j(r) ?
njr + ??
1 (7)where nrf is the number of times feature f isassigned to relation r in relation assignments z,and njr is the number of times relation r is assignedto document j.
Next, z is optimized while keeping ?and ?
fixed.
This step is divided into two parts.
Thealgorithm first deals with all zi which appear only intrivial groundings, i.e., groundings whose indicatorfunctions 1g are not affected by the latent relationassignment z.
As zi only appears in the last term of419Equation (5), the algorithm needs only optimize thefollowing term:zi = argmaxr=1...R?di(r)?k?pi?zi(fk) (8)The second part deals with the remaining zi that ap-pear in non-trivial groundings in the first term ofEquation (5).
We follow Andrzejewski et al(2011)in relaxing (5) into a continuous optimization prob-lem and refer the reader to their paper for a morein depth treatment.
Suffice it to say that once thebinary variables zir ?
{0, 1} are relaxed to contin-uous values zir ?
[0, 1], it is possible to introducethe relational LDA term in the equation and com-pute the gradient using the Entropic Mirror DescentAlgorithm (Beck and Teboulle, 2003):argmaxz?
[0,1]|KB|L?l?g?G(?l)?l1g(z)+?i,rzir log ?di(r)?k?pi?zi(fk)s.t zir ?
0 ,?i,rzir = 1(9)In every iteration the approximation algorithmrandomly samples a term from the objective func-tion (Equation (9)).
The sampled term can bea particular ground rule g or the relational LDAterm (?r zir log ?di(r)?k?pi?zi(fk)) for someuniformly sampled index i.
The sampling of theterms is weighted according to the rule weight (?l)and the grounded value (G(?l)) in the case of logicrules, and the size of corpus in tuples (|zKB|) for re-lational LDA.
Once we choose term f and take thegradient, we can apply the Entropic Mirror Descentupdate:zir ?zir exp(?Ozirf)?r?
zir?
exp(?Ozir?f)(10)Finally, zi is recovered by rounding to argmaxr zir.The main advantage of this approach is that it re-quires only a means to sample groundings g for eachrule?l, and can avoid fully grounding the FOL rules.4.4 Logic RulesOur model assigns relations to tuples rather than top-ics to words.
Since our tuples are described in termsof features our logic rules must reflect this too.
Forour experiments we defined two very general typesof rules described below.Must-link Tuple The motivation behind this ruleis that tuples which share features probably expressthe same underlying relation.
The rule must spec-ify which feature has to be shared for the tuplesto be clustered together.
For example, the rule be-low states that tuples containing the dependencypath PATH:?appos?president?prep?of?pobj?should go in the same cluster:?i, j, k : F(i, PATH:is the president of ) ?
P(j, fi)?P(k, fi)?
?Z(j, t) ?
Z(k, r)Cannot-link Tuple We also define rules prohibit-ing tuples to be clustered together because they donot share any features.
For example, tuples withORG-LOC entities, probably express a Location re-lation and should not be clustered together withPER-PER tuples, which in all likelihood express adifferent relationship (e.g., Family).
The rule belowexpresses this constraint:?i, j, k, l : F(i, NEPAIR:PER-PER)?F(j, NEPAIR:ORG-LOC)?P(k, fi) ?
P(l, fj)?
?Z(k, r) ?
?Z(l, r)The specification of the first order logic rules isan integral part of the model.
The rules expressknowledge about the task at hand, the domain in-volved, and the way the relation extraction problemis modeled (i.e., tuples expressed as features).
Sofar, we have abstractly formulated the rules withoutexplaining how they are specifically instantiated inour model.
We could write them down by hand afterinspecting some data or through consultation with adomain expert.
Instead, we obtain logic rules au-tomatically from a corpus following the proceduredescribed in Section 5.5 Experimental SetupData We trained our model on the New YorkTimes (years 2000?2007) corpus created by Yao etal.
(2011).
The corpus contains approximately 2Mentity tuples.
The latter were extracted from428K documents.
After post-processing (tokeniza-tion, sentence-splitting, and part-of-speech tagging),420Must-link TupleF(i, NEPAIR:PER-PER, TRIGGER:wife) ?
P(j, fi) ?
P(k, fi)?
?Z(j, t) ?
Z(k, r)F(i, NEPAIR:PER-LOC, TRIGGER:die) ?
P(j, fi) ?
P(k, fi)?
?Z(j, t) ?
Z(k, r)F(i, PATH:?nsubj?die?prep?in?pobj?)
?
P(j, fi) ?
P(k, fi)?
?Z(j, t) ?
Z(k, r)F(i, SOURCE:Kobe, DEST:Lakers) ?
P(j, fi) ?
P(k, fi)?
?Z(j, t) ?
Z(k, r)Cannot-link TupleF(i, NEPAIR:ORG-LOC) ?
F(j, NEPAIR:PER-PER) ?
P(k, fi) ?
P(l, fj)?
?Z(k, r) ?
?Z(l, r)F(i, NEPAIR:LOC-LOC) ?
F(j, TRIGGER:president) ?
P(k, fi) ?
P(l, fj)?
?Z(k, r) ?
?Z(l, r)F(i, NEPAIR:PER-LOC) ?
F(j, TRIGER:member) ?
P(k, fi) ?
P(l, fj)?
?Z(k, r) ?
?Z(l, r)F(i, NEPAIR:PER-PER) ?
F(j, TRIGER:sell) ?
P(k, fi) ?
P(l, fj)?
?Z(k, r) ?
?Z(l, r)Table 2: Examples of automatically extracted Must-link and Cannot-link tuple rules.named entities were automatically recognized andlabeled with PER, ORG, LOC, and MISC (Finkelet al 2005).
Dependency paths for each pair ofnamed entity mentions were extracted from the out-put of the MaltParser (Nivre et al 2004).
In ourexperiments, we discarded tuples with paths longerthan 10 edges (Lin and Pantel, 2001).
We evalu-ated our model on the test partition of the ACE 2007(English) RDC dataset which is labeled with goldstandard entity mentions and their relations.
Thereare six general relation types and 18 subtypes.
Weused 25% of the ACE training partition as a devel-opment set for parameter tuning.Logic Rule Extraction We automatically ex-tracted logic rules from the New York Times(NYT) corpus as follows.
The intuition behindMust-link rules is that tuples with common featuresshould cluster together.
Although we do not knowwhich features would yield the best rules, wenaively assume that good features are frequentlyco-occurring features.
Using the log-likelihoodratio (Dunning, 1993), we first discarded lowconfidence feature co-occurrences (p < 0.05).
Twofeatures co-occur if they are both found withinthe same sentence.
We then sorted the remainingco-occurrences by their frequency and retained theN -best ones.
We only considered unigram andbigram features since higher-order ones tend tobe sparse.
An example of a bigram feature wouldbe (PATH:?nsubj?grow?prep?in?pobj?,DEST:Chicago).The main intuition behind Cannot-link rules isthat tuples without any common features shouldnot cluster together.
So, if two features neverco-occur, they probably express different relations.For every unigram and bigram feature in the re-spective N -best list, we find the features it doesnot co-occur with in the NYT corpus.
For ex-ample, NEPAIR:PER?LOC does not co-occur withDEST:Yankees and the bigram DEST:United Na-tions, NEPAIR:PER?ORG does not co-occur withSOURCE:Mr. Bush, NEPAIR:PER?LOC.
Cannot-link rules are then based on such non-co-occurringfeature pairs.We optimized N empirically on the developmentset.
We experimented with values ranging from 20to 500.
We obtained 20 Must-link rules for coarse-grained relations and 400 rules for their subtypes.We extracted 1,814 Cannot-link rules for general re-lations (N = 50) and 34,522 rules for subtypes(N = 400).
The number of features involved in theMust-link rules was 25 for coarse-grained relationsand 422 for fine-grained relations.
For Cannot-linkrules, 62 features were involved in coarse-grainedrelations and 422 in fine-grained relations.Examples of the rules we extracted are shown inTable 2.
The first rule in the upper half of the ta-ble states that tuples must cluster together if theirsource and target entities are PER and contain thetrigger word wife in their dependency path.
The sec-ond rule is similar, the source entity here is PER,the target LOC and the trigger word is die.
Ac-cording to the third rule, tuples featuring the pathPATH:?nsubj?die?prep?in?pobj?
should bein the same cluster.
The fourth rule forces tupleswhose source entity is Kobe and target entity is Lak-ers to cluster together.
The second half of the tableillustrates Cannot-link tuple rules.
The first rule pre-vents tuples with ORG-LOC entities to cluster to-421gether with PER-PER tuples.
The second rule statesthat we cannot link LOC-LOC tuples with thosewhose trigger word is president, and so on.Parameter Tuning Our framework has severalparameters that must be adjusted for an optimal clus-tering solution.
These include the hyperparame-ters ?
and ?
as well as the number of clusters.
Inaddition, we have to assign a weight to each FOLrule grounding.
An exhaustive search on the hy-perparameters and rule weights is not possible.
Wetherefore followed a step-wise approximation proce-dure.
First, we find the best ?
and ?
values, whilstvarying the number of clusters.
Once we have thebest hyperparameters for each clustering, we set theweights for the FOL rules.
We varied the numberof relations from 5 to 50.
We experimented with ?values in the range of [0.05 ?
0.5] and ?
values inthe range of [0.05 ?
0.5].
These values were opti-mized separately for coarse- and fine-grained rela-tions.
Table 3 shows the optimal number of clustersfor different model variants and relation types.The FOL weights can also make a difference inthe final output; the bigger the weight the moretimes the rule will be sampled in the Mirror Descentalgorithm.
We experimented with two weightingschemes: (a) we gave a weight of 1 or 0.5 to eachrule grounding and (b) we scaled the weights so asto make their contribution comparable to relationalLDA.
We obtained best results on the developmentset with the former scheme.Baselines We compared our FOL relational LDAmodel against standard LDA (Blei et al 2003) andrelational LDA without the FOL component.
In thecase of standard LDA, we estimated topics (rela-tions) over words, and used the context of the en-tity mentions pairs as a bag of words feature to se-lect the most likely cluster at test time.
Parametersfor LDA and relational LDA were optimized follow-ing the same parameter tuning procedure describedabove.We also compared our model against the unsuper-vised method introduced in Hasegawa et al(2004).Their key idea is to cluster pairs of co-occurringnamed entities according to the similarity of theirsurrounding contexts.
Following their approach, wemeasured context similarity using the vector spacemodel and the cosine metric and grouped NE pairsinto clusters using a complete linkage hierarchicalclustering algorithm.
We adopted the same parame-ter values as detailed in their paper (e.g., cosine sim-ilarity threshold, length of context vectors).
At testtime, instances were assigned to the relation clustermost similar to them (according to the cosine mea-sure).Evaluation We evaluated the clusters obtained byour model and the comparison systems using the Fs-core measure introduced in the SemEval 2007 task(Agirre and Soroa, 2007); it is the harmonic meanof precision and recall defined as the number of cor-rect members of a cluster divided by the number ofitems in the cluster and the number of items in thegold-standard class, respectively.6 ResultsOur results are summarized in Table 3 which reportsFscore for (Hasegawa et al 2004), LDA, relationalLDA (RelLDA), and our model with the FOL com-ponent.
To assess the impact of the rules on theclustering, we conducted several rule ablation stud-ies.
We thus present results with a model that in-cludes both Must-link and Cannot-link tuple rules(CLT+MLT), and models that include either Must-link (MLT) or Cannot-link (CLT) rules but not both.We show the performance of these models with theentire feature set (see (ALL) in the table) and with asubset consisting solely of NE pair related features(see (NEPAIR) in the table).
We report results againstcoarse- and fine-grained relations (6 and 18 relationtypes in ACE, respectively).
The table shows theoptimal number of relation clusters (in parentheses)per model and relation type.We also wanted to examine the quality of the logicrules.
Recall that we learn these heuristically fromthe NYT corpus.
We thus trained an additional vari-ant of our model with rules extracted from the ACEtraining set (75%) which contains relation annota-tions.
The extraction procedure was similar to theunsupervised case, save that the relation types wereknown and thus informative features could be minedmore reliably.
For Must-link rules, we extracted un-igram and bigram feature frequencies for each re-lation type and applied TF-IDF weighting in orderto discover the most discriminative ones.
We cre-ated logic rules for the 10 best feature combinationsin each relation type.
Regarding Cannot-link rules,we enumerated the features (unigrams and bigrams)422Model Subtype TypeHASEGAWA 26.1 (12) 34.7 (12)LDA 23.4 (10) 29.0 (5)RelLDA 30.4 (40) 38.6 (5)U-MLT (ALL) 36.6 (10) 48.0 (5)U-CLT (ALL) 30.5 (5) 39.3 (5)U-CLT+MLT (ALL) 29.8 (5) 42.0 (5)U-MLT (NEPAIR) 36.5 (10) 47.2 (5)U-CLT (NEPAIR) 28.8 (50) 40.5 (5)U-CLT+MLT (NEPAIR) 30.9 (10) 41.5 (5)S-MLT (ALL) 37.0 (10) 47.0 (5)S-CLT (ALL) 31.4 (50) 40.9 (5)S-CLT+MLT (ALL) 32.3 (10) 42.5 (5)S-MLT (NEPAIR) 37.0 (10) 47.6 (10)S-CLT (NEPAIR) 31.4 (10) 40.1 (5)S-CLT+MLT (NEPAIR) 37.1 (10) 46.0 (5)Table 3: Model performance on the ACE 2007 test setusing Fscore.
Results are shown for six main relationtypes and their subtypes (18 in total).
(ALL) models con-tain rules extracted from the entire feature set.
For (NE-PAIR) models, rules were extracted from NEPAIR-relatedfeatures only.
Prefix U- denotes models that use unsu-pervised rules; prefix S- highlights models using super-vised rules.
The optimal number of relations per modelis shown in parentheses.that did not co-occur in any relation type and appliedTF-IDF weighting.
Again, we created rules for the10 most discriminative features.
We defined rulesover the entire feature set (466 Must-link and 26,074Cannot-link rules) and a subset containing only NEpairs.
In Table 3, prefixes S- and U- indicate modelvariants with supervised and unsupervised rules, re-spectively.Our results show that standard LDA is not suit-able for relation extraction.
The obtained clustersare not informative enough to induce semantic re-lations, whereas RelLDA yields substantially bet-ter Fscores.
This is not entirely surprising, giventhat RelLDA is a relation extraction specific model.Hasegawa et als (2004) model lies somewhere inthe middle between LDA and RelLDA.
The com-bination of RelLDA with automatically extractedFOL rules improves over RelLDA across the board(see the U- models in Table 3).
MLT rules deliverthe largest improvement for both coarse and fine-grained relation types.
In general, CLT models per-form worse as well as models using both types ofrules (MLT+CLT).
The inferior performance of therule combination may be due to the fact that MLTand CLT rules contain conflicting information andto a certain extent cancel each other out.
The useof many rules might also negatively impact infer-ence, i.e., discriminative rules are sampled less andcannot influence the model towards a better solu-tion.
Restricting the number of features and rulesto named entity pairs only incurs a negligible dropin performance.
This is good news for scaling pur-poses, since a small number of rules can greatlyspeed-up inference.
Interestingly, model variantswhich use supervised FOL rules (see the prefix S-in Table 3) perform on par with unsupervised mod-els.
Again, MLT rules perform best in the super-vised case, whereas CLT rules marginally improveover RelLDA.We assessed whether differences in performanceare statistically significant (p < 0.05) using boot-strap resampling (Noreen, 1989).
All models acrossall relation types are significantly better than LDAand Hasegawa et al(2004).
FOL-based models per-form significantly better than RelLDA, with the ex-ception of all CLT models and U-CLT+MLT (ALL).MLT models are significantly better than any otherrule-based model, except those that only use NE-PAIR features.
We also measured whether differ-ent models agree on their topic assignments usingCohen?s Kappa.2 RelLDA agrees least with MLTmodels and most with CLT models (i.e., ?
= 0.50for U-MLT (ALL) and ?
= 0.65 for U-CLT (ALL)).This suggests that the CLT rules do not affect theoutput of RelLDA as much as MLT ones.
Examplesof relation clusters discovered by the U-MLT (ALL)model are shown in Table 4.A last note on parameter selection.
Our experi-ments explored the parameter space extensively inorder to examine any interactions between the in-duced relations and the logic rules.
For most modelvariants inferring subtype relations, the preferrednumber of clusters is 10.
For coarse-grained rela-tions, the optimal number of clusters is five.
Over-all, we found that the quality of the output is highlycorrelated with the quality of the logic rules and thata few good rules are more important than the opti-mal number of clusters.
We consider these findingsrobust enough to apply across domains and datasets.2For all comparison models the number of relation clusterswas set to 10.423SOURCE PATH DESTRepublican president of SenateSenate director of YankeesHouse professor at RepublicanBush chairman of CongressDemocrat spokesman for HouseMr.
Bush executive of MetsDemocrats director at U. of CaliforniaRepublican analyst at United NationsEmploymentSOURCE PATH DESTYankees defeat World SeriesMets win OlympicUnited States beat World CupGiants play YankeesJets win Super BowlNets lose OlympicsKnicks sign MetsRangers victory over GiantsSportsTable 4: Clusters discovered by the U-MLT (ALL) modelindicating employment- and sports-type relations.
For thesake of readability, we do not display the syntactic depen-dencies between words in a path.7 ConclusionsIn this paper we presented a new model for unsu-pervised relation extraction which operates over tu-ples representing a syntactic relationship betweentwo named entities.
Our model clusters such tuplesinto underlying semantic relations (e.g., Located,Family) by incorporating general domain knowledgewhich we encode as First Order Logic rules.
Specif-ically, we combine a topic model developed for therelation extraction task with domain relevant rules,and present an algorithm for estimating the param-eters of this model.
Evaluation results on the ACE2007 (English) RDC task show that our model out-performs competitive unsupervised approaches by awide margin and is able to produce clusters shapedby both the data and the rules.In the future, we would like to explore additionaltypes of rules such as seed rules, which would as-sign tuples complying with the ?seed?
informationto distinct relations.
Aside from devising new ruletypes, an obvious next step would be to explore dif-ferent ways of extracting the rule set based on differ-ent criteria (e.g., the most general versus most spe-cific rules).
Also note that in the current frameworkrule weights are set manually by the domain expert.An appealing direction would be to learn these auto-matically e.g., via a procedure that optimizes someclustering objective.
Finally, it should be interestingto use some form of distant supervision (Mintz et al2009) either as a means of obtaining useful rules orto discard potentially noisy or uninformative rules.AcknowledgmentsWe gratefully acknowledge financial support fromthe Department of Education, Universities and Re-search of the Basque Government (BFI-2011-442).We also thank Limin Yao and Sebastian Riedel forsharing their corpus with us and the members of theProbabilistic Models reading group at the Universityof Edinburgh for helpful feedback.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the 5th ACM International Confer-ence on Digital Libraries, pages 85?94, San Antonio,Texas.Eneko Agirre and Aitor Soroa.
2007.
Semeval-2007task 02: Evaluating word sense induction and discrim-ination systems.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 7?12,Prague, Czech Republic.David Andrzejewski, Xiaojin Zhu, Mark Craven, and BenRecht.
2011.
A framework for incorporating generaldomain knowledge into latent Dirichlet alcation us-ing first-order logic.
In Proceedings of the 22nd In-ternational Joint Conference on Artificial Intelligence,pages 1171?1177, Barcelona, Spain.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In Proceedingsof the 20th International Joint Conference on ArtificialIntelligence, pages 2670?2676, Hyderabad, India.Amir Beck and Marc Teboulle.
2003.
Mirror de-scent and nonlinear projected subgradient methods forconvex optimization.
Operations Research Letters,31(3):167?175.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Razvan Bunescu and Raymond Mooney.
2007.
Learningto extract relations from the web using minimal su-pervision.
In Proceedings of the 45th Annual Meeting424of the Association of Computational Linguistics, pages576?583, Prague, Czech Republic.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Meeting of the Association for ComputationalLinguistics, Main Volume, pages 423?429, Barcelona,Spain.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, 19(1):61?74.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages363?370, Ann Arbor, Michigan.David Gondek and Thomas Hofmann.
2004.
Non-redundant data clustering.
In IEEE International Con-ference on Data Mining, pages 75?82.
IEEE ComputerSociety.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(1):5228?5235.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics, pages 415?422, Barcelona, Spain.D.
Koller and N. Friedman.
2009.
Probabilistic Graphi-cal Models: Principles and Techniques.
MIT Press.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
discoveryof inference rules from text.
In Proceedings of the 7thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 323?328, SanFrancisco, California.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In Proceedings of the Joint Con-ference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 1003?1011,Suntec, Singapore.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
In Proceedingsof the 8th Conference on Computational Natural Lan-guage Learning, pages 49?56, Boston, Massachusetts.Eric W. Noreen.
1989.
Computer-intensive Methods forTesting Hypotheses: An Introduction.
John Wiley andSons Inc.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 113?120, Sydney, Aus-tralia.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Suntec, Singapore.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning,62(1?2):107?136.Ellen Riloff and Rosie Jones.
1999.
Learning dictionar-ies for information extraction.
In Proceedings of the16th International Joint Conference on Artificial Intel-ligence, pages 474?479, Stockholm, Sweden.Stefan Schoenmackers, Jesse Davis, Oren Etzioni, andDaniel Weld.
2010.
Learning first-order Horn clausesfrom web text.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1088?1098, Cambridge, MA, October.
As-sociation for Computational Linguistics.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemptiveinformation extraction using unrestricted relation dis-covery.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Main Conference,pages 304?311, New York City, USA.Mihai Surdeanu and Massimiliano Ciaramita.
2007.
Ro-bust information extration with perceptrons.
In Pro-ceedings of the NIST 2007 Automatic Content Extrac-tion Workshop.Kiri Wagstaff, Claire Cardie, C Rogers, and S Schro?dl.2001.
Constrained k-means clustering with back-ground knowledge.
In International Conference onMachine Learning, pages 577?584.
Morgan Kauf-mann.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery usinggenerative models.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1456?1466, Edinburgh, Scotland, UK.GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-ing Zhu.
2007.
Tree kernel-based relation extractionwith context-sensitive structured parse tree informa-tion.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 728?736, Prague, Czech Republic.425
