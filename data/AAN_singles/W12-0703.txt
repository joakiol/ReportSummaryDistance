Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19?27,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsSweeping through the Topic Space:Bad luck?
Roll again!Martin Riedl and Chris BiemannUbiquitous Knowledge Processing LabComputer Science Department, Technische Universita?t DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germanyriedl@ukp.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.deAbstractTopic Models (TM) such as Latent Dirich-let Allocation (LDA) are increasingly usedin Natural Language Processing applica-tions.
At this, the model parameters andthe influence of randomized sampling andinference are rarely examined ?
usually,the recommendations from the original pa-pers are adopted.
In this paper, we ex-amine the parameter space of LDA topicmodels with respect to the application ofText Segmentation (TS), specifically target-ing error rates and their variance across dif-ferent runs.
We find that the recommendedsettings result in error rates far from opti-mal for our application.
We show substan-tial variance in the results for different runsof model estimation and inference, and giverecommendations for increasing the robust-ness and stability of topic models.
Run-ning the inference step several times and se-lecting the last topic ID assigned per token,shows considerable improvements.
Similarimprovements are achieved with the modemethod: We store all assigned topic IDsduring each inference iteration step and se-lect the most frequent topic ID assigned toeach word.
These recommendations do notonly apply to TS, but are generic enough totransfer to other applications.1 IntroductionWith the rise of topic models such as pLSI (Hof-mann, 2001) or LDA (Blei et al, 2003) in Nat-ural Language Processing (NLP), an increasingnumber of works in the field use topic models tomap terms from a high-dimensional word spaceto a lower-dimensional semantic space.
TMsare ?the new Latent Semantic Analysis?
(LSA),(Deerwester et al, 1990), and it has been shownthat generative models like pLSI and LDA notonly have a better mathematical foundation rootedin probability theory, but also outperform LSA indocument retrieval and classification, e.g.
(Hof-mann, 2001; Blei et al, 2003; Biro et al, 2008).To estimate the model parameters in LDA, the ex-act computation that was straightforward in LSA(matrix factorization) is replaced by a randomizedMonte-Carlo sampling procedure (e.g.
variationalBayes or Gibbs sampling).Aside from the main parameter, the numberof topics or dimensions, surprisingly little atten-tion has been spent to understand the interac-tions of hyperparameters, the number of sam-pling iterations in model estimation and inter-ference, and the stability of topic assignmentsacross runs using different random seeds.
Whileprogress in the field of topic modeling is mainlymade by adjusting prior distributions (e.g.
(Satoand Nakagawa, 2010; Wallach et al, 2009)), ordefining more complex model mixtures (Heinrich,2011), it seems unclear whether improvements,reached on intrinsic measures like perplexity oron application-based evaluations, are due to animproved model structure or could originate fromsub-optimal parameter settings or literally ?badluck?
due to the randomized nature of the sam-pling process.In this paper, we address these issues by sys-tematically sweeping the parameter space.
Forthis, we pick LDA since it is the most commonlyused TM in the field of NLP.
To evaluate the con-tribution of the TM, we choose the task of TS:this task has received considerable interest fromthe NLP community, standard datasets and eval-uation measures are available for testing, and it19has been shown that this task considerably bene-fits from the use of TMs, see (Misra et al, 2009;Sun et al, 2008; Eisenstein, 2009).This paper is organized as follows: In the nextsection, we present related work regarding textsegmentation using topic models and topic modelparameter evaluations.
Section 3 defines the Top-icTiling text segmentation algorithm, which is asimplified version of TextTiling (Hearst, 1994),and makes direct use of topic assignments.
Itssimplicity allows us to observe direct conse-quences of LDA parameter settings.
Further, wedescribe the experimental setup, our application-based evaluation methodology including the dataset and the LDA parameters we vary in Section 4.Results of our experiments in Section 5 indi-cate that a) there is an optimal range for the num-ber of topics, b) there is considerable variance inperformance for different runs for both model es-timation and inference, c) increasing the numberof sampling iterations stabilizes average perfor-mance but does not make TMs more robust, but d)combining the output of several independent sam-pling runs does, and additionally leads to large er-ror rate reductions.
Similar results are obtained bye) the mode method with less computational costsusing the most frequent topic ID that is assignedduring different inference iteration steps.
In theconclusion, we give recommendations to add sta-bility and robustness for TMs: aside from opti-mization of the hyperparameters, we recommendcombining the topic assignments of different in-ference iterations, and/or of different independentinference runs.2 Related Work2.1 Text Segmentation with Topic ModelsBased on the observation of Halliday and Hasan(1976) that the density of coherence relations ishigher within segments than between segments,most algorithms compute a coherence score tomeasure the difference of textual units for inform-ing a segmentation decision.
TextTiling (Hearst,1994) relies on the simplest coherence relation ?word repetition ?
and computes similarities be-tween textual units based on the similarities ofword space vectors.
The task of text segmenta-tion is to decide, for a given text, how to split thistext into segments.Related to our algorithm (see Section 3.1) arethe approaches described in Misra et al (2009)and Sun et al (2008): topic modeling is used toalleviate the sparsity of word vectors by mappingwords into a topic space.
This is done by extend-ing the dynamic programming algorithms from(Utiyama and Isahara, 2000; Fragkou et al, 2004)using topic models.
At this, the topic assignmentshave to be inferred for each possible segment.2.2 LDA and Topic Model EvaluationFor topic modeling, we use the widely appliedLDA (Blei et al, 2003), This model uses a train-ing corpus of documents to create document-topicand topic-word distributions and is parameterizedby the number of topics T as well as by twohyperparameters.
To generate a document, thetopic proportions are drawn using a Dirichlet dis-tribution with hyperparameter ?.
Adjacent foreach word w a topic zdw is chosen according toa multinomial distribution using hyperparameter?zdw .
The model is estimated using m itera-tions of Gibbs sampling.
Unseen documents canbe annotated with an existing topic model usingBayesian inference methods.
At this, Gibbs sam-pling with i iterations is used to estimate the topicID for each word, given the topics of the otherwords in the same sentential unit.
After inference,every word in every sentence receives a topic ID,which is the sole information that is used by theTopicTiling algorithm to determine the segmenta-tion.
We use the GibbsLDA implementation byPhan and Nguyen (2007) for all our experiments.The article of Blei et al (2003) compares LDAwith pLSI and Mixture Unigram models using theperplexity of the model.
In a collaborative filter-ing evaluation for different numbers of topics theyobserve that using too many topics leads to over-fitting and to worse results.In the field of topic model evaluations, Griffithsand Steyvers (2004) use a corpus of abstracts pub-lished between 1991 and 2001 and evaluate modelperplexity.
For this particular corpus, they achievethe lowest perplexity using 300 topics.
Further-more, they compare different sampling methodsand show that the perplexity converges faster withGibbs sampling than with expectation propaga-tion and variational Bayes.
On a small artificialtestset, small variations in perplexity across dif-ferent runs were observed in early sampling itera-tions, but all runs converged to the same limit.20In Wallach et al (2009) topic models are eval-uated with symmetric and asymmetric hyperpa-rameters based on the perplexity.
They observea benefit using asymmetric parameters for ?, butcannot show improvement with asymmetric priorsfor ?.3 Method3.1 TopicTilingFor the evaluation of the topic models, a text seg-mentation algorithm called TopicTiling is usedhere.
This algorithm is a newly developed al-gorithm based on TextTiling (Hearst, 1994) andachieves state of the art results using the Choidataset, which is a standard dataset for TS eval-uation.
The algorithm uses sentences as minimalunits.
Instead of words, we use topic IDs thatare assigned to each word using the LDA infer-ence running on sentence units.
The LDA modelshould be estimated on a corpus of documents thatis similar to the to-be-segmented documents.To measure the coherence cp between two sen-tences around position p, the cosine similarity(vector dot product) between these two adjacentsentences is computed.
Each sentence is repre-sented as a T -dimensional vector, where T is thenumber of topic IDs defined in the topic model.The t-th element of the vector contains the num-ber of times the t-th topic is observed in the sen-tence.
Similar to the TextTiling algorithm, lo-cal minima calculated from these similarity scoresare taken as segmentation candidates.This is illustrated in Figure 1, where the simi-larity scores between adjacent sentences are plot-ted.
The vertical lines in this plot indicate all localminima found.0 5 10 15 20 25 300.00.20.4SentencecosinesimilarityFigure 1: Cosine similarity scores of adjacent sen-tences based on topic distribution vectors.
Verticallines (solid and dashed) indicate local minima.
Solidlines mark segments that have a depth score above achosen threshold.Following the TextTiling definition, not theminimum score cp at position p itself is used, buta depth score dp for position p computed bydi = 1/2 ?
(cp?1 ?
cp + cp+1 ?
cp).
(1)In contrast to TextTiling, the directly neighboringsimilarity scores of the local minima are used, ifthey are higher than cp.
When using topics insteadof words, it can be expected that sentences withinone segment have many topics in common, whichleads to cosine similarities close to 1.
Further, us-ing topic IDs instead of words greatly increasessparsity.
A minimum in the curve indicates achange in topic distribution.
Segment boundariesare set at the positions of the n highest depth-scores, which is common practice in text segmen-tation algorithms.
An alternative to a given nwould be the selection of segments according toa depth score threshold.4 Experimental SetupAs dataset the Choi dataset (Choi, 2000) is used.This dataset is an artificially generated corpus thatconsists of 700 documents.
Each document con-sists of 10 segments and each segment has 3?11 sentences extracted from a document of theBrown corpus.
For the first setup, we perform a10-fold Cross Validation (CV) for estimating theTM (estimating on 630 documents at a time), forthe other setups we use 600 documents for TMestimation and the remaining 100 documents fortesting.
While we aim to neglect using the samedocuments for training and testing, it is not guar-anteed that all testing data is unseen, since thesame source sentences can find their way in sev-eral artificially crafted ?documents?.
This prob-lem, however, applies for all evaluations on thisdataset that use any kind of training, be it LDAmodels in Misra et al (2009) or TF-IDF values inFragkou et al (2004).For the evaluation of the Topic Model in combi-nation of Text Segmentation, we use the Pk mea-sure (Beeferman et al, 1999), which is a stan-dard measure for error rates in the field of TS.This measure compares the gold standard seg-mentation with the output of the algorithm.
APk value of 0 indicates a perfect segmentation,the averaged state of the art on the Choi Datasetis Pk = 0.0275 (Misra et al, 2009).
To assessthe robustness of the TM, we sweep over varying21configurations of the LDA model, and plot the re-sults using Box-and-Whiskers plots: the box in-dicates the quartiles and the whiskers are maxi-mal 1.5 times of the Interquartile Range (IQR) orequal to the data point that is no greater to the 1.5IQR.
The following parameters are subject to ourexploration:?
T : Number of topics used in the LDA model.Common values vary between 50 and 500.?
?
: Hyperparameter that regulates the sparse-ness topic-per-document distribution.
Lowervalues result in documents being representedby fewer topics (Heinrich, 2004).
Recom-mended: ?
= 50/T (Griffiths and Steyvers,2004)?
?
: Reducing ?
increases the sparsity oftopics, by assigning fewer terms to eachtopic, which is correlated to how relatedwords need to be, to be assigned to a topic(Heinrich, 2004).
Recommended: ?
={0.1, 0.01} (Griffiths and Steyvers, 2004;Misra et al, 2009)?
m Model estimation iterations.
Recom-mended / common settings: m = 500?5000(Griffiths and Steyvers, 2004; Wallach et al,2009; Phan and Nguyen, 2007)?
i Inference iterations.
Recommended / com-mon settings: 100 (Phan and Nguyen, 2007)?
d Mode of topic assignments.
At each in-ference iteration step, a topic ID is assignedto each word within a document (representedas a sentence in our application).
With thisoption, we count these topic assignments foreach single word in each iteration.
After all iinference iterations, the most frequent topicID is chosen for each word in a document.?
r Number of inference runs: We repeat theinference r times and assign the most fre-quently assigned topic per word at the fi-nal inference run for the segmentation algo-rithm.
High r values might reduce fluctua-tions due to the randomized process and leadto a more stable word-to-topic assignment.All introduced parameters parameterize the TM.We are not aware of any research that has usedseveral inference runs r and the mode of topic as-signments d to increase stability and varying TMparameters in combinations with measures otherthen perplexity.5 ResultsIn this section, we present the results we obtainedfrom varying the parameters under examination.5.1 Number of Topics TTo provide a first impression of the data, a 10-foldCV is calculated and the segmentation results arevisualized in Figure 2.Topic NumberP_kvalue0.00.10.20.30.40.53 10 20 50 100 250 500lll lllll l l l l ll l llllll lllllllllllllllllll ll llllll lllllllllllllllllllllllll lllllllllllllll ll lllFigure 2: Box plots for different number of topics T .Each box plot is generated from the average Pk valueof 700 documents, ?
= 50/T , ?
= 0.1, m = 1000,i = 100, r = 1.
These documents are segmented withTopicTiling using a 10-folded CV.Each box plot is generated from the Pk valuesof 700 documents.
As expected, there is a contin-uous range of topic numbers, namely between 50and 150 topics, where we observe the lowest Pkvalues.
Using too many topics leads to overfittingof the data and too few topics result in too gen-eral distinctions to grasp text segments.
This is inline with other studies, that determine an optimumfor T , cf.
(Griffiths and Steyvers, 2004), which isspecific to the application and the data set.5.2 Estimation and Inference iterationsThe next step examines the robustness of the topicmodel according to the number of model estima-tion iterations m needed to achieve stable results.600 documents are used to train the LDA model22that is applied by TopicTiling to segment the re-maining 100 documents.
From Figure 2 we knowthat sampling 100 topics leads to good results.To have an insight into unstable topic regions wealso inspect performance at different sampling it-erations using 20 and 250 topics.
To assess sta-bility across different model estimation runs, wetrained 30 LDA models using different randomseeds.
Each box plot in Figures 3 and 4 is gen-erated from 30 mean values, calculated from thePk values of the 100 documents.
The variationindicates the score variance for the 30 differentmodels.Number of topics: 100number of sample iterationsP_kvalue0.00.10.20.30.42 3 5 10 20 50 100 300 500 1000l l l l l lllllll l l l l l l l l l l l l lllllll l0.020.040.060.080.1050 100 300 500 1000llll l l l l l l l l l l l llll lFigure 3: Box plots with different model estimationiterations m, with T=100, ?
= 50/T , ?
= 0.1, i =100, r = 1.
Each box plot is generated from 30 meanvalues calculated from 100 documents.Using 100 topics (see Figure 3), the burn-inphase starts with 8?10 iterations and the mean Pkvalues stabilize after 40 iterations.
But lookingat the inset for large m values, significant vari-ations between the different models can be ob-served: note that the Pk error rates are almostdouble between the lower and the upper whisker.These remain constant and do not disappear forlargerm values: The whiskers span error rates be-tween 0.021 - 0.037 for model estimation on doc-ument unitsWith 20 topics, the Pk values are worse as with100 topics, as expected from Figure 2.
Here theconvergence starts at 100 sample iterations.
Moreinteresting results are achieved with 250 topics.A robust range for the error rates can be found be-tween 20 and 100 sample iterations.
With moreiterations m, the results get both worse and un-stable: as the ?natural?
topics of the collectionhave to be split in too many topics in the model,perplexity optimizations that drive the estimationprocess lead to random fluctuations, which theTopicTiling algorithm is sensitive to.
Manual in-spection of models for T = 250 revealed that infact many topics do not stay stable across estima-tion iterations.number of inference iterationsP_kvalue0.010.020.030.042 3 5 10 20 50 100ll l l l l l l l l l l l l l llll lFigure 5: Figure of box plots for different inferenceiterations i and m = 1000, T = 100, ?
= 50/T ,?
= 0.1, r = 1 .In the next step we sweep over several infer-ence iterations i.
Starting from 5 iterations, errorrates do not change much, see Figure 5.
But thereis still substantial variance, between about 0.019 -0.038 for inference on sentence units.5.3 Number of inference runs rTo decrease this variance, we assign the topic notonly from a singe inference run, but repeat the in-ference calculations several times, denoted by theparameter r. Then the frequency of assigned topicIDs per token is counted across the r runs, and weassign the most frequent topic ID (frequency tiesare broken randomly).
The box plot for severalevaluated values of r is shown in Figure 6.This log-scaled plot shows that both varianceand Pk error rate can be substantially decreased.Already for r = 3, we observe a significant im-provement in comparison to the default setting ofr = 1 and with increasing r values, the error ratesare reduced even more: for r = 20, variance anderror rates are is cut in less than half of their orig-inal values using this simple operation.23Number of topics: 20number of sample iterationsP_kvalue0.10.20.30.42 3 5 10 20 50 100 300 500 1000l l l l l l llllllll l l l l l l l l l l lll l llllll l l lll0.020.040.060.080.1050 100 300 500 1000lll l l l l l l l l l l llllll l l lllNumber of topics: 250number of sample iterationsP_kvalue0.10.20.30.42 3 5 10 20 50 100 300 500 1000l l l l l lllllll l l l l l ll l l l l l lllllllllll0.020.040.060.080.1050 100 300 500 1000lll l l l l l ll l l l l l llllllFigure 4: Box plots with varying model estimation iterations m applied with T = 20 (left) and T = 250 (right)topics, ?
= 50/T , ?
= 0.1, i = 100, r = 1number of repeated inferencesP_kvalue0.010.020.030.041 3 5 10 20llll llFigure 6: Box plot for several inference runs r, to as-sign the topics to a word with m = 1000, i = 100,T = 100, ?
= 50/T , ?
= 0.1.5.4 Mode of topic assignment dIn the previous experiment, we use the topic IDsthat have been assigned most frequently at the lastinference iteration step.
Now, we examine some-thing similar, but for all i inference steps of a sin-gle inference run: we select the mode of topicID assignments for each word across all inferencesteps.
The impact of this method on error andvariance is illustrated in Figure 7.
Using a sin-gle inference iteration, the topic IDs are almostassigned randomly.
After 20 inference iterationsPk values below 0.02 are achieved.
Using furtheriterations, the decrease of the error rate is onlynumber of inference iterationsP_kvalue0.010.020.030.042 3 5 10 20 50 100lll l l llllFigure 7: Box plot using the mode method d = truewith several inference iterations i with m = 500, T =100, ?
= 50/T , ?
= 0.1.marginal.
In comparison to the repeated inferencemethod, the additional computational costs of thismethod are much lower as the inference iterationshave to be carried out anyway in the default appli-cation setting.5.5 Hyperparameters ?
and ?In many previous works, hyperparameter settings?
= 50/T and ?
= {0.1, 0.01} are commonlyused.
In the next series of experiments we inves-tigate how different parameters of these both pa-rameters can change the TS task.For ?
values, shown in Figure 8, we can seethat the recommended value for T = 100 , ?
=240.5 leads to sub-optimal results, and an error ratereduction of about 40% can be realized by setting?
= 0.1.alpha valuesP_kvalue0.010.020.030.040.01 0.02 0.03 0.05 0.1 0.2 0.5 1l l l ll llll lllllllllFigure 8: Box plot for several alpha values ?withm =500, i = 100, T = 100, ?
= 0.1, r = 1.Regarding values of ?, we find that Pk ratesand their variance are relatively stable betweenthe recommended settings of 0.1 and 0.01.
Valueslarger than 0.1 lead to much worse performance.Regarding variance, no patterns within the stablerange emerge, see Figure 9.beta valuesP_kvalue0.050.100.150.01 0.02 0.03 0.05 0.1 0.2 0.5l l l l ll l l lllllllllllFigure 9: Box plot for several beta values ?
with m =500, i = 100, T = 100, ?
= 50/T , r = 1.5.6 Putting it all togetherUntil this point, we have examined different pa-rameters with respect to stability and error ratesone at the time.
Now, we combine what we haveSystem Pk error ?2 var.red.
red.default 0.0302 0.00% 2.02e-5 0.00%?
= 0.1 0.0183 39.53% 1.22e-5 39.77%r = 20 0.0127 57.86% 4.65e-6 76.97%d = true 0.0137 54.62% 3.99e-6 80.21%combined 0.0141 53.45% 9.17e-6 54.55%Table 1: Comparison of single parameter optimiza-tions, and combined system.
Pk averages and varianceare computed over 30 runs, together with reductionsrelative to the default setting.
Default: ?
= 0.5, r = 1.combined: ?
= 0.1, r = 20, d = truelearned from this and strive at optimal system per-formance.
For this, we contrast TS results ob-tained with the default LDA configuration withthe best systems obtained by optimization of sin-gle parameters, as well as to a system that usesthese optimal settings for all parameters.
Table 1shows Pk error rates for the different systems.
Atthis, we fixed the following parameters: T = 100,m = 500, i = 100, ?
= 0.1.
For the computa-tions we use 600 documents for the LDA modelestimation, apply TopicTiling and compute the er-ror rate for the 100 remaining documents and re-peat this 30 times with different random seeds.We can observe a massive improvement for op-timized single parameters.
The ?-tuning tuningresults in an error rate reduction of 39.77% incomparison to the default configurations.
Usingr = 20, the error rate is cut in less than halfits original value.
Also for the mode mechanism(d = true) the error rate is halved but slightlyworse than than when using the repeated infer-ence.
Using combined optimized parameters doesnot result to additional error decreases.
We at-tribute the slight decline of the combined methodin both in the error rate Pk and in the variance tocomplex parameter interactions that shall be ex-amined in further work.
In Figure 10, we visual-ize these results in a density plot.
It becomes clearthat repeated inference leads to slightly better andmore robust performance (higher peak) than themode method.
We attribute the difference to sit-uations, where there are several highly probabletopics in our sampling units, and by chance thesame one is picked for adjacent sentences that be-long to different segments, resulting in failure torecognize the segmentation point.
However, sincethe differences are miniscule, only using the modemethod might be more suitable for practical pur-poses since its computational cost is lower.250.00 0.01 0.02 0.03 0.04 0.05050100150P_k valuesDensitydefault valuesalpha=0.01r=20d=truecombinedFigure 10: Density plot of the error distributions forthe systems listed in Table 16 ConclusionIn this paper, we examined the robustness of LDAtopic models with respect to the application ofText Segmentation by sweeping through the topicmodel parameter space.
To our knowledge, this isthe first attempt to systematically assess the sta-bility of topic models in a NLP task.The results of our experiments are summarizedas follows:?
Perform the inference r times using the samemodel and choosing the assigned topic IDper word token taken from the last infer-ence iteration, improves both error rates andstability across runs with different randomseeds.?
Almost equal performance in terms of er-ror and stability is achieved with the modemechanism: choose the most frequent topicID assignment per word across inferencesteps.
While error rates were slightly higherfor our data set, this method is probablypreferable in practice because of its lowercomputation costs.?
As found in other studies, there is a range forthe number of topics T , where optimal re-sults are obtained.
In our task, performanceshowed to be robust in the range of 50 - 150topics.?
The default setting for LDA hyperparameters?
and ?
can lead to sub-optimal results.
Es-pecially ?
should be optimized for the task athand, as the utility of the topic model is verysensitive to this parameter.?
While the number of iterations for model es-timation and inference needed for conver-gence is depending on the number of topics,the size of the sampling unit (document) andthe collection, it should be noted that afterconvergence the variance between differentsampling runs does not decrease for a largernumber of iterations.Equipped with the insights gained from exper-iments on single parameter variation, we wereable to implement a very simple algorithm for textsegmentation that improves over the state of theart on a standard dataset by a large margin.
Atthis, the combination of the optimal ?, and a highnumber of inference repetitions r and the modemethod (d = true) produced slightly more errorsthan a high r alone.
While the purpose of this pa-per was mainly to address robustness and stabilityissues of topic models, we are planning to applythe segmentation algorithm to further datasets.The most important takeaway, however, is thatespecially for small sampling units like sentences,tremendous improvements in applications can beobtained when looking at multiple inference as-signments and using the most frequently assignedtopic ID in subsequent processing ?
either acrossdiffeent inference steps or across diffeent infer-ence runs.
These two new strategies seem to beable to offset sub-optimal hyperparameters to acertain extent.
This scheme is not only applica-ble to Text Segmentation, but in all applicationswhere performance crucially depends on stabletopic ID assignments per token.
Extensions tothis scheme, like ignoring tokens with a high topicvariability (stop words or general terms) or dy-namically deciding to conflate several topics be-cause of their per-token co-occurrence, are left forfuture work.7 AcknowledgmentsThis work has been supported by the Hessianresearch excellence program ?Landes-Offensivezur Entwicklung Wissenschaftlich-konomischerExzellenz?
(LOEWE) as part of the research cen-ter ?Digital Humanities?.
We would also thankthe anonymous reviewers for their comments,which greatly helped to improve the paper.26ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1999.Statistical models for text segmentation.
Machinelearning, 34(1):177?210.Istvan Biro, Andras Benczur, Jacint Szabo, and AnaMaguitman.
2008.
A comparative analysis of la-tent variable models for web page classification.
InProceedings of the 2008 Latin American Web Con-ference, pages 23?28, Washington, DC, USA.
IEEEComputer Society.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedings ofthe 1st North American chapter of the Associationfor Computational Linguistics conference, NAACL2000, pages 26?33, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Sci-ence, 41(6):391?407.Jacob Eisenstein.
2009.
Hierarchical text segmenta-tion from multi-scale lexical cohesion.
Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics on -NAACL ?09, page 353.P.
Fragkou, V. Petridis, and Ath.
Kehagias.
2004.
ADynamic Programming Algorithm for Linear TextSegmentation.
Journal of Intelligent InformationSystems, 23(2):179?197, September.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
PNAS, 101(suppl.
1):5228?5235.M A K Halliday and Ruqaiya Hasan.
1976.
Cohesionin English, volume 1 of English Language Series.Longman.Marti a. Hearst.
1994.
Multi-paragraph segmentationof expository text.
Proceedings of the 32nd annualmeeting on Association for Computational Linguis-tics, (Hearst):9?16.Gregor Heinrich.
2004.
Parameter estimation for textanalysis.
Technical report.Gregor Heinrich.
2011.
Typology of mixed-membership models: Towards a design method.In Machine Learning and Knowledge Discovery inDatabases, volume 6912 of Lecture Notes in Com-puter Science, pages 32?47.
Springer Berlin / Hei-delberg.
10.1007/978-3-642-23783-6 3.Thomas Hofmann.
2001.
Unsupervised Learning byProbabilistic Latent Semantic Analysis.
Computer,pages 177?196.Hemant Misra, Joemon M Jose, and Olivier Cappe?.2009.
Text Segmentation via Topic Modeling : AnAnalytical Study.
In Proceeding of the 18th ACMConference on Information and Knowledge Man-agement - CIKM ?09, pages 1553?-1556.Xuan-Hieu Phan and Cam-Tu Nguyen.
2007.GibbsLDA++: A C/C++ implementa-tion of latent Dirichlet alocation (LDA).http://jgibblda.sourceforge.net/.Issei Sato and Hiroshi Nakagawa.
2010.
Topic mod-els with power-law using pitman-yor process cate-gories and subject descriptors.
Science And Tech-nology, (1):673?681.Qi Sun, Runxin Li, Dingsheng Luo, and Xihong Wu.2008.
Text segmentation with LDA-based Fisherkernel.
Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics onHuman Language Technologies Short Papers - HLT?08, (June):269.Masao Utiyama and Hitoshi Isahara.
2000.
A Statis-tical Model for Domain-Independent Text Segmen-tation.
Communications.Hanna Wallach, David Mimno, and Andrew McCal-lum.
2009.
Rethinking lda: Why priors matter.
InNIPS.27
