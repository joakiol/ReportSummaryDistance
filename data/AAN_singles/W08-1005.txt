Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 33?39,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsParsing German with Latent Variable GrammarsSlav Petrov and Dan Klein{petrov,klein}@cs.berkeley.eduUniversity of California at BerkeleyBerkeley, CA 94720AbstractWe describe experiments on learning latentvariable grammars for various German tree-banks, using a language-agnostic statisticalapproach.
In our method, a minimal ini-tial grammar is hierarchically refined using anadaptive split-and-merge EM procedure, giv-ing compact, accurate grammars.
The learn-ing procedure directly maximizes the likeli-hood of the training treebank, without the useof any language specific or linguistically con-strained features.
Nonetheless, the resultinggrammars encode many linguistically inter-pretable patterns and give the best publishedparsing accuracies on three German treebanks.1 IntroductionProbabilistic context-free grammars (PCFGs) under-lie most high-performance parsers in one way or an-other (Collins, 1999; Charniak, 2000; Charniak andJohnson, 2005).
However, as demonstrated in Char-niak (1996) and Klein and Manning (2003), a PCFGwhich simply takes the empirical rules and probabil-ities off of a treebank does not perform well.
Thisnaive grammar is a poor one because its context-freedom assumptions are too strong in some ways(e.g.
it assumes that subject and object NPs sharethe same distribution) and too weak in others (e.g.it assumes that long rewrites do not decompose intosmaller steps).
Therefore, a variety of techniqueshave been developed to both enrich and generalizethe naive grammar, ranging from simple tree anno-tation and symbol splitting (Johnson, 1998; Kleinand Manning, 2003) to full lexicalization and intri-cate smoothing (Collins, 1999; Charniak, 2000).We view treebank parsing as the search for anoptimally refined grammar consistent with a coarsetraining treebank.
As a result, we begin with theprovided evaluation symbols (such as NP, VP, etc.
)but split them based on the statistical patterns inthe training trees.
A manual approach might takethe symbol NP and subdivide it into one subsymbolNP?S for subjects and another subsymbol NP?VPfor objects.
However, rather than devising linguis-tically motivated features or splits, we take a fullyautomated approach, in which each symbol is splitinto unconstrained subsymbols.
For example, NPwould be split into NP-1 through NP-8.
We usethe Expectation-Maximization (EM) to then fit oursplit model to the observed trees; therein the vari-ous subsymbols will specialize in ways which mayor may not correspond to our linguistic intuitions.This approach is relatively language independent,because the hidden subsymbols are induced auto-matically from the training trees based solely on datalikelihood, though of course it is most applicable tostrongly configurational languages.In our experiments, we find that we can learncompact grammars that give the highest parsing ac-curacies in the 2008 Parsing German shared task.Our F1-scores of 69.8/84.0 (TIGER/TueBa-D/Z) aremore than four points higher than those of thesecond best systems.
Additionally, we investigatethe patterns that are learned and show that the la-tent variable approach recovers linguistically inter-pretable phenomena.
In our analysis, we pay partic-ular attention to similarities and differences between33FRAGRBNotNPDTthisNNyear..(a)ROOTFRAG-xFRAG-xRB-xNotNP-xDT-xthisNN-xyear.-x.
(b)Figure 1: (a) The original tree.
(b) The binarized treewith latent variables.grammars learned from the two treebanks.2 Latent Variable ParsingIn latent variable parsing (Matsuzaki et al, 2005;Prescher, 2005; Petrov et al, 2006), we learnrule probabilities on latent annotations that, whenmarginalized out, maximize the likelihood of theunannotated training trees.
We use an automatic ap-proach in which basic nonterminal symbols are al-ternately split and merged to maximize the likeli-hood of the training treebank.In this section we briefly review the main ideasin latent variable parsing.
This work has been pre-viously published and we therefore provide onlya short overview.
For a more detailed exposi-tion of the learning algorithm the reader is re-ferred to Petrov et al (2006).
The correspond-ing inference procedure is described in detail inPetrov and Klein (2007).
The parser, code,and trained models are available for download athttp://nlp.cs.berkeley.edu.2.1 LearningStarting with a simple X-bar grammar, we use theExpectation-Maximization (EM) algorithm to learna new grammar whose nonterminals are subsymbolsof the original evaluation nonterminals.
The X-bargrammar is created by binarizing the treebank trees;for each local tree rooted at an evaluation nonter-minal X, we introduce a cascade of new nodes la-beled X so that each node has at most two children,see Figure 1.
This initialization is the absolute mini-mum starting grammar that distinguishes the evalua-tion nonterminals (and maintains separate grammarsfor each of them).In Petrov et al (2006) we show that a hierarchicalsplit-and-merge strategy learns compact but accurategrammars, allocating subsymbols adaptively wherethey are most effective.
Beginning with the base-line grammar, we repeatedly split and re-train thegrammar.
In each iteration, we initialize EM withthe results of the previous round?s grammar, splittingevery previous symbol in two and adding a smallamount of randomness (1%) to break the symme-try between the various subsymbols.
Note that wesplit all nonterminal symbols, including the part-of-speech categories.
While creating more latent an-notations can increase accuracy, it can also lead tooverfitting via oversplitting.
Adding subsymbols di-vides grammar statistics into many bins, resulting ina tighter fit to the training data.
At the same time,each bin has less support and therefore gives a lessrobust estimate of the grammar probabilities.
Atsome point, the fit no longer generalizes, leading tooverfitting.To prevent oversplitting, we could measure theutility of splitting each latent annotation individu-ally and then split the best ones first.
However, notonly is this impractical, requiring an entire trainingphase for each new split, but it assumes the contri-butions of multiple splits are independent.
In fact,extra subsymbols may need to be added to severalnonterminals before they can cooperate to pass in-formation along the parse tree.
This point is cru-cial to the success of our method: because all splitsare fit simultaneously, local splits can chain togetherto propagate information non-locally.
We thereforeaddress oversplitting in the opposite direction; aftertraining all splits, we measure for each one the lossin likelihood incurred by removing it.
If this lossis small, the new annotation does not carry enoughuseful information and can be removed.
Another ad-vantage of evaluating post-hoc merges is that, unlikethe likelihood gain from splitting, the likelihood lossfrom merging can be efficiently approximated.To summarize, splitting provides an increasinglytight fit to the training data, while merging improvesgeneralization and controls grammar size.
In orderto further overcome data fragmentation and overfit-ting, we also smooth our parameters along the splithierarchy.
Smoothing allows us to add a larger num-ber of annotations, each specializing in only a frac-tion of the data, without overfitting our training set.342.2 InferenceAt inference time, we want to use the learned gram-mar to efficiently and accurately compute a parsetree for a give sentence.For efficiency, we employ a hierarchical coarse-to-fine inference scheme (Charniak et al, 1998;Charniak and Johnson, 2005; Petrov and Klein,2007) which vastly improves inference time with noloss in test set accuracy.
Our method considers thesplitting history of the final grammar, projecting itonto its increasingly refined prior stages.
For eachsuch projection of the refined grammar, we estimatethe projection?s parameters from the source PCFGitself (rather than the original treebank), using tech-niques for infinite tree distributions and iterated fix-point equations.
We then rapidly pre-parse with eachrefinement stage in sequence, such that any itemX:[i, j] with sufficiently low posterior probabilitytriggers the pruning of its further refined variants inall subsequent finer parses.Our refined grammars G are over symbols of theform X-k where X is an evaluation symbol (such asNP) and k is some indicator of a subsymbol, whichmay encode something linguistic like a parent anno-tation context, but which is formally just an integer.G therefore induces a derivation distribution overtrees labeled with split symbols.
This distributionin turn induces a parse distribution over (projected)trees with unsplit evaluation symbols.
We haveseveral choices of how to select a tree given theseposterior distributions over trees.
Since computingthe most likely parse tree is NP-complete (Sima?an,1992), we settle for an approximation that allows usto (partially) sum out the latent annotation.
In Petrovand Klein (2007) we relate this approximation toGoodman (1996)?s labeled brackets algorithm ap-plied to rules and to Matsuzaki et al (2005)?s sen-tence specific variational approximation.
This pro-cedure is substantially superior to simply erasing thelatent annotations from the the Viterbi derivation.2.3 ResultsIn Petrov and Klein (2007) we trained models forEnglish, Chinese and German using the standardcorpora and setups.
We applied our latent variablemodel directly to each of the treebanks, without any?
40 words allParser LP LR LP LRENGLISHCharniak et al (2005) 90.1 90.1 89.5 89.6Petrov and Klein (2007) 90.7 90.5 90.2 89.9ENGLISH (reranked)Charniak et al (2005) 92.4 91.6 91.8 91.0GERMAN (NEGRA)Dubey (2005) F1 76.3 -Petrov and Klein (2007) 80.8 80.7 80.1 80.1CHINESEChiang et al (2002) 81.1 78.8 78.0 75.2Petrov and Klein (2007) 86.9 85.7 84.8 81.9Table 1: Our split-and-merge latent variable approachproduces the best published parsing performance onmany languages.language dependent modifications.
Specifically, thesame model hyperparameters (merging percentageand smoothing factor) were used in all experiments.Table 1 summarizes the results: automatically in-ducing latent structure is a technique that generalizeswell across language boundaries and results in stateof the art performance for Chinese and German.
OnEnglish, the parser is outperformed by the rerankedoutput of Charniak and Johnson (2005), but it out-performs their underlying lexicalized parser.3 ExperimentsWe conducted experiments on the two treebanksprovided for the 2008 Parsing German shared task.Both treebanks are annotated collections of Ger-man newspaper text, covering from similar top-ics.
They are annotated with part-of-speech (POS)tags, morphological information, phrase structure,and grammatical functions.
TueBa-D/Z addition-ally uses topological fields to describe fundamentalword order restrictions in German clauses.
However,the treebanks differ significantly in their annotationschemes: while TIGER relies on crossing branchesto describe long distance relationships, TueBa-D/Zuses planar tree structures with designated labelsthat encode long distance relationships.
Addition-ally, the annotation in TIGER is relatively flat on thephrasal level, while TueBa-D/Z annotates more in-ternal phrase structure.We used the standard splits into training and de-35606570758085900  1  2  3  4  5F1Split & Merge IterationsTIGERTueBa-D/ZFigure 2: Parsing accuracy improves when the amount oflatent annotation is increased.velopment set, containing roughly 16,000 trainingtrees and 1,600 development trees, respectively.
Allparsing figures in this section are on the develop-ment set, evaluating on constituents and grammat-ical functions using gold part-of-speech tags, un-less noted otherwise.
Note that even when we as-sume gold evaluation part-of-speech tags, we stillassign probabilities to the different subsymbols ofthe provided evaluation tag.
The parsing accuraciesin the final results section are the official results ofthe 2008 Parsing German shared task.3.1 Latent AnnotationAs described in Section 2.1, we start with a mini-mal X-Bar grammar and learn increasingly refinedgrammars in a hierarchical split-and-merge fashion.We conjoined the constituency categories with theirgrammatical functions, creating initial categorieslike NP-PD and NP-OA which were further splitautomatically.
Figure 2 shows how held-out accu-racy improves when we add latent annotation.
Ourbaseline grammars have low F1-scores (63.3/72.8,TIGER/TueBa-D/Z), but performance increases asthe complexity of latent annotation increases.
Afterfour split-and-merge iterations, performance levelsoff.
Interestingly, the gap in performance betweenthe two treebanks increases from 9.5 to 13.4 F1-points.
It appears that the latent variable approachis better suited for capturing the rich structure of theTueBa-D/Z treebank.As languages vary in their phrase-internal head-TIGER TueBa-D/ZF1 EX F1 EXAuto Tags 71.12 28.91 83.18 18.46Gold Tags 71.74 34.04 85.10 20.98Table 2: Parsing accuracies (F1-score and exact match)with gold POS tags and automatic POS tags.
Many parseerrors are due to incorrect tagging.edness, we varied the binarization scheme, but, con-sistent with our experience in other languages, no-ticed little difference between right and left bina-rization.
We also experimented with starting froma more constrained baseline by adding parent andsibling annotation.
Adding initial structural annota-tion results in a higher baseline performance.
How-ever, since it fragments the grammar, adding latentannotation has a smaller effect, eventually resultingin poorer performance compared to starting from asimple X-Bar grammar.
Essentially, the initial gram-mar is either mis- or oversplit to some degree.3.2 Part-of-speech taggingWhen gold parts-of-speech are not assumed, manyparse errors can be traced back to part-of-speech(POS) tagging errors.
It is therefore interesting to in-vestigate the influence of tagging errors on the over-all parsing accuracy.
For the shared task, we couldassume gold POS tags: during inference we only al-lowed (and scored) the different subsymbols of thecorrect tags.
However, this assumption cannot bemade in a more realistic scenario, where we want toparse text from an unknown source.
Table 2 com-pares the parsing performance with gold POS tagsand with automatic tagging.
While POS tagging er-rors have little influence on the TIGER treebank,tagging errors on TueBa-D/Z cause an substantialnumber of subsequent parse errors.3.3 Two pass parsingIn the previous experiments, we conflated thephrasal categories and grammatical functions intosingle initial grammar symbol.
An alternative isto first determine the categorical constituency struc-ture and then to assign grammatical functions to thechosen constituents in a separate, second pass.
Toachieve this, we trained latent variable grammarsfor base constituency parsing by stripping off the36grammatical functions.
After four rounds of splitand merge training, these grammars achieve verygood constituency accuracies of 85.1/94.1 F1-score(TIGER/TueBa-D/Z).
For the second pass, we es-timated (but did not split) X-Bar style grammarson the grammatical functions only.
Fixing the con-stituency structure from the first pass, we used thoseto add grammatical functions.
Unfortunately, thisapproach proved to be inferior to the unified, onepass approach, giving F1-scores of only 50.0/69.4(TIGER/TueBa-D/Z).
Presumably, the degradationcan be attributed to the fact that grammatical func-tions model long-distance relations between the con-stituents, which can only be captured poorly by anunsplit, highly local X-bar style grammar.3.4 Final ResultsThe final results of the shared task evaluation areshown in Table 3.
These results were produced bya latent variable grammar that was trained for foursplit-and-merge iterations, starting from an X-Bargrammar over conjoined categorical/grammaticalsymbols, with a left-branching binarization.
Ourautomatic latent variable approach serves better forGerman disambiguation than the competing ap-proaches, despite its being very language agnostic.4 AnalysisIn this section, we examine the learned grammars,discussing what is learned.
Because the grammat-ical functions significantly increase the number ofbase categories and make the grammars more diffi-cult to examine, we show examples from grammarsthat were trained for categorical constituency pars-ing by initially stripping off all grammatical functionannotations.4.1 Lexical SplitsSince both treebanks use the same part-of-speechcategories, it is easy to compare the learned POSsubcategories.
To better understand what is beinglearned, we selected two grammars after two splitand merge iterations and examined the word dis-tributions of the subcategories of various symbols.The three most likely words for a number of POStags are shown in Table 4.
Interestingly, the sub-categories learned from the different treebanks ex-hibit very similar patterns.
For example, in bothcases, the nominal category (NE) has been splitinto subcategories for first and last names, abbrevi-ations and places.
The cardinal numbers (CARD)have been split into subcategories for years, spelledout numbers, and other numbers.
There are of-ten subcategories distinguishing sentence initial andsentence medial placement (KOND, PDAT, ART,APPR, etc.
), as well as subcategories capturing casedistinctions (PDAT, ART, etc.
).A quantitative way of analyzing the complexity ofwhat is learned is to compare the number of subcat-egories that our split-and-merge procedure has allo-cated to each category.
Table 5 shows the automat-ically determined number of subcategories for eachPOS tag.
While many categories have been split intocomparably many of subcategories, the POS tags inthe TIGER treebank have in general been refinedmore heavily.
This increased refinement can be ex-plained by our merging criterion.
We compute theloss in likelihood that would be incurred from re-moving a split, and we merge back the least usefulsplits.
In this process, lexical and phrasal splits com-pete with each other.
In TueBa-D/Z the phrasal cat-egories have richer internal structure and thereforeget split more heavily.
As a consequence, the lexi-cal categories are often relatively less refined at anygiven stage than in TIGER.
Having different merg-ing thresholds for the lexical and phrasal categorieswould eliminate this difference and we might expectthe difference in lexical refinement to become lesspronounced.
Of course, because of the different un-derlying statistics in the two treebanks, we do notexpect the number of subcategories to become ex-actly equal in any case.4.2 Phrasal splitsAnalyzing the phrasal splits is much more difficult,as the splits can model internal as well as exter-nal context (as well as combinations thereof) and,in general, several splits must be considered jointlybefore their patterning can be described.
Further-more, the two treebanks use different annotationstandards and different constituent categories.
Over-all, the phrasal categories of the TueBa-D/Z tree-bank have been more heavily refined, in order to bet-ter capture the rich internal structures.
In both tree-banks, the most heavily split categories are the noun,verb and prepositional phrase categories (NP/NX,37TIGER TueBa-D/ZLP LR F1 LP LR F1Berkeley Parser 69.23 70.41 69.81 83.91 84.04 83.97Va?xjo?
Parser 67.06 63.40 65.18 76.20 74.56 75.37Stanford Parser 58.52 57.63 58.07 79.26 79.22 79.24Table 3: Final test set results of the 2008 Parsing German shared task (labeled precision, labeled recall and F1-score)on both treebanks (including grammatical functions and using gold part-of-speech tags).NEKohl Klaus SPD DeutschlandRabin Helmut USA dpaLafontaine Peter CDU BonnCARD1996 zwei 000 zwei1994 drei 100 31991 vier 20 2KONDUnd und sondern undDoch oder aber oderAber aber bis sowiePDATDiese dieser diesem -Dieser dieses diese -Dieses diese dieser -ARTDie der der dieDer des den derDas Die die denAPPRIn als in vonVon nach von inNach vor mit fu?rPDSDas dessen das -Dies deren dies -Diese die diese -NEMilosevic Peter K. BerlinMu?ller Wolfgang W. tazClinton Klaus de KosovoCARD1998 zwei 500 zwei1999 drei 100 202000 fu?nf 20 18KONDUnd und sondern undAber oder weder DennDoch aber sowohl oderPDATDieser diese diesem dieserDiese dieser dieser dieseDieses dieses diesen diesesARTDie die die derdie Die der dieDer das den denAPPRIn bis in vonMit Von auf inNach Bis mit fu?rPDSdem dessen das Dasdas die Das dasjene denen dies dieseTable 4: The three most likely words for several part-of-speech (sub-)categories.
The left column corresponds to theTIGER treebank the right column to the TueBa-D/Z treebank.
Similar subcategories are learned for both treebanks.38POS Ti TueADJA 32 17NN 32 32NE 31 32ADV 30 15ADJD 30 19VVFIN 29 5VVPP 29 4APPR 25 24VVINF 18 7CARD 18 16ART 10 7PIS 9 14PPER 9 2PIDAT - 9POS Ti TuePIAT 8 7VAFIN 8 3KON 8 8$[ 7 11PROAV 7 -APPRART 6 5$ 6 2PDS 5 5PPOSAT 4 4$.
4 5PDAT 4 5KOUS 4 3VMFIN 4 1PRELS 3 1POS Ti TueVVIZU 3 2VAINF 3 3PTKNEG 3 1FM 3 8PWS 2 2PWAV 2 5XY 2 2TRUNC 2 4KOUI 2 1PTKVZ 2 1VAPP 2 2KOKOM 2 5PROP - 2VVIMP 1 1POS Ti TueVAIMP 1 1VMPP 1 2PPOSS 1 1PRELAT 1 1NNE 1 -APPO 1 1PTKA 1 2PTKANT 1 2PWAT 1 2PRF 1 1PTKZU 1 1APZR 1 1VMINF 1 1ITJ 1 2Table 5: Automatically determined number of subcategories for the part-of-speech tags.
The left column correspondsto the TIGER treebank the right column to the TueBa-D/Z treebank.
Many categories are split in the same number ofsubcategories, but overall the TIGER categories have been more heavily refined.PP/PX, VP/VX*) as well as the sentential categories(S/SIMPX).
Categories that are rare or that have lit-tle internal structure, in contrast, have been splitlightly or not at all.5 ConclusionsWe presented a series of experiments on pars-ing German with latent variable grammars.
Weshowed that our latent variable approach is verywell suited for parsing German, giving the bestparsing figures on several different treebanks, de-spite being completely language independent.
Ad-ditionally, we examined the learned grammarsand showed examples illustrating the linguisticallymeaningful patterns that were learned.
The parser,code, and models are available for download athttp://nlp.cs.berkeley.edu.ReferencesE.
Charniak and M. Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt Discriminative Reranking.In ACL?05.E.
Charniak, S. Goldwater, and M. Johnson.
1998.
Edge-based best-first chart parsing.
6th Workshop on VeryLarge Corpora.E.
Charniak.
1996.
Tree-bank grammars.
In AAAI ?96,pages 1031?1036.E.
Charniak.
2000.
A maximum?entropy?inspiredparser.
In NAACL ?00, pages 132?139.D.
Chiang and D. Bikel.
2002.
Recovering latent infor-mation in treebanks.
In COLING ?02, pages 183?189.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, UPenn.A.
Dubey.
2005.
What to do when lexicalization fails:parsing German with suffix analysis and smoothing.In ACL ?05.J.
Goodman.
1996.
Parsing algorithms and metrics.
ACL?96.M.
Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24:613?632.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL ?03, pages 423?430.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL ?05.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL ?07.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.D.
Prescher.
2005.
Inducing head-driven PCFGs with la-tent heads: Refining a tree-bank grammar for parsing.In ECML?05.K.
Sima?an.
1992.
Computatoinal complexity of proba-bilistic disambiguation.
Grammars, 5:125?151.39
