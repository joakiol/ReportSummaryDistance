Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 231?234,New York, June 2006. c?2006 Association for Computational LinguisticsDetecting Emotion in Speech: Experiments in Three DomainsJackson LiscombeColumbia Universityjaxin@cs.columbia.eduAbstractThe goal of my proposed dissertation workis to help answer two fundamental questions:(1) How is emotion communicated in speech?and (2) Does emotion modeling improve spo-ken dialogue applications?
In this paper I de-scribe feature extraction and emotion classifi-cation experiments I have conducted and planto conduct on three different domains: EPSaT,HMIHY, and ITSpoke.
In addition, I plan toimplement emotion modeling capabilities intoITSpoke and evaluate the effectiveness of do-ing so.1 IntroductionThe focus of my work is the expression of emotion inhuman speech.
As normally-functioning people, we areeach capable of vocally expressing and aurally recogniz-ing the emotions of others.
How often have you beenput off by the ?tone in someone?s voice?
or tickled otherswith the humorous telling of a good story?
Though weas everyday people are intimately familiar with emotion,we as scientists do not actually know precisely how it isthat emotion is conveyed in human speech.
This is of spe-cial concern to us as engineers of natural language tech-nology; in particular, spoken dialogue systems.
Spokendialogue systems enable users to interact with computersystems via natural dialogue, as they would with humanagents.
In my view, a current deficiency of state-of-the-art spoken dialogue systems is that the emotional state ofthe user is not modeled.
This results in non-human-likeand even inappropriate behavior on the part of the spokendialogue system.There are two central questions I would like to at lestpartially answer with my dissertation research: (1) Howis emotion communicated in speech?
and (2) Does emo-tion modeling improve spoken dialogue applications?
Inan attempt to answer the first question, I have adoptedthe research paradigm of extracting features that charac-terize emotional speech and applying machine learningalgorithms to determine the prediction accuracy of eachfeature.
With regard to the second research question, Iplan to implement an emotion modeler ?
one that detectsand responds to uncertainty and frustration ?
into an In-telligent Tutoring System.2 Completed WorkThis section describes my current research on emotionclassification in three domains and forms the foundationof my dissertation.
For each domain, I have adopted anexperimental design wherein each utterance in a corpusis annotated with one or more emotion labels, featuresare extracted from these utterances, and machine learn-ing experiments are run to determine emotion predictionaccuracy.2.1 EPSaTThe publicly-available Emotional Prosody Speech andTranscription corpus1 (EPSaT) comprises recordings ofprofessional actors reading short (four syllables each)dates and numbers (e.g., ?two-thousand-four?)
with dif-ferent emotional states.
I chose a subset of 44 utterancesfrom 4 speakers (2 male, 2 female) from this corpus andconducted a web-based survey to subjectively label eachutterance for each of 10 emotions, divided evenly for va-lence.
These emotions included the positive emotion cat-egories: condent, encouraging, friendly, happy, inter-ested; and the negative emotion categories: angry, anx-ious, bored, frustrated, sad.Several features were extracted from each utterance inthis corpus, each one designed to capture emotional con-tent.
Global acoustic-prosodic information ?
e.g., speak-ing rate and minimum, maximum, and mean pitch and in-tensity ?
has been well known since the 1960s and 1970s1LDC Catalog No.
: LDC2002S28.231to convey emotion to some extent (e.g., (Davitz, 1964;Scherer et al, 1972)).
In addition to these features, I alsoincluded linguistically meaningful prosodic informationin the form of ToBI labels (Beckman et al, 2005), as wellas the spectral tilt of the vowel in each utterance bearingthe nuclear pitch accent.In order to evaluate the predictive power of each fea-ture extracted from the EPSaT utterances, I ran machinelearning experiments using RIPPER, a rule-learning al-gorithm.
The EPSaT corpus was divided into training(90%) and testing (10%) sets.
A binary classificationscheme was adopted based on the observed ranking dis-tributions from the perception survey: ?not at all?
wasconsidered to be the absence of emotion x; all other rankswas recorded as the presence of emotion x. Performanceaccuracy varied with respect to emotion, but on average Iobserved 75% prediction accuracy for any given emotion,representing an average 22% improvement over chanceperformance.
The most predictive included the globalacoustic-prosodic features, but interesting novel findingsemerged as well; most notably, significant correlationwas observed between negative emotions and pitch con-tours ending in a plateau boundary tone, whereas positiveemotions correlated with the standard declarative phrasalending (in ToBI, these would be labeled as /H-L%/ and/L-L%/, respectively).
Further discussion of such find-ings can be found in (Liscombe et al, 2003).2.2 HMIHY?How May I Help YouSM?
(HMIHY) is a natural lan-guage human-computer spoken dialogue system devel-oped at AT&T Research Labs.
The system enables AT&Tcustomers to interact verbally with an automated agentover the phone.
Callers can ask for their account bal-ance, help with AT&T rates and calling plans, explana-tions of certain bill charges, or identification of num-bers.
Speech data collected from the deployed systemhas been assembled into a corpus of human-computerdialogues.
The HMIHY corpus contains 5,690 com-plete human-computer dialogues that collectively con-tain 20,013 caller turns.
Each caller turn in the corpuswas annotated with one of seven emotional labels: posi-tive/neutral, somewhat frustrated, very frustrated, some-what angry, very angry, somewhat other negative2, veryother negative.
However, the distribution of the labelswas so skewed (73.1% were labeled as positive/neutral)that the emotions were collapsed to negative and non-negative.In addition to the set of automatic acoustic-prosodicfeatures found to be useful for emotional classification ofthe EPSaT corpus, the features I examined in the HMIHYcorpus were designed to exploit the discourse information2?Other negative?
refers to any emotion that is perceivednegatively but is not anger nor frustration.available in the domain of spontaneous human-machineconversation.
Transcriptive features ?
lexical items, filledpauses, and non-speech human noises ?
we recorded asfeatures, as too were the dialogue acts of each caller turn.In addition, I included contextual features that were de-signed to track the history of the previously mentionedfeatures over the course of the dialogue.
Specifically,contextual information included the rate of change of theacoustic-prosodic features of the previous two turns plusthe transcriptive and pragmatic features of the previoustwo turns as well.The corpus was divided into training (75%) and testing(25%) sets.
The machine learning algorithm employedwas BOOSTEXTER, an algorithm that forms a hypothesisby combining the results of several iterations of weak-learner decisions.
Classification accuracy using the auto-matic acoustic-prosodic features was recorded to be ap-proximately 75%.
The majority class baseline (alwaysguessing non-negative) was 73%.
By adding the otherfeature-sets one by one, prediction accuracy was itera-tively improved, as described more fully in (Liscombe etal., 2005b).
Using all the features combined ?
acoustic-prosodic, lexical, pragmatic, and contextual ?
the result-ing classification accuracy was 79%, a healthy 8% im-provement over baseline performance and a 5% improve-ment over the automatic acoustic-prosodic features alone.2.3 ITSpokeThis section describes more recent research I have beenconducting with the University of Pittsburgh?s Intelli-gent Tutoring Spoken Dialogue System (ITSpoke) (Lit-man and Silliman, 2004).
The goal of this research is towed spoken language technology with instructional tech-nology in order to promote learning gains by enhanc-ing communication richness.
ITSpoke is built upon theWhy2-Atlas tutoring back-end (VanLehn et al, 2002), atext-based Intelligent Tutoring System designed to tutorstudents in the domain of qualitative physics using naturallanguage interaction.
Several corpora have been recordedfor development of ITSpoke, though most of the workpresented here involves tutorial data between a studentand human tutor.
To date, we have labeled the human-human corpus for anger, frustration, and uncertainty.As this work is an extension of previous work, I choseto extract most of the same features I had extracted fromthe EPSaT and HMIHY corpora.
Specifically, I extractedthe same set of automatic acoustic-prosodic features, aswell as contextual features measuring the rate of changeof acoustic-prosodic features of past student turns.
Anew feature set was introduced as well, which I referto as the breath-group feature set, and which is an auto-matic method for segmenting utterances into intonation-ally meaningful units by identifying pauses using back-ground noise estimation.
The breath group feature set232comprises the number of breath-groups in each turn, thepause time, and global acoustic-prosodic features calcu-lated for the first, last, and longest breath-group in eachstudent turn.I used the WEKA machine learning software packageto classify whether a student answer was perceived to beuncertain, certain, or neutral3 in the ITSpoke human-human corpus.
As a predictor, C4.5, a decision-treelearner, was boosted with AdaBoost, a learning strategysimilar to the one presented in Section 2.2.
The datawere randomly split into a training set (90%) and a test-ing set (10%).
The automatic acoustic-prosodic featuresperformed at 75% accuracy, a relative improvement of13% over the baseline performance of always guessingneutral.
By adding additional feature-sets ?
contextualand breath-group information ?
I observed an improvedprediction accuracy of 77%.
Thus indicating that breath-group features are useful.
I refer the reader to (Liscombeet al, 2005a) for in-depth implications and further analy-sis of these results.
In the immediate future, I will extractfeatures previously mentioned in Section 2.2 as well asthe exploratory features I will discuss in the followingsection.3 Work-in-progressIn this section I describe research I have begun to con-duct and plan to complete in the coming year, as agreed-upon in February, 2006 by my dissertation committee.
Iwill explore features that are not well studied in emotionclassification research, primarily pitch contour and voicequality approximation.
Furthermore, I will outline how Iplan to implement and evaluate an emotion detection andresponse module into ITSpoke.3.1 Pitch Contour ClusteringThe global acoustic-prosodic features used in most emo-tion prediction studies capture meaningful prosodic vari-ation, but are not capable of describing the linguisti-cally meaningful intonational behavior of an utterance.Though phonological labeling methods exist, such asToBI, annotation of this sort is time-consuming and mustbe done manually.
Instead, I propose an automatic al-gorithm that directly compares pitch contours and thengroups them into classes based on abstract form.
Specif-ically, I intend to use partition clustering to define adisjoint set of similar prosodic contour types over ourdata.
I hypothesize that the resultant clusters will be the-oretically meaningful and useful for emotion modeling.The similarity metric used to compare two contours willbe edit distance, calculated using dynamic time warpingtechniques.
Essentially, the algorithm finds the best fitbetween two contours by stretching and shrinking each3With respect to certainness.contour as necessary.
The score of a comparison is calcu-lated as the sum of the normalized real-valued distancesbetween mapped points in the contours.3.2 Voice QualityVoice quality is a term used to describe a perceptual col-oring of the acoustic speech signal and is generally be-lieved to play an important role in the vocal communica-tion of emotion.
However, it has rarely been used in au-tomatic classification experiments because the exact pa-rameters defining each quality of voice (e.g., creaky andbreathy) are still largely unknown.
Yet, some researchersbelieve much of what constitutes voice quality can bedescribed using information about glottis excitation pro-duced by the vocal folds, most commonly referred toas the glottal pulse waveform.
While there are ways ofdirectly measuring the glottal pulse waveform, such aswith an electroglottograph, these techniques are too inva-sive for practical purposes.
Therefore, the glottal pulsewaveform is usually approximated by inverse filtering ofthe speech signal.
I will derive glottal pulse waveformsfrom the data using an algorithm that automatically iden-tifies voiced regions of speech, obtains an estimate of theglottal flow derivative, and then represents this using theLiljencrants-Fant parametric model.
The final result is aglottal pulse waveform, from which features can be ex-tracted that describe the shape of this waveform, such asthe Open and Skewing Quotients.3.3 ImplementationThe motivating force behind much of the research I havepresented herein is the common assumption in the re-search community that emotion modeling will improvespoken dialogue systems.
However, there is little to noempirical proof testing this claim (See (Pon-Barry et al,In publication) for a notable exception.).
For this rea-son, I will implement functionality for detecting and re-sponding to student emotion in ITSpoke (the IntelligentTutoring System described in Section 2.3) and analyzethe effect it has on student behavior, hopefully showing(quantitatively) that doing so improves the system?s ef-fectiveness.Research has shown that frustrated students learn lessthan non-frustrated students (Lewis and Williams, 1989)and that human tutors respond differently in the face ofstudent uncertainty than they do when presented with cer-tainty (Forbes-Riley and Litman, 2005).
These findingsindicate that emotion plays an important role in Intelli-gent Tutoring Systems.
Though I do not have the abilityto alter the discourse-flow of ITSpoke, I will insert activelistening prompts on the part of ITSpoke when the sys-tem has detected either frustration or uncertainty.
Activelistening is a technique that has been shown to diffusenegative emotion in general (Klein et al, 2002).
I hy-233pothesize that diffusing user frustration and uncertaintywill improve ITSpoke.After collecting data from an emotion-enabled IT-Spoke I will compare evaluation metrics with those ofa control study conducted with the original ITSpoke sys-tem.
One such metric will be learning gain, the differ-ence between student pre- and post-test scores and thestandard metric for quantifying the effectiveness of edu-cational devices.
Since learning gain is a crude measureof academic achievement and may overlook behavioraland cognitive improvements, I will explore other metricsas well, such as: the amount of time taken for the stu-dent to produce a correct answer, the amount of negativeemotional states expressed, the quality and correctness ofanswers, the willingness to continue, and subjective post-tutoring assessments.4 ContributionsI see the contributions of my dissertation to be the extentto which I have helped to answer the questions I posed atthe outset of this paper.4.1 How is emotion communicated in speech?The experimental design of extracting features from spo-ken utterances and conducting machine learning experi-ments to predict emotion classes identifies features im-portant for the vocal communication of emotion.
Most ofthe features I have described here are well established inthe research community; statistic measurements of fun-damental frequency and energy, for example.
However, Ihave also described more experimental features as a wayof improving upon the state-of-the-art in emotion mod-eling.
These exploratory features include breath-groupsegmentation, contextual information, pitch contour clus-tering, and voice quality estimation.
In addition, explor-ing three domains will allow me to comparatively ana-lyze the results, with the ultimate goal of identifying uni-versal qualities of spoken emotions as well as those thatmay particular to specific domains.
The findings of sucha comparative analysis will be of practical benefit to fu-ture system builders and to those attempting to define auniversal model of human emotion alike.4.2 Does emotion modeling help?By collecting data of students interacting with anemotion-enabled ITSpoke, I will be able to report quan-titatively the results of emotion modeling in a spoken di-alogue system.
Though this is the central motivation formost researchers in this field, there is currently no defini-tive evidence either supporting or refuting this claim.ReferencesM.
E. Beckman, J. Hirschberg, and S. Shattuck-Hufnagel,2005.
Prosodic Typology ?
The Phonology of Intona-tion and Phrasing, chapter 2 The original ToBI sys-tem and the evolution of the ToBI framework.
Oxford,OUP.J.
R. Davitz, 1964.
The Communication of EmotionalMeaning, chapter 8 Auditory Correlates of Vocal Ex-pression of Emotional Feeling, pages 101?112.
NewYork: McGraw-Hill.Kate Forbes-Riley and Diane J. Litman.
2005.
Usingbigrams to identify relationships between student cer-tainness states and tutor responses in a spoken dialoguecorpus.
In Proceedings of 6th SIGdial Workshop onDiscourse and Dialogue,, Lisbon, Portugal.J.
Klein, Y.
Moon, and R. W. Picard.
2002.
This com-puter responds to user frustration: Theory, design, andresults.
Interacting with Computers, 14(2):119?140,February.V.
E. Lewis and R. N. Williams.
1989.
Mood-congruentvs.
mood-state-dependent learning: Implications for aview of emotion.
D. Kuiken (Ed.
), Mood and Mem-ory: Theory, Research, and Applications, Special Is-sue of the Journal of Social Behavior and Personality,4(2):157?171.Jackson Liscombe, Jennifer Venditti, and JuliaHirschberg.
2003.
Classifying subject ratingsof emotional speech using acoustic features.
InProceedings of Eurospeech, Geneva, Switzerland.Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-ditti.
2005a.
Detecting certainness in spoken tutorialdialogues.
In Proceedings of Interspeech, Lisbon, Por-tugal.Jackson Liscombe, Guiseppe Riccardi, and DilekHakkani-Tu?r.
2005b.
Using context to improve emo-tion detection in spoken dialogue systems.
In Proceed-ings of Interspeech, Lisbon, Portugal.Diane Litman and Scott Silliman.
2004.
Itspoke: An in-telligent tutoring spoken dialogue system.
In Proceed-ings of the 4th Meeting of HLT/NAACL (CompanionProceedings), Boston, MA, May.Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt,Brady Clark, and Stanley Peters.
In publication.
Re-sponding to student uncertainty in spoken tutorial dia-logue systems.
International Journal of Articial In-telligence in Education (IJAIED).K.
R. Scherer, J. Koivumaki, and R. Rosenthal.
1972.Minimal cues in the vocal communication of affect:Judging emotions from content-masked speech.
Jour-nal of Psycholinguistic Research, 1:269?285.K.
VanLehn, P. Jordan, and C. P. Rose.
2002.
The archi-tecture of why2-atlas: A coach for qualitative physicsessay writing.
In Proceedings of the Intelligent Tutor-ing Systems Conference, Biarritz, France.234
