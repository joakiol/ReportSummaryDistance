Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1580?1590,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA convex relaxation for weakly supervised relation extraction?Edouard GraveEECS DepartmentUniversity of California, Berkeleygrave@berkeley.eduAbstractA promising approach to relation extrac-tion, called weak or distant supervision,exploits an existing database of facts astraining data, by aligning it to an unla-beled collection of text documents.
Usingthis approach, the task of relation extrac-tion can easily be scaled to hundreds ofdifferent relationships.
However, distantsupervision leads to a challenging multi-ple instance, multiple label learning prob-lem.
Most of the proposed solutions to thisproblem are based on non-convex formu-lations, and are thus prone to local min-ima.
In this article, we propose a newapproach to the problem of weakly su-pervised relation extraction, based on dis-criminative clustering and leading to aconvex formulation.
We demonstrate thatour approach outperforms state-of-the-artmethods on the challenging dataset intro-duced by Riedel et al.
(2010).1 IntroductionInformation extraction refers to the broad taskof automatically extracting structured informationfrom unstructured documents.
An example is theextraction of named entities and the relations be-tween those entities from natural language texts.In the age of the world wide web and big data,information extraction is quickly becoming perva-sive.
For example, in 2013, more than 130, 000scientific articles were published about cancer.Keeping track with that quantity of informationis almost impossible, and it is thus of utmost im-portance to transform the knowledge contained inthis massive amount of documents into structureddatabases.Traditional approaches to information extrac-tion relies on supervised learning, yielding highKnowledge baser e1e2BornIn Lichtenstein New York CityDiedIn Lichtenstein New York CitySentences Latent labelsRoy Lichtenstein was born inNew York City, into an upper-middle-class family.BornInIn 1961, Leo Castelli starteddisplaying Lichtenstein?s workat his gallery in New York.NoneLichtenstein died of pneumoniain 1997 in New York City.DiedInFigure 1: An example of a knowledge databasecomprising two facts and training sentences ob-tained by aligning this database to unlabeled text.precision and recall results (Zelenko et al.,2003).
Unfortunately, these approaches need largeamount of labeled data, and thus do not scale wellto the great number of different types of fact foundon the Web or in scientific articles.
A promisingapproach, called distant or weak supervision, isto exploit an existing database of facts as trainingdata, by aligning it to an unlabeled collection oftext documents (Craven and Kumlien, 1999).In this article, we are interested in weakly super-vised extraction of binary relations.
A challengepertaining to weak supervision is that the obtainedtraining data is noisy and ambiguous (Riedel etal., 2010).
Let us start with an example: if thefact Attended(Turing, King?s College) existsin the knowledge database and we observe the sen-tenceTuring studied as an undergraduate from1931 to 1934 at King?s College, Cambridge.which contains mentions of both entities Turing1580and King?s College, then this sentence might ex-press the fact that Alan Turing attended King?sCollege, and thus, might be a useful example forlearning to extract the relation Attended.
How-ever, the sentenceCelebrations for the centenary of Alan Tur-ing are being planned at King?s College.also contains mentions of Turing andKing?s College, but do not express the re-lation Attended.
Thus, weak supervision leadto noisy examples.
As noted by Riedel et al.
(2010), such negative extracted sentences forexisting facts can represent more than 30% ofthe data.
Moreover, a given pair of entities,such as (Roy Lichtenstein, New York City),car verify multiple relations, such as BornInand DiedIn.
Weak supervision thus lead toambiguous examples.This challenge is illustrated in Fig.
1.
A solutionto address it is to formulate the task of weakly su-pervised relation extraction as a multiple instance,multiple label learning problem (Hoffmann et al.,2011; Surdeanu et al., 2012).
However, these for-mulations are often non-convex and thus sufferfrom local minimum.In this article, we make the following contribu-tions:?
We propose a new convex relaxation for theproblem of weakly supervised relation ex-traction, based on discriminative clustering,?
We propose an efficient algorithm to solve theassociated convex program,?
We demonstrate that our approach obtainsstate-of-the-art results on the dataset intro-duced by Riedel et al.
(2010).To our knowledge, this paper is the first to proposea convex formulation for solving the problem ofweakly supervised relation extraction.2 Related workSupervised learning.
Many approaches basedon supervised learning have been proposed tosolve the problem of relation extraction, and thecorresponding literature is to large to be summa-rized here.
One of the first supervised method forrelation extraction was inspired by syntactic pars-ing: the system described by Miller et al.
(1998)combines syntactic and semantic knowledge, andthus, part-of-speech tagging, parsing, named en-tity recognition and relation extraction all happenat the same time.
The problem of relation ex-traction was later formulated as a classificationproblem: Kambhatla (2004) proposed to solve thisproblem using maximum entropy models usinglexical, syntactic and semantic features.
Kernelmethods for relation extraction, based on shallowparse trees or dependency trees were introducedby Zelenko et al.
(2003), Culotta and Sorensen(2004) and Bunescu and Mooney (2005).Unsupervised learning.
The open informationextraction paradigm, simultaneously proposed byShinyama and Sekine (2006) and Banko et al.
(2007), does not rely on any labeled data or evenexisting relations.
Instead, open information ex-traction systems only use an unlabeled corpus, andoutput a set of extracted relations.
Such systemsare based on clustering (Shinyama and Sekine,2006) or self-supervision (Banko et al., 2007).One of the limitations of these systems is the factthat they extract uncanonicalized relations.Weakly supervised learning.
Weakly super-vised learning refers to a broad class of meth-ods, in which the learning system only have ac-cess to partial, ambiguous and noisy labeling.Craven and Kumlien (1999) were the first to pro-pose a weakly supervised relation extractor.
Theyaligned a knowledge database (the Yeast ProteinDatabase) with scientific articles mentioning a par-ticular relation, and then used the extracted sen-tences to learn a classifier for extracting that rela-tion.Later, many different sources of weak label-ings have been considered.
Bellare and McCallum(2007) proposed a method to extract bibliographicrelations based on conditional random fields andused a database of BibTex entries as weak super-vision.
Wu and Weld (2007) described a methodto learn relations based on Wikipedia infoboxes.Knowledge databases, such as Freebase1(Mintz etal., 2009; Sun et al., 2011) and YAGO2(Nguyenand Moschitti, 2011) were also considered as asource of weak supervision.Multiple instance learning.
The methods wepreviously mentionned transform the weakly su-pervised problem into a fully supervised one, lead-ing to noisy training datasets (see Fig.
1).
Mul-1www.freebase.com2www.mpi-inf.mpg.de/yago-naga/yago1581tiple instance learning (Dietterich et al., 1997) isa paradigm in which the learner receives bags ofexamples instead of individual examples.
A pos-itively labeled bag contains at least one positiveexample, but might also contains negative exam-ples.
In the context of relation extraction, Bunescuand Mooney (2007) introduced a kernel methodfor multiple instance learning, while Riedel et al.
(2010) proposed a solution based on a graphicalmodel.Both these methods allow only one label perbag, which is an asumption that is not true forrelation extraction (see Fig.
1).
Thus, Hoffmannet al.
(2011) proposed a multiple instance, multi-ple label method, based on an undirected graphicalmodel, to solve the problem of weakly supervisedrelation extraction.
Finally, Surdeanu et al.
(2012)also proposed a graphical model to solve this prob-lem.
One of their main contributions is to cap-ture dependencies between relation labels, such asthe fact that two labels cannot be generated jointly(e.g.
the relations SpouseOf and BornIn).Discriminative clustering.
Our approach isbased on the discriminative clustering framework,introduced by Xu et al.
(2004).
The goal of dis-criminative clustering is to find a labeling of thedata points leading to a classifier with low classifi-cation error.
Different formulations of discrimina-tive clustering have been proposed, based on sup-port vector machines (Xu et al., 2004), the squaredloss (Bach and Harchaoui, 2007) or the logisticloss (Joulin et al., 2010).
A big advantage of dis-criminative clustering is that weak supervision orprior information can easily be incorporated.
Ourwork is closely related to the method proposed byBojanowski et al.
(2013) for learning the names ofcharacters in movies.3 Weakly supervised relation extractionIn this article, our goal is to extract binaryrelations between entities from natural lan-guage text.
Given a set of entities, a binaryrelation r is a collection of ordered pairs ofentities.
The statement that a pair of entities(e1, e2) belongs to the relation r is denoted byr(e1, e2) and this triple is called a fact or relationinstance.
For example, the fact that ErnestHemingway was born in Oak Park is denotedby BornIn(Ernest Hemingway, Oak Park).A given pair of entities, such as(Edouard Manet, Paris), can belong todifferent relations, such as BornIn and DiedIn.An entity mention is a contiguous sequence oftokens refering to an entity, while a pair mentionor relation mention candidate is a sequence of textin which a pair of entities is mentioned.
In thefollowing, relation mention candidates will be re-stricted to pair of entities that are mentioned in thesame sentence.
For example, the sentence:Ernest Hemingway was born in Oak Park.contains two entity mentions, correspondingto two relation mention candidates.
In-deed, the pairs (Hemingway, Oak Park) and(Oak Park, Hemingway) are two distinct pairs ofentities, where only the first one verifies the rela-tion BornIn.Given a text corpus, aggregate extraction corre-sponds to the task of extracting a set of facts, suchthat each extracted fact is expressed at least once inthe corpus.
On the other hand, the task of senten-tial extraction corresponds to labeling each rela-tion mention candidate by the relation it expresses,or by a None label if it does not express any rela-tion.
Given a solution to the sentential extractionproblem, it is possible to construct a solution forthe aggregate extraction problem by returning allthe facts that were detected.
We will follow thisapproach, by building an instance level classifier,and aggregating the results by extracting the factsthat were detected at least once in the corpus.In the following, we will describe a method tolearn such a classifier using a database of facts in-stead of a set of labeled sentences.
This settingis known as distant supervision or weak supervi-sion, since we do not have access to labeled dataon which we could directly train a sentence levelrelation extractor.4 General approachIn this section, we propose a two step procedure tosolve the problem of weakly supervised relationextraction:1.
First, we describe a method to infer the re-lation labels corresponding to each relationmention candidate of our training set,2.
Second, we train a supervised instance levelrelation extractor, using the labels inferedduring step 1.In the second step of our approach, we will simplyuse a multinomial logistic regression model.
We1582(Lichtenstein, New York City)Roy Lichtenstein wasborn in New York City.Lichtenstein left NewYork to study in Ohio.BornInDiedInN relation mention candidatesrepresented by vectors xnI pairs of entities piK relationsEinRikFigure 2: Instance of the weakly supervised relation extraction problem, with notations used in the text.now describe the approach we propose for the firststep.4.1 NotationsLet (pi)1?i?Ibe a collection of I pairs of entities.We suppose that we have N relation mention can-didates, represented by the vectors (xn)1?n?N.LetE ?
RI?Nbe a matrix such thatEin= 1 if therelation mention candidate n corresponds to thepair of entities i, and Ein= 0 otherwise.
The ma-trix E thus indicates which relation mention can-didate corresponds to which pair of entities.
Wesuppose that we have K relations, indexed by theintegers {1, ...,K}.
Let R ?
RI?Kbe a matrixsuch that Rik= 1 if the pair of entities i verifiesthe relation k, and Rik= 0 otherwise.
The matrixR thus represents the knowledge database.
SeeFig.
2 for an illustration of these notations.4.2 Problem formulationOur goal is to infer a binary matrixY ?
{0, 1}N?
(K+1), such that Ynk= 1 ifthe relation mention candidate n express therelation k and Ynk= 0 otherwise (and thus, theinteger K + 1 represents the relation None).We take an approach inspired by the discrimi-native clustering framework of Xu et al.
(2004).We are thus looking for a (K + 1)-class indicatormatrix Y, such that the classification error of anoptimal multiclass classifier f is minimum.
Givena multiclass loss function ` and a regularizer ?,this problem can be formulated as:minYminfN?n=1`(yn, f(xn)) + ?(f),s.t.
Y ?
Ywhere ynis the nth line of Y.
The constraintsY ?
Y are added in order to take into accountthe information from the weak supervision.
Wewill describe in the next section what kind of con-straints are considered.4.3 Weak supervision by constraining YIn this section, we show how the informationfrom the knowledge base can be expressed as con-straints on the matrix Y.First, we suppose that each relation mentioncandidate express exactly one relation (includingthe None relation).
This means that the matrix Ycontains exactly one 1 per line, which is equivalentto the constraint:?n ?
{1, ..., N},K?k=1Ynk= 1.Second, if the pair i of entities verifies the rela-tion k we suppose that at least one relation men-tion candidate indeed express that relation.
Thuswe want to impose that for at least one relationmention candidate n such that Ein= 1, we haveYnk= 1.
This is equivalent to the constraint:?
(i, k) such that Rik= 1,N?n=1EinYnk?
1.Third, if the pair i of entities does not verify the re-lation k, we suppose that no relation mention can-didate express that relation.
Thus, we impose thatfor all mention candidate n such that Ein= 1, wehave Ynk= 0.
This is equivalent to the constraint:?
(i, k) such that Rik= 0,N?n=1EinYnk= 0.Finally, we do not want too many relation men-tion candidates to be classified as None.
We thusimpose?i ?
{1, ..., I},N?n=1EinYn(K+1)?
cN?n=1Ein,where c is the proportion of relation mention can-didates that do not express a relation, for entitypairs that appears in the knowledge database.1583We can rewrite these constraints using only ma-trix operations in the following way:Y1 = 1(EY) ?
S ?
?R, (1)where ?
is the Hadamard product (a.k.a.
the ele-mentwise product), the matrix S ?
RI?
(K+1)isdefined bySik={1 if Rik= 1?1 if Rik= 0 or k = K + 1,and the matrix?R ?
RI?
(K+1)is defined by?R = [R,?cE1].The set Y is thus defined as the set of matricesY ?
{0, 1}N?
(K+1)that verifies those two linearconstraints.
It is important to note that besides theboolean constraints, the two other constraints areconvex.5 Squared loss and convex relaxationIn this section, we describe the problem we ob-tain when using the squared loss, and its associatedconvex relaxation.
We then introduce an efficientalgorithm to solve this problem, by computing itsdual.5.1 Primal problemFollowing Bach and Harchaoui (2007), we use lin-ear classifiers W ?
RD?
(K+1), the squared lossand the squared `2-norm as the regularizer.
In thatcase, our formulation becomes:minY,W12?Y ?XW?2F+?2?W?2F,s.t.
Y ?
{0, 1}N?
(K+1)Y1 = 1,(EY) ?
S ?
R.where ?
?
?Fis the Frobenius norm and the ma-trix X = [x1, ...,xN]>?
RN?Drepresents therelation mention candidates.
Thanks to using thesquared loss, we have a closed form solution forthe matrix W:W = (X>X + ?ID)?1X>Y.Replacing the matrix W by its optimal solution,we obtain the following cost function:minY12Y>(IN?X(X>X + ?ID)?1X>)Y.Then, by applying the Woodbury matrix identityand relaxing the constraint Y ?
{0, 1}N?
(K+1)into Y ?
[0, 1]N?
(K+1), we obtain the followingconvex quadratic problem in Y:minY12tr(Y>(XX>+ ?IN)?1Y),s.t.
Y ?
0,Y1 = 1,(EY) ?
S ?
R.Since the inequality constraints might be in-feasible, we add the penalized slack variables?
?
RI?
(K+1), finally obtaining:minY,?12tr(Y>(XX>+ ?IN)?1Y)+ ????1s.t.
Y ?
0, ?
?
0,Y1 = 1,(EY) ?
S ?
R?
?.This convex problem is a quadratic program.
Inthe following section, we will describe how tosolve this problem efficiently, by exploiting thestructure of its dual problem.5.2 Dual problemThe matrix Q = (XX>+ ?IN) appearing in thequadratic program is an N by N matrix, whereN is the number of mention relation candidates.Computing its inverse is thus expensive, since Ncan be large.
Instead, we propose to solve thedual of this problem.
Introducing dual variables?
?
RI?
(K+1), ?
?
RN?
(K+1)and ?
?
RN,the dual problem is equal tomin?,?,?12tr(Z>QZ)?
tr(?>R)?
?>1s.t.
0 ?
?ik?
?, 0 ?
?nk,whereZ = E>(S ?
?)
+ ?
+ ?1>.The derivation of this dual problem is given in Ap-pendix A.Solving the dual problem instead of the primalhas two main advantages.
First, the dual does notdepend on the inverse of the matrix Q, while theprimal does.
Since traditional features used for re-lation extraction are indicators of lexical, syntacticand named entities properties of the relation men-tion candidates, the matrix X is extremely sparse.1584Using the dual problem, we can thus exploit thesparsity of the matrix X in the optimization pro-cedure.
Second, the constraints imposed on dualvariables are simpler than constraints imposed onprimal variables.
Again, we will exploit this struc-ture in the proposed optimization procedure.Given a solution of the dual problem, the asso-ciated primal variable Y is equal to:Y = (XX>+ ?IN)Z.Thus, we do not need to compute the inverse of thematrix (XX>+ ?IN) to obtain a solution to theprimal problem once we have solved the dual.5.3 Optimization of the dual problemWe propose to solve the dual problem usingthe accelerated projected gradient descent algo-rithm (Nesterov, 2007; Beck and Teboulle, 2009).Indeed, computing the gradient of the dual costfunction is efficient, since the matrix X is sparse.Moreover, the constraints on the dual variables aresimple and it is thus efficient to project onto thisset of constraints.
See Appendix B for more de-tails.Complexity.
The overall complexity of one stepof the accelerated projected gradient descent al-gorithm is O(NFK), where F is the averagenumber of features per relation mention candi-date.
This means that the complexity of solvingthe quadratic problem corresponding to our ap-proach is linear with respect to the number N ofrelation mention candidates, and thus our algo-rithm can scale to large datasets.5.4 DiscussionBefore moving to the experimental sections of thisarticle, we would like to discuss some propertiesof our approach.Kernels.
First of all, one should note that ourproposed formulation only depends on the (lin-ear) kernel matrix XXT.
It is thus possible to re-place this matrix by any other kernel.
However,in the case of a general kernel, the optimizationalgorithm presented in the previous section has aquadratic complexity O(KN2) with respect to thenumber N of relation mention candidates, and itis thus not applicable as is.
We plan to explore theuse of kernels in future work.Rounding.
Given a continuous solution Y ?
[0, 1]N?
(K+1)of the relaxed problem, a very sim-ple way to obtain a relation label for each relationmention candidate of the training set is to com-pute the orthogonal projection of the matrix Y onthe set of indicator matrices{M ?
{0, 1}N?
(K+1)|M1 = 1}.This projection consists in taking the maximumvalue along the rows of the matrix Y.
It shouldbe noted that the obtained matrix does not neces-sarily verify the inequality constraints defined inEq.
1.
In the following, we will use this rounding,refered to as argmax rounding, to obtain relationlabels for each relation mention candidate.6 Dataset and featuresIn this section, we describe the dataset used in theexperimental section and the features used to rep-resent the data.6.1 DatasetWe consider the dataset introduced by Riedel etal.
(2010).
This dataset consists of articles fromthe New York Times corpus (Sandhaus, 2008),from which named entities where extracted andtagged using the Stanford named entity recog-nizer (Finkel et al., 2005).
Consecutive tokenswith the same category were treated as a singlemention.
These named entity mentions were thenaligned with the Freebase knowledge database, byusing a string match between the mentions and thecanonical names of entities in Freebase.6.2 FeaturesWe use the features extracted by Riedel et al.
(2010), which were first introduced by Mintz etal.
(2009).
These features capture how two en-tity mentions are related in a given sentence, basedon syntactic and lexical properties.
Lexical fea-tures include: the sequence of words between thetwo entities, a window of k words before the firstentity and after the second entity, the correspond-ing part-of-speech tags, etc.. Syntactic features arebased on the dependency tree of the sentence, andinclude: the path between the two entities, neigh-bors of the two entities that do not belong to thepath.
The OpenNLP3part-of-speech tagger andthe Malt parser (Nivre et al., 2007) were used toextract those features.3opennlp.apache.org15850.00 0.05 0.10 0.15 0.20 0.25 0.30Recall0.20.30.40.50.60.70.80.9PrecisionMintz et al.
(2009)Hoffmann et al.
(2011)Surdeanu et al.
(2012)This workFigure 3: Precision/recall curves for different methods on the Riedel et al.
(2010) dataset, for the task ofaggregate extraction.6.3 Implementation detailsIn this section, we discuss some important imple-mentation details.Kernel normalization.
We normalized the ker-nel matrix XX>, so that its diagonal coefficientsare equal to 1.
This corresponds to normalizingthe vectors xnso that they have a unit `2-norm.Choice of parameters.
We kept 20% of the ex-amples from the training set as a validation set, inorder to choose the parameters of our method.
Wethen re-train a model on the whole training set, us-ing the chosen parameters.7 Experimental evaluationIn this section, we evaluate our approach to weaklysupervised relation extraction by comparing it tostate-of-the art methods.7.1 BaselinesWe now briefly present the different methods wecompare to.Mintz et al.
This baseline corresponds to themethod described by Mintz et al.
(2009).
Weuse the implementation of Surdeanu et al.
(2012),which slightly differs from the original method:each relation mention candidate is treated inde-pendently (and not collapsed across mentions fora given entity pair).
This strategy allows to predictmultiple labels for a given entity pair, by OR-ingthe predictions for the different mentions.Hoffmann et al.
This method, introduced byHoffmann et al.
(2011), is based on probabilis-tic graphical model of multi-instance multi-labellearning.
They proposed a learning methodfor this model, based on the perceptron algo-rithm (Collins, 2002) and a greedy search for theinference.
We use the publicly available code ofHoffmann et al.4.Surdeanu et al.
Finally, we compare ourmethod to the one described by Surdeanu et al.(2012).
This method is based on a two-layergraphical model, the first layer corresponding to4www.cs.washington.edu/ai/raphaelh/mr/15860.0 0.1 0.2 0.3 0.4 0.5 0.6Recall0.00.20.40.60.81.0Precision/location/location/contains/people/person/place_lived/person/person/nationality/people/person/place_of_birth/business/person/companyFigure 4: Precision/recall curves per relation for our method, on the Riedel et al.
(2010) dataset, for thetask of aggregate extraction.a relation classifier at the mention level, while thesecond layer is aggregating the different predic-tion for a given entity pair.
In particular, this sec-ond layer capture dependencies between relationlabels, such as the fact that two labels cannot begenerated jointly (e.g.
the relations SpouseOf andBornIn).
This model is trained by using harddiscriminative Expectation-Maximization.
We usethe publicly available code of Surdeanu et al.5.7.2 Precision / recall curvesFollowing standard practices in relation extrac-tion, we report precision/recall curves for the dif-ferent models.
In order to rank aggregate extrac-tions for our model, the score of an extracted factr(e1, e2) is set to the maximal score of the differ-ent extractions of that fact.
This is sometimes ref-ered to as the soft-OR function.7.3 DiscussionComparison with the state-of-the-art.
We re-port results for the different methods on the dataset5nlp.stanford.edu/software/mimlre.shtmlintroduced by Riedel et al.
(2010) in Fig.
3.
Weobserve that our approach generally outperformsthe state of the art.
Indeed, at equivalent recall,our method achieves better (or similar) precisionthan the other methods, except for very low re-call (smaller than 0.05).
The improvement overthe methods proposed by Hoffmann et al.
(2011)and Surdeanu et al.
(2012), which are currentlythe best published results on this dataset, can beas high as 5 points in precision for the same recallpoint.
Moreover, our method achieves a higher re-call (0.30) than these two methods (0.25).Performance per relation.
The dataset in-troduced by Riedel et al.
(2010) is highlyunbalanced: for example, the most commonrelation, /location/location/contains, rep-resents almost half of the positive relations, whilesome relations are mentioned less than ten times.We thus decided to also report precision/recallcurves for the five most common relations ofthat dataset in Fig.
4.
First, we observe that theperfomances vary a lot from a relation to another.The frequence of the different relations is not the15870.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Recall0.40.50.60.70.80.91.0PrecisionHoffmann et al.
(2011)This workFigure 5: Precision/recall curves for the taskof sentential extraction, on the manually labeleddataset of Hoffmann et al.
(2011).only factor in those discrepancies.
Indeed, therelation /people/person/place lived and therelation /people/person/place of birthare more frequent than the relation/business/person/company, but the ex-traction of the later works much better than theextraction of the two first.Upon examination of the data, this canpartly be explained by the fact that al-most no sentences extracted for the relation/people/person/place of birth in factexpress this relation.
In other words, manyfacts present in Freebase are not expressed inthe corpus, and are thus impossible to extract.On the other hand, most facts for the relation/people/person/place lived are missing inFreebase.
Therefore, many extractions producedby our system are considered false, but are infact true positives.
The problem of incompleteknowledge base was studied by Min et al.
(2013).Sentential extraction.
We finally report preci-sion/recall curves for the task of sentential extrac-tion, in Fig.
5, using the manually labeled datasetof Hoffmann et al.
(2011).
We observe that formost values of recall, our method achieves simi-lar precision that the one proposed by Hoffmannet al.
(2011), while extending the highest recallfrom 0.52 to 0.68.
Thanks to this higher recall, ourmethod achieves a highest F1 score of 0.66, com-pared to 0.61 obtained by the method proposed byHoffmann et al.
(2011).Method RuntimeMintz et al.
(2009) 7 minHoffmann et al.
(2011) 2 minSurdeanu et al.
(2012) 3 hoursThis work 3 hoursTable 1: Comparison of running times for the dif-ferent methods compared in the experimental sec-tion.8 ConclusionIn this article, we introduced a new formulationfor weakly supervised relation extraction.
Ourmethod is based on a constrained discriminativeformulation of the multiple instance, multiple la-bel learning problem.
Using the squared loss,we obtained a convex relaxation of this formula-tion, allowing us to obtain an approximate solu-tion to the initial integer quadratic program.
Thus,our method is not sensitive to initialization.
Wedemonstrated the competitiveness of our approachon the dataset introduced by Riedel et al.
(2010),on which our method outperforms the state of theart methods for weakly supervised relation extrac-tion, on both aggregate and sentential extraction.As noted earlier, another advantage of ourmethod is the fact that it is easily kernelizable.We would like to explore the use of kernels, suchas the ones introduced by Zelenko et al.
(2003),Culotta and Sorensen (2004) and Bunescu andMooney (2005), in future work.
We believe thatsuch kernels could improve the relatively low re-call obtained so far by weakly supervised methodfor relation extraction.AcknowledgmentsThe author is supported by a grant from Inria(Associated-team STATWEB) and would like tothank Armand Joulin for helpful discussions.ReferencesFrancis Bach and Za?
?d Harchaoui.
2007.
DIFFRAC: adiscriminative and flexible framework for clustering.In Adv.
NIPS.Michele Banko, Michael J Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction for the web.
In IJCAI.Amir Beck and Marc Teboulle.
2009.
A fast iterativeshrinkage-thresholding algorithm for linear inverseproblems.
SIAM Journal on Imaging Sciences, 2(1).1588Kedar Bellare and Andrew McCallum.
2007.
Learn-ing extractors from unlabeled text using relevantdatabases.
In Sixth international workshop on in-formation integration on the web.Piotr Bojanowski, Francis Bach, Ivan Laptev, JeanPonce, Cordelia Schmid, and Josef Sivic.
2013.Finding actors and actions in movies.
In Proceed-ings of ICCV.Razvan Bunescu and Raymond Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proceedings of HLT-EMNLP.Razvan Bunescu and Raymond Mooney.
2007.
Learn-ing to extract relations from the web using minimalsupervision.
In Proceedings of the ACL.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In ISMB, volume 1999.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the ACL.Thomas G Dietterich, Richard H Lathrop, and Tom?asLozano-P?erez.
1997.
Solving the multiple instanceproblem with axis-parallel rectangles.
Artificial in-telligence, 89(1).Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the ACL.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the ACL.Armand Joulin, Jean Ponce, and Francis Bach.
2010.Efficient optimization for discriminative latent classmodels.
In Adv.
NIPS.Nanda Kambhatla.
2004.
Combining lexical, syntac-tic, and semantic features with maximum entropymodels for information extraction.
In Proceedingsof the ACL.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone, andRalph Weischedel.
1998.
Algorithms that learn toextract information.
In Proceedings of MUC-7.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In Proceedings of HLT-NAACL.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theACL-IJCNLP.Yurii Nesterov.
2007.
Gradient methods for minimiz-ing composite objective function.Truc-Vien T Nguyen and Alessandro Moschitti.
2011.End-to-end relation extraction using distant super-vision from external semantic repositories.
In Pro-ceedings of the ACL.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02).Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Machine Learning and Knowl-edge Discovery in Databases.Evan Sandhaus.
2008.
The new york times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,6(12).Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of the HLT-NAACL.Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.2011.
New york university 2011 system for kbp slotfilling.
In Proceedings of the Text Analytics Confer-ence.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of EMNLP-CoNLL.Fei Wu and Daniel S Weld.
2007.
Autonomouslysemantifying wikipedia.
In Proceedings of the six-teenth ACM conference on Conference on informa-tion and knowledge management.Linli Xu, James Neufeld, Bryce Larson, and DaleSchuurmans.
2004.
Maximum margin clustering.In Adv.
NIPS.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
The Journal of Machine Learning Re-search, 3.1589Appendix A Derivation of the dualIn this section, we derive the dual problem of thequadratic program of section 5.
We introduce dualvariables ?
?
RI?
(K+1), ?
?
RN?(K+1),?
?
RI?
(K+1)and ?
?
RN, such that ?
?
0,?
?
0 and ?
?
0.The Lagrangian of the problem is12tr(Y>(XX>+ ?IN)?1Y)+ ??i,k?ik?
tr(?>((EY) ?
S?R + ?))?
tr(?>Y)?
tr(?>?)?
?>(Y1?
1).To find the dual function g we minimize the La-grangian over Y and ?.
Minimizing over ?, wefind that the dual function is equal to ??
unless???ik?
?ik= 0, in which case, we are left with12tr(Y>(XX>+ ?IN)?1Y)?
tr((?
?
S)>EY)?
tr(?>Y)?
tr(1?>Y)+ tr(?>R) + ?>1.Minimizing over Y, we then obtainY = (XX>+ ?IN)(E>(S ?
?)
+ ?
+ ?1>).Replacing Y by its optimal value, we then obtainthe dual function?12tr(Z>QZ)+ tr(?>R)+ ?>1.whereQ = (XX>+ ?IN),Z = E>(S ?
?)
+ ?
+ ?1>.Thus, the dual problem ismax?,?,?
?12tr(Z>QZ)+ tr(?>R)+ ?>1s.t.
0 ?
?ik, 0 ?
?nk, 0 ?
?ik,??
?ik?
?ik= 0.We can then eliminate the dual variable ?, sincethe constraints ?ik= ?
?
?ikand ?ik?
0 areequivalent to ?
?
?ik.
We finally obtainmax?,?,?
?12tr(Z>QZ)+ tr(?>R)+ ?>1s.t.
0 ?
?ik?
?, 0 ?
?nk.Appendix B Optimization detailsGradient of the dual cost function.
The gradi-ent of the dual cost function f with respect to thedual variables ?, ?
and ?
is equal to?
?f = (XX>+ ?IN)Z,?
?f =((XX>+ ?IN)ZE>)?
S?R,?
?f = (XX>+ ?IN)Z1?
1.The most expensive step to compute those gra-dients is to compute the matrix product XX>Z.Since the matrix X is sparse, we efficiently com-pute this product by first computing the productX>Z, and then by left multiplying the result byX.
The complexity of these two operations isO(NFK), where F is the average number of fea-tures per relation mention candidate.Projecting ?
and ?.
The componentwise pro-jection operators associated to the constraints on?
and ?
are defined by:proj?
(?nk) = max(0,?nk),proj?
(?ik) = max(0,min(?,?ik)).The complexity of projecting ?
and ?
is O(NK).Thus, the cost of those operations is ne gligiblecompared to the cost of computing the gradientsof the dual cost function.1590
