A S IMPLE  PROBABIL IST IC  APPROACH TOCLASS IF ICAT ION AND ROUTINGLou ise  Guthr ie/James Le is tensn iderLockheed Martin CorporationP.O.
Box 8048Philadelphia, PA 19101guthrie,leistens@mds.lmco.com1.
ABSTRACTSeveral classification and routing methods were im-plemented and compared.
The experiments u ed FBISdocuments from four categories, and the measures usedwere the ff.idf and Cosine similarity measures, and amaximum likelihood estimate based on ass~lming aMultinomial Distribution for the various topics (popula-tions).
In addition, the SMART program was run with'lnc.ltc' weighting and compared to the others.Decisions for both our classification scheme (docu-ments are put into any number of disjoint categories)and our routing scheme (documents are assigned a'score' and ranked relative to each category) are basedon the highest probability for correct classification orrouting.
All of the techniques described here are fullyautomatic, and use a training set of relevant documentsto produce lists of distin~i~hin?
terms and weights.
Allmethods (ours and the ones we compared to) gave excel-lent results for the classification task, while the onebased on the Multinomial Distribution produced thebest results on the routing task.2.
INTRODUCTIONOne of the goals of the TIPSTER Phase H ExtractionProject \[Contract Number 94-F133200-000\] has beento integrate xtraction and detection technologies.
Inthis paper we extend previous work (Guthde, et al \[1\]on classifying texts into categories, and develop ameth-odology based on the classification technique for rout-ing documents.By classifying and routing texts into categories wemean to include a variety of applications; categorizingtexts by topic, by the language the text is written in, orby relevance to a specified task.
The techniques usedhere are not language specific and can be applied to anylanguage or domain.2.1.
The Intuitive ModelThe mathematical model we use in this paper for-maliTes the intuitive notion that humans can identify thetopic of an UlffamilJar article based on the occurrence oftopic specific words and phrases.
Note that most peoplecan tell that he first passage below is about music, eventhough the word 'music' is not in the passage.
Similarly,most people can tell that the second passage is from asports article, even though the word 'sport' is nevermentioned.
"Before the release of his last studio album, 1993"s'Ten Summoner's Tales', Sting commented that he couldno longer put his whole heart into his work; it left himfeeling too vulnerable.
Not surprisingly, that disc waswell-crafted, but a bit void of feeling--unfortunate,considering the wondrous ynergy of heart and craft onSting's masterwork, 1987's 'Nothing Like the Sun'.Sadly, 'Mercury Falling' makes 'Ten Summoner's Tales'seem brilliant by comparison, l f  s as if Sting only madeit because he looked at his calendar one day and real-ized, by golly, that it was time to make another ecord.Easily the worst album of what has until now been a re-markably successful career, the disc is aptly named: thetemperature never seems to rise on this turgid effort.
"I21"Walter McCarty scored 24 points and AntoineWalker had 14 and nine rebounds as Kentucky pulledaway in the second half to beat upstart San Jose State,110-72, in the first round of the Midwest Regional inDallas.The Wildcats (28-3), who are seeking their first na-tional championship since 1978, will meet he winner ofthe Wisconsin-Green Bay-Virginia Tech game on Satur-day at Reunion Arena.San Jose State, which was making its first NCAATournament appearance, gave Kentucky all it couldhandle in the first half, tying the game at 37-37 with2:50 to play.
The Wildcats then closed out the first half1 67with an 11-4 run to build a 47-41 advantage at the inter-mission.Olivier Saint-Jean finished with 18 points and sevenrebounds for the Spartans (13-17), who were one of twoteams in the NCAA Tournament with a losing record.
"IS1The music passage has many music related wordssuch as 'studio', 'album', 'disc', and 'record', and thesports passage has many sports related words such as'scored', 'beat', 'championship', game', and 're-bounds'.
Any of these words taken singly would notnecessarily give a strong indication about the passagetopic, but taken together they can predict with a high de-gree of certainty the topic of the passage.2.2.
The Mathematical ModelThe mathematical model used here is to representeach category as a multinomial distribution.
Parametersare estimated from the frequency of certain sets of wordsand phrases (the'distinguishing word sets') found in thetraining collections.Previous results (Guthrie t al 1994) indicate that hesimple statistical technique of the maximum likelihoodratio test would, under certain conditions, give rise to anexcellent classification scheme for documents.
Pre-vious theoretical results were verified using two classesof documents, and excellent recall and precision scoreswere achieved for distinguishing topics (previous testswere conducted in both Japanese and English).
In thispaper we both extend the classification scheme to in-clude any number of topics and modify the scheme toalso perform routing.In modeling a class of text, our technique requiresthat we identify a set of key concepts, or distinguishingwords and phrases.
The intuition is given in the exampleabove, but in this work we want to automate the processof choosing word sets in a way that results in sets of 'dis-tinguishing concepts'.In (Guthrie t al 1994), it was shown that if the prob-abilities of the distinguishing word sets in each of theclasses is known, we can predict he probability of cor-rect classification.
Our goal eventually is to define analgorithm for choosing 'distinmlishing word sets' in anoptimal way; i.e.
a way that will maximize the probabil-ity of correct classification.
The method we use now(described in section 4.1.)
is empirical, but allows us toguarantee excellent classification results.2.3.
Common ApproachesSchemes for classification and routing all teild tofollow a particular paradigm:1.
Represent each class (or topic or profile orbucket) as a numerical object.2.
Represent each new document that arrives asa numerical object.3.
Measure the 'similarity' between the newdocument and each of the classes.4.
For Classification - Place the new documentin the category corresponding to the class (orbucket or prc~'fle) to which it is most similar.For Routing - Rank the document in theclass using some function of the similaritymeasure.Althon?h many similarity measures have been stu-died, two of them seem to have gained popularity in therecent literature: the Cosine and tf.idf measures.
TheCosine measure isused when a document isrepresentedas a multi-dimensional vector, and a document is de-freed as more similar to Class 1 than Class 2 if its corre-sponding vector is closer to that of Class 1 than to thatof Class 2.
In ff.idf a document ismore similar to Class1 than Class 2 if more terms match the Class 1 terms thando the Class 2 terms.
In our work a document is moresimilar to Class 1 than Class 2 if the probability of  it be-longing to Class 1 is greater than the probability of it be-longing to Class 2.In choosing arepresentation f a class or a represen-tation of a document, much of the current research inclassification and routing is focused on choosing thebest set of terms (in our case, we call them Distinguish-ing Terms) to represent i .
Many systems tart withprevalent but not common (so that words such as 'the'and 'to' are not used) words and phrases in the classtraining set.
The training set may be as small as the ini-tial query which defined the class or as large as all of thedocuments which are available which are deemed to berelevant to the class.
If this set of terms is too small,feedback isgenerally employed in which the full corpusof documents o be classified and routed is compared tothe set, prevalent words and phrases from highly rankedretrieved ocuments are added to the set, and the fullcorpus is run again against the larger set of terms.2.4.
Probabilistic Classification ApproachUsing Multinomial DistributionA probabilistic method for classification was pro-posed by Guthrle and Walker \[1\], which assumed eachclass was distributed by the multinomial distribution.Elementary statistics tells us that a maximum likelihoodratio test is the best way to calculate the probability thata set of outcomes was produced by a given input.
In theexample below, we assume a multinomial distributionfor our dice and fred the largest conditional probabilityof getting acertain output given a certain input.
For ex-168ample, consider the set of outcomes produced by rollingone of two single six-sided ice.
One of the dice is fairand one is loaded to be more likely to give a '6' out-come.
Let us assign the expected probabilities for theoutcomes for each of the two dice.DieFairLoadedOutcome1 2 3 4 5 6Probability1/6 1/6 1/6 1/6 1/6 1/61/10 1/10 1/10 1/10 1/10 1/2Table 2.3-1.
Expected ProbabilitiesNow let us defme three sets of outputs.Outputset 1set 2set 3Outcome1 2 3 4 5 6Count5 4 4 6 5 42 3 1 2 4 103 4 2 5 4 8Table 2.3-2.
OutputsUsing the multinomiai distribution, we may calcu-late which is the more likely die to have produced eachof the outputs.
The multinomial equation is shown be-low, for the case of 6 possible outcomes.p=n!nl\[ n2!
n3!
n4!
ns!
n6\[I nl n2 n3 n4 n5 n61pl P2 P3 P4 P5 P6 .JUsing the probabilities assigned to each die for Plthrough P6, and the number of times each outcome oc-curred for nl through n6, and the total number of out-comes for n, the following probabilities of producingeach output given that a particular die was used are cal-culated.Output Fair Die Loaded Dieset 1 3.46 x 10 -4 1.33 x 10- 7set 2 4.09 x 10- 6 5.25 x 10- 4set 3 7.07 x 10- s 4.71 x 10- 5Table 2.3-3.
Probability of OutputThe most likely die to produce ach output is the onewith the maximum probability.
We can see that theseprobabilities are an excellent measure for determiningwhich of the dice was more likely to be used to generateeach of the sets of outcomes.
Set 1, which has a fairlyuniform distribution, is much more likely to have beencreated with the fair die than the loaded one.
Set 2,which has nearly half of the outcomes as '6', is muchmore likely to have been created with the loaded diethan the fair one.
Set 3 does not have an obvious dis-tribution.
It has more '6' outccanes than would be ex-pected with the fair die.
but not as many as would be ex-pected with the loaded die.
As it turns out, it is justslightly more likely that he fair die was used to generateset 3.Applying this approach to the document classifica-tion problem, we may define the outcomes to be the setsof Distinguish Terms which deAr'me the classes.
The ex-pected probabilities are then the sum of the frequenciesof the Distinguishing Terms in each of the classes di-vided by the training set lengths.
The outputs are thecounts of how many of the Distinguishing Terms fromeach class are evident in a document.
Since to create amultinc~nial distribution all possible outcomes must beaccounted for, an additional count is kept of all of thewords in a document are not members of any of the Dis-finguishing Term sets.
The expected probability for thisset of words is 1.0 minus the sum of the probabilities ofall of the Distinguishing Terms in the Iraining set.2.5.
Probabi l i s t ic  Rout ing  Approach  Us-ing Mu l t inomia l  D is t r ibut ionExpa~dino?
this approach to the routing problem, wewant o fred the most likely class given the probabilitiesof the outputs.
This can be calculated with Bayes' Theo-reln, using the assumption that all classes have equallylikely occurrences.P(output I classi)P(classi I output) = P(output)Continuing the example with the fair and the loadeddie, the sets are assigned probabilities that they belongto each of the classes given the fact that hey have a cer-tain set of outcomes.
This would result in the followingprobabilities.Output Fair Die Loaded Dieset I 0.999616 0.000384set 2 0.007730 0.992270set 3 0.600170 0.388830Table 2.3-4.
Probability of ClassSorting these probabilities, we get the expected re-suits; set 1 is the output most likely to have been createdwith the fair die and set 2 the least, and set 2 is the outputmost likely to have been created with the loaded ie andset 1 the least.Comparing these routing results to the classificationresults, the question may be raised why the probabilitythat a set is from a class needs to be calculated.
Rankingwith the probability of getting the outputs (Table 2.3-3)would have given the same ranking.
But now considerthe case in which set 3 was ten times larger, as shown inthe table below.169Outputset 1set 2set 3Outcome1 2 3 4 5 6Count5 4 4 6 5 42 3 1 2 4 1030 40 20 50 40 80Table 2.3-5.
OutputsOur expectation is still that set 3 should be ranked inthe middle, between sets 1 and 2 for each die.
Calculat-ing the probabilities of getting these outputs, we get thefollowing table.Output Fair Die Loaded Dieset 1 3.46 x 10 --4 1.33 x 10 -7set 2 4.09 x 10- 6 5.25 x 10- 4set 3 1.96 x 10-16 3.39 x 10-18Table 2.3-6.
Probability of OutputUsing these probabilities directly for ranking wouldplace set 3 on the bottom of each list, which does notagree with intuition.
Note that his problem is the sameproblem that document retrieval systems have with doe-uments of varying lengths; longer documents are rankedlower than they should be.
But now we take the secondstep of calculating the probability that an output is in aclass.Output Fair Die Loaded Dieset 1 0.999616 0.000384set 2 0.007730 0.992270set 3 0.982998 0.017002Table 2.3-7.
Probability of ClassWe can see that now the rankings are as we expect;set 1 is the output most likely to have been created withthe fair die and set 2 the least, and set 2 is the output mostlikely to have been created with the loaded ie and set1 the least.
So using this multinomial distribution torank documents i  less likely to be adversely affected byvarying document lengths.3.
APPROACHBelow is a description of the different approachesimplemented for calculating the match between adocu-ment and a class profile.
The class scores are thencompared to each other to determine the classificationand routing results.3.1.
Class Scoring Techniques~.idfThe weight associated with each term in the trainingset is the log of the number of classes divided by thenumber of classes which contain the term.The class score is calculated by the following equa-tion \[2\].
This equation has been modified from the ref-erence by dividing by the sum over the class of the termweights, to normalize the results when DistinguishingTerm sets are used which have different lengths.~ (weight x ( 4 + ))- count  _1.2 2 max.
countdocumentscore  =Z weightclassCosineThe weight associated with each term in the trainingset is calculated by the following equation \[1\].weight = log number of classes with term + 1The class score is calculated by the following equa-tion \[ll.Z (weight x log(cotmt + 1))documentSCOI'e =J Z (weigh02 xZ (log(count+l))2class documentMultinomial DistributionA number of weights are associated with each termin the training set.
A weight is calculated for each of theclasses for each term, and the weight is the probabilityof the term occurrence in the class.
This is approxi-mated by taking the frequency of the term occurrence inthe training set divided by the size of the training set.The weights for all of the Distinguishing Terms in a setare combined into a single value, called the set weight.An additional weight is calculated, which is necessaryfor the multinomial distribution.
This is the probabilitythat a term is not a Distinguiqhlng Term, and is calcu-lated as 1.0 minus the sum of the probabilities of all ofthe Distinguishing Terms in the training set.
Since theclass scores calculated with this approach are exceed-ingly small, the log of the probability equation is usedto avoid computational difficulties.170The class score is calculated by the following equa-tion \[3\].score = og ( ) (ni x log(weighti))nl\[ ...Ilk\[Ilk+l\[ i l ln -- number of words in documentk = number of classesni = number of terms from the ith setnk+l = number of words which do not match any setFor routing, the score is the probability for each classcalculated given the words in the document.
This isdone with the following equafiou for each class.routing score =scoresum of all scoresSMARTThe SMART program independently calculates thescores for the Distinguishing Terms and for the docu-ment based upon the word frequencies in the entirecollection available for classif'mation and routing, andtakes the score as the sum of the products of the Distin-guisking Term and document weights.
A variety ofweighting schemes are possible, and a common oue iscalled 'lnc.ltc'.
The weight associated with each termin the Distingui.qhing Term set is calculated by the fol-lowing equation \[6\].km,o,Eweight == number of classes= number of classes with termThe class score is calculated by the following equa-tion \[6\].SCOre = ZdocumentI log (coun0 + 1)\ [~  Zclass (l?g (count) + 1)x weight\]3.2.
Classification and Routing Tech-niquesClassificationFor classification the document isclassified into theclass wMch has the maximum score.RoutingIn routing the top ranked documents for each classare returned.
For the tf.idf, Cosine, and SMART meth-ods the class score is used to rank the documents, for theMultinomial Distribution method the routing score isused.4.
IMPLEMENTATIONThe following methods were used to determine theDistingui,qhing Terms, calculate the weights associatedwith those terms, and to compare documents othe Dis-tinguLqhing Terms to get class scores and classificationand routing determinations.4.1, Selection of  Dist inguishing TermsEach class has a set of Distinguishing Terms, whichare those individual terms which occur more often in theclass than in other classes, and which can be used to dis-tinguish the class from the other classes.
The better thisset of Distinguishing Terms is, the better the results willbe for routing and classification.The Distinguishing Terms are found by processinga training set of documents which are representative ofthe class.
This training set must be of a sufficient sizeto produce good statistics of the terms in the class andthe frequencies of the terms.In each document, he header information up to theheadline is removed.
This eliminates the class andsource information which is added by the collectionagent, which would bias the word set.
The remainingwords are separated at blank spaces onto individuallines, and stemming is performed to remove mbeddedSGML syntax, possessives, punctuation, and some suf-fixes (see Appendix A).The words are then counted and sorted by frequency,and the word probability in the class is calculated by di-viding the frequency by the number of words in thetraining set.At this point he Distinguishing Terms for each classcan be chosen.
For this report, three different methodswere implemented and experimented with.1.
Use all of the words in the training set.2.
Use the high frequency words in each listwhich are not the high fiequency words inany other list, by selecting the words which171.are in the highest so many on the list and notin the highest so many on any other list.Use the high frequency words in each listwhich occur with low frequency on all of theother lists, by selecting only the words whichoccur more often in one list than in all otherlists combined, until enough words havebeen chosen.4.2.
Calculation of  Term WeightsEach of the selection methods requires a weight obe calculated for each Distinguishing Term.
The tf.idfand Cosine methods all calculate the weight using thenumber of classes which contain the term, while theMultinomial Distribution method calculates the weightusing the term probabilities.~.idfI numher  of classes 1weight = log number of classes with termCosine\[ numher of classes 1weight = log number of classes with term + 1Multinomial DistributionEach term has a weight for each class.weighhlass i = probability in class iSMARTkmweight =J Z l?g Ek~m.~class= number of classes= number of classes with term4.3.
Document  Classif icat ionEach document o he classified is processed thesame as the training sets are up to the selection of Distin-guishing Terms; the header information is removed, re-maining words are separated atblank spaces onto indi-vidual flues, and stemming is performed to removeembedded SGML syntax, possessives, punctuation, andmany suffixes.
The words are then counted and sortedby frequency.The document words are compared to each of theDistinguishing Terms sets, and a class score is calcu-lated according to the selection method being used.
Forclassification, the document is classified into the classwhich has the maximum score.For routing, the routing score is calculated from theclass scores.
Mter all of the documents have been clas-s/fled the routing scores are sorted, with the highestranking documents being those which are the most likethe class profile than any other profde.5.
EXAMPLE SELECT ION OF  D IST IN-GUISHING WORDS AND WEIGHTSTo help illustrate the procedure, a small example isdescribed.
Consider two different classes, each repre-sented by a training set.
Each training set consists of asingle document.
Class 1 is 'Nursery Rhymes', repre-sented with 'Mary Had a Little Lamb', and Class 2 is'U.S.
Documents', represented with the 'The Pledge ofAllegiance'.
These documents are shown below.<article hum=l><pub>NR-96<bktype>Nursery Rhyme<hl>Mary Had A Little Lamb<txt>Mary had a little lamb whose fleece was white as snow.Everywhere that Mary went, her lamb was sure to go.<txt>It followed her to school one day, that was against the rule.It made the children laugh and play to see a lamb at school.</article>Figure 5-1.
Text of Class 1<article num=46><pub>US-96<bktype>US Document<h l>The Pledge of Allegiance<txt>I pledge allegiance to the flag of the United States of America ndto the Republic for which it stands,one Nation under God, indivisible, with liberty and justice for all.</article>Figure 5-2.
Text of Class 2Mter removing the header material, separating thewords, stemming, sorting by frequency, and calculatingthe probabilities, the following lists would result.
No-rice that the stemming does not always work perfectly;'united' is shortened to 'unite', but 'followed' is short-ened to 'foUowe'.
Overall, though, the stemming worksmuch more often than it fails.1720.07843 LAMB 0.11429 THE0.05882 WAS 0.08571 OF0.05882 TO 0.05714 TO0.05882 MARY 0.05714 PLEDGE0.05882 A 0.05714 FOR0.03922 THE 0.05714 AND0.03922 THAT 0.05714 ALLEGIANCE0.03922 SCHOOL 0.02857 WITH0.03922 L1TFI.,E 0.02857 WHICH0.03922 1T 0.02857 UNITE0.03922 HER 0.02857 UNDER0.03922 HAD 0.02857 STATES0.01961 WHOSE 0.02857 STAND0.01961 WHITE 0.02857 REPUBLIC0.01961 WENT 0.02857 ONE0.01961 SURE 0.02857 NATION0.01961 SNOW 0.02857 LIBERTY0.01961 SEE 0.02857 JUSTICE0.01961 RULE 0.02857 IT0.01961 PLAY 0.02857 INDIVISIBLE0.01961 ONE 0.02857 I0.01961 MADE 0.02857 GOD0.01961 LAUGH 0.02857 FLAG0.01961 GO 0.02857 AMERICA0.01961 FOLLOWE 0.02857 ALL0.01961 FLEECE0.01961 EVERYWHERE0.01961 DAY0.01961 CHILDREN0.01961 AT0.01961 AS0.01961 AND0.01961 AGAINSTTable 5-1.
Word ListsThe Distinguishing Terms are then chosen, by one ofthree methods.
The first is to choose all of the words ineach fist.
The second is to select he words which are inthe highest so many on each fist and not in the highestso many on the other fist.
For this example, let us choosethe words that are in the top 15 on each list and not in thetop 10 on the other fist.
This would produce the follow-ing lists.
The words 'the' and 'to' were eliminated fromeach list.0.07843 LAMB 0.08571 OF0.05882 WAS 0.05714 PLEDGE0.05882 MARY 0.05714 FOR0.05882 A 0.05714 AND0.03922 THAT 0.05714 ALLEGIANCE0.03922 SCHOOL 0.02857 WITH0.03922 LrlTLE 0.02857 WHICH0.03922 IT 0.02857 UNITE0.03922 HER 0.02857 UNDER0.039222 HAD 0.02857 STATES0.01961 WHOSE 0.02857 STAND0.01961 WHITE 0.02857 REPUBLIC0.01961 WENT 0.02857 ONETable 5-2.
Highest Ranking WordsThe third way to choose Distinguishing Terms is toselect only the words which occur more often in one listthan in all other lists combined until enough words havebeen chosen.
For this example, let us choose wordswhich occur more often in one list than in the other listuntil the sum of the probabilities of the chosen words isat least 40%.
This would produce the following fists.0.07843 LAMB 0.11429 THE0.058822 WAS 0.08571 OF0.058822 TO 0.05714 PLEDGE0.05882 MARY 0.05714 FOR0.05882 A 0.05714 AND0.03922 THAT 0.05714 ALLEGIANCE0.03922 SCHOOL0.03922 LITILETable 5-3.
Most Likely WordsThen the weight for each word is calculated.
This isdone here for each selection method for the last set ofdistinguishing words.tfidf0.69 LAMB 0.00 THE0.69 WAS 0.69 OF0,00 TO 0.69 PLEDGE0.69 MARY 0.69 FOR0.69 A 0.00 AND0.69 THAT 0.69 ALLEGIANCE0.69 SCHOOL0.69 LFFFLETable 5-4. tf.idf Weighting on Most Likely WordsCosine1.10 LAMB 0.69 THE1.10 WAS I.
10 OF0.69 TO 1.I0 PLEDGE1 .10 MARY 1.10 FOR1.10 A 0.69 AND1.10 THAT 1.10 ALLEGIANCE1.
I0 SCHOOL1 .10 UTILETable 5-5.
Cosine Weighting on Most Likely WordsMultinomial DistributionEach word has a weight for each class.0.078 0.000 LAMB 0.039 0.114 THE0.059 0.000 WAS 0.000 0.086 OF0.059 0.057 TO 0.000 0.057 PI.EDGE0.059 0.000 MARY 0.000 0.057 FOR0.059 0.000 A 0.020 0.057 AND0.039 0.000 THAT 0.000 0.057 ALLEGIANCE0.039 0.000 SCHOOL0.039 0.000 LFITLETable 5-6.
Multinomial Distribution Weighting onMost Likely WordsSMARTWeights are not kept from the training set, only thefist of words is kept.
New weights are calculated fromthe corpus of documents obe classified and routed.
Butmaking the assumption that he training set and the cor-pus have the same distribution of words, the followingweights wonld be calculated.1730.31 LAMB 0.00 THE0.31 WAS 0.42 OF0.00 TO 0.42 PLEDGE0.31 MARY 0.42 FOR0.31 A 0.00 AND0.3 !
THAT 0.42 ALLEGIANCE0.31 SCHOOL0.31 LrITLETable 5-7.
SMART Weighting on Most LikelyWords6.
TEST INGThe methods were tested against asmall set of avail-able documents.
These were FBIS documents fromJune and July of 1991 on four different topics.Number1234Topic Number of DocumentsViemam: Tap Chi Cong San 20Science and Technology / Japan 25Arms Control 57Soviet Union / Military Affairs 36Table 6-1.
Document Classes6.1.
Selection of Distinguishing TermsTen documents randomly chosen from each classwere used as training.
These training documents werethen eliminated from the set of documents o be classi-fied.
The following table shows some informationabout he training documents.Set Number of WordsShortest Longest Total1 53 ddd5 168102 181 479 31183 161 1059 54984 145 6446 18191Table 6.1-1.
Document ClassesSet 1 contained editorials from Vietnam.
Some ex-tremely short documents were included which were nolonger than the header information (which was strippedbefore use), the rifle, author and source, and a note thatthe article was in Viemamese and had not been trans-lated.
Many of the high frequency words were politicalor economic.Set 2 contained abstracts from Japanese technicalpapers.
Many of the high frequency words were techno-logical or were Japanese locations and companies.Set 3 contained articles about arms control from allover the world.
Many of the high frequency words werelocation, military, or negoriarion related.Set 4 contained articles from the Soviet Union aboutvarious military affairs, including those in other coun-tries.
Many of the high frequency words were SovietUnion locations or military related.After experimenting with the Distinguishing Termselection methods, itwas found that using the most fre-quent 300 words which were not the most frequent 300words in any other class worked best for the ff.idf meth-od.
The Cosine method worked best when the Distin-guishing Terms for each class were the words whichwere more likely to be in the class than in the sum of therest of the classes, until the sum of the probabilities ofthe chosen words was at least 20%.
The MultinomialDistribution method works best if the DistingalishlngTerms for each class are more lilfely to be in the classthan in another class, so the method which worked bestwas to choose the words which occur more often in onelist than in all other lists combined until the sum of theprobabilities of the chosen words was at least 25%.6.2.
Results for ClassificationTopics 3 and 4 had a significant overlap in distin-guiqhing words, and this created the most difficulty inchoosing the proper class.
For example, one topic 4 doc-ument described arms control efforts in France, and thiswas always misclassified as topic 3.The following charts show the classifmation preci-sion and recall for each of the classes.
The ff.idf methodgave the poorest results, while the SMART.
Cosine, andMulrinomial Distribution methods produced better e-sults.100.~ 908o7060-20MNDCOStf.idfSMT30 40 50 60 70 80RecallMultinomial DistributionCosineff.idfSMART90 100MNDCOS sm~ tf.idfFigure 6.2-1.
Set I Classification Results174100 90 I8070MNDCOS SMTtf.idf6020 30 40 50 60 70 80 90 100RecallFigure 6.2-2.
Set 2 Classification Results6.3.
Results for Rout ingThe TREC precision versus recall curves are shownbelow.80 ~ s~rr, tfAdf 60 : i?
1201009070X bI ISMT" COS20 30 40 50 60 70 80 90 100RecallFigure 6.2-3.
Set 3 Classification Results10090.1-qso70?
MNDy tf.idf0 J20 30 40 50 60 70 80 90 100RecallFigure 6.2-4.
Set 4 Classification ResultsSimplifying the charts to a single number F measure(average of precision plus recall) gives the followingcomparison.Method F measureSMART 194Multinomial Distribution 193Cosine 193tf.idf 188Table 6.2-1.
Classification F Measures0"  L0 20 40 60 80 100RecallFigure 6.3-1.
Routing ResultsSimplifying the clam to a single number measure(area under the curve) gives the following comparison.Method AreaMultinomial Distribution 983Cosine 963SMART 933tf.idf 882Table 6.3-1.
Routing Areas7.
CONCLUSIONS AND FUTUREWORKFor the small test performed, all of the methods pro-duced about he same classification result, and the MUl-tinomial Distribution method produced the best routingresult.
Future work with TREC data will determinewhether these are repeatable r sults or whether the smalltest data was particularly well tuned to the MultinomialDistribution method.Although we anticipate improvements toall of themethods through the use of phrases, feedback, term ex-pansion and clustering, these have not yet been imple-mented.
Future fforts will investigate hese modifica-tions?This test for classification and routing was muchsimpler than the TREC task, since the size of the corpuswas significantly smaller and less diverse and everydocument was relevant to a single category.
This pro-duced results which were close to perfect for all of themethods, and the Multinomial Distribution method wasless than 1% different than the SMART method in clas-175sification, and only 5% better in routing.
However.since the TREC data is very diverse and is classified intofifty classes, the Mulfinomial Distribution method is ex-pected to perform even better than the other methods, asit is particularly good at distingui~qhing fine detail be-tween classes.8.
REFERENCES.
Guthrie, L., Walker, E., and Guthrie, J.; "Docu-ment Classification By Machine: Theory andPractice".
in Proceedings ofthe 16th Intemafion-al Conference on Computational Linguistics(COLING 94); Kyoto, Japan; 1059-1063; 1994.2.
Mr. Showbiz, Starwave Corporation; 1996.3.
SportsLine, SportsTicker Enterprises L.P.; 1996.4.
Wilkenson, R., Zobel, J., and Sacks-Davis, R.;"Similarity Measures for Short Queries", in TextRetrieval Conference (TREC-4); 1995.5.
Schutze, H., and Pederson, J.; "A Cooccurrence-Based Thesaurus and Two Applications to In-formation Retrieval"; 1994.6.
SMART on-line documentation.176APPENDIX  A.
STEMMING PROCEDUREI..3..?6....Discard a word  if it is an embedded state-merit (surrounded by < and >).Change it to upper case.Scan for and remove any remaining em-bedded statements.Remove possessives.If the last character is an apostrophe, removeit.If the last two characters are 's, removethem.Remove any remainin?
punctuation.Discard the word if the previous teps haveremoved all of it.Remove 'ies'.If the last three characters are 'ies', changethem to 'y'.Remove 'ied'.If the last three characters are 'ied', changethem to 'y'.Remove plural' s'.If the last character is 's '  and the next m lastis any consonant except 's', remove the 's'.Examples: winds -> wind, pass -> pass.10.
Remove 'ing'.Do nothing if the word is 'during' or 'th' pre-cedes the'ing'.If the last three characters are'ing ', removethem.Examples: wil~ding -> wind.If the two characters prior to the 'ing' are thesame and riot's', remove the second one.Examples: stepping -> step, passing -> pass.If the character prior to the 'ing' is a conso-nant except 'y', the previous character is avowel, and the next character isnot a vowel,add an 'e' to the end of the word.Examples: mining -> mine, keying -> key,joining ->join.11.
Remove 'ed'.Do nothing if the word is four characters orless.I f  the hst two characters are 'ed', removethem.Examples: winded -> wind.If the two characters prior to the 'ed' are thesame and not 's', remove the second one.Examples: stepped -> step.
passed -> pass.If the character prior to the 'ed' is a conso-nant except 'y', the previous character is avowel, and the next character isnot a vowel,add an'e'  to the end of the word.Examples: mined -> mine, keyed -> key,joined -> join.177
