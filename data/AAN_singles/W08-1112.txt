Accurate and Robust LFG-Based Generation for ChineseYuqing GuoNCLT, School of ComputingDublin City UniversityDublin 9, Irelandyguo@computing.dcu.ieHaifeng WangToshiba (China)Research and Development CenterBeijing, 100738, Chinawanghaifeng@rdc.toshiba.com.cnJosef van GenabithNCLT, School of ComputingDublin City UniversityIBM CAS, Dublin, Irelandjosef@computing.dcu.ieAbstractWe describe three PCFG-based models forChinese sentence realisation from Lexical-Functional Grammar (LFG) f-structures.
Boththe lexicalised model and the history-basedmodel improve on the accuracy of a simplewide-coverage PCFG model by adding lexicaland contextual information to weaken inap-propriate independence assumptions implicitin the PCFG models.
In addition, we pro-vide techniques for lexical smoothing and rulesmoothing to increase the generation cover-age.
Trained on 15,663 automatically LFG f-structure annotated sentences of the Penn Chi-nese treebank and tested on 500 sentences ran-domly selected from the treebank test set, thelexicalised model achieves a BLEU score of0.7265 at 100% coverage, while the history-based model achieves a BLEU score of 0.7245also at 100% coverage.1 IntroductionSentence generation, or surface realisation can bedescribed as the problem of producing syntacti-cally, morphologically, and orthographically cor-rect sentences from a given abstract semantic /logical representation according to some linguistictheory, e.g.
Lexical Functional Grammar (LFG),Head-Driven Phrase Structure Grammar (HPSG),Combinatory Categorial Grammar (CCG), Tree Ad-joining Grammar (TAG) etc.
Grammars, such asthese, are declarative formulations of the correspon-dences between semantic and syntactic representa-tions.
Traditionally, grammar rules have been care-fully handcrafted, such as those used in LinGo (Car-roll et al, 1999), OpenCCG (White, 2004) andXLE (Crouch et al, 2007).
As handcrafting gram-mar rules is time-consuming, language-dependentand domain-specific, recent years have witnessed re-search on extracting wide-coverage grammars auto-matically from annotated corpora, for both parsingand generation.
FERGUS (Bangalore and Rambow,2000) took dependency structures as inputs, and pro-duced XTAG derivations by a stochastic tree modelautomatically acquired from an annotated corpus.Nakanishi et al (2005) presented log-linear modelsfor a chart generator using a HPSG grammar ac-quired from the Penn-II Treebank.
From the sametreebank, Cahill and van Genabith (2006) automati-cally extracted wide-coverage LFG approximationsfor a PCFG-based generation model.In addition to applying statistical techniques toautomatically acquire generation grammars, over thelast decade, there has been a lot of interest in agenerate-and-select paradigm for surface realisation.The paradigm is characterised by a separation be-tween generation and selection, in which symbolicor rule-based methods are used to generate a spaceof possible paraphrases, and statistical methods areused to select one or more outputs from the space.Starting from Langkilde (2002) who used a n-gramlanguage model to rank generated output strings, asubstantial number of traditional handcrafted sur-face realisers have been augmented with sophisti-cated stochastic rankers (Velldal and Oepen, 2005;White et al, 2007; Cahill et al, 2007).It is interesting to note that, while the study ofhow the granularity of context-free grammars (CFG)affects the performance of a parser (e.g.
in the form86n1:IP[?=?]n2:NP[?SUBJ=?]n4:NR[?=?]L?JiangZeminn3:VP[?=?]n5:VV[?=?]??interviewn6:NP[?OBJ=?]n7:NR[???ADJUNCT]IThain8:NN[?=?]onpresidentf1?????????????????????
?PRED ???
?SUBJ f2??
?PRED ?
L?
?NTYPE properNUM sg??
?OBJ f3??????????
?PRED ?on?NTYPE commonNUM sgADJUNCT?????f4??
?PRED ?I?NTYPE properNUM sg??????????????????????????????????????????
: N ?
F?(n1)=?(n3)=?
(n5)=f1 ?(n2)=?
(n4)=f2 ?(n6)=?
(n8)=f3 ?
(n7)=f4Figure 1: C- and f-structures with ?
links for the sentence ?
L??
?Ion?of grammar transforms (Johnson, 1998) and lexical-isation (Collins, 1997)) has attracted substantial at-tention, to our knowledge, there has been a lot lessresearch on this subject for surface realisation, a pro-cess that is generally regarded as the reverse pro-cess of parsing.
Moreover, while most of the re-search so far has concentrated on English or Euro-pean languages, we are also interested in generationfor other languages with diverse properties, such asChinese which is currently a focus language in pars-ing (Bikel, 2004; Cao et al, 2007).In this paper, we investigate three generativePCFG models for Chinese generation based onwide-coverage LFG grammars automatically ex-tracted from the Penn Chinese Treebank (CTB).
Ourwork is couched in the framework of Lexical Func-tional Grammar and is implemented in a chart-stylegenerator.
We briefly describe LFG and the basicgeneration model in Section 2.
We improve thebaseline PCFG model by weakening the indepen-dence assumptions in two disambiguation models inSection 3.
Section 4 describes the smoothing algo-rithms adopted for the chart generator and Section 5gives the experimental details and results.2 LFG-Based Generation2.1 Lexical Functional GrammarLexical Functional Grammar (Kaplan and Bres-nan, 1982) is a constraint-based grammar formal-ism which postulates (minimally) two levels of rep-resentation: c(onstituent)-structure and f(unctional)-structure.
C-structure takes the form of phrase struc-ture trees and captures surface grammatical config-urations.
F-structure encodes more abstract gram-matical functions (GFs) such as SUBJ(ect), OBJ(ect),ADJUNCT and TOPIC etc., in the form of hierar-chical attribute-value matrices.
C-structures andf-structures are related by a piecewise correspon-dence function ?
that goes from the nodes of a c-structure tree into units of f-structure spaces (Ka-plan, 1995).
As illustrated in Figure 1, given ac-structure node ni, the corresponding f-structurecomponent fj is ?(ni).
Admissible c-structuresare specified by a context-free grammar.
The cor-responding f-structures are derived from functionalannotations attached to the CFG rewriting rules.
(1) a. IP ??
NP VP[?SUBJ=?]
[?=?]b.
VP ??
VV NP[?=?]
[?OBJ=?]c.
NP ??
NR NN[?ADJ=?]
[?=?]d.
NP ??
NR[?=?
](1) shows a miniature set of annotated CFG rules(lexical entries omitted) which generates the c- andf-structure in Figure 1.
In the functional annotations,(?)
refers to the f-structure associated with the localc-structure node ni, i.e.
?
(ni), and (?)
refers to the87Model Grammar Rule ConditionsPCFG VP[?=?]
?
VV[?=?]
NP[?OBJ=?]
VP[?=?
], {PRED, SUBJ, OBJ}HB-PCFG VP[?=?]
?
VV[?=?]
NP[?OBJ=?]
VP[?=?
], {PRED, SUBJ, OBJ}, TOPLEX-PCFG VP(??)[?=?]
?
VV(??)[?=?]
NP(on)[?OBJ=?]
VP(??)[?=?
], {PRED, SUBJ, OBJ}Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different modelsf-structure associated with the mother (M ) node ofni, i.e.
?
(M(ni)).2.2 Generation from f-StructuresThe generation task in LFG is to determine whichsentences correspond to a specified f-structure,given a particular grammar, such as (1).
Kaplanand Wedekind (2000) proved that the set of stringsgenerated by an LFG grammar from fully speci-fied f-structures is a context-free language.
Basedon this theoretical cornerstone, Cahill and van Gen-abith (2006) presented a PCFG-based chart genera-tor using wide-coverage LFG approximations auto-matically extracted from the Penn-II treebank.
TheLFG-based statistical generation model defines theconditional probability P (T |F ), for each candidatefunctionally annotated c-structure tree T (whichfully specifies a surface realisation) given an f-structure F .
The generation model searches for theTbest that maximises P (T |F ) (Eq.
1).
P (T |F ) isthen decomposed as the product of the probabilitiesof all the functionally annotated CFG rewriting rulesX ?
Y (conditioned on the left hand side (LHS) Xand local features of the corresponding f-structure?
(X)) contributing to the tree T (Eq.
2).
The firstline (PCFG) of Table 1 shows the f-structure anno-tated CFG rule to expand node n3 in Figure 1.Tbest = argmaxTP (T |F ) (1)P (T |F ) =?X ?
Y in TFeats = {ai|ai ?
?
(X)}P (X ?
Y |X,Feats) (2)3 Disambiguation ModelsThe basic generation model presented in (Cahilland van Genabith, 2006) used simple probabilis-tic context-free grammars.
However, the indepen-dence assumptions implicit in PCFG models maynot be appropriate to best capture natural languagephenomena.
Methodologies such as lexicalisa-tion (Collins, 1997; Charniak, 2000) and tree trans-formations (Johnson, 1998), weaken the indepen-dence assumptions and have been applied success-fully to parsing and shown significant improvementsover simple PCFGs.
In this section we study the ef-fect of such methods in LFG-based generation forChinese.3.1 A History-Based ModelThe history-based (HB) approach which incorpo-rates more context information has worked wellin parsing (Collins, 1997; Charniak, 2000).
Re-sembling history-based models for parsing, Hoganet al (2007) presented a history-based generationmodel to overcome some of the inappropriate inde-pendence assumptions in the basic generation modelof (Cahill and van Genabith, 2006).
The history-based model increases the context by simply includ-ing the parent grammatical function GF of the f-structure in addition to the local ?-linked feature setin the conditioning context (Eq.
3).
The f-structureannotated CFG rule expanding n3 in the history-based model is shown in the second line (HB-PCFG)of Table 1.1P (T |F ) =?X ?
Y in TFeats = {ai|ai ?
?
(X)}?f (f GF) = ?
(X)P (X ?
Y |X,Feats,GF) (3)The history-based model is motivated by Englishdata, for example, to generate the appropriate casefor pronouns in subject position and object position,respectively.
Though Chinese does not distinguishcases, we expect the f-structure parent GF to helppredict grammar rule expansions more accurately inthe tree derivation than the simple PCFG model.
Wewill investigate how the HB model performs whilemigrating it from English to Chinese data.1The parent grammatical function of the outermost f-structure is assumed to be a dummy GF TOP.883.2 A Lexicalised ModelCompared to the HB model which includes the par-ent grammatical function in the conditioning con-text, lexicalised grammar rules contain more fine-grained categorial information.
To the best of ourknowledge, lexicalised parsers (Bikel, 2004) outper-form unlexicalised parsers for Chinese.
The expec-tation is that a lexicalised PCFG model also worksbetter than a simple PCFG model in Chinese gen-eration, considering e.g.
prepositional phrase (PP)modification in Chinese.
Some prepositions indicat-ing directions can occur either before or after themain verbs, for instance both (2a) and (2b) are ac-ceptable in Chinese.
However, most PP modifiersonly act as adverbial adjuncts between the subjectsand verbal predicates.
For instance ??/to?
neverfollows a verb as exemplified in the ungrammaticalsentence (3b).
(2) a. ?
4 ?m  ?this CLS train run to Beijing?The train is bound for Beijing.?b.
?
4 ?
?
mthis CLS train to Beijing run(3) a.
Ion ?
?I ?1 ?
?Thai president to China make visit?The Thai president paid a visit to China.?b.
*Ion ?1 ???
?IThai president make visit to ChinaIn order to model phenomena such as these, wehead-lexicalise our grammar by associating eachnon-terminal node with the head word2 in the c-structure tree along the head-projection line.
A non-terminal node is written as X(x), where x is the lex-ical head of X.
The example generation grammarrule in the lexicalised model is shown in the last line(LEX-PCFG) of Table 1.As in CKY chart parsing, generation grammarsare binarised in our chart generator.
Thus all gram-mar rules are either unary of the form X ?
H orbinary X ?
Y H (or X ?
HY ), where H is thehead constituent and Y is the modifier.
To handle theproblem of sparse data while estimating rule proba-bilities, a back-off to baseline model is employed.As, from a linguistic perspective, it is the modifier2We use a mechanism similar to (Collins, 1997) but adaptedto Chinese data to find lexical heads in the treebank data.rather than the head word which plays the main rolein determining word order, a back-off to partial lexi-calisation on the modifier only is also used for bi-nary rules.
As a result, the probabilities of lexi-calised unary and binary CFG rules are calculatedas in Eq.
(4) and Eq.
(5), respectively.Pbk(H(h)|X(h)) = ?1P (H(h)|X(h))+?2P (H |X) (4)Pbk(Y (y)H(h)|X(h)) = ?1P (Y (y)H(h)|X(h))+?2P (Y (y)H |X) + ?3P (Y H |X) (5)where?i=1?i = 1In principle, grammars binarisation from left-to-right (left-) or from right-to-left (right-) are equiva-lent to represent the original grammar and the prob-ability distributions.
However the head word is thefinal constituent for most phrasal categories in Chi-nese.3 In lexicalised model, the head word imme-diately projects to the top level in a left-binary tree,and as a result, the intermediate NP nodes cannotbe lexicalised with the head word as illustrated inFigure (2b).
By contrast, right-binary rules are lex-icalised and the head word is percolated from thebottom of the tree (Figure (2c)).
Therefore we adoptthe right binarisation method in our generation algo-rithm.4 Chart Generation and SmoothingAlgorithms4.1 Chart Generation AlgorithmThe PCFG-based generation algorithms are imple-mented in terms of a chart generator (Kay, 1996).In the generation algorithm, each (sub-)f-structureindexes a (sub-)chart.
Each local chart generatesthe most probable trees for the local f-structure ina bottom-up manner:?
generating lexical edges from the the local GFPRED and some atomic features representingfunction words, mood or aspect etc.3Except for prepositional phrases, localiser and some verbalphrases.89NP(m)NR[???ADJUNCT]??ShanghaiNN[???ADJUNCT]?tennisNN[???ADJUNCT]??mastersNN[?=?]mcup(a.)
the original treeNP(m)NP(null)[?=?]NP(null)[?=?]NR[???ADJUNCT]??ShanghaiNN[???ADJUNCT]?tennisNN[???ADJUNCT]??mastersNN[?=?]mcupNP(m)NR[???ADJUNCT]??ShanghaiNP(m)[?=?]NN[???ADJUNCT]?tennisNP(m)[?=?]NN[???ADJUNCT]??mastersNN[?=?]mcup(b.)
left-binarisation (c.) right-binarisationFigure 2: Lexicalised binary trees?
applying unary rules and binary rules to gener-ate new edges until no any new edges can begenerated in the current local chart.?
propagating compatible edges to the upper-level chart.For efficiency, the generation algorithm doesViterbi-pruning for each local chart, viz.
if twoedges have equivalent categories and lexical cover-age, only the most probable one is kept.The generation coverage is impacted on by un-known words4 and unmatched grammar rules inchart generation.
We present a lexical smoothingand a rule smoothing strategy in the following sub-sections.4.2 Lexical SmoothingIn LFG f-structure, the surface form of the lemmais represented via lexical rules involving a particularset of features, e.g.
the lemma ?on /president?
isrepresented as {?PRED=?on ?, ?NTYPE=common,?NUM=sg}.
Particular lexical rules can be cap-tured in general lexical macros abstracting away4We use unknown words as a cover term to refer to all wordsoccurring in the test set but not in the training set.from particular surface forms to lemmas, e.g.
thelexical macro encapsuling the above lexical rule is{?PRED=$LEMMA, ?NTYPE=common, ?NUM=sg},which generally associates to common nouns NN inthe CTB.
According to the assumption that unknownwords have a probability distribution similar to ha-pax legomenon (Baayen and Sproat, 1996), we pre-dict the part-of-speech of unknown words from in-frequent words in the training set by automaticallyextracting lexical macros corresponding to the par-ticular set of f-structure features.
The probability ofthe potential POS tag t associated to a feature set fis estimated according to Eq.
(6).P (t|f) = count(t, f)?ni=1 count(ti, f)(6)4.3 Rule SmoothingThe coverage of grammar rules increases with thesize of training data and in theory all the rules canbe fully covered by a training set, if it is big enough.With limited training resources we have to resort tofuzzy matching of grammar rules.
Two smoothingstrategies are carried out at the level of grammarrules.90Mathched Grammar RuleNonsmooth VP[?=?]
?
VV[?=?]
NP[?OBJ=?
], {SUBJ, OBJ, PRED}Feature smooth VP[?=?]
?
VV[?=?]
NP[?OBJ=?
]Partial match VP ?
VV [?OBJ=?
], {SUBJ, OBJ, PRED}Table 2: Smoothing of CFG rules?
Reducing the conditioning f-structure featuresduring rule matching;?
Applying partial match during rule application.A node in each unlexicalised grammar rule X ?Y H includes two parts: constituent category c, suchas IP, NP, VP etc.
; functional f-structure annotationa, such as [?SUBJ=?
], [?=?]
etc.
As a heuristic basedon linguistic experience, we define the order of im-portance of these elements as follows:X(c) > H(c) > Y (a) > Y (c) > X(a) > H(a)(4) IP[?COMP=?]
?
NP[?SUBJ=?]
VP[?=?
]For the above example rule (4), the importance ofthe elements is:IP > VP > [?SUBJ=?]
> NP > [?COMP=?]
> [?=?
]The elements can be deleted from the rules in an im-portance order from low to high.5 The partial rulesadopted in our system ignore the least important 3elements, viz.
the functional annotation of the headnode H(a), the functional annotation on LHS X(a)and constituent category of the modifier node Y (c).Examples of the two types of smoothed rules areshown in Table 2.5 Experimental ResultsOur experiments are carried out on the newlyreleased Penn Chinese treebank version 6.0(CTB6) (Xue et al, 2005), excluding the portion ofACE broadcast news.
We follow the recommendedsplits (in the list-of-file of CTB6) to divide thedata into test set, development set and training set.The training set includes 756 files with a total of15,663 sentences.
The CTB trees of the training setwere automatically annotated with LFG f-structureequations following (Guo et al, 2007).
Table 3shows the number of different grammar rule typesextracted from the training set.
From the test files,5However c and a on the same node can?t be deleted at thesame time.we randomly select 500 sentences as test datawith minimal sentence length 5 words, maximallength 80 words, and average length 28.84 words.The development set alo includes 500 sentencesrandomly selected from the development files withsentence length between 5 and 80 words.
Thec-structure trees of the test and development datawere also automatically converted to f-structures asinput to the generator.Type with features without featuresPCFG 22,372 8,548HB-PCFG 28,487 11,969LEX-PCFG 325,094 286,468Table 3: Number of rules in the training setThe generation system is evaluated against theraw text of the test data in terms of accuracy and cov-erage.
Following (Langkilde, 2002) and other workon general-purpose generators, we adopt BLEUscore (Papineni et al, 2002), average simple stringaccuracy (SSA) and percentage of exactly matchedsentences for accuracy evaluation.6 For coverageevaluation, we measure the percentage of input f-structures that generate a sentence.Table 4 reports the initial experiments on the sim-ple PCFG, HB-based PCFG and lexicalised PCFGmodels.
The results in the left column evaluate allinput f-structures, the right column evaluate onlythose f-structures which yield a complete sentence.The results show that the lexicalised model outper-forms the baseline PCFG model.
The HB model isthe most accurate for complete sentences, but withreduced coverage compared to the other two mod-els.
However the low coverage of sentences com-pletely generated due to unknown words and un-matched rules makes the results unusable in prac-6We are aware of the limitations in fully automatic evalua-tion metrics, and in an ideal scenario, we would complement theBLEU and SSA scores by a human evaluation.
Unfortunately,this is beyond the scope of the current paper.91All Output Strings Complete Output SentencesCoverage ExMatch BLEU SSA Coverage ExMatch BLEU SSAPCFG 100% 7.2% 0.5401 0.6261 36.40% 19.78% 0.7101 0.7687HB-PCFG 100% 8.60% 0.5474 0.6281 34.80% 24.71% 0.7513 0.8092LEX-PCFG 100% 9.40% 0.5687 0.6537 37.00% 25.41% 0.7431 0.8024Table 4: Results without smoothingAll Output Strings Complete Output SentencesCoverage ExMatch BLEU SSA Coverage ExMatch BLEU SSAPCFG 100% 11.00% 0.6894 0.7240 94.20% 11.68% 0.7047 0.7388HB-PCFG 100% 11.80% 0.7108 0.7348 94.00% 12.55% 0.7284 0.7506LEX-PCFG 100% 14.00% 0.7152 0.7595 94.40% 14.83% 0.7302 0.7754Table 5: Results with lexical smoothingPartial match Feature smoothComplete Sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSAPCFG 97.20% 11.32% 0.7022 0.7356 100% 11.20% 0.7021 0.7330HB-PCFG 96.20% 12.27% 0.7263 0.7458 100% 12.00% 0.7245 0.7413LEX-PCFG 97.80% 14.31% 0.7265 0.7696 100% 14.20% 0.7265 0.7675Table 6: Results with lexical and rule smoothingtice.Table 5 gives the results with lexical smoothing.The coverage for complete sentences increases bynearly 60% absolute for all models.
The increasedcoverage also improves the overall results evaluatedagainst all sentences.
The HB model performs betterthan the simple PCFG model in nearly all respectsand in turn the lexicalised model comprehensivelyoutperforms the HB model.The final results with both lexical smoothing andrule smoothing by two different strategies are tabu-lated in Table 6.
The left column provides the resultsof smoothing by partial match and the right columnthe results by reducing conditioning f-structure fea-tures.
All results are evaluated for completely gen-erated sentences only.
The feature smoothing re-sults in a full coverage of 100%, while slightly de-grading the quality of sentences generated comparedwith partial match smoothing.
We feel the tradeoffat the cost of a small decrease in quality is still worththe full coverage.
Throughout the experiments, thelexicalised model exhibits consistently better perfor-mance than the unlexicalised models, which provesour intuition that successful techniques in parsingalso work well in generation.6 Conclusion and Further WorkWe have presented an accurate, robust chart genera-tor for Chinese based on treebank-based, automati-cally acquired LFG resources.
Our model improvesthe baseline provided by (Cahill and van Genabith,2006): (i) accuracy is increased by creating a lexi-calised PCFG grammar and enriching conditioningcontext with parent f-structure features; and (ii) cov-erage is increased by providing lexical smoothingand fuzzy matching techniques for rule smoothing.The combinational explosion of grammar rulesencountered in the chart generator is similar to thatin parsing.
In the current system, we only keep themost probable realisation for each input f-structure.An alternative model in line with the generate-and-select paradigm, would pack all the locally equiva-lent edges in a forest and re-rank all the realisationsby a separate language model.
This might help us toreduce some errors caused in our current model, forinstance, the generation of function words in fixedphrases.
As shown in ex.
(5), the function word???
is incorrectly generated as ??.
This is be-cause they share the same part-of-speech DEG inCTB, however ??
has a much higher frequencythan ???
in Chinese text and thus has a higher prob-ability to be generated.92(5) a.
?
?
?all things DE in?among all things?b.
*?
 ?all things DE inAcknowledgmentsThe research reported in this paper is supported byScience Foundation Ireland grant 04/IN/I527.
Also,we would like to thank Aoife Cahill for many help-ful and insightful discussions on the work.
And wegratefully acknowledge the anonymous reviewers.ReferencesHarald Baayen and Richard Sproat.
1996.
Estimat-ing Lexical Priors for Low-Frequency Morphologi-cally Ambiguous Forms.
Computational Linguistics,22(2): 155?166.Srinivas Bangalore and Owen Rambow.
2000.
Ex-ploiting a Probabilistic Hierarchical Model for Gen-eration.
Proceedings of the 18th International Con-ference on Computational Linguistics, pages 42?48.Saarbru?cken, Germany.Daniel M. Bikel.
2004.
On the Parameter Space of Gen-erative Lexicalized Statistical Parsing Models.
Ph.D.Thesis of Department of Computer & Information Sci-ence, University of Pennsylvania.Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-Based Generation Using Automatically Ac-quired LFG Approximations.
Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 1033?1040.
Syd-ney, Australia.Aoife Cahill and Martin Forst and Christian Rohrer.2007.
Stochastic Realisation Ranking for a Free WordOrder Language.
Proceedings of the 11th EuropeanWorkshop on Natural Language Generation, pages17?24.
Schloss Dagstuhl, Germany.John Carroll and Ann Copestake and Dan Flickinger andVictor Poznanski.
1999.
An efficient chart genera-tor for (semi-)lexicalist grammars.
Proceedings of the7th European Workshop on Natural Language Gener-ation, pages 86?95.
Toulouse, France.Hailong Cao and Yujie Zhang and Hitoshi Isahara.
2007.Empirical study on Parsing Chinese Based on Collins?Model.
Proceedings of the 10th Conference of the Pa-cific Association for Computational Linguistics, pages113?119.
Melbourne, Australia.Eugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
Proceedings of the 1st Annual Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 132?139.
Seattle, WA.Michael Collins.
1997.
Three Generative, LexicalizedModels for Statistical Parsing.
Proceedings of the 35thAnnual Meeting of the Association for ComputationalLinguistics, pages 16?23.
Madrid, Spain.Dick Crouch and Mary Dalrymple and Ron Kaplan andTracy King and John Maxwell and Paula Newman.2007.
XLE Documentation.
Palo Alto Research Cen-ter, CA.Yuqing Guo and Josef van Genabith and Haifeng Wang.2007.
Treebank-based Acquisition of LFG Resourcesfor Chinese.
Proceedings of LFG07 Conference,pages 214?232.
Stanford, CA, USA.Deirdre Hogan and Conor Cafferkey and Aoife Cahilland Josef van Genabith.
2007.
Exploiting Multi-WordUnits in History-Based Probabilistic Generation.
Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning, pages 267?276.Prague, Czech Republic.Mark Johnson.
1998.
PCFG Models of Linguistic TreeRepresentations.
Computational Linguistics, 24(4):613?632.
MIT Press, Cambridge, MA,Ronald M. Kaplan.
1995.
The formal architecture oflexical-functional grammar.
Formal Issues in Lexical-Functional Grammar, pages 7?27.
CSLI Publications,Standford, USA.Ronald M. Kaplan and Joan Bresnan.
1982.
LexicalFunctional Grammar: a Formal System for Grammat-ical Representation.
The Mental Representation ofGrammatical Relations, pages 173-282.
MIT Press,Cambridge, MA.Ronald M. Kaplan and Jurgen Wedekind.
2000.LFG Generation Produces Context-free Languages.Proceedings of the 18th International Conferenceon Computational Linguistics, pages 425?431.Saarbru?cken, Germany.Martin Kay.
1996.
Chart Generation.
Proceedings of the34th Annual Meeting of the Association for Computa-tional Linguistics, pages 200?204.
Santa Cruz, USA.Irene Langkilde.
2000.
Forest-Based Statistical Sen-tence Generation.
Proceedings of 1st Meeting of theNorth American Chapter of the Association for Com-putational Linguistics, pages 170?177.
Seattle, WA.Langkilde, Irene.
2002.
An Empirical Verification ofCoverage and Correctness for a General-Purpose Sen-tence Generator.
Proceedings of the Second Interna-tional Conference on Natural Language Generation,17?24.
New York, USA.93Hiroko Nakanishi and Yusuke Nakanishi and Jun?ichiTsujii.
2005.
Probabilistic Models for Disambigua-tion of an HPSG-Based Chart Generator.
Proceedingsof the 9th International Workshop on Parsing Technol-ogy, pages 93?102.
Vancouver, British Columbia.Kishore Papineni and Salim Roukos and Todd Ward andWei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311-318.
Philadelphia,USA.Erik Velldal and Stephan Oepen.
2005.
Maximum en-tropy models for realization ranking.
Proceedings ofthe MTSummit ?05.Michael White.
2004.
Reining in CCG Chart Realiza-tion.
Proceedings of the third International NaturalLanguage Generation Conference.
Hampshire, UK.Michael White and Rajakrishnan Rajkumar and ScottMartin.
2007.
Towards Broad Coverage Surface Re-alization with CCG.
Proceedings of the MT SummitXI Workshop on Language Generation and MachineTranslation, pages 22?30.
Copenhagen, Danmark.Nianwen Xue and Fei Xia and Fu dong Chiou and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 11(2): 207?238.94
