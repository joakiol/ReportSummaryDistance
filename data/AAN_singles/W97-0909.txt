NLP and Industry: Transfer and Reuse of Technologies*Leo Obrst and Krishna Nanda JhaBoeing Defense & Space GroupHelicopters DivisionAdvanced Computing TechnologiesP.O.
Box 16858, MS P29-99Philadelphia, PA 19142-0858{ leo.obrst, krishna.n.jha} @boeing.comAbstractThis paper describes a useful set of NLP toolswhich has been successfully applied to manydifferent kinds of industrial requirementsspanning multiple domains and applications atBoeing.
The tools can be combined toconstitute a full-spectrum natural languagesystem and can be customized for newdomains relatively easily.
To date, this arrayof formal and natural language processingtechnologies has been used to perform masschanges to legacy textual databases and tofacilitate user interfacing to relationaldatabases and software applications.1 IntroductionIndustry has many uses for NLP technology.
Becausethe range of possible application is so varied and thepracticality constraints which industry imposessometimes quite confining, NLP components must bereusable and extendible.
This paper describes a set ofNLP tools which has been successfully applied to manydifferent requirements at Boeing.
The tools can becombined to constitute a full-spectrum natural anguagesystem and can be customized for new domainsrelatively easily.
We describe the tools and a typicalreal application which uses them.2 Example: Mass Change of Textin that the procedure must search for candidatestructured paraphrase sets (while abstracting away fromsurface noise) and then apply generation rules which arecontext dependent on the structures.
The onlyalternative solution to this problem is to inspect andchange the texts manually, a solution which is error-fraught and expensive.
The original problem can bemitigated, however, by controlling the syntax andsemantics of the text prior to populating the database byus ingan authoring tool (for example, Boeing's"simplified English" system \[17\]).In the Boeing Company, millions of manufacturingoperations texts exist in legacy databases.
These textsare used by an on-line planning system to stage themanufacturing of aircraft.
Because there are manymanufacturing process threads, with varying degrees ofchangeability, and many analysts and other personnelwho contribute to the collection of these texts, thedatabases are in constant flux and contain significantnoise.When a sequence of operations must be modified, aswhen high volatile ozone-depleting organic compoundsneed to be replaced by those having low volatility, thenall relevant texts must be retrieved, interpreted tounderstand whether they match the relevant conditionsof the mass change, and then modified according tospecified rules.
Such a textual modification processrequires robust normalization, complex patternrecognition, syntactic parsing, and semanticunderstanding of domain reference.
Furthermore,inference is required to generate new texts based uponarbitrary change criteria.2.1 The ProblemOn-line legacy databases are used daily by industry.Some of these databases consist of large amounts ofrelatively unconstrained texts constituting manufacturingplans and procedures, for example.
These textualdatabases require periodic "mass changes" to correcterrors and update procedures.
"Mass change" meansmore than a simple "global search and replace" of text,2.2 The ApplicationUsing formal language and NLP components, wecustomized.a procedure to effect he mass change of on-line textual databases for the circumscribed domain ofchemical treatment, prime, and finish operations.
Theseoperations (represented as texts) are performed on theshop floor in a precisely determined sequence,dependent on the aircraft design requirements and the* This paper has benefited greatly from discussions with Gary Coen of the ACT center at Boeing, Philadelphia.57part under construction.
The finish and rinse operationsinclude applications of anodizers, primers, overcoats,and topcoats of a variety of compounds, thicknesses, andnumbers of coats, to a range of treated or untreated partsof diverse material composition, and describe themanner in which the parts must be manipulated.
Thetexts refer to these materials and processes directly (i.e.,they name the materials and processes), indirectly (i.e.,they name documents and standards which refer to thematerials and processes), and in manners which combinedirect and indirect reference.
Various types of temporaland spatial information are present in the texts, includingduration of finish application and drying time, and thelocation of areas to be finished or protected.
Alsopresent in the text are references to other documents,color codes, and miscellanous additional operations.Though circumscribed, the semantics of this domain isrichly structured.Examples of some simple plan texts from thisdomain are displayed below (excluding database keyinformation):(1) PRIME (1) COAT ZOINC CHROMATE PRIMERPER VFI.1(2) TOUCH UP REWORK AREA ONLY APPLY (1)COAT OF BMS10-11 TYPE 1 PRIMER PER BAC5736(F18.01) REATTACH IDENTIFICATION TAGThese examples exhibit misspellings, irregularpunctuation and nomenclature, and direct, indirect, andmixed reference, which indicate the prospectiveusefulness of an NLP approach.2.3 The NLP Solution: a Process ViewBecause these are production databases and constantlyundergoing change, freezing these databases entailstemporarily removing them from production use, whichcan be a very expensive undertaking.
Hence, theautomated mass-change process must be able to runreliably in a very small window of time.
By distributingthe processing of the texts across many Unixworkstations, the time required for a typical run (rangingfrom 6500 to 130,000 texts) has been reduced toapproximately 1.5 hours, thus minimizing downtimecost.Figure 1 schematically represents the mass-changeprocess.
Initially, a subset of the on-line database'srecords are extracted and downloaded (1).
The recordsare divided into key and text portions, made unique, andnormalized (2).
The plan set is then partitioned (3)according to the type of operation and/or finish material,and these partitioned sets are distributed for subsequentprocessing across available workstations.Then, for each partition, the plans undergo spellingcorrection (4), driven by a mutual information model \[1\]constructed by prior exposure to and generalization overlarge amounts of test corpora.
This process, discussed inmore detail in the next section, feeds the NLP systemproper.
The NLP system spans the continuum fromlexical tokenization (5), including the use of the two-level morphology tool PCKIMMO \[2, 9, 8\] whichallows for a finite-state structured lexicon, throughphrase structure parsing using a hybrid syntactic-semantic grammar (6), to semantic and discourseinterpretation (8), and finally to the new plan generationstage (9).
The tokenization and grammaticalsubprocesses are implemented in the C programminglanguage.
Text strings are tokenized by employing asubsystem built around lex, a Unix lexical analysis tool\[1\].
The grammatical processing is performed by a yacc-like LR(1) parser \[1, 16\] extended to includebacktracking, inheritance, token-stream anipulation,and the use of semantic hierarchies, described in the nextsection.
The semantic hierarchies (7) are also used bythe later interpretation a d generation modules.
Most ofthe interpretation and generation modules areimplemented in Prolog because robust inference isrequired.
The semantic representations of those textswhich fit the requirements of the change rules thenundergo generation: working from the input semanticrepresentation f an individual text and the generationrule set, a new plan is generated for each appropriateoperation text.
Once all texts in every partition havebeen fully processed, resulting in multiple sets of plans,the texts are reattached to their original keys (10) andformatted (11) to various specifications (a report to beinspected by analysts, etc.
), including a database recordformat.
The set of new database records are thenuploaded to the mainframe database, and the database isagain placed into production.3 ComponentsThis section describes in more detail key components ofthe NLP tool set.
These include spelling correction,parsing, and semantic interpretation.
The discussion ofthese three modules will similarly center on the mass-change application of the previous section, withadditional comments on the interpretation componentprovided with respect o another application, that of aquery interface to a project and program schedulingsystem.
The mass change plan generation process is alsodescribed.t_,58lKey Processing, NormalizationPrelexical Analysis,LexiconMorphology.TI SpellingCorrection~__~f P r .... 1?~'~ ~ Fin al Inte rpretati?n "~~:'~!I fOcma'.i o"e'.
.... o ..... Iil.\[ FormattinguploadFigure 1.
Mass Change Process Flow3.1 Spelling CorrectionThe spelling correction process represented by node (4)in Figure 1 utilizes a statistical mutual informationmodel \[5\] to detect and correct spelling errors, based onthe observation that spelling errors are statisticallyabnormal patterns.
The intent therefore of spellingcorrection is to modit3, the word sequence minimally tomake it statistically normal.
The approach we havepursued is to use a bigram mutual information model,created by pre-processing a huge domain-specific textualcorpus (obtained perhaps, as in our case, bydownloading an entire textual database), to guidespelling correction over new text within that domain(Figure 2).
A new model is created each time the domainchanges; this is especially important if the domains arenarrowly circumscribed and company-specific.
In themass change procedure, spelling correction is applied tothe new corpus en masse at node (4).
Statisticallyunlikely words are corrected to statistically likelycandidates.In general, there are problems inherent to thedetection of spelling errors.
For example, all unknownwords encountered are not necessarily errors; they maysimply not have been seen before.
Furthermore, allknown words are not necessarily correct; these areepitomized by typographic variations and incongruousword sequences.
The mass change corpus exhibited thefollowing occurrences (with intended word bracketed tothe right):(3) a. Typographic VariationsAPPLY  2 COSTS OF EPOXY <COATS>MARK BORE AND HOLE <MASK>b.
IncongmousWordSequencesCLEAN ACRYL IC<CLEAR> FOLLOWED WITH<FOLLOW>59(Off-line Process)Figure 2.
Spelling correctionOther anomalies which a spelling correction routinemust contend with are split words (with one or morespaces intervening) and run-on words (where no spaceseparates using bigram model two words).
In addition,there is the possibility that the error-to-correctionmapping is non-invariant.A statistical approach to spelling correction has someadvantages and some disadvantages.
Among theadvantages are: it corrects the majority of errors, thoseclassified as nonwords, misspelled words, word-splits,and run-ons; the automated acquisition of domain-specific data is easily maintainable; and the use of astatistical model enforces consistent lexical usage.
Adisadvantage is that correlated recall and precision maynot be high, i.e.
some errors may be missed and somemay be corrected incorrectly.
However, reasonablygood recall (>75%) coupled with very high accuracy(>95%) can be expected.
Other disadvantages are: thereis no clear strategy for multi-error detection andcorrection, and the tact that such a large corpus (20megabytes in our mass change corpus) is required tocreate a good statistical model.3.2 ParsingFor parsing, we use a generalized LR(1) shift/reduceparser \[16, 1, 10\].
Like yacc (which, given a grammar,generates a parser for that grammar), our parserprecompiles the CFG grammar into a state-transitiontable.
The parser exercises CFG grammar rulesannotated with syntactic and semantic action routines,thus allowing for synthesized and inherited attributes.
Inaddition to the rules, other knowledge stores integratedinto the parser's processing are a thematic role hierarchyand a semantic domain network, both of which are alsoused by lexical entries in a morphologically partitionedlexicon.
The parser uses a linked list of structuredtokens (displayed in 4 below), and returns only oneparse.
To facilitate robust parsing, the parser also allowsthe developer to activate grammar-directed tokendropping, token hypothesizing, and token type coercion.
(4) Token Structure<id: numerical identifier for tokensurface form (i.e.
actual string) for the sulfform:tokenrootform:value:assertions:scat:feature:nexl:root form of the tokenvalue (semrep) associated with id\[Isubcategorization requirement for thetoken, where the scat format is(ext arg int_argl int_arg2 ...), andwhere each argument must be agrammar symbol (exception: int arglmay be a string enclosed within # e.g.#into#contact~with#); ext_arg may beNULL/nil;feature associated with tokenptr to next polysemous token>The parser permits arbitrary backtracking, including thatover polysemous or composed tokens (idioms), overgrammar rules, and over object hierarchies (entity,property, and predicate types in the hybrid domainmodel), though in practice time and node limits are set.The backtracking facility also includes the developer-specified cut, an operator to force the termination of agrammar rule.
An example of backtracking overpolysemous tokens is displayed in the followingabbreviated trace from the mass-change process.
Asnoted, we employ a hybrid syntactic-semantic grammar,primarily because such a hybrid permits generality (athigher nonterminals) and specificity (at terminals andlower nonterminals).
(5) BacktrackingoverPolysemousTokensLexicon (abbreviated):F IN ISH : F IN ISH_~RB: F IN ISH_NOUNMATERIAL)( i saP~sing:F IN ISH 1 COAT OF F IN ISHmismatched s t r ing  SCAT \ [required:code\] / \[found: i\[I\]\].
?
?
?Di f f i cu l ty  in pars ing:  no t rans i t ionfor token NUMBER\[168\ ]  f rom s ta te  83cur rent  s tack  (in reverse) :  s tate-stack\[ l \ ] :  \ [F INISH\[124\]  \]backt rack  ...nbar  : ENT ITYa_nbar  : nbara_nbar  : a_nbar  NUMBERnphrase  : a_nbarD i f f i cu l ty  in pars ing:  no t rans i t ionfor token COMPOSIT ION\ [69 \ ]  f rom state75cur rent  s tack  (in reverse) :  s tate-stack\[2\] :  \ [COATING\[62\]  nphrase \ [4109\ ]\]60cur rent  s tack  (ins tack\ [4 \ ] :COMPOSIT ION\ [69 \ ]F IN ISH~TERB\ [126\ ]  \]backt rack  ...f in_n  : MATERIALf in_np : f in_no f_np  : COMPOSIT IONreverse) :  state-\ [F INISH\[124\]f in qfr \ [4126\]f in_npf in_np  : f in_qf r  o f _nps_ imp : F IN ISH_VERB f in_npsentence  : s_ impd iscourse  : sentencetop : d i scourse  EOSWhen enabled, the token-dropping option allows agrammar rule to be matched by dropping a token (from apre-specified set of droppable tokens), and is onlyapplied when a sentence will not parse without droppingthe token.
In addition, the parser will also hypothesize atoken when the input sentence will not parse strictly byusing the grammar ules.
Similarly, the parser willcoerce the unexpected type of a token to a type which isacceptable, should the parse otherwise fail.3.3 Semantic InterpretationThe mass-change procedure does not require thecomplex referential semantics that NLIs require.
Thesemantics and the discourse components can be simplerbecause the application requirements are simpler.
In allour NLP applications, however, both domain-dependentand -independent information constitute the semanticmodel, which is jointly used by the grammatical module(written in C) and the interpretation/generation m dule(written in Prolog).
Each token has a semantic markerwhich acts as an index into the semantic domain model.The morphologically generative l xicon is the primaryknowledge store-associating the input (surface) texttbrm, its tokenization, and the semantic marker.
Thegrammatical module uses the lexicon to drive its work,but also uses the semantic model directly to enable typeinheritance and, in some cases, the type coercion ofsemantic markers.The semantic domain model consists of a set ofassertions of the formobject(Child, \[Relation, Parent\])where Relation is either 'isa' or 'ispart', and the threepossible roots of the hierarchies are 'entity', 'predicate',and 'property'.
These are defined by a developer andentered into a the GraphEd tool \[14\], a graph editorwhich outputs an ascii representation f a network.
Theascii form can be transformed and used by both theparser and the backend Prolog interpretation processes.The output of the grammatical module is a combinedsyntactic-semantic representation f the input plan textin the form of a list of binary predicates capturing thetree structure.
Each predicate is of the form:predicate(skolem-constant, v lue)with skolem constants representing the nodes of the tree.The semantic entity markers are those items which arethe values of "instance' predicates, asinstance(n9, person)asserts that 'n9' is an 'instance' of semantic class'person'.
The syntactic-semantic representation is thenasserted as the primary knowledge store in the finaiinterpretation a d generation module.Additional knowledge sources used in the Prologinterpretation and generation module are: a database offinish codes and their associated information, includingthe number of coats of application required, colornumber, color name, and material type of each relevantfinish Code; a set of material-specific databases whichinclude the materials and the associated generationrequirements rules; and a task-driven tree-walker thattraverses the semantic representation f a plan to extractinformation requested by the generator.3.4 Plan GenerationThe text plan generator directly executes rulesrepresenting the output requirements of the new plans.Prior to executing these rules, however, the generatordetermines whether the original input plan is well-formed, valid, and consistent.
Then, using the domainmodel, the finish code and material databases, therequirements rules, and the semantic tree-walker, thegenerator c eates new plans.In other cases, the generator detects that a meta-constraint such as "Only one operation should exist perplan text" is violated.
It flags the text as anomalous,indicating the constraint violation, but still tries togenerate a reasonable output text.
A post-generationprocess diverts constraint violations to a separate streamwhich results eventually in the creation of a specialreport.
Texts which violate constraints are not changedand uploaded; instead, these are evaluated by a humandomain expert, who adjudicates the suggested changesindividually.
For example, in (6) the original planconsists of multiple run-on sentences with nopunctuation.
The NLP system determines that there areactually three sentences, two of which refer toapplication of finishes.
With this information, thegenerator determines that one of its meta-constraints hasbeen violated, generates its best guess at an output ext,and then annotates that text with the constraint violationmessage.61(6) Example of Generated TextInput:TOUCH UP REWORK AREA ONLY APPLY (I)COAT OF BMS10-11 TYPE 1 PRIMER PERBAC5736 (F18.01) REATrACH IDENTIFICATIONTAGGenerated Text:<<FOLLOWUP: MULTIPLE OPERATIONSSPECIFIED.>>TOUCH-UP FINISH REWORK AREA ONLY AS/IFREQUIRED PER ENG.
DWG.
PRIME PER F-18.01.REATTACH IDENTIFICATION TAG.3.5 Interpretation and OtherApplicationsThe mass-change application is fairly simple.
Morecomplicated NLP applications require ellipsis andpronominal resolution, and more richer referentialsemantics.
An NLI to a relational database, for example,requires an explicit recursive semantic compositionprocess.
This is why our deeper semantics in Prologclosely parallels that which a categorial analysis wouldfurnish, i.e:, using function application and compositionover lambda forms, per treatments such as \[11, 12\] andusing a semantic theory such as DRT \[7\].
Such anapproach allows one to compose a semantics in aprincipled manner and to interpret with respect o thedomain model.
Nevertheless, to this point, in anintert:ace to a project and program scheduling system,we have attempted only to render semantics for scope-underspecified quantifiers, negation, and numerical andtemporal constraints.
Tense and aspect (e.g., \[15\]),distinctions among plural readings of noun phrases, anda deeper lexical semantics, have so t~ not beenelaborated, but are planned.
In \[3\], e.g., a lexicalsemantics based on \[6\] will be developed.
Finally, a toollike \[4\]'s Prolog-to-SQL compiler can prove useful formapping the final referential semantics to a specificdatabase or domain model.4 ConclusionThe NLP tools described in this paper have been used anumber of times to effect the mass-change of on-linetextual databases.
The cost savings over other methodshas been significant (we estimate, for example, that infour years, 20,000 man-hours have been saved overmanual methods).
By representing core semanticcomponents in Prolog, we expect o minimize the workneeded to accommodate radical domain changes in thefuture, though application-specific manual work musts011 be performed to update the lexicon, modify thegrammar, and elaborate new referential semantics.These same NLP tools, modified to accommodateprimarily lexical difl'erences, a more complicatedsemantic domain model, and deeper interpretation, havebeen employed in building NLIs to legacy databases andapplications, in a resource-conserving manner.References\[1\] Aho, A.; Sethi, R.; and Ullman, J.
(1986).Compilers: Principles, Techniques and Tools.Reading, MA: Addison-Wesley\[2\] Antworth, Evan L. 1990.
PCKIMMO: A Two-LevelProcessor for Morphological Analysis.
Dallas, TX:Summer Institute of Linguistics.\[3\] Barrett, Tom; Coen, Gary; Hirsh, Joel; Obrst, Leo;Spering, Judith; Trainer, Asa.
1997.
MADEsmart:An Integrated Design Environment.
Submitted to1997 ASME Design for Manufacturing Symposium.\[4\] Draxler, Christoph.
1993.
A Powerful Prolog toSQL Compiler.
CIS Centre for Information andLanguage Processing.
Ludwig-Maximilians-Universit~it, Mtinchen, Germany.
August 16, 1993.\[5\] Dunning, Ted (1993).
Accurate Methods for theStatistics of Surprise and Coincidence.Computational Linguistics 19: I, pp.
61-74.\[6\] Jackendoff, R.S.
1990.
Semantic Structures.Cambridge, MA: MIT Press.\[7\] Kamp, Hans; Reyle, Uwe.
1993.
From Discourse toLogic: Introduction to Modeltheoretic Semantics ofNatural Language, Formal Logic, and DiscourseRepresentation Theory.
Dordrecht: KluwerAcademic.\[8\] Kartunnen, Lauri.
1983.
KIMMO: A GeneralMorphological Processor.
Texas Linguistic Forum22: 163-186.
University of Texas, Austin, TX.\[9\] Koskeniemi, Kimmo.
1983.
Two-LevelMorphology: A General Computational Model forWord-Form Recognition and Production.Publication No.
11.
Helsinki: University ofHelsinki Department ofGeneral Linguistics.\[lO\] Marcus, Mitchell.
1980.
A Theory of SyntacticRecognition for Natural Language.
New York:McGraw-Hill.\[11\] Moortgat, Michael.
1988.
CategorialInvestigations: Logical and Linguistic Aspects of62the Lambek Calculus.
Foris Publications,Dordrecht, Holland.\[12\] Morrill, Glyn.
1994.
Type Logical Grammar.Dordrecht: Kluwer Academic.\[13\] Obrst, Leo; Nanda Jha, Krishna; Coen, Gary.
1996.Mass Change of On-line Textual Databases UsingNatural Language Processing.
IndustrialApplications of Prolog Conference and Symposium(INAP-96), Tokyo, Japan.\[14\] Paulisch, Francis Newbery.
1993.
The Design ofan Extendible Graph Editor.
Lecture Notes inComputer Science 704.
Berlin, Heidelberg, NewYork: Springer-Verlag.\[15\] Verkuyl, Henk J.
1996.
A Theory of Aspectuality:The Interaction Between Temporal and AtemporalStructure.
Cambridge: Cambridge University Press.\[16\] Tomita, Masaru (1985).
Efficient Parsing forNatural Language.
Dordrecht: Kluwer AcademicPublishers.\[17\] Wojcik, Richard; Harrison, Philip; Bremer, John.1993.
Using Bracketed Parses to Evaluate aGrammar Checking Application.
Proceedings of the1993 ACL Conference.6364
