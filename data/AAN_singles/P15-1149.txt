A Data-Driven, Factorization Parser for CCG Dependency StructuresYantao Du, Weiwei Sun?
and Xiaojun WanInstitute of Computer Science and Technology, Peking UniversityThe MOE Key Laboratory of Computational Linguistics, Peking University{ws,duyantao,wanxiaojun}@pku.edu.cnAbstractThis paper is concerned with buildingCCG-grounded, semantics-oriented deepdependency structures with a data-driven,factorization model.
Three types of fac-torization together with different higher-order features are designed to capturedifferent syntacto-semantic properties offunctor-argument dependencies.
Integrat-ing heterogeneous factorizations resultsin intractability in decoding.
We pro-pose a principled method to obtain opti-mal graphs based on dual decomposition.Our parser obtains an unlabeled f-score of93.23 on the CCGBank data, resulting inan error reduction of 6.5% over the bestpublished result.
which yields a signifi-cant improvement over the best publishedresult in the literature.
Our implementa-tion is available at http://www.icst.pku.edu.cn/lcwm/grass.1 IntroductionCombinatory Categorial Grammar (CCG; Steed-man, 2000) is a linguistically expressive gram-mar formalism which has a transparent yet el-egant interface between syntax and semantics.By assigning each lexical category a dependencyinterpretation, we can derive typed dependencystructures from CCG derivations (Clark et al,2002), providing a useful approximation to theunderlying meaning representations.
To date,CCG parsers are among the most competitive sys-tems for generating such deep bi-lexical depen-dencies that appropriately encode a wide rangeof local and non-local syntacto-semantic infor-mation (Clark and Curran, 2007a; Bender et al,2011).
Such semantic-oriented dependency struc-tures have been shown very helpful for NLP ap-?Email correspondence.plications e.g.
Question Answering (Reddy et al,2014).Traditionally, CCG graphs are generated as aby-product by grammar-guided parsers (Clark andCurran, 2007b; Fowler and Penn, 2010).
The mainchallenge is that a deep-grammar-guided modelusually can only produce limited coverage andcorresponding parsing algorithms is of relativelyhigh complexity.
Robustness and efficiency, thus,are two major problems for handling practicaltasks.
To increase the applicability of such parsers,lexical or syntactic pruning has been shown nec-essary (Clark and Curran, 2004; Matsuzaki et al,2007; Sagae et al, 2007; Zhang and Clark, 2011).In the past decade, the techniques for data-driven dependency parsing has made a greatprogress (McDonald et al, 2005a,b; Nivre et al,2004; Torres Martins et al, 2009; Koo et al,2010).
The major advantage of the data-drivenarchitecture is complementary to the grammar-driven one.
On one hand, data-driven approachesmake essential uses of machine learning from lin-guistic annotations and are flexible to produceanalysis for arbitrary sentences.
On the otherhand, without hard constraints, parsing algorithmsfor spanning specific types of graphs, e.g.
projec-tive (Eisner, 1996) and 1-endpoint-crossing trees(Pitler et al, 2013), can be of low complexity.This paper proposes a new data-driven depen-dency parser that efficiently produces globally op-timal CCG dependency graphs according to a dis-criminative, factorization model.
The design ofthe factorization is motivated by three essentialproperties of the CCG dependencies.
First, all ar-guments associated with the same predicate arehighly correlated due to the nature that they ap-proximates type-logical semantics.
Second, allpredicates govern the same argument exhibit thehybrid syntactic/semantic, i.e.
head-complement-adjunct, relationships.
Finally, the CCG depen-dency graphs are not but look very much liketrees, which have many good computational prop-erties.
Simultaneously modeling the three prop-erties yields intrinsically heterogeneous factoriza-tions over the same graph, and hence results in in-tractability in decoding.
Inspired by (Koo et al,2010; Rush et al, 2010), we employ dual decom-position to perform principled decoding.
Thoughnot always, we can obtain the optimal solutionmost of time.
The time complexity of our parseris O(n3) when various 1st- and 2nd-order featuresare incorporated.We conduct experiments on English CCGBank(Hockenmaier and Steedman, 2007).
Thoughour parser does not use any grammar informa-tion, including both lexical categories and syntac-tic derivations, it produces very accurate CCG de-pendency graphs with respect to both token andcomplete matching.
Our parser obtains an unla-beled f-score of 93.23, resulting in, perhaps sur-prisingly, an error reduction of up to 6.5% overthe best published performance reported in (Auliand Lopez, 2011).
Our work indicates that high-quality data-driven parsers can be built for produc-ing more general dependency graphs, rather thantrees.
Nevertheless, empirical evaluation indicatesthat explicitly or implicitly using tree-structuredinformation plays an essential role.
The result alsosuggests that a wider range of complicated linguis-tic phenomena beyond surface syntax can be wellmodeled even without explicitly using grammars.Our algorithm is also applicable to other graph-structured representations, e.g.
HPSG predicate-argument analysis (Miyao et al, 2004).2 Related WorkHockenmaier and Steedman (2007) developed lin-guistic resources, namely CCGBank, from thePenn Treebank (PTB; Marcus et al, 1993).
InCCGBank, PTB phrase-structure trees have beentransformed into normal-form CCG derivations,and deep bi-lexical dependency graphs that encodefunctor-argument strcutures have been extractedfrom these derivations using coindexation infor-mation.
The typed dependency analysis providesa useful approximation to the underlying meaningrepresentations, and has been shown very help-ful for NLP applications e.g.
Question Answering(Reddy et al, 2014).Traditionally, CCG graphs are generated as aby-product by deep parsers with a core gram-mar (Clark et al, 2002; Clark and Curran, 2007b;Fowler and Penn, 2010).
On the other hand, mod-eling these dependencies within a CCG parser hasbeen shown very effective to improve the pars-ing accuracy (Clark and Curran, 2007b; Xu et al,2014).
Besides CCG, similar deep dependencystructures can be also extracted from parsers underother deep grammar formalisms, e.g.
LFG (Kinget al, 2003) and HPSG (Miyao et al, 2004).In recent years, data-driven dependency pars-ing has been well studied and widely applied tomany NLP tasks.
Research on data-driven ap-proach to producing dependency graphs that arenot limited to tree or forest structures has also beeninitialized.
Sagae and Tsujii (2008) introduceda transition-based parser that is able to handleprojective directed dependency graphs for HPSG-style predicate-argument analysis.
McDonald andPereira (2006) presented a graph-based parser thatcan generate graphs in which a word may dependon multiple heads, and evaluated it on the DanishTreebank.
Encouraged by their work, we studyfactorization models as well as principled decod-ing for CCG-grounded, graph-structured represen-tations.Dual decomposition, and more generally La-grangian relaxation, is a classical method for solv-ing combinatorial optimization problems.
It hasbeen successfully applied to several NLP tasks,including parsing (Koo et al, 2010; Rush et al,2010) and machine translation (Rush and Collins,2011).
To provide principled decoding for our fac-torization parser, we employ the dual decomposi-tion technique.
Our work directly follows (Kooet al, 2010).
The two basic factorizations aresimilar to the model introduced in (Martins andAlmeida, 2014).
Llu?
?s et al (2013) introduceda dual decomposition based joint model for jointsyntactic and semantic parsing.
They are con-cerned with shallow semantic representation, i.e.Semantic Role Labeling, whose graphs are sparse.Different from their concern on integrating syntac-tic parsing and semantic role labeling under 1st-order factorization, we are interested in designinghigher-order factorization models for more denseand general linguistic graphs.3 Graph Factorization3.1 Background NotationsConsider a sentence s = ?w,p?
with words w =w1w2 ?
?
?wn and POS-tags p = p1p2 ?
?
?
pn.
Firstwe add one more virtual word w0 = #Wroot#changes would exempt ... executives from(S\NP)/(S\NP) ((S\NP)/PP)NParg1 arg2arg1arg3arg2Figure 1: Examples to illustrate the predicate-centric view.with POS-tag p0 = #Proot# which is convention-ally considered as the root node of trees or graphson the sentence.
Then we denote the index setof all possible dependencies as I = {(i, j)|i ?
{0, ?
?
?
, n}, j ?
{1, ?
?
?
, n}, i 6= j}.
A depen-dency parse then can be represented as a vectory = {y(i, j) : (i, j) ?
I},where y(i, j) = 1 if a dependency with predicate iand argument j is in the graph, 0 otherwise.
Notethat y is not a matrix but a long vector though weuse two indexes to index it.
In this paper, we onlyconsider the unlabeled parsing task.
Nevertheless,it is quite straightforward to extend our models tolabeled parsing.
Let Y denote the set of all possi-ble y.
Given a function f : Y ?
R that assignsscores to parse graphs, the optimal parse isy?
= arg maxy?Yf(y).Following recent advances in discriminative de-pendency parsing, we build disambiguation mod-els based on global linear models, as in (McDon-ald et al, 2005a).
In this framework, we score adependency graph using a linear model:f?
(y) = ?>?
(s,y),where ?
(s,y) produces a d-dimensional vectorrepresentation of the event that a CCG graph y isassigned to sentence s. In order to perform thedecoding efficiently, we assume that the depen-dency graphs can be factored into smaller pieces.The main goal of this paper is to design ap-propriate factorization models, namely differenttypes of f?
?s, to reflect essential properties of thesemantics-oriented CCG dependency graphs.3.2 Predicate-Centric FactorizationThe very fundamental view of the CCG de-pendency graphs is based on their lexicalized,predicate-centric nature.
Every word is assigneda lexical category, which directly encodes its sub-categorization information.
Due to the type-transparency nature of the formalism, this lexi-cal category provides sufficient information fornot only syntactic derivation but also semanticcomposition.
It is important to capture functor-argument relations by putting all arguments ofone particular predicate together.
Figure 1 givesan example.
The predicate ?exempt?
is of type?((S\NP)/PP)/NP,?
indicating that it takes threesemantic dependents.
This part of information isvery similar to Semantic Role Labeling (SRL),whose goal is to find semantic roles for ver-bal predicates as well as their normalization.However, functor-argument analysis grounded inCCG is approximation of underlying logic formsand thus provides bi-lexical relations for almost allwords.
For instance, the second word in focus??would?
?captures structural information to orga-nize other predicates yet entities.In order to perform maximization efficientlyin this view, we treat each predicate separately.Given a vector yp, we defineypiy = {y(i, j) : j ?
{1, ?
?
?
, n}, j 6= i}and assume that f(yp) takes the formfp(yp) =n?i=0fpi (ypiy)To capture the relationships of all arguments toone particular predicate as a whole, we employ aMarkov model.
Let a1, ?
?
?
, am be the sequence ofthe arguments of the word wi under ypiy.
To keepthe arguments in order, we constrain 1 ?
aj1 <aj2 ?
n if j1 < j2.
In a k-th order predicate-centric model, we definefpi (ypiy) =m+k?1?j=1?>p ?p(aj?
(k?1), ..., aj , i,w,p)where aj (j ?
0 or j ?
m + 1) are treated asspecific initial or end state.Higher-order rather than arc-factored featurescan be conveniently extracted from adjacent argu-ments.
This is similar to the sibling factorizationdefined by a number of syntactic tree parsers, e.g.
(McDonald and Pereira, 2006), (Koo and Collins,2010) and (Ma and Zhao, 2012).In an Oct. 19 review of ...(S/S)/N NP/N N/N N/N N (NP\NP)/NParg2arg1 arg1arg1 arg1Figure 2: An example to illustrate the argument-centric view.3.3 Argument-Centric FactorizationThe syntactic principle for tree annotation treatsthe dependency relations between two words assyntactic projection.
In another word, the head de-termines the syntactic category of the whole struc-ture.
The (type-logical) semantic principle deter-mines a dependency according the types of the twowords.
The two kinds of dependency are coherentbut not necessarily the same.
In particular, an ad-junct is a syntactic dependent but usually a seman-tic predicate of its syntactic head.
Figure 2 givesan example to illustrate the idea.
The argumentin focus is ?review?
that is the complement of thepreposition ?in.?
The direction of this semantic de-pendency is the same to its corresponding syntac-tic dependency.
Other predicates that semanticallygovern ?review?
are actually its modifiers, so thedirection of these semantic dependencies are theopposite of their syntactic counterparts.
It is im-portant to capture head-complement-adjunct rela-tions by putting all predicates of one particular ar-gument together.Similar to the predicate-centric model, we treatthe graph fragment involved by each argument asindependent, and capture the relationships amongall predicates that governs the same argument us-ing a Markov model.
In the definition of predicate-centric model, if we exchange predicates and ar-guments, then we get our argument-centric model.Formally, we defineyayj = {y(i, j) : i ?
{0, ?
?
?
, n}, j 6= i}.Let p1, ?
?
?
, pm be the sequence of the predicates(in linear word order) that semantically governsthe word j under yayj .
A k-th order argument-centric model scores the dependency graph asfa(y) =n?j=1faj (yayj)=n?j=1m+k?1?i=1?>a ?a(pi?
(k?1), ..., pi, j,w,p)Similarly, we define the initial and end states forpi (i < 0 or i ?
m+ 1).3.4 Tree Approximation ModelTree structures exhibit many computationally-good properties, and have been widely applied tomodel linguistic, especially syntactic, structures.Tree-structured representation is an essential pre-requisite for both the parsing algorithms and themachine learning methods in state-of-the-art syn-tactic dependency parsers.
The CCG dependencygraphs are not but look very much like trees.
Wethus argue that a tree-centric model can on onehand capture some topologically essential charac-teristics and on the other hand benefit from maturetree parsing techniques.To this end, we propose tree approximation toobtain CCG sub-graphs under the factorization us-ing tree parsing algorithms.
In particular, weintroduce an algorithm to associate every graphwith a projective dependency tree, which we callweighted conversion.
The tree reflects partial in-formation about the corresponding graph.
In thisalgorithm, we assign heuristic weights to all pos-sible edges, and then find the tree with maxi-mum weights.
The key idea behind is to find atree frame of a given graph.
Given an arbitraryCCG graph, the conversion is perhaps imperfect inthe sense that information about a small portion ofedges is ?lost.?
As a result, our tree approximationmodel can only generate partial graphs.
Neverthe-less, we will show (in Section 3.5 and 4.2) thatsuch a model can be combined with predicate- andargument-centric factorization models in an ele-gant way.3.4.1 Weighted ConversionWe assign weights to all the possible edges, i.e.
allpairs of words, and then determine which edgesto be kept by finding the maximum spanning tree.More formally, given a graph y = {y(i, j)}, eachpossible edge (i, j) is assigned a heuristic weight?
(i, j).
The maximum spanning tree t = {t(i, j)}contains the maximum sum of values of edges:tmax = arg maxt?
(i,j)t(i, j)?
(i, j)We separate the ?
(i, j) into three parts(?
(i, j) = A(i, j) + B(i, j) + C(i, j)) that aredefined as below.?
A(i, j) = a ?
max{y(i, j), y(j, i)}: a is theweight for the existing edges on graph ignor-ing direction.?
B(i, j) = b ?
y(i, j): b is the weight for theforward edges on the graph.?
C(i, j) = n?|i?
j|: This term estimates theimportance of an edge where n is the lengthof the given sentence.
For dependency pars-ing, we consider edges with short distance tobe more important because those edges canbe predicted more accurately in future pars-ing process.?
a  b  n or a > bn > n2: The convertedtree should contain arcs in original graph asmany as possible, and the direction of the arcsshould not be changed if possible.
The rela-tionship of a, b, and c guarantees this.After all edges are weighted, we can use max-imum spanning tree (MST) algorithms to get theconverted tree.
To get the projective tree, wechoose Eisner?s algorithm.
However, the obtainedtree must be labeled in order to encode the origi-nal graph.
Here we introduce a label vector l ={l(i, j)}.
For each (i, j) ?
I, we assign a labell(i, j) to edge (i, j) as follows.Case y(i, j) = 1: label ?X?
;Case y(i, j) = 0 ?
y(j, i) = 1: label ?X?R?
;Case y(i, j) = 0 ?
y(j, i) = 0: label ?None?.We can convert the labeled tree back to graph andobtain yt.
Tough some edges are lost during theconversion, a lot more are kept.
In fact, accordingto our evaluation, 92.74% of edges in the trainingset are retained after conversion.3.4.2 Factorizing TreesWe use the tree parsing model proposed in(Bohnet, 2010) to score the converted trees.
Themodel factorizes a tree into 1st-order and 2nd-order factors.
When decoding, the model searchesfor a tree with the best score.
The score definedfor graphs as well as trees isf t(yt) = gt(t, l) = ?>t ?t(s, t, l)= ?1>t ?1t (s, t, l) + ?2>t ?2t (s, t, l)=( ?
(i,j)?It(i, j)?1>t ?1t (l(i, j),w,p))+?2>t ?2t (s, t, l),where ?1t is the 1st-order features and ?2t is the2nd-order features.3.5 Parsing as OptimizationMotivated by linguistic properties of thesemantics-oriented CCG dependencies, wehave designed three single factorization modelsfrom heterogeneous views.
Our single modelsexhibit different predictive strengths consideringthat they are designed to capture different prop-erties separately.
Integrating them can generatebetter graphs, but is provably hard.
To this end,we formulate the parsing problem as the followingconstrained optimization problem.maximize fp(yp) + fa(ya) + f t(yt)subject to yp(i, j) = ya(i, j),yp(i, j) ?
yt(i, j),ya(i, j) ?
yt(i, j) for all (i, j)The equality constraint says that the graph givenby the predicate- and the argument-centric modelmust be identical, while the inequality constraintssay that the frame of graph given by the tree ap-proximation model must be a subgraph of what isgiven by the first two models.4 Decoding4.1 Easiness and Hardness of DecodingThe three factorization models are all solvable inpolynomial time.
The predicate-centric model andthe argument-centric model can be decoded us-ing dynamic programming.
We provide the de-tailed description of such an algorithm in our sup-plementary note.
The decoding method for k-th(k ?
2) order model costs time of O(nk+1) wheren is the length of the sentence.
The tree approxi-mation model can re-use existing dependency treeparsing algorithms.Unfortunately, the exact joint decoding of 2nd-order predicate- and argument-centric models isalready NP-hard, not to mention other model com-binations.
The following gives a brief proof forthe problem of combining the 2nd-order predicate-and argument-centric models.Proof.
Formally, we want to find a graph y whichmaximizes F (y) = fp(y)+fa(y).
We can designthe feature function ?a and the parameter ?a, suchthat for all 1 ?
i1 ?
i2 ?
n,???????
?>a ?a(0, i1, j,w,p) = 0?>a ?a(i1, n+ 1, j,w,p) = 0?>a ?a(i1, i2, j,w,p) = ??
?>a ?a(0, n+ 1, j,w,p) = ?
?where n is the length of the sentence.
Note thatthose 4 equations make the nodes except the rootnode in the optimal graph each have exactly oneincoming edge.
So the problem of finding a treet maximizing fp(t) is reduced to this problem.Moreover, the NP-hard problem 3DM can be re-duced to the problem of finding a tree t maxi-mizing fp(t) (see McDonald and Pereira (2006)),leading to the NP-hardness of both of the prob-lems.4.2 Decoding via Dual DecompositionTo solve the joint decoding problem, optimizationtechniques based on decomposition with couplingvariables are applicable.
In this paper, we proposeto solve it via dual decomposition.
The experimentresults show that though not always, we can obtainthe optimal solution most of time.
To simplify thedescription, we only consider the 2nd-order casefor all three models.4.2.1 Lagrangian RelaxationNotice that yp(i, j) ?
yt(i, j) can be written asyp(i, j) ={1, if yt(i, j) = 1;arbitrary, if yt(i, j) = 0.So the constraint can be written asApyp+Aaya+Atyt = 0, whereAp =??IDyt0?
?Aa =???I0Dyt?
?At =??0?Dyt?Dyt?
?I is the identity matrix and Dyt is a diagonal ma-trix whose main diagonal is the vector yt.The Lagrangian of the optimization problem isL(yp,ya,yt;u) = fp(yp) + fa(ya) + f t(yt)+u>(Apyp +Aaya +Atyt),where u is the Lagrangian multiplier.Omitting the constraints, the dual objective isL(u) = maxyp,ya,ytL(yp,ya,yt;u)= maxyp(fp(yp) + u>Apyp)+ maxya(fa(ya) + u>Aaya)+ maxyt(f t(yt) + u>Atyt)Let L?
be the maximized value of L(yp,ya,yt;u)subjected to the constraints, then L?
=minu L(u), according to the duality principle.4.2.2 Decoding AlgorithmThere are two challenges in solving the dual prob-lem.
One challenge is to find the minimum valueof the dual objective.
For this, we can use subgra-dient method, as is demonstrated in Algorithm 1.The other is the evaluation of L(u).
For this, wedecompose the dual objective into three optimiza-tion problems.
Let Bp = u>Ap, Ba = u>Aa,Bt = u>At, andCtl (i, j) ={Bt(i, j), if l(i, j) = X;Bt(j, i), if l(i, j) = X ?
R;we can just redefinefpi (yiy) =m+1?j=1(?>p ?p(aj?1, aj , i,w,p)+Bp(i, j))faj (yyj) =m+1?i=1(?>a ?a(pi?1, pi, j,w,p)+Ba(i, j))f t(y) =( ?
(i,j)?It(i, j)(?1>t ?1t (l(i, j),w,p)+Ctl (i, j)))+ ?2>t ?2t (s, t, l),and decode according to the new scores.
In fact,this equals to attach some new weights to 1st-orderfactors, without changing the decoding algorithmsfor the subproblems.
This nice property also al-lows using higher-order models for subproblems.Algorithm 1: Joint decoding algorithmInitialization: set u(0) to 0for k = 1 toK doyp(k) ?
arg maxy fp(y) + u(k)Apyya(k) ?
arg maxy fa(y) + u(k)Aayyt(k) ?
arg maxy ft(y) + u(k)Atyif Apyp(k) +Aaya(k) +Atyt(k) = 0 thenreturn yau(k) ?
u(k?1)?
?k(Apyp(k) +Aaya(k) +Atyt(k))Algorithm 1 is our decoding algorithm.
In ev-ery iteration, we first compute the optimal y?s ofthe three subproblems.
If the y?s satisfies the con-straints, then we?ve find the optimal solution forthe original problem.
If not, we update the La-grangian multiplier u, towards the negative sub-gradient.
We initialized u to be a zero vector,and use ?k to be the step length of each iteration.When we decode the subproblems, the Dyc in Apand Aa is derived from the yc obtained in the cur-rent iteration.We can also assign weights to different factor-ization models.
If we choose to do so, the La-grangian becomesL(yp,ya,yc;u) = wpfp(yp) + wafa(ya)+wcf t(yt) + u>(Apyp +Aaya +Atyt).And the decoding of subproblems in our algorithmbecomesyp(k) ?
arg maxyfp(y) +1wpu(k)Apy;ya(k) ?
arg maxyfa(y) +1wau(k)Aay;yt(k) ?
arg maxyf t(y) +1wcu(k)Aty.The algorithm we give here is the joint decod-ing for all the three models.
We can also decodeusing any two of them, and it is trivial to adapt thealgorithm to the decoding.4.3 PruningIn order to improve the efficiency of the algorithm,we also do some pruning.
One idea is that, inpredicate-centric model, different type of predi-cates has different number of arguments.
For ex-ample, the predicates POS-tagged ?DT?
each hasonly one argument in most cases.
Therefore, weassign each POS-tag a max number of arguments.When decoding, we search at most those numberof arguments instead of all the words in the sen-tence.
This pruning method can also be appliedto the argument-centric model.
The other idea isthat, some pairs of types never form a predicate-argument relation.
So we can skip extracting fea-ture of those POS-tag pairs, just take ??
to betheir scores.5 Evaluation and Analysis5.1 Experimental SetupCCGbank is a translation of the Penn Treebankinto a corpus of CCG derivations (HockenmaierDevel.
TestHMM Tagger 96.74% 97.23%Transition-based Parser 93.48% 93.09%Graph-based Parser 93.47% 93.19%Table 1: The accuracy of the POS tagger and theUAS of the syntax tree parsers.and Steedman, 2007).
CCGbank pairs syntac-tic derivations with sets of word-word dependen-cies which approximate the underlying functor-argument structure.
Our experiments were per-formed using CCGBank which was split into threesubsets for training (Sections 02-21), developmenttesting (Section 00) and the final test (Section 23).We also use the syntactic dependency trees pro-vided by the CCGBank to obtain necessary in-formation for graph parsing.
However, differentfrom experiments in the CCG parsing literature, weuse no grammar information.
Neither lexical cate-gories nor CCG derivations are utilized.All experiments were performed using automat-ically assigned POS-tags that are generated by asymbol-refined generative HMM tagger1 (Huanget al, 2010), and automatically parsed dependencytrees that are generated by our in-house implemen-tation of the transition-based model presented in(Zhang and Nivre, 2011) as well as a 2nd-ordergraph-based parser2 (Bohnet, 2010).
The accu-racy of these preprocessors is shown in Table 1.We ran 5-fold jack-knifing on the gold-standardtraining data to obtain imperfect dependency trees,splitting off 4 of 5 sentences for training and theother 1/5 for testing, 5 times.
For each split, were-trained the tree parsers on the training portionand applied the resulting model to the test portion.Previous research on dependency parsing showsthat structured perceptron (Collins, 2002) is one ofthe strongest discriminative learning algorithms.To estimate ?
?s of different models, we utilize theaveraged perceptron algorithm.
We implement ourown the predicate- and argument-centric models.To perform tree parsing, we re-use the open-sourceimplementation provided by the mate-tool.
Seethe source code attached for details.
We set it-eration 5 to train predicate- and argument-centricmodels and 10 for the tree approximation model.To perform dual decomposition, we set the maxi-mum iteration 200.1www.code.google.com/p/umd-featured-parser/2www.code.google.com/p/mate-tools/Tree Model UP UR UF UEMNoPC 91.85 87.26 89.50 18.77AC 91.94 87.06 89.43 16.47TA 92.85 86.39 89.51 14.48PC+AC 93.84 88.18 90.93 23.05PC+TA 91.80 91.69 91.74 27.29AC+TA 90.19 92.88 91.52 25.51PC+AC+TA 93.01 92.08 92.54 32.83GrPC 94.01 90.76 92.36 30.16AC 94.14 90.44 92.25 27.71TA 93.07 86.59 89.71 15.16PC+AC 94.66 91.09 92.84 33.19PC+TA 92.98 92.68 92.83 35.02AC+TA 92.47 93.13 92.80 33.93PC+AC+TA 93.66 92.73 93.19 37.64TrPC 93.93 90.85 92.37 30.21AC 93.94 90.66 92.27 28.23TA 93.19 86.68 89.82 14.79PC+AC 94.58 91.14 92.83 31.94PC+TA 92.93 92.71 92.82 35.34AC+TA 92.33 93.16 92.74 33.61PC+AC+TA 93.50 92.73 93.11 37.69Table 2: Parsing performance on the developmentdata.
The column ?Tree?
denotes the parsers thatgive dependency tree of development set: no tree(No), transition-based (Tr) or graph-based (Gr).?PC,?
?AC?
and ?TA?
in the second column de-notes the predicate-centric, the argument-centricand the tree approximation models, respectively.5.2 Effectiveness of Data-driven ModelsTable 2 summarizes parsing performance on de-velopment set with different configurations.
Wereport unlabeled precision (UP), recall (UR), f-score (UF) as well as complete match (UEM).
Itcan be clearly seen that the data-driven modelsobtains high-quality graphs with respect to tokenmatch.
Even without any syntactic information(see the top block associated with ?No Tree?
), ourparser with all three factorization models obtainsan f-score of 92.5. when assisted by a syntac-tic parser, this figure goes up to over 93.1.
If thepredicate- or argument-centric model is applied byitself, either one can achieve a competitive accu-racy, especially when syntactic features are uti-lized.5.3 Effectiveness of Multiple FactorizationWe use dual decomposition to perform joint de-coding.
First we combine the predicate-centricmodel and the argument-centric model.
Comparedto each single model, an error reduction of about7% on f-score (UF) on average is achieved.
Fur-0 20 40 60 80 100 120 140 160 180 2000.20.30.40.50.60.70.80.91IterationPercentageof exact decodingPC+ACPC+AC+TAFigure 3: The exact decoding rate.thermore, we ensemble all the three models.
If nosyntactic features are extracted, the ?TA?
modelbrings in a remarkable further absolute gain of1.02 with respect to token match.
If syntacticfeatures are used, the ?PC?
and ?AC?
models al-ready achieves relatively good performance, andthe ?TA?
model does not contribute much consid-ering token match.
The join of the tree approxi-mation model lowers the precision, it increases therecall further, resulting in a modest improvementof the f-score.
Nonetheless, the ?TA?
model stillsignificantly improve the complete match metric.It is noticeable that in all setting, the ?TA?
modelresult in very significant boost in complete match.The dual decomposition does not guarantee tofind an exact solution (in a limited number of it-erations) in theory but usually works very well inpractice.
We calculate the percentage of findingexact decoding below k iterations, and the result isshow in Figure 3.
The transition-based tree parseris utilized here.
We can see that for most sen-tences, dual decomposition practically gives theexact solutions.5.4 Importance of Tree StructuresOur factorization parser (with best experimentalsetting) does not utilize a grammar but do usesyntactic information in the dependency formal-ism.
In particular, the parser extracts the so-calledpath features from dependency trees.
The syntac-tic trees is very importance to our parser, whichprovide a critical set of features for the predicate-centric and the argument-centric models.
With-out the syntactic trees, their performances de-crease significantly.
We try two different parsersto obtain the syntax tree parses.
One is of thetransition-based architecture, and the other graph-Tree Model UP UR UF UEMNo PC+AC+TA 93.03 92.03 92.53 32.61Gr PC+AC+TA 93.71 92.72 93.21 38.14Tr PC+AC+TA 93.63 92.83 93.23 37.47Auli and Lopez 93.08 92.44 92.76 -Xu et al 93.15 91.06 92.09 37.56Table 3: Comparing the state-of-art with our mod-els on test set.based.
The architecture of the syntactic tree parserdoes not affect the results much.
The two treeparsers give identical attachment scores, and leadto similar graph parsing accuracy.
This result issomehow non-obvious given that the combinationof a graph-based and transition-based parser usu-ally gives significantly better parsing performance(Nivre and McDonald, 2008; Torres Martins et al,2008).Although the target representation of our parseris general graphs rather trees, implicitly or explic-itly using tree-structured information plays an es-sential role.
Syntactic features are able to im-prove the f-score achieved by the ?PC+AC?
modelfrom 90.9 to 92.8, while the ?TA?
model can bringin an absolute gain of 1.6.
Note that the ?TA?model does not utilize any syntactic tree infor-mation.
The converted trees are automatically in-duced from the CCG graphs.
Even when syntactictrees are available, the automatically induced treescan still significantly improve the complete matchwith respect to the whole sentence.5.5 Comparison to the State-of-the-artWe compare our results with the best publishedCCG parsing performance obtained by the modelspresented in (Auli and Lopez, 2011) and (Xu et al,2014)3.
Auli and Lopez (2011) reported best nu-meric performance.
The performance is evaluatedon sentences that can be parsed by their model.Xu et al (2014) reported the best published resultsfor sentences with full coverage.
All results on thetest set is shown in Table 3.
Even without any syn-tactic features, our parser achieves accuracies thatare superior to Xu et al?s parser and comparableto Auli and Lopez?s system.
When unlabeled syn-tactic trees are provided, our parser outperform thestate-of-the-art.3The unlabeled parsing results are not reported in the orig-inal paper.
The figures presented in are provided by WenduanXu.6 ConclusionIn this paper, we have presented a factoriza-tion parser for building CCG-grounded depen-dency graphs.
It achieves substantial improvementover the state-of-the-art.
Perhaps surprisingly, ourdata-driven, grammar-free parser yields a supe-rior accuracy to all CCG parsers in the literature.Our work indicates that high-quality data-drivenparsers can be built for producing more generaldependency graphs, rather than trees.
Our methodis also applicable to other deep dependency struc-tures, e.g.
HPSG predicate-argument analysis(Miyao et al, 2004), as well as other graph-structured semantic representations, e.g.
AbstractMeaning Representations (Banarescu et al, 2013).AcknowledgementThe work was supported by NSFC(61300064), National High-Tech R&D Pro-gram (2015AA015403), NSFC (61331011) andNSFC (61170166).ReferencesMichael Auli and Adam Lopez.
2011.
Train-ing a log-linear parser with loss functions viasoftmax-margin.
In Proceedings of EMNLP,pages 333?343.
Association for ComputationalLinguistics, Edinburgh, Scotland, UK.Laura Banarescu, Claire Bonial, Shu Cai,Madalina Georgescu, Kira Griffitt, Ulf Herm-jakob, Kevin Knight, Philipp Koehn, MarthaPalmer, and Nathan Schneider.
2013.
Abstractmeaning representation for sembanking.
In Pro-ceedings of the 7th Linguistic Annotation Work-shop and Interoperability with Discourse, pages178?186.
Association for Computational Lin-guistics, Sofia, Bulgaria.Emily M. Bender, Dan Flickinger, Stephan Oepen,and Yi Zhang.
2011.
Parser evaluation over lo-cal and non-local deep dependencies in a largecorpus.
In Proceedings of EMNLP, pages 397?408.
Association for Computational Linguis-tics, Edinburgh, Scotland, UK.Bernd Bohnet.
2010.
Top accuracy and fast de-pendency parsing is not a contradiction.
In Pro-ceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010),pages 89?97.
Coling 2010 Organizing Commit-tee, Beijing, China.Stephen Clark and James Curran.
2007a.Formalism-independent parser evaluation withccg and depbank.
In Proceedings of the 45thAnnual Meeting of the Association of Computa-tional Linguistics, pages 248?255.
Associationfor Computational Linguistics, Prague, CzechRepublic.Stephen Clark and James R. Curran.
2004.
Theimportance of supertagging for wide-coverageCCG parsing.
In Proceedings of Coling 2004,pages 282?288.
COLING, Geneva, Switzer-land.Stephen Clark and James R. Curran.
2007b.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Comput.
Linguist.,33(4):493?552.Stephen Clark, Julia Hockenmaier, and MarkSteedman.
2002.
Building deep dependencystructures using a wide-coverage CCG parser.In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics,July 6-12, 2002, Philadelphia, PA, USA., pages327?334.Michael Collins.
2002.
Discriminative trainingmethods for hidden markov models: Theoryand experiments with perceptron algorithms.
InProceedings of EMNLP, pages 1?8.
Associationfor Computational Linguistics.Jason M. Eisner.
1996.
Three new probabilis-tic models for dependency parsing: an explo-ration.
In Proceedings of the 16th conferenceon Computational linguistics - Volume 1, pages340?345.
Association for Computational Lin-guistics, Stroudsburg, PA, USA.Timothy A. D. Fowler and Gerald Penn.
2010.Accurate context-free parsing with combinatorycategorial grammar.
In Proceedings of ACL,pages 335?344.
Association for ComputationalLinguistics, Uppsala, Sweden.Julia Hockenmaier and Mark Steedman.
2007.CCGbank: A corpus of CCG derivationsand dependency structures extracted from thepenn treebank.
Computational Linguistics,33(3):355?396.Zhongqiang Huang, Mary Harper, and SlavPetrov.
2010.
Self-training with products oflatent variable grammars.
In Proceedings ofEMNLP, pages 12?22.
Association for Compu-tational Linguistics, Cambridge, MA.Tracy Holloway King, Richard Crouch, StefanRiezler, Mary Dalrymple, and Ronald M. Ka-plan.
2003.
The PARC 700 dependency bank.In In Proceedings of the 4th InternationalWorkshop on Linguistically Interpreted Cor-pora (LINC-03), pages 1?8.Terry Koo and Michael Collins.
2010.
Efficientthird-order dependency parsers.
In Proceedingsof ACL, pages 1?11.
Association for Computa-tional Linguistics, Uppsala, Sweden.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projectivehead automata.
In Proceedings of EMNLP,pages 1288?1298.
Association for Computa-tional Linguistics, Cambridge, MA.Xavier Llu?
?s, Xavier Carreras, and Llu?
?s Ma`rquez.2013.
Joint arc-factored parsing of syntactic andsemantic dependencies.
TACL, 1:219?230.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order de-pendency parsing.
In Proceedings of COLING2012: Posters, pages 785?796.
The COLING2012 Organizing Committee, Mumbai, India.Mitchell P. Marcus, Mary Ann Marcinkiewicz,and Beatrice Santorini.
1993.
Building a largeannotated corpus of english: the penn treebank.Comput.
Linguist., 19(2):313?330.Andre?
F. T. Martins and Mariana S. C. Almeida.2014.
Priberam: A turbo semantic parser withsecond order features.
In Proceedings of Se-mEval 2014, pages 471?476.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichiTsujii.
2007.
Efficient hpsg parsing with su-pertagging and cfg-filtering.
In Proceedings ofthe 20th international joint conference on Ar-tifical intelligence, pages 1671?1676.
MorganKaufmann publishers Inc., San Francisco, CA,USA.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training ofdependency parsers.
In Proceedings of the 43rdAnnual Meeting of the Association for Compu-tational Linguistics (ACL?05), pages 91?98.
As-sociation for Computational Linguistics, AnnArbor, Michigan.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency pars-ing algorithms.
In Proceedings of 11th Con-ference of the European Chapter of the Asso-ciation for Computational Linguistics (EACL-2006)), volume 6, pages 81?88.Ryan McDonald, Fernando Pereira, Kiril Ribarov,and Jan Hajic.
2005b.
Non-projective depen-dency parsing using spanning tree algorithms.In Proceedings of EMNLP, pages 523?530.
As-sociation for Computational Linguistics, Van-couver, British Columbia, Canada.Yusuke Miyao, Takashi Ninomiya, and Jun ichiTsujii.
2004.
Corpus-oriented grammar de-velopment for acquiring a head-driven phrasestructure grammar from the penn treebank.
InIJCNLP, pages 684?693.Joakim Nivre, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.
InHwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conferenceon Computational Natural Language Learn-ing (CoNLL-2004), pages 49?56.
Associationfor Computational Linguistics, Boston, Mas-sachusetts, USA.Joakim Nivre and Ryan McDonald.
2008.
In-tegrating graph-based and transition-based de-pendency parsers.
In Proceedings of ACL-08:HLT, pages 950?958.
Association for Compu-tational Linguistics, Columbus, Ohio.Emily Pitler, Sampath Kannan, and Mitchell Mar-cus.
2013.
Finding optimal 1-endpoint-crossingtrees.
TACL, 1:13?24.Siva Reddy, Mirella Lapata, and Mark Steed-man.
2014.
Large-scale semantic parsing with-out question-answer pairs.
Transactions ofthe Association for Computational Linguistics(TACL).Alexander M. Rush and Michael Collins.
2011.Exact decoding of syntactic translation mod-els through lagrangian relaxation.
In The 49thAnnual Meeting of the Association for Compu-tational Linguistics: Human Language Tech-nologies, Proceedings of the Conference, 19-24June, 2011, Portland, Oregon, USA, pages 72?82.Alexander M Rush, David Sontag, MichaelCollins, and Tommi Jaakkola.
2010.
On dualdecomposition and linear programming relax-ations for natural language processing.
InProceedings of EMNLP, pages 1?11.
Associa-tion for Computational Linguistics, Cambridge,MA.Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Hpsg parsing with shallow depen-dency constraints.
In Proceedings of the 45thAnnual Meeting of the Association of Computa-tional Linguistics, pages 624?631.
Associationfor Computational Linguistics, Prague, CzechRepublic.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reduce dependency DAG parsing.
In Pro-ceedings of the 22nd International Conferenceon Computational Linguistics, pages 753?760.Coling 2008 Organizing Committee, Manch-ester, UK.Mark Steedman.
2000.
The syntactic process.
MITPress, Cambridge, MA, USA.Andre Torres Martins, Noah Smith, and Eric Xing.2009.
Concise integer linear programming for-mulations for dependency parsing.
In Proceed-ings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural LanguageProcessing of the AFNLP, pages 342?350.
As-sociation for Computational Linguistics, Sun-tec, Singapore.Andre?
Filipe Torres Martins, Dipanjan Das,Noah A. Smith, and Eric P. Xing.
2008.
Stack-ing dependency parsers.
In Proceedings ofEMNLP, pages 157?166.
Association for Com-putational Linguistics, Honolulu, Hawaii.Wenduan Xu, Stephen Clark, and Yue Zhang.2014.
Shift-reduce ccg parsing with a depen-dency model.
In Proceedings of the 52nd An-nual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers),pages 218?227.
Association for ComputationalLinguistics, Baltimore, Maryland.Yue Zhang and Stephen Clark.
2011.
Shift-reduceCCG parsing.
In Proceedings of the 49th An-nual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies, pages 683?692.
Association for Computa-tional Linguistics, Portland, Oregon, USA.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-localfeatures.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 188?193.
Association for ComputationalLinguistics, Portland, Oregon, USA.
