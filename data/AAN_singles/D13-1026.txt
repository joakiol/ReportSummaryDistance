Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 255?264,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsMax-Margin Synchronous Grammar Induction for Machine TranslationXinyan Xiao and Deyi Xiong?School of Computer Science and TechnologySoochow UniversitySuzhou 215006, Chinaxyxiao.cn@gmail.com, dyxiong@suda.edu.cnAbstractTraditional synchronous grammar inductionestimates parameters by maximizing likeli-hood, which only has a loose relation to trans-lation quality.
Alternatively, we propose amax-margin estimation approach to discrim-inatively inducing synchronous grammars formachine translation, which directly optimizestranslation quality measured by BLEU.
Inthe max-margin estimation of parameters, weonly need to calculate Viterbi translations.This further facilitates the incorporation ofvarious non-local features that are defined onthe target side.
We test the effectiveness of ourmax-margin estimation framework on a com-petitive hierarchical phrase-based system.
Ex-periments show that our max-margin methodsignificantly outperforms the traditional two-step pipeline for synchronous rule extractionby 1.3 BLEU points and is also better than pre-vious max-likelihood estimation method.1 IntroductionSynchronous grammar induction, which refers tothe process of learning translation rules from bilin-gual corpus, still remains an open problem in sta-tistical machine translation (SMT).
Although state-of-the-art SMT systems model the translation pro-cess based on synchronous grammars (includingbilingual phrases), most of them still learn trans-lation rules via a pipeline with word-based heuris-tics (Koehn et al 2003).
This pipeline first buildsword alignments using heuristic combination strate-gies, then heuristically extracts rules that are consis-tent with word alignments.
Such heuristic pipeline?Corresponding authoris not elegant theoretically.
It brings an undesirablegap that separates modeling and learning in an SMTsystem.Therefore, researchers have proposed alternativeapproaches to learning synchronous grammars di-rectly from sentence pairs without word alignments,via generative models (Marcu and Wong, 2002;Cherry and Lin, 2007; Zhang et al 2008; DeNeroet al 2008; Blunsom et al 2009; Cohn and Blun-som, 2009; Neubig et al 2011; Levenberg et al2012) or discriminative models (Xiao et al 2012).Theoretically, these approaches describe how sen-tence pairs are generated by applying sequences ofsynchronous rules in an elegant way.
However, theylearn synchronous grammars by maximizing likeli-hood,1 which only has a loose relation to transla-tion quality (He and Deng, 2012).
Moreover, gen-erative models are normally hard to be extended toincorporate useful features, and the discriminativesynchronous grammar induction model proposed byXiao et al(2012) only incorporates local featuresdefined on parse trees of the source language.
Non-local features, which encode information from parsetrees of the target language, have never been ex-ploited before due to the computational complexityof normalization in max-likelihood estimation.Consequently, we would like to learn syn-chronous grammars in a discriminative way that candirectly maximize the end-to-end translation qualitymeasured by BLEU (Papineni et al 2002), and isalso able to incorporate non-local features from tar-get parse trees.We thus propose a max-margin estimation method1More precisely, the discriminative model by Xiao et al(2012) maximizes conditional likelihood.255to discriminatively induce synchronous grammar di-rectly from sentence pairs without word alignments.We try to maximize the margin between a referencetranslation and a candidate translation with transla-tion errors that are measured by BLEU.
The moreserious the translation errors, the larger the margin.In this way, our max-margin method is able to learnsynchronous grammars according to their translationperformance.
We further incorporate various non-local features defined on target parse trees.
We ef-ficiently calculate the non-local feature values of atranslation over its exponential derivation space us-ing the inside-outside algorithm.
Because our max-margin estimation optimizes feature weights only bythe feature values of Viterbi and reference transla-tions, we are able to efficiently perform optimizationeven with non-local features.We apply the proposed max-margin estimationmethod to learn synchronous grammars for a hi-erarchical phrase-based translation system (Chiang,2007) which typically produces state-of-the-art per-formance.
With non-local features defined on tar-get parse trees, our max-margin method significantlyoutperforms the baseline that uses synchronousrules learned from the traditional pipeline by 1.3BLEU points on large-scale Chinese-English bilin-gual training data.The remainder of this paper is organized as fol-lows.
Section 2 presents the discriminative syn-chronous grammar induction model with the non-local features.
In Section 3, we elaborate our max-margin estimation method which is able to directlyoptimize BLEU, and discuss how we induce gram-mar rules.
Local and non-local features are de-scribed in Section 4.
Finally, in Section 5, we verifythe effectiveness of our method through experimentsby comparing it against both the traditional pipelineand max-likelihood estimation method.2 Discriminative Model with Non-localFeaturesLet S denotes the set of all strings in a source lan-guage.
Given a source sentence s ?
S , T (s) denotesall candidate translations in the target language thatcan be generated by a synchronous grammar G. Atranslation t ?
T (s) is generated by a sequence oftranslation steps (r1, ..., rn), where we apply a syn-??
?
??
??
?
?10 2 3 4 5[1,3][1,5][0,5][1,6][0,6][4,6]10 2 3 4 5 6bushi yu shalong juxing huitanr1: ?
yu shalong?
with Sharon ?r2: ?
X juxing huitan?
held a talk X ?r3: ?
bushi X ?
Bush X ?Figure 1: A derivation of a sentence pair represented bya synchronous tree.
The above and below part are theparses in the source language side and the target languageside respectively.
Left subscript of a node X denotes thesource span, while right subscript denotes the target span.A dashed line denotes an alignment from a source spanto a target span.
The annotation for a dashed line cor-responds to the rewriting rule used in the correspondingstep of the derivation.chronous rule r ?
G in one step.
We refer to sucha sequence of translation steps as a derivation (SeeFigure 1) and denote it as d ?
D(s), where D(s)represents the derivation space of a source sentence.Given an input source sentence s, we output a pair?t,d?
in SMT.
Thus, we study the triple ?s, t,d?
inSMT.In our discriminative model, we calculate thevalue of a triple ?s, t,d?
according to the followingscoring function:f(s, t,d) = ?T?
(s, t,d) (1)where ?
?
?
is a feature weight vector, and ?
is thefeature function.There are exponential outputs in SMT.
Thereforeit is necessary to factorize the feature function in or-der to perform efficient calculation over the SMToutput space using dynamic programming.
We de-compose the feature function of a triple ?s, t,d?
into256?
??[1,5]??
??
[1,6]Figure 2: Example features for the derivation in Figure 1.Shaded nodes denote information encoded in the feature.a sum of values of each synchronous rule in thederivation d.?
(s, t,d) =?r?d?
(r, s)?
??
?local+?r?d?
(r, s, t)?
??
?non-local(2)Our feature functions include both local and non-local features.
A feature is a local feature if andonly if it can be factored among the translation stepsin a derivation.
In other words, the value of a lo-cal feature for ?s, t,d?
can be calculated as a sum oflocal scores in each translation step, and the calcula-tion of each local score only requires to look at therule used in corresponding step and the input sen-tence.
Otherwise, the feature is a non-local feature.Our discriminative model allows to incorporate non-local features that are defined on target translations.For example, a rule feature in Figure 2(a), whichindicates the application of a specific rule in aderivation, is a local feature.
A source span bound-ary feature in Figure 2(b) that is defined on thesource parse tree is also a local feature.
However,a target span boundary feature in Figure 2(c), whichassesses the target parse structure, is a non-local fea-ture.
According to Figure 1, the span is parsed instep r2, but it also depends on the translation bound-ary word ?held?
generated in previous step r1.
Wewill describe the details of both local and non-localfeatures that we use in Section 4.Non-local features enable us to model the targetparse structure in a derivation.
However, it is com-putationally expensive to calculate the expected val-ues of non-local features over D(s), as non-localfeatures require to record states of target boundarys, S, S s is a sentence in a source language;S means source training sentences;S denotes all the possible sentences;t, T, T symbols for the target language thatsimilar to s, S, S;d, D derivation and derivation space;D(s) space of derivations fora source sentence;D(s, t) space of derivations fora source sentence with its translation;H(s) hypergraph that represents D(s);H(s, t) hypergraph that represents D(s, t);Table 1: Notations in this paper.
We give an abstract ofrelated notations for clarity.words and result in an extremely large number ofstates during dynamic programming.
Fortunately,when integrating out derivations over the derivationspace D(s, t) of a source sentence and its transla-tion, we can efficiently calculate the non-local fea-tures.
Because all derivations in D(s, t) share thesame translation, there is no need to maintain statesfor target boundary words.
We will discuss this com-putational problem in details in Section 3.3.
In theproposed max-margin estimation described in nextsection, we only need to integrate out derivationfor a Viterbi translation and a reference translationwhen updating feature weights.
Therefore, the de-fined non-local features allow us to not only exploreuseful knowledge on the target parse trees, but alsocompute them efficiently over D(s, t) during max-margin estimation.3 Max-Margin EstimationIn this section, we describe how we use a paralleltraining corpus {S,T} = {(s(i), t(i))}Ni=1 to esti-mate feature weights ?, which contain parameters ofthe induced synchronous grammars and the definednon-local features.We choose the parameters that maximize thetranslation quality measured by BLEU using themax-margin estimation (Taskar et al 2004).
Mar-gin refers to the difference of the model score be-tween a reference translation t(i) and a candidatetranslation t. We hope that the worse the transla-tion quality of t, the larger the margin between tand t(i).
In this way, we penalize larger translation257errors more severely than smaller ones.
This intu-ition is expressed by the following equation.min 12??
?2 (3)s.t.
f(s(i), t(i))?
f(s(i), t) ?
cost(t(i), t)?t ?
T (s(i))Here, f(s, t) is the feature function of a translation,and cost function cost(t(i), t) measures the trans-lation errors of a candidate translation t comparingwith a reference translation t(i).
We define the costfunction via the widely-used translation evaluationmetric BLEU.
We use the smoothed sentence levelBLEU-4 (Lin and Och, 2004) here:cost(t(i), t) = 1?
BLEU-4(t(i), t) (4)In Section 3.1, we will discuss how we use thescoring function f(s, t,d) to calculate f(s, t).
Thenin Section 3.2, we recast the equation (3) as an un-constrained empirical loss minimization problem,and describe the learning algorithm for optimizing?
and inducing G. Finally, we give the details ofinference for the learning algorithm in Section 3.3.3.1 Integrate Out Derivation by AveragingAlthough we only model the triple ?s, t,d?
in theequation (1), it?s necessary to calculate the scoringfunction f(s, t) of a translation by integrating outthe variable of derivation as derivation is not ob-served in the training data.We use an averaging computation over all possi-ble derivations of a translation D(s, t).
We call thisan average derivation based estimation:f(s, t) = 1|D(s, t)|?d?D(s,t)f(s, t,d) (5)The ?average derivation?
can be considered as thegeometric central point in the space D(s, t).Another possible way to deal with the latentderivation is max-derivation, which uses the max-operator over D(s, t).
The max derivation methodsets f(s, t) as maxd?D(s,t) f(s, t,d).
It is oftenadopted in traditional SMT systems.
Nevertheless,we instead use average-derivation for two reasons.22Imagine that H(s, t) in the Algorithm 1 is replaced by amaximum derivation inH(s, t).First, as a translation has an exponential number ofderivations, finding the max derivation of a refer-ence translation for learning is nontrivial (Chiang etal., 2009).
Second, the max derivation estimationwill result in a low rule coverage, as rules in a maxderivation only covers a small fraction of rules inthe D(s, t).
Because rule coverage is important insynchronous grammar induction, we would like toexplore the entire derivation space using the averageoperator.3.2 Learning AlgorithmWe reformulate the equation (3) as an unconstrainedempirical loss minimization problem as follows:min ?2??
?2 + 1NN?n=1L(s(i), t(i), ?)
(6)Where ?
denotes the regularization strength forL2-norm.
The loss function of a sentence pairL(s(i), t(i), ?)
is a convex hinge loss function de-noted by:max{0,?f(s(i), t(i)) (7)+ maxt?T (s(i))(f(s(i), t) + cost(t(i), t))}According to the second max-operator in thehinge loss function, the optimization towards BLEUis expressed by cost-augmented inference.
Cost-augmented inference finds a translation that has amaximum model score augmented with cost.t?
= maxt?T (s(i))(f(s(i), t) + cost(t(i), t))(8)We applied the Pegasos algorithm for the op-timization of equation (6) (Shalev-Shwartz et al2007).
This is an online algorithm, which alternatesbetween stochastic gradient descent steps and pro-jection steps.
When the loss function is non-zero, itupdates weights according to the sub-gradient of thehinge loss function.
Using the average scoring func-tion in the equation (5), the sub-gradient of hingeloss function for a sentence pair is the difference ofaverage feature values between a Viterbi translation258Algorithm 1 UPDATE(s, t, ?,G) ?
One step in online algorithm.
s, t are short for s(i), t(i) here1: H(s, t)?
BIPARSE(s, t, ?)
?
Build hypergraph of reference translation2: G?G +H(s, t) ?
Discover rules fromH(s, t)3: t?, d??
argmax?t?,d??
?D(s) f(s, t?,d?)
+ cost(t, t?)
?
Find Viterbi translation4: H(s, t?)?
BIPARSE(s, t?, ?)
?
Build hypergraph of Viterbi translation5: if f(s, t) < f(s, t?)
+ cost(t, t?)
then6: ?
?
(1?
??)?
+ ?
?
?L??
(H(s, t),H(s, t?))
?
Update ?
by gradient?L??
and learning rate ?7: ?
?
min {1, 1/????? }
?
?
?
Projection by scaling8: return G, ?and a reference translation.?L?
?= 1|D(s(i), t(i))|?d?D(s(i),t(i))?
(s(i), t(i),d)?
1|D(s(i), t?)|?d?D(s(i),t?)?
(s(i), t?,d) (9)Algorithm 1 shows the procedure of one step inthe online optimization algorithm.
The procedurediscovers rules and updates weights in an onlinefashion.
In the procedure, we first biparse the sen-tence pair to construct a synchronous hypergraph ofa reference translation (line 1).
In the biparsing al-gorithm, synchronous rules for constructing hyper-edges are not required to be in G, but can be anyrules that follow the form defined in Chiang (2007).Thus, the biparsing algorithm can discover new rulesthat are not in G. Then we collect the translationrules discovered in the hypergraph of the referencetranslation (line 2), which are rules indicated by hy-peredges in the hypergraph.
We then calculate theViterbi translation according to the scoring functionand cost function (see Section 3.3) (line 3), and buildthe synchronous hypergraph for the Viterbi transla-tion (line 4).
Finally, we update weights according tothe Pegasos algorithm (line 5).
The sub-gradient iscalculated based on the hypergraph of Viterbi trans-lation and reference translation.In practice, in order to process the data in a paral-lel manner, we use a larger step size of 1000 for thelearning algorithm.
In each step of our online opti-mization algorithm, we first biparse 1000 referencesentence pairs in parallel.
Then, we collect grammarrules from the generated reference hypergraphs.
Af-ter that, we compute the gradients of 1000 sentencepairs in parallel, by calculating feature weights overreference hypergraphs and Viterbi hypergraphs.
Fi-nally, we update the feature weights using the sumof these gradients.3.3 InferenceThere are two parts that need to be calculated inthe learning algorithm: finding a cost-augmentedViterbi translation according to the scoring func-tion and cost function (Equation 8), and constructingsynchronous hypergraphs for the Viterbi and refer-ence translation so as to discover rules and calculateaverage feature values in Equation (9).
Followingthe traditional decoding procedure, we resort to thecube-pruning based algorithm for approximation.To find the Viterbi translation, we run the tra-ditional translation decoding algorithm (Chiang,2007) to get the best derivation.
Then we usethe translation yielded by the best derivation as theViterbi translation.
In order to obtain the BLEUscore in the cost function, we need to calculate thengram precision.
It is calculated in a way similar tothe calculation of the ngram language model.
Thecomputation of BLEU-4 requires to record 3 bound-ary words in both the left and right side during dy-namic programming.
Therefore, even when we usea language model whose order is less than 4, we stillexpands the states to record 3 boundary words so asto calculate the cost measured by BLEU.We build synchronous hypergraphs using thecube-pruning based biparsing algorithm (Xiao et al2012).
Algorithm 2 shows the procedure.
Usinga chart, the biparsing algorithm constructs k-bestalignments for every source word (lines 1-5) and k-best hyperedges for every source span (lines 6-13)from the bottom up.
Thus, a synchronous hyper-graph is generated during the construction of thechart.
More specifically, for a source span, it firstcreates cubes L for all source parses ?
that are in-259Algorithm 2 BIPARSE(s, t, ?)
?
(Xiao et al 2012) Create k-best alignments for each source word1: for i?
1, .., |s| do2: for j ?
1, .., |t| do3: Lj ?
{?, tj} ?
si aligns to tj or not4: L?
?L1, ..., L|t|?5: chart[s, i]?
KBEST(L,?,?
) Create k-best hyperedges for each source span6: H?
?7: for h?
1, .., |s| do ?
h is the size of span8: for all i, j s.t.
j ?
i = h do9: L?
?10: for ?
inferable from chart do11: L?
L + ?chart[?1], ..., chart[?|?|]?12: chart[X, i, j]?
KBEST(L,?,?
)13: H?H + chart[X, i, j] ?
save hyperedges14: returnHferable from the chart (lines 9-11).
Here ?i is a par-tial source parse that covers either a single sourceword or a span of source words.
Then it uses thecube pruning algorithm to keep the top k derivationsamong all partial derivations that share the samesource span [i, j] (line 12).
Notably, this biparsingalgorithm does not require specific translation rulesas input.
Instead, it is able to discover new syn-chronous grammar rules when constructing a syn-chronous hypergraph: extracting each hyperedge inthe hypergraph as a synchronous rule.Based on the biparsing algorithm, we are able toconstruct the reference hypergraph H(s(i), t(i)) andViterbi hypergraph H(s(i), t?).
By the reference hy-pergraph, we collect new synchronous translationrules and record them in the grammar G. We alsocalculate the average feature values of hypergraphsusing the inside-outside algorithm (Li et al 2009),so as to compute the gradients.4 FeaturesOne advantage of the discriminative method is thatit enables us to incorporate arbitrary features.
Asshown in Section 2, our model incorporates both lo-cal and non-local features.4.1 Local FeaturesRule features We associate each rule with an indi-cator feature.
Each indicator feature counts the num-ber of times that a rule appears in a derivation.
Inthis way, we are able to learn a weight for every ruleaccording to the entire structure of sentence.Word association features Lexicalized featuresare widely used in traditional SMT systems.
Herewe adopt two lexical weights called noisy-or fea-tures (Zens and Ney, 2004).
The noisy-or featureis estimated by word translation probabilities outputby GIZA++.
We set the initial weight of these twolexical scores with equivalent positive values.
Thelexical weights enable our system to score and rankthe hyperedges at the beginning.
Although wordalignment features are used, we do not constrain thederivation space of a sentence pair by prefixed wordalignment, and do not require any heuristic align-ment combination strategy.Length feature We integrate the length of targettranslation that is used in traditional SMT system asour feature.Source span boundary features We use this kindof feature to assess the source parse tree in a deriva-tion.
Previous work (Xiong et al 2010) has shownthe importance of phrase boundary features fortranslation.
Actually, this kind of feature is a goodcue for deciding the boundary where a rule is to belearnt.
Following Taskar et al(2004), for a bispan[i, j, k, l] in a derivation, we define the feature tem-plates that indicates the boundaries of a span by itsbeginning and end words: {B : si+1;E : sj ;BE :si+1, sj}.Source span orientation features Orientationfeatures are only used for those spans that are swap-ping.
In Figure 1, the translation of source span [1, 3]is swapping with that of span [4, 5] by r2, thus ori-entation feature for span [1, 3] is activated.
We alsodefine three feature templates for a swapping spansimilar to the boundary features: {B : si+1;E :sj ;BE : si+1, sj}.
In practice, we add a prefix tothe orientation features so as to distinguish these fea-tures from the boundary features.4.2 Non-local FeaturesTarget span boundary features We also want toassess the target tree structure in a derivation.
Wedefine these features in a way similar to source spanboundary features.
For a bispan [i, j, k, l] in a deriva-tion, we define the feature templates that indicates260System Grammar Size MT03 MT04 MT05 Avg.Moses 302.5M 34.26 36.56 32.69 34.50Baseline 77.8M 33.83 35.81 33.23 34.29Max-margin 59.4M 34.62 37.14 34.00 35.25+Sparse feature 35.48 37.31 34.07 35.62Table 2: Experiment results.
Baseline is an in-house implementation of hierarchical phrase based system.
Mosesdenotes the implementation of hierarchical phrased-model in Moses (Koehn et al 2007).
+Sparsefeature meansthat those sparse features used in the grammar induction are also used during decoding.
The improvement of max-margin over Baseline is statistically significant (p < 0.01).target span boundary as: {B : tk+1;E : tl;BE :tk+1, tl}.Target span orientation features Similar targetorientation features are used for a swapping span[i, j, k, l] with feature templates {B : tk+1;E :tl;BE : tk+1, tl}.Relative position features Following Blunsomand Cohn (Blunsom and Cohn, 2006), we integratefeatures indicating the closeness to the alignmentmatrix diagonal.
For an aligned word pair withsource position i and target position j, the value ofthis feature is | i|s| ?j|t| |.
As this feature dependson the length of the target sentence, it is a non-localfeature.Language model We also incorporate an ngramlanguage model which is an important componentin SMT.
For efficiency, we use a 3-gram languagemodel trained on the target side of our training dataduring the induction of synchronous grammars.5 ExperimentIn this section, we present our experiments on theNIST Chinese-to-English translation tasks.
We firstcompare our max-margin based method with the tra-ditional pipeline on a large bitext which contains1.1 million sentences.
We then present a detailedcomparison on a smaller dataset, in order to analyzethe effectiveness of max-margin estimation compar-ing with the max likelihood estimation (Xiao et al2012), and also the effectiveness of the non-localfeatures that are defined on the target side.5.1 SetupThe baseline system is the hierarchical phrase basedsystem (Chiang, 2007).
We used a bilingual corpusthat contains 1.1M sentences (44.6 million words)of up to length 40 from the LDC data.3 Our 5-gramlanguage model was trained by SRILM toolkit (Stol-cke, 2002).
The monolingual training data includesthe Xinhua section of the English Gigaword corpusand the English side of the entire LDC data (432 mil-lion words).We used the NIST 2002 (MT02) as our develop-ment set, and the NIST 2003-2005 (MT03-05) as thetest set.
Case-insensitive NIST BLEU-4 (Papineniet al 2002) is used to measure translation perfor-mance, and also the cost function in the max-marginestimation.
Statistical significance in BLEU differ-ences was tested by paired bootstrap re-sampling(Koehn, 2004).
We used minimum error rate train-ing (MERT) (Och, 2003) to optimize feature weightsfor the traditional log-linear model.We used the same decoder as the baseline systemin all estimation methods.
Without special explana-tion, we used the same features as those in the tra-ditional pipeline: forward and backward translationprobabilities, forward and backward lexical weights,count of extracted rules, count of glue rules, lengthof translation, and language model.
For the lexicalweights we used the noisy-or in all configurationsincluding the baseline system.
For the discrimina-tive grammar induction, rule translation probabili-ties were calculated using the expectations of rulesin the synchronous hypergraphs of sentence pairs.As our max-margin synchronous grammar induc-tion is trained on the entire bitext, it is necessary toload all the rules into the memory during training.To control the size of rule table, we used Viterbi-3Including LDC2002E18, LDC2003E07, LDC2003E14,LDC2004T07, LDC2005T06 and Hansards portion ofLDC2004T08.261System Feature Function MT03 MT04 MT05 Avg.Baseline ?
31.76 33.08 31.06 31.96Max-likelihood local 32.84 34.54 31.61 33.00Max-margin local 32.97 34.92 31.99 33.29local,non-local 33.27 34.83 32.32 33.47Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus.
For max-margin method, wepresent two results according to the usages of non-local features.
The max-margin with non-local features significantlyoutperforms the Baseline (p < 0.01) and also the max-likelihood estimation (p < 0.05).pruning (Huang, 2008) when collecting rules asshown in line 2 of optimization procedure in Section3.2.
Furthermore, we aggressively discarded thoselarge rules (The number of source symbols or thenumber of target symbols are more than two) thatoccur only in one sentence.
Whenever the learningalgorithm processes 50K sentences, we performedthis discarding operation for large rules.5.2 Result on Large DatasetTable 2 shows the translation results.
Our methodinduces 59.4 million synchronous rules, which are76.3% of the grammar size of baseline.
Note thatMoses allows the boundary words of a phrase to beunaligned, while our baseline constraints the initialphrase to be tightly consistent with word alignment.Therefore, Moses extract a much larger rule tablethan that of our baseline.With fewer translation rules, our method obtainsan average improvement of +0.96 BLEU points onthe three test sets over the Baseline.
As the differ-ence between the baseline and our max-margin syn-chronous grammar induction model only lies in thegrammar, this result clearly denotes that our learntgrammar does outperform the grammar extracted bythe traditional two-step pipeline.We also incorporate the sparse features during de-coding in a way similar to Xiao et al(2012) andDyer et al(2011).
In order to optimize these sparsefeatures with the dense features by MERT, we groupfeatures of the same type into one coarse ?summaryfeature?, and get three such features including: rule,phrase-boundary and phrase orientation features.
Inthis way, we rescale the weights of the three ?sum-mary features?
with the 8 dense features by MERT.We achieve a further improvement of +0.37 BLEUpoints.
Therefore, our training algorithm is able tolearn the useful information encoded by the sparsefeatures for translation.5.3 Comparison of Estimation Objective andNon-Local FeatureWewant to investigate whether the max-margin esti-mation is able to outperform the max-likelihood es-timation method (Xiao et al 2012).
Therefore wecarried out experiments to compare them directly.As the max-margin method is able to use non-localfeatures, we compare two settings of features for themax-margin method.
One uses only local features,the other uses both local and non-local features.
Be-cause the training procedure need to run on the entirecorpus, which is time consuming, we therefore usea smaller corpus containing 50K sentences from theentire bitext for comparison.Table 3 shows the results.
When using only localfeatures, the max-margin method consistently out-performs the max-likelihood method in all three testsets.
This clearly shows the advantage of learninggrammars by optimizing BLEU over likelihood.When incorporating the non-local features intothe max-margin method, we achieve further im-provement against the max-margin method with-out non-local features.
With non-local features,our max-margin estimation method outperforms thebaseline by 1.5 BLEU points, and is better thanthe max-likelihood estimation by 0.5 BLEU points.Based on these results, we believe that non-local fea-tures, which encode information from target parsestructures, are helpful for grammar induction.
Thisfurther confirms the advance of the max-margin es-timation, as it provides us a convenient way to usenon-local features.2626 Related WorkAs the synchronous grammar is the key compo-nent in SMT systems, researchers have proposedvarious methods to improve the quality of gram-mars.
In addition to the generative and discrimina-tive models introduced in Section 1, researchers alsohave made efforts on word alignment and grammarweight rescoring.The first line is to modify word alignment by ex-ploring information of syntactic structures (May andKnight, 2007; DeNero and Klein, 2010; Pauls etal., 2010; Burkett et al 2010; Riesa et al 2011).Such syntactic information is combined with wordalignment via a discriminative framework.
Thesemethods prefer word alignments that are consistentwith syntactic structure alignments.
However, la-beled word alignment data are required in order tolearn the discriminative model.Yet another line is to rescore the weights of trans-lation rules.
This line of work tries to improve therelative frequency estimation used in the traditionalpipeline.
They rescore the weights or probabilitiesof extracted rules.
The rescoring is done by usingthe similar latent log-linear model as ours (Blun-som et al 2008; Ka?a?ria?inen, 2009; He and Deng,2012), or incorporating various features using la-beled word aligned bilingual data (Huang and Xi-ang, 2010).
However, in rescoring, translation rulesare still extracted by the heuristic two-step pipeline.Therefore these previous work still suffers from theinelegance problem of the traditional pipeline.Our work also relates to the discriminative train-ing (Och, 2003; Watanabe et al 2007; Chiang et al2009; Xiao et al 2011; Gimpel and Smith, 2012)that has been widely used in SMT systems.
Notably,these discriminative training methods are not used tolearn grammar.
Instead, they assume that grammarare extracted by the traditional two-step pipeline.7 ConclusionIn this paper we have presented a max-margin esti-mation for discriminative synchronous grammar in-duction.
By associating the margin with the transla-tion quality, we directly learn translation rules thatoptimize the translation performance measured byBLEU.
Max-margin estimation also provides us aconvenient way to incorporate non-local features.Experiment results validate the effectiveness of opti-mizing parameters by BLEU, and the importance ofincorporating non-local features defined on the tar-get language.
These results confirm the advantage ofour max-margin estimation framework as it can bothoptimize BLEU and incorporate non-local features.Feature engineering is very important for discrim-inative models.
Researchers have proposed varioustypes of features for machine translation, which areoften estimated from word alignments.
We wouldlike to investigate whether further improvement canbe achieved by incorporating such features, espe-cially the context model (Shen et al 2009) in thefuture.
Because our proposed model is quite general,we are also interested in applying this method toinduce linguistically motivated synchronous gram-mars for syntax-based SMT.AcknowledgmentsThe first author was partially supported by 863State Key Project (No.
2011AA01A207) andNational Key Technology R&D Program (No.2012BAH39B03).
We are grateful to the anony-mous reviewers for their insightful comments.
Wealso thank Yi Lin for her invaluable feedback.ReferencesPhil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
InProb.
ACL 2006, July.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
ACL 2008.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proc.
ACL 2009.David Burkett, John Blitzer, and Dan Klein.
2010.Joint parsing and alignment with weakly synchronizedgrammars.
In Proc.
NAACL 2010.Colin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.In Proc.
SSST 2007, NAACL-HLT Workshop on Syntaxand Structure in Statistical Translation, April.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proc.
NAACL 2009.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.263Trevor Cohn and Phil Blunsom.
2009.
A Bayesian modelof syntax-directed tree to string grammar induction.
InProc.
EMNLP 2009.John DeNero and Dan Klein.
2010.
Discriminative mod-eling of extraction sets for machine translation.
InProc.
ACL 2010.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proc.
EMNLP 2008.Chris Dyer, Kevin Gimpel, Jonathan H. Clark, andNoah A. Smith.
2011.
The cmu-ark german-englishtranslation system.
In Proc.
WMT 2011.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.
InProc.
NAACL 2012.Xiaodong He and Li Deng.
2012.
Maximum expectedbleu training of phrase and lexicon translation models.In Proc.
ACL 2012.Fei Huang and Bing Xiang.
2010.
Feature-rich discrimi-native phrase rescoring for smt.
In Proc.
Coling 2010.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
ACL 2008.Matti Ka?a?ria?inen.
2009.
Sinuhe ?
statistical machinetranslation using a globally trained conditional expo-nential family translation model.
In Proc.
EMNLP2009.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proc.ACL 2007 (demonstration session).Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP2004.Abby Levenberg, Chris Dyer, and Phil Blunsom.
2012.A bayesian model for learning scfgs with discontigu-ous rules.
In Proc.
EMNLP 2012.
Association forComputational Linguistics, July.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proc.
ACL 2009.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metrics formachine translation.
In Pro.
Coling 2004.Daniel Marcu andWilliamWong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
EMNLP 2002.Jonathan May and Kevin Knight.
2007.
Syntactic re-alignment models for machine translation.
In Proc.EMNLP 2007.Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-suke Mori, and Tatsuya Kawahara.
2011.
An unsuper-vised model for joint phrase alignment and extraction.In Proc.
ACL 2011.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL 2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proc.
ACL 2002.Adam Pauls, Dan Klein, David Chiang, and KevinKnight.
2010.
Unsupervised syntactic alignment withinversion transduction grammars.
In Proc.
NAACL2010.Jason Riesa, Ann Irvine, and Daniel Marcu.
2011.Feature-rich language-independent syntax-basedalignment for statistical machine translation.
In Proc.EMNLP 2011.Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.2007.
Pegasos: Primal estimated sub-gradient solverfor svm.
In Proc.
ICML 2007.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proc.
EMNLP 2009.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, andChristopher Manning.
2004.
Max-margin parsing.
InProc.
EMNLP 2004.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proc.
EMNLP-CoNLL2007.Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.2011.
Fast generation of translation forest for large-scale smt discriminative training.
In Proc.
EMNLP2011.Xinyan Xiao, Deyi Xiong, Yang Liu, Qun Liu, andShouxun Lin.
2012.
Unsupervised discriminative in-duction of synchronous grammar for machine transla-tion.
In Proc.
Coling 2012.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Learn-ing translation boundaries for phrase-based decoding.In Proc.
NAACL2010.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical machine translation.
In Prob.NAACL 2004.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InProc.
ACL 2008.264
