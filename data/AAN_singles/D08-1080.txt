Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 763?772,Honolulu, October 2008. c?2008 Association for Computational LinguisticsTopic-Driven Multi-Document Summarizationwith Encyclopedic Knowledge and Spreading ActivationVivi NastaseEML Research gGmbHHeidelberg, Germanynastase@eml-research.deAbstractInformation of interest to users is often dis-tributed over a set of documents.
Userscan specify their request for information as aquery/topic ?
a set of one or more sentencesor questions.
Producing a good summary ofthe relevant information relies on understand-ing the query and linking it with the associ-ated set of documents.
To ?understand?
thequery we expand it using encyclopedic knowl-edge in Wikipedia.
The expanded query islinked with its associated documents throughspreading activation in a graph that representswords and their grammatical connections inthese documents.
The topic expanded wordsand activated nodes in the graph are used toproduce an extractive summary.
The methodproposed is tested on the DUC summariza-tion data.
The system implemented ranks highcompared to the participating systems in theDUC competitions, confirming our hypothesisthat encyclopedic knowledge is a useful addi-tion to a summarization system.1 IntroductionTopic-driven summarization reflects a user-basedsummarization task: from a set of documents de-rive a summary that contains information on a spe-cific topic of interest to a user.
Producing a goodsummary relies on ?understanding?
the user?s infor-mation request, and the documents to be summa-rized.
It is commonly agreed that the verbal partof a text provides pointers to a much larger body ofknowledge we assume the listener has.
An Amer-ican citizen, for example, when told There will befireworks on July 4th, understands that there willbe a celebration involving fireworks on the occasionof the U.S.
Independence Day.
Understanding anutterance implies lexical, common-sense and ency-clopedic knowledge.
Lexical knowledge is usuallyincorporated in systems through machine readabledictionaries, wordnets or thesauri.
Common-senseand encyclopedic knowledge were harder to capture,but recently Wikipedia has opened the possibility ofaccessing such knowledge on a large scale, and innumerous languages.To ?understand?
a user?s information request ?one or more sentences or questions (the topic ofthe summary) ?
summarization systems try to ex-pand it.
This will provide later stages of process-ing with more keywords/keyphrases for retrievingfrom the documents relevant fragments.
In this pa-per we experiment with Wikipedia for topic expan-sion.
The body of research involving Wikipediaas a source of knowledge is growing fast, as theNLP community finds more and more applicationsof this useful resource: it is used to acquire knowl-edge (Suchanek et al, 2007; Auer et al, 2007);to induce taxonomies and compute semantic relat-edness (Ponzetto & Strube, 2007b; 2007a); as asource of features for text classification (Gabrilovich& Markovitch, 2006) and for answering questions(Ahn et al, 2004; Katz et al, 2005).
The work pre-sented here uses hyperlinks in Wikipedia articles toexpand keywords and keyphrases extracted from thequery.
Ambiguous words are disambiguated usingthe context provided by the query.?Understanding?
the documents to be summa-rized implies identifying the entities mentioned, how763they are connected, and how they are related to theentities in the topic.
For this, we start again from thetopic, and spread an activation signal in a large graphthat covers all documents for this topic ?
nodes arewords/named entities in the texts, links are gram-matical relations.
This way we cross from the topicto the documents, and combine information whichis important in the topic with information which isimportant and relevant in the documents.
We takethe most highly activated nodes as additional topicexpansions, and produce an extractive summary bychoosing from the sentences that connect the topicexpansion words in the large document graph.The experiments confirm that Wikipedia is asource of useful knowledge for summarization, andthat further expanding the topic within the associ-ated set of documents improves the summarizationresults even more.
We compare the performanceof the summarization system to that of participatingsystems in the DUC competitions.
The system wedescribe ranks 2nd, 9th and 5th in terms of ROUGE-SU4 on the DUC 2005, DUC 2006 and DUC 2007data respectively.2 Related WorkWhile the recent exponential increase in the amountof information with which we must cope makessummarization a very desirable tool in the present,summarization is not a novel task.
Rath et al (1961)and Edmundson (1969) have explored extractivesummary formation, and have raised important eval-uation issues for extractive summaries when com-pared to several human produced gold standards.Nowadays, summarization methods try to incorpo-rate tools, methodologies and resources developedover the past decades.
The NIST organized com-petitions under the Document Understanding Con-ferences ?
DUC (since 2008, Text Analysis Confer-ence (TAC))1 events provide a forum for the compar-ison of a variety of approaches, ranging from knowl-edge poor ?
Gotti et al (2007) rely exclusively ona parser, without any additional sources of informa-tion ?
to knowledge rich and complex ?
GISTexter(Hickl et al, 2007) combines question answering,textual entailment, topic signature modules and a va-1http://duc.nist.gov/, http://www.nist.gov/tac.riety of knowledge sources for summarization.The most frequently used knowledge source inNLP in general, and also for summarization, isWordNet (Fellbaum, 1998).
Barzilay & Elhadad(1999) use WordNet to model a text?s content rel-ative to a topic based on lexical chains.
The sen-tences intersected by the most and strongest chainsare chosen for the extractive summary.
Alterna-tive sources for query expansion and document pro-cessing have also been explored.
Amini & Usunier(2007) use the documents to be summarized them-selves to cluster terms, and thus expanding the query?internally?.
More advanced methods for query ex-pansion use ?topic signatures?
?
words and gram-matically related pairs of words that model the queryand even the expected answer from sets of docu-ments marked as relevant or not (Lin & Hovy, 2000;Harabagiu, 2004).Graph-based methods for text summarizationwork usually at the level of sentences (Erkan &Radev, 2004; Mihalcea & Tarau, 2004).
Edgeweights between sentences represent a similaritymeasure, and a PageRank algorithm is used to deter-mine the sentences that are the most salient from acollection of documents and closest to a given topic.At the word level, Leskovec et al (2004) builda document graph using subject-verb-object triples,semantic normalization and coreference resolution.They use several methods (node degree, PageRank,Hubs, etc.)
to compute statistics for the nodes inthe network, and use these as attribute values ina machine learning algorithm, where the attributethat is learned is whether the node should appearin the final summary or not.
Annotations for train-ing come from human produced summaries.
Mo-hamed & Rajasekaran (2006) incrementally builda graph for a document collection by combininggraph-representations of sentences.
Links betweenentities in a sentence can be isa (within an NP)or related to (between different phrases in a sen-tence).
Nodes and relations are weighted accordingto their connectivity, and sentence selection for thefinal summary is based on the most highly connectednodes.
Ye & Chua (2006) build an extractive sum-mary based on a concept lattice, which captures ina hierarchical structure co-occurrences of conceptsamong sentences.
Nodes higher in this structure cor-respond to frequently co-occurring terms, and are764<topic><num> D0704A < /num><title> Amnesty International < /title><narr>What is the scope of operations of Amnesty Internationaland what are the international reactions to its activities?Give examples of charges lodged by the organization andcomplaints against it.< /narr><docs>...< /docs>< /topic><topic><num> D0740I < /num><title> round-the-world balloon flight < /title><narr>Report on the planning, attempts and first success-ful balloon circumnavigation of the earth by BertrandPiccard and his crew.< /narr><docs>...< /docs>< /topic>Figure 1: Sample topics from DUC 2007assumed to be more representative with respect tothe document topic.Mani & Bloedorn (1999) build a ?chronologi-cal?
graph, in which sentence order is respected andeach occurrence of a concept is a separate node.Edges between nodes cover several types of rela-tions: adjacency (ADJ); identity ?
instance of thesame word (SAME); other semantic links, in par-ticular synonymy and hypernymy; PHRASE linksconnect components of a phrase; NAME indicatenamed entities; COREF link coreferential name in-stances.
Among other things, they identify regionsof the text salient to a user?s query, based on spread-ing activation starting from query words in this doc-ument graph.
Spreading activation was introducedin the 60s and 70s to model psychological processesof memory activation in humans (Quillian, 1967;Collins & Loftus, 1975).In this approach we use Wikipedia as a source ofknowledge for related concepts ?
the texts of hyper-links in an article describing a concept are taken asits related concepts.
The query is further expandedby using spreading activation to move away from thetopic in a large graph that covers all documents fora given topic.
From the nodes thus reached we se-lect using a PageRank algorithm the ones that aremost important in the documents.
We study the im-pact of a decay parameter which controls how farto move from the topic, and the number of highestranked nodes to be added to the expanded topic.
Thesummary is built based on word associations in thedocuments?
graph.3 Topic Expansion with EncyclopedicKnowledge or WordNetIn DUC topic-driven multi-document summariza-tion, the topic has a title, an ID that links it to a set ofdocuments, and one or more sentences and/or ques-tions, as illustrated in Figure 1.Topic processing is done in several steps:1.
Preprocessing: Produce the dependency pairrepresentation of the topics using the StanfordParser2.
Pairs that have closed-class words are fil-tered out, and the remaining words are lemmatized3.We extract named entities (NEs), as the parserworks at the word level.
In the dependency pairswe replace an NE?s fragments with the complete NE.2a.
Query expansion with Wikipedia: Extractall open-class words and NEs from the topic, andexpand them using Wikipedia articles whose titlesare these words or phrases.For each Wikipedia article we extract as relatedconcepts the texts of the hyperlinks in the first para-graph (see Figure 24).
The reason for not includinglinks from the entire article body is that apart fromthe first paragraph, which is more focused, oftentimes hyperlinks are included whenever the under-lying concept appears in Wikipedia, without it being2http://nlp.stanford.edu/software/lex-parser.shtml3Using XTAG morphological database ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-1.5.tar.gz.4The left side shows the first paragraph as it appears on thepage, the right side shows the corresponding fragment from thesource file, with the annotations specific to Wikipedia.765MiningMining is the extraction of valuableminerals or other geological materi-als from the earth, usually (but not al-ways) from an ore body, vein or (coal)seam.
Materials recovered by min-ing include bauxite, coal, copper, gold,silver, diamonds, iron, precious met-als, lead, limestone, magnesite, nickel,phosphate, oil shale, rock salt, tin, ura-nium and molybdenum .
Any materialthat cannot be grown from agriculturalprocesses, or created artificially in alaboratory or factory , is usually mined.Mining in a wider sense comprises ex-traction of any non-renewable resource(e.g.
petroleum, natural gas ,or evenwater ).???Mining???
is the extraction of [[value(economics)|valuable]] [[mineral]]s or other[[geology|geological]] materials from theearth, usually (but not always) from an[[ore]] body, [[vein (geology)|vein]] or (coal)seam.
Materials recovered by mining include[[bauxite]], [[coal]], [[copper]], [[gold]],[[silver]], [[diamond]]s, [[iron]], [[preciousmetal]]s, [[lead]], [[limestone]], [[magnesite]],[[nickel]], [[phosphate]], [[oil shale]], [[Sodiumchloride|rock salt]], [[tin]], [[uranium]] and[[molybdenum]].
Any material that cannot be grownfrom [[agriculture|agricultural]] processes, orcreated [[Chemical synthesis|artificially]] in a[[laboratory]] or [[factory]], is usually mined.Mining in a wider sense comprises extraction of any[[non-renewable resource]] (e.g., [[petroleum]],[[natural gas]], or even [[fossil water|water]]).Extracted related concepts for mining:value (economics), valuable, mineral, geology, geological, ore, vein (geology), vein, coal, bauxite,copper, gold, silver, diamond, iron, precious metal, lead, limestone, magnesite, nickel, phosphate,oil shale, Sodium chloride, rock salt, agriculture, agricultural, Chemical synthesis, artificially,laboratory, factory, non-renewable resource, petroleum, natural gas, fossil water, water.Figure 2: First paragraph for article Mining in the English Wikipedia, and the extracted related concepts.Word Wikipedia expansion WordNet expansionmining lead, agricultural, mineral, gold, ore, productionpetroleum, nickel, iron, coal, tin, value,copper, water, bauxite, silver, diamondflight lift, air pass, trip, lam, overflight, ballooning,nonstop flight, aviation, soaring, air,flying, solo, break, escapestatus registered way, situation, mode, position, place,par, need, light, danger, health, state,standing, face, rank, demand,command, controlSouthern Poverty racism, American, United States, ?Law Center research, civil rights, litigationTable 1: Expanded concepts from DUC 2007 topics, after filtering based on the documents to be summarized.particularly relevant to the current article.To expand a word (or NE) W from the query, wesearch for an article having W as the title, or part ofthe title.1.
If one exact match is found (e.g.
SouthernPoverty Law Center), extract the related con-cepts for this article.2.
If several exact or partial matches are found,use the larger context of the query to narrowdown to the intended meaning.
For example,Turkey ?
referring to the country ?
appears inseveral topics in the DUC 2007 data.
Thereare multiple entries for ?Turkey?
in Wikipedia?
for the country, the bird, cities with this namein the U.S. among others.
We use a Lesk-likemeasure, and compute the overlap between thetopic query and the set of hyperlinks in the firstparagraph (Lesk, 1986).
We choose the ex-pansion for the entry with the highest overlap.If the query context does not help in disam-biguation, we use the expansions for all partialmatches that tie for the highest overlap.3.
If an article with the required name does notexist, the word will not be expanded.2b.
Query expansion with WordNet: Extract allnouns and NEs from the topic, and expand themwith hypernyms, hyponyms and antonyms in Word-Net 2.0:7661.
If an word (or NE) W from the query corre-sponds to an unambiguous entry in WordNet,expand that entry.2.
If W has multiple senses, choose the sense(s)which have the highest overlap with the query.To compute overlap, for a sense we take its ex-pansions (one step hypernyms, hyponyms andantonyms) and the words from the definition.3.
If W has no senses in WordNet, the word willnot be expanded.3.
Expansion filtering: Filter the list of relatedconcepts: keep only terms that appear in the docu-ment collection for the current topic.Table 1 includes the expansions obtained fromWikipedia and from WordNet respectively for anumber of words in topics from the DUC 2007 col-lection.
mining is a specific activity, involving a lim-ited set of materials.
While such connections cannotbe retrieved through hypernym, meronym or othersemantic relations in WordNet, they are part of ency-clopedic knowledge, and can be found in Wikipedia.flight is a more general concept ?
there are spe-cific types of flight, which appear as hyponymsin WordNet, while in Wikipedia it is more gener-ally described as the motion of an object throughair, which does not provide us with interesting re-lated concepts.
status is a very general concept,and rather vague, for which neither WordNet norWikipedia can provide very useful information.
Fi-nally, Wikipedia is rich in named entities, which arenot in the scope of a semantic lexicon.
WordNetdoes contain named entities, but not on the scale onwhich Wikipedia does.For the 45 topics from DUC 2007, the expansionwith Wikipedia generated 1054 additional words,while with WordNet 2510.
This difference comesfrom the fact that with Wikipedia it is mostly theNEs that are expanded, whereas with WordNet thecommon nouns, which are more numerous in thetopics.
The overlap between the two sets of expan-sions is 48 words (0.046 relative to Wikipedia ex-pansions, 0.019 relative to WordNet).4 Topic Expansion with SpreadingActivation and PageRankConcepts related to the ones in the topic provide agood handle on the documents to summarize ?
theyindicate parts of the document that should be in-cluded in the summary.
It is however obvious thatthe summary should contain more than that, andthis information comes from the documents to besummarized.
Amini & Usunier (2007) have shownthat expanding the query within the set of docu-ments leads to good results.
Following this idea, tofind more relevant concepts we look for words/NEswhich are related to the topic, and at the same timeimportant in the collection of documents for thegiven topic.
The methods described in this sectionare applied on a large graph that covers the entiredocument collection for one topic.
The documentsare processed in a similar way to the query ?
parsedwith the Stanford Parser, output in dependency rela-tion format, lemmatized using XTag?s morpholog-ical data file.
The graph consists of nodes corre-sponding to lemmatized words and NEs in the doc-uments, and edges correspoding to grammatical de-pendency relations.4.1 Spreading ActivationTo find words/NEs related to the topic we spread anactivation signal starting from the topic words andtheir expansions (in a manner similar to (Mani &Bloedorn, 1999), and using an algorithm inspired by(Anderson, 1983)), which are given a node weightof 1.
As we traverse the graph starting from thesenodes, the signal is propagated by assigning a weightto each edge and each node traversed based on thesignal strength.
The signal strength diminishes withthe distance from the node of origin depending on asignal decay parameter, according to the formula:wn(N0) = 1;st = (1 ?
decay) ?wn(Nt)Out(Nt);wn(Nt+1) = st;we(Nt, Nt+1)t+1 = we(Nt, Nt+1)t + st;where Nt is the current node; Nt+1 is the node weare moving towards; wn(Nt) is the weight of nodeNt; st is the signal strength at step t; Out(Nt)767Topic Topic expanded words Top ranked nodesD0738What is the status of miningin central and South Amer-ica?
Include obstacles en-countered.status, registered, South America, cen-tral, 1998, obstacle, mining, lead, agri-cultural, mineral, gold, ore, petroleum,nickel, iron, coal, tin, value, copper,water, bauxite, silver, diamond, in-clude, encountercompany, dollar, project, sector, iron,mine, silver, percent, big, value, indus-try, source, overturn, regulate, link, of-ficial, decree, financing, expert, firm,activity, estimate, state, For Peru, Peru,third, already, top, 12th, creation, tonD0717Describe the various law-suits against AmericanHome Products whichresulted from the use offenfluramine, also knownas Pondimin, and half ofthe diet drug combinationcalled ?fen-phen?.combination, set, half, American HomeProducts, know, fenfluramine, phen-termine, obesity, release, dexfenflu-ramine, use, United States, Wal-Mart,fen, describe, diet, call, drug, drugs,medication, patients, medicine, law-suit, right, court, damages, defendant,plaintiff, also, various, Pondimin, re-sultdrug, market, company, settle, re-dux, claim, American Home Products,make, cause, seek, cover, people, al-low, agree, dismiss, other, sue, case,Pondimin, state, link, million, award,user, estimate, thousand, file, think,note, damages, Harris CountyTable 2: Top ranked nodes after expanding the topic with spreading activation and PageRankis the number of outgoing edges from node Nt;we(Nt, Nt+1)t is the weight of the edge betweenNt and Nt+1 at time t (i.e., before actually travers-ing the edge and spreading the activation from Nt);we(Nt, Nt+1)t+1 is the weight of the edge afterspreading activation.
The weight of the edges is cu-mulative, to gather strength from all signals that passthrough the edge.
Activation is spread sequentiallyfrom each node in the (expanded) topic.The decay parameter is used to control how farthe influence of the starting nodes should reach ?
thelower the decay, the farther the signal can reach.4.2 PageRankThe previous step has assigned weights to edges inthe graph, such that higher weights are closer totopic and/or topic expanded words.
After this ini-tialization of the graph, we run a PageRank algo-rithm (Brin & Page, 1998) to determine more impor-tant nodes.
By running this algorithm after initializ-ing the graph edge weights, from the nodes that arecloser to topic and topic expanded words we boostthose that are more important in the documents.The starting point of the PageRank algorithm isthe graph with weighted edges obtained in the pre-vious step.
The node weights are initialized with1 (the starting value does not matter).
Analysis ofthe documents graph for several topics has revealedthat there is a large highly interconnected structure,and many disconnected small (2-3 nodes) fragments.Page Rank will run on this dense core structure.The PageRank algorithm is guaranteed to convergeif the graph is aperiodic and irreducible (Grimmett& Stirzaker, 1989).
Aperiodicity implies that thegreatest common divisor of the graph?s cycles is 1?
this condition is met.
Irreducibility of the graphmeans that it has no leaves, and there are no twonodes with the same set of neighbours.
The rem-edy in such cases is to connect each leaf to all othernodes in the graph, and conflate nodes with the sameset of neighbours.Once the graph topology meets the PageRankconvergence conditions, we run the algorithm.
Theoriginal formula for computing the rank of a node ateach iteration step is:PR(ni) =1 ?
dN+ d?nj?AdjniPR(nj)Out(nj)where ni is a node, d is the damping factor (usuallyd = 0.85 and this is the value we use as well), Nis the number of nodes in the graph, PR(ni) is therank of node ni, Adjni is the set of nodes adjacentto ni, and Out(nj) is the number of outgoing edgesfrom nj (our graph is non-directed, so this numberis the total number of edges with one end in nj).We adjust this formula to reflect the weights of theedges, and the version used is the following:PR(ni) =1 ?
dN+ d?nj?AdjniPR(nj)wout(nj);768Expansion ROUGE-2 ROUGE-SU4 BEnone 0.09270 (0.08785 - 0.09762) 0.14587 (0.14019 - 0.1514) 0.04958 (0.04559 - 0.05413)WNwith WSD 0.09494 (0.09086 - 0.09900) 0.15295 (0.14897 - 0.15681) 0.04985 (0.04606 - 0.05350)WNno WSD 0.09596 (0.09189 - 0.09990) 0.15357 (0.14947 - 0.15741) 0.05173 (0.04794 - 0.05550)Wiki 0.10173 (0.09721 - 0.10608) 0.15725 (0.15345 - 0.16130) 0.05542 (0.05125 - 0.05967)WNno WSD + Wiki 0.09604 (0.09228 - 0.09980) 0.15315 (0.14923 - 0.15694) 0.05292 (0.04912 - 0.05647)Table 3: Comparison of topic expansion methods with 95% confidence intervals.wout(nj) =?nk?Adjnjwe(nk, nj)In Table 2 we show examples of top ranked nodesfor several topics, extracted with this algorithm.
Thewords in italics are keywords/phrases from the topicquery, and the top ranked nodes are listed in decreas-ing order of their rank.5 SummarizationThe summarization method implemented is basedon the idea that the entities or events mentioned inthe query are somehow connected to each other, andthe documents to be summarized contain informa-tion that allows us to make these connections.
Weuse again the graph for all the documents in the col-lection related to one topic, built using the depen-dency relation representation of the texts.
The nodesin this graph are words/NEs, and the links are gram-matical relations.We extract from this graph the subgraph that cov-ers connections between all open class words/NEsin the topic or expanded topic query.
Each edge inthe extracted subgraph corresponds to a grammati-cal relation in a sentence of a document.
We col-lect all sentences thus represented in the subgraph,and rerank them based on the number of edges theycover, and the occurrence of topic or expanded topicterms.
We use the following formula to compute asentence score:Score(S) = topicWords ?
wword+ expandedWords ?
wexpandedWord+ topRankedWords ?
wtopRankedWord+ edgesCovered ?
wsubgraphEdge+ depRelation ?
wdepRelationwword, wexpandedWord, wtopRankedWord,wsubgraphEdge and wdepRelation are weight pa-rameters that give different importance to exactwords from the topic, expanded words, top rankedwords and edges covered in the extracted subgraph.During all experiments these parameters are fixed.5To form the summary we traverse the ranked listof sentences starting with the highest ranked one,and add sentences to a summary, or delete from theexisting summary, based on a simple lexical overlapmeasure.
We stop when the desired summary lengthis reached ?
for DUC 2005?2007, 250 words (lastsentence may be truncated to fill the summary up tothe allowed word limit).6 EvaluationExperiments are run on DUC 2007 main summa-rization task data, for the last experiment we usedthe DUC 2005 and DUC 2006 data as well.
Perfor-mance is evaluated in terms of ROUGE-2, ROUGE-SU4 and BE recall, following the methodology andusing the same parameters as in the DUC summa-rization events.We analyze several types of topic expansion: noexpansion, WordNet, Wikipedia, and within doc-ument collection expansion using spreading acti-vation and Page Rank.
The spreading activationmethod has several parameters whose values mustbe determined.We first compare the summaries produced withno topic expansion, WordNet (WN) and Wikipedia(Wiki) respectively.
Table 3 shows the results interms of ROUGE and BE recall on the DUC 2007(main) data.
Word sense disambiguation (WSD) forexpansion with WordNet did not work very well,as evidenced by the lower results for disambiguatedexpansion (WN with WSD) compared to the non-5The values used were set following a small number of ex-periments on DUC 2007 data, as the purpose was not to tunethe system for best performance, but rather to study the impactof more interesting parameters, in particular expansion type,decay and node ranking.
The values used are the following:wword = 5, wexpandedWord = 2.5, wtopRankedWord = 0.5,wsubgraphEdge = 2, wdepRelation = 0.769disambiguated one.
A better disambiguation algo-rithm may reverse the situation.
Expanding a topiconly with Wikipedia hyperlinks gives the best re-sults.
At the document level, the results are not asclear cut.
Figure 3 shows a comparison in terms ofROUGE-SU4 recall scores at the document level ofthe Wikipedia and WN (no WSD) expansion meth-ods, sorted in increasing order of the Wikipedia-based expansion scores.
The points are connectedto allow the reader to follow the results for eachmethod.0.080.10.120.140.160.180.20.220  5  10  15  20  25  30  35  40  45ROUGE-SU4RecallDUC 2007 documentsWikiWNFigure 3: Comparison of Wikipedia and WN ROUGE-SU4 per-document recall results.Because the overlap between Wikipedia andWordNet expanded queries was very low, we ex-pected the two types of expansion to be complemen-tary, and the combination to give better results thaneither expansion by itself.
An analysis of resultsfor each document with the three expansion meth-ods ?
Wikipedia, WordNet, and their combination ?showed that the simple combination of the expandedwords cannot take advantage of the situations whenone of the two methods performs better.
In futurework we will explore how to detect, based on thewords in the query, which type of expansion is best,and how to combine them using a weighting scheme.We choose the best configuration from above(Wikipedia expansion), and further expand the querythrough spreading activation and PageRank.
Thisnew type of expansion has two main parameterswhich influence the summarization outcome: num-ber of top ranked nodes to add to the topic expan-sion, and the decay of the spreading activation algo-rithm.0.0980.10.1020.1040.1060.1080.110  0.2  0.4  0.6  0.8  1decayROUGE-20.1540.1550.1560.1570.1580.1590.160.1610.1620.1630.1640.1650  0.2  0.4  0.6  0.8  1decayROUGE-SU40.0530.0540.0550.0560.0570.0580.0590.060.0610.0620  0.2  0.4  0.6  0.8  1decayBEFigure 4: Impact of signal decay in spreading activationon summarization performance.The decay parameter determines how far the in-fluence of the starting nodes (words from query orWikipedia-expanded query) should be felt.
The re-sults in Figure 4 ?
for decay values 0.1, 0.5, 0.95,0.99, 0.999, 0.9999, 1 ?
indicate that faster decay(reflected through a higher decay value) keeps thesummary more focused around the given topic, andleads to better results.6 For a high enough decay?
and eventually a decay of 1 ?
the weights of theedges become extremely small, and due to real num-ber representation in memory, practically 0.
In thissituation PageRank has no effect, and all nodes havethe same rank.We fix the decay parameter to 0.9999, and westudy the impact of the number of top nodes chosenafter ranking with PageRank.
Figure 5 shows the re-sults when the number of top ranked nodes chosenvaries.
Adding highly ranked nodes benefits the per-formance of the system only up to a certain limit.6During this set of experiments all other parameters arefixed, the number of top ranked nodes added to the topic ex-pansion is 30.7700.1020.1030.1040.1050.1060.1070.1080.1090.110  10  20  30  40  50number of nodesROUGE-20.16050.1610.16150.1620.16250.1630.16350.1640.16450  10  20  30  40  50number of nodesROUGE-SU40.0550.0560.0570.0580.0590.060.0610.0620.0630  10  20  30  40  50number of nodesBEFigure 5: Impact of the number of top ranked nodesadded to the expanded topic on summarization perfor-mance.From the values we tested, the best results were ob-tained when adding 40 nodes to the expanded topic.The best system configuration from the ones ex-plored7 is run on the DUC 2005, 2006 and 2007(main) data.
The performance and rank (in parenthe-ses) compared to participating systems is presentedin Table 4.DUC ROUGE-2 ROUGE-SU4 BE2005 (32) 0.07074 (3) 0.13002 (2) ?2006 (35) 0.08091 (11) 0.14022 (9) 0.04223 (11)2007 (32) 0.11048 (6) 0.16479 (5) 0.06250 (5)Table 4: System performance (and rank) on the DUC2005, 2006 and 2007 (main) data.
The number in paren-thesis after the DUC year indicates the number of com-peting systems.7Wikipedia expansion + 40 top nodes after spreading acti-vation and PageRank, decay = 0.9999, wexpandedWord = 3.5,wdepRelation = 1, the other parameters have the same valuesas before.7 ConclusionsThe experiments conducted within the summa-rization framework of the Document Understand-ing Conference have confirmed that encyclopedicknowledge extracted from Wikipedia can benefit thesummarization task.
Wikipedia articles are a sourceof relevant related concepts, that are useful for ex-panding a summarization query.
Furthermore, in-cluding information from the documents to be sum-marized by choosing relevant concepts ?
based oncloseness to topic keywords and relative importance?
improves even more the quality of the summaries,judged through ROUGE-2, ROUGE-SU4 and BErecall scores, as it is commonly done in the DUCcompetitions.
The topic expansion methods ex-plored lead to high summarization performance ?ranked 2nd, 9th and 5th on DUC 2005, 2006 and2007 respectively according to ROUGE-SU4 scores?
compared to (more than 30) DUC participatingsystems.The graph representation of the documents is cen-tral to the summarization method we described.
Be-cause of this, we plan to improve this representationby collapsing together coreferential nodes and clus-tering together related concepts, and verify whethersuch changes impact the summarization results, aswe expect they would.Being able to move away from the topic withinthe set of documents and discover new relevantnodes is an important issue, especially from thepoint of view of a new summarization style ?updates.
In update summaries the starting point isa topic, which a summarization system must trackin consecutive sets of documents.
We can adjustthe spreading activation parameters to how far anew set of documents is from the topic.
Futurework includes testing the spreading activation andpage ranking method in the context of the updatesummarization task and exploring methods ofextracting related concepts from the full text ofWikipedia articles.Acknowledgments This work was funded by theKlaus Tschira Foundation, Heidelberg, Germany.We thank the anonymous reviewers for insightfulcomments and suggestions.771ReferencesAhn, D., V. Jijkoun, G. Mishne, K. Mu?ller, M. de Rijke& S. Schlobach (2004).
Using Wikipedia at the TRECQA track.
In Proc.
of TREC-13.Amini, M. R. & N. Usunier (2007).
A contextual queryexpansion approach by term clustering for robust textsummarization.
In Proc.
of DUC-07.Anderson, J. R. (1983).
A spreading activation theoryof memory.
Journal of Verbal Learning and VerbalBehaviour, 22:261?295.Auer, S., C. Bizer, J. Lehmann, G. Kobilarov, R. Cyga-niak & Z. Ives (2007).
DBpedia: A nucleus for a Webof open data.
In Proc.
of ISWC 2007 + ASWC 2007,pp.
722?735.Barzilay, R. & M. Elhadad (1999).
Using lexical chainsfor text summarization.
In I. Mani & M. T. May-bury (Eds.
), Advances in Automatic Text Summariza-tion, pp.
111?121.
Cambridge, Mass.
: MIT Press.Brin, S. & L. Page (1998).
The anatomy of a large-scalehypertextual web search engine.
Computer Networksand ISDN Systems, 30(1?7):107?117.Collins, A. M. & E. F. Loftus (1975).
A spreading-activation theory of semantic processing.
Psychologi-cal Review, (82):407?428.Edmundson, H. (1969).
New methods in automatic ex-tracting.
Journal of the Association for ComputingMachinery, 16(2):264?285.Erkan, G. & D. R. Radev (2004).
LexRank: Graph-basedlexical centrality as salience in text summarization.Journal of Artificial Intelligence Research, 22:457?479.Fellbaum, C.
(Ed.)
(1998).
WordNet: An Electronic Lex-ical Database.
Cambridge, Mass.
: MIT Press.Gabrilovich, E. & S. Markovitch (2006).
Overcomingthe brittleness bottleneck using Wikipedia: Enhancingtext categorization with encyclopedic knowledge.
InProc.
of AAAI-06, pp.
1301?1306.Gotti, F., G. Lapalme, L. Nerima & E. Wehrli (2007).GOFAIsum: a symbolic summarizer for DUC.
InProc.
of DUC-07.Grimmett, G. & D. Stirzaker (1989).
Probability andRandom Processes.
Oxford University Press.Harabagiu, S. (2004).
Incremental topic representations.In Proc.
of COLING-04, pp.
583?589.Hickl, A., K. Roberts & F. L. C. C. Lacatusu (2007).LCC?s GISTexter at DUC 2007: Machine reading forupdate summarization.
In Proc.
of DUC-07.Katz, B., G. Marton, G. Borchardt, A. Brownell,S.
Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,F.
Mora, S. Stiller, O. Uzuner & A. Wilcox (2005).External knowledge sources for Question Answering.In Proc.
of TREC-14.Lesk, M. (1986).
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pine conefrom an ice cream cone.
In Proc.
of ACSD-86, pp.
24?26.Leskovec, J., M. Grobelnik & N. Milic-Frayling (2004).Learning sub-structures of document semantic graphsfor document summarization.
In Proc.
of LinkKDD-04.Lin, C.-Y.
& E. Hovy (2000).
The automated acquisitionof topic signatures for automatic summarization.
InProc.
of COLING-00, pp.
495?501.Mani, I.
& E. Bloedorn (1999).
Summarizing similaritiesand differences among related documents.
InformationRetrieval, 1(1):35?67.Mihalcea, R. & P. Tarau (2004).
TextRank: Bringing or-der into texts.
In Proc.
EMNLP-04, pp.
404?411.Mohamed, A.
A.
& S. Rajasekaran (2006).
Query-basedsummarization based on document graphs.
In Proc.
ofDUC-06.Ponzetto, S. P. & M. Strube (2007a).
Deriving a largescale taxonomy from Wikipedia.
In Proc.
of AAAI-07,pp.
1440?1445.Ponzetto, S. P. & M. Strube (2007b).
Knowledge derivedfrom Wikipedia for computing semantic relatedness.Journal of Artificial Intelligence Research, 30:181?212.Quillian, M. R. (1967).
Word concepts: A theory andsimulation of some basic semantic capabilities.
Be-havioural Science, 12(5):410?430.Rath, G., A. Resnick & T. Savage (1961).
The formationof abstracts by the selection of sentences.
AmericanDocumentation, 12(2):139?143.Suchanek, F. M., G. Kasneci & G. Weikum (2007).YAGO: A core of semantic knowledge.
In Proc.
ofWWW-07, pp.
697?706.Ye, S. & T.-S. Chua (2006).
NUS at DUC 2006: Doc-ument concept lattice for summarization.
In Proc.
ofDUC-06.772
