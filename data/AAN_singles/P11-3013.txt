Proceedings of the ACL-HLT 2011 Student Session, pages 69?74,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsEffects of Noun Phrase Bracketing in Dependency Parsing and MachineTranslationNathan GreenCharles University in PragueInstitute of Formal and Applied LinguisticsFaculty of Mathematics and Physicsgreen@ufal.mff.cuni.czAbstractFlat noun phrase structure was, up until re-cently, the standard in annotation for the PennTreebanks.
With the recent addition of inter-nal noun phrase annotation, dependency pars-ing and applications down the NLP pipelineare likely affected.
Some machine translationsystems, such as TectoMT, use deep syntaxas a language transfer layer.
It is proposedthat changes to the noun phrase dependencyparse will have a cascading effect down theNLP pipeline and in the end, improve ma-chine translation output, even with a reduc-tion in parser accuracy that the noun phrasestructure might cause.
This paper examinesthis noun phrase structure?s effect on depen-dency parsing, in English, with a maximumspanning tree parser and shows a 2.43%, 0.23Bleu score, improvement for English to Czechmachine translation.1 IntroductionNoun phrase structure in the Penn Treebank has upuntil recently been only considered, due to under-specification, a flat structure.
Due to the annota-tion and work of Vadas and Curran (2007a; 2007b;2008), we are now able to create Natural LanguageProcessing (NLP) systems that take advantage of theinternal structure of noun phrases in the Penn Tree-bank.
This extra internal structure introduces ad-ditional complications in NLP applications such asparsing.Dependency parsing has been a prime focus ofNLP research of late due to its ability to help parselanguages with a free word order.
Dependency pars-ing has been shown to improve NLP systems incertain languages and in many cases is consideredthe state of the art in the field.
Dependency pars-ing made many improvements due to the CoNLL Xshared task (Buchholz and Marsi, 2006).
However,in most cases, these systems were trained with a flatnoun phrase structure in the Penn Treebank.
Vadas?internal noun phrase structure has been used in pre-vious work on constituent parsing using Collin?sparser (Vadas and Curran, 2007c), but has yet to beanalyzed for its effects on dependency parsing.Parsing is very early in the NLP pipeline.
There-fore, improvements in parsing output could have animprovement on other areas of NLP in many cases,such as Machine Translation.
At the same time, anyerrors in parsing will tend to propagate down theNLP pipeline.
One would expect parsing accuracyto be reduced when the complexity of the parse is in-creased, such as adding noun phrase structure.
But,for a machine translation system that is reliant onparsing, the new noun phrase structure, even with re-duced parser accuracy, may yield improvements dueto a more detailed grammatical structure.
This isparticularly of interest for dependency relations, asit may aid in finding the correct head of a term in acomplex noun phrase.This paper examines the results and errors in pars-ing and machine translation of dependency parsers,trained with annotated noun phrase structure, againstthose with a flat noun phrase structure.
These re-sults are compared with two systems: a BaselineParser with no internally annotated noun phrases anda Gold NP Parser trained with data which contains69gold standard internal noun phrase structure anno-tation.
Additionally, we analyze the effect of theseimprovements and errors in parsing down the NLPpipeline on the TectoMT machine translation sys-tem (Z?abokrtsky?
et al, 2008).Section 2 contains background informationneeded to understand the individual components ofthe experiments.
The methodology used to carry outthe experiments is described in Section 3.
Resultsare shown and discussed in Section 4.
Section 5concludes and discusses future work and implica-tions of this research.2 Related Work2.1 Dependency ParsingDependence parsing is an alternative view to thecommon phrase or constituent parsing techniquesused with the Penn Treebank.
Dependency relationscan be used in many applications and have beenshown to be quite useful in languages with a freeword order.
With the influx of many data-driventechniques, the need for annotated dependency re-lations is apparent.
Since there are many data setswith constituent relations annotated, this paper usesfree conversion software provided from the CoNLL2008 shared task to create dependency relations (Jo-hansson and Nugues, 2007; Surdeanu et al, 2008).2.2 Dependency ParsersDependency parsing comes in two main forms:Graph algorithms and Greedy algorithms.
Thetwo most popular algorithms are McDonald?s MST-Parser (McDonald et al, 2005) and Nivre?s Malt-Parser (Nivre, 2003).
Each parser has its advantagesand disadvantages, but the accuracy overall is ap-proximately the same.
The types of errors madeby each parser, however, are very different.
MST-Parser is globally trained for an optimal solution andthis has led it to get the best results on longer sen-tences.
MaltParser on the other hand, is a greedy al-gorithm.
This allows it to perform extremely well onshorter sentences, as the errors tend to propagate andcause more egregious errors in longer sentences withlonger dependencies (McDonald and Nivre, 2007).We expect each parser to have different errors han-dling internal noun phrase structure, but for this pa-per we will only be examining the globally trainedMSTParser.2.3 TectoMTTectoMT is a machine translation framework basedon Praguian tectogrammatics (Sgall, 1967) whichrepresents four main layers: word layer, morpho-logical layer, analytical layer, and tectogrammaticallayer (Popel et al, 2010).
This framework is pri-marily focused on the translation from English intoCzech.
Since much of dependency parsing workhas been focused on Czech, this choice of machinetranslation framework logically follows as TectoMTmakes direct use of the dependency relationships.The work in this paper primarily addresses the nounphrase structure in the analytical layer (SEnglishAin Figure 1).Figure 1: Translation Process in TectoMT in whichthe tectogrammatical layer is transfered from English toCzech.TectoMT is a modular framework built in Perl.This allows great ease in adding the two differentparsers into the framework since each experimentcan be run as a separate ?Scenario?
comprised of dif-ferent parsing ?Blocks?.
This allows a simple com-parison of two machine translation system in whicheverything remains constant except the dependencyparser.2.4 Noun Phrase StructureThe Penn Treebank is one of the most well knownEnglish language treebanks (Marcus et al, 1993),consisting of annotated portions of the Wall StreetJournal.
Much of the annotation task is painstak-ingly done by annotators in great detail.
Some struc-tures are not dealt with in detail, such as noun phrasestructure.
Not having this information makes it dif-ficult to tell the dependencies on phrases such as70?crude oil prices?
(Vadas and Curran, 2007c).
With-out internal annotation it is ambiguous whether thephrase is stating ?crude prices?
(crude (oil prices))or ?crude oil?
((crude oil) prices).crude   oil   prices crude   oil   pricesFigure 2: Ambiguous dependency caused by internalnoun phrase structure.Manual annotation of these phrases would bequite time consuming and as seen in the exampleabove, sometimes ambiguous and therefore proneto poor inter-annotator agreement.
Vadas and Cur-ran have constructed a Gold standard version Penntreebank with these structures.
They were alsoable to train supervised learners to an F-score of91.44% (Vadas and Curran, 2007a; Vadas and Cur-ran, 2007b; Vadas and Curran, 2008).
The addi-tional complexity of noun phrase structure has beenshown to reduce parser accuracy in Collin?s parserbut no similar evaluation has been conducted for de-pendency parsers.
The internal noun phrase struc-ture has been used in experiments prior but withoutevaluation with respect to the noun phrases (Galleyand Manning, 2009).3 MethodologyThe Noun Phrase Bracketing experiments consist ofa comparison two systems.1.
The Baseline system is McDonald?s MST-Parser trained on the Penn Treebank in Englishwithout any extra noun phrase bracketing.2.
The Gold NP Parser is McDonald?s MSTParsertrained on the Penn Treebank in English withgold standard noun phrase structure annota-tions (Vadas and Curran, 2007a).3.1 Data SetsTo maintain a consistent dataset to compare to pre-vious work we use the Wall Street Journal (WSJ)section of the Penn Treebank since it was used inthe CoNLL X shared task on dependency parsing(Buchholz and Marsi, 2006).
Using the same com-mon breakdown of datasets, we use WST section02-21 for training and section 22 for testing, whichallows us to have comparable results to previousworks.
To test the effects of the noun phrase struc-ture on machine translation, ACL 2008?s Workshopon Statistical Machine translation?s (WMT) data areused.3.2 Process FlowFigure 3: Experiment Process Flow.
PTB (Penn TreeBank), NP (Noun Phrase Structure), LAS (Labeled Ac-curacy Score), UAS (Unlabeled Accuracy Score), WallStreet Journal (WSJ)We begin the the experiments by constructing twodata sets:1.
The Penn Treebank with no internal nounphrase structure (PTB w/o NP structure).2.
The Penn Treebank with gold standard nounphrase annotations provided by Vadas and Cur-ran (PTB w/ gold standard NP structure).From these datasets we construct two separateparsers.
These parsers are trained using McDonald?sMaximum Spanning Tree Algorithm (MSTParser)(McDonald et al, 2005).Both of the parsers are then tested on a subset ofthe WSJ corpus, section 22, of the Penn Treebankand the UAS and LAS scores are generated.
Errorsgenerated by each of these systems are then com-pared to discover where the internal noun phrasestructure affects the output.
Parser accuracy is notnecessarily the most important aspect of this work.71The effect of this noun phrase structure down theNLP pipeline is also crucial.
For this, the parsers areinserted into the TectoMT system.3.3 MetricsLabeled Accuracy Score (LAS) and UnlabeledAccuracy Score (UAS) are the primary ways to eval-uate dependency parsers.
UAS is the percentage ofwords that are correctly linked to their heads.
LAS isthe percentage of words that are connected to theircorrect heads and have the correct dependency la-bel.
UAS and LAS are used to compare one systemagainst another, as was done in CoNLL X (Buch-holz and Marsi, 2006).The Bleu (BiLingual Evaluation Understudy)score is an automatic scoring mechanism for ma-chine translation that is quick and can be reused as abenchmark across machine translation tasks.
Bleu iscalculated as the geometric mean of n-grams com-paring a machine translation and a reference text(Papineni et al, 2002).
This experiment comparesthe two parsing systems against each other using theabove metrics.
In both cases the test set data is sam-pled 1,000 times without replacement to calculatestatistical significance using a pairwise comparison.4 Results and DiscussionWhen applied, the gold standard annotationschanged approximately 1.5% of the edges in thetraining data.
Once trained, both parsers were testedagainst section 22 of their respective annotated cor-pora.
As Table 1 shows, the Baseline Parser obtainednear identical LAS and UAS scores.
This was ex-pected given the additional complexity of predictingthe noun phrase structure and the previous work onnoun phrase bracketing?s effect on Collin?s parser.Systems LAS UASBaseline Parser 88.12% 91.11%Gold NP Parser 88.10% 91.10%Table 1: Parsing results for the Baseline and Gold NPParsers.
Each is trained on Section 02-21 of the WSJ andtested on Section 22While possibly more error prone, the 1.5% changein edges in the training data did appear to add moreuseful syntactic structure to the resulting parses ascan be seen in Table 2.
With the additional nounphrase bracketing, the resulting Bleu score increased0.23 points or 2.43%.
The improvement is statis-tically significant with 95% confidence using pair-wise bootstrapping of 1,000 test sets randomly sam-pled with replacement (Koehn, 2004; Zhang et al,2004).
In Figure 4 we can see that the difference be-tween each of the 1,000 samples was above 0, mean-ing the Gold NP Parser performed consistently bet-ter given each sample.Systems BleuBaseline Parser 9.47Gold NP Parser 9.70Table 2: TectoMT results of a complete system run withboth the Baseline Parser and Gold NP Parser.
Both aretested on WMT08 data.
Results are an average of 1,000bootstrapped test sets with replacement.Figure 4: The Gold NP Parser shows statistically signif-icant improvement with 95% confidence.
The differencein Bleu score is represented on the Y-axis and the boot-strap iteration is displayed on the X-axis.
The sampleswere sorted by the difference in bleu score.Visually, changes can be seen in the English sideparse that affect the overall translation quality.
Sen-tences that contained incorrect noun phrase structuresuch as ?The second vice-president and Economyminister, Pedro Solbes?
as seen in Figure 5 and Fig-ure 6 were more correctly parsed in the Gold NPParser.
In Figure 5 ?and?
is incorrectly assigned tothe bottom of a noun phrase and does not connectany segments together in the output of the BaselineParser, while it connects two phrases in Figure 6which is the output of the Gold NP Parser.
This shiftin bracketing also allows the proper noun, which isshaded, to be assigned to the correct head, the right-most noun in the phrase.72Figure 5: The parse created with the data with flat struc-tures does not appear to handle noun phrases with moredepth, in this case the ?and?
does not properly connect thetwo components.Figure 6: With the addition of noun phrase structure inparser, the complicated noun phrase appears to be betterstructured.
The ?and?
connects two components insteadof improperly being a leaf node.5 ConclusionThis paper has demonstrated the benefit of addi-tional noun phrase bracketing in training data for usein dependency parsing and machine translation.
Us-ing the additional structure, the dependency parser?saccuracy was minimally reduced.
Despite this re-duction, machine translation, much further downthe NLP pipeline, obtained a 2.43% jump in Bleuscore and is statistically significant with 95% confi-dence.
Future work should examine similar experi-ments with MaltParser and other machine translationsystems.6 AcknowledgementsThis research has received funding from the Euro-pean Commissions 7th Framework Program (FP7)under grant agreement n?
238405 (CLARA), andfrom grant MSM 0021620838.
I would like to thankZdene?k Z?abokrtsky?
for his guidance in this researchand also the anonymous reviewers for their com-ments.ReferencesSabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning, CoNLL-X ?06, pages 149?164, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Michel Galley and Christopher D. Manning.
2009.Quadratic-time dependency parsing for machine trans-lation.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 773?781, Suntec, Singapore,August.
Association for Computational Linguistics.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of NODALIDA 2007, pages 105?112,Tartu, Estonia, May 25-26.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Comput.
Linguist.,19:313?330, June.73Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency parsingmodels.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 122?131.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proceedings of theconference on Human Language Technology and Em-pirical Methods in Natural Language Processing, HLT?05, pages 523?530, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proceedings of the 8th In-ternational Workshop on Parsing Technologies (IWPT,pages 149?160.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Martin Popel, Zdene?k Z?abokrtsky?, and Jan Pta?c?ek.
2010.Tectomt: Modular nlp framework.
In IceTAL, pages293?304.Petr Sgall.
1967.
Generativn??
popis jazyka a c?eska?
dek-linace.
Academia, Prague, Czech Republic.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
Theconll-2008 shared task on joint parsing of syntacticand semantic dependencies.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, CoNLL ?08, pages 159?177, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.David Vadas and James Curran.
2007a.
Adding nounphrase structure to the penn treebank.
In Proceedingsof the 45th Annual Meeting of the Association of Com-putational Linguistics, pages 240?247, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.David Vadas and James R. Curran.
2007b.
Large-scalesupervised models for noun phrase bracketing.
InConference of the Pacific Association for Computa-tional Linguistics (PACLING), pages 104?112, Mel-bourne, Australia, September.David Vadas and James R. Curran.
2007c.
Parsing in-ternal noun phrase structure with collins?
models.
InProceedings of the Australasian Language Technol-ogy Workshop 2007, pages 109?116, Melbourne, Aus-tralia, December.David Vadas and James R. Curran.
2008.
Parsing nounphrase structure with CCG.
In Proceedings of ACL-08: HLT, pages 335?343, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas.
2008.Tectomt: highly modular mt system with tectogram-matics used as transfer layer.
In Proceedings of theThird Workshop on Statistical Machine Translation,StatMT ?08, pages 167?170, Morristown, NJ, USA.Association for Computational Linguistics.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
In-terpreting bleu/nist scores: How much improvementdo we need to have a better system.
In In Proceedingsof Proceedings of Language Resources and Evaluation(LREC-2004, pages 2051?2054.74
