Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 249?252,Prague, June 2007. c?2007 Association for Computational LinguisticsNUS-ML: Improving Word Sense Disambiguation Using Topic FeaturesJun Fu Cai, Wee Sun LeeDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{caijunfu, leews}@comp.nus.edu.sgYee Whye TehGatsby Computational Neuroscience UnitUniversity College London17 Queen Square, London WC1N 3AR, UKywteh@gatsby.ucl.ac.ukAbstractWe participated in SemEval-1 Englishcoarse-grained all-words task (task 7), En-glish fine-grained all-words task (task 17,subtask 3) and English coarse-grained lex-ical sample task (task 17, subtask 1).
Thesame method with different labeled data isused for the tasks; SemCor is the labeledcorpus used to train our system for the all-words tasks while the labeled corpus thatis provided is used for the lexical sam-ple task.
The knowledge sources includepart-of-speech of neighboring words, singlewords in the surrounding context, local col-locations, and syntactic patterns.
In addi-tion, we constructed a topic feature, targetedto capture the global context information,using the latent dirichlet alocation (LDA)algorithm with unlabeled corpus.
A modi-fied na?
?ve Bayes classifier is constructed toincorporate all the features.
We achieved81.6%, 57.6%, 88.7% for coarse-grained all-words task, fine-grained all-words task andcoarse-grained lexical sample task respec-tively.1 IntroductionSupervised corpus-based approach has been themost successful in WSD to date.
However, this ap-proach faces severe data scarcity problem, resultingfeatures being sparsely represented in the trainingdata.
This problem is especially prominent for thebag-of-words feature.
A direct consequence is thatthe global context information, which the bag-of-words feature is supposed to capture, may be poorlyrepresented.Our system tries to address this problem byclustering features to relieve the scarcity problem,specifically on the bag-of-words feature.
In the pro-cess, we construct topic features, trained using thelatent dirichlet alocation (LDA) algorithm.
We trainthe topic model (Blei et al, 2003) on unlabeled data,clustering the words occurring in the corpus to a pre-defined number of topics.
We then use the resultingtopic model to tag the bag-of-words in the labeledcorpus with topic distributions.We incorporate the distributions, called the topicfeatures, using a simple Bayesian network, modifiedfrom na?
?ve Bayes model, alongside other featuresand train the model on the labeled corpus.2 Feature Construction2.1 Baseline FeaturesFor both the lexical sample and all-words tasks, weuse the following standard baseline features.POS Tags For each word instance w, we includePOS tags for P words prior to as well as after wwithin the same sentence boundary.
We also includethe POS tag of w. If there are fewer than P wordsprior or after w in the same sentence, we denote thecorresponding feature as NIL.Local Collocations We adopt the same 11 col-location features as (Lee and Ng, 2002), namelyC?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,C?3,?1, C?2,1, C?1,2, and C1,3.249Bag-of-Words For each training or testing word,w, we get G words prior to as well as after w, withinthe same document.
These features are position in-sensitive.
The words we extract are converted backto their morphological root forms.Syntactic Relations We adopt the same syntacticrelations as (Lee and Ng, 2002).
For easy reference,we summarize the features into Table 1.POS of w FeaturesNoun Parent headword hPOS of hRelative position of h to wVerb Left nearest child word of w, lRight nearest child word of w, rPOS of lPOS of rPOS of wVoice of wAdjective Parent headword hPOS of hTable 1: Syntactic Relations FeaturesThe exact values of P and G for each task are setaccording to validation result.2.2 Latent Dirichlet AllocationWe present here the latent dirichlet alocation algo-rithm and its inference procedures, adapted from theoriginal paper (Blei et al, 2003).LDA is a probabilistic model for collections ofdiscrete data and has been used in document mod-eling and text classification.
It can be representedas a three level hierarchical Bayesian model, showngraphically in Figure 1.
Given a corpus consisting ofM documents, LDA models each document using amixture over K topics, which are in turn character-ized as distributions over words.In the generative process of LDA, for each doc-ument d we first draw the mixing proportion overtopics ?d from a Dirichlet prior with parameters ?.Next, for each of the Nd words wdn in document d, atopic zdn is first drawn from a multinomial distribu-tion with parameters ?d.
Finally wdn is drawn fromthe topic specific distribution over words.
The prob-ability of a word token w taking on value i giventhat topic z = j was chosen is parameterized using?wz?
?NMFigure 1: Graphical Model for LDAa matrix ?
with ?ij = p(w = i|z = j).
Integratingout ?d?s and zdn?s, the probability p(D|?, ?)
of thecorpus is thus:M?d=1?p(?d|?
)(Nd?n=1?zdnp(zdn|?d)p(wdn|zdn, ?
))d?dIn variational inference, the latent variables ?dand zdn are assumed independent and updates tothe variational posteriors for ?d and zdn are derived(Blei et al, 2003).
It can be shown that the varia-tional posterior for ?d is a Dirichlet distribution, saywith variational parameters ?d, which we shall usein the following to construct topic features.2.3 Topic FeaturesWe first select an unlabeled corpus, such as 20Newsgroups, and extract individual words from it(excluding stopwords).
We choose the number oftopics, K, for the unlabeled corpus and we apply theLDA algorithm to obtain the ?
parameters, where ?represents the probability of a word w = i given atopic z = j, p(w = i|z = j) = ?ij .The model essentially clusters words that oc-curred in the unlabeled corpus according to K top-ics.
The conditional probability p(w = i|z = j) =?ij is later used to tag the words in the unseen testexample with the probability of each topic.We also use the document-specific ?d parameters.Specifically, we need to run the inference algorithmon the labeled corpus to get ?d for each document din the corpus.
The ?d parameter provides an approx-imation to the probability of selecting topic i in thedocument:p(zi|?d) =?di?K ?dk.
(1)2503 Classifier ConstructionWe construct a variant of the na?
?ve Bayes networkas shown in Figure 2.
Here, w refers to the word.s refers to the sense of the word.
In training, s isobserved while in testing, it is not.
The features f1to fn are baseline features mentioned in Section 2.1(including bag-of-words) while z refers to the la-tent topic that we set for clustering unlabeled corpus.The bag-of-words b are extracted from the neigh-bours of w and there are L of them.
Note that L canbe different from G, which is the number of bag-of-words in baseline features.
Both will be determinedby the validation result.?
?
??
??
?baselinefeatureswsfnf1bzLFigure 2: Graphical Model with LDA featureThe log-likelihood of an instance, `(w, s, F, b)where F denotes the set of baseline features, can bewritten as= logp(w) + logp(s|w) +?Flog(p(f |s))+?Llog(?Kp(zk|s)p(bl|zk)).The log p(w) term is constant and thus can beignored.
The first portion is normal na?
?ve Bayes.And second portion represents the additional LDAplate.
We decouple the training process into separatestages.
We first extract baseline features from thetask training data, and estimate, using normal na?
?veBayes, p(s|w) and p(f |s) for all w, s and f .Next, the parameters associated with p(b|z) areestimated using LDA from unlabeled data, which is?.
To estimate p(z|s), we perform LDA inferenceon the training corpus in order to obtain ?d for eachdocument d. We then use the ?d and ?
to obtainp(z|b) for each word usingp(zi|bl, ?d) =p(bl|zi)p(zi|?d)?K p(bl|zk)p(zk|?d),where equation (1) is used for estimation of p(zi|?d).This effectively transforms b to a topical distri-bution which we call a soft tag where each softtag is probability distribution t1, .
.
.
, tK on topics.We then use this topical distribution for estimatingp(z|s).
Let si be the observed sense of instance iand tij1 , .
.
.
, tijK be the soft tag of the j-th bag-of-word feature of instance i.
We estimate p(z|s) asp(zjk|s) =?si=s tijk?si=s?k?
tijk?
(2)This approach requires us to do LDA inference onthe corpus formed by the labeled training data, butnot the testing data.
This is because we need ?
toget transformed topical distribution in order to learnp(z|s) in the training.
In the testing, we only applythe learnt parameters to the model.4 Experimental SetupWe describe here the experimental setup on the En-glish lexical sample task and all-words task.
Notethat we do not distinguish the two all-words tasks asthe same parameters will be applied.For lexical sample task, we use 5-fold cross val-idation on the training data provided to determineour parameters.
For all-words task, we use SemCoras our training data and validate on Senseval-2 andSenseval-3 all-words test data.We use MXPOST tagger (Adwait, 1996) for POStagging, Charniak parser (Charniak, 2000) for ex-tracting syntactic relations, and David Blei?s versionof LDA1 for LDA training and inference.
All defaultparameters are used unless mentioned otherwise.For the all-word tasks, we use sense 1 as back-offfor words that have not appeared in SemCor.
We usethe same fine-grained system for both the coarse andfine-grained all-words tasks.
We make predictions1http://www.cs.princeton.edu/?blei/lda-c/251for all words for all the systems - precision, recalland accuracy scores are all the same.Baseline features For lexical sample task, wechoose P = 3 and G = 3.
For all-words task, wechoose P = 3 and G = 1.
(G = 1 means only thenearest word prior and after the test word.
)Smoothing For all standard baseline features, weuse Laplace smoothing but for the soft tag (equation(2)), we use a smoothing parameter value of 2 forall-words task and 0.1 for lexical sample task.Unlabeled Corpus Selection The unlabeled cor-pus we select from for LDA training include 20Newsgroups, Reuters, SemCor, Senseval-2 lexicalsample data, Senseval-3 lexical sample data andSemEval-1 lexical sample data.
Although the lastfour are labeled corpora, we only need the wordsfrom these corpora and thus they can be regarded asunlabeled too.
For lexical sample data, we define thewhole passage for each training and testing instanceas one document.For lexical sample task, we use all the unlabeledcorpus mentioned with K = 60 and L = 18.
Forall-words task, we use a corpora consisting only 20Newsgroups and SemCor with K = 40 and L = 14.Validation Result Table 2 shows the results weget on the validation sets.
We give both the systemaccuracy (named as Soft Tag) and the na?
?ve Bayesresult with only standard features as baseline.Validation Set Soft Tag NB baselineSE-2 All-words 66.3 63.7SE-3 All-words 66.1 64.6Lexical Sample 89.3 87.9Table 2: Validation set results (best configuration).5 Official ResultsWe now present the official results on all three taskswe participated in, summarized in Table 3.The system ranked first, fourth and second inthe lexical sample task, fine-grained all-words taskand coarse-grained all-words task respectively.
Forcoarse-grained all-words task, we obtained 86.1,88.3, 81.4, 76.7 and 79.1 for each document, fromd001 to d005.Task Precision/RecallLexical sample(Task 17) 88.7Fine-grained all-words(Task 17) 57.6Course-grained all-words(Task 7) 81.6Table 3: Official Results5.1 Analysis of ResultsFor the lexical sample task, we compare the re-sults to that of our na?
?ve Bayes baseline and Sup-port Vector Machine (SVM) (Vapnik, 1995) base-line.
Our SVM classifier (using SVMlight) followsthat of (Lee and Ng, 2002), which ranked the thirdin Senseval-3 English lexical sample task.
We alsoanalyse the result according to the test instance?spart-of-speech and find that the improvements areconsistent for both noun and verb.System Noun Verb TotalSoft Tag 92.7 84.2 88.7NB baseline 91.7 83.5 87.8SVM baseline 91.6 83.1 87.6Table 4: Analysis on different POS on English lexi-cal sample taskOur coarse-grained all-words task result outper-formed the first sense baseline score of 0.7889 byabout 2.7%.ReferencesY.
K. Lee and H. T. Ng.
2002.
An Empirical Evaluationof Knowledge Sources and Learning Algorithms forWord Sense Disambiguation.
In Proc.
of EMNLP.D.
M. Blei and A. Y. Ng and M. I. Jordan.
2003.
La-tent Dirichlet Allocation.
Journal of Machine Learn-ing Research.A.
Ratnaparkhi 1996.
A Maximum Entropy Model forPart-of-Speech Tagging.
In Proc.
of EMNLP.E.
Charniak 2000.
A Maximum-Entropy-InspiredParser.
In Proc.
of the 1st Meeting of the North Ameri-can Chapter of the Association for Computational Lin-guistics.V.
N. Vapnik 1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.252
