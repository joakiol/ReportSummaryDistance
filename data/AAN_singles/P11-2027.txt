Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 153?158,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAM-FM: A Semantic Framework for Translation Quality AssessmentRafael E. Banchs Haizhou LiHuman Language Technology Department Human Language Technology DepartmentInstitute for Infocomm Research Institute for Infocomm Research1 Fusionopolis Way, Singapore 138632 1 Fusionopolis Way, Singapore 138632rembanchs@i2r.a-star.edu.sg hli@i2r.a-star.edu.sgAbstractThis work introduces AM-FM, a semanticframework for machine translation evalua-tion.
Based upon this framework, a newevaluation metric, which is able to operatewithout the need for reference translations,is implemented and evaluated.
The metricis based on the concepts of adequacy andfluency, which are independently assessedby using a cross-language latent semanticindexing approach and an n-gram basedlanguage model approach, respectively.Comparative analyses with conventionalevaluation metrics are conducted on twodifferent evaluation tasks (overall qualityassessment and comparative ranking) overa large collection of human evaluations in-volving five European languages.
Finally,the main pros and cons of the proposedframework are discussed along with futureresearch directions.1 IntroductionEvaluation has always been one of the major issuesin Machine Translation research, as both humanand automatic evaluation methods exhibit veryimportant limitations.
On the one hand, althoughhighly reliable, in addition to being expensive andtime consuming, human evaluation suffers frominconsistency problems due to inter- and intra-annotator agreement issues.
On the other hand,while being consistent, fast and cheap, automaticevaluation has the major disadvantage of requiringreference translations.
This makes automatic eval-uation not reliable in the sense that good transla-tions not matching the available references areevaluated as poor or bad translations.The main objective of this work is to proposeand evaluate AM-FM, a semantic framework forassessing translation quality without the need forreference translations.
The proposed framework istheoretically grounded on the classical concepts ofadequacy and fluency, and it is designed to accountfor these two components of translation quality inan independent manner.
First, a cross-language la-tent semantic indexing model is used for assessingthe adequacy component by directly comparing theoutput translation with the input sentence it wasgenerated from.
Second, an n-gram based languagemodel of the target language is used for assessingthe fluency component.Both components of the metric are evaluated atthe sentence level, providing the means for defin-ing and implementing a sentence-based evaluationmetric.
Finally, the two components are combinedinto a single measure by implementing a weightedharmonic mean, for which the weighting factor canbe adjusted for optimizing the metric performance.The rest of the paper is organized as follows.Section 2, presents some background work and thespecific dataset that has been used in the experi-mental work.
Section 3, provides details on theproposed AM-FM framework and the specific met-ric implementation.
Section 4 presents the resultsof the conducted comparative evaluations.
Finally,section 5 presents the main conclusions and rele-vant issues to be dealt with in future research.1532 Related Work and DatasetAlthough BLEU (Papineni et al, 2002) has be-come a de facto standard for machine translationevaluation, other metrics such as NIST (Dodding-ton, 2002) and, more recently, Meteor (Banerjeeand Lavie, 2005), are commonly used too.
Regard-ing the specific idea of evaluating machine trans-lation without using reference translations, severalworks have proposed and evaluated different ap-proaches, including round-trip translation (Somers,2005; Rapp, 2009), as well as other regression- andclassification-based approaches (Quirk, 2004; Ga-mon et al, 2005; Albrecht and Hwa, 2007; Speciaet al, 2009).As part of the recent efforts on machine transla-tion evaluation, two workshops have been organiz-ing shared-tasks and evaluation campaigns over thelast four years: the NIST Metrics for MachineTranslation Challenge 1  (MetricsMATR) and theWorkshop on Statistical Machine Translation 2(WMT); which were actually held as one singleevent in their most recent edition in 2010.The dataset used in this work corresponds toWMT-07.
This dataset is used, instead of a morerecent one, because no human judgments on ade-quacy and fluency have been conducted in WMTafter year 2007, and human evaluation data is notfreely available from MetricsMATR.In this dataset, translation outputs are availablefor fourteen tasks involving five European lan-guages: English (EN), Spanish (ES), German (DE),French (FR) and Czech (CZ); and two domains:News Commentaries (News) and European Par-liament Debates (EPPS).
A complete descriptionon WMT-07 evaluation campaign and dataset isavailable in Callison-Burch et al (2007).System outputs for fourteen of the fifteen sys-tems that participated in the evaluation are availa-ble.
This accounts for 86 independent systemoutputs with a total of 172,315 individual sentencetranslations, from which only 10,754 were ratedfor both adequacy and fluency by human judges.The specific vote standardization procedure de-scribed in section 5.4 of Blatz et al (2003) wasapplied to all adequacy and fluency scores for re-moving individual voting patterns and averagingvotes.
Table 1 provides information on the corre-sponding domain, and source and target languages1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2 http://www.statmt.org/wmt10/for each of the fourteen translation tasks, alongwith their corresponding number of system outputsand the amount of sentence translations for whichhuman evaluations are available.Task Domain Src.
Tgt.
Syst.
Sent.T1 News CZ EN 3 727T2 News EN CZ 2 806T3 EPPS EN FR 7 577T4 News EN FR 8 561T5 EPPS EN DE 6 924T6 News EN DE 6 892T7 EPPS EN ES 6 703T8 News EN ES 7 832T9 EPPS FR EN 7 624T10 News FR EN 7 740T11 EPPS DE EN 7 949T12 News DE EN 5 939T13 EPPS ES EN 8 812T14 News ES EN 7 668Table 1: Domain, source language, target lan-guage, system outputs and total amount of sentencetranslations (with both adequacy and fluency hu-man assessments) included in the WMT-07 dataset3 Semantic Evaluation FrameworkThe framework proposed in this work (AM-FM)aims at assessing translation quality without theneed for reference translations, while maintainingconsistency with human quality assessments.
Dif-ferent from other approaches not using referencetranslations, we rely on a cross-language version oflatent semantic indexing (Dumais et al, 1997) forcreating a semantic space where translation outputsand inputs can be directly compared.A two-component evaluation metric, based onthe concepts of adequacy and fluency (White et al,1994) is defined.
While adequacy accounts for theamount of source meaning being preserved by thetranslation (5:all, 4:most, 3:much, 2:little, 1:none),fluency accounts for the quality of the target lan-guage in the translation (5:flawless, 4:good, 3:non-native, 2:disfluent, 1:incomprehensible).3.1 Metric DefinitionFor implementing the adequacy-oriented compo-nent (AM) of the metric, the cross-language latentsemantic indexing approach is used (Dumais et al,1997), in which the source sentence originating thetranslation is used as evaluation reference.
Accord-154ing to this, the AM component can be regarded tobe mainly adequacy-oriented as it is computed on across-language semantic space.For implementing the fluency-oriented compo-nent (FM) of the proposed metric, an n-gram basedlanguage model approach is used (Manning andSchutze, 1999).
This component can be regarded tobe mainly fluency-oriented as it is computed on thetarget language side in a manner that is totally in-dependent from the source language.For combining both components into a singlemetric, a weighted harmonic mean is proposed:AM-FM = AM FM / (?
AM + (1-?)
FM) (1)where ?
is a weighting factor ranging from ?=0(pure AM component) to ?=1 (pure FM compo-nent), which can be adjusted for maximizing thecorrelation between the proposed metric AM-FMand human evaluation scores.3.2 Implementation DetailsThe adequacy-oriented component of the metric(AM) was implemented by following the proce-dure proposed by Dumais et al (1997), where abilingual collection of data is used to generate across-language projection matrix for a vector-spacerepresentation of texts (Salton et al, 1975) byusing singular value decomposition: SVD (Goluband Kahan, 1965).According to this formulation, a bilingual term-document matrix Xab of dimensions M*N, whereM=(Ma+Mb) are vocabulary terms in languages aand b, and N are documents (sentences in ourcase), can be decomposed as follows:Xab = [Xa;Xb] = Uab ?ab Vab T (2)where [Xa;Xb] is the concatenation of the twomonolingual term-document matrices Xa and Xb(of dimensions Ma*N and Mb*N) corresponding tothe available parallel training collection, Uab andVab are unitary matrices of dimensions M*M andN*N, respectively, and ?
is an M*N diagonal matrixcontaining the singular values associated to the de-composition.From the singular value decomposition depictedin (2), a low-dimensional representation for anysentence vector xa or xb, in language a or b, can becomputed as follows:ya T  =  [xa ;0] T  UabM*L (3.a)yb T  =  [0; xb] T  UabM*L (3.b)where ya and yb represent the L-dimensional vec-tors corresponding to the projections of the full-dimensional sentence vectors xa and xb, respective-ly; and UabM*L is a cross-language projection matrixcomposed of the first L column vectors of theunitary matrix Uab obtained in (2).Notice, from (3a) and (3b), how both sentencevectors xa and xb are padded with zeros at eachcorresponding other-language vocabulary locationsfor performing the cross-language projections.
Assimilar terms in different languages would havesimilar occurrence patterns, theoretically, a closerepresentation in the cross-language reduced spaceshould be obtained for terms and sentences that aresemantically related.
Therefore, sentences can becompared across languages in the reduced space.The AM component of the metric is finally com-puted in the projected space by using the cosinesimilarity between the source and target sentences:AM = [s;0]TP ([0;t]TP)T / |[s;0]TP| / |[0;t]TP| (4)where P is the projection matrix UabM*L describedin (3a) and (3b), [s;0] and [0;t] are vector spacerepresentations of the source and target sentencesbeing compared (with their target and sourcevocabulary elements set to zero, respectively), and| | is the L2-norm operator.
In a final implementa-tion stage, the range of AM is restricted to theinterval [0,1] by truncating negative results.For computing the projection matrices, randomsets of 10,000 parallel sentences3 were drawn fromthe available training datasets.
The only restrictionwe imposed to the extracted sentences was thateach should contain at least 10 words.
Seven pro-jection matrices were constructed in total, one foreach different combination of domain and lan-guage pair.
TF-IDF weighting was applied to theconstructed term-document matrices while main-taining all words in the vocabularies (i.e.
no stop-words were removed).
All computations related toSVD, sentence projections and cosine similaritieswere conducted with MATLAB.3 Although this accounts for a small proportion of the datasets(20% of News and 1% of European Parliament), it allowed formaintaining computational requirements under control whilestill providing a good vocabulary coverage.155The fluency-oriented component FM is imple-mented by using an n-gram language model.
Inorder to avoid possible effects derived from dif-ferences in sentence lengths, a compensation factoris introduced in log-probability space.
Accordingto this, the FM component is computed as follows:FM  =  exp(?n=1:N log(p(wn|wn-1,?
))/N) (5)where p(wn|wn-1,?)
represent the target languagen-gram probabilities and N is the total number ofwords in the target sentence being evaluated.By construction, the values of FM are also re-stricted to the interval [0,1]; so, both componentvalues range within the same interval.Fourteen language models were trained in total,one per task, by using the available training data-sets.
The models were computed with the SRILMtoolbox (Stolcke, 2002).As seen from (4) and (5), different from con-ventional metrics that compute matches betweentranslation outputs and references, in the AM-FMframework, a semantic embedding is used for as-sessing the similarities between outputs and inputs(4) and, independently, an n-gram model is usedfor evaluating output language quality (5).4 Comparative EvaluationsIn order to evaluate the AM-FM framework, twocomparative evaluations with standard metricswere conducted.
More specifically, BLEU, NISTand Meteor were considered, as they are the met-rics most frequently used in machine translationevaluation campaigns.4.1 Correlation with Human ScoresIn this first evaluation, AM-FM is compared withstandard evaluation metrics in terms of their corre-lations with human-generated scores.
Differentfrom Callison-Burch et al (2007), where Spear-man?s correlation coefficients were used, we usehere Pearson?s coefficients as, instead of focusingon ranking; this first evaluation exercise focuses onevaluating the significance and noisiness of theassociation, if any, between the automatic metricsand human-generated scores.Three parameters should be adjusted for theAM-FM implementation described in (1): the di-mensionality of the reduced space for AM, the or-der of n-gram model for FM, and the harmonicmean weighting parameter ?.
Such parameters canbe adjusted for maximizing the correlation coeffi-cient between the AM-FM metric and human-generated scores.
4  After exploring the solutionspace, the following values were selected, dimen-sionality for AM: 1,000; order of n-gram model forFM: 3; and, weighting parameter ?
: 0.30In the comparative evaluation presented here,correlation coefficients between the automatic met-rics and human-generated scores were computed atthe system level (i.e.
the units of analysis were sys-tem outputs), by considering all 86 available sys-tem outputs (see Table 1).
For computing humanscores and AM-FM at the system level, averagevalues of sentence-based scores for each systemoutput were considered.Table 2 presents the Pearson?s correlation coef-ficients computed between the automatic metrics(BLEU, NIST, Meteor and our proposed AM-FM)and the human-generated scores (adequacy, fluen-cy and the harmonic mean of both; i.e.
2af/(a+f)).All correlation coefficients presented in the tableare statistically significant with p<0.01 (where p isthe probability of getting the same correlationcoefficient, with a similar number of 86 samples,by chance).Metric Adequacy Fluency H MeanBLEU 0.4232 0.4670 0.4516NIST 0.3178 0.3490 0.3396Meteor 0.4048 0.3920 0.4065AM-FM 0.3719 0.4558 0.4170Table 2: Pearson?s correlation coefficients (com-puted at the system level) between automatic met-rics and human-generated scoresAs seen from the table, BLEU is the metric ex-hibiting the largest correlation coefficients withhuman-generated scores, followed by Meteor andAM-FM, while NIST exhibits the lowest correla-tion coefficient values.
Recall that our proposedAM-FM metric is not using reference translationsfor assessing translation quality, while the otherthree metrics are.In a similar exercise, the correlation coefficientswere also computed at the sentence level (i.e.
theunits of analysis were sentences).
These results aresummarized in Table 3.
As metrics are computed4 As no development dataset was available for this particulartask, a subset of the same evaluation dataset had to be used.156at the sentence level, smoothed-bleu (Lin and Och,2004) was used in this case.
Again, all correlationcoefficients presented in the table are statisticallysignificant with p<0.01.Metric Adequacy Fluency H MeansBLEU 0.3089 0.3361 0.3486NIST 0.1208 0.0834 0.1201Meteor 0.3220 0.3065 0.3405AM-FM 0.2142 0.2256 0.2406Table 3: Pearson?s correlation coefficients (com-puted at the sentence level) between automaticmetrics and human-generated scoresAs seen from the table, in this case, BLEU andMeteor are the metrics exhibiting the largestcorrelation coefficients, followed by AM-FM andNIST.4.2 Reproducing RankingsIn addition to adequacy and fluency, the WMT-07dataset includes rankings of sentence translations.To evaluate the usefulness of AM-FM and itscomponents in a different evaluation setting, wealso conducted a comparative evaluation on theircapacity for predicting human-generated rankings.As ranking evaluations allowed for ties amongsentence translations, we restricted our analysis toevaluate whether automatic metrics were able topredict the best, the worst and both sentence trans-lations for each of the 4,060 available rankings5.The number of items per ranking varies from 2 to5, with an average of 4.11 items per ranking.
Table4 presents the results of the comparative evaluationon predicting rankings.As seen from the table, Meteor is the automaticmetric exhibiting the largest ranking predictioncapability, followed by BLEU and NIST, while ourproposed AM-FM metric exhibits the lowest rank-ing prediction capability.
However, it still performswell above random chance predictions, which, forthe given average of 4 items per ranking, is about25% for best and worst ranking predictions, andabout 8.33% for both.
Again, recall that the AM-FM metric is not using reference translations,while the other three metrics are.
Also, it is worthmentioning that human rankings were conducted5 We discarded those rankings involving the translation systemfor which translation outputs were not available that, conse-quently, only had one translation output left.by looking at the reference translations and not thesource.
See Callison-Burch et al (2007) for detailson the human evaluation task.Metric Best Worst BothsBLEU 51.08% 54.90% 37.86%NIST 49.56% 54.98% 37.36%Meteor 52.83% 58.03% 39.85%AM-FM 35.25% 41.11% 25.20%AM 37.19% 46.92% 28.47%FM 34.01% 39.01% 24.11%Table 4: Percentage of cases in which each auto-matic metric is able to predict the best, the worst,and both ranked sentence translationsAdditionally, results for the individual compo-nents, AM and FM, are also presented in the table.Notice how the AM component exhibits a betterranking capability than the FM component.5 Conclusions and Future WorkThis work presented AM-FM, a semantic frame-work for translation quality assessment.
Two com-parative evaluations with standard metrics havebeen conducted over a large collection of human-generated scores involving different languages.Although the obtained performance is below stand-ard metrics, the proposed method has the mainadvantage of not requiring reference translations.Notice that a monolingual version of AM-FM isalso possible by using monolingual latent semanticindexing (Landauer et al, 1998) along with a set ofreference translations.
A detailed evaluation of amonolingual implementation of AM-FM can befound in Banchs and Li (2011).As future research, we plan to study the impactof different dataset sizes and vector space modelparameters for improving the performance of theAM component of the metric.
This will include thestudy of learning curves based on the amount oftraining data used, and the evaluation of differentvector model construction strategies, such as re-moving stop-words and considering bigrams andword categories in addition to individual words.Finally, we also plan to study alternative uses ofAM-FM within the context of statistical machinetranslation as, for example, a metric for MERToptimization, or using the AM component alone asan additional feature for decoding, rescoring and/orconfidence estimation.157ReferencesJoshua S. Albrecht and Rebeca Hwa.
2007.
Regressionfor sentence-level MT evaluation with pseudoreferences.
In Proceedings of the 45th AnnualMeeting of the Association of ComputationalLinguistics, 296-303.Rafael E. Banchs and Haizhou Li.
2011.
MonolingualAM-FM: a two-dimensional machine translationevaluation method.
Submitted to the Conference onEmpirical Methods in Natural Language Processing.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:an automatic metric for MT evaluation withimproved correlation with human judgments.
InProceedings of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/orSummarization, 65-72.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis and Nicola Ueffing.
2003.
Confidenceestimation for machine translation.
Final ReportWS2003 CLSP Summer Workshop, Johns HopkinsUniversityChris Callison-Burch, Cameron Fordyce,Philipp Koehn,Christof Monz and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Proceedings ofStatistical Machine Translation Workshop, 136-158.George Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the HumanLanguage Technology Conference.Susan Dumais, Thomas K. Landauer and Michael L.Littman.
1997.
Automatic cross-linguisticinformation retrieval using latent semantic indexing.In Proceedings of the SIGIR Workshop on Cross-Lingual Information Retrieval, 16-23.Michael Gamon, Anthony Aue and Martine Smets.2005.
Sentence-level MT evaluation withoutreference translations: beyond language modeling.
InProceedings of the 10th Annual Conference of theEuropean Association for Machine Translation, 103-111.G.
H. Golub and W. Kahan.
1965.
Calculating thesingular values and pseudo-inverse of a matrix.Journal of the Society for Industrial and AppliedMathematics: Numerical Analysis, 2(2):205-224.Thomas K. Landauer, Peter W. Foltz and DarrellLaham.
1998.
Introduction to Latent SemanticAnalysis.
Discourse Processes, 25:259-284.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metricsfor machine translation.
In Proceedings of the 20thinternational conference on ComputationalLinguistics, pp 501, Morristown, NJ.Christopher D. Manning and Hinrich Schutze.
1999.Foundations of Statistical Natural LanguageProcessing (Chapter 6).
Cambridge, MA: The MITPress.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jung Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe Association for Computational Linguistics, 311-318.Christopher B. Quirk.
2004.
Training a sentence-levelmachine translation confidence measure.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation, 825-828.Reinhard Rapp.
2009.
The back-translation score:automatic MT evaluation at the sentences levelwithout reference translations.
In Proceedings of theACL-IJCNLP, 133-136.Gerard M. Salton, Andrew K. Wong and C. S. Yang.1975.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613-620.Harold Somers.
2005.
Round-trip translation: what is itgood for?
In proceedings of the AustralasianLanguage Technology Workshop, 127-133.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang and John Shawe-Taylor.
2009.
Improving theconfidence of machine translation quality estimates.In Proceedings of MT Summit XII.
Ottawa, Canada.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing.John S. White, Theresa O?Cornell and Francis O?Nava.1994.
The ARPA MT evaluation methodologies:evolution, lessons and future approaches.
InProceedings of the Association for MachineTranslation in the Americas, 193-205.158
