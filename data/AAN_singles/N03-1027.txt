Supervised and unsupervised PCFG adaptation to novel domainsBrian Roark and Michiel BacchianiAT&T Labs - Research{roark,michiel}@research.att.comAbstractThis paper investigates adapting a lexicalizedprobabilistic context-free grammar (PCFG) toa novel domain, using maximum a posteriori(MAP) estimation.
The MAP framework is gen-eral enough to include some previous modeladaptation approaches, such as corpus mixing inGildea (2001), for example.
Other approachesfalling within this framework are more effec-tive.
In contrast to the results in Gildea (2001),we show F-measure parsing accuracy gains of asmuch as 2.5% for high accuracy lexicalized pars-ing through the use of out-of-domain treebanks,with the largest gains when the amount of in-domain data is small.
MAP adaptation can also bebased on either supervised or unsupervised adap-tation data.
Even when no in-domain treebank isavailable, unsupervised techniques provide a sub-stantial accuracy gain over unadapted grammars,as much as nearly 5% F-measure improvement.1 IntroductionA fundamental concern for nearly all data-driven ap-proaches to language processing is the sparsity of la-beled training data.
The sparsity of syntactically anno-tated corpora is widely remarked upon, and some recentpapers present approaches to improving performance inthe absence of large amounts of annotated training data.Johnson and Riezler (2000) looked at adding features to amaximum entropy model for stochastic unification-basedgrammars (SUBG), from corpora that are not annotatedwith the SUBG, but rather with simpler treebank annota-tions for which there are much larger treebanks.
Hwa (2001)demonstrated how active learning techniques can reducethe amount of annotated data required to converge on thebest performance, by selecting from among the candidatestrings to be annotated in ways which promote more in-formative examples for earlier annotation.
Hwa (1999) andGildea (2001) looked at adapting parsing models trained onlarge amounts of annotated data from outside of the domainof interest (out-of-domain), through the use of a relativelysmall amount of in-domain annotated data.
Hwa (1999)used a variant of the inside-outside algorithm presentedin Pereira and Schabes (1992) to exploit a partially labeledout-of-domain treebank, and found an advantage to adapta-tion over direct grammar induction.
Gildea (2001) simplyadded the out-of-domain treebank to his in-domain trainingdata, and derived a very small benefit for his high accuracy,lexicalized parser, concluding that even a large amount ofout-of-domain data is of little use for lexicalized parsing.Statistical model adaptation based on sparse in-domaindata, however, is neither a new problem nor unique to pars-ing.
It has been studied extensively by researchers work-ing on acoustic modeling for automatic speech recognition(ASR) (Legetter and Woodland, 1995; Gauvain and Lee,1994; Gales, 1998; Lamel et al, 2002).
One of the meth-ods that has received much attention in the ASR literature ismaximum a posteriori (MAP) estimation (Gauvain and Lee,1994).
In MAP estimation, the parameters of the model areconsidered to be random variables themselves with a knowndistribution (the prior).
The prior distribution and the max-imum likelihood distribution based on the in-domain obser-vations then give a posterior distribution over the parame-ters, from which the mode is selected.
If the amount of in-domain (adaptation) data is large, the mode of the posteriordistribution is mostly defined by the adaptation sample; ifthe amount of adaptation data is small, the mode will nearlycoincide with the mode of the prior distribution.
The intu-ition behind MAP estimation is that once there are sufficientobservations, the prior model need no longer be relied upon.Bacchiani and Roark (2003) investigated MAP adapta-tion of n-gram language models, in a way that is straight-forwardly applicable to probabilistic context-free grammars(PCFGs).
Indeed, this approach can be used for any gen-erative probabilistic model, such as part-of-speech taggers.In their language modeling approach, in-domain counts aremixed with the out-of-domain model, so that, if the num-ber of observations within the domain is small, the out-of-domain model is relied upon, whereas if the number ofobservations in the domain is high, the model will movetoward a Maximum Likelihood (ML) estimate on the in-domain data alone.
The case of a parsing model trained viarelative frequency estimation is identical: in-domain countscan be combined with the out-of-domain model in just sucha way.
We will show below that weighted count mergingis a special case of MAP adaptation; hence the approachof Gildea (2001) cited above is also a special case of MAPEdmonton, May-June 2003Main Papers , pp.
126-133Proceedings of HLT-NAACL 2003adaptation, with a particular parameterization of the prior.This parameterization is not necessarily the one that opti-mizes performance.In the next section, MAP estimation for PCFGs is pre-sented.
This is followed by a brief presentation of the PCFGmodel that is being learned, and the parser that is usedfor the empirical trials.
We will present empirical resultsfor multiple MAP adaptation schema, both starting fromthe Penn Wall St. Journal treebank and adapting to theBrown corpus, and vice versa.
We will compare our su-pervised adaptation performance with the results presentedin Gildea (2001).
In addition to supervised adaptation, i.e.with a manually annotated treebank, we will present resultsfor unsupervised adaptation, i.e.
with an automatically an-notated treebank.
We investigate a number of unsupervisedapproaches, including multiple iterations, increased samplesizes, and self-adaptation.2 MAP estimationIn the maximum a posteriori estimation framework de-scribed in detail in Gauvain and Lee (1994), the model pa-rameters ?
are assumed to be a random vector in the space?.
Given an observation sample x, the MAP estimate is ob-tained as the mode of the posterior distribution of ?
denotedas g(.
| x)?MAP = argmax?g(?
| x) = argmax?f(x | ?)g(?)
(1)In the case of n-gram model adaptation, as discussed inBacchiani and Roark (2003), the objective is to estimateprobabilities for a discrete distribution across words, en-tirely analogous to the distribution across mixture compo-nents within a mixture density, which is a common use forMAP estimation in ASR.
A practical candidate for the priordistribution of the weights ?1, ?2, ?
?
?
, ?K , is its conjugateprior, the Dirichlet density,g(?1, ?2, ?
?
?
, ?K | ?1, ?2, ?
?
?
, ?K) ?K?i=1?
?i?1i (2)where ?i > 0 are the parameters of the Dirichlet distribu-tion.
With such a prior, if the expected counts for the i-thcomponent is denoted as ci, the mode of the posterior distri-bution is obtained as?
?i =(?i ?
1) + ci?Kk=1(?k ?
1) +?Kk=1 ck1 ?
i ?
K. (3)We can use this formulation to estimate the posterior, but wemust still choose the parameters of the Dirichlet.
First, letus introduce some notation.
A context-free grammar (CFG)G = (V, T, P, S?
), consists of a set of non-terminal symbolsV , a set of terminal symbols T , a start symbol S?
?
V , anda set of rule productions P of the form: A ?
?, whereA ?
V and ?
?
(V ?
T )?.
A probabilistic context-freegrammar (PCFG) is a CFG with a probability assigned toeach rule, such that the probabilities of all rules expanding agiven non-terminal sum to one; specifically, each right-handside has a probability given the left-hand side of the rule1.LetA denote the left-hand side of a production, and ?i thei-th possible expansion of A.
Let the probability estimatefor the production A ?
?i according to the out-of-domainmodel be denoted as P?
(?i | A) and let the expected adapta-tion counts be denoted as c(A ?
?i).
Then the parametersof the prior distribution for left-hand side A are chosen as?Ai = ?AP?
(?i | A) + 1 1 ?
i ?
K. (4)where ?A is the left-hand side dependent prior weighting pa-rameter.
This choice of prior parameters defines the MAPestimate of the probability of expansion ?i from the left-hand side A asP?
(?i | A) =?AP?
(?i | A) + c(A?
?i)?A +?Kk=1 c(A?
?k)1 ?
i ?
K. (5)Note that the MAP estimates with this parameterization re-duce to the out-of-domain model parameters in the absenceof adaptation data.Each left-hand side A has its own prior distribution, pa-rameterized with ?A.
This presents an over-parameterizationproblem.
We follow Gauvain and Lee (1994) in adopt-ing a parameter tying approach.
As pointed out inBacchiani and Roark (2003), two methods of parameter ty-ing, in fact, correspond to two well known model mixingapproaches, namely count merging and model interpolation.Let P?
and c?
denote the probabilities and counts from theout-of-domain model, and let P and c denote the probabili-ties and counts from the adaptation model (i.e.
in-domain).2.1 Count MergingIf the left-hand side dependent prior weighting parameter ischosen as?A = c?(A)?
?, (6)the MAP adaptation reduces to count merging, scaling theout-of-domain counts with a factor ?
and the in-domaincounts with a factor ?:P?
(?i | A) =c?(A)??
P?
(?i | A) + c(A?
?i)c?(A)??
+ c(A)=?c?(A?
?i) + ?c(A?
?i)?c?
(A) + ?c(A)(7)1An additional condition for well-formedness is that the PCFGis consistent or tight, i.e.
there is no probability mass lost to in-finitely large trees.
Chi and Geman (1998) proved that this con-dition is met if the rule probabilities are estimated using relativefrequency estimation from a corpus.2.2 Model InterpolationIf the left-hand side dependent prior weighting parameter ischosen as?A ={c(A) ?1??
, 0 < ?
< 1 if c(A) > 01 otherwise(8)the MAP adaptation reduces to model interpolation usinginterpolation parameter ?:P?
(?i | A) =c(A) ?1??
P?
(?i | A) + c(A?
?i)c(A) ?1??
+ c(A)=?1??
P?
(?i | A) + P(?i | A)?1??
+ 1= ?P?
(?i | A) + (1?
?
)P(?i | A) (9)2.3 Other Tying CandidatesWhile we will not be presenting empirical results for otherparameter tying approaches in this paper, we should pointout that the MAP framework is general enough to allowfor other schema, which could potentially improve perfor-mance over simple count merging and model interpolationapproaches.
For example, one may choose a more com-plicated left-hand side dependent prior weighting parametersuch as?A ={c(A) ?1??
, 0 < ?
< 1 if c?
(A) c(A) > ?c?(A)??
otherwise(10)for some threshold ?.
Such a schema may do a better jobof managing how quickly the model moves away from theprior, particularly if there is a large difference in the respec-tive sizes of the in-domain and out-of domain corpora.
Weleave the investigation of such approaches to future research.Before providing empirical results on the count mergingand model interpolation approaches, we will introduce theparser and parsing models that were used.3 Grammar and parserFor the empirical trials, we used a top-down, left-to-right(incremental) statistical beam-search parser (Roark, 2001a;Roark, 2003).
We refer readers to the cited papers for de-tails on this parsing algorithm.
Briefly, the parser maintainsa set of candidate analyses, each of which is extended toattempt to incorporate the next word into a fully connectedpartial parse.
As soon as ?enough?
candidate parses havebeen extended to the next word, all parses that have notyet attached the word are discarded, and the parser moveson to the next word.
This beam search is parameterizedwith a base beam parameter ?, which controls how manyor how few parses constitute ?enough?.
Candidate parsesare ranked by a figure-of-merit, which promotes better can-didates, so that they are worked on earlier.
The figure-of-merit consists of the probability of the parse to that pointtimes a look-ahead statistic, which is an estimate of howmuch probability mass it will take to connect the parse withthe next word.
It is a generative parser that does not requireany pre-processing, such as POS tagging or chunking.
It hasbeen demonstrated in the above papers to perform compet-itively on standard statistical parsing tasks with full cover-age.
Baseline results below will provide a comparison withother well known statistical parsers.The PCFG is a Markov grammar (Collins, 1997; Char-niak, 2000), i.e.
the production probabilities are estimatedby decomposing the joint probability of the categories on theright-hand side into a product of conditionals via the chainrule, and making a Markov assumption.
Thus, for example,a first order Markov grammar conditions the probability ofthe category of the i-th child of the left-hand side on the cat-egory of the left-hand side and the category of the (i-1)-thchild of the left-hand side.
The benefits of Markov gram-mars for a top-down parser of the sort we are using is de-tailed in Roark (2003).
Further, as in Roark (2001a; 2003),the production probabilities are conditioned on the label ofthe left-hand side of the production, as well as on featuresfrom the left-context.
The model is smoothed using standarddeleted interpolation, wherein a mixing parameter ?
is esti-mated using EM on a held out corpus, such that probabilityof a production A ?
?, conditioned on j features from theleft context, Xj1 = X1 .
.
.
Xj , is defined recursively asP(A?
?
| Xj1) = P(?
| A,Xj1) (11)= (1?
?)P?(?
| A,Xj1) + ?P(?
| A,Xj?11 )where P?
is the maximum likelihood estimate of the condi-tional probability.
These conditional probabilities decom-pose via the chain rule as mentioned above, and a Markovassumption limits the number of previous children alreadyemitted from the left-hand side that are conditioned upon.These previous children are treated exactly as other con-ditioning features from the left context.
Table 1 gives theconditioning features that were used for all empirical trialsin this paper.
There are different conditioning features forparts-of-speech (POS) and non-POS non-terminals.
Deletedinterpolation leaves out one feature at a time, in the reverseorder as they are presented in the table 1.The grammar that is used for these trials is a PCFG thatis induced using relative frequency estimation from a trans-formed treebank.
The trees are transformed with a selec-tive left-corner transformation (Johnson and Roark, 2000)that has been flattened as presented in Roark (2001b).
Thistransform is only applied to left-recursive productions, i.e.productions of the form A ?
A?.
The transformed treeslook as in figure 1.
The transform has the benefit for a top-down incremental parser of this sort of delaying many ofthe parsing decisions until later in the string, without un-duly disrupting the immediate dominance relationships thatprovide conditioning features for the probabilistic model.
(a)NPNPNPNNPJimbbPOS?sHHHNNdogPPPPPP,INwith .
.
.lNP(b)NPNNPJimPOS?sXXXXXNP/NPNNdogHHHNP/NPPPINwith .
.
.lNP(c)NPNNPJim!!
!POS?slNP/NPNNdog``````NP/NPPP,INwith .
.
.lNPFigure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-cornerrepresentation; and (c) a flat structure that is unambiguously equivalent to (b)Features for non-POS left-hand sides0 Left-hand side (LHS)1 Last child of LHS2 2nd last child of LHS3 3rd last child of LHS4 Parent of LHS (PAR)5 Last child of PAR6 Parent of PAR (GPAR)7 Last child of GPAR8 First child of conjoined category9 Lexical head of current constituentFeatures for POS left-hand sides0 Left-hand side (LHS)1 Parent of LHS (PAR)2 Last child of PAR3 Parent of PAR (GPAR)4 POS of C-Commanding head5 C-Commanding lexical head6 Next C-Commanding lexical headTable 1: Conditioning features for the probabilistic CFGused in the reported empirical trialsThe parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evalua-tion2.For the trials reported in the next section, the base beamparameter is set at ?
= 10.
In order to avoid being pruned, aparse must be within a probability range of the best scoringparse that has incorporated the next word.
Let k be the num-ber of parses that have incorporated the next word, and let p?be the best probability from among that set.
Then the prob-ability of a parse must be above p?k310?
to avoid being pruned.2See Johnson (1998) for a presentation of the transform/de-transform paradigm in parsing.4 Empirical trialsThe parsing models were trained and tested on treebanksfrom the Penn Treebank II.
For the Wall St. Journal portion,we used the standard breakdown: sections 2-21 were kepttraining data; section 24 was held-out development data; andsection 23 was for evaluation.
For the Brown corpus por-tion, we obtained the training and evaluation sections usedin Gildea (2001).
In that paper, no held-out section was usedfor parameter tuning3, so we further partitioned the trainingdata into kept and held-out data.
The sizes of the corporaare given in table 2, as well as labels that are used to refer tothe corpora in subsequent tables.4.1 Baseline performanceThe first results are for parsing the Brown corpus.
Table3 presents our baseline performance, compared with theGildea (2001) results.
Our system is labeled as ?MAP?.
Allparsing results are presented as labeled precision and recall.Whereas Gildea (2001) reported parsing results just for sen-tences of length less than or equal to 40, our results are forall sentences.
The goal is not to improve upon Gildea?sparsing performance, but rather to try to get more benefitfrom the out-of-domain data.
While our performance is 0.5-1.5 percent better than Gildea?s, the same trends hold ?
loweighties in accuracy when using the Wall St. Journal (out-of-domain) training; mid eighties when using the Brown corpustraining.
Notice that using the Brown held out data with theWall St. Journal training improved precision substantially.Tuning the parameters on in-domain data can make a bigdifference in parser performance.
Choosing the smoothingparameters as Gildea did, based on the distribution withinthe corpus itself, may be effective when parsing within thesame distribution, but appears less so when using the tree-bank for parsing outside of the domain.3According to the author, smoothing parameters for his parserwere based on the formula from Collins (1999).Corpus;Sect Used for Sentences WordsWSJ;2-21 Training 39,832 950,028WSJ;24 Held out 1,346 32,853WSJ;23 Eval 2,416 56,684Brown;T Training 19,740 373,152Brown;H Held out 2,078 40,046Brown;E Eval 2,425 45,950Table 2: Corpus sizesSystem Training Heldout LR LPGildea WSJ;2-21 80.3 81.0MAP WSJ;2-21 WSJ;24 81.3 80.9MAP WSJ;2-21 Brown;H 81.6 82.3Gildea Brown;T,H 83.6 84.6MAP Brown;T Brown;H 84.4 85.0Table 3: Parser performance on Brown;E, baselines.
Notethat the Gildea results are for sentences ?
40 words inlength.Table 4 gives the baseline performance on section 23 ofthe WSJ Treebank.
Note, again, that the Gildea results arefor sentences ?
40 words in length, while all others are forall sentences in the test set.
Also, Gildea did not report per-formance of a Brown corpus trained parser on the WSJ.
Ourperformance under that condition is not particularly good,but again using an in-domain held out set for parameter tun-ing provided a substantial increase in accuracy, somewhatmore in terms of precision than recall.
Our baseline resultsfor a WSJ section 2-21 trained parser are slightly better thanthe Gildea parser, at more-or-less the same level of perfor-mance as Charniak (1997) and Ratnaparkhi (1999), but sev-eral points below the best reported results on this task.4.2 Supervised adaptationTable 5 presents parsing results on the Brown;E test set formodels using both in-domain and out-of-domain trainingdata.
The table gives the adaptation (in-domain) treebankthat was used, and the ?A that was used to combine the adap-tation counts with the model built from the out-of-domaintreebank.
Recall that ?c?
(A) times the out-of-domain modelyields count merging, with ?
the ratio of out-of-domainto in-domain counts; and ?c(A) times the out-of-domainmodel yields model interpolation, with ?
the ratio of out-of-domain to in-domain probabilities.
Gildea (2001) mergedthe two corpora, which just adds the counts from the out-of-domain treebank to the in-domain treebank, i.e.
?
= 1.This resulted in a 0.25 improvement in the F-measure.
Inour case, combining the counts in this way yielded a halfa point, perhaps because of the in-domain tuning of thesmoothing parameters.
However, when we optimize ?
em-pirically on the held-out corpus, we can get nearly a fullpoint improvement.
Model interpolation in this case per-System Training Heldout LR LPMAP Brown;T Brown;H 76.0 75.4MAP Brown;T WSJ;24 76.9 77.1Gildea WSJ;2-21 86.1 86.6MAP WSJ;2-21 WSJ;24 86.9 87.1Charniak (1997) WSJ;2-21 WSJ;24 86.7 86.6Ratnaparkhi (1999) WSJ;2-21 86.3 87.5Collins (1999) WSJ;2-21 88.1 88.3Charniak (2000) WSJ;2-21 WSJ;24 89.6 89.5Collins (2000) WSJ;2-21 89.6 89.9Table 4: Parser performance on WSJ;23, baselines.
Notethat the Gildea results are for sentences ?
40 words inlength.
All others include all sentences.forms nearly identically to count merging.Adaptation to the Brown corpus, however, does not ad-equately represent what is likely to be the most commonadaptation scenario, i.e.
adaptation to a consistent domainwith limited in-domain training data.
The Brown corpus isnot really a domain; it was built as a balanced corpus, andhence is the aggregation of multiple domains.
The reversescenario ?
Brown corpus as out-of-domain parsing modeland Wall St. Journal as novel domain ?
is perhaps a morenatural one.
In this direction, Gildea (2001) also reportedvery small improvements when adding in the out-of-domaintreebank.
This may be because of the same issue as with theBrown corpus, namely that the optimal ratio of in-domain toout-of-domain is not 1 and the smoothing parameters needto be tuned to the new domain; or it may be because the newdomain has a million words of training data, and hence hasless use for out-of-domain data.
To tease these apart, we par-titioned the WSJ training data (sections 2-21) into smallertreebanks, and looked at the gain provided by adaptation asthe in-domain observations grow.
These smaller treebanksprovide a more realistic scenario: rapid adaptation to a noveldomain will likely occur with far less manual annotation oftrees within the new domain than can be had in the full PennTreebank.Table 6 gives the baseline performance on WSJ;23, withmodels trained on fractions of the entire 2-21 test set.
Sec-tions 2-21 contain approximately 40,000 sentences, and wepartitioned them by percentage of total sentences.
From ta-ble 6 we can see that parser performance degrades quite dra-matically when there is less than 20,000 sentences in thetraining set, but that even with just 2000 sentences, the sys-tem outperforms one trained on the Brown corpus.Table 7 presents parsing accuracy when a model trainedon the Brown corpus is adapted with part or all of the WSJtraining corpus.
From this point forward, we only presentresults for count merging, since model interpolation con-sistently performed 0.2-0.5 points below the count mergingSystem Training Heldout Adapt ?A Baseline Adapted ?FLR LP F LR LP FGildea WSJ;2-21 Brown;T,H c?
(A) 83.6 84.6 84.1 83.9 84.8 84.35 0.25MAP WSJ;2-21 Brown;H Brown;T c?
(A) 84.4 85.0 84.7 84.9 85.6 85.25 0.55MAP WSJ;2-21 Brown;H Brown;T 0.25?c(A) 84.4 85.0 84.7 85.4 85.9 85.65 0.95MAP WSJ;2-21 Brown;H Brown;T 0.20c(A) 84.4 85.0 84.7 85.3 85.9 85.60 0.90Table 5: Parser performance on Brown;E, supervised adaptationSystem Training % Heldout LR LPMAP WSJ;2-21 100 WSJ;24 86.9 87.1MAP WSJ;2-21 75 WSJ;24 86.6 86.8MAP WSJ;2-21 50 WSJ;24 86.3 86.4MAP WSJ;2-21 25 WSJ;24 84.8 85.0MAP WSJ;2-21 10 WSJ;24 82.6 82.6MAP WSJ;2-21 5 WSJ;24 80.4 80.6Table 6: Parser performance on WSJ;23, baselinesapproach4.
The ?A mixing parameter was empirically opti-mized on the held out set when the in-domain training wasjust 10% of the total; this optimization makes over a pointdifference in accuracy.
Like Gildea, with large amounts ofin-domain data, adaptation improved our performance byhalf a point or less.
When the amount of in-domain datais small, however, the impact of adaptation is much greater.4.3 Unsupervised adaptationBacchiani and Roark (2003) presented unsupervised MAPadaptation results for n-gram models, which use the samemethods outlined above, but rather than using a manuallyannotated corpus as input to adaptation, instead use an auto-matically annotated corpus.
Their automatically annotatedcorpus was the output of a speech recognizer which used theout-of-domain n-gram model.
In our case, we use the pars-ing model trained on out-of-domain data, and output a setof candidate parse trees for the strings in the in-domain cor-pus, with their normalized scores.
These normalized scores(posterior probabilities) are then used to give weights to thefeatures extracted from each candidate parse, in just the waythat they provide expected counts for an expectation maxi-mization algorithm.For the unsupervised trials that we report, we collectedup to 20 candidate parses per string5.
We were interested ininvestigating the effects of adaptation, not in optimizing per-formance, hence we did not empirically optimize the mixingparameter ?A for the new trials, so as to avoid obscuring theeffects due to adaptation alone.
Rather, we used the best4This is consistent with the results presented inBacchiani and Roark (2003), which found a small but con-sistent improvement in performance with count merging versusmodel interpolation for n-gram modeling.5Because of the left-to-right, heuristic beam-search, the parserdoes not produce a chart, rather a set of completed parses.performing parameter from the supervised trials, namely0.20c?(A).
Since we are no longer limited to manually anno-tated data, the amount of in-domain WSJ data that we caninclude is essentially unlimited.
Hence the trials reported gobeyond the 40,000 sentences in the Penn WSJ Treebank, toinclude up to 5 times that number of sentences from otheryears of the WSJ.Table 8 shows the results of unsupervised adaptation aswe have described it.
Note that these improvements are hadwithout seeing any manually annotated Wall St. Journaltreebank data.
Using the approximately 40,000 sentencesin f2-21, we derived a 3.8 percent F-measure improvementover using just the out of domain data.
Going beyond thesize of the Penn Treebank, we continued to gain in accuracy,reaching a total F-measure improvement of 4.2 percent with200 thousand sentences, approximately 5 million words.
Asecond iteration with this best model, i.e.
re-parsing the 200thousand sentences with the adapted model and re-training,yielded an additional 0.65 percent F-measure improvement,for a total F-measure improvement of 4.85 percent over thebaseline model.A final unsupervised adaptation scenario that we inves-tigated is self-adaptation, i.e.
adaptation on the test set it-self.
Because this adaptation is completely unsupervised,thus does not involve looking at the manual annotations atall, it can be equally well applied using the test set as the un-supervised adaptation set.
Using the same adaptation proce-dure presented above on the test set itself, i.e.
producing thetop 20 candidates from WSJ;23 with normalized posteriorprobabilities and re-estimating, we produced a self-adaptedparsing model.
This yielded an F-measure accuracy of 76.8,which is a 1.1 percent improvement over the baseline.5 ConclusionWhat we have demonstrated in this paper is that maximum aposteriori (MAP) estimation can make out-of-domain train-ing data beneficial for statistical parsing.
In the most likelyscenario ?
porting a parser to a novel domain for which thereis little or no annotated data ?
the improvements can be quitelarge.
Like active learning, model adaptation can reduce theamount of annotation required to converge to a best levelof performance.
In fact, MAP coupled with active learningmay reduce the required amount of annotation further.There are a couple of interesting future directions for thisSystem % of ?A Baseline Adapted ?FWSJ;2-21 LR LP F LR LP FGildea 100 c?
(A) 86.1 86.6 86.35 86.3 86.9 86.60 0.25MAP 100 0.20?c(A) 86.9 87.1 87.00 87.2 87.5 87.35 0.35MAP 75 0.20?c(A) 86.6 86.8 86.70 87.1 87.3 87.20 0.50MAP 50 0.20?c(A) 86.3 86.4 86.35 86.7 86.9 86.80 0.45MAP 25 0.20?c(A) 84.8 85.0 84.90 85.3 85.5 85.40 0.50MAP 10 0.20?c(A) 82.6 82.6 82.60 84.3 84.4 84.35 1.75MAP 10 c?
(A) 82.6 82.6 82.60 83.2 83.4 83.30 0.70MAP 5 0.20?c(A) 80.4 80.6 80.50 83.0 83.1 83.05 2.55Table 7: Parser performance on WSJ;23, supervised adaptation.
All models use Brown;T,H as the out-of-domain treebank.Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank.Adaptation Iter- LR LP F- ?FSentences ation measure0 0 76.0 75.4 75.704000 1 78.6 77.9 78.25 2.5510000 1 78.9 78.0 78.45 2.7520000 1 79.3 78.5 78.90 3.2030000 1 79.7 78.9 79.30 3.6039832 1 79.9 79.1 79.50 3.80100000 1 79.7 79.2 79.45 3.75200000 1 80.2 79.6 79.90 4.20200000 2 80.6 80.5 80.55 4.85Table 8: Parser performance on WSJ;23, unsupervisedadaptation.
For all trials, the base training is Brown;T, theheld out is Brown;H plus the parser output for WSJ;24, andthe mixing parameter ?A is 0.20c?(A).research.
First, a question that is not addressed in this paperis how to best combine both supervised and unsupervisedadaptation data.
Since each in-domain resource is likely tohave a different optimal mixing parameter, since the super-vised data is more reliable than the unsupervised data, thisbecomes a more difficult, multi-dimensional parameter op-timization problem.
Hence, we would like to investigate au-tomatic methods for choosing mixing parameters, such asEM.
Also, an interesting question has to do with choosingwhich treebank to use for out-of-domain data.
For a newdomain, is it better to choose as prior the balanced Browncorpus, or rather the more robust Wall St. Journal treebank?Perhaps one could use several out-of-domain treebanks aspriors.
Most generally, one can imagine using k treebanks,some in-domain, some out-of-domain, and trying to find thebest mixture to suit the particular task.The conclusion in Gildea (2001), that out-of-domain tree-banks are not particularly useful in novel domains, was pre-mature.
Instead, we can conclude that, just as in other sta-tistical estimation problems, there are generalizations to behad from these out-of-domain trees, providing more robustestimates, especially in the face of sparse training data.ReferencesMichiel Bacchiani and Brian Roark.
2003.
Unsupervisedlanguage model adaptation.
In Proceedings of the In-ternational Conference on Acoustics, Speech, and SignalProcessing (ICASSP).Eugene Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceedings ofthe Fourteenth National Conference on Artificial Intelli-gence, pages 598?603.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics, pages 132?139.Zhiyi Chi and Stuart Geman.
1998.
Estimation of proba-bilistic context-free grammars.
Computational Linguis-tics, 24(2):299?305.Michael J. Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the 35thAnnual Meeting of the Association for ComputationalLinguistics, pages 16?23.Michael J. Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.Michael J. Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In The Proceedings of the 17thInternational Conference on Machine Learning.M.
J. F. Gales.
1998.
Maximum likelihood linear transfor-mations for hmm-based speech recognition.
ComputerSpeech and Language, pages 75?98.Jean-Luc Gauvain and Chin-Hui Lee.
1994.
Maximuma posteriori estimation for multivariate gaussian mixtureobservations of markov chains.
IEEE Transactions onSpeech and Audio Processing, 2(2):291?298.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of the Sixth Conference on Empir-ical Methods in Natural Language Processing (EMNLP-01).Rebecca Hwa.
1999.
Supervised grammar induction us-ing training data with limited constituent information.
InProceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics.Rebecca Hwa.
2001.
On minimizing training corpus forparser acquisition.
In Proceedings of the Fifth Computa-tional Natural Language Learning Workshop.Mark Johnson and Stefan Riezler.
2000.
Exploiting aux-iliary distributions in stochastic unification-based gram-mars.
In Proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics.Mark Johnson and Brian Roark.
2000.
Compact non-left-recursive grammars using the selective left-corner trans-form and factoring.
In Proceedings of the 18th Interna-tional Conference on Computational Linguistics (COL-ING), pages 355?361.Mark Johnson.
1998.
PCFG models of linguistic tree rep-resentations.
Computational Linguistics, 24(4):617?636.L.
Lamel, J.-L. Gauvain, and G. Adda.
2002.
Unsupervisedacoustic model training.
In Proceedings of the Interna-tional Conference on Acoustics, Speech, and Signal Pro-cessing (ICASSP), pages 877?880.C.
J. Legetter and P.C.
Woodland.
1995.
Maximum like-lihood linear regression for speaker adaptation of contin-uous density hidden markov models.
Computer Speechand Language, pages 171?185.Fernando C.N.
Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.
InProceedings of the 30th Annual Meeting of the Associa-tion for Computational Linguistics, pages 128?135.Adwait Ratnaparkhi.
1999.
Learning to parse natural lan-guage with maximum entropy models.
Machine Learn-ing, 34:151?175.Brian Roark.
2001a.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Brian Roark.
2001b.
Robust Probabilistic PredictiveSyntactic Processing.
Ph.D. thesis, Brown University.http://arXiv.org/abs/cs/0105019.Brian Roark.
2003.
Robust garden path parsing.
NaturalLanguage Engineering, 9(2):1?24.
