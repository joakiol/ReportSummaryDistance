Tutorials, NAACL-HLT 2013, pages 13?15,Atlanta, Georgia, June 9 2013. c?2013 Association for Computational LinguisticsSpectral Learning Algorithms forNatural Language ProcessingShay Cohen?, Michael Collins?, Dean P.
Foster?, Karl Stratos?, Lyle Ungar?
?Columbia University?University of Pennsylvaniascohen,mcollins,stratos@cs.columbia.edudean@foster.netungar@cis.upenn.edu1 IntroductionRecent work in machine learning and NLP has developed spectral algorithms formany learning tasks involving latent variables.
Spectral algorithms rely on sin-gular value decomposition as a basic operation, usually followed by some simpleestimation method based on the method of moments.
From a theoretical pointof view, these methods are appealing in that they offer consistent estimators (andPAC-style guarantees of sample complexity) for several important latent-variablemodels.
This is in contrast to the EM algorithm, which is an extremely success-ful approach, but which only has guarantees of reaching a local maximum of thelikelihood function.From a practical point of view, the methods (unlike EM) have no need forcareful initialization, and have recently been shown to be highly efficient (as oneexample, in work under submission by the authors on learning of latent-variablePCFGs, a spectral algorithm performs at identical accuracy to EM, but is around20 times faster).2 OutlineIn this tutorial we will aim to give a broad overview of spectral methods, describingtheoretical guarantees, as well as practical issues.
We will start by covering thebasics of singular value decomposition and describe efficient methods for doingsingular value decomposition.
The SVD operation is at the core of most spectralalgorithms that have been developed.13We will then continue to cover canonical correlation analysis (CCA).
CCA is anearly method from statistics for dimensionality reduction.
With CCA, two or moreviews of the data are created, and they are all projected into a lower dimensionalspace which maximizes the correlation between the views.
We will review thebasic algorithms underlying CCA, give some formal results giving guarantees forlatent-variable models and also describe how they have been applied recently tolearning lexical representations from large quantities of unlabeled data.
This ideaof learning lexical representations can be extended further, where unlabeled data isused to learn underlying representations which are subsequently used as additionalinformation for supervised training.We will also cover how spectral algorithms can be used for structured predic-tion problems with sequences and parse trees.
A striking recent result by Hsu,Kakade and Zhang (2009) shows that HMMs can be learned efficiently using aspectral algorithm.
HMMs are widely used in NLP and speech, and previous al-gorithms (typically based on EM) were guaranteed to only reach a local maximumof the likelihood function, so this is a crucial result.
We will review the basic me-chanics of the HMM learning algorithm, describe its formal guarantees, and alsocover practical issues.Last, we will cover work about spectral algorithms in the context of naturallanguage parsing.
We will show how spectral algorithms can be used to estimatethe parameter models of latent-variable PCFGs, a model which serves as the basefor state-of-the-art parsing models such as the one of Petrov et al(2007).
We willshow what are the practical steps that are needed to be taken in order to make spec-tral algorithms for L-PCFGs (or other models in general) practical and comparableto state of the art.3 Speaker BiosShay Cohen1 is a postdoctoral research scientist in the Department of ComputerScience at Columbia University.
He is a computing innovation fellow.
His re-search interests span a range of topics in natural language processing and machinelearning.
He is especially interested in developing efficient and scalable parsingalgorithms as well as learning algorithms for probabilistic grammars.Michael Collins2 is the Vikram S. Pandit Professor of computer science atColumbia University.
His research is focused on topics including statistical pars-ing, structured prediction problems in machine learning, and applications includingmachine translation, dialog systems, and speech recognition.
His awards include a1http://www.cs.columbia.edu/?scohen/2http://www.cs.columbia.edu/?mcollins/14Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-tics at the Wharton School of the University of Pennsylvania.
His current researchinterests are machine learning, stepwise regression and computational linguistics.He has been searching for new methods of finding useful features in big data sets.His current set of hammers revolve around fast matrix methods (which decompose2nd moments) and tensor methods for decomposing 3rd moments.Karl Stratos4 is a Ph.D. student in the Department of Computer Science atColumbia.
His research is focused on machine learning and natural language pro-cessing.
His current research efforts are focused on spectral learning of latent-variable models, or more generally, uncovering latent structure from data.Lyle Ungar5 is a professor at the Computer and Information Science Depart-ment at the University of Pennsylvania.
His research group develops scalable ma-chine learning and text mining methods, including clustering, feature selection,and semi-supervised and multi-task learning for natural language, psychology, andmedical research.
Example projects include spectral learning of language models,multi-view learning for gene expression and MRI data, and mining social media tobetter understand personality and well-being.3http://gosset.wharton.upenn.edu/?foster/index.pl4http://www.cs.columbia.edu/?stratos/5http://www.cis.upenn.edu/?ungar/15
