Semantic Similarity Applied to Spoken Dialogue SummarizationIryna Gurevych and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/english/homes/{gurevych|strube}AbstractWe present a novel approach to spoken dialoguesummarization.
Our system employs a set ofsemantic similarity metrics using the noun por-tion of WordNet as a knowledge source.
So far,the noun senses have been disambiguated man-ually.
The algorithm aims to extract utterancescarrying the essential content of dialogues.
Weevaluate the system on 20 Switchboard dia-logues.
The results show that our system out-performs LEAD, RANDOM and TF*IDF base-lines.1 IntroductionResearch in automatic text summarization began inthe late 1950s and has been receiving more atten-tion again over the last decade.
The maturity of thisresearch area is indicated by recent large-scale eval-uation efforts (Radev et al, 2003).
In comparison,speech summarization is a rather new research areawhich emerged only a few years ago.
However, thedemand for speech summarization is growing be-cause of the increasing availability of (digitally en-coded) speech databases (e.g.
spoken news, politi-cal speeches).Our research is concerned with the developmentof a system for automatically generating summariesof conversational speech.
As a potential applica-tion we envision the automatic generation of meet-ing minutes.
The approach to spoken dialoguesummarization presented herein unifies corpus- andknowledge-based approaches to summarization, i.e.we develop a shallow knowledge-based approach.Our system employs a set of semantic similar-ity metrics which utilize WordNet as a knowledgesource.
We claim that semantic similarity betweena given utterance and the dialogue as a whole is anappropriate criterion for the selection of utteranceswhich carry the essential content of the dialogue, i.e.relevant utterances.
?
In order to study the perfor-mance of semantic similarity methods, we removethe noise from the pre-processing modules by man-ually disambiguating lexical noun senses.In Section 2, we briefly describe research on sum-marization and how spoken dialogue summarizationdiffers from text summarization.
Section 3 givesthe semantic similarity metrics we use and describeshow they are applied to the summarization problem.Section 4 provides information about the data usedin our experiments, while Section 5 describes theexperiments and the results together with their sta-tistical significance.2 Text, Speech and DialogueSummarizationMost research on automatic summarization dealtwith written text.
This work was based either oncorpus-based, statistical methods or on knowledge-based techniques (for an overview over both strandsof research see Mani & Maybury (1999)).
Re-cent advances in text summarization are mostly dueto statistical techniques with some additional us-age of linguistic knowledge, e.g.
(Marcu, 2000;Teufel & Moens, 2002), which can be applied to un-restricted input.Research on speech summarization focusedmainly on single-speaker, written-to-be-spoken text(e.g.
spoken news, political speeches, etc.).
Themethods were mostly derived from work on textsummarization, but extended it by exploiting partic-ular characteristics of spoken language, e.g.
acous-tic confidence scores or intonation.
Difficulties arisebecause speech recognition systems are not perfect.Therefore, spoken dialogue summarization systemshave to deal with errors in the input.
There are nosentence boundaries in spoken language either.Work on spoken dialogue summarization is stillin its infancy (Reithinger et al, 2000; Zechner,2002).
Multiparty dialogue is much more difficultto process than written text.
In addition to the dif-ficulties speech summarization has to face, spokendialogue contains a whole range of dialogue phe-nomena as disfluencies, hesitations, interruptions,etc.
Also, the information to be summarized may becontributed by different speakers (e.g.
in question-answer pairs).
Finally, the language used in spokendialogue differs from language used in texts.
Be-cause discourse participants are able to immediatelyclarify misunderstandings, the language used doesnot have to be that explicit.3 Semantic Similarity3.1 Semantic Similarity MetricsExperiments reported here employed Ted Peder-sen?s (2002) semantic similarity package.
We ap-plied five of the metrics, which rely on WordNet asa knowledge base and were developed in the contextof work on word sense disambiguation.
The firstmeasure is Leacock and Chodorow?s (1998) Nor-malized Path Length (we will refer to it as lch).
Se-mantic similarity sim between words w1 and w2 isdefined as given in Equation 1:simc1,c2 = ?loglen(c1, c2)2 ?
D (1)c1 and c2 are concepts corresponding to w1 and w2.1len(c1, c2) is the length of the shortest path betweenthem.
D is the maximum depth of the taxonomy.The following measures incorporate an addi-tional, qualitatively different knowledge sourcebased on some kind of corpus analysis.
The ex-tended gloss overlaps measure introduced by Baner-jee & Pedersen (2003) (referred to as lesk in thefollowing) is based on the number of shared words(overlaps) in the WordNet definitions (glosses) ofthe respective concepts.
It also extends the glossesto include the definitions of concepts related to theconcept under consideration based on the WordNethierarchy.
Formally, semantic relatedness sim be-tween words w1 and w2 is defined by the followingequation:simc1,c2 =?score(R1(c1), R2(c2)) (2)where R is a set of semantic relations, score() isa function accepting two glosses as input, findingoverlaps between them, and returning a correspond-ing relatedness score.The remaining three methods require an addi-tional knowledge source, an information content file(ICF).
This file contains information content valuesfor WordNet concepts, which are needed for com-puting the semantic similarity score for two con-cepts.
Information content values are based on thefrequency counts for respective concepts.
Resnik(1995) (res for short) calculates the informationcontent of the concept that subsumes the given two1This also refers to the rest of the methods.concepts in the taxonomy (see Equation 3):simc1,c2 = maxc?S(c1,c2)[?
log p(c)] (3)where S(c1, c2) is the set of concepts which sub-sume both c1 and c2 and ?
log p(c) is the negativelog likelihood (information content).
The probabil-ity p is computed as the relative frequency of theconcept.
Resnik?s measure is based on the intuitionthat the semantic similarity between concepts maybe quantified on the basis of information shared be-tween them.
In this case the WordNet hierarchyis used to determine the closest super-ordinate of apair of concepts.Jiang & Conrath (1997) proposed to combineedge- and node-based techniques in counting theedges and enhancing it by the node-based calcu-lation of the information content as introduced byResnik (1995) (the method is abbreviated as jcn).The distance between two concepts c1 and c2 is for-malized as given in Equation 4:distc1,c2 = IC(c1) + IC(c2) ?
2 ?
IC(lso(c1, c2))(4)where IC is the information content value of theconcept, and lso(c1, c2) is the closest subsumer ofthe two concepts.The last method is that of Lin (1998) (we call thismetric lin).
He defined semantic similarity using aformula derived from information theory.
This mea-sure is sometimes called a universal semantic sim-ilarity measure as it is supposed to be application-,domain-, and resource independent.
According tothis method, the similarity is given in Equation 5:simc1,c2 =2 ?
log p(lso(c1, c2))log p(c1) + log p(c2)(5)3.2 Semantic Similarity in SummarizationThe process of automatic dialogue summarization,as defined in the context of this work, means to ex-tract the most relevant utterances from the dialogue.We restate this as a classification problem, whichis similar to the definition given by Kupiec et al(1995).
This means that utterances are classified asrelevant or irrelevant for the summary of a specificdialogue.
By relevant utterances we mean those car-rying the most essential parts of the dialogue?s con-tent.
The summarization task is, then, to extract theset of utterances from the transcript, which a humanwould use to make a dialogue summary.The key idea behind the algorithm presented hereis to quantify the degree of semantic similarity be-tween a given utterance and the whole dialogue.
Weargue that semantic similarity between an utteranceA.
: Utt1 Okay.Utt2 Tell me about your home.B.
: Utt3 Well, it?s an older home.Utt4 It was made back in the early sixties.Utt5 It?s a pier beam house.A.
: Utt6 Huh-uh.B.
: Utt7 Got three bedrooms, one bathUtt8 and that just makes me scream.A.
: Utt9 That?s pretty tough.Utt10 What area do you live in?B.
: Utt11 I live in Houston.Table 1: Switchboard dialogue fragmentNumber Concepts Sense NumberCRUtt1 ?
?CRUtt2 home 2CRUtt3 home 2CRUtt4 sixties 1CRUtt5 pier, beam, house 2, 2, 1CRUtt6 ?
?CRUtt7 bedrooms, bath 1, 5CRUtt8 ?
?CRUtt9 ?
?CRUtt10 area 1CRUtt11 Houston 1Table 2: Utterances mapped to WordNet conceptsand the dialogue as a whole represents an appropri-ate criterion for the selection of relevant utterances.We describe each of the processing steps, employ-ing the example dialogue D from Table 1.
Thisexample consists of the set of utterances {Utt1,...,Utt11}.3.2.1 Creating conceptual representationsThe semantic similarity algorithms introduced inSection 3.1 operate on the noun portion of WordNet.Our approach to dialogue summarization, as previ-ously stated, is to compute semantic similarity for agiven pair {Uttn,D}.
In order to do that, we requirea WordNet-based conceptual representation of bothUttn, i.e.
CRUttn , and D, i.e.
CRD, and com-pare them using the semantic similarity measures.Therefore, we map the nouns contained in the utter-ances to their respective WordNet senses and oper-ate on these representations in the subsequent steps.The results of this operation are shown in Table 2.The number in the last column indicates the disam-biguated WordNet sense.The resulting dialogue representation CRD willbe the set of concepts resulting from adding individ-ual utterance representations, i.e.
CRD = {home,home, sixties, pier, beam, house, bedrooms, bath,area, Houston}.3.2.2 Computing average semantic similarityFor each utterance Uttn, we create a two-dimensional matrix C with the dimen-sions (#CRD ?
#CRUttn), where # de-notes the number of elements in the set.C = (cij)i=1,...,#CRD,j=1,...,#CRUttn , see Ta-ble 3.
Then, we compute the semantic similaritySSscore(i, j) employing any of the semantic sim-ilarity metrics described above for each pair ofconcepts.
The semantic similarity score SSfinal forCRUttn and CRD is then defined as the averagepairwise semantic similarity between all conceptsin CRUttn and CRD:SSfinal =?#CRUttni=1?#CRDj=1 SSscore(i, j)#CRUttn ?
#CRDComputing SSfinal results in a list of utteranceswith scores from the respective scoring methods,Table 4.
Note that the absolute utterance scores aretaken from the real data, i.e.
they have been nor-malized w.r.t.
the conceptual representation for thewhole dialogue, and not for the dialogue fragmentgiven in Table 1.
The rankings were produced forthis specific example to make it more illustrative.3.2.3 Extracting relevant utterancesIn order to produce a summary of the dialogue, theutterances first have to be sorted numerically, i.e.ranked on the basis of their scores, see Table 4for the results of the ranking procedure.2 Given acompression rate COMP with the range [1,100],the number of utterances classified as relevant byan individual scoring method PNr is a functionof the total number of utterances in the dialogue:PNr = (COMP/100) ?
Numbertotal .3 Then,given a specific compression rate COMP , the top-ranked PNr utterances will be automatically classi-fied as relevant.
?
Returning to the example in Table1, we obtain the summaries given in Table 5.COMP Selected Utterances20% I live in Houston.Got three bedrooms, one bath.35% I live in Houston.Got three bedrooms, one bath.Tell me about your home.Well, it?s an older home.Table 5: Summaries based on Resnik?s measure2If two or more utterances get an equal score, they areranked according to the order of their occurrence.3Note that this number must be rounded to a natural number.home home sixties pier beam house bedrooms bath area Houstonbedrooms 3.8021 3.8021 0 2.5158 2.5158 3.8021 9.3157 5.8706 0.8287 0.8287bath 3.8021 3.8021 0 2.5158 2.5158 3.8021 5.8706 10.7821 0.8287 0.8287Table 3: Concept matrix C for Utt7 from Table 1 based on Resnik?s measureNumber Utterance Resnik?s score RankUtt1 Okay.
?
8Utt2 Tell me about your home.
1.4181106409372 3Utt3 Well, it?s an older home.
1.4181106409372 4Utt4 It was made back in the early sixties.
0.551830914995721 7Utt5 It?s a pier beam house.
1.18821772523631 6Utt6 Huh-uh.
?
9Utt7 Got three bedrooms, one bath 1.50689651387565 2Utt8 and that just makes me scream.
?
10Utt9 That?s pretty tough.
?
11Utt10 What area do you live in?
1.25186984433606 5Utt11 I live in Houston.
1.51301080520959 1Table 4: Utterance scores based on Resnik?s measureTokens Utterances TurnsTotal 34830 3275 1852Average 1741.5 163.75 92.6Table 6: Descriptive corpus statistics4 DataThe data used in the experiments are 20 randomlychosen Switchboard dialogues (Greenberg, 1996).These dialogues contain two-sided telephone con-versations among American speakers of at least 10minutes duration.
The callers were given a cer-tain topic for discussion.
The recordings of spon-taneous speech were, then, transcribed.
Statisticaldata about the corpus, i.e.
total numbers and aver-ages for separate dialogues, are given in Table 6.
To-kens are defined as running words and punctuation.An utterance is a complete unit of speech spoken bya single speaker, while a turn is a joint sequence ofutterances produced by one speaker.In the annotation experiments, we tested whetherhumans could reliably determine the utterances con-veying the overall meaning of the dialogue.
There-fore, each utterance is assumed to be a markable,i.e.
the expression to be annotated resulting in a to-tal of 3275 markables in the corpus.
Three anno-tators were instructed to select the most importantutterances.
They were supposed to first read the di-alogue und then to mark about 10% of all utterancesin the dialogue as being relevant.
Then, we pro-duced two kinds of Gold Standards from these data.Gold Standard 1 included the utterances which weremarked by all three annotators as being relevant.Gold Standard 2 included the utterances which wereselected by at least two annotators.Table 7 shows the results of these experiments.We present the absolute number of markables se-lected as relevant by separate annotators and in twoGold Standards.
Also, we indicate the percentage,given the total number of markables 3275.
As thetable shows, Gold Standard 1 includes only 3.69%of all markables.
Therefore, we used Gold Stan-dard 2 in the evaluation reported in Section 5.
TheKappa coefficient for inter-annotator agreement var-ied from 0.1808 to 0.6057 for individual dialogues.An examination of the particular dialogue with thevery low Kappa rate showed that this was one of theshortest ones.
It did not have a well-defined topicalstructure, resulting in a low agreement rate betweenannotators.
For the whole corpus, the Kappa co-efficient yielded 0.4309.
While this is not a highagreement rate on a general scale, it is compara-ble to what has been reported concerning the taskof summarization and in particular dialogue sum-marization.5 Evaluation5.1 Evaluation Metrics and BaselinesWe reformulated the problem in terms of stan-dard information retrieval evaluation metrics:Precision = PP/PNr, Recall = PP/NP , andFmeasure = 2 ?
Prec ?
Rec/(Prec + Rec).
PPis the number of cases where the individual scoringmethod and the Gold Standard agree.
PNr is com-puted according to the definition given in Section 3.Annotator 1 Annotator 2 Annotator 3 Gold Standard 1 Gold Standard 2?
417 12.73% 350 10.69% 347 10.6% 121 3.69% 310 9.47%Table 7: Number of markables labeled as relevantNP is the total number of utterances marked as rel-evant in the Gold Standard.
For comparison, threebaseline systems were implemented.
The first sys-tem is the RANDOM baseline, where relevant ut-terances (depending on the compression rate) wereselected by chance.
The second baseline system isbased on the TF*IDF scoring metric.
A large cor-pus is required to make this method fully power-ful.
Therefore, we computed TF*IDF scores forevery word on the basis of 2431 Switchboard dia-logues (ca.
19.3 MB of ASCII text).
Then, an av-erage TF*IDF score for each utterance of the 20 di-alogues in our corpus was computed by adding theindividual scores for all words in the utterance andnormalizing by the number of words.
The LEADbaseline is based on the intuition that the most im-portant utterances tend to occur at the beginning ofthe discourse.
While this observation is true for thedomain of news, the LEAD baseline is not necessar-ily efficient for the genre of spontaneous dialogues.However, given the Switchboard experimental datacollection setup, the dialogues usually directly startwith the discussions of the topic.
This hypothesiswas supported by evidence from our own annota-tion experiments, too.5.2 ResultsExperiments were performed using the semanticsimilarity package V0.05 (Pedersen, 2002) andWordNet 1.7.1.
We employed Gold Standard 2(see Section 4).
Three of the methods, namely res,lin, jcn, require the information content file (ICF).A method for computing the information contentof concepts from large corpora of text is given inResnik (1995).
ICF contains a list of synsets alongwith their part of speech and frequency count.
Wecompare the results obtained with 2 different ICFs:?
a WordNet-based ICF, provided at the time ofthe installation of the similarity package withpre-computed frequency values on the basis ofWordNet (WD ICF);?
an ICF, generated specifically on the basis of2431 Switchboard dialogues with the help ofutilities distributed together with the similaritypackage (SW ICF).Figures 1 and 2 indicate the performance of allmethods in terms of F-measure.
The results of the00.050.10.150.20.250.30.350.40 5 10 15 20 25 30 35 40F-measureCompression rate in %reslinjcnleskLEADTF*IDFlchRANDOMFigure 1: Results based on WordNet ICF00.050.10.150.20.250.30.350.40 5 10 15 20 25 30 35 40F-measureCompression rate in %reslinjcnleskLEADTF*IDFlchRANDOMFigure 2: Results based on Switchboard ICFsemantic similarity methods making use of the in-formation content file generally improve when theSwitchboard-based ICF is used.
The improvementsare especially significant for the jcn and lin mea-sures, while this does not seem to be the case for theres measure (depending on a specific compressionrate).The summarization methods perform best for thecompression rates in the interval [20,30].
Giventhese rates and the Switchboard-based ICF, the com-peting methods display the following performance(in descending order): jcn, res, lin, lesk, lch, tf*idf,lead, random.
For the default ICF the picture isslightly different: res, jcn and lesk, lch, lin, tf*idf,lead, random (see Table 8).
lch relying on WordNetstructure only performs worse than the rest of simi-larity metrics incorporating some corpus evidence.A direct comparison of our evaluation with alter-native results, e.g., Zechner?s (2002) is problematic.Though Zechner?s results are based on Switchboard,too, he employs a different evaluation scheme.
Theevaluation is broken down to the word level.
The re-sults are compared with multiple human annotationsinstead of a Gold Standard.5.3 Statistical Significance and Error AnalysisFor determining whether there is a significant differ-ence between the summarization approaches pair-wise, we use a paired related t-test (as the parentdistribution is unknown).
The null hypothesis statesthere is no difference between the two distributions.On consulting the t-test tables, we obtain the signif-icance values presented in Table 9, given the com-pression rate 25%4 and the Switchboard ICF.
Theseresults indicate that there is no statistically signifi-cant difference in the performance between the res,lin, jcn and lesk methods.
However, all of themsignificantly outperform the LEAD, TF*IDF andRANDOM baselines.The maximum Recall of the semantic similarity-based summarization methods in the current imple-mentation is limited to about 90%, given COMP =100%.
This means that if the system compiled a100% ?summary?, it would miss 10% of all utter-ances marked as relevant.
The reason lies in the factthat the algorithm operates on the concepts createdby mapping nouns to their WordNet senses.
Thus,the relevant utterances which do not have nouns onthe surface, but contain for example anaphorical ex-pressions realized as pronouns, are missed in the in-put.
Resolving anaphorical expressions in the pre-processing stage may eliminate this error source.6 Concluding RemarksWe introduced a new approach to spoken dialoguesummarization.
Our approach combines statistical,i.e.
corpus-based, and knowledge-based techniques.It utilizes the knowledge encoded in the noun partof WordNet and applies a set of semantic similar-ity metrics to dialogue summarization.
All seman-tic similarity-based summarization methods outper-form RANDOM, LEAD and TF*IDF baseline sys-tems.
In the following, we discuss some remainingchallenges and future research.More sophisticated data pre-processing.
We plan4Roughly speaking, the differences are most evident forcompression rates between 20% and 30%.to incorporate the pre-processing components usedby Zechner (2002) and evaluate their contribution toour task.
Including an anaphora resolution compo-nent would also result in better Recall.Automatic word sense disambiguation.
Switch-board conversational speech is highly ambiguous.Automatic disambiguation of noun senses to Word-Net concepts is important in order to integrate ourapproach into real-life summarization systems.Investigating other types of information in parallel.A clear desideratum will be assessing the overall co-herence of the discourse, speaker info, turn type, in-formation about non-nouns.Application to text and speech summarization.
Ourapproach can be applied to written-to-be-spokenspeech and text summarization.
It will be interest-ing to investigate whether conceptual structures oftexts (the input to our system) are comparable to theconceptual structures found in dialogues.Readability, coherence, and usability of the sum-maries produced.
A close examination of sum-maries based on human comprehension will be in-teresting.
It may be necessary to introduce filteringor other post-processing techniques improving thequality of summaries.Even without very sophisticated pre-processing ofthe dialogue data, our algorithm yields promisingresults.
It was evaluated on the Switchboard data,which is a challenging evaluation corpus.
Our vi-sion is to adopt the summarization approach pre-sented here in a system used for the automatic pro-duction of meeting minutes.AcknowledgmentsThis work has been funded by the Klaus TschiraFoundation.
We thank Christoph Zwirello forhis valuable contributions, the annotators TatjanaMedvedeva, Vanessa Michelli and Iryna Zhmaka,and Ted Pederson and colleagues for their software.ReferencesBanerjee, S. & T. Pedersen (2003).
Extended gloss over-lap as a measure of semantic relatedness.
In Proceed-ings of the 18th International Joint Conference on Ar-tificial Intelligence, Acapulco, Mexico, 9?15 August,2003, pp.
805?810.Greenberg, S. (1996).
The Switchboard transcriptionproject.
In Proceedings of the Large Vocabulary Con-tinuous Speech Recognition Summer Research Work-shop, Baltimore, Maryland, USA, April 1996.Jiang, J. J.
& D. W. Conrath (1997).
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the 10th International Conference onResearch in Computational Linguistics (ROCLING),pp.
19?33.
Tapei, Taiwan.RANDOM LEAD TF*IDF Res Lin Jcn Lesk LchPrecision 10% WD ICF .08563 .23242 .21101 .25076SW ICF .07645 .18654 .17125 .23853 .22936 .26606 .22936 .2507620% WD ICF .1026 .23933 .23018 .24390SW ICF .07963 .16159 .19512 .23780 .24085 .24695 .23780 .2378030% WD ICF .10429 .22380 .21261 .21668SW ICF .09407 .14140 .17192 .22075 .22482 .23093 .22584 .21567Recall 10% WD ICF .09032 .24516 .22258 .26452SW ICF .08065 .19677 .18065 .25161 .24194 .28065 .24194 .2645220% WD ICF .21613 .50645 .48710 .51613SW ICF .16774 .34194 .41290 .50323 .50968 .52258 .50323 .5032330% WD ICF .32903 .70968 .67419 .68710SW ICF .29677 .44839 .54516 .70000 .71290 .73226 .71613 .68387F-measure 10% WD ICF .08791 .23862 .21664 .25746SW ICF .07849 .19152 .17582 .24490 .23548 .27316 .23548 .2574620% WD ICF .13915 .32505 .31263 .33126SW ICF .108 .21946 .26501 .32298 .32712 .33540 .32298 .3229830% WD ICF .15839 .34029 .32328 .32947SW ICF .14286 .21500 .26141 .33565 .34184 .35112 .34339 .32792Table 8: Precision, Recall and F-measure for 10%, 20% and 30% and two ICFsres lin jcn lesk lead tf*idf lch randomres XXX p>0.05 p>0.05 p>0.05 p<0.01 p<0.01 p>0.05 p<0.01lin XXX p>0.05 p>0.05 p<0.01 p<0.05 p>0.05 p<0.01jcn XXX p>0.05 p<0.01 p<0.01 p<0.05 p<0.01lesk XXX p<0.01 p<0.05 p>0.05 p<0.01lead XXX p>0.05 p<0.01 p<0.01tf*idf XXX p>0.05 p<0.01lch XXX p<0.01random XXXTable 9: Statistical significance of results at COMP=25% and based on SW ICFKupiec, J., J. O. Pedersen & F. Chen (1995).
A trainabledocument summarizer.
In Research and Developmentin Information Retrieval, pp.
68?73.Leacock, C. & M. Chodorow (1998).
Combining localcontext and WordNet similarity for word sense iden-tification.
In C. Fellbaum (Ed.
), WordNet: An Elec-tronic Lexical Database, pp.
265?283.
Cambridge:MIT Press.Lin, D. (1998).
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, pp.
296?304.
Mor-gan Kaufmann, San Francisco, CA.Mani, I.
& M. T. Maybury (Eds.)
(1999).
Advances inAutomatic Text Summarization.
Cambridge/MA, Lon-don/England: MIT Press.Marcu, D. (2000).
The Theory and Practice of Dis-course Parsing and Summarization.
Cambridge/MA:The MIT Press.Pedersen, T. (2002).
Semantic Similarity Package.http://www.d.umn.edu/?tpederse/similarity.html.Radev, D. R., S. Teufel, H. Saggion, W. Lam, J. Blitzer,H.
Qi, A. Celebi, D. Liu & E. Drabek (2003).
Evalu-ation challenges in large-scale document summariza-tion.
In Proceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics, Sapporo,Japan, 7?12 July 2003, pp.
375?382.Reithinger, N., M. Kipp, R. Engel & J. Alexandersson(2000).
Summarizing multilingual spoken negotiationdialogues.
In Proceedings of the 38th Annual Meet-ing of the Association for Computational Linguistics,Hong Kong, 1?8 August 2000, pp.
310?317.Resnik, P. (1995).
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedings ofthe 14th International Joint Conference on ArtificialIntelligence, Montre?al, Canada, 1995, Vol.
1, pp.
448?453.Teufel, S. & M. Moens (2002).
Summarizing scientificarticles: Experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4):409?445.Zechner, K. (2002).
Automatic summarization of open-domain multiparty dialogues in diverse genres.
Com-putational Linguistics, 28(4):447?485.
