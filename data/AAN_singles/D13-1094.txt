Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 914?923,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsPredicting the Presence of Discourse ConnectivesGary Patterson and Andrew KehlerDepartment of LinguisticsUC San Diego9500 Gilman Drive #0108La Jolla, CA 92093{gpatterson,akehler}@ucsd.eduAbstractWe present a classification model that predictsthe presence or omission of a lexical connec-tive between two clauses, based upon linguis-tic features of the clauses and the type of dis-course relation holding between them.
Themodel is trained on a set of high frequencyrelations extracted from the Penn DiscourseTreebank and achieves an accuracy of 86.6%.Analysis of the results reveals that the most in-formative features relate to the discourse de-pendencies between sequences of coherencerelations in the text.
We also present resultsof an experiment that provides insight into thenature and difficulty of the task.1 IntroductionA central goal of natural language generation andsummarization systems is to produce interpretable,coherent text that rivals material a human would pro-duce.
Doing so requires that systems not only havethe ability to generate clauses that are grammaticaland easy for people to process, but also the abilityto employ the appropriate discourse structuring de-vices needed to yield fluid transitions between theseclauses.
This is a tricky issue in that it requires thata balance be achieved between the opposing goalsof communicative expressiveness and economy.
Onthe one hand, insufficient cueing of inter-clausal re-lationships can lead to a discourse that is at best dif-ficult to process, and at worst misunderstood.
On theother hand, too much explicit marking can result ina clunky and even redundant sounding discourse.Here we consider the question of when to ex-plicitly mark the COHERENCE RELATIONS in dis-course, that is, the inter-clausal relationships thatthe language producer intends the interpreter to in-fer between the meanings of clauses (Hobbs, 1979;Mann and Thompson, 1988; Kehler, 2002; Asherand Lascarides, 2003).
Consider, for example, theEXPLANATION coherence relation that holds in (1),in which the second clause provides a cause or rea-son for the eventuality described in the first:(1) a. Max will visit Australia this summer be-cause his father is turning 65.b.
Max will visit Australia this summer.
Hisfather is turning 65.As example (1) shows, coherence relations canbe marked explicitly?by using lexical connectivessuch as coordinating or subordinating conjunctions(e.g., because in 1a) or certain types of prepositionalor adverbial phrases?or left implicit as in (1b).
Ei-ther way, establishing the relation itself requires thereader to go through a complex inferential processnecessitating that a variety of assumptions be made,typically supported by context and/or world knowl-edge, that are not explicitly asserted by the actuallinguistic material.
In (1), for instance, such infer-ences would include that Max intends to see his fa-ther when he travels to Australia, that his father re-sides in that country, and that the birthday will tak-ing place during the time of the visit.
Importantly,the role of the connective in (1a) is therefore not toestablish that an EXPLANATION relation holds.
In-stead, connectives serve the function of directing theaddressee?s inference processes toward a smaller setof coherence relations than might otherwise be avail-able, among other possible roles.The fact that both (1a) and (1b) are felicitous maylead us to believe that the choice to insert a connec-tive between clauses is simply optional.
This is not914always the case, however.
Sometimes the use of aconnective is required, since omitting it would likelyresult in incorrect inferences being drawn by the ad-dressee.
For example, the use of when in (2a) im-plies a backward temporal ordering of events, whichis reversed if the connective is left out, as in (2b).
(2) a. Maggie fell over in shock when Saul of-fered to help her.b.
Maggie fell over in shock.
Saul offered tohelp her.On the other hand, a connective can seem unnec-essary if the relation between the two clauses is suf-ficiently implied by other cues in the text.
For in-stance, since the act of throwing a vase against aconcrete wall would normally be expected to causethe vase to break, the adverbial phrase as a resultin (3a), while felicitous, seems overly verbose andperhaps even redundant.
(3) a. Susan threw the fragile vase against theconcrete wall.
As a result, it broke.b.
Susan threw the fragile vase against theconcrete wall.
It broke.The foregoing examples suggest that the appropri-ateness of including an explicit connective is inher-ently gradient, and is in fact correlated with ease ofinference: the more difficult recovering the correctrelation would be without a connective, the morenecessary it is to include one.
This characterizationin turn suggests that predicting whether a connec-tive should be included might be a difficult problemfor an NLP system to address, since current-day sys-tems lack the requisite world knowledge and capac-ity for inference that would be necessary to evaluatethe ease with which coherence relations can be es-tablished on arbitrary examples.
However, it is alsopossible that the decision to include a connective de-pends in part on stylistic and other types of factors aswell, such that there might be predictive informationin the kinds of shallow linguistic and textual featuresthat systems do have access to.
This is the questiontaken up in this work: Given two adjacent clausesin a text, the type of coherence relation holding be-tween them and a candidate connective that could beused to signal the relation, we ask how well a sys-tem can predict whether or not that connective wasused by the author of the text.
This capability wouldbe useful to generation systems as a post-cursor todiscourse-level message planning and sentence real-ization processes, as well as summarization systemsthat take existing sentences and have to reconsiderconnective placement upon reassembling them.To our knowledge, there is no work in the lit-erature that addresses this issue directly.
There isa growing body of research (Sporleder and Las-carides, 2008; Pitler et al 2009; Lin et al 2009;Zhou et al 2010) that focuses on building super-vised models for classifying implicit relations usinga variety of contextual features, such as the polar-ity of clauses, the semantic class and tense/aspect ofverbs, and information from syntactic parses.
Withrespect to explicit relations, Elhadad and McKe-own (1990) sketch a procedure to select an appro-priate connective to link two propositions as part ofa larger text generation system, using linguistic fea-tures derived from the sentences.
The procedure se-lects the best connective from a given set of candi-dates, but does not allow for the option of leavingthe relation implicit.
More recently, Asr and Dem-berg (2012a) look at both explicit and implicit re-lations, and make the observation that certain rela-tion types are more likely to be realized explicitlythan others.
Relatedly, Asr and Demberg (2012b)discuss which connectives are the strongest predic-tors of which relation types.
However, there is nowork of which we are aware that specifically pre-dicts whether connectives should be used or omitted.2 Classification ModelOur model is a binary classifier trained on data ex-tracted from the Penn Discourse Treebank (PDTB;Prasad et al(2008)), a large-scale corpus of an-notated discourse coherence relations covering theone-million-word Wall Street Journal corpus of thePenn Treebank (Marcus et al 1993).2.1 DataFor every relation in the PDTB, the following com-ponents are annotated: (i) the connective used tosignal the relation; (ii) the textual spans of the twoclausal arguments that constitute the relation; (iii)the semantic sense of the relation, according to a hi-erarchical tagset of senses; and (iv) the attribution ofthe assertions and beliefs expressed in the text to the915relevant individuals.
Crucially for our purposes, forthe implicit relations the corpus indicates the mostsuitable connective if the relation were instead sig-naled explicitly.
For example, the annotators de-cided that the best connective to signal the REASONrelation in (4) would be because, rather than otherplausible candidates, such as as or since.
(4) It?s a shame their meeting never took place.
[IMPLICIT=because] Mr. Katzenstein certainlywould have learned something.
(WSJ0037)In total, there are 18,459 explicit and 16,053 im-plicit relations annotated in the PDTB.
We excludeda subset of these cases in training our model basedon two criteria.
First, whereas explicit relations inthe PDTB can hold between spans of text that ei-ther are or are not adjacent, we excluded the non-adjacent cases.
This was done to ensure consistencyin discourse structure between the relations consid-ered in the model, since only the implicit relationsbetween adjacent clauses were annotated.
Second,we excluded relations that have lower frequency se-mantic senses or use low frequency connectives.
Asa result, the model considers only the eight mostcommon semantic senses of relations, which in to-tal account for just less than 90% of the relationsin the corpus.1 Further, for each relation, we onlyconsider the connectives that account for more than5% of the instances of that relation.
After applyingthese filters, the resulting corpus comprised 10,039explicit and 11,690 implicit relations.Table 1 shows the eight relations that were mod-eled.
The majority of these relations exist at themiddle layer of the three-level hierarchy of seman-tic senses annotated in the PDTB.2 Relations at thehighest level ?
representing the four major semanticcategories COMPARISON, CONTINGENCY, EXPAN-SION and TEMPORAL ?
were deemed too broad tobe of practical use in a generation system, whereasthe lowest-level senses were considered either un-necessarily fine-grained or have too few tokens inthe corpus to allow for meaningful statistical model-ing.
Two exceptions were made for REASON andRESULT relations, which do appear at the lowest1The next most common relation type, CONDITION, was ex-cluded because it is always marked explicitly in the corpus.2For more details of the PDTB sense hierarchy, see Prasadet al(2008).level in the PDTB hierarchy (beneath the CAUSE cat-egory).
These were included because they are bothattested frequently in the corpus and are undeniablycontrastive: with REASON, the second clause pro-vides an explanation for the proposition expressedin the first clause, whereas with RESULT, the sec-ond clause describes a consequence of the first.
Itis reasonable to want these relations to be modeledseparately.Sections 2-22 of the corpus were used as the train-ing set, and sections 23 and 24 were used as the testset.
Sections 0 and 1 of the corpus were set asideas a development set for feature design and parame-ter optimization.
The training set comprised 18,218tokens, distributed as shown in Table 1.Relation Type Explicit ImplicitAsynchronous 1,120 (70%) 469 (30%)Conjunction 2,940 (61%) 1,906 (39%)Contrast 2,044 (66%) 1,054 (34%)Instantiation 203 (16%) 1,093 (84%)Reason 771 (28%) 1,938 (72%)Restatement 76 (4%) 2,081 (96%)Result 354 (21%) 1,295 (79%)Synchronous 748 (86%) 126 (14%)Total 8,256 (45%) 9,962 (55%)Table 1: Distribution of Training SetAs Table 1 shows, the preference for an overtconnective varies significantly according to the typeof relation.
The ASYNCHRONOUS, CONJUNCTION,CONTRAST and SYNCHRONOUS relations are real-ized explicitly the majority of the time, whereas IN-STANTIATION, REASON, RESTATEMENT and RE-SULT relations are more often left implicit.
We canalso see that some relation types (such as RESTATE-MENT, INSTANTIATION and SYNCHRONOUS) ex-hibit a strong preference to be realized in a particularform, whereas other types show more variability inwhether they are realized explicitly or implicitly.The distribution of tokens in Table 1 can be usedto determine a baseline accuracy against which theperformance of our model is evaluated.
A naivemodel that uses the semantic type of the coherencerelation as the sole predictive feature makes a bi-nary classification based simply on the majority cat-egory for that relation type.
A baseline model usingthis methodology results in classification accuracyof 77.0% over the held-out test set.9162.2 ModelWe built a composite model containing binary logis-tic regression classifiers for each coherence relation,trained on a set of linguistic features extracted fromeach token in the training set.
Logistic regressionwas chosen because it produces a model with highperformance and results that are easily interpretable.The features included in the model fall into the fol-lowing three broad classes: relation-level, argument-level, and discourse-level.Relation-level featuresIn addition to the semantic type of the relation, weinclude as a feature the connective used to signal therelation in the text (or, for the implicit relations, theconnective indicated by the annotators as most ap-propriate).
This feature (Connect) is included basedupon the observation that connectives vary as to theirrates of being realized explicitly?even for connec-tives that signal relations with the same semanticsense.
Consequently, given a relation of a partic-ular semantic type, an indication of the best fittingconnective may be a consistent predictor of whetheror not this relation is realized explicitly.We also include a feature reflecting the attributionof the relation.
As mentioned above, the PDTB isannotated to describe the attribution of the proposi-tions expressed within a relation to individuals or en-tities in the text.
For example, in the relation shownin (5), the first clause contains a direct quotation,clearly attributing the proposition expressed to theindividual Rep. Stark.
However, the second clausecontains no such indication of attribution to an entityin the text, and so the proposition is instead assumedto be asserted by the writer of the article.
(5) ?No magic bullet will be discovered next year,an election year,?
says Rep. Stark.
But 1991could be a window for action.
(WSJ0314)Inspection of the corpus data suggests that when oneargument of a relation contains a proposition that isattributable to an individual in the text (either by di-rect or indirect quotation) but the other is assumedby default to be attributed to the author, this rela-tion is more likely to be realized explicitly.
Thismay well have an explanation based on sentenceprocessing: the intervening attribution phrase ?saysRep.
Stark?
may serve as a distraction, with the re-sult that the intended coherence relation is harder toinfer without a connective.
Consequently, we in-clude a factor (AttMismatch) indicating if the twoarguments are not attributed to the perspective of thesame individual.Finally, in any particular genre there may be for-mulaic prose whose systematic features can be ex-ploited by a system tasked with generating textwithin that same genre.
In this case, the genre rep-resented by the corpus data comprises copy-editedarticles from the Wall Street Journal, many of whichrefer to company earnings reports or other financialevents, and are written in a highly prescribed style.Accordingly, we may suspect that there is a greaterprevalence of implicit relations in these cases, sincethe reader is assumed to be habituated to the wayin which the information in this type of article ispresented.
Consequently, for the domain at handwe include a binary feature (Financial) indicatingwhether the relation pertains to financial informa-tion.
This feature takes the value 1 if the textualspans of both arguments in the relation contain per-centage amounts or dollar figures.Argument-level featuresFor each relation, the model includes features cap-turing the size or complexity of each of its two argu-ments.
The arguments were identified by the annota-tors according to a principle of minimality, wherebythe annotations indicate the shortest text spans nec-essary for the appropriate coherence relation to beinterpreted.
However, the annotators also indicatedother text that is in some way relevant to the interpre-tation of the arguments.
This supplementary mate-rial can include unrestricted relative clauses, apposi-tives, or other parenthetical information.
Our obser-vation of the data indicates that relations which havesupplementary material annotated alongside one orboth of their arguments are more often than not re-alized explicitly with connectives.
As a result, weinclude binary features (Supp1, Supp2) indicatingwhether the first and second arguments of the rela-tion include such supplementary information.
Wealso include features (Length1, Length2) reflecting asimple measure of the length of each argument, cal-culated as the log transformed count of the numberof words in the arguments?
minimal text spans.One measure of the complexity of an argument917is the number of clauses it contains.
It might bethought that the greater the syntactic complexity ofan argument, the more likely it is that the relationcontaining it is marked explicitly, so as to give thereader more help in drawing the correct intendedinference between the arguments.
As a proxy forthe number of clauses in each argument, we includefeatures (NPSbj1, NPSbj2) equal to the total num-ber of main, subordinate, or complement clause sub-jects included within the textual spans of the re-spective arguments, determined using the syntacticparses available in the corpus data.We also consider whether the underlying richnessof the informational content expressed by the ar-gument may influence the presence or omission ofa connective.
Considering the way in which read-ers process text in real time, it would intuitivelybe more difficult to infer the intended relation be-tween two clauses without the aid of a connective ifthe arguments themselves had greater processing de-mands owing to increased lexical retrieval, referenceand anaphor resolution requirements, and so forth.Given this intuition, we may expect that argumentswith higher density of information are correlatedwith the increased use of connectives as a meansof facilitating the inference of the relation type andthereby easing the overall processing burden.
Con-sequently, our model includes features (ContDen-sity1, ContDensity2) calculated as the ratio of thecount of words in each argument that are contentwords (i.e.
ignoring articles, prepositions and pro-nouns), divided by the total number of words, aswell as features (PronDensity1, PronDensity2) cal-culated as the ratio of pronouns in each argument tothe number of noun phrases.Finally, the accessibility of the subject of the sec-ond argument in a relation may play a role in de-termining whether the relation is explicitly marked.Specifically, informal observation of the data sug-gests that there is a tendency for the second ar-gument of an implicit relation to begin with alonger, contentful noun phrase, rather than a pro-noun.
Consequently, our model includes a binaryfeature (FirstA2Pron) indicating whether the firstword in the second argument is a pronoun.Discourse-level featuresThe final class of features takes account of the wayin which a relation fits into the broader discoursestructure in the text.
In their work on implicit re-lation classification, Pitler et al(2008) identifiedvarious dependencies between bigram sequences ofexplicitly- and implicitly-realized relations of differ-ent semantic types.
These results suggest that thesemantic type and the presence of a connective inone relation may be predictive of whether or notthe following relation in the text is marked with aconnective.
Consequently, we include features in-dicating the semantic type of the relation occurringimmediately prior in the text (PrevSemType), andwhether this relation was marked implicitly or ex-plicitly (PrevForm).The other discourse-level features take account ofthe dependencies between the relation in questionand its neighboring relations in the text.
As part ofa supervised learning model developed to classifythe semantic class of implicit relations in the PDTB,Lin et al(2009) found features based on the twomain types of discourse dependency pattern in thecorpus (?shared?
and ?fully embedded?
arguments) tobe highly predictive.
We speculatively include simi-lar features in our model to see if they are helpful inpredicting the presence of connectives.The first type of dependency between adjacent re-lations is one where the second argument of one re-lation is also the first argument of the following re-lation, as in Figure 1.
Accordingly, we include twobinary features indicating whether an argument isshared with the preceding relation (Arg1isPrevArg2)or the following relation (Arg2isNextArg1) in thecorpus.Figure 1: Shared argumentThe other main type of discourse dependency, a?fully embedded?
dependency, is one where an entirerelation (including both of its arguments) is com-pletely embedded within one argument of an adja-cent relation in the text, as in Figure 2.
To capture918this type of dependency structure, we include twobinary features (EmbedNext, EmbedPrev) indicatingwhether the current relation is embedded within ei-ther one of its adjacent relations.
We also includetwo binary features (Arg1Embed, Arg2Embed) to in-dicate whether either argument of the current rela-tion completely contains an embedded relation.Figure 2: Fully embedded argumentThe two relations in (6) exemplify a typical in-stantiation of this embedded dependency structure.
(6) It is an overwhelming job.
[IMPLICIT= be-cause] There are so many possible proportionswhen you consider how many things are madeout of eggs and butter and milk.
(WSJ0261)In this example, there is an implicit REASON relationholding between the two complete sentences, andan explicit SYNCHRONOUS relation signaled by theconnective when holding between the two clausesof the second sentence.
Since the REASON relationfully embeds another relation within its second argu-ment, the feature Arg2Embed for this relation takesthe value 1.
For the SYNCHRONOUS relation, thefeature EmbedPrev takes the value 1 since the entirerelation is fully contained within the second argu-ment of the preceding relation in the text.3 Results and Evaluation3.1 Classification accuracyThe model was evaluated by assessing the accuracyof its predictions against the unseen test set.
Themodel achieved an overall accuracy of 86.6%, an im-provement of 9.6% above baseline.3 Table 2 showsthe model accuracy for each relation type, togetherwith the baseline performance based on the majoritycategory for that type.3During the preparation of the final version of this paper,a model was trained with an SVM using the same set of fea-tures, which resulted in a modest improvement in performance(87.3%).
The ensuing discussion of results, however, will con-tinue to pertain to the regression model.Relation Type Accuracy BaselineAsynchronous 91.7% 79.7%Conjunction 84.5% 78.2%Contrast 81.1% 65.0%Instantiation 83.3% 82.5%Reason 88.2% 68.3%Restatement 95.2% 95.2%Result 84.4% 76.9%Synchronous 96.5% 92.9%Total 86.6% 77.0%Table 2: Classification Accuracy by Relation TypeThe model achieved an improvement in accuracyacross all relation types but one: RESTATEMENT re-lations, for which the baseline accuracy was alreadyclose to 100%.
The greatest improvement in accu-racy was seen for REASON relations, for which themodel accuracy was 19.9% above baseline.
We nowdiscuss which of the factors in each of the featureclasses turned out to be the most predictive.3.2 Significant predictorsWe trained the model on subsets of the features toinvestigate the predictive power of the different fea-ture classes.
The accuracy assessed against the testset is shown in Table 3.Feature Class # Features AccuracyRelation Level 4 80.4%Argument Level 11 77.2%Discourse Level 8 80.9%Rel + Arg Levels 15 82.8%Rel + Disc Levels 12 85.1%Arg + Disc Levels 19 82.4%All Features 23 86.6%Table 3: Classification Accuracy by Feature ClassThe classes of Relation-level and Discourse-levelfeatures each separately yielded significantly betterperformance over baseline (one-sided tests of pro-portion, z=2.50 and z=2.92, respectively; p<0.01for both), whereas the Argument-level features aloneperformed only marginally better than baseline.However, all three classes of features are needed toattain the highest model performance.Across all relation types, we found that the fea-tures relating to the discourse dependencies betweena relation and its neighbors were the strongest and919most consistent predictors of whether that relationis explicit or implicit.
A relation that is fully em-bedded within a single argument of an adjacent re-lation in the text (indicated by the features Em-bedPrev and EmbedNext) has a much higher like-lihood of being signaled explicitly.
Conversely, arelation that fully contains another relation withinone of its arguments (indicated by Arg1Embed andArg2Embed) has a significantly higher likelihood ofbeing implicit.
The result is consistent with the em-bedded discourse dependency shown in (6), in whichthe implicit REASON relation fully contains an ex-plicit SYNCHRONOUS relation within its second ar-gument.The model also found that the features which in-dicate whether a relation has shared arguments witheither the preceding or following relations in the text(Arg1isPrevArg2, Arg2isNextArg1) are both predic-tors of an implicit outcome.
In other words, if aclause in the text serves as the argument for two ad-jacent relations, then both of these relations are morelikely to be realized implicitly.The next most predictive feature was the connec-tive used to signal the relation (Connect).
This fea-ture was a significant predictor for every relationtype.
Eliminating this feature from the final modelreduces the overall accuracy by 2.5%.
The otherfeatures in the model were less significantly predic-tive, and generally worked in the expected direction.Longer arguments (Length1, Length2) and the in-dication of a financial genre (Financial) were gen-erally associated with predicted implicit outcomes,whereas the presence of supplementary material(Supp1, Supp2), a mismatch of attribution (AttMis-match), more ?content rich?
arguments (ContDen-sity1, ContDensity2), and a pronoun appearing asthe first word of the second argument (FirstA2Pron)all tended to increase the odds in favor of a predictedexplicit outcome.The features indexing syntactic complexity(NPSbj1 and NPSbj2) were found to be marginallypredictive of an explicit outcome for most relationtypes, but the overall effect in the model was rel-atively small?resulting in only a 0.2% improve-ment?meaning that the level of performance re-ported on this task depends very little on the modelhaving access to full syntactic parses.
Somewhat un-expectedly, the factors indicating the semantic typeof the previous relation in the text (PrevSemType)and whether or not this relation was explicitly sig-naled by a connective (PrevForm) were found not tobe significant predictors.
Our analysis of the trainingdata confirmed the findings of Pitler et al(2008) inthat certain bigrams of coherence relation types aresignificantly more prevalent than others.
However,the differences in the frequencies were evidently notsufficiently correlated with the explicit/implicit dis-tinction as to make the type or form of the previousrelation a significant feature in the model.3.3 Error analysisWe analyzed a sample of cases incorrectly predictedby the model to see if there were any consistenttraits.
We focus our attention here on the CONTRASTrelations, which is the type with the lowest modelaccuracy.
The majority of these errors were caseswhere the model predicted that the relation would beexplicit?the most likely outcome for a CONTRASTrelation?whereas in the corpus the intended rela-tion was signaled by linguistic cues other than anovert connective.
For instance, the strong syntacticparallelism of the two arguments in (7), and the op-posite polarity of the lexical items delight and detri-ment, combine to induce a contrastive relationshipwithout the need for a connective.
(7) To the delight of some doctors, the bill droppeda plan passed by the Finance Committee.
[IMPLICIT=but] To the detriment of many low-income people, efforts to boost Medicaid fund-ing were also stricken.
(WSJ2372)Other ways that the contrast relation is signaledimplicitly include contrasting temporal modifiers (Itwasn?t so long ago X.
Now, Y), repetition of thepredicate in the argument (.
.
.
it could only happenonce.
.
.
.
it?s happening again), or even by the useof punctuation such as a semicolon.
Previous work(Sporleder and Lascarides, 2008; Lin et al 2009)has sought to make use of such cues to identify andclassify implicit relations in the text.
The resultsof this brief error analysis suggest that such indi-rect cues could also be useful factors in determiningwhether to choose to use a connective for a givenrelation type when generating text.9204 Judgment StudyThe system described in the last section outper-formed a baseline majority-category classifier on thetask of deciding whether a relation should be madeexplicit or left implicit.
This result might be con-sidered surprising, for two reasons that we havepreviously discussed.
First, the system was ableto make this improvement using relatively shallowfeatures extracted from the text, without access tothe richer types of contextual information and worldknowledge required for establishing coherence rela-tions during actual discourse comprehension.
Sec-ond, the data suggest that the appropriateness of in-cluding a connective is not as cut-and-dried as a bi-nary classification task may suggest, but is insteadgradient, with many cases for which the inclusionof a connective appears to be optional.
Obviously,the PDTB does not avail us of the opportunity toevaluate this gradience directly (or even use a 3-way required/optional/redundant distinction), sincethe producer of the actual text samples in the corpushad to ultimately decide whether or not to use a con-nective.
The apparent optionality of many examplesthus puts limits on how well we can expect a systemto perform, since there is no way to reliably predictcases in which the decision is made arbitrarily.This observation leads us to ask how well humansperform on this same task.
Do they make highlyaccurate predictions, or does optionality limit theirperformance?
In order to shed light on this question,we carried out an experiment to see how consistentlyhumans choose to use lexical connectives to signalintended coherence relations between clauses.4.1 MethodologyWe selected a balanced sample of 100 clause-pairtokens from the test set, reflecting the distribution ofthe different major relation types (six relations wererepresented in the sample).
This sample comprised44 explicit and 56 implicit tokens, consistent withthe distribution in the overall corpus.
The experi-mental stimulus for each item consisted of two ver-sions of the same clause pair, one including a con-nective between the clauses, and the other without.For relations that were realized explicitly in the cor-pus, as in (8a), the alternative implicit stimulus omit-ted the connective and showed the second argumentas a separate sentence, as in (8b).
(8) a. Mr. Nesbit also said the FDA has askedBolar Pharmaceutical Co. to recall at theretail level its urinary tract antibiotic, butso far the company hasn?t complied withthat request.b.
Mr. Nesbit also said the FDA has askedBolar Pharmaceutical Co. to recall at theretail level its urinary tract antibiotic.
Sofar the company hasn?t complied with thatrequest.For the implicit relations, the alternative explicitstimulus for the experiment used the connective an-notated in the PDTB as the one being most appro-priate.
For each item, a short passage was createdincluding the preceding and following sentences inthe text to serve as context.
The relative orderingof the presentation of the explicit and implicit formswas randomized, without regard to the actual corpusoutcome for that stimulus.Using Amazon?s Mechanical Turk, judges werepresented with the two passages for each item.
Theywere told to assume that the passages had the sameintended meaning, and were asked to judge whichof the two sounded more natural.
We collected 30responses for each item.44.2 ResultsWe classified each experimental item as either ex-plicit or implicit, based on the majority response ofthe judges.
Using this classification, the judges?
re-sponses matched the actual outcomes in 68 of the100 cases.5 The distribution of correctly-judgeditems across relation types is shown in Table 4.The judgments for REASON relations most closelymatched the corpus outcomes, with 9 out of the 12explicit tokens and all 6 implicit tokens in the cor-4The data from a small number of judges were discardeddue to an unreasonably fast response time or because their judg-ments showed a unanimous preference across every experimen-tal item.
This left a total of 2,925 judgments over the 100 ex-perimental items, from 113 different judges.5Using the majority response of judges for each item tomeasure classification accuracy is consistent with the statisti-cal model, whereby probabilities are rounded up or down to ar-rive at a binary classification.
If accuracy is instead calculatedin terms of average correctness over the individual responses,performance drops to 60.4%.921pus correctly identified by the judges.
The lowestscoring relation type was CONTRAST, for which 9of the 10 explicit tokens were judged correctly butonly 4 out of the 11 implicit tokens were correctlyidentified.Relation Type Items Correct AccuracyConjunction 22 15 68.2%Contrast 21 13 61.9%Instantiation 9 6 66.7%Reason 18 15 83.3%Restatement 16 9 62.5%Result 14 10 64.2%Total 100 68 68.0%Table 4: Results of Mechanical Turk studyThere were hence 32 experimental items forwhich the majority response by the judges did notmatch the actual corpus outcome.
In two-thirds(21) of these cases, the judges indicated a prefer-ence for a connective when the relation in the corpuswas implicit.
These mismatches occurred across therange of relation types.
This suggests that the judgestended to err on the side on inserting a connective,even when it may not have been strictly necessary.While the reason for this is not clear, one possibilityis that the texts reflected the genre and the highly-prescribed editing guidelines for the newspaper arti-cles that comprise the corpus, under which unneces-sary or redundant words are excised.
Without suchpressures to edit the copy down to a minimal form,the judges may have preferred to see the relationssignaled explicitly in cases in which either decisionwould result in a felicitous passage.In the remaining 11 cases, for which the relationsin the corpus were explicitly signaled with a connec-tive, the judges on average indicated a preference toleave the relation implicit.
Interestingly, all of thesecases were either CONJUNCTION or CONTRAST re-lations, semantic types which are usually signaledexplicitly with a connective.
We inspected thesecases to ascertain why judges may have preferredan outcome opposite to that actually seen in the text.We found that all 7 of the CONTRAST mismatcheswere instances where the second argument of the re-lation in the corpus was a sentence beginning withthe coordinating conjunction but, as in (9).Similarly, three of the mismatched CONJUNCTION(9) At those levels stocks are set up to be ham-mered by index arbitragers.
But nobody knowsat what level the futures and stocks will opentoday.
(WSJ2300)relations had a sentential second argument begin-ning with the conjunction and.
The responses ofthe judges to these cases may simply reflect a dis-preference for sentence-initial conjunctions, a prac-tice which is frowned upon in prescriptive grammarbooks, but apparently allowed by the Wall StreetJournal style sheet.For this sample of 100 relations, the modelachieves a classification accuracy of 84%.
This mayseem at first blush to be an odd result, since it ap-pears that the model is surpassing human perfor-mance.
As we have suggested, however, this couldbe the result of our experimental judges having dif-ferent preferences than the writers and editors at theWall Street Journal for cases in which connectiveplacement is truly optional.
We therefore sought toevaluate the effect of optionality on these results.If inaccurate predictions are associated with op-tionality of connective use, we might expect thatboth human judges and the classification modelwould be less certain about their categorizations ofthese examples than for the cases that were cor-rectly classified.
This was indeed the case.
First,there was a significant difference in the variabilityof judges?
responses between items that were incor-rectly classified and those that were correct (66% vs.73%, respectively; two-sample t test: t=2.60, df=73,p<0.02).
Thus, as a group the judges were less sureof themselves in those cases in which they incor-rectly decided to use or omit the connective, sug-gesting that either option may have been acceptable.Second, we analyzed the levels of confidence ourmodel had for its judgments on correctly and incor-rectly categorized cases, measured in terms of theprobability of the predicted outcome assigned bythe model.
The analysis revealed that the averagemodel confidence for the relations that were incor-rectly classified was significantly lower than the av-erage model confidence for the correctly-classifieditems (71% vs. 88%, respectively; t=5.65, df=25,p<0.001).
Taken together, these results are consis-tent with the idea that, at least for a significant por-tion of the data, the incorrect judgments made by922both the judges and the model may have occurredon passages for which either including or omittingthe connective would have been acceptable.5 ConclusionWe have presented a model that predicts whether thecoherence relation holding between two clauses ismarked explicitly with a lexical connective or leftimplicit.
Whereas there is reason to think that anauthor?s decision to use a connective is in part in-fluenced by properties of the extra-linguistic con-text that are inaccessible to NLP systems (such assemantics and world knowledge), we find that rel-atively simple linguistic features derivable from theclauses and from local discourse dependencies canbe exploited to reach a level of performance signifi-cantly greater than that achieved by a baseline.
Thevariability in the judgments of native speakers whenpresented with these data suggests that the use of aconnective is in many cases simply optional; in suchcases the decision may reflect lower-level stylisticchoices on the part of the author.
This in turn in-dicates that there may be an inherent upper boundto the performance of computational systems on thistask.AcknowledgmentsWe thank Roger Levy for useful discussions aboutthis work and three anonymous reviewers for theirhelpful feedback.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofconversation.
Cambridge University Press.Fatemeh Torabi Asr and Vera Demberg.
2012a.
Implicit-ness of discourse relations.
In Proceedings of the 24thInternational Conference on Computational Linguis-tics, pages 2669?2684.Fatemeh Torabi Asr and Vera Demberg.
2012b.
Mea-suring the strength of linguistic cues for discourse re-lations.
In Proceedings of the Workshop on Advancesin Discourse Analysis and its Computational Aspects(ADACA), pages 33?42.Michael Elhadad and Kathleen R McKeown.
1990.
Gen-erating connectives.
In Proceedings of the 13th Con-ference on Computational Linguistics-Volume 3, pages97?101.Jerry R Hobbs.
1979.
Coherence and coreference.
Cog-nitive science, 3(1):67?90.Andrew Kehler.
2002.
Coherence, reference, and thetheory of grammar.
CSLI Publications, Stanford.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1, pages 343?351.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind K Joshi.
2008.
Eas-ily identifiable discourse relations.
In Proceedings ofthe 22nd International Conference on ComputationalLinguistics.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2, pages 683?691.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.In Proceedings of the 6th International Conference onLanguage Resources and Evaluation.Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetoricalrelations: An assessment.
Natural Language Engi-neering, 14(3):369?416.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, JianSu, and Chew Lim Tan.
2010.
Predicting discourseconnectives for implicit discourse relation recognition.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, pages 1507?1514.923
