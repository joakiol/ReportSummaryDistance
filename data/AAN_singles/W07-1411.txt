Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 66?71,Prague, June 2007. c?2007 Association for Computational LinguisticsA Perspective-Based Approach for Solving Textual Entailment RecognitionO?scar Ferra?ndez, Daniel Micol, Rafael Mun?oz, and Manuel PalomarNatural Language Processing and Information Systems GroupDepartment of Computing Languages and SystemsUniversity of AlicanteSan Vicente del Raspeig, Alicante 03690, Spain{ofe, dmicol, rafael, mpalomar}@dlsi.ua.esAbstractThe textual entailment recognition systemthat we discuss in this paper representsa perspective-based approach composed oftwo modules that analyze text-hypothesispairs from a strictly lexical and syntacticperspectives, respectively.
We attempt toprove that the textual entailment recognitiontask can be overcome by performing indi-vidual analysis that acknowledges us of themaximum amount of information that eachsingle perspective can provide.
We comparethis approach with the system we presentedin the previous edition of PASCAL Recognis-ing Textual Entailment Challenge, obtainingan accuracy rate 17.98% higher.1 IntroductionTextual entailment recognition has become a popu-lar Natural Language Processing task within the lastfew years.
It consists in determining whether onetext snippet (hypothesis) entails another one (text)(Glickman, 2005).
To overcome this problem sev-eral approaches have been studied, being the Recog-nising Textual Entailment Challenge (RTE) (Bar-Haim et al, 2006; Dagan et al, 2006) the most re-ferred source for determining which one is the mostaccurate.Many of the participating groups in previous edi-tions of RTE, including ourselves (Ferra?ndez et al,2006), designed systems that combined a variety oflexical, syntactic and semantic techniques.
In ourcontribution to RTE-3 we attempt to solve the tex-tual entailment recognition task by analyzing twodifferent perspectives separately, in order to ac-knowledge the amount of information that an indi-vidual perspective can provide.
Later on, we com-bine both modules to obtain the highest possible ac-curacy rate.
For this purpose, we analyze the pro-vided corpora by using a lexical module, namelyDLSITE-1, and a syntactic one, namely DLSITE-2.Once all results have been obtained we perform avoting process in order to take into account all sys-tem?s judgments.The remainder of this paper is structured as fol-lows.
Section two describes the system we havebuilt, providing details of the lexical and syntacticperspectives, and explains the difference with theone we presented in RTE-2.
Third section presentsthe experimental results, and the fourth one providesour conclusions and describes possible future work.2 System SpecificationThis section describes the systemwe have developedin order to participate in RTE-3.
It is based on sur-face techniques of lexical and syntactic analysis.
Asthe starting point we have used our previous systempresented in the second edition of the RTE Chal-lenge (Ferra?ndez et al, 2006).
We have enrichedit with two independent modules that are intendedto detect some misinterpretations performed by thissystem.
Moreover, these new modules can also rec-ognize entailment relations by themselves.
The per-formance of each separate module and their combi-nation with our previous system will be detailed insection three.Next, Figure 1 represents a schematic view of thesystem we have developed.66Figure 1: System architecture.As we can see in the previous Figure, our sys-tem is composed of three modules that are coordi-nated by an input scheduler.
Its commitment is toprovide the text-hypothesis pairs to each module inorder to extract their corresponding similarity rates.Once all rates for a given text-hypothesis pair havebeen calculated, they will be processed by an outputgatherer that will provide the final judgment.
Themethod used to calculate the final entailment deci-sion consists in combining the outputs of both lex-ical and syntactic modules, and these outputs withour RTE-2 system?s judgment.
The output gathererwill be detailed later in this paper when we describethe experimental results.2.1 RTE-2 SystemThe approach we presented in the previous edition ofRTE attempts to recognize textual entailment by de-termining whether the text and the hypothesis are re-lated using their respective derived logic forms, andby finding relations between their predicates usingWordNet (Miller et al, 1990).
These relations havea specific weight that provide us a score represent-ing the similarity of the derived logic forms and de-termining whether they are related or not.For our participation in RTE-3 we decided to ap-ply our previous system because it allows us to han-dle some kinds of information that are not correctlymanaged by the new approaches developed for thecurrent RTE edition.2.2 Lexical ModuleThis method relies on the computation of a wide va-riety of lexical measures, which basically consists ofoverlap metrics.
Although in other related work thiskind of metrics have already been used (Nicholsonet al, 2006), the main contribution of this module isthe fact that it only deals with lexical features with-out taking into account any syntactic nor semanticinformation.
The following paragraphs list the con-sidered lexical measures.Simple matching: initialized to zero.
A booleanvalue is set to one if the hypothesis word appears inthe text.
The final weight is calculated as the sum ofall boolean values and normalized dividing it by thelength of the hypothesis.Levenshtein distance: it is similar to simple match-ing.
However, in this case we use the mentioneddistance as the similarity measure between words.When the distance is zero, the increment value isone.
On the other hand, if such value is equal to one,the increment is 0.9.
Otherwise, it will be the inverseof the obtained distance.Consecutive subsequence matching: this measureassigns the highest relevance to the appearance ofconsecutive subsequences.
In order to perform this,we have generated all possible sets of consecutivesubsequences, from length two until the length inwords, from the text and the hypothesis.
If we pro-ceed as mentioned, the sets of length two extractedfrom the hypothesis will be compared to the sets ofthe same length from the text.
If the same element ispresent in both the text and the hypothesis set, thena unit is added to the accumulated weight.
This pro-cedure is applied for all sets of different length ex-tracted from the hypothesis.
Finally, the sum of theweight obtained from each set of a specific length isnormalized by the number of sets corresponding to67this length, and the final accumulated weight is alsonormalized by the length of the hypothesis in wordsminus one.
This measure is defined as follows:CSmatch =|H|?i=2f(SHi)|H| ?
1(1)where SHi contains the hypothesis?
subsequencesof length i, and f(SHi) is defined as follows:f(SHi) =?j?SHimatch(j)|H| ?
i+ 1(2)being match(j) equal to one if there exists an ele-ment k that belongs to the set that contains the text?ssubsequences of length i, such that k = j.One should note that this measure does not con-sider non-consecutive subsequences.
In addition, itassigns the same relevance to all consecutive sub-sequences with the same length.
Furthermore, thelonger the subsequence is, the more relevant it willbe considered.Tri-grams: two sets containing tri-grams of lettersbelonging to the text and the hypothesis were cre-ated.
All the occurrences in the hypothesis?
tri-grams set that also appear in the text?s will increasethe accumulated weight in a factor of one unit.
Theweight is normalized by the size of the hypothesis?tri-grams set.ROUGE measures: considering the impact of n-gram overlap metrics in textual entailment, we be-lieve that the idea of integrating these measures1 intoour system is very appealing.
We have implementedthem as defined in (Lin, 2004).Each measure is applied to the words, lemmas andstems belonging to the text-hypothesis pair.
Withinthe entire set of measures, each one of them is con-sidered as a feature for the training and test stagesof a machine learning algorithm.
The selected onewas a Support Vector Machine due to the fact that itsproperties are suitable for recognizing entailment.2.3 Syntactic ModuleThe syntactic module we have built is composed offew submodules that operate collaboratively in order1The considered measures were ROUGE-N with n=2 andn=3, ROUGE-L, ROUGE-W and ROUGE-S with s=2 and s=3.to obtain the highest possible accuracy by using onlysyntactic information.The commitment of the first two submodules isto generate an internal representation of the syntac-tic dependency trees generated by MINIPAR (Lin,1998).
For this purpose we obtain the output of suchparser for the text-hypothesis pairs, and then processit to generate an on-memory internal representationof the mentioned trees.
In order to reduce our sys-tem?s noise and increase its accuracy rate, we onlykeep the relevant words and discard the ones that webelieve do not provide useful information, such asdeterminants and auxiliary verbs.
After this step hasbeen performed we can proceed to compare the gen-erated syntactic dependency trees of the text and thehypothesis.The graph node matching, termed alignment, be-tween both the text and the hypothesis consists infinding pairs of words in both trees whose lemmasare identical, no matter whether they are in the sameposition within the tree.
Some authors have alreadydesigned similar matching techniques, such as theone described in (Snow et al, 2006).
However, theseinclude semantic constraints that we have decidednot to consider.
The reason of this decision is that wedesired to overcome the textual entailment recogni-tion from an exclusively syntactic perspective.
Theformula that provides the similarity rate between thedependency trees of the text and the hypothesis inour system, denoted by the symbol ?, is shown inEquation 3:?
(?, ?)
=?????(?)
(3)where ?
and ?
represent the text?s and hypothesis?syntactic dependency trees, respectively, and ?
is theset that contains all synsets present in both trees, be-ing ?
= ?
?
?
??
?
?, ?
?
?.
As we can observe inEquation 3, ?
depends on another function, denotedby the symbol ?, which provides the relevance ofa synset.
Such a weight factor will depend on thegrammatical category and relation of the synset.
Inaddition, we believe that the most relevant words ofa phrase occupy the highest positions in the depen-dency tree, so we desired to assign different weightsdepending on the depth of the synset.
With all thesefactors we define the relevance of a word as shown68in Equation 4:?(?)
= ?
?
?
?
????
(4)where ?
is a synset present in both ?
and ?, ?
rep-resents the weight assigned to ?
?s grammatical cat-egory (Table 1), ?
the weight of ?
?s grammaticalrelationship (Table 2), ?
an empirically calculatedvalue that represents the weight difference betweentree levels, and ??
the depth of the node that containsthe synset ?
in ?.
The performed experiments revealthat the optimal value for ?
is 1.1.Grammatical category WeightVerbs, verbs with one argument, verbs withtwo arguments, verbs taking clause as com-plement1.0Nouns, numbers 0.75Be used as a linking verb 0.7Adjectives, adverbs, noun-noun modifiers 0.5Verbs Have and Be 0.3Table 1: Weights assigned to the relevant grammati-cal categories.Grammatical relationship WeightSubject of verbs, surface subject, object ofverbs, second object of ditransitive verbs1.0The rest 0.5Table 2: Weights assigned to the grammatical rela-tionships.We would like to point out that a requirement ofour system?s similarity measure is to be independentof the hypothesis length.
Therefore, we must de-fine the normalized similarity rate, as represented inEquation 5:?
(?, ?)
=?????(?)?????(?
)(5)Once the similarity value has been calculated, itwill be provided to the user together with the cor-responding text-hypothesis pair identifier.
It will behis responsibility to choose an appropriate thresholdthat will represent the minimum similarity rate to beconsidered as entailment between text and hypothe-sis.
All values that are under such a threshold willbe marked as not entailed.3 System EvaluationIn order to evaluate our system we have generatedseveral results using different combinations of allthree mentioned modules.
Since the lexical one usesa machine learning algorithm, it has to be run withina training environment.
For this purpose we havetrained our system with the corpora provided in theprevious editions of RTE, and also with the develop-ment corpus from the current RTE-3 challenge.
Onthe other hand, for the remainder modules the devel-opment corpora was used to set the thresholds thatdetermine if the entailment holds.The performed tests have been obtained by per-forming different combinations of the describedmodules.
First, we have calculated the accuracyrates using only each single module separately.Later on we have combined those developed by ourresearch group for this year?s RTE challenge, whichare DLSITE-1 (the lexical one) and DLSITE-2 (thesyntactic one).
Finally we have performed a votingprocess between these two systems and the one wepresented in RTE-2.The combination of DLSITE-1 and DLSITE-2 isdescribed as follows.
If both modules agree, then thejudgement is straightforward, but if they do not, wethen decide the judgment depending on the accuracyof each one for true and false entailment situations.In our case, DLSITE-1 performs better while dealingwith negative examples, so its decision will prevailover the rest.
Regarding the combination of the threeapproaches, we have developed a voting strategy.The results obtained by our system are representedin Table 3.
As it is reflected in such table, the high-est accuracy rate obtained using the RTE-3 test cor-pus was achieved applying only the lexical module,namely DLSITE-1.
On the other hand, the syntac-tic one had a significantly lower rate, and the samehappened with the system we presented in RTE-2.Therefore, a combination of them will most likelyproduce less accurate results than the lexical mod-ule, as it is shown in Table 3.
However, we wouldlike to point out that these results depend heavily onthe corpus idiosyncrasy.
This can be proven with theresults obtained for the RTE-2 test corpus, where thegrouping of the three modules provided the highestaccuracy rates of all possible combinations.69RTE-2 test RTE-3 dev RTE-3 testOverall Overall Overall IE IR QA SUMRTE-2 system 0.5563 0.5523 0.5400 0.4900 0.6050 0.5100 0.5550DLSITE-1 0.6188 0.7012 0.6563 0.5150 0.7350 0.7950 0.5800DLSITE-2 0.6075 0.6450 0.5925 0.5050 0.6350 0.6300 0.6000DLSITE-1&2 0.6212 0.6900 0.6375 0.5150 0.7150 0.7400 0.5800Voting 0.6300 0.6900 0.6375 0.5250 0.7050 0.7200 0.6000Table 3: Results obtained with the corpora from RTE-2 and RTE-3.3.1 Results AnalysisWe will now perform an analysis of the resultsshown in the previous section.
First, we would liketo mention the fact that our system does not be-have correctly when it has to deal with long texts.Roughly 11% and 13% of the false positives ofDLSITE-1 and DLSITE-2, respectively, are causedby misinterpretations of long texts.
The underlyingreason of these failures is the fact that it is easier tofind a lexical and syntactic match when a long textis present in the pair, even if there is not entailment.In addition, we consider very appealing to showthe accuracy rates corresponding to true and falseentailment pairs individually.
Figure 2 represents thementioned rates for all system combinations that wedisplayed in Table 3.Figure 2: Accuracy rates obtained for true and falseentailments using the RTE-3 test corpus.As we can see in Figure 2, the accuracy ratesfor true and false entailment pairs vary significantly.The modules we built for our participation in RTE-3obtained high accuracy rates for true entailment text-hypothesis pairs, but in contrast they behaved worsein detecting false entailment pairs.
This is the oppo-site to the system we presented in RTE-2, since it hasa much higher accuracy rate for false cases than trueones.
When we combinedDLSITE-1 andDLSITE-2,their accuracy rate for true entailments diminished,although, on the other hand, the rate for false onesraised.
The voting between all three modules pro-vided a higher accuracy rate for false entailments be-cause the system we presented at RTE-2 performedwell in these cases.Finally, we would like to discuss some examplesthat lead to failures and correct forecasts by our twonew approaches.Pair 246 entailment=YES task=IRT: Overall the accident rate worldwide for commercial aviationhas been falling fairly dramatically especially during the periodbetween 1950 and 1970, largely due to the introduction of newtechnology during this period.H: Airplane accidents are decreasing.Pair 246 is incorrectly classified by DLSITE-1due to the fact that some words of the hypothesis donot appear in the same manner in the text, althoughthey have similar meaning (e.g.
airplane andaviation).
However, DLSITE-2 is able to establish atrue entailment for this pair, since the hypothesis?syntactic dependency tree can be matched within thetext?s, and the similarity measure applied betweenlemmas obtains a high score.
This fact producesthat, in this case, the voting also achieves a correctprediction for pair 246.Pair 736 entailment=YES task=SUMT: In a security fraud case, Michael Milken was sentenced to 10years in prison.H: Milken was imprisoned for security fraud.Pair 736 is correctly classified by DLSITE-1 sincethere are matches for all hypothesis?
words (exceptimprisoned) and some subsequences.
In contrast,DLSITE-2 does not behave correctly with this exam-ple because the main verbs do not match, being thisfact a considerable handicap for the overall score.704 Conclusions and Future WorkThis research provides independent approaches con-sidering mainly lexical and syntactic information.
Inorder to achieve this, we expose and analyze a widevariety of lexical measures as well as syntactic struc-ture comparisons that attempt to solve the textual en-tailment recognition task.
In addition, we proposeseveral combinations between these two approachesand integrate them with our previous RTE-2 systemby using a voting strategy.The results obtained reveal that, although thecombined approach provided the highest accuracyrates for the RTE-2 corpora, it has not accom-plished the expected reliability in the RTE-3 chal-lenge.
Nevertheless, in both cases the lexical-basedmodule achieved better results than the rest of the in-dividual approaches, being the optimal for our par-ticipation in RTE-3, and obtaining an accuracy rateof about 70% and 65% for the development and testcorpus, respectively.
One should note that these re-sults depend on the idiosyncrasies of the RTE cor-pora.
However, these corpora are the most reliableones for evaluating textual entailment recognizers.Future work can be related to the developmentof a semantic module.
Our system achieves goodlexical and syntactic comparisons between texts, butwe believe that we should take advantage of the se-mantic resources in order to achieve higher accuracyrates.
For this purpose we plan to build a modulethat constructs characterized representations basedon the text using named entities and role labeling inorder to extract semantic information from a text-hypothesis pair.
Another future research line couldconsist in applying different recognition techniquesdepending on the type of entailment task.
We havenoticed that the accuracy of our approach differswhen the entailment is produced mainly by lexicalor syntactic implications.
We intend to establish anentailment typology and tackle each type by meansof different points of view or approaches.AcknowledgmentsThis research has been partially funded by theQALL-ME consortium, which is a 6th FrameworkResearch Programme of the European Union (EU),contract number FP6-IST-033860 and by the Span-ish Government under the project CICyT numberTIN2006-1526-C06-01.
It has also been supportedby the undergraduate research fellowships financedby the Spanish Ministry of Education and Science,and the project ACOM06/90 financed by the Span-ish Generalitat Valenciana.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The Second PASCAL Recognising Textual En-tailment Challenge.
Proceedings of the Second PAS-CAL Challenges Workshop on Recognising TextualEntailment, pages 1?9.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual Entail-ment Challenge.
In Quin?onero-Candela et al, edi-tors, MLCW 2005, LNAI Volume 3944, pages 177?190.Springer-Verlag.Oscar Ferra?ndez, Rafael M. Terol, Rafael Mun?oz, Patri-cio Mart?
?nez-Barco, and Manuel Palomar.
2006.
Anapproach based on Logic forms and wordnet relation-ships to textual entailment performance.
In Proceed-ings of the Second PASCAL Challenges Workshop onRecognising Textual Entailment, pages 22?26, Venice,Italy.Oren Glickman.
2005.
Applied Textual Entailment Chal-lenge.
Ph.D. thesis, Bar Ilan University.Dekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems, Granada, Spain.Chin-Yew Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
In Stan Szpakow-icz Marie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop,pages 74?81, Barcelona, Spain, July.
Association forComputational Linguistics.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.
In-troduction to WordNet: An On-line Lexical Database.International Journal of Lexicography, 3(4):235?244.Jeremy Nicholson, Nicola Stokes, and Timothy Baldwin.2006.
Detecting Entailment Using an Extended Imple-mentation of the Basic Elements Overlap Metrics.
InProceedings of the Second PASCAL Challenges Work-shop on Recognising Textual Entailment, pages 122?127, Venice, Italy.Rion Snow, Lucy Vanderwende, and Arul Menezes.2006.
Effectively using syntax for recognizing falseentailment.
In Proceedings of the North AmericanAssociation of Computational Linguistics, New YorkCity, New York, United States of America.71
