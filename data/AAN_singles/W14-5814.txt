Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 111?119,Coling 2014, Dublin, Ireland, August 24 2014.A Database of Paradigmatic Semantic Relation Pairs forGerman Nouns, Verbs, and AdjectivesSilke Scheible and Sabine Schulte im WaldeInstitut f?ur Maschinelle SprachverarbeitungUniversit?at Stuttgart{scheible,schulte}@ims.uni-stuttgart.deAbstractA new collection of semantically related word pairs in German is presented, which was compiledvia human judgement experiments and comprises (i) a representative selection of target lexi-cal units balanced for semantic category, polysemy, and corpus frequency, (ii) a set of human-generated semantically related word pairs based on the target units, and (iii) a subset of thegenerated word pairs rated for their relation strength, including positive and negative relationevidence.
We address the three paradigmatic relations antonymy, hypernymy and synonymy, andsystematically work across the three word classes of adjectives, nouns, and verbs.A series of quantitative and qualitative analyses demonstrates that (i) antonyms are more canon-ical than hypernyms and synonyms, (ii) relations are more or less natural with regard to the spe-cific word classes, (iii) antonymy is clearly distinguishable from hypernymy and synonymy, buthypernymy and synonymy are often confused.
We anticipate that our new collection of seman-tic relation pairs will not only be of considerable use in computational areas in which semanticrelations play a role, but also in studies in theoretical linguistics and psycholinguistics.1 IntroductionThis paper describes the collection of a database of paradigmatically related word pairs in German whichwas compiled via human judgement experiments hosted on Amazon Mechanical Turk.
While paradig-matic relations (such as synonymy, antonymy, hypernymy, and hyponymy) have been extensively re-searched in theoretical linguistics and psycholinguistics, they are still notoriously difficult to identifyand distinguish computationally, because their distributions in text tend to be very similar.
For example,in The boy/girl/person loves/hates the cat, the nominal co-hyponyms boy, girl and their hypernym per-son as well as the verbal antonyms love and hate occur in identical contexts, respectively.
A dataset ofparadigmatic relation pairs would thus represent a valuable test-bed for research on semantic relatedness.For the compilation of the relation dataset we aimed for a sufficiently large amount of human-labelleddata, which may both serve as seeds for a computational approach, and provide a gold-standard for evalu-ating the resulting computational models.
This paper describes our efforts to create such a paradigmaticrelation dataset in a two-step process, making use of two types of human-generated data: (1) human sug-gestions of semantically related word pairs, and (2) human ratings of semantic relations between wordpairs.
Furthermore, we are the first to explicitly work across word classes (covering adjective, nounand verb targets), and to incorporate semantic classes, corpus frequency and polysemy as balancingcriteria into target selection.
The resulting dataset1consists of three parts:1.
A representative selection of target lexical units drawn from GermaNet, a broad-coverage lexical-semantic net for German, using a principled sampling technique and taking into account the threemajor word classes adjectives, nouns, and verbs, which are balanced according to semantic category,polysemy, and type frequency.2.
A set of human-generated semantically related word pairs, based on the target lexical units.3.
A subset of semantically related word pairs, rated for the strength of the relation between them.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1The dataset is available from http://www.ims.uni-stuttgart.de/data/sem-rel-database.111We anticipate that our new collection of semantic relation pairs will not only be of considerable use incomputational areas in which semantic relations play a role (such as Distributional Semantics, NaturalLanguage Understanding/Generation, and Opinion Mining), but also in studies in theoretical linguisticsand psycholinguistics.
In addition, our dataset may be of major interest for research groups working onautomatic measures of semantic relatedness, as it allows a principled evaluation of such tools.
Finally,since the target lexical units are drawn from the GermaNet database, our results will be directly relevantfor assessing, developing, and maintaining this resource.2 Related workOver the years a number of datasets have been made available for studying and evaluating semanticrelatedness.
For English, Rubenstein and Goodenough (1965) obtained similarity judgements from 51subjects on 65 noun pairs, a seminal study which was later replicated by Miller and Charles (1991), andResnik (1995).
Finkelstein et al.
(2002) created a set of 353 English noun-noun pairs rated by 16 subjectsaccording to their semantic relatedness on a scale from 0 to 10.
For German, Gurevych (2005) replicatedRubenstein and Goodenough?s experiments by translating the original 65 word pairs into German.
Inlater work, she used the same experimental setup to increase the number of word pairs to 350 (Gurevych,2006).The dataset most similar to ours is BLESS (Baroni and Lenci, 2011), a freely available dataset thatincludes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities, and grouped into 17 broad classes such as bird, fruit.
For each target concept, BLESS con-tains several relata, connected to it through a semantic relation (hypernymy, co-hyponymy, meronymy,attribute, event), or through a null-relation.
BLESS thus includes two paradigmatic relations (hypernymy,co-hyponymy) but does not focus on paradigmatic relations.
Furthermore, it is restricted to concretenouns, rather than working across word classes.3 Paradigmatic relationsThe focus of this work is on semantic relatedness, and in particular on paradigmatic semantic relations.This section discusses the theoretical background of the notion of paradigmatic semantic relations.
Theterm paradigmatic goes back to de Saussure (1916), who introduced a distinction between linguisticelements based on their position relative to each other.
This distinction derives from the linear nature oflinguistic elements, which is reflected in the fact that speech sounds follow each other in time.
Saussurerefers to successive linguistic elements that combine with each other as ?syntagma?, and thus the relationbetween these elements is called ?syntagmatic?.
On the other hand, elements that can be found in thesame position in a syntagma, and which could be substituted for each other, are in a ?paradigmatic?relationship.
While syntagmatic and paradigmatic relations can hold between a variety of linguistic units(such as morphemes, phonemes, clauses, or sentences), the focus of this research is on the relationsbetween words.Many studies in computational linguistics work on the assumption that paradigmatic semantic relationshold between words.
As will become apparent in the course of this work, it is necessary to move beyondthese definitions for an appropriate investigation of paradigmatic semantic relations.
According to Cruse(1986), sense is defined as ?the meaning aspect of a lexical unit?, and he states that ?semantic relations?hold between lexical units, not between lexemes.The goal of this work is to create a database of semantic relations for German adjectives, nouns andverbs, focussing on the three types of paradigmatic relations referred to as sense-relations by Lyons(1968) and Lyons (1977): synonymy, antonymy, and hypernymy.4 Experimental setupOur aim was to collect semantically related word pairs for the paradigmatic relations antonymy, syn-onymy, and hypernymy for the three word classes nouns, verbs, and adjectives.
For this purpose weimplemented two experiments involving human participants.
Starting with a set of target words, in thefirst experiment participants were asked to propose suitable antonyms, synonyms and hypernyms for112each of the targets.
For example, for the target verb befehlen (?to command?
), participants proposedantonyms such as gehorchen (?to obey?
), synonyms such as anordnen (?to order?
), and hypernyms suchas sagen (?to say?
).In the second experiment, participants were asked to rate the strength of a given semantic relation withrespect to a word pair on a 6-point scale.
For example, workers would be presented with a pair ?wild ?free?
and asked to rate the strength of antonymy between the two words.
All word pairs were assessedwith respect to all three relation types.Both experiments will be described in further detail in Sections 5 and 6.
The current section aims toprovide an overview of GermaNet, a lexical-semantic word net for German, from which the set of targetwords was drawn (4.1).
We then describe the selection of target words from GermaNet, which used astratified sampling approach (4.2).
Finally, we introduce the platform used to implement the experiments,Amazon Mechanical Turk (4.3).4.1 Target source: GermaNetGermaNet is a lexical-semantic word net that aims to relate German nouns, verbs, and adjectives seman-tically.
GermaNet has been modelled along the lines of the Princeton WordNet for English (Miller et al.,1990; Fellbaum, 1998) and shares its general design principles (Hamp and Feldweg, 1997; Kunze andWagner, 1999; Lemnitzer and Kunze, 2007).
For example, lexical units denoting the same concept aregrouped into synonym sets (?synsets?).
These are in turn interlinked via conceptual-semantic relations(such as hypernymy) and lexical relations (such as antonymy).
For each of the major word classes, thedatabases further take a number of semantic categories into consideration, expressed via top-level nodesin the semantic network (such as ?Artefakt/artifact?, ?Geschehen/event?, ?Gef?uhl/feeling?).
However, incontrast to WordNet, GermaNet also includes so-called ?artificial concepts?
to fill lexical gaps and thusenhance network connectivity, and to avoid unsuitable co-hyponymy (e.g.
by providing missing hyper-nyms or hyponyms).
GermaNet also differs from WordNet in the way in which it handles part of speech.For example, while WordNet employs a clustering approach to structuring adjectives, GermaNet uses ahierarchical structure similar to the one employed for the noun and verb hierarchies.
Finally, the latestreleases of WordNet and GermaNet also differ in size: While WordNet 3.0 contains at total of 117,659synsets and 155,287 lexical units, the respective numbers for GermaNet 6.0 are considerably lower, with69,594 synsets and 93,407 lexical units.Since GermaNet is the largest database of its kind for German, and as it encodes all types of relationsthat are of interest for us (synonymy, antonymy, and hypernymy), it represents a suitable starting pointfor our purposes.4.2 Target selectionThe purpose of collecting the set of targets was to acquire a broad range of lexical items which couldbe used as input for generating semantically related word pairs (cf.
Section 5).
Relying on GermaNetversion 6.0 and the respective JAVA API, we used a stratified sampling technique to randomly select 99nouns, 99 adjectives and 99 verbs from the GermaNet files.
The random selection was balanced for1.
the size of the semantic classes,2accounting for the 16 semantic adjective classes and the 23 se-mantic classes for both nouns and verbs, as represented by the file organisation;2. three polysemy classes according to the number of GermaNet senses:I) monosemous, II) two senses and III) more than two senses;3. three frequency classes according to type frequency in the German web corpus SdeWaC (Faa?
andEckart, 2013), which contains approx.
880 million words:I) low (200?2,999), II) mid (3,000?9,999) and III) high (?10,000).The total number of 99 targets per word class resulted from distinguishing 3 sense classes and 3 frequencyclasses, 3?3 = 9 categories, and selecting 11 instances from each category, in proportion to the semanticclass sizes.2For example, if an adjective GermaNet class contained 996 word types, and the total number of adjectives over all semanticclasses was 8,582, and with 99 stimuli collected in total, we randomly selected 99?996/8, 582 = 11 adjectives from this class.1134.3 Experimental platform: Mechanical TurkThe experiments described below were implemented in Amazon Mechanical Turk (AMT)3, a web-basedcrowdsourcing platform which allows simple tasks (so-called HITs) to be performed by a large numberof people in return for a small payment.
In our first experiment, human associations were collectedfor different semantic relation types, where AMT workers were asked to propose suitable synonyms,antonyms, and hypernyms for each of the targets.
The second experiment was based on a subset of thegenerated synonym/antonym/hypernym pairs and asked the workers to rate each pair for the strengthof antonymy, synonymy, and hypernymy between them, on a scale between 1 (minimum strength) and6 (maximum strength).
To control for non-native speakers of German and spammers, each batch ofHITs included two examples of ?non-words?
(invented words following German morphotactics such asBlapselheit, gekortiert) in a random position.
If participants did not recognise the invented words, weexcluded all their ratings from consideration.
While we encouraged workers to complete all HITs in agiven batch, we also accepted a smaller number of submitted HITs, as long as the workers had a goodoverall feedback score.5 Generation experiment5.1 MethodThe goal of the generation experiment was to collect human associations for the semantic relation typesantonymy, hypernymy, and synonymy.
For each of our 3?99 adjective, noun, and verb targets, we asked10 participants to propose a suitable synonym, antonym, and hypernym.
Targets were bundled randomlyin 9 batches per word class, each including 9 targets plus two invented words.
The experiment consistedof separate runs for each relation type to avoid confusion between them, with participants first generatingsynonyms, then antonyms, and finally hypernyms for the targets, resulting in 3 word classes?
99 targets?
3 relations ?
10 participants = 8, 910 target?response pairs.5.2 Results and discussion(a) Total number of responses:Table 1 illustrates how the number of generated word pairs distributes across word classes and relations.The total number per class and relation is 990 tokens (99 targets ?
10 participants).
From the maximumnumber of generated pairs, a total of 131 types (211 tokens) were discarded because the participantsprovided no response.
These cases had been accepted via AMT nevertheless because the participantswere approved workers and we assumed that the empty responses showed the difficulty of specific word?relation constellations, see below.
For example, six out of ten participants failed to provide a synonymfor the adjective bundesrepublikanisch ?federal republic?.ANT HYP SYN alltypes tokens types tokens types tokens types tokensADJ 524 990 676 990 597 990 1,797 2,970NOUN 708 990 701 990 621 990 2,030 2,970VERB 636 990 662 990 620 990 1,918 2,970all 1,868 2,970 2,039 2,970 1,838 2,970 5,745 8,910Table 1: Number of generated relation pairs across word classes.
(b) Number of ambiguous responses:An interesting case is provided by pairs that were generated with regard to different relations but for thesame target word.
Table 2 lists the number of types of such ambiguous pairs, and the intersection ofthe tokens.
For example, if five participants generated a pair with regard to a target and relation x, andtwo participants generated the same pair with regard to relation y, the intersection is 2.
The intersection3https://www.mturk.com114is more indicative of ambiguity here, because in most cases of ambiguity the intersection is only 1,which might as well be the result of an erroneously generated pair (e.g., because the participant didnot pay attention to the task), rather than genuine ambiguity.
Examples of ambiguous responses with anintersection > 1 are Gegenargument?Argument ?counter argument ?
argument?, which was provided fivetimes as an antonymy pair and twice as a hypernymy pair; freudlos?traurig ?joyless ?
sad?, which wasprovided four times as a synonymy pair and five times as a hypernymy pair; and beseitigen?entfernen?eliminate ?
remove?, which was provided five times as a synonymy pair and five times as a hypernymypair.ANT+HYP ANT+SYN HYP+SYN ANT+HYP+SYNtypes tokens types tokens types tokens types tokensADJ 6 6 4 4 195 342 2 2NOUN 15 16 17 17 93 117 5 6VERB 4 4 8 8 182 290 5 6all 25 26 29 29 470 749 12 14Table 2: Number of ambiguous relation pairs across word classes.The ambiguities in Table 2 indicate that humans are quite clear about what distinguishes antonymsfrom synonyms, and what distinguishes antonyms from hypernyms.
On the other hand, the line dividinghypernymy and synonymy is less clear, and the large amount of confusion between the two relationslends support to theories claiming that hypernymy should be considered a type of synonymy, and thatreal synonymy does not exist in natural languages for economical reasons.
Furthermore, the confusionis most obvious for adjectives and verbs, for which the relation is considered less natural than for nouns,cf.
Miller and Fellbaum (1991).
(c) Number of (different) responses across word classes and relations:An analysis of the number of different antonyms, hypernyms and synonyms generated for a given targetshows no noticeable difference at first glance: on average, 6.04 different antonyms were generated forthe targets, while the number is minimally higher for synonyms with 6.08 different responses on average;hypernyms received considerably more (6.78) different responses on average.
However, the distributionof the numbers of different antonym, hypernym, and synonym responses across the targets shows that theantonymy generation task results in more targets with a small number of different responses comparedto the synonymy and the hypernym task (Figure 1): there are 10 targets for which all ten participantsgenerated the same antonym (x = number of different responses = 1), such as dunkel?hell ?dark ?
light?and verbieten?erlauben ?to forbid ?
to allow?, while there are 17 targets where they generated exactly two(x=2), and 29 targets where they suggested three different antonyms (x=3).
In contrast, for hypernymyand synonymy, there are 0/3 targets where all participants agreed on the same response, and there areonly 5/10 targets where they generated exactly two, and 8/21 targets where they generated only threedifferent hypernyms/synonyms.These results are in line with previous findings for English and Swedish by Paradis and Willners (2006)and Paradis et al.
(2009), who argue against the strict contrast between ?direct?
and ?indirect?
antonymswhich has been assumed in the literature (see, for example, Gross et al.
(1989)) in favour of a scale of?canonicity?
where some word pairs are perceived as more antonymic than others.
In particular, theypropose that the weaker the degree of canonicity, the more different responses the target items will yieldin an elicitation experiment.
Similar to the current findings for German, they found that for Englishand Swedish there is a small core of highly opposable couplings which have been conventionalised asantonym pairs in text and discourse, while all other couplings form a scale from more to less stronglyrelated.
The ten targets for which all participants generated the same antonym response are thus likelyto represent highly ?canonical?
pairings.
The fact that the hypernymy and synonymy generation exper-iments results in fewer targets with only one or two different responses suggests that hypernymy andsynonymy have a lower level of canonicity than antonymy.115Figure 1: Number of targets plotted against the number of different responses.Figure 2 demonstrates that the overall distributions of the frequency of responses are, however, verysimilar for antonyms, hypernyms and synonyms: between 72% and 77% of the responses were onlygiven once, with the curves following a clear downward trend.
Note that a strength of 10 in Figure 2refers to the case of one different response (x=1) in Figure 1.Figure 2: Response magnitude.Finally, Figure 3 compares the number of blank responses (types and tokens) regarding antonyms,hypernyms and synonyms.
Across word classes, 74/115 targets (types/tokens) received blank antonymresponses, while only 25/34 targets received blank hypernym responses and only 32/62 targets receivedblank synonym responses.
These numbers indicate that participants find it harder to come up withantonyms than hypernyms or synonyms.
Breaking the proportions down by word class, Figure 3 demon-strates that in each case the number of missing antonyms (left panel: types; right panel: tokens) is largerthan those of missing hypernyms/synonyms.
Figure 3 also shows that the difficulty to provide rela-tion pairs varies across word classes.
While antonyms are the most difficult relation in general, thereare more blank responses regarding adjectives and nouns, in comparison to verbs.
Hypernymy seemssimilarly difficult across classes, and synonymy is more difficult for nouns than for adjectives or verbs.Figure 3: Blank responses (types and tokens).116(d) Comparison with GermaNet:The results of the generation experiment can be used to extend and develop GermaNet, the resource thetargets were drawn from: a large proportion of responses are not covered in GermaNet.
Table 3 belowshows for the three parts of speech and the three relation types how many responses were covered by boththe generation experiment (EXP) and GermaNet (GN) (column ?Both?
), how many of them only appearin the generation experiment (column ?EXP?
), and how many are only listed in GermaNet (?GN?).
Blankand multi-word responses in the experimental results were excluded from consideration.
The comparisonshows that the variety of semantic relation types in our experimental dataset is considerably larger than inGermaNet, while the overlap is marginal.
Especially for antonyms, the coverage in GermaNet seems tobe quite low, across word classes.
For hypernymy and synonymy, the semantic relation types complementeach other to a large extent, with each resource containing relations that are not part of the other resource.In sum, the tables confirm that extending GermaNet with our relation types should enrich the manualresource.ANT HYP SYNBoth EXP GN Both EXP GN Both EXP GNADJ 33 453 5 100 561 237 66 496 160NOUN 3 633 2 108 561 393 59 516 150VERB 10 542 2 132 507 260 40 554 109Table 3: Relation coverage in Generation Experiment (EXP) and GermaNet (GN).6 Rating experiment6.1 MethodIn the second experiment, Mechanical Turk workers were asked to rate the strength of a given semanticrelation with respect to a word pair on a 6-point scale.
The main purpose of this experiment was toidentify and distinguish between ?strong?
and ?weak?
examples of a specific relation.
The numberof times a specific response was given in the generation experiment does not necessarily indicate thestrength of the relation.
This is especially true for responses that were suggested by only one or twoparticipants, where it is difficult to tell if the response is an error, or if it relates to a idiosyncratic senseof the target word that the other participants did not think of in the first instance.
Crucially, in the ratingexperiment all word pairs were assessed with respect to all three relation types, thus asking not only forpositive but also negative evidence of semantic relation instances.The set of word pairs used as input is a carefully selected subset of responses acquired in the generationexperiment.4For each of the 99 targets and each of the semantic relations antonymy, synonymy, andhypernymy two responses were included (if available): the response with the highest frequency (randomchoice if several available), and a response with a lower frequency (2, if available, otherwise 1; randomchoice if several available).
Multi-word responses and blanks were excluded from consideration.
Amanual post-processing step aimed to address the issue of duplicate pairs in the randomly generateddataset, where the same responses had been generated for two of the relations.In theory, each target should have 6 associated pairs (2xANT, 2xHYP, 2xSYN).
In practice, there aresometimes fewer than 6 pairs per target in the dataset, because (i) for some targets, only one responseis available for a given relation (e.g., if all 10 participants provided the same response), or (ii) no validresponse of the required frequency type is available.
The resulting dataset includes 1,684 target-responsepairs altogether, 546 of which are adjective pairs, 574 noun pairs, and 564 verb pairs.
To avoid confusion,the ratings were collected in separate experimental settings, i.e., for each word class and each relationtype, all generated pairs were first judged for their strength of one relation, and then for their strength ofanother relation.4For time and money reasons, we could not collect the 8, 910?
3 ?
10 = 267, 300 ratings for all responses.1176.2 Results and discussionIn the following, we present the results of the rating experiment in terms of mean rating scores for eachword pair.
The mean rating scores were calculated across all ten ratings per pair.
The purpose of theanalysis was to verify that the responses generated in the generation experiment are in fact perceived asexamples of the given relation type by other raters.
We thus looked at all responses for a given relationtype in the data set and calculated the average value of all mean ratings for this relation type.
For example,Figure 4 (left panel) shows that the responses generated as antonyms are clearly perceived as antonymsin the case of adjectives, with an average rating score of 4.95.
Verb antonyms are also identified as suchwith a rating of 4.38.
The situation for nouns, however, is less clear: an average rating of 3.70 is onlyminimally higher than the middle point of the rating scale (3.50).
These findings support the commonassumption that antonymy is a relation that applies well to adjectives and verbs, but less so to nouns.Responses generated as synonyms (plot omitted for space reasons), on the other hand, are identified assuch for all three words classes, with average rating values of 4.78 for adjectives, 4.48 for nouns, and4.66 for verbs.Figure 4: Average ratings of antonym/hypernym responses as ANT or SYN, across word classes.Finally, Figure 4 (right panel) shows the average ratings as synonyms/antonyms for responses gener-ated as hypernyms.
The findings corroborate our analysis of synonym/hypernym confusion in Section 5:the distribution looks fairly similar to the one for synonyms, with low antonymy ratings, but an averagesynonymy rating of 4.43 for adjectives, 3.08 for nouns, and 3.89 for verbs.
The results suggest thathypernymy is particularly difficult to distinguish from synonymy in the case of adjectives.7 ConclusionThis article presented a new collection of semantically related word pairs in German which was compiledvia human judgement experiments.
The database consists of three parts:1.
A representative selection of target lexical units drawn from GermaNet, using a principled samplingtechnique and taking into account the three major word classes adjectives, nouns, and verbs, whichare balanced according to semantic category, polysemy, and type frequency.2.
A set of 8,910 human-generated semantically related word pairs, based on the target lexical units.3.
A subset of 1,684 semantically related word pairs, rated for the strengths of relations.To our knowledge, our dataset is the first that (i) focuses on multiple paradigmatic relations, (ii) system-atically works across word classes, (iii) explicitly balances the targets according to semantic category,polysemy and type frequency, and (iv) explicitly provides positive and negative rating evidence.
We de-scribed the generation and the rating experiments, and presented a series of quantitative and qualitativeanalyses.
The analyses showed that (i) antonyms are more canonical than hypernyms and synonyms,(ii) relations are more or less natural with regard to the specific word classes, (iii) antonymy is clearlydistinguishable from hypernymy and synonymy, and (iv) hypernymy and synonymy are often confused.AcknowledgementsThe research was funded by the DFG Sachbeihilfe SCHU-2580/2-1 (Silke Scheible) and the DFGHeisenberg Fellowship SCHU-2580/1-1 (Sabine Schulte im Walde).118ReferencesMarco Baroni and Alessandro Lenci.
2011.
How we BLESSed Distributional Semantic Evaluation.
In Proceed-ings of the EMNLP Workshop on Geometrical Models for Natural Language Semantics, pages 1?10, Edinburgh,UK.D.
Allan Cruse.
1986.
Lexical Semantics.
Cambridge Textbooks in Linguistics.
Cambridge University Press,Cambridge, UK.Ferdinand de Saussure.
1916.
Cours de Linguistique G?en?erale.
Payot.Gertrud Faa?
and Kerstin Eckart.
2013.
SdeWaC ?
a Corpus of Parsable Sentences from the Web.
In Proceedingsof the International Conference of the German Society for Computational Linguistics and Language Technology,pages 61?68, Darmstadt, Germany.Christiane Fellbaum, editor.
1998.
WordNet ?
An Electronic Lexical Database.
Language, Speech, and Commu-nication.
MIT Press, Cambridge, MA.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-pin.
2002.
Placing Search in Context: The Concept Revisited.
ACM Transactions on Information Systems,20(1):116?131.Derek Gross, Ute Fischer, and George A. Miller.
1989.
Antonymy and the Representation of Adjectival Meanings.Memory and Language, 28(1):92?106.Iryna Gurevych.
2005.
Using the Structure of a Conceptual Network in Computing Semantic Relatedness.
InProceedings of the 2nd International Joint Conference on Natural Language Processing, pages 767?778, JejuIsland, Korea.Iryna Gurevych.
2006.
Thinking beyond the Nouns - Computing Semantic Relatedness across Parts of Speech.In Sprachdokumentation & Sprachbeschreibung, 28.
Jahrestagung der Deutschen Gesellschaft f?ur Sprachwis-senschaft, Bielefeld, Germany.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet ?
a Lexical-Semantic Net for German.
In Proceedings ofthe ACL Workshop on Automatic Information Extraction and Building Lexical Semantic Resources for NLPApplications, pages 9?15, Madrid, Spain.Claudia Kunze and Andreas Wagner.
1999.
Integrating GermaNet into EuroWordNet, a Multilingual Lexical-Semantic Database.
Sprache und Datenverarbeitung, 23(2):5?19.Lothar Lemnitzer and Claudia Kunze.
2007.
Computerlexikographie.
Gunter Narr Verlag, T?ubingen, Germany.John Lyons.
1968.
Introduction to Theoretical Linguistics.
Cambridge University Press.John Lyons.
1977.
Semantics.
Cambridge University Press.George A. Miller and Walter G. Charles.
1991.
Contextual Correlates of Semantic Similarity.
Language andCognitive Processes, 6(1):1?28.George A. Miller and Christiane Fellbaum.
1991.
Semantic Networks of English.
Cognition, 41:197?229.George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller.
1990.
Introduc-tion to Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?244.Carita Paradis and Caroline Willners.
2006.
Antonymy and Negation: The Boundedness Hypothesis.
Journal ofPragmatics, 38:1051?1080.Carita Paradis, Caroline Willners, and Steven Jones.
2009.
Good and Bad Opposites: Using Textual and Experi-mental Techniques to Measure Antonym Canonicity.
The Mental Lexicon, 4(3):380?429.Philip Resnik.
1995.
Using Information Content to Evaluate Semantic Similarity in a Taxonomy.
In Proceedingsof the 14th International Joint Conference on Artificial Intelligence, pages 448?453, San Francisco, CA.Herbert Rubenstein and John B. Goodenough.
1965.
Contextual Correlates of Synonymy.
Communications of theACM, 8(10):627?633.119
