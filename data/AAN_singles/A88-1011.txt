TRIPHONE ANALYSIS: A COMBINED METHOD FOR THE CORRECTIONOF ORTHOGRAPHICAL AND TYPOGRAPHICAL ERRORS.Brigitte van Berkelt and Koenraad De Smedt*1" Institute for Applied Computer Science (ITI), TNOSchoemakerstraat 97, 2628 VK Delft, The Netherlands*Language Technology Project, Psychology Dept., University of NijmegenMontessofilaan 3,6525 HR Nijmegen, The NetherlandsABSTRACTMost existing systems for the correction of wordlevel errors are oriented toward either typographical ororthographical errors.
Triphone analysis is a newcorrection strategy which combines phonemictranscription with trigram analysis.
It corrects bothkinds of errors (also in combination) and is superiorfor orthographical errors.1.
INTRODUCTION1.1 Error typesAny method for the correction of word level errorsin written texts must be carefully tuned.
On the onehand, the number of probable corrections should bemaximized; on the other hand, the number of unlikelycorrections should be minimized.
In order to achievethese goals, the characteristics of specific error typesmust be exploited as much as possible.
In this articlewe distinguish two major types of word level errors:orthographical errors and typographical errors.
Theyhave some clearly different characteristics.Orthographical errors are cognitive rrors consistingof the substitution of a deviant spelling for a correctone when the author either simply doesn't know thecorrect spelling for a correct spelling, forgot it, ormisconceived it.
An important characteristic oforthographical errors is that they generally result in astring which is phonologically identical or verysimilar to the correct string (e.g.
indicies instead ofindices1).
As a consequence, orthographical errors aredependent on the correspondence between spelling andpronunciation in a particular language.
Anothercharacteristic is that proper names, infrequent wordsand foreign words are particularly prone to ortho-graphical errors.1 All examples of errors given in this article wereactually found by the authors in texts written bynative speakers of the language in question.Typographical errors are motoric errors caused byhitting the wrong sequence of keys.
Hence their char-acteristics depend on the use of a particular keyboardrather than on a particular language.
Roughly eightypercent of these rrors can be described as single dele-tions (e.g.
continous) insertions (e.g.
explaination),substitutions (e.g.
anyboby) or transpositions (e.g.autoamtically) while the remaining twenty percent arecomplex errors (Peterson, 1980).
Some statisticalfacts about typographical errors are that word-initialerrors are rare, and doubling and undoubling (e.g.succeeed, discusion) are common.
In general,typographical errors do not lead to a string which ishomophonous with the correct string.Most of the correction methods currently in use inspelling checkers are biased toward the correction oftypographical errors.
We argue that this is not thefight thing to do.
Even if orthographical errors are notas frequent as typographical errors, they are not to beneglected for a number of good reasons.
First,orthographical errors are cognitive rrors, so they aremore persistent than typographical errors: proof-reading by the author himself will often fail to lead tocorrection.
Second, orthographical errors leave aworse impression on the reader than typographicalerrors.
Third, the use of orthographical correction forstandardization purposes (e.g.
consistent use of eitherBritish or American spelling) is an importantapplication appreciated by editors.
In this context, ourresearch pays special attention to Dutch, which has apreferred standard spelling but allows alternatives for agreat many foreign words, e.g.
architect (preferred) vs.architekt (allowed and commonly used in Dutch).Editors of books generally prefer a consistent use ofthe standard spelling.Finally, we would like to point out that methodsfor orthographical error correction can not only beapplied in text processing, but also in databaseretrieval.
In fact, our research was prompted partly bya project proposal for a user interface to an electronicencyclopedia.
One or our experiments involving alists of some five thousand worldwide geographical77names (mainly in Dutch spelling, e.g.
Noordkorea,Nieuwzeeland) has yielded very positive results.
Inthis context, the correction of orthographical errors isobviously more important han the correction oftypographical errors.1.2 Correction strategiesDaelemans, Bakker & Schotel (1984) distinguishbetween two basic kinds of strategies: tatistical ndlinguistic strategies.
Statistical strategies are based onstring comparison techniques, often augmented byspecific biases using statistical characteristics of someerror types, such as the fact that typographical errorsdo not frequently occur in the beginning of a word.Since these strategies do not exploit any specificlinguistic knowledge, they will generally work betterfor typographical errors than for orthographical errors.Linguistic strategies exploit the fact that orthog-raphical errors often result in homophonous strings(sound-alikes.
e.g.
consistancy and consistency).
Theynormally involve some kind of phonemic tran-scription.
Typographical errors which do not severelyaffect the pronunciation, such as doubling andundoubling, may be covered as well, but in general,linguistic strategies will do a poor job on all othertypographical errors.Because ach type of strategy isoriented toward oneclass of errors only, what is needed in our opinion isa combined method for orthographical nd typo-graphical errors.
Our research has explored oneapproach to this problem, namely, the combinationof a linguistic strategy with a statistical one.The remainder of this document is structured asfollows.
First we will discuss and criticize someexisting statistical and linguistic orrection methods.Then we will introduce triphone analysis.
Finally wewill report some results of an experiment with thismethod.2.
SOME EXISTING CORRECTIONMETHODS2.1 SpellIn Peterson's SPELL (Peterson, 1980), all probablecorrections are directly generated from an incorrectstring by considering the four major single errortypes.
The program first makes a list of all stringsfrom which the incorrect string can be derived by asingle deletion, insertion, substitution or trans-position.
This list is then matched against thedictionary: all strings occuring in both the list and thedictionary are considered probable corrections.Although the number of derivations is relativelysmall for short strings, they often lead to severalprobable corrections because many of them willactually occur in the dictionary.
For longer strings,many possible derivations are considered but most ofthose will be non-existent words.An advantage of SPELL with respect o all othermethods is that short words can be corrected equallywell as long ones.
A disadvantage is that all complexerrors and many orthographical errors fall outside thescope of SPELL.2.2 SpeedcopSPEEDCOP (Pollock & Zamora, 1984) uses aspecial technique for searching and comparing strings.In order to allow a certain measure of similarity,strings are converted into similarity keys which inten-tionally blur the characteristics of the original strings.The key of the misspelling is looked up in a list ofkeys for all dictionary entries.
The keys found in thelist within a certain distance of the target key areconsidered probable corrections.The blurring of the similarity keys must becarefully finetuned.
On the one hand, if too muchinformation is lost, too many words collate to thesame key.
If, on the other hand, too much infor-mation is retained, the key will be too sensitive toalterations by misspellings.
Two similarity keys areused in SPEEDCOP: a skeleton key and an omissionkey.
These keys are carefully designed in order topartially preserve the characters in a string and theirinterrelationships.
The information contained in thekey is ordered according to some characteristics oftypographical errors, e.g.
the fact that word-initialerrors are infrequent and that the sequence ofconsonants i  often undisturbed.The skeleton key contains the first letter of a string,then the remaining consonants and finally theremaining vowels (in order, without duplicates).
E.g.the skeleton key of information would be infrmtoa.The advantage of using this key is that some frequenterror types such as doubling and undoubling ofcharacters as well as transpositions involving oneconsonant and one vowel (except for an initial vowel)results in keys which are identical to the keys of theoriginal strings.The most vulnerable aspect of the skeleton key isits dependence on the first few consonants.
Thisturned out to be a problem, especially for omissions.Therefore, a second key, the omission key, wasdeveloped.
According to Pollock & Zamora (1984),consonants are omitted in the following declining or-78der of frequency: RSTNLCHDPGMFBYWVZXQKJ.
Theomission key is construed by first putting theconsonants in increasing order of omission frequencyand adding the vowels in order of occurrence.
E g. theomission key for information isfmntrioa.SPEEDCOP exploits the statistical properties oftypographical errors well, so it deals better withfrequent kinds of typographical errors than withinfrequent ones.
Because of this emphasis on typo-graphical errors, its performance on orthographicalerrors will be poor.
A specific disadvantage is itsdependence on the correctness of initial characters.Even when the omission key is used, word-initialerrors involving e.g.
j or k do not lead to anappropriate correction.2.3 Tr igram analysis: Fuzzie and AcuteTrigram analysis, as used in FUZZIE (De Heer,1982) and ACUTE (Angell, 1983), uses a moregeneral similarity measure.
The idea behind thismethod is that a word can be divided in a set of smalloverlapping substrings, called n-grams, which eachcarry some information about he identity of a word.When a misspelling has at least one undisturbed n-gram, the correct spelling spelling can still be traced.For natural languages, trigrams seem to have themost suitable length.
E.g., counting one surroundingspace, the word trigram is represented bythe trigrams#tr, tri, rig, igr, gra, ram, and am#.
B/grams are ingeneral too short to contain any useful identifyinginformation while tetragrams and larger n-grams arealready close to average word length.Correction using trigrams proceeds as follows.
Thetrigrams in a misspelling are looked up in an invertedfile consisting of all trigrams extracted from thedictionary.
With each trigram in this inverted file, alist of all words containing the trigram is associated.The words retrieved by means of the trigrams in themisspelling are probable corrections.The difference between FUZZIE and ACUTE ismainly in the criteria which are used to restrict henumber of possible corrections.
FUZZIE emphasizesfrequency as a selection criterium whereas ACUTEalso uses word length.
Low frequency trigrams areassumed to have a higher identifying value than highfrequency trigrams.
In FUZZIE, only the correctioncandidates associated with the n least frequenttrigrams, which are called selective trigrams, areconsidered.
ACUTE offers the choice between givinglow frequency trigrams a higher value and giving alltrigrams the same value.Taking trigram frequency into account hasadvantages a well as disadvantages.
On the one hand,there is a favorable distribution of trigrams in naturallanguages in the sense that there is a large number oflow frequency trigrams.
Also, the majority of wordscontain at least one selective trigram.
On the otherhand, typographical errors may yield very lowfrequency trigrams which inevitably get a highinformation value.In general, trigram analysis works better for longwords than for short ones, because a single error maydisturb all or virtually all trigrams in a short word.Some advantages of this method are that the errorposition is not important and that complex errors(e.g.
differenent),  and, to a certain extent,orthographical errors, can often be corrected.
Adisadvantage which is specific to this method is thattranspositions disturb more trigrams than other typesof errors and will thus be more difficult o correct.Trigram analysis lends itself well to extensions.
Byfirst selecting a large group of intermediate solutions,i.e.
all words which share at least one selectivetrigram with the misspelling, there is a lot of roomfor other factors to decide which words will eventuallybe chosen as probable corrections.
ACUTE forexample uses word length as an important criterium.2.4 The PF-474 chipThe PF-474 chip is a special-purpose VLSI circuitdesigned for very fast comparison of a string withevery entry in a dictionary (Yianilos, 1983).
Itconsists of a DMA controller for handling input froma data base (the dictionary), a proximity computer forcomputing the proximity (similarity) of two strings,and a ranker for ranking the 16 best solutionsaccording to their proximity values.The proximity value (PV) of two strings is afunction of the number of corresponding characters ofboth strings counted in forward and backward irec-tions.
It is basically expressed as the following ratio:2*(ABforwar d + ABbackward)PV-AAforw ard+AAbackward+BB forward +BBbackwardThis value can be influenced by manipulating theparameters weight, bias and compensation.
The para-meter weight makes some characters more importantthan others.
This parameter can e.g.
be manipulatedto reflect the fact that consonants carry moreinformation than vowels.
The parameter bias maycorrect he weight of a character in either word-initialor word-final position.
The parameter compensationdetermines the importance of an occurrence of acertain character within the word.
By using a high79compensation/weight ratio, for example, substitutionof characters will be less severe than omission.
Onemay force two characters tobe considered i entical byequalizing their compensation a d weight values.An advantage of the PF-474 chip, apart from itshigh speed, is that it is a general string comparisontechnique which is not biased to a particular kind oferrors.
By carefully manipulating the parameters,many orthographical errors may be corrected inaddition to typographical errors.2.5 Spell TherapistSPELL THERAPIST (Van Berkel, 1986) is alinguistic method for the correction of orthographicalerrors.
The misspelling is transcribed into aphonological code which is subsequently looked up ina dictionary consisting of phonological codes withassociated spellings.
The phonemic transcription,based on the GRAFON system (Daelemans, 1987), isperformed in three steps.
First the character string issplit into syllables.
Then a rule-based system con-verts each syllable into a phoneme string by means oftransliteration rules.
These syllabic phoneme stringsare further processed by phonological rules which takethe surrounding syllable context into account and arefinally concatenated.The transliteration rules in SPELL THERAPISTare grouped into three ordered lists: one for the onsetof the syllable, one for the nucleus, and one for thecoda.
Each rule consists of a graphemic selectionpattern, a graphemic onversion pattern, and aphoneme string.
The following rules are someexamples for Dutch onsets:( ( sc ( -  h i e y)) c /k/)(( qu ) qu (/k//kw/))(( a ( consonantp )) a /a / )The first rule indicates that in a graphemic patternconsisting of sc which is not followed by either h, i,e or y, the grapheme c is to be transcribed as thephoneme/k/.The transcription proceeds as follows.
The onset ofa syllable is matched with the graphemic selectionpatterns in the onset rule list.
The first rule whichmatches i  selected.
Then the characters which matchwith the conversion pattern are converted into thephoneme string.
The same procedure is then per-formed for the nucleus and coda of the syllable.The result of the transcription is then processed bymeans of phonological rules, which convert asequence of phonemes into another sequence ofphonemes in a certain phonological context on thelevel of the word.
An example for Dutch is the clusterreduction rule which deletes a/t / in certain consonantclusters:((( obstruent-p ) /t/ ( obstruent-p )) /t/ //)Such rules account for much of the power of SPELLTHERAPIST because many homophonousorthographic errors seem to be related to rules such asassimilation (e.g.
inplementation) or cluster eductionand degemination (e.g.
Dutch kunstof instead ofkunststo\]).This method is further enhanced by the followingrefinements.
First, a spelling may be transcribed intomore than one phonological code in order to accountfor possible pronunciation variants, especially thosedue to several possible stress patterns.
Second, thephonological code itself is designed to intentionallyblur some finer phonological distinctions.
E.g.
inorder to account for the fact that short vowels inunstressed syllables are prone to misspellings (e.g.optomization, incoded) such vowels are always re-duced to a schwa /3/.
As a result, misspellings ofthis type will collocate.It is clear that this method is suited only for errorswhich result in completely homophonous spellings(e.g.
issuing, inplementation).
A somewhat lessstringent similarity measure is created by using acoarse phonological coding, as mentioned above.Still, this method is not suitable for most typo-graphical errors.
Moreover, orthographical errorsinvolving 'hard' phonological differences (e.g.managable, recommand) fail to lead to correction.3.
AN INTEGRATED METHOD3.1 Combining methodsOf the methods described in the previous chapter,no single method sufficiently covers the wholespectrum of errors.
Because each method has itsstrengths and weaknesses, it is advantageous tocombine two methods which supplement each other.Because orthographical errors are the most difficultand persistent, we chose to take a linguistic methodas a starting point and added another method to coverits weaknesses.
SPELL THERAPIST has two weakpoints.
First, most typographical errors cannot becorrected.
Second, even though the phonologicalcodes are somewhat blurred, at least one possibletranscription of the misspelling must match exactlywith the phonological code of the intended word.A possible solution to both problems consists inapplying a general string comparison technique tophonological codes rather than spellings.
We decided80to combine SPELL THERAPIST with trigramanalysis by using sequences of three phonemesinstead of three characters.
We call such a sequence atriphone and the new strategy triphone analysis.3.2 Trlphone analysisTriphone analysis is a fast and efficient method forcorrecting orthographical nd typographical errors.When carefully implemented, it is not significantlyslower than trigram analysis.
The new method usesonly one dictionary in the form of an inverted file oftriphones.
Such a file is created by first computingphonological variants for each word, then splittingeach code into triphones, and finally addingbackpointers from each triphone in the file to eachspelling in which it occurs.
Also, a frequency value isassociated with each triphone.The way this inverted file is used during correctionis virtually the same as in FUZZIE, except hat f'trstall phonological variants of the misspelling have tobe generated.
The grapheme-to-phoneme conversion issimilar to that of SPELL THERAPIST, except hatthe phonological code is made even coarser by meansof various simplifications., e.g.
by removing thedistinction between tense and lax vowels and by notapplying certain phonological rules.The easiest way to select probable corrections froman inverted file is the method used by FUZZIE,because the similarity measure used by ACUTErequires that the number of triphones in the possiblecorrection be known in advance.
The problem withthis requirement is that phonological variants mayhave different string lengths and hence a varyingnumber of triphones.Using the FUZZIE method, each phonologicalvariant may select probable corrections by means ofthe following steps:1.
The phonological code is split into triphones.2.
Each triphone receives an information valuedepending on its frequency.
The sum of allvalues is I.3.
The selective triphones (those with a frequencybelow a certain preset value) are looked up inthe inverted file.4.
For all correction candidates found in this way,the similarity with the misspelling isdetermined by computing the sum of theinformation values of all triphones sharedbetween the candidate and the misspelling.If a certain candidate for correction is found by morethan one phonological variant, only the highestinformation value for that candidate is retained.
Aftercandidates have been selected for all variants, they areordered by their similarity values.
A possibleextension could be realized by also taking intoaccount he difference in string length between themisspelling and each candidate.Because processing time increases with eachphonological variant, it is important to reduce thenumber of variants as much as possible.
A consid-erable reduction is achieved by not generating aseparate variant for each possible stress pattern.
Theresulting inaccuracy is largely compensated by thefact that a perfect match is no longer equired by thenew method.Although this method yields very satisfactoryresults for both orthographical nd typographicalerrors and for combinations of them, it does havesome shortcomings for typographical errors in shortwords.
One problem is that certain deletions causetwo surrounding letters to be contracted into verydifferent phonemes.
Consider the deletion of the r invery: the pronunciation of the vowels in the resultingspelling, vey, changes ubstantially.
Counting onesurrounding space, the misspelling does not have asingle triphone in common with the original and so itcannot be corrected.A second problem is that a character (or charactercluster) leading to several possible phonemes carriesmore information than a character leading to a singlephoneme.
Consequently, an error affecting such acharacter disturbs more triphones.3.3 An experimentThe triphone analysis method presented here hasbeen implemented ona Symbolics LISP Machine andon an APOLLO workstation running Common LISP.After the programs had been completed, we decided totest the new method and compare its qualitativeperformance with that of the other methods.For a first, preliminary test we chose our domaincarefully.
The task domain had to be very error-prone,especially with respect o orthographical errors, sothat we could elicit errors from human subjects undercontrolled circumstances.
Given these requirements,we decided to choose Dutch surnames as the taskdomain.
In Dutch, many surnames have very differentspellings.
For example, there are 32 different nameswith the same pronunciation as Theyse, and even 124ways to spell Craeybeckx!
When such a name iswritten in a dictation task (e.g.
during a telephoneconversation) the chance of the right spelling beingchosen is quite small.81For our experiment, we recorded eviant spellingsof Dutch surnames generated by native speakers ofDutch in a writing-to-dictation task.
A series of 123Dutch surnames was randomly chosen from atelephone directory.
The names were dictated to 10subjects via a cassette tape recording.
A comparisonof the subjects' spelling with the intended spellingsshowed that on the average, subjects wrote down37.6% of the names in a deviant way.
The set of 463tokens of misspellings contained 188 different ypes,which were subsequently given as input to imple-mentations of each of the methods 2.
The dictionaryconsisted of 254 names (the 123 names mentionedabove plus I31 additional Dutch surnames randomlyselected from a different source).
The results of thecorrection are presented inTables 1 and 2.Table 1.
Results of the evaluation study.
Thenumbers refer to percentages of recognized(first, second or third choice) or notrecognized surnames (n = 188).Ist choice 2nd or 3rd not foundSPELL 58.5 1.1 40.4SPEEDCOP 53.7 1.1 45.2FUT_7_,IE 86.2 9.6 4.2ACUTE 89.9 6.9 3.2PF-474 84.0 14.9 1.1SPELLTHERAPIST 86.2 1.1 12.8TRIPHONEANALYSIS 94.1 5.9 0.0Table 2.
Results of the evaluation study.
Thenumbers refer to percentages of recognized(first, second or third choice) or notrecognized surnames multiplied by theirSPELLSPEEDCOPFUZZIEACUTEPF-474SPELLTHERAPISTTRIPHONEANALYSISfrequencies (n= 463).1st choice 2nd or 3rd not found63.7 2.2 34.155.7 2.2 42.187.7 8.4 3.990.3 6.7 3.085.5 14.1 0.490.5 2.2 7.395.2 4.8 0.02 The PF-474 method was simulated in softwareinstead of using the special hardware.3.4 D iscuss ionThe experiment was designed in order to minimizetypographical errors and to maximize orthographicalerrors.
Hence it is not surprising that SPELL andSPEEDCOP, which are very much dependent on thecharacteristics of typographical errors, do very poorly.What is perhaps most surprising is that SPELLTHERAPIST, a method primarily aiming at thecorrection of orthographical errors, shows worseresults than FUZZIE, ACUTE and the PF-474method, which are general string comparisonmethods.
The reason is that a certain number oforthographical errors turned out to involve realphonological differences.
These were probably causedby mishearings rather than misspellings.
Poor soundquality of the cassette recorder and dialectal differencesbetween speaker and hearer are possible causes.
Asexpected, triphone analysis yielded the best results:not a single misspelling could not be corrected, andonly about one out of twenty failed to be returned asthe most likely correction.4.
CONCLUSIONWe have demonstrated that an integration ofcomplementary correction methods performs betterthan single methods.
With respect o orthographicalerrors, triphone analysis performs better than eithergrapheme-to-phoneme conversion or trigram analysisalone.
Its capacity to correct ypographical errors isstill to be evaluated, but it is already clear that it willbe better than that of SPELL THERAPIST althoughsomewhat worse than trigram analysis in those caseswhere a typographical error drastically alters thepronunciation.
In practice, however, one always findsboth kinds of errors.
Therefore, it would be interes-ting to compare the various methods in actual use.Future research will go into a number of variantson the basic ideas presented here.
From a linguisticpoint of view, it is possible to make the pho-nological matching less stringent.
One way to do thisis to use a comparison at the level of phonologicalfeatures rather than phonemes.
However, greateremphasis on orthographical errors may deteriorateperformance on the correction of typing errors.An area of current research is the extension oftriphone analysis toward the correction of compounds.In languages like Dutch and German, new compoundssuch as taaltechnologie (language technology) arenormally written as one word.
Correction of errors insuch compounds i difficult because the constitutingwords should be corrected separately but there is no82,easy way to find the right segmentation.
We havedeveloped some heuristics to solve this problem.Of course, other combinations of methods arepossible.
One possibility which looks promising isto combine phonemic transcription with the PF-474chip.
Although triphone analysis is fairly fast, use ofthe PF-474 chip might further increase the speed.
Forthe correction of large quantities of word material,speed is an essential factor.
However, it should bekept in mind that there is a linear correlation betweenthe size of the dictionary and the required processingtime, and that the correlation curve is steeper for thePF-474 chip than for triphone analysis.
This meansthat triphone analysis will still be faster for very largedictionaries.With an eye to commercial pplications, TNO-ITIis extending the basic method with data compressiontechniques and an improved formalism for grapheme-to-phoneme conversion.ACKNOWLEDGEMENTSA prototype of triphone analysis was implementedat the Dept.
of Psychology of the University ofNijmegen under ESPRIT project OS-82.
Parts of theexperiment and the port to the APOLLO were carriedout at TNO-ITI, which also developed FUZZIE.We are indebted to Prof. Dr. Gerard Kempen(University of Nijmegen) and to Adriaan van Paassen(TNO-ITI) for their stimulation of the research and forthe helpful comments, and to Hil Weber for typingthe paper.REFERENCESAngell, R.C., Freund, G.E.
& Willett, P. (1983)Automatic spelling correction using a trigramsimilarity measure.
Information Processing &Management, 19,255-261.Barendregt, L.G., Benschop, C.A.
& De Heer, T.(1985) Subjective trial of the performance of theinformation trace method.
Information Processing& Management, 21,103-111.Daelemans, W. (1987) Studies in Languagetechnology.
Ph.D. Dissertation, Linguistics Dept.,University of Leuven.Daelemans, W., Bakker, D. & Schotel, H. (1984)Automatische detectie n correctie van spelfouten.lnformatie, 26, 949-1024.Damerau, F.J. (1964) A technique for computerdetection and correction of spelling errors.
CACM,7, 171-177.De Heer, T. (1982) The application of the concept ofhomeosemy to natural language informationretrieval.
Information Processing & Management,18, 229-236.Peterson, J.L.
(1980) Computer programs fordetecting and correcting spelling errors.
CACM,23,676-687.Pollock, J.J. & Zamora, A.
(1984) Automaticspelling correction in scientific and scholarly text.CACM, 27, 358-368.Van Berkel, B.
(1986) SPELTERAPUIT: eenalgoritme voor spel- en typefoutencorrectiegebaseerd op grafeem-foneemomzetting.
Master'sthesis, Dept.
of Psychology, University ofNijmegen.Yianilos, P.N.
(1983) A dedicated comparatormatches symbol strings fast and intelligently.Electronics, December 1983, 113-117.83
