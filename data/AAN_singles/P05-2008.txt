Proceedings of the ACL Student Research Workshop, pages 43?48,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsUsing Emoticons to reduce Dependency inMachine Learning Techniques for Sentiment ClassificationJonathon ReadDepartment of InformaticsUniversity of SussexUnited Kingdomj.l.read@sussex.ac.ukAbstractSentiment Classification seeks to identifya piece of text according to its author?sgeneral feeling toward their subject, be itpositive or negative.
Traditional machinelearning techniques have been applied tothis problem with reasonable success, butthey have been shown to work well onlywhen there is a good match between thetraining and test data with respect to topic.This paper demonstrates that match withrespect to domain and time is also impor-tant, and presents preliminary experimentswith training data labeled with emoticons,which has the potential of being indepen-dent of domain, topic and time.1 IntroductionRecent years have seen an increasing amount of re-search effort expended in the area of understandingsentiment in textual resources.
A sub-topic of thisresearch is that of Sentiment Classification.
Thatis, given a problem text, can computational meth-ods determine if the text is generally positive or gen-erally negative?
Several diverse applications existfor this potential technology, ranging from the auto-matic filtering of abusive messages (Spertus, 1997)to an in-depth analysis of market trends and con-sumer opinions (Dave et al, 2003).
This is a com-plex and challenging task for a computer to achieve?
consider the difficulties involved in instructing acomputer to recognise sarcasm, for example.Previous work has shown that traditional text clas-sification approaches can be quite effective whenapplied to the sentiment analysis problem.
Modelssuch as Na?
?ve Bayes (NB), Maximum Entropy (ME)and Support Vector Machines (SVM) can determinethe sentiment of texts.
Pang et al (2002) used a bag-of-features framework (based on unigrams and bi-grams) to train these models from a corpus of moviereviews labelled as positive or negative.
The best ac-curacy achieved was 82.9%, using an SVM trainedon unigram features.
A later study (Pang and Lee,2004) found that performance increased to 87.2%when considering only those portions of the textdeemed to be subjective.However, Engstro?m (2004) showed that the bag-of-features approach is topic-dependent.
A clas-sifier trained on movie reviews is unlikely to per-form as well on (for example) reviews of automo-biles.
Turney (2002) noted that the unigram unpre-dictable might have a positive sentiment in a moviereview (e.g.
unpredictable plot), but could be neg-ative in the review of an automobile (e.g.
unpre-dictable steering).
In this paper, we demonstratehow the models are also domain-dependent ?
howa classifier trained on product reviews is not effectivewhen evaluating the sentiment of newswire articles,for example.
Furthermore, we show how the modelsare temporally-dependent ?
how classifiers are bi-ased by the trends of sentiment apparent during thetime-period represented by the training data.We propose a novel source of training data basedon the language used in conjunction with emoticonsin Usenet newsgroups.
Training a classifier usingthis data provides a breadth of features that, while it43TestingFIN M&A MIXTrainingNB FIN 80.3 75.5 74.0M&A 77.5 75.3 75.8MIX 70.7 62.9 84.6SVM FIN 78.8 72.7 68.9M&A 74.5 75.5 75.5MIX 72.0 68.9 81.1Figure 1: Topic dependency in sentiment classification.
Ac-curacies, in percent.
Best performance on a test set for eachmodel is highlighted in bold.does not perform to the state-of-the-art, could func-tion independent of domain, topic and time.2 Dependencies in Sentiment Classification2.1 Experimental SetupIn this section, we describe experiments we havecarried out to determine the influence of domain,topic and time on machine learning based sentimentclassification.
The experiments use our own imple-mentation of a Na?
?ve Bayes classifier and Joachim?s(1999) SVM light implementation of a Support Vec-tor Machine classifier.
The models were trained us-ing unigram features, accounting for the presenceof feature types in a document, rather than the fre-quency, as Pang et al (2002) found that this is themost effective strategy for sentiment classification.When training and testing on the same set, themean accuracy is determined using three-fold cross-validation.
In each case, we use a paired-samplet-test over the set of test documents to determinewhether the results produced by one classifier arestatistically significantly better than those from an-other, at a confidence interval of at least 95%.2.2 Topic DependencyEngstro?m (2004) demonstrated how machine-learning techniques for sentiment classification canbe topic dependent.
However, that study focusedon a three-way classification (positive, negative andneutral).
In this paper, for uniformity across differ-ent data sets, we focus on only positive and negativesentiment.
This experiment also provides an oppor-tunity to evaluate the Na?
?ve Bayes classifier as theprevious work used SVMs.We use subsets of a Newswire dataset (kindly pro-TestingNewswire Polarity 1.0TrainingNB Newswire 78.2 57.6Polarity 1.0 53.2 78.9SVM Newswire 78.2 63.2Polarity 1.0 63.6 81.5Figure 2: Domain dependency in sentiment classification.Accuracies, in percent.
Best performance on a test set for eachmodel is highlighted in bold.vided by Roy Lipski of Infonic Ltd.) that relate tothe topics of Finance (FIN), Mergers and Aquisi-tions (M&A) and a mixture of both topics (MIX).Each subset contains further subsets of articles ofpositive and negative sentiment (selected by inde-pendent trained annotators), each containing 100stories.
We trained a model on a dataset relating toone topic and tested that model using the other top-ics.
Figure 1 shows the results of this experiment.The tendency seems to be that performance in agiven topic is best if the training data is from thesame topic.
For example, the Finance-trained SVMclassifier achieved an accuracy of 78.8% against ar-ticles from Finance, but only 72.7% when predictingthe sentiment of articles from M&A.
However, sta-tistical testing showed that the results are not signifi-cantly different when training on one topic and test-ing on another.
It is interesting to note, though, thatproviding a dataset of mixed topics (the sub-corpusMIX) does not necessarily reduce topic dependency.Indeed, the performance of the classifiers suffers agreat deal when training on mixed data (confidenceinterval 95%).2.3 Domain DependencyWe conducted an experiment to compare the ac-curacy when training a classifier on one domain(newswire articles or movie reviews from the Polar-ity 1.0 dataset used by Pang et al (2002)) and testingon the other domain.
In Figure 2, we see a clear in-dication that models trained on one domain do notperform as well on another domain.
All differencesare significant at a confidence interval of 99.9%.2.4 Temporal DependencyTo investigate the effect of time on sentiment clas-sification, we constructed a new set of movie re-44TestingPolarity 1.0 Polarity 2004TrainingNB Polarity 1.0 78.9 71.8Polarity 2004 63.2 76.5SVM Polarity 1.0 81.5 77.5Polarity 2004 76.5 80.8Figure 3: Temporal dependency in sentiment classification.Accuracies, in percent.
Best performance on a test set for eachmodel is highlighted in bold.views, following the same approach used by Panget al (2002) when they created the Polarity 1.0dataset.
The data source was the Internet Movie Re-view Database archive1 of movie reviews.
The re-views were categorised as positive or negative usingautomatically extracted ratings.
A review was ig-nored if it was not written in 2003 or 2004 (ensuringthat the review was written after any in the Polar-ity 1.0 dataset).
This procedure yielded a corpus of716 negative and 2,669 positive reviews.
To createthe Polarity 20042 dataset we randomly selected 700negative reviews and 700 positive reviews, matchingthe size and distribution of the Polarity 1.0 dataset.The next experiment evaluated the performanceof the models first against movie reviews from thesame time-period as the training set and then againstreviews from the other time-period.
Figure 3 showsthe resulting accuracies.These results show that while the models performwell on reviews from the same time-period as thetraining set, they are not so effective on reviews fromother time-periods (confidence interval 95%).
It isalso apparent that the Polarity 2004 dataset performsworse than the Polarity 1.0 dataset (confidence inter-val 99.9%).
A possible reason for this is that Polarity2004 data is from a much smaller time-period thanthat represented by Polarity 1.0.3 Sentiment Classification usingEmoticonsOne way of overcoming the domain, topic and timeproblems we have demonstrated above would be tofind a source of much larger and diverse amountsof general text, annotated for sentiment.
Users of1http://reviews.imdb.com/Reviews/2The new datasets described in this paper are available athttp://www.sussex.ac.uk/Users/jlr24/dataGlyph Meaning Frequency:-) smile 3.8739;-) wink 2.4350:-( frown 0.4961:-D wide grin 0.1838:-P tongue sticking out 0.1357:-O surprise 0.0171:-| disappointed 0.0146:?
( crying 0.0093:-S confused 0.0075:-@ angry 0.0038:-$ embarrassed 0.0007Figure 4: Examples of emoticons and the frequency of usageobserved in Usenet articles, in percent.
For example, 2.435% ofdownloaded Usenet articles contained a wink emoticon.electronic methods of communication have devel-oped visual cues that are associated with emotionalstates in an attempt to state the emotion that their textrepresents.
These have become known as smileysor emoticons and are glyphs constructed using thecharacters available on a standard keyboard, repre-senting a facial expression of emotion ?
see Figure4 for some examples.
When the author of an elec-tronic communication uses an emoticon, they are ef-fectively marking up their own text with an emo-tional state.
This marked-up text can be used to traina sentiment classifier if we assume that a smile in-dicates generally positive text and a frown indicatesgenerally negative text.3.1 Emoticon Corpus ConstructionWe collected a corpus of text marked-up with emoti-cons by downloading Usenet newsgroups and savingan article if it contained an emoticon listed in Figure4.
This process resulted in 766,730 articles beingstored, from 10,682,455 messages in 49,759 news-groups inspected.
Figure 4 also lists the percentageof documents containing each emoticon type, as ob-served in the Usenet newsgroups.We automatically extracted the paragraph(s) con-taining the emoticon of interest (a smile or a frown)from each message and removed any superfluousformatting characters (such as those used to indi-cate article quotations in message threads).
In orderto prevent quoted text from being considered morethan once, any paragraph that began with exactly thesame thirty characters as a previously observed para-graph was disregarded.
Finally, we used the classi-fier developed by Cavnar and Trenkle (1994) to filter45Finance M&A MixedNB 46.0 ?
2.1 55.8 ?
3.8 49.0 ?
1.6SVM 50.3 ?
1.7 57.8 ?
6.5 55.5 ?
2.7Figure 5: Performance of Emoticon-trained classifier acrosstopics.
Mean accuracies with standard deviation, in percent.Newswire Polarity 1.0NB 50.3 ?
2.2 56.8 ?
1.8SVM 54.4 ?
2.8 54.0 ?
0.8Figure 6: Performance of Emoticon-trained classifiers acrossdomains.
Mean accuracies with standard deviation, in percent.out any paragraphs of non-English text.
This pro-cess yielded a corpus of 13,000 article extracts con-taining frown emoticons.
As investigating skew be-tween positive and negative distributions is outsidethe scope of this work, we also extracted 13,000 arti-cle extracts containing smile emoticons.
The datasetis referred to throughout this paper as Emoticons andcontains 748,685 words.3.2 Emoticon-trained Sentiment ClassificationThis section describes how the Emoticons corpus3was optimised for use as sentiment classificationtraining data.
2,000 articles containing smiles and2,000 articles containing frowns were held-out asoptimising test data.
We took increasing amountsof articles from the remaining dataset (from 2,000to 22,000 in increments of 1,000, an equal numberbeing taken from the positive and negative sets) asoptimising training data.
For each set of trainingdata we extracted a context of an increasing num-ber of tokens (from 10 to 1,000 in increments of 10)both before and in a window4 around the smile orfrown emoticon.
The models were trained using thisextracted context and tested on the held-out dataset.The optimisation process revealed that the best-performing settings for the Na?
?ve Bayes classifierwas a window context of 130 tokens taken from thelargest training set of 22,000 articles.
Similarly, thebest performance for the SVM classifier was foundusing a window context of 150 tokens taken from3Note that in these experiments the emoticons are used asanchors from which context is extracted, but are removed fromtexts before they are used as training or test data.4Context taken after an emoticon was also investigated, butwas found to be inferior.
This is because approximately two-thirds of article extracts end in an emoticon so when using after-context few features are extracted.Polarity 1.0 Polarity 2004NB 56.8 ?
1.8 56.7 ?
2.2SVM 54.0 ?
0.8 57.8 ?
1.8Figure 7: Performance of Emoticon-trained classifier acrosstime-periods.
Mean accuracies with standard deviation, in per-cent.20,000 articles.The classifiers?
performance in predicting thesmiles and frowns of article extracts was verified us-ing these optimised parameters and ten-fold cross-validation.
The mean accuracy of the Na?
?ve Bayesclassifier was 61.5%, while the SVM classifier was70.1%.Using these same classifiers to predict the senti-ment of movie reviews in Polarity 1.0 resulted in ac-curacies of 59.1% (Na?
?ve Bayes) and 52.1% (SVM).We repeated the optimisation process using aheld-out set of 100 positive and 100 negative re-views from the Polarity 1.0 dataset, as it is possi-ble that this test needs different parameter settings.This revealed an optimum context of a window of50 tokens taken from a training set of 21,000 arti-cles for the Na?
?ve Bayes classifier.
Interestingly, theoptimum context for the SVM classifier appeared tobe a window of only 20 tokens taken from a mere2,000 training examples.
This is clearly an anomaly,as these parameters resulted in an accuracy of 48.9%when testing against the reserved reviews of Polarity1.0.
We attribute this to the presence of noise, bothin the training set and in the held-out set, and dis-cuss this below (Section 4.2).
The second-best pa-rameters according to the optimisation process werea context of 510 tokens taken before an emoticon,from a training set of 20,000 examples.We used these optimised parameters to evaluatethe sentiments of texts in the test sets used to eval-uate dependency in Section 2.
Figures 5, 6 and 7show the final, optimised results across topics, do-mains and time-periods respectively.
These tablesreport the average accuracies over three folds, withthe standard deviation as a measure of error.4 DiscussionThe emoticon-trained classifiers perform well (up to70% accuracy) when predicting the sentiment of ar-ticle extracts from the Emoticons dataset, which isencouraging when one considers the high level of46Training Testing CoveragePolarity 1.0 Polarity 1.0 69.8(three-fold cross-validation)Emoticons FIN 54.9M&A 58.1MIX 60.2Newswire 46.1Polarity 1.0 41.1Polarity 2004 42.6Figure 8: Coverage of classifiers, in percent.noise that is likely to be present in the dataset.However, they perform only a little better than onewould expect by chance when classifying movie re-views, and are not effective in predicting the senti-ment of newswire articles.
This is perhaps due to thenature of the datasets ?
one would expect languageto be informal in movie reviews, and even more soin Usenet articles.
In contrast, language in newswirearticles is far more formal.
We might therefore infera further type of dependence in sentiment classifica-tion, that of language-style dependency.Also, note that neither machine-learning modelconsistently out-performs the other.
We speculatethat this, and the generally mediocre performance ofthe classifiers, is due (at least) to two factors; poorcoverage of the features found in the test domainsand a high level of noise found in Usenet article ex-tracts.
We investigate these factors below.4.1 CoverageFigure 8 shows the coverage of the Emoticon-trainedclassifiers on the various test sets.
In these exper-iments, we are interested in the coverage in termsof unique token types rather than the frequency offeatures, as this more closely reflects the training ofthe models (see Section 2.1).
The mean coverageof the Polarity 1.0 dataset during three-fold cross-validation is also listed as an example of the cov-erage one would expect from a better-performingsentiment classifier.
The Emoticon-trained classifierhas much worse coverage in the test sets.We analysed the change in coverage of theEmoticon-trained classifiers on the Polarity 1.0dataset.
We found that the coverage continued to im-prove as more training data was provided; the cov-erage of unique token types was improving by about0.6% per 1,000 training examples when the Emoti-48505254565860300060009000120001500018000Training Size  100 200300 400500 600700 800900 1000Context Size48505254565860Accuracy (%)Figure 9: Change in Performance of the SVM Classifier onheld-out reviews from Polarity 1.0, varying training set size andwindow context size.
The datapoints represent 2,200 experi-ments in total.cons dataset was exhausted.It appears possible that more training data will im-prove the performance of the Emoticon-trained clas-sifiers by increasing the coverage.
Potential sourcesfor this include online bulletin boards, chat forums,and further newsgroup data from Usenet and GoogleGroups5.
Future work will utilise these sources tocollect more examples of emoticon use and analyseany improvement in coverage and accuracy.4.2 Noise in Usenet Article ExtractsThe article extracts collected in the Emoticonsdataset may be noisy with respect to sentiment.
TheSVM classifier seems particularly affected by thisnoise.
Figure 9 depicts the change in performanceof the SVM classifier when varying the training setsize and size of context extracted.
There are signif-icant spikes apparent for the training sizes of 2,000,3,000 and 6,000 article extracts (as noted in Section3.2), where the accuracy suddenly increases for thetraining set size, then quickly decreases for the nextset size.
This implies that the classifier is discover-ing features that are useful in classifying the held-out set, but the addition of more, noisy, texts soonmakes the information redundant.Some examples of noise taken from the Emoti-cons dataset are: mixed sentiment, e.g.5http://groups.google.com47?Sorry about venting my frustration here but Ijust lost it.
:-( Happy thanks giving everybody:-)?,sarcasm, e.g.
?Thank you so much, that?s really encouraging:-(?,and spelling mistakes, e.g.
?The movies where for me a major desapoint-ment :-(?.In future work we will investigate ways to removenoisy data from the Emoticons dataset.5 Conclusions and Future WorkThis paper has demonstrated that dependency in sen-timent classification can take the form of domain,topic, temporal and language style.
One might sup-pose that dependency is occurring because classi-fiers are learning the semantic sentiment of textsrather than the general sentiment of language used.That is, the classifiers could be learning authors?sentiment towards named entities (e.g.
actors, direc-tors, companies, etc.).
However, this does not seemto be the case.
In a small experiment, we part-of-speech tagged the Polarity 2004 dataset and auto-matically replaced proper nouns with placeholders.Retraining on this modified text did not significantlyaffect performance.But it may be that something more subtle is hap-pening.
Possibly, the classifiers are learning thewords associated with the semantic sentiment of en-tities.
For example, suppose that there has been awell-received movie about mountaineering.
Duringthis movie, there is a particularly stirring scene in-volving an ice-axe and most of the reviewers men-tion this scene.
During training, the word ?ice-axe?would become associated with a positive sentiment,whereas one would suppose that this word does notin general express any kind of sentiment.In future work we will perform further tests to de-termine the nature of dependency in machine learn-ing techniques for sentiment classification.
One wayof evaluating the ?ice-axe?
effect could be to build a?pseudo-ontology?
of the movie reviews ?
a mapof the sentiment-bearing relations that would enablethe analysis of the dependencies created by the train-ing process.
Other extensions of this work are tocollect more text marked-up with emoticons, and toexperiment with techniques to automatically removenoisy examples from the training data.AcknowledgementsThis research was funded by a UK EPSRC stu-dentship.
I am very grateful to Thorsten Joachims,Roy Lipski, Bo Pang and John Trenkle for kindlymaking their data or software available, and to theanonymous reviewers for their constructive com-ments.
Thanks also to Nick Jacobi for his discus-sion of the ?ice-axe?
effect.
Special thanks to my su-pervisor, John Carroll, for his continued advice andencouragement.ReferencesW.
B. Cavnar and J. M. Trenkle.
1994.
N-Gram-BasedText Categorization.
In Proceedings of the Third An-nual Symposium on Document Analysis and Informa-tion Retrieval, pages 161?175, Las Vegas, Nevada.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the Peanut Gallery: Opinion Extrac-tion and Semantic Classification of Product Reviews.In Proceedings of the International World Wide WebConference, Budapest, Hungary.Charlotta Engstro?m.
2004.
Topic Dependence in Sen-timent Classification.
Master?s thesis, University ofCambridge, July.T.
Joachims.
1999.
Making large-Scale SVM LearningPractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
MIT Press.Bo Pang and Lillian Lee.
2004.
A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Summa-rization Based on Minimum Cuts.
In Proceedings ofthe 42nd Annual Meeting of the Association for Com-putational Linguistics, Barcelona, Spain.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing, University of Pennsylvania.Ellen Spertus.
1997.
Smokey: Automatic Recognitionof Hostile Messages.
In Proceedings of the InnovativeApplications of Artificial Intelligence.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), pages 417?424, Philadelphia, Pennsyl-vania.48
