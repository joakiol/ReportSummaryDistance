Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383?386,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsMSS: Investigating the Effectiveness of Domain Combinations andTopic Features for Word Sense DisambiguationSanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki ShindoNTT Communication Science Laboratories{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jpAbstractWe participated in the SemEval-2010Japanese Word Sense Disambiguation(WSD) task (Task 16) and focused onthe following: (1) investigating domaindifferences, (2) incorporating topic fea-tures, and (3) predicting new unknownsenses.
We experimented with SupportVector Machines (SVM) and MaximumEntropy (MEM) classifiers.
We achieved80.1% accuracy in our experiments.1 IntroductionWe participated in the SemEval-2010 JapaneseWord Sense Disambiguation (WSD) task (Task 16(Okumura et al, 2010)), which has two new char-acteristics: (1) Both training and test data across3 or 4 domains.
The training data include booksor magazines (called PB), newspaper articles (PN),and white papers (OW).
The test data also includedocuments from a Q&A site on the WWW (OC);(2) Test data include new senses (called X) that arenot defined in dictionary.There is much previous research on WSD.
Inthe case of Japanese, unsupervised approachessuch as extended Lesk have performed well (Bald-win et al, 2010), although they are outperformedby supervised approaches (Tanaka et al, 2007;Murata et al, 2003).
Therefore, we selected a su-pervised approach and constructed Support VectorMachines (SVM) and Maximum Entropy (MEM)classifiers using common features and topic fea-tures.
We performed extensive experiments to in-vestigate the best combinations of domains fortraining.We describe the data in Section 2, and our sys-tem in Section 3.
Then in Section 4, we show theresults and provide some discussion.2 Data Description2.1 Given DataWe show an example of Iwanami Kokugo Jiten(Nishio et al, 1994), which is a dictionary used asa sense inventory.
As shown in Figure 1, each en-try has POS information and definition sentencesincluding example sentences.We show an example of the given training datain (1).
The given data are morphologically ana-lyzed and partly tagged with Iwanami?s sense IDs,such as '37713-0-0-1-1( in (1).
(1) <mor pos='??-?
}( rd='??
( bfm='??
( sense= '37713-0-0-1-1( >1<</mor>This task includes 50 target words that weresplit into 219 senses in Iwanami; among them, 143senses including two Xs that were not defined inIwanami, appear in the training data.
In the testdata, 150 senses including eight Xs appear.
Thetraining and test data share 135 senses includingtwo Xs; that is, 15 senses including six Xs in thetest data are unseen in the training data.2.2 Data Pre-processingWe performed two preliminary pre-processingsteps.
First, we restored the base forms becausethe given training and test data have no informa-tion about the base forms.
(1) shows an exampleof the original morphological data, and then weadded the base form (lemma), as shown in (2).
(2) <mor pos=' ?
?-? }
( rd=' ?
?
(bfm=' ?
?
( sense='37713-0-0-1-1(lemma='1d(>1<</mor>Secondly, we extracted example sentences fromIwanami, which is used as a sense inventory.
Tocompensate for the lack of training data, we an-alyzed examples with a morphological analyzer,Mecab1 UniDic version, because the training andtest data were tagged with POS based on UniDic.1http://mecab.sourceforge.net/383????
?HEADWORD Ad91d[ddNd:take (?
; Transitive Verb)37713-0-0-1-0[<1> ?
?<8[GCBk3D?= to get something left into one?s hand]37713-0-0-1-1[<y> 3??=53k-<??
(6take and hold by hand.
'to lead someone by the hand(]????
?Figure 1: Simplified Entry for Iwanami Kokugo Jiten: Ad takeFor example, from the entry for Ad take, asshown in Figure 1, we extracted an example sen-tence and morphologically analyzed it, as shownin (3)2, for the second sense, 37713-0-0-1-1.
In(3), the underlined part is the headword and istagged with 37713-0-0-1-1.
(3) 3handkACC1<take?and?(lead?
(I) take someone?s hand and lead him/her?3 System Description3.1 FeaturesIn this section, we describe the features we gener-ated.3.1.1 Baseline FeaturesFor each target word w, we used the surface form,the base form, the POS tag, and the top POS cat-egories, such as nouns, verbs, and adjectives ofw.
Here the target is the ith word, so we alsoused the same information of i?
2, i?
1, i+ 1, andi+2th words.
We used bigrams, trigrams, and skip-bigrams back and forth within three words.
We re-fer to the model that uses these baseline featuresas bl.3.1.2 Bag-of-WordsFor each target word w, we got all base forms ofthe content words within the same document orwithin the same article for newspapers (PN).
Werefer to the model that uses these baseline featuresas bow.3.1.3 Topic FeaturesIn the SemEval-2007 English WSD tasks, a sys-tem incorporating topic features achieved thehighest accuracy (Cai et al, 2007).
Inspired by(Cai et al, 2007), we also used topic features.Their approach uses Bayesian topic models (La-tent Dirichlet Allocation: LDA) to infer topics inan unsupervised fashion.
Then the inferred topics2We use ACC as an abbreviation of accusativepostposition.are added as features to reduce the sparsity prob-lem with word-only features.In our proposed approach, we use the inferredtopics to find 'related?
( words and directly addthese word counts to the bag-of-words representa-tion.We applied gibbslda++3 to the training and testdata to obtain multiple topic classification per doc-ument or article for newspapers (PN).
We used thedocument or article topics for newspapers (PN) in-cluding the target word.
We refer to the modelthat uses these topic features as tpX, where X isthe number of topics and tpdistX with the topicsweighted by distributions.
In particular, the topicdistribution of each document/article is inferred bythe LDA topic model using standard Gibbs sam-pling.We also add the most typical words in the topicas a bag-of-words.
For example, one topic mightinclude ?
city, ??
Tokyo, ?
train line, ?
wardand so on.
A second topic might include ??
dis-section, ?
after, ??
medicine, U grave and soon.
If a document is inferred to contain the firsttopic, then the words (?
city, ??
Tokyo, ?
trainline, ...) are added to the bag-of-words feature.
Werefer to these features as twdY, including the mosttypical Y words as bag-of-words.3.2 Investigation between DomainsIn preliminary experiments, we used both SVM4and MEM (Nigam et al, 1999), with optimizationmethod L-BFGS (Liu and Nocedal, 1989) to trainthe WSD model.First, we investigated the effect between do-mains (PN, PB, and OW).
For training data, we se-lected words that occur in more than 50 sentences,separated the training data by domain, and testeddifferent domain combinations.Table 1 shows the SVM results of the domaincombinations.
For Table 1, we did a 5-fold crossvalidation for the self domain and for comparison3http://gibbslda.sourceforge.net/4http://www.csie.ntu.edu.tw/?cjlin/libsvm/384Table 1: Investigation of Domain Combinationson Training data (features: bl + bow, SVM)Target Words 77, No.
of Instances > 50Domain Acc.
(%) Diff.
CommentPN 78.7 - 63 words,PN +OW 79.25 0.55 1094 instancesPN +PB 79.43 0.73PN +ALL 79.34 0.64PB 79.29 - 75 words,PB +PN 78.85 -0.45 2463 instancesPB +OW 78.56 -0.73PB +ALL 78.4 -0.89OW 87.91 - 42 words,OW +PN 89.05 1.14 703 instancesOW +PB 88.34 0.43OW +ALL 89.05 1.14with the results after adding the other domain data.In Table 1, Diff.
shows the differences to the selfdomain.As shown in Table 1, for PN and OW, using otherdomains improved the results, but for PB, other do-mains degraded the results.
So we decided to se-lect the domains for each target word.In the formal run, for each pair of domain andtarget words, we selected the combination of do-main and dictionary examples that got the bestcross-validation result in the training data.
Notethat in the case of no training data for the test datadomain, for example, since no OCs have trainingdata, we used all training data and dictionary ex-amples.We show the number of selected domain combi-nations for each target domain in Table 2.
Becausethe distribution of target words is very unbalancedin domains, not all types of target words appear inevery domain, as shown in Table 2.3.3 Method for Predicting New SensesWe also tried to predict new senses (X) that didn?tappear in the training data by calculating the en-tropy for each target given in the MEM.
We as-sumed that high entropy (when the probabilitiesof classes are uniformly dispersed) was indicativeof X; i.e., if [entropy > threshold] => predict X;else => predict with MEM?s output sense tag.Note that we used the words that were taggedwith Xs in the training data, except for the targetwords.
We compared the entropies of X and notX of the words and heuristically tuned the thresh-old based on the differences among entropies.
Ourthree official submissions correspond to differentthresholds.Table 2: Used Domain CombinationsUsed MEM SVMDomain No.
(%) No.
(%)Target: PB (48 types of target words)ALL +EX 26 54.2 23 47.9ALL 4 8.3 6 12.5PB 11 22.9 8 16.7PB +EX 1 2.1 1 2.1PB +OW 1 2.1 3 6.3PB +PN 5 10.4 7 14.6Target: PN (46 types of target words)ALL +EX 30 65.2 30 65.2ALL 4 8.7 4 8.7PN 4 8.7 1 2.2PN +EX 0 0 1 2.2PN +OW 2 4.3 2 4.3PN +PB 6 13 8 17.4Target: OW (16 types of target words)ALL +EX 5 31.3 5 31.3ALL 2 12.5 1 6.3OW 6 37.5 3 18.8OW +PB 3 18.8 3 18.8OW +PN 0 0 4 25.0Target: OC (46 types of target words)ALL +EX 46 100 46 1004 Results and DiscussionsOur cross-validation experiments on the trainingset showed that selecting data by domain combi-nations works well, but unfortunately this failedto achieve optimal results on the formal run.
Inthis section, we show the results using all of thetraining data with no domain selections (also afterfixing some bugs).Table 3 shows the results for the combinationof features on the test data.
MEM greatly outper-formed SVM.
Its effective features are also quitedifferent.
In the case of MEM, baseline features(bl) almost gave the best result, and the topic fea-tures improved the accuracy, especially when di-vided into 200 topics.
But for SVM, the topicfeatures are not so effective, and the bag-of-wordsfeatures improved accuracy.For MEM with bl +tp200, which produced thebest result, the following are the best words: ?outside (accuracy is 100%), C^ economy (98%),?
!d think (98%), d& big (98%), and %Zculture (98%).
On the other hand, the followingare the worst words: 1d take (36%), ?
good(48%), ?+d raise (48%), w2 put out (50%),and ?= stand up (54%).In Table 4, we show the results for each POS (bl+tp200, MEM).
The results for the verbs are com-parably lower than the others.
In future work, wewill consider adding syntactic features that mayimprove the results.385Table 3: Comparisons among Features and Test dataTYPE Precision (%)MEM SVM ExplainBase Line 68.96 68.96 Most Frequent Sensebl 79.3 69.6 Base Line Featuresbl +bow 77.0 70.8 + Bag-of-Words (BOW)bl +bow +tp100 76.4 70.7 +BOW + Topics (100)bl +bow +tp200 77.0 70.7 +BOW + Topics (200)bl +bow +tp300 77.4 70.7 +BOW + Topics (300)bl +bow +tp400 76.8 70.7 +BOW + Topics (400)bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distributionbl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic wordsbl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic wordsbl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic wordswithout bowbl +tp100 79.3 69.6 + Topics (100)bl +tp200 80.1 69.6 + Topics (200)bl +tp300 79.6 69.6 + Topics (300)bl +tp400 79.6 69.6 + Topics (400)bl +tpdist100 79.3 69.6 + Topics (100)*distributionbl +tpdist200 79.3 69.6 + Topics (200)*distributionbl +tpdist300 79.3 69.6 + Topics (300)*distributionbl +tp200 +twd100 74.6 69.6 + Topics (200) with 100 topic wordsbl +tp300 +twd10 74.4 69.4 + Topics (300) with 10 topic wordsbl +tp300 +twd20 75.2 69.3 + Topics (300) with 20 topic wordsbl +tp300 +twd50 74.8 69.2 + Topics (300) with 50 topic wordsbl +tp300 +twd200 74.6 69.6 + Topics (300) with 200 topic wordsbl +tp300 +twd300 75.0 69.6 + Topics (300) with 300 topic wordsbl +tp400 +twd100 74.1 69.6 + Topics (400) with 100 topic wordsbl+tpdist100 +twd20 79.3 69.6 + Topics (100)*distribution with 20 topic wordsbl+tpdist200 +twd20 79.3 69.6 + Topics (200)*distribution with 20 topic wordsbl+tpdist400 +twd20 79.3 69.6 + Topics (400)*distribution with 20 topic wordsTable 4: Results for each POS (bl +tp200, MEM)POS No.
of Types Acc.
(%)Nouns 22 85.5Adjectives 5 79.2Transitive Verbs 15 76.9Intransitive Verbs 8 71.8Total 50 80.1In the formal run, we selected training datafor each pair of domain and target words andused entropy to predict new unknown senses.
Al-though these two methods worked well in ourcross-validation experiments, they did not performwell for the test data, probably due to domain mis-match.Finally, we also experimented with SVM andMEM, and MEM gave better results.ReferencesTimothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-jita, David Martinez, and Takaaki Tanaka.
2010.
A Re-examination of MRD-based Word Sense Disambiguation.Transactions on Asian Language Information Process, As-sociation for Computing Machinery (ACM), 9(4):1?21.Jun Fu Cai, Wee Sun Lee, and YW Teh.
2007.
Improv-ing Word Sense Disambiguation using Topic Features.
InProceedings of EMNLP-CoNLL-2007, pp.
1015?1023.Dong C. Liu and Jorge Nocedal.
1989.
On the Limited Mem-ory BFGS Method for Large Scale Optimization.
Math.Programming, 45(3, (Ser.
B)):503?528.Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, QingMa, and Hitoshi Isahara.
2003.
CRL at Japanesedictionary-based task of SENSEVAL-2.
Journal of Nat-ural Language Processing, 10(3):115?143.
(in Japanese).Kamal Nigam, John Lafferty, and Andrew McCallum.
1999.Using Maximum Entropy for Text Classification.
InIJCAI-99 Workshop on Machine Learning for InformationFiltering, pp.
61?67.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.1994.
Iwanami Kokugo Jiten Dai Go Han [IwanamiJapanese Dictionary Edition 5].
Iwanami Shoten, Tokyo.
(in Japanese).Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, andHikaru Yokono.
2010.
SemEval-2010 Task: JapaneseWSD.
In SemEval-2: Evaluation Exercises on SemanticEvaluation.Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-jita, and Chikara Hashimoto.
2007.
Word Sense Disam-biguation Incorporating Lexical and Structural SemanticInformation.
In Proceedings of EMNLP-CoNLL-2007, pp.477?485.386
