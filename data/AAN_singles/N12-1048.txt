2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 437?445,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsReal-time Incremental Speech-to-Speech Translation of DialogsSrinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash KolanLadan Golipour, Aura JimenezAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932, USAvkumar,srini,pkolan,ladan,aura@research.att.comAbstractIn a conventional telephone conversation be-tween two speakers of the same language, theinteraction is real-time and the speakers pro-cess the information stream incrementally.
Inthis work, we address the problem of incre-mental speech-to-speech translation (S2S) thatenables cross-lingual communication betweentwo remote participants over a telephone.
Weinvestigate the problem in a novel real-timeSession Initiation Protocol (SIP) based S2Sframework.
The speech translation is per-formed incrementally based on generation ofpartial hypotheses from speech recognition.We describe the statistical models comprisingthe S2S system and the SIP architecture forenabling real-time two-way cross-lingual dia-log.
We present dialog experiments performedin this framework and study the tradeoff in ac-curacy versus latency in incremental speechtranslation.
Experimental results demonstratethat high quality translations can be generatedwith the incremental approach with approxi-mately half the latency associated with non-incremental approach.1 IntroductionIn recent years, speech-to-speech translation (S2S)technology has played an increasingly importantrole in narrowing the language barrier in cross-lingual interpersonal communication.
The improve-ments in automatic speech recognition (ASR), statis-tical machine translation (MT), and, text-to-speechsynthesis (TTS) technology has facilitated the serialbinding of these individual components to achieveS2S translation of acceptable quality.Prior work on S2S translation has primarily fo-cused on providing either one-way or two-way trans-lation on a single device (Waibel et al, 2003; Zhouet al, 2003).
Typically, the user interface requiresthe participant(s) to choose the source and target lan-guage apriori.
The nature of communication, eithersingle user talking or turn taking between two userscan result in a one-way or cross-lingual dialog inter-action.
In most systems, the necessity to choose thedirectionality of translation for each turn does takeaway from a natural dialog flow.
Furthermore, singleinterface based S2S translation (embedded or cloud-based) is not suitable for cross-lingual communica-tion when participants are geographically distant, ascenario more likely in a global setting.
In such ascenario, it is imperative to provide real-time andlow latency communication.In a conventional telephone conversation betweentwo speakers of the same language, the interactionis real-time and the speakers process the informa-tion stream incrementally.
Similarly, cross-lingualdialog between two remote participants will greatlybenefit through incremental translation.
While in-cremental decoding for text translation has beenaddressed previously in (Furuse and Iida, 1996;Sankaran et al, 2010), we address the problem ina speech-to-speech translation setting for enablingreal-time cross-lingual dialog.
We address the prob-lem of incrementality in a novel session initiationprotocol (SIP) based S2S translation system that en-ables two people to interact and engage in cross-lingual dialog over a telephone (mobile phone orlandline).
Our system performs incremental speechrecognition and translation, allowing for low latencyinteraction that provides an ideal setting for remotedialog aimed at accomplishing a task.We present previous work in this area in Section 2and introduce the problem of incremental translationin Section 3.
We describe the statistical models usedin the S2S translation framework in Section 4 fol-lowed by a description of the SIP communication437framework for real-time translation in Section 5.
InSection 6, we describe the basic call flow of our sys-tem following which we present dialog experimentsperformed using our framework in Section 8.
Fi-nally, we conclude in Section 9 along with directionsfor future work.2 Previous WorkMost previous work on speech-to-speech transla-tion systems has focused on a single device model,i.e., the user interface for translation is on one de-vice (Waibel et al, 1991; Metze et al, 2002; Zhouet al, 2003; Waibel et al, 2003).
The device typi-cally supports multiple source-target language pairs.A user typically chooses the directionality of transla-tion and a toggle feature is used to switch the direc-tionality.
However, this requires physical presenceof the two conversants in one location.On the other hand, text chat between users overcell phones has become increasingly popular in thelast decade.
While the language used in the inter-action is typically monolingual, there have been at-tempts to use statistical machine translation to en-able cross-lingual text communication (Chen andRaman, 2008).
But this introduces a significantoverhead as the users need to type in the responsesfor each turn.
Moreover, statistical translation sys-tems are typically unable to cope with telegraphictext present in chat messages.
A more user friendlyapproach would be to use speech as the modality forcommunication.One of the first attempts for two-way S2S trans-lation over a telephone between two potentially re-mote participants was made as part of the Verbmobilproject (Wahlster, 2000).
The system was restrictedto certain topics and speech was the only modality.Furthermore, the spontaneous translation of dialogswas not incremental.
One of the first attempts at in-cremental text translation was demonstrated in (Fu-ruse and Iida, 1996) using a transfer-driven machinetranslation approach.
More recently, an incremen-tal decoding framework for text translation was pre-sented in (Sankaran et al, 2010).
To the best ofour knowledge, incremental speech-to-speech trans-lation in a dialog setting has not been addressed inprior work.
In this work, we address this problemusing first of a kind SIP-based large vocabulary S2Stranslation system that can work with both smart-phones and landlines.
The speech translation is per-formed incrementally based on generation of partialhypotheses from speech recognition.
Our systemdisplays the recognized and translated text in an in-cremental fashion.
The use of SIP-based technologyalso supports an open form of cross-lingual dialogwithout the need for attention phrases.3 Incremental Speech-to-SpeechTranslationIn most statistical machine translation systems, theinput source text is translated in entirety, i.e., thesearch for the optimal target string is constrainedon the knowledge of the entire source string.
How-ever, in applications such as language learning andreal-time speech-to-speech translation, incremen-tally translating the source text or speech can pro-vide seamless communication and understandingwith low latency.
Let us assume that the input string(either text or speech recognition hypothesis) is f =f1, ?
?
?
, fJ and the target string is e = e1, ?
?
?
, eI .Among all possible target sentences, we will choosethe one with highest probability:e?
(f) = argmaxePr(e|f) (1)In an incremental translation framework, we do notobserve the entire string f .
Instead, we observe Qssequences, S = s1 ?
?
?
sk ?
?
?
sQs , i.e., each sequencesk = [fjkfjk+1 ?
?
?
fj(k+1)?1], j1 = 1, jQs+1 =J + 11.
Let the translation of each foreign sequencesk be denoted by tk = [eikeik+1 ?
?
?
ei(k+1)?1], i1 =1, iQs+1 = I+1.
Given this setting, we can performdecoding using three different approaches.
Assum-ing that each partial source input is translated inde-pendently, i.e., chunk-wise translation, we get,e?
(f) = argmaxt1Pr(t1|s1) ?
?
?
argmaxtkPr(tk|sk)(2)We call the decoding in Eq.
2 as partial decoding.The other option is to translate the partial source in-1For simplicity, we assume that the incremental and non-incremental hypotheses are equal in length438put conditioned on the history, i.e.,e?
(f) = argmaxt1Pr(t1|s1) ?
?
?argmaxtkPr(tk|s1, ?
?
?
, sk, t?1, ?
?
?
, t?k?1) (3)where t?i denotes the best translation for source se-quence si.
We term the result obtained through Eq.
3as continue-partial.
The third option is to wait forall the partials to be generated and then decode thesource string which we call complete decoding, i.e.,e?
(f) = argmaxePr(e|s1, ?
?
?
, sk) (4)Typically, the hypothesis e?
will be more accuratethan e?
as the translation process is non-incremental.In the best case, one can obtain e?
= e?.
While the de-coding described in Eq.
2 has the lowest latency, itis likely to result in inferior performance in compari-son to Eq.
1 that will have higher latency.
One of themain issues in incremental speech-to-speech trans-lation is that the translated sequences need to be im-mediately synthesized.
Hence, there is tradeoff be-tween the amount of latency versus accuracy as thesynthesized audio cannot be revoked in case of longdistance reordering.
In this work, we focus on incre-mental speech translation and defer the problem ofincremental synthesis to future work.
We investigatethe problem of incrementality using a novel SIP-based S2S translation system, the details of whichwe discuss in the subsequent sections.4 Speech-to-Speech TranslationComponentsIn this section, we describe the training data, pre-processing steps and statistical models used in theS2S system.4.1 Automatic Speech RecognitionWe use the AT&T WATSONSM real-time speechrecognizer (Goffin et al, 2004) as the speech recog-nition module.
WATSONSM uses context-dependentcontinuous density hidden Markov models (HMM)for acoustic modeling and finite-state networks fornetwork optimization and search.
The acoustic mod-els are Gaussian mixture tied-state three-state left-to-right HMMs.
All the acoustic models in this workwere initially trained using the Maximum Likeli-hood Estimation (MLE) criterion, and followed bydiscriminative training through Minimum Phone Er-ror (MPE) criterion.
We also employed GaussianSelection (Bocchieri, 1993) to decrease the real-timefactor during the recognition procedure.The acoustic models for English and Span-ish were mainly trained on short utterances inthe respective language, acquired from SMS andsearch applications on smartphones.
The amountof training data for the English acoustic modelis around 900 hours of speech, while the datafor training the Spanish is approximately half thatof the English model.
We used a total of 107phonemes for the English acoustic model, com-posed of digit-specific, alpha-specific, and generalEnglish phonemes.
Digit-specific and alpha-specificphonemes were applied to improve the recognitionaccuracy of digits and alphas in the speech.
Thenumber of phonemes for Spanish was 34, and, nodigit- or alpha-specific phonemes were included.The pronunciation dictionary for English is a hand-labeled dictionary, with pronunciation for unseenwords being predicted using custom rules.
A rule-based dictionary was used for Spanish.We use AT&T FSM toolkit (Mohri et al, 1997)to train a trigram language model (LM).
The lan-guage model was linearly interpolated from 18 and17 components for English and Spanish, respec-tively.
The data for the the LM components wasobtained from several sources that included LDC,Web, and monolingual portion of the parallel datadescribed in section 4.2.
An elaborate set of lan-guage specific tokenization and normalization ruleswas used to clean the corpora.
The normalizationincluded spelling corrections, conversion of numer-als into words while accounting for telephone num-bers, ordinal, and, cardinal categories, punctuation,etc.
The interpolation was performed by tuning thelanguage model weights on a development set us-ing perplexity metric.
The development set was 500sentences selected randomly from the IWSLT cor-pus (Paul, 2006).
The training vocabulary size forEnglish acoustic model is 140k and for the languagemodel is 300k.
For the Spanish model, the train-ing vocabulary size is 92k, while for testing, thelanguage model includes 370k distinct words.
Inour experiments, the decoding and LM vocabularies439were the same.4.2 Machine TranslationThe phrase-based translation experiments reportedin this work was performed using the Moses2toolkit (Koehn et al, 2007) for statistical machinetranslation.
Training the translation model startsfrom the parallel sentences from which we learnword alignments by using GIZA++ toolkit (Ochand Ney, 2003).
The bidirectional word alignmentsobtained using GIZA++ were consolidated by us-ing the grow-diag-final option in Moses.
Subse-quently, we learn phrases (maximum length of 7)from the consolidated word alignments.
A lexical-ized reordering model (msd-bidirectional-fe optionin Moses) was used for reordering the phrases inaddition to the standard distance based reordering(distortion-limit of 6).
The language models wereinterpolated Kneser-Ney discounted trigram models,all constructed using the SRILM toolkit (Stolcke,2002).
Minimum error rate training (MERT) wasperformed on a development set to optimize the fea-ture weights of the log-linear model used in trans-lation.
During decoding, the unknown words werepreserved in the hypotheses.The parallel corpus for phrase-based transla-tion was obtained from a variety of sources: eu-roparl (Koehn, 2005), jrc-acquis corpus (Steinbergeret al, 2006), opensubtitle corpus (Tiedemann andLars Nygaard, 2004), web crawling as well as hu-man translation.
The statistics of the data used forEnglish-Spanish is shown in Table 1.
About 30% ofthe training data was obtained from the Web (Ran-garajan Sridhar et al, 2011).
The development set(identical to the one used in ASR) was used inMERT training as well as perplexity based optimiza-tion of the interpolated language model.
The lan-guage model for MT and ASR was constructed fromidentical data.4.3 Text-to-speech synthesisThe translated sentence from the machine trans-lation component is synthesized using the AT&TNatural VoicesTM text-to-speech synthesis en-gine (Beutnagel et al, 1999).
The system uses unitselection synthesis with half phones as the basic2http://www.statmt.org/mosesen-esData statistics en es# Sentences 7792118 7792118# Words 98347681 111006109Vocabulary 501450 516906Table 1: Parallel data used for training translationmodelsunits.
The database was recorded by professionalspeakers of the language.
We are currently using fe-male voices for English as well as Spanish.5 SIP Communication Framework forReal-time S2S TranslationThe SIP communication framework for real-timelanguage translation comprises of three main com-ponents.
Session Initiation Protocol (SIP) is becom-ing the de-facto standard for signaling control forstreaming applications such as Voice over IP.
Wepresent a SIP communication framework that usesReal-time Transport Protocol (RTP) for packetiz-ing multimedia content and User Datagram Proto-col (UDP) for delivering the content.
In this work,the content we focus on is speech and text infor-mation exchanged between two speakers in a cross-lingual dialog.
For two users conversing in two dif-ferent languages (e.g., English and Spanish), the me-dia channels between them will be established asshown in Figure 1.
In Figure 1, each client (UA) isresponsible for recognition, translation, and synthe-sis of one language input.
E.g., the English-SpanishUA recognizes English text, converts it into Spanish,and produces output Spanish audio.
Similarly, theSpanish-English UA is responsible for recognitionof Spanish speech input, converting it into English,and producing output English audio.
We describethe underlying architecture of the system below.5.1 Architecture1.
End point SIP user agents: These are the SIPend points that exchange SIP signaling mes-sages with the SIP Application server (AS) forcall control.2.
SIP User Agents: Provide a SIP interface to thecore AT&T WATSONSM engine that incorpo-rates acoustic and language models for speech440SIP UA(en->es)SIP UA(es->en)APPSERVERCaller(English)Callee(Spanish)Caller Eng AudioCallee English Audio (Translated)Callee Spanish AudioCaller Spanish Audio (Translated)CallerEnglishText(Recognized)CalleeEnglishText(Translated)CalleeSpanishText(Recognized)CalleeEnglishText(Translated)CallerSpanishText(Translated)CalleeSpanishText(Recognized)CallerEnglishText(Recognized)CallerSpanishText(Translated)SIP Channel for Signaling Setup and Text (recognized + translated)Media Channel for RTP AudioFigure 1: SIP communication framework used for real-time speech-to-speech translation.
The exampleshows the setup between two participants in English(en) and Spanish (es)recognition.3.
SIP Application Server (AS): A standard SIPB2BUA (back to back user agent) that receivesSIP signaling messages and forwards them tothe intended destination.
The machine transla-tion component (server running Moses (Koehnet al, 2007)) is invoked from the AS.In our communication framework, the SIP AS re-ceives a call request from the calling party.
The ASinfers the language preference of the calling partyfrom the user profile database and forwards the callto the called party.
Based on the response, AS in-fers the language preference of the called party fromthe user profile database.
If the languages of thecalling and called parties are different, the AS in-vites two SIP UAs into the call context.
The AS ex-changes media parameters derived from the callingand called party SIP messages with that of the SIPUAs.
The AS then forwards the media parametersof the UAs to the end user SIP agents.The AS, the end user SIP UAs, and the SIP UAsare all RFC 3261 SIP standard compliant.
The enduser SIP UAs are developed using PJSIP stack thatuses PJMedia for RTP packetization of audio andnetwork transmission.
For our testing, we haveimplemented the end user SIP UAs to run on Ap-ple IOS devices.
The AS is developed using E4SS(Echarts for SIP Servlets) software and deployed onSailfin Java container.
It is deployed on a Linux boxinstalled with Cent OS version 5.
The SIP UAs arewritten in python for interfacing with external SIPdevices, and use proprietary protocol for interfacingwith the core AT&T WATSONSM engine.6 Typical Call FlowFigure 2 shows the typical call flow involved in set-ting up the cross-lingual dialog.
The caller choosesthe number of the callee from the address book orenters it using the keypad.
Subsequently, the call isinitiated and the underlying SIP channels are estab-lished to facilitate the call.
The users can then con-verse in their native language with the hypothesesdisplayed in an IM-like fashion.
The messages ofthe caller appear on the left side of the screen whilethose of the callee appear on the right.
Both therecognition and translation hypotheses are displayedincrementally for each side of the conversation.
Inour experiments, the caller and the callee naturallyfollowed a protocol of listening to the other party?ssynthesized output before speaking once they wereaccustomed to the interface.
One of the issues dur-ing speech recognition is that, the user can poten-tially start speaking as the TTS output from the other441Figure 2: Illustration of call flow.
The call is established using SIP and the real-time conversation appearsin the bubbles in a manner similar to Instant Messaging.
For illustration purposes, the caller (Spanish) andcallee (English) are assumed to have set their language preferences in the setup menu.participant is being played.
We address the feedbackproblem from the TTS output by muting the micro-phone when TTS output is played.7 Dialog DataThe system described above provides a natural wayto collect cross-lingual dialog data.
We used oursystem to collect a corpus of 40 scripted dialogs inEnglish and Spanish.
A bilingual (English-Spanish)speaker created dialog scenarios in the travel andhospitality domain and the scripted dialog was usedas reference material in the call.
Two subjects partic-ipated in the data collection, a male English speakerand female Spanish speaker.
The subjects were in-structed to read the lines verbatim.
However, due toASR errors, the subjects had to repeat or improvisefew turns (about 10%) to sustain the dialog.
The av-erage number of turns per scenario in the collectedcorpus is 13; 6 and 7 turns per scenario for Englishand Spanish, respectively.
An example dialog be-tween two speakers is shown in Table 2.8 ExperimentsIn this section, we describe speech translation ex-periments performed on the dialog corpus collectedthrough our system.
We present baseline results fol-lowed by results of incremental translation.8.1 Baseline ExperimentsThe models described in Section 4 were used to es-tablish baseline results on the dialog corpus.
NoA: Hello, I am calling from room four twenty onethe T.V.
is not working.
Do you think you can sendsomeone to fix it please?B: Si, Sen?or enseguida enviamos a alguien para quela arregle.
Si no le cambiaremos de habitacio?n.A: Thank you very much.B: Estamos aqu para servirle.
Lla?menos si necesitaalgo ma?s.Table 2: Example of a sample dialog scenario.contextual information was used in these experi-ments, i.e., the audio utterances were decoded in-dependently.
The ASR WER for English and Span-ish sides of the dialogs is shown in Figure 3.
Theaverage WER for English and Spanish side of theconversations is 27.73% and 22.83%, respectively.The recognized utterances were subsequently trans-lated using the MT system described above.
TheMT performance in terms of Translation Edit Rate(TER) (Snover et al, 2006) and BLEU (Papineniet al, 2002) is shown in Figure 4.
The MT per-formance is shown across all the turns for both ref-erence transcriptions and ASR output.
The resultsshow that the performance of the Spanish-EnglishMT model is better in comparison to the English-Spanish model on the dialog corpus.
The perfor-mance on ASR input drops by about 18% comparedto translation on reference text.44208.517.025.534.0Reference ASR23.8728.2126.9633.58BLEUSpanish-EnglishEnglish-Spanish017.535.052.570.0Reference ASR63.4259.1955.3447.26TERSpanish-EnglishEnglish-SpanishFigure 4: TER (%) and BLEU of English-Spanish and Spanish-English MT models on reference transcriptsand ASR outputFigure 3: WER (%) of English and Spanish acousticmodels on the dialog corpus8.2 Segmentation of ASR output for MTTurn taking in a dialog typically involves the sub-jects speaking one or more utterances in a turn.Since, machine translation systems are trained onchunked parallel texts (40 words or less), it is ben-eficial to segment the ASR hypotheses before trans-lation.
Previous studies have shown significant im-provements in translation performance through thesegmentation of ASR hypotheses (Matusov et al,2007).
We experimented with the notion of seg-mentation defined by silence frames in the ASR out-put.
A threshold of 8-10 frames (100 ms) was foundto be suitable for segmenting the ASR output intosentence chunks.
We did not use any lexical fea-tures for segmenting the turns.
The BLEU scores fordifferent silence thresholds used in segmentation isshown in Figure 5.
The BLEU scores improvementfor Spanish-English is 1.6 BLEU points higher thanthe baseline model using no segmentation.
The im-provement for English-Spanish is smaller but statis-tically significant.
Analysis of the dialogs revealedthat the English speaker tended to speak his turnswithout pausing across utterance chunks while theSpanish speaker paused a lot more.
The results in-dicate that in a typical dialog interaction, if the par-ticipants observe inter-utterance pause (80-100 ms)within a turn, it serves as a good marker for segmen-tation.
Further, exploiting such information can po-tentially result in improvements in MT performanceas the model is typically trained on sentence levelparallel text.12.013.815.617.419.221.022.824.626.428.230.050 80 110 140 170 200 50025.21 25.2024.8624.8024.3424.2723.8728.76 28.7628.2128.1827.7627.6226.96BLEUSilence threshold for segmentation (ms)Figure 5: BLEU score of English-Spanish andSpanish-English MT models on the ASR output us-ing silence segmentation8.3 Incremental Speech Translation ResultsFigure 6 shows the BLEU score for incrementalspeech translation described in Section 3.
In the fig-ure, partial refers to Eq.
2, continue-partial refers toEq.
3 and complete refers to Eq.
4.
The continue-partials option was exercised by using the continue-44302.785.568.3311.1113.8916.6719.4422.2225.0010 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000BLEUSpeech Recognizer timeouts (msec)Partial (Eq.
1)Moses ?continue-partial?
(Eq.
2)Complete (Eq.
3)Figure 6: BLEU score (Spanish-English) for incremental speech translation across varying timeout periodsin the speech recognizerpartial-translation parameter in Moses (Koehn et al,2007).
The partial hypotheses are generated as afunction of speech recognizer timeouts.
Timeout isdefined as the time interval with which the speechrecognizer generates partial hypotheses.
For eachtimeout interval, the speech recognizer may or maynot generate a partial result based on the search pathat that instant in time.
As the timeout interval in-creases, the performance of incremental translationapproaches that of non-incremental translation.
Thekey is to choose an operating point such that theuser perception of latency is minimal with accept-able BLEU score.
It is interesting that very goodperformance can be attained at a timeout of 500 msin comparison with non-incremental speech trans-lation, i.e., the latency can be reduced in half withacceptable translation quality.
The continue-partialoption in Moses performs slightly better than thepartial case as it conditions the decision on priorsource input as well as translation.In Table 3, we present the latency measurementsof the various components in our framework.
We donot have a row for ASR since it is not possible to getthe start time for each recognition run as the RTPpackets are continuously flowing in the SIP frame-work.
The latency between various system compo-nents is very low (5-30 ms).
While the average timetaken for translation (incremental) is ?
100 ms, theTTS takes the longest time as it is non-incrementalin the current work.
It can also been seen that theaverage time taken for generating incremental MToutput is half that of TTS that is non-incremental.The overall results show that the communication inour SIP-based framework has low latency.Components Caller Callee AverageASR output to MT input 6.8 0.1 3.4MT 100.4 108.8 104.6MT output to TTS 22.1 33.1 27.6TTS 246 160.3 203.1Table 3: Latency measurements (in ms) for the S2Scomponents in the real-time SIP framework.9 ConclusionIn this paper, we introduced the problem of incre-mental speech-to-speech translation and presentedfirst of a kind two-way real-time speech-to-speechtranslation system based on SIP that incorporatesthe notion of incrementality.
We presented detailsabout the SIP framework and demonstrated the typ-ical call flow in our application.
We also presenteda dialog corpus collected using our framework andbenchmarked the performance of the system.
Ourframework allows for incremental speech transla-tion and can provide low latency translation.
Weare currently working on improving the accuracy ofincremental translation.
We are also exploring newalgorithms for performing reordering aware incre-mental speech-to-speech translation, i.e., translatingsource phrases such that text-to-speech synthesis canbe rendered incrementally.444ReferencesM.
Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, andA.
Syrdal.
1999.
The AT&T Next-Gen TTS sys-tem.
In Proceedings of Joint Meeting of ASA, EAAand DEGA.E.
Bocchieri.
1993.
Vector quantization for the efficientcomputation of continuous density likelihoods.
Pro-ceedings of ICASSP.Charles L. Chen and T. V. Raman.
2008.
Axsjax: a talk-ing translation bot using google im: bringing web-2.0applications to life.
In Proceedings of the 2008 inter-national cross-disciplinary conference on Web acces-sibility (W4A).O.
Furuse and H. Iida.
1996.
Incremental translation uti-lizing constituent boundary patterns.
In Proc.
of Col-ing ?96.Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,Dilek Hakkani Tur, Andrej Ljolje, and SarangarajanParthasarathy.
2004.
The AT&T Watson Speech Rec-ognizer.
Technical report, September.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, Shen W.,C.
Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.E.
Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-Tu?r, M. Ostendorf, and H. Ney.
2007.
Improvingspeech translation with automatic boundary predic-tion.
In Proceedings of Interspeech.F.
Metze, J. McDonough, H. Soltau, A. Waibel, A. Lavie,S.
Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi,R.
Cattoni, G. Lazzari, N. Mana, and E. Pianta.
2002.The NESPOLE!
speech-to-speech translation system.M.
Mohri, F. Pereira, and M. Riley.
1997.
Attgeneral-purpose finite-state machine software tools,http://www.research.att.com/sw/tools/fsm/.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL.M.
Paul.
2006.
Overview of the iwslt 2006 evaluationcampaign.
In Proceedings of the International Work-shop of Spoken Language Translation, Kyoto, Japan.V.
K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.2011.
A scalable approach to building a parallel cor-pus from the Web.
In Proceedings of Interspeech.B.
Sankaran, A. Grewal, and A. Sarkar.
2010.
Incre-mental decoding for phrase-based statistical machinetranslation.
In Proceedings of the fifth Workshop onStatistical Machine Translation and Metrics.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit rate withtargeted human annotation.
In Proceedings of AMTA.R.
Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-javec, and D. Tufis.
2006.
The JRC-Acquis: A multi-lingual aligned parallel corpus with 20+ languages.
InProceedings of LREC.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of ICSLP.J.
Tiedemann and L. Lars Nygaard.
2004.
The OPUScorpus - parallel & free.
In Proceedings of LREC.Wolfgang Wahlster, editor.
2000.
Verbmobil: Founda-tions of Speech-to-Speech Translation.
Springer.A.
Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G.Hauptmann, and J. Tebelskis.
1991.
JANUS: aspeech-to-speech translation system using connection-ist and symbolic processing strategies.
In Proceedingsof ICASSP, pages 793?796, Los Alamitos, CA, USA.A.
Waibel, A. Badran, A. W. Black, R. Frederk-ing, G. Gates, A. Lavie, L. Levin, K. Lenzo,L.
M. Tomokiyo, J. Reichert, T. Schultz, W. Dorcas,M.
Woszczyna, and J. Zhang.
2003.
Speechalator:two-way speech-to-speech translation on a consumerPDA.
In Proceedings of the European Conference onSpeech Communication and Technology, pages 369?372.B.
Zhou, Y. Gao, J. Sorenson, D. Dechelotte, andM.
Picheny.
2003.
A hand-held speech-to-speechtranslation system.
In Proceedings of ASRU.445
