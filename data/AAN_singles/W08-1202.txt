Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 2?7Manchester, August 2008Analyzing DisagreementsBeata Beigman Klebanov, Eyal Beigman, Daniel DiermeierKellogg School of BusinessNorthwestern University{beata,e-beigman,d-diermeier}@northwestern.eduAbstractWe address the problem of distinguishingbetween two sources of disagreement inannotations: genuine subjectivity and slipof attention.
The latter is especially likelywhen the classification task has a defaultclass, as in tasks where annotators need tofind instances of the phenomenon of inter-est, such as in a metaphor detection taskdiscussed here.
We apply and extend a dataanalysis technique proposed by BeigmanKlebanov and Shamir (2006) to first dis-till reliably deliberate (non-chance) anno-tations and then to estimate the amount ofattention slips vs genuine disagreement inthe reliably deliberate annotations.1 IntroductionClassification tasks fall into two broad categories.Those in the first category proceed by requiringthat every item is explicitly assigned a tag out ofa given set of tags; part-of-speech tagging is anexample (Santorini, 1990).In the second group of tasks, the annotator isasked to identify a phenomenon of interest, thusimplicitly classifying items as belonging to thephenomenon (marked) and not belonging to it (leftunmarked).
When the studied phenomenon is ex-pected to have low incidence, this is a time-savingstrategy, as annotators do not need to bother withexplicitly marking (almost) everything as a non-phenomenon.
A recent example of such a task isBeigman Klebanov and Shamir (2006), where an-notators were asked to provide anchors for wordsc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.deemed anchored in the text (i.e.
associativelyconnected to a previous item in the text), thus leav-ing words that did not receive an anchor implic-itly marked as un-anchored.
Psychological exper-iments where people are asked to respond to theoccurrence of a given phenomenon can also beviewed as implicit classifications; for example, seeSpiro?s (2007) work on identification of bound-aries of musical phrases by listeners.
The taskof metaphor detection discussed in this paper alsofalls under the implicit classification category.While such a strategy uses annotators?
time effi-ciently, some of the observed disagreements couldbe due to an annotator missing an occurrence ofthe relevant phenomenon, rather than genuinelydisagreeing on the matter of occurrence.We show in section 2 that our metaphoridentification task features less-than-perfect inter-annotator agreement.
Section 3 uses Beigman Kle-banov and Shamir?s (2006) methodology to findannotations that can be reliably attributed to a de-liberate decision by at least some of the annotators.We then discuss the use of validation experiment todistinguish between slips of attention and genuinedisagreements (sections 4,5).2 Metaphor Detection StudyFor a project studying the use of metaphors in pub-lic discourse, a dataset of 151 articles from theBritish press was subjected to annotation.1 Partic-ipants were asked to mark paragraphs that containoccurrences of metaphors from LOVE, VEHICLE,AUTHORITY and BUILDING domains (hence-forth, metaphor types).For example, the following paragraph in 20September 1992 issue of Sunday Times contains an1This is part of the data discussed in (Musolff, 2000).2extended metaphor from the VEHICLE domain:Thatcher warned EC leaders to stop theirendless round of summits and take no-tice of their own people.
?There is afear that the European train will thunderforward, laden with its customary cargoof gravy, towards a destination nei-ther wished for nor understood by elec-torates.
But the train can be stopped,?she said.The title2 of one of the articles in the 19 Octo-ber 1999 issue of The Guardian contains a LOVEmetaphor:Euro-flirting is not only a matter of de-sire.The discussion in this paper is based on the out-put of 9 annotators who performed metaphor iden-tification (henceforth, production task), and of 7annotators (out of 9) who took part in the sub-sequent validation study (henceforth, validationtask).
Subjects were not told about validation untilafter they finished production on the whole of thedataset.
A time gap of 2 weeks existed betweenthe end of the production study and the start ofthe validation, each of the tasks taking 6 weeks,in weekly installments of 25 articles each.For the production task, the annotators wereinstructed to mark every paragraph where ametaphor from the given metaphor type appeared;the 151-article dataset yields 2364 paragraphs.This paradigm corresponds to the implicit clas-sification task discussed earlier, in that only thepositive (metaphor-containing) cases are given anexplicit markup.
The incidence of positive casesis quite low ?
VEHICLE, the most ubiquitoustype, featured in 4% of the paragraphs, on averageacross annotators.We note that the appearances of the differentmetaphor types are not mutually exclusive, and,indeed, there is no a-priori reason to supposeany relationship between them.
For example, thefollowing paragraph from the leading article in15 November 1995 issue of The Guardian wasmarked by some annotators as containing bothLOVE and VEHICLE metaphors:The first European bank notes - proba-bly to be called ?euros?
- will not be in2A title is treated as a paragraph in our annotations.circulation until 2002 judging by yester-day?s report from the European Mone-tary Institute.
But this doesn?t mean thatmonetary union has been delayed be-yond 1999 because the printing of Euro-pean bank notes will have been precededby a period of three years when na-tional currencies will have been lockedtogether in indissoluble monetary matri-mony [...] Although France looks as if itmight buckle under the strain of meet-ing the fiscal criteria and in Germanythe SDP is having doubts (though onlyabout whether the new currency will bestrong enough) the Maastricht train isstill theoretically on the rails.
Nobodyhas changed the timetable.We therefore treat the detection of metaphorsfrom each metaphor type as a separate binaryclassification task.
Table 1 shows the inter-annotator agreement for the production task usingthe ?
statistic (Carletta, 1996; Krippendorff, 1980;Siegel and Castellan, 1988).Table 1: Metaphor annotation data (production),by metaphor type.
The third column shows thepercentage of paragraphs (out of 2364) marked ashaving a metaphor of the given type, on averageacross 9 annotators.Type ?
markedVEHICLE 0.66 4.0%LOVE 0.66 2.5%AUTHORITY 0.39 2.7%BUILD 0.43 1.7%Clearly, it is not the case that the whole of thedataset was reliably annotated, even for the better-agreed-upon metaphor types like VEHICLE andLOVE.
Hence, additional procedures are neededto distill reliable annotations.
We apply BeigmanKlebanov and Shamir?s (2006) statistical tech-nique to find a subset of the data that is sufficientlyreliable, and later corroborate the statistical analy-sis through the validation task.3 Reliably Deliberate AnnotationsIn Beigman Klebanov and Shamir (2006), 22subjects performed the anchoring annotation; theoverall inter-annotator agreement was ?=0.45.3Thus, some of the data was clearly unreliable, asin our metaphor detection task, but the possibilityexisted that some other part was in fact annotatedsufficiently reliably.Beigman Klebanov and Shamir?s (2006) analy-sis proceeded thus: Suppose each of the 20 anno-tators3 (i = 1...20) was flipping a coin with theprobability of heads piequal to the proportion of?anchored?
markups in annotator i?s data.
Whatis the level of agreement for which this scenario issufficiently improbable?
For their data, the randomanchoring hypothesis could be rejected with 99%confidence for cases marked by at least 13 people.Items featuring at least this level of agreement canbe considered, with high probability, as deliber-ately annotated as ?anchored?, as at least some ofthose who marked them were not flipping a coin.Following the procedure in Beigman Klebanovand Shamir (2006), we wish to determine a re-liably deliberate subset of our metaphor annota-tions.
We induce 9 random pseudo-annotatorsfrom the 9 actual ones, each marking paragraphsat random as containing a metaphor of a giventype or not.
Pseudo-annotator i flips a coinwith p(heads) = pi, which is the proportion ofmetaphor markups by the i?th annotator for themost common metaphor type (VEHICLE).Assuming each annotator flips her coin, we cal-culate the probability of 3 or more coins coming upheads simultaneously;4 this probability is 0.0045.Thus, with 99.5% confidence, a metaphor markupby at least 3 people is not a result of coinflip, atleast for some of the annotators.
We note, how-ever, that 99.5% confidence is insufficient for ourcase: It allows for random highly agreed markupin 0.5% of the instances.
Given that only up to4% of the instances have positive markups, thiswould yield a high percentage of random itemsin the positive instances.
The probability of 4 ormore pseudo-annotators having their coins comeup heads simultaneously is below 0.0003; we con-sider this sufficient confidence for our case, andregard metaphor markups produced by at least 4people as reliably deliberate.Note that we cannot find a similar threshold forno-metaphor annotations, as a lack of metaphor3Two people were excluded as outliers.4In Beigman Klebanov and Shamir (2006), a normal ap-proximation is used to handle collective decision making by20 pseudo-annotators.
In the current case, 9 annotators is asufficiently small number to allow an exact probability calcu-lation over the 512 possibilities.annotation could happen by chance with a highprobability (p = 0.69).
In view of the potential useof the dataset for evaluating metaphor detectionalgorithms, a putative metaphor suggested by thealgorithm cannot be rejected based on the lack ofmetaphor annotation in the data.
A complementaryprocedure would be needed, for example, collect-ing human judgments for the putative metaphorsseparately.4 Attention Slips vs GenuineDisagreementsDeliberate annotation does not guarantee agree-ment.
It remained the case that some of the reliablydeliberate data in Beigman Klebanov and Shamir(2006) was actually produced by only some of theoriginal subjects.
Indeed, some of the deliberatelymarked metaphors were annotated by only 4 outof the 9 participants.
For cases where the posi-tive annotations were produced deliberately, whatis the status of negative annotations accorded tothe same items?
Were these mere attention slips,or genuine differences of opinion?
Note that thisquestion cannot be meaningfully posed regardingthe parts of annotations for which the hypothesisof random positive marking could not be rejectedwith sufficiently high probability, since, obviously,apparent disagreements there could be simply a re-sult of different coinflip outcomes.Beigman Klebanov and Shamir (2006) hypoth-esized that dissenting annotations of the reliablepairs would be cases of attention slips, rather thangenuine differences of opinion.
In other words,while there was no initial agreement, these itemswere potentially agreeable.
To test the hypothe-sis, they devised a validation experiment, wheresubjects were presented with all pairs marked byat least one annotator, plus some random pairs,and were asked to cross out things they disagreewith.
The reasoning was as follows: If attentionslip was the cause for a dissenting negative anno-tation, when the subject is asked about the relevantitem, i.e.
it is explicitly brought to her attention,she would accept it, whereas if a case is that ofa genuine disagreement, she would reject it.
Tocontrol for the possibility that people just accepteverything so that not to be dissonant with others,some random annotations were also included.The results reported by Beigman Klebanov andShamir (2006) largely bore out the hypothesis.First, people did not tend to accept everything,4as only 15% of judgments of random annota-tions and only 62% of judgments on all human-generated annotations were ?accept?
judgments.However, 94% of judgments of the reliable anno-tations were ?accept?
judgments.
Hence, the rateof genuine disagreement on the reliably deliberatepart of Beigman Klebanov and Shamir?s (2006)data turned out to be quite low.We are interested in estimating the degree ofgenuine disagreements in metaphor production.Using Beigman Klebanov and Shamir?s method-ology, we collected all paragraphs marked as con-taining a metaphor of a given type by at least oneof the 9 annotators, plus added random markups.This data was submitted to 7 subjects for valida-tion.Table 2: Percentage of ?Accept?
validations forrandom (Rand) and human (Hum) metaphor pro-duction data, as well as for the partition of thehuman data into reliably deliberate (Rel) and unre-liable (URel) subsets.
For each subset, the numberof data instances covered by the subset is shown.Subscripts indicate metaphor type: (V)EHICLE,(L)OVE, (A)UTHORITY, (B)UILD.
The bottomline shows the average over metaphor types.Subset # Acc Subset # AccRandV94 5% HumV194 73%RandL56 6% HumL137 64%RandA62 12% HumA258 51%RandB40 1% HumB126 68%Rand 252 6% Hum 715 62%Subset # Acc Subset # AccURelV92 49% RelV102 94%URelL81 43% RelL56 95%URelA218 42% RelA40 96%URelB86 55% RelB40 96%URel 477 46% Rel 238 95%Table 2 reports the percentage of ?accept?
votesfor random and human metaphor production data,as well as for reliably deliberate and unreliablesubsets of the human data.
As in Beigman Kle-banov and Shamir?s case, the validation experi-ment clearly distinguishes between random, hu-man in general, and reliably deliberate subsets, andputs the estimated degree of genuine disagreementin metaphor identification at 5% on average, withlittle variation across the metaphor types.
Thatis, given that, with high probability, at least somehumans deliberately identified a paragraph as con-taining a metaphor, the chance for its rejection isabout 5%.
The rest of observed production dis-agreements, for the reliably deliberate subset, areremedied at validation time, thus probably consti-tuting attention slips during production.
The reli-ably deliberate subset contains 33% (238/715) ofall human-generated data.5 Separating self and othersOne potential confounder in the above analysisis conflation of self-consistency with affirmationof someone else?s annotations.
It is possible thatmany of the validation-time ?accept?
votes arecases of people accepting their own earlier annota-tion; the proportion of such cases is expected to in-crease the more people marked the metaphor dur-ing production.
Therefore, to get a more preciseestimate of the degree of genuine disagreement,we control for self-affirmation, and calculate theproportion of ?accept?
validations in cases wherethe person did not mark the metaphor during pro-duction.
Specifically, if X of the 7 people who par-ticipated in both production and validation markedthe metaphor at production,5 we check the split ofthe remaining 7-X votes during validation.
Table 3presents average other-affirmation rates for the re-liably deliberate and unreliable human produceddata.
Note that only 184 out of the 238 deliberatelyreliable cases can be used, as the remaining 54 arecases where all 7 annotators produced the markup,so there is no disagreement.Table 3: Percentage of ?Accept?
validations for re-liably deliberate (Rel) and unreliable (URel) sub-sets of the metaphor production data, given that thesubject himself did NOT produce the metaphor.Subset # Acc Subset # AccURelV92 44% RelV78 90%URelL81 39% RelL38 92%URelA218 35% RelA30 91%URelB86 53% RelB38 91%URel 477 41% Rel 184 91%5The actual total of the production annotations could be upto X+2, as there were 2 more annotators in production thanin validation.5According to the table, 91% cases of disagree-ments in the reliably deliberate data are remediedat validation time.
That is, given that, with highprobability, at least some human deliberately iden-tified a paragraph as containing a metaphor, thechance for its rejection by a human who initiallyapparently disagreed with the annotation is onlyabout 9%.Finally, validation data allows an investigationof the stability of people?s judgments by calculat-ing self-rejection rates, i.e.
estimating the prob-ability of rejecting during validation an instancethat the same annotator marked as containing ametaphor during production.
Table 4 shows theresults.Table 4: Percentage of ?Reject?
validations for re-liably deliberate (Rel) and unreliable (URel) sub-sets of the metaphor production data, given that thesubject himself produced the annotation.Subset # Rej Subset # RejURelV72 25% RelV102 4%URelL55 26% RelL56 5%URelA198 22% RelA40 2%URelB60 23% RelB40 2%URel 3856 23% Rel 238 4%For the reliably deliberate data, i.e.
cases whereat least 4 people produced the markup, the averageself-rejection rate is 4%.
This low figure furthersupports the designation of the reliably deliberatesubset as such, i.e.
containing stable annotations,as in 96% of the cases a person who produced themarkup is likely to re-affirm it when asked again,even after a substantial time delay.7For the ?unreliable?
data, i.e.
cases where onlyone or two people marked the metaphor duringproduction, the average self-rejection rate is 23%.Self-rejection means either that the initial positivemarkup was a mistake, or that it is difficult for theannotator to make up his mind about the annota-tion of the item.
In any case, high self-rejection6Note that only 385 of the 477 items in the unreliable datacould be used for the calculation.
The remaining items werenot produced by any of the 7 people who participated in bothproduction and validation, but only by one or both of the 2additional production-task annotators.7The time difference between production and validationper article ranged between 4 and 8 weeks, due to differencesin the order in which the different subjects were given thearticles.rate means that the relevant production annotationscannot be trusted to contain a settled judgment thatcould be then agreed or disagreed with by other an-notators, or indeed replicated by a computationalmodel.We consider self-rejected cases potential indica-tors of a difficulty on the annotator?s part to de-cide on the correct markup.
We plan a more de-tailed investigation of the materials to see whetherthese cases exhibit any interesting common prop-erties that could help characterize the difficulties inmetaphor identification task.6 ConclusionIn this article, we showed an application ofBeigman Klebanov and Shamir?s (2006) method-ology for analyzing annotation data to metaphoridentification annotations.
The approach allowedestablishing an agreement threshold beyond whichthe annotations are reliably deliberate, in the sensethat, with high probability, at least some of theannotators who detected a metaphor were not flip-ping a coin.
This threshold is agreement of 4 outof 9 annotators, for 99.9% reliability.To investigate the nature of disagreements in thereliably deliberate subset, we followed BeigmanKlebanov and Shamir (2006) in conducting a val-idation study, where subjects were asked to ac-cept or reject markups produced during the ini-tial annotation study, as well as some randomannotations.
Sharpening the methodology some-what, we showed that in 91% of reliably deliber-ate cases where an annotator did not produce themarkup himself, he accepted it during validation.Hence, the bulk of the initial disagreements wereamended during validation, with the residual 9%being likely locations for genuine difference ofopinion.Further analysis of validation data revealed thatthe reliably deliberate subset features low self-rejection rates, meaning that people are consis-tent with their own production.
This was not thecase for the subset deemed unreliable during sta-tistical analysis, where a 23% self-rejection ratewas observed.
We hypothesize that some of thesewould be hard-to-decide cases with respect to themetaphor identification task, and hence warrant acloser look in order to characterize annotator diffi-culties with the task.67 AcknowledgmentWe would like to thank an anonymous reviewerfor a thorough and insightful review that helped asimprove this article significantly.ReferencesBeigman Klebanov, Beata and Eli Shamir.
2006.Reader-based exploration of lexical cohesion.
Lan-guage Resources and Evaluation, 40(2):109?126.Carletta, Jean.
1996.
Assessing agreement on clas-sification tasks: the kappa statistic.
ComputationalLinguistics, 22(2):249?254.Krippendorff, Klaus.
1980.
Content Analysis.
SagePublications.Musolff, Andreas.
2000.
Mirror images of Europe:Metaphors in the public debate about Europe inBritain and Germany.
Mu?nchen: Iudicium.Santorini, Beatrice.
1990.
Part-of-speechtagging guidelines for the Penn Tree-bank project (3rd revision, 2nd printing).ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz.Siegel, Sidney and John Castellan.
1988.
Nonparamet-ric Statistics for the Behavioral Sciences.
McGraw-Hill Book Company.Spiro, Neta.
2007.
What contributes to the percep-tion of musical phrases in Western classical music?Ph.D.
thesis, University of Amsterdam, The Nether-lands.7
