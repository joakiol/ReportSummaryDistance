Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 484?494,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNeural Summarization by Extracting Sentences and WordsJianpeng Cheng Mirella LapataILCC, School of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABjianpeng.cheng@ed.ac.uk mlap@inf.ed.ac.ukAbstractTraditional approaches to extractivesummarization rely heavily on human-engineered features.
In this work wepropose a data-driven approach based onneural networks and continuous sentencefeatures.
We develop a general frame-work for single-document summarizationcomposed of a hierarchical documentencoder and an attention-based extractor.This architecture allows us to developdifferent classes of summarization modelswhich can extract sentences or words.
Wetrain our models on large scale corporacontaining hundreds of thousands ofdocument-summary pairs1.
Experimentalresults on two summarization datasetsdemonstrate that our models obtain resultscomparable to the state of the art withoutany access to linguistic annotation.1 IntroductionThe need to access and digest large amounts oftextual data has provided strong impetus to de-velop automatic summarization systems aiming tocreate shorter versions of one or more documents,whilst preserving their information content.
Mucheffort in automatic summarization has been de-voted to sentence extraction, where a summary iscreated by identifying and subsequently concate-nating the most salient text units in a document.Most extractive methods to date identifysentences based on human-engineered features.These include surface features such as sentenceposition and length (Radev et al, 2004), the wordsin the title, the presence of proper nouns, contentfeatures such as word frequency (Nenkova et al,2006), and event features such as action nouns (Fi-latova and Hatzivassiloglou, 2004).
Sentences are1Resources are available for download at http://homepages.inf.ed.ac.uk/s1537177/resources.htmltypically assigned a score indicating the strengthof presence of these features.
Several methodshave been used in order to select the summary sen-tences ranging from binary classifiers (Kupiec etal., 1995), to hidden Markov models (Conroy andO?Leary, 2001), graph-based algorithms (Erkanand Radev, 2004; Mihalcea, 2005), and integer lin-ear programming (Woodsend and Lapata, 2010).In this work we propose a data-driven approachto summarization based on neural networks andcontinuous sentence features.
There has been asurge of interest recently in repurposing sequencetransduction neural network architectures for NLPtasks such as machine translation (Sutskever etal., 2014), question answering (Hermann et al,2015), and sentence compression (Rush et al,2015).
Central to these approaches is an encoder-decoder architecture modeled by recurrent neu-ral networks.
The encoder reads the source se-quence into a list of continuous-space representa-tions from which the decoder generates the targetsequence.
An attention mechanism (Bahdanau etal., 2015) is often used to locate the region of focusduring decoding.We develop a general framework for single-document summarization which can be used toextract sentences or words.
Our model includesa neural network-based hierarchical documentreader or encoder and an attention-based contentextractor.
The role of the reader is to derive themeaning representation of a document based on itssentences and their constituent words.
Our modelsadopt a variant of neural attention to extract sen-tences or words.
Contrary to previous work whereattention is an intermediate step used to blend hid-den units of an encoder to a vector propagating ad-ditional information to the decoder, our model ap-plies attention directly to select sentences or wordsof the input document as the output summary.Similar neural attention architectures have beenpreviously used for geometry reasoning (Vinyalset al, 2015), under the name Pointer Networks.484One stumbling block to applying neural net-work models to extractive summarization is thelack of training data, i.e., documents with sen-tences (and words) labeled as summary-worthy.Inspired by previous work on summarization(Woodsend and Lapata, 2010; Svore et al, 2007)and reading comprehension (Hermann et al,2015) we retrieve hundreds of thousands of newsarticles and corresponding highlights from theDailyMail website.
Highlights usually appear asbullet points giving a brief overview of the infor-mation contained in the article (see Figure 1 foran example).
Using a number of transformationand scoring algorithms, we are able to match high-lights to document content and construct two largescale training datasets, one for sentence extractionand the other for word extraction.
Previous ap-proaches have used small scale training data in therange of a few hundred examples.Our work touches on several strands of researchwithin summarization and neural sequence model-ing.
The idea of creating a summary by extractingwords from the source document was pioneered inBanko et al (2000) who view summarization as aproblem analogous to statistical machine transla-tion and generate headlines using statistical mod-els for selecting and ordering the summary words.Our word-based model is similar in spirit, how-ever, it operates over continuous representations,produces multi-sentence output, and jointly se-lects summary words and organizes them into sen-tences.
A few recent studies (Kobayashi et al,2015; Yogatama et al, 2015) perform sentence ex-traction based on pre-trained sentence embeddingsfollowing an unsupervised optimization paradigm.Our work also uses continuous representations toexpress the meaning of sentences and documents,but importantly employs neural networks more di-rectly to perform the actual summarization task.Rush et al (2015) propose a neural attentionmodel for abstractive sentence compression whichis trained on pairs of headlines and first sentencesin an article.
In contrast, our model summarizesdocuments rather than individual sentences, pro-ducing multi-sentential discourse.
A major archi-tectural difference is that our decoder selects out-put symbols from the document of interest ratherthan the entire vocabulary.
This effectively helpsus sidestep the difficulty of searching for the nextoutput symbol under a large vocabulary, with low-frequency words and named entities whose rep-resentations can be challenging to learn.
Gu etal.
(2016) and Gulcehre et al (2016) propose asimilar ?copy?
mechanism in sentence compres-sion and other tasks; their model can accommo-date both generation and extraction by selectingwhich sub-sequences in the input sequence to copyin the output.We evaluate our models both automatically (interms of ROUGE) and by humans on two datasets:the benchmark DUC 2002 document summariza-tion corpus and our own DailyMail news high-lights corpus.
Experimental results show thatour summarizers achieve performance compara-ble to state-of-the-art systems employing hand-engineered features and sophisticated linguisticconstraints.2 Problem FormulationIn this section we formally define the summariza-tion tasks considered in this paper.
Given a doc-ument D consisting of a sequence of sentences{s1, ?
?
?
,sm} and a word set {w1, ?
?
?
,wn}, we areinterested in obtaining summaries at two levels ofgranularity, namely sentences and words.Sentence extraction aims to create a sum-mary from D by selecting a subset of j sentences(where j < m).
We do this by scoring each sen-tence within D and predicting a label yL?
{0,1}indicating whether the sentence should be in-cluded in the summary.
As we apply supervisedtraining, the objective is to maximize the likeli-hood of all sentence labels yL= (y1L, ?
?
?
,ymL) giventhe input document D and model parameters ?
:log p(yL|D;?)
=m?i=1log p(yiL|D;?)
(1)Although extractive methods yield naturallygrammatical summaries and require relativelylittle linguistic analysis, the selected sentencesmake for long summaries containing much redun-dant information.
For this reason, we also de-velop a model based on word extraction whichseeks to find a subset of words2in D andtheir optimal ordering so as to form a summaryys= (w?1, ?
?
?
,w?k),w?i?
D. Compared to sentenceextraction which is a sequence labeling problem,this task occupies the middle ground betweenfull abstractive summarization which can exhibita wide range of rewrite operations and extractive2The vocabulary can also be extended to include a smallset of commonly-used (high-frequency) words.485AFL star blames vomiting cat for speeding::::::::Adelaide::::::Crows:::::::::defender::::::Daniel:::::Talia::::has::::kept:::his:::::::driving::::::::license,::::::telling::a:::::court:::he::::was::::::::speeding:::::36km::::over::::the::::limit::::::::because::he::::was:::::::::distracted:::by:::his::::sick::::cat.The 22-year-old AFL star, who drove 96km/h in a 60km/h road works zone on the South Easternexpressway in February, said he didn?t see the reduced speed sign because he was so distracted by hiscat vomiting violently in the back seat of his car.
::In::::the:::::::::Adelaide:::::::::::magistrates:::::court:::on::::::::::::Wednesday,::::::::::Magistrate:::::Bob:::::::Harrap::::::fined:::::Talia::::::$824:::for:::::::::exceeding:::the::::::speed::::limit:::by:::::more::::than::::::::30km/h.He lost four demerit points, instead of seven, because of his significant training commitments.?
Adelaide Crows defender Daniel Talia admits to speeding but says he didn?t see road signs be-cause his cat was vomiting in his car.?
22-year-old Talia was fined $824 and four demerit points, instead of seven, because of his ?signif-icant?
training commitments.Figure 1: DailyMail news article with highlights.
Underlined sentences bear label 1, and 0 otherwise.summarization which exhibits none.
We formu-late word extraction as a language generation taskwith an output vocabulary restricted to the originaldocument.
In our supervised setting, the traininggoal is to maximize the likelihood of the generatedsentences, which can be further decomposed byenforcing conditional dependencies among theirconstituent words:log p(ys|D;?
)=k?i=1log p(w?i|D,w?1,?
?
?,w?i?1;?)
(2)In the following section, we discuss the data elici-tation methods which allow us to train neural net-works based on the above defined objectives.3 Training Data for SummarizationData-driven neural summarization models requirea large training corpus of documents with labelsindicating which sentences (or words) should bein the summary.
Until now such corpora havebeen limited to hundreds of examples (e.g., theDUC 2002 single document summarization cor-pus) and thus used mostly for testing (Woodsendand Lapata, 2010).
To overcome the paucity ofannotated data for training, we adopt a methodol-ogy similar to Hermann et al (2015) and createtwo large-scale datasets, one for sentence extrac-tion and another one for word extraction.In a nutshell, we retrieved3hundreds of thou-sands of news articles and their correspondinghighlights from DailyMail (see Figure 1 for an ex-ample).
The highlights (created by news editors)3The script for constructing our datasets is modified fromthe one released in Hermann et al (2015).are genuinely abstractive summaries and thereforenot readily suited to supervised training.
To cre-ate the training data for sentence extraction, wereverse approximated the gold standard label ofeach document sentence given the summary basedon their semantic correspondence (Woodsend andLapata, 2010).
Specifically, we designed a rule-based system that determines whether a documentsentence matches a highlight and should be la-beled with 1 (must be in the summary), and 0 oth-erwise.
The rules take into account the positionof the sentence in the document, the unigram andbigram overlap between document sentences andhighlights, the number of entities appearing in thehighlight and in the document sentence.
We ad-justed the weights of the rules on 9,000 documentswith manual sentence labels created by Woodsendand Lapata (2010).
The method obtained an accu-racy of 85% when evaluated on a held-out set of216 documents coming from the same dataset andwas subsequently used to label 200K documents.Approximately 30% of the sentences in each doc-ument were deemed summary-worthy.For the creation of the word extraction dataset,we examine the lexical overlap between the high-lights and the news article.
In cases where all high-light words (after stemming) come from the orig-inal document, the document-highlight pair con-stitutes a valid training example and is added tothe word extraction dataset.
For out-of-vocabulary(OOV) words, we try to find a semantically equiv-alent replacement present in the news article.Specifically, we check if a neighbor, represented486by pre-trained4embeddings, is in the original doc-ument and therefore constitutes a valid substitu-tion.
If we cannot find any substitutes, we discardthe document-highlight pair.
Following this pro-cedure, we obtained a word extraction dataset con-taining 170K articles, again from the DailyMail.4 Neural Summarization ModelThe key components of our summarization modelinclude a neural network-based hierarchical doc-ument reader and an attention-based hierarchicalcontent extractor.
The hierarchical nature of ourmodel reflects the intuition that documents aregenerated compositionally from words, sentences,paragraphs, or even larger units.
We therefore em-ploy a representation framework which reflects thesame architecture, with global information beingdiscovered and local information being preserved.Such a representation yields minimum informa-tion loss and is flexible allowing us to apply neuralattention for selecting salient sentences and wordswithin a larger context.
In the following, we firstdescribe the document reader, and then present thedetails of our sentence and word extractors.4.1 Document ReaderThe role of the reader is to derive the meaning rep-resentation of the document from its constituentsentences, each of which is treated as a sequenceof words.
We first obtain representation vectorsat the sentence level using a single-layer convo-lutional neural network (CNN) with a max-over-time pooling operation (Kalchbrenner and Blun-som, 2013; Zhang and Lapata, 2014; Kim et al,2016).
Next, we build representations for docu-ments using a standard recurrent neural network(RNN) that recursively composes sentences.
TheCNN operates at the word level, leading to theacquisition of sentence-level representations thatare then used as inputs to the RNN that acquiresdocument-level representations, in a hierarchicalfashion.
We describe these two sub-componentsof the text reader below.Convolutional Sentence Encoder We opted fora convolutional neural network model for repre-senting sentences for two reasons.
Firstly, single-layer CNNs can be trained effectively (withoutany long-term dependencies in the model) andsecondly, they have been successfully used for4We used the Python Gensim library and the300-dimensional GoogleNews vectors.sentence-level classification tasks such as senti-ment analysis (Kim, 2014).
Let d denote thedimension of word embeddings, and s a docu-ment sentence consisting of a sequence of n words(w1, ?
?
?
,wn) which can be represented by a densecolumn matrix W ?
Rn?d.
We apply a tempo-ral narrow convolution between W and a kernelK ?
Rc?dof width c as follows:fij= tanh(Wj: j+c?1?K+b) (3)where ?
equates to the Hadamard Product fol-lowed by a sum over all elements.
fijdenotes thej-th element of the i-th feature map fiand b is thebias.
We perform max pooling over time to obtaina single feature (the ith feature) representing thesentence under the kernel K with width c:si,K= maxjfij(4)In practice, we use multiple feature maps tocompute a list of features that match the dimen-sionality of a sentence under each kernel width.
Inaddition, we apply multiple kernels with differentwidths to obtain a set of different sentence vectors.Finally, we sum these sentence vectors to obtainthe final sentence representation.
The CNN modelis schematically illustrated in Figure 2 (bottom).In the example, the sentence embeddings have sixdimensions, so six feature maps are used undereach kernel width.
The blue feature maps havewidth two and the red feature maps have widththree.
The sentence embeddings obtained undereach kernel width are summed to get the final sen-tence representation (denoted by green).Recurrent Document Encoder At the docu-ment level, a recurrent neural network composes asequence of sentence vectors into a document vec-tor.
Note that this is a somewhat simplistic attemptat capturing document organization at the level ofsentence to sentence transitions.
One might viewthe hidden states of the recurrent neural networkas a list of partial representations with each fo-cusing mostly on the corresponding input sentencegiven the previous context.
These representationsaltogether constitute the document representation,which captures local and global sentential infor-mation with minimum compression.The RNN we used has a Long Short-TermMemory (LSTM) activation unit for ameliorat-ing the vanishing gradient problem when train-ing long sequences (Hochreiter and Schmidhuber,487Figure 2: A recurrent convolutional documentreader with a neural sentence extractor.1997).
Given a document d = (s1, ?
?
?
,sm), thehidden state at time step t, denoted by ht, is up-dated as:????itftot?ct????=???????tanh???
?W ?
[ht?1st](5)ct= ftct?1+ it?ct(6)ht= ottanh(ct) (7)where W is a learnable weight matrix.
Next, wediscuss a special attention mechanism for extract-ing sentences and words given the recurrent docu-ment encoder just described, starting from the sen-tence extractor.4.2 Sentence ExtractorIn the standard neural sequence-to-sequence mod-eling paradigm (Bahdanau et al, 2015), an atten-tion mechanism is used as an intermediate stepto decide which input region to focus on in orderto generate the next output.
In contrast, our sen-tence extractor applies attention to directly extractsalient sentences after reading them.The extractor is another recurrent neural net-work that labels sentences sequentially, taking intoaccount not only whether they are individuallyrelevant but also mutually redundant.
The com-plete architecture for the document encoder andthe sentence extractor is shown in Figure 2.
Ascan be seen, the next labeling decision is madeFigure 3: Neural attention mechanism for wordextraction.with both the encoded document and the previ-ously labeled sentences in mind.
Given encoderhidden states (h1, ?
?
?
,hm) and extractor hiddenstates (?h1, ?
?
?
,?hm) at time step t, the decoder at-tends the t-th sentence by relating its current de-coding state to the corresponding encoding state:?ht= LSTM(pt?1st?1,?ht?1) (8)p(yL(t) = 1|D) = ?
(MLP(?ht: ht)) (9)where MLP is a multi-layer neural network with asinput the concatenation of?htand ht.
pt?1repre-sents the degree to which the extractor believes theprevious sentence should be extracted and memo-rized (pt?1=1 if the system is certain; 0 otherwise).In practice, there is a discrepancy between train-ing and testing such a model.
During trainingwe know the true label pt?1of the previous sen-tence, whereas at test time pt?1is unknown andhas to be predicted by the model.
The discrep-ancy can lead to quickly accumulating predictionerrors, especially when mistakes are made early inthe sequence labeling process.
To mitigate this,we adopt a curriculum learning strategy (Bengioet al, 2015): at the beginning of training whenpt?1cannot be predicted accurately, we set it tothe true label of the previous sentence; as traininggoes on, we gradually shift its value to the pre-dicted label p(yL(t?1) = 1|d).4.3 Word ExtractorCompared to sentence extraction which is a purelysequence labeling task, word extraction is closerto a generation task where relevant content mustbe selected and then rendered fluently and gram-matically.
A small extension to the structure ofthe sequential labeling model makes it suitablefor generation: instead of predicting a label forthe next sentence at each time step, the model di-rectly outputs the next word in the summary.
The488model uses a hierarchical attention architecture:at time step t, the decoder softly5attends eachdocument sentence and subsequently attends eachword in the document and computes the probabil-ity of the next word to be included in the summaryp(w?t= wi|d,w?1, ?
?
?
,w?t?1) with a softmax classi-fier:?ht= LSTM(w?t?1,?ht?1)6(10)atj= zTtanh(We?ht+Wrhj),hj?
D (11)btj= softmax(atj) (12)?ht=m?j=1btjhj(13)uti= vTtanh(We??ht+Wr?wi),wi?
D (14)p(w?t= wi|D,w?1, ?
?
?
,w?t?1) = softmax(uti) (15)In the above equations, wicorresponds to the vec-tor of the i-th word in the input document, whereasz, We, Wr, v, We?, and Wr?are model weights.The model architecture is shown in Figure 3.The word extractor can be viewed as a con-ditional language model with a vocabulary con-straint.
In practice, it is not powerful enough toenforce grammaticality due to the lexical diversityand sparsity of the document highlights.
A pos-sible enhancement would be to pair the extractorwith a neural language model, which can be pre-trained on a large amount of unlabeled documentsand then jointly tuned with the extractor duringdecoding (Gulcehre et al, 2015).
A simpler al-ternative which we adopt is to use n-gram featurescollected from the document to rerank candidatesummaries obtained via beam decoding.
We incor-porate the features in a log-linear reranker whosefeature weights are optimized with minimum errorrate training (Och, 2003).5 Experimental SetupIn this section we present our experimental setupfor assessing the performance of our summariza-tion models.
We discuss the datasets used for5A simpler model would use hard attention to select a sen-tence first and then a few words from it as a summary, but thiswould render the system non-differentiable for training.
Al-though hard attention can be trained with the REINFORCEalgorithm (Williams, 1992), it requires sampling of discreteactions and could lead to high variance.6We empirically found that feeding the previous sentence-level attention vector as additional input to the LSTM wouldlead to small performance improvements.
This is not shownin the equation.training and evaluation, give implementation de-tails, briefly introduce comparison models, and ex-plain how system output was evaluated.Datasets We trained our sentence- and word-based summarization models on the two datasetscreated from DailyMail news.
Each dataset wassplit into approximately 90% for training, 5% forvalidation, and 5% for testing.
We evaluated themodels on the DUC-2002 single document sum-marization task.
In total, there are 567 documentsbelonging to 59 different clusters of various newstopics.
Each document is associated with two ver-sions of 100-word7manual summaries producedby human annotators.
We also evaluated our mod-els on 500 articles from the DailyMail test set(with the human authored highlights as goldstan-dard).
We sampled article-highlight pairs so thatthe highlights include a minimum of 3 sentences.The average byte count for each document is 278.Implementation Details We trained our mod-els with Adam (Kingma and Ba, 2014) with ini-tial learning rate 0.001.
The two momentum pa-rameters were set to 0.99 and 0.999 respectively.We performed mini-batch training with a batchsize of 20 documents.
All input documents werepadded to the same length with an additional maskvariable storing the real length for each document.The size of word, sentence, and document em-beddings were set to 150, 300, and 750, respec-tively.
For the convolutional sentence model, wefollowed Kim et al (2016)8and used a list of ker-nel sizes {1, 2, 3, 4, 5, 6, 7}.
For the recurrent doc-ument model and the sentence extractor, we usedas regularization dropout with probability 0.5 onthe LSTM input-to-hidden layers and the scoringlayer.
The depth of each LSTM module was 1.All LSTM parameters were randomly initializedover a uniform distribution within [-0.05, 0.05].The word vectors were initialized with 150 dimen-sional pre-trained embeddings.9Proper nouns pose a problem for embedding-based approaches, especially when these are rare7According to the DUC2002 guidelines http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html, the generated summary should be within 100words.8The CNN-LSTM architecture is publicly available athttps://github.com/yoonkim/lstm-char-cnn.9We used the word2vec (Mikolov et al, 2013) skip-grammodel with context window size 6, negative sampling size 10and hierarchical softmax 1.
The model was trained on theGoogle 1-billion word benchmark (Chelba et al, 2014).489or unknown (e.g., at test time).
Rush et al (2015)address this issue by adding a new set of featuresand a log-linear model component to their sys-tem.
As our model enjoys the advantage of gener-ation by extraction, we can force the model to in-spect the context surrounding an entity and its rel-ative position in the sentence in order to discoverextractive patterns, placing less emphasis on themeaning representation of the entity itself.
Specif-ically, we perform named entity recognition withthe package provided by Hermann et al (2015)and maintain a set of randomly initialized entityembeddings.
During training, the index of the en-tities is permuted to introduce some noise but alsorobustness in the data.
A similar data augmenta-tion approach has been used for reading compre-hension (Hermann et al, 2015).A common problem with extractive methodsbased on sentence labeling is that there is no con-straint on the number of sentences being selectedat test time.
We address this by reranking the posi-tively labeled sentences with the probability scoresobtained from the softmax layer (rather than thelabel itself).
In other words, we are more inter-ested in is the relative ranking of each sentencerather than their exact scores.
This suggests thatan alternative to training the network would be toemploy a ranking-based objective or a learning torank algorithm.
However, we leave this to futurework.
We use the three sentences with the highestscores as the summary (also subject to the word orbyte limit of the evaluation protocol).Another issue relates to the word extractionmodel which is challenging to batch since eachdocument possesses a distinct vocabulary.
Wesidestep this during training by performing neg-ative sampling (Mikolov et al, 2013) which trimsthe vocabulary of different documents to the samelength.
At each decoding step the model is trainedto differentiate the true target word from 20 noisesamples.
At test time we still loop through thewords in the input document (and a stop-word list)to decide which word to output next.System Comparisons We compared the outputof our models to various summarization meth-ods.
These included the standard baseline of sim-ply selecting the ?leading?
three sentences fromeach document as the summary.
We also builta sentence extraction baseline classifier using lo-gistic regression and human engineered features.The classifier was trained on the same datasetsas our neural network models with the follow-ing features: sentence length, sentence position,number of entities in the sentence, sentence-to-sentence cohesion, and sentence-to-document rel-evance.
Sentence-to-sentence cohesion was com-puted by calculating for every document sentenceits embedding similarity with every other sentencein the same document.
The feature was the nor-malized sum of these similarity scores.
Sentenceembeddings were obtained by averaging the con-stituent word embeddings.
Sentence-to-documentrelevance was computed similarly.
We calculatedfor each sentence its embedding similarity with thedocument (represented as bag-of-words), and nor-malized the score.
The word embeddings used inthis baseline are the same as the pre-trained onesused for our neural models.In addition, we included a neural abstractivesummarization baseline.
This system has a similararchitecture to our word extraction model exceptthat it uses an open vocabulary during decoding.It can also be viewed as a hierarchical document-level extension of the abstractive sentence summa-rizer proposed by Rush et al (2015).
We trainedthis model with negative sampling to avoid the ex-cessive computation of the normalization constant.Finally, we compared our models to three previ-ously published systems which have shown com-petitive performance on the DUC2002 single doc-ument summarization task.
The first approach isthe phrase-based extraction model of Woodsendand Lapata (2010).
Their system learns to producehighlights from parsed input (phrase structuretrees and dependency graphs); it selects salientphrases and recombines them subject to length,coverage, and grammar constraints enforced viainteger linear programming (ILP).
Like ours, thismodel is trained on document-highlight pairs, andproduces telegraphic-style bullet points rather thanfull-blown summaries.
The other two systems,TGRAPH (Parveen et al, 2015) and URANK (Wan,2010), produce more typical summaries and repre-sent the state of the art.
TGRAPH is a graph-basedsentence extraction model, where the graph is con-structed from topic models and the optimizationis performed by constrained ILP.
URANK adopts aunified ranking system for both single- and multi-document summarization.Evaluation We evaluated the quality of thesummaries automatically using ROUGE (Lin andHovy, 2003).
We report unigram and bigram over-490DUC 2002 ROUGE-1 ROUGE-2 ROUGE-LLEAD 43.6 21.0 40.2LREG 43.8 20.7 40.3ILP 45.4 21.3 42.8NN-ABS 15.8 5.2 13.8TGRAPH 48.1 24.3 ?URANK 48.5 21.5 ?NN-SE 47.4 23.0 43.5NN-WE 27.0 7.9 22.8DailyMail ROUGE-1 ROUGE-2 ROUGE-LLEAD 20.4 7.7 11.4LREG 18.5 6.9 10.2NN-ABS 7.8 1.7 7.1NN-SE 21.2 8.3 12.0NN-WE 15.7 6.4 9.8Table 1: ROUGE evaluation (%) on the DUC-2002and 500 samples from the DailyMail.lap (ROUGE-1,2) as a means of assessing infor-mativeness and the longest common subsequence(ROUGE-L) as a means of assessing fluency.In addition, we evaluated the generated sum-maries by eliciting human judgments for 20 ran-domly sampled DUC 2002 test documents.
Par-ticipants were presented with a news article andsummaries generated by a list of systems.
Theseinclude two neural network systems (sentence-and word-based extraction), the neural abstrac-tive system described earlier, the lead baseline, thephrase-based ILP model10of Woodsend and La-pata (2010), and the human authored summary.Subjects were asked to rank the summaries frombest to worst (with ties allowed) in order of in-formativeness (does the summary capture impor-tant information in the article?)
and fluency (is thesummary written in well-formed English?).
Weelicited human judgments using Amazon?s Me-chanical Turk crowdsourcing platform.
Partici-pants (self-reported native English speakers) saw2 random articles per session.
We collected 5 re-sponses per document.6 ResultsTable 1 (top half) summarizes our results on theDUC 2002 test dataset using ROUGE.
NN-SErepresents our neural sentence extraction model,10We are grateful to Kristian Woodsend for giving us ac-cess to the output of his system.
Unfortunately, we do nothave access to the output of TGRAPH or URANK for inclusionin the human evaluation.Models 1st2nd3rd4th5th6thMeanRLEAD 0.10 0.17 0.37 0.15 0.16 0.05 3.27ILP 0.19 0.38 0.13 0.13 0.11 0.06 2.77NN-SE 0.22 0.28 0.21 0.14 0.12 0.03 2.74NN-WE 0.00 0.04 0.03 0.21 0.51 0.20 4.79NN-ABS 0.00 0.01 0.05 0.16 0.23 0.54 5.24Human 0.27 0.23 0.29 0.17 0.03 0.01 2.51Table 2: Rankings (shown as proportions) andmean ranks given to systems by human partici-pants (lower is better).NN-WE our word extraction model, and NN-ABSthe neural abstractive baseline.
The table also in-cludes results for the LEAD baseline, the logisticregression classifier (LREG), and three previouslypublished systems (ILP, TGRAPH, and URANK).The NN-SE outperforms the LEAD and LREGbaselines with a significant margin, while per-forming slightly better than the ILP model.
Thisis an encouraging result since our model hasonly access to embedding features obtained fromraw text.
In comparison, LREG uses a set ofmanually selected features, while the ILP systemtakes advantage of syntactic information and ex-tracts summaries subject to well-engineered lin-guistic constraints, which are not available to ourmodels.
Overall, our sentence extraction modelachieves performance comparable to the state ofthe art without sophisticated constraint optimiza-tion (ILP, TGRAPH) or sentence ranking mech-anisms (URANK).
We visualize the sentenceweights of the NN-SE model in the top half of Fig-ure 4.
As can be seen, the model is able to locatetext portions which contribute most to the overallmeaning of the document.ROUGE scores for the word extraction modelare less promising.
This is somewhat expectedgiven that ROUGE is n-gram based and not verywell suited to measuring summaries which containa significant amount of paraphrasing and may de-viate from the reference even though they expresssimilar meaning.
However, a meaningful com-parison can be carried out between NN-WE andNN-ABS which are similar in spirit.
We observethat NN-WE consistently outperforms the purelyabstractive model.
As NN-WE generates sum-maries by picking words from the original docu-ment, decoding is easier for this model comparedto NN-ABS which deals with an open vocabulary.The extraction-based generation approach is morerobust for proper nouns and rare words, whichpose a serious problem to open vocabulary mod-491sentence extraction:a gang of at least three people poured gasoline on a car that stopped to fill up at entity5 gas station early on Saturday morning and set the vehicle on firethe driver of the car, who has not been identified, said he got into an argument with the suspects while he was pumping gas at a entity13 in entity14the group covered his white entity16 in gasoline and lit it ablaze while there were two passengers insideat least three people poured gasoline on a car and lit it on fire at a entity14 gas station explosive situationthe passengers and the driver were not hurt during the incident but the car was completely ruinedthe man?s grandmother said the fire was lit after the suspects attempted to carjack her grandson, entity33 reportedshe said:?
he said he was pumping gas and some guys came up and asked for the car?
they pulled out a gun and he took off running?
they took the gas tank and started spraying?
no one was injured during the fire , but the car ?s entire front end was torched , according to entity52the entity53 is investigating the incident as an arson and the suspects remain at largesurveillance video of the incident is being used in the investigationbefore the fire , which occurred at 12:15am on Saturday , the suspects tried to carjack the man hot casethe entity53 is investigating the incident at the entity67 station as an arsonword extraction:gang poured gasoline in the car, entity5 Saturday morning.
the driver argued with the suspects.
his grandmother said the fire was lit by the suspects attempted tocarjack her grandson.entities:entity5:California entity13:76-Station entity14: South LA entity16:Dodge Charger entity33:ABC entity52:NBC entity53:LACFD entity67:LA76Figure 4: Visualization of the summaries for a DailyMail article.
The top half shows the relative attentionweights given by the sentence extraction model.
Darkness indicates sentence importance.
The lower halfshows the summary generated by the word extraction.els.
An example of the generated summaries forNN-WE is shown at the lower half of Figure 4.Table 1 (lower half) also shows system resultson the 500 DailyMail news articles (test set).
Ingeneral, we observe similar trends to DUC 2002,with NN-SE performing the best in terms of allROUGE metrics.
Note that scores here are gener-ally lower compared to DUC 2002.
This is dueto the fact that the gold standard summaries (akahighlights) tend to be more laconic and as a resultinvolve a substantial amount of paraphrasing.The results of our human evaluation study areshown in Table 2.
Specifically, we show, propor-tionally, how often our participants ranked eachsystem 1st, 2nd, and so on.
Perhaps unsurpris-ingly, the human-written descriptions were con-sidered best and ranked 1st 27% of the time, how-ever closely followed by our NN-SE model whichwas ranked 1st 22% of the time.
The ILP systemwas mostly ranked in 2nd place (38% of the time).The rest of the systems occupied lower ranks.
Wefurther converted the ranks to ratings on a scale of1 to 6 (assigning ratings 6. .
.1 to rank placements1.
.
.6).
This allowed us to perform Analysis ofVariance (ANOVA) which revealed a reliable ef-fect of system type.
Specifically, post-hoc Tukeytests showed that NN-SE and ILP are significantly(p < 0.01) better than LEAD, NN-WE, and NN-ABSbut do not differ significantly from each other orthe human goldstandard.7 ConclusionsIn this work we presented a data-driven summa-rization framework based on an encoder-extractorarchitecture.
We developed two classes of mod-els based on sentence and word extraction.
Ourmodels can be trained on large scale datasets andlearn informativeness features based on continu-ous representations without recourse to linguisticannotations.
Two important ideas behind our workare the creation of hierarchical neural structuresthat reflect the nature of the summarization taskand generation by extraction.
The later effectivelyenables us to sidestep the difficulties of generat-ing under a large vocabulary, essentially coveringthe entire dataset, with many low-frequency wordsand named entities.Directions for future work are many and var-ied.
One way to improve the word-based modelwould be to take structural information into ac-count during generation, e.g., by combining it witha tree-based algorithm (Cohn and Lapata, 2009).
Itwould also be interesting to apply the neural mod-els presented here in a phrase-based setting similarto Lebret et al (2015).
A third direction would beto adopt an information theoretic perspective anddevise a purely unsupervised approach that selectssummary sentences and words so as to minimizeinformation loss, a task possibly achievable withthe dataset created in this work.AcknowledgmentsWe would like to thank three anonymous review-ers and members of the ILCC at the School of In-formatics for their valuable feedback.
The supportof the European Research Council under awardnumber 681760 ?Translating Multiple Modalitiesinto Text?
is gratefully acknowledged.492ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings ofICLR 2015, San Diego, California.Michele Banko, Vibhu O. Mittal, and Michael J. Wit-brock.
2000.
Headline generation based on statis-tical translation.
In Proceedings of the 38th ACL,pages 318?325, Hong Kong.Samy Bengio, Oriol Vinyals, Navdeep Jaitly, andNoam Shazeer.
2015.
Scheduled sampling for se-quence prediction with recurrent neural networks.In Advances in Neural Information Processing Sys-tems 28, pages 1171?1179.
Curran Associates, Inc.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2014.
One billion word benchmark for measur-ing progress in statistical language modeling.
arXivpreprint arXiv:1312.3005.Trevor Anthony Cohn and Mirella Lapata.
2009.
Sen-tence compression as tree transduction.
Journal ofArtificial Intelligence Research, pages 637?674.Conroy and O?Leary.
2001.
Text summarization viahidden Markov models.
In Proceedings of the 34thAnnual ACL SIGIR, pages 406?407, New Oleans,Louisiana.G?unes?
Erkan and Dragomir R. Radev.
2004.
Lex-pagerank: Prestige in multi-document text summa-rization.
In Proceedings of the 2004 EMNLP, pages365?371, Barcelona, Spain.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based extractive summarization.
In Stan Sz-pakowicz Marie-Francine Moens, editor, Text Sum-marization Branches Out: Proceedings of the ACL-04 Workshop, pages 104?111, Barcelona, Spain.Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.Li.
2016.
Incorporating copying mechanism insequence-to-sequence learning.
In Proceedings ofthe 54th ACL, Berlin, Germany.
to appear.Caglar Gulcehre, Orhan Firat, Kelvin Xu, KyunghyunCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2015.
Onusing monolingual corpora in neural machine trans-lation.
arXiv preprint arXiv:1503.03535.Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,Bowen Zhou, and Yoshua Bengio.
2016.
Point-ing the unknown words.
In Proceedings of the 54thACL, Berlin, Germany.
to appear.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems 28, pages 1684?1692.
Curran Associates, Inc.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentconvolutional neural networks for discourse compo-sitionality.
In Proceedings of the Workshop on Con-tinuous Vector Space Models and their Composition-ality, pages 119?126, Sofia, Bulgaria.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush.
2016.
Character-aware neural lan-guage models.
In Proceedings of the 30th AAAI,Phoenix, Arizon.
to appear.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014EMNLP, pages 1746?1751, Doha, Qatar.Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Hayato Kobayashi, Masaki Noguchi, and Taichi Yat-suka.
2015.
Summarization based on embeddingdistributions.
In Proceedings of the 2015 EMNLP,pages 1984?1989, Lisbon, Portugal.Julian Kupiec, Jan O. Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of the 18th Annual International ACM SI-GIR, pages 68?73, Seattle, Washington.R?emi Lebret, Pedro O Pinheiro, and Ronan Collobert.2015.
Phrase-based image captioning.
In Proceed-ings of the 32nd ICML, Lille, France.Chin-Yew Lin and Eduard H. Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLTNAACL, pages 71?78, Edmonton, Canada.Rada Mihalcea.
2005.
Language independent extrac-tive summarization.
In Proceedings of the ACL In-teractive Poster and Demonstration Sessions, pages49?52, Ann Arbor, Michigan.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.
Curran Associates,Inc.Ani Nenkova, Lucy Vanderwende, and Kathleen McK-eown.
2006.
A compositional context sensitivemulti-document summarizer: exploring the factorsthat influence summarization.
In Proceedings of the29th Annual ACM SIGIR, pages 573?580, Washing-ton, Seattle.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st ACL, pages 160?167, Sapporo, Japan.493Daraksha Parveen, Hans-Martin Ramsl, and MichaelStrube.
2015.
Topical coherence for graph-basedextractive summarization.
In Proceedings of the2015 EMNLP, pages 1949?1954, Lisbon, Portugal,September.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, et al 2004.
Mead-a platform for mul-tidocument multilingual text summarization.
Tech-nical report, Columbia University Academic Com-mons.Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015EMNLP, pages 379?389, Lisbon, Portugal.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Pro-cessing Systems 27, pages 3104?3112.
Curran As-sociates, Inc.Krysta Svore, Lucy Vanderwende, and ChristopherBurges.
2007.
Enhancing single-document sum-marization by combining RankNet and third-partysources.
In Proceedings of the 2007 EMNLP-CoNLL, pages 448?457, Prague, Czech Republic.Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.2015.
Pointer networks.
In Advances in Neural In-formation Processing Systems 28, pages 2674?2682.Curran Associates, Inc.Xiaojun Wan.
2010.
Towards a unified approach tosimultaneous single-document and multi-documentsummarizations.
In Proceedings of the 23rd COL-ING, pages 1137?1145.Ronald J Williams.
1992.
Simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
Machine learning, 8(3-4):229?256.Kristian Woodsend and Mirella Lapata.
2010.
Auto-matic generation of story highlights.
In Proceedingsof the 48th ACL, pages 565?574, Uppsala, Sweden.Dani Yogatama, Fei Liu, and Noah A. Smith.
2015.Extractive summarization by maximizing semanticvolume.
In Proceedings of the 2015 EMNLP, pages1961?1966, Lisbon, Portugal.Xingxing Zhang and Mirella Lapata.
2014.
Chinesepoetry generation with recurrent neural networks.In Proceedings of 2014 EMNLP, pages 670?680,Doha, Qatar.494
