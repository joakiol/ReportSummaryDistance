Using Semantics in Non-Context-Free Parsingof Montague Grammar 1David Scott WarrenDepartment of Computer ScienceSUNY at Stony BrookLong Island, NY 11794Joyce FriedmanUniversity of MichiganAnn Arbor, MIIn natural language processing, the question of the appropriate interaction of syntaxand semantics during sentence analysis has long been of interest.
Montague grammar withits fully formalized syntax and semantics provides a complete, well-defined context in whichthese questions can be considered.
This paper describes how semantics can be used duringparsing to reduce the combinatorial explosion of syntactic ambiguity in Montague grammar.A parsing algorithm, called semantic equivalence parsing, is presented and examples of itsoperation are given.
The algorithm is applicable to general non-context-free grammarsthat include a formal semantic component.
The second portion of the paper placessemantic equivalence parsing in the context of the very general definition of an interpretedlanguage as a homomorphism between syntactic and semantic algebras (Montague 1970).IntroductionThe close interrelation between syntax and seman-tics in Montague grammar provides a good frameworkin which to consider the interaction of syntax andsemantics in sentence analysis.
Several different ap-proaches are possible in this framework and they canbe developed rigorously for comparison.
In this paperwe develop an approach called semantic equivalenceparsing that introduces logical translation into the on-going parsing process.
We compare this with our ear-lier directed process implementation i which syntacticparsing is completed prior to translation to logicalform.Part I of the paper gives an algorithm that parses aclass of grammars that contains both essentiallycontext-free rules and non-context- free rules as inMontague's 1973 PTQ.
Underlying this algorithm is a1 A preliminary version of this paper was presented at thesymposium on Modelling Human Parsing Strategies at the Universi-ty of Texas at Austin, March 24-26, 1981.
The work of the firstauthor was supported in part by NSF grant IST 80-10834.nondeterministic syntactic program expressed as anATN.
The algorithm introduces equivalence parsing,which is a general execution method for nondetermin-istic programs that is based on a recall table, a gener-alization of the well-formed substring table.
Semanticequivalence, based on logical equivalence of formulasobtained as translations, is used.
We discuss the con-sequences of incorporating semantic processing intothe parser and give examples of both syntactic andsemantic parsing.
In Part II the semantic parsing al-gorithm is related to earlier tabular context-free recog-nition methods.
Relating our algorithm to its prede-cessors gives a new way of viewing the technique.The algorithmic description is then replaced by a de-scription in terms of refined grammars.
Finally wesuggest how this notion might be generalized to thefull class of Montague grammars.The particular version of Montague grammar usedhere is that of PTQ, with which the reader is assumedto be conversant.
The syntactic component of PTQ isan essentially context- free grammar, augmented bysome additional rules of a different form.
The non-Copyright 1982 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or  specific permission.0362-613X/82/030123-16503.00American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 123David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsingcontext-free aspects arise in the treatment of quantifi-er scope and pronouns and their antecedents.
Syntac-tically each antecedent is regarded as substituted intoa place marked by a variable.
This is not unlike theway fillers are inserted into gaps in Gazdar 's  1979treatment.
However,  Montague's  use of variablesallows complicated interactions between differentvariable-antecedent pairs.
Each substitution rule sub-stitutes a term phrase (NP) for one or more occurrenc-es of a free variable in a phrase (which may be a sen-tence, common noun phrase, or intransitive verbphrase).
The first occurrence of the variable is re-placed by the phrase; later occurrences are replaced byappropriate pronouns.
The translation of the resultingphrase expresses the coreferential ity of the nounphrase and the pronouns.
With substitution, but with-out pronouns, the only function of substitution is todetermine quantifier scope.Directed Process  ApproachOne computational approach to processing a sen-tence is the directed process approach, which is a se-quential analysis that follows the three-part presenta-tion in PTQ.
The three steps are as follows.
A purelysyntactic analysis of a sentence yields a set of parsetrees, each an expression in the disambiguated lan-guage.
Each parse tree is then translated byMontague's rules into a formula of intentional ogic towhich logical reductions are immediately applied.
Thereduced formulas can then be interpreted in a model.The directed process approach is the one taken in thesystem described by Friedman, Moran, and Warren1978a,b.Semantic equivalence parsing is motivated by theobservation that the directed process approach, inwhich all of the syntactic processing is completed be-fore any semantic processing begins, does not takemaximal advantage of the coupling of syntax and se-mantics in Montague grammars.
Compositionality andthe fact that for each syntactic rule there is a transla-tion rule suggest that it would be possible to do acombined syntactic-semantic parse.
In this approach,as soon as a subphrase is parsed, its logical formula isobtained and reduced to an extensionalized normalform.
Two parses for the same phrase can then beregarded equivalent if they have the same formula.The approach to parsing suggested by Cooper 's1975 treatment of quantified noun phrases is like oursemantic equivalence parsing in storing translations asone element of the tuple corresponding to a nounphrase.
Cooper's  approach differs from the approachfol lowed here because he has an intermediate stagethat might be called an "autonomous yntax tree".The frontier of the tree is the sentence; the scope ofthe quantifier of a noun phrase is not yet indicated.Cooper 's  approach has been fol lowed by the GPSGsystem (Gawron et al 1982) and by Rosenschein andShieber 1982.
Neither of those systems treats pro-nouns.
In Montague's  approach, which we followhere, the trees produced by the parser are expressionsin the disambiguated language, so scope is determined,pronoun antecedents are indicated, and each tree has aunique (unreduced) translation.
The descriptions ofthe systems that use Cooper 's  approach seem to implythat they use a second pass over the syntax tree todetermine the actual quantifier scopes in the final logi-cal forms.
Were these systems to use a single pass toproduce the final logical forms, the results described inthis paper would be directly applicable.1.
Equivalence ParsingAmbigui tyAmbiguity in Montague grammar is measured bythe number of different meanings.
In this view syn-tactic structure is of no interest in its own right, butonly as a vehicle for mapping semantics.
Syntacticambiguity does not directly correspond to semanticambiguity, and there may be many parses with thesame semantic interpretation.
Further, sentences withscope ambiguity, such as A man loves every woman,require more than one parse, because the syntacticderivation determines quantifier scope.In PTQ there is infinite syntactic ambiguity arisingfrom three sources: alphabetic variants of variables,variable for variable substitutions, and vacuous varia-ble substitution.
However,  these semantically unnec-essary constructs can be eliminated, so that the set ofsyntactic sources for any sentence is finite, and a par-ser that finds the full set is possible.
(This corre-sponds to the "variable principle" enunciated by Jans-sen 1980 and used by Landsbergen 1980.)
This ap-proach was the basis of our earlier PTQ parser(Friedman and Warren 1978).However,  even with these reductions the number ofremaining parses for a sentence of reasonable com-plexity is still large compared to the number of non-equivalent translations.
In the directed process ap-proach this is treated by first finding all the parses,next finding for each parse a reduced translation, andthen finally obtaining the set of reduced translations.Each reduced translation may, but does not necessari-ly, represent a different sentence meaning.
No mean-ings are lost.
Further reductions of the set of transla-tions would be possible, but the undecidability of logi-cal equivalence precludes algorithmic reduction to aminimal set.The ATN ProgramIn the underlying parser the grammar is expressedas an augmented transition network (ATN) (Woods1973).
Both the syntactic and the semantic parsersuse this same ATN.
The main difficulty in construct-124 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsinging the ATN was, as usual, the non-context-free as-pects of the grammar, in particular the incorporationof a treatment of substitution rules and variables.
Thegrammar given in PTQ generates infinitely many deriv-ations for each sentence.
All but finitely many ofthese are unnecessary variations on variables and wereeliminated in the construction of the ATN.
The ATNrepresents only the reduced set of structures, and musttherefore be more complex.Equivalence Test ingIn order to say what we mean by semantic equiva-lence parsing, we use Harel 's 1979 notion of executionmethod for nondeterministic programs.
An executionmethod is a deterministic procedure for finding thepossible execution paths through a nondeterministicprogram given an input.
For an ATN, these executionpaths correspond to different parses.
Viewing parsingin this way, the only difference between the usualsyntactic parsing and semantic equivalence parsing is adifference in the execution method.
As will be seen,semantic equivalence parsing uses semantic tests aspart of the execution method.We call the execution method we use to process ageneral ATN equivalence parsing (Warren 1979).Equivalence parsing is based on a recall table.
Therecall table is a set of buckets used to organize andhold partial syntactic structures while larger ones areconstructed.
Equivalence parsing can be viewed asprocessing an input sentence and the ATN to defineand fill in the buckets of the recall table.
The use ofthe recall table reduces the amount of redundant proc-essing in parsing a sentence.
Syntactic structuresfound along one execution path through the ATN neednot be reconstructed but can be directly retrieved fromthe recall table and used on other paths.
The recalltable is a generalization of the familiar wel l - formedsubstring table (WFST) to arbitrary programs thatcontain procedure calls.
Use of the WFST in ATNparsing is noted in Woods 1973 and Bates 1978.Bates observes that the WFST is complicated by theHOLDs and SENDRs in the ATN.
These are the ATNactions that correspond to parameter passing in proce-dures and are required in the ATN for PTQ to correctlytreat the substitution rules.In the Woods system the WFST is viewed as a pos-sible optimization, to be turned on when it improvesparsing efficiency.
In our system the recall table is anintrinsic part of the parsing algorithm.
Because anyATN that naturally represents PTQ must contain leftrecursion, the usual depth-f irst (or breadth-f irst orbest-f irst) ATN parsing algorithm would go into aninfinite loop when trying to find all the parses of anysentence.
The use of the recall table in equivalenceparsing handles left-recursive ATNs without specialconsideration (Warren 1981).
As a result there is noneed to rewrite the grammar to eliminate left-recursiverules as is usually necessary.In a general nondeterministic program, a bucket inthe recall table corresponds to a particular subroutineand a set of values for the calling parameters and re-turn parameters.
For an ATN a bucket is indexed by atriple: (1) a grammatical category, that is, a subnet towhich a PUSH is made, (2) the contents of the SENDRregisters at the PUSH and the current string, and (3)the contents of the LIFTR registers at the POP and thethen-current string.
A bucket contains the membersof an equivalence class of syntactic structures; precise-ly what they are depends on what type of equivalenceis being used.What makes equivalence parsing applicable to non-context-free grammars is that its buckets are moregeneral than the cells in the standard tabular context-free algorithms.
In the C-K-Y algorithm (Kasami1965), for example, a cell is indexed only by the start-ing position and the length of the parsed segment, i.e.,the current string at PUSH and POP.
The cell contentsare nonterminals.
In our case all three are part of thebucket index, which also includes SENDR and LIFTRregister values.
The bucket contents are equivalenceclasses of structures.Sentence  Recogni t ionFor sentence recognition all parses are equivalent.So it is enough to determine, for each bucket of therecall table, whether or not it is empty.
A sentence isin the language if the bucket corresponding to thesentence category (with empty SENDR registers andfull string, and empty LIFTR registers and null string)is nonempty.
The particular forms of the syntacticstructures in the bucket are irrelevant; the contents ofthe buckets are only a superfluous record of the spe-cific syntactic structures.
The syntactic structure isnever tested and so does not affect the flow of con-trol.
Thus which buckets are nonempty depends onlyon what other buckets are nonempty and not on whatthose other buckets contain.
For sentence recognition,when the execution method constructs a new memberof a bucket that is already nonempty, it may or maynot add the new substructure, but it does not need touse it to construct any larger syntactic structures.
Thisis because the earlier member has already verified thisbucket as nonempty.
Therefore this fact is alreadyknown and is already being used to determine thenonemptiness of other buckets.
To find all parses,however, equivalence parsing does use all members ofeach bucket to construct larger structures.It would be possible first to do recognition anddetermine all the nonempty buckets in the recall table,and then to go back and take all variants of one singleparse that can be obtained by replacing any substruc-ture .by  another substructure from the same bucket.American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 125David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingThis is essentially how the context-free parsing algor-ithms constructed from the tabular recognition me-thods work.
This is not how the equivalence parsingalgorithm works.
When it obtains a substructure, itimmediately tries to use it to construct larger struc-tures.The difference described above between sentencerecognition and sentence parsing is a difference onlyin the execution methods used to execute the ATN andnot in the ATN itself.
This difference is in the test forequivalence of bucket contents.
In sentence recogni-tion any two syntactic structures in a bucket are equiv-alent since we only care whether or not the substringcan be parsed to the given category.
At the otherextreme, in finding all parses, two entries are equiva-lent only if they are the identical structure.
For mostreasonable ATNs, including our ATN for PTQ, thiswould not happen; distinct paths lead to distinct struc-tures.Semantic parsing is obtained by aga inmodi fy ingonly the equivalence test used in the execution methodto test bucket contents.
For semantic parsing twoentries are equivalent if their logical translations, afterlogical reduction and extensionalization, are identicalto within change of bound variable.Small GrammarFor our examples, we introduce in Figure 1 a smallsubnet of the ATN for PTQ.
Arcs with fully capital-ized labels are PUSH arcs; those with lower case labelsare CAT arcs.
Structure-building operations are indi-cated in parentheses.
This net implements just threerules of PTQ.
Rule $4 forms a sentence by concaten-ating a term phrase and an intransitive verb phrase;S l l  conjoins two sentences, and S14,i substitutes aterm phrase for the syntactic variable he i in a sen-tence.
$4 and S l l  are context-free rules; S14,i is oneof the substitution rules that make the grammar non-context-free and is basic to the handling of quantifiers,pronouns, and antecedents.
The ATN handles thesubstitution by using a LIFTR to carry the variable-binding information.
The LIFTR is not used for thecontext-free rules.ITETSTSIV ($4 TERM IVP)POP SENT l(S11 SENTI (S14,i TERM SENT)SENT2)TEbte i l l POP heiIVbiv POP IVPFigure 1.
Subnet of the ATN for PTQ,126 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingExample 1: Bill walksThe first example is the sentence Bill walks.
Thissentence has the obvious parse using only the context-free rule $4.
It also has the parse using the substitu-tion rule.
We will carry through the details of itsparse to show how this substitution rule is treated inthe parsing process.In the trace PUSHes and POPs in the syntactic anal-ysis of this sentence are shown.
The entries are inchronological order.
The PUSHes are numbered se-quentially for identification.
The PUSH number uni-quely determines a) the category to which the PUSH ismade, b) the remainder of the sentence being parsedat the time of the PUSH, and c) the contents of theSENDR registers at the time of the PUSH, called thePUSH environment.
At each POP a bucket and anelement in that bucket are returned.
The bucket nameat a POP is made up of the corresponding PUSH num-ber, the remaining input string, and the contents of theLIFTR registers, which are called the POP environ-ment.
The element in the bucket is the tree that isreturned.
For brevity we use in the trace only the firstletters of the words in the sentence; for example, Billwalks becomes Bw.Trace of Bill walksPUSH: Bucket: Contents:# CAT Str Env from Str Env Tree1 TS Bw null\[Parsing begins with a PUSH to the sentence category passing the entire string and an empty or null environment.\]2 TE Bw null\[In the sentence subnet we first PUSH to find a TE.\]2 w null Bill\[The TE subnet finds and POPs the term Bill to return from PUSH 2.\]3 IV w null3 e null walk1 e null ($4 Bill walk)\[Now since a tree is returned to the top level, covers the whole string, and the returned environment is null, thetree is printed.
The parses are always the trees in bucket 1-e-null.
The execution method now backs up; thereare no more POPs from PUSH 3; there is another from PUSH 2.\]2 w (he0 B) he0\[Continuing forward with the new environment...\]4 IV w (he0 B)4 e null walk\[Note that this is not the same bucket as on the previous PUSH 3 because the PUSH environments differ.\]1 e (he0 B) ($4 he0 walk)\[The tree has been returned and covers the whole string.
However, the returned environment is not null so theparse fails, and the execution method backs up to seek another eturn from PUSH 1.\]1 e null (S14,0 Bill ($4 he0 walk))\[This is another element in bucket 1-e-null;  it is a successful parse so it is printed out.
Execution continues butthere are no more parses of the sentence.\]DiscussionIn this trace bucket t -e -nu l l  is the only bucketwith more than one entry.
The execution method wassyntactic parsing, so each of the two entries was re-turned and printed out.
For recognition, these twoentries in the bucket would be considered the sameand the second would not have been POPped.
Insteadof continuing the computation up in the subnet fromwhich the PUSH was made, this path would be made tofail and the execution method would back up.
Forsemantic equivalence parsing, the bucket contentsthroughout would not be the syntax trees, but wouldinstead be their reduced extensionalized logical formu-las.
(Each such logical formula represents the equiva-lence class of the syntactic structures that correspondto the formula.)
For example, bucket 2-w-null wouldcontain ~,Pp{Ab} and bucket 3-c-null would containwalk'.
The first entry to bucket 1-E-null would be theformula for ($4 Bill walk), that is, walk . '
(b) .
Theentry to bucket 1-e-null  on the last line of the traceAmerican Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 127David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsingwould be the formula for (S14,0 Bill ($4 he0 walk)),which is also walk . '
(b) .
Therefore, this second entrywould not be POPped.Buckets also serve to reduce the amount of repeat-ed computation.
Suppose we have a second PUSH tothe same category with the same string and environ-ment as an earlier PUSH.
The buckets resulting fromthis new PUSH would come out to be the same as thebuckets from the earlier PUSH.
Therefore the bucketsneed not be recomputed; the results of the earlierbuckets can be used directly.
This is called a"FAKEPUSH" because we don't  actually do the PUSHto continue through the invoked subnet but simply doa "FAKEPOP" using the contents of the previouslycomputed buckets.Consider, as an example of FAKEPOP, the partialtrace of the syntactic parse of the sentence Bill walksand Mary runs (or Bw&Mr for short).
The initial partof this trace, through step 4, is essentially the same asthe trace above for the shorter sentence Bill walks.Trace of Bill walks and Mary runsPUSH: Bucket:# CAT Str Env from StrContents:Env Tree1 TS Bw&Mr null2 TE Bw null3 IV w null2 w&Mr null Bill3 &Mr null walk1 &Mr null ($4 Bill walk)\[A tree has been returned to the top level, but it does not cover the whole sentence, so the path fails and theexecution method backs up.\]2 w&Mr (he0 B) he04 IV w&Mr (he0 B)4 &Mr null walk1 &Mr (he0 B ($4 he0 walk)\[Again a tree has been returned to the top level, but it does not span the whole string, nor is the returnedenvironment null, so we fail.\]1 &Mr null (S14,0 Bill ($4 he0 walk))\[Again we are the top level; again we do not span the whole string, so again we fail.\]5 TS Bw&Mr null\[This is the second arc from the TS node of Figure 1.
The PUSH to TE (2 above) has completely failed.However, this PUSH, TS-Bw&Mr-null has been done before; it is PUSH 1.
We already have two buckets fromthat PUSH: 1-&Mr-null containing two trees, and 1-&Mr-(he0 B) with one tree.
There is no need to re-enter thissubnet; the buckets and their contents tell us what would happen.
Therefore we FAKEPOP one subtree and itsbucket and follow that computation to the end; later we will return to FAKEPOP the next one.\]1(5) &Mr null ($4 Bill walk)6 TS Mr null7 TE Mr null7 r null Mary\[This computation continues and parses the second half of this sentence.
Two parses are produced:(S11 ($4 Bill walk) ($4 Mary run)) and(S11 ($4 Bill walk) (S14,0 Mary ($4 he0 run)))After this, the execution method fails back to the FAKEPOP at PUSH 5, and another subtree from a bucket fromPUSH 2 is FAKEPOPped.\]1(5) &Mr null (S14,0 Bill (he0 walk))\[And the computation continues, eventually producing a total of ten parses for this sentence.\](In the earlier example of Bill  walks, theseFAKEPOPs are done, but their computations immedi-ately fail, because they are looking for a conjunctionbut are at the end of the sentence.
)128 American Journal  of Computat ional  Linguistics, Volume 8, Number 3-4, Ju ly-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingResults of ParsingThe sentence Bill walks and Mary runs has tensyntactic structures with respect to the PTQ grammar.The rules $4, S l l ,  and S14,i can be used in variousorders.
Figure 2 shows the ten different structures inthe order they are produced by the syntactic parser.The nodes in the trees of Figure 2 that are in italicsare the syntactic structures used for the first time.The nodes in standard type are structures used previ-ously, and thus either are part of an execution path incommon with an earlier parse, or are retrieved from abucket in the recall table to be used again.
Thus thenumber of italicized nodes measures in a crude waythe amount of work required to find all the parses forthis sentence.S l lII I$4 $4I ti i i IBill walk Mary runb.IBillI$4IIwalkS14,0 MaryI$11IIhe0I$4iIrune ,  S l l1I$4LI IBill walkIhe0IS14,0 MaryI$4IIrunI$4I Ihe0 walk$14,0 BillI$11ii$4fIMaryIrune.
S14,1 MaryI$14,0 BillI$11II$4II I Ihe0 walk helI I$4 $4I II I wJlk run he0S14,0 BillIS14,1 MaryIS l lII$4IIhelIrunFigure 2.
Ten parses for Bill walks and Mary runs,American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 129David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsingg.I$4II Ihe0 walkS14,0 BillI$11IIS14,1 MaryI$4II Ihel run$11II I$14,0 Bill $4i I $4I I MaryI IheO walkIrunS14,1 MaryI$11II IS14,0 Bill $4i I S4I I helI IheO walkrun$11Ihe0I IS14,0 Bill S14,1 MaryI I$4 $4I II I Iwalk he 1 runFigure 2. continuedExample of Semantic  Equivalence ParsingThis sentence, Bill walks and Mary runs, is one forwhich semantic parsing is substantially faster.
It isunambiguous; its only reduced extensionalized logicaltranslation is "wa lk , ' (b )&run , ' (m)" .
In the directedprocess parser, all ten trees of Figure 2 are found.They will all have the same translation.
In semanticparsing on'ly one is found.
Here the method works toadvantage because both parses of the initial string Billwalks result in the same environment for parsing Maryruns.
These two parses go into the same bucket soonly one needs to be used to construct larger struc-tfires.
We trace the example.PUSH: Bucket:# CAT Str Env from Str=Contents:Env Formula ?1 TS Bw&Mr null2 TE Bw null3 IV\[Fail\]4 IV\[Fail\]2w null312w&Mr (he0 B)41w&Mr null )tPp{Ab} y&Mr null walk' y&Mr null wa lk , ' (b )  yw&Mr (he0 B) hPP{x0} y&Mr null walk' y&Mr (he0 B) walk'(Vx0) y130 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing1 &Mr null wa lk , ' (b)  n\[This formula is the translation of the syntactic structure using S14,0 to substitute Bill into "he0 walks".
This isthe same bucket and the same translation as obtained at the return from 1 after PUSH 3 above, so we do not POP(indicated by the 'n'  in the final column), but instead fail back.\]5 TS Bw&Mr null\[FAKEPOP, since this is a repeat PUSH to this category with these parameters.
There are two buckets: 1-&Mr-null, which in syntactic parsing had two trees but now has only one translation, and bucket 1-&Mr-(he0 B) withone translation.
So we FAKEPOP 1-&Mr-null.\]1(5) &Mr null wa lk , ' (b)  y (FAKEPOP)6 TS Mr null7 TE Mr null7 r null y8 IV r null8 E null y6 e null y1 ~ null  y\[This is a successful parse.
The top level prints out the translation and then the execution method fails back.\]9 IV r (he0 M)Maryrun'run , ' (m)wa lk , ' (b )&run, ' (m)7 (he0 M) null ~PP{x0} y9 e null run' y6 E null run,'(Vx0) y1 E (he0 M) walk, ' (b)&run, '(Vx0) y\[Fail because we are at the top level and the environment is not null.\]1 ~ null wa lk , ' (b )&run, ' (m)  n\[Again we want to enter a translation into bucket 1-E-null.
This translation duplicates the one already there.it is not returned and we fail back.\]6 E null run , ' (m)  n\[This again duplicates a bucket and its contents, so we fail back to the second FAKEPOP from PUSH 5.use the other bucket: 1-&Mr-(he0 B).\]1(5) &Mr (he0 B) walk,'(Vx0) y (FAKEPOP)(he0 B)(he0 B)(he0 B)10 TS Mr11 r null11 TE Mr12 IV rSoNow we~P{xO} yrun' yrun , ' (m)  ywalk, ' (Vx0)&run, '(m) y12 ?
null10 e null1 E (he0 B)\[Fail at the top level since the environment is not null.\]1 e (he0 B) wa lk , ' (b )&run, ' (m)\[This duplicates a bucket and its contents, so we do not POP it but fail back.\]l l  r (hel M) ~PP{xl}13 e null10 e (hel M)1 e (he0 B)(hel M)13 IV r (he0 B)(hel M)run'run, ' (~xl )walk, '(~x0) &run,'(Vx 1 )(hel M) walk , ' (b)&run, ' (Vxl )null wa lk , ' (b )&run, ' (m)Y\[Fail at top level because nvironment is not null.\]1 c\[Fail at top level because environment is not null.\]1 e\[Duplicate bucket and translation, so fail.\]American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 131David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing1 c (he0 B)\[Duplicate bucket and translation, so fail.\]1 c null\[Duplicate, so fail.\]10 e null\[Duplicate, so fail.\]walk, ' (Vx0)&run, ' (m) nwa lk , ' (b )&run, ' (m)  nrun , ' (m)  nThis completes the trace of the semantic parse of the sentence.Results of ParsingFigure 3 displays in graphical form the syntacticstructures built during the semantic parsing of Billwalks and Mary runs traced above.
A horizontal lineover a particular node in a tree indicates that thetranslation of the structure duplicated a translationalready in its bucket, so no larger structures were builtusing it.
Only parse a) is a full parse of the sentenceand thus it is the only parse returned.
All the othersare aborted when they are found equivalent o earlierpartial results.
These points of abortion in the compu-tation are the points in the trace above at which a POPfails due to the duplication of a bucket and its con-tents.a.
S l lII I$4 $4I I1 I I IBill walk Mary runb.
S14,0 MaryS l lII 1$4 $4I II I I IBill walk he0 runS14,0 Mary$4II Ihe0 rund.
$14,0 Bill$11II I$4 $4I II I I Ihe0 walk Mary runFigure 3.
Semantic parses of Bill walks and Mary runs.132 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingI$4IIhe0e .
S14,1 MaryI$14,0 BillI$11II$4II I Iwalk he 1 runIhe0$14,0 BillI$14,1 MaryIS l lII I$4 $4I II Iwalk he 1Irung.
S14,1 MaryI$4II Ihe 1 runFigure 3. continuedNote that construction of parse c) is halted when atranslation is built that duplicates the translation ofthe right $4 subtree of parse a).
This corresponds tothe failure due to duplicate bucket contents in bucket6-E-null following PUSH 9 in the trace above.
Simi-larly parse g) is aborted before the entire tree is built.This corresponds to the failure in the final line of thetrace due to a duplicate translation in bucket10-E-null.
Semantic parses that would correspond tosyntactic parses h), i), and j) of Figure 2 are not con-sidered at all.
This is because bucket 1-&Mr-null con-tains two syntactic structures, but only one translation.Thus in semantic equivalence parsing we only do oneFAKEPOP for this bucket for PUSH 5.
In syntacticparsing the other parses are generated by theFAKEPOP of the other structure in this bucket.Reducing the Env i ronmentThe potential advantage of semantic equivalenceparsing derives from treating partial results as an equi-valence class in proceeding.
A partial result consistsof a structure, its extensionalized reduced translation,and a set of parameters of the parse to that point.These parameters are the environment for parsing thephrase.
Consider the sentence John loves Mary and itsparses:(1) ($4 John ($5 love Mary))(2) ($4 John (S16,0 Mary ($5 love he0)))(3) (S14,0 John ($4 (he0 ($5 love Mary)))(4) (S14,0 John ($4 he0 (S16,1 Mary($5 love he l ) ) ) )(plus 3 more)On reaching the phrase love Mary in parse (3) theparameters are not the same as they were at that pointin parse (1), because the pair (he0 John) is in theenvironment.
Thus the parser is not able to consultthe recall table and immediately return the alreadyparsed substructure.
Instead it must reparse love Maryin the new context.This environment problem arises because the ATNis designed to follow PTQ in treating pronouns by thenon-context- f ree substitution rules.
We have alsoconsidered, but have not to this point implemented,alternative ways of treating variables to make partialresults equal.
One way would be not to pass variablebindings down into lower nets at all.
Thus the PUSHenvironment would always be null.
Since these bind-ings are used to find the antecedent for a pronoun, theway antecedents are determined would have to bechanged.
An implementation might be as follows: Onencountering a pronoun during parsing, replace it by anew he-variable.
Then pass back up the tree informa-tion concerning both the variable number used and thepronoun's gender.
At a higher point in the tree, wherethe substitution rule is to be applied, a determinationcan be made as to which of the substituted termscould be the antecedent for the pronoun.
The variablenumber of the pronoun can then be changed to agreewith the variable number of its antecedent term by avariable-for-variable substitution.
Finally the substitu-tion rule can be used to substitute the term into thephrase for all occurrences of the variable.
Note thatthis alternative process would construct trees that dohave substitution rules to substitute variables for varia-American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 133David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsingbles, contrary to the variable principle mentionedabove.
We also note that with this modification apronoun is not associated with its antecedent when it isfirst encountered.
Instead the pronoun is saved and atsome later point in the parse the association is made.This revised treatment is related computationally tothat proposed in Cooper 1975.Evaluation of Semantic Equivalence ParsingThe question of the interaction of syntax and se-mantics in parsing was introduced early in computa-tional linguistics.
Winograd 1971 argued for the in-corporation of semantics as early as possible in therecognition process, in order to reduce the amount ofsyntactic processing that would be needed.
Partialparses that had no interpretation did not need to becontinued.
The alternative position represented byWoods's early work (Woods and Kaplan 1971) wasbasically the inverse: less semantic processing wouldbe needed if only completed parses were interpreted.This argument is based on the idea of eliminating un-interpretable parses as soon as possible.This advantage, if it is one, of integrated syntacticand semantic procedures does not occur here becausethe semantic aspect does not eliminate any logicalanalyses.
The translation of a structure to a formula isalways successful, so no partial parse is ever eliminat-ed for lack of a translation.
What happens instead isthat several partial parses are found to be equivalentbecause they have the same translation.
In this caseonly a representative of the set of partial parses needsto be carried forward.A further expansion of equivalence parsing wouldbe interpretation equivalence parsing.
Sentence process-ing would take place in the context of a specified mod-el.
Two structures would be regarded as equivalent ifthey had the same denotation in the model.
Morepartial structures would be found equivalent under theequivalence relation than under the reduce-extensionalize relation, and fewer structures wouldneed to be constructed.
Further, with the interpreta-tion equivalence relation, we might be able to use aninconsistent denotation to eliminate an incorrect par-tial parse.
For example, consider a sentence such asSandy and Pat are running and she is talking to him.
Inthis case, since the gender of Sandy and Pat cannot bedetermined syntactically, these words would have tobe marked in the lexicon with both genders.
Thiswould result in multiple logical formulas for this sen-tence, one for each gender assumption.
However,during interpretation equivalence parsing, the referentsfor Sandy and Pat would be found in the model andthe meaning with the incorrect coreference could berejected.Logical normal forms other than the reduced, ex-tensionalized form used above lead to other reasonableversions of equivalence parsing.
For example, wecould further process the reduced, extensionalizedform to obtain a prenex normal form with the matrixin clausal form.
We would use some standard conven-tions for naming variables, ordering sequences of thesame quantifier in the prefix, and ordering the literalsin the clauses of the matrix.
This would allow thealgorithm to eliminate, for example, multiple parsesarising from various equivalent scopes and orderings ofexistential quantifiers.The semantic equivalence processor has been im-plemented in Franz Lisp.
We have applied it to thePTQ grammar and tested it on various examples.
Forpurposes of comparison the directed process versionincludes syntactic parse, translation to logical formulaand reduction, and finally the reduction of the list offormulas to a set of formulas.
The mixed strategyyields exactly this set of formulas, with one parse treefor each.
Experiments with the combined parser andthe directed parser show that they take approximatelythe same time for reasonably simple sentences.
Formore complicated sentences the mixed strategy usuallyresults in less processing time and, in the best cases,results in about a 40 percent speed-up.
The distin-guishing characteristic of a string for which the me-thod yields the greatest speed-up is that the environ-ment resulting from parsing an initial segment is thesame for several distinct parses.The two parsing method we have described, thesequential process and the mixed process, were obvi-ously not developed with psychological modeling inmind.
The directed process version of the system canbe immediately rejected as a possible psychologicalmodel, since it involves obtaining and storing all thestructures for a sentence before beginning to interpretany one of them.
However, a reorganization of theprogramwould  make it possible to interpret eachstructure immediately after it is obtained.
This wouldhave the same cost in time as the first version, butwould not require storing all the parses.Although semantic equivalence parsing was devel-oped in the specific context of the grammar of PTQ, itis more general in its applicability.
The strict compos-itionality of syntax and semantics in PTQ is the mainfeature on which it depends.
The general idea of equi-valence parsing can be applied whenever syntacticstructure is used as an intermediate form and there is asyntax-directed translation to an output form on whichan equivalence relation is defined.2.
Input-Refined GrammarsWe now switch our point of view and examineequivalence parsing not in algorithmic terms but informal grammatical terms.
This will then lead intoshowing how equivalence parsing relates to Universal134 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingGrammar (UG) (Montague 1970).
The basic conceptto be used is an input-refined grammar.
We begin bydefining this concept for context- free grammars andusing it to relate the tabular context-free recognitionalgorithms of Earley 1970, Cocke-Kasami-Younger(Kasami 1965), and Sheil 1976 to each other andeventually to our algorithm.Given a context-free grammar G and a string s overthe terminal symbols of G, we define from G and s anew grammar Gs, called an input-refinement of G.This new grammar G s will bear a particular relation-ship to G: L(Gs) = {s}nL(G), i.e., L(Gs) is the single-ton set {s} if s is in L(G), and empty otherwise.
Fur-thermore, there is a direct one-to-one relationshipbetween the derivations of s in G and the derivationsof s in G s. Thus the problem of recognizing s in G isreduced to the problem of determining emptiness forthe grammar G s. Also, the problem of parsing s withrespect to the grammar G reduces to the problem ofexhaustive generation of the derivations of G s (there isat most one string).
Each of the tabular context-freerecognition algorithms can be viewed as implicitlydefining this grammar G s and testing it for emptiness.Emptiness testing is essentially done by reducing thegrammar, that is by eliminating useless symbols andproductions.
The table-constructing portion of a tabu-lar recognition algorithm, in effect, constructs andreduces the grammar Gs, thus determining whether ornot it is empty.
The tabular methods differ in theconstruction and reduction algorithm used.In each case, to turn a tabular recognition methodinto a parsing algorithm, the table must first be con-structed and then reprocessed to generate all the pars-es.
This corresponds to reprocessing the grammar Gs t,the result of reducing the grammar Gs, and using it toexhaustively generate all derivations in G s.Rather than formally defining G s from a context-free grammar G and a string s in the general case, weillustrate the definition by example.
The general defi-nition should be clear.Let G be the following context-free grammar:Terminals: {a,b}Nonterminals: {S}Start Symbol: SProductions: S-~S S aS-~bS~e(S produces the empty string)bba.
Gbb a is defined from G and Let s be the stringbba:Terminals:Nonterminals:{a,b}{al,a2,a3,b!,b2,b3,(t i for t a terminal of Gand 1 <i<lcngth(s))S123,512,51,S23,52,S 3,(A x for each nonterminal A of Gand each x a nonempty subse-quence of < 1,2,3 ..... length(s)>)S?,Sl,S2,S 3}(A i for each nonterminal A of Gand i, 0<i<length(s) )Start Symbol: S123Productions: \[from G production: S-~S S a\]S123~S12S2a3S123~S~S2a3S123~S Sl2a 3S 12"~ SIS a 2S12--~ SuSla2S 1 ~ S?S?a tS23--~ S~52a3$23 ~ S'S2a 3$2~$1Sla2$3~$2S2a3\[from G production: S-~b\]Sl~b 1S2~b 2$3--~ 3\[from G production: S-~ e\]S?-~ ?s ly ?$2~ ?$3~ ?\[for the terminals\]b l -~bb2- -ba3~aThese productions for G s were constructed by begin-ning with a production of G, adding a subscript or asuperscript o the nonterminal on the LHS to obtain anonterminal of Gs, adding single subscripts to all ter-minals and sequence subscripts to some nonterminalson the RHS so that the concatenation of all subscriptson the RHS equals the subscript on the LHS.
For theRHS nonterminals without subscripts, add the appro-priate subscript.
Also, to handle the terminals, foreach t i add the production T i~t  where t is the i th sym-bol in s.It is straightforward to show inductively that if anonterminal symbol generates any string at all it gen-erates exactly the substring of s that its subscript de-termines.
Symbols with superscripts generate the emp-ty string.
Also a parse tree of G s can be converted toa parse tree of G by first deleting all terminals (each isdominated by the same symbol with a subscript) andthen erasing all superscripts and subscripts on all sym-bols in the tree.
Conversely, any parse tree for s in Gcan be converted to a parse tree of s in G s by addingappropriate subscripts and superscripts to all the sym-bols of the tree and then adding the terminal symbolsat the leaves.American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 135David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingIt is clear that G s is not in general a reduced gram-mar.
G s can be reduced to Gs ~ by eliminating unpro-ductive and unreachable symbols and the rules involv-ing them.
Reducing the grammar will determinewhether or not L(Gs) is empty.
By the above discus-sion, this will determine whether s is in L(G), and thusan algorithm for constructing and reducing the refinedgrammar G s from G and s yields a recognition algor-ithm.
Also, given the reduced grammar Gs I, it isstraightforward, in light of the above discussion, togenerate all parses of s in G: simply exhaustively gen-erate the parse trees of Gs ~ and delete subscripts andsuperscripts.The tabular context- free recognition methods ofCocke-Kasami-Younger,  Earley, and Sheil can all beunderstood as variations of this general approach.
TheC-K-Y recognition algorithm uses the standard bottom-up method to determine emptiness of G s. It startswith the terminals and determines which G s nontermi-nals are productive, eventually finding whether or notthe start symbol is productive.
The matrix it con-structs is essentially the set of productive nonterminalsof G s.Sheil's well-formed substring table algorithm is themost obviously and directly related.
His simplest al-gorithm constructs the refined grammar and reduces ittop-down.
It uses a top-down control mechanism todetermine the productivity only of nonterminals thatare reachable from the start symbol.
The well-formedsubstring table again consists essentially of the reacha-ble, productive nonterminals of G s.Earley's recognition algorithm is more complicatedbecause it simultaneously constructs and reduces therefined grammar.
It can be viewed as manipulatingsets of subscripted nonterminals and sets of prod-uctions of G s. The items on the item lists, however,correspond quite directly to reachable, productivenonterminals of G s.The concept of input-refined grammar provides aunified view of the tabular context-free recognitionmethods.
Equivalence parsing as described in Part Iabove is also a tabular method, although it is notcontext-free.
It applies to context-free grammars andalso to some grammars such as PTQ that are notcontext-free.
We next relate it to the very generalclass of grammars defined by Montague in UG.Universal Grammar and Equivalence ParsingIn the following discussion of the problem of pars-ing in the general context of Montague's definitions ofa language (which might more naturally be called agrammar) and an interpretation, we assume the readeris familiar with the definitions in UG (Montague1970).
We begin with a formal definition of a refine-ment of a general disambiguated language.
A particu-lar type of refinement, input-refinement, leads to anequivalence parsing algorithm.
This generalizes theprocedure for input-refining a grammar shown abovefor the special case of a context-free grammar.
Wethen discuss the implications for equivalence parsing ofusing the formal interpretation of the language.
Final-ly we show how the ATN for PTQ and semantic equi-valence parsing fit into this general framework.Recall that a disambiguated language f~ = <A, Fv,X 8, S, 80>v~r ,~a can be regarded as consisting of analgebra <A,F~,>~,eF, with proper expressions A andoperations Fv, basic expressions X 8 for each categoryindex d eA, a set of syntactic rules S, and a sentencecategory index 80EA.
A language is a pair <~2,R>where ~2 is a disambiguated language and R is a binaryrelation with domain included in A.
Given a disambig-uated language~2 = <A, F~, X#, S, 80>~EF, ~EA,a disambiguated languagef~v = <A, F~,Xts,, S t , 80v>~,EF, 8'EA'is a refinement of 12 if there is a ref inement functiond:AW-.A from the category indices of fl '  to those of f~such that1) Xt~ _c Xd(8 , )  '2) If <F~,<~lW,~2t ..... 8n1>,St> E S v, then<F~,<d(81' ) ,d(~2' )  ..... d (~n ' )>,  d (8 ' )> E S',  and3) d(80')  = 60 .
(Note that the proper expressions A, the operat ionindexing set F, and the operations Fy of ~ and 12 ~ arethe same.
)The word refinement refers to the fact that thecatgories of lZ are split into finer categories.
Condi-tion 1 requires that the basic expressions of a refinedcategory come from the basic expressions of the cate-gory it refines.
Condit ion 2 requires that the newsyntactic rules be consistent with the old ones.
Notethat Condition 2 is not a biconditional.If 12 t is a refinement of ~2 with ref inement functiond, <C '8 ,>~, ,~,  is the family of syntactic categories of~2' and <C0>0E a is the family of syntactic categoriesof ~2, then C'~,-cCd(~, ).As a simple example of a refinement, consider anarbitrary disambiguated language ~2 t = <A, Fy, Xts,,d0w>yEr,8, Ea,.
NOW let ~2 be the disambiguated lan-guage <A, Fy, Xa, S, a>yEi-, in which the set of cate-gory names is the singleton set {a}.
X a = O~,EA, X~,.Let S be {<Fr, <a,a ..... a>,  a> : yeF  and the numberof a's agrees with the arity of F}.
Then f~ is a refine-ment of ~, with ref inement function d:At-~{a}, d(8 ~)= a for all d~?A ~.
Note that the disambiguated lan-guage ~2 is completely determined by the algebra<A,Fy>yeF, and is the natural disambiguated languageto associate with it.
Thus in a formal sense, we canview a disambiguated language as a ref inement of itsalgebra.136 American Journal of Computational Linguistics, Volume 8, Number 3-4, Ju ly -December 1982David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free ParsingAs a more intuitive example of refinement, consideran English-like language with categories term (TE) andintransitive verb phrase (IV) that both include singularand plural forms.
The language generated would thenallow subject-verb disagreement (assuming the ambig-uating relation R does not filter them out).
By refin-ing category TE to TEsing and TEpl and category IV toIVsing and IVpl, and having syntactic rules that com-bine category TEsing with IVsing and TEpl with IVplonly, we obtain a refined language that has subject-verb agreement.
A similar kind of refinement couldeliminate such combinations as "colorless greenideas", if so desired.With this definition of refinement, we return to theproblem of parsing a language L = <~, R>.
Theproblem can now be restated: find an algorithm that,given a string ~, constructs a disambiguated language~2~ that is an input-refinement of fL That is, f~ is arefinement in which the sentence category Cts, is ex-actly the set of parses of ~ in L. Finding this algor-ithm is equivalent to solving the parsing problem.
Forgiven such an algorithm, the parsing problem reducesto the problem of generating all members of C'80,.In the case of a general anguage <~, R>, it maybe the case that for ~ a string, the input-refined lan-guage f~ has finitely many categories.
In this case thereduced grammar can be computed and a recursiveparsing algorithm exists.
If the reduced grammar hasinfinitely many categories, then the string has infinitelymany parses and we are not, in general, interested intrying to parse such languages.
It may happen, how-ever, that ~2~ has infinitely many categories, eventhough its reduction has only finitely many.
In thiscase, we are not guaranteed a recursive parsing algor-ithm.
However, if this reduced language can be effec-tively constructed, a recursive parsing algorithm stillexists.The ATN for PTQ represents the disambiguatedlanguage for PTQ in the UG sense.
The categories ofthis disambiguated language correspond to the set ofpossible triples: PTQ category name, contents ofSENDR registers at a PUSH to that subnet, contents ofthe LIFTR registers at the corresponding POP.
Theinput-refined categories include the remainder of theinput string at the PUSH and POP.
Thus the bucketsin the recall table are exactly the input-refined cate-gories.
The syntactic execution method is thus anexhaustive generation of all expressions in the sen-tence category of the input-refined disambiguatedlanguage.Semantic Equivalence Parsing in OGIn UG, Montague inclues a theory of meaning byproviding a definition of interpretation for a language.Let L = <<A,F,r,Xs,S,t~0>.rEF,SEA,R> be a language.An interpretation ,t' for L is a system <B,G~,,f>3,EFsuch that <B,Gv>v~ r is an algebra similar to<A,F./>3,eF; i.e., for each ~, E F, Fy and G./ have thesame number of arguments, and f is a function fromO,EAX 8 into B.
Note that the algebra <B,G~,>.rE Fneed not be a free algebra (even though <A,Fy>v?
rmust be).
B is the set of meanings of the interpreta-tion ,I,; Gv is the semantic rule corresponding to syn-tactic rule Fv; f assigns meanings to the basic expres-sions Xv.
The meaning assignment for L determinedby if' is the unique homomorphism g from <A,F.r>~,EFinto <B,Gy>,/E F that is an extension of f.There are two ways to proceed in order to find allthe meanings of a sentence ~ in a language L = <f~,R> with interpretation ~.
The first method is to gen-erate all members of the sentence category Cts0 , ofthe input-refined language ~2~.
As discussed above,this is done in the algebra <A,F./>~,cF of ~ ,  using thesyntactic functions Fv to inductively construct mem-bers of A from the basic categories of f~ and membersof A constructed earlier and then applying g. Thesecond method is to use the fact that g is a homomor-phism from <A,F.~>~,EF into <B,G.
/>~ F. Because gis a homomorphism, we can carry out the constructionof the image of the sentence category entirely in thealgebra <B,G~,>~,eF of the interpretation q'.
We mayuse the G functions to construct inductively membersof B from the basic semantic categories, that is, theimages under g (and f) of the basic syntactic categor-ies, and members of B already constructed.
The ad-vantage of carrying out the construction in the algebraof ,t, is that this algebra may not be free, i.e., someelement of B may have multiple construction se-quences.
By carrying out the construction there, suchinstances can be noticed and used to advantage, thuseliminating some redundant search.
There are addi-tional costs, however, associated with parsing in theinterpretation algebra q'.
Usually, the cost of evaluat-ing a G function in the semantic algebra is greaterthan the cost of the corresponding F function in thesyntactic algebra.
Also in semantic parsing, eachmember of B as it is constructed is compared to theother members of the same refined category that werepreviously constructed.In the PTQ parsing system discussed above, theinterpretation algebra is the set of reduced transla-tions.
The semantic functions are those obtained fromthe functions given in the T-rules in PTQ, and reducingand extensionalizing their results.
The directed proc-ess version of the parser finds the meanings in thisalgebra by the first method, generating all parses inthe syntactic algebra and then taking their images un-der the interpretation homomorphism.
Semantic equi-valence parsing for PTQ uses the second method, car-rying out the construction of the meaning entirelywithin the semantic algebra.
The savings in the exam-ple sentence Bil l  walks and Mary runs comes aboutAmerican Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 137David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsingbecause the algebra of reduced translations is not afree algebra, and the redundant search thus eliminatedmore than made up for the increase in the cost oftranslating and comparing formulas.SummaryWe have descr ibed a parsing algor i thm for the lan-guage of PTQ v iewed as consist ing of two parts, anondetermin is t ic  program and an execut ion  method.We showed how, with only a change to an equiva lencerelat ion used in the execut ion method,  the parser be-comes a recognizer.
We then discussed the addit ion ofthe semantic  component  of PTQ to the parser.
Withagain only a change to the equivalence relat ion of theexecut ion  method,  the semant ic  parser  is obta ined.The semantic equiva lence relat ion is equal i ty (to with-in change of bound var iable)  of reduced extensional -ized translations.
Examples  were given to compare thetwo parsing methods.In the f ina lpor t ion  of the paper we descr ibed howthe pars ing method  init ial ly presented in procedura lterms can be v iewed in formal  grammat ica l  terms.The not ion of input - re f inement  for context - f ree  gram-mars was in t roduced by example,  and the tabularcontext - f ree  recogni t ion algorithms were descr ibed inthese terms.
We then indicated how this not ion ofre f inement  can be extended to the UG theory of lan-guage and suggested how our semantic  parser is essen-tially parsing in the algebra of an interpretat ion for thePTQ language.ReferencesBates, Madeleine 1978 The theory and practise of augmentedtransition network grammars.
In Bole, Ed., Natural LanaugeCommunication with Computers.
New York: 191-260.Cooper, R. 1975 Montague's semantic theory and transformation-al syntax.
Ph.D. thesis.
Amherst, MA: University of Massa-chusetts.Earley, Jay 1970 An efficient context-free parsing algorithm.Comm.
ACM 13, 94-102.Friedman, J., Moran, D., and Warren, D.S.
1978a EvaluatingEnglish sentences in a logical model.
Abstract 16, InformationAbstracts, 7th International Conference on ComputationalLinguistics.
Norway: University of Bergen (11 pp.
).Friedman, J., Moran, D., and Warren, D.S.
1978b EvaluatingEnglish sentences in a logical model, presented to the 7th Inter-national Conference on Computation Linguistics, University ofBergen, Norway (August 14-18).
Report N-15.
Ann Arbor,MI: University of Michigan, Computer and CommunicationSciences Department (mimeographed).Friedman, J. and Warren, D.S.
1978 A parsing method for Mon-tague grammars.
Lingustics and Philosophy 2, 347-372.Gawron, J.M., et al 1982 The GPSG linguistic system.
In Pro-ceedings 20th Annual Meeting of  the Association for ComputationalLinguistics, 74- 81.Gazdar, G. 1979 English as a context-free language University ofSussex (mimeograph).Harel, David 1979 On the total correctness of nondeterministicprograms.
IBM Research Report RC 7691.Hintikka, J., Moravcsik, J., and Suppes, P., Eds.
1973 Approachesto Natural Language.
Dordrecht: D. Reidel.Janssen, T.W.V.
1978 Compositionality and the form of rules inMontague grammar.
In Groenenijk, J. and Stokhof, M., Eds.,Proceedings of the Second Amsterdam Colloquium on MontagueGrammar and Related Topics.
Amsterdam Papers in FormalGrammar, Volume II.
University of Amsterdam, 211-234.Janssen, T.W.V.
1980 On problems concerning the quantificationrules in Montague grammar.
In Roher, G., Ed., Time, Tense,and Quantifiers.
Tuebingen, Max Niemeyer Verlag.Kasami, T. 1965 An efficient recognition and syntax-analysisalgorithm for context-free languages.
Science Report AFCRL-65-758.
Bedford, MA: Air Force Cambridge Research Labora-tory.Landsbergen, S.P.J.
1980 Adaptation of Montague grammar tothe requirements of parsing.
M.S.
11.646.
Eindhoven, TheNetherlands: Philips Research Laboratories.Montague, Richard 1970 Universal grammar (UG).
Theoria 36,373-398.Montague, Richard 1973 The proper treatment of quantificationin ordinary English.
In Hintikka, Moravcsik, and Suppes 1973.Reprinted in Montague 1974, 247-270.Montague, Richard 1974 Formal Philosophy: Selected Papers ofRichard Montague.
Edited and with an introduction by Rich-mond Thomason.
New Haven, CT: Yale University Press.Rosenschein, S.J.
and Shieber, S.M.
1982 Translating English intological form.
In Proceedings 20th Annual Meeting of  the Associa-tion for Computational Linguistics, 1-8.Sheil, B.A.
1976 Observations on context-free parsing.
StatisticalMethods in Linguistics 71-109.Warren, David S. 1979 Syntax and semantics in parsing: an appli-cation to Montague grammar.
Ph.D. thesis.
Ann Arbor, MI:University of Michigan.Winograd, T.A.
1972 Understanding Natural Language.
New York:Academic Press.Woods, W.A.
and Kaplan, R.M.
1971 The Lunar Sciences NaturalLanguage Information System.
BBN Report No.
2265.
Cam-bridge, MA Bolt Beranek and Newman.Woods, W.A.
1973 An experimental parsing system for transitionnetwork grammars.
In Rustin, R., Ed., Natural LanguageProcessing.
New York: Algorithmics Press, Inc., 111-154.138 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982
