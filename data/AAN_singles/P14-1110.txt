Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1166?1176,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPolylingual Tree-Based Topic Models for Translation Domain AdaptationYuening Hu?Computer ScienceUniversity of Marylandynhu@cs.umd.eduKe Zhai?Computer ScienceUniversity of Marylandzhaike@cs.umd.eduVladimir EidelmanFiscalNote Inc.Washington DCvlad@fiscalnote.comJordan Boyd-GraberiSchool and UMIACSUniversity of Marylandjbg@umiacs.umd.eduAbstractTopic models, an unsupervised techniquefor inferring translation domains improvemachine translation quality.
However, pre-vious work uses only the source languageand completely ignores the target language,which can disambiguate domains.
We pro-pose new polylingual tree-based topic mod-els to extract domain knowledge that con-siders both source and target languages andderive three different inference schemes.We evaluate our model on a Chinese to En-glish translation task and obtain up to 1.2BLEU improvement over strong baselines.1 IntroductionProbabilistic topic models (Blei and Lafferty,2009), exemplified by latent Dirichlet aloca-tion (Blei et al, 2003, LDA), are one of the mostpopular statistical frameworks for navigating largeunannotated document collections.
Topic modelsdiscover?without any supervision?the primarythemes presented in a dataset: the namesake topics.Topic models have two primary applications: toaid human exploration of corpora (Chang et al,2009) or serve as a low-dimensional representa-tion for downstream applications.
We focus onthe second application, which has been fruitful forcomputer vision (Li Fei-Fei and Perona, 2005),computational biology (Perina et al, 2010), andinformation retrieval (Kataria et al, 2011).In particular, we use topic models to aid statisti-cal machine translation (Koehn, 2009, SMT).
Mod-ern machine translation systems use millions ofexamples of translations to learn translation rules.These systems work best when the training corpushas consistent genre, register, and topic.
Systemsthat are robust to systematic variation in the train-ing set are said to exhibit domain adaptation.?
indicates equal contributions.As we review in Section 2, topic models area promising solution for automatically discover-ing domains in machine translation corpora.
How-ever, past work either relies solely on monolingualsource-side models (Eidelman et al, 2012; Hasleret al, 2012; Su et al, 2012), or limited modelingof the target side (Xiao et al, 2012).
In contrast,machine translation uses inherently multilingualdata: an SMT system must translate a phrase or sen-tence from a source language to a different targetlanguage, so existing applications of topic mod-els (Eidelman et al, 2012) are wilfully ignoringavailable information on the target side that couldaid domain discovery.This is not for a lack of multilingual topic mod-els.
Topic models bridge the chasm between lan-guages using document connections (Mimno etal., 2009), dictionaries (Boyd-Graber and Resnik,2010), and word alignments (Zhao and Xing, 2006).In Section 2, we review these models for discover-ing topics in multilingual datasets and discuss howthey can improve SMT.However, no models combine multiple bridgesbetween languages.
In Section 3, we create amodel?the polylingual tree-based topic models(ptLDA)?that uses information from both externaldictionaries and document alignments simultane-ously.
In Section 4, we derive both MCMC andvariational inference for this new topic model.In Section 5, we evaluate our model on the taskof SMT using aligned datasets.
We show that ptLDAoffers better domain adaptation than other topicmodels for machine translation.
Finally, in Sec-tion 6, we show how these topic models improveSMT with detailed examples.2 Topic Models for Machine TranslationBefore considering past approaches using topicmodels to improve SMT, we briefly review lexicalweighting and domain adaptation for SMT.11662.1 Statistical Machine TranslationStatistical machine translation casts machine trans-lation as a probabilistic process (Koehn, 2009).
Fora parallel corpus of aligned source and target sen-tences (F , E), a phrase?f ?
F is translated to aphrase e?
?
E according to a distribution pw(e?|?f).One popular method to estimate the probabilitypw(e?|?f) is via lexical weighting features.Lexical Weighting In phrase-based SMT, lexi-cal weighting features estimate the phrase pairquality by combining lexical translation probabil-ities of words in a phrase (Koehn et al, 2003).Lexical conditional probabilities p(e|f) are maxi-mum likelihood estimates from relative lexical fre-quencies c(f, e)/?ec(f, e) , where c(f, e) is thecount of observing lexical pair (f, e) in the train-ing dataset.
The phrase pair probabilities pw(e?|?f)are the normalized product of lexical probabili-ties of the aligned word pairs within that phrasepair (Koehn et al, 2003).
In Section 2.2, we createtopic-specific lexical weighting features.Cross-Domain SMT A SMT system is usu-ally trained on documents with the same genre(e.g., sports, business) from a similar style (e.g.,newswire, blog-posts).
These are called domains.Translations within one domain are better thantranslations across domains since they vary dra-matically in their word choices and style.
A correcttranslation in one domain may be inappropriate inanother domain.
For example, ????
in a newspa-per usually means ?underwater diving?.
On socialmedia, it means a non-contributing ?lurker?.Domain Adaptation for SMT Training a SMTsystem using diverse data requires domain adap-tation.
Early efforts focus on building separatemodels (Foster and Kuhn, 2007) and adding fea-tures (Matsoukas et al, 2009) to model domaininformation.
Chiang et al (2011) combine theseapproaches by directly optimizing genre and col-lection features by computing separate translationtables for each domain.However, these approaches treat domains ashand-labeled, constant, and known a priori.
Thissetup is at best expensive and at worst infeasible forlarge data.
Topic models provide a solution wheredomains can be automatically induced from rawdata: treat each topic as a domain.11Henceforth we will use the term ?topic?
and ?domain?interchangeably: ?topic?
to refer to the concept in topic modelsand ?domain?
to refer to SMT corpora.2.2 Inducing Domains with Topic ModelsTopic models take the number of topics K and acollection of documents as input, where each docu-ment is a bag of words.
They output two distribu-tions: a distribution over topics for each documentd; and a distribution over words for each topic.
Ifeach topic defines a SMT domain, the document?stopic distribution is a soft domain assignment forthat document.Given the soft domain assignments, Eidelman etal.
(2012) extract lexical weighting features condi-tioned on the topics, optimizing feature weights us-ing the Margin Infused Relaxed Algorithm (Cram-mer et al, 2006, MIRA).
The topics come fromsource documents only and create topic-specificlexical weights from the per-document topic distri-bution p(k | d).
The lexical probability conditionedon the topic is expected count ek(e, f) of a wordtranslation pair under topic k,c?k(e, f) =?dp(k|d)cd(e, f), (1)where cd(?)
is the number of occurrences of theword pair in document d. The lexical probabilityconditioned on topic k is the unsmoothed probabil-ity estimate of those expected countspw(e|f ; k) =c?k(e,f)?ec?k(e,f), (2)from which we can compute the phrase pair proba-bilities pw(e?|?f ; k) by multiplying the lexical prob-abilities and normalizing as in Koehn et al (2003).For a test document d, the document topic dis-tribution p(k | d) is inferred based on the topicslearned from training data.
The feature value of aphrase pair (e?,?f) isfk(e?|?f) = ?
log{pw(e?|?f ; k) ?
p(k|d)}, (3)a combination of the topic dependent lexical weightand the topic distribution of the document, fromwhich we extract the phrase.
Eidelman et al (2012)compute the resulting model score by combiningthese features in a linear model with other standardSMT features and optimizing the weights.Conceptually, this approach is just reweightingexamples.
The probability of a topic given a docu-ment is never zero.
Every translation observed inthe training set will contribute to pk(e|f); many ofthe expected counts, however, will be less than one.This obviates the explicit smoothing used in otherdomain adaptation systems (Chiang et al, 2011).1167We adopt this framework in its entirety.
Ourcontribution are topics that capture multilingualinformation and thus better capture the domains inthe parallel corpus.2.3 Beyond Vanilla Topic ModelsEidelman et al (2012) ignore a wealth of infor-mation that could improve topic models and helpmachine translation.
Namely, they only use mono-lingual data from the source language, ignoring alltarget-language data and available lexical semanticresources between source and target languages.Different complement each other to reduce ambi-guity.
For example, ????
in a Chinese documentcan be either ?hobbyhorse?
in a children?s topic,or ?Trojan virus?
in a technology topic.
A shortChinese context obscures the true topic.
However,these terms are unambiguous in English, revealingthe true topic.While vanilla topic models (LDA) can only beapplied to monolingual data, there are a numberof topic models for parallel corpora: Zhao andXing (2006) assume aligned word pairs share sametopics; Mimno et al (2009) connect different lan-guages through comparable documents.
Thesemodels take advantage of word or document align-ment information and infer more robust topics fromthe aligned dataset.On the other hand, lexical information can in-duce topics from multilingual corpora.
For in-stance, orthographic similarity connects words withthe same meaning in related languages (Boyd-Graber and Blei, 2009), and dictionaries are amore general source of information on which wordsshare meaning (Boyd-Graber and Resnik, 2010).These two approaches are not mutually exclu-sive, however; they reveal different connectionsacross languages.
In the next section, we combinethese two approaches into a polylingual tree-basedtopic model.3 Polylingual Tree-based Topic ModelsIn this section, we bring existing tree-based topicmodels (Boyd-Graber et al, 2007, tLDA) andpolylingual topic models (Mimno et al, 2009,pLDA) together and create the polylingual tree-based topic model (ptLDA) that incorporates bothword-level correlations and document-level align-ment information.Word-level Correlations Tree-based topic mod-els incorporate the correlations between words byencouraging words that appear together in a con-cept to have similar probabilities given a topic.These concepts can come from WordNet (Boyd-Graber and Resnik, 2010), domain experts (An-drzejewski et al, 2009), or user constrains (Hu etal., 2013).
When we gather concepts from bilin-gual resources, these concepts can connect differentlanguages.
For example, if a bilingual dictionarydefines ????
as ?computer?, we combine thesewords in a concept.We organize the vocabulary in a tree structurebased on these concepts (Figure 1): words in thesame concept share a common parent node, andthen that concept becomes one of many children ofthe root node.
Words that are not in any concept?uncorrelated words?are directly connected tothe root node.
We call this structure the tree prior.When this tree serves as a prior for topic models,words in the same concept are correlated in topics.For example, if ????
has high probability in atopic, so will ?computer?, since they share the sameparent node.
With the tree priors, each topic is nolonger a distribution over word types, instead, it is adistribution over paths, and each path is associatedwith a word type.
The same word could appear inmultiple paths, and each path represents a uniquesense of this word.Document-level Alignments Lexical resourcesconnect languages and help guide the topics.
How-ever, these resources are sometimes brittle and maynot cover the whole vocabulary.
Aligned documentpairs provide a more corpus-specific, flexible asso-ciation across languages.Polylingual topic models (Mimno et al, 2009)assume that the aligned documents in different lan-guages share the same topic distribution and eachlanguage has a unique topic distribution over itsword types.
This level of connection between lan-guages is flexible: instead of requiring the exactmatching on words and sentences, only a coarsedocument alignment is necessary, as long as thedocuments discuss the same topics.Combine Words and Documents We proposepolylingual tree-based topic models (ptLDA),which connect information across different lan-guages by incorporating both word correlation (asin tLDA) and document alignment information (asin pLDA).
We initially assume a given tree struc-ture, deferring the tree?s provenance to the end ofthis section.1168Generative Process As in LDA, each word to-ken is associated with a topic.
However, tree-basedtopic models introduce an additional step of select-ing a concept in a topic responsible for generatingeach word token.
This is represented by a path yd,nthrough the topic?s tree.The probability of a path in a topic depends onthe transition probabilities in a topic.
Each concepti in topic k has a distribution over its children nodesis governed by a Dirichlet prior: pik,i?
Dir(?i).Each path ends in a word (i.e., a leaf node) andthe probability of a path is the product of all ofthe transitions between topics it traverses.
Topicshave correlations over words because the Dirichletparameters can encode positive or negative correla-tions (Andrzejewski et al, 2009).With these correlated in topics in hand, the gen-eration of documents are very similar to LDA.
Forevery document d, we first sample a distributionover topics ?dfrom a Dirichlet prior Dir(?).
Forevery token in the documents, we first sample atopic zdnfrom the multinomial distribution ?d, andthen sample a path ydnalong the tree according tothe transition distributions specified by topic zdn.Because every path ydnleads to a word wdnin lan-guage ldn, we append the sampled word wdntodocument dldn.
Aligned documents have words inboth languages; monolingual documents only havewords in a single language.The full generative process is:1: for topic k ?
1, ?
?
?
,K do2: for each internal node nido3: draw a distribution piki?
Dir(?i)4: for document set d ?
1, ?
?
?
, D do5: draw a distribution ?d?
Dir(?
)6: for each word in documents d do7: choose a topic zdn?
Mult(?d)8: sample a path ydnwith probability?
(i,j)?ydnpizdn,i,j9: ydnleads to word wdnin language ldn10: append token wdnto document dldnIf we use a flat symmetric Dirichlet prior insteadof the tree prior, we recover pLDA; and if all docu-ments are monolingual (i.e., with distinct distribu-tions over topics ?
), we recover tLDA.
ptLDA con-nects different languages on both the word level (us-ing the word correlations) and the document level(using the document alignments).
We comparethese models?
machine translation performance inSection 5.computer, market, ?government, ?
?science, ?
?Dictionary: Vocabulary: English (0), Chinese (1)computermarket ?government??science???
?scientific policy0    scientific0    policy1    1    ?0    computer0    market0    government0    science1    ?
?1    ?
?1    ?
?Prior Tree:0  1Figure 1: An example of constructing a prior treefrom a bilingual dictionary: word pairs with thesame meaning but in different languages are con-cepts; we create a common parent node to groupwords in a concept, and then connect to the root; un-correlated words are connected to the root directly.Each topic uses this tree structure as a prior.Build Prior Tree Structures One remainingquestion is the source of the word-level connectionsacross languages for the tree prior.
We considertwo resources to build trees that correlate wordsacross languages.
The first are a multilingual dic-tionaries (dict), which match words with the samemeaning in different languages together.
These re-lations between words are used as the concepts inthe prior tree (Figure 1).In addition, we extract the word alignments fromaligned sentences in a parallel corpus.
The wordpairs define concepts for the prior tree (align).
Weuse both resources for our models (denoted asptLDA-dict and ptLDA-align) in our experiments(Section 5) and show that they yield comparableperformance in SMT.4 InferenceInference of probabilistic models discovers the pos-terior distribution over latent variables.
For a col-lection of D documents, each of which containsNdnumber of words, the latent variables of ptLDAare: transition distributions pikifor every topic kand internal node i in the prior tree structure; multi-nomial distributions over topics ?dfor every docu-ment d; topic assignments zdnand path ydnfor thenthword wdnin document d. The joint distributionof polylingual tree-based topic models isp(w, z,y,?,pi;?, ?)
=?k?ip(piki|?i) (4)??dp(?d|?)
??d?np(zdn|?d)?
?d?n(p(ydn|zdn,pi)p(wdn|ydn)).Exact inference is intractable, so we turn to ap-1169proximate posterior inference to discover the latentvariables that best explain our data.
Two widelyused approximation approaches are Markov chainMonte Carlo (Neal, 2000, MCMC) and variationalBayesian inference (Blei et al, 2003, VB).
Bothframeworks produce good approximations of theposterior mode (Asuncion et al, 2009).
In addition,Mimno et al (2012) propose hybrid inference thattakes advantage of parallelizable variational infer-ence for global variables (Wolfe et al, 2008) whileenjoying the sparse, efficient updates for local vari-ables (Neal, 1993).
In the rest of this section, wediscuss all three methods in turn.We explore multiple inference schemes becausewhile all of these methods optimize likelihood be-cause they might give different results on the trans-lation task.4.1 Markov Chain Monte Carlo InferenceWe use a collapsed Gibbs sampler for tree-basedtopic models to sample the path ydnand topic as-signment zdnfor word wdn,p(zdn= k, ydn= s|?zdn,?ydn,w;?,?)?
I [?
(s) = wdn] ?Nk|d+??k?(Nk?|d+?)??i?j?sNi?j|k+?i?j?j?(Ni?j?|k+?i?j?
),where ?
(s) represents the word that path s leadsto, Nk|dis the number of tokens assigned to topic kin document d and Ni?j|kis the number of timesedge i?
j in the tree assigned to topic k, exclud-ing the topic assignment zdnand its path ydnofcurrent token wdn.
In practice, we sample the la-tent variables using efficient sparse updates (Yao etal., 2009; Hu and Boyd-Graber, 2012).4.2 Variational Bayesian InferenceVariational Bayesian inference approximates theposterior distribution with a simplified variationaldistribution q over the latent variables: documenttopic proportions ?, transition probabilities pi, topicassignments z, and path assignments y.Variational distributions typically assume amean-field distribution over these latent variables,removing all dependencies between the latent vari-ables.
We follow this assumption for the transi-tion probabilities q(pi |?)
and the document topicproportions q(?
|?
); both are variational Dirichletdistributions.
However, due to the tight couplingbetween the path and topic variables, we mustmodel this joint distribution as one multinomial,q(z,y |?).
If word token wdnhas K topics andS paths, it has a K ?
S length variational multino-mial ?dnks, which represents the probability thatthe word takes path s in topic k. The completevariational distribution isq(?,pi, z,y|?,?,?)
=?dq(?d|?d)?
(5)?k?iq(piki|?ki) ?
?d?nq(zdn, ydn|?dn).Our goal is to find the variational distribution qthat is closest to the true posterior, as measured bythe Kullback-Leibler (KL) divergence between thetrue posterior p and variational distribution q. Thisinduces an ?evidence lower bound?
(ELBO, L) as afunction of a variational distribution q: L =Eq[log p(w, z,y,?,pi)]?
Eq[log q(?,pi, z,y)]=?k?iEq[log p(piki|?i)]+?dEq[log p(?d|?
)]+?d?nEq[log p(zdn, ydn|?d,pi)p(wdn|ydn)]+ H[q(?)]
+ H[q(pi)] + H[q(z,y)], (6)where H[?]
represents the entropy of a distribution.Optimizing L using coordinate descent providesthe following updates:?dnkt?
exp{?(?dk)??
(?k?dk) (7)+?i?j?s(?(?k,i?j)??(?j??k,i?j?
))};?dk= ?k+?n?s??
?1(wdn)?dnkt; (8)?k,i?j= ?i?j(9)+?d?n?s???
(wdn)?dnktI [i?
j ?
s] ;where ??
(wdn) is the set of all paths that lead towordwdnin the tree, and t represents one particularpath in this set.
I [i?
j ?
s] is the indicator ofwhether path s contains an edge from node i to j.4.3 Hybrid Stochastic InferenceGiven the complementary strengths of MCMC andVB, and following hybrid inference proposed byMimno et al (2012), we also derive hybrid infer-ence for ptLDA.The transition distributions pi are treated identi-cally as in variational inference.
We posit a varia-tional Dirichlet distribution ?
and choose the onethat minimizes the KL divergence between the trueposterior and the variational distribution.For topic z and path y, instead of variationalupdates, we use a Gibbs sampler within a document.We sample zdnand ydnconditioned on the topic1170and path assignments of all other document tokens,based on the variational expectation of pi,q(zdn= k, ydn= s|?zdn,?ydn;w) ?
(10)(?+?m 6=nI [zdm= k])?
exp{Eq[log p(ydn|zdn,pi)p(wdn|ydn)]}.This equation embodies how this is a hybrid algo-rithm: the first term resembles the Gibbs samplingterm encoding how much a document prefers atopic, while the second term encodes the expecta-tion under the variational distribution of how mucha path is preferred by this topic,Eq[log p(ydn|zdn,pi)p(wdn|ydn)] = I[?(ydn)=wdn]?
?i?j?ydnEq[log ?zdn,i?j].For every document, we sweep over all its to-kens and resample their topic zdnand path ydnconditioned on all the other tokens?
topic and pathassignments ?zdnand ?ydn.
To avoid bias, wediscard the first B burn-in sweeps and take thefollowing M samples.
We then use the empiricalaverage of these samples update the global varia-tional parameter q(pi|?)
based on how many timeswe sampled these paths?k,i?j=1M?d?n?s??
?1(wdn)(I [i?
j ?
s]?
I [zdn= k, ydn= s])+ ?i?j.
(11)For our experiments, we use the recommended set-tingsB = 5 andM = 5 from Mimno et al (2012).5 ExperimentsWe evaluate our new topic model, ptLDA, and exist-ing topic models?LDA, pLDA, and tLDA?on theirability to induce domains for machine translationand the resulting performance of the translationson standard machine translation metrics.Dataset and SMT Pipeline We use the NIST MTChinese-English parallel corpus (NIST), excludingnon-UN and non-HK Hansards portions as our train-ing dataset.
It contains 1.6M sentence pairs, with40.4M Chinese tokens and 44.4M English tokens.We replicate the SMT pipeline of Eidelman et al(2012): word segmentation (Tseng et al, 2005),align (Och and Ney, 2003), and symmetrize (Koehnet al, 2003) the data.
We train a modified Kneser-Ney trigram language model on English (Chen andGoodman, 1996).
We use CDEC (Dyer et al, 2010)for decoding, and MIRA (Crammer et al, 2006)for parameter training.
To optimize SMT system,we tune the parameters on NIST MT06, and reportresults on three test sets: MT02, MT03 and MT05.2Topic Models Configuration We compare ourpolylingual tree-based topic model (ptLDA) againsttree-based topic models (tLDA), polylingual topicmodels (pLDA) and vanilla topic models (LDA).3We also examine different inference algorithms?Gibbs sampling (gibbs), variational inference(variational) and hybrid approach (variational-hybrid)?on the effects of SMT performance.
Inall experiments, we set the per-document Dirichletparameter ?
= 0.01 and the number of topics to10, as used in Eidelman et al (2012).Resources for Prior Tree To build the tree fortLDA and ptLDA, we extract the word correla-tions from a Chinese-English bilingual dictio-nary (Denisowski, 1997).4We filter the dictionaryusing the NIST vocabulary, and keep entries map-ping single Chinese and single English words.
Theprior tree has about 1000 word pairs (dict).We also extract the bidirectional word align-ments between Chinese and English usingGIZA++ (Och and Ney, 2003).
We then removethe word pairs appearing more than 50K times orfewer than 500 times and construct a second priortree with about 2500 word pairs (align).We apply both trees to tLDA and ptLDA, denotedas tLDA-dict, tLDA-align, ptLDA-dict, and ptLDA-align.
However, tLDA-align and ptLDA-align doworse than tLDA-dict and ptLDA-dict, so we omittLDA-align in the results.Domain Adaptation using Topic Models Weexamine the effectiveness of using topic modelsfor domain adaptation on standard SMT evalua-tion metrics?BLEU (Papineni et al, 2002) andTER (Snover et al, 2006).
We report the resultson three different test sets (Figure 2), and all SMTresults are averaged over five runs.We refer to the SMT model without domain adap-tation as baseline.5LDA marginally improves ma-chine translation (less than half a BLEU point).2The NIST datasets contain 878, 919, 1082 and 1664 sen-tences for MT02, MT03, MT05 and MT06 respectively.3For Gibbs sampling, we use implementations available inHu and Boyd-Graber (2012) for tLDA; and Mallet (McCallum,2002) for LDA and pLDA.4This is a two-level tree structure.
However, one couldbuild a more sophisticated tree prior with a hierarchical dictio-nary such as multilingual WordNet.5Our replication of Eidelman et al (2012) yields slightlyhigher baseline performance, but the trend is consistent.1171gibbs variational variational?hybrid34.8 +0.3 +0.6 +0.4+1.2 +0.535.1 +0.1 +0.3 +0.2 +0.7 +0.431.4 +0.4 +0.7 +0.4 +1 +0.434.8 +0.4 +0.5 +0.4 +0.8 +0.535.1?0.1 +0.2 ?0.1 +0.2 +0.231.4 +0.3 +0.5 +0.3 +0.8 +0.434.8 +0.2 +0.4 +0.2 +0.7 +0.435.1?0.1 ?0.1 ?0.1 +0.2 +0.231.4 +0.3 +0.3 +0.1 +0.6 +0.3313233343536373132333435363731323334353637mt02mt03mt05BLEU Scoremodel baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dictgibbs variational variational?hybrid61.9 ?0.1?1 ?1.2?2.5 ?1.160.1?0.3?0.9 ?0.8?1.9 ?0.963.3?0.9?1.3 ?1.2?2.6 ?1.161.9?0.4?1 ?0.6?1.6 ?1.360.1?0.2?0.5 ?0.1?1 ?0.763.3?0.5?1 ?0.4?1.5 ?1.261.9?0.3?0.7 ?0.1?1.6 ?0.960.1 0 ?0.2 +0.2?1.1 ?0.563.3?0.4?0.7 ?0.1?1.6 ?0.8565860626466565860626466565860626466mt02mt03mt05TER Scoremodel baseline LDA pLDA ptLDA?align ptLDA?dict tLDA?dictFigure 2: Machine translation performance for different models and inference algorithms against thebaseline, on BLEU (top, higher the better) and TER (bottom, lower the better) scores.
Our proposed ptLDAperforms best.
Results are averaged over 5 random runs.
For model ptLDA-dict with different inferenceschemes, the BLEU improvement on three test sets is mostly significant with p = 0.01, except the resultson MT03 using variational and variational-hybrid inferences.Polylingual topic models pLDA and tree-basedtopic models tLDA-dict are consistently better thanLDA, suggesting that incorporating additional bilin-gual knowledge improves topic models.
These im-provements are not redundant: our new ptLDA-dictmodel, which has aspects of both models yields thebest performance among these approaches?up to a1.2 BLEU point gain (higher is better), and -2.6 TERimprovement (lower is better).
The BLEU improve-ment is significant (Koehn, 2004) at p = 0.01,6except on MT03 with variational and variational-hybrid inference.While ptLDA-align performs better than base-line SMT and LDA, it is worse than ptLDA-dict,possibly because of errors in the word alignments,making the tree priors less effective.Scalability While gibbs has better translationscores than variational and variational-hybrid, itis less scalable to larger datasets.
With 1.6M NIST6Because we have multiple runs of each topic model (andthus different translation models), we select the run closest tothe average BLEU for the translation significance test.training sentences, gibbs takes nearly a week torun 1000 iterations.
In contrast, the parallelizedvariational and variational-hybrid approaches,which we implement in MapReduce (Dean andGhemawat, 2004; Wolfe et al, 2008; Zhai et al,2012), take less than a day to converge.6 DiscussionIn this section, we qualitatively analyze the trans-lation results and investigate how ptLDA and itscousins improve SMT.
We also discuss other ap-proaches to improve unsupervised domain adapta-tion for SMT.6.1 How do Topic Models Help SMT?We present two examples of how topic models canimprove SMT.
The first example shows both LDAand ptLDA improve the baseline.
The second exam-ple shows how LDA introduce biases that misleadSMT and how ptLDA?s bilingual constraints correctthese mistakes.Figure 3 shows a sentence about a company1172source ??????????
? , ???
?referencesony has already sold about 570,000 units of narrowband connectionkits in north america at the price of about 39 us dollars and some 20compatible games .baselineLDAptLDA?
internet links set ...?
internet links kit ??
internet links kit ??
with about 20 of the game .?
, there are about 20 compatible games .?
, there are about 20 compatible games .source ?
 ... ?
???
LDA-Topic 0 (business)ptLDA-Topic 0 (business)reference?
connection kits ... ?
some 20 compatible games .	, ???
??
(company), ??
(China), ?(service), ?(market), ?
(technology), ?(industry), ??
(provide), (develop), ?
(year), (product),?, ??
(coorporate), ?, ??
(manage), ?(invest), (economy), ?(international), ?
(system), (bank)??
(company), ?(service), ?(market), ?
(technology), china, ?(industry), (product), market, company, technology, services,?
(system), year, industry, products, business, (economy), information, ??
(manage), ?(invest), percent, ?
(internet), companies, world,system, ??
(information), ?(increase), (device), service, (service)Figure 3: Better SMT result using topic models for domain adaptation.
Top row: the source sentence andits reference translation.
Middle row: the highlighted translations from different approaches.
Bottom row:the change of relevant translation probabilities after incorporating the domain knowledge from LDA andptLDA.
Right: most-probable words of the topic the source sentence is assigned to under LDA (top) andptLDA (bottom).
The Chinese translations are in parenthesis.introducing new technology gadgets where bothLDA and ptLDA improve translations.
The base-line translates ????
to ?set?
(red), and ????
to?with?
(blue), which do not capture the referencemeaning of a add-on device that works with com-patible games.
Both LDA and ptLDA assign thissentence to a business domain, which makes thetranslations probabilities shift toward correct trans-lations: the probability of translating ????
to?compatible?
and the probability of translating ????
to ?kit?
in the business domain are both signif-icantly larger than without the domain knowledge;and the probabilities of translating ????
to ?with?and the probability of translating ?set?
to ???
?in the business domain decrease.The second example (Figure 4) illustrates howptLDA offers further improvements over LDA.
Thesource sentence discusses foreign affairs.
Thebaseline correctly translates the word ????
to?affect?.
However, LDA?which only takes mono-lingual information from the source language?assigns this sentence to economic development.This misleads SMT to lower the probability forthe correct translation ?affect?
; it chooses ?impact?instead.
In contrast, ptLDA?which incorporatesbilingual constraints?successfully labels this sen-tence as foreign affairs and produces a softer, morenuanced translation that better matches the refer-ence.
The translation of ????
is very similar,except in this case, both the baseline and LDAproduce the incorrect translation ?the commitmentof?.
This is possible because the probabilities oftranslating ????
to ?promised to?
and translat-ing ?promised to?
to ????
(the correct transla-tion, in both directions) increase when conditionedon ptLDA?s correct topic but decrease when condi-tioned on LDA?s incorrect topic.6.2 Other ApproachesOther approaches have used topic models for ma-chine translation.
Xiao et al (2012) present a topicsimilarity model based on LDA that produces a fea-ture that weights grammar rules based on topiccompatibility.
They also model the source and tar-get side of rules and compare the target similarityduring decoding by projecting the target distribu-tion into the source space.
Hasler et al (2012)use the source-side topic assignments from hiddentopic Markov models (Gruber et al, 2007, HTMM)which models documents as a Markov chain andassign one topic to the whole sentence, instead ofa mixture of topics.
Su et al (2012) also applyHTMM to monolingual data and apply the results tomachine translation.
To our knowledge, however,this is the first work to use multilingual topic mod-els for domain adaptation in machine translation.6.3 Improving Language ModelsTopic models capture document-level propertiesof language, but a critical component of machinetranslation systems is the language model, whichprovides local constraints and preferences.
Do-main adaptation for language models (Bellegarda,2004; Wood and Teh, 2009) is an important avenuefor improving machine translation.
Models that si-multaneously discover global document themes aswell as local, contextual domain-specific informa-1173source???
?, ????????
?, ???????????????
??, ?????????
??, ?????????
?????
?, ?????????
?, ?????????????
?referencesources said rok embassy personnel told chinese officials that rok has not backed any dpr koreans to get to rok in such a mannerand rok would not like such things happen again to affect relationship between china and the two sides of the korean peninsula .rok also promised to assist china in the administration of koreans in beijing .baselineLDAptLDA?
does not want ...?
does not hope that ...?
does not hope that ...source ?
???
...LDA-Topic 5 (economic development)ptLDA-Topic 2 (foreign affairs)?
so as to avoid impact the relations??
so as not to affect the relations??
so as not to affect the relations?
?
south korea and the commitment of the chinese side ...?
the rok side , and the commitment of the chinese side ...?
south korea has promised to the chinese side ...?
??...?????...
?
???????????reference?
would not like ... ?
to affect the relationship ?
?
rok also promised to the chinese side ...(develop), ?
(country), ?
(two), ??
(China), ??
(relation),?, ??
(cooperate),(economy), ??
(people), ??(friendly),??
(country), ?
(new), (problem), ?, ??
(emphasize), ??
(important), ??
(peace), ??
(together), ?(build), ??
(world)china, (issue), military, united, president, ??
(country), ??
(area), minister, ???
(Iraq), ??
(peace), nuclear, people, (president), peace, security, 	(UN),(military), ???
(Israel), iraq, foreign, international, ?(army), beijing,world, defense, south, ?
?
(security), war, (agreement), ?(conference)Figure 4: Better SMT result using ptLDA compared to LDA and the baseline.
Top row: the source sentenceand a reference translation.
Second row: the highlighted translations from different models.
Third row:the change of relevant translation probabilities after incorporating domain knowledge from LDA andptLDA.
Bottom row: most-probable words for the topics the source sentence is assigned to under LDA(left) and ptLDA (right).
The meanings of Chinese words are in parenthesis.tion (Wallach, 2006; Boyd-Graber and Blei, 2008)may offer further improvements.6.4 External DataThe topic models presented here only require weakalignment between documents at the documentlevel.
Extending to larger datasets for learningtopics is straightforward in principle.
For exam-ple, ptLDA could learn domains from a much largercorpus like Wikipedia and then apply the extracteddomains to machine translation data.
However,this presents further challenges, as Wikipedia?s do-mains are not representative of newswire machinetranslation datasets; a flexible hierarchical topicmodel (Teh et al, 2006) would better distinguishuseful domains from extraneous ones.7 ConclusionTopic models generate great interest, but their usein ?real world?
applications still lags; this is par-ticularly true for multilingual topic models.
Astopic models become more integrated in common-place applications, their adoption, understanding,and robustness will improve.This paper contributes to the deeper integrationof topic models into critical applications by present-ing a new multilingual topic model, ptLDA, com-paring it with other multilingual topic models ona machine translation task, and showing that thesetopic models improve machine translation.
ptLDAmodels both source and target data to induce do-mains from both dictionaries and alignments.
Fur-ther improvement is possible by incorporating topicmodels deeper in the decoding process and addingdomain knowledge to the language model.AcknowledgmentsWe would like to thank the anonymous reviewers,Doug Oard, and John Morgan for their helpful com-ments, and thank Junhui Li and Ke Wu for insight-ful discussions.
This work was supported by NSFGrant IIS-1320538.
Boyd-Graber is also supportedby NSF Grant CCF-1018625.
Any opinions, find-ings, conclusions, or recommendations expressedhere are those of the authors and do not necessarilyreflect the view of the sponsor.ReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via Dirichlet forest priors.
In Proceedingsof the International Conference of Machine Learn-ing.Arthur Asuncion, Max Welling, Padhraic Smyth, andYee Whye Teh.
2009.
On smoothing and inferencefor topic models.
In Proceedings of Uncertainty inArtificial Intelligence.Jerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
vol-ume 42, pages 93?108.1174David M. Blei and John D. Lafferty.
2009.
Visualizingtopics with Multi-Word expressions.
arXiv.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3.Jordan Boyd-Graber and David M. Blei.
2008.
Syn-tactic topic models.
In Proceedings of Advances inNeural Information Processing Systems.Jordan Boyd-Graber and David M. Blei.
2009.
Multi-lingual topic models for unaligned text.
In Proceed-ings of Uncertainty in Artificial Intelligence.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingualsupervised latent Dirichlet alocation.
In Proceed-ings of Emperical Methods in Natural LanguageProcessing.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambigua-tion.
In Proceedings of Emperical Methods in Natu-ral Language Processing.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InProceedings of Advances in Neural Information Pro-cessing Systems.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the Association forComputational Linguistics.David Chiang, Steve DeNeefe, and Michael Pust.
2011.Two easy improvements to lexical weighting.
InProceedings of the Human Language TechnologyConference.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Jeffrey Dean and Sanjay Ghemawat.
2004.
MapRe-duce: Simplified data processing on large clusters.In Symposium on Operating System Design and Im-plementation.Paul Denisowski.
1997.
CEDICT.http://www.mdbg.net/chindict/.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL System Demonstrations.Vladimir Eidelman, Jordan Boyd-Graber, and PhilipResnik.
2012.
Topic models for dynamic translationmodel adaptation.
In Proceedings of the Associationfor Computational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation.Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.2007.
Hidden topic Markov models.
In ArtificialIntelligence and Statistics.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse lexicalised features and topic adaptation forSMT.
In Proceedings of IWSLT.Yuening Hu and Jordan Boyd-Graber.
2012.
Efficienttree-based topic modeling.
In Proceedings of the As-sociation for Computational Linguistics.Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff,and Alison Smith.
2013.
Interactive topic modeling.Machine Learning Journal.Saurabh S. Kataria, Krishnan S. Kumar, Rajeev R. Ras-togi, Prithviraj Sen, and Srinivasan H. Sengamedu.2011.
Entity disambiguation with hierarchical topicmodels.
In Knowledge Discovery and Data Mining.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEmperical Methods in Natural Language Process-ing.Philipp Koehn.
2009.
Statistical Machine Translation.Cambridge University Press.Li Fei-Fei and Pietro Perona.
2005.
A Bayesian hier-archical model for learning natural scene categories.In Computer Vision and Pattern Recognition.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proceedings of Em-perical Methods in Natural Language Processing.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.David Mimno, Hanna Wallach, Jason Naradowsky,David Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of Emper-ical Methods in Natural Language Processing.David Mimno, Matthew Hoffman, and David Blei.2012.
Sparse stochastic inference for latent Dirich-let alocation.
In Proceedings of the InternationalConference of Machine Learning.Radford M. Neal.
1993.
Probabilistic inference usingMarkov chain Monte Carlo methods.
Technical Re-port CRG-TR-93-1, University of Toronto.1175Radford M. Neal.
2000.
Markov chain sampling meth-ods for Dirichlet process mixture models.
Journal ofComputational and Graphical Statistics, 9(2):249?265.Franz Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.In Computational Linguistics, volume 29(21), pages19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the Association for Computational Linguistics,pages 311?318.Alessandro Perina, Pietro Lovato, Vittorio Murino, andManuele Bicego.
2010.
Biologically-aware latentDirichlet alocation (balda) for the classification ofexpression microarray.
In Proceedings of the 5thIAPR international conference on Pattern recogni-tion in bioinformatics, PRIB?10.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In In Proceedings of Association for MachineTranslation in the Americas.Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,Xiaodong Shi, Huailin Dong, and Qun Liu.
2012.Translation model adaptation for statistical machinetranslation with monolingual topic information.
InProceedings of the Association for ComputationalLinguistics.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter.
In SIGHANWorkshop on Chinese Language Processing.Hanna M. Wallach.
2006.
Topic modeling: Beyondbag-of-words.
In Proceedings of the InternationalConference of Machine Learning.Jason Wolfe, Aria Haghighi, and Dan Klein.
2008.Fully distributed EM for very large datasets.
In Pro-ceedings of the International Conference of MachineLearning, pages 1184?1191.Frank Wood and Yee Whye Teh.
2009.
A hierarchi-cal nonparametric Bayesian approach to statisticallanguage model domain adaptation.
In Proceedingsof the International Conference on Artificial Intelli-gence and Statistics, volume 12.Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, andShouxun Lin.
2012.
A topic similarity model for hi-erarchical phrase-based translation.
In Proceedingsof the Association for Computational Linguistics.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.
In KnowledgeDiscovery and Data Mining.Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and Mo-hamad Alkhouja.
2012.
Mr. LDA: A flexible largescale topic modeling package using variational infer-ence in mapreduce.
In Proceedings of World WideWeb Conference.Bing Zhao and Eric P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the Association for Computational Lin-guistics.1176
