Proceedings of the 43rd Annual Meeting of the ACL, pages 622?629,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsRandomized Algorithms and NLP: Using Locality Sensitive Hash Functionfor High Speed Noun ClusteringDeepak Ravichandran, Patrick Pantel, and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292.
{ravichan, pantel, hovy}@ISI.EDUAbstractIn this paper, we explore the power ofrandomized algorithm to address the chal-lenge of working with very large amountsof data.
We apply these algorithms to gen-erate noun similarity lists from 70 millionpages.
We reduce the running time fromquadratic to practically linear in the num-ber of elements to be computed.1 IntroductionIn the last decade, the field of Natural Language Pro-cessing (NLP), has seen a surge in the use of cor-pus motivated techniques.
Several NLP systems aremodeled based on empirical data and have had vary-ing degrees of success.
Of late, however, corpus-based techniques seem to have reached a plateauin performance.
Three possible areas for future re-search investigation to overcoming this plateau in-clude:1.
Working with large amounts of data (Banko andBrill, 2001)2.
Improving semi-supervised and unsupervised al-gorithms.3.
Using more sophisticated feature functions.The above listing may not be exhaustive, but it isprobably not a bad bet to work in one of the abovedirections.
In this paper, we investigate the first twoavenues.
Handling terabytes of data requires moreefficient algorithms than are currently used in NLP.We propose a web scalable solution to clusteringnouns, which employs randomized algorithms.
Indoing so, we are going to explore the literature andtechniques of randomized algorithms.
All cluster-ing algorithms make use of some distance similar-ity (e.g., cosine similarity) to measure pair wise dis-tance between sets of vectors.
Assume that we aregiven n points to cluster with a maximum of k fea-tures.
Calculating the full similarity matrix wouldtake time complexity n2k.
With large amounts ofdata, say n in the order of millions or even billions,having an n2k algorithm would be very infeasible.To be scalable, we ideally want our algorithm to beproportional to nk.Fortunately, we can borrow some ideas from theMath and Theoretical Computer Science communityto tackle this problem.
The crux of our solution liesin defining Locality Sensitive Hash (LSH) functions.LSH functions involve the creation of short signa-tures (fingerprints) for each vector in space such thatthose vectors that are closer to each other are morelikely to have similar fingerprints.
LSH functionsare generally based on randomized algorithms andare probabilistic.
We present LSH algorithms thatcan help reduce the time complexity of calculatingour distance similarity atrix to nk.Rabin (1981) proposed the use of hash func-tions from random irreducible polynomials to cre-ate short fingerprint representations for very largestrings.
These hash function had the nice propertythat the fingerprint of two identical strings had thesame fingerprints, while dissimilar strings had dif-ferent fingerprints with a very small probability ofcollision.
Broder (1997) first introduced LSH.
Heproposed the use of Min-wise independent functionsto create fingerprints that preserved the Jaccard sim-622ilarity between every pair of vectors.
These tech-niques are used today, for example, to eliminate du-plicate web pages.
Charikar (2002) proposed theuse of random hyperplanes to generate an LSH func-tion that preserves the cosine similarity between ev-ery pair of vectors.
Interestingly, cosine similarity iswidely used in NLP for various applications such asclustering.In this paper, we perform high speed similaritylist creation for nouns collected from a huge webcorpus.
We linearize this step by using the LSHproposed by Charikar (2002).
This reduction incomplexity of similarity computation makes it pos-sible to address vastly larger datasets, at the cost,as shown in Section 5, of only little reduction inaccuracy.
In our experiments, we generate a simi-larity list for each noun extracted from 70 millionpage web corpus.
Although the NLP communityhas begun experimenting with the web, we knowof no work in published literature that has appliedcomplex language analysis beyond IR and simplesurface-level pattern matching.2 TheoryThe core theory behind the implementation of fastcosine similarity calculation can be divided into twoparts: 1.
Developing LSH functions to create sig-natures; 2.
Using fast search algorithm to find near-est neighbors.
We describe these two components ingreater detail in the next subsections.2.1 LSH Function Preserving Cosine SimilarityWe first begin with the formal definition of cosinesimilarity.Definition: Let u and v be two vectors in a kdimensional hyperplane.
Cosine similarity is de-fined as the cosine of the angle between them:cos(?
(u, v)).
We can calculate cos(?
(u, v)) by thefollowing formula:cos(?
(u, v)) =|u.v||u||v|(1)Here ?
(u, v) is the angle between the vectors uand v measured in radians.
|u.v| is the scalar (dot)product of u and v, and |u| and |v| represent thelength of vectors u and v respectively.The LSH function for cosine similarity as pro-posed by Charikar (2002) is given by the followingtheorem:Theorem: Suppose we are given a collection ofvectors in a k dimensional vector space (as written asRk).
Choose a family of hash functions as follows:Generate a spherically symmetric random vector rof unit length from this k dimensional space.
Wedefine a hash function, hr, as:hr(u) ={1 : r.u ?
00 : r.u < 0(2)Then for vectors u and v,Pr[hr(u) = hr(v)] = 1??
(u, v)pi(3)Proof of the above theorem is given by Goemansand Williamson (1995).
We rewrite the proof herefor clarity.
The above theorem states that the prob-ability that a random hyperplane separates two vec-tors is directly proportional to the angle between thetwo vectors (i,e., ?
(u, v)).
By symmetry, we havePr[hr(u) 6= hr(v)] = 2Pr[u.r ?
0, v.r < 0].
Thiscorresponds to the intersection of two half spaces,the dihedral angle between which is ?.
Thus, wehave Pr[u.r ?
0, v.r < 0] = ?
(u, v)/2pi.
Proceed-ing we have Pr[hr(u) 6= hr(v)] = ?
(u, v)/pi andPr[hr(u) = hr(v)] = 1 ?
?
(u, v)/pi.
This com-pletes the proof.Hence from equation 3 we have,cos(?
(u, v)) = cos((1?
Pr[hr(u) = hr(v)])pi)(4)This equation gives us an alternate method forfinding cosine similarity.
Note that the above equa-tion is probabilistic in nature.
Hence, we generate alarge (d) number of random vectors to achieve theprocess.
Having calculated hr(u) with d randomvectors for each of the vectors u, we apply equation4 to find the cosine distance between two vectors.As we generate more number of random vectors, wecan estimate the cosine similarity between two vec-tors more accurately.
However, in practice, the num-ber (d) of random vectors required is highly domaindependent, i.e., it depends on the value of the totalnumber of vectors (n), features (k) and the way thevectors are distributed.
Using d random vectors, we623can represent each vector by a bit stream of lengthd.Carefully looking at equation 4, we can ob-serve that Pr[hr(u) = hr(v)] = 1 ?
(hamming distance)/d1 .
Thus, the above theo-rem, converts the problem of finding cosine distancebetween two vectors to the problem of finding ham-ming distance between their bit streams (as given byequation 4).
Finding hamming distance between twobit streams is faster and highly memory efficient.Also worth noting is that this step could be consid-ered as dimensionality reduction wherein we reducea vector in k dimensions to that of d bits while stillpreserving the cosine distance between them.2.2 Fast Search AlgorithmTo calculate the fast hamming distance, we use thesearch algorithm PLEB (Point Location in EqualBalls) first proposed by Indyk and Motwani (1998).This algorithm was further improved by Charikar(2002).
This algorithm involves random permuta-tions of the bit streams and their sorting to find thevector with the closest hamming distance.
The algo-rithm given in Charikar (2002) is described to findthe nearest neighbor for a given vector.
We mod-ify it so that we are able to find the top B closestneighbor for each vector.
We omit the math of thisalgorithm but we sketch its procedural details in thenext section.
Interested readers are further encour-aged to read Theorem 2 from Charikar (2002) andSection 3 from Indyk and Motwani (1998).3 Algorithmic ImplementationIn the previous section, we introduced the theory forcalculation of fast cosine similarity.
We implementit as follows:1.
Initially we are given n vectors in a huge k di-mensional space.
Our goal is to find all pairs ofvectors whose cosine similarity is greater thana particular threshold.2.
Choose d number of (d << k) unit randomvectors {r0, r1, ......, rd} each of k dimensions.A k dimensional unit random vector, in gen-eral, is generated by independently sampling a1Hamming distance is the number of bits which differ be-tween two binary strings.Gaussian function with mean 0 and variance 1,k number of times.
Each of the k samples isused to assign one dimension to the randomvector.
We generate a random number froma Gaussian distribution by using Box-Mullertransformation (Box and Muller, 1958).3.
For every vector u, we determine its signatureby using the function hr(u) (as given by equa-tion 4).
We can represent the signature of vec-tor u as: u?
= {hr1(u), hr2(u), ......., hrd(u)}.Each vector is thus represented by a set of a bitstreams of length d. Steps 2 and 3 takes O(nk)time (We can assume d to be a constant sinced << k).4.
The previous step gives n vectors, each of themrepresented by d bits.
For calculation of fasthamming distance, we take the original bit in-dex of all vectors and randomly permute them(see Appendix A for more details on randompermutation functions).
A random permutationcan be considered as random jumbling of thebits of each vector2.
A random permutationfunction can be approximated by the followingfunction:pi(x) = (ax + b)mod p (5)where, p is prime and 0 < a < p , 0 ?
b < p,and a and b are chosen at random.We apply q different random permutation forevery vector (by choosing random values for aand b, q number of times).
Thus for every vec-tor we have q different bit permutations for theoriginal bit stream.5.
For each permutation function pi, we lexico-graphically sort the list of n vectors (whose bitstreams are permuted by the function pi) to ob-tain a sorted list.
This step takes O(nlogn)time.
(We can assume q to be a constant).6.
For each sorted list (performed after applyingthe random permutation function pi), we calcu-late the hamming distance of every vector with2The jumbling is performed by a mapping of the bit indexas directed by the random permutation function.
For a givenpermutation, we reorder the bit indexes of all vectors in similarfashion.
This process could be considered as column reordingof bit vectors.624B of its closest neighbors in the sorted list.
Ifthe hamming distance is below a certain prede-termined threshold, we output the pair of vec-tors with their cosine similarity (as calculatedby equation 4).
Thus, B is the beam parameterof the search.
This step takes O(n), since wecan assume B, q, d to be a constant.Why does the fast hamming distance algorithmwork?
The intuition is that the number of bitstreams, d, for each vector is generally smaller thanthe number of vectors n (ie.
d << n).
Thus, sort-ing the vectors lexicographically after jumbling thebits will likely bring vectors with lower hammingdistance closer to each other in the sorted lists.Overall, the algorithm takes O(nk+nlogn) time.However, for noun clustering, we generally have thenumber of nouns, n, smaller than the number of fea-tures, k.
(i.e., n < k).
This implies logn << k andnlogn << nk.
Hence the time complexity of ouralgorithm is O(nk + nlogn) ?
O(nk).
This is ahuge saving from the original O(n2k) algorithm.
Inthe next section, we proceed to apply this techniquefor generating noun similarity lists.4 Building Noun Similarity ListsA lot of work has been done in the NLP communityon clustering words according to their meaning intext (Hindle, 1990; Lin, 1998).
The basic intuitionis that words that are similar to each other tend tooccur in similar contexts, thus linking the semanticsof words with their lexical usage in text.
One mayask why is clustering of words necessary in the firstplace?
There may be several reasons for clustering,but generally it boils down to one basic reason: if thewords that occur rarely in a corpus are found to bedistributionally similar to more frequently occurringwords, then one may be able to make better infer-ences on rare words.However, to unleash the real power of clusteringone has to work with large amounts of text.
TheNLP community has started working on noun clus-tering on a few gigabytes of newspaper text.
Butwith the rapidly growing amount of raw text avail-able on the web, one could improve clustering per-formance by carefully harnessing its power.
A corecomponent of most clustering algorithms used in theNLP community is the creation of a similarity ma-trix.
These algorithms are of complexity O(n2k),where n is the number of unique nouns and k is thefeature set length.
These algorithms are thus notreadily scalable, and limit the size of corpus man-ageable in practice to a few gigabytes.
Clustering al-gorithms for words generally use the cosine distancefor their similarity calculation (Salton and McGill,1983).
Hence instead of using the usual naive cosinedistance calculation between every pair of words wecan use the algorithm described in Section 3 to makenoun clustering web scalable.To test our algorithm we conduct similarity basedexperiments on 2 different types of corpus: 1.
WebCorpus (70 million web pages, 138GB), 2.
Newspa-per Corpus (6 GB newspaper corpus)4.1 Web CorpusWe set up a spider to download roughly 70 millionweb pages from the Internet.
Initially, we use thelinks from Open Directory project3 as seed links forour spider.
Each webpage is stripped of HTML tags,tokenized, and sentence segmented.
Each docu-ment is language identified by the software TextCat4which implements the paper by Cavnar and Trenkle(1994).
We retain only English documents.
The webcontains a lot of duplicate or near-duplicate docu-ments.
Eliminating them is critical for obtaining bet-ter representation statistics from our collection.
Theproblem of identifying near duplicate documents inlinear time is not trivial.
We eliminate duplicate andnear duplicate documents by using the algorithm de-scribed by Kolcz et al (2004).
This process of dupli-cate elimination is carried out in linear time and in-volves the creation of signatures for each document.Signatures are designed so that duplicate and nearduplicate documents have the same signature.
Thisalgorithm is remarkably fast and has high accuracy.This entire process of removing non English docu-ments and duplicate (and near-duplicate) documentsreduces our document set from 70 million web pagesto roughly 31 million web pages.
This representsroughly 138GB of uncompressed text.We identify all the nouns in the corpus by us-ing a noun phrase identifier.
For each noun phrase,we identify the context words surrounding it.
Ourcontext window length is restricted to two words to3http://www.dmoz.org/4http://odur.let.rug.nl/?vannoord/TextCat/625Table 1: Corpus descriptionCorpus Newspaper WebCorpus Size 6GB 138GBUnique Nouns 65,547 655,495Feature size 940,154 1,306,482the left and right of each noun.
We use the contextwords as features of the noun vector.4.2 Newspaper CorpusWe parse a 6 GB newspaper (TREC9 andTREC2002 collection) corpus using the dependencyparser Minipar (Lin, 1994).
We identify all nouns.For each noun we take the grammatical context ofthe noun as identified by Minipar5.
We do not usegrammatical features in the web corpus since pars-ing is generally not easily web scalable.
This kind offeature set does not seem to affect our results.
Cur-ran and Moens (2002) also report comparable resultsfor Minipar features and simple word based proxim-ity features.
Table 1 gives the characteristics of bothcorpora.
Since we use grammatical context, the fea-ture set is considerably larger than the simple wordbased proximity feature set for the newspaper cor-pus.4.3 Calculating Feature VectorsHaving collected all nouns and their features, wenow proceed to construct feature vectors (andvalues) for nouns from both corpora using mu-tual information (Church and Hanks, 1989).
Wefirst construct a frequency count vector C(e) =(ce1, ce2, ..., cek), where k is the total number offeatures and cef is the frequency count of featuref occurring in word e. Here, cef is the numberof times word e occurred in context f .
We thenconstruct a mutual information vector MI(e) =(mie1,mie2, ...,miek) for each word e, where miefis the pointwise mutual information between word eand feature f , which is defined as:mief = logcefN?ni=1cifN ?
?kj=1cejN(6)where n is the number of words and N =5We perform this operation so that we can compare the per-formance of our system to that of Pantel and Lin (2002).
?ni=1?mj=1 cij is the total frequency count of allfeatures of all words.Having thus obtained the feature representation ofeach noun we can apply the algorithm described inSection 3 to discover similarity lists.
We report re-sults in the next section for both corpora.5 EvaluationEvaluating clustering systems is generally consid-ered to be quite difficult.
However, we are mainlyconcerned with evaluating the quality and speed ofour high speed randomized algorithm.
The web cor-pus is used to show that our framework is web-scalable, while the newspaper corpus is used to com-pare the output of our system with the similarity listsoutput by an existing system, which are calculatedusing the traditional formula as given in equation1.
For this base comparison system we use the onebuilt by Pantel and Lin (2002).
We perform 3 kindsof evaluation: 1.
Performance of Locality SensitiveHash Function; 2.
Performance of fast Hammingdistance search algorithm; 3.
Quality of final simi-larity lists.5.1 Evaluation of Locality sensitive HashfunctionTo perform this evaluation, we randomly choose 100nouns (vectors) from the web collection.
For eachnoun, we calculate the cosine distance using thetraditional slow method (as given by equation 1),with all other nouns in the collection.
This processcreates similarity lists for each of the 100 vectors.These similarity lists are cut off at a threshold of0.15.
These lists are considered to be the gold stan-dard test set for our evaluation.For the above 100 chosen vectors, we also calcu-late the cosine similarity using the randomized ap-proach as given by equation 4 and calculate the meansquared error with the gold standard test set usingthe following formula:errorav =?
?i(CSreal,i ?
CScalc,i)2/total(7)where CSreal,i and CScalc,i are the cosine simi-larity scores calculated using the traditional (equa-tion 1) and randomized (equation 4) technique re-626Table 2: Error in cosine similarityNumber of ran-dom vectors dAverage error incosine similarityTime (in hours)1 1.0000 0.410 0.4432 0.5100 0.1516 31000 0.0493 243000 0.0273 7210000 0.0156 241spectively.
i is the index over all pairs of elementsthat have CSreal,i >= 0.15We calculate the error (errorav) for various val-ues of d, the total number of unit random vectors rused in the process.
The results are reported in Table26.
As we generate more random vectors, the errorrate decreases.
For example, generating 10 randomvectors gives us a cosine error of 0.4432 (which is alarge number since cosine similarity ranges from 0to 1.)
However, generation of more random vectorsleads to reduction in error rate as seen by the val-ues for 1000 (0.0493) and 10000 (0.0156).
But aswe generate more random vectors the time taken bythe algorithm also increases.
We choose d = 3000random vectors as our optimal (time-accuracy) cutoff.
It is also very interesting to note that by usingonly 3000 bits for each of the 655,495 nouns, weare able to measure cosine similarity between everypair of them to within an average error margin of0.027.
This algorithm is also highly memory effi-cient since we can represent every vector by only afew thousand bits.
Also the randomization processmakes the the algorithm easily parallelizable sinceeach processor can independently contribute a fewbits for every vector.5.2 Evaluation of Fast Hamming DistanceSearch AlgorithmWe initially obtain a list of bit streams for all thevectors (nouns) from our web corpus using the ran-domized algorithm described in Section 3 (Steps 1to 3).
The next step involves the calculation of ham-ming distance.
To evaluate the quality of this searchalgorithm we again randomly choose 100 vectors(nouns) from our collection.
For each of these 100vectors we manually calculate the hamming distance6The time is calculated for running the algorithm on a singlePentium IV processor with 4GB of memorywith all other vectors in the collection.
We only re-tain those pairs of vectors whose cosine distance (asmanually calculated) is above 0.15.
This similaritylist is used as the gold standard test set for evaluatingour fast hamming search.We then apply the fast hamming distance searchalgorithm as described in Section 3.
In particular, itinvolves steps 3 to 6 of the algorithm.
We evaluatethe hamming distance with respect to two criteria: 1.Number of bit index random permutations functionsq; 2.
Beam search parameter B.For each vector in the test collection, we take thetop N elements from the gold standard similarity listand calculate how many of these elements are actu-ally discovered by the fast hamming distance algo-rithm.
We report the results in Table 3 and Table 4with beam parameters of (B = 25) and (B = 100)respectively.
For each beam, we experiment withvarious values for q, the number of random permu-tation function used.
In general, by increasing thevalue for beam B and number of random permu-tation q , the accuracy of the search algorithm in-creases.
For example in Table 4 by using a beamB = 100 and using 1000 random bit permutations,we are able to discover 72.8% of the elements of theTop 100 list.
However, increasing the values of q andB also increases search time.
With a beam (B) of100 and the number of random permutations equalto 100 (i.e., q = 1000) it takes 570 hours of process-ing time on a single Pentium IV machine, whereaswith B = 25 and q = 1000, reduces processing timeby more than 50% to 240 hours.We could not calculate the total time taken tobuild noun similarity list using the traditional tech-nique on the entire corpus.
However, we estimatethat its time taken would be at least 50,000 hours(and perhaps even more) with a few of Terabytes ofdisk space needed.
This is a very rough estimate.The experiment was infeasible.
This estimate as-sumes the widely used reverse indexing technique,where in one compares only those vector pairs thathave at least one feature in common.5.3 Quality of Final Similarity ListsFor evaluating the quality of our final similarity lists,we use the system developed by Pantel and Lin(2002) as gold standard on a much smaller data set.We use the same 6GB corpus that was used for train-627Table 3: Hamming search accuracy (Beam B = 25)Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 10025 6.1% 4.9% 4.2% 3.1% 2.4% 1.9%50 6.1% 5.1% 4.3% 3.2% 2.5% 1.9%100 11.3% 9.7% 8.2% 6.2% 5.7% 5.1%500 44.3% 33.5% 30.4% 25.8% 23.0% 20.4%1000 58.7% 50.6% 48.8% 45.0% 41.0% 37.2%Table 4: Hamming search accuracy (Beam B = 100)Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 10025 9.2% 9.5% 7.9% 6.4% 5.8% 4.7%50 15.4% 17.7% 14.6% 12.0% 10.9% 9.0%100 27.8% 27.2% 23.5% 19.4% 17.9% 16.3%500 73.1% 67.0% 60.7% 55.2% 53.0% 50.5%1000 87.6% 84.4% 82.1% 78.9% 75.8% 72.8%ing by Pantel and Lin (2002) so that the results arecomparable.
We randomly choose 100 nouns andcalculate the top N elements closest to each noun inthe similarity lists using the randomized algorithmdescribed in Section 3.
We then compare this outputto the one provided by the system of Pantel and Lin(2002).
For every noun in the top N list generatedby our system we calculate the percentage overlapwith the gold standard list.
Results are reported inTable 5.
The results shows that we are able to re-trieve roughly 70% of the gold standard similaritylist.
In Table 6, we list the top 10 most similar wordsfor some nouns, as examples, from the web corpus.6 ConclusionNLP researchers have just begun leveraging the vastamount of knowledge available on the web.
Bysearching IR engines for simple surface patterns,many applications ranging from word sense disam-biguation, question answering, and mining seman-tic resources have already benefited.
However, mostlanguage analysis tools are too infeasible to run onthe scale of the web.
A case in point is generat-ing noun similarity lists using co-occurrence statis-tics, which has quadratic running time on the inputsize.
In this paper, we solve this problem by pre-senting a randomized algorithm that linearizes thistask and limits memory requirements.
Experimentsshow that our method generates cosine similaritiesbetween pairs of nouns within a score of 0.03.In many applications, researchers have shown thatmore data equals better performance (Banko andBrill, 2001; Curran and Moens, 2002).
Moreover,at the web-scale, we are no longer limited to a snap-shot in time, which allows broader knowledge to belearned and processed.
Randomized algorithms pro-vide the necessary speed and memory requirementsto tap into terascale text sources.
We hope that ran-domized algorithms will make other NLP tools fea-sible at the terascale and we believe that many al-gorithms will benefit from the vast coverage of ournewly created noun similarity list.AcknowledgementWe wish to thank USC Center for High PerformanceComputing and Communications (HPCC) for help-ing us use their cluster computers.ReferencesBanko, M. and Brill, E. 2001.
Mitigating the paucity of dat-aproblem.
In Proceedings of HLT.
2001.
San Diego, CA.Box, G. E. P. and M. E. Muller 1958.
Ann.
Math.
Stat.
29,610?611.Broder, Andrei 1997.
On the Resemblance and Containment ofDocuments.
Proceedings of the Compression and Complex-ity of Sequences.Cavnar, W. B. and J. M. Trenkle 1994.
N-Gram-Based TextCategorization.
In Proceedings of Third Annual Symposiumon Document Analysis and Information Retrieval, Las Ve-gas, NV, UNLV Publications/Reprographics, 161?175.628Table 5: Final Quality of Similarity ListsTop 1 Top 5 Top 10 Top 25 Top 50 Top 100Accuracy 70.7% 71.9% 72.2% 71.7% 71.2% 71.1%Table 6: Sample Top 10 Similarity ListsJUST DO IT computer science TSUNAMI Louis Vuitton PILATESHAVE A NICE DAY mechanical engineering tidal wave PRADA Tai ChiFAIR AND BALANCED electrical engineering LANDSLIDE Fendi CardioPOWER TO THE PEOPLE chemical engineering EARTHQUAKE Kate Spade SHIATSUNEVER AGAIN Civil Engineering volcanic eruption VUITTON CalisthenicsNO BLOOD FOR OIL ECONOMICS HAILSTORM BURBERRY AyurvedaKINGDOM OF HEAVEN ENGINEERING Typhoon GUCCI AcupressureIf Texas Wasn?t Biology Mudslide Chanel QigongBODY OF CHRIST environmental science windstorm Dior FELDENKRAISWE CAN PHYSICS HURRICANE Ferragamo THERAPEUTIC TOUCHWeld with your mouse information science DISASTER Ralph Lauren ReflexologyCharikar, Moses 2002.
Similarity Estimation Techniques fromRounding Algorithms In Proceedings of the 34th AnnualACM Symposium on Theory of Computing.Church, K. and Hanks, P. 1989.
Word association norms, mu-tual information, and lexicography.
In Proceedings of ACL-89.
pp.
76?83.
Vancouver, Canada.Curran, J. and Moens, M. 2002.
Scaling context space.
InProceedings of ACL-02 pp 231?238, Philadelphia, PA.Goemans, M. X. and D. P. Williamson 1995.
Improved Ap-proximation Algorithms for Maximum Cut and SatisfiabilityProblems Using Semidefinite Programming.
JACM 42(6):1115?1145.Hindle, D. 1990.
Noun classification from predicate-argumentstructures.
In Proceedings of ACL-90.
pp.
268?275.
Pitts-burgh, PA.Lin, D. 1998.
Automatic retrieval and clustering of similarwords.
In Proceedings of COLING/ACL-98.
pp.
768?774.Montreal, Canada.Indyk, P., Motwani, R. 1998.
Approximate nearest neighbors:towards removing the curse of dimensionality Proceedingsof 30th STOC, 604?613.A.
Kolcz, A. Chowdhury, J. Alspector 2004.
Improved ro-bustness of signature-based near-replica detection via lexi-con randomization.
Proceedings of ACM-SIGKDD (2004).Lin, D. 1994 Principar - an efficient, broad-coverage,principle-based parser.
Proceedings of COLING-94, pp.
42?48.
Kyoto, Japan.Pantel, Patrick and Dekang Lin 2002.
Discovering WordSenses from Text.
In Proceedings of SIGKDD-02, pp.
613?619.
Edmonton, CanadaRabin, M. O.
1981.
Fingerprinting by random polynomials.Center for research in Computing technology , Harvard Uni-versity, Report TR-15-81.Salton, G. and McGill, M. J.
1983.
Introduction to ModernInformation Retrieval.
McGraw Hill.Appendix A.
Random PermutationFunctionsWe define [n] = {0, 1, 2, ..., n?
1}.
[n] can thus be considered as a set of integers from0 to n?
1.Let pi : [n] ?
[n] be a permutation function chosenat random from the set of all such permutation func-tions.Consider pi : [4] ?
[4].A permutation function pi is a one to one mappingfrom the set of [4] to the set of [4].Thus, one possible mapping is:pi : {0, 1, 2, 3} ?
{3, 2, 1, 0}Here it means: pi(0) = 3, pi(1) = 2, pi(2) = 1,pi(3) = 0Another possible mapping would be:pi : {0, 1, 2, 3} ?
{3, 0, 1, 2}Here it means: pi(0) = 3, pi(1) = 0, pi(2) = 1,pi(3) = 2Thus for the set [4] there would be 4!
= 4?3?2 =24 possibilities.
In general, for a set [n] there wouldbe n!
unique permutation functions.
Choosing a ran-dom permutation function amounts to choosing oneof n!
such functions at random.629
