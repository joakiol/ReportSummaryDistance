Knowledge Intensive Word Alignment with KNOWAEmanuele PIANTA and Luisa BENTIVOGLIITC-irstVia Sommarie, 1838050 Povo - TrentoItaly{pianta,bentivo}@itc.itAbstractIn this paper we present KNOWA, anEnglish/Italian word aligner, developed at ITC-irst,which relies mostly on information contained inbilingual dictionaries.
The performances ofKNOWA are compared with those of GIZA++, astate of the art statistics-based alignment algorithm.The two algorithms are evaluated on the EuroCorand MultiSemCor tasks, that is on twoEnglish/Italian publicly available parallel corpora.The results of the evaluation show that, given thenature and the size of the available English-Italianparallel corpora, a language-resource-based wordaligner such as KNOWA can outperform a fullystatistics-based algorithm such as GIZA++.1 IntroductionAligning a text and its translation (also known asbitext) at the word level is a basic NaturalLanguage Processing task that has found variousapplications in recent years.
Word level alignmentscan be used to build bilingual concordances forhuman browsing, to feed machine learning-basedtranslation algorithms, or as a basis for sensedisambiguation algorithms or for automaticprojection of linguistic annotations from onelanguage to another.A number of word alignment algorithms havebeen presented in the literature, see for instance(V?ronis, 2000) and (Melamed, 2001).
Sharedevaluation procedures have been established,although there are still open issues on someevaluation details (Ahrenberg et al 2000).Most of the known alignment algorithms arestatistics-based and do not exploit externallinguistic resources, or use them to a very limitedextent.
The main attractive of such algorithms isthat they are language independent, and onlyrequire a parallel corpus of reasonable size to betrained.However, word alignment can be used fordifferent purposes and in different applicationscenarios; different kinds of alignment strategiesproduce different kinds of results (for instance interms of precision/recall) which can be more orless suitable to the goal to be achieved.
Therequirement of having a parallel corpus ofadequate size available for training the  statistics-based algorithms may be difficult to meet, giventhat parallel corpora are a precious but often rareresource.
For the most common languages, such asEnglish, French, German, Chinese, etc., referenceparallel corpora of adequate size are available, andindeed statistics-based algorithms are evaluated onsuch reference corpora.
Unfortunately, if one needsto replicate in a different corpus the resultsobtained for the reference corpora, finding aparallel corpus of adequate size can be difficulteven for the most common languages.
Considerthat one of the most appealing features of statistics-based algorithms is their ability to inducealignment models for bitexts belonging to veryspecific domains, an ability which seems to be outof reach for algorithms based on generic linguisticresources.
However, for the statistics-basedalgorithms to achieve their objective, a parallelcorpus for the specific domain needs to beavailable, a requirement that in some cases cannotbe met easily.For these reasons, we claim that in some casesalgorithms based on external, linguistics resources,if available, can be a useful alternative to statistics-based algorithms.
In the rest of this paper we willcompare the results obtained by a statistics-basedand a linguistic resource-based algorithm whenapplied to the EuroCor and MultiSemCorEnglish/Italian corpora.The statistics-based algorithm to be evaluated isdescribed in  (Och and Ney, 2003).
For itsevaluation we used an implementation by theauthors themselves, called GIZA++, which isfreely available to the scientific community (Och,2003).
The second algorithm to be evaluated iscrucially based on a bilingual dictionary and amorphological analyzer.
It is called KNOWA(KNowledge intensive Word Aligner) and has beendeveloped at ITC-irst by the authors of this paper.The results of the comparative evaluation showthat, given specific application goals, and given theavailability of Italian/English resources, KNOWAobtains results that are comparable or better thanthe results obtained with GIZA++.Section 2 describes the basic KNOWAalgorithm.
Sections 3 and 4 illustrate two enhancedversions of the KNOWA algorithm.
Section 5reports an experiment in which both KNOWA andGIZA++ are first applied to the alignment of areference parallel corpus, EuroCor, and then to theMultiSemCor corpus.
Section 6 adds someconclusive remarks.2 KNOWA ?
the basic algorithmKNOWA is an English/Italian word aligner, whichrelies mostly on information contained in theCollins bilingual dictionary, available in electronicformat.
KNOWA also exploits a morphologicalanalyzer and a multiword recognizer, for bothItalian and English.
It does not require any corpusfor training.
However the input bitext must besentence-aligned.For each sentence pair, KNOWA produces wordalignments according to the following strategy:?
The morphological analysis produces a setof candidate lemmas for each English andItalian word.?
The candidate lemmas are ordered from themost to the least probable by means of arule-based PoS ordering algorithm.?
A three phase incremental alignmentprocedure takes as input the two sentencesannotated with sets of ordered candidatelemmas and outputs a set of pairwise wordalignments.The alignment procedure is crucially based onthe relation of potential correspondence betweenEnglish and Italian words:Given an English word wE and an Italian wordwI, wI is the potential correspondent of wE if one ofthe candidate lemmas of wI is the translationequivalent of one of the candidate lemmas of wE,according to a bilingual dictionary.The potential correspondence relation holdsbetween words, but is relative to a lemma pair.
Forinstance we say that the words dreams and sognaare potential correspondents relative to the lemmapair <dream/verb, sognare/verb>.
Two words canbe potential correspondents relative to more thanone lemma pair.
For instance the words dream andsogno are potential correspondents relative to thetwo lemmas pairs <dream/verb, sognare/verb> and<dream/ noun, sogno/noun>.
In fact dream andsogno can be either first singular person of the verbto dream and sognare, or singular forms of thenoun dream and sogno respectively.The correspondence relation is called potentialbecause in real texts, tokens that are potentialcorrespondents may not in fact be translations ofeach other.
Take for instance the followingtranslation pair: ?ll cane e il gatto?, ?the dog andthe cat?.
The first occurrence of the Italian article?il?
is a potential correspondent of bothoccurrences of the word ?the?
in the Englishsentence, but is the translation of only the first one.In the first phase of the alignment procedure thepotential correspondence relation is exploited inthe English to Italian direction:For each English word wE in a certain position p:1.
Get the most probable candidate lemma of wE.2.
Get the Italian word wI in the same position p.3.
Check if there is a candidate lemma of wIwhich is a potential correspondent of wErelative to the current English candidatelemma, on the basis of a bilingual lexicon.4.
If yes, align wE and wI and record theirlemmas.5.
Otherwise consider the next probablecandidate lemma of wE and go back to step 2.6.
If no aligment is found, progressively extendthe Italian word window and go back to step 1.By extending the Italian word window we meanconsidering Italian words in position p ?
Delta,where p is the position of the English word andDelta can vary from 1 to a MaxDelta value.
Thevalue of MaxDelta is adjustable, but a number ofexperiments have shown that the best results areobtained when MaxDelta=14.
Note that if thealignment is not found within the Italian wordwindow, the English word is left unaligned.
InTable 1 the box in the Italian column shows themaximal text window in which the potentialcorrespondent of dream is searched (MaxDelta=5).The search starts from 15-precedente and endsafter the first extension of the text window assogno can be found in position p-1.In the second phase of the alignment procedurethe potential correspondence relation is exploitedfrom Italian to English.
For each Italian wordwhich has not been aligned in the first phase, thesame procedure is applied as above.In the third and last phase, the algorithm tries toalign the words which are still unaligned, resortingto the graphemic similarity of the Italian andEnglish words.
See (Yzaguirre et al, 2000) for asimilar approach.Note that given the way in which the alignmentprocedure works, finding an alignment implies alsoselecting a PoS and a lemma for both English andItalian words.
The selected PoS and lemma can bedifferent from the ones that were considered mostprobable by the PoS ordering algorithm, due to theconstraints added by the potential correspondencerelation.?
?9-the 9-l'10-exact 10-esatta11-pattern 11-riproduzione12-of 12-di13-a 13-un14-previous 14-sogno15-dream 15-precedente16-we 16-abbiamo17-have 17-un18-an 18-caso19-instance 19-di20-of 20-deja_vu21-deja_vu 21-,?
?Table 1: An example of a maximal text windowThe KNOWA algorithm needs to be able to copewith at least two problematic aspects.
The first aremultiwords.
To work properly, KNOWA needs toidentify them in the source and target sentences,and needs knowledge about their translationequivalents.
We have tried to exploit theinformation about multiwords contained in theCollins bilingual dictionary.
However it is wellknown that dictionaries contain only a small part ofmultiwords actually used in language.
Thus, thereis still wide room to improve KNOWA's capabilityto handle multiwords.The second problematic aspect has to do withmultiple potential correspondence relations.
Givena source word in one language, more than onepotential correspondent can be found within themaximal word window in the target language.
Thisis particularly true in a full text alignment task, thatis trying to align also functional words.
Articlesand determiners can occur repeatedly in anysentence, and almost any Italian preposition can bethe translation of any English preposition; thismakes the task of aligning determiners andpreposition on the basis of the potentialcorrespondence relation and the absolute positionin the sentence hard.
Whatever the number ofpotential correspondents, the alignment procedureselects the potential correspondent whose positionis nearest to the position of the source word by firstconsidering the most probable PoS of the sourceword.
Unfortunately, the potential correspondentselected in this way is not always the right one.Thus multiple potential correspondents can be asource of alignment errors for KNOWA.
In thefollowing section we describe an extension of thebasic KNOWA algorithm that tries to cope withthis limitation.3 KNOWA ?
the pivot extensionIn this section we illustrate a variation of the basicKNOWA algorithm, which tries to solve theproblem of multiple potential correspondencerelations.
To illustrate the problem, let us considerthe example in Table 2, where wrong alignmentsare marked with a cross.1-the 1-il2-boy 2-cane3-likes 3-piace4-the 4-al5-dog 5-bambinoTable 2: Errors due to multiple potentialcorrespondence relationsIn the Italian translation the order of the Englishnoun phrase is inverted.
This is due to the fact thatthe Italian translation of ?likes?
follows a differentverb subcategorization pattern.
What is an object inEnglish becomes a subject in Italian, causing aproblem to the basic KNOWA algorithm.
In fact,KNOWA correctly aligns 2-boy with 5-bambino,and 5-dog with 2-cane, even if the English andItalian nouns are not in the same position in therespective sentences, thanks to a search in theItalian word window.
However, KNOWA wouldalso align 1-the with 1-il, and 4-the with 4-al.Actually 1-the is a potential correspondent of both1-il, and 4-al (the correct translation), butKNOWA chooses 1-il because its position isnearest to 1-the.To solve these problems we need to use adifferent strategy.
The solution is based on theobservation that content words tend to be lessinvolved in multiple potential correspondencesthan function words, and that function words tendto be attached to content words.
Thus the basicidea amounts to trying first the alignment ofcontent words, and only in a second phase tryingthe alignment of function words relative to theposition of content words to which they areattached.
Alignments between content words actas pivots, around which the alignment of functionwords is tried.In the example above, first the algorithm findsthe following correct alignments:2-boy <> 5-bambino3-likes <> 3-piace5-dog <> 2-caneThen, it takes the first alignment and tries to alignthe word before 2-boy and the word before 5-bambino, finding the correct alignment between 1-the and 4-al, and so on.We do not expect that all content words areequally good pivots.
To assess the goodness ofnouns, verbs, adjectives, and adverbs as pivotwords, we run various experiments, taking only thecontent words of a specific PoS and somecombinations of them as pivot words.
The resultsof these experiments show that nouns, taken aloneas pivots, produce the best results in comparisonwith other PoS or combinations of PoS.We also considered an alternative strategy forselecting pivots words.
Instead of using the PoS asa predictor for the goodness of a word as pivot,which actually amounts to saying that words in acertain PoS can be aligned with a lower error ratethan others, we selected as pivots the words forwhich the potential correspondence relation withtheir translation equivalents in the other languageis one-to-one.
Given a word wE in the Englishsentence and a word wI in its Italian translation, weselect wE as a pivot word if, and only if,  wI is theonly potential correspondent of wE, and wE is theonly potential correspondent of wI.
Of course,content words, and nouns in particular, tend tohave such property much more frequently thanwords with other PoS.
However, not all nouns havethis characteristics.
On the other hand certainfunction words, for instance conjunctions, may beinvolved in a one-to-one potential correspondencerelation.Table 3 shows a complete English sentence withits translation, taken from MultiSemCor.
All thepivot words involved in one-to-one potentialcorrespondence relations, according the Collinsdictionary, are connected by a solid line.
Note thatthe relation between 2-temperatures and 2-clima isindeed one-to-one, but is not recorded in thereference dictionary, so it is marked with a dottedline in the table.Table 4 exemplifies instead typical cases of non-pivot words: 9-rovente is the only potentialtranslation of 1-sizzling, but 9-rovente can alsotranslate 2-hot, so neither 1-sizzling nor 4-hot areselected as pivot words.The pivot extension of KNOWA has strongsimilarities with a strategy that is used by variousstatistics-based algorithms, aiming at selecting atfirst the translation correspondents that are mostprobably correct.
Once these pivotalcorrespondences have been established, theremaining alignments are derived using the pivotsas fixed points.
Given that fact that thesealgorithms do not exploit bilingual dictionaries, theselection of the pivotal translation correspondentmay be based on cognates, or specific frequencyconfigurations.
See among others (Simmard andPlamondon, 1998) and (Ribeiro et al, 2000).The results obtained by applying the one-to-onepotential correspondence as criterion for selectingpivot words are illustrated further on in Section 5.1-Sizzling2-temperatures3-and4-hot5-summer6-pavements7-are8-anything9-but10-kind11-to12-the13-feet1-Il2-clima3-torrido4-e5-i6-marciapiedi7-dell?8-estate9-rovente10-non11-sono12-niente13-di14-buono15-per16-i17-piediTable 3: pivot words involved in one-to-onepotential correspondences1-Sizzling2-temperatures3-and4-hot5-summer6-pavements7-are8-anything9-but10-kind?1-Il2-clima3-torrido4-e5-i6-marciapiedi7-dell?8-estate9-rovente10-non?Table 4: typical potential correspondences fornon-pivot words4 KNOWA - the breadth-first extensionThe pivot extension to the basic KNOWAalgorithm is based on two main hypotheses: first,certain words, which we call pivot words andwhich are mainly content words, are easier to alignthan others; second, the position of the otherwords, mainly function words, is anchored to theposition of pivot words.
This means for instancethat if an article is near to a noun in Italian, weexpect the English translation of the article to benear the English translation of the noun.However if we look closer to the way the basicalgorithm explores the search space of the potentialcorrespondent in the word window, we will seethat such strategy is inconsistent with the abovetwo hypotheses.
Suppose that we start from a pivotword wE1, in position pE1, as illustrated in Table 5,where pivot words are included in box.
Then, wetry to align a non-pivot word wE2 occurring inposition pE1+1.
If the correspondent of wE1, that iswI1, occurs in position pI1, then we expect thecorrespondent of wE2, to occur in position pI1+1.Now, if wI2 turns out not to be the  potentialcorrespondent of wE2, possibly because wE2 has notbeen translated, KNOWA will extend the wordwindow of wI2, and search the potentialcorrespondents in position pI1 ?
2, pI1 ?
3, and soon, up to MaxDelta.
We describe this by sayingthat the basic algorithm searches potentialcorrespondents in the word window following adepth-first search strategy.
Unfortunately, suchstrategy can cause alignment errors.
Suppose thatwE3 is another pivot word in position pE3, to bealigned with wI3 in position pI3, and that wE4 is anon-pivot word in position pE3+1, to be alignedwith wI4, in position pI3+1.
Suppose also that wE2 isa potential correspondent of wI4.
Because of thedepth-first search strategy, the basic KNOWAalgorithm will align wE2 and wI4 wrongly.
This kindof error can be avoided by adopting what can becalled a breadth-first search strategy.
In practice,for each pivot word we first search the potentialcorrespondent in a word window of 0, that is in theexpected initial position, then for each pivot wordwe search potential correspondents in a window of?1, and so on up to the MaxDelta.
The results oftesting these strategy are reported in the followingsection.1-2- wE13- wE24-5-6- wE37- wE48-9-10-?1-2-3-4- wI15- wI26-7-8- wI39- wI410-?Table 5: Wrong alignment caused by the first-depth search strategy in the word window 1-9.5 The experimentsWe have run the experiments on two tasks, theEuroCor and the MultiSemCor alignment tasks.We call EuroCor a reduced and revised version ofEuroParl, a multilingual corpus extracted from theproceedings of the European Parliament, see(Koehn, unpublished).
EuroParl includes texts in11 European languages, automatically aligned atthe sentence level, whereas EuroCor includes onlya part of the texts in EuroParl and only for Englishand Italian.
On the other hand, MultiSemCor is areference English/Italian corpus being developed atITC-irst, including SemCor (part of the BrownCorpus) and its Italian translations.
MultiSemCorhas been created with the purpose of automaticallytransfer lexical semantic annotations from Englishto Italian (Bentivogli and Pianta, 2002).For our experiments on EuroCor, we used asgold standard (and test set) a text that,  followingthe EuroParl naming conventions, can beidentified as ep-98-09-18.
The revised version ofthis text includes 385 sentences, and has beenmanually aligned at the word level.
Also sentencealignment has been manually revised.For our experiments on MultiSemCor we used agold standard composed of 6 files, manuallyaligned.
Three of them have been exploited asdevelopment set and three as test set.
In order tokeep the test set as unseen as possible, theexperiments whose main goal is tuning thealgorithm by comparing various alignmentstrategies or parameters have been run on thedevelopment set.
Once the best configuration hasbeen obtained on the development set, we gave theresults of running the algorithm with suchconfiguration on the test set.In our first experiment we run GIZA++ on bothEuroCor and MultiSemCor.
At first, we runGIZA++ on the entire English/Italian part ofEuroParl, including around 694,000 sentences.
Thetraining of GIZA++ on this big corpus took aroundtwo weeks only for the English-to-Italian direction,on a high-level Sun Spark with 4 GB of memory.For this reason we decided to run the subsequentexperiments on EuroCor, a reduced version ofEuroParl, including around 21,000 sentences.EuroCor includes the following texts fromEuroParl: ep-96-05-08, ep-97-04-07, ep-98-04-01,ep-90-11-04, ep-99-01-14, ep-99-10-05, ep-00-06-13, ep-00-09-04, ep-01-04-02, ep-01-04-03.
thefile in the gold standard, ep-98-09-18, should beadded to these texts.
These texts where chosenrandomly, sampling them from as diverse periodsof time as possible.
Note that GIZA++ cannot betested on a test set distinct from the training set.Thus we trained GIZA++ on the whole EuroCorcorpus, including the file in the test set.
Given thefact that we are simply using GIZA++ as a blackbox without having access to the internals of thealignment program, this seems acceptably safefrom a methodological point of view.
In all ourexperiments with GIZA++ we adopted aconfiguration of the system which is reported bythe  authors to produce optimal results, that is15H5344454, where the number in the base refers tothe IBM models 1, 3, 4, and 5, H refers to theHMM training, and the superscript figures refer tothe number of iterations.5.1 The EuroCor taskThe first training of GIZA++ on EuroCor gave thefollowing disappointing results on all-wordsalignment: 59.7% precision, 14.1% recall.
Afterinspection of the corpus, we realized that theoriginal files in EuroParl contain tokenizationerrors, and what counts more, a big number ofsentence alignment errors.
For this reason weproduced a revised version of EuroCor, fixingthese errors as extensively as possible.A new run of GIZA++ on the revised EuroCorgave the following result: P:62.0%, R:34.7% on allword alignment; P:53.2%, R:38.3% on contentwords only.
These results compare badly withthose reported by (Och and Ney, 2003) on theHansard alignment task.
For this task, the authorsreport a precision of 79.6%, for a training on acorpus of 8,000 sentences.
Explaining such adifference is not easy.
A first explanation can bethe fact the EuroCor task is inherently harder thanthe Hansard task.
Whereas in the Hansard corpusthe texts are direct translations of each other, in theEuroCor corpus it happens quite frequently that theEnglish and Italian texts are both translation of atext in a third language.
As a consequence, thetexts are much more difficult to align.
A better andmore systematic revision of the sentencealignments could also improve the performance ofGIZA++.The basic version of KNOWA run on theEuroCor test file gives the results reported in Table6.
These results confirm the difficulty of theEuroCor task, but are quite encouraging forKNOWA, given that no special tuning was madeto obtain them.
It is interesting to note that whereasGIZA++ performs better on the all-word task thanon the content-only-word task, KNOWA getsbetter results on the content-word-only task.Although it is true that aligning function wordsseems inherently more difficult than aligningcontent word, the worse result obtained by astatistics-based algorithm such as GIZA++ on thecontent-words-only task may be explained by thefact that data about content words are more sparsethat data about function words.Precision Recallall 62.0 34.7 GIZA++22k content 53.2 38.3all 63.4 41.6 KNOWAbasic content 85.5 53.2Table 6: GIZA++ and KNOWA-basic on theEuroCor task5.2 The MultiSemCor taskThe training of GIZA++ on MultiSemCor has beenquite problematic, due to the small dimensions ofMultiSemCor.
In the current phase of the project,only 2,948 sentences are available.
This is a smallcorpus which allows for only an approximatecomparison with the experiment reported by Ochand Ney (2003) on a set of 8,000 sentences fromthe Hansard corpus.
Also, the authors report animprovement of around 7 points in precision, inpassing from a corpus of 8,000 to 128,000sentences.
As the ultimate version of MultiSemCoris expected to include more than 20,000 sentences,we can expect a non negligible improvement inprecision when GIZA++ will be applied to thefinal version of MultiSemCor.To simulate at least partly the improvement thatone can expect from an increase in the size ofMultiSemCor, we trained GIZA++ on the union ofthe available MultiSemCor and EuroCor.
Theresults of the training on MultiSemCor only, andon the union of MultiSemCor and EuroCor arereported in Table 7.
Besides the row for the all-word task, the table contains also a SemCor row.This task concerns all the words that have beenmanually tagged in SemCor, and roughlycorresponds to the content-word task.
As thepurpose of MultiSemCor is transferring lexicalannotations from the English annotated words tothe corresponding Italian words, it is particularlyimportant that the alignment for the annotatedwords be correct.
The results showed that GIZA++works consistently better in the Italian-to-Englishdirection, rather than vice versa, so we report theformer direction.
Only for the training on the unionof the MultiSemCor and EuroCor data, we alsoreport the results calculated by resorting to thesymmetrization by intersection of the twoalignments.
Table 7 below shows that theMultiSemCor task is less difficult than theEuroCor Task; that GIZA++ consistently performsworse on content words; and finally that theincrease in the dimensions of the training corpusproduces a non marginal improvement in theprecision, although not in the recall measure.Symmetrization produces a big improvement inprecision but also an unacceptable worsening ofthe recall measure for GIZA++.The two last rows in the table report theperformances of the basic version of KNOWA inthe same two tasks.
These results show that giventhe available resources, KNOWA outperformsGIZA++ in all tasks.
This is even clearer if weconsider the extended versions of KNOWA, asreported in Table 8.
Finally Table 9 reports theresults of KNOWA on the test set.task Prec.
Recallall 68.9 53.5 GIZA++ 3k(MSC) It ->En semcor 60.4 55.1all 73.4 55.2 GIZA++ 25k(MSC+EC) It ->En semcor 81.9 52.9all 95.2 38.8 GIZA++ 25k(MSC+EC) intersec semcor 95.8 37.1all 84.5 63.7 KNOWAbasic semcor 92.0 73.4Table 7: GIZA++ and KNOWA-basic on theMultiSemCor task (development set)KNOWA version task Prec.
Recallall 86.8 65.3 pivot (nouns)depth-first semcor 92.5 73.6all 88.1 66.5 pivot (1-to-1)depth-first semcor 92.8 74.4all 89.4 67.5 pivot (1-to-1)breadth-first semcor 93.0 74.6Table 8: KNOWA-enhanced on constrainedtranslation (development-set)KNOWA version task Prec.
Recallall 82.1 56.9 best(on free tran.)
semcor 89.1 66.5all 87.0 66.6 best(constr.
tran.)
semcor 91.8 72.8Table 9: KNOWA-best on test set (free andconstrained translation)6 ConclusionIn this paper we compared the performances of twoword aligners, one exclusively based on statisticalprinciples, and the other intensively based onlinguistic resources.
Although statistics-basedalgorithms are very appealing, because they arelanguage independent, and only need a parallelcorpus of reasonable size to be trained, we haveshown that, from a practical point of view, the lackof parallel corpora with the necessarycharacteristics can hamper the performances of thestatistical algorithms.
In these cases, an algorithmbased on linguistic resources, if available, canoutperform a statistics-based algorithm.Also, knowledge-intensive word aligners may bemore effective when word alignment is needed forspecial purposes such as annotation transfer fromone language to another.
This is  the case forinstance of the MultiSemCor project, in which,apart from a better performance in terms ofprecision and recall, a word aligner based ondictionaries, such as KNOWA, has the advantagethat it will fail to align words that are notsynonyms.
The alignment of non-synonymoustranslation equivalents, which are hardly found inbi-lingual dictionaries, is usually a strength ofcorpus-based word aligners, but turns out to be adisadvantage in the MultiSemCor case, where thealignment of non synonyoums words causes thetransfer of wrong word sense annotations from onelanguage to the other.ReferencesLars Ahrenberg, Magnus Merkel, Anna S?gvallHein and J?rg Tiedemann.
2000.
Evaluation ofword alignment systems.
In Proceedings ofLREC 2000, Athens, Greece.Luisa Bentivogli and Emanuele Pianta.
2002.Opportunistic Semantic Tagging.
In Proceedingsof LREC-2002, Las Palmas, Canary Islands,Spain (2002).Philipp Koehn.
Unpublished.
Europarl: AMultilingual Corpus for Evaluation of MachineTranslation, unpublised draft, available at http://www.isi.edu/~koehn/publications/europarl.ps.Dan I. Melamed.
2001.
Empirical Methods forExploiting Parallel Texts.
The MIT Press,Cambridge, Massachussets.Franz J. Och.
2003.
GIZA++: Training ofstatistical translation models.
Available athttp://www.isi.edu/~och/GIZA++.html.Franz.
J. Och and H. Ney.
2003.
A SystematicComparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19-51.Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia.2000.
Using Confidence Bands for Parallel TextsAlignment.
In Proceedings of the 38thConference of the Association for ComputationalLinguistics (ACL 2000), Hong Kong, China,2000 October 3?6.
pp.
432?439.Michel Simard and Pierre Plamondon.
1998.Bilingual Sentence Alignment: BalancingRobustness and Accuracy.
In MachineTranslation, 13(1):59-80.Jean V?ronis (ed.).
2000.
Parallel Text Processing.Dordrecht: Kluwer Academic Publishers.Llu?s de Yzaguirre, M. Ribas, J. Vivaldi and M. T.Cabr?.
2000.
Some technical aspects aboutaligning near languages.
In Proceedings ofLREC 2000, Athens, Greece
