Proceedings of the ACL-HLT 2011 System Demonstrations, pages 115?120,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsSciSumm: A Multi-Document Summarization System for Scientific ArticlesNitin AgarwalLanguage Technologies InstituteCarnegie Mellon Universitynitina@cs.cmu.eduRavi Shankar ReddyLanguage Technologies Resource CenterIIIT-Hyderabad, Indiakrs reddy@students.iiit.ac.inKiran GvrLanguage Technologies Resource CenterIIIT-Hyderabad, Indiakiran gvr@students.iiit.ac.inCarolyn Penstein Rose?Language Technologies InstituteCarnegie Mellon Universitycprose@cs.cmu.eduAbstractIn this demo, we present SciSumm, an inter-active multi-document summarization systemfor scientific articles.
The document collec-tion to be summarized is a list of papers citedtogether within the same source article, oth-erwise known as a co-citation.
At the heartof the approach is a topic based clustering offragments extracted from each article based onqueries generated from the context surround-ing the co-cited list of papers.
This analy-sis enables the generation of an overview ofcommon themes from the co-cited papers thatrelate to the context in which the co-citationwas found.
SciSumm is currently built overthe 2008 ACL Anthology, however the gen-eralizable nature of the summarization tech-niques and the extensible architecture makes itpossible to use the system with other corporawhere a citation network is available.
Evalu-ation results on the same corpus demonstratethat our system performs better than an exist-ing widely used multi-document summariza-tion system (MEAD).1 IntroductionWe present an interactive multi-document summa-rization system called SciSumm that summarizesdocument collections that are composed of lists ofpapers cited together within the same source arti-cle, otherwise known as a co-citation.
The inter-active nature of the summarization approach makesthis demo session ideal for its presentation.When users interact with SciSumm, they requestsummaries in context as they read, and that contextdetermines the focus of the summary generated fora set of related scientific articles.
This behaviour isdifferent from some other non-interactive summa-rization systems that might appear as a black boxand might not tailor the result to the specific infor-mation needs of the users in context.
SciSumm cap-tures a user?s contextual needs when a user clicks ona co-citation.
Using the context of the co-citation inthe source article, we generate a query that allowsus to create a summary in a query-oriented fash-ion.
The extracted portions of the co-cited articlesare then assembled into clusters that represent themain themes of the articles that relate to the contextin which they were cited.
Our evaluation demon-strates that SciSumm achieves higher quality sum-maries than a state-of-the-art multidocument sum-marization system (Radev, 2004).The rest of the paper is organized as follows.
Wefirst describe the design goals for SciSumm in 2 tomotivate the need for the system and its usefulness.The end-to-end summarization pipeline has been de-scribed in Section 3.
Section 4 presents an evalua-tion of summaries generated from the system.
Wepresent an overview of relevant literature in Section5.
We end the paper with conclusions and some in-teresting further research directions in Section 6.2 Design GoalsConsider that as a researcher reads a scientific arti-cle, she/he encounters numerous citations, most ofthem citing the foundational and seminal work thatis important in that scientific domain.
The text sur-rounding these citations is a valuable resource asit allows the author to make a statement about her115viewpoint towards the cited articles.
However, to re-searchers who are new to the field, or sometimes justas a side-effect of not being completely up-to-datewith related work in a domain, these citations maypose a challenge to readers.
A system that couldgenerate a small summary of the collection of citedarticles that is constructed specifically to relate tothe claims made by the author citing them would beincredibly useful.
It would also help the researcherdetermine if the cited work is relevant for her ownresearch.As an example of such a co-citation consider thefollowing citation sentence:Various machine learning approaches have beenproposed for chunking (Ramshaw and Marcus,1995; Tjong Kim Sang, 2000a; Tjong Kim Sang etal.
, 2000; Tjong Kim Sang, 2000b; Sassano andUtsuro, 2000; van Halteren, 2000).Now imagine the reader trying to determine aboutwidely used machine learning approaches for nounphrase chunking.
He would probably be requiredto go through these cited papers to understand whatis similar and different in the variety of chunkingapproaches.
Instead of going through these individ-ual papers, it would be quicker if the user could getthe summary of the topics in all those papers thattalk about the usage of machine learning methodsin chunking.
SciSumm aims to automatically dis-cover these points of comparison between the co-cited papers by taking into consideration the con-textual needs of a user.
When the user clicks on aco-citation in context, the system uses the text sur-rounding that co-citation as evidence of the informa-tion need.3 System OverviewA high level overview of our system?s architectureis presented in Figure 1.
The system provides a webbased interface for viewing and summarizing re-search articles in the ACL Anthology corpus, 2008.The summarization proceeds in three main stages asfollows:?
A user may retrieve a collection of articlesof interest by entering a query.
SciSumm re-sponds by returning a list of relevant articles,including the title and a snippet based sum-mary.
For this SciSumm uses standard retrievalfrom a Lucene index.?
A user can use the title, snippet summary andauthor information to find an article of inter-est.
The actual article is rendered in HTML af-ter the user clicks on one of the search results.The co-citations in the article are highlighted inbold and italics to mark them as points of inter-est for the user.?
If a user clicks on one, SciSumm responds bygenerating a query from the local context of theco-citation.
That query is then used to selectrelevant portions of the co-cited articles, whichare then used to generate the summary.An example of a summary for a particular topic isdisplayed in Figure 2.
This figure shows one ofthe clusters generated for the citation sentence ?Var-ious machine learning approaches have been pro-posed for chunking (Ramshaw and Marcus, 1995;Tjong Kim Sang, 2000a; Tjong Kim Sang et al ,2000; Tjong Kim Sang, 2000b; Sassano and Utsuro,2000; van Halteren, 2000)?.
The cluster has a la-bel Chunk, Tag, Word and contains fragments fromtwo of the papers discussing this topic.
A rankedlist of such clusters is generated, which allows forswift navigation between topics of interest for a user(Figure 3).
This summary is tremendously useful asit informs the user of the different perspectives ofco-cited authors towards a shared problem (in thiscase ?Chunking?).
More specifically, it informs theuser as to how different or similar approaches arethat were used for this research problem (which is?Chunking?
).3.1 System DescriptionSciSumm has four primary modules that are centralto the functionality of the system, as displayed inFigure 1.
First, the Text Tiling module takes careof obtaining tiles of text relevant to the citation con-text.
Next, the clustering module is used to generatelabelled clusters using the text tiles extracted fromthe co-cited papers.
The clusters are ordered accord-ing to relevance with respect to the generated query.This is accomplished by the Ranking Module.In the following sections, we discuss each of themain modules in detail.116Figure 1: SciSumm summarization pipeline3.2 TexttilingThe Text Tiling module uses the TextTiling algo-rithm (Hearst, 1997) for segmenting the text of eacharticle.
We have used text tiles as the basic unitfor our summary since individual sentences are tooshort to stand on their own.
This happens as a side-effect of the length of scientific articles.
Sentencespicked from different parts of several articles assem-bled together would make an incoherent summary.Once computed, text tiles are used to expand on thecontent viewed within the context associated with aco-citation.
The intuition is that an embedded co-citation in a text tile is connected with the topic dis-tribution of its context.
Thus, we can use a computa-tion of similarity between tiles and the context of theco-citation to rank clusters generated using FrequentTerm based text clustering.3.3 Frequent Term Based ClusteringThe clustering module employs Frequent TermBased Clustering (Beil et al, 2002).
For each co-citation, we use this clustering technique to clusterall the of the extracted text tiles generated by seg-menting each of the co-cited papers.
We settled onthis clustering approach for the following reasons:?
Text tile contents coming from different papersconstitute a sparse vector space, and thus thecentroid based approaches would not work verywell for integrating content across papers.?
Frequent Term based clustering is extremelyfast in execution time as well as and relativelyefficient in terms of space requirements.?
A frequent term set is generated for each clus-ter, which gives a comprehensible descriptionthat can be used to label the cluster.Frequent Term Based text clustering uses a groupof frequently co-occurring terms called a frequentterm set.
We use a measure of entropy to rank thesefrequent term sets.
Frequent term sets provide aclean clustering that is determined by specifying thenumber of overlapping documents containing morethan one frequent term set.
The algorithm uses thefirst k term sets if all the documents in the documentcollection are clustered.
To discover all the possi-ble candidates for clustering, i.e., term sets, we usedthe Apriori algorithm (Agrawal et al, 1994), whichidentifies the sets of terms that are both relativelyfrequent and highly correlated with one another.3.4 Cluster RankingThe ranking module uses cosine similarity betweenthe query and the centroid of each cluster to rank allthe clusters generated by the clustering module.
Thecontext of a co-citation is restricted to the text of thesegment in which the co-citation is found.
In thisway we attempt to leverage the expert knowledge ofthe author as it is encoded in the local context of theco-citation.4 EvaluationWe have taken great care in the design of the evalu-ation for the SciSumm summarization system.
In a117Figure 2: Example of a summary generated by our system.
We can see that the clusters are cross cutting acrossdifferent papers, thus giving the user a multi-document summary.typical evaluation of a multi-document summariza-tion system, gold standard summaries are created byhand and then compared against fixed length gen-erated summaries.
It was necessary to prepare ourown evaluation corpus, consisting of gold standardsummaries created for a randomly selected set of co-citations because such an evaluation corpus does notexist for this task.4.1 Experimental SetupAn important target user population for multi-document summarization of scientific articles isgraduate students.
Hence to get a measure of howwell the summarization system is performing, weasked 2 graduate students who have been workingin the computational linguistics community to creategold standard summaries of a fixed length (8 sen-tences ?
200 words) for 10 randomly selected co-citations.
We obtained two different gold standardsummaries for each co-citation (i.e., 20 gold stan-dard summaries total).
Our evaluation is designedto measure the quality of the content selection.
Infuture work, we will evaluate the usability of theSciSumm system using a task based evaluation.In the absence of any other multi-document sum-marization system in the domain of scientific ar-ticle summarization, we used a widely used andfreely available multi-document summarization sys-tem called MEAD (Radev, 2004) as our baseline.MEAD uses centroid based summarization to cre-ate informative clusters of topics.
We use the de-fault configuration of MEAD in which MEAD useslength, position and centroid for ranking each sen-tence.
We did not use query focussed summarizationwith MEAD.
We evaluate its performance with thesame gold standard summaries we use to evaluateSciSumm.
For generating a summary from our sys-tem we used sentences from the tiles that are clus-tered in the top ranked cluster.
Once all of the ex-tracts included in that entire cluster are exhausted,we move on to the next highly ranked cluster.
In thisway we prepare a summary comprising of 8 highlyrelevant sentences.4.2 ResultsFor measuring performance of the two summariza-tion systems (SciSumm and MEAD), we computethe ROUGE metric based on the 2 * 10 gold standardsummaries that were manually created.
ROUGE hasbeen traditionally used to compute the performancebased on the N-gram overlap (ROUGE-N) betweenthe summaries generated by the system and the tar-get gold standard summaries.
For our evaluationwe used two different versions of the ROUGE met-ric, namely ROUGE-1 and ROUGE-2, which corre-spond to measures of the unigram and bigram over-lap respectively.
We computed four metrics in orderto get a complete picture of how SciSumm performsin relation to the baseline, namely ROUGE-1 F-measure, ROUGE-1 Recall, ROUGE-2 F-measure,and ROUGE-2 Recall.From the results presented in Figure 4 and 5, wecan see that our system performs well on average incomparison to the baseline.
Especially important is118Figure 3: Clusters generated in response to a user click on the co-citation.
The list of clusters in the left pane gives abird-eye view of the topics which are present in the co-cited papersTable 1: Average ROUGE results.
* represents improve-ment significant at p < .05, ?
at p < .01.Metric MEAD SciSummROUGE-1 F-measure 0.3680 0.5123 ?ROUGE-1 Recall 0.4168 0.5018ROUGE-1 Precision 0.3424 0.5349 ?ROUGE-2 F-measure 0.1598 0.3303 *ROUGE-2 Recall 0.1786 0.3227 *ROUGE-2 Precision 0.1481 0.3450 ?the performance of the system on recall measures,which shows the most dramatic advantage over thebaseline.
To measure the statistical significance ofthis result, we carried out a Student T-Test, the re-sults of which are presented in the results sectionin Table 1.
It is apparent from the p-values gener-ated by T-Test that our system performs significantlybetter than MEAD on three of the metrics on whichboth the systems were evaluated using (p < 0.05)as the criterion for statistical significance.
This sup-ports the view that summaries perceived as higher invalue are generated using a query focused technique,where the query is generated automatically from thecontext of the co-citation.5 Previous WorkSurprisingly, not many approaches to the problem ofsummarization of scientific articles have been pro-posed in the past.
Qazvinian et al (2008) presenta summarization approach that can be seen as theconverse of what we are working to achieve.
Ratherthan summarizing multiple papers cited in the samesource article, they summarize different viewpointsexpressed towards the same paper from different pa-pers that cite it.
Nanba et al (1999) argue in theirwork that a co-citation frequently implies a consis-tent viewpoint towards the cited articles.
Anotherapproach that uses bibliographic coupling has beenused for gathering different viewpoints from whichto summarize a document (Kaplan et al, 2008).
Inour work we make use of this insight by generatinga query to focus our multi-document summary fromthe text closest to the citation.6 Conclusion And Future WorkIn this demo, we present SciSumm, which is an in-teractive multi-document summarization system forscientific articles.
Our evaluation shows that theSciSumm approach to content selection outperformsanother widely used multi-document summarizationsystem for this summarization task.Our long term goal is to expand the capabilitiesof SciSumm to generate literature surveys of largerdocument collections from less focused queries.This more challenging task would require more con-trol over filtering and ranking in order to avoid gen-erating summaries that lack focus.
To this end, afuture improvement that we plan to use is a vari-ant on MMR (Maximum Marginal Relevance) (Car-bonell et al, 1998), which can be used to optimizethe diversity of selected text tiles as well as the rel-evance based ordering of clusters, i.e., so that morediverse sets of extracts from the co-cited articles willbe placed at the ready fingertips of users.Another important direction is to refine the inter-action design through task-based user studies.
Aswe collect more feedback from students and re-searchers through this process, we will used the in-sights gained to achieve a more robust and effectiveimplementation.119Figure 4: ROUGE-1 Recall Figure 5: ROUGE-2 Recall7 AcknowledgementsThis research was supported in part by NSF grantEEC-064848 and ONR grant N00014-10-1-0277.ReferencesAgrawal R. and Srikant R. 1994.
Fast Algorithm forMining Association Rules In Proceedings of the 20thVLDB Conference Santiago, Chile, 1994Baxendale, P. 1958.
Machine-made index for technicalliterature - an experiment.
IBM Journal of Researchand DevelopmentBeil F., Ester M. and Xu X 2002.
Frequent-Term basedText Clustering In Proceedings of SIGKDD ?02 Ed-monton, Alberta, CanadaCarbonell J. and Goldstein J.
1998.
The Use of MMR,Diversity-Based Reranking for Reordering Documentsand Producing Summaries In Research and Develop-ment in Information Retrieval, pages 335?336Councill I. G. , Giles C. L. and Kan M. 2008.
ParsCit:An open-source CRF reference string parsing pack-age INTERNATIONAL LANGUAGE RESOURCESAND EVALUATION European Language ResourcesAssociationEdmundson, H.P.
1969.
New methods in automatic ex-tracting.
Journal of ACM.Hearst M.A.
1997 TextTiling: Segmenting text intomulti-paragraph subtopic passages In proceedings ofLREC 2004, Lisbon, Portugal, May 2004Joseph M. T. and Radev D. R. 2007.
Citation analysis,centrality, and the ACL AnthologyKupiec J. , Pedersen J. , Chen F. 1995.
A training doc-ument summarizer.
In Proceedings SIGIR ?95, pages68-73, New York, NY, USA.
28(1):114?133.Luhn, H. P. 1958.
IBM Journal of Research Develop-ment.Mani I. , Bloedorn E. 1997.
Multi-Document Summa-rization by graph search and matching In AAAI/IAAI,pages 622-628.
[15, 16].Nanba H. , Okumura M. 1999.
Towards Multi-paperSummarization Using Reference Information In Pro-ceedings of IJCAI-99, pages 926?931 .Paice CD.
1990.
Constructing Literature Abstracts byComputer: Techniques and Prospects InformationProcessing and Management Vol.
26, No.1, pp, 171-186, 1990Qazvinian V. , Radev D.R 2008.
Scientific Papersummarization using Citation Summary Networks InProceedings of the 22nd International Conference onComputational Linguistics, pages 689?696 Manch-ester, August 2008Radev D. R .
, Jing H. and Budzikowska M. 2000.Centroid-based summarization of multiple documents:sentence extraction, utility based evaluation, and userstudies In NAACL-ANLP 2000 Workshop on Auto-matic summarization, pages 21-30, Morristown, NJ,USA.
[12, 16, 17].Radev, Dragomir.
2004.
MEAD - a platform for mul-tidocument multilingual text summarization.
In pro-ceedings of LREC 2004, Lisbon, Portugal, May 2004.Teufel S. , Moens M. 2002.
Summarizing ScientificArticles - Experiments with Relevance and RhetoricalStatus In Journal of Computational Linguistics, MITPress.Hal Daume III , Marcu D. 2006.
Bayesian query-focussed summarization.
In Proceedings of the Con-ference of the Association for Computational Linguis-tics, ACL.Eisenstein J , Barzilay R. 2008.
Bayesian unsupervisedtopic segmentation In EMNLP-SIGDAT.Barzilay R , Lee L. 2004.
Catching the drift: Probabilis-tic content models, with applications to generation andsummarization In Proceedings of 3rd Asian SemanticWeb Conference (ASWC 2008), pp.182-188,.Kaplan D , Tokunaga T. 2008.
Sighting citation sights:A collective-intelligence approach for automatic sum-marization of research papers using C-sites In HLT-NAACL.120
