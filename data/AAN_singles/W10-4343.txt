Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 237?240,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsCross-Domain Speech Disfluency DetectionKallirroi Georgila, Ning Wang, Jonathan GratchInstitute for Creative Technologies, University of Southern California12015 Waterfront Drive, Playa Vista, CA 90094, USA{kgeorgila,nwang,gratch}@ict.usc.eduAbstractWe build a model for speech disfluencydetection based on conditional randomfields (CRFs) using the Switchboard cor-pus.
This model is then applied to anew domain without any adaptation.
Weshow that a technique for detecting speechdisfluencies based on Integer Linear Pro-gramming (ILP) (Georgila, 2009) signifi-cantly outperforms CRFs.
In particular, interms of F-score and NIST Error Rate theabsolute improvement of ILP over CRFsexceeds 20% and 25% respectively.
Weconclude that ILP is an approach withgreat potential for speech disfluency detec-tion when there is a lack or shortage of in-domain data for training.1 IntroductionSpeech disfluencies (also known as speech re-pairs) occur frequently in spontaneous speech andcan pose difficulties to natural language process-ing (NLP) since most NLP tools (e.g.
parsers andpart-of-speech taggers) are traditionally trained onwritten language.
However, speech disfluenciesare not noise.
They are an integral part of howhumans speak, may provide valuable informationabout the speaker?s cognitive state, and can be crit-ical for successful turn-taking (Shriberg, 2005).Speech disfluencies have been the subject of muchresearch in the field of spoken language process-ing, e.g.
(Ginzburg et al, 2007).Speech disfluencies can be divided into threeintervals, the reparandum, the editing term, andthe correction (Heeman and Allen, 1999; Liu etal., 2006).
In the example below, ?it left?
is thereparandum (the part that will be repaired), ?Imean?
is the editing term, and ?it came?
is the cor-rection:(it left) * (I mean) it cameThe asterisk marks the interruption point atwhich the speaker halts the original utterance inorder to start the repair.
The editing term is op-tional and consists of one or more filled pauses(e.g.
uh, um) or discourse markers (e.g.
you know,well).
Our goal here is to automatically detect rep-etitions (the speaker repeats some part of the ut-terance), revisions (the speaker modifies the orig-inal utterance), or restarts (the speaker abandonsan utterance and starts over).
We also deal withcomplex disfluencies, i.e.
a series of disfluenciesin succession (?it it was it is sounds great?
).In previous work many different approaches todetecting speech disfluencies have been proposed.Different types of features have been used, e.g.lexical features only, acoustic and prosodic fea-tures only, or a combination of both (Liu et al,2006).
Furthermore, a number of studies havebeen conducted on human transcriptions whileother efforts have focused on detecting disfluen-cies from the speech recognition output.In our previous work (Georgila, 2009), we pro-posed a novel two-stage technique for speech dis-fluency detection based on Integer Linear Pro-gramming (ILP).
ILP has been applied success-fully to several NLP problems, e.g.
(Clarkeand Lapata, 2008).
In the first stage of ourmethod, we trained state-of-the-art classifiers forspeech disfluency detection, in particular, Hidden-Event Language Models (HELMs) (Stolcke andShriberg, 1996), Maximum Entropy (ME) mod-els (Ratnaparkhi, 1998), and Conditional RandomFields (CRFs) (Lafferty et al, 2001).
Then inthe second stage and during testing, each classifierproposed possible labels which were then assessedin the presence of local and global constraints us-ing ILP.
These constraints are hand-crafted and en-code common disfluency patterns.
ILP makes the237final decision taking into account both the outputof the classifier and the constraints.
Our approachis similar to the work of (Germesin et al, 2008) inthe sense that they also combine machine learningwith hand-crafted rules.
However, we use differ-ent machine learning techniques and ILP.When we evaluated this approach on theSwitchboard corpus (available from LDC andmanually annotated with disfluencies) using lex-ical features, we found that ILP significantly im-proves the performance of HELMs and ME mod-els with negligible cost in processing time.
How-ever, the improvement of ILP over CRFs was onlymarginal.
These results were achieved when eachclassifier was trained on approx.
35,000 occur-rences of disfluencies.
Then we experimentedwith varying training set sizes in Switchboard.
Assoon as we started reducing the amount of data fortraining the classifiers, the improvement of ILPover CRFs rose and became very significant, ap-prox.
4% absolute reduction of error rate with 25%of the training set (approx.
9,000 occurrences ofdisfluencies) (Georgila, 2009).
This result showedthat ILP is particularly helpful when there is nomuch training data available.However, Switchboard is a unique corpus be-cause the amount of disfluencies that it containsis very large.
Thus even 25% of our training setcontains more disfluencies than a typical corpusof human-human or human-machine interactions.In this paper, we investigate what happens whenwe move to a new domain when there is no in-domain data annotated with disfluencies to be usedfor training.
This is usually the case when we startdeveloping a dialogue system in a new domain,when the system has not been fully implementedyet, and thus no data from users interacting withthe system has been collected.
Since the improve-ment of ILP over HELMs and ME models wasvery large even when the models were both trainedand tested on Switchboard (approx.
15% and 20%absolute reduction of error rate when 100% and25% of the training set was used for training theclassifiers respectively (Georgila, 2009)), in thispaper we focus only on comparing CRFs versusCRFs+ILP.
Our goal is to evaluate if and howmuch ILP improves CRFs in the case that no train-ing data is available at all.The structure of the paper is as follows: In sec-tion 2 we describe our data sets.
In section 3 weconcisely describe our approach.
Then in section 4we present our experiments.
Finally in section 5we present our conclusion.2 Data SetsTo train our classifiers we use Switchboard (avail-able from LDC), which is manually annotatedwith disfluencies, and is traditionally used forspeech disfluency experiments.
We transformedthe Switchboard annotations into the followingformat:it BE was IE a IP it was goodBE (beginning of edit) is the point where thereparandum starts and IP is the interruption point(the point before the repair starts).
In the aboveexample the beginning of the reparandum is thefirst occurrence of ?it?, the interruption point ap-pears after ?a?, and every word between BE andIP is tagged as IE (inside edit).
Sometimes BEand IP occur at the same point, e.g.
?it BE-IP itwas?.
In (Georgila, 2009) we divided Switchboardinto training, development, and test sets.
Here weuse the same training and development sets as in(Georgila, 2009) containing 34,387 occurrences ofBE labels and 39,031 occurrences of IP labels, and3,146 occurrences of BE labels and 3,499 occur-rences of IP labels, respectively.We test our approach on a smaller corpus col-lected in the framework of the Rapport project(Gratch et al, 2007).
The goal of the Rap-port project is to study how rapport is achievedin human-human and human-machine interaction.By rapport we mean the harmony, fluidity, syn-chrony and flow that someone feels when they areengaged in a good conversation.The Rapport agent is a virtual human designedto elicit rapport from human participants withinthe confines of a dyadic narrative task (Gratch etal., 2007).
In this setting, a speaker narrates somepreviously observed series of events, i.e.
the eventsin a sexual harassment awareness and preventionvideo, and the events in a video of the Tweetycartoon.
The central challenge for the Rapportagent is to provide the non-verbal listening feed-back associated with rapportful interaction (e.g.head nods, postural mirroring, gaze shifts, etc.
).Our ultimate goal is to investigate possible cor-relations between disfluencies and these types offeedback.We manually annotated 70 sessions of the Rap-port corpus with disfluencies using the labels de-scribed above (BE, IP, IE and BE-IP).
In each ses-sion the speaker narrates the events of one video.These annotated sessions served as our referencedata set (gold-standard), which contained 738 and865 occurrences of BE and IP labels respectively.2383 MethodologyIn the first stage we train our classifier.
Any clas-sifier can be used as long as it provides more thanone possible answer (i.e.
tag) for each word in theutterance.
Valid tags are BE, BE-IP, IP, IE or O.The O tag indicates that the word is outside thedisfluent part of the utterance.
ILP will be appliedto the output of the classifier during testing.Let N be the number of words of each utter-ance and i the location of the word in the utterance(i=1,...,N ).
Also, let CBE(i) be a binary variable(1 or 0) for the BE tag.
Its value will be determinedby ILP.
If it is 1 then the word will be tagged asBE.
In the same way, we use CBE?IP (i), CIP (i),CIE(i), CO(i) for tags BE-IP, IP, IE and O re-spectively.
Let PBE(i) be the probability given bythe classifier that the word is tagged as BE.
In thesame way, let PBE?IP (i), PIP (i), PIE(i), PO(i)be the probabilities for tags BE-IP, IP, IE and Orespectively.
Given the above definitions, the ILPproblem formulation can be as follows:max[?Ni=1[PBE(i)CBE(i) + PBE?IP (i)CBE?IP (i)+PIP (i)CIP (i) + PIE(i)CIE(i) + PO(i)CO(i)]](1)subject to constraints, e.g.
:CBE(i) + CBE?IP (i) + CIP (i) + CIE(i)+CO(i) = 1 ?i ?
(1, ..., N) (2)Equation 1 is the linear objective function thatwe want to maximize, i.e.
the overall probabilityof the utterance.
Equation 2 says that each wordcan have one tag only.
In the same way, we candefine constraints on which labels are allowed atthe start and end of an utterance.
There are alsosome constraints that define the transitions that areallowed between tags.
For example, IP cannotfollow an O directly, which means that we can-not start a disfluency with an IP.
There has to bea BE after O and before IP.
Details are given in(Georgila, 2009).We also formulate some additional rules thatencode common disfluency patterns.
The ideahere is to generalize from these patterns.
Be-low is an example of a long-context rule.
If wehave the sequence of words ?she was trying towell um she was talking to a coworker?, we ex-pect this to be tagged as ?she BE was IE try-ing IE to IP well O um O she O was O talk-ing O to O a O coworker O?, if we do not takeinto account the context in which this pattern oc-curs.
Basically the pattern here is that two se-quences of four words separated by a discoursemarker (?well?)
and a filled pause (?um?)
differonly in their third word.
That is, ?trying?
and?talking?
are different words but have the samepart-of-speech tag (gerund).
We incorporate thisrule into our ILP problem formulation as follows:Let (w1,...,wN ) be a sequence of N words whereboth w3 and wN?3 are verbs (gerund), the wordsequence w1,w2,w4 is the same as the sequencewN?5,wN?4,wN?2, and all the words in between(w5,...,wN?6) are filled pauses or discourse mark-ers.
Then the probabilities given by the classi-fier are modified as follows: PBE(1)=PBE(1)+b1,PIE(2)=PIE(2)+b2, PIE(3)=PIE(3)+b3, andPIP (4)=PIP (4)+b4, where b1, b2, b3 and b4 areempirically set boosting paremeters with valuesbetween 0.5 and 1 computed using our Switch-board development set.
We use more complexrules to cover cases such as ?she makes he doesn?tmake?, and boost the probabilities that this istagged as ?she BE makes IP he O doesn?t O makeO?.In total we apply 17 rules and each rule can haveup to 5 more specific sub-rules.
The largest con-text that we take into account is 10 words, not in-cluding filled pauses and discourse markers.4 ExperimentsFor building the CRF model we use the CRF++toolkit (available from sourceforge).
Weused only lexical features, i.e.
words and part-of-speech (POS) tags.
Switchboard includes POSinformation but to annotate the Rapport corpuswith POS labels we used the Stanford POS tag-ger (Toutanova and Manning, 2000).
We ex-perimented with different sets of features andwe achieved the best results with the follow-ing setup (i is the location of the word or POSin the sentence): Our word features are ?wi?,?wi+1?, ?wi?1, wi?, ?wi, wi+1?, ?wi?2, wi?1, wi?,?wi, wi+1, wi+2?.
Our POS features have thesame structure as the word features.
For ILP weuse the lp solve software also available fromsourceforge.
We train on Switchboard andtest on the Rapport corpus.For evaluating the performance of our modelswe use standard metrics proposed in the litera-ture, i.e.
Precision, Recall, F-score, and NIST Er-ror Rate.
We report results for BE and IP.
F-scoreis the harmonic mean of Precision and Recall (weequally weight Precision and Recall).
Precision isthe ratio of the correctly identified tags X to all thetags X detected by the model (where X is BE orIP).
Recall is the ratio of the correctly identifiedtags X to all the tags X that appear in the reference239BEPrec Rec F-score ErrorCRF 74.52 36.45 48.95 73.44CRF+ILP 77.44 64.63 70.46 47.56IPPrec Rec F-score ErrorCRF 86.36 41.73 56.27 64.62CRF+ILP 88.75 72.95 80.08 35.61Table 1: Comparative results between our models.utterance.
The NIST Error Rate is the sum of in-sertions, deletions and substitutions divided by thetotal number of reference tags (Liu et al, 2006).Table 1 presents comparative results betweenour models.
As we can see, now the improve-ment of ILP over CRFs is not marginal as inSwitchboard.
In fact, in terms of F-score andNIST Error Rate the absolute improvement of ILPover CRFs exceeds 20% and 25% respectively.The results are statistically significant (p<10?8,Wilcoxon signed-rank test).
The main gain of ILPcomes from the large improvement in Recall.
Thisresult shows that using ILP has great potential forspeech disfluency detection when there is a lack ofin-domain data for training, and when we use lex-ical features and human transcriptions.
Further-more, the cost of applying ILP is negligible sincethe process is fast and applied during testing.Note that the improvement of ILP over CRFs issignificant even though the two corpora, Switch-board and Rapport, differ in genre (conversationversus narrative).The reason for the large improvement of ILPover CRFs is the fact that as explained aboveILP takes into account common disfluency pat-terns and generalizes from them.
CRFs can po-tentially learn similar patterns from the data butdo not generalize that well.
For example, if theCRF model learns that ?she she?
is a repetition itwill not necessarily infer that any sequence of thesame two words is a repetition (e.g.
?and and?
).Of course here, since we deal with human tran-scriptions we do not worry about speech recogni-tion errors.
Preliminary results with speech recog-nition output showed that ILP retains its advan-tages but more modestly.
In this case, when decid-ing which boosting rules to apply, it makes senseto consider speech recognition confidence scoresper word.
For example, a possible repetition ?toto?
could be the result of a misrecognition of ?todo?.
But these types of problems also affect plainCRFs, so in the end ILP is expected to continueoutperforming CRFs.
This is one of the issues forfuture work together with using prosodic features.5 ConclusionWe built a model for speech disfluency detec-tion based on CRFs using the Switchboard cor-pus.
This model was then applied to a new do-main without any adaptation.
We showed that atechnique for detecting speech disfluencies basedon ILP significantly outperforms CRFs.
In partic-ular, in terms of F-score and NIST Error Rate theabsolute improvement of ILP over CRFs exceeds20% and 25% respectively.
We conclude that ILPis an approach with great potential for speech dis-fluency detection when there is a lack or shortageof in-domain data for training.AcknowledgmentsThis work was sponsored by the U.S. Army Research, Devel-opment, and Engineering Command (RDECOM).
The con-tent does not necessarily reflect the position or the policy ofthe Government, and no official endorsement should be in-ferred.ReferencesJ.
Clarke and M. Lapata.
2008.
Global inference for sentencecompression: An integer linear programming approach.Journal of Artificial Intelligence Research, 31:399?429.K.
Georgila.
2009.
Using integer linear programming fordetecting speech disfluencies.
In Proc.
of NAACL.S.
Germesin, T. Becker, and P. Poller.
2008.
Hybrid multi-step disfluency detection.
In Proc.
of MLMI.J.
Ginzburg, R. Ferna?ndez, and D. Schlangen.
2007.
Unify-ing self- and other-repair.
In Proc.
of DECALOG.J.
Gratch, N. Wang, J. Gerten, E. Fast, and R. Duffy.
2007.Creating rapport with virtual agents.
In Proc.
of Interna-tional Conference on Intelligent Virtual Agents (IVA).P.
Heeman and J. Allen.
1999.
Speech repairs, intonationalphrases and discourse markers: Modeling speakers?
ut-terances in spoken dialogue.
Computational Linguistics,25:527?571.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting andlabeling sequence data.
In Proc.
of ICML.Y.
Liu, E. Shriberg, A. Stolcke, D. Hillard, M. Ostendorf,and M. Harper.
2006.
Enriching speech recognition withautomatic detection of sentence boundaries and disfluen-cies.
IEEE Trans.
Audio, Speech and Language Process-ing, 14(5):1526?1540.A.
Ratnaparkhi.
1998.
Maximum Entropy Models for naturallanguage ambiguity resolution.
Ph.D. thesis, University ofPennsylvania.E.
Shriberg.
2005.
Spontaneous speech: How people reallytalk, and why engineers should care.
In Proc.
of Inter-speech.A.
Stolcke and E. Shriberg.
1996.
Statistical language mod-eling for speech disfluencies.
In Proc.
of ICASSP.K.
Toutanova and C.D.
Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proc.
of EMNLP/VLC.240
