Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 329?336,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMachine learning methods for comparative and time-orientedQuality Estimation of Machine Translation outputEleftherios Avramidis and Maja Popovic?German Research Center for Artificial Intelligence (DFKI GmbH)Language Technology LabAlt Moabit 91c, 10559 Berlineleftherios.avramidis@dfki.de and maja.popovic@dfki.deAbstractThis paper describes a set of experi-ments on two sub-tasks of Quality Esti-mation of Machine Translation (MT) out-put.
Sentence-level ranking of alternativeMT outputs is done with pairwise classi-fiers using Logistic Regression with black-box features originating from PCFG Pars-ing, language models and various counts.Post-editing time prediction uses regres-sion models, additionally fed with newelaborate features from the Statistical MTdecoding process.
These seem to be betterindicators of post-editing time than black-box features.
Prior to training the models,feature scoring with ReliefF and Informa-tion Gain is used to choose feature sets ofdecent size and avoid computational com-plexity.1 IntroductionDuring the recent years, Machine Translation(MT) has reached levels of performance which al-low for its integration into real-world translationworkflows.
Despite the high speed and various ad-vantages of this technology, the fact that the MTresults are rarely perfect and often require man-ual corrections has raised a need to assess theirquality, predict the required post-editing effort andcompare outputs from various systems on applica-tion time.
This has been the aim of current re-search on Quality Estimation, which investigatessolutions for several variations of such problems.We describe possible solutions for two prob-lems of MT Quality Estimation, as part ofthe 8th Shared Task on Machine Translation:(a) sentence-level quality ranking (1.2) of multi-ple translations of the same source sentence and(b) prediction of post-editing time (1.3).
Wepresent our approach on acquiring (section 2.1)and selecting features (section 2.2), we explainthe generation of the statistical estimation systems(section 2.3) and we evaluate the developed solu-tions with some of the standard metrics (section 3).2 Methods: Quality Estimation asmachine learningThese two Quality Estimation solutions have beenseen as typical supervised machine learning prob-lems.
MT output has been given to humans, so thatthey perform either (a) ranking of the multiple MTsystem outputs in terms of meaning or (b) post-editing of single MT system output, where timeneeded per sentence is measured.
The output ofthese tasks has been provided by the shared taskorganizers as a training material, whereas a smallkeep-out set has been reserved for testing pur-poses.Our task is therefore to perform automatic qual-ity analysis of the translation output and the trans-lation process in order to provide features for thesupervised machine learning mechanism, which isthen trained over the corresponding to the respec-tive human behaviour.
The task is first optimizedin a development phase in order to produce the twobest shared task submissions for each task.
Theseare finally tested on the keep-out set so that theirperformance is compared with the ones submittedby all other shared-task participants.2.1 Feature acquisitionWe acquire two types of sentence-level features,that are expected to provide hints about the qualityof the generated translation, depending on whetherthey have access to internal details of the MT de-coding process (glass-box) or they are only de-rived from characteristics of the processed andgenerated sentence text (black-box).3292.1.1 Black-box featuresFeatures of this type are generated as a result ofautomatic analysis of both the source sentence andthe MT output (when applicable), whereas manyof them are already part of the baseline infrastruc-ture.
For all features we also calculate the ratiosof the source to the target sentence.
These featuresinclude:PCFG Features: We parse the text with a PCFGgrammar (Petrov et al 2006) and we derive thecounts of all node labels (e.g.
count of VPs, NPsetc.
), the parse log-likelihood and the number ofthe n-best parse trees generated (Avramidis et al2011).Rule-based language correction is a result ofhand-written controlled language rules, that indi-cate mistakes on several pre-defined error cate-gories (Naber, 2003).
We include the number oferrors of each category as a feature.Language model scores include the smoothedn-gram probability and the n-gram perplexity ofthe sentence.Count-based features include count and per-centage of tokens, unknown words, punctuationmarks, numbers, tokens which do or do not con-tain characters ?a-z?
; the absolute difference be-tween number of tokens in source and target nor-malized by source length, number of occurrencesof the target word within the target hypothesis av-eraged for all words in the hypothesis (type/tokenratio).Source frequency: A set of eight features in-cludes the percentage of uni-grams, bi-grams andtri-grams of the processed sentence in frequencyquartiles 1 (lower frequency words) and 4 (higherfrequency words) in the source side of a parallelcorpus (Callison-Burch et al 2012).Contrastive evaluation scores: For the rankingtask, each translation is scored with an automaticmetric (Papineni et al 2002; Lavie and Agarwal,2007), using the other translations as references(Soricut et al 2012).2.1.2 Glass-box featuresGlass-box features are available only for the time-prediction task, as a result of analyzing the verboseoutput of the Minimum Bayes Risk decoding pro-cess.Counts from the best hypothesis: Countof phrases, tokens, average/minimum/maximumphrase length, position of longest and shortestphrase in the source sentence; count of wordsunknown to the phrase table, average number ofunknown words first/last position of an unknownword in the sentence normalized to the number oftokens, variance and deviation of the position ofthe unknown words.Log probability (pC) and future cost esti-mate (c) of the phrases chosen as part of the besttranslation: minimum and maximum values andtheir position in the sentence averaged to the num-ber of sentences, and also their average, variance,standard deviation; count of the phrases whoseprobability or future cost estimate is lower andhigher than their standard deviation; the ratio ofthese phrases to the total number of phrases.Alternative translations from the search pathof the decoder: average phrase length, average ofthe average/variance/standard deviation of phraselog probability and future cost estimate, count ofalternative phrases whose log probability or futurecost estimate is lower and higher than their stan-dard deviation.2.2 Feature selectionFeature acquisition results in a huge number offeatures.
Although the machine learning mech-anisms already include feature selection or regu-larization, huge feature sets may be unusable fortraining, due to the high processing needs and thesparsity or noise they may infer.
For this purposewe first reduce the number of features by scoringthem with two popular correlation measurementmethods.2.2.1 Information gainInformation gain (Hunt et al 1966) estimates thedifference between the prior entropy of the classesand the posterior entropy given the attribute val-ues.
It is useful for estimating the quality of eachattribute but it works under the assumption thatfeatures are independent, so it is not suitable whenstrong feature inter-correlation exists.
Informationgain is only used for the sentence ranking task af-ter discretization of the feature values.2.2.2 ReliefFReliefF assesses the ability of each feature to dis-tinguish between very similar instances from dif-330ferent classes (Kononenko, 1994).
It picks up anumber of instances in random and calculates afeature contribution based on the nearest hits andmisses.
It is a robust method which can deal withincomplete and noisy data (Robnik-S?ikonja andKononenko, 2003).2.3 Machine learning algorithmsMachine learning is performed for the two sub-tasks using common pairwise classification andregression methods, respectively.2.3.1 Ranking with pairwise binaryclassifiersFor the sub-task on sentence-ranking we used pair-wise classification, so that we can take advantageof several powerful binary classification methods(Avramidis, 2012).
We used logistic regression,which optimizes a logistic function to predict val-ues in the range between zero and one (Cameron,1998), given a feature set X:P (X) = 11 + e?1(a+bX) (1)The logistic function is fitted using the Newton-Raphson algorithm to iteratively minimize theleast squares error computed from training data(Miller, 2002).
Experiments are repeated with twovariations of Logistic Regression concerning inter-nal features treatment: Stepwise Feature Set Selec-tion (Hosmer, 1989) and L2-Regularization (Linet al 2007).2.3.2 RegressionFor the sub-task on post-editing time prediction,we experimented with several regression meth-ods, such as Linear Regression, Partial LeastSquares (Stone and Brooks, 1990), MultivariateAdaptive Regression Splines (Friedman, 1991),LASSO (Tibshirani, 1996), Support Vector Regres-sion (Basak et al 2007) and Tree-based regres-sors.
Indicatively, Linear regression optimizes co-efficient ?
for predicting a value y, given a featurevector X:y = X?
+ ?
(2)2.4 EvaluationThe ranking task is evaluated by measuring cor-relation between the predicted and the humanranking, with the use of Kendall tau (Kendall,1938) including penalization of ties.
We addi-tionally consider two more metrics specialized inranking tasks: Mean Reciprocal Rank - MRR(Voorhees, 1999) and Normalized Discounted Cu-mulative Gain - NDGC (Ja?rvelin and Keka?la?inen,2002), which give better scores to models whenhigher ranks (i.e.
better translations) are orderedcorrectly, as these are more important than lowerranks.The regression task is evaluated in terms of RootMean Square Error (RMSE) and Mean AverageError (MAE).3 Experiment and Results3.1 ImplementationRelieff is implemented for k=5 nearest neighbourssampling m=100 reference instances.
Informationgain is calculated after discretizing features inton=100 valuesN-gram features are computed with the SRILMtoolkit (Stolcke, 2002) with an order of 5, basedon monolingual training material from Europarl(Koehn, 2005) and News Commentary (Callison-Burch et al 2011).
PCFG parsing features aregenerated on the output of the Berkeley Parser(Petrov and Klein, 2007) trained over an English,a German and a Spanish treebank (Taule?
et al2008).
The open source language tool1 is usedto annotate source and target sentences with lan-guage suggestions.
The annotation process is or-ganised with the Ruffus library (Goodstadt, 2010)and the learning algorithms are executed using theOrange toolkit (Dems?ar et al 2004).3.2 Sentence-rankingThe sentence-ranking sub-task has provided train-ing data for two language pairs, German-Englishand English-Spanish.
For both sentence pairs,we train the systems using the provided an-notated data sets WMT2010, WMT2011 andWMT2012, while the data set WMT2009 is usedfor the evaluation during the development phase.Data sets are analyzed with black-box feature gen-eration.
For each language pair, the two systemswith the highest correlation are submitted.We start the development with two feature setsthat have shown to perform well in previous ex-periments: #24 (Avramidis, 2012) including fea-tures from PCFG parsing, and #31 which is thebaseline feature set of the previous year?s sharedtask (Callison-Burch et al 2012) and we combinethem (#33).
Additionally, we create feature sets by1Open source at http://languagetool.org331de-en en-esid feature-set tau MRR NDGC tau MRR NDGC#24 previous (Avramidis, 2012) 0.28 0.57 0.78 0.09 0.52 0.75#31 baseline WMT2012 0.04 0.51 0.74 -0.16 0.43 0.69#32 vanilla WMT2013 0.04 0.51 0.74 -0.13 0.45 0.70#33 combine #24 and #31 0.29 0.57 0.78 0.10 0.53 0.75#41 ReliefF 15 best 0.20 0.56 0.77 0.02 0.48 0.72#411 ReliefF 5 best 0.22 0.53 0.76 0.19 0.49 0.73#42 InfGain 15 best 0.15 0.53 0.75 -0.14 0.43 0.69#43 combine #41 and #42 0.22 0.56 0.77 -0.12 0.44 0.70#431 combine #41, #42 and #24 0.27 0.60 0.80 0.11 0.54 0.75Table 1: Development experiments for task 1.2, reporting correlation and ranking scores, tested on thedevelopment set WMT2009.target feature ?avg target word occurrence 2.18pseudoMETEOR 0.71count of unknown words 0.55count of dots -0.25count of commas 0.15count of tokens -0.13count of VPs -0.06PCFGlog -0.02lmprob 0.01Table 3: Beta coefficients of the best fitted logisticregression on the German-English data set (set #33with Stepwise Feature Set Selection)scoring features with ReliefF (features #41x) andInformation Gain (#42).
Many combinations of allthe above feature-sets are tested and the most im-portant of them are shown in Table 1.
Feature setsare described briefly in Table 2.For German-English, we experiment with 14feature sets, using both variations of Logistic Re-gression.
The two highest tau scores are given byStepwise Feature Set Selection using feature sets#33 and #24.
We see that although baseline fea-tures #31 alone have very low correlation, whencombined with previously successful #24, providethe best system in terms of tau.
Feature set #431(which combines the 15 features scored higherwith ReliefF, the 15 features scored higher with In-formation Gain and the feature set #24) succeedspretty well on the additional metrics MRR andNDGC, but it provides slightly lower tau correla-tion.For English-Spanish, the correlation of the pro-duced systems is significantly lower and it ap-pears that the L2-regularized logistic regressionperforms better as classification method.
We ex-periment with 24 feature sets, after more scor-ing with ReliefF and Inf.
Gain.
Surprisinglyenough, Kendall tau correlation indicates that thebest model is trained only with features basedtarget feature ?count of unknown words -0.55count of VPs 0.19count of of PCFG parse trees -0.16count of tokens 0.15% of tokens with only letters -0.07lmprob -0.06pseudoMETEOR precision -0.05source/target ratio of parse trees -0.03Table 4: Most indicative beta coefficients ofthe best fitted logistic regression on the English-Spanish data set (set #431 with L2-regularization)on counts of numbers and punctuation, combinedwith contrastive BLEU score.
This seems to ratheroverfit a peculiarity of the particular developmentset and indeed performs much lower on the finaltest set of the shared task (tau=0.04).
The secondbest feature set (#431) has been described aboveand luckily generalizes better on an unknown set.It is interesting to see that this issue would havebeen avoided, if the decision was taken based onthe ranking metrics MRR and NDGC, which pri-oritize other feature sets.
We assume that furtherwork is needed to see whether these measures aremore expressive and reliable than Kendall tau forsimilar tasks.The fitted ?
coefficients (in tables 3 and 4) givean indication of the importance of each feature(see equation 1), for each language pair.
In bothlanguage pairs, target-side features prevail uponother features.
On the comparison of the modelsfor the two language pairs (and the ?
coefficientsas well) we can see that the model settings andperformance may vary from one language pair toanother.
This also requires further investigation,given that Kendall tau and the other two metricsindicate different models as the best ones.The fact that the German-English set is bet-ter fitted with Stepwise Feature Set Selec-332set features#24 From previous work (Avramidis, 2012):[s+t]: PCFGlog , count of: unknown words, tokens, PCFG trees, VPs[t]: pseudoMETEOR#31 Baseline from WMT12 (Callison-Burch et al 2012)[s+t]: tokensavg , lmprob, count of: commas, dots, tokens, avg translations per source word[s]: avg freq.
of low and high freq.
bi-grams/tri-grams, % of distinct uni-grams in the corpus[t]: type/token radio#32 All 50 ?vanilla?
features provided by shared-task baseline software ?Quest?#411 ReliefF best 5 features[s+t]: % of numbers, difference between periods of source and target (plain and averaged)[t]: pseudoBLEUTable 2: Description of most important feature sets for task 1.2, before internal feature selection ofLogistic Regression is applied.
[s] indicates source, [t] indicates targetde-en en-esset StepFSS L2reg StepFSS L2reg#24 0.28 0.25 0.09 0.09#33 0.29 0.26 0.08 0.10#411 0.22 0.17 -0.25 0.19#431 0.27 0.25 0.09 0.11Table 5: Higher Kendall tau correlation (on thedev.
set) is achieved on German-English by us-ing Stepwise Feature Set Selection, whereas onEnglish-Spanish by using L2-regularizationtion, whereas the English-Spanish one with L2-Regularization (table 5) may be explained bythe statistical theory about these two methods:The Stepwise method has has been proven to betoo bound to particular characteristics of the de-velopment set (Flom and Cassell, 2007).
L2-Regularization has been suggested as an alterna-tive, since it generalizes better on broader datasets, which is probably the case for English-Spanish.Our method also seems to perform well whencompared to evaluation metrics which have accessto reference translations, as shown in this year?sMetrics Shared Task (Macha?c?ek and Ondr?ej,2013).3.3 Post-editing time predictionThe training for the model predicting post-editingtime is performed over the entire given data setand the evaluation is done with 10-fold cross-validation.
We evaluated 8 feature sets with 6 re-gression methods each, ending up with 48 experi-ments.The evaluation of the most indicative regressionmodels (two best performing ones per feature set)can be seen in Table 6.
We start with a glass-1 7310 19 28 37 46 55 64 82 91 100109118127136145154163172181190199208217226235050100150200250300350400450500REFHYPFigure 1: Graphical representation of the valuespredicted by the linear regression model with fea-ture set #6 (blue) against the actual values of thedevelopment set (red)box feature set, scored with ReliefF and conse-quently add black-box features.
We note the mod-els that have the lowest Root Mean Square Errorand Mean Average Error.Our best model seems to be the one built linearregression using feature set #6.
This feature set ischosen by collecting the 17 best features as scoredby ReliefF and includes both black-box and glass-box features.
How well this model fits the devel-opment test is represented in Figure 1.The second best feature set (#8) includes 29glass-box features with the highest absolute Reli-efF, joined with the black-box features of the suc-cessful feature set #6.More details about the contribution of the mostimportant features in the linear regression (equa-tion 2) can be seen in table 7, where the fitted ?coefficients of each feature are given.
The vastmajority of the best contributing features are glass-box features.
Some draft conclusions out of thecoefficients may be that post-editing time is lowerwhen:333id feature set method RMSE MAE#1 20 glass-box features with highest absolute ReliefF MARS 91.54 59.07SVR 93.57 55.87#2 9 glass-box features with highest positive ReliefF Lasso 83.20 51.57Linear 83.32 51.72#3 16 glass-box features with highest positive ReliefF Lasso 77.54 47.16Linear 77.60 47.27#4 22 glass-box features with highest positive ReliefF Lasso 76.05 46.37Linear 76.17 46.48#5 Combination of feature sets #1 and #2 MARS 91.54 59.07SVR 93.57 55.87#6 17 features of any type with highest positive ReliefF Linear 74.70 45.20Lasso 74.75 44.99#8 Combination of #5 and #6 + counts of tokens Lasso 75.14 44.99PLS 77.63 47.48#6 First submission Linear 84.27 52.41#8 Second submission PLS 88.34 53.49Best models 82.60 47.52Table 6: Development and submitted experiments for task 1.3?
the longest of the source phrases used for pro-ducing the best hypothesis appears closer tothe end of the sentence?
the phrases with the highest and the lowestprobability appear closer to the end of thetranslated sentence?
there are more determiners in the sourceand/or less determiners in the translation?
there are more verbs in the translation and/orless verbs in the source?
there are fewer alternative phrases with veryhigh probabilityFurther conclusions can be drawn after examiningthese observations along with the exact operationof the statistical MT system, which is subject tofurther work.4 ConclusionWe describe two approaches for two respectiveproblems of quality estimation, namely sentence-level ranking of alternative translations and pre-diction of time for post-editing MT output.
Wepresent efforts on compiling several feature setsand we examine the final contribution of the fea-tures after training Machine Learning models.Elaborate decoding features seem to be quite help-ful for predicting post-editing time.feature ?best hyp: position of the longest alignedphrase in the source sentence averaged tothe number of phrases-16.652best hyp: position of phrase with highestprob.
averaged to the num.
of phrases -14.824source: number of determiners -9.312best hyp: number of determiners 6.189best hyp: position of phrase with lowestprob.
averaged to the num.
of phrases -5.261best hyp: position of phrase with lowestfuture cost estimate averaged to thenumber of phrases-4.282best hyp: number of verbs -2.818best hyp: position of phrase with highestfuture cost estimate averaged to thenumber of phrases1.002search: number of alternative phraseswith very low future cost est.
-0.528source: number of verbs 0.467search: number of alternative phraseswith very high probability 0.355search: total num.
of translation options -0.153search: number of alternative phraseswith very high future cost estimate -0.142best hyp: number of parse trees 0.007source: number of parse trees 0.002search: total number of hypotheses 0.001Table 7: Linear regression coefficients for featureset #6 indicate the contribution of each feature inthe fitted model334AcknowledgmentsThis work has been developed within the TaraXU?project, financed by TSB TechnologiestiftungBerlin ?
Zukunftsfonds Berlin, co-financed by theEuropean Union ?
European fund for regional de-velopment.
Many thanks to Prof. Hans Uszko-reit for the supervision, Dr. Aljoscha Burchardt,and Dr. David Vilar for their useful feedback andto Lukas Poustka for his technical help on featureacquisition.ReferencesAvramidis, E. (2012).
Comparative Quality Estima-tion: Automatic Sentence-Level Ranking of Multi-ple Machine Translation Outputs.
In Proceedingsof 24th International Conference on ComputationalLinguistics, pages 115?132, Mumbai, India.
TheCOLING 2012 Organizing Committee.Avramidis, E., Popovic, M., Vilar, D., and Burchardt,A.
(2011).
Evaluate with Confidence Estimation :Machine ranking of translation outputs using gram-matical features.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 65?70, Edinburgh, Scotland.
Association for Computa-tional Linguistics.Basak, D., Pal, S., and Patranabis, D. C. (2007).Support vector regression.
Neural InformationProcessing-Letters and Reviews, 11(10):203?224.Callison-Burch, C., Koehn, P., Monz, C., Post, M.,Soricut, R., and Specia, L. (2012).
Findings of the2012 Workshop on Statistical Machine Translation.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, pages 10?51, Montre?al,Canada.
Association for Computational Linguistics.Callison-Burch, C., Koehn, P., Monz, C., and Zaidan,O.
(2011).
Findings of the 2011 Workshop on Sta-tistical Machine Translation.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 22?64, Edinburgh, Scotland.
Association forComputational Linguistics.Cameron, A.
(1998).
Regression analysis of countdata.
Cambridge University Press, Cambridge UK;New York NY USA.Dems?ar, J., Zupan, B., Leban, G., and Curk, T. (2004).Orange: From Experimental Machine Learning toInteractive Data Mining.
In Principles of Data Min-ing and Knowledge Discovery, pages 537?539.Flom, P. L. and Cassell, D. L. (2007).
Stopping step-wise: Why stepwise and similar selection methodsare bad, and what you should use.
In NorthEastSAS Users Group Inc 20th Annual Conference, Bal-timore, Maryland.
2007.Friedman, J. H. (1991).
Multivariate Adaptive Regres-sion Splines.
The Annals of Statistics, 19(1):1?67.Goodstadt, L. (2010).
Ruffus: a lightweight Pythonlibrary for computational pipelines.
Bioinformatics,26(21):2778?2779.Hosmer, D. (1989).
Applied logistic regression.
Wiley,New York [u.a.
], 8th edition.Hunt, E., Martin, J., and Stone, P. (1966).
Experimentsin Induction.
Academic Press, New York.Ja?rvelin, K. and Keka?la?inen, J.
(2002).
Cumulatedgain-based evaluation of IR techniques.
ACM Trans.Inf.
Syst., 20(4):422?446.Kendall, M. G. (1938).
A New Measure of Rank Cor-relation.
Biometrika, 30(1-2):81?93.Koehn, P. (2005).
Europarl: A parallel corpus for sta-tistical machine translation.
Proceedings of the tenthMachine Translation Summit, 5:79?86.Kononenko, I.
(1994).
Estimating attributes: analy-sis and extensions of RELIEF.
In Proceedings ofthe European conference on machine learning onMachine Learning, pages 171?182, Secaucus, NJ,USA.
Springer-Verlag New York, Inc.Lavie, A. and Agarwal, A.
(2007).
METEOR: An Au-tomatic Metric for MT Evaluation with High Levelsof Correlation with Human Judgments.
In Proceed-ings of the Second Workshop on Statistical MachineTranslation, pages 228?231, Prague, Czech Repub-lic.
Association for Computational Linguistics.Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trustregion Newton methods for large-scale logistic re-gression.
In Proceedings of the 24th internationalconference on Machine learning - ICML ?07, pages561?568, New York, New York, USA.
ACM Press.Macha?c?ek, M. .
and Ondr?ej, B.
(2013).
Results of theWMT13 Metrics Shared Task.
In Proceedings of the8th Workshop on Machine Translation, Sofia, Bul-garia.
Association for Computational Linguistics.Miller, A.
(2002).
Subset Selection in Regression.Chapman & Hall, London, 2nd edition.Naber, D. (2003).
A rule-based style and grammarchecker.
Technical report, Bielefeld University,Bielefeld, Germany.Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.(2002).
BLEU: a Method for Automatic Evalua-tion of Machine Translation.
In Proceedings of the40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA.
Association for ComputationalLinguistics.Petrov, S., Barrett, L., Thibaux, R., and Klein, D.(2006).
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
In Proceedings of the 21st335International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 433?440, Syd-ney, Australia.
Association for Computational Lin-guistics.Petrov, S. and Klein, D. (2007).
Improved inference forunlexicalized parsing.
In Proceedings of the 2007Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics, Rochester, New York.
Association for Compu-tational Linguistics.Robnik-S?ikonja, M. and Kononenko, I.
(2003).
Theo-retical and Empirical Analysis of ReliefF and RRe-liefF.
Machine Learning, 53(1-2):23?69.Soricut, R., Wang, Z., and Bach, N. (2012).
The SDLLanguage Weaver Systems in the WMT12 QualityEstimation Shared Task.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation,pages 145?151, Montre?al, Canada.
Association forComputational Linguistics.Stolcke, A.
(2002).
SRILM ?
An Extensible LanguageModeling Toolkit.
In Proceedings of the SeventhInternational Conference on Spoken Language Pro-cessing, pages 901?904.
ISCA.Stone, M. and Brooks, R. J.
(1990).
Continuum re-gression: cross-validated sequentially constructedprediction embracing ordinary least squares, par-tial least squares and principal components regres-sion.
Journal of the Royal Statistical Society SeriesB Methodological, 52(2):237?269.Taule?, M., Mart?
?, A., and Recasens, M. (2008).
An-Cora: Multilevel Annotated Corpora for Catalan andSpanish.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC?08), Marrakech, Morocco.
European Lan-guage Resources Association (ELRA).Tibshirani, R. (1996).
Regression shrinkage and selec-tion via the lasso.
Series B:267?288.Voorhees, E. (1999).
TREC-8 Question AnsweringTrack Report.
In 8th Text Retrieval Conference,pages 77?82, Gaithersburg, Maryland, USA.336
