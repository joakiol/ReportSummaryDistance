Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 464?467,Prague, June 2007. c?2007 Association for Computational LinguisticsUTH: Semantic Relation Classification using Physical SizesEiji ARAMAKI Takeshi IMAI Kengo MIYO Kazuhiko OHEThe University of Tokyo Hospital department7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japanaramaki@hcc.h.u-tokyo.ac.jpAbstractAlthough researchers have shown increas-ing interest in extracting/classifying seman-tic relations, most previous studies have ba-sically relied on lexical patterns betweenterms.
This paper proposes a novel way toaccomplish the task: a system that capturesa physical size of an entity.
Experimentalresults revealed that our proposed method isfeasible and prevents the problems inherentin other methods.1 IntroductionClassification of semantic relations is important toNLP as it would benefit many NLP applications,such as machine translation and information re-trieval.Researchers have already proposed variousschemes.
For example, Hearst (1992) manually de-signed lexico-syntactic patterns for extracting is-arelations.
Berland and Charniak (1999) proposed asimilar method for part-whole relations.
Brin (1998)employed a bootstrapping algorithm for more spe-cific relations (author-book relations).
Kim andBaldwin (2006) and Moldovan et al(2004) focusedon nominal relations in compound nouns.
Turney(2005) measured relation similarity between twowords.
While these methods differ, they all utilizelexical patterns between two entities.Within this context, our goal was to utilize infor-mation specific to an entity.
Although entities con-tain many types of information, we focused on thephysical size of an entity.
Here, physical size refersto the typical width/height of an entity.
For example,we consider book to have a physical size of 20?25cm, and book to have a size of 10?10 m, etc.We chose to use physical size for the followingreasons:1.
Most entities (except abstract entities) have aphysical size.2.
Several semantic relations are sensitive to phys-ical size.
For example, a content-container rela-tion (e1 content-container e2) naturally meansthat e1 has a smaller size than e2.
A book isalso smaller than its container, library.
A part-whole relation has a similar constraint.Our next problem was how to determine physi-cal sizes.
First, we used Google to conduct Websearches using queries such as ?book (*cm x*cm)?and ?library (*m x*m)?.
Next, we extracted numericexpressions from the search results and used the av-erage value as the physical size.Experimental results revealed that our proposedapproach is feasible and prevents the problems in-herent in other methods.2 CorpusWe used a corpus provided by SemEval2007 Task#4 training set.
This corpus consisted of 980 anno-tated sentences (140 sentences?7 relations).
Table1 presents an example.Although the corpus contained a large quantity ofinformation such as WordNet sense keys, comments,etc., we used only the most pertinent information:entity1 (e1), entity2 (e2), and its relation (true/false)464The <e1>library</e1> contained <e2>books</e2> of guidance on the processes.WordNet(e1) = "library\%1:14:00::",WordNet(e2) = "book\%1:10:00::",Content-Container(e2, e1) = "true",Query = "the * contained books"Table 1: An Example of Task#4 Corpus.Figure 1: Three types of Features.1.
For example, we extracted a triple example (li-brary, book, true from Table 1.3 MethodWe applied support vector machine (SVM)-basedlearning (Vapnik, 1999) using three types of fea-tures: (1) basic pattern features (Section 3.1), (2) se-lected pattern features (Section 3.2), and (3) physicalsize features (Section 3.3).
Figure 1 presents someexamples of these features.3.1 Basic Pattern FeaturesFirst, the system finds lexical patterns that co-occurwith semantic relations between two entities (e1 ande2).
It does so by conducting searches using twoqueries ?e1 * e2?
and ?e2 * e1?.
For example, twoqueries, ?library * book?
and ?book * library?, aregenerated from Table 1.Then, the system extracts the word (or word se-quences) between two entities from the snippets inthe top 1,000 search results.
We considered the ex-tracted word sequences to be basic patterns.
For ex-ample, given ?...library contains the book...?, the ba-sic pattern is ?
(e1) contains the (e2)?
2.1Our system is classified as an A4 system, and thereforedoes not use WordNet or Query.2This operation does not handle any stop-words.
Therefore,We gathered basic patterns for each relation, andidentified if each pattern had been obtained as aSVM feature or not (1 or 0).
We refer to these fea-tures as basic pattern features.3.2 Selected Pattern FeaturesBecause basic pattern features are generated onlyfrom snippets, precise co-occurrence statistics arenot available.
Therefore, the system searches againwith more specific queries, such as ?library containsthe book?.
However, this second search is a heavyburden for a search engine, requiring huge numbersof queries (# of samples ?
# of basic patterns).We thus selected the most informative n patterns(STEP1) and conducted specific searches (# of sam-ples ?
n basic patterns)(STEP2) as follows:STEP1: To select the most informative patterns,we applied a decision tree (C4.5)(Quinlan,1987) and selected the basic patterns located inthe top n branches 3.STEP2: Then, the system searched again us-ing the selected patterns.
We considered logweighted hits (log10|hits|) to be selected pat-tern features.
For example, if ?library containsthe book?
produced 120,000 hits in Google, ityields the value log10(12, 000) = 5.3.3 Physical Size FeaturesAs noted in Section 1, we theorized that an entity?ssize could be a strong clue for some semantic rela-tions.We estimated entity size using the followingqueries:1.
?< entity > (* cm x * cm)?,2.
?< entity > (* x * cm)?,3.
?< entity > (* m x * m)?,4.
?< entity > (* x * m)?.In these queries, < entity > indicates a slot foreach entity, such as ?book?, ?library?, etc.
Then, thesystem examines the search results for the numerousexpressions located in ?*?
and considers the averagevalue to be the size.?
(e1) contains THE (e2)?
and ?
(e1) contains (e2)?
are differentpatterns.3In the experiments in Section 4, we set n = 10.465Precision Recall F?=1PROPOSED 0.57 (=284/497) 0.60 (=284/471) 0.58+SEL 0.56 (=281/496) 0.59 (=281/471) 0.57+SIZE 0.53 (=269/507) 0.57 (=269/471) 0.54BASELINE 0.53 (=259/487) 0.54 (=259/471) 0.53Table 2: Results.When results of size expressions were insufficient(numbers < 10), we considered the entity to be non-physical, i.e., to have no size.By applying the obtained sizes, the system gener-ates a size feature, consisting of six flags:1.
LARGE-e1: (e1?s X > e2?s X) and (e1?s Y > e2?s Y)2.
LARGE-e2: (e1?s X < e2?s X) and (e1?s Y < e2?s Y)3.
NOSIZE-e1: only e1 has no size.4.
NOSIZE-e2: only e2 has no size.5.
NOSIZE-BOTH: Both e1 and e2 have no size.6.
OTHER: Other.4 Experiments4.1 Experimental Set-upTo evaluate the performance of our system, weused a SemEval-Task No#4 training set.
We com-pared the following methods using a ten-fold cross-validation test:1.
BASELINE: with only basic pattern features.2.
+SIZE: BASELINE with size features.3.
+SEL: BASELINE with selected pattern features.4.
PROPOSED: BASELINE with both size and selectedpattern features.For SVM learning, we used TinySVM with a lin-ear kernel4.4.2 ResultsTable 2 presents the results.
PROPOSED was themost accurate, demonstrating the basic feasibility ofour approach.Table 3 presents more detailed results.
+SIZEmade a contribution to some relations (REL2 andREL4).
Particularly for REL4, +SIZE significantlyboosted accuracy (using McNemar tests (Gillick and4http://chasen.org/ taku/software/TinySVM/Figure 2: The Size of a ?Car?.Cox, 1989); p = 0.05).
However, contrary to our ex-pectations, size features were disappointing for part-whole relations (REL6) and content-container rela-tions (REL7).The reason for this was mainly the difficulty in es-timating size.
Table 4 lists the sizes of several enti-ties, revealing some strange results, such as a librarysized 12.1 ?
8.4 cm, a house sized 53 ?
38 cm, anda car sized 39 ?
25 cm.
These sizes are unusuallysmall for the following reasons:1.
Some entities (e.g.?car?)
rarely appear withtheir size,2.
In contrast, entities such as ?toy car?
or ?minicar?
frequently appear with a size.Figure 2 presents the size distribution of ?car.
?Few instances appeared of real cars sized approxi-mately 500 ?
400 cm, while very small cars smallerthan 100 ?
100 cm appeared frequently.
Our currentmethod of calculating average size is ineffective un-der this type of situation.In the future, using physical size as a clue for de-termining a semantic relation will require resolvingthis problem.5 ConclusionWe briefly presented a method for obtaining the sizeof an entity and proposed a method for classifyingsemantic relations using entity size.
Experimentalresults revealed that the proposed approach yieldedslightly higher performance than a baseline, demon-strating its feasibility.
If we are able to estimate en-466Relation PROPOSED +SEL +SIZE BASELINEPrecision 0.60 (=50/83) 0.56 (=53/93) 0.54 (=53/98) 0.50 (=53/106)REL1 Recall 0.68 (=50/73) 0.72 (=53/73) 0.72 (=53/73) 0.72 (=53/73)(Cause-Effect) F?=10.64 0.63 0.59 0.61Precision 0.59 (=43/72) 0.60 (=44/73) 0.56 (=45/79) 0.55 (=44/79)REL2 Recall 0.60 (=43/71) 0.61 (=44/71) 0.63 (=45/71) 0.61 (=44/71)(Instrument-Agency) F?=10.60 0.61 0.59 0.58Precision 0.70 (=56/80) 0.73 (=55/75) 0.65 (=54/82) 0.68 (=51/74)REL3 Recall 0.65 (=56/85) 0.64 (=55/85) 0.63 (=54/85) 0.60 (=51/85)(Product-Producer) F?=10.67 0.68 0.64 0.64Precision 0.41 (=23/56) 0.35 (=18/51) 0.48 (=24/49) 0.52 (=13/25)REL4 Recall 0.42 (=23/54) 0.33 (=18/54) 0.44 (=24/54) 0.24 (=13/54)(Origin-Entity) F?=10.41 0.34 0.46 0.32Precision 0.62 (=40/64) 0.61 (=40/65) 0.56 (=28/50) 0.56 (=29/51)REL5 Recall 0.68 (=40/58) 0.68 (=40/58) 0.48 (=28/58) 0.50 (=29/58)(Theme-Tool) F?=10.65 0.65 0.51 0.53Precision 0.45 (=46/101) 0.46 (=46/100) 0.41 (=49/118) 0.43 (=53/123)REL6 Recall 0.70 (=46/65) 0.70 (=46/65) 0.75 (=49/65) 0.81 (=53/65)(Part-Whole) F?=10.55 0.55 0.53 0.56Precision 0.63 (26/41) 0.64 (=25/39) 0.51 (=16/31) 0.55 (=16/29)REL7 Recall 0.40 (26/65) 0.38 (=25/65) 0.24 (=16/65) 0.24 (=16/65)(Content-Container) F?=10.49 0.48 0.33 0.34Table 3: Detailed Results.entity # sizelibrary 51 12.1?8.4 mroom 204 5.4?3.5 mman 75 1.5?0.5 mbenches 33 93?42 cmgranite 68 76?48 cmsink 34 57?25 cmhouse 86 53?38 cmbooks 50 46?24 cmcar 91 39?25 cmturtles 15 38?23 cmfood 38 35?26 cmoats 16 24?13 cmtumor shrinkage 6 -habitat degradation 5 -Table 4: Some Examples of Entity Sizes.?#?
indicates the number of obtained size expressions.?-?
indicates a ?NO-SIZE?
entity.tity sizes more precisely in the future, the systemwill become much more accurate.ReferencesMatthew Berland and Eugene Charniak.
1999.
Finding partsin very large corpora.
In Proceedings of the Annual Con-ference of the Association for Computational Linguistics(ACL1999), pages 57?64.Sergey Brin.
1998.
Extracting patterns and relations from theworld wide web.
In WebDB Workshop at 6th InternationalConference on Extending Database Technology, EDBT?98,pages 172?183.L.
Gillick and SJ Cox.
1989.
Some statistical issues in the com-parison of speech recognition algorithms.
In Proceedings ofIEEE International Conference on Acoustics, Speech, andSignal Processing, pages 532?535.M.
Hearst.
1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In Proceedings of International Confer-ence on Computational Linguistics (COLING1992), pages539?545.Su Nam Kim and Timothy Baldwin.
2006.
Interpreting seman-tic relations in noun compounds via verb semantics.
In Pro-ceedings of the COLING/ACL 2006 Main Conference PosterSessions, pages 491?498.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.2004.
Models for the semantic classification of nounphrases.
Proceedings of HLT/NAACL-2004 Workshop onComputational Lexical Semantics.J.R.
Quinlan.
1987.
Simplifying decision trees.
InternationalJournal of Man-Machine Studies, 27(1):221?234.Peter D. Turney.
2005.
Measuring semantic similarity by latentrelational analysis.
In Proceedings of the Nineteenth Inter-national Joint Conference on Artificial Intelligence (IJCAI-05), pages 1136?1141.Vladimir Vapnik.
1999.
The Nature of Statistical LearningTheory.
Springer-Verlag.467
