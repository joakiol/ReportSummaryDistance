Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsExtraction and Approximation of Numerical Attributes from the WebDmitry DavidovICNCThe Hebrew UniversityJerusalem, Israeldmitry@alice.nc.huji.ac.ilAri RappoportInstitute of Computer ScienceThe Hebrew UniversityJerusalem, Israelarir@cs.huji.ac.ilAbstractWe present a novel framework for auto-mated extraction and approximation of nu-merical object attributes such as heightand weight from the Web.
Given anobject-attribute pair, we discover and ana-lyze attribute information for a set of com-parable objects in order to infer the desiredvalue.
This allows us to approximate thedesired numerical values even when no ex-act values can be found in the text.Our framework makes use of relationdefining patterns and WordNet similarityinformation.
First, we obtain from theWeb and WordNet a list of terms similar tothe given object.
Then we retrieve attributevalues for each term in this list, and infor-mation that allows us to compare differentobjects in the list and to infer the attributevalue range.
Finally, we combine the re-trieved data for all terms from the list toselect or approximate the requested value.We evaluate our method using automatedquestion answering, WordNet enrichment,and comparison with answers given inWikipedia and by leading search engines.In all of these, our framework provides asignificant improvement.1 IntroductionInformation on various numerical properties ofphysical objects, such as length, width and weightis fundamental in question answering frameworksand for answering search engine queries.
Whilein some cases manual annotation of objects withnumerical properties is possible, it is a hard andlabor intensive task, and is impractical for dealingwith the vast amount of objects of interest.
Hence,there is a need for automated semantic acquisitionalgorithms targeting such properties.In addition to answering direct questions, theability to make a crude comparison or estimationof object attributes is important as well.
For ex-ample, it allows to disambiguate relationships be-tween objects such as X part-of Y or X inside Y.Thus, a coarse approximation of the height of ahouse and a window is sufficient to decide thatin the ?house window?
nominal compound, ?win-dow?
is very likely to be a part of house and notvice versa.
Such relationship information can, inturn, help summarization, machine translation ortextual entailment tasks.Due to the importance of relationship and at-tribute acquisition in NLP, numerous methodswere proposed for extraction of various lexical re-lationships and attributes from text.
Some of thesemethods can be successfully used for extractingnumerical attributes.
However, numerical attributeextraction is substantially different in two aspects,verification and approximation.First, unlike most general lexical attributes, nu-merical attribute values are comparable.
It usuallymakes no sense to compare the names of two ac-tors, but it is meaningful to compare their ages.The ability to compare values of different objectsallows to improve attribute extraction precision byverifying consistency with attributes of other sim-ilar objects.
For example, suppose that for Toy-ota Corolla width we found two different values,1.695m and 27cm.
The second value can be eitheran extraction error or a length of a toy car.
Ex-tracting and looking at width values for differentcar brands and for ?cars?
in general we find:?
Boundaries: Maximal car width is 2.195m,minimal is 88cm.?
Average: Estimated avg.
car width is 1.7m.?
Direct/indirect comparisons: Toyota Corollais wider than Toyota Corona.?
Distribution: Car width is distributed nor-mally around the average.1308Usage of all this knowledge allows us to select thecorrect value of 1.695m and reject other values.Thus we can increase the precision of value ex-traction by finding and analyzing an entire groupof comparable objects.Second, while it is usually meaningless and im-possible to approximate general lexical attributevalues like an actor?s name, numerical attributescan be estimated even if they are not explicitlymentioned in the text.In general, attribute extraction frameworks usu-ally attempt to discover a single correct value (e.g.,capital city of a country) or a set of distinct correctvalues (e.g., actors of a movie).
So there is es-sentially nothing to do when there is no explicitinformation present in the text for a given objectand an attribute.
In contrast, in numerical attributeextraction it is possible to provide an approxima-tion even when no explicit information is presentin the text, by using values of comparable objectsfor which information is provided.In this paper we present a pattern-based frame-work that takes advantage of the properties of sim-ilar objects to improve extraction precision andallow approximation of requested numerical ob-ject properties.
Our framework comprises threemain stages.
First, given an object name we uti-lize WordNet and pattern-based extraction to finda list of similar objects and their category labels.Second, we utilize a predefined set of lexical pat-terns in order to extract attribute values of theseobjects and available comparison/boundary infor-mation.
Finally, we analyze the obtained informa-tion and select or approximate the attribute valuefor the given (object, attribute) pair.We performed a thorough evaluation using threedifferent applications: Question Answering (QA),WordNet (WN) enrichment, and comparison withWikipedia and answers provided by leading searchengines.
QA evaluation was based on a designeddataset of 1250 questions on size, height, width,weight, and depth, for which we created a goldstandard and compared against it automatically1.For WN enrichment evaluation, our frameworkdiscovered size and weight values for 300 WNphysical objects, and the quality of results wasevaluated by human judges.
For interactive search,we compared our results to information obtainedthrough Wikipedia, Google and Wolfram Alpha.1This dataset is available in the authors?
websites for theresearch community.Utilization of information about comparable ob-jects provided a significant boost to numerical at-tribute extraction quality, and allowed a meaning-ful approximation of missing attribute values.Section 2 discusses related work, Section 3 de-tails the algorithmic framework, Section 4 de-scribes the experimental setup, and Section 5presents our results.2 Related workNumerous methods have been developed for ex-traction of diverse semantic relationships fromtext.
While several studies propose relationshipidentification methods using distributional analy-sis of feature vectors (Turney, 2005), the major-ity of the proposed open-domain relations extrac-tion frameworks utilize lexical patterns connect-ing a pair of related terms.
(Hearst, 1992) man-ually designed lexico-syntactic patterns for ex-tracting hypernymy relations.
(Berland and Char-niak, 1999; Girju et al 2006) proposed a set ofpatterns for meronymy relations.
Davidov andRappoport (2008a) used pattern clusters to disam-biguate nominal compound relations.
Extensiveframeworks were proposed for iterative discov-ery of any pre-specified (e.g., (Riloff and Jones,1999; Chklovski and Pantel, 2004)) and unspec-ified (e.g., (Banko et al, 2007; Rosenfeld andFeldman, 2007; Davidov and Rappoport, 2008b))relation types.The majority of the above methods utilize thefollowing basic strategy.
Given (or discoveringautomatically) a set of patterns or relationship-representing term pairs, these methods mine theweb for these patterns and pairs, iteratively obtain-ing more instances.
The proposed strategies gen-erally include some weighting/frequency/context-based algorithms (e.g.
(Pantel and Pennacchiotti,2006)) to reduce noise.
Some of the methods aresuitable for retrieval of numerical attributes.
How-ever, most of them do not exploit the numericalnature of the attribute data.Our research is related to a sub-domain of ques-tion answering (Prager, 2006), since one of theapplications of our framework is answering ques-tions on numerical values.
The majority of theproposed QA frameworks rely on pattern-basedrelationship acquisition (Ravichandran and Hovy,2009).
However, most QA studies focus on dif-ferent types of problems than our paper, includingquestion classification, paraphrasing, etc.1309Several recent studies directly target the acqui-sition of numerical attributes from the Web andattempt to deal with ambiguity and noise of theretrieved attribute values.
(Aramaki et al, 2007)utilize a small set of patterns to extract physicalobject sizes and use the averages of the obtainedvalues for a noun compound classification task.
(Banerjee et al 2009) developed a method fordealing with quantity consensus queries (QCQs)where there is uncertainty about the answer quan-tity (e.g.
?driving time from Paris to Nice?).
Theyutilize a textual snippet feature and snippet quan-tity in order to select and rank intervals of therequested values.
This approach is particularlyuseful when it is possible to obtain a substantialamount of a desired attribute values for the re-quested query.
(Moriceau, 2006) proposed a rule-based system which analyzes the variation of theextracted numerical attribute values using infor-mation in the textual context of these values.A significant body of recent research deals withextraction of various data from web tables andlists (e.g., (Cafarella et al, 2008; Crestan andPantel, 2010)).
While in the current research wedo not utilize this type of information, incorpo-ration of the numerical data extracted from semi-structured web pages can be extremely beneficialfor our framework.All of the above numerical attribute extractionsystems utilize only direct information availablein the discovered object-attribute co-occurrencesand their contexts.
However, as we show, indirectinformation available for comparable objects cancontribute significantly to the selection of the ob-tained values.
Using such indirect information isparticularly important when only a modest amountof values can be obtained for the desired object.Also, since the above studies utilize only explic-itly available information they were unable to ap-proximate object values in cases where no explicitinformation was found.3 The Attribute Mining FrameworkOur algorithm is given an object and an attribute.In the WN enrichment scenario, it is also giventhe object?s synset.
The algorithm comprises threemain stages: (1) mining for similar objects anddetermination of a class label; (2) mining for at-tribute values and comparison statements; (3) pro-cessing the results.3.1 Similar objects and class labelTo verify and estimate attribute values for thegiven object we utilize similar objects (co-hyponyms) and the object?s class label (hyper-nym).
In the WN enrichment scenario we can eas-ily obtain these, since we get the object?s synset asinput.
However, in Question Answering (QA) sce-narios we do not have such information.
To obtainit we employ a strategy which uses WordNet alngwith pattern-based web mining.Our web mining part follows common pattern-based retrieval practice (Davidov et al, 2007).
Weutilize Yahoo!
Boss API to perform search enginequeries.
For an object name Obj we query theWeb using a small set of pre-defined co-hyponymypatterns like ?as * and/or [Obj]?2.
In the WN en-richment scenario, we can add the WN class la-bel to each query in order to restrict results to thedesired word sense.
In the QA scenario, if weare given the full question and not just the (ob-ject, attribute) pair we can add terms appearing inthe question and having a strong PMI with the ob-ject (this can be estimated using any fixed corpus).However, this is not essential.We then extract new terms from the retrievedweb snippets and use these terms iteratively to re-trieve more terms from the Web.
For example,when searching for an object ?Toyota?, we executea search engine query [ ?as * and Toyota?]
andwe might retrieve a text snippet containing ?.
.
.
asHonda and Toyota .
.
.
?.
We then extract from thissnippet the additional word ?Honda?
and use it foriterative retrieval of additional similar terms.
Weattempt to avoid runaway feedback loop by requir-ing each newly detected term to co-appear with theoriginal term in at least a single co-hyponymy pat-tern.WN class labels are used later for the retrievalof boundary values, and here for expansion of thesimilar object set.
In the WN enrichment scenario,we already have the class label of the object.
In theQA scenario, we automatically find class labels asfollows.
We compute for each WN subtree a cov-erage value, the number of retrieved terms foundin the subtree divided by the number of subtreeterms, and select the subtree having the highestcoverage.
In all scenarios, we add all terms foundin this subtree to the retrieved term list.
If no WNsubtree with significant (> 0.1) coverage is found,2?*?
means a search engine wildcard.
Square bracketsindicate filled slots and are not part of the query.1310we retrieve a set of category labels from the Webusing hypernymy detection patterns like ?
* suchas [Obj]?
(Hearst, 1992).
If several label candi-dates were found, we select the most frequent.Note that we perform this stage only once foreach object and do not need to repeat it for differ-ent attribute types.3.2 Querying for values, bounds andcomparison dataNow we would like to extract the attribute valuesfor the given object and its similar objects.
Wewill also extract bounds and comparison informa-tion in order to verify the extracted values and toapproximate the missing ones.To allow us to extract attribute-specific informa-tion, we provided the system with a seed set of ex-traction patterns for each attribute type.
There arethree kinds of patterns: value extraction, boundsand comparison patterns.
We used up to 10 pat-terns of each kind.
These patterns are the onlyattribute-specific resource in our framework.Value extraction.
The first pattern group,Pvalues, allows extraction of the attribute valuesfrom the Web.
All seed patterns of this groupcontain a measurement unit name, attribute name,and some additional anchoring words, e.g., ?Objis * [height unit] tall?
or ?Obj width is * [widthunit]?.
As in Section 3.1, we execute search en-gine queries and collect a set of numerical val-ues for each pattern.
We extend this group it-eratively from the given seed as commonly donein pattern-based acquisition methods.
To do thiswe re-query the Web with the obtained (object, at-tribute value, attribute name) triplets (e.g., ?
[Toy-ota width 1.695m]?).
We then extract new pat-terns from the retrieved search engine snippets andre-query the Web with the new patterns to obtainmore attribute values.We provided the framework with unit namesand with an appropriate conversion table whichallows to convert between different measurementsystems and scales.
The provided names includecommon abbreviations like cm/centimeter.
Allvalue acquisition patterns include unit names, sowe know the units of each extracted value.
At theend of the value extraction stage, we convert allvalues to a single unit format for comparison.Boundary extraction.
The second group,Pboundary, consists of boundary-detection patternslike ?the widest [label] is * [width unit]?.
Thesepatterns incorporate the class labels discovered inthe previous stage.
They allow us to find maximaland minimal values for the object category definedby labels.
If we get several lower bounds andseveral upper bounds, we select the highest upperbound and the lowest lower bound.Extraction of comparison information.
Thethird group, Pcompare, consists of comparison pat-terns.
They allow to compare objects directlyeven when no attribute values are mentioned.
Thisgroup includes attribute equality patterns such as?
[Object1] has the same width as [Object2]?, andattribute inequality ones such as ?
[Object1] iswider than [Object2]?.
We execute search queriesfor each of these patterns, and extract a set of or-dered term pairs, keeping track of the relationshipsencoded by the pairs.We use these pairs to build a directed graph(Widdows and Dorow, 2002; Davidov and Rap-poport, 2006) in which nodes are objects (not nec-essarily with assigned values) and edges corre-spond to extracted co-appearances of objects in-side the comparison patterns.
The directions ofedges are determined by the comparison sign.
Iftwo objects co-appear inside an equality patternwe put a bidirectional edge between them.3.3 Processing the collected dataAs a result of the information collection stage, foreach object and attribute type we get:?
A set of attribute values for the requested ob-ject.?
A set of objects similar or comparable tothe requested object, some of them annotatedwith one or many attribute values.?
Upper and lowed bounds on attribute valuesfor the given object category.?
A comparison graph connecting some of theretrieved objects by comparison edges.Obviously, some of these components may bemissing or noisy.
Now we combine these informa-tion sources to select a single attribute value forthe requested object or to approximate this value.First we apply bounds, removing out-of-range val-ues, then we use comparisons to remove inconsis-tent comparisons.
Finally we examine the remain-ing values and the comparison graph.Processing bounds.
First we verify that indeedmost (?
50%) of the retrieved values fit the re-trieved bounds.
If the lower and/or upper bound1311contradicts more than half of the data, we rejectthe bound.
Otherwise we remove all values whichdo not satisfy one or both of the accepted bounds.If no bounds are found or if we disable the boundretrieval (see Section 4.1), we assign the maximaland minimal observed values as bounds.Since our goal is to obtain a value for the singlerequested object, if at the end of this stage we re-main with a single value, no further processing isneeded.
However, if we obtain a set of values orno values at all, we have to utilize comparison datato select one of the retrieved values or to approx-imate the value in case we do not have an exactanswer.Processing comparisons.
First we simplify thecomparison graph.
We drop all graph componentsthat are not connected (when viewing the graph asundirected) to the desired object.Now we refine the graph.
Note that each graphnode may have a single value, many assigned val-ues, or no assigned values.
We define assignednodes as nodes that have at least one value.
Foreach directed edge E(A ?
B), if both A andB are assigned nodes, we check if Avg(A) ?Avg(B)3.
If the average values violate the equa-tion, we gradually remove up to half of the highestvalues for A and up to half of the lowest valuesfor B till the equation is satisfied.
If this cannotbe done, we drop the edge.
We repeat this processuntil every edge that connects two assigned nodessatisfies the inequality.Selecting an exact attribute value.
The goalnow is to select an attribute value for the givenobject.
During the first stage it is possible thatwe directly extract from the text a set of valuesfor the requested object.
The bounds processingstep rejects some of these values, and the com-parisons step may reject some more.
If we stillhave several values remaining, we choose the mostfrequent value based on the number of web snip-pets retrieved during the value acquisition stage.If there are several values with the same frequencywe select the median of these values.Approximating the attribute value.
In the casewhen we do not have any values remaining afterthe bounds processing step, the object node willremain unassigned after construction of the com-parison graph, and we would like to estimate itsvalue.
Here we present an algorithm which allows3Avg.
is of values of an object, without similar objects.us to set the values of all unassigned nodes, includ-ing the node of the requested object.In the algorithm below we treat all node groupsconnected by bidirectional (equality) edges as asame-value group, i.e., if a value is assigned to onenode in the group, the same value is immediatelyassigned to the rest of the nodes in the same group.We start with some preprocessing.
We createdummy lower and upper bound nodes L and Uwith corresponding upper/lower bound values ob-tained during the previous stage.
These dummynodes will be used when we encounter a graphwhich ends with one or more nodes with no avail-able numerical information.
We then connectthem to the graph as follows: (1) if A has no in-coming edges, we add an edge L ?
A; (2) if Ahas no outgoing edges, we add an edge A ?
U .We define a legal unassigned path as a di-rected path A0 ?
A1 ?
.
.
.
?
An ?
An+1where A0 and An+1 are assigned satisfyingAvg(A0) ?
Avg(An+1) and A1 .
.
.
An areunassigned.
We would like to use dummy boundnodes only in cases when no other information isavailable.
Hence we consider paths L ?
.
.
.
?
Uconnecting both bounds are illegal.
First weassign values for all unassigned nodes that belongto a single legal unassigned path, using a simplelinear combination:V al(Ai)i?
(1...n) =n + 1?
in + 1 Avg(A0) +in + 1Avg(An+1)Then, for all unassigned nodes that belong tomultiple legal unassigned paths, we compute nodevalue as above for each path separately and assignto the node the average of the computed values.Finally we assign the average of all extractedvalues within bounds to all the remaining unas-signed nodes.
Note that if we have no compari-son information and no value information for therequested object, the requested object will receivethe average of the extracted values of the whole setof the retrieved comparable objects and the com-parison step will be essentially empty.4 Experimental SetupWe performed automated question answering(QA) evaluation, human-based WN enrichmentevaluation, and human-based comparison of ourresults to data available through Wikipedia and tothe top results of leading search engines.13124.1 Experimental conditionsIn order to test the main system components, weran our framework under five different conditions:?
FULL: All system components were used.?
DIRECT: Only direct pattern-based acqui-sition of attribute values (Section 3.2, valueextraction) for the given object was used, asdone in most general-purpose attribute acqui-sition systems.
If several values were ex-tracted, the most common value was used asan answer.?
NOCB: No boundary and no comparisondata were collected and processed (Pcompareand Pbounds were empty).
We only collectedand processed a set of values for the similarobjects.?
NOB: As in FULL but no boundary data wascollected and processed (Pbounds was empty).?
NOC: As in FULL but no comparison datawas collected and processed (Pcompare wasempty).4.2 Automated QA EvaluationWe created two QA datasets, Web and TRECbased.Web-based QA dataset.
We created QAdatasets for size, height, width, weight, and depthattributes.
For each attribute we extracted fromthe Web 250 questions in the following way.First, we collected several thousand questions,querying for the following patterns: ?Howlong/tall/wide/heavy/deep/high is?,?What is thesize/width/height/depth/weight of?.
Then wemanually filtered out non-questions and heavilycontext-specific questions, e.g., ?what is the widthof the triangle?.
Next, we retained only a singlequestion for each entity by removing duplicates.For each of the extracted questions we manu-ally assigned a gold standard answer using trustedresources including books and reliable Web data.For some questions, the exact answer is the onlypossible one (e.g., the height of a person), whilefor others it is only the center of a distribution(e.g., the weight of a coffee cup).
Questionswith no trusted and exact answers were eliminated.From the remaining questions we randomly se-lected 250 questions for each attribute.TREC-based QA dataset.
As a small comple-mentary dataset we used relevant questions fromthe TREC Question Answering Track 1999-2007.From 4355 questions found in this set we collected55 (17 size, 2 weight, 3 width, 3 depth and 30height) questions.Examples.
Some example questions from ourdatasets are (correct answers are in parentheses):How tall is Michelle Obama?
(180cm); How tallis the tallest penguin?
(122cm); What is the heightof a tennis net?
(92cm); What is the depth of theNile river?
(1000cm = 10 meters); How heavyis a cup of coffee?
(360gr); How heavy is a gi-raffe?
(1360000gr = 1360kg); What is the widthof a DNA molecule?
(2e-7cm); What is the widthof a cow?
(65cm).Evaluation protocol.
Evaluation against thedatasets was done automatically.
For each ques-tion and each condition our framework returneda numerical value marked as either an exact an-swer or as an approximation.
In cases where nodata was found for an approximation (no similarobjects with values were found), our frameworkreturned no answer.We computed precision4, comparing results tothe gold standard.
Approximate answers are con-sidered to be correct if the approximation is within10% of the gold standard value.
While a choice of10% may be too strict for some applications andtoo generous for others, it still allows to estimatethe quality of our framework.4.3 WN enrichment evaluationWe manually selected 300 WN entities from about1000 randomly selected objects below the objecttree in WN, by filtering out entities that clearlydo not possess any of the addressed numerical at-tributes.Evaluation was done using human subjects.
Itis difficult to do an automated evaluation, sincethe nature of the data is different from that of theQA dataset.
Most of the questions asked over theWeb target named entities like specific car brands,places and actors.
There is usually little or no vari-ability in attribute values of such objects, and themajor source of extraction errors is name ambigu-ity of the requested objects.WordNet physical objects, in contrast, are muchless specific and their attributes such as size and4Due to the nature of the task recall/f-score measures areredundant here1313weight rarely have a single correct value, but usu-ally possess an acceptable numerical range.
Forexample, the majority of the selected objects like?apple?
are too general to assign an exact size.Also, it is unclear how to define acceptable val-ues and an approximation range.
Crudeness ofdesired approximation depends both on potentialapplications and on object type.
Some objectsshow much greater variability in size (and hence agreater range of acceptable approximations) thanothers.
This property of the dataset makes it diffi-cult to provide a meaningful gold standard for theevaluation.
Hence in order to estimate the qualityof our results we turn to an evaluation based onhuman judges.In this evaluation we use only approximate re-trieved values, keeping out the small amount ofreturned exact values5.We have mixed (Object, Attribute name, At-tribute value) triplets obtained through each of theconditions, and asked human subjects to assignthese to one of the following categories:?
The attribute value is reasonable for the givenobject.?
The value is a very crude approximation ofthe given object attribute.?
The value is incorrect or clearly misleading.?
The object is not familiar enough to me so Icannot answer the question.Each evaluator was provided with a random sam-ple of 40 triplets.
In addition we mixed in 5 manu-ally created clearly correct triplets and 5 clearly in-correct ones.
We used five subjects, and the agree-ment (inter-annotator Kappa) on shared evaluatedtriplets was 0.72.4.4 Comparisons to search engine outputRecently there has been a significant improvementboth in the quality of search engine results and inthe creation of manual well-organized and anno-tated databases such as Wikipedia.Google and Yahoo!
queries frequently provideattribute values in the top snippets or in searchresult web pages.
Many Wikipedia articles in-clude infoboxes with well-organized attribute val-ues.
Recently, the Wolfram Alpha computationalknowledge engine presented the computation ofattribute values from a given query text.5So our results are in fact higher than shown.Hence it is important to test how well our frame-work can complement the manual extraction of at-tributes from resources such as Wikipedia and topGoogle snippets.
In order to test this, we randomlyselected 100 object-attribute pairs from our WebQA and WordNet datasets and used human sub-jects to test the following:1.
Go1: Querying Google for [object-nameattribute-name] gives in some of the firstthree snippets a correct value or a good ap-proximation value6 for this pair.2.
Go2: Querying Google for [object-nameattribute-name] and following the first threelinks gives a correct value or a good approxi-mation value.3.
Wi: There is a Wikipedia page for the givenobject and it contains an appropriate attributevalue or an approximation in an infobox.4.
Wf: A Wolfram Alpha query for [object-name attribute-name] retrieves a correctvalue or a good approximation value5 Results5.1 QA resultsWe applied our framework to the above QAdatasets.
Table 1 shows the precision and the per-centage of approximations and exact answers.Looking at %Exact+%Approx, we can see thatfor all datasets only 1-9% of the questions re-main unanswered, while correct exact answersare found for 65%/87% of the questions forWeb/TREC (% Exact and Prec(Exact) in the ta-ble).
Thus approximation allows us to answer 13-24% of the requested values which are either sim-ply missing from the retrieved text or cannot be de-tected using the current pattern-based framework.Comparing performance of FULL to DIRECT, wesee that our framework not only allows an approx-imation when no exact answer can be found, butalso significantly increases the precision of exactanswers using the comparison and the boundaryinformation.
It is also apparent that both bound-ary and comparison features are needed to achievegood performance and that using both of themachieves substantially better results than each ofthem separately.6As defined in the human subject questionnaire.1314FULL DIRECT NOCB NOB NOCWeb QASize%Exact 80 82 82 82 80Prec(Exact) 76 40 40 54 65%Approx 16 - 14 14 16Prec(Appr) 64 - 34 53 46Height%Exact 79 84 84 84 79Prec(Exact) 86 56 56 69 70%Approx 16 - 11 11 16Prec(Appr) 72 - 25 65 53Width%Exact 74 76 76 76 74Prec(Exact) 86 45 45 60 72%Approx 17 - 15 15 17Prec(Appr) 75 - 26 63 55Weight%Exact 71 73 73 73 71Prec(Exact) 82 57 57 64 70Prec(Appr) 24 - 22 22 24%Approx 61 - 39 51 46Depth%Exact 82 82 82 82 82Prec(Exact) 89 60 60 71 78%Approx 19 - 19 19 19Prec(Appr) 92 - 58 76 63Total average%Exact 77 79 79 79 77Prec(Exact) 84 52 52 64 71%Approx 18 - 16 16 19Prec(Appr) 72 - 36 62 53TREC QA%Exact 87 90 90 90 87Prec(Exact) 100 62 62 84 76%Approx 13 - 9 9 13Prec(Appr) 57 - 20 40 57Table 1: Precision and amount of exact and approximateanswers for QA datasets.Comparing results for different question typeswe can see substantial performance differences be-tween the attribute types.
Thus depth shows muchbetter overall results than width.
This is likely dueto a lesser difficulty of depth questions or to a moreexact nature of available depth information com-pared to width or size.5.2 WN enrichmentAs shown in Table 2, for the majority of examinedWN objects, the algorithm returned an approxi-mate value, and only for 13-15% of the objects (vs.70-80% in QA data) the algorithm could retrieveexact answers.Note that the common pattern-based acquisitionframework, presented as the DIRECT condition,could only extract attribute values for 15% of theobjects since it does not allow approximations andFULL DIRECT NOCB NOB NOCSize%Exact 15.3 18.0 18.0 18.0 15.3%Approx 80.3 - 38.2 20.0 23.6Weight%Exact 11.8 12.5 12.5 12.5 11.8%Approx 71.7 - 38.2 20.0 23.6Table 2: Percentage of exact and approximate values for theWordNet enrichment dataset.FULL NOCB NOB NOCSize%Correct 73 21 49 28%Crude 15 54 31 49%Incorrect 8 21 16 19Weight%Correct 64 24 46 38%Crude 24 45 30 41%Incorrect 6 25 18 15Table 3: Human evaluation of approximations for the WNenrichment dataset (the percentages are averaged over the hu-man subjects).may only extract values from the text where theyexplicitly appear.Table 3 shows human evaluation results.
Wesee that the majority of approximate values wereclearly accepted by human subjects, and only 6-8% were found to be incorrect.
We also observethat both boundary and comparison data signifi-cantly improve the approximation results.
Notethat DIRECT is missing from this table since noapproximations are possible in this condition.Some examples for WN objects and approx-imate values discovered by the algorithm are:Sandfish, 15gr; skull, 1100gr; pilot, 80.25kg.
Thelatter value is amusing due to the high variabil-ity of the value.
However, even this value is valu-able, as a sanity check measure for automated in-ference systems and for various NLP tasks (e.g.,?pilot jacket?
likely refers to a jacket used by pi-lots and not vice versa).5.3 Comparison with search engines andWikipediaTable 4 shows results for the above datasets incomparison to the proportion of correct results andthe approximations returned by our framework un-der the FULL condition (correct exact values andapproximations are taken together).We can see that our framework, due to its ap-proximation capability, currently shows signifi-cantly greater coverage than manual extraction ofdata from Wikipedia infoboxes or from the first1315FULL Go1 Go2 Wi WfWeb QA 83 32 40 15 21WordNet 87 24 27 18 5Table 4: Comparison of our attribute extraction frameworkto manual extraction using Wikipedia and search engines.search engine results.6 ConclusionWe presented a novel framework which allowsan automated extraction and approximation of nu-merical attributes from the Web, even when no ex-plicit attribute values can be found in the text forthe given object.
Our framework retrieves simi-larity, boundary and comparison information forobjects similar to the desired object, and com-bines this information to approximate the desiredattribute.While in this study we explored only severalspecific numerical attributes like size and weight,our framework can be easily augmented to workwith any other consistent and comparable attributetype.
The only change required for incorpora-tion of a new attribute type is the development ofattribute-specific Pboundary , Pvalues, and Pcomparepattern groups; the rest of the system remains un-changed.In our evaluation we showed that our frame-work achieves good results and significantly out-performs the baseline commonly used for generallexical attribute retrieval7.While there is a growing justification to relyon extensive manually created resources such asWikipedia, we have shown that in our case auto-mated numerical attribute acquisition could be apreferable option and provides excellent coveragein comparison to handcrafted resources or man-ual examination of the leading search engine re-sults.
Hence a promising direction would be touse our approach in combination with Wikipediadata and with additional manually created attributerich sources such as Web tables, to achieve the bestpossible performance and coverage.We would also like to explore the incorpora-tion of approximate discovered numerical attributedata into existing NLP tasks such as noun com-pound classification and textual entailment.7It should be noted, however, that in our DIRECT base-line we used a basic pattern-based retrieval strategy; moresophisticated strategies for value selection might bring betterresults.ReferencesEiji Aramaki, Takeshi Imai, Kengo Miyo and KazuhikoOhe.
2007 UTH: SVM-based Semantic RelationClassification using Physical Sizes.
Proceedingsof the Fourth International Workshop on SemanticEvaluations (SemEval-2007).Somnath Banerjee, Soumen Chakrabarti and GaneshRamakrishnan.
2009.
Learning to Rank for Quan-tity Consensus Queries.
SIGIR ?09.Michele Banko, Michael J Cafarella , Stephen Soder-land, Matt Broadhead and Oren Etzioni.
2007.Open information extraction from the Web.
IJCAI?07.Matthew Berland, Eugene Charniak, 1999.
Findingparts in very large corpora.
ACL ?99.Michael Cafarella, Alon Halevy, Yang Zhang, DaisyZhe Wang and Eugene Wu.
2008.
WebTables: Ex-ploring the Power of Tables on the Web.
VLDB ?08.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: mining the Web for fine-grained semantic verbrelations.
EMNLP ?04.Eric Crestan and Patrick Pantel.
2010.
Web-ScaleKnowledge Extraction from Semi-Structured Ta-bles.
WWW ?10.Dmitry Davidov and Ari Rappoport.
2006.
EfficientUnsupervised Discovery of Word Categories Us-ing Symmetric Patterns and High Frequency Words.ACL-Coling ?06.Dmitry Davidov, Ari Rappoport and Moshe Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
ACL ?07.Dmitry Davidov and Ari Rappoport.
2008a.
Classifi-cation of Semantic Relationships between NominalsUsing Pattern Clusters.
ACL ?08.Dmitry Davidov and Ari Rappoport.
2008b.
Unsu-pervised Discovery of Generic Relationships UsingPattern Clusters and its Evaluation by AutomaticallyGenerated SAT Analogy Questions.
ACL ?08.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2006.
Automatic discovery of part-whole relations.Computational Linguistics, 32(1).Marty Hearst, 1992.
Automatic acquisition of hy-ponyms from large text corpora.
COLING ?92.Veronique Moriceau, 2006.
Numerical Data Integra-tion for Cooperative Question-Answering.
EACL -KRAQ06 ?06.John Prager, 2006.
Open-domain question-answering.In Foundations and Trends in Information Re-trieval,vol.
1, pp 91-231.1316Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: leveraging generic patterns for automat-ically harvesting semantic relations.
COLING-ACL?06.Deepak Ravichandran and Eduard Hovy.
2002 Learn-ing Surface Text Patterns for a Question AnsweringSystem.
ACL ?02.Ellen Riloff and Rosie Jones.
1999.
Learning Dic-tionaries for Information Extraction by Multi-LevelBootstrapping.
AAAI ?99.Benjamin Rosenfeld and Ronen Feldman.
2007.Clustering for unsupervised relation identification.CIKM ?07.Peter Turney, 2005.
Measuring semantic similarity bylatent relational analysis, IJCAI ?05.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised Lexical acquisition.
COL-ING ?02.1317
