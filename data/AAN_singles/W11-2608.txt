Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 65?69,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsPhone set selection for HMM-based dialect speech synthesisMichael PucherTelecommunicationsResearch Center (FTW)Vienna, Austriapucher@ftw.atNadja Kerschhofer-PuhaloAcoustics ResearchInstitute (ARI)Vienna, Austrianadja.kerschhofer@oeaw.ac.atDietmar SchabusTelecommunicationsResearch Center (FTW)Vienna, Austriaschabus@ftw.atAbstractThis paper describes a method for selecting anappropriate phone set in dialect speech synthe-sis for a so far undescribed dialect by applyinghidden Markov model (HMM) based trainingand clustering methods.
In this pilot study weshow how a phone set derived from the pho-netic surface can be optimized given a smallamount of dialect speech training data.1 IntroductionIn acoustic modeling for dialect speech synthesis weare confronted with two closely related major prob-lems1, (1) to find an appropriate phone set for syn-thesis and (2) to design a recording script with suf-ficient phonetic and prosodic coverage.
In HMM-based synthesis, we can use the training process ofthe voices itself to analyze the used phone set and totry to optimize it for synthesis.2 Corpus and phone set designGoiserian, the dialect of Bad Goisern in the mostsouthern part of Upper Austria, is a local dialectof the Middle Bavarian/Southern Bavarian transitionzone.
The target variety for speech synthesis de-scribed here demonstrates the typical problems re-lated to scarcity of data.
While several varieties ofthe central and northern part of Upper Austria arequite well described, detailed descriptions of the va-rieties in this region do not exist.
Lacking a lexi-con, a phonological description, orthographic rules1Apart from additional problems that have to do with textanalysis, orthography, and grapheme-to-phoneme conversion.or a transcription system, a speech corpus and anappropriate phone set have to be created.
Our cur-rent project aims at audio-visual dialect synthesis,which is based on a systematic description of speechdata collected from spontaneous speech, word listsand translated sentences by 10 speakers of the samedialect.
Although it would be ideal to use con-versational speech data for dialect speech synthe-sis (Campbell, 2006) we decided to use a hybrid ap-proach for our full corpus where we plan to collecta set of prompts from conversational dialect speech,which will be realized by the dialect speakers.The speech data for the first preliminary studypresented here consists of 150 sentences and col-loquial phrases spoken in Goiserian by a femalespeaker who can be described as a conservativespeaker of the original basic dialect of the region.The prompts were translated spontaneously by thespeaker from Standard German into Goiserian andcontain typical phonetic and phonological character-istics of local Bavarian varieties in multiple occur-rences.3 Voice buildingThe data was originally recorded at 96kHz, 24 bitand was downsampled to 16kHz, 16 bit for synthesisand voice building.
A preliminary phone set (PS1)was created on the basis of a fine phonetic transcrip-tion including sub-phonemic details (e.g.
nasaliza-tion of vowels before nasals ?VN?).
Phones occur-ring less than twice were substituted prior to voicetraining with phonetically similar phones or repre-sentatives of the same phoneme.
This leaves us witha set of 72 phones (see Table 1 and 2).65The TRA voice was trained with a HMM-based speaker-dependent system.
Given the limitedamount of training data (150 prompts) and to be ableto analyze the decision trees we only used the cur-rent, 2 preceding, and 2 succeeding phones as fea-tures.HTK IPA # HTK IPA #s s 207 t t 204d d 179 n n 171m m 115 k k 98h h 84 g g 79v v 79 f f 62r r 61 S S 49N n"42 l l 41b b 31 ts ?
27ng N 19 p p 17w B 14 L l"12X x 11 c c 10RX X 9 j j 7R R 6 ks ks 3pf pf 3Table 1: Consonants (27) in phone set PS1 for training(72 phones) (Blue = not in PS2).Based on a phonetic transcription of the trainingcorpus, flat-start forced alignment with HTK wascarried out.
Stops are split into two parts, one forthe closure and one for plosion plus burst.
Ad-ditionally, we applied forced alignment using pro-nunciation variants2, which is the preferred methodwhen building a voice for dialect synthesis using alarger corpus (Pucher, 2010).
With this method itis not necessary to have a phonetic transcription ofthe recordings.
Given our small corpus, this methodproduced several errors ([tsvoa] / [tsvai], [tsum] /[tsun] etc.)
which led us to use the standard align-ment method from a transcription of the corpus.
Af-ter the transcription we had to correct severe align-ment errors.
These errors are simple to find sinceseveral segments within the utterance are affected.From this corpus we selected 5 prompts contain-ing only phonemes that appear at least more than 3times in the rest of the corpus.
This leaves us witha training corpus of 145 prompts and a 5 prompt2In a previous project on Viennese dialect synthesis, 33% ofthe lexicon entries are pronunciation variants.HTK IPA # HTK IPA #a a 138 aa a: 10A 6 80 AA 6: 3AN 6?
80 Ai 6i 3AuN 6?u 7e e 100 ee e: 9ei ei 22 eiN e?i 10E E 20 EE E: 11EN E?
4 EiN E?i 6i i 175 ii i: 7iN i?
6o o 45 oo o: 3ou ou 4 Ou O 4u u 20 U U 15UN U?
3q ?
9 qY ?Y 3QY ?Y 4y y 9 yy y: 3Y Y 4eV @ 11 aV 5 89ai ai 24 aiN a?i 9au au 24 ea e5 7eaN e?5 4 ia i5 30oa o5 16 oaN o?5 9Oi Oi 6 oi oi 26ua u5 21 ui ui 6Table 2: Vowels (33) and diphtongs (12) in phone set PS1for training (72 phones) (Blue = not in PS2, Red = not inPS2 and PS3, green = not in PS3).test set.
For the subjective evaluation, the entire re-synthesized corpus was used to show us how wellthe used phone set covers the data.The 145 prompts were then used for traininga speaker-dependent HMM-based synthetic voice.Figure 1 shows the architecture of the HMM-basedspeaker dependent system (Zen, 2005).
For synthe-sis we used the full-context label files of the corpuswithout duration information.
By that text analysisis not necessary for synthesis.
Our implicit assump-tion is that the letter-to-sound rules and text analysisproduce exactly the string of phones from the tran-scription.
In this way we can evaluate the acousticmodeling part separately, independently from textanalysis.66ExcitationparameterextractionSpectralparameterextractionTraining of MSD-HSMMParameter generationfrom MSD-HSMMText analysisExcitationgeneration SynthesisfilterSpeech signalLabelsSingle speakerspeech databaseTrainingSynthesisSpectral parametersExcitation parametersLabelsContext-dependentmulti-stream MSD-HSMMsTEXTSYNTHESIZEDSPEECHFigure 1: HMM-based speaker dependent speech synthe-sis system.4 Voice analysisTo be able to analyze the decision trees we usedphone features only.
The HMM-based voice con-sists of a mel-cepstrum, duration, F0, and an aperi-odicity model.
In a first step we defined the phonesthat are not used for modeling, or are used for a cer-tain model only.Figure 3 shows those phones that are not used forclustering of the different models.
This may be dueto their rare occurrence in the data (3-4 times) or dueto possible inconsistencies in their phonetic realiza-tion.
The F0 model is not shown since all phonemeswere used in the F0 tree in some context.To define other possible phone sets we decidedto substitute the phones only occurring in the F0model but not in the other 3 models, namely themel-cepstrum, duration, and the aperiodicity model.We therefore merged ?Ai?, ?AuN?, ?EN?, ?ks?, ?L?,?Ou?, ?qY?, ?yy?
with their phonetically most sim-ilar equivalents (e.g.
syllabic ?L?
with ?l?, ?ks?with ?k?+?s?, or a nasalized ?EN?
or ?AuN?
beforenasals with the non-nasal phone) and thus obtaineda new smaller phone set (PS2), which was used fortraining a second voice model.Another possible set of phones (PS 3) is definedby merging long (VV) and short (V) vowels of thesame quality, namely ?ii?, ?yy?, ?ee?, ?EE?, ?aa?,?AA?, ?oo?
with their short counterpart.
From a lin-guistic point of view, the phonemic status of vowelC-silC-snomcep_s4_1yesC-AnoL-silyesC-tnomcep_s4_2yesmcep_s4_35nomcep_s4_34yesC-nnomcep_s4_3yesC-mnomcep_s4_4yesC-enomcep_s4_5yesC-anoL-hyesC-ANnomcep_s4_6yesmcep_s4_37nomcep_s4_36yesC-knomcep_s4_7yesC-dnomcep_s4_8yesC-gnomcep_s4_9yesC-bnomcep_s4_10yesC-Nnomcep_s4_11yesC-fnomcep_s4_12yesC-inomcep_s4_13yesC-rnomcep_s4_14yesC-hnomcep_s4_15yesC-Xnomcep_s4_16yesC-cnomcep_s4_17yesC-oanomcep_s4_18yesC-uanomcep_s4_19yesC-oaNnomcep_s4_20yesC-EEnomcep_s4_21yesC-ianomcep_s4_22yesC-einomcep_s4_23yesC-ngnomcep_s4_24yesC-aunomcep_s4_25yesC-ainomcep_s4_26yesC-aVnomcep_s4_27yesC-vnomcep_s4_28yesC-eanomcep_s4_29yesC-aanomcep_s4_30yesC-eenomcep_s4_31yesC-oinomcep_s4_32yesR-nnomcep_s4_33yesC-ynoC-OiyesC-eVnomcep_s4_38yesmcep_s4_40nomcep_s4_39yesmcep_s4_42nomcep_s4_41yesFigure 2: Part of the mel-cepstrum clustering tree for the3rd state of the HMM.duration as a primary feature in Austrian Germanis a controversial issue.
While differences in lengthdo exist at the phonetic surface, these differences arenot necessarily of phonemic relevance (Moosmu?ller,2007; Scheutz, 1985).
We obtain thus a third phoneset (PS3) by merging long and short vowels.Model # #C #L #LL #R #RRMel-cep.
42 38 2 0 1 0Aperiod.
36 31 0 3 0 1F0 196 54 37 38 30 36Duration 83 32 14 9 14 13Table 3: Number of models and questions in mel-cepstrum, aperiodicity, F0, and duration model for centralHMM state.4.1 Mel-cepstrum and aperiodicity modelThe mel-cepstrum model contains a separate modelfor each phone that is used in the cepstral clus-tering.
In Figure 2 this is shown with the model?mcep s4 32?, which is used in case that the cur-rent phone is an ?ee?
(C-ee) and with the model?mcep s4 33?, which is used in case that the cur-rent phone is an ?oi?.
These two models are specialmodels which only cover certain phones.
The onlyeffect of the clustering is that some phones are notmodeled separately, resulting in an unbalanced tree.However there is one instance of context cluster-67MEL-CEP DURATIONAPERIODICITYaa ea ii iN ouAi AuN EN ksL Ou qY yyAA ai aiN aueaN ee EE ei EiN jng oa oaN oi Oi qQY R u ua w X yE eiNoo pf RXU ui UN YpFigure 3: Phones that were not used for clustering in thetrees for mel-cepstrum, duration, and aperiodicity in anycontext (current, 2 preceding, and 2 succeeding phones)and any of the 5 states.ing in the central state of the mel-cepstrum HMMs.If the right phone is an ?n?
(R-n) there are two dif-ferent models used (?mcep s4 39?, ?mcep s4 40?
),depending on whether the current phone is an ?Oi?
(C-Oi) or not (Figure 2).All phones that are not modeled through a sepa-rate model are modeled by the model at the end ofthe tree (model ?mcep s4 42?
).The aperiodicity model is very similar to the mel-cepstrum model, as can be seen in Table 3 and Fig-ure 3.4.2 F0 and duration modelThe F0 model uses all phones as shown in Figure 3and is the most complex model in terms of contextquestions as can be seen from Table 3.The duration model contains the lowest number ofphone related questions as shown by Figure 3 but isstill more complex than the spectrum related modelsin terms of context-dependent questions as shownin Table 3.
Similarly to the F0 model, it is ratherdifficult to analyze this model directly.5 Voice evaluationAfter the analysis of the voice that was trained withour basic phoneset PS1 we defined two new phone-sets PS2 and PS3.
These phonesets were used totrain additional voice models for the same speaker.With these voice models, we synthesized our smallset of 5 test sentences.
To evaluate the suitabil-ity of the phonesets for the training data, we re-synthesized the training corpus of 145 prompts.In a pair-wise comparison test of the 150 promptswe evaluated the three voice models in a subjectivelistening test with three expert listeners.
The expertslistened to a set of prompts, each prompt synthesizedwith two different voice models.
They were askedto compare them and to decide which prompt theywould prefer in terms of overall quality, or whetherthey would rate them as ?equally good?.PS1 PS2 PS356 102 105Table 4: Number of winning comparisons per phone set(PS1-PS3).Table 4 illustrates that both approaches to reduceand redefine the phoneset (PS2, PS3) improved theoverall quality estimation considerably compared tothe initial phoneset PS1.6 ConclusionOne major challenge for speech synthesis of so farundescribed varieties is the lack of an appropriatephoneset and sufficient training data.
We met thischallenge by deriving a phoneset directly from thephonetic surface of a very restricted corpus of natu-ral speech.
This phone set was used for voice train-ing.
Based on the outcome of the first voice trainingwe reconsidered the choice of phones and creatednew phone sets following 2 approaches: (1) remov-ing phones that are not used in the clustering, and(2) a linguistically motivated choice of phone sub-stitutions based on clustering results.
Both methodsyielded a considerable improvement of voice qual-ity.
Thus, HMM-based machine learning methodsand supervised optimization can be used for the def-inition of the phoneset of an unkown dialect.
Ourfuture work will elaborate this method with dialectspeech training corpora of different size to showwhether it can be applied to adaptive methods in-volving multiple-speaker training.
The considera-tion of inter- and intra-speaker variation and styleshifting will be a crucial question for further study.68ReferencesNick Campbell.
2006.
Conversational speech synthesisand the need for some laughter.
IEEE Transactionson Speech and Audio Processing, 14(4), pages 1171-1178.Michael Pucher, Friedrich Neubarth, Volker Strom,Sylvia Moosmu?ller, Gregor Hofer, Christian Kranzler,Gudrun Schuchmann and Dietmar Schabus.
2010.
Re-sources for speech synthesis of Viennese varieties.
InProceedings of the 7th International Conference onLanguage Resources and Evaluation (LREC), Valletta,Malta.Sylvia Moosmu?ller.
2007.
Vowels in Standard Aus-trian German.
An Acoustic-Phonetic and Phonologi-cal Analysis.
Habilitationsschrift, Vienna.Hannes Scheutz.
1985.
Strukturen der Lautvera?nderung.Braumu?ller, Vienna.Heiga Zen and Tomoki Toda.
2005.
An Overviewof Nitech HMM-based Speech Synthesis System forBlizzard Challenge 2005.
In Proceedings of the 9thEuropean Conference on Speech Communication andTechnology (INTERSPEECH), Lisboa, Portugal.69
