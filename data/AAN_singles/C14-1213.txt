Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2260?2269, Dublin, Ireland, August 23-29 2014.?One Entity per Discourse?
and ?One Entity per Collocation?Improve Named-Entity DisambiguationAnder Barrena*, Eneko Agirre*, Bernardo Cabaleiro**, Anselmo Pe?nas**, Aitor Soroa**IXA NLP Group / University of the Basque Country, Basque Countryabarrena014@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es**UNED NLP & IR Group, Madridanselmo@lsi.uned.es, bcabaleiro@lsi.uned.esAbstractThe ?one sense per discourse?
(OSPD) and ?one sense per collocation?
(OSPC) hypotheses havebeen very influential in Word Sense Disambiguation.
The goal of this paper is twofold: (i) toexplore whether these hypotheses hold for entities, that is, whether several mentions in the samediscourse (or the same collocation) tend to refer to the same entity or not, and (ii) test their impactin Named-Entity Disambiguation (NED).
Our experiments show consistent results on differentcollections and three state-of-the-art NED system.
OSPD hypothesis holds in around 96%-98%of documents whereas OSPC hypothesis holds in 91%-98% of collocations.
Furthermore, asimple NED post-processing in which the majority entity is promoted, produces a gain in perfor-mance in all cases, reaching up to 8 absolute points of improvement in F-measure.
These resultsshow that NED systems would benefit of considering these hypotheses into their implementation.1 IntroductionThe ?one sense per discourse?
(OSPD) hypothesis was introduced by Gale et al.
(1992), and stated that aword tends to preserve its meaning when occurring multiple times in a discourse.
They estimated that theprobability of two occurrences of the same polysemous noun drawn from one document having the samesense to be around 94% for documents from Grolier encyclopedia, and 96% for documents from Brown,based on word senses from the Oxford Advanced Learner?s Dictionary and a handful of examples.
A fewyears later, Krovetz (1998) reported 66% on larger corpora (SemCor and DSO) annotated with WordNetsenses by third parties, but, unfortunately, he only reported how many polysemous nouns occurred witha single sense in all documents, not in each document.
In the context of statistical machine translation,Carpuat (2009) reported that, 80% of the time, words occurring multiple times in a source document aretranslated into a single word in the target language.In the case of entities, OSPD is closely related to coreference, where the task is to find whether twodifferent mentions (perhaps using different surface strings like ?John?
and ?he?)
in a document referto the same entity or not.
For instance, the coreference system presented by (Lee et al., 2013), uses aheuristic which links mentions in a document that share the same surface string: ?This sieve [heuristic]accounts for approximately 16 CoNLL F1 points improvement, which proves that a signicant percentageof mentions in text are indeed repetitions of previously seen concepts?.
Our paper actually quantifies theamount of those repetitions for entities, providing additional evidence for the heuristic.The ?one sense per collocation?
(OSPC) hypothesis was introduced by Yarowsky (1993), stating thata word tends to preserve its meaning when occurring with the same collocate.
Yarowsky tested hishypothesis for several definitions of collocate, including positional collocates (word to left or right)and syntactic collocations (governing verb of object, governing verb of subject, modifying adjective).He reported entropy on train data, as well as disambiguation performance on unseen data, with theprecision ranging between 90% and 99% for a handful of words with two distinct homograph senses,like, e.g.
?bass?
or ?colon?.
In larger-scale research, Martinez and Agirre (2000) measured the precisionThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2260Abbott Beefs Up Litigation Reserves NORTH CHICAGO, Ill. (AP) Abbott Laboratories Inc., bracingfor a costly settlement in a federal investigation involving the prostate-cancer drug Lupron, said Fridayit was increasing litigation reserves by $344 million.
As part of the announcement, Abbott said it hadrestated its quarterly results and is now reporting a loss of $319.9 million for the first three monthsof this year rather than a profit.
The move comes amid long-running negotiations between the U.S.Department of Justice and TAP Pharmaceutical Products, the 50-50 joint venture between Abbott andTakeda Chemical Industries of Japan that made Lupron.
Abbott said in January ...Figure 1: Example of OSPD for entities.
All occurrences of ?Abbott?
refer to ?Abbott Laboratories?.of similar collocations on corpora (Semcor and DSO) annotated by third parties with finer-grained sensesfrom WordNet, reporting lower figures around 70%.In this paper, we take a collocation to be a word (or multiword term) that co-occurs with the targetnamed-entity more often than would be expected by chance.
In our case we use syntactic dependenciesto extract co-occurring terms.These two hypotheses have been very influential, and have inspired multiple heuristics and methodsin Word Sense Disambiguation research (Agirre and Edmonds, 2007, Chapters 5,7,10,11).
In this workwe are going to show that both hypotheses hold for named-entities as well, and that the hypotheses canbe used to post-process the output of any Named-Entity Disambiguation system (NED) to improve itsperformance.
NED, also known as Entity Linking, takes as input a named-entity mention in context andassigns it a specific entity from a given entity repository (Hachey et al., 2012; Daiber et al., 2013).In the first part of this work we are going to test whether the two hypotheses hold for entity mentionswith respect to a repository of entities extracted from Wikipedia.
For instance, do all occurrences ofmention ?Abbott?
in a document refer to the same entity?
Do all occurrences of mention ?CPI?
assubject of verb ?rise?
refer to the same entity?
Do all occurrences of ?CDU?
in relation to ?Merkel?
referto the same entity?
The examples in Figures 1 and 2 show evidence that this is indeed the case.
Theexperiments aim at quantifying in which degree OSPD and OSPC hypotheses hold for entities1.In the second part of the paper, we will explore a simple method to incorporate OSPD and OSPChypotheses to any existing NED system, showing their potential.
After running the NED system, we takeits output and observe, for each mention string, which is the entity returned most often for a given docu-ment (or collocation), assigning to all occurrences the majority entity.
We tested the improvements with afreely available NED system (Daiber et al., 2013), a reimplementation of a strong Bayesian NED system(Han and Sun, 2011) and an in-house graph-based system.
We got statistically significant improvementsfor all systems and ?one sense?
hypotheses that we tested, with a couple exceptions.In order to check the OSPD and OSPC hypotheses for entities, we first looked into existing datasets.AIDA (Hoffart et al., 2011)2is a publicly available hand-tagged corpus based on the CoNLL named-entity recognition and disambiguation task dataset.
AIDA contains links of all entity mentions in fulldocuments, so it is a natural fit for OSPD.
We estimated OSPD based on more than 4,000 mentions thatoccur multiple times in a document.
For completeness, we also estimated OSPD at the collection level.OSPD and OSPC are independent of each other, as one is applied at the document level and the otherat the corpus level, focusing on the entities that occur with a specific collocation.
Multiple occurrencesof a target string in a document usually occur with different collocations, and conversely, multiple occur-rences of a target string with a specific collocation typically occur in different documents.
Note also thatsingletons (entities that are only mentioned once in a document) are not affected by OSPD, but could beaffected by OSPC.In order to estimate OSPC, no available corpus existed, so we decided to base our dataset on the TACKBP 2009 Entity Linking dataset3(TAC2009 for short) (Ji et al., 2010).
The TAC2009 dataset involves138 mention strings, which have been annotated in several documents drawn primarily from Gigaword4.1For the sake of clarity we will also refer to OSPD and OSPC for entities as OSPD and OSPC.2http://www.mpi-inf.mpg.de/yago-naga/aida/downloads.html3http://www.nist.gov/tac/2013/KBP/EntityLinking/index.html4http://catalog.ldc.upenn.edu/LDC2003T052261CPI subject-of rise:China?s consumer price index, or CPI, rose 2.8 percent last December.In the 10 months to October, the CPI rose 1.35 percent, the core price index grew 1.13 percent ...Measured on a month-on-month basis, March CPI rose 2.3 percent from February, ...... still lower than in China, Hong Kong and Singapore, whose CPIs have rised 8.0 percent, ...The core CPI rose 0.2 percent, in line with Wall Street expectations.Angela Merkel has CDU:... who share power with Merkel?s CDU nationally in an uneasy ?
grand coalition ?
...Economy Minister Michael Glos, also from the CSU, the sister party to Merkel?s CDU ...In the past Merkel?s CDU had been able to rely on the CSU?s strength in Bavaria ...... but while her conservative CDU wanted new legal tools to do so, ...The new development has put a further strain on Merkel?s CDU ...Figure 2: Examples of OSPC for entities, showing five examples for a syntactic collocation (top row)and fie examples for a more specific proposition (bottom row).
?CPI?
might refer to ?Comunist Partyof India?
or ?Consumer Price Index?, among others, but refers to the second in all cases.
?CDU?
canrefer to the German ?Christian Democratic Union?
or ?Catholic Distance University?, among others, butrefers to the first in all cases.We extracted several syntactic collocations for those 138 mention strings from Gigaword, and hand-annotated them, yielding an estimate for the OSPC.
Note that TAC2009 only provides the annotation fora specific mention in a document, so we had to annotate by hand the rest of occurrences in the documents.For instance, we analyzed examples of ?CPI?
as subject of the verb ?rise?
(cf.
Figure 2).
Some of thesyntactic collocations like the subjects of verb ?has?
seemed very uninformative, so we decided to alsocheck the OSPC hypothesis on more specific collocations, involving more complete argument structures.For instance, we checked ?ABC?
occurring as subject of ?has?
with object ?radio?.
We call this morespecific collocations propositions (Pe?nas and Hovy, 2010).The paper is structured as follows.
We will first present the resources used in this study.
Section 3presents the results of OSPD.
Section 3.1 extends OSPD when, instead of documents, we take the com-plete collection.
Section 4 presents the study of OSPC both for syntactic dependencies and propositions.Section 5 presents the experiments where OSPD and OSPC are used to improve the performance ofexisting systems.
Finally, we draw the conclusions and future work.2 Resources usedAIDA is based on the corpus used in the CONLL named-entity recognition and classification task, whereall entities in full documents had been linked to the referred Wikipedia articles (using the 2010 Wikipediadump).
We use the full AIDA dataset, with 1,393 documents, 34,140 disambiguated entity mentions,where 27,240 are linked to a Wikipedia article.
All in all there are 6,877 distinct mention strings (types)which are linked at least once to a Wikipedia article.
The rest refer to articles not in Wikipedia (NILinstances), and were discarded.
This corpus covers news from a sample of a few days spanning from1996-05-28 to 1996-12-07.In order to prepare our dataset for OSPC, we chose the dataset of the TAC KBP 2009 Entity Linkingcompetition, as this dataset have been extensively used in Entity Linking evaluation.
In addition, the cor-pus used in the task was very large, allowing us to mine relevant collocations (see below).
We manuallyannotated the occurrences in the extracted collocations, producing two datasets, one for each kind of col-location (cf.
Section 4).
Note that the TAC KBP organizers only annotated one specific mention in eachtarget document.
For completeness, we also tagged the rest of the occurrences of the target mentions inthe documents, thus allowing us to provide OSPD estimated based on TAC2009 data as well.
This isthe third dataset that we annotated by hand.
The hand-annotation was performed by a single person, andlater reviewed by the rest of the authors.
The three annotation datasets are publicly available5.
Hand-5http://ixa2.si.ehu.es/OEPDC2262NHasN ?U.S.
dollar?NPN ?condition of anonymity?NVN ?official tells AFP?NVNPN ?article maintains interest within layout?NVPN ?others steal from input?VNPN ?includes link to website?Table 1: List of the six patterns used to extract propositions, with some examples.tagging is costly, so we tagged around 250 examples of syntactic collocations and around 250 examplesof propositions.Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia articlebecause the mention referred to an entity which was not listed in the entity inventory.
We ignored allthose cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity theyrefer to.The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 mil-lion documents, 1.3 millions from newswire and 0.5 millions from the web.
We have parsed them withthe Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependen-cies (De Marneffe and Manning, 2008).
We selected subject, object, prepositional complements andadjectival modifiers as the source for syntactic collocations.
In order to provide more specific collo-cations, we implemented the syntactic patterns proposed in (Pe?nas and Hovy, 2010), which produceso-called propositions.
The result is a database with 16 million distinct propositions.
Table 1 shows thesix patterns used in this work, together with some examples.In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia whichlists, for each string mention, which entities it can refer to.
We followed the construction method of(Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyper-links to find mention strings that can be used to refer to entities.
Contrary to them, we could not accesshyperlinks in the web, so we could use only those in Wikipedia.
According to our dictionary, the am-biguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions inAIDA, and 62.6 entities on average for the mentions in TAC2009.3 One entity per discourseIn order to estimate OSPD we divided the number of times a mention string referred to different entitiesin the document with the number of times a mention string occurred multiple times in the document.
Inthe denominator and numerator we count each mention-document pair once.Regarding AIDA, we found 12,084 occurrences of mentions which occurred more than once in adocument, making 4,265 unique mention-document pairs6(cf.
Table 2).
In the vast majority of thecases those mentions refer to a single entity in the document, and only in 170 cases the mentions in thedocument refer to several entities.
The last row in Table 2 shows the ratio between those values, 96.01%,showing that OSPD is strong in this dataset.We also checked OSPD in the TAC2009 dataset.
Out of the 138 distinct mention strings used in thetask, we discarded those only linked to NIL (that is, no corresponding Wikipedia article existed) andthose which were not ambiguous (that is, they had only one entity in the dictionary, cf.
Section 2).
Thatleaves 105 mention strings, occurring 1,776 times in 918 different documents, which we annotated byhand.
The 105 strings occurred 1,776 times in 918 documents.
Removing the cases where the mentionoccurred only once, we were left with 1,173 occurrences, which make 334 unique mention-documentpairs, of which only 6 occurred with more than one sense (rightmost row in Table 2).
This yields anestimate for OSPD of 98.2%.6By unique mention-document pairs we mean that we only count once for a mention occurring multiple times in a document.For instance if mention Smith occurs 10 times in the whole corpus, 8 times in document A and 2 times in document B, we counttwo unique mention-document pairs.2263AIDA TAC2009Mention-document pairs 4,265 334Ambiguous pairs 170 6OSPD 96.0% 98.2%Table 2: One entity per discourse: per document statistics in AIDA and TAC2009 datasets.
Pairs standfor the number of unique mention-document pairs.
The 4,265 pairs in AIDA correspond to 12,084occurrences of mentions, and the 334 pairs in TAC2009 correspond to 1,173 occurrences.All mentions First mentionAIDA TAC2009 AIDA TAC2009Mention types 3,363 105 2,731 105Ambiguous types 475 26 454 25OSPD (collections) 85.9% 75.2% 83.4% 76.2%Table 3: One entity per collection: statistics in AIDA and TAC2009.
In the first two columns (?Allmentions?)
we consider all mention types (3, 363 types in AIDA correspond to 23, 726 occurrences ofmentions, and 105 types in TAC2009 correspond to 1, 776 occurrences).
In the second two columns(?First mention?)
we leave only the first mention of each document (in this case, there are 2, 731 mentiontypes in AIDA which correspond to 15, 275 occurrences, and 105 types in TAC2009 corresponding to941 occurrences).Finally, we also thought about measuring OSPD on the Wikipedia articles, where many mentionshave been manually linked to their respective article.
Unfortunately, we noted that Wikipedia guidelinesexplicitly prevent authors linking a mention multiple times: Generally, a link should appear only oncein an article, but if helpful for readers, links may be repeated in infoboxes, tables, image captions,footnotes, and at the first occurrence after the lead7.
The fact that Wikipedia editors did not explicitlystate exceptions to the above rule (e.g.
for cases where the word or phrase is used to refer to two differentarticles, thus breaking the OSPD hypothesis) is remarkable, and might indicate that Wikipedia editorshad not felt the need to challenge the OSPD hypothesis.3.1 One entity per collectionWe took the opportunity to also explore ?one entity per collection?, which gives an idea of what isthe spread of entities for whole document collections.
In this case, there is no need to count mention-document pairs, as there is one single document, the collection, so we estimate the hypothesis accordingto mention types.
The first two columns in table 3 shows that, overall, mentions which occurred morethan once in the collection tend to refer to the same entity 85.9% of the time in AIDA, and 75.2% of thetime in TAC2009.As we know that multiple mentions in a document tend to refer to one entity, the second two columnsin table 3 offers the statistics when factoring out multiple occurrences of mention in a document, that is,leaving the first mention in each document.
The statistics are very similar, with minor variations.We think that the lower estimate for TAC2009 is an artifact of how the TAC KBP organizers set up thedataset, as they were explicitly looking for cases where the target string would refer to different entities,making the task more challenging for NED systems.
This fact does not affect OSPD for documents, asthose strings still tend to refer to a single entity per document, but given the need to find occurrencesfor different entities, the organizers (Ji et al., 2010) did focus on strings occurring with different entitiesacross the document collection.
This is in contrast with AIDA, where they tagged all named-entitiesoccurring in the target documents.
Had the organizers of TAC2009 focused on a random choice ofstrings and documents, the one entity per collection would also hold to the high degree exhibited inAIDA, as the genre of most of the documents is also news (as in AIDA).7http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#What_generally_should_be_linked2264Syn.
coll.
PropositionsMention-collocation pairs 58 61Ambiguous pairs 5 1OSPC 91.4% 98.4%Table 4: One entity per collocation: statistics for syntactic collocations and propositions.
The 58mention-collocation pairs correspond to 262 occurrences, and the 61 mention-proposition pairs to 279.4 One entity per collocationIn order to estimate OSPC for syntactic collocations, we manually annotated several occurrences of the138 mention strings of the TAC2009 dataset.
Hand-tagging mention entities is a costly process, so wechose (at random) one syntactic dependency relation for each of the 138 mention strings that occurredmore than five times in the corpus.
We then hand-tagged at random five occurrences of each collocation(cf.
Figure 2).
This method would provide a maximum of 5 examples for each of the 138 mentions, butafter checking the minimum frequency of the collocations, the quality of the context, repeated sentences,mentions that are not ambiguous in the dictionary, and whether the mention could be attached to anentity in the database, the actual number was lower.
All in all we found 58 mention-collocation pairs(262 occurrences) for syntactic collocations (cf.
middle column in Table 4).
Only 5 mentions referred tomore than one entity per collocation, yielding that OSPC for syntactic collocation is around 91.4%.To gather the dataset for propositions, we followed the same method as for the syntactic collocations,that is, we chose (at random) one propositions involving one of the 138 mention strings that occurredmore than five times in the corpus, and hand-tagged at random five occurrences of each proposition(cf.
Figure 2.
As with syntactic collocations, we also found a limited number of mentions filling thedesired properties.
That left 61 mention-collocation pairs (279 occurrences) for propositions (cf.
rightcolumn in Table 4).
Only 1 mention referred to more than one entity per proposition, yielding OSPCfor propositions around 98.4%.
This shows that the more specific the context is, the stronger is the linkbetween mention and entity.5 Improving performanceIn order to check whether any of the ?one sense?
hypothesis above could improve the performance ofa NED system, we followed a simple procedure: After running the NED system, we take its outputand observe, for each mention string, which is the entity returned most often for a given document (orcollocation), assigning to all occurrences the majority entity.
In case of ties, we return the entity with thehighest support from the NED system.
We tested the improvements on three NED systems: the freelyavailable DBpedia Spotlight, a reimplementation of a strong Bayesian NED system and a graph-basedsystem.DBpedia Spotlight is a freely available NED system (Daiber et al., 2013), based on a generative proba-bilistic model (Han and Sun, 2011).
Nowadays it is one of the most widely used NED systems and attainsperformances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parametersfor all the experiments in this paper.We also tested an in-house reimplementation of the generative probabilistic model presented in (Hanand Sun, 2011).
This is a state-of-the-art system which got the same accuracy as the best participant(72.0) when evaluated in the non-NIL subset of TAC2013.UKB is a freely-available system for performing Word Sense Disambiguation and Similarity basedon random walks on graphs (Agirre and Soroa, 2009).
Instead of using it on WordNet, we representedWikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hy-perlinks among Wikipedia pages, effectively implementing a NED system.
We used a Wikipedia dumpfrom 2013 in our experiments.
UKB is a competitive, state-of-the-art system which attained a score of69.0 when evaluated in the non-NIL subset of the TAC2013 dataset.The input of the systems is the context of each mention to be disambiguated, in the form of a 100token window centered in the target mention.
In NED, the identification of the correct mention to be2265Mention in context EntityAbbott Beefs Up Litigation ... Abbot KinneyAbbott Laboratories Inc., bracing ... Abbott LaboratoriesAbbott said it had restated ... Abbott Laboratoriesventure between Abbott and Takeda ... Abbott LaboratoriesAbbott said in January ... Abbott LaboratoriesFigure 3: Applying OSPD: Each of the five occurrences of Abbott in the document in Figure 1 has beentagged independently by a NED systems, which return the correct entity in all but one case (precision80%).
Applying OSPD would return the correct entity (Abbott Laboratories) in all cases, improvingprecision to 100%.AIDA TAC 2009Prec.
Recall F1 Prec.
Recall F1Spotlight 83.24 63.90 72.30 64.48 46.44 53.99+ OSPD Discourse 84.17 70.01 76.44 64.65 48.50 55.42+ OSPD Collection 84.02 74.64 79.05 56.24 47.98 51.78UKB 70.09 69.03 69.55 67.70 67.64 67.67+ OSPD Discourse 71.30 70.23 70.76 70.21 70.21 70.21+ OSPD Collection 75.79 74.64 75.21 68.84 68.84 68.84(Han and Sun, 2011) 65.71 65.11 65.41 65.49 65.49 65.49+ OSPD Discourse 67.77 67.37 67.57 66.27 66.27 66.27+ OSPD Collection 74.29 73.89 74.09 68.24 68.24 68.24Table 5: Applying OSPD: NED performance on AIDA and TAC2009 OSPD datasets, including each ofthe three NED systems, and the results after applying OSPD at the document and collections levels.
Boldmarks best result for each system.disambiguated is part of the problem.
AIDA does provide gold mentions, but TAC2009 only provides aquery string which might be just a substring of the real mention in the document.
We treated both corpusin the same way.
In the case of DBpedia Spotlight we use the built-in mention spotter.
In the case of ourin-house implementations, we use the longest string that matches a valid entity mention in the system, asgiven by the dictionary (cf.
Section 3).Some of the NED systems do not return an entity for all mentions, so we evaluate precision, recall andthe harmonic mean (F1 measure).
Statistical significance has been estimated using Wilcoxon.
We reusedthe same corpora as in the previous sections for the evaluation, and also removed all NIL mentions (i.e.mentions which refer to an entity not in Wikipedia).5.1 One entity per discourseWe report the improvements using OSPD for both document and collection levels.
At the documentlevel, we relabel mentions that occur multiple times in a document using the entity returned most timesby the NED system in that document.
Figure 3 illustrates the idea for a NED system on the same sampledocument as in Figure 1.
At the collection level, we relabel mentions using the entity returned mosttimes by the NED systems in the whole collection.Table 5 reports the results of the performance as evaluated on mentions occurring multiple times inthe AIDA and TAC2009 datasets.
The numbers in the left part of the table correspond to the perfor-mance as evaluated on mentions occurring multiple times in AIDA documents.
Note that the number ofoccurrences where OSPD at the collection level can be applied is larger (a superset of those for OSPDat the document level), as, for instance, a mention string occurring once in three different documentswon?t be affected by OSPD at the document level, but it could be relabeled at the collection level.
Wewere especially interested in making the numbers between OSPD at the document and collection levels2266CPI subject-of rise Angela Merkel has CDU:Consumer price index Christian Democratic Union (Germany)Consumer price index Catholic Distance UniversityCommunist Party of India Christian Democratic Union (Germany)Communist Party of India Christian Democratic Union (Germany)Consumer price index Christian Democratic Union (Germany)Figure 4: Applying OSPC: A NED system system tagged each example in Figure 2 independently.
ForCPI, the precision is 60%, but after relabeling with OSPC it would be 100%.
For CDU, the improvementis from 80% to 100%.Syntactic collocations Propositionsprec.
recall F1 prec.
recall F1Spotlight 82.46 66.41 73.57 74.67 60.22 66.67+ OSPC 82.63 67.18 74.11 74.79 62.72 68.23UKB 75.86 75.57 75.72 67.87 67.38 67.63+ OSPC 78.54 78.24 78.39 68.59 68.10 68.35(Han and Sun, 2011) 75.57 75.57 75.57 71.33 71.33 71.33+ OSPC 78.24 78.24 78.24 73.12 73.12 73.12Table 6: Applying OSPC: NED performance on TAC2009, including each of the three NED systems,and the results after applying OSPC for syntactic collocations and propositions.
Bold is used for bestresults for each system.directly comparable, and therefore report the results on the same occurrences, that is, the occurrenceswhere OSPD at the document level can be applied.The results show a small but consistent improvement for OSPD at the document level in precision,recall and F1 for the three NED systems, around 1 or 2 absolute points.
The improvements when applyingOSPD at the collection level are also consistent, but remarkably larger, between 5 and 9 absolute points.All improvements are statistically significant (p-value below 0.01).Table 5 also reports the results after applying OSPD to TAC2009 instances which occurred morethan once in a document.
Results for OSPD at document level and collection level follow the samemethodology as for AIDA.
The improvement at the collection level is not so consistent, with a loss inperformance for Spotlight, a small improvement for UKB, and a larger improvement for (Han and Sun,2011).
All differences across the table are statistically significant (p-value below 0.01).While the OSPD at the document level is strong in both corpora, Section 3.1 showed that the OSPDat the collection level is only strong in AIDA, with a much lower estimate in TAC2009.
This factwould explain why the improvement with OSPD at the collection level is not consistent.
Followingthe rationale in Section 3.1, we think that had the organizers of the task chosen strings and documentsat random, the improvement in TAC 2009 at the collection level would be also as high as in AIDA.
Thehigh improvement in AIDA at the collection level compared to the more modest improvement at thedocument level, despite having a lower OSPD estimate (cf.
Section 3.1), could be caused by the fact thatthere are more occurrences and evidence in favor of the majority entity.5.2 One entity per collocationFigure 4 shows the application of OSPC to the output of a NED system to two sample collocations in ourdataset.
In this case, the application of OSPC would increase precision to 100%.
The actual result on thedatasets produced in Section 4 for syntactic collocations and propositions is reported on table 6.Regarding syntactic collocations, table 6 shows that the improvement is small but consistent for thethree systems on precision, recall and F1, ranging from 0.5 to 2.5 absolute points in F1 score.
The resultsfor propositions also show the same trend, with consistent improvements across the table.
All differences2267in the two tables are statistically significant (p-value < 0.01), except for UKB.6 Conclusions and future workOur study shows that OSPD holds for 96%-98% (in the AIDA and TAC2009 datasets, respectively)of the mentions that occur multiple times in documents.
We also measured OSPD at the collectionlevel (86% and 75%, respectively).
OSPC holds for 91% of the mentions that occur multiple times inthe syntactic collocations that we studied, and 98% of the mentions that occur multiple times in morespecific collocations.
We reused the publicly available AIDA dataset for estimating OSPD.
In addition,we created a dataset to study OSPC based on the TAC KBP Entity Linking 2009 task dataset, which ispublicly available8.We carefully chose to estimate both OPSD and OSPC on TAC2009, in order to make the numbersbetween OSPD and OSPC comparable.
The OSPD numbers for AIDA are very similar to those obtainedon TAC2009, providing complementary evidence.
Although the high estimate of OSPD for entities wassomehow expected, the high estimate of OSPC for the syntactic collocations, especially the propositions,was somehow unexpected, given the high ambiguity rate of the discussed strings, and the fact that theambiguity included similar entities, like for instance ?ABC?
which can refer, among other 190 entities,to the American Broadcasting Company or the Australian Broadcasting Corporation.Our results also show that a simple application of the OSPD and OSPC hypotheses to the output ofthree different NED systems improves the results in all cases.
Remarkably, the highest performance gain,8 absolute points, was for OSPD at the collection level in the AIDA corpus.The results presented here could be largely dependent on the domain and genre of the documents,as well as the definition of collocation.
Our work is a strong basis for claiming that OSPD and OSPChold for entities, but the evidence could be further extended exploring alternative operationalization ofcollocations and a larger breadth of genres and domains.For the future we would like to check whether these hypotheses can be further used to improve currentNED systems.
The OSPD hypothesis can be used to jointly disambiguate all occurrences of a mentionin a document.
The OSPC hypothesis could be used to acquire important disambiguation features, or toperform large-scale joint entity linking.
The OSPD for whole collections could be useful for documentson specific domains, and for domain adaptation scenarios.AcknowledgementsThis work was partially funded by MINECO (CHIST-ERA READERS project ?
PCIN-2013-002- C02-01) and the European Commission (QTLEAP ?
FP7-ICT-2013.4.1-610516, OPENER ?
FP7-ICT-2011-SME-DCL-296451).
Ander Barrena is supported by a PhD grant from the University of the BasqueCountry.ReferencesEneko Agirre and Philip Edmonds.
2007.
Word Sense Disambiguation: Algorithms and Applications.
SpringerPublishing Company, Incorporated, 1st edition.Eneko Agirre and Aitor Soroa.
2009.
Personalizing pagerank for word sense disambiguation.
In Proceedingsof the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ?09,pages 33?41.Marine Carpuat.
2009.
One translation per discourse.
In Proceedings of the Workshop on Semantic Evaluations:Recent Achievements and Future Directions, DEW ?09, pages 19?27, Stroudsburg, PA, USA.
Association forComputational Linguistics.Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes.
2013.
Improving efficiency and accuracyin multilingual entity extraction.
In Proceedings of the 9th International Conference on Semantic Systems,I-SEMANTICS ?13, pages 121?124, New York, NY, USA.
ACM.8http://ixa2.si.ehu.es/OEPDC2268Marie-Catherine De Marneffe and Christopher D. Manning.
2008.
Stanford typed dependencies manual.
URLhttp://nlp.
stanford.
edu/software/dependencies manual.
pdf.William A. Gale, Kenneth W. Church, and David Yarowsky.
1992.
One sense per discourse.
In Proceedings ofthe workshop on Speech and Natural Language, HLT ?91, page 233237, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran.
2012.
Evaluating EntityLinking with Wikipedia.
Artif.
Intell., 194:130?150, January.Xianpei Han and Le Sun.
2011.
A Generative Entity-mention Model for Linking Entities with Knowledge Base.In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies, volume 1, pages 945?954.Johannes Hoffart, Mohamed A. Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, BilyanaTaneva, Stephan Thater, and Gerdhard Weikum.
2011.
Robust Disambiguation of Named Entities in Text.In Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, United Kingdom2011, pages 782?792.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis.
2010.
Overview of the tac 2010 knowledgebase population track.
In Third Text Analysis Conference (TAC 2010).Dan Klein and Christopher D Manning.
2003.
Accurate unlexicalized parsing.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for Computa-tional Linguistics.Robert Krovetz.
1998.
More than one sense per discourse.
In NEC Princeton NJ Labs., Research Memorandum.Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Linguistics,39(4).David Martinez and Eneko Agirre.
2000.
One sense per collocation and genre/topic variations.
In Proceedingsof the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very largecorpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics -Volume 13, EMNLP ?00, page 207215, Stroudsburg, PA, USA.
Association for Computational Linguistics.Anselmo Pe?nas and Eduard Hovy.
2010.
Filling knowledge gaps in text for machine reading.
In Proceedingsof the 23rd International Conference on Computational Linguistics: Posters, pages 979?987.
Association forComputational Linguistics.Valentin I. Spitkovsky and Angel X. Chang.
2012.
A Cross-lingual Dictionary for English Wikipedia Concepts.Eighth International Conference on Language Resources and Evaluation (LREC 2012).David Yarowsky.
1993.
One sense per collocation.
In Proceedings of the workshop on Human Language Tech-nology, HLT ?93, page 266271, Stroudsburg, PA, USA.
Association for Computational Linguistics.2269
