From Single to Multi-document Summarization:A Prototype System and its EvaluationChin-Yew Lin and Eduard HovyUniversity of Southern California / Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292{cyl,hovy}@isi.eduAbstractNeATS is a multi-documentsummarization system that attemptsto extract relevant or interestingportions from a set of documentsabout some topic and present themin coherent order.
NeATS is amongthe best performers in the large scalesummarization evaluat ion DUC2001.1 IntroductionIn recent years, text summarization has beenenjoying a period of revival.
Two workshopson Automatic Summarization were held in2000 and 2001.
However, the area is stillbeing fleshed out: most past efforts havefocused only on single-documentsummarization (Mani 2000), and no standardtest sets and large scale evaluations have beenreported or made available to the English-speaking research community except theTIPSTER SUMMAC Text Summarizationevaluation (Mani et al 1998).To address these issues, the DocumentUnderstanding Conference (DUC) sponsoredby the National Institute of Standards andTechnology (NIST) started in 2001 in theUnited States.
The Text SummarizationChallenge (TSC) task under the NTCIR (NII-NACSIS Test Collection for IR Systems)project started in 2000 in Japan.
DUC andTSC both aim to compile standard training andtest collections that can be shared amongresearchers and to provide common and largescale evaluations in single and multipledocument summarization for their participants.In this paper we describe a multi-documentsummarization system NeATS.
It attempts toextract relevant or interesting portions from aset of documents about some topic and presentthem in coherent order.
We outline theNeATS system and describe how it performscontent selection, filtering, and presentation inSection 2.
Section 3 gives a brief overview ofthe evaluation procedure used in DUC -2001(DUC 2001).
Section 4 discusses evaluationmetrics, and Section 5 the results.
Weconclude with future directions.2 NeATSNeATS is an extraction-based multi-documentsummarization system.
It leverages techniquesproved effective in single documentsummarization such as: term frequency (Luhn1969), sentence position (Lin and Hovy 1997),stigma words (Edmundson 1969), and asimplified version of MMR (Goldstein et al1999) to select and filter content.
To improvetopic coverage and readability, it uses termclustering, a ?buddy system?
of pairedsentences, and explicit time annotation.Most of the techniques adopted by NeATS arenot new.
However, applying them in theproper places to summarize multipledocuments and evaluating the results on largescale common tasks are new.Given an input of a collection of sets ofnewspaper articles, NeATS generatessummaries in three stages: content selection,filtering, and presentation.
We describe eachstage in the following sections.2.1 Content SelectionThe goal of content selection is to identifyimportant concepts mentioned in a documentcollection.
For example, AA flight 11, AAflight 77, UA flight 173, UA flight 93, NewYork, World Trade Center, Twin Towers,Osama bin Laden, and al-Qaida are keyconcepts for a document collection about theSeptember 11 terrorist attacks in the US.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
457-464.Proceedings of the 40th Annual Meeting of the Association forIn a key step for locating important sentences,NeATS computes the likelihood ratio l(Dunning, 1993) to identify key concepts inunigrams, bigrams, and trigrams1, using theon- topic document collection as the relevantset and the off-topic document collection as theirrelevant set.
Figure 1 shows the top 5concepts with their relevancy scores (-2l) forthe topic ?Slovenia Secession fromYugoslavia?
in the DUC-2001 test collection.This is similar to the idea of topic signatureintroduced in (Lin and Hovy 2000).With the individual key concepts available, weproceed to cluster these concepts in order toidentify major subtopics within the main topic.Clusters are formed through strict lexicalconnection.
For example, Milan and Kucanare grouped as ?Milan Kucan?
since ?MilanKucan?
is a key bigram concept; whileCroatia, Yugoslavia, Slovenia, republic, andare joined due to the connections as follows:?
Slovenia Croatia?
Croatia Slovenia?
Yugoslavia Slovenia?
republic Slovenia1 Closed class words (of, in, and, are, and so on)were ignored in constructing unigrams, bigrams andtrigrams.?
Croatia republicEach sentence in the document set is thenranked, using the key concept structures.
Anexample is shown in Figure 2.
The rankingalgorithm rewards most specific concepts first;for example, a sentence containing ?MilanKucan?
has a higher score than a sentencecontains only either Milan or Kucan.
Asentence containing both Milan and Kucan  butnot in consecutive order gets a lower score too.This ranking algorithm performs relativelywell, but it also results in many ties.Therefore, it is necessary to apply somefiltering mechanism to maintain a reasonablysized sentence pool for final presentation.2.2 Content FilteringNeATS uses three different filters: sentenceposition, stigma words, and maximummarginal relevancy.2.2.1  Sentence PositionSentence position has been used as a goodimportant content filter since the late 60s(Edmundson 1969).
It was also used as abaseline in a preliminary multi-documentsummarization study by Marcu and Gerber(2001) with relatively good results.
We applya simple sentence filter that only retains thelead 10 sentences.2.2.2  Stigma WordsSome sentences start with?
conjunctions (e.g., but, although, however),?
the verb say and its derivatives,?
quotation marks,?
pronouns such as he, she, and they,and usually cause discontinuity in summaries.Since we do not use discourse level selectioncriteria ?
la (Marcu 1999), we simply reducethe scores of these sentences to avoid includingthem in short summaries.2.2.3   Maximum Marginal RelevancyFigure 2.
Top 5 unigram, bigram, and trigram concepts for topic "Slovenia Secession from Yugoslavia".Rank Unigram (-2l) Bigram (-2l) Trigram (-2l)1 Slovenia 319.48 federal army 21.27 Slovenia central bank 5.802 Yugoslavia 159.55 Slovenia Croatia 19.33 minister foreign affairs 5.803 Slovene 87.27 Milan Kucan 17.40 unallocated federal debt 5.804 Croatia 79.48 European Community 13.53 Drnovsek prime minister 3.865 Slovenian 67.82 foreign exchange 13.53 European Community countries 3.86Figure 1.
Sample key concept structure.n1(:S URF " WEBCL -SUMM MARIZ ER-KU CAN":C AT S- NP:C LASS I-EN- WEBCL -SIGN ATURE -KUCAN:L EX  0 .6363 63636 36363 6:S UBS( ((KUC AN-0)(:S URF " Milan  Ku can":C AT S- NP:C LASS I-EN- WEBCL -SIGN ATURE -KUCAN:L EX 0.
63636 36363 63636:S UBS((( KUCAN -1)(:S URF " Ku can":C AT S- NP:C LASS I-EN- WEBCL -SIGN ATURE -KUCAN:L EX 0.
63636 36363 63636 ))(( KUCAN -2)(:S URF " Milan ":C AT S- NP:C LASS I-EN- WEBCL -SIGN ATURE -KUCAN:L EX 0.
63636 36363 63636 ))))) ))The content selection and filtering methodsdescribed in the previous section only  concernindividual sentences.
They do not consider theredundancy issue when two top rankedsentences refer to similar things.
To addressthe problem, we use a simplified version ofCMU?s MMR (Goldstein et al 1999)algorithm.
A sentence is added to thesummary if and only if its content has less thanX percent overlap with the summary.
Theoverlap ratio is computed using simplestemmed word overlap and the threshold X isset empirically.2.3 Content PresentationNeATS so far only considers featurespertaining to individual sentences.
As wementioned in Section 2.2.2, we can demotesome sentences containing stigma words toimprove the cohesion and coherence ofsummaries.
However, we still face twoproblems: definite noun phrases and eventsspread along an extended timeline.
Wedescribe these problems and our solutions inthe following sections.2.3.1  A Buddy System of Paired SentencesThe problem of definite noun phrases can beillustrated in Figure 3.
These sentences arefrom documents of the DUC -2001 topic USDrought of 1988.
According to pure sentencescores, sentence 3 of document AP891210-0079 has a higher score (34.60) than sentence1 (32.20) and should be included in the shortersummary (size=?50?).
However, if we selectsentence 3 without also including sentence 1,the definite noun phrase ?The record $3.9billion drought relief program of 1988?
seemsto come without any context.
To remedy thisproblem, we introduce a buddy system toimprove cohesion and coherence.
Eachsentence is paired with a suitable introductorysentence unless it is already an introductorysentence.
In DUC -2001 we simply used thefirst sentence of its document.
This assumeslead sentences provide introduction andcontext information about what is coming next.2.3.2  Time Annotation and SequenceOne main problem in multi-documentsummarization is that documents in acollection might span an extended time period.For example, the DUC-2001 topic ?SloveniaSecession from Yugoslavia?
contains 11documents dated from 1988 to 1994, from 5different sources 2.
Although a sourcedocument for single-document summarizationmight contain information collected across anextended time frame and from multiplesources, the author at least would synchronizethem and present them in a coherent order.
Inmulti-document summarization, a dateexpression such as Monday occurring in twodifferent documents might mean the same dateor different dates.
For example, sentences inthe 100 word summary shown in Figure 4come from 3 main time periods, 1990, 1991,and 1994.
If no absolute time references aregiven, the summary might mislead the readerto think that all the events mentioned in thefour summary sentences occurred in a singleweek.
Therefore, time disambiguation andnormalization are very important in multi-document summarization.
As the first attempt,we use publication dates as reference pointsand compute actual dates for the followingdate expressions:?
weekdays (Sunday, Monday, etc);?
(past | next | coming) + weekdays;?
today, yesterday, last night.We then order the summary sentences in theirchronological order.
Figure 4 shows an2 Sources include Associated Press, ForeignBroadcast Information Service, Financial Times,San Jose Mercury News, and Wall Street Journal.<multi size="50" docset="d50i">AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially inWashington where politicians pushed through the largest disaster relief measure in U.S. history.AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ?</multi><multi size="100" docset="d50i">AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially inWashington where politicians pushed through the largest disaster relief measure in U.S. history.AP891210-0079 3 (41.18) (12/10/89) The record $3.9 billion drought relief program of 1988, hailed assalvation for small farmers devastated by a brutal dry spell, became much more _ an unexpected, election-year windfall for thousands of farmers who collected millions of dollars for nature's normal quirks.AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ?</multi>Figure 3.
50 and 100 word summaries for topic "US Drought of 1988".example 100 words summary with timeannotations.
Each sentence is marked with itspublication date and a reference date(MM/DD/YY) is inserted after every dateexpression.3 DUC 2001Before we present our results, we describe thecorpus and evaluation procedures of theDocument Understanding Conference 2001(DUC 2001).DUC is a new evaluation series supported byNIST under TIDES, to further progress insummarization and enable researchers toparticipate in large-scale experiments.
Therewere three tasks in 2001:(1) Fully automatic summarization of a singledocument.
(2) Fully automatic summarization of multipledocuments: given a set of document on asingle subject, participants were required tocreate 4 generic summaries of the entire setwith approximately 50, 100, 200, and 400words.
30 document sets of approximately 10documents each were provided with their 50,100, 200, and 400 human written summariesfor training (training set) and another 30unseen sets were used for testing (test set).
(3) Exploratory summarization: participantswere encouraged to investigate alternativeapproaches in summarization and report theirresults.NeATS participated only in the fully automaticmulti-document summarization task.
A totalof 12 systems participated in that task.The training data were distributed in earlyMarch of 2001 and the test data weredistributed in mid-June of 2001.
Results weresubmitted to NIST for evaluation by July 1st.3.1 Evaluation ProceduresNIST assessors who created the ?ideal?
writtensummaries did pairwise comparisons of theirsummaries to the system-generated summaries,other assessors?
summaries, and baselinesummaries.
In addition, two baselinesummaries were created automatically asreference points.
The first baseline, leadbaseline, took the first 50, 100, 200, and 400words in the last document in the collection.The second baseline, coverage baseline, tookthe first sentence in the first document, the firstsentence in the second document and so onuntil it had a summary of 50, 100, 200, or 400words.3.2 Summary EvaluationEnvironmentNIST used the Summary EvaluationEnvironment (SEE) 2.0 developed by one ofthe authors (Lin 2001) to support its humanevaluation process.
Using SEE, the assessorsevaluated the quality of the system?s text (thepeer text) as compared to an ideal (the modeltext).
The two texts were broken into lists ofunits and displayed in separate windows.
InDUC-2001 the sentence was used as thesmallest unit of evaluation.SEE 2.0 provides interfaces for assessors tojudge the quality of summaries ingrammatically3, cohesion4, and coherence5 atfive different levels: all, most, some, hardlyany, or none.
It also allow s assessors to stepthrough each model unit, mark all system unitssharing content with the current model unit,and specify that the marked system units3 Does a summary follow the rule of Englishgrammatical rules independent of its content?4 Do sentences in a summary fit in with theirsurrounding sentences?5 Is the content of a summary expressed andorganized in an effectiv e way?Figure 4.
100 word summary with explicit time annotation.<multi size="100" docset="d45h">AP900625-0160  1 (26.60) (06/25/90) The republic of Slovenia plans to begin work on a constitutionthat will give it full sovereignty within a new Yugoslav confederation, the state Tanjug news agencyreported Monday (06/25/90).WSJ910628-0109 3 (9.48)  (06/28/91) On Wednesday (06/26/91), the Slovene soldiers manning this borderpost raised a new flag to mark Slovenia's independence from Yugoslavia.WSJ910628-0109 5 (53.77) (06/28/91) Less than two days after Slovenia and Croatia, two of Yugoslavia'ssix republics, unilaterally seceded from the nation, the federal government in Belgrade mobilizedtroops to regain control.FBIS3-30788    2 (49.14) (02/09/94) In the view of Yugoslav diplomats, the normalization of relationsbetween Slovenia and the Federal Republic of Yugoslavia will certainly be a strenuous and long-termproject.</multi>express all, most, some or hardly any of thecontent of the current model unit.4 Evaluation MetricsOne goal of DUC-2001 was to debug theevaluation procedures and identify stablemetrics that could serve as common referencepoints.
NIST did not define any officialperformance metric in DUC-2001.
It releasedthe raw evaluation results to DUC -2001participants and encouraged them to proposemetrics that would help progress the field.4.1.1  Recall, Coverage, Retention andWeighted RetentionRecall at different compression ratios has beenused in summarization research (Mani 2001) tomeasure how well an automatic system retainsimportant content of original documents.Assume we have a system summary Ss and amodel summary Sm.
The number of sentencesoccurring in both Ss and Sm is Na, the numberof sentences in Ss is Ns, and the number ofsentences in Sm is Nm.
Recall is defined asNa/Nm.
The Compression Ratio is defined asthe length of a summary (by words orsentences) divided by the length of its originaldocument.
DUC-2001 set the compressionlengths to 50, 100, 200, and 400 words for themulti-document summarization task.However, applying recall in DUC-2001without modification is not appropriatebecause:1.
Multiple system units contribute tomultiple model units.2.
Ss and Sm do not exactly overlap.3.
Overlap judgment is not binary.For example, in an evaluation session anassessor judged system units S1.1 and S10.4 assharing some content with model unit M2.2.Unit S1.1 says ?Thousands of people arefeared dead?
and unit M2.2 says ?3,000 andperhaps ?
5,000 people have been killed?.Are ?thousands?
equivalent to ?3,000 to5,000?
or not?
Unit S10.4 indicates it was an?earthquake of magnitude 6.9?
and unit M2.2says it was ?an earthquake measuring 6.9 onthe Richter scale?.
Both of them report a ?6.9?earthquake.
But the second part of systemunit S10.4, ?in an area so isolated?
?, seemsto share some content with model unit M4.4?the quake was centered in a remotemountainous area?.
Are these two equivalent?This example highlights the difficulty ofjudging the content coverage of systemsummaries against model summaries and theinadequacy of using recall as defined.As we mentioned earlier, NIST assessors notonly marked the sharing relations amongsystem units (SU) and model units (MU), theyalso indicated the degree of match, i.e., all,most , some, hardly any,  or none.
This enablesus to compute weighted recall.Different versions of weighted recall wereproposed by DUC-2001 participants.McKeown et al (2001) treated thecompleteness of coverage as threshold: 4 forall, 3 for most  and above, 2 for some andabove, and 1 for hardly any and above.
Theythen proceeded to compare systemperformances at different threshold levels.They defined recall at threshold t, Recallt, asfollows:summary model in the MUs ofnumber  Totalaboveor at  marked MUs ofNumber tWe used the completeness of coverage ascoverage score, C, instead of threshold: 1 forall, 3/4 for most, 1/2 for some, and 1/4 forhardly any, 0 for none.
To avoid confusionwith the recall used in information retrieval,we call our metric weighted retention,Retentionw, and define it as follows:summary model in the MUs ofnumber  Totalmarked) MUs of(Number C?if we ignore C and set it always to 1, we obtainan unweighted retention, Retention1.
We usedRetention1 in our evaluation to illustrate thatrelative system performance changes whendifferent evaluation metrics are chosen.Therefore, it is important to have common andagreed upon metrics to facilitate large scaleevaluation efforts.4.1.2  Precision and Pseudo PrecisionPrecision is also a common measure.Borrowed from information retrieval research,precision is used to measure how effectively asystem generates good summary sentences.
Itis defined as Na/ Ns.
Precision in a fixed lengthsummary output is equal to recall since N s =Nm.
However, due to the three reasons statedat the beginning of the previous section, nostraightforward computation of the traditionalprecision is available in DUC-2001.If we count the number of model units that aremarked as good summary units and areselected by systems, and use the number ofmodel units in various summary lengths as thesample space, we obtain a precision metricequal to Retention1.
Alternatively, we cancount how many unique system units sharecontent with model units and use the totalnumber of system units as the sample space.We define this as pseudo precision, Precisionp,as follows:summary system in the SUs ofnumber  Totalmarked SUs ofNumberMost of the participants in DUC-2001 reportedtheir pseudo precision figures.5 Results and DiscussionWe present the performance of NeATS inDUC-2001 in content and quality measures.5.1 ContentWith respect to content, we computedRetention1, Retention w, and Precisionp usingthe formulas defined in the previous section.The scores are shown in Table 1 (overallaverage and per size).
Analyzing all systems?results according to these, we made thefollowing observations.
(1) NeATS (system N) is consistently rankedamong the top 3 in average and per sizeRetention1 and Retention w.(2) NeATS?s performance for averaged pseudoprecision equals human?s at about 58% (Pp all).
(3) The performance in weighted retention isreally low.
Even humans6 score only 29% (Rwall).
This indicates low inter-human agreement(which we take to reflect the undefinedness ofthe ?generic summary?
task).
However, theunweighted retention of humans is 53%.
Thissuggests assessors did write something similarin their summaries but not exactly the same;once again illustrating the difficulty ofsummarization evaluation.
(4) Despite the low inter -human agreement,humans score better than any system.
Theyoutscore the nearest system by about 11% inaveraged unweighted retention (R1 all : 53% vs.42%) and weighted retention (Rw all : 29% vs.18%).
There is obviously still considerableroom for systems to improve.
(5) System performances are separated intotwo major groups by baseline 2 (B2: coveragebaseline) in averaged weighted retention.
Thisconfirms that lead sentences are goodsummary sentence candidates and that onedoes need to cover all documents in a topic toachieve reasonable performance in multi-document summarization.
NeATS?s strategiesof filtering sentences by position and addinglead sentences to set context are provedeffective.
(6) Different metrics result in differentperformance rankings.
This is demonstratedby the top 3 systems T, N, and Y.
If we usethe averaged unweighted retention (R1 all), Y is6 NIST assessors wrote two separate summaries pertopic.
One was used to judge all system summariesand the two baselines.
The other was used todetermine the (potential) upper bound.Table 1.
Pseudo precision, unweighted retention, and weighted retention for all summary lengths: overallaverage, 400, 200, 100, and 50 words.SYS Pp All R1 All Rw Al l Pp  4 0 0 R1  4 0 0 Rw  4 0 0 Pp  2 0 0 R1 200 Rw  2 0 0 Pp 100 R1 100 Rw 100 Pp  50 R1 50 Rw 50HM 58.71% 53.00% 28.81% 59.33% 52.95% 33.23% 59.91% 57.23% 33.82% 58.73% 54.67% 27.54% 56.87% 47.16% 21.62%T 48.96% 35.53% (3) 18.48% (1) 56.51% (3) 38.50% (3) 25.12% (1) 53.85% (3) 35.62% 21.37% (1) 43.53% 32.82% (3) 14.28% (3) 41.95% 35.17% (2) 13.89% (2)N* 58.72% (1) 37.52% (2) 17.92% (2) 61.01% (1) 41.21% (1) 23.90% (2) 63.34% (1) 38.21% (3) 21.30% (2) 58.79% (1) 36.34% (2) 16.44% (2) 51.72% (1) 34.31% (3) 10.98% (3)Y 41.51% 41.58% (1) 17.78% (3) 49.78% 38.72% (2) 20.04% 43.63% 39.90% (1) 16.86% 34.75% 43.27% (1) 18.39% (1) 37.88% 44.43% (1) 15.55% (1)P 49.56% 33.94% 15.78% 57.21% (2) 37.76% 22.18% (3) 51.45% 37.49% 19.40% 46.47% 31.64% 13.92% 43.10% 28.85% 9.09%L 51.47% (3) 33.67% 15.49% 52.62% 36.34% 21.80% 53.51% 36.87% 18.34% 48.62% (3) 29.00% 12.54% 51.15% (2) 32.47% 9.90%B2 47.27% 30.98% 14.56% 60.99% 33.51% 18.35% 49.89% 33.27% 17.72% 47.18% 29.48% 14.96% 31.03% 27.64% 8.02%S 52.53% (2) 30.52% 12.89% 55.55% 36.83% 20.35% 58.12% (2) 38.70% (2) 19.93% (3) 49.70% (2) 26.81% 10.72% 46.43% (3) 19.23% 4.04%M 43.39% 27.27% 11.32% 54.78% 33.81% 19.86% 45.59% 27.80% 13.27% 41.89% 23.40% 9.13% 31.30% 24.07% 5.05%R 41.86% 27.63% 11.19% 48.63% 24.80% 12.15% 43.96% 31.28% 15.17% 38.35% 27.61% 11.46% 36.49% 26.84% 6.17%O 43.76% 25.87% 11.19% 50.73% 27.53% 15.76% 42.94% 26.80% 13.07% 40.55% 25.13% 9.36% 40.80% 24.02% 7.03%Z 37.98% 23.21% 8.99% 47.51% 31.17% 17.38% 46.76% 25.65% 12.83% 28.91% 17.29% 5.45% 28.74% 18.74% 3.23%B1 32.92% 18.86% 7.45% 33.48% 17.58% 9.98% 43.13% 18.60% 8.65% 30.23% 17.42% 6.05% 24.83% 21.84% 4.20%W 30.08% 20.38% 6.78% 38.14% 25.89% 12.10% 26.86% 21.01% 7.93% 28.31% 19.15% 5.36% 27.01% 15.46% 3.21%U 23.88% 21.38% 6.57% 31.49% 29.76% 13.17% 24.20% 22.64% 8.49% 19.13% 17.54% 3.77% 20.69% 15.57% 3.04%the best, followed by N, and then T; if wechoose averaged weighted retention (Rw all), Tis the best, followed by N, and then Y.  Thereversal of T and Y due to different metricsdemonstrates the importance of commonagreed upon metrics.
We believe that metricshave to take coverage score (C, Section 4.1.1)into consideration to be reasonable since mostof the content sharing among system units andmodel units is partial.
The recall at threshold t,Recallt (Section 4.1.1), proposed by(McKeown et al 2001), is a good example.
Intheir evaluation, NeATS ranked second at t=1,3, 4 and first at t=2.
(7) According to Table 1, NeATS performedbetter on longer summaries (400 and 200words) based on weighted retention than it didon shorter ones.
This is the result of thesentence extraction-based nature of NeATS.We expect that systems that use syntax-basedalgorithms to compress their output willthereby gain more space to include additionalimportant material.
For example, System Ywas the best in shorter summaries.
Its 100-and 50-word summaries contain onlyimportant headlines.
The results confirm thisis a very effective strategy in composing shortsummaries.
However, the quality of thesummaries suffered because of theunconventional syntactic structure of newsheadlines (Table 2).5.2 QualityTable 2 shows the macro-averaged scores forthe humans, two baselines, and 12 systems.We assign a score of 4 to all, 3 to most, 2 tosome, 1 to hardly any, and 0 to none.
Thevalue assignment is for convenience ofcomputing averages, since it is moreappropriate to treat these measures as steppedvalues instead of continuous ones.
With this inmind, we have the following observations.
(1) Most systems scored well ingrammaticality.
This is not a surprise sincemost of the participants extracted sentences assummaries.But no system or human scored perfect ingrammaticality.
This might be due to theartifact of cutting sentences at the 50, 100, 200,and 400 words boundaries.
Only system Yscored lower than 3, which reflects its headlineinclusion strategy.
(2) When it came to the measure for cohesionthe results are confusing.
If even the human-made summaries score only 2.74 out of 4, it isunclear what this category means, or how theassessors arrived at these scores.
However, thehumans and baseline 1 (lead baseline) didscore in the upper range of 2 to 3 and all othershad scores lower than 2.5.
Some of thesystems (including B2) fell into the range of 1to 2 meaning some or hardly any cohesion.The lead baseline (B1), taking the first 50, 100,200, 400 words from the last document of atopic, did well.
On the contrary, the coveragebaseline (B2) did poorly.
This indicates thedifficulty of fitting sentences from differentdocuments together.
Even selectingcontinuous sentences from the same document(B1) seems not to work well.
We need todefine this metric more clearly and improvethe capabilities of systems in this respect.
(3) Coherence scores roughly track cohesionscores.
Most systems did better in coherencethan in cohesion.
The human is the only onescoring above 3.
Again the room forimprovement is abundant.
(4) NeATS did not fare badly in qualitymeasures.
It was in the same categories asother top performers: grammaticality isbetween most and all, cohesion, some andmost , and coherence, some and most.
Thisindicates the strategies employed by NeATS(stigma word filtering, adding lead sentence,and time annotation) worked to some extentbut left room for improvement.6 ConclusionsTable 2.
Averaged grammaticality, cohesion, andcoherence over all summary sizes.SYS Grammar Cohesion CoherenceHuman 3.74 2.74 3.19B1 3.18 2.63 2.8B2 3.26 1.71 1.65L 3.72 1.83 1.9M 3.54 2.18 2.4N* 3.65 2 2.22O 3.78 2.15 2.33P 3.67 1.93 2.17R 3.6 2.16 2.45S 3.67 1.93 2.04T 3.51 2.34 2.61U 3.28 1.31 1.11W 3.13 1.48 1.28Y 2.45 1.73 1.77Z 3.28 1.8 1.94We described a multi-documentsummarization system, NeATS, and itsevaluation in DUC-2001.
We were encouragedby the content and readability of the results.As a prototype system, NeATS deliberatelyused simple methods guided by a fewprinciples:?
Extracting important concepts based onreliable statistics.?
Filtering sentences by their positions andstigma words.?
Reducing redundancy using MMR.?
Presenting summary sentences in theirchronological order with time annotations.These simple principles worked effectively.However, the simplicity of the system alsolends itself to further improvements.
Wewould like to apply some compressiontechniques or use linguistic units smaller thansentences to improve our retention score.
Thefact that NeATS performed as well as thehuman in pseudo precision but did less well inretention indicates its summaries might includegood but duplicated information.
Workingwith sub-sentence units should help.To improve NeATS?s capability in contentselection, we have started to parse sentencescontaining key unigram, bigram, and trigramconcepts to identify their relations within theirconcept clusters.To enhance cohesion and coherence, we arelooking into incorporating discourseprocessing techniques (Marcu 1999) or Radevand McKeown?s (1998) summary operators.We are analyzing the DUC evaluation scoresin the hope of suggesting improved and morestable metrics.ReferencesDUC.
2001.
The Document UnderstandingWorkshop 2001. http://www-nlpir.nist.gov/projects/duc/2001.html.Dunning, T. 1993.
Accurate Methods for theStatistics of Surprise and Coincidence.Computational Linguistics 19, 61?74.Edmundson, H.P.
1969.
New Methods inAutomatic Abstracting.
Journal of theAssociation for Computing Machinery.16(2).Goldstein, J., M. Kantrowitz, V. Mittal, and J.Carbonell.
1999.
Summarizing TextDocuments: Sentence Selection andEvaluation Metrics.
Proceedings of the 22ndInternational ACM Conference onResearch and Development in InformationRetrieval (SIGIR-99), Berkeley, CA, 121?128.Lin, C.-Y.
and E.H. Hovy.
2000.
TheAutomated Acquisition of TopicSignatures for Text Summarization.Proceedings of the COLINGConference.
Saarbr?cken , Germany.Lin, C.-Y.
2001.
Summary EvaluationEnvironment.
http://www.isi.edu/~cyl/SEE.Luhn, H. P. 1969.
The Automatic Creation ofLiterature Abstracts.
IBM Journal ofResearch and Development 2(2), 1969.Mani, I., D. House, G. Klein, L. Hirschman, L.Obrst, T. Firmin, M. Chrzanow ski, and B.Sundheim.
1998.
The TIPSTER SUMMACText Summarization Evaluation: FinalReport.
MITRE Corp. Tech.
Report.Mani, I.
2001.
Automatic Summarization.
JohnBenjamins Pub Co.Marcu, D. 1999.
Discourse trees are goodindicators of importance in text.
In I. Maniand M. Maybury (eds), Advances inAutomatic Text Summarization, 123?136.MIT Press.Marcu, D. and L. Gerber.
2001.
An Inquiryinto the Nature of MultidocumentAbstracts, Extracts, and their Evaluation.Proceedings of the NAACL -2001 Workshopon Automatic Summarization.
Pittsburgh,PA.McKeown, K., R. Barzilay, D. Evans, V.Hatzivassiloglou, M-Y Kan, B, Schiffman,and S. Teufel 2001.
Columbia Multi-Document Summarization: Approach andEvaluation.
DUC-01 Workshop on TextSummarization.
New Orleans, LA.Radev, D.R.
and K.R.
McKeown.
1998.Generating Natural Language Summariesfrom Multiple On-line Sources.Computational Linguistics, 24(3):469?500.
