Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200?207,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Grammar-driven Convolution Tree Kernel for Se-mantic Role ClassificationMin ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3Guodong ZHOU1,4     Ting LIU2     Sheng LI21Institute for Infocomm Research{mzhang, aaiti}@i2r.a-star.edu.sg2Harbin Institute of Technology{car, tliu}@ir.hit.edu.cnlisheng@hit.edu.cn3National University of Singaporetancl@comp.nus.edu.sg4 Soochow Univ., China 215006gdzhou@suda.edu.cnAbstractConvolution tree kernel has shown promis-ing results in semantic role classification.However, it only carries out hard matching,which may lead to over-fitting and less ac-curate similarity measure.
To remove theconstraint, this paper proposes a grammar-driven convolution tree kernel for semanticrole classification by introducing more lin-guistic knowledge into the standard treekernel.
The proposed grammar-driven treekernel displays two advantages over the pre-vious one: 1) grammar-driven approximatesubstructure matching and 2) grammar-driven approximate tree node matching.
Thetwo improvements enable the grammar-driven tree kernel explore more linguisticallymotivated structure features than the previ-ous one.
Experiments on the CoNLL-2005SRL shared task show that the grammar-driven tree kernel significantly outperformsthe previous non-grammar-driven one inSRL.
Moreover, we present a compositekernel to integrate feature-based and treekernel-based methods.
Experimental resultsshow that the composite kernel outperformsthe previously best-reported methods.1 IntroductionGiven a sentence, the task of Semantic Role Label-ing (SRL) consists of analyzing the logical formsexpressed by some target verbs or nouns and someconstituents of the sentence.
In particular, for eachpredicate (target verb or noun) all the constituents inthe sentence which fill semantic arguments (roles)of the predicate have to be recognized.
Typical se-mantic roles include Agent, Patient, Instrument, etc.and also adjuncts such as Locative, Temporal,Manner, and Cause, etc.
Generally, semantic roleidentification and classification are regarded as twokey steps in semantic role labeling.
Semantic roleidentification involves classifying each syntacticelement in a sentence into either a semantic argu-ment or a non-argument while semantic role classi-fication involves classifying each semantic argumentidentified into a specific semantic role.
This paperfocuses on semantic role classification task with theassumption that the semantic arguments have beenidentified correctly.Both feature-based and kernel-based learningmethods have been studied for semantic role classi-fication (Carreras and M?rquez, 2004; Carreras andM?rquez, 2005).
In feature-based methods, a flatfeature vector is used to represent a predicate-argument structure while, in kernel-based methods,a kernel function is used to measure directly thesimilarity between two predicate-argument struc-tures.
As we know, kernel methods are more effec-tive in capturing structured features.
Moschitti(2004) and Che et al (2006) used a convolutiontree kernel (Collins and Duffy, 2001) for semanticrole classification.
The convolution tree kerneltakes sub-tree as its feature and counts the numberof common sub-trees as the similarity between twopredicate-arguments.
This kernel has shown very200promising results in SRL.
However, as a generallearning algorithm, the tree kernel only carries outhard matching between any two sub-trees withoutconsidering any linguistic knowledge in kernel de-sign.
This makes the kernel fail to handle similarphrase structures (e.g., ?buy a car?
vs. ?buy a redcar?)
and near-synonymic grammar tags (e.g., thePOS variations between ?high/JJ degree/NN?
1 and?higher/JJR degree/NN?)
2.
To some degree, it maylead to over-fitting and compromise performance.This paper reports our preliminary study in ad-dressing the above issue by introducing more lin-guistic knowledge into the convolution tree kernel.To our knowledge, this is the first attempt in thisresearch direction.
In detail, we propose a gram-mar-driven convolution tree kernel for semanticrole classification that can carry out more linguisti-cally motivated substructure matching.
Experimentalresults show that the proposed method significantlyoutperforms the standard convolution tree kernel onthe data set of the CoNLL-2005 SRL shared task.The remainder of the paper is organized as fol-lows: Section 2 reviews the previous work and Sec-tion 3 discusses our grammar-driven convolutiontree kernel.
Section 4 shows the experimental re-sults.
We conclude our work in Section 5.2 Previous WorkFeature-based Methods for SRL: most featuresused in prior SRL research are generally extendedfrom Gildea and Jurafsky (2002), who used a linearinterpolation method and extracted basic flat fea-tures from a parse tree to identify and classify theconstituents in the FrameNet (Baker et al, 1998).Here, the basic features include Phrase Type, ParseTree Path, and Position.
Most of the following workfocused on feature engineering (Xue and Palmer,2004; Jiang et al, 2005) and machine learningmodels (Nielsen and Pradhan, 2004; Pradhan et al,2005a).
Some other work paid much attention to therobust SRL (Pradhan et al, 2005b) and post infer-ence (Punyakanok et al, 2004).
These feature-based methods are considered as the state of the artmethods for SRL.
However, as we know, the stan-dard flat features are less effective in modeling the1 Please refer to http://www.cis.upenn.edu/~treebank/ for thedetailed definitions of the grammar tags used in the paper.2 Some rewrite rules in English grammar are generalizations ofothers: for example, ?NP?
DET JJ NN?
is a specialized ver-sion of ?NP?
DET NN?.
The same applies to POS.
The stan-dard convolution tree kernel is unable to capture the two cases.syntactic structured information.
For example, inSRL, the Parse Tree Path feature is sensitive tosmall changes of the syntactic structures.
Thus, apredicate argument pair will have two differentPath features even if their paths differ only for onenode.
This may result in data sparseness and modelgeneralization problems.Kernel-based Methods for SRL: as an alternative,kernel methods are more effective in modelingstructured objects.
This is because a kernel canmeasure the similarity between two structured ob-jects using the original representation of the objectsinstead of explicitly enumerating their features.Many kernels have been proposed and applied tothe NLP study.
In particular, Haussler (1999) pro-posed the well-known convolution kernels for adiscrete structure.
In the context of it, more andmore kernels for restricted syntaxes or specific do-mains (Collins and Duffy, 2001; Lodhi et al, 2002;Zelenko et al, 2003; Zhang et al, 2006) are pro-posed and explored in the NLP domain.Of special interest here, Moschitti (2004) proposedPredicate Argument Feature (PAF) kernel for SRLunder the framework of convolution tree kernel.
Heselected portions of syntactic parse trees as predicate-argument feature spaces, which include salient sub-structures of predicate-arguments, to define convo-lution kernels for the task of semantic role classifi-cation.
Under the same framework, Che et al (2006)proposed a hybrid convolution tree kernel, whichconsists of two individual convolution kernels: a Pathkernel and a Constituent Structure kernel.
Che et al(2006) showed that their method outperformed PAFon the CoNLL-2005 SRL dataset.The above two kernels are special instances ofconvolution tree kernel for SRL.
As discussed inSection 1, convolution tree kernel only carries outhard matching, so it fails to handle similar phrasestructures and near-synonymic grammar tags.
Thispaper presents a grammar-driven convolution treekernel to solve the two problems3 Grammar-driven Convolution TreeKernel3.1 Convolution Tree KernelIn convolution tree kernel (Collins and Duffy,2001), a parse tree T  is represented by a vector ofinteger counts of each sub-tree type (regardless ofits ancestors): ( )T?
= ( ?, # subtreei(T), ?
), where201# subtreei(T) is the occurrence number of the ithsub-tree type (subtreei) in T. Since the number ofdifferent sub-trees is exponential with the parse treesize, it is computationally infeasible to directly usethe feature vector ( )T?
.
To solve this computa-tional issue, Collins and Duffy (2001) proposed thefollowing parse tree kernel to calculate the dotproduct between the above high dimensional vec-tors implicitly.1 1 2 21 1 2 21 2 1 21 21 2( , ) ( ), ( )( ) ( )( , )(( ) ( ))i isubtree subtreei n N n Nn N n NK T T T TI n I nn n?
??
??
?=< >== ???
?
??
?where N1 and N2 are the sets of nodes in trees T1 andT2, respectively, and ( )isubtreeI n  is a function that is1 iff the subtreei occurs with root at node n and zerootherwise, and 1 2( , )n n?
is the number of the com-mon subtrees rooted at n1 and n2, i.e.,1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n?
= ?
?1 2( , )n n?
can be further computed efficiently by thefollowing recursive rules:Rule 1: if the productions (CFG rules) at 1n  and2n  are different, 1 2( , ) 0n n?
= ;Rule 2: else if both 1n  and 2n  are pre-terminals(POS tags), 1 2( , ) 1n n ??
= ?
;Rule 3: else,1( )1 2 1 21( , ) (1 ( ( , ), ( , )))nc njn n ch n j ch n j?=?
= + ??
,where 1( )nc n is the child number of 1n , ch(n,j) isthe jth child of node n  and ?
(0< ?
<1) is the decayfactor in order to make the kernel value less vari-able with respect to the subtree sizes.
In addition,the recursive Rule 3 holds because given twonodes with the same children, one can constructcommon sub-trees using these children and com-mon sub-trees of further offspring.
The time com-plexity for computing this kernel is 1 2(| | | |)O N N?
.3.2 Grammar-driven Convolution TreeKernelThis Subsection introduces the two improvementsand defines our grammar-driven tree kernel.Improvement 1: Grammar-driven approximatematching between substructures.
The conven-tional tree kernel requires exact matching betweentwo contiguous phrase structures.
This constraintmay be too strict.
For example, the two phrasestructures ?NP?DT JJ NN?
(NP?a red car) and?NP?DT NN?
(NP->a car) are not identical, thusthey contribute nothing to the conventional kernelalthough they should share the same semantic rolegiven a predicate.
In this paper, we propose agrammar-driven approximate matching mechanismto capture the similarity between such kinds ofquasi-structures for SRL.First, we construct reduced rule set by definingoptional nodes, for example, ?NP->DT [JJ] NP?
or?VP-> VB [ADVP]  PP?, where [*] denotes op-tional nodes.
For convenience, we call ?NP-> DTJJ NP?
the original rule and ?NP->DT [JJ] NP?
thereduced rule.
Here, we define two grammar-drivencriteria to select optional nodes:1) The reduced rules must be grammatical.
Itmeans that the reduced rule should be a valid rulein the original rule set.
For example, ?NP->DT [JJ]NP?
is valid only when ?NP->DT NP?
is a validrule in the original rule set while ?NP->DT [JJNP]?
may not be valid since ?NP->DT?
is not avalid rule in the original rule set.2) A valid reduced rule must keep the headchild of its corresponding original rule and has atleast two children.
This can make the reduced rulesretain the underlying semantic meaning of theircorresponding original rules.Given the reduced rule set, we can then formu-late the approximate substructure matching mecha-nism as follows:11 2 1 2,( , ) ( ( , ) )a bi ji jT r ri jM r r I T T ?+= ??
(1)where 1r is a production rule, representing a sub-treeof depth one3, and 1irT is the ith variation of the sub-tree 1r by removing one ore more optional nodes4,and likewise for 2r and 2jrT .
( , )TI ?
?
is a functionthat is 1 iff the two sub-trees are identical and zerootherwise.
1?
(0?
1?
?1) is a small penalty to penal-3 Eq.
(1) is defined over sub-structure of depth one.
The ap-proximate matching between structures of depth more than onecan be achieved easily through the matching of sub-structuresof depth one in the recursively-defined convolution kernel.
Wewill discuss this issue when defining our kernel.4 To make sure that the new kernel is a proper kernel, we haveto consider all the possible variations of the original sub-trees.Training program converges only when using a proper kernel.202ize optional nodes and the two parameters ia  andjb stand for the numbers of occurrence of removedoptional nodes in subtrees 1irT and 2jrT , respectively.1 2( , )M r r returns the similarity (ie., the kernelvalue) between the two sub-trees 1r and 2r  by sum-ming up the similarities between all possible varia-tions of the sub-trees 1r and 2r .Under the new approximate matching mecha-nism, two structures are matchable (but with a smallpenalty 1? )
if the two structures are identical afterremoving one or more optional nodes.
In this case,the above example phrase structures ?NP->a redcar?
and ?NP->a car?
are matchable with a pen-alty 1?
in our new kernel.
It means that one co-occurrence of the two structures contributes 1?
toour proposed kernel while it contributes zero to thetraditional one.
Therefore, by this improvement, ourmethod would be able to explore more linguisticallyappropriate features than the previous one (which isformulated as 1 2( , )TI r r ).Improvement 2: Grammar-driven tree nodes ap-proximate matching.
The conventional tree kernelneeds an exact matching between two (termi-nal/non-terminal) nodes.
But, some similar POSsmay represent similar roles, such as NN (dog) andNNS (dogs).
In order to capture this phenomenon,we allow approximate matching between node fea-tures.
The following illustrates some equivalentnode feature sets:?
JJ, JJR, JJS?
VB, VBD, VBG, VBN, VBP, VBZ?
?
?where POSs in the same line can match each otherwith a small penalty 0?
2?
?1.
We call this casenode feature mutation.
This improvement furthergeneralizes the conventional tree kernel to get bet-ter coverage.
The approximate node matching canbe formulated as:21 2 1 2,( , ) ( ( , ) )a bi ji jfi jM f f I f f ?+= ??
(2)where 1f is a node feature, 1if is the ith mutationof 1f and ia is 0 iff 1if and 1f are identical and 1 oth-erwise, and likewise for 2f .
( , )fI ?
?
is a functionthat is 1 iff the two features are identical and zerootherwise.
Eq.
(2) sums over all combinations offeature mutations as the node feature similarity.The same as Eq.
(1), the reason for taking all thepossibilities into account in Eq.
(2) is to make surethat the new kernel is a proper kernel.The above two improvements are grammar-driven, i.e., the two improvements retain the under-lying linguistic grammar constraints and keep se-mantic meanings of original rules.The Grammar-driven Kernel Definition: Giventhe two improvements discussed above, we can de-fine the new kernel by beginning with the featurevector representation of a parse tree T as follows:( )T?
=?
(# subtree1(T), ?, # subtreen(T))where # subtreei(T) is the occurrence number of theith sub-tree type (subtreei) in T. Please note that,different from the previous tree kernel, here weloosen the condition for the occurrence of a subtreeby allowing both original and reduced rules (Im-provement 1) and node feature mutations (Im-provement 2).
In other words, we modify the crite-ria by which a subtree is said to occur.
For example,one occurrence of the rule ?NP->DT JJ NP?
shallcontribute 1 times to the feature ?NP->DT JJ NP?and 1?
times to the feature ?NP->DT NP?
in thenew kernel while it only contributes 1 times to thefeature ?NP->DT JJ NP?
in the previous one.
Nowwe can define the new grammar-driven kernel1 2( , )GK T T as follows:1 1 2 21 1 2 21 2 1 21 21 2( , ) ( ), ( )( ) ( )( , )(( ) ( ))i iGsubtree subtreei n N n Nn N n NK T T T TI n I nn n?
??
??
??
?=< >?
?=?= ???
?
??
?
(3)where N1 and N2 are the sets of nodes in trees T1 andT2, respectively.
( )isubtreeI n?
is a function that is1 2a b?
??
iff the subtreei occurs with root at node nand zero otherwise, where a and b are the numbersof removed optional nodes and mutated node fea-tures, respectively.
1 2( , )n n??
is the number of thecommon subtrees rooted at n1 and n2, i.e.
,1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n?
?
??
= ??
(4)Please note that the value of 1 2( , )n n??
is no longeran integer as that in the conventional one since op-tional nodes and node feature mutations are consid-ered in the new kernel.
1 2( , )n n??
can be furthercomputed by the following recursive rules:203============================================================================Rule A: if 1n and 2n are pre-terminals, then:1 2 1 2( , ) ( , )n n M f f???
= ?
(5)where 1f and 2f are features of nodes 1n and 2n re-spectively, and 1 2( , )M f f  is defined at Eq.
(2).Rule B: else if both 1n and 2n are the same non-terminals, then generate all variations of the subtreesof depth one rooted by 1n and 2n (denoted by 1nTand 2nT  respectively) by removing different optionalnodes, then:111 2 1 2,( , )1 21( , ) ( ( , )(1 ( ( , , ), ( , , )))a bi ji jT n ni jnc n ikn n I T Tch n i k ch n j k?
?+=??
= ?
???
+ ???(6)where?
1inT and 2jnT stand for the ith and jth variations insub-tree set 1nT and 2nT , respectively.?
( , )TI ?
?
is a function that is 1 iff the two sub-trees are identical and zero otherwise.?
ia and jb stand for the number of removed op-tional nodes in subtrees 1inT and 2jnT , respectively.?
1( , )nc n i returns the child number of 1n in its ithsubtree variation 1inT .?
1( , , )ch n i k  is the kth child of node 1n  in its ithvariation subtree 1inT , and likewise for 2( , , )ch n j k .?
Finally, the same as the previous tree kernel,?
(0< ?
<1) is the decay factor (see the discussionin Subsection 3.1).Rule C: else 1 2( , ) 0n n??
=============================================================================Rule A accounts for Improvement 2 while RuleB accounts for Improvement 1.
In Rule B, Eq.
(6)is able to carry out multi-layer sub-tree approxi-mate matching due to the introduction of the recur-sive part while Eq.
(1) is only effective for sub-trees of depth one.
Moreover, we note that Eq.
(4)is a convolution kernel according to the definitionand the proof given in Haussler (1999), and Eqs (5)and (6) reformulate Eq.
(4) so that it can be com-puted efficiently, in this way, our kernel defined byEq (3) is also a valid convolution kernel.
Finally,let us study the computational issue of the newconvolution tree kernel.
Clearly, computing Eq.
(6)requires exponential time in its worst case.
How-ever, in practice, it may only need  1 2(| | | |)O N N?
.This is because there are only 9.9% rules (647 outof the total 6,534 rules in the parse trees) have op-tional nodes and most of them have only one op-tional node.
In fact, the actual running time is evenmuch less and is close to linear in the size of thetrees since 1 2( , ) 0n n??
=  holds for many nodepairs (Collins and Duffy, 2001).
In theory, we canalso design an efficient algorithm to compute Eq.
(6) using a dynamic programming algorithm (Mo-schitti, 2006).
We just leave it for our future work.3.3 Comparison with previous workIn above discussion, we show that the conventionalconvolution tree kernel is a special case of thegrammar-driven tree kernel.
From kernel functionviewpoint, our kernel can carry out not only exactmatching (as previous one described by Rules 2and 3 in Subsection 3.1) but also approximatematching (Eqs.
(5) and (6) in Subsection 3.2).
Fromfeature exploration viewpoint, although they ex-plore the same sub-structure feature space (definedrecursively by the phrase parse rules), their featurevalues are different since our kernel captures thestructure features in a more linguistically appropri-ate way by considering more linguistic knowledgein our kernel design.Moschitti (2006) proposes a partial tree (PT)kernel which can carry out partial matching be-tween sub-trees.
The PT kernel generates a muchlarger feature space than both the conventional andthe grammar-driven kernels.
In this point, one cansay that the grammar-driven tree kernel is a spe-cialization of the PT kernel.
However, the impor-tant difference between them is that the PT kernelis not grammar-driven, thus many non-linguistically motivated structures are matched inthe PT kernel.
This may potentially compromisethe performance since some of the over-generatedfeatures may possibly be noisy due to the lack oflinguistic interpretation and constraint.Kashima and Koyanagi (2003) proposed a con-volution kernel over labeled order trees by general-izing the standard convolution tree kernel.
The la-beled order tree kernel is much more flexible thanthe PT kernel and can explore much larger sub-treefeatures than the PT kernel.
However, the same asthe PT kernel, the labeled order tree kernel is notgrammar-driven.
Thus, it may face the same issues204(such as over-generated features) as the PT kernelwhen used in NLP applications.Shen el al.
(2003) proposed a lexicalized treekernel to utilize LTAG-based features in parsereranking.
Their methods need to obtain a LTAGderivation tree for each parse tree before kernelcalculation.
In contrast, we use the notion of op-tional arguments to define our grammar-driven treekernel and use the empirical set of CFG rules to de-termine which arguments are optional.4 Experiments4.1 Experimental SettingData: We use the CoNLL-2005 SRL shared taskdata (Carreras and M?rquez, 2005) as our experi-mental corpus.
The data consists of sections of theWall Street Journal part of the Penn TreeBank(Marcus et al, 1993), with information on predi-cate-argument structures extracted from the Prop-Bank corpus (Palmer et al, 2005).
As defined bythe shared task, we use sections 02-21 for training,section 24 for development and section 23 for test.There are 35 roles in the data including 7 Core(A0?A5, AA), 14 Adjunct (AM-) and 14 Reference(R-) arguments.
Table 1 lists counts of sentencesand arguments in the three data sets.Training Development TestSentences 39,832 1,346 2,416Arguments 239,858 8,346 14,077Table 1: Counts on the data setWe assume that the semantic role identificationhas been done correctly.
In this way, we can focuson the classification task and evaluate it more accu-rately.
We evaluate the performance with Accu-racy.
SVM (Vapnik, 1998) is selected as our classi-fier and the one vs. others strategy is adopted andthe one with the largest margin is selected as thefinal answer.
In our implementation, we use the bi-nary SVMLight (Joachims, 1998) and modify theTree Kernel Tools (Moschitti, 2004) to a grammar-driven one.Kernel Setup: We use the Constituent, Predicate,and Predicate-Constituent related features, whichare reported to get the best-reported performance(Pradhan et al, 2005a), as the baseline features.
Weuse Che et al (2006)?s hybrid convolution tree ker-nel (the best-reported method for kernel-basedSRL) as our baseline kernel.
It is defined as(1 )  (0 1)hybrid path csK K K?
?
?= + ?
?
?
(for the de-tailed definitions of pathK and csK , please refer toChe et al (2006)).
Here, we use our grammar-driven tree kernel to compute pathK and csK , and wecall it grammar-driven hybrid tree kernel while Cheet al (2006)?s is non-grammar-driven hybrid convo-lution tree kernel.We use a greedy strategy to fine-tune parameters.Evaluation on the development set shows that ourkernel yields the best performance when ?
(decayfactor of tree kernel), 1?
and 2?
(two penalty factorsfor the grammar-driven kernel), ?
(hybrid kernelparameter) and c (a SVM training parameter tobalance training error and margin) are set to 0.4,0.6, 0.3, 0.6 and 2.4, respectively.
For other parame-ters, we use default setting.
In the CoNLL 2005benchmark data, we get 647 rules with optionalnodes out of the total 6,534 grammar rules and de-fine three equivalent node feature sets as below:?
JJ, JJR, JJS?
RB, RBR, RBS?
NN, NNS, NNP, NNPS, NAC, NXHere, the verb feature set ?VB, VBD, VBG, VBN,VBP, VBZ?
is removed since the voice informationis very indicative to the arguments of ARG0(Agent, operator) and ARG1 (Thing operated).Methods Accuracy (%)Baseline: Non-grammar-driven 85.21+Approximate Node Matching 86.27+Approximate SubstructureMatching87.12Ours: Grammar-driven Substruc-ture and Node Matching87.96Feature-based method with poly-nomial kernel (d = 2)89.92Table 2: Performance comparison4.2 Experimental ResultsTable 2 compares the performances of differentmethods on the test set.
First, we can see that thenew grammar-driven hybrid convolution tree kernelsignificantly outperforms ( 2?
test with p=0.05) the205non-grammar one with an absolute improvement of2.75 (87.96-85.21) percentage, representing a rela-tive error rate reduction of 18.6% (2.75/(100-85.21)).
It suggests that 1) the linguistically motivatedstructure features are very useful for semantic roleclassification and 2) the grammar-driven kernel ismuch more effective in capturing such kinds of fea-tures due to the consideration of linguistic knowl-edge.
Moreover, Table 2 shows that 1) both thegrammar-driven approximate node matching and thegrammar-driven approximate substructure matchingare very useful in modeling syntactic tree structuresfor SRL since they contribute relative error rate re-duction of 7.2% ((86.27-85.21)/(100-85.21)) and12.9% ((87.12-85.21)/(100-85.21)), respectively; 2)the grammar-driven approximate substructurematching is more effective than the grammar-drivenapproximate node matching.
However, we find thatthe performance of the grammar-driven kernel isstill a bit lower than the feature-based method.
Thisis not surprising since tree kernel methods only fo-cus on modeling tree structure information.
In thispaper, it captures the syntactic parse tree structurefeatures only while the features used in the feature-based methods cover more knowledge sources.In order to make full use of the syntactic structureinformation and the other useful diverse flat fea-tures, we present a composite kernel to combine thegrammar-driven hybrid kernel and feature-basedmethod with polynomial kernel:(1 )      (0 1)comp hybrid polyK K K?
?
?= + ?
?
?Evaluation on the development set shows that thecomposite kernel yields the best performance when?
is set to 0.3.
Using the same setting, the systemachieves the performance of 91.02% in Accuracyin the same test set.
It shows statistically significantimprovement (?2 test with p= 0.10) over using thestandard features with the polynomial kernel (?
= 0,Accuracy = 89.92%) and using the grammar-drivenhybrid convolution tree kernel (?
= 1, Accuracy =87.96%).
The main reason is that the tree kernelcan capture effectively more structure featureswhile the standard flat features can cover someother useful features, such as Voice, SubCat, whichare hard to be covered by the tree kernel.
The ex-perimental results suggest that these two kinds ofmethods are complementary to each other.In order to further compare with other methods,we also do experiments on the dataset of EnglishPropBank I (LDC2004T14).
The training, develop-ment and test sets follow the conventional split ofSections 02-21, 00 and 23.
Table 3 compares ourmethod with other previously best-reported methodswith the same setting as discussed previously.
Itshows that our method outperforms the previousbest-reported one with a relative error rate reductionof 10.8% (0.97/(100-91)).
This further verifies theeffectiveness of the grammar-driven kernel methodfor semantic role classification.Method Accuracy (%)Ours (Composite Kernel)      91.97Moschitti (2006): PAF kernel only    87.7Jiang et al (2005): feature based    90.50Pradhan et al (2005a): feature based    91.0Table 3: Performance comparison between ourmethod and previous workTraining Time Method4 Sections  19 SectionsOurs: grammar-driven tree kernel~8.1 hours ~7.9 daysMoschitti (2006):non-grammar-driventree kernel~7.9 hours ~7.1 daysTable 4: Training time comparisonTable 4 reports the training times of the two ker-nels.
We can see that 1) the two kinds of convolu-tion tree kernels have similar computing time.
Al-though computing the grammar-driven one requiresexponential time in its worst case, however, inpractice, it may only need 1 2(| | | |)O N N?
or lin-ear and 2) it is very time-consuming to train a SVMclassifier in a large dataset.5 Conclusion and Future WorkIn this paper, we propose a novel grammar-drivenconvolution tree kernel for semantic role classifica-tion.
More linguistic knowledge is considered inthe new kernel design.
The experimental resultsverify that the grammar-driven kernel is more ef-fective in capturing syntactic structure features thanthe previous convolution tree kernel because it al-lows grammar-driven approximate matching ofsubstructures and node features.
We also discussthe criteria to determine the optional nodes in a206CFG rule in defining our grammar-driven convolu-tion tree kernel.The extension of our work is to improve the per-formance of the entire semantic role labeling systemusing the grammar-driven tree kernel, including allfour stages: pruning, semantic role identification,classification and post inference.
In addition, amore interesting research topic is to study how tointegrate linguistic knowledge and tree kernelmethods to do feature selection for tree kernel-based NLP applications (Suzuki et al, 2004).
Indetail, a linguistics and statistics-based theory thatcan suggest the effectiveness of different substruc-ture features and whether they should be generatedor not by the tree kernels would be worked out.ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley FrameNet Project.
COLING-ACL-1998Xavier Carreras and Llu?s M?rquez.
2004.
Introduction tothe CoNLL-2004 shared task: Semantic role labeling.CoNLL-2004Xavier Carreras and Llu?s M?rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.CoNLL-2005Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings ofNAACL-2000Wanxiang Che, Min Zhang, Ting Liu and Sheng Li.2006.
A hybrid convolution tree kernel for semanticrole labeling.
COLING-ACL-2006(poster)Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
NIPS-2001Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288David Haussler.
1999.
Convolution kernels on discretestructures.
Technical Report UCSC-CRL-99-10Zheng Ping Jiang, Jia Li and Hwee Tou Ng.
2005.
Se-mantic argument classification exploiting argumentinterdependence.
IJCAI-2005T.
Joachims.
1998.
Text Categorization with SupportVecor Machine: learning with many relevant fea-tures.
ECML-1998Kashima H. and Koyanagi T. 2003.
Kernels for Semi-Structured Data.
ICML-2003Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini and Chris Watkins.
2002.
Text classifica-tion using string kernels.
Journal of Machine Learn-ing Research, 2:419?444Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-trice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330Alessandro Moschitti.
2004.
A study on convolution ker-nels for shallow statistic parsing.
ACL-2004Alessandro Moschitti.
2006.
Syntactic kernels for natu-ral language learning: the semantic role labelingcase.
HLT-NAACL-2006 (short paper)Rodney D. Nielsen and Sameer Pradhan.
2004.
Mixingweak learners in semantic parsing.
EMNLP-2004Martha Palmer, Dan Gildea and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of seman-tic roles.
Computational Linguistics, 31(1)Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, WayneWard, James H. Martin and Daniel Jurafsky.
2005a.Support vector learning for semantic argument classi-fication.
Journal of Machine LearningSameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin and Daniel Jurafsky.
2005b.
Semantic role la-beling using different syntactic views.
ACL-2005Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-mak.
2004.
Semantic role labeling via integer linearprogramming inference.
COLING-2004Vasin Punyakanok, Dan Roth and Wen Tau Yih.
2005.The necessity of syntactic parsing for semantic rolelabeling.
IJCAI-2005Libin Shen, Anoop Sarkar and A. K. Joshi.
2003.
UsingLTAG based features in parse reranking.
EMNLP-03Jun Suzuki, Hideki Isozaki and Eisaku Maede.
2004.Convolution kernels with feature selection for Natu-ral Language processing tasks.
ACL-2004Vladimir N. Vapnik.
1998.
Statistical Learning Theory.WileyNianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
EMNLP-2004Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-ardella.
2003.
Kernel methods for relation extraction.Machine Learning Research, 3:1083?1106Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with both Flat and Structured Fea-tures.
COLING-ACL-2006207
