Labeling Chinese Predicates withSemantic RolesNianwen Xue?University of Colorado at BoulderIn this article we report work on Chinese semantic role labeling, taking advantage of two recentlycompleted corpora, the Chinese PropBank, a semantically annotated corpus of Chinese verbs, andthe Chinese Nombank, a companion corpus that annotates the predicate?argument structure ofnominalized predicates.
Because the semantic role labels are assigned to the constituents in aparse tree, we first report experiments in which semantic role labels are automatically assignedto hand-crafted parses in the Chinese Treebank.
This gives us a measure of the extent to which se-mantic role labels can be bootstrapped from the syntactic annotation provided in the treebank.
Wethen report experiments using automatic parses with decreasing levels of human annotation inthe input to the syntactic parser: parses that use gold-standard segmentation and POS-tagging,parses that use only gold-standard segmentation, and fully automatic parses.
These experimentsgauge how successful semantic role labeling for Chinese can be in more realistic situations.
Ourresults show that when hand-crafted parses are used, semantic role labeling accuracy for Chineseis comparable to what has been reported for the state-of-the-art English semantic role labelingsystems trained and tested on the English PropBank, even though the Chinese PropBank issignificantly smaller in size.
When an automatic parser is used, however, the accuracy of oursystem is significantly lower than the English state of the art.
This indicates that an improvementin Chinese parsing is critical to high-performance semantic role labeling for Chinese.1.
IntroductionSemantic role labeling (SRL) is the task of identifying arguments for a predicate andassigning semantically meaningful labels to them.
A semantic role represents a semanticrelation between a predicate and one of its arguments.
Typical semantic roles includeagent, patient, source, goal, and so forth, that are core to a predicate, as well as location,time, manner, cause, and so on, that are peripheral.
Such semantic information is im-portant in answering who, what, when, where, and why questions therefore is crucial tonatural language processing (NLP) tasks such as question-answering (Narayanan andHarabagiu 2004), information extraction (Surdeanu et al 2005), summarization (Melliet al 2005), and machine translation (Boas 2002).
Any NLP task that requires some formof semantic interpretation could potentially benefit from a high performance semanticrole labeling system.For an automatic system, a semantic role labeling task involves locating the linguis-tic units, typically words or phrases, in natural language text that serve as arguments?
1777 Exposition Drive, Boulder, CO 80309.
E-mail: Nianwen.Xue@colorado.edu.Submission received: 15 July 2006; revised submission received: 4 May 2007; accepted for publication:19 June 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 2to a predicate and assigning semantic role labels to them based on the context in whichthey occur.
Since the seminal work of Gildea and Jurafsky (2002), statistical andmachinelearning approaches have been the predominant research paradigm in semantic rolelabeling, like most of the subfields in natural language processing and computationallinguistics.
A prerequisite for statistical and machine learning approaches to semanticrole labeling is the availability of a significant amount of semantically interpretedcorpora from which automatic systems can learn.
The recent activities in semantic rolelabeling (Carreras and Ma`rquez 2004b, 2005; Litkowski 2004) have in large part beendriven by the availability of semantically annotated corpora such as the FrameNet(Baker, Fillmore, and Lowe 1998), Proposition Bank (Palmer, Gildea, and Kingsbury2005), andNombank (Meyers et al 2004) projects for English; the tectogrammatical layerannotation of the Prague Dependency Treebank (Sgall, Panevova?, and Hajic?ova?
2004)for Czech; and the Salsa Project for German (Burchardt et al 2006).
These semanticallyannotated corpora not only provide the training and test material for the developmentof machine learning systems, but also effectively define semantic role labeling as a task.PropBank and FrameNet have been the two most widely used corpora in de-veloping automatic semantic role labeling systems.
Although both corpora providepredicate?argument structure annotation, they use very different semantic role labels,especially for the core arguments of each predicate.
In FrameNet, the semantic rolesof a predicate (called a Lexical Unit (LU)) are organized by semantic frames, whichare conceptual structures that describe a particular situation or event along with theirparticipants, which are called frame elements (FEs).
All LUs in the same semantic frameshare one set of semantic roles.
For example, the verbs buy and sell both belong to thesemantic frame Commercial_transaction, which involves a Buyer and Seller exchangingMoney and Goods.
In addition to these four core FEs, there are also three Non-Core FEs:Means, the manner in which the transaction takes place; Rate, the price of payment perunit of Goods; and Unit, the unit of measure for the Goods.
Semantic role labeling basedon FrameNet annotation attempts to identify the syntactic constituents in a sentenceand assign FEs to them (1a).
Notice that for any given sentence, not all FEs have to berealized and they do not have to be realized in the same syntactic position.
(1) a. FrameNet[Buyer We] always [LU bought] [Goods a few dark-red carnations] [Sellerfrom her]During the later part of the nineteenth century, [Seller the landowners] [LUsold] [Goods the land] [Buyer to developers] in very small lots.b.
PropBank[Arg0 We] always [Rel bought] [Arg1 a few dark-red carnations] [Arg2 fromher]During the later part of the nineteenth century, [Arg0 the landowners] [Relsold] [Arg1 the land] [Arg2 to developers] in very small lots.Like FrameNet, PropBank also assigns semantic role labels to syntactic constituents(rather than to the heads in a dependency structure) in a sentence.
Unlike FrameNet,there is no reference ontology like the semantic frame that provides a general set ofsemantic roles.
Instead, for the core arguments, the PropBank uses a set of predicate-specific semantic role labels represented by an integer prefixed by Arg: Arg0 throughArg5.
Predicates vary on the number of core arguments they take, but generally thetotal number of core arguments does not exceed six.
These core arguments are definedin frame files, with one frame file for each predicate.
Within a frame file, the core226Xue Semantic Role Labeling of Chinese Predicatesarguments are organized by framesets, which are themajor senses of a predicate.
A newframeset is postulated only when it takes a different set of core arguments from existingframesets.
In addition to the core arguments, there is also a finite set of roles reserved foradjunct-like arguments.
Each adjunct-like argument is represented as ArgM, indicatingthat it is a modifier argument, followed by a secondary tag indicating the type ofmodifier.
Secondary tags are for semantic information such as location, manner, andtime that are not specific to a particular verb or even a particular class of verbs andthey are defined based on a general set of guidelines.
There is thus a dichotomy inthe representation of the semantic roles for the core and peripheral arguments in thePropBank annotation.The predicate-specific nature of the PropBank semantic roles is clear when com-pared with the FrameNet FE.
In (1b), for example, the seller is always labeled Seller andthe buyer is always labeled Buyer in the FrameNet annotation whether the predicateis buy or sell.
In contrast, in the PropBank annotation, the buyer is Arg0 when thepredicate is buy and Arg2 when the predicate is sell.
Conversely, the seller is Arg0 whenthe predicate is sell and Arg2 when the predicate is buy.
While FrameNet annotatesthe semantic roles of both verbal and nominal predicates, the annotation of PropBankis limited to verbs, with the nominal predicates annotated in a separate but relatedproject, the Nombank Project (Meyers et al 2004).
The NomBank Project adopted thesame predicate-specific approach in representing the core arguments of a predicate asPropBank, with special treatment for noun-specific phenomena such as support verbs.There is considerable work on English semantic role labeling with both anno-tation conventions.
Gildea and Jurafsky (2002) did their seminal work using datafrom FrameNet.
The Senseval-3 international competition on semantic role labeling(Litkowski 2004) also used the FrameNet annotation.
There is an even larger body ofwork using PropBank because it has a larger amount of annotated data on a well-established data set, the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).Using the standard training and test sets in the Penn Treebank, there has been arapid improvement in performance due to the use of more advanced machine-learningtechniques and more informative linguistic features.
The performance using automaticparses on Section 23 of the Penn Treebank has approached 0.81 F-score (Pradhan, Wardet al 2005).
There have also been two consecutive CoNLL competitions (Carreras andMa`rquez 2004b, 2005) on semantic role labeling using the PropBank data.Research on Chinese semantic role labeling is still in its infancy.
Work on Chinesesemantic role labeling has been scant and sporadic, mostly due to the lack of a pub-licly available semantically annotated corpus of significant size.
Although most of themachine-learning techniques used in English semantic role labeling are readily trans-ferable to Chinese, such technological transfer is only possible with similarly annotateddata.
To our knowledge, there are only two such data sets, which all used a smallcorpus that the authors created on their own.
Sun and Jurafsky (2004) did preliminarywork on Chinese semantic role labeling on 10 selected verbs using Support VectorMachines and reported promising early results.1 Noting that Chinese syntactic parsingis an especially challenging task, Kwong and T?sou (2005) reformulated semantic rolelabeling as a task of detecting and classifying the heads of arguments to avoid thehard problem of getting the correct text spans for the arguments.
In this article, we1 They restated their results in Chen, Sun, and Jurafsky (2005) due to an error in retraining the Collinsparser (Collins 1999) on Chinese, which led to inflated Chinese syntactic parsing and thereforesemantic role labeling results.227Computational Linguistics Volume 34, Number 2report work on the semantic role labeling of Chinese predicates for both verbs and theirnominalizations, exploiting two recently completed corpora, the Chinese PropBank(Xue and Palmer 2003), a corpus that annotates the predicate?argument structure ofverbs, and the Chinese NomBank (Xue 2006a), a companion corpus that annotates thepredicate-argument structure of nominalized predicates in Chinese.
Both corpora arebuilt on top of the Chinese Treebank (Xue et al 2005), in the sense that the semantic rolelabels are assigned to constituents in the parse tree.The Chinese PropBank and Nombank adopted the English PropBank predicate-specific approach in representing the semantic roles of the core arguments.
In theabsence of a Chinese linguistic ontology like the semantic frames developed for theEnglish FrameNet Project, using the PropBank-style of semantic roles allows faster de-velopment.
The predicate-specific approach of the PropBank annotation builds a solidfoundation for making high-level generalizations in a bottom-up manner, if broadergeneralizations are needed.
The Chinese PropBank focuses on the context-sensitivecomponent of the semantic role annotation, using frame files to guide its annotation.The semantic roles defined in the frame files are for expected arguments, that is, allpossible arguments for each frameset of a predicate.
In a particular sentence, an ex-pected argument may not always be realized, and when it is, it may not always berealized in the same syntactic position as a result of syntactic alternations (Levin 1993)or other syntactic processes.
In addition, different framesets of a verb take different setsof arguments that demonstrate different syntactic patterns.
Thus, predicate?argumentstructure analysis at the PropBank annotation level represents a crucial leap towardsproper representation of semantic structure from the syntactic structure.
Should theneed for more general semantic roles arise, these predicate-specific semantic roles can bemapped (Yi, Loper, and Palmer 2007) to FrameNet-style or even VerbNet-style (Kipperet al 2006) labels.Using the semantic annotation of the Chinese PropBank and NomBank as trainingand test material, we were able for the first time to develop a Chinese semantic rolelabeling system that is trained and tested on semantically annotated Chinese corporaof significant sizes.
Using parses produced with different levels of automation (a fullyautomatic parser, a parser with correct segmentation, a parser with both correct seg-mentation and POS-tagging, and treebank gold-standard parses), we were able to quan-tify the impact of different Chinese language processing components on the semanticanalysis of Chinese predicates.
Using a Maximum Entropy?based (McCallum 2002)machine learning system, our experimental results show that just by using the featuresreported in the English semantic role labeling literature, our baseline system achieveda very high accuracy on Chinese verbs when the gold-standard treebank parses areused.
This suggests that these features port very well between English and Chinese.There is a gradual degradation in semantic role labeling performance with a decreasinglevel of human annotation (from gold-standard treebank parses to fully automaticparses).
We were able to achieve a modest improvement with additional features tai-lored to the Chinese language, bringing the overall accuracy to F1-scores of 0.92 and0.67, respectively, when using treebank and fully automatic parses.
We were able toachieve a larger improvement on the semantic role labeling of nominalized predicatesby using noun-specific features (F-scores of 0.70 and 0.57, respectively, for treebankand fully automatic parses), but our results still show that the semantic role labelingof nominalized predicates is a much more challenging task than that of verbs.
This ispartly due to the smaller training set for nominalized predicates, with the number ofnominalized predicates being less than one third of the number of verb instances inthe same corpus.
More importantly, the arguments of nominalized predicates have a228Xue Semantic Role Labeling of Chinese Predicatesmuchmore uneven distribution: Arguments of a nominalized predicate can occur eitherinside the NP headed by the predicate or outside when a support verb is present (seeSection 2 for examples).
This makes it particularly challenging to determine whether aconstituent in the parse tree is an argument or not.
This observation is supported bythe large margin in performance between the semantic role labeling results we achievedwhen the constituents are known and unknown.This article is structured as follows.
In Section 2, we discuss the semantic annotationof the Chinese PropBank and NomBank in greater detail.
In Section 3, we describe thegeneral architecture of our system, focusing on shared components for both verbs andtheir nominalizations.
In Sections 4 and 5 we present our experiments on verbs andnouns, respectively.
Section 6 discusses related work and Section 7 concludes this articleand discusses future work.2.
The Chinese PropBank and NomBankThe Chinese PropBank and the Chinese NomBank adopt the descriptive framework ofthe English Proposition Bank in which semantic arguments and adjuncts are treated dif-ferently.
The semantic arguments of a predicate are labeled with a contiguous sequenceof integers, in the form of ArgN, whereN is an integer between 0 and 5.
These labels canonly be interpreted in the context of a specific predicate.
In other words, these argumentlabels are not meaningful without knowing what the predicate is.
In fact, as we willshow later in this section, these numbered labels are meaningful only within a particularsense of a predicate.
In general, like English, a Chinese predicate takes fewer than 6arguments.
The assignment of numbered argument labels is illustrated in Example (2),where the predicate is the verb  (?investigate?).
Its subject  (?the police?)
islabeled Arg0 and its object  (?accident?)
 (?cause?)
Arg1.
The semantic rolelabels added to the parse tree are in bold.
(2) IPArg0 VPNP-SBJ ArgM-TMP ArgM-MNR VPpoliceADVP-TMP ADVP-MNR Rel Arg1nowthoroughlyVV NP-OBJinvestigateNN NNaccidentcause?The police are thoroughly investigating the cause of the accident.
?The semantic adjuncts, on the other hand, are annotated as such with the labelArgM followed by a secondary tag that represents the semantic classification of theadjunct.
Unlike the numbered argument labels for semantic arguments, the secondary229Computational Linguistics Volume 34, Number 2tags represent information that is not predicate-specific.
For instance, the adverbialmodifiers (?right now?)
and (?thorough?)
in Example (2) are labeled ArgM-TMP and ArgM-MNR respectively, where the secondary tag TMP indicates a temporalmodifier and MNR indicates manner.
The secondary tags are not predicate-specific inthe sense that they are not required by this particular predicate and they are not selectivewith regard to the predicate they can occur with.
There is a limited set of such secondarytags that are used in the Chinese PropBank and the Chinese NomBank and the completelist of such secondary tags is presented in Table 1.The same approach is taken to annotate the nominalized predicates in the ChineseNomBank.
This is illustrated in Example (3), a close paraphrase of Example (2), wherethe nominalized predicate (?investigation?)
takes the same arguments as its verbalcounterpart.
 (?the police?)
is again Arg0 and  (?toward?)
 (?accident?
) (?cause?
), despite its categorial change from a noun phrase to a prepositionalphrase, remains Arg1.
There are also two semantic adjuncts: ArgM-TMP (?now?
)and ArgM-MNR (?thorough?).
It is worth noting, however, that in this particularcase, the nominalized predicate needs a support verb  (?conduct?)
to satisfy thegrammatical constraint that there be a verb in the sentence, and it is explicitly markedas Sup.
In addition, despite the categorial change of from adverb to adjective, thesemantic role label remains unchanged.
In this sense, the semantic annotation abstractsaway from the underlying syntactic annotation.
(3) IPArg0 VPNP-SBJArgM-TMP Arg1 VPpoliceADVP-TMP PP Sup NP-OBJnowP NP VV ArgM-MNR ReltowardNN NN conductADJP NNaccidentcausethoroughinvestigation?The police are conducting a thorough investigation of the cause of the accident.
?Table 1The complete list of functional tags defined in the Chinese Propbank and NomBank.ADV adverbial FRQ frequencyBNF beneficiary LOC locativeCND condition MNR mannerDIR direction PRP purpose or reasonDIS discourse marker TMP temporalDGR degree TPC topicEXT extent230Xue Semantic Role Labeling of Chinese PredicatesNot all occurrences of a nominalized predicate need to be accompanied by a supportverb.
In fact, it is often the case that all arguments of a nominalized predicate occurin a noun phrase headed by the nominalized predicate.
For example, in Example (4),both Arg0 (?police?)
and Arg1 (?toward?
) (?accident?
) (?cause?)
aresyntactic modifiers of the nominalized predicate (?investigation?).
(4) IPNP-SBJ VPArg0 DNP NP ADVP VPNP Arg1 DEG Rel alreadyendpolicePP DENNP NP investigationtowardNN NNaccidentcause?The police investigation of the cause of the accident has ended.
?The PropBank-style annotation is designed to account for syntactic variations, thatis, the different ways in which the same predicate?argument structure is realized.
InExamples (2) and (3), we have already seen where essentially the same predicate is real-ized as a verb or a noun, and its arguments are realized as different syntactic categoriesin different syntactic positions.
Syntactic variations occur even without the categorialchange of the predicate.
Levin (1993) demonstrates extensively how the argumentstructure of English verbs can be realized differently through diathesis alternations.Similar alternations can also be observed in Chinese, and Example (5) shows this:(5) a.
[Arg1Chinathe U.S.contactDEdoor] [relopen]ASP.
?The door of contact between China and the U.S. has opened.?b.
[ArgM-TMP70sbeginning],[Arg0Chinathe U.S.twocountryleader] [ArgM-ADVdecisively] [rel open]ASP[Arg1Chinathe U.S.contactDEdoor].
?In the beginning of the 1970s, the leaders of China and the U.S. decisivelyopened the door of contact between China and the U.S.?Note that even though (?China?
) (?the U.S.?
) (?contact?)
 (?DE?
) (?door?)
occurs in different syntactic positions in (5a) and (5b), it is labeled Arg1 inboth cases.
The semantic role label an argument receives is independent of its syntactic231Computational Linguistics Volume 34, Number 2realizations.
The semantic roles or expected arguments can be realized syntactically indifferent ways.
It should also be pointed out that the line drawn between argumentsand adjuncts here is not based on the obligatory/optional dichotomy.
In some cases,some constituents are clearly arguments but they are also clearly optional.
For example,in the unaccusative (or pseudo-passive) construction, the agent is clearly optional syn-tactically and it is equally clear that it is an argument.
In Example (5a), for example, the?door-opener?
is optional but is clearly an argument.The Chinese PropBank also adds a coarse-grained sense tag to the predicate.
Thesenses of a predicate are motivated by the argument structure of this predicate andare thus an integral part of the predicate?argument structure annotation.
Sense disam-biguation is performed only when different senses of a predicate require different sets ofarguments.
For example, the ?evolve?
sense of the verb ??
expects five arguments:The cause of the evolution, which is often not realized, the entity evolving, the startingpoint of the evolution, the end point of the evolution, and the range of the evolution.When it means ?recruit,?
two arguments are expected: the recruiter and the entityrecruited.
Because each of these senses can be realized in different subcategorizationframes related through syntactic alternations, in the PropBank annotation convention,these senses are formally called framesets, meaning sets of subcategorization framesthat realize a particular sense.
The examples in (6) illustrate two of the framesetsof ??.
(6) Frameset 1: ?evolve?Semantic roles:Arg0: cause of evolutionArg1: entity evolvingArg2: evolving fromArg3: evolving toArg4: range of evolutiona.
[Arg1Russiadomesticforindustrial productdemand] [Arg3inmid- and upper scaledirection] [Reldevelop].
?Russia?s domestic demand for industrial products is evolving in the directionof mid- and upper scale products.
?Frameset 2: ?recruit?Arg0: recruiterArg1: entity recruitedb.
[Arg0CEC] [ArgM-TMPpresently] [ArgM-LOCinthe world160CLcountryandregion] [Relrecruited]ASP[Arg1more than eight thousandCLsubscriber].
?CEC presently have recruited over eight thousand subscribers from 160 coun-tries and regions in the world.
?The Chinese NomBank uses the same framesets as defined for verbs because itsannotation is guided by the same frame files.
However, typically only a subset of theframesets for verbs have corresponding nominalized forms.
For example, Frameset 1 in232Xue Semantic Role Labeling of Chinese PredicatesExample (6) has a corresponding nominalized form as illustrated in Example (7), butFrameset 2 does not.
(7) Taiwan Straitstwosidefrom now oncantogetherplan[Arg1cross-Straitrelations]DE[Reldevelopment].
?The two sides of the Taiwan Straits can plan the development of the cross-Straitrelations hereafter.?3.
System OverviewAssuming the availability of a parse tree (either hand-crafted parses in a treebank orparses generated by an automatic parser) as input, to assign the semantic role labelsdescribed in Section 2 automatically involves first of all identifying which constituentsin the parse tree are semantic arguments to the predicate in question and then assigningappropriate semantic role labels to them.
The predominant approach to the semanticrole labeling task is to formulate it as a classification problem (Pradhan, Ward et al2004; Xue and Palmer 2004) that can be solved with machine-learning techniques.
Onecan imagine a classification task in which each constituent in the parse tree is labeledeither with one of the numbered argument labels (Arg0 through Arg5), or with oneof the semantic adjunct labels ArgM-TMP, ArgM-MNR, and so on, or with the NULLlabel, indicating the constituent is neither an argument nor adjunct to the predicate.This simple formulation of the classification problem is rarely practiced in the semanticrole labeling literature for the simple reason that the majority of the constituents in aparse tree are generally not related to the predicate in question.
For machine-learningapproaches, this means that the negative samples, constituents that are labeled NULL,would far outweigh the positive samples, constituents that are actual semantic argu-ments or adjuncts.
Such an imbalance would lead to poor performance for machine-learning systems, so in practice, most semantic role labeling systems work in stages,whichminimally consist of an argument detection stage and an argument classificationstage.
Argument detection is generally formulated as a binary classification task thatseparates constituents that are arguments or adjuncts to a predicate from those that arenot related to the predicate in question.
By lumping together argument and adjunctlabels, the positive and negative sample imbalance is alleviated somewhat.
In addition,it has been shown that argument detection and argument classification need differentsets of features (Xue and Palmer 2004).
A system cannot take advantage of this if bothare done in one fell swoop.
With a powerful machine-learning algorithm, argumentdetection can be done with high accuracy (Hacioglu et al 2004; Pradhan, Ward et al2004), provided that the appropriate features are used.The positive and negative sample imbalance can only be partially addressed byhaving a separate argument detection stage.
Even with a binary classification task, thenumber of negative samples is still overwhelmingly larger than the positive samples.In addition, it does not take advantage of the fact that the arguments and adjuncts of apredicate, verbal or nominalized, are related to the predicate itself in linguistically well-understood structural configurations.
The overwhelmingmajority of the arguments andadjuncts are populated along the spine of the parse tree that the predicate projects.
Asubstantial number of the constituents can be eliminated from further considerationas negative samples with a high degree of certainty.
(See Sections 4.1 and 5.1 for an233Computational Linguistics Volume 34, Number 2evaluation of how effectively the pruning algorithm works for verbs and nouns.)
Thiswas proved to be a successful strategy by Xue and Palmer (2004) for the semantic rolelabeling of English verbs; they use a heuristic algorithm to first prune out irrelevant con-stituents before the remaining candidates are fed into an argument detection algorithm.This strategy has also been effectively adopted by others (Cohn and Blunsom 2005;Punyakanok, Roth, and Yih 2005) and is used here.
As wewill show in later sections, thepruning algorithm needs to be slightly different for verbal and nominal predicates andthey do not work equally well for all experimental conditions.
Generally, the efficacy ofthe pruning algorithm correlates with the quality of syntactic parses that the semanticrole labeling system takes as input.
That is, it works much better with treebank parsesthan with automatic parses.
It also works more effectively for verbs than for nouns, thearguments of which have a more diverse distribution.Argument classification, which classifies the constituents into a category that corre-sponds to one of the semantic role labels, is a natural multi-category classification prob-lem.
It has been generally shown in the literature (Pradhan et al 2003) that it is a goodidea to bias the argument detection stage toward high recall so that reasonably goodcandidates can be passed along to the argument classification stage, and this means thetag set for argument classification also includes the NULL label.
Many classificationtechniques?SVM (Pradhan, Ward et al 2004), perceptrons (Carreras and Ma`rquez2004a), MaximumEntropy (Xue and Palmer 2004), and so forth?have been successfullyused to solve the semantic role labeling problem.
In the work we report here, for bothargument detection and classification tasks, we used aMaximumEntropy classifier witha tunable Gaussian prior in the Mallet Toolkit (McCallum 2002).
TheMaximum Entropyclassifier does multi-category classification and thus can be straightforwardly appliedto the problem here.
The classifier can be tuned to minimize overfitting by adjusting theGaussian prior.In summary, in our system, the semantic role labeling is done in three stages, asillustrated in Figure 1: pruning, argument detection, and argument classification.4.
Semantic Role Labeling of VerbsIn this section we report our experiments on the semantic role labeling of Chinese verbs,using the Chinese PropBank as training and test material.
There are two variables inour experimental settings.
The first variable is the level of human annotation in thesyntactic parses that serve as input to our semantic role labeling system.We used parsesthat are fully automatic, parses that assume correct segmentation, parses that assumecorrect segmentation as well as POS-tagging, and hand-crafted treebank parses.
Thesecond experimental variable is whether it is known which constituents in the parseFigure 1System architecture.234Xue Semantic Role Labeling of Chinese Predicatestree are arguments.
If the constituents are known, the semantic role labeling reduces toa pure classification problem where each class is one of the semantic role labels.
Thesemantic role labeling system only needs to determine what the correct semantic roleshould be.
If it is unknown which constituents are arguments or adjuncts and whichones are irrelevant to the predicate in question, the system then needs to first figureout which constituents, out of all the constituents in the parse tree, are arguments tothe predicate and then decide what the correct semantic role labels should be.
We didnot experiment with all combinations of these two variables and the known constituentexperiment is only done for treebank parses.In this section, we start by describing our pruning procedure for verbs in Section4.1.
We then present the features for our experiments on verbs in Section 4.2.
In Section4.3 we briefly describe the parsers we used for our experiments and we discuss ourexperimental results in Section 4.4.4.1 Pruning for VerbsSection 3 demonstrated the need for and the feasibility of using a heuristic algorithmto address the imbalance of positive and negative samples and in this section we showhow this algorithm is implemented for verbal predicates.
The algorithm starts from thepredicate that anchors the annotation, and first collects all the syntactic complementsof this predicate, which are represented as sisters to the predicate.
It then iterativelymoves one level up to the parent of the current node till it reaches the root of the tree.At each level, the system tries to determine whether or not that level is a coordinationstructure.
The system only considers a constituent to be a potential candidate if it is amodifier or a complement to the current node.
In the case of a coordination structure,the conjunct that the predicate does not occur in and all its children are eliminated aspossible arguments to the predicate in question.
Punctuation marks at all levels areignored.
It is worth pointing out that the functional tags and traces, which would havebeen useful for this purpose, are not used to determine the candidates to allow for a faircomparison between experiments on hand-crafted parses and parses generated by anautomatic parser.
Typically, current parsers do not predict functional tags and traces.2 Touse Example (8) as a walk-through example, assuming the predicate we are interestedin is  (?investigate?
), the algorithm starts from this predicate and adds the NP  (?accident?)
 (?cause?)
to the candidate list because it is a complement to thepredicate.
Then it goes one level up to the VP and adds its two sisters, the ADVPs,  (?right now?)
and (?carefully?)
to the candidate list because they are modifiers.Then the algorithm goes another level up to another VP, and determines that the twoVPs at this level are conjoined by the punctuation mark, and no candidate is addedat this level because it is a coordination structure.
The algorithm then goes up to thehighest VP level, and adds its sister, the NP (?police?)
to the list of candidates.
Thealgorithm terminates at the highest IP3 level.
The candidates collected by this algorithmare in circles.
The nodes traversed are linked by dotted lines.It is perhaps not surprising that the pruning algorithm works better with treebankparses than with automatic parses.
When the treebank parses are used, our pruning2 There are some ongoing efforts to develop parsers that produce functional tags and traces (Gabbard,Kulick, and Marcus 2006).3 IP in the Chinese Treebank roughly corresponds to S in the Penn Treebank.235Computational Linguistics Volume 34, Number 2algorithm can recall over 99% (8,052 out of 8,121 arguments in the test data) of thearguments while pruning out over 93% (258,959 out of 276,734) nodes in the parse trees.When automatic parses (Maxent segmentation + Bikel parser) are used as input to oursemantic role labeling system, out of 87% of the arguments that have a correspondingconstituent in the parse tree, our pruning algorithm can recall 74% of the argumentswhile pruning out over 92% (247,530 out of 267,381) of the nodes in the parses.
Ourexperiments show that even when the automatic parses are used, the results are farbetter when the pruning algorithm is used than when it is not used.
If the pruningalgorithm is not used, the recall improves somewhat, but the precision plummets.
Theless-than-expected drop in recall when the pruning algorithm is used is perhaps due tothe fact that the arguments that are pruned out also happen to be the hardest for thesemantic role labeling system to get right.
(8) IPNP VPpoliceVP PU VPADVP VP  ADVP ADVP VPalreadyVV NP nowthoroughlyVV NParrivesceneinvestigateNN NNaccidentcause?The police have arrived at the scene and are thoroughly investigating the causeof the accident.
?4.2 FeaturesOne characteristic of feature-based semantic role modeling is that the feature space isgenerally large.
This is in contrast with a low-level NLP task such as POS tagging, whichgenerally has a small feature space.
A wide range of features have been shown to beuseful in previous work on semantic role labeling (Gildea and Jurafsky 2002; Pradhan,Ward et al 2004; Xue and Palmer 2004) and we suspect that many more will be testedbefore the field will settle down to a core set of features.
In their preliminary work onChinese semantic role labeling, Sun and Jurafsky (2004) successfully ported a numberof the features used in Gildea and Jurafsky (2002) to Chinese.
In our experiments weadapted more features that have been described in recent work on English semanticrole labeling to Chinese.
We used a combination of features from Gildea and Jurafsky(G&J) (2002), Pradhan, Ward et al (P et al) (2004), and Xue and Palmer (X&P) (2004),and these are used as baseline features.
In addition, we proposed a set of new featuresthat used verb class information induced from the frame files of the Chinese PropBank,as well as features that were designed to exploit the grammatical constructions that236Xue Semantic Role Labeling of Chinese Predicatesare unique to Chinese, specifically the BA (Bender 2000) and BEI (Huang 1999) con-structions.
We briefly discuss these features and where necessary explain at an intuitivelevel why they are useful for semantic role labeling.
It has been well-established inthe semantic role labeling literature that features are not equally effective for argumentdetection and argument classification (Xue and Palmer 2004).
Our experimental resultson Chinese semantic role labeling generally support this observation.
The features weused for the semantic role labeling of verbs are listed below.
Features that are marked as?C?
are used only in the argument classification task.
Features that are marked as ?D?are for argument detection only.
Features that are used in both argument detection andargument classification stages are marked as ?C,D?.I.
Baseline Features:C Position: The position is defined in relation to the predicate verb and thevalues are before and after.
(G&J)C Subcat frame: The rule that expands the parent of the verb, for example,VP?VV+NP.
(G&J)C Phrase type: The syntactic category of the constituent in focus, for example,NP, PP.
(G&J)C First and last word of the constituent in focus.
(P et al)C Phrase type of the sibling to the left.
(P et al)C Subcat frame+: The subcat frame that consists of the NPs that surround thepredicate verb.
This feature is defined by the position of the constituent infocus in relation to this syntactic frame.
(X&P)C,D Predicate: The verb itself.
(G&J)C,D Path: The path between the constituent in focus and the predicate.
(G&J)C,D Head word and its part of speech: The head word and its part of speech areoften good indicators of the semantic role of a constituent.
(G&J)C,D Combination features: Predicate head word combination, predicate phrasetype combination.
(X&P)II.
New featuresC,D Path to BA and BEI: BA and BEI are function words that impact the orderof the arguments.
BA words are a closed set and in the Chinese Treebankthey have the POS tag BA.
Similarly, BEI words are also a closed set andthey are POS-tagged SB (for short BEI) and LB (for long BEI).C,D Verb class: Verb class itself, verb class + head word combination, verb class+ phrase type combination.The position feature is useful because constituents receiving a particular semanticrole label may occur in some typical positions.
For example, the majority of the adjuncts,ARGMs, occur before the verb in Chinese.
The path feature, defined as the route from theconstituent in focus to the predicate, represents amore ?fine-grained?
position.Whereasthe values for the simple position feature are just BEFORE or AFTER, the values forthe path feature can represent syntactic notions like subject or object.
For example, a237Computational Linguistics Volume 34, Number 2subject may be represented as ?NP?IP?VP?VV?
and an object may be represented as?VV?VP?NP.?
Intuitively, path features are more informative than simple positionfeatures but they are also sparse because they are more specific.
The path featureproves to be particularly effective for the argument detection task, which is perhaps notunexpected.
As we have shown in Section 4.1, the arguments and adjuncts of a predicatetend to be populated along the spine of a parse tree anchored by the predicate, and thisinformation is captured very nicely by the path from the predicate to the constituent inquestion.The head word and its part of speech are clearly informative for semantic rolelabeling.
For example, a noun phrase headed by  (?today?)
is very likely to be atemporal element; so is a prepositional phrase with the head word (?at?).
However,for prepositional phrases, the preposition is not always the most informative element.Sometimes the head word of its NP complement is more predictive of the semanticcategory.
For example, in the prepositional phrase  (?at?)
	 (?Beijing?
), the NPhead 	 (?Beijing?)
is more telling of the fact that it indicates a location.
So forprepositional phrases we use both the preposition and the head noun as features in oursystem.
As has been discussed by Sun and Jurafsky, the head word feature also tendsto be sparse, especially given the smaller size of the Chinese Treebank.
The chance ofseeing a word in the test data that also occurs in the training data is small.
The POStag serves as one form of backoff: Constituents headed by words that have the samepart-of-speech are likely to receive the same semantic role labels as well.The subcat feature, as implemented in previous work (Gildea and Jurafsky 2002), isdefined as the rule that expands the VP dominating the verbal predicate.
By definition, itdoes not vary with the constituents in a parse tree.
In other words, all constituents in theparse tree share the same subcat feature.
Tomake up for the weakness of this feature, weimplemented another feature called subcat+, which is the syntactic frame feature in Xueand Palmer (2004).
The subcat+ feature heuristically identifies the key NP argumentsfor a given predicate, and the feature value of a given constituent is determined by itsposition in relation to these NPs and the predicate.
In this way this feature varies withthe constituent being classified and it also partially addresses the issue that the semanticrole of one constituent is not independent of other arguments for this predicate.As we pointed out in Section 2, the argument labels in the PropBank annotationare verb-specific.
Given a head word or phrase type, the system will be more certainof the semantic role label when it also knows what the predicate is.
The same headword or phrase type may be associated with different semantic role labels for differentpredicates.
The head word + predicate and the phase type + predicate features aredesigned to capture this linguistic intuition.
The other type of combination features areverb class + head word and verb class + phrase type.
We will discuss the use of verbclasses as features in detail in Section 4.2.1.The first-word-in-the-constituent and the phrase label of the left sibling features arefrom Pradhan, Ward et al (2004) and the interested reader is referred to their work foran explanation of why these are useful features.
Because Chinese is a language withmixed headedness, namely, some phrases are left-headed and some phrases are right-headed, the first word and last word are a more robust but sloppier way of finding thehead when the head-finding heuristics fail.Some of the linguistic phenomena that impact the syntactic realization of argumentstructures in Chinese are the BA and BEI constructions.
In the Chinese Treebank, BAand BEI represent closed sets of light verbs that take clausal complements.
The subjectof the clausal complement of BA tends to be Arg1 instead of Arg0 in a canonical clausestructure.
The BEI construction is the Chinese passive construction in which the subject238Xue Semantic Role Labeling of Chinese Predicatesof the clause headed by BEI is typically Arg1.
In order to capture this information, weadded as features the path from the BEI and BA words to the constituent in focus.BA and BEI are not predicates themselves, so these features are only invoked for thepredicate in the complement clause of the BA and BEI.4.2.1 Using Verb Classes to Improve Semantic Role Labeling.With the current experimentalsetup, as is also the case in most of the work on semantic role labeling, training data andtest data are not divided by verb instances but by the number of articles.
As a result,it is expected that the verb instances are not evenly divided.
It is entirely possible thatsome verbs can only be found in the training data and others can only be found in thetest data.
By our count, there are 4,526 verb types in the training data and 1,038 verbtypes in the test data.
One hundred seventy-six verb types that occur in the test data areabsent from the training data.
Because the semantic role labels are defined with regardto the individual verbs, this can be a real problem because the model learned in thetraining process does not optimally fit with the test data if different verbs are involved.Fortunately, many verbs have similar argument structures and therefore are annotatedwith similar semantic role labels in the Chinese PropBank.
For example, verbs like (?enlarge?
),(?make more drastic?
), (?accelerate?
), (?strengthen?
), (?deepen?
), (?accelerate?
), (?give more weight?
), (?make higher?)
alltake two arguments, a theme that undergoes a change of state and an external forceor agent that brings about the change of state.
These verbs are uniformly annotatedand they all have two numbered arguments with Arg0 denoting the cause and Arg1denoting the theme.
It would make sense to group these verbs together into a class anduse this information in the features as has been done for English using VerbNet (Yi,Loper, and Palmer 2007).
Having a membership in a particular class says somethingabout the predicate?argument structure of a verb.
When a verb is absent in the trainingdata, which is a familiar sparse data problem, the class information may tell the systemhow to label the semantic roles of this verb based on its semantic class.Although to our knowledge no such classification exists for Chinese verbs based onthe predicate?argument structure, a rough classification can be automatically derivedfrom the frame files, which are created to guide the PropBank annotation.
We classifiedthe verbs along three dimensions: the number of arguments, the number of framesets,and selected syntactic alternations.Number of arguments Verbs in the Chinese PropBank can have one to five argu-ments, with the majority of them having one, two, or three arguments.
Verbs withzero arguments are auxiliary verbs4 like (?will?
), (?be able to?
),	 (?should?),(?dare?
),  (?may?
),  (?be willing to?
),  (?can?
),  (?can?
),  (?must?
), (?should?
), and some other light verbs.
Verbs that have five arguments are change ofstate verbs like  (?lengthen?
),  (?shorten?
),  (?lower?
),  (?increase?
), (?enlarge?
), 	 (?make smaller?).
These verbs generally take as arguments atheme that undergoes the change of state, the original state, the new state, the rangeof the change, and the cause or agent that brings about the change.Number of framesets A frameset roughly corresponds to a major sense.
This infor-mation is used because it is common that the different framesets of a verb can havedifferent numbers of arguments.
For example, verbs like(?balance?)
can be usedeither as a non-stative verb, in which case it means ?balance,?
or a stative verb, in4 One could say that the argument of the auxiliary verb is the entire proposition, but in this phase of theChinese PropBank, auxiliary verbs are not annotated.239Computational Linguistics Volume 34, Number 2which case it means ?balanced.?
When it is used as a non-stative verb, it takes twoarguments, the thing or situation that is balanced and the balancer, the entity thatmaintains the balance.
When it is used as a stative verb, obviously it only takes a singleargument.Syntactic alternations We also represent certain types of syntactic alternations.
Onesalient type of syntactic alternation is the well-known ?subject of intransitive / objectof transitive?
alternation described in detail in Levin (1993).
Chinese verbs that demon-strate this alternation pattern include (?publish?).
For example, (?this?
) (CL) (?book?)
plays the same semantic role even though it is the subject in ?/this/CL/book /publish /AS?
and the object in ?/this /CL /publishing/house/publish/ASP/this/CL/book.
?Thus each verb will belong to a class with a symbol representing each of thethree dimensions.
For example, a given verb may belong to the class ?C1C2a,?
whichmeans that this verb has two framesets, with the first frameset having one argu-ment and the second having two arguments.
The ?a?
in the second frameset repre-sents a type of syntactic alternation.
Forty classes were semi-automatically derived inthis manner.Such a classification scheme will undoubtedly prove to be linguistically unsophis-ticated.
Verbs that have the same number of arguments may have different types ofarguments, and the current classification system does not pick up these distinctions.However, our experiments show that even such a simple classification can be used toprovide features that improve the semantic role labeling performance.4.3 Using Automatic ParsesPrevious work (Sun and Jurafsky 2004) on Chinese semantic role labeling uses aparser that assumes correct (hand-crafted) segmentation.
As word segmentation isa very challenging problem that has attracted a large body of research by itself, it isstill unclear how well semantic role tagging in Chinese can be performed in realisticsituations.
In our experiments, we implemented a Maximum Entropy?based parsersimilar to Luo (2003).
The parser performs Chinese word segmentation, POS tagging,and parsing in one integrated system.
The parser is trained on the Xinhua news andBroadcast news portion of the Chinese Treebank, which has 498K words.
Tested on theheld-out test data, the parser achieved an unlabeled precision and recall of 0.889 and0.868, respectively, for the combined word segmentation and parsing accuracy.
Whenthe word segmentation is singled out for evaluation, the parser achieved an F-score of0.969.
It is important to point out that these results cannot be directly compared withmost of the results reported in the literature, where correct segmentation is assumed.In addition, in order to account for the differences in segmentation, each character hasto be treated as a leaf of the parse tree.
This is in contrast with word-based parserswhere words are terminals.
For comparison purposes, we also used the Bikel parser(Bikel 2004).
Because the Bikel parser assumes segmented sentences as input, weextracted the segmentation from the output of our parser and fed it into the Bikelparser.
We also experimented with using gold-standard segmentation and POS fromthe Chinese Treebank as input to the Bikel parser to measure the effect of segmentationand POS tagging on the performance on the semantic role labeling.
Because semanticrole labeling is performed on the output of a syntactic parser, only constituents in theparse tree are candidates.
If there is no constituent in the parse tree that shares the sametext span with an argument in the manual annotation, the system cannot possibly get240Xue Semantic Role Labeling of Chinese PredicatesTable 2Semantic role labeling results for verbal predicates.parse constituents feature set precision recall F1 measuretreebank known baseline n/a n/a .931 (acc)treebank known all n/a n/a .941 (acc)treebank unknown baseline .920 .900 .910treebank unknown all .930 .910 .920maxent parser unknown baseline .689 .597 .639maxent parser unknown all .694 .602 .645Bikel parser (auto seg) unknown baseline .745 .596 .662Bikel parser (auto seg) unknown all .748 .603 .668Bikel parser (gold seg) unknown all .768 .625 .689Bikel parser (gold pos) unknown all .795 .656 .719a correct annotation.
In other words, the best the system can do is to correctly label allarguments that have a constituent with the same text span as in the parse tree.4.4 Results and Discussion4.4.1 Data.
In all our experiments we use the Chinese Proposition Bank Version 1.0.5 Thisversion of the Chinese PropBank (Xue and Palmer 2003) consists of standoff annotationon the first 760 articles (chtb_001.fid to chtb_931.fid) of the Chinese Treebank.
Thischunk of the data has 250K words and 10,364 sentences.
The total number of verbtypes in this chunk of the data is 4,854.6 Following the convention of the Englishsemantic role labeling experiments, we divide the training and test data by the numberof articles, not by the verb instances.
For all our experiments on semantic role labelingof verbs, 72 files (chtb_001.fid to chtb_040.fid and chtb_900.fid to chtb_931.fid)are held out as test data,7 40 files (chtb_041.fid to chtb_080.fid) are used as devel-opment set, and the remaining 648 files (chtb_081.fidto chtb_899.fid) are used astraining data.
The training, development, and test sets have 30,280; 1,971; and 3,454propositions, respectively.
Our parser is trained on the training and development setplus 275K words of broadcast news that have been recently annotated as part of theChinese Treebank Project.8 That is, in addition to the training data for the semanticrole labeling experiments, it also uses a portion of the treebank which has not yet beenpropbanked.4.4.2 Results.
The results of the semantic role labeling for both hand-crafted and au-tomatic parses are presented in Table 2.
These results represent an improvement overwhat has been reported in Xue and Palmer (2005) due to the improved parsing resultsand new features.
To be used in real-world natural language applications, a semantic5 This data is publicly available through the Linguistic Data Consortium.6 These include the so-called stative verbs, which roughly correspond to adjectives in English.7 This chunk of data is chosen as test data because it is double annotated and adjudicated.8 We did not use the Sinorama portion of the Chinese Treebank because it is a very different genre andadding it to the training data hurts parser performance (Bikel 2004).241Computational Linguistics Volume 34, Number 2role tagger has to use automatically produced constituent boundaries either from aparser or by some other means, but experiments with hand-crafted parses will help usevaluate how much of a challenge it is to map a syntactic representation to a semanticrepresentation, which may very well vary from language to language.
When hand-crafted parses in the Chinese Treebank are used as input, our system achieved anF-score of 0.92 for combined argument detection and classification.
This accuracy isachieved when the new features are added.
Without the new features, the accuracydrops about one percentage point.
When the arguments are known, the accuracy isat 94.1% when the new features are used, up one percentage point from the baseline.This accuracy is fairly high considering the fact that the state-of-the-art for semanticrole labeling systems trained on the English PropBank (Palmer, Gildea, and Kingsbury2005) is about 93% percent (Pradhan, Ward et al 2004; Xue and Palmer 2004) whenthe arguments are known and the English PropBank is a much larger corpus that hasone million words.
The high baseline accuracy also suggests that the features usedfor the English semantic role labeling port very well to Chinese.
In addition, there areseveral facilitating factors for Chinese semantic role labeling when hand-crafted parsesare provided as input.
First of all, Chinese verbs appear to be less polysemous, at leastthe ones that occur in the Chinese Treebank.
Of the 4,854 verbs in this version of theChinese Proposition bank, only 62 verbs have three or more framesets.
In contrast, 294verbs out of the 3,300 verbs in the Penn English PropBank have three or more framesets.When a verb is less polysemous, the arguments of the verb tend to be realized in a moreuniform manner in syntax.
As a result, the argument labels are easier to predict fromtheir structure.
Chinese seems to compensate for this fact by using a larger number ofverbs.
This becomes obvious when we consider the fact that the 4,854 verbs are fromjust 250K words and the 3,300 verbs in the English PropBank are from one millionwords.
A related fact is that adjectives in Chinese are traditionally counted as verbsand they generally have only one argument with a much simpler syntactic realization.For example, (?inexpensive?)
and  (?thin?)
are considered stative verbs in theChinese Treebank.We also believe that a more subtle explanation for the higher semantic role labelingaccuracy given the annotation of the Chinese Treebank is the fact that the ChineseTreebank has richer structure (see Xue et al [2005] for a comparison of the Penn EnglishTreebank and the Chinese Treebank).
By using less flat and more hierarchical structures,the Chinese Treebank resolves some of the attachment ambiguities that impair semanticrole labeling.
For example, the complement and adjunct in a VP in the Chinese Treebankare attached in different syntactic configurations with regard to the verb.
Because com-plements are generally numbered arguments and adjuncts are generally ARGMs, thesemantic role labeler can take advantage of this information when it tries to determinewhen a constituent is a numbered argument or an adjunct.This apparent advantage in Chinese semantic role labeling is diminished when anautomatic parser is used.
First of all, the hierarchical structures in the hand-craftedparses that aid semantic role labeling are hard to recover with an automatic parser.Resolving the many attachment ambiguities caused by the hierarchical structures inlanguage is one of the most difficult problems in the parsing literature.
Parsing Chinesein a realistic scenario is especially difficult given that it has to build structures fromcharacters rather than words, and Chinese also has few morphological clues to help inmaking parsing decisions.
Our results show that the semantic role labeling accuracyimproves by 2.1% in F-score when the correct segmentation is used as input to the Bikelparser.
When the correct POS tags are used, the semantic role labeling accuracy im-proves another 3%.
At present, improvement in Chinese parsing is also hindered by the242Xue Semantic Role Labeling of Chinese Predicatessmaller training set.
Although the Chinese Treebank 5.1 has a decent size of 500Kwords,it consists of data from very different sources.
Due to their very different styles, trainingon one portion of the data does not help or may even hurt the parsing accuracy on theother portion (see Bikel [2004] for a discussion of this issue).
The situation improvessomewhat with the addition of the 275K words from broadcast news,9 which leads toan improvement in parsing accuracy.
We believe further improvement in semantic rolelabeling accuracy will be to a large extent contingent on the parsing accuracy, whichrequires more training materials that are similar in style.5.
Semantic Role Labeling of Nominalized PredicatesIn this section, we describe our experiments on nominalized predicates in Chinese,using the Chinese NomBank as training and test data.
In Section 5.1 we show thatthe pruning algorithm for nominalized predicates needs to account for two disjointcases.
When a support verb is present, the pruning algorithm needs to go outsidethe NP headed by the predicate to search for potential arguments.
When there is nosupport verb, the arguments can generally be found inside the NP headed by thepredicate.
In Section 5.2, we describe the features used in the semantic role labeling ofnominalized predicates.
There are three groups of features: features used in the semanticrole labeling of verbs minus a few features that do not carry over to nouns, featuresused for verbs that need to be substantially adapted, and new features we designedspecifically for nominalized predicates.
The experiments we conducted on nominalizedpredicates largely parallel those of verbs, for an easier comparison.
Again there aretwo experimental variables, the level of human annotation in the input to the semanticrole labeling system and whether the constituents for the arguments are known.
Theexperimental results are presented in Section 5.3.5.1 Pruning for NominalizationsLike verbal predicates, the arguments and adjuncts of a nominalized predicate arerelated to the predicate itself in linguistically well-understood structural configurations.As we pointed out in Section 2, most of the arguments for nominalized predicates areinside the NP headed by the predicate unless the NP is the object of a support verb, inwhich case its arguments can occur outside the NP.
Typically the subject of the supportverb is also an argument of the nominalized predicate, as illustrated in Example (3).The majority of the constituents are not related to the predicate in question, especiallybecause the sentences in the treebank tend to be very long.
There are two distinct casesthat need to be handled differently, depending on the presence or absence of a supportverb for the nominalized predicate.
When the nominalized predicate does not occurwith a support verb, generally all of its arguments are realized within the NP of whichit is the head.
The pruning algorithm starts from the predicate, collects its sisters, andadds them to the candidate list.
It then iteratively goes up one level and collects thesisters of that constituent until it reaches the top-level NP of which it is the head.
Anexception is made when the constituent is DNP, in which case the candidate added isthe first daughter of the DNP, not the DNP itself.
This is illustrated in Example (9),9 This new data set will soon be available via the LDC in another Chinese Treebank release.243Computational Linguistics Volume 34, Number 2where the algorithm starts from the nominalized predicate (?investigation?
), and,because it does not have any sisters, it does not add anything to the candidate list atthis level.
It then goes up to its parent NP, and collects its sisters NP ( ?police?
)and DNP ( ?toward? ?accident? ?cause? ?DE?).
In the case of DNP, thecandidate added is actually its first daughter, the PP.
(9) IPNP VPNP DNP NP ADVP VPpolicePP DEG NN alreadyendP NP DEinvestigationtowardNN NNaccidentcause?The police investigation of the cause of the accident has ended.
?When a nominalized predicate occurs with a support verb, the NP headed by thenominalized predicate is generally the object of the support verb.
Arguments can oftenbe found both inside and outside this object NP.
The pruning algorithm starts fromthe nominalized predicate and collects its sisters.
It then iteratively goes one level upuntil it reaches the top-level IP node.
At each level, the sisters of the current node areadded to the list of candidates.
Note that the algorithm does not stop at the top NPlevel, so that arguments outside the NP can also be captured.
In practice, it is generallynot known to the algorithm whether the governing verb, the verb that takes the NPheaded by the nominalized predicate as object, is a support verb or not.
Support verbsare often light verbs and they are only a subset of all governing verbs.
The system simplyassumes that all verbs taking the NP headed by a nominalized predicate as its object aresupport verbs, adds constituents outside the NP as candidates, and lets the machine-learning algorithm figure out whether they are arguments or not.
This pruning processis illustrated in Example (10), where the algorithm starts from the nominalized predicate (?investigation?).
It first collects its sister ADJP ( ?thorough?
), and then it willgo one level up to the NP, and adds the support verb ( ?conduct?)
to the candidatelist.
It will go another level up to the VP and adds its sisters ADVP ( ?now?)
andPP ( ?toward?
 ?accident?
 ?cause?)
to the candidate list.
It then goes onemore level up and decides this is a coordination structure; no candidate is added at thislevel.
At the next VP level it adds (?police?)
to the list of candidates.
The algorithmterminates at the IP node.244Xue Semantic Role Labeling of Chinese Predicates(10) IPNP VPpoliceVP PU VPADVP VP  ADVP PP VPalreadyVV NP nowP NP VV NParrivescenetowardNN NN conductADJP NNaccidentcausethoroughinvestigation?The police has arrived at the scene and is thoroughly investigating the cause ofthe accident.
?Overall, pruning works less effectively for nouns than for verbs.
When treebankparses are used, our pruning algorithm can recall over 94% of the arguments whilepruning out 93% (87,724 out of 93,916) of the nodes.
When automatic parses (maxentsegmentation + Bikel parser) are used, our pruning algorithm can recall 73% of thearguments out of the 88% of arguments that have a constituent in the parse tree, whilepruning out 93% (85,160 out of 91,356) of the nodes.
However, although there is a smalldrop in recall (from 0.569 to 0.529) compared with when the pruning algorithm is notused, there is a huge gain in precision (from 0.146 to 0.623), a similar trend to that whichwe have observed for the semantic role labeling of verbs.5.2 FeaturesThe features we use for the semantic role labeling of nominalized predicates fall intothree groups.
The baseline features we used are the same features we used for thesemantic role labeling of verbs.
The second group of features are adapted from featuresused in the semantic role labeling of verbs.
In particular, the path feature is redefinedin the semantic role labeling of nominalized predicates.
A significant number of NPs inthe Chinese Treebank are flat and they consist of a sequence of nouns.
When there arenouns on both sides of a predicate, which is a noun itself, the path from the predicateto the preceding or following noun has the same value.
However, the preceding andfollowing nouns do not have the same probability of being an argument.
We thereforeneed to clearly mark the position of the predicate (e.g, P=NN?NP?NN is not the sameas NN?NP?NN=P).
Such a problem does not exist for the semantic role labeling ofverbs because their arguments are rarely a verb as well.
The third group of featuresare new features we added specifically for the semantic role labeling of nominalizedpredicates.
Like the features for the semantic role labeling of verbal predicates, thefeatures for argument detection only are marked as ?D?
and the features for argument245Computational Linguistics Volume 34, Number 2classification only are marked as ?C.?
The features for both argument detection andargument classification are marked as ?C,D.?
The complete list of features is listed here.I.
Baseline Features:C Position: The position is defined in relation to the predicate and the valuesare before and after.
Because most of the arguments for nominalizedpredicates in Chinese are before the predicates, this feature is not asdiscriminative as when it is used for verbal predicates where argumentscan be both before or after the predicate.
(G&J)C Phrase type: The syntactic category of the constituent being classified.
(G&J)C First and last word of the constituent being classified.
(P et al)C,D Predicate: The nominalized predicate itself.
(G&J)C,D Predicate combination features: Predicate + head word combination,predicate + phrase type combination.
(X&P)C,D Predicate class: The verb class the predicate belongs to; same predicate classas those used for verbs.C,D Predicate class combination features.
Predicate class + head combination,predicate class + phrase type combination.C,D Head word and its part of speech: The head word and its part of speech.
(G&J)C,D Path: The path between the constituent being classified and the predicate.(G&J)II.
Adapted features:C,D Path: The path between the constituent being classified and the predicate,with the predicate clearly identified.III.
New Features:D Topic NP: A binary feature indicating whether the constituent is a topic ifthe predicate is the subject.D Inside NP headed by the predicate: A binary feature indicating whether theconstituent in focus is inside the NP headed by the predicate.D Position of the constituent in relation to the support verb: The value can bebefore or after the support verb, or is the support verb itself.C,D Sisterhood with predicate: A binary feature that indicates whether theconstituent is a sister to the predicate.C,D Path + governing verb.
The path feature combined with the governing verb.Several features that we used for the semantic role labeling of verbal predicates weredropped from our experiments with nominalized predicates.
Specifically, the subcatfeature and subcat+ features were not used because it is not clear how these featurescan be defined for a nominalized predicate.
A couple of new features were addedto the feature set for semantic role labeling of nominalized predicates.
As we havedemonstrated in Section 5.1, a support verb to a large extent determines whether or246Xue Semantic Role Labeling of Chinese Predicatesnot the arguments of a nominalized predicate can occur outside the NP of which itis the head.
Therefore it is effective information for discriminating arguments fromnon-arguments.
It is also indicative of the specific semantic role of an argument in theargument classification task.
To capture this observation, we used a combined feature ofpath+ governing verb that was only invoked when there was an intervening governingverb between the constituent being classified and the predicate.
The governing verbis used as an approximation of the support verb for this feature because the systemdoes not have prior knowledge of whether a verb is a support verb or not absent someexternal resource that provides a list of possible support verbs.
The governing verb, onthe other hand, can be approximated by looking at the syntactic configuration betweenthe nominalized predicate and the verb.
This feature is used for both argument detectionand argument classification.
Another feature we specifically used for the semantic rolelabeling of nominalized predicates is the sisterhood feature.
When looking at the data,we found a substantial number of NPs headed by a nominalized predicate have a flatstructure with their sisters as their arguments.
The sisterhood feature is designed tocapture this observation and it is also used for both argument detection and argumentclassification.
The other three new features were used for argument detection only.When a nominalized predicate is in the subject position, the NP in the topic positiontends to be its argument.
A binary feature is invoked when the constituent in focus isan NP that is the left sister of the subject NP headed by the predicate.
Whether an NP isa subject is also determined heuristically: An NP is considered to be subject if its parentis an IP and its right sister is a VP.
Another binary feature used for argument detectionis whether the constituent in focus is inside the NP headed by the predicate.
Finally, theposition of the constituent in relation to the support verb is also used as a feature forargument detection.
The value for this feature can be before or after the support verb,or it can be the support verb itself.5.3 Experiments5.3.1 Data.
Our system is trained and tested on a pre-release version of the ChineseNomBank.
This version of the Chinese NomBank consists of standoff annotation onthe first 760 articles (chtb_001.fid to chtb_931.fid) of the Chinese Treebank.
Thisis the same chunk of treebank data as used in our experiments on verbs.
It has 1,227nominalized predicate types and 10,497 nominalized predicate instances, in comparisonwith the 4,854 verb predicate types and 37,183 verb predicate instances in the samechunk of data.
By instance, the NomBank is between a quarter and one third of thesize of the Chinese PropBank.
Similarly to our experiments on verbs, we divide thetraining, development, and test data by the number of articles, not by the predicateinstances.
For all our experiments, we used the same data split as that of the verbs: 648files (chtb_081.fid to chtb_899.fid) are used as training data, 40 files (chtb_041.fidto chtb_080.fid) are used as development data, and the other 72 files (chtb_001.fid tochtb_040.fid and chtb_900.fid to chtb_931.fid) are held out as test data.
The sameparsers are used for the semantic role labeling experiments for verbs and nouns.5.3.2 Results and Discussion.
Parallel to our experiments on verbs, we also presentexperiments using hand-crafted and automatic parses.
The experimental results arepresented in Table 3, which represents an improvement from what has been reportedin Xue (2006b).
The baseline results are obtained using the subset of features used in thesemantic role labeling of verbs, minus the subcat and subcat+ features.
We also reportimproved results by using additional new features and adapting the path feature.
The247Computational Linguistics Volume 34, Number 2Table 3Semantic role labeling results for nominalized predicates.parse constituents feature set precision recall F1 measuretreebank known baseline n/a n/a .843 (acc)treebank known all n/a n/a .849 (acc)treebank unknown baseline .722 .608 .660treebank unknown all .734 .661 .696maxent parser unknown baseline .60 .471 .526maxent parser unknown all .60 .502 .547Bikel parser (auto seg) unknown baseline .611 .492 .545Bikel parser (auto seg) unknown all .623 .529 .573Bikel parser (gold seg) unknown all .629 .531 .576Bikel parser (gold pos) unknown all .657 .560 .604use of adapted and new features leads to significant improvement in all experimentsettings except when the constituents are already known and treebank parses are used.This is not surprising given that more new features were added to the argument detec-tion task than the argument classification task.Compared with the 94.1% for verbal predicates on the same data, the 84.3% thesystem achieved for nominalized predicates on treebank parses when the constituentsare given is considerably lower, suggesting that the semantic role labeling for nominal-ized predicates is a much more challenging task.
The difference between the semanticrole labeling accuracy for verbal and nominalized predicates is even greater when theconstituents are not given and the system has to identify the arguments to be classified.Our system achieves an F-score of 0.696 when treebank parses are used, and this isin contrast with the F-score of 0.92 for verbal predicates under similar experimentalconditions.For our experiments using automatic parses, we used the same parsers for nomi-nalized and verbal predicates.
The first parser is the character-basedMaximum Entropyparser that we developed in-house; and it does word segmentation, POS-tagging, andsyntactic parsing in one integrated system.
The second parser is the Bikel parser thattakes three different kinds of input.
In its fully automatic mode, it uses the segmentationextracted from the output of our Maxent parser.
We also experimented with usingcorrect segmentation and correct segmentation plus correct POS-tagging as input tothe Bikel parser to measure the degradation in performance with decreasing levels ofhuman annotation.
Our results show that the Bikel parser outperforms our Maxentparser 0.028 (F-score) in semantic role labeling accuracy when using fully automaticparses.
When the Bikel parser is used, the system achieves an F-score of 0.573, in com-parison with the 0.547 achieved by the Maxent parser.
There is a gradual degradation inperformancewith less human annotation, consistent with our experiments on verbs.
It issomewhat surprising that the segmentation does not affect the semantic role labeling fornominalized predicates as it does for verbs.
Using correct POS tags as input to the Bikelparser, however, leads to a significant improvement of 0.028 in F-score over using correctsegmentation only, from 0.576 to 0.604.
Overall, there is a smaller gap between whentreebank parses are used and when automatic parses are used.
There are two possibleexplanations.
One is that the NP structures are more local and less prone to parsingerrors, so there is less of a difference between treebank and automatic parses.
This is248Xue Semantic Role Labeling of Chinese Predicatesconsistent with the fact that 88% of the arguments for nominalized predicates wererecovered by the parser, in contrast with the 87% of the arguments for verbal predicates.Another possible explanation is that argument detection is challenging even with gold-standard treebank parses, which makes the gap between treebank and automatic parsessmaller.5.3.3 Error Analysis.
The much lower accuracy in the semantic role labeling of nomi-nalized predicates warrants a closer examination.
One thing we looked at is the factthat arguments of nominalized predicates can occur either inside or outside the NPheaded by the predicate.
Of the 1,124 predicate instances in the test data, 331 of themhave arguments that occur outside the NP headed by the predicate.
The remaining 793instances have all their arguments inside the NP.
We found a significant difference inthe semantic role labeling accuracy for the two types of predicates in the experimentsetting where the input to the semantic role labeling system is treebank parses and theconstituents are unknown.
For the predicates that have arguments outside the NP, thesystem achieved a precision and recall of 0.868 and 0.633, respectively.
For the predicatesthat have all their arguments inside the NP, the precision and recall are 0.661 and 0.695,respectively.
We believe the large difference in precision is the result of the systemerroneously identifying arguments outside the NP when the predicate heads an NPthat is the object of a verb, even if the verb is not a support verb.
With a small data set,there is insufficient training data for the system to tell whether or not a verb is a supportverb that licenses arguments outside the NP headed by the predicate.
We also examinedcases where the predicate heads an NP that is the head of a relative clause.
Because theNP headed by the predicate is semantically associated with a trace inside the relativeclause, its arguments can generally be found inside the relative clause.
Out of the 1,124predicate instances, 138 are the heads of relative clauses.
The precision and recall forthese predicates are 0.668 and 0.5, respectively, in comparison with the 0.749 and 0.696for predicates that are not the head of a relative clause.
The much lower recall suggeststhe arguments for the head of a relative clause are much harder to identify.6.
Related WorkComputational approaches to semantic interpretation have a long tradition, but theline of research that this work follows is relatively young.
Gildea and Jurafsky (2002)provided the seminal work on the semantic role labeling, using the FrameNet corpusas training and test material.
Since then, there has been rapid improvement in the se-mantic role labeling accuracy of English verbs, fueled by the development of PropBank(Palmer, Gildea, and Kingsbury 2005), which annotates the verbs in the one-million-word Penn Treebank with semantic role labels.
A wide range of statistical and machinelearning techniques have been applied to the semantic role labeling of verbs, usingPropBank as training and test material.
The machine-learning techniques used includeSupport VectorMachines (Pradhan,Ward et al 2004; Tsai et al 2005), MaximumEntropy(Xue and Palmer 2004; Haghighi, Toutanova, and Manning 2005; Liu et al 2005; Yiand Palmer 2005), Conditional Random Fields (Cohn and Blunsom 2005), and manyothers.
Because semantic role labeling is a complex task based on a wide range oflower level natural language techniques, many different preprocessing, integration, andcombination techniques have been explored.
The relative merits of using a full syntacticparser that provides hierarchical structures (Xue and Palmer 2004) vs. a shallow chunker(Pradhan, Hacioglu et al 2005; Hacioglu et al 2004) has been studied extensively.Noting that parsing errors are difficult or even impossible to recover at the semantic249Computational Linguistics Volume 34, Number 2role labeling stage, Yi and Palmer (2005) experimented with integrating semantic rolelabeling with aMaximum Entropy-based parser, effectively treating semantic role labelsas function tags on the constituents in a parse tree.
Koomen et al (2005), Pradhan, Wardet al (2005), Ma`rquez et al (2005), and Tsai et al (2005) pursued alternative approachesto make their semantic role labeling systems more robust by combining the output ofmultiple systems.
Punyakanok, Roth, and Yi (2005), in particular, achieved the best per-formance (F1 = 0.794) on the WSJ test set in the 2005 CoNLL shared task by combiningmultiple semantic role labeling systems using an integer linear programming technique(Punyakanok et al 2004).
Pradhan, Hacioglu et al (2005) reported the best result (F1 =0.684) on the Brown test set using the WSJ data as the training set by combining theoutput of different semantic role labeling classifiers using a chunking procedure.
Theyalso reported the state-of-the-art result (F1 = 0.81) on the standard PropBank test set,using the same techniques.
Most of the early systems consider each argument on itsown when assigning the semantic role labels, allowing the theoretical possibility thatmore than one core argument may share the same semantic role label, violating thelinguistic constraint that the same semantic role label cannot be assigned to more thanone core argument.
Toutanova, Haghighi, and Manning (2005) address this by using ajoint-learning strategy to rule out such conflicting argument labels.The semantic role labeling performance on the FrameNet data set has alsoimproved significantly from Gildea and Jurafsky?s (2002) early results, thanks mostlyto the Senseval-3 semantic role labeling competition (Litkowski 2004).
Participants ofSenseval-3 have used a variety of machine learning algorithms to tackle the semanticrole labeling problem: Maximum Entropy (Baldewein et al 2004; Ngai et al 2004;Kwon, Fleischman, and Hovy 2004); Boosting, SNOW, and Decision Lists (Ngai et al2004); SVM (Bejan et al 2004; Moldovan et al 2004; Ngai et al 2004); Memory-basedlearning (Baldewein et al 2004), as well as Generative models (Thompson, Patwardhan,and Arnold 2004).
Bejan et al (2004) achieved the best result using an SVM classifiercombined with improved linguistic features.
They achieved an F1 measure of 0.763 intheir internal evaluation, and 0.831 using the more lenient official Senseval-3 scorer.Compared with the large body of work on the semantic role labeling on verbs, theargument structure analysis of nominal predicates has so far received less attention.Jiang and Ng (2006) reports a semantic role labeling system on nominal predicates,also using the maximum entropy approach.
Their system achieves F1 scores of 0.727and 0.691, respectively, on gold-standard and automatic parses, indicating semantic rolelabeling of nominal predicates is a much more difficult problem than that of verbs forEnglish as well.
Outside the narrow domain of semantic role labeling, there has been asteady accumulation of work on semantic analysis of nouns and a gradual expansionof the domain in which the semantic analysis is performed.
Lapata (2002) developed aprobabilistic model for the interpretation of nominalizations, focusing on the semanticrelation between the noun head and its prenominal modifier in a nominalized com-pound (i.e., whether the prenominal modifier is an underlying subject or direct object ofthe verb fromwhich the nominalized head is derived).
Theirmodel achieved a very highaccuracy of 0.861 when evaluated on data extracted from the British National Corpus.Girju et al (2004) andMoldovan and Badulescu (2005) extended the domain of linguisticanalysis to that of noun phrases.
In particular, they focused on the study of four nominalconstructions: complex nominals in which a head noun is modified by other nounsor adjectives derived from nouns, genitives, adjective phrases, and adjective clauses.In general, previous work on nominals, perhaps with the exception of Nakov andHearst (2006) all attempt to specify a finite set of semantic relations between the nounsand their modifiers in the spirit of Levi (1979).
The PropBank/NomBank approach to250Xue Semantic Role Labeling of Chinese Predicatessemantic role labeling adopted here represents a departure from this tradition in thatthe semantic relations in PropBank and NomBank are predicate-specific.
There is noserious attempt to induce cross-predicate semantic relations.
In addition, the semanticrelations represented by the PropBank/NomBank semantic roles are not pairwise rela-tions between the predicate and one of its arguments as they are in previous work.
Athird difference is that the arguments of a nominalized predicate can be found outsidethe noun phrase headed by the predicate (Meyers, Reeves, and Macleod 2004; Meyerset al 2004), making argument identification a much more challenging task.
Whereas theEnglish NomBank annotates both relational nouns and nominalizations, the ChineseNomBank only deals with nominalization, making it a more coherent task.Work on Chinese semantic role labeling is still in its infancy.
Lacking a Chinese cor-pus annotated with semantic roles, the few prior works generally relied on annotatinga small corpus for their experiments.
Sun and Jurasfky (2004) did preliminary work onthe semantic role labeling of Chinese verbs by annotating 10 selected verbs that have afrequency ranging from 41 to 230, using the Chinese PropBank annotation guidelines.Pradhan, Sun et al (2004) extended that work to Chinese nominalizations, and reportedpreliminary work for analyzing the predicate?argument structure of 630 propositionsfor 22 nominalizations taken from the Chinese Treebank.
Noting the difficulty ofChinese parsing, Kwong and T?sou (2005) approached the semantic role labeling taskas one of identifying and labeling the head word of the arguments.
They annotatedthe semantic roles for 41 verbs in 980 sentences in a primary school textbook corpusand the same verbs in 2,122 sentences in a news corpus.
Perhaps not surprisingly, theyreported F1 scores of 0.529 and 0.444, respectively, for the textbook and news corporawhen training and test data are from the same corpus, and 0.463 and 0.398, respectively,when the training and test data are from different corpora.
As far as we know, thework reported here is the first to use sizable Chinese semantically annotated corpora.The approach adopted in the present work emphasizes the integration of linguisticallyinformed heuristics and machine-learning approaches, and the exploration of the un-derlining linguistic insights behind the features used in machine-learning systems.
Webelieve semantic role labeling provides an ideal stage where linguistic observations canbe formalized as features and fed into a general machine-learning framework for testingand verification and natural language technologies can be advanced in the process.7.
Conclusions and Future WorkWehave presented the first experimental results on Chinese semantic role labeling usingthe Chinese PropBank and the Chinese NomBank.
We have shown that given gold-standard parses, Chinese semantic role labeling can be performed with considerableaccuracy on Chinese verbs.
In fact, even though the Chinese PropBank is a significantlysmaller corpus than the English PropBank, we achieved results that are comparablewith the state-of-the-art English semantic role labeling systems.We suggest three factorsthat are particularly conducive to the semantic role labeling of Chinese verbs whenthe hand-crafted treebank parses are used as input.
One is that Chinese verbs tendto be less polysemous compared with English, which contributes to a more uniformmapping between the predicate?argument structure and its syntactic realization.
An-other facilitating factor is that stative verbs, which generally translate into adjectives inEnglish, account for a large proportion of all the verbs in the Chinese PropBank and theytend to have simple argument structures.
Finally, we suggest that the richer structurein the Chinese Treebank makes certain aspects of the semantic role labeling simpler.251Computational Linguistics Volume 34, Number 2One such example is that the clear structural distinction between syntactic argumentsand adjuncts makes it easier for the semantic role labeling system to differentiate corearguments and adjuncts for Chinese verbs.
These all translate into lower confusabilityalong the lines of Erk and Pado?
(2005) in the mapping from the syntactic structure tothe semantic role labels.When the semantic role labeling takes raw text as input, it cannot take advantageof the rich syntactic structure of the treebank unless it can be reproduced with highaccuracy by an automatic parser.
Even though our experiments using fully automaticparses yield promising initial results, the accuracy is significantly lower than the Englishstate of the art.
Our parsing accuracy is hampered by a significantly smaller training setthat is only half the size of the Penn Treebank.
We also suggest that there are a fewinherent linguistic properties of the Chinese language that make syntactic parsing aparticularly challenging task.
The first has to do with the fact that Chinese text doesnot come with word boundaries and our parser has to build structures from charactersrather than words.
The second has to do with the fact that Chinese has very littleinflectional morphology that the parser can exploit when deciding the part-of-speechtags of the words.
Both word segmentation and POS-tagging difficulties will lead toparsing errors when larger phrase structures are built.Our experimental results also show a substantial gap between system performanceon verbs and nominalized predicates.
This difference can be partially attributed to thesmaller corpus size of the Chinese Nombank, with fewer instances of nominalizedpredicates than verbs in the underlying Chinese Treebank, but we believe the mainreason is that the semantic role labeling is more challenging for nominalized predicatesthan for verbs.
This again can be explained in terms of confusability in the mappingfrom syntactic structure to the predicate?argument structure.
In general, the NPs in theChinese Treebank have flatter structures compared with verbs.
For example, there is noclear structural distinction between arguments and adjuncts for nominalized predicatesthat are analogous to the argument/adjunct distinction for verbs.
Another reason forthe lower accuracy for nominalized predicates is the more diverse distribution of theirarguments.
Arguments can be found either inside or outside the NP headed by thepredicate, or even in relative clauses that modify the NP headed by the predicate.There are many directions we can go from here for future work.
There are manyproven techniques that can be implemented for Chinese, the most important of whichis to make Chinese parsers more robust.
One thing we plan to experiment with is thecombination of multiple parsers and multiple semantic role labeling systems.
We alsobelieve that we have not settled on an ?optimal?
set of features for Chinese semantic rolelabeling and more language-specific customization is necessary.
We believe that joint-learning is also a promising avenue to pursue, especially for verbs where generallymorecore arguments are realized.AcknowledgmentsWe would like to thank Martha Palmer forher comments on this manuscript and earlyversions of the paper, and more importantlyfor her steadfast support for this line ofresearch.
We also would like to thank ScottCotton for providing a PropBank library thatgreatly simplified our implementation.Thanks also to the anonymous reviewers fortheir invaluable comments.
This work issupported by the NSF ITR via grant130-1303-4-541984-XXXX-2000-1070.ReferencesBaker, Collin F., Charles J. Fillmore, andJohn B. Lowe.
1998.
The BerkeleyFrameNet project.
In Proceedings ofCOLING/ACL, pages 86?90, Montreal,Canada.252Xue Semantic Role Labeling of Chinese PredicatesBaldewein, Ulrike, Katrin Erk, SebastianPado?, and Detlef Prescher.
2004.
Semanticrole labelling with similarity-basedgeneralization using em-based clustering.In Rada Mihalcea and Phil Edmonds,editors, Proceedings of Senseval-3,pages 64?68, Barcelona, Spain.Bejan, Cosmin Adrian, Alessandro Moschitti,Paul Mora?rescu, Gabriel Nicolae, andSanda Harabagiu.
2004.
Semantic parsingbased on framenet.
In Rada Mihalcea andPhil Edmonds, editors, Proceedings ofSenseval-3, pages 73?76, Barcelona,Spain.Bender, Emily.
2000.
The syntax of Mandarin-ba.
Journal of East Asian Linguistics,9(2):105?145.Bikel, Daniel M. 2004.
On the Parameter Spaceof Generative Lexicalized Statistical ParsingModels.
Ph.D. thesis, University ofPennsylvania.Boas, Hans C. 2002.
Bilingual FrameNetdictionaries for machine translation.
InProceedings of LREC 2002, pages 1364?1371,Las Palmas, Spain.Burchardt, A., K. Erk, A. Frank, A. Kowalski,S.
Pado, and M. Pinkal.
2006.
The SALSACorpus: a German corpus resource forlexical semantics.
In Proceedings of LREC2006, pages 969?974, Genoa, Italy.Carreras, Xavier and Llu?
?s Ma`rquez.
2004a.Hierarchical recognition of propositionalarguments with perceptrons.
In Proceedingsof the Eighth Conference on Natural LanguageLearning, pages 106?109, Boston, MA.Carreras, Xavier and Llu?
?s Ma`rquez.
2004b.Introduction to the CoNLL-2004 sharedtask: Semantic role labeling.
In Proceedingsof the Eighth Conference on NaturalLanguage Learning, pages 89?97,Boston, MA.Carreras, Xavier and Llu?
?s Ma`rquez.
2005.Introduction to the CoNLL-2005 sharedtask: Semantic role labeling.
In Proceedingsof the Nineth Conference on Natural LanguageLearning, pages 152?164, Ann Arbor, MI.Chen, Ying, Hongling Sun, and Dan Jurafsky.2005.
A corrigendum to Sun and Jurafsky(2004) ?Shallow Semantic Parsing ofChinese.?
Technical ReportTR-CSLR-2005-01, University of Coloradoat Boulder CSLR Tech Report.Cohn, Trevor and Philip Blunsom.
2005.Semantic role labeling with tree conditionalrandom fields.
In Proceedings of CoNLL2005,pages 169?172, Ann Arbor, MI.Collins, Michael.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Erk, K. and S Pado?.
2005.
Analyzingmodels for semantic role assignmentusing confusability.
In Proceedings ofHLT-EMNLP, pages 891?898, Vancouver,British Columbia, Canada.Gabbard, Ryan, Seth Kulick, and MitchellMarcus.
2006.
Fully parsing the Penntreebank.
In Proceedings of HLT-NAACL2006, pages 184?191, New York, NY.Gildea, D. and D. Jurafsky.
2002.
Automaticlabeling for semantic roles.
ComputationalLinguistics, 28(3):245?288.Girju, Roxana, Ana-Maria Giuglea, MarianOlteanu, Ovidiu Fortu, Orest Bolohan,and Dan Moldovan.
2004.
Supportvector machines applied to theclassification of semantic relations.In Proceedings of the HLT/NAACLWorkshop on Computational LexicalSemantics, pages 68?75, Boston, MA.Hacioglu, Kadri, Sameer Pradhan, WayneWard, James H. Martin, and DanielJurafsky.
2004.
Semantic role labelingby tagging syntactic chunks.
InProceedings of CoNLL-2004, pages 110?113,Ann Arbor, MI.Haghighi, Aria, Kristina Toutanova, andChristopher Manning.
2005.
A joint modelfor semantic role labeling.
In Proceedings ofCoNLL, pages 173?176, Ann Arbor, MI.Huang, James C. T. 1999.
Chinese passives incomparative perspective.
Tsing Hua Journalof Chinese Studies, 29:423?509.Jiang, Zheng Ping and Hwee Tou Ng.
2006.Semantic role labeling of NomBank:A maximum entropy approach.
InProceedings of the EMNLP, pages 138?145,Sydney, Australia.Kipper, K., A. Korhonen, N. Bryant, andM.
Palmer.
2006.
Extending verbNet withnovel verb classes.
In Proceedings of LREC,Genoa, Italy.Koomen, Peter, Vasin Punyakanok,Dan Roth, and Wen tau Yih.
2005.Generalized inference with multiplesemantic role labeling systems.
InProceedings of the Nineth Conference onNatural Language Learning, pages 181?184,Ann Arbor, MI.Kwon, Namhee, Michael Fleischman, andEduard Hovy.
2004.
Senseval automaticlabeling of semantic roles using MaximumEntropy models.
In Rada Mihalcea andPhil Edmonds, editors, Proceedings ofSenseval-3, pages 129?132, Barcelona,Spain.Kwong, Oi Yee and Benjamin K. T?sou.
2005.Data homogeneity and semantic roletagging in Chinese.
In Proceedings of the253Computational Linguistics Volume 34, Number 2ACL-SIGLEX Workshop on Deep LexicalAcquisition, pages 1?9, Ann Arbor, MI.Lapata, Maria.
2002.
The disambiguation ofnominalizations.
Computational Linguistics,28(3):357?388.Levi, Judith.
1979.
The Syntax and Semantic ofComplex Nominals.
New York: AcademicPress.Levin, Beth.
1993.
English Verbs andAlternations: A Preliminary Investigation.Chicago: The Unversity of Chicago Press.Litkowski, Ken.
2004.
Senseval-3 task:Automatic labeling of semantic roles.
InRada Mihalcea and Phil Edmonds, editors,Proceedings of Senseval-3, pages 9?12,Barcelona, Spain.Liu, T., W. Che, S. Li, Y. Hu, and H. Liu.2005.
Semantic role labeling with amaximum entropy classifier.
InProceedings of CoNLL-2005, pages 189?192,Ann Arbor, MI.Luo, Xiaoqiang.
2003.
A maximum entropyChinese character-based parser.
InProceedings of EMNLP, Sapporo, Japan.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Ma`rquez, Llu?
?s, Mihai Surdeanu, PereComas, and Jordi Turmo.
2005.
A robustcombination strategy for semantic rolelabeling.
In Proceedings of HLT/EMNLP2005, pages 644?651, Vancouver, Canada.McCallum, Andrew Kachites.
2002.
Mallet: Amachine learning for language toolkit.Available at http://mallet.cs.umass.edu.Melli, G., Y. Wang, Y. Liu, M. M. Kashani,Z.
Shi, B. Gu, A. Sarkar, and F. Popowich.2005.
Description of SQUASH, the SFUquestion and summary handler for theDUC-2005 summarization task.
InDocument Understanding Conference 2005,Vancouver, Canada.Meyers, A., R. Reeves, C. Macleod,R.
Szekely, V. Zielinska, B.
Young, andR.
Grishman.
2004.
The NomBank Project:An interim report.
In Proceedings of theNAACL/HLT Workshop on Frontiers inCorpus Annotation, pages 24?31,Boston, MA.Meyers, A., R. Reeves, and CatherineMacleod.
2004.
NP-External arguments: Astudy of argument sharing in English.
InThe ACL 2004 Workshop on MultiwordExpressions: Integrating Processing,pages 96?103, Barcelona, Spain.Moldovan, Dan and Adriana Badulescu.2005.
A semantic scattering model for theautomatic interpretation of genitives.
InProceedings of HLT-EMNLP, pages 891?898,Vancouver, Canada.Moldovan, Dan, Roxana Girju, MarianOlteanu, and Ovidiu Fortu.
2004.
SVMclassification of FrameNet semantic roles.In Proceedings of Senseval-3, pages 167?170,Barcelona, Spain.Nakov, Preslav and Marti Hearst.
2006.Using verbs to characterize noun-nounrelations.
In Proceedings of AIMSA,pages 233?244, Varna, Bulgaria.Narayanan, Srini and Sanda Harabagiu.2004.
Question answering based onsemantic structures.
In Proceedingsof the 20th International Conference onComputational Linguistics, pages 693?701,Geneva, Switzerland.Ngai, Grace, Dekai Wu, Marine Carpuat,Chi-Shing Wang, and Chi-Yung Wang.2004.
Semantic role labeling withboosting, svms, maximum entropy,snow, and decision lists.
In RadaMihalcea and Phil Edmonds, editors,Proceedings of Senseval-3, pages 183?186,Barcelona, Spain.Palmer, Martha, Daniel Gildea, and PaulKingsbury.
2005.
The Proposition Bank:An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?106.Pradhan, Sameer, Kadri Hacioglu,Wayne Ward and James H. Martin, andDaniel Jurafsky.
2005.
Semantic rolechunking combining complementarysyntactic views.
In Proceedings of CoNLL2005, pages 217?220, Ann Arbor, MI.Pradhan, Sameer, Kadri Hacioglu,Wayne Ward, James H. Martin, andDaniel Jurafsky.
2003.
Semantic roleparsing: Adding semantic structureto unstructured text.
In Proceedingsof the International Conference on DataMining (ICDM-2003), pages 629?632,Melbourne, FL.Pradhan, Sameer, Honglin Sun, WayneWard, James H. Martin, and DanielJurafsky.
2004.
Parsing arguments ofnominalizations in English and Chinese.In Proceedings of NAACL-HLT 2004,pages 141?144, Boston, MA.Pradhan, Sameer, Wayne Ward, KadriHacioglu, James H. Martin, and DanJurafsky.
2005.
Semantic role labeling usingdifferent syntactic views.
In Proceedings ofACL 2005, pages 581?588, Ann Arbor, MI.Pradhan, Sameer, Wayne Ward, KadriHacioglu, James H. Martin, and DanielJurafsky.
2004.
Shallow semantic parsingusing support vector machines.
In254Xue Semantic Role Labeling of Chinese PredicatesProceedings of NAACL-HLT 2004,pages 233?240, Boston, MA.Punyakanok, Vasin, Dan Roth, and W. Yih.2005.
The necessity of syntactic parsing forsemantic role labeling.
In Proceedings ofIJCAI-2005, pages 1124?1129,Edinburgh, UK.Punyakanok, Vasin, Dan Roth, Wen Tau.
Yih,and Dav Zimak.
2004.
Semantic rolelabeling via integer programminginference.
In Proceedings of COLING-2004,pages 1346?1352, Geneva, Switzerland.Sgall, Petr, Jarmila Panevova?, and EvaHajic?ova?.
2004.
Deep syntactic annotation:Tectogrammatical representation andbeyond.
In A. Meyers, editor, Proceedings ofthe HLT-NAACL 2004 Workshop: Frontiers inCorpus Annotation, pages 32?38, Boston,MA.Sun, Honglin and Daniel Jurafsky.
2004.Shallow semantic parsing of Chinese.
InProceedings of NAACL 2004, pages 249?256,Boston, MA.Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams, and Paul Aarseth.
2005.
Usingpredicate-argument structures forinformation extraction.
In Proceedings ofACL 2003, Ann Arbor, MI.Thompson, Cynthia, Siddharth Patwardhan,and Carolin Arnold.
2004.
Generativemodels for semantic role labeling.
In RadaMihalcea and Phil Edmonds, editors,Proceedings of Senseval-3, pages 235?238,Barcelona, Spain.Toutanova, Kristina, Aria Haghighi, andChristopher Manning.
2005.
Joint learningimproves semantic role labeling.
InProceedings of ACL-2005, pages 589?596,Ann Arbor, MI.Tsai, Tzong-Han, Chia-Wei Wu, Yu-ChunLin, and Wen lian Hsu.
2005.
Exploitingfull parsing information to label semanticroles using an ensemble of ME and SVMvia integer linear programming.
InProceedings of CoNLL-2005, pages 233?236,Ann Arbor, MI.Xue, Nianwen.
2006a.
Annotating thepredicate-argument structure ofChinese nominalizations.
In Proceedingsof the Fifth International Conference onLanguage Resources and Evaluation,pages 1382?1387, Genoa, Italy.Xue, Nianwen.
2006b.
Semantic role labelingof nominalized predicates in Chinese.In Proceedings of HLT-NAACL 2006,pages 431?438, New York, NY.Xue, Nianwen and Martha Palmer.
2003.Annotating the propositions in the PennChinese Treebank.
In The Proceedings of the2nd SIGHANWorkshop on Chinese LanguageProcessing, Sapporo, Japan.Xue, Nianwen and Martha Palmer.
2004.Calibrating features for semanticrole labeling.
In Proceedings of 2004Conference on Empirical Methods in NaturalLanguage Processing, pages 88?94,Barcelona, Spain.Xue, Nianwen and Martha Palmer.
2005.Automatic semantic role labeling forChinese verbs.
In Proceedings of theNineteenth International Joint Conference onArtificial Intelligence, pages 1160?1165,Edinburgh, Scotland.Xue, Nianwen, Fei Xia, Fu dong Chiou, andMartha Palmer.
2005.
The Penn ChineseTreeBank: Phrase structure annotation of alarge corpus.
Natural Language Engineering,11(2):207?238.Yi, Szuting, Edward Loper, and MarthaPalmer.
2007.
Can semantic rolesgeneralize across genres?
In Proceedingsof NAACL-2007, pages 548?555,Rochester, NY.Yi, Szuting and Martha Palmer.
2005.
Theintegration of syntactic parsing andsemantic role labeling.
In Proceedingsof CoNLL-2005, pages 237?240,Ann Arbor, MI.255
