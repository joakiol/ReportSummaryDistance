Chinese Word Segmentation in FTRD BeijingHeng LIFrance Telecom R&D Bei-jingheng.li@francetelecom.comYuan DONGFrance Telecom R&D Beijingyuan.dong@francetelecom.comXinnian MAOFrance Telecom R&D Bei-jingxin-nian.mao@francetelecom.comHaila WANGFrance Telecom R&D Bei-jinghaila.wang@francetelecom.comWu LIUBeijing University of Postsand Telecommunicationswu.liu@francetelecom.com.cnAbstractThis paper presents a word segmenta-tion system in France Telecom R&DBeijing, which uses a unified approachto word breaking and OOV identifica-tion.
The output can be customized tomeet different segmentation standardsthrough the application of an orderedlist of transformation.
The system par-ticipated in all the tracks of the seg-mentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open,HK-closed, MSR-open and MSR-closed -- and achieved the state-of-the-art performance in MSR-open, MSR-close and PK-open tracks.
Analysis ofthe results shows that each componentof the system contributed to the scores.1 IntroductionThe development of the Chinese word segmen-tation system presented in this bakeoff began inFeb.
this year, and will last for one year with thesupport of the ILAB Beijing initial projectwithin France Telecom R&D.Although the project last only half year bynow, the main components of the system hasbeen implemented, including code identificationand conversion, basic segmentation, factoid de-tection, morphological analysis, name entityidentification, segmentation standards adaptor,except the components of code identificationand conversion and segmentation standardsadaptors, other components are integrated in astatistical framework of n-gram language model.2 System Description2.1 Code identification and conversionFor processing both Simplified and TraditionalChinese text from a variety of locales, includingMainland China, Hong Kong and Taiwan, wechoose UTF-8 as internal character representa-tion within the system.
The ability to transpar-ently handle Chinese text from any Chineselocale greatly simplifies the logic of the segmen-tation system.2.2 N-gram language modelIn our system, Chinese words can be categorizedinto one of the following types: lexicon words,morphological words, factoids, name entities.These types of words are processed in differentways in our system, and are incorporated into aunified statistical framework of the trigram lan-guage model.2.2.1 Basic segmentationEach input sentence is first segmented into indi-vidual characters.
These characters and the char-acter strings are then looked up in a lexicon.
Forthe efficient search, the lexicon is represented bya TRIE compressed in a double-array data struc-150ture.
Given a character string, all its prefixstrings that form lexicon words can be retrievedefficiently by browsing the TRIE whose rootrepresents its first character.2.2.2 Factoid detectionThere are twenty four kinds of factoid words,such as time, date, money, etc.
All the factoidwords are represented as regular expressions,and compiled into a compressed DFA with therow-index algorithm.2.2.3 Morphological analysisAs (Wu 2003) discussed in the paper, it is thosemorphologically derived words (MDWs hereaf-ter) that are most controversial and most likelyto be treated differently in different standardsand different systems.
In our system, there aresix main categories of morphological processes,affixation, directional verb, resultative verb,splitting verb, reduplication and merging, andwe employ a chart parsing algorithm augmentedwith word lattices structure which incorporatesthe morphological rules especially designed forChinese languages with restrictive CFG.2.2.4 Name entity identificationOur NE identification concentrates on threetypes of NEs, namely, personal names (PERs),location names (LOCs) and organization names(ORGs).
For Chinese person names, we onlyconsider PN candidates that begin with a familyname stored in the family name list and follow agiven name which is of one or two characterslong.
For transliterations of foreign personnames, a PN candidate would be generated if itcontains only characters stored in a transliteratedcharacter list.
For location names and organiza-tions names, we only use the LN list and ON listto generate the candidates.2.3 Segmentation standards adaptorIn this bakeoff, there are four segmentationstandards and slightly different from ours.
Stan-dard adaptation is conducted with the applica-tion of an ordered list of transformations on theoutput of our segmentation system.
The methodwe use is Transformation-Based Learning, andthe transformation templates are lexicalizedtemplates.
In our system, we designed 14 lexi-calized templates.2.4 SpeedAs we optimized our lexicon and decodingprocess, the speed of segmentation is very fast.On a single 2.80 GHz, 1G bytes memory, Xeonmachine, the system is able to process about0.73 Mega bytes per second.The speed may vary according to the sen-tence lengths: given texts of the same size, thosecontaining longer sentences will take more time.The number reported here is an average of thetime taken to process the test sets of the eighttracks we participated in.3 Evaluation3.1 Open tracksIn the open tracks, we used four lexicons of210,319 entries, 165,103 entries, 174,268 entries,165,655 entries respectively on AS-open, HK-open, MSR-open, PK-open tracks, which in-clude the entries of 2,430 MDWs, 12,487 PNs,22,907 LNs and 29,032 ONs, 10,414 four-character idioms, plus the word lists generatedfrom the training data provided by the bakeoff.We use the training data provided by the bakeofffor training our trigram word-based languagemodel.
We also used a family name list (whichcontains 399 entries in our system), and a 1,021-entry transliterated name character list.3.2 Closed tracksIn the close tracks, the lexicon we use couldonly be generated from the training data pro-vided by the bakeoff.
We could only use thetraining data provided by the bakeoff for train-ing our word-based language model.
Also, sincethe training data we used is only from the bake-off, there does not exist any different standards,standards adaptor component is not necessarilyneeded.3.3 Result analysisOur system is designed so that components suchas the factoid detection and NE identificationcan be switched on or off, so that we can inves-tigate the relative contribution of each compo-nent to the overall word segmentationperformance.
The results are summarized in thetable 1.
For comparison, we also include in thetable (Row 1) the results of using FMM.
Row 2shows the baseline results of our system, whereonly the lexicon is used.
Each cell in the tablehas six fields.
From the top, there are respec-tively Precision, Recall, F-measure, OOV Recall,IV Recall and Speed (Mega bytes/second).
Wedon't list the speed in Row 6 since it decreases afactor of 10 to 60 because of application ofthousands of TBL rules.151PKo PKc MSRo MSRc ASo ASc HKo HKc1.
FMM0.8570.9250.8910.1430.9472.4350.8410.9060.8720.0690.9572.5700.9210.9680.9450.1070.9712.9510.9170.9570.9360.0250.9823.0900.8710.9250.8980.0970.9472.8130.8640.9110.8870.0140.9522.9370.8420.9280.8850.1750.9612.7480.8380.9080.8720.1620.9682.8502.
Baseline0.8690.9410.9050.2350.9600.9670.8550.9280.8900.0690.9871.0170.9310.9730.9520.2750.9870.8790.9260.9690.9470.0250.9950.9230.8910.9430.9170.1320.9820.7030.8770.9420.9080.0140.9840.7280.8630.9300.8970.1940.9850.9210.8510.9290.8880.1620.9900.9563.
2+FT0.9460.9510.9480.7480.9630.8190.9190.9500.9340.4480.9800.8790.9500.9730.9610.3960.9900.7790.9400.9730.9560.2050.9440.7870.9030.9450.9240.1800.9790.6310.9000.9470.9230.1560.9830.6350.8730.9320.9020.2920.9830.8210.8620.9320.8950.2150.9890.8304.
3+MA0.9460.9510.9480.7480.9630.8070.9190.9500.9340.4480.9800.8790.9500.9730.9610.3710.9890.7530.9400.9730.9560.2050.9440.7870.9030.9450.9240.1810.9790.6260.9000.9470.9230.1560.9830.6350.8730.9320.9020.2950.9830.8150.8620.9320.8950.2150.9890.8305.
4+NE0.9510.9570.9540.7880.9670.6790.9190.9500.9340.4480.9800.8790.9560.9730.9650.4540.9560.7160.9400.9730.9560.2050.9440.7870.9200.9490.9340.3300.9770.6040.9000.9470.9230.1560.9830.6350.9000.9380.9180.4110.9800.7480.8620.9320.8950.2150.9890.8306.
5+adaptation0.9600.9640.9620.7880.9740.9190.9500.9340.4490.9800.9570.9750.9660.4530.9890.9400.9740.9570.2100.9950.9190.9520.9350.3110.9810.9000.9480.9230.1580.9830.9010.9400.9200.4100.9820.8620.9320.8950.2150.989Table 1.
Our system results on all the tracks.From Table 1 we can find that, in rows 1 and 2,the dictionary-based methods already achievequite good recall, but the precisions are not verygood because they cannot correctly identify un-known words that are not in the lexicon such asfactoids and name entities.
We also find thateven using the same lexicon, our approach thatis based on the N-gram language models outper-forms the greedy approach because the use ofcontext model resolves more ambiguities insegmentation.
As shown in Rows 3 to 5, whencomponents are switched on in turn, the overallword segmentation performance increases con-sistently.
The morphological analysis has nocontribution to the overall performance in Row4.
The main reason is that the number of MDWsused in our system is very small (only 2,430)and there may exist very small MDWs in the testsets.
The similar cases occur on NE identifica-tion in the close tracks in Row 5 since we wouldnot do NE identification at all in the close tracks.We also notice that the contribution of NE iden-tification is very little in the open tracks, whichshows that the performance of NE identificationis not very good in our system, and explainswhy our OOV recall is not very high compared152with other participants in the bakeoff.
This isone area of our future work to improve.
The re-sults of standards adaptation on four bakeoff testsets are shown in Row 6.
It turns out that per-formance except IV recall improves slightlyacross the board in all four test sets.
The mainreason is that the training data and lexicon weused are mainly from the four providers in thebakeoff, there does not exist any different seg-mentation standards.4 ConclusionsThe evaluation results show that the closed testsis not very good compared with other partici-pants, the one main reason is that the word-based language model we used is not competi-tive compared with other algorithms in theclosed tracks.
One area of our future work is toapply other machine learning algorithm, likeMaximum Entropy (ME), Support Vector Ma-chine (SVM), Conditional Random Field (CRF),etc.AcknowledgementsThe work reported here was a team effort.
Wethank Wu Liu, Haitao Zeng, Nan He for theirhelp in the experimentation and evaluation ofthe system.ReferencesAndi Wu.
2003.
Customizable segmentation of mor-phologically derived words in Chinese.
Interna-tional Journal of Computational Linguistics andChinese Language Processing, 8(1): 1-27.Aoe, J.
1989.
An Efficient Digital Search Algorithmby Using a Double-Array Structure.
IEEE Trans-actions on Software Engineering, Vol.
15, 9:1066-1077.George Anton Kiraz.
1999.
Compressed Storage ofSparse Finite-State Transducers.
4th InternationalWorkshop on Automata Implementation, Pages:109-121.Jian Sun, Ming Zhou and Jianfeng Gao.
2003.
Chi-nese named entity identification using class-basedlanguage model.
International Journal of Compu-tational Linguistics and Chinese Language Proc-essing, 8(1).Jianfeng Gao, Mu Li, Andi Wu and Chang-NingHuang.
2004a.
Chinese word segmentation: apragmatic approach.
Microsoft Research Techni-cal Report, MSR-TR-2004-123.Julia Hockenmaier, Chris Brew.
1998.
Error drivensegmentation of Chinese.
Communications ofCOLIPS, 8(1): 69-84.Xinnian Mao, Heng Li, Yuan Dong, Haila Wang.2005.
Chinese Morphological Analyzer.
IEEENLP-KE 2005, submitted.153
