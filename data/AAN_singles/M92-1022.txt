The LINK System :MUC-4 Test Results and AnalysisSteven L .
Lytinen, Sayan Bhattacharyya, Robert R .
Burridge ,Peter M. Hastings, Christian Huyck, Karen A .
Lipinsky,Eric S. McDaniel, and Karenann K. TerrellArtificial Intelligence Laborator yThe University of MichiganAnn Arbor, MI 48109E-mail: lytinenOcaen.engin .umich .eduResult sThe University of Michigan 's natural language processing system, called LINK, was usedin the Fourth Message Understanding System Evaluation (MUC-4) .
LINK 's performance onMUC-4's two test corpora is summarized in figure 1 .Although we only tested LINK in a single configuration, there were several parameters tha tcould have been varied in the system.
They include the following :1.
What to do with undefined words .
When the system identified a group of undefinedwords as a likely noun phrase, it was assumed that this noun phrase referred to some kin dof HUMAN or PLACE.
1 Thus, these noun phrases were potential candidates to fill theLOCATION, PERP, PHYS TGT, or HUM TGT fields of a template .2.
When to generate templates .
A template was only generated if an appropriate fille rfor the PERP, PHYS TGT or HUM TGT field had been extracted from the text .3.
When to merge templates .
Every time a new template was generated for an article,the system considered merging it with existing templates .
A merge was performed ifanother template with the same INCIDENT TYPE already existed, and if there were noexplicit contradictions between the existing template's filled fields and the new template .For example, if the two templates had different DATE fields, they were not merged .
Inaddition, BOMBING and ATTACK templates were merged if they had no contradictoryfields .Amount of effortWe estimate that 1 .5 person-years were spent on our MUC-4 effort .
Figure 2 shows thebreakdown of this effort on different parts of the system .Prior to MUC-4, LINK had been used in several smaller-scale applications, including th eextraction of information from free-form textual descriptions of automobile malfunctions an dthe repairs that were made to fix them ; as well as an application involving free-form textualinstructions for assembly line workers .Little modification was required of the parser itself for MUC-4 .
However, several new mod-ules were built around the parser .
In particular, since both of our prior applications involve d'See our accompanying system summary paper for details.159TST3SLOTPOS ACTICOR PAR INCIICR IPAISPU MIS NONIREC PRE OVGFAL+	 +	 +	 +MATCHED/MISSING1540 11031557 155 1411 6 1011250 687 10711 41 58 2 3MATCHED/SPURIOUS1117 15881557 155 1411 6 1011735 264 11351 57 40 46MATCHED ONLY1117 11031557 155 1411 6 1011250 264 6681 57 58 2 3ALL TEMPLATES1540 15881557 155 1411 6 1011735 687 15381 41 40 4 6SET FILLS ONLY741 5491303 58 631 0 361125 317 4881 45 60 231STRING FILLS ONLY398 2491118 20 401 4 201 71 220 2991 32 51 28+	 ?
+	 +	 +PtR2P*RP&2RF-MEASURES40 .4940 .240 .
8TST4SLOTPOS ACTICOR PAR INCIICR IPAISPU MIS NONIREC PRE OVGFALMATCHED/MISSING1188 7301374 121 991 8 631136 594 8021 36 60 19MATCHED/SPURIOUS764 12641374 121 991 8 631670 170 9761 57 34 53MATCHED ONLY764 7301374 121 991 8 631136 170 473) 57 60 19ALL TEMPLATES1188 12641374 121 991 8 631670 594 13051 36 34 53SET FILLS ONLY580 3571211 35 491 3 141 62 285 3621 39 64 170STRING FILLS ONLY307 1711 88 19 251 1 181 39 175 2271 32 67 2 3P&R2PtRP&2RF-MEASURES34 .9734 .3835.58Figure 1 : LINK's performance on the TST3 and TST4 corpor areading only single-sentence texts, with no need to monitor context, there was a need to en-hance the system so that multi-sentence texts could be processed .
The reader is referred to ouraccompanying system summary paper for a description of each module .Development time was definitely the limiting factor in our system's performance .
Althoughwe felt that our knowledge base was approaching completion toward the end of the developmen ttime, considerably more effort could have been expended toward improving our system's abilit yto handle multi-sentence input had more time been available .
We will discuss this further i nsection .Tokenizer2Preprocessor2Knowledge base development9Postprocessor :Template generation3Template merging1Reference resolution1Figure 2 : Breakdown of MUC-4 effort by module (person-months)16 0Training of the systemWe used the MUC-3 development corpus answer keys to help develop the knowledge bas efor our system .
Some of this development was partially automated, although not as much as w ehad originally hoped .
The answer keys contained a great deal of information about how variou slexical items should map to the HUM TGT, PHYS TGT, and INSTRUMENT TYPE fields in th eMUC-4 templates .
For example, the appearance of LAW ENFORCEMENT : "POLICEMEN "in field 20 of several answer key templates, along with PLURAL : " POLICEMEN" in field 21 ,suggested that "POLICEMEN" should be defined in our lexicon as a plural noun which mean sLAW ENFORCEMENT.
We were able to use this information to define a substantial percentag eof the nouns in our lexicon .Unfortunately, there was no such source of information for other types of words that were o finterest in the domain, such as verbs, adjectives, prepositions, etc .
An INCIDENT : DESCRIP-TION field in the template would have provided information for verbs, but no field existed in th eMUC-4 templates .
Thus, the remainder of the lexicon was constructed entirely by hand .
Ourtest configuration system contained a total of 6700 lexical entries, with 7532 distinct definition s(i .e ., some words were defined with more than one sense) .The system's grammar was also developed by hand .
The grammar in the test configuratio nof our system contained 565 rules .
Although many rules were not related to the terrorism do -main, and thus could presumably be used in a different domain, about half of the rules wer edomain-specific, and could not transfer to a new domain without some inspection and modifi-cation .
For example, rules about combining noun groups often contained semantic informatio nwhich was specific to the domain (e .g., a noun meaning BOMB followed by a noun meaningATTACK maps to a BOMBING with the INSTRUMENT field filled by the BOMB noun) .What workedIn a large-scale natural language application such as MUC-4, it is virtually certain that a nNLP system will not be able to produce a complete syntactic and semantic analysis for multi-sentence or multi-paragraph articles .
Developing a complete lexicon, grammar, or set of semanti cinterpretation rules for such an application is virtually impossible .
Thus, it is very importantfor a system to have strategies to deal with texts which cannot be completely processed .
Oursystem's strategies for incomplete processing were vital to its ability to perform at the level tha tit did .
These strategies included the following:1.
Preprocessing : identifying noun phrases .
The preprocessor, explained in detail in ou rsystem summary paper, grouped together words which were candidate noun phrases .
TheseNP 's often included words which were not in the system's lexicon.
As a result, undefinedwords did not interfere with the system's ability to parse a sentence .
Although our lexiconcontained 6700 entries, we estimate that nearly 14,000 distinct lexical items appear in theMUC-3 training corpus .
Thus, an effective approach for dealing with undefined words wascritical to our system's performance .2.
Identifying important partial parses .
Even with the enhancement provided by thepreprocessor, our system did not succeed in parsing the majority of sentences that i tencountered .
However, information was extracted from these sentences by examining the161constituents that were built, even though they did not lead to a complete parse .
Thisability was vital to the performance of our system, and is described in more detail in ou rsystem summary paper .What didn't workOur system's ability to correctly integrate information extracted from multiple sentences wasits weakest point .
Most of the decisions as to how information should be integrated were madein the postprocessor ; thus, this module is clearly the best candidate for rewriting .Several problems existed in the postprocessor .
First, its strategies for deciding when two tem-plates should be merged were not very effective.
As described earlier, this decision relied purelyon the information contained in the two templates which were being considered for merging .
Bydefault, templates were merged unless the information they contained explicitly contradictedeach other .
This resulted in templates being merged even when the text contained obvious cue sthat two separate events were being described .
For example, if a BOMBING template had al -ready been generated for an article, a sentence beginning with "Another bombing occurred .
.
.
"would not generate a second bombing template unless information about LOCATION, PHY STGT, etc ., contradicted information in the first template .Related to our system's poor merging heuristics was its lack of a sophisticated referenc eresolution strategy.
Two kinds of reference resolution existed in the system, for names an dpronouns .
Whenever a name of a person was identified in the text, a list was searched fo rprevious occurrences of that name, or of a longer name containing the new name .
If a match wasfound, additional information about the person, which could be used to fill the DESCRIPTIONor TYPE field, could be obtained from the prior mention of that person .Pronominal reference in our system was extremely simplistic .
When a pronoun was encoun-tered, its referent was resolved to the most recent NP prior to it in the text which met simpl esemantic restrictions .
If the pronoun was assigned to be the PERP of an event, then its referenthad to be a type of TERRORIST.
If it was assigned to be the HUM TGT, then its referent ha dto be a HUMAN who was not a TERRORIST .
These simple heuristics obviously could havebeen improved greatly.Finally, additional information about a template which appeared in a subsequent sentenceoften was not extracted .
Lists of victims, additional information about perpetrators or victims ,and so on that appeared in a separate sentence from the initial mention of a terrorist act wer enot usually added to the template .What we learnedPerhaps the most important lesson of MUC is that in a large-scale natural language applica-tion, it is not yet possible to construct a knowledge base which will enable complete processin gof even a majority of input texts .
The domain is simply too large, and the possible variationsin language too great .
Thus, as we said earlier, it is very important for a system to have robus tstrategies for dealing with texts which cannot be completely processed .Due to time constraints, we devoted very little effort to discourse processing .
The lesson welearned here was twofold : on the one hand, we were a bit surprised that we could achieve eve n40% recall with only the simplest heuristics for integrating information from multiple sentences .162Single sentences often contained enough information for our system to generate a template wit hsufficient information to match the answer key .
On the other hand, we felt that we were nearin gthe maximum score that we could have achieved without further developing this aspect of ou rsystem.
Thus, in another MUC-like task our group would devote a great deal of our effort i nthis area .Finally, as we analyzed our system's results during development, we realized that the recaland precision scores used for evaluation would change significantly with relatively minor adjust-ments in the criteria used by the scoring program .
Perhaps the prime factor that affected ourown score was the criteria for what constituted a match between a response template and theanswer key.
Our system often erroneously merged two templates into a single template .
Thus ,correct fills of PHYS TGT and HUM TGT fields were often split between two templates in th eanswer key.
At other times, our system generated two or more templates when a single templat eshould have been generated.
In this case, although correct information was split between theresponse templates, the scoring program only allowed a single match between response template sand the answer key, and counted additional response templates as spurious, even though the ymight have contained information which matched some of the information in the single templat ein the answer key.163
