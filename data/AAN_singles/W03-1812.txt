An Empirical Model of Multiword Expression DecomposabilityTimothy Baldwin?, Colin Bannard?, Takaaki Tanaka?
and Dominic Widdows??
CSLIStanford UniversityStanford CA 94305, USA{tbaldwin,dwiddows}@csli.stanford.edu?
School of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, UKc.j.bannard@ed.ac.uk?
Communication ScienceLabsNTT CorporationKyoto, Japantakaaki@cslab.kecl.ntt.co.jpAbstractThis paper presents a construction-inspecific model of multiword expressiondecomposability based on latent semanticanalysis.
We use latent semantic analysisto determine the similarity between amultiword expression and its constituentwords, and claim that higher similaritiesindicate greater decomposability.
Wetest the model over English noun-nouncompounds and verb-particles, and eval-uate its correlation with similarities andhyponymy values in WordNet.
Based onmean hyponymy over partitions of dataranked on similarity, we furnish evidencefor the calculated similarities being corre-lated with the semantic relational contentof WordNet.1 IntroductionThis paper is concerned with an empirical model ofmultiword expression decomposability.
Multiwordexpressions (MWEs) are defined to be cohesive lex-emes that cross word boundaries (Sag et al, 2002;Copestake et al, 2002; Calzolari et al, 2002).
Theyoccur in a wide variety of syntactic configurationsin different languages (e.g.
in the case of English,compound nouns: post office, verbal idioms: pullstrings, verb-particle constructions: push on, etc.
).Decomposability is a description of the degree towhich the semantics of an MWE can be ascribedto those of its parts (Riehemann, 2001; Sag et al,2002).
Analysis of the semantic correlation betweenthe constituent parts and whole of an MWE is per-haps more commonly discussed under the banner ofcompositionality (Nunberg et al, 1994; Lin, 1999).Our claim here is that the semantics of the MWE aredeconstructed and the parts coerced into often id-iosyncratic interpretations to attain semantic align-ment, rather than the other way around.
One id-iom which illustrates this process is spill the beans,where the semantics of reveal?(secret?)
are de-composed such that spill is coerced into the idiosyn-cratic interpretation of reveal?
and beans into theidiosyncratic interpretation of secret?.
Given thatthese senses for spill and beans are not readily avail-able at the simplex level other than in the contextof this particular MWE, it seems fallacious to talkabout them composing together to form the seman-tics of the idiom.Ideally, we would like to be able to differ-entiate between three classes of MWEs: non-decomposable, idiosyncratically decomposable andsimple decomposable (derived from Nunberg et al?ssub-classification of idioms (1994)).
With non-decomposable MWEs (e.g.
kick the bucket, shootthe breeze, hot dog), no decompositional anal-ysis is possible, and the MWE is semanticallyimpenetrable.
The only syntactic variation thatnon-decomposable MWEs undergo is verbal in-flection (e.g.
kicked the bucket, kicks the bucket)and pronominal reflexivisation (e.g.
wet oneself ,wet themselves).
Idiosyncratically decomposableMWEs (e.g.
spill the beans, let the cat out of thebag, radar footprint) are decomposable but co-erce their parts into taking semantics unavailableoutside the MWE.
They undergo a certain degreeof syntactic variation (e.g.
the cat was let out ofthe bag).
Finally, simple decomposable MWEs(also known as ?institutionalised?
MWEs, e.g.
kin-dle excitement, traffic light) decompose into simplexsenses and generally display high syntactic variabil-ity.
What makes simple decomposable expressionstrue MWEs rather than productive word combina-tions is that they tend to block compositional al-ternates with the expected semantics (termed anti-collocations by Pearce (2001b)).
For example, mo-tor car cannot be rephrased as *engine car or *mo-tor automobile.
Note that the existence of anti-collocations is also a test for non-decomposable andidiosyncratically decomposable MWEs (e.g.
hot dogvs.
#warm dog or #hot canine).Our particular interest in decomposability stemsfrom ongoing work on grammatical means for cap-turing MWEs.
Nunberg et al (1994) observed thatidiosyncratically decomposable MWEs (in particu-lar idioms) undergo much greater syntactic variationthan non-decomposable MWEs, and that the vari-ability can be partially predicted from the decompo-sitional analysis.
We thus aim to capture the decom-posability of MWEs in the grammar and use this toconstrain the syntax of MWEs in parsing and gen-eration.
Note that it is arguable whether simple de-composable MWEs belong in the grammar proper,or should be described instead as lexical affinitiesbetween particular word combinations.As the first step down the path toward an empir-ical model of decomposability, we focus on demar-cating simple decomposable MWEs from idiosyn-cratically decomposable and non-decomposableMWEs.
This is largely equivalent to classifyingMWEs as being endocentric (i.e., a hyponym oftheir head) or exocentric (i.e., not a hyponym oftheir head: Haspelmath (2002)).We attempt to achieve this by looking at the se-mantic similarity between an MWE and its con-stituent words, and hypothesising that where thesimilarity between the constituents of an MWE andthe whole is sufficiently high, the MWE must be ofsimple decomposable type.The particular similarity method we adopt is la-tent semantic analysis, or LSA (Deerwester et al,1990).
LSA allows us to calculate the similaritybetween an arbitrary word pair, offering the advan-tage of being able to measure the similarity betweenthe MWE and each of its constituent words.
ForMWEs such as house boat, therefore, we can expectto capture the fact that the MWE is highly similar inmeaning to both constituent words (i.e.
the modifierhouse and head noun boat).
More importantly, LSAmakes no assumptions about the lexical or syntac-tic composition of the inputs, and thus constitutes afully construction- and language-inspecific methodof modelling decomposability.
This has clear advan-tages over a more conventional supervised classifier-style approach, where training data would have to becustomised to a particular language and constructiontype.Evaluation is inevitably a difficulty when it comesto the analysis of MWEs, due to the lack of con-cise consistency checks on what MWEs should andshould not be incorporated into dictionaries.
Whilerecognising the dangers associated with dictionary-based evaluation, we commit ourselves to thisparadigm and focus on searching for appropriatemeans of demonstrating the correlation betweendictionary- and corpus-based similarities.The remainder of this paper is structured as fol-lows.
Section 2 describes past research on MWEcompositionality of relevance to this effort.
Sec-tion 3 provides a basic outline of the resources usedin this research, LSA, the MWE extraction methods,and measures used to evaluate our method.
Section 4then provides evaluation of the proposed method,and the paper is concluded with a brief discussionin Section 5.2 Past researchAlthough there has been some useful work on com-positionality in statistical machine translation (e.g.Melamed (1997)), there has been little work on de-tecting ?non-compositional?
(i.e.
non-decomposableand idiosyncratically decomposable) items of vari-able syntactic type in monolingual corpora.
One in-teresting exception is Lin (1999), whose approach isexplained as follows:The intuitive idea behind the method isthat the metaphorical usage of a non-compositional expression causes it tohave a different distributional characteris-tic than expressions that are similar to itsliteral meaning.The expressions he uses are taken from a colloca-tion database (Lin, 1998b).
These ?expressions thatare similar to [their] literal meaning?
are found bysubstituting each of the words in the expression withthe 10 most similar words according to a corpus de-rived thesaurus (Lin, 1998a).
Lin models the dis-tributional difference as a significant difference inmutual information.
Significance here is defined asthe absence of overlap between the 95% confidenceinterval of the mutual information scores.
Lin pro-vides some examples that suggest he has identifieda successful measure of ?compositionality?.
He of-fers an evaluation where an item is said to be non-compositional if it occurs in a dictionary of idioms.This produces the unconvincing scores of 15.7% forprecision and 13.7% for recall.We claim that substitution-based tests are use-ful in demarcating MWEs from productive wordcombinations (as attested by Pearce (2001a) in aMWE detection task), but not in distinguishing thedifferent classes of decomposability.
As observedabove, simple decomposable MWEs such as mo-tor car fail the substitution test not because of non-decomposability, but because the expression is in-stitutionalised to the point of blocking alternates.Thus, we expect Lin?s method to return a wide ar-ray of both decomposable and non-decomposableMWEs.Bannard (2002) focused on distributional tech-niques for describing the meaning of verb-particleconstructions at the level of logical form.
Thesemantic similarity between a multiword expres-sion and its head was used as an indicator ofdecomposability.
The assumption was that if averb-particle was sufficiently similar to its headverb, then the verb contributed its simplex mean-ing.
It gave empirical backing to this assump-tion by showing that annotator judgements for verb-particle decomposability correlate significantly withnon-expert human judgements on the similarity be-tween a verb-particle construction and its head verb.Bannard et al (2003) extended this research in look-ing explicitly at the task of classifying verb-particlesas being compositional or not.
They successfullycombined statistical and distributional techniques(including LSA) with a substitution test in analysingcompositionality.
McCarthy et al (2003) also tar-geted verb-particles for a study on compositionality,and judged compositionality according to the degreeof overlap in the N most similar words to the verb-particle and head verb, e.g., to determine composi-tionality.We are not the first to consider applying LSA toMWEs.
Schone and Jurafsky (2001) applied LSA tothe analysis of MWEs in the task of MWE discov-ery, by way of rescoring MWEs extracted from acorpus.
The major point of divergence from this re-search is that Schone and Jurafsky focused specifi-cally on MWE extraction, whereas we are interestedin the downstream task of semantically classifyingattested MWEs.3 Resources and TechniquesIn this section, we outline the resources used in eval-uation, give an informal introduction to the LSAmodel, sketch how we extracted the MWEs fromcorpus data, and describe a number of methodsfor modelling decomposability within a hierarchicallexicon.3.1 Resources and target MWEsThe particular reference lexicon we use to eval-uate our technique is WordNet 1.7 (Miller etal., 1990), due to its public availability, hier-archical structure and wide coverage.
Indeed,Schone and Jurafsky (2001) provide evidence thatsuggests that WordNet is as effective an evaluationresource as the web for MWE detection methods,despite its inherent size limitations and static nature.Two MWE types that are particularly well repre-sented in WordNet are compound nouns (47,000 en-tries) and multiword verbs (2,600 entries).
Of these,we chose to specifically target two types of MWE:noun-noun (NN) compounds (e.g.
computer net-work, work force) and verb-particles (e.g.
look on,eat up) due to their frequent occurrence in both de-composable and non-decomposable configurations,and also their disparate syntactic behaviours.We extracted the NN compounds from the 1996Wall Street Journal data (WSJ, 31m words), andthe verb-particles from the British National Corpus(BNC, 90m words: Burnard (2000)).
The WSJ datais more tightly domain-constrained, and thus a moresuitable source for NN compounds if we are to ex-pect sentential context to reliably predict the seman-tics of the compound.
The BNC data, on the otherhand, contains more colloquial and prosaic texts andis thus a richer source of verb-particles.3.2 Description of the LSA modelOur goal was to compare the distribution of differ-ent compound terms with their constituent words, tosee if this indicated similarity of meaning.
For thispurpose, we used latent semantic analysis (LSA) tobuild a vector space model in which term-term sim-ilarities could be measured.LSA is a method for representing words as pointsin a vector space, whereby words which are relatedin meaning should be represented by points whichare near to one another, first developed as a methodfor improving the vector model for information re-trieval (Deerwester et al, 1990).
As a technique formeasuring similarity between words, LSA has beenshown to capture semantic properties, and has beenused successfully for recognising synonymy (Lan-dauer and Dumais, 1997), word-sense disambigua-tion (Schu?tze, 1998) and for finding correct transla-tions of individual terms (Widdows et al, 2002).The LSA model we built is similar to that de-scribed in (Schu?tze, 1998).
First, 1000 frequent con-tent words (i.e.
not on the stoplist)1 were chosenas ?content-bearing words?.
Using these content-bearing words as column labels, the 50,000 mostfrequent terms in the corpus were assigned rowvectors by counting the number of times they oc-1A ?stoplist?
is a list of frequent words which have littleindependent semantic content, such as prepositions and deter-miners (Baeza-Yates and Ribiero-Neto, 1999, p167).curred within the same sentence as a content-bearingword.
Singular-value decomposition (Deerwester etal., 1990) was then used to reduce the number ofdimensions from 1000 to 100.
Similarity betweentwo vectors (points) was measured using the cosineof the angle between them, in the same way as thesimilarity between a query and a document is oftenmeasured in information retrieval (Baeza-Yates andRibiero-Neto, 1999, p28).
Effectively, we could useLSA to measure the extent to which two words orMWEs x and y usually occur in similar contexts.Since the corpora had been tagged with parts-of-speech, we could build syntactic distinctions into theLSA models ?
instead of just giving a vector forthe string test we were able to build separate vec-tors for the nouns, verbs and adjectives test.
Thiscombination of technologies was also used to goodeffect by Widdows (2003): an example of the con-tribution of part-of-speech information to extractingsemantic neighbours of the word fire is shown inTable 1.
As can be seen, the noun fire (as in thesubstance/element) and the verb fire (mainly usedto mean firing some sort of weapon) are related toquite different areas of meaning.
Building a singlevector for the string fire confuses this distinction ?the neighbours of fire treated just as a string includewords related to both the meaning of fire as a noun(more frequent in the BNC) and as a verb.
The ap-propriate granularity of syntactic classifications is anopen question for this kind of research: treating allthe possible verbs categories as different (e.g.
dis-tinguishing infinitive from finite from gerund forms)led to data sparseness, and instead we considered?verb?
as a single part-of-speech type.3.3 MWE extraction methodsNN compounds were extracted from the WSJ byfirst tagging the data with fnTBL 1.0 (Ngai and Flo-rian, 2001) and then simply taking noun bigrams(adjoined on both sides by non-nouns to assure thebigram is not part of a larger compound nominal).Out of these, we selected those compounds that arelisted in WordNet, resulting in 5,405 NN compoundtypes (208,000 tokens).Extraction of the verb-particles was consider-ably more involved, and drew on the method ofBaldwin and Villavicencio (2002).
Essentially, weused a POS tagger and chunker (both built usingfnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tagthe BNC.
This allowed us to extract verb-particle to-kens through use of the particle POS and chunk tagsreturned by the two systems.
This produces high-precision, but relatively low-recall results, so weperformed the additional step of running a chunk-based grammar over the chunker output to detectcandidate mistagged particles.
In the case that anoun phrase followed the particle candidate, we per-formed attachment disambiguation to determine thetransitivity of the particle candidate.
These threemethods produced three distinct sets of verb-particletokens, which we carried out weighted voting overto determine the final set of verb-particle tokens.
Atotal of 461 verb-particles attested in WordNet wereextracted (160,765 tokens).For both the NN compound and verb-particledata, we replaced each token occurrence with asingle-word POS-tagged token to feed into the LSAmodel.3.4 Techniques for evaluating correlation withWordNetIn order to evaluate our approach, we employed thelexical relations as defined in the WordNet lexicalhierarchy (Miller et al, 1990).
WordNet groupswords into sets with similar meaning (known as?synsets?
), e.g.
{car, auto, automobile, machine,motorcar } .
These are organised into a hierarchyemploying multiple inheritance.
The hierarchy isstructured according to different principles for eachof nouns, verbs, adjectives and adverbs.
The nounsare arranged according to hyponymy or ISA rela-tions, e.g.
a car is a kind of automobile.
The verbsare arranged according to troponym or ?manner-of?relations, where murder is a manner of killing, sokill immediately dominates murder in the hierarchy.We used WordNet for evaluation by way of look-ing at: (a) hyponymy, and (b) semantic distance.Hyponymy provides the most immediate way ofevaluating decomposability.
With simple decompos-able MWEs, we can expect the constituents (andparticularly the head) to be hypernyms (ancestornodes) or synonyms of the MWE.
That is, simpledecomposable MWEs are generally endocentric, al-though there are some exceptions to this generali-sation such as vice president arguably not being ahyponym of president.
No hyponymy relation holdswith non-decomposable or idiosyncratically decom-posable MWEs (i.e., they are exocentric), as even ifthe semantics of the head noun can be determinedthrough decomposition, by definition this will notcorrespond to a simplex sense of the word.We deal with polysemy of the constituent wordsand/or MWE by simply looking for the exis-tence of a sense of the constituent words whichfire (string only) fire nn1 fire vvifire 1.000000 fire nn1 1.000000 fire vvi 1.000000flames 0.709939 flames nn2 0.700575 guns nn2 0.663820smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000arson 0.528909 arson nn1 0.522844 volley nn1 0.458717accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523blast 0.488617 burning vvg 0.502052 alert aj0 0.443610guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886Table 1: Semantic neighbours of fire with different parts-of-speech.
The scores are cosine similaritiessubsumes a sense of the MWE.
The functionhyponym(word i,mwe) thus returns a value of 1 ifsome sense of word i subsumes a sense of mwe , anda value of 0 otherwise.A more proactive means of utilising the WordNethierarchy is to derive a semantic distance based onanalysis of the relative location of senses in Word-Net.
Budanitsky and Hirst (2001) evaluated the per-formance of five different methods that measurethe semantic distance between words in the Word-Net Hierarchy, which Patwardhan et al (2003) havethen implemented and made available for generaluse as the Perl package distance-0.11.2 We fo-cused in particular on the following three measures,the first two of which are based on information the-oretic principles, and the third on sense topology:?
Resnik (1995) combined WordNet with corpusstatistics.
He defines the similarity betweentwo words as the information content of thelowest superordinate in the hierarchy, definingthe information content of a concept c (wherea concept is the WordNet class containing theword) to be the negative of its log likelihood.This is calculated over a corpus of text.?
Lin (1998c) also employs the idea of corpus-derived information content, and defines thesimilarity between two concepts in the follow-ing way:sim(C1, C2) =2 log P (C0)log P (C1) + log P (C2)(1)where C0 is the lowest class in the hierarchythat subsumes both classes.2http://www.d.umn.edu/?tpederse/distance.html?
Hirst and St-Onge (1998) use a system of ?re-lations?
of different strength to determine thesimilarity of word senses, conditioned on thetype, direction and relative distance of edgesseparating them.The Patwardhan et al (2003) implementation thatwe used calculates the information values fromSemCor, a semantically tagged subset of the Browncorpus.
Note that the first two similarity measuresoperate over nouns only, while the last can be ap-plied to any word class.The similarity measures described above calcu-late the similarity between a pair of senses.
In thecase that a given constituent word and/or MWE oc-cur with more than one sense, we calculate a similar-ity for sense pairing between them, and average overthem to produce a consolidated similarity value.4 EvaluationLSA was used to build models in which MWEscould be compared with their constituent words.Two models were built, one from the WSJ corpus(indexing NN compounds) and one from the BNC(indexing verb-particles).
After removing stop-words, the 50,000 most frequent terms were indexedin each model.
From the WSJ, these 50,000 termsincluded 1,710 NN compounds (with corpus fre-quency of at least 13) and from the BNC, 461 verb-particles (with corpus frequency of at least 49).We used these models to compare different words,and to find their neighbours.
For example, the neigh-bours of the simplex verb cut and the verb-particlescut out and cut off (from the BNC model) are shownin Table 2.
As can be seen, several of the neighboursof cut out are from similar semantic areas as thoseof cut, whereas those of cut off are quite different.cut (verb) cut out (verb) cut off (verb)cut verb 1.000000 cut out verb 1.000000 cut off verb 1.000000trim verb 0.529886 fondant nn 0.516956 knot nn 0.448871slash verb 0.522370 fondant jj 0.501266 choke verb 0.440587cut nns 0.520345 strip nns 0.475293 vigorously rb 0.438071cut nn 0.502100 piece nns 0.449555 suck verb 0.413003reduce verb 0.465364 roll nnp 0.440769 crush verb 0.412301cut out verb 0.433465 stick jj 0.434082 ministry nn 0.408702pull verb 0.431929 cut verb 0.433465 glycerol nn 0.395148fall verb 0.426111 icing nn 0.432307 tap verb 0.383932hook verb 0.419564 piece nn 0.418780 shake verb 0.381581recycle verb 0.413206 paste nn 0.416581 jerk verb 0.381284project verb 0.401246 tip nn 0.413603 put down verb 0.380368recycled jj 0.396315 hole nns 0.412813 circumference nn 0.378097prune verb 0.395656 straw nn 0.411617 jn nnp 0.375634pare verb 0.394991 hook nn 0.402947 pump verb 0.373984tie verb 0.392964 strip nn 0.399974 nell nnp 0.373768Table 2: Semantic neighbours of the verbs cut, cut out, and cut off .Construction Method Pearson R2Resnik .108 .012NN compound Lin .101 .010HSO .072 .005verb-particle HSO .255 .065Table 3: Correlation between LSA and WordNetsimilaritiesThis reflects the fact that in most of its instances theverb cut off is used to mean ?forcibly isolate?.In order to measure this effect quantitatively, wecan simply take the cosine similarities between theseverbs, finding that sim(cut, cut out) = 0.433 andsim(cut, cut off) = 0.183 from which we infer di-rectly that, relative to the sense of cut, cut out is aclearer case of a simple decomposable MWE thancut off .4.1 Statistical analysisIn order to get an initial feel for how wellthe LSA-based similarities for MWEs and theirhead words correlate with the WordNet-basedsimilarities over those same word pairs, wedid a linear regression and Pearson?s correla-tion analysis of the paired data (i.e.
the pair-ing ?simLSA(word i,mwe), simWN(word i,mwe)?for each WordNet similarity measure simWN).
Forboth tests, values closer to 0 indicate random distri-bution of the data, whereas values closer to 1 indi-cate a strong correlation.
The correlation results forNN compounds and verb-particles are presented inTable 3, where R2 refers to the output of the linearregression test and HSO refers to Hirst and St-Ongesimilarity measure.
In the case of NN compounds,the correlation with LSA is very low for all tests,that is LSA is unable to reproduce the relative sim-ilarity values derived from WordNet with any reli-0.10.20.30.40.50.60.70.80.911  2  3MeanHyponymyPartition No.VPC(head)VPC(head)       ALLHIGHNN(mod)NN(head)       ALLALLNN(head)         LOWNN(head)          HIGHVPC(head)         LOWFigure 1: Hyponymy correlationability.
With verb-particles, correlation is notablyhigher than for NN compounds,3 but still at a lowlevel.Based on these results, LSA would appear tocorrelate poorly with WordNet-based similarities.However, our main interest is not in similarity perse, but how reflective LSA similarities are of the de-composability of the MWE in question.
While tak-ing note of the low correlation with WordNet simi-larities, therefore, we move straight on to look at thehyponymy test.4.2 Hyponymy-based analysisWe next turn to analysis of correlation between LSAsimilarities and hyponymy values.
Our expectationis that for constituent word?MWE pairs with higherLSA similarities, there is a greater likelihood of theMWE being a hyponym of the constituent word.
Wetest this hypothesis by ranking the constituent word?MWE pairs in decreasing order of LSA similarity,3Recall that HSO is the only similarity measure which oper-ates over verbs.and partitioning the ranking up into m partitions ofequal size.
We then calculate the average number ofhyponyms per partition.
If our hypothesis is correct,the earlier partitions (with higher LSA similarities)will have higher occurrences of hyponyms than thelatter partitions.Figure 1 presents the mean hyponymy valuesacross partitions of the NN compound data and verb-particle data, with m set to 3 in each case.
For theNN compounds, we derive two separate rankings,based on the similarity between the head noun andNN compound (NN(head)) and the modifier nounand the NN compound (NN(mod)).
In the case ofthe verb-particle data, WordNet has no classificationof prepositions or particles, so we can only calcu-late the similarity between the head verb and verb-particle (VPC(head)).
Looking to the curves forthese three rankings, we see that they are all fairlyflat, nondescript curves.
If we partition the data upinto low- and high-frequency MWEs, as defined by athreshold of 100 corpus occurrences, we find that thegraphs for the low-frequency data (NN(head)LOWand VPC(head)LOW) are both monotonically de-creasing, whereas those for high-frequency data(NN(head)HIGH and VPC(head)HIGH) are more hap-hazard in nature.
Our hypothesis of lesser instancesof hyponymy for lower similarities is thus supportedfor low-frequency items but not for high-frequencyitems, suggesting that LSA similarities are morebrittle over high-frequency items for this particu-lar task.
The results for the low-frequency itemsare particularly encouraging given that the LSA-based similarities were found to correlate poorlywith WordNet-derived similarities.
The results forNN(mod) are more erratic for both low- and high-frequency terms, that is the modifier noun is not asstrong a predictor of decomposability as the headnoun.
This is partially supported by the statistics onthe relative occurrence of NN compounds in Word-Net subsumed by their head noun (71.4%) as com-pared to NN compounds subsumed by their modifier(13.7%).In an ideal world, we would hope that the val-ues for mean hyponymy were nearly 1 for the firstpartition and nearly 0 for the last.
Naturally, thispresumes perfect correlation of the LSA similaritieswith decomposability, but classificational inconsis-tencies in WordNet alo work against us.
For ex-ample, vice chairman is an immediate hyponym ofboth chairman and president, but vice president isnot a hyponym of president.
According to LSA,however, sim(chairman, vice chairman) = .508 andsim(president, vice president) = .551.It remains to be determined why LSA should per-form better over low-frequency items, although thehigher polysemy of high-frequency items is one po-tential cause.
We intend to further investigate thismatter in future research.5 DiscussionWhile evaluation pointed to a moderate correlationbetween LSA similarities and occurrences of hy-ponymy, we have yet to answer the question ofexactly where the cutoffs between simple decom-posable, idiosyncratically decomposable and non-decomposable MWEs lie.
While it would be pos-sible to set arbitrary thresholds to artificially parti-tion up the space of MWEs based on LSA similarity(or alternatively use statistical tests to derive confi-dence intervals for similarity values), we feel thatmore work needs to be done in establishing exactlywhat different LSA similarities for different MWE?constituent word combinations mean.One area in which we plan to extend this researchis the analysis of MWEs in languages other thanEnglish.
Because of LSA?s independence from lin-guistic constraints, it is equally applicable to all lan-guages, assuming there is some way of segmentinginputs into constituent words.To summarise, we have proposed a construction-inspecific empirical model of MWE decomposabil-ity, based on latent semantic analysis.
We evaluatedthe method over English NN compounds and verb-particles, and showed it to correlate moderately withWordNet-based hyponymy values.AcknowledgementsThis material is partly based upon work supported by the Na-tional Science Foundation under Grant No.
BCS-0094638 andalso the Research Collaboration between NTT CommunicationScience Laboratories, Nippon Telegraph and Telephone Corpo-ration and CSLI, Stanford University.
We would like to thankthe anonymous reviewers for their valuable input on this re-search.ReferencesRicardo Baeza-Yates and Berthier Ribiero-Neto.
1999.
ModernInformation Retrieval.
Addison Wesley / ACM press.Timothy Baldwin and Aline Villavicencio.
2002.
Extractingthe unextractable: A case study on verb-particles.
In Proc.
ofthe 6th Conference on Natural Language Learning (CoNLL-2002), Taipei, Taiwan.Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003.A statistical approach to the semantics of verb-particles.
InProc.
of the ACL-2003 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment.
(this volume).Colin Bannard.
2002.
Statistical techniques for automati-cally inferring the semantics of verb-particle constructions.LinGO Working Paper No.
2002-06.Alexander Budanitsky and Graeme Hirst.
2001.
Semantic dis-tance in WordNet: An experimental, application-orientedevaluation of five measures.
In Workshop on Wordnet andOther Lexical Resources, Second meeting of the NAACL,Pittsburgh, USA.Lou Burnard.
2000.
User Reference Guide for the British Na-tional Corpus.
Technical report, Oxford University Comput-ing Services.Nicoletta Calzolari, Charles Fillmore, Ralph Grishman, NancyIde, Alessandro Lenci, Catherine MacLeod, and AntonioZampolli.
2002.
Towards best practice for multiword ex-pressions in computational lexicons.
In Proceedings of theThird International Conference on Language Resources andEvaluation (LREC 2002), pages 1934?40, Las Palmas, Ca-nary Islands.Ann Copestake, Fabre Lambeau, Aline Villavicencio, FrancisBond, Timothy Baldwin, Ivan A.
Sag, and Dan Flickinger.2002.
Multiword expressions: Linguistic precision andreusability.
In Proc.
of the 3rd International Conferenceon Language Resources and Evaluation (LREC 2002), pages1941?7, Las Palmas, Canary Islands.Scott Deerwester, Susan Dumais, George Furnas, Thomas Lan-dauer, and Richard Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society for In-formation Science, 41(6):391?407.Martin Haspelmath.
2002.
Understanding Morphology.Arnold Publishers.Graeme Hirst and David St-Onge.
1998.
Lexical chains asrepresentations of context for the detection and correctionof malapropism.
In Christiane Fellbaum, editor, WordNet:An Electronic Lexical Database, pages 305?32.
MIT Press,Cambridge, USA.Thomas Landauer and Susan Dumais.
1997.
A solution toPlato?s problem: The latent semantic analysis theory of ac-quisition.
Psychological Review, 104(2):211?240.Dekang Lin.
1998a.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of the 36th Annual Meeting of theACL and 17th International Conference on ComputationalLinguistics (COLING/ACL-98).Dekang Lin.
1998b.
Extracting collocations from text corpora.In First Workshop on Computational Terminology.Dekang Lin.
1998c.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th International Confer-ence on Machine Learning.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proc.
of the 37th Annual Meetingof the ACL, pages 317?24, College Park, USA.Diana McCarthy, Bill Keller, and John Carroll.
2003.
Detectinga continuum of compositionality in phrasal verbs.
In Proc.
ofthe ACL-2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment.
(this volume).I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Proc.
of the2nd Conference on Empirical Methods in Natural LanguageProcessing (EMNLP-97), Providence, USA.George A. Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine J. Miller.
1990.
Introductionto WordNet: an on-line lexical database.
International Jour-nal of Lexicography, 3(4):235?44.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd Annual Meetingof the North American Chapter of Association for Compu-tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,USA.Geoffrey Nunberg, Ivan A.
Sag, and Tom Wasow.
1994.
Id-ioms.
Language, 70:491?538.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.2003.
Using measures of semantic relatedness for wordsense disambiguation.
In Proc.
of the 4th International Con-ference on Intelligent Text Processing and ComputationalLinguistics (CICLing-2003), Mexico City, Mexico.Darren Pearce.
2001a.
Synonymy in collocation extraction.
InProc.
of the NAACL 2001 Workshop on WordNet and OtherLexical Resources: Applications, Extensions and Customiza-tions, Pittsburgh, USA.Darren Pearce.
2001b.
Using conceptual similarity for collo-cation extraction.
In Proc.
of the 4th UK Special InterestGroup for Computational Linguistics (CLUK4).Philip Resnik.
1995.
Using information content to evaluatesemantic similarity.
In Proceedings of the 14th InternationalJoint Conference on Artificial Intelligence.Susanne Riehemann.
2001.
A Constructional Approach to Id-ioms and Word Formation.
Ph.D. thesis, Stanford.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copestake,and Dan Flickinger.
2002.
Multiword expressions: A pain inthe neck for NLP.
In Proc.
of the 3rd International Confer-ence on Intelligent Text Processing and Computational Lin-guistics (CICLing-2002), pages 1?15, Mexico City, Mexico.Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-freeinduction of multiword unit dictionary headwords a solvedproblem?
In Proc.
of the 6th Conference on Empirical Meth-ods in Natural Language Processing (EMNLP 2001), pages100?108.Hinrich Sch u?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.Dominic Widdows, Beate Dorow, and Chiu-Ki Chan.
2002.Using parallel corpora to enrich multilingual lexical re-sources.
In Third International Conference on Language Re-sources and Evaluation, pages 240?245, Las Palmas, Spain,May.Dominic Widdows.
2003.
Unsupervised methods for develop-ing taxonomies by combining syntactic and statistical infor-mation.
In Proc.
of the 3rd International Conference on Hu-man Language Technology Research and 4th Annual Meet-ing of the NAACL (HLT-NAACL 2003).
(to appear).
