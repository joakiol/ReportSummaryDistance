Proceedings of NAACL HLT 2007, pages 572?579,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsComputing Semantic Similarity between Skill Statements for Approxi-mate MatchingFeng Pan Robert G. FarrellUSC Information Sciences Institute IBM T. J. Watson Research CenterMarina del Rey, CA 90292 Hawthorne, NY 10532pan@isi.edu robfarr@us.ibm.comAbstractThis paper explores the problem of com-puting text similarity between verbphrases describing skilled human behav-ior for the purpose of finding approximatematches.
Four parsers are evaluated on alarge corpus of skill statements extractedfrom an enterprise-wide expertise taxon-omy.
A similarity measure utilizing com-mon semantic role features extracted fromparse trees was found superior to an in-formation-theoretic measure of similarityand comparable to the level of humanagreement.1 IntroductionKnowledge-intensive industries need to becomemore efficient at deploying the right expertise asquickly and smoothly as possible, thus it is desiredto have systems that can quickly match and deployskilled individuals to meet customer needs.
Thesearches in most of the current matching systemsare based on exact matches between skill state-ments.
However, exact matching is very likely tomiss individuals who are very good matches to thejob but didn?t select the exact skills that appearedin the open job description.It is always hard for individuals to find the per-fect skills to describe their skill sets.
For example,an individual might not know whether to choose askill stating that refers to ?maintaining?
a givenproduct or ?supporting?
it or whether to choose askill about maintaining a ?database?
or aboutmaintaining ?DB2?.
Thus, it is desirable for the jobsearch system to be able to find approximatematches, instead of only exact matches, betweenavailable individuals and open job positions.
Morespecifically, a skill similarity computation isneeded to allow searches to be expanded to relatedskills, and return more potential matches.In this paper, we present our work on develop-ing a skill similarity computation based upon se-mantic commonalities between skill statements.Although there has been much work on text simi-larity metrics (Lin, 1998a; Corley and Mihalcea,2005), most approaches treat texts as a bag ofwords and try to find shared words with certainstatistical properties based on corpus frequencies.As a result, the structural information in the text isignored in these approaches.
We will describe anew semantic approach that takes the structuralinformation of the text into consideration andmatches skill statements on corresponding seman-tic roles.
We will demonstrate that it can outper-form standard statistical text similarity techniques,and reach the level of human agreement.In Section 2, we first describe the skill state-ments we extracted from an enterprise-wide exper-tise taxonomy.
In Section 3, we describe theperformance of a standard statistical approach onthis task.
This motivates our semantic approach ofmatching skill statements on corresponding seman-tic roles.
We also compare and evaluate the per-formance of four natural language parsers (theCharniak parser, the Stanford parser, the ESGparser, and MINIPAR) for the purpose of our task.An inter-rater agreement study and evaluation of572our approach will be presented in Section 4.
Weend with a discussion and conclusion.2 Skill StatementsAn expertise taxonomy is a standardized, enter-prise-wide language and structure to describe jobrole requirements and people capabilities (skillsets) across a corporation.
In the taxonomy we util-ize for this study, skills are associated with jobroles.
The taxonomy has 10667 skills.
Each skillhas a title, for example, ?Advise BAAN eBusinessASP.?
We refer to this title as the skill statement.The official taxonomy update policies requirethat skill statements be verb phrases using one of18 valid skill verbs (e.g., Advise, Architect, Code,Design, Implement, Sell, and Support).3 Computing Semantic Similarities be-tween Skill StatementsIn this section, we first explain a statistical infor-mation-theoretic approach we used as a baseline,and show examples of how it performs for ourtask.
The error analysis of this approach motivatesour semantic approach that takes the structural in-formation of the text into consideration.
In the re-mainder of this section, we describe how weextract semantic role information from the syntac-tic parse trees of the skill statements.
Four naturallanguage parsers are compared and evaluated forthe purpose of our task.3.1 Statistical ApproachIn order to compute semantic similarities betweenskill statements, we first adopted one of the stan-dard statistical approaches to the problem of com-puting text similarities based on Lin?s information-theoretic similarity measure (Lin 1998a).
Lin de-fined the commonality between A and B as)),(( BAcommonIwhere common(A, B) is a proportion that states thecommonalities between A and B and where theamount of information in proposition s is)(log)( sPsI ?=The similarity between A and B is then defined asthe ratio between the amount of informationneeded to state the commonality of A and B andthe information needed to fully describe A and B:)),((log)),((log),(BAndescriptioPBAcommonPBASim =In order to compute common(A,B) and descrip-tion(A,B), we use standard bag-of-words features,i.e., unigram features -- the frequency of wordscomputed from the entire corpus of the skill state-ments.
Thus common(A,B) is the unigrams thatboth skill statements share, and description(A,B) isthe union of the unigrams from both skill state-ments.The words are stemmed first so that the wordswith the same root (e.g., managing & manage-ment) can be found as commonalities between twoskill statements.
A stop-word list is also used sothat the commonly used words in most of the docu-ments (e.g., the, a) are not used as features.
A for-mal evaluation of this approach will be presentedin Section 4 where the similarity between 75 pairsof skill statements will be evaluated against humanjudgments, but we discuss some examples here.In order to see how to improve Lin?s statisticalsimilarity measure, we examine sample skill state-ment pairs which achieve high similarity scoresfrom Lin?s measure but were rated consistently asdissimilar by human subjects in our evaluation.Here are two examples:1.
Advise Business Knowledge of CAD function-ality for FEMAdvise on Business Knowledge of Process forFEM2.
Advise on Money MarketAdvise on Money Center BankingIn these two examples, although many words areshared between the two pairs of skill statements(Advise Business Knowledge of ... for FEM for thefirst pair; Advise on Money for the second pair),they are not similar to human judges.
We conjec-ture that this judgment of dissimilarity is due to thedifferences between the key components of theskill statements (CAD functionality vs.
Process inthe first pair; Money Market vs. Money CenterBanking in the second pair).This kind of error is common for most statisticalapproaches to the problem, where common infor-mation is computed without considering the struc-tural information in the text.
From the aboveexamples, we can see that the similarity computa-tion would be more accurate if the verb phrasesmatch on corresponding semantic roles, instead of573matching words from any location in the skillstatements.
By identifying semantic roles, we canprovide more weights to those semantic roles criti-cal for our task, i.e., the key components of theskill statements.3.2 Identifying and Assigning SemanticRolesThe following example shows the kind of semanticroles we want to be able to identify and assign.
[action Apply] [theme Knowledge of [concept IBM E-business Middleware]] to [purpose PLM Solu-tions]In this example, ?Apply?
is the ?action?
of theskill; ?Knowledge of IBM E-business Middle-ware?
is the ?theme?
of the skill, where the ?con-cept?
semantic role (IBM E-business Middleware)specifies the key component of the skill require-ment and is the most important role for skillmatching; ?PLM Solutions?
is the ?purpose?
of theskill.Our goal was to extract all such semantic rolepatterns for all the skill statements, and match oncorresponding semantic roles.
Although there ex-ists some automatic semantic role taggers (Gildeaand Jurafsky, 2002; Giuglea and Moschitti, 2006),most of them were trained on PropBank (Palmeret.
al., 2005) and/or FrameNet (Johnson et.
al.,2003), and perform much worse in other corpora(Pradhan et.
al., 2004).
Our corpus is from a verydifferent domain (information technology) andthere are many domain-specific terms in the skillstatements, such as product names, companynames, and company-specific nomenclature forproduct offerings.
Given this, we would expectpoor performance from these automatic semanticrole taggers.
Moreover, the semantic role informa-tion we need to extract is more detailed and deeperthan most of the automatic semantic role taggerscan identify and extract (e.g., the ?concept?
roleembedded within the ?theme?
role).We developed a specialized parser that extractssemantic role patterns from each of the 18 skillverbs.
This semantic role parser can achieve amuch higher performance than the general-purposesemantic role taggers.
The inputs needed for thesemantic role parser are syntactic parse trees gen-erated by a natural language parse of the originalskill statements.3.3 Preprocessing for ParsingWe first used the Charniak parser (2000) to parsethe original skill statements.
However, among allthe 10667 skill statements, 1217 were not parsed asverb phrases, leading to very poor performance.After examining the error cases, we found that ab-breviations are used widely in the skill statements.For example,Advise Solns Supp Bus Proc Reeng for E&EEng ProcsThese abbreviations made the system unable todetermine the part of speech of some words, result-ing in incorrect parses.
Thus, the first step of thepreprocessing was to expand abbreviations.There were 225 valid abbreviations alreadyidentified by the expertise taxonomy team.
How-ever, we found many abbreviations that appearedin the skill statements but were not listed there.Since most abbreviations are not words found in adictionary, in order to find the abbreviations thatappear frequently in the skill statements, we firstfound all the words in the skill statements thatwere not in WordNet (Miller, 1990).
We thenranked them based on their frequencies, and manu-ally identified high frequency abbreviations.
Usingthis approach, we added another 187 abbreviationsto the list (a total of 412).From the error cases, we also found that manywords were mistagged as proper nouns, For exam-ple, ?Technically?
inAdvise Technically for Simulationwas parsed as a proper noun.
We realized the rea-son for this error was that all the words, except forprepositions, are capitalized in the original state-ments and the parser tends to tag them as propernouns.
To solve this problem, we changed all thecapitalized words to lower case, except for the firstword and the acronyms (words that have all letterscapitalized, e.g., IBM).
After applying these twosteps of preprocessing, we parsed the skill state-ments again.
This time, more than 200 additionalskill statements were parsed as verb phrases afterthe preprocessing.When we examined the error cases moreclosely, we found the errors occur mostly when theskill verbs can be both a noun and a verb (e.g., de-sign, plan).
In those cases, the parser may parse theentire statement as one noun phrase, instead of averb phrase.
In order to disambiguate such cases,574we added a subject (?Employees?)
to all the skillstatements to convert them into full sentences.
Af-ter applying this additional step of preprocessing,we parsed the skill statements again.
This time,only 28 skill statements were not parsed as sen-tences containing verb phrases, a significant im-provement.
The remaining errors were due to theuse of some words as skill verbs, e.g., ?architect?1,not recognized as verbs by the parser.3.4 Parser Evaluation and ComparisonWhile the Charniak parser performed well in ourinitial verb phrase (VP) test, we decided to com-pare the Charniak parser?s performance with otherparsers.
For this evaluation, we compared it withthe Stanford parser, the ESG parser, andMINIPAR.The Stanford parser (Klein and Manning,2003) is an unlexicalized statistical syntactic parserthat was trained on the same corpus as theCharniak parser (the Penn TreeBank).
Its parse treehas the same structure as the Charniak parser.The ESG (English Slot Grammar) parser(McCord, 1980) is a rule-based parser based on theslot grammar where each phrase has a head anddependent elements, and is also marked with a syn-tactic role.MINIPAR (Lin, 1998b), as a dependencyparser, is very similar to the ESG parser in terms ofits output.
It represents sentence structures as a setof dependency relationships between head words.Since our purpose is to use the syntactic parsesas inputs to extract semantic role patterns, the cor-rectness of the bracketing of the parses and thesyntactic labels of the phrases (e.g., NP, VP, andPP) are the most important information for our pur-poses, whereas the POS (Part-Of-Speech) labels ofindividual words (e.g., nouns vs. proper nouns) arenot that important (also, there are too many do-main-specific terms in our data).
Thus, our evalua-tion of the parses is only on the correctness of thebracketing and the syntactic labels of the phrases,not the correctness of the entire parse.
For our task,the correctness of the prepositional phrase attach-ment is especially important for extracting accuratesemantic role patterns (Gildea and Jurafsky, 2002).For example, for the sentence1 ?Architect?
has no verb sense in WordNet and many otherdictionaries, but it does have a verb sense in the Oxford Eng-lish Dictionary (http://dictionary.oed.com/).Apply Knowledge of IBM E-business Middle-ware to PLM Solutions.the correct bracketing should beApply [Knowledge [of [IBM E-business Mid-dleware]]] [to [PLM Solutions]].Thus the parser needs to correctly attach ?of IBME-business Middleware?
to ?Knowledge?
and at-tach ?to PLM Solutions?
to ?Apply?, not ?Knowl-edge?.To evaluate the performance of the parsers, werandomly picked 100 skill statements from our cor-pus, preprocessed them, and then parsed them us-ing the four different parsers.
We then evaluatedthe parses using the above evaluation measures.The parses were rated as correct or incorrect.
Nopartial score was given.
Figure 1 shows the evalua-tion results.
The error analysis reveals four majorsources of error for all the parsers, most of whichare specific to the domain we are working on:(1) Many domain specific terms and acronyms.For example, ?SAP?
in ?Employees advise onSAP R/3 logistics basic data.?
was alwaystagged as a verb by the parsers.
(2) Many long noun phrases.
For example, ?Em-ployees perform JD edwards foundation suiteaddress book.?
(3) Some specialized use of punctuation.
For ex-ample, ?Employees perform business transpor-tation consultant-logistics.sys.?
(4) Prepositional phrase attachment can be diffi-cult.
For example, in ?Employees apply IBMinfrastructure knowledge for IDBS?, ?forIDBS?
should attach to ?apply?, but manyparsers mistakenly attach it to ?IBM infrastruc-ture knowledge?.0%10%20%30%40%50%60%70%80%Charniak Stanford ESG MINIPARFigure 1.
An Evaluation of Four Parsers on theTask of Parsing Human Skill-related Verb PhrasesWe noticed that MINIPAR performed muchworse compared with the other parsers.
The main575reason is that it always parses the phrase ?VERBknowledge of Y?
(e.g., ?Employees apply knowl-edge of web technologies.?)
incorrectly -- the parseresult always mistakenly attaches ?of Y?
(e.g., ofweb technologies) to the VERB (e.g., apply), not?knowledge?.
Since there were so many of phrasesin the test set and in the corpus, this kind of errorsignificantly reduced the performance for our task.These kinds of errors on prepositional phrase at-tachment in MINIPAR were also mentioned in(Pantel and Lin, 2000).From the evaluation and comparison results wecan see that the Charniak parser performs the bestfor our task among all the four parsers.
This resultis consistent with a more thorough evaluation(Swanson and Gordon, 2006) on a different corpuswith a set of different target verbs, which showedthe Charniak parser performed the best amongthree parsers (including the Stanford parser andMINPAR) for labeling semantic roles.
We notethat although the ESG parser performed a littleworse than the Charniak parser, its parses containmuch richer syntactic (e.g., subject, object) andsemantic (e.g., word senses) slot-filling informa-tion, which can be very useful to many natural lan-guage applications.3.5 Extracted Semantic Role PatternsFrom the parse trees generated by the Charniakparser, we first automatically extracted patterns foreach of the 18 skill verbs (e.g., ?Advise on NP forNP?
), and then we manually identified the seman-tic roles.
For example, the semantic role patternsidentified for the skill verb ?Advise?
are:?
Advise [Theme] (for [Purpose])?
Advise (technically) on/about [Theme] (for[Purpose])?
Advise clients/customers/employees/userson/regarding [Theme]The corpus also contains embedded sub-semantic-role patterns, for example, for the ?Theme?
role weextracted the following sub-patterns:?
(application) knowledge of/for [Concept]?
sales of [Concept]?
(technical) implementation of [Concept]We have extracted and identified a total of 74 suchsemantic role patterns from the skill statements.4 EvaluationIn order to evaluate the two approaches (semanticrole parsing and statistical) to computing semanticsimilarity of skill statements in our domain, wefirst conducted an experiment to evaluate how hu-mans agree on this task, which also provides uswith an upper bound accuracy for the task.4.1 Inter-Rater Agreement and UpperBound AccuracyTo assess inter-rater agreement, we randomly se-lected 75 skill pairs from the expertise taxonomy.Since random pairs of verbs would have little or nosimilarity, we selected skill pairs that share thesame job role, or same secondary or primary jobcategory, or from across the entire expertise taxon-omy.These 75 skill pairs are then given to three ratersto independently judge their similarities on a 5point scale from 1 as very similar to 5 as very dis-similar.
Since this 5 point scale is very fine-grained, we also converted the judgments to amore coarse-grained measure -- binary judgment: 1and 2 count as similar; 3-5 as not similar.The metric we used is the kappa statistic (Car-letta, 1996), which factors out the agreement that isexpected by chance:)(1)()(EPEPAP?
?=?where P(A) is the observed agreement among theraters, and P(E) is the expected agreement, i.e., theprobability that the raters agree by chance.Since the judgment on the 5 point scale is ordi-nal data, the weighted kappa statistic is used totake the distance of disagreement into considera-tion (e.g., the disagreement between 1 and 2 issmaller than that between 1 and 5).The inter-rater agreement results for both thefine-grained and coarse-grained judgments areshown in Table 1.
In general, a kappa value above0.80 represents perfect agreement, 0.60-0.80 repre-sents significant agreement, 0.40-0.60 representsmoderate agreement, and 0.20-0.40 is fair agree-ment (Chklovski and Mihalcea, 2003).
We can seethat the agreement on the fine-grained judgment ismoderate, whereas the agreement on the coarse-grained (binary) judgment is significant.Fine-Grained  Coarse-GrainedKappa 0.412 0.602Table 1.
Inter-Rater Agreement Results.576From the inter-rater agreement evaluation, wecan also get an upper bound accuracy for our task,i.e., human agreement without factoring out theagreement expected by chance (i.e., P(A) in thekappa statistic).
The average P(A) for the coarse-grained (binary) judgment is 0.81, and that consti-tutes the upper bound accuracy for our task.4.2 Evaluation of the Statistical ApproachWe use the 75 skill pairs as test data to evaluateour semantic similarity approach against humanjudgments.
Considering the reliability of the data,only the coarse-grained (binary) judgments areused.
The gold standard is obtained by majorityvoting from the three raters, i.e., for a given skillpair, if two or more raters judge it as similar, thenthe gold standard answer is ?similar?, otherwise itis ?not similar?.We first evaluated Lin?s statistical approach de-scribed in Section 3.1.
Among 75 skill pairs, 53 ofthem were rated correctly according to the humanjudgments, that is, 70.67% accuracy.
The erroranalysis shows that many of the errors can be cor-rected if the skills are matched on their correspond-ing semantic roles.
We then evaluated the utility ofthe extracted semantic role information to seewhether it can outperform the statistical approach.4.3 Evaluation of Semantic Role MatchingApproachFor simplicity, we will only report on evaluatingsemantic role matching on the "concept" role thatspecifies the key component of the skills, as intro-duced in Section 3.2.There are at least two straightforward ways ofperforming semantic role matching for the skillsimilarity computation: 1) match on the entire se-mantic role; 2) match on the head nouns only.
Butboth have their drawbacks: the first approach is toostrict and will miss many similar skill statements;the second approach may not only miss the similarskill statements, e.g.,Perform [Web Services Planning]2Perform [Web Services Assessment]but also misclassify dissimilar ones as similar, e.g.,2 The ?concept?
role is identified with brackets, and the headnouns are italic.Advise about [Async Transfer Mode (ATM)Solutions]Advise about [CTI Solutions]In order to solve these problems, we used a simplematching criterion from Tversky (1977).
The simi-larity of two texts t1 and t2 is determined by:Similarity(t1, t2) =2121tand in t features  total#) tand between t featurescommon  (#  2?This equation states that two texts are similar ifshared features are a large percentage of the totalfeatures.
We set a threshold of 0.5, requiring that atleast 50% of the features be shared.
We apply thiscriterion to the text contained in the ?concept?
role.The words in the calculation are preprocessedfirst: abbreviations are expanded, stop-words areexcluded (e.g., the and of don't count as sharedwords), and the remaining words are stemmed(e.g., manager and management are counted asshared words), as was done in our previous infor-mation-theoretic approach.
Words connected bypunctuation (e.g., e-business, software/hardware)are treated as separate words.
For example,Advise on [Field/Force Management] for Tele-comApply Knowledge of [Basic Field Force Auto-mation]The shared words between the two ?concept?
roles(bracketed) are ?Field?
and ?Force?, and theirshared percentage is (2*2)/7 = 57.14% > 50%, sothey are similar.We have also evaluated this approach on our testset with the 75 skill pairs.
Among 75 skill pairs, 60of them were rated correctly (i.e., 80% accuracy),which significantly outperforms the statistical ap-proach, and is very close to the upper bound accu-racy, i.e., human agreement (81%), as shown inFigure 2.64.00%66.00%68.00%70.00%72.00%74.00%76.00%78.00%80.00%82.00%Lin's Information-Theoretic Metric Semantic Role Matching Human AgreementFigure 2.
Evaluation on Semantic Similarity be-tween Skill Statements577The difference between this approach and Lin?sinformation content approach is that this computa-tion is local -- no corpus statistics is used.
Also,using this approach, it is easier to set an intuitivethreshold (e.g., 50%) for a classification problem(e.g., similar or not for our task).
With this ap-proach, however, there are also cases that aremistagged as similar, for example,Apply Knowledge of [Basic Field Force Auto-mation]Advise on [Sales Force Automation]Although ?Field Force Automation?
and ?SalesForce Automation?
seem similar on their surfaceform, they are two quite different concepts.
Deeperdomain knowledge (such as an ontology) is neededto distinguish such cases.5 DiscussionWe have also investigated several approaches toimproving the semantic role text similarity meas-ure we described.
One approach is to also considersimilarities between skill verbs.
In this example:Implement Domino Mail ManagerDevelop for Domino Mail Manageralthough the key components of the skill state-ments (Domino Mail Manager) are the same, theirskill verbs are different (implement vs. developfor).
The skills required for ?implementing?
a sys-tem or software product are usually different fromthose required for ?developing for?
the same sys-tem or software product.
This example shows thata semantic similarity computation between skillverbs is required to distinguishing such cases.Many approaches to the problem ofword/concept similarities are based on taxonomies,e.g., WordNet.
The simplest approach is to countthe number of nodes on the shortest path betweentwo concepts in the taxonomy (Quillian, 1972).The fewer nodes on the path, the more similar thetwo concepts are.
The assumption for this shortestpath approach is that the links in the taxonomy rep-resent uniform distances.
However, in most tax-onomies, sibling concepts deep in the taxonomyare usually more closely related than those higherup.
Different approaches have been proposed todiscount the depth of the concepts to overcome theproblem.
Budanitsky and Hirst (2006) thoroughlyevaluated six of the approaches (Hirst and St-Onge, Leacock and Chodorow, Jiang and Conrath,Lin, Resnik, Wu and Palmer), and found that Jiangand Conrath (1997) was superior to the other ap-proaches based on their evaluation experiments.For our task, we compared two approaches tocomputing skill verb similarities: shortest path vs.Jiang and Conrath.
Since the words are comparedbased on their specific senses, we first manuallyassigned one most appropriate sense for each of the18 skill verbs from WordNet.
We then used thelibrary developed by Pedersen et al (2004) tocompute their similarity scores.Table 2 shows the top nine pairs of skill verbswith the highest similarity scores from the two ap-proaches.
We can see that the two approachesagree on the top four pairs, but disagree on the restin the list.
One intuitive example is the pair ?Lead?and ?Manage?
which is ranked the 5th by the Jiangand Conrath approach but ranked the 46th by theshortest path approach.
It seems that the Jiang andConrath approach matches better with our humanintuition for this example.
While we didn?t com-pare these results with human performance, in gen-eral most of the similar skill verb pairs listed in thetable don?t look very similar for our domain.
Thismay be due to the fact that WordNet is a general-purpose taxonomy -- although we have alreadyselected the most appropriate sense for each verb,their relationship represented in the taxonomy maystill be quite different from the relationship in ourdomain.
A domain-specific taxonomy for skillverbs may improve the performance.
The otherreason may be due to the structure of WordNet?sverb taxonomy, as mentioned in (Resnik and Diab,2000), which is considerably wider and shallowerthan WordNet?s noun taxonomy.
A different verblexicon, e.g., VerbNet (Kipper et al, 2000), can beexplored.Shortest Path Jiang and ConrathApply Use Apply UseDesign Plan Design PlanApply Implement Apply ImplementImplement Use Implement UseAnalyze  Apply Lead ManageAnalyze Perform Apply SupportAnalyze Support Support UseAnalyze Use Apply SellPerform Support Sell Use?
?
?
?Table 2.
Top Similar Skill Verb Pairs5786 ConclusionIn this paper, we have presented our work on a se-mantic similarity computation for skill statementsin natural language.
We compared and evaluatedfour different natural language parsers for our task,and matched skills on their corresponding semanticroles extracted from the parse trees generated byone of these parsers.
The evaluation results showedthat the skill similarity computation based on se-mantic role matching can outperform a standardstatistical approach and reach the level of humanagreement.The extracted semantic role information can alsobe incorporated into the standard statistical ap-proaches as additional features.
One way is to givehigher weights to those semantic role featuresdeemed most important.
This approach hasachieved a high performance for a text categoriza-tion task when combining extracted keywords withthe full text (Hulth and Megyesi, 2006).We have shown that good results can beachieved for a domain-specific text matching taskby performing a simple word-based feature com-parison on corresponding structural elements oftexts.
We have shown that the structural elementsof importance can be identified by domain-specificpattern analysis of corresponding parse trees.
Webelieve this approach can generalize to other do-mains where phrases, sentences, or other shorttexts need to be compared.AcknowledgementsThe majority of this work was performed while thefirst author was a summer intern at IBM T. J. Wat-son Research Center in Hawthorne, NY.
Thanks toYael Ravin and Jennifer Lai for supporting thiswork, Brian White for his help on the software,Michael McCord for assistance with the IBM ESGparser, and the IBM Expertise Taxonomy team forletting us use their data.ReferencesA.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based Measures of Lexical Semantic Relatedness.Computational Linguistics.
32(1):13-47.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2):249?254.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL.T.
Chklovski and R. Mihalcea.
2003.
Exploiting Agree-ment and Disagreement of Human Annotators forWord Sense Disambiguation.
In Proceedings ofRANLP.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3):245 ?
288.A.
Giuglea and A. Moschitti.
2006.
Semantic Role La-beling via FrameNet, VerbNet and PropBank.
InProceedings of COLING-ACL.A.
Hulth and B.
B. Megyesi.
2006.
A Study on Auto-matically Extracted Keywords in Text Categoriza-tion.
In Proceedings of COLING-ACL.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of ROCLING X.C.
Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-penhofer, and C. Fillmore.
2003.
Framenet: Theoryand practice.
Berkeley, California.K.
Kipper, H. T. Dang, M. Palmer.
2000.
Class-BasedConstruction of a Verb Lexicon.
In Proceedings ofAAAI.D.
Klein and C. D. Manning.
2003.
Accurate Unlexical-ized Parsing.
In Proceedings of ACL.D.
Lin.
1998a.
An information-theoretic definition ofsimilarity.
In Proceedings of ICML.D.
Lin.
1998b.
Dependency-based evaluation ofMINIPAR.
In Proceedings of the Workshop at LRECon The Evaluation of Parsing Systems.M.
C. McCord.
1980.
Slot grammars.
ComputationalLinguistics, 6: 31-43.G.
A. Miller.
1990.
WordNet: an On-line Lexical Data-base.
International Journal of Lexicography 3(4).M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).P.
Pantel and D. Lin.
2000.
An unsupervised approachto prepositional phrase attachment using contextuallysimilar words.
In Proceedings of ACL.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.Wordnet::similarity - measuring the relatedness ofconcepts.
In Proceedings of AAAI, Intelligent Sys-tems Demonstration.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, and D.Jurafsky.
2004.
Shallow Semantic Parsing usingSupport Vector Machines.
In Proceedings ofHLT/NAACL.M.
R. Quillian.
1972.
Semantic Memory, Semantic In-formation Processing.
Semantic information process-ing, Cambridge.P.
Resnik and M. Diab.
2000.
Measuring verb similar-ity.
In Proceedings of COGSCI.R.
Swanson and A. S. Gordon.
2006.
A Comparison ofAlternative Parse Tree Paths for Labeling SemanticRoles.
In Proceedings of COLING/ACL.A.
Tversky.
1977.
Features of Similarity, PsychologicalReview, vol.
84, no.
4, pages 327-352.579
