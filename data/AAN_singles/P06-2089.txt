Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691?698,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Best-First Probabilistic Shift-Reduce ParserKenji Sagae and Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213{sagae,alavie}@cs.cmu.eduAbstractRecently proposed deterministic classifier-based parsers (Nivre and Scholz, 2004;Sagae and Lavie, 2005; Yamada and Mat-sumoto, 2003) offer attractive alternativesto generative statistical parsers.
Determin-istic parsers are fast, efficient, and sim-ple to implement, but generally less ac-curate than optimal (or nearly optimal)statistical parsers.
We present a statis-tical shift-reduce parser that bridges thegap between deterministic and probabilis-tic parsers.
The parsing model is essen-tially the same as one previously usedfor deterministic parsing, but the parserperforms a best-first search instead of agreedy search.
Using the standard sec-tions of the WSJ corpus of the Penn Tree-bank for training and testing, our parserhas 88.1% precision and 87.8% recall (us-ing automatically assigned part-of-speechtags).
Perhaps more interestingly, the pars-ing model is significantly different fromthe generative models used by other well-known accurate parsers, allowing for asimple combination that produces preci-sion and recall of 90.9% and 90.7%, re-spectively.1 IntroductionOver the past decade, researchers have devel-oped several constituent parsers trained on an-notated data that achieve high levels of accu-racy.
Some of the more popular and more ac-curate of these approaches to data-driven parsing(Charniak, 2000; Collins, 1997; Klein and Man-ning, 2002) have been based on generative mod-els that are closely related to probabilistic context-free grammars.
Recently, classifier-based depen-dency parsing (Nivre and Scholz, 2004; Yamadaand Matsumoto, 2003) has showed that determin-istic parsers are capable of high levels of accu-racy, despite great simplicity.
This work has led tothe development of deterministic parsers for con-stituent structures as well (Sagae and Lavie, 2005;Tsuruoka and Tsujii, 2005).
However, evaluationson the widely used WSJ corpus of the Penn Tree-bank (Marcus et al, 1993) show that the accuracyof these parsers still lags behind the state-of-the-art.A reasonable and commonly held assumption isthat the accuracy of deterministic classifier-basedparsers can be improved if determinism is aban-doned in favor of a search over a larger space ofpossible parses.
While this assumption was shownto be true for the parser of Tsuruoka and Tsu-jii (2005), only a moderate improvement resultedfrom the addition of a non-greedy search strategy,and overall parser accuracy was still well belowthat of state-of-the-art statistical parsers.We present a statistical parser that is based ona shift-reduce algorithm, like the parsers of Sagaeand Lavie (2005) and Nivre and Scholz (2004), butperforms a best-first search instead of pursuing asingle analysis path in deterministic fashion.
Theparser retains much of the simplicity of determin-istic classifier-based parsers, but achieves resultsthat are closer in accuracy to state-of-the-art statis-tical parsers.
Furthermore, a simple combinationof the shift-reduce parsing model with an existinggenerative parsing model produces results with ac-curacy that surpasses any that of any single (non-reranked) parser tested on the WSJ Penn Tree-bank, and comes close to the best results obtainedwith discriminative reranking (Charniak and John-691son, 2005).2 Parser DescriptionOur parser uses an extended version of the basicbottom-up shift-reduce algorithm for constituentstructures used in Sagae and Lavie?s (2005) de-terministic parser.
For clarity, we will first de-scribe the deterministic version of the algorithm,and then show how it can be extended into a proba-bilistic algorithm that performs a best-first search.2.1 A Shift-Reduce Algorithm forDeterministic Constituent ParsingIn its deterministic form, our parsing algorithmis the same single-pass shift-reduce algorithm asthe one used in the classifer-based parser of Sagaeand Lavie (2005).
That algorithm, in turn, is sim-ilar to the dependency parsing algorithm of Nivreand Scholz (2004), but it builds a constituent treeand a dependency tree simultaneously.
The al-gorithm considers only trees with unary and bi-nary productions.
Training the parser with arbi-trary branching trees is accomplished by a sim-ple procedure to transform those trees into treeswith at most binary productions.
This is doneby converting each production with n children,where n > 2, into n ?
1 binary productions.This binarization process is similar to the one de-scribed in (Charniak et al, 1998).
Additional non-terminal nodes introduced in this conversion mustbe clearly marked.
Transforming the parser?s out-put into arbitrary branching trees is accomplishedusing the reverse process.The deterministic parsing algorithm involvestwo main data structures: a stack S, and a queueW .
Items in S may be terminal nodes (part-of-speech-tagged words), or (lexicalized) subtrees ofthe final parse tree for the input string.
Items in Ware terminals (words tagged with parts-of-speech)corresponding to the input string.
When parsingbegins, S is empty and W is initialized by insert-ing every word from the input string in order, sothat the first word is in front of the queue.The algorithm defines two types of parser ac-tions, shift and reduce, explained below:?
Shift: A shift action consists only of remov-ing (shifting) the first item (part-of-speech-tagged word) from W (at which point thenext word becomes the new first item), andplacing it on top of S.?
Reduce: Reduce actions are subdivided intounary and binary cases.
In a unary reduction,the item on top of S is popped, and a newitem is pushed onto S. The new item consistsof a tree formed by a non-terminal node withthe popped item as its single child.
The lex-ical head of the new item is the same as thelexical head of the popped item.
In a binaryreduction, two items are popped from S insequence, and a new item is pushed onto S.The new item consists of a tree formed by anon-terminal node with two children: the firstitem popped from S is the right child, and thesecond item is the left child.
The lexical headof the new item may be the lexical head of itsleft child, or the lexical head of its right child.If S is empty, only a shift action is allowed.
IfW is empty, only a reduce action is allowed.
Ifboth S and W are non-empty, either shift or re-duce actions are possible, and the parser must de-cide whether to shift or reduce.
If it decides to re-duce, it must also choose between a unary-reduceor a binary-reduce, what non-terminal should be atthe root of the newly created subtree to be pushedonto the stack S, and whether the lexical head ofthe newly created subtree will be taken from theright child or the left child of its root node.
Fol-lowing the work of Sagae and Lavie, we considerthe complete set of decisions associated with a re-duce action to be part of that reduce action.
Pars-ing terminates when W is empty and S containsonly one item, and the single item in S is the parsetree for the input string.2.2 Shift-Reduce Best-First ParsingA deterministic shift-reduce parser based on thealgorithm described in section 2.1 does not handleambiguity.
By choosing a single parser action ateach opportunity, the input string is parsed deter-ministically, and a single constituent structure isbuilt during the parsing process from beginning toend (no other structures are even considered).A simple extension to this idea is to eliminatedeterminism by allowing the parser to choose sev-eral actions at each opportunity, creating differentpaths that lead to different parse trees.
This is es-sentially the difference between deterministic LRparsing (Knuth, 1965) and Generalized-LR pars-ing (Tomita, 1987; Tomita, 1990).
Furthermore,if a probability is assigned to every parser action,the probability of a parse tree can be computed692simply as the product of the probabilities of eachaction in the path that resulted in that parse tree(the derivation of the tree).
This produces a prob-abilistic shift-reduce parser that resembles a gen-eralized probabilistic LR parser (Briscoe and Car-roll, 1993), where probabilities are associated withan LR parsing table.
In our case, although thereis no LR table, the action probabilities are associ-ated with several aspects of the current state of theparser, which to some extent parallel the informa-tion contained in an LR table.
Instead of havingan explicit LR table and pushing LR states ontothe stack, the state of the parser is implicitly de-fined by the configurations of the stack and queue.In a way, there is a parallel between how mod-ern PCFG-like parsers use markov grammars asa distribution that is used to determine the proba-bility of any possible grammar rules, and the waya statistical model is used in our parser to assigna probability to any transition of parser states (in-stead of a symbolic LR table).Pursuing every possible sequence of parser ac-tions creates a very large space of actions foreven moderately sized sentences.
To find the mostlikely parse tree efficiently according to the prob-abilistic shift-reduce parsing scheme described sofar, we use a best-first strategy.
This involves anextension of the deterministic shift-reduce algo-rithm into a best-first shift-reduce algorithm.
Todescribe this extension, we first introduce a newdata structure Ti that represents a parser state,which includes a stack Si and a queue Wi.
Inthe deterministic algorithm, we would have a sin-gle parser state T that contains S and W .
Thebest-first algorithm, on the other hand, has a heapH containing multiple parser states T1 ... Tn.These states are ordered in the heap according totheir probabilities, so that the state with the highestprobability is at the top.
State probabilities are de-termined by multiplying the probabilities of eachof the actions that resulted in that state.
Parser ac-tions are determined from and applied to a parserstate Ti popped from the top of H .
The parseractions are the same as in the deterministic ver-sion of the algorithm.
When the item popped fromthe top of the heap H contains a stack Si with asingle item and an empty queue (in other words,meets the acceptance criteria for the determinis-tic version of the algorithm), the item on top ofSi is the tree with the highest probability.
At thatpoint, parsing terminates if we are searching forthe most probable parse.
To obtain a list of n-bestparses, we simply continue parsing once the firstparse tree is found, until either n trees are found,or H is empty.We note that this approach does not use dy-namic programming, and relies only on the best-first search strategy to arrive at the most prob-able parse efficiently.
Without any pruning ofthe search space, the distribution of probabilitymass among different possible actions for a parsestate has a large impact on the behavior of thesearch.
We do not use any normalization to ac-count for the size (in number of actions) of dif-ferent derivations when calculating their probabili-ties, so it may seem that shorter derivations usuallyhave higher probabilities than longer ones, causingthe best-first search to approximate a breadth-firstsearch in practice.
However, this is not the case iffor a given parser state only a few actions (or, ide-ally, only one action) have high probability, and allother actions have very small probabilities.
In thiscase, only likely derivations would reach the top ofthe heap, resulting in the desired search behavior.The accuracy of deterministic parsers suggest thatthis may in fact be the types of probabilities a clas-sifier would produce given features that describethe parser state, and thus the context of the parseraction, specifically enough.
The experiments de-scribed in section 4 support this assumption.2.3 Classifier-Based Best-First ParsingTo build a parser based on the deterministic al-gorithm described in section 2.1, a classifier isused to determine parser actions.
Sagae and Lavie(2005) built two deterministic parsers this way,one using support vector machines, and one usingk-nearest neighbors.
In each case, the set of fea-tures and classes used with each classifier was thesame.
Items 1 ?
13 in figure 1 shows the featuresused by Sagae and Lavie.
The classes producedby the classifier encode every aspect of a parseraction.
Classes have one of the following forms:SHIFT : represents a shift action;REDUCE-UNARY-XX : represents a unary re-duce action, where the root of the new sub-tree pushed onto S is of type XX (where XXis a non-terminal symbol, typically NP , V P ,PP , for example);REDUCE-LEFT-XX : represents a binary re-duce action, where the root of the new sub-693tree pushed onto S is of non-terminal typeXX.
Additionally, the head of the new subtreeis the same as the head of the left child of theroot node;REDUCE-RIGHT-XX : represents a binary re-duce action, where the root of the new sub-tree pushed onto S is of non-terminal typeXX.
Additionally, the head of the new sub-tree is the same as the head of the right childof the root node.To implement a parser based on the best-first al-gorithm, instead of just using a classifier to de-termine one parser action given a stack and aqueue, we need a classification approach that pro-vides us with probabilities for different parser ac-tions associated with a given parser state.
Onesuch approach is maximum entropy classification(Berger et al, 1996), which we use in the formof a library implemented by Tsuruoka1 and usedin his classifier-based parser (Tsuruoka and Tsujii,2005).
We used the same classes and the same fea-tures as Sagae and Lavie, and an additional featurethat represents the previous parser action appliedthe current parser state (figure 1).3 Related WorkAs mentioned in section 2, our parsing approachcan be seen as an extension of the approach ofSagae and Lavie (2005).
Sagae and Lavie eval-uated their deterministic classifier-based parsingframework using two classifiers: support vectormachines (SVM) and k-nearest neighbors (kNN).Although the kNN-based parser performed poorly,the SVM-based parser achieved about 86% preci-sion and recall (or 87.5% using gold-standard POStags) on the WSJ test section of the Penn Tree-bank, taking only 11 minutes to parse the test set.Sagae and Lavie?s parsing algorithm is similar tothe one used by Nivre and Scholz (2004) for de-terministic dependency parsing (using kNN).
Ya-mada and Matsumoto (2003) have also presenteda deterministic classifier-based (SVM-based) de-pendency parser, but using a different parsing al-gorithm, and using only unlabeled dependencies.Tsuruoka and Tsujii (2005) developed aclassifier-based parser that uses the chunk-parsingalgorithm and achieves extremely high parsingspeed, but somewhat low recall.
The algorithm1The SS MaxEnt library is publicly available fromhttp://www-tsujii.is.s.u-tokyo.ac.jp/ tsuruoka/maxent/.is based on reframing the parsing task as severalsequential chunking tasks.Finally, our parser is in many ways similar tothe parser of Ratnaparkhi (1997).
Ratnaparkhi?sparser uses maximum-entropy models to deter-mine the actions of a parser based to some extenton the shift-reduce framework, and it is also capa-ble of pursuing several paths and returning the top-n highest scoring parses for a sentence.
However,in addition to using different features for parsing,Ratnaparkhi?s parser uses a different, more com-plex algorithm.
The use of a more involved algo-rithm allows Ratnaparkhi?s parser to work with ar-bitrary branching trees without the need of the bi-narization transform employed here.
It breaks theusual reduce actions into smaller pieces (CHECKand BUILD), and uses two separate passes (notincluding the part-of-speech tagging pass) for de-termining chunks and higher syntactic structuresseparately.
Instead of keeping a stack, the parsermakes multiple passes over the input string, likethe dependency parsing algorithm used by Ya-mada and Matsumoto.
Our parser, on the otherhand, uses a simpler stack-based shift-reduce (LR-like) algorithm for trees with only unary and bi-nary productions.4 ExperimentsWe evaluated our classifier-based best-first parseron the Wall Street Journal corpus of the Penn Tree-bank (Marcus et al, 1993) using the standard split:sections 2-21 were used for training, section 22was used for development and tuning of parame-ters and features, and section 23 was used fortesting.
Every experiment reported here was per-formed on a Pentium4 3.2GHz with 2GB of RAM.Each tree in the training set had empty-node andfunction tag information removed, and the treeswere lexicalized using the same head-table rules asin the Collins (1999) parser (these rules were takenfrom Bikel?s (2002) implementation of the Collinsparser).
The trees were then converted into treescontaining only unary and binary productions, us-ing the binarization transform described in section2.
Classifier training instances of features pairedwith classes (parser actions) were extracted fromthe trees in the training set, and the total numberof training instances was about 1.9 million.
It is in-teresting to note that the procedure of training thebest-first parser is identical to the training of a de-terministic version of the parser: the deterministic694Let:S(n) denote the nth item from the top of the stack S, andW (n) denote the nth item from the front of the queue W .Features:1.
The head-word (and its POS tag) of: S(0), S(1), S(2), andS(3)2.
The head-word (and its POS tag) of: W (0), W (1), W (2) and W (3)3.
The non-terminal node of the root of: S(0), and S(1)4.
The non-terminal node of the left child of the root of: S(0), and S(1)5.
The non-terminal node of the right child of the root of: S(0), and S(1)6.
The POS tag of the head-word of the left child of the root of: S(0), andS(1)7.
The POS tag of the head-word of the right child of the root of: S(0),and S(1)8.
The linear distance (number of words apart) between the head-words ofS(0) and S(1)9.
The number of lexical items (words) that have been found (so far) tobe dependents of the head-words of: S(0), and S(1)10.
The most recently found lexical dependent of the head-word of S(0)that is to the left of S(0)?s head11.
The most recently found lexical dependent of the head-word of S(0)that is to the right of S(0)?s head12.
The most recently found lexical dependent of the head-word of S(1)that is to the left of S(1)?s head13.
The most recently found lexical dependent of the head-word of S(1)that is to the right of S(1)?s head14.
The previous parser action applied to the current parser stateFigure 1: Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005).
Thefeatures described in items 1 ?
7 are more directly related to the lexicalized constituent trees that are builtduring parsing, while the features described in items 8 ?
13 are more directly related to the dependencystructures that are built simultaneously to the constituent structures.695algorithm is simply run over all sentences in thetraining set, and since the correct trees are knownin advance, we can simply record the features andcorrect parser actions that lead to the constructionof the correct tree.Training the maximum entropy classifier withsuch a large number (1.9 million) of training in-stances and features required more memory thanwas available (the maximum training set size wewere able to train with 2GB of RAM was about200,000 instances), so we employed the trainingset splitting idea used by Yamada and Matsumoto(2003) and Sagae and Lavie (2005).
In our case,we split the training data according to the part-of-speech (POS) tag of the head-word of the itemon top of the stack, and trained each split of thetraining data separately.
At run-time, every trainedclassifier is loaded, and the choice of classifierto use is made by looking at the head-word ofthe item on top of the stack in the current parserstate.
The total training time (a single machinewas used and each classifier was trained in se-ries) was slightly under nine hours.
For compar-ison, Sagae and Lavie (2005) report that train-ing support vector machines for one-against-allmulti-class classification on the same set of fea-tures for their deterministic parser took 62 hours,and training a k-nearest neighbors classifier took11 minutes.When given perfectly tagged text (gold part-of-speech tags extracted from the Penn Treebank),our parser has labeled constituent precision and re-call of 89.40% and 88.79% respectively over allsentences in the test set, and 90.01% and 89.32%over sentences with length of at most 40 words.These results are at the same level of accuracy asthose obtained with other state-of-the-art statisti-cal parsers, although still well below the best pub-lished results for this test set (Bod, 2003; Char-niak and Johnson, 2005).
Although the parser isquite accurate, parsing the test set took 41 minutes.By implementing a very simple pruning strategy,the parser can be made much faster.
Pruning thesearch space is done by only adding a new parserstate to the heap if its probability is greater than1/b of the probability of the most likely state inthe heap that has had the same number of parseractions.
By setting b to 50, the parser?s accuracyis only affected minimally, and we obtain 89.3%precision and 88.7% recall, while parsing the testset in slightly under 17 minutes and taking lessthan 60 megabytes of RAM.
Under the same con-ditions, but using automatically assigned part-of-speech tags (at 97.1% accuracy) using the SVM-Tool tagger (Gimenez and Marquez, 2004), weobtain 88.1% precision and 87.8% recall.
It islikely that the deterioration in accuracy is aggra-vated by the training set splitting scheme based onPOS tags.A deterministic version of our parser, obtainedby simply taking the most likely parser action asthe only action at each step (in other words, by set-ting b to 1), has precision and recall of 85.4% and84.8%, respectively (86.5% and 86.0% using gold-standard POS tags).
More interestingly, it parsesall 2,416 sentences (more than 50,000 words) inonly 46 seconds, 10 times faster than the deter-ministic SVM parser of Sagae and Lavie (2005).The parser of Tsuruoka and Tsujii (Tsuruoka andTsujii, 2005) has comparable speed, but we obtainmore accurate results.
In addition to being fast,our deterministic parser is also lean, requiring onlyabout 25 megabytes of RAM.A summary of these results is shown in table 1,along with the results obtained with other parsersfor comparison purposes.
The figures shown intable 1 only include experiments using automat-ically assigned POS tags.
Results obtained withgold-standard POS tags are not shown, since theyserve little purpose in a comparison with existingparsers.
Although the time figures reflect the per-formance of each parser at the stated level of ac-curacy, all of the search-based parsers can tradeaccuracy for increased speed.
For example, theCharniak parser can be made twice as fast at thecost of a 0.5% decrease in precision/recall, or tentimes as fast at the cost of a 4% decrease in preci-sion/recall (Roark and Charniak, 2002).4.1 Reranking with the ProbabililsticShift-Reduce ModelOne interesting aspect of having an accurate pars-ing model that is significantly different from otherwell-known generative models is that the com-bination of two accurate parsers may produceeven more accurate results.
A probabilistic shift-reduce LR-like model, such as the one used inour parser, is different in many ways from a lex-icalized PCFG-like model (using markov a gram-mar), such as those used in the Collins (1999)and Charniak (2000) parsers.
In the probabilis-tic LR model, probabilities are assigned to tree696Precision Recall F-score Time (min)Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17Deterministic (MaxEnt) (this paper) 85.4 84.8 85.1 < 1Charniak & Johnson (2005) 91.3 90.6 91.0 UnkBod (2003) 90.8 90.7 90.7 145*Charniak (2000) 89.5 89.6 89.5 23Collins (1999) 88.3 88.1 88.2 39Ratnaparkhi (1997) 87.5 86.3 86.9 UnkTsuruoka & Tsujii (2005): deterministic 86.5 81.2 83.8 < 1*Tsuruoka & Tsujii (2005): search 86.8 85.0 85.9 2*Sagae & Lavie (2005) 86.0 86.1 86.0 11*Table 1: Summary of results on labeled precision and recall of constituents, and time required to parsethe test set.
We first show results for the parsers described here, then for four of the most accurate ormost widely known parsers, for the Ratnaparkhi maximum entropy parser, and finally for three recentclassifier-based parsers.
For the purposes of direct comparisons, only results obtained with automaticallyassigned part-of-speech tags are shown (tags are assigned by the parser itself or by a separate part-of-speech tagger).
* Times reported by authors running on different hardware.derivations (not the constituents themselves) basedon the sequence of parser shift/reduce actions.PCFG-like models, on the other hand, assign prob-abilities to the trees directly.
With models that dif-fer in such fundamental ways, it is possible thatthe probabilities assigned to different trees are in-dependent enough that even a very simple combi-nation of the two models may result in increasedaccuracy.We tested this hypothesis by using the Char-niak (2000) parser in n-best mode, producing thetop 10 trees with corresponding probabilities.
Wethen rescored the trees produced by the Charniakparser using our probabilistic LR model, and sim-ply multiplied the probabilities assigned by theCharniak model and our LR model to get a com-bined score for each tree2.
On development datathis resulted in a 1.3% absolute improvement in f-score over the 1-best trees produced by the Char-niak parser.
On the test set (WSJ Penn Treebanksection 23), this reranking scheme produces preci-sion of 90.9% and recall of 90.7%, for an f-scoreof 90.8%.2The trees produced by the Charniak parser may includethe part-of-speech tags AUX and AUXG, which are not partof the original Penn Treebank tagset.
See (Charniak, 2000)for details.
These are converted deterministically into the ap-propriate Penn Treebank verb tags, possibly introducing asmall number of minor POS tagging errors.
Gold-standardtags or the output of a separate part-of-speech tagger are notused at any point in rescoring the trees.5 ConclusionWe have presented a best-first classifier-basedparser that achieves high levels of precision andrecall, with fast parsing times and low memory re-quirements.
One way to view the parser is as anextension of recent work on classifier-based deter-ministic parsing.
It retains the modularity betweenparsing algorithms and learning mechanisms asso-ciated with deterministic parsers, making it simpleto understand, implement, and experiment with.Another way to view the parser is as a variant ofprobabilistic GLR parsers without an explicit LRtable.We have shown that our best-first strategy re-sults in significant improvements in accuracy overdeterministic parsing.
Although the best-firstsearch makes parsing slower, we have imple-mented a beam strategy that prunes much of thesearch space with very little cost in accuracy.
Thisstrategy involves a parameter that can be used tocontrol the trade-off between accuracy and speed.At one extreme, the parser is very fast (more than1,000 words per second) and still moderately ac-curate (about 85% f-score, or 86% using gold-standard POS tags).
This makes it possible toapply parsing to natural language tasks involv-ing very large amounts of text (such as question-answering or information extraction with largecorpora).
A less aggressive pruning setting resultsin an f-score of about 88% (or 89%, using gold-standard POS tags), taking 17 minutes to parse theWSJ test set.697Finally, we have shown that by multiplying theprobabilities assigned by our maximum entropyshift-reduce model to the probabilities of the 10-best trees produced for each sentence by the Char-niak parser, we can rescore the trees to obtainmore accurate results than those produced by ei-ther model in isolation.
This simple combinationof the two models produces an f-score of 90.8%for the standard WSJ test set.AcknowledgementsWe thank John Carroll for insightful discussions atvarious stages of this work, and the reviewers fortheir detailed comments.
This work was supportedin part by the National Science Foundation undergrant IIS-0414630.ReferencesA.
Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.D.
Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceed-ings of HLT2002.
San Diego, CA.R.
Bod.
2003.
An efficient implementation of a newdop model.
In Proceedings of the European chapterof the 2003 meeting of the Association for Computa-tional Linguistics.
Budapest, Hungary.E.
Briscoe and J. Carroll.
1993.
Generalised proba-bilistic lr parsing of natural language (corpora) withunification-based grammars.
Computational Lin-guistics, 19(1):25?59.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd meeting ofthe Association for Computational Linguistics.
AnnArbor, MI.Eugene Charniak, Sharon Goldwater, and Mark John-son.
1998.
Edge-based best-first chart parsing.
InProceedings of the Sixth Workshop on Very LargeCorpora.
Montreal, Canada.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the First Meet-ing of the North American Chapter of the Associa-tion for Computational Linguistics, pages 132?139.Seattle, WA.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics, pages 16?23.M.
Collins.
1999.
Head-Driven Models for NaturalLanguage Parsing.
Phd thesis, University of Penn-sylvania.J.
Gimenez and L. Marquez.
2004.
Svmtool: A gen-eral pos tagger generator based on support vectormachines.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation.Lisbon, Portugal.Dan Klein and Christopher D. Manning.
2002.
Fastexact inference with a factored model for naturallanguage parsing.
In Advances in Neural Informa-tion Processing Systems 15 (NIPS 2002).
Vancouver,BC.D.
E. Knuth.
1965.
On the translation of lan-guages from left to right.
Information and Control,8(6):607?639.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewics.1993.
Building a large annotated corpus of english:The penn treebank.
Computational Linguistics, 19.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of english text.
In Proceedingsof the 20th International Conference on Computa-tional Linguistics, pages 64?70.
Geneva, Switzer-land.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In Proceedings of the Second Conference on Empir-ical Methods in Natural Language Processing.
Prov-idence, RI.B.
Roark and E. Charniak.
2002.
Measuring effi-ciency in high-accuracy, broad coverage statisticalparsing.
In Proceedings of the Efficiency in Large-scale Parsing Systems Workshop at COLING-2000.Luxembourg.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnologies.
Vancouver, BC.Masaru Tomita.
1987.
An efficient augmentedcontext-free parsing algorithm.
Computational Lin-guistics, 13:31?46.Masaru Tomita.
1990.
The generalized lrparser/compiler - version 8.4.
In Proceedings ofthe International Conference on Computational Lin-guistics (COLING?90), pages 59?63.
Helsinki, Fin-land.Y.
Tsuruoka and K. Tsujii.
2005.
Chunk parsingrevisited.
In Proceedings of the Ninth Interna-tional Workshop on Parsing Technologies.
Vancou-ver, Canada.H.
Yamada and Yuji Matsumoto.
2003.
Statistical de-pendency analysis using support vector machines.In Proceedings of the Eighth International Work-shop on Parsing Technologies.
Nancy, France.698
