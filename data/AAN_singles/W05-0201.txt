Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,pages 1?8, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Applications of Lexical Information for Algorithmically ComposingMultiple-Choice Cloze ItemsChao-Lin Liu?
Chun-Hung Wang?
Zhao-Ming Gao?
Shang-Ming Huang?
?Department of Computer Science, National Chengchi University, Taipei 11605, Taiwan?Dept.
of Foreign Lang.
and Lit., National Taiwan University, Taipei 10617, Taiwan?chaolin@nccu.edu.tw, ?zmgao@ntu.edu.twABSTRACT1We report experience in applying techniques for nat-ural language processing to algorithmically generat-ing test items for both reading and listening clozeitems.
We propose a word sense disambiguation-based method for locating sentences in which des-ignated words carry specific senses, and apply acollocation-based method for selecting distractorsthat are necessary for multiple-choice cloze items.Experimental results indicate that our system wasable to produce a usable item for every 1.6 items itreturned.
We also attempt to measure distance be-tween sounds of words by considering phonetic fea-tures of the words.
With the help of voice synthe-sizers, we were able to assist the task of compos-ing listening cloze items.
By providing both readingand listening cloze items, we would like to offer asomewhat adaptive system for assisting Taiwanesechildren in learning English vocabulary.1 IntroductionComputer-assisted item generation (CAIG) allowsthe creation of large-scale item banks, and has at-tracted active study in the past decade (Deane andSheehan, 2003; Irvine and Kyllonen, 2002).
Ap-plying techniques for natural language processing(NLP), CAIG offers the possibility of creating alarge number of items of different challenging lev-els, thereby paving a way to make computers moreadaptive to students of different competence.
More-over, with the proliferation of Web contents, onemay search and sift online text files for candidatesentences, and come up with a list of candidate cloze1A portion of results reported in this paper will be expandedin (Liu et al, 2005; Huang et al, 2005).items economically.
This unleashes the topics of thetest items from being confined by item creators?
per-sonal interests.NLP techniques serve to generate multiple-choicecloze items in different ways.
(For brevity, we usecloze items or items for multiple-choice cloze itemshenceforth.)
One may create sentences from scratchby applying template-based methods (Dennis et al,2002) or more complex methods based on some pre-determined principles (Deane and Sheehan, 2003).Others may take existing sentences from a corpus,and select those that meet the criteria for becomingtest items.
The former approach provides specificand potentially well-controlled test items at the costsof more complex systems than the latter, e.g., (Shee-han et al, 2003).
Nevertheless, as the Web providesample text files at our disposal, we may filter thetext sources stringently for obtaining candidate testitems of higher quality.
Administrators can then se-lect really usable items from these candidates at arelatively lower cost.Some researchers have already applied NLP tech-niques to the generation of sentences for multiple-choice cloze items.
Stevens (1991) employs the con-cepts of concordance and collocation for generatingitems with general corpora.
Coniam (1997) relies onfactors such as word frequencies in a tagged corpusfor creating test items of particular types.There are other advanced NLP techniques thatmay help to create test items of higher quality.
Forinstance, many words in English may carry multiplesenses, and test administrators usually want to test aparticular usage of the word in an item.
In this case,blindly applying a keyword matching method, suchas a concordancer, may lead us to a list of irrelevantsentences that would demand a lot of postprocess-1Figure 1: A multiple-choice cloze item for Englishing workload.
In addition, composing a cloze itemrequires not just a useful sentence.Figure 1 shows a multiple-choice item, where wecall the sentence with a gap the stem, the answer tothe gap the key, and the other choices the distrac-tors.
Given a sentence, we still need distractors fora multiple-choice item.
The selection of distractorsaffects the item facility and item discrimination ofthe cloze items (Poel and Weatherly, 1997).
There-fore, the selection of distractors calls for deliberatestrategies, and simple considerations alone, such asword frequencies, may not satisfy the demands.To remedy these shortcomings, we employ thetechniques for word sense disambiguation (WSD)for choosing sentences in which the keys carries spe-cific senses, and utilize the techniques for comput-ing collocations (Manning and Schu?tze, 1999) forselecting distractors.
Results of empirical evaluationshow that our methods could create items of satisfac-tory quality, and we have actually used the generatedcloze items in freshmen-level English classes.For broadening the formats of cloze items, wealso design software that assists teachers to createlistening cloze items.
After we defining a metricfor measuring similarity between pronunciations ofwords, our system could choose distractors for lis-tening cloze items.
This addition opens a door tooffering different challenging levels of cloze items.We sketch the flow of the item generation pro-cess in Section 2, and explain the preparation of thesource corpus in Section 3.
In Section 4, we elab-orate on the application of WSD to selecting sen-tences for cloze items, and, in Section 5, we delveinto the application of collocations to distractor gen-eration.
Results of evaluating the created readingcloze items are presented in Section 6.
We thenoutline methods for creating listening cloze items inSection 7 before making some concluding remarks.2 System ArchitectureFigure 2 shows major steps for creating cloze items.Constrained by test administrator?s specificationsand domain dependent requirements, the SentenceRetriever chooses a candidate sentence from the7DJJHG&RUSXV7DUJHW'HSHQGHQW,WHP5HTXLUHPHQWV,WHP6SHFLILFDWLRQ7DUJHW6HQWHQFH6HQWHQFH5HWULHYHUZLWK:6''LVWUDFWRU*HQHUDWRU&OR]H,WHPFigure 2: Main components of our item generatorTagged Corpus.
Target-Dependent Item Require-ments specify general principles that should be fol-lowed by all items for a particular test.
For example,the number of words in cloze items for College En-trance Examinations in Taiwan (CEET) ranges be-tween 6 and 28 (Liu et al, 2005), and one may wantto follow this tradition in creating drill tests.Figure 3 shows the interface to the Item Specifi-cation.
Through this interface, test administratorsselect the key for the desired cloze item, and specifypart-of-speech and sense of the key that will be usedin the item.
Our system will attempt to create the re-quested number of items.
After retrieving the targetsentence, the Distractor Generator considers suchconstraining factors as word frequencies and collo-cations in selecting the distractors at the second step.Figure 3: Interface for specifying cloze itemsFigure 4 shows a sample output for the specifica-tion shown in Figure 3.
Given the generated items,the administrator may choose and edit the items, andsave the edited items into the item bank.
It is possi-ble to retrieve previously saved items from the itembank, and compile the items for different tests.3 Source Corpus and LexiconsEmploying a web crawler, we retrieve the con-tents of Taiwan Review <publish.gio.gov.tw>, Tai-wan Journal <taiwanjournal.nat.gov.tw>, and ChinaPost <www.chinapost.com.tw>.
Currently, we have127,471 sentences that consist of 2,771,503 wordsin 36,005 types in the corpus.
We look for use-ful sentences from web pages that are encoded inthe HTML format.
We need to extract texts from2Figure 4: An output after Figure 3the mixture of titles, main body of the reports,and multimedia contents, and then segment the ex-tracted paragraphs into individual sentences.
Wesegment sentences with the help of MXTERMINA-TOR (Reynar and Ratnaparkhi, 1997).
We then tok-enize words in the sentences before assigning usefultags to the tokens.We augment the text with an array of tags thatfacilitate cloze item generation.
We assign tags ofpart-of-speech (POS) to the words with MXPOSTthat adopts the Penn Treebank tag set (Ratnaparkhi,1996).
Based on the assigned POS tags, we annotatewords with their lemmas.
For instance, we annotateclassified with classify and classified, respectively,when the original word has VBN and JJ as its POStag.
We also employ MINIPAR (Lin, 1998) to ob-tain the partial parses of sentences that we use exten-sively in our system.
Words with direct relationshipscan be identified easily in the partially parsed trees,and we rely heavily on these relationships betweenwords for WSD.
For easy reference, we will callwords that have direct syntactic relationship with aword W as W ?s signal words or simply signals.Since we focus on creating items for verbs, nouns,adjectives, and adverbs (Liu et al, 2005), we careabout signals of words with these POS tags in sen-tences for disambiguating word senses.
Specifically,the signals of a verb include its subject, object, andthe adverbs that modify the verb.
The signals of anoun include the adjectives that modify the noun andthe verb that uses the noun as its object or predicate.For instance, in ?Jimmy builds a grand building.
?,both ?build?
and ?grand?
are signals of ?building?.The signals of adjectives and adverbs include thewords that they modify and the words that modifythe adjectives and adverbs.When we need lexical information about Englishwords, we resort to electronic lexicons.
We useWordNet <www.cogsci.princeton.edu/?wn/> whenwe need definitions and sample sentences of wordsfor disambiguating word senses, and we employHowNet <www.keenage.com> when we need infor-mation about classes of verbs, nouns, adjectives, andadverbs.HowNet is a bilingual lexicon.
An entry inHowNet includes slots for Chinese words, Englishwords, POS information, etc.
We rely heavily on theslot that records the semantic ingredients related tothe word being defined.
HowNet uses a limited setof words in the slot for semantic ingredient, and theleading ingredient in the slot is considered to be themost important one generally.4 Target Sentence RetrieverThe sentence retriever in Figure 2 extracts qualifiedsentences from the corpus.
A sentence must containthe desired key of the requested POS to be consid-ered as a candidate target sentence.
Having identi-fied such a candidate sentence, the item generatorneeds to determine whether the sense of the key alsomeets the requirement.
We conduct this WSD taskbased on an extended notion of selectional prefer-ences.4.1 Extended Selectional PreferencesSelectional preferences generally refer to the phe-nomenon that, under normal circumstances, someverbs constrain the meanings of other words ina sentence (Manning and Schu?tze, 1999; Resnik,1997).
We can extend this notion to the relation-ships between a word of interest and its signals, withthe help of HowNet.
Let w be the word of interest,and pi be the first listed class, in HowNet, of a signalword that has the syntactic relationship ?
with w.We define the strength of the association of w and pias follows:A?
(w, pi) = Pr?
(w, pi)Pr?
(w) , (1)where Pr?
(w) is the probability of w participating inthe ?
relationship, and Pr?
(w, pi) is the probabilitythat both w and pi participate in the ?
relationship.4.2 Word Sense DisambiguationWe employ the generalized selectional preferencesto determine the sense of a polysemous word in asentence.
Consider the task of determining the sense3of ?spend?
in the candidate target sentence ?Theysay film makers don?t spend enough time developinga good story.?
The word ?spend?
has two possiblemeanings in WordNet.1.
(99) spend, pass ?
(pass (time) in a specificway; ?How are you spending your summer va-cation??)2.
(36) spend, expend, drop ?
(pay out; ?I spendall my money in two days.?
)Each definition of the possible senses include (1)the head words that summarize the intended mean-ing and (2) a sample sentence for sense.
When wework on the disambiguation of a word, we do notconsider the word itself as a head word in the follow-ing discussion.
Hence, ?spend?
has one head word,i.e., ?pass?, in the first sense and two head words,i.e., ?extend?
and ?drop?, in the second sense.An intuitive method for determining the mean-ing of ?spend?
in the target sentence is to replace?spend?
with its head words in the target sentence.The head words of the correct sense should go withthe target sentence better than head words of othersenses.
This intuition leads to the a part of the scoresfor senses, i.e., St that we present shortly.In addition, we can compare the similarity of thecontexts of ?spend?
in the target sentence and sam-ple sentences, where context refers to the classes ofthe signals of the word being disambiguated.
For thecurrent example, we can check whether the subjectand object of ?spend?
in the target sentence have thesame classes as the subjects and objects of ?spend?in the sample sentences.
The sense whose samplesentence offers a more similar context for ?spend?
inthe target sentence receives a higher score.
This in-tuition leads to the other part of the scores for senses,i.e., Ss that we present below.Assume that the key w has n senses.
Let ?
={?1, ?2, ?
?
?
, ?n} be the set of senses of w. Assumethat sense ?j of word w has mj head words in Word-Net.
(Note that we do not consider w as its own headword.)
We use the set ?j = {?j,1, ?j,2, ?
?
?
, ?j,mj}to denote the set of head words that WordNet pro-vides for sense ?j of word w.When we use the partial parser to parse the tar-get sentence T for a key, we obtain informationabout the signal words of the key.
Moreover, foreach of these signals, we look up their classes inHowNet, and adopt the first listed class for each ofthe signals when the signal covers multiple classes.Assume that there are ?
(T ) signals for the keyw in a sentence T .
We use the set ?
(T,w) ={?1,T , ?2,T , ?
?
?
, ??
(T ),T } to denote the set of sig-nals for w in T .
Correspondingly, we use ?j,T to de-note the syntactic relationship between w and ?j,Tin T , use ?
(T,w) = {?1,T , ?2,T , ?
?
?
, ??
(T ),T } forthe set of relationships between signals in ?
(T,w)and w, use pij,T for the class of ?j,T , and use?
(T,w) = {pi1,T , pi2,T , ?
?
?
, pi?
(T ),T } for the set ofclasses of the signals in ?
(T,w).Equation (2) measures the average strength of as-sociation of the head words of a sense with signalsof the key in T , so we use (2) as a part of the scorefor w to take the sense ?j in the target sentence T .Note that both the strength of association and St fallin the range of [0,1].St(?j |w, T )= 1mjmj?k=11?
(T )?
(T )?l=1A?l,T (?j,k, pil,T ) (2)In (2), we have assumed that the signal wordsare not polysemous.
If they are polysemous, we as-sume that each of the candidate sense of the signalwords are equally possible, and employ a slightlymore complicated formula for (2).
This assumptionmay introduce errors into our decisions, but relievesus from the needs to disambiguate the signal wordsin the first place (Liu et al, 2005).Since WordNet provides sample sentences for im-portant words, we also use the degrees of similaritybetween the sample sentences and the target sen-tence to disambiguate the word senses of the keyword in the target sentence.
Let T and S be the tar-get sentence of w and a sample sentence of sense ?jof w, respectively.
We compute this part of score,Ss, for ?j using the following three-step procedure.If there are multiple sample sentences for a givensense, say ?j of w, we will compute the score in (3)for each sample sentence of ?j , and use the averagescore as the final score for ?j .Procedure for computing Ss(?j |w, T )1.
Compute signals of the key and their relation-ships with the key in the target and sample sen-tences.4?
(T,w) = {?1,T , ?2,T , ?
?
?
, ??
(T ),T },?
(T,w) = {?1,T , ?2,T , ?
?
?
, ??
(T ),T },?
(S,w) = {?1,S , ?2,S , ?
?
?
, ??
(S),S}, and?
(S,w) = {?1,S , ?2,S , ?
?
?
, ??(S),S}2.
We look for ?j,T and ?k,S such that ?j,T =?k,S , and then check whether pij,T = pik,S .Namely, for each signal of the key in T , wecheck the signals of the key in S for matchingsyntactic relationships and word classes, andrecord the counts of matched relationship inM(?j , T ) (Liu et al, 2005).3.
The following score measures the proportion ofmatched relationships among all relationshipsbetween the key and its signals in the target sen-tence.Ss(?j |w, T ) = M(?j , T )?
(T ) (3)The score for w to take sense ?j in a target sen-tence T is the sum of St(?j |w, T ) defined in (2)and Ss(?j |w, T ) defined in (3), so the sense of win T will be set to the sense defined in (4) whenthe score exceeds a selected threshold.
When thesum of St(?j |w, T ) and Ss(?j |w, T ) is smaller thanthe threshold, we avoid making arbitrary decisionsabout the word senses.
We discuss and illustrate ef-fects of choosing different thresholds in Section 6.argmax?j?
?St(?j |w, T ) + Ss(?j |w, T ) (4)5 Distractor GenerationDistractors in multiple-choice items influence thepossibility of making lucky guesses to the answers.Should we use extremely impossible distractors inthe items, examinees may be able to identify thecorrect answers without really knowing the keys.Hence, we need to choose distractors that appear tofit the gap, and must avoid having multiple answersto items in a typical cloze test at the same time.There are some conceivable principles and al-ternatives that are easy to implement and follow.Antonyms of the key are choices that average exam-inees will identify and ignore.
The part-of-speechtags of the distractors should be the same as thekey in the target sentence.
We may also take cul-tural background into consideration.
Students inTaiwan tend to associate English vocabularies withtheir Chinese translations.
Although this learningstrategy works most of the time, students may findit difficult to differentiate English words that havevery similar Chinese translations.
Hence, a culture-dependent strategy is to use English words that havesimilar Chinese translations with the key as the dis-tractors.To generate distractors systematically, we employranks of word frequencies for selecting distractors(Poel and Weatherly, 1997).
Assume that we aregenerating an item for a key whose part-of-speechis ?, that there are n word types whose part-of-speech may be ?
in the dictionary, and that the rankof frequency of the key among these n types is m.We randomly select words that rank in the range[m?n/10,m+n/10] among these n types as candi-date distractors.
These distractors are then screenedby their fitness into the target sentence, where fitnessis defined based on the concept of collocations ofword classes, defined in HowNet, of the distractorsand other words in the stem of the target sentence.Recall that we have marked words in the corpuswith their signals in Section 3.
The words that havemore signals in a sentence usually contribute more tothe meaning of the sentence, so should play a moreimportant role in the selection of distractors.
Sincewe do not really look into the semantics of the tar-get sentences, a relatively safer method for selectingdistractors is to choose those words that seldom col-locate with important words in the target sentence.Let T = {t1, t2, ?
?
?
, tn} denote the set of wordsin the target sentence.
We select a set T ?
?
T suchthat each t?i ?
T ?
has two or more signals in T and isa verb, noun, adjective, or adverb.
Let ?
be the firstlisted class, in HowNet, of the candidate distractor,and ?
= {?i|?i is the first listed class of a t?i ?
T ?
}.The fitness of a candidate distractor is defined in (5).?1|?|??i?
?log Pr(?, ?i)Pr(?)
Pr(?i) (5)The candidate whose score is better than 0.3 willbe admitted as a distractor.
Pr(?)
and Pr(?i) arethe probabilities that each word class appears indi-vidually in the corpus, and Pr(?, ?i) is the proba-bility that the two classes appear in the same sen-tence.
Operational definitions of these probabilities5Table 1: Accuracy of WSDPOS baseline threshold=0.4 threshold=0.7verb 38.0%(19/50) 57.1%(16/28) 68.4%(13/19)noun 34.0%(17/50) 63.3%(19/30) 71.4%(15/21)adj.
26.7%(8/30) 55.6%(10/18) 60.0%(6/10)adv.
36.7%(11/30) 52.4%(11/21) 58.3%(7/12)are provided in (Liu et al, 2005).
The term in thesummation is a pointwise mutual information, andmeasures how often the classes ?
and ?i collocatein the corpus.
We negate the averaged sum so thatclasses that seldom collocate receive higher scores.We set the threshold to 0.3, based on statistics of (5)that are observed from the cloze items used in the1992-2003 CEET.6 Evaluations and Applications6.1 Word Sense DisambiguationDifferent approaches to WSD were evaluated in dif-ferent setups, and a very wide range of accuracies in[40%, 90%] were reported (Resnik, 1997; Wilks andStevenson, 1997).
Objective comparisons need to becarried out on a common test environment like SEN-SEVAL, so we choose to present only our results.We arbitrarily chose, respectively, 50, 50, 30,and 30 sentences that contained polysemous verbs,nouns, adjectives, and adverbs for disambiguation.Table 1 shows the percentage of correctly disam-biguated words in these 160 samples.The baseline column shows the resulting accu-racy when we directly use the most frequent sense,recorded in WordNet, for the polysemous words.The rightmost two columns show the resulting accu-racy when we used different thresholds for applying(4).
As we noted in Section 4.2, our system selectedfewer sentences when we increased the threshold, sothe selected threshold affected the performance.
Alarger threshold led to higher accuracy, but increasedthe rejection rate at the same time.
Since the cor-pus can be extended to include more and more sen-tences, we afford to care about the accuracy morethan the rejection rate of the sentence retriever.We note that not every sense of all words havesample sentences in the WordNet.
When a sensedoes not have any sample sentence, this sense willreceive no credit, i.e., 0, for Ss.
Consequently,our current reliance on sample sentences in Word-Table 2: Correctness of the generated sentencesPOS of the key # of items % of correct sentencesverb 77 66.2%noun 62 69.4%adjective 35 60.0%adverb 26 61.5%overall 65.5%Table 3: Uniqueness of answersitem category key?s POS number of items resultsverb 64 90.6%noun 57 94.7%cloze adjective 46 93.5%adverb 33 84.8%overall 91.5%Net makes us discriminate against senses that do nothave sample sentences.
This is an obvious draw-back in our current design, but the problem is notreally detrimental and unsolvable.
There are usuallysample sentences for important and commonly-usedsenses of polysemous words, so the discriminationproblem does not happen frequently.
When we dowant to avoid this problem once and for all, we cancustomize WordNet by adding sample sentences toall senses of important words.6.2 Cloze Item GenerationWe asked the item generator to create 200 items inthe evaluation.
To mimic the distribution over keysof the cloze items that were used in CEET, we used77, 62, 35, and 26 items for verbs, nouns, adjectives,and adverbs, respectively, in the evaluation.In the evaluation, we requested one item at a time,and examined whether the sense and part-of-speechof the key in the generated item really met the re-quests.
The threshold for using (4) to disambiguateword sense was set to 0.7.
Results of this experi-ment, shown in Table 2, do not differ significantlyfrom those reported in Table 1.
For all four majorclasses of cloze items, our system was able to re-turn a correct sentence for less than every 2 itemsit generated.
In addition, we checked the quality ofthe distractors, and marked those items that permit-ted unique answers as good items.
Table 3 showsthat our system was able to create items with uniqueanswers for another 200 items most of the time.6Figure 5: A phonetic concordancer6.3 More ApplicationsWe have used the generated items in real tests in afreshman-level English class at National ChengchiUniversity, and have integrated the reported itemgenerator in a Web-based system for learning En-glish.
In this system, we have two major subsys-tems: the authoring and the assessment subsystems.Using the authoring subsystem, test administratorsmay select items from the interface shown in Fig-ure 4, save the selected items to an item bank, editthe items, including their stems if necessary, and fi-nalize the selection of the items for a particular ex-amination.
Using the assessment subsystem, stu-dents answer the test items via the Internet, andcan receive grades immediately if the administra-tors choose to do so.
The answers of students arerecorded for student modelling and analysis of theitem facility and the item discrimination.7 Generating Listening Cloze ItemsWe apply the same infrastructure for generatingreading cloze items, shown in Figure 2, for the gen-eration of listening cloze items (Huang et al, 2005).Due to the educational styles in Taiwan, studentsgenerally find it more difficult to comprehend mes-sages by listening than by reading.
Hence, we canregard listening cloze tests as an advanced format ofreading cloze tests.
Having constructed a databaseof sentences, we can extract sentences that containthe key for which the test administrator would liketo have a listening cloze, and employ voice synthe-sizers to create the necessary recordings.Figure 5 shows an interface through which ad-ministrators choose and edit sentences for listeningcloze items.
Notice that we employ the concept thatis related to ordinary concordance in arranging theextracted sentences.
By defining a metric for mea-suring similarity between sounds, we can put sen-tences that have similar phonetic contexts around thekey near each other.
We hope this would better helpteachers in selecting sentences by this rudimentaryFigure 6: The most simple form of listening clozeclustering of sentences.Figure 6 shows the most simple format of listen-ing cloze items.
In this format, students click on theoptions, listen to the recorded sounds, and choosethe option that fit the gap.
The item shown in thisfigure is very similar to that shown in Figure 1, ex-cept that students read and hear the options.
Fromthis most primitive format, we can image and imple-ment other more challenging formats.
For instance,we can replace the stem, currently in printed form inFigure 6, into clickable links, demanding studentsto hear the stem rather than reading the stem.
Amiddle ground between this more challenging for-mat and the original format in the figure is to allowthe gap to cover more words in the original sentence.This would require the students to listen to a longerstream of sound, so can be a task more challengingthan the original test.
In addition to controlling thelengths of the answer voices, we can try to modulatethe speed that the voices are replayed.
Moreover,for multiple-word listening cloze, we may try to findword sequences that sound similar to the answer se-quence to control the difficulty of the test item.Defining a metric for measuring similarity be-tween two recordings is the key to support the afore-mentioned functions.
In (Huang et al, 2005), weconsider such features of phonemes as place andmanner of pronunciation in calculating the similaritybetween sounds.
Using this metric we choose as dis-tractors those sounds of words that have similar pro-nunciation with the key of the listening cloze.
Wehave to define the distance between each phonemeso that we could employ the minimal-edit-distancealgorithm for computing the distance between thesounds of different words.78 Concluding RemarksWe believe that NLP techniques can play an impor-tant role in computer assisted language learning, andthis belief is supported by papers in this workshopand the literature.
What we have just explored islimited to the composition of cloze items for Englishvocabulary.
With the assistance of WSD techniques,our system was able to identify sentences that werequalified as candidate cloze items 65% of the time.Considering both word frequencies and collocation,our system recommended distractors for cloze items,resulting in items that had unique answers 90% ofthe time.
In addition to assisting the compositionof cloze items in the printed format, our system isalso capable of helping the composition of listeningcloze items.
The current system considers featuresof phonemes in computing distances between pro-nunciations of different word strings.We imagine that NLP and other software tech-niques could empower us to create cloze items for awide range of applications.
We could control the for-mats, contents, and timing of the presented materialto manipulate the challenging levels of the test items.As we have indicated in Section 7, cloze items in thelistening format are harder than comparable items inthe printed format.
We can also control when andwhat the students can hear to fine tune the difficul-ties of the listening cloze items.We must admit, however, that we do not have suf-ficient domain knowledge in how human learn lan-guages.
Consequently, tools offered by computingtechnologies that appear attractive to computer sci-entists or computational linguists might not provideeffective assistance for language learning or diagno-sis.
Though we have begun to study item compari-son from a mathematical viewpoint (Liu, 2005), thecurrent results are far from being practical.
Exper-tise in psycholinguistics may offer a better guidanceon our system design, we suppose.AcknowledgementsWe thank anonymous reviewers for their invaluablecomments on a previous version of this report.
Wewill respond to some suggestions that we do not havespace to do so in this report in the workshop.
Thisresearch was supported in part by Grants 93-2213-E-004-004 and 93-2411-H-002-013 from the NationalScience Council of Taiwan.ReferencesD.
Coniam.
1997.
A preliminary inquiry into using corpusword frequency data in the automatic generation of Englishlanguage cloze tests.
Computer Assisted Language Instruc-tion Consortium, 16(2?4):15?33.P.
Deane and K. Sheehan.
2003.
Automatic item gen-eration via frame semantics.
Education Testing Service:http://www.ets.org/research/dload/ncme03-deane.pdf.I.
Dennis, S. Handley, P. Bradon, J. Evans, and S. Nestead.2002.
Approaches to modeling item-generative tests.
InItem generation for test development (Irvine and Kyllonen,2002), pages 53?72.S.-M. Huang, C.-L. Liu, and Z.-M. Gao.
2005.
Computer-assisted item generation for listening cloze tests and dictationpractice in English.
In Proc.
of the 4th Int.
Conf.
on Web-based Learning.
to appear.S.
H. Irvine and P. C. Kyllonen, editors.
2002.
Item Genera-tion for Test Development.
Lawrence Erlbaum Associates,Mahwah, NJ.D.
Lin.
1998.
Dependency-based evaluation of MINIPAR.
InProc.
of the Workshop on the Evaluation of Parsing Systemsin the 1st Int.
Conf.
on Language Resources and Evaluation.C.-L. Liu, C.-H. Wang, and Z.-M. Gao.
2005.
Using lexi-cal constraints for enhancing computer-generated multiple-choice cloze items.
Int.
J. of Computational Linguistics andChinese Language Processing, 10:to appear.C.-L. Liu.
2005.
Using mutual information for adaptive itemcomparison and student assessment.
J. of Educational Tech-nology & Society, 8(4):to appear.C.
D. Manning and H. Schu?tze.
1999.
Foundations of Statisti-cal Natural Language Processing.
MIT Press, Cambridge.C.
J. Poel and S. D. Weatherly.
1997.
A cloze look at place-ment testing.
Shiken: JALT (Japanese Assoc.
for LanguageTeaching) Testing & Evaluation SIG Newsletter, 1(1):4?10.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tag-ger.
In Proc.
of the Conf.
on Empirical Methods in NaturalLanguage Processing, pages 133?142.P.
Resnik.
1997.
Selectional preference and sense disambigua-tion.
In Proc.
of the Applied NLP Workshop on Tagging Textwith Lexical Semantics: Why, What and How, pages 52?57.J.
C. Reynar and A. Ratnaparkhi.
1997.
A maximum entropyapproach to identifying sentence boundaries.
In Proc.
of theConf.
on Applied Natural Language Processing, pages 16?19.K.
M. Sheehan, P. Deane, and I. Kostin.
2003.
A partially auto-mated system for generating passage-based multiple-choiceverbal reasoning items.
Paper presented at the Nat?l Councilon Measurement in Education Annual Meeting.V.
Stevens.
1991.
Classroom concordancing: vocabulary ma-terials derived from relevant authentic text.
English for Spe-cific Purposes, 10(1):35?46.Y.
Wilks and M. Stevenson.
1997.
Combining independentknowledge sources for word sense disambiguation.
In Proc.of the Conf.
on Recent Advances in Natural Language Pro-cessing, pages 1?7.8
