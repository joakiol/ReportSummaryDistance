Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482?1491,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLearning Arguments and Supertypes of Semantic Relations usingRecursive PatternsZornitsa Kozareva and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{kozareva,hovy}@isi.eduAbstractA challenging problem in open informa-tion extraction and text mining is the learn-ing of the selectional restrictions of se-mantic relations.
We propose a mini-mally supervised bootstrapping algorithmthat uses a single seed and a recursivelexico-syntactic pattern to learn the ar-guments and the supertypes of a diverseset of semantic relations from the Web.We evaluate the performance of our algo-rithm on multiple semantic relations ex-pressed using ?verb?, ?noun?, and ?verbprep?
lexico-syntactic patterns.
Human-based evaluation shows that the accuracyof the harvested information is about 90%.We also compare our results with existingknowledge base to outline the similaritiesand differences of the granularity and di-versity of the harvested knowledge.1 IntroductionBuilding and maintaining knowledge-rich re-sources is of great importance to information ex-traction, question answering, and textual entail-ment.
Given the endless amount of data we have atour disposal, many efforts have focused on miningknowledge from structured or unstructured text,including ground facts (Etzioni et al, 2005), se-mantic lexicons (Thelen and Riloff, 2002), ency-clopedic knowledge (Suchanek et al, 2007), andconcept lists (Katz et al, 2003).
Researchers havealso successfully harvested relations between en-tities, such as is-a (Hearst, 1992; Pasca, 2004) andpart-of (Girju et al, 2003).
The kinds of knowl-edge learned are generally of two kinds: groundinstance facts (New York is-a city, Rome is the cap-ital of Italy) and general relational types (city is-alocation, engines are part-of cars).A variety of NLP tasks involving inference orentailment (Zanzotto et al, 2006), including QA(Katz and Lin, 2003) and MT (Mt et al, 1988),require a slightly different form of knowledge, de-rived from many more relations.
This knowledgeis usually used to support inference and is ex-pressed as selectional restrictions (Wilks, 1975)(namely, the types of arguments that may fill agiven relation, such as person live-in city and air-line fly-to location).
Selectional restrictions con-strain the possible fillers of a relation, and hencethe possible contexts in which the patterns ex-pressing that relation can participate in, therebyenabling sense disambiguation of both the fillersand the expression itself.To acquire this knowledge two common ap-proaches are employed: clustering and patterns.While clustering has the advantage of being fullyunsupervised, it may or may not produce the typesand granularity desired by a user.
In contrastpattern-based approaches are more precise, butthey typically require a handful to dozens of seedsand lexico-syntactic patterns to initiate the learn-ing process.
In a closed domain these approachesare both very promising, but when tackling an un-bounded number of relations they are unrealistic.The quality of clustering decreases as the domainbecomes more continuously varied and diverse,and it has proven difficult to create collections ofeffective patterns and high-yield seeds manually.In addition, the output of most harvesting sys-tems is a flat list of lexical semantic expressionssuch as ?New York is-a city?
and ?virus causesflu?.
However, using this knowledge in inferencerequires it to be formulated appropriately and or-ganized in a semantic repository.
(Pennacchiottiand Pantel, 2006) proposed an algorithm for au-tomatically ontologizing semantic relations intoWordNet.
However, despite its high precision en-tries, WordNet?s limited coverage makes it impos-sible for relations whose arguments are not presentin WordNet to be incorporated.
One would like aprocedure that dynamically organizes and extends1482its semantic repository in order to be able to ac-commodate all newly-harvested information, andthereby become a global semantic repository.Given these considerations, we address in thispaper the following question: How can the selec-tional restrictions of semantic relations be learnedautomatically from the Web with minimal effort us-ing lexico-syntactic recursive patterns?The contributions of the paper are as follows:?
A novel representation of semantic relationsusing recursive lexico-syntactic patterns.?
An automatic procedure to learn the se-lectional restrictions (arguments and super-types) of semantic relations from Web data.?
An exhaustive human-based evaluation of theharvested knowledge.?
A comparison of the results with some largeexisting knowledge bases.The rest of the paper is organized as follows.
Inthe next section, we review related work.
Section3 addresses the representation of semantic rela-tions using recursive patterns.
Section 4 describesthe bootstrapping mechanism that learns the selec-tional restrictions of the relations.
Section 5 de-scribes data collection.
Section 6 discusses the ob-tained results.
Finally, we conclude in Section 7.2 Related WorkA substantial body of work has been done in at-tempts to harvest bits of semantic information, in-cluding: semantic lexicons (Riloff and Shepherd,1997), concept lists (Lin and Pantel, 2002), is-a relations (Hearst, 1992; Etzioni et al, 2005;Pasca, 2004; Kozareva et al, 2008), part-of re-lations (Girju et al, 2003), and others.
Knowl-edge has been harvested with varying success bothfrom structured text such as Wikipedia?s infoboxes(Suchanek et al, 2007) or unstructured text suchas the Web (Pennacchiotti and Pantel, 2006; Yateset al, 2007).
A variety of techniques have beenemployed, including clustering (Lin and Pantel,2002), co-occurrence statistics (Roark and Char-niak, 1998), syntactic dependencies (Pantel andRavichandran, 2004), and lexico-syntactic pat-terns (Riloff and Jones, 1999; Fleischman andHovy, 2002; Thelen and Riloff, 2002).When research focuses on a particular relation,careful attention is paid to the pattern(s) that ex-press it in various ways (as in most of the workabove, notably (Riloff and Jones, 1999)).
But ithas proven a difficult task to manually find ef-fectively different variations and alternative pat-terns for each relation.
In contrast, when re-search focuses on any relation, as in TextRun-ner (Yates et al, 2007), there is no standardizedmanner for re-using the pattern learned.
TextRun-ner scans sentences to obtain relation-independentlexico-syntactic patterns to extract triples of theform (John, fly to, Prague).
The middle string de-notes some (unspecified) semantic relation whilethe first and third denote the learned arguments ofthis relation.
But TextRunner does not seek spe-cific semantic relations, and does not re-use thepatterns it harvests with different arguments in or-der to extend their yields.Clearly, it is important to be able to specify boththe actual semantic relation sought and use its tex-tual expression(s) in a controlled manner for max-imal benefit.The objective of our research is to combine thestrengths of the two approaches, and, in addition,to provide even richer information by automati-cally mapping each harvested argument to its su-pertype(s) (i.e., its semantic concepts).
For in-stance, given the relation destination and the pat-tern X flies to Y, automatically determining thatJohn, Prague) and (John, conference) are twovalid filler instance pairs, that (RyanAir, Prague)is another, as well as that person and airline aresupertypes of the first argument and city and eventof the second.
This information provides the se-lectional restrictions of the given semantic rela-tion, indicating that living things like people canfly to cities and events, while non-living things likeairlines fly mainly to cities.
This is a significantimprovement over systems that output a flat listof lexical semantic knowledge (Thelen and Riloff,2002; Yates et al, 2007; Suchanek et al, 2007).Knowing the sectional restrictions of a semanticrelation supports inference in many applications,for example enabling more accurate informationextraction.
(Igo and Riloff, 2009) report that pat-terns like ?attack on ?NP??
can learn undesirablewords due to idiomatic expressions and parsing er-rors.
Over time this becomes problematic for thebootstrapping process and leads to significant de-terioration in performance.
(Thelen and Riloff,2002) address this problem by learning multiplesemantic categories simultaneously, relying on theoften unrealistic assumption that a word cannotbelong to more than one semantic category.
How-1483ever, if we have at our disposal a repository of se-mantic relations with their selectional restrictions,the problem addressed in (Igo and Riloff, 2009)can be alleviated.In order to obtain selectional restriction classes,(Pennacchiotti and Pantel, 2006) made an attemptto ontologize the harvested arguments of is-a,part-of, and cause relations.
They mapped eachargument of the relation into WordNet and identi-fied the senses for which the relation holds.
Un-fortunately, despite its very high precision en-tries, WordNet is known to have limited cover-age, which makes it impossible for algorithms tomap the content of a relation whose argumentsare not present in WordNet.
To surmount thislimitation, we do not use WordNet, but employa different method of obtaining superclasses of afiller term: the inverse doubly-anchored patternsDAP?1 (Hovy et al, 2009), which, given two ar-guments, harvests its supertypes from the sourcecorpus.
(Hovy et al, 2009) show that DAP?1 isreliable and it enriches WordNet with additionalhyponyms and hypernyms.3 Recursive PatternsA singly-anchored pattern contains one exampleof the seed term (the anchor) and one open posi-tion for the term to be learned.
Most researchersuse singly-anchored patterns to harvest semanticrelations.
Unfortunately, these patterns run out ofsteam very quickly.
To surmount this obstacle, ahandful of seeds is generally used, and helps toguarantee diversity in the extraction of new lexico-syntactic patterns (Riloff and Jones, 1999; Snow etal., 2005; Etzioni et al, 2005).Some algorithms require ten seeds (Riloff andJones, 1999; Igo and Riloff, 2009), while othersuse a variation of 5, 10, to even 25 seeds (Taluk-dar et al, 2008).
Seeds may be chosen at ran-dom (Davidov et al, 2007; Kozareva et al, 2008),by picking the most frequent terms of the desiredclass (Igo and Riloff, 2009), or by asking humans(Pantel et al, 2009).
As (Pantel et al, 2009) show,picking seeds that yield high numbers of differ-ent terms is difficult.
Thus, when dealing withunbounded sets of relations (Banko and Etzioni,2008), providing many seeds becomes unrealistic.Interestingly, recent work reports a class of pat-terns that use only one seed to learn as much infor-mation with only one seed.
(Kozareva et al, 2008;Hovy et al, 2009) introduce the so-called doubly-anchored pattern (DAP) that has two anchor seedpositions ??type?
such as ?seed?
and *?, plus oneopen position for the terms to be learned.
Learnedterms can then be replaced into the seed positionautomatically, creating a recursive procedure thatis reportedly much more accurate and has muchhigher final yield.
(Kozareva et al, 2008; Hovy etal., 2009) have successfully applied DAP for thelearning of hyponyms and hypernyms of is-a rela-tions and report improvements over (Etzioni et al,2005) and (Pasca, 2004).Surprisingly, this work was limited to the se-mantic relation is-a.
No other study has describedthe use or effect of recursive patterns for differ-ent semantic relations.
Therefore, going beyond(Kozareva et al, 2008; Hovy et al, 2009), we hereintroduce recursive patterns other than DAP thatuse only one seed to harvest the arguments and su-pertypes of a wide variety of relations.
(Banko and Etzioni, 2008) show that seman-tic relations can be expressed using a handfulof relation-independent lexico-syntactic patterns.Practically, we can turn any of these patterns intorecursive form by giving as input only one of thearguments and leaving the other one as an openslot, allowing the learned arguments to replace theinitial seed argument directly.
For example, forthe relation ?fly to?, the following recursive pat-terns can be built: ?
* and ?seed?
fly to *?, ?
?seed?and * fly to *?, ?
* fly to ?seed?
and *?, ?
* fly to *and ?seed?
?, ??seed?
fly to *?
or ?
* fly to ?seed?
?,where ?seed?
is an example like John or Ryanair,and (?)
indicates the position on which the ar-guments are learned.
Conjunctions like and, orare useful because they express list constructionsand extract arguments similar to the seed.
Poten-tially, one can explore all recursive pattern varia-tions when learning a relation and compare theiryield, however this study is beyond the scope ofthis paper.We are particularly interested in the usage of re-cursive patterns for the learning of semantic re-lations not only because it is a novel method,but also because recursive patterns of the DAPfashion are known to: (1) learn concepts withhigh precision compared to singly-anchored pat-terns (Kozareva et al, 2008), (2) use only oneseed instance for the discovery of new previouslyunknown terms, and (3) harvest knowledge withminimal supervision.14844 Bootstrapping Recursive Patterns4.1 Problem FormulationThe main goal of our research is:Task Definition: Given a seed and a semantic relation ex-pressed using a recursive lexico-syntactic pattern, learn inbootstrapping fashion the selectional restrictions (i.e., thearguments and supertypes) of the semantic relation froman unstructured corpus such as the Web.Figure 1 shows an example of the task and thetypes of information learned by our algorithm.
* and John fly to *seed = Johnrelation = fly toBrianKatepoliticianspeopleartistsDeltaAlaskaairlinescarriersbeesanimalsparty eventItalyFrance countriesNew York cityflowerstrees plantsFigure 1: Bootstrapping Recursive Patterns.Given a seed John and a semantic relation fly toexpressed using the recursive pattern ?
* and Johnfly to *?, our algorithm learns the left side argu-ments {Brian, Kate, bees, Delta, Alaska} and theright side arguments {flowers, trees, party, NewYork, Italy, France}.
For each argument, the algo-rithm harvests supertypes such as {people, artists,politicians, airlines, city, countries, plants, event}among others.
The colored links between the rightand left side concepts denote the selectional re-strictions of the relation.
For instance, people flyto events and countries, but never to trees or flow-ers.4.2 System ArchitectureWe propose a minimally supervised bootstrap-ping algorithm based on the framework adopted in(Kozareva et al, 2008; Hovy et al, 2009).
The al-gorithm has two phases: argument harvesting andsupertype harvesting.
The final output is a rankedlist of interlinked concepts which captures the se-lectional restrictions of the relation.4.2.1 Argument HarvestingIn the argument extraction phase, the first boot-strapping iteration is initiated with a seed Y and arecursive pattern ?X?
and Y verb+prep|verb|nounZ?
?, where X?
and Z?
are the placeholders for thearguments to be learned.
The pattern is submit-ted to Yahoo!
as a web query and all unique snip-pets matching the query are retrieved.
The newlylearned and previously unexplored arguments onthe X?
position are used as seeds in the subse-quent iteration.
The arguments on the Z?
posi-tion are stored at each iteration, but never usedas seeds since the recursivity is created using theterms on X and Y .
The bootstrapping process isimplemented as an exhaustive breadth-first algo-rithm which terminates when all arguments are ex-plored.We noticed that despite the specific lexico-syntactic structure of the patterns, erroneous in-formation can be acquired due to part-of-speechtagging errors or flawed facts on the Web.
Thechallenge is to identify and separate the erroneousfrom the true arguments.
We incorporate the har-vested arguments on X and Y positions in a di-rected graph G = (V,E), where each vertexv ?
V is a candidate argument and each edge(u, v) ?
E indicates that the argument v is gener-ated by the argument u.
An edge has weight w cor-responding to the number of times the pair (u, v)is extracted from different snippets.
A node uis ranked by u=??(u,v)?Ew(u,v)+??
(v,u)?Ew(v,u)|V |?1which represents the weighted sum of the outgo-ing and incoming edges normalized by the totalnumber of nodes in the graph.
Intuitively, our con-fidence in a correct argument u increases when theargument (1) discovers and (2) is discovered bymany different arguments.Similarly, to rank the arguments standing onthe Z position, we build a bipartite graph G?
=(V ?, E?)
that has two types of vertices.
One setof vertices represents the arguments found on theY position in the recursive pattern.
We will callthese Vy.
The second set of vertices represents thearguments learned on the Z position.
We will callthese Vz .
We create an edge e?
(u?, v?)
?
E?
be-tween u?
?
Vy and v?
?
Vz when the argument onthe Z position represented by v?
was harvested bythe argument on the Y position represented by u?.The weight w?
of the edge indicates the numberof times an argument on the Y position found Z.Vertex v?
is ranked as v?=??(u?,v?)?E?w(u?,v?
)|V ?|?1 .
Ina very large corpus, like the Web, we assume thata correct argument Z is the one that is frequentlydiscovered by various arguments Y .14854.2.2 Supertype HarvestingIn the supertype extraction phase, we take all<X,Y> argument pairs collected during the argu-ment harvesting stage and instantiate them in theinverse DAP?1 pattern ?
* such as X and Y?.
Thequery is sent to Yahoo!
as a web query and all 1000snippets matching the pattern are retrieved.
Foreach <X,Y> pair, the terms on the (*) position areextracted and considered as candidate supertypes.To avoid the inclusion of erroneous supertypes,again we build a bipartite graph G??
= (V ?
?, E??
).The set of vertices Vsup represents the supertypes,while the set of vertices Vp corresponds to the?X,Y?
pair that produced the supertype.
An edgee??(u?
?, v??)
?
E?
?, where u??
?
Vp and v??
?
Vsupshows that the pair ?X,Y?
denoted as u??
harvestedthe supertype represented by v?
?.For example, imagine that the argument X?=Ryanair was harvested in the previous phase bythe recursive pattern ?X?
and EasyJet fly to Z?
?.Then the pair ?Ryanair,EasyJet?
forms a new Webquery ?
* such as Ryanair and EasyJet?
whichlearns the supertypes ?airlines?
and ?carriers?.The bipartite graph has two vertices v?
?1 and v?
?2 forthe supertypes ?airlines?
and ?carriers?, one ver-tex u?
?3 for the argument pair ?Ryanair, EasyJet?,and two edges e??1(u?
?3, v?
?1) and e??2(u?
?3, v??1).
A vertexv??
?
Vsup is ranked by v??=??(u??,v??)?E??w(u??,v??
)|V ?
?|?1 .Intuitively, a supertype which is discovered mul-tiple times by various argument pairs is rankedhighly.However, it might happen that a highly rankedsupertype actually does not satisfy the selectionalrestrictions of the semantic relation.
To avoid suchsituations, we further instantiate each supertypeconcept in the original pattern1.
For example,?aircompanies fly to *?
and ?carriers fly to *?.
Ifthe candidate supertype produces many web hitsfor the query, then this suggests that the term is arelevant supertype.Unfortunately, to learn the supertypes of the Zarguments, currently we have to form all possi-ble combinations among the top 150 highly rankedconcepts, because these arguments have not beenlearned through pairing.
For each pair of Z argu-ments, we repeat the same procedure as describedabove.1Except for the ?dress?
and ?person?
relations, wherethe targeted arguments are adjectives, and the supertypes arenouns.5 Semantic RelationsSo far, we have described the mechanism thatlearns from one seed and a recursive pattern theselectional restrictions of any semantic relation.Now, we are interested in evaluating the per-formance of our algorithm.
A natural questionthat arises is: ?How many patterns are there??.
(Banko and Etzioni, 2008) found that 95% of thesemantic relations can be expressed using eightlexico-syntactic patterns.
Space prevents us fromdescribing all of them, therefore we focus on thethree most frequent patterns which capture a largediversity of semantic relations.
The relative fre-quency of these patterns is 37.80% for ?verbs?,22.80% for ?noun prep?, and 16.00% for ?verbprep?.5.1 Data CollectionTable 1 shows the lexico-syntactic pattern and theinitial seed we used to express each semantic rela-tion.
To collect data, we ran our knowledge har-vesting algorithm until complete exhaustion.
Foreach query submitted to Yahoo!, we retrieved thetop 1000 web snippets and kept only the uniqueones.
In total, we collected 30GB raw data whichwas part-of-speech tagged and used for the argu-ment and supertype extraction.
Table 1 shows theobtained results.recursive pattern seed X arg Z arg #iterX and Y work for Z Charlie 2949 3396 20X and Y fly to Z EasyJet 772 1176 19X and Y go to Z Rita 18406 27721 13X and Y work in Z John 4142 4918 13X and Y work on Z Mary 4126 5186 7X and Y work at Z Scott 1084 1186 14X and Y live in Z Harry 8886 19698 15X and Y live at Z Donald 1102 1175 15X and Y live with Z Peter 1344 834 11X and Y cause Z virus 12790 52744 19X and Y celebrate Jim 6033 ?
12X and Y drink Sam 1810 ?
13X and Y dress nice 1838 ?
8X and Y person scared 2984 ?
17Table 1: Total Number of Harvested Arguments.An interesting characteristic of the recursivepatterns is the speed of leaning which can be mea-sured in terms of the number of unique argu-ments acquired during each bootstrapping itera-tion.
Figure 2 shows the bootstrapping process forthe ?cause?
and ?dress?
relations.
Although bothrelations differ in terms of the total number of it-erations and harvested items, the overall behaviorof the learning curves is similar.
Learning startsof very slowly and as bootstrapping progresses a1486rapid growth is observed until a saturation point isreached.01000020000300004000050000600001  2  3  4  5  6  7  8  9  10  11 12  13 14  15 16  17 18  19#ItemsLearnedIterationsX and Y Cause ZXZ05001000150020001  2  3  4  5  6  7  8#ItemsLearnedIterationsX and Y DressXFigure 2: Items extracted in 10 iterations.The speed of leaning is related to the connectiv-ity behavior of the arguments of the relation.
In-tuitively, a densely connected graph takes shortertime (i.e., fewer iterations) to be learned, as in the?work on?
relation, while a weakly connected net-work takes longer time to harvest the same amountof information, as in the ?work for?
relation.6 ResultsIn this section, we evaluate the results of ourknowledge harvesting algorithm.
Initially, we de-cided to conduct an automatic evaluation compar-ing our results to knowledge bases that have beenextracted in a similar way (i.e., through pattern ap-plication over unstructured text).
However, it isnot always possible to perform a complete com-parison, because either researchers have not fullyexplored the same relations we have studied, or forthose relations that overlap, the gold standard datawas not available.The online demo of TextRunner2 (Yates et al,2007) actually allowed us to collect the argumentsfor all our semantic relations.
However, due toWeb based query limitations, TextRunner returnsonly the first 1000 snippets.
Since we do not havethe complete and ranked output of TextRunner,comparing results in terms of recall and precisionis impossible.Turning instead to results obtained from struc-tured sources (which one expects to have highcorrectness), we found that two of our relationsoverlap with those of the freely available ontologyYago (Suchanek et al, 2007), which was harvestedfrom the Infoboxes tables in Wikipedia.
In addi-tion, we also had two human annotators judge asmany results as we could afford, to obtain Preci-sion.
We conducted two evaluations, one for thearguments and one for the supertypes.2http://www.cs.washington.edu/research/textrunner/6.1 Human-Based Argument EvaluationIn this section, we discuss the results of the har-vested arguments.
For each relation, we selectedthe top 200 highly ranked arguments.
We hiredtwo annotators to judge their correctness.
We cre-ated detailed annotation guidelines that define thelabels for the arguments of the relations, as shownin Table 2.
(Previously, for the same task, re-searchers have not conducted such an exhaustiveand detailed human-based evaluation.)
The anno-tation was conducted using the CAT system3.TYPE LABEL EXAMPLESCorrect Person John, MaryRole mother, presidentGroup team, JapanesePhysical yellow, shabbyNonPhysical ugly, thoughtNonLiving airplaneOrganization IBM, parliamentLocation village, New York, in the houseTime at 5 o?clockEvent party, prom, earthquakeState sick, anrgyManner live in happinessMedium work on Linux, WordFixed phrase go to warIncorrect Error wrong part-of-speech tagOther none of the aboveTable 2: Annotation Labels.We allow multiple labels to be assigned to thesame concept, because sometimes the concept canappear in different contexts that carry various con-ceptual representations.
Although the labels canbe easily collapsed to judge correct and incorrectterms, the fine-grained annotation shown here pro-vides a better overview of the information learnedby our algorithm.We measured the inter-annotator agreement forall labels and relations considering that a singleentry can be tagged with multiple labels.
TheKappa score is around 0.80.
This judgement isgood enough to warrant using these human judge-ments to estimate the accuracy of the algorithm.We compute Accuracy as the number of examplestagged as Correct divided by the total number ofexamples.Table 4 shows the obtained results.
The over-all accuracy of the argument harvesting phase is91%.
The majority of the occurred errors are dueto part-of-speech tagging.
Table 3 shows a sam-ple of 10 randomly selected examples from the top200 ranked and manually annotated arguments.3http://cat.ucsur.pitt.edu/default.aspx1487Relation Arguments(X) Dress: stylish, comfortable, expensive, shabby, gorgeoussilver, clean, casual, Indian, black(X) Person: honest, caring, happy, intelligent, giftedfriendly, responsible, mature, wise, outgoing(X) Cause: pressure, stress, fire, bacteria, cholesterolflood, ice, cocaine, injuries, warsGoTo (Z): school, bed, New York, the movies, the park, a barthe hospital, the church, the mall, the beachLiveIn (Z): peace, close proximity, harmony, Chicago, townNew York, London, California, a house, AustraliaWorkFor (Z): a company, the local prison, a gangster, the showa boss, children, UNICEF, a living, HispanicsTable 3: Examples of Harvested Arguments.6.2 Comparison against Existing ResourcesIn this section, we compare the performance of ourapproach with the semantic knowledge base Yago4that contains 2 million entities5, 95% of whichwere manually confirmed to be correct.
In thisstudy, we compare only the unique arguments ofthe ?live in?
and ?work at?
relations.
We providePrecision scores using the following measures:PrY ago =#terms found in Y ago#terms harvested by systemPrHuman =#terms judged correct by human#terms harvested by systemNotInY ago = #terms judged correct by human but not in Y agoTable 5 shows the obtained results.We carefully analyzed those arguments thatwere found by one of the systems but were miss-ing in the other.
The recursive patterns learn infor-mation about non-famous entities like Peter andfamous entities like Michael Jordan.
In contrast,Yago contains entries mostly about famous enti-ties, because this is the predominant knowledge inWikipedia.
For the ?live in?
relation, both repos-itories contain the same city and country names.However, the recursive pattern learned argumentslike pain, effort which express a manner of living,and locations like slums, box.
This information ismissing from Yago.
Similarly for the ?work at?relation, both systems learned that people workat universities.
In addition, the recursive patternlearned a diversity of company names absent fromYago.While it is expected that our algorithm findsmany terms not contained in Yago?specifically,the information not deemed worthy of inclusionin Wikipedia?we are interested in the relativelylarge number of terms contained in Yago but notfound by our algorithm.
To our knowledge, no4http://www.mpi-inf.mpg.de/yago-naga/yago/5Names of cities, people, organizations among others.X WorkFor A1 A2 WorkFor Z A1 A2Person 148 152 Organization 111 110Role 5 7 Person 60 60Group 12 14 Event 4 2Organization 8 7 Time 4 5NonPhysical 22 23 NonPhysical 18 19Other 5 5 Other 3 4Acc.
.98 .98 Acc.
.99 .98X Cause A1 A2 Cause Z A1 A2PhysicalObj 82 75 PhysicalObj 15 20NonPhysicalObj 69 66 NonPhysicalObj 89 91Event 21 24 Event 72 72State 29 31 State 50 50Other 3 4 Other 5 4Acc.
.99 .98 Acc.
.98 .98X GoTo A1 A2 GoTo Z A1 A2Person 190 188 Location 163 155Role 4 4 Event 21 30Group 3 3 Person 11 13NonPhysical 1 3 NonPhysical 2 1Other 2 2 Other 3 1Acc.
.99 .99 Acc.
.99 .99X FlyTo A1 A2 FlyTo Z A1 A2Person 140 139 Location 199 198Organization 54 57 Event 1 2NonPhysical 2 2 Person 0 0Other 4 2 Other 0 0Acc.
.98 .99 Acc.
1 1X WorkOn A1 A2 WorkOn Z A1 A2Person 173 172 Location 110 108Role 2 3 Organization 27 25Group 4 5 Manner 38 40Organization 6 6 Time 4 4NonPhysical 15 14 NonPhysical 18 21Error 1 1 Medium 8 8Other 1 1 Other 13 15Acc.
.99 .99 Acc.
.94 .93X WorkIn A1 A2 WorkIn Z A1 A2Person 117 118 Location 104 111Group 10 9 Organization 10 25Organization 3 3 Manner 39 40Fixed 3 1 Time 4 4NonPhysical 55 59 NonPhysical 22 21Error 12 10 Medium 8 8Other 0 0 Error 13 15Acc.
.94 .95 Acc.
.94 .93X WorkAt A1 A2 WorkAt Z A1 A2Person 193 192 Organization 189 190Role 1 1 Manner 5 4Group 1 1 Time 3 3Organization 0 0 Error 3 2Other 5 6 Other 0 1Acc.
.98 .97 Acc.
.99 .99X LiveIn A1 A2 LiveIn Z A1 A2Person 185 185 Location 182 186Role 3 4 Manner 6 8Group 9 8 Time 1 2NonPhysical 1 2 Fixed 5 2Other 2 1 Other 6 2Acc.
.99 .99 Acc.
.97 .99X LiveAt A1 A2 LiveAt Z A1 A2Person 196 195 Location 158 157Role 1 1 Person 5 7NonPhysical 0 1 Manner 1 2Other 3 3 Error 36 34Acc.
.99 .99 Acc.
.82 .83X LiveWith A1 A2 LiveWith Z A1 A2Person 188 187 Person 165 163Role 6 6 Animal 2 4Group 2 2 Manner 15 15NonPhysical 2 3 NonPhysical 15 15Other 2 2 Other 3 3Acc.
.99 .99 Acc.
.99 .99X Dress A1 A2 X Person A1 A2Physical 72 59 Physical 8 2NonPhysical 120 136 NonPhysical 188 194Other 8 5 Other 4 4Acc .96 .98 Acc.
.98 .98X Drink A1 A2 X Celebrate A1 A2Living 165 174 Living 157 164NonLiving 8 2 NonLiving 42 35Error 27 24 Error 1 1Acc .87 .88 Acc.
.99 .99Table 4: Harvested Arguments.1488PrY ago PrHuman NotInYagoX LiveIn .19 (2863/14705) .58 (5165)/8886 2302LiveIn Z .10 (495/4754) .72 (14248)/19698 13753X WorkAt .12(167/1399) .88 (959)/1084 792WorkAt Z .3(15/525) .95 (1128)/1186 1113Table 5: Comparison against Yago.other automated harvesting algorithm has everbeen compared to Yago, and our results here forma baseline that we aim to improve upon.
And inthe future, one can build an extensive knowledgeharvesting system combining the wisdom of thecrowd and Wikipedia.6.3 Human-Based Supertype EvaluationIn this section, we discuss the results of harvest-ing the supertypes of the learned arguments.
Fig-ure 3 shows the top 100 ranked supertypes for the?cause?
and ?work on?
relations.
The x-axis in-dicates a supertype, the y-axis denotes the numberof different argument pairs that lead to the discov-ery of the supertype.0100200300400500600700800900100010  20  30  40  50  60  70  80  90  100#PairsDiscoveringtheSupertypeSupertypeWorkOnCauseFigure 3: Ranked Supertypes.The decline of the curve indicates that certainsupertypes are preferred and shared among differ-ent argument pairs.
It is interesting to note that thetext on the Web prefers a small set of supertypes,and to see what they are.
These most-popular har-vested types tend to be the more descriptive terms.The results indicate that one does not need an elab-orate supertype hierarchy to handle the selectionalrestrictions of semantic relations.Since our problem definition differs from avail-able related work, and WordNet does not containall harvested arguments as shown in (Hovy et al,2009), it is not possible to make a direct compar-ison.
Instead, we conduct a manual evaluation ofthe most highly ranked supertypes which normallyare the top 20.
The overall accuracy of the super-types for all relations is 92%.
Table 6 shows theRelation Arguments(Supx) Celebrate: men, people, nations, angels, workers, childrencountries, teams, parents, teachers(Supx) Dress: colors, effects, color tones, activities, patternsstyles, materials, size, languages, aspects(Supx) FlyTo: airlines, carriers, companies, giants, peoplecompetitors, political figures, stars, celebsCause (Supz): diseases, abnormalities, disasters, processes, issesdisorders, discomforts, emotions, defects, symptomsWorkFor (Supz) organizations, industries, people, markets, menautomakers, countries, departments, artists, mediaGoTo (Supz) : countries, locations, cities, people, eventsmen, activities, games, organizations,FlyTo (Supz) places, countries, regions, airports, destinationslocations, cities, area, eventsTable 6: Examples of Harvested Supertypes.top 10 highly ranked supertypes for six of our re-lations.7 ConclusionWe propose a minimally supervised algorithm thatuses only one seed example and a recursive lexico-syntactic pattern to learn in bootstrapping fash-ion the selectional restrictions of a large class ofsemantic relations.
The principal contribution ofthe paper is to demonstrate that this kind of pat-tern can be applied to almost any kind of se-mantic relation, as long as it is expressible ina concise surface pattern, and that the recursivemechanism that allows each newly acquired termto restart harvesting automatically is a signifi-cant advance over patterns that require a handfulof seeds to initiate the learning process.
It alsoshows how one can combine free-form but undi-rected pattern-learning approaches like TextRun-ner with more-controlled but effort-intensive ap-proaches like commonly used.In our evaluation, we show that our algorithm iscapable of extracting high quality non-trivial in-formation from unstructured text given very re-stricted input (one seed).
To measure the perfor-mance of our approach, we use various semanticrelations expressed with three lexico-syntactic pat-terns.
For two of the relations, we compare resultswith the freely available ontology Yago, and con-duct a manual evaluation of the harvested terms.We will release the annotated and the harvesteddata to the public to be used for comparison byother knowledge harvesting algorithms.The success of the proposed framework opensmany challenging directions.
We plan to use thealgorithm described in this paper to learn the se-lectional restrictions of numerous other relations,in order to build a rich knowledge repository1489that can support a variety of applications, includ-ing textual entailment, information extraction, andquestion answering.AcknowledgmentsThis research was supported by DARPA contractnumber FA8750-09-C-3705.ReferencesMichele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT, pages 28?36, June.Dmitry Davidov, Ari Rappoport, and Moshel Koppel.2007.
Fully unsupervised discovery of concept-specific relationships by web mining.
In Proc.
ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 232?239, June.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:an experimental study.
Artificial Intelligence,165(1):91?134, June.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proceed-ings of the 19th international conference on Compu-tational linguistics, pages 1?7.Roxana Girju, Adriana Badulescu, and Dan Moldovan.2003.
Learning semantic constraints for the auto-matic discovery of part-whole relations.
In Proc.
ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology, pages 1?8.Marti Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proc.
of the14th conference on Computational linguistics, pages539?545.Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.2009.
Toward completeness in concept extractionand classification.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 948?957.Sean Igo and Ellen Riloff.
2009.
Corpus-based se-mantic lexicon induction with web-based corrobora-tion.
In Proceedings of the Workshop on Unsuper-vised and Minimally Supervised Learning of LexicalSemantics.Boris Katz and Jimmy Lin.
2003.
Selectively using re-lations to improve precision in question answering.In In Proceedings of the EACL-2003 Workshop onNatural Language Processing for Question Answer-ing, pages 43?50.Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-des, Gregory Marton, and Federico Mora.
2003.Integrating web-based and corpus-based techniquesfor question answering.
In Proceedings of thetwelfth text retrieval conference (TREC), pages 426?435.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the web withhyponym pattern linkage graphs.
In Proceedings ofACL-08: HLT, pages 1048?1056.Dekang Lin and Patrick Pantel.
2002.
Concept dis-covery from text.
In Proc.
of the 19th internationalconference on Computational linguistics, pages 1?7.Characteristics Of Mt, John Lehrberger, LaurentBourbeau, Philadelphia John Benjamins, and RitaMccardell.
1988.
Machine Translation: LinguisticCharacteristics of Mt Systems and General Method-ology of Evaluation.
John Benjamins PublishingCo(1988-03).Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In Proc.
of Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 321?328.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scale distributional similarity and entity set expan-sion.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 938?947, August.Marius Pasca.
2004.
Acquisition of categorized namedentities for web search.
In Proc.
of the thirteenthACM international conference on Information andknowledge management, pages 137?145.Marco Pennacchiotti and Patrick Pantel.
2006.
On-tologizing semantic relations.
In ACL-44: Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 793?800.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In AAAI ?99/IAAI ?99: Proceedingsof the Sixteenth National Conference on Artificial in-telligence.Ellen Riloff and Jessica Shepherd.
1997.
A Corpus-Based Approach for Building Semantic Lexicons.In Proc.
of the Second Conference on EmpiricalMethods in Natural Language Processing, pages117?124.Brian Roark and Eugene Charniak.
1998.
Noun-phrase co-occurrence statistics for semiautomaticsemantic lexicon construction.
In Proceedings of the17th international conference on Computational lin-guistics, pages 1110?1116.1490Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In Advances in Neural Information Pro-cessing Systems 17, pages 1297?1304.
MIT Press.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In WWW ?07: Proceedings of the 16th inter-national conference on World Wide Web, pages 697?706.Partha Pratim Talukdar, Joseph Reisinger, MariusPasca, Deepak Ravichandran, Rahul Bhagat, andFernando Pereira.
2008.
Weakly-supervised acqui-sition of labeled class instances using graph randomwalks.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP 2008, pages 582?590.Michael Thelen and Ellen Riloff.
2002.
A Bootstrap-ping Method for Learning Semantic Lexicons UsingExtraction Pattern Contexts.
In Proc.
of the 2002Conference on Empirical Methods in Natural Lan-guage Processing, pages 214?221.Yorick Wilks.
1975.
A preferential pattern-seeking,semantics for natural language inference.
ArtificialIntelligence, 6(1):53?74.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
Textrunner: open information ex-traction on the web.
In NAACL ?07: Proceedings ofHuman Language Technologies: The Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Demonstra-tions on XX, pages 25?26.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using se-lectional preferences.
In ACL-44: Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 849?856.1491
