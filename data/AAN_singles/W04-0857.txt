Generative Models for Semantic Role LabelingCynthia A. Thompsoncindi@cs.utah.eduSiddharth Patwardhansidd@cs.utah.eduSchool of ComputingUniversity of UtahSalt Lake City, UT 84112Carolin Arnoldcsarnold@cs.utah.eduAbstractThis paper describes the four entries from the Uni-versity of Utah in the semantic role labeling taskof SENSEVAL-3.
All the entries took a statisti-cal machine learning approach, using the subsetof the FrameNet corpus provided by SENSEVAL-3as training data.
Our approach was to develop amodel of natural language generation from seman-tics, and train the model using maximum likelihoodand smoothing.
Our models performed satisfacto-rily in the competition, and can flexibly handle vary-ing permutations of provided versus inferred infor-mation.1 IntroductionThe goal in the SENSEVAL-3 semantic role labelingtask is to identify roles and optionally constituentboundaries for each role, given a natural languagesentence, target, and frame.
The Utah approach tothis task is to apply machine learning techniquesto create a model capable of semantically analyz-ing unseen sentences.
We have developed a set ofgenerative models (Jordan, 1999) that have the ad-vantages of flexibility, power, and ease of applica-bility for semi-supervised learning scenarios.
Wecan supplement any of the generative models witha constituent classifier that determines, given a sen-tence and parse, which parse constituents are mostlikely to correspond to a role.
We apply the com-bination to the ?hard,?
or restricted version of therole labeling task, in which the system is providedonly with the sentence, target, and frame, and mustdetermine which sentence constituents to label withroles.We discuss our overall model, the constituentclassifier we use in the hard task, and the classifier?suse at role-labeling time.
We entered four sets ofanswers, as discussed in Section 5.
The first twocorrespond to the ?easy?
task, in which the role-bearing constituents ?
those parts of the sentencecorresponding to a role ?
are provided to the systemwith the target and frame.
The second two are vari-ants for the ?hard?
task.
Finally, we discuss FutureWork and conclude the paper.2 Role LabelerOur general approach is to use a generative modeldefining a joint probability distribution over targets,frames, roles, and constituents.
The advantage ofsuch a model is its generality: it can determine theprobability of any subset of the variables given val-ues for the others.
Three of our entries used thegenerative model illustrated in Figure 1, and thefourth used a model grouping all roles together, asdescribed further below.
The first model functionsTFR1R2   RnC1C2   CnFigure 1: First Order Model.as follows.
First, a target, T , is chosen, which thengenerates a frame, F .
The frame generates a (lin-earized) role sequence, R1through Rnwhich inturn generates each constituent of the sentence, C1through Cn.
Note that, conditioned on a particularframe, the model is just a first-order Hidden MarkovModel.The second generative model treats all roles as agroup.
It is no longer based on a Hidden Markovmodel, but all roles are generated, in order, simul-taneously.
Therefore, the role sequence in Figure 1is replaced by a single node containing all n roles.This can be compared to a case-based approach thatmemorizes all seen role sequences and calculatestheir likelihood.
It is also similar to Gildea & Juraf-sky?s (2002) frame element groups, though we dis-tinguish between different role orderings, whereasAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsthey do not.
However, we still model constituentgeneration sequentially.The FrameNet corpus contains annotations for allof the model components described above.
We rep-resent each constituent by its phrasal category to-gether with its head word.
As in Gildea & Jurafsky?s(2002) approach, we determine head words from thesentence?s syntactic parse, using a simple heuristic1when syntactic alignment with a parse is not avail-able.We estimate most of the model parameters us-ing a straightforward maximum likelihood estimatebased on fully labeled training data.
We smoothemission probabilities with phrase type labels, dueto the sparseness of head words.
To label a test ex-ample, consisting of a target, frame, and constituentsequence, with a role label, we use the Viterbi algo-rithm.
For further details, see Thompson (2003).3 Constituent Classification for RoleLabelingTo address the ?hard?
task we build a constituentclassifier, whose goal is to detect the role-bearingconstituents of a sentence.
We use a Naive Bayesclassifier from the Weka Machine Learning toolkit(Witten and Frank, 2000) to classify every sentenceconstituent as role-bearing or not.
In our cross-validation studies, Naive Bayes was both accurateand efficient.
To generate the training examples forthe classifier, we generate a parse tree for every sen-tence in the SENSEVAL-3 training data, using theCollins (1996) statistical parser.
We call each nodein this tree a constituent.
Once it is trained, the clas-sifier can sift through a new constituent list and de-cide which are likely to be role-bearing.
The se-lected constituents are passed on to the Role La-beler for labeling with semantic roles, as describedin Section 4.We train the classifier on examples extracted fromthe SENSEVAL-3 training data.
Each example is alist of attributes corresponding to a constituent ina sentence?s parse tree, along with its classificationas role-bearing or not.
We extract the attributes bytraversing each sentence?s parse tree from the rootnode down to nodes all of whose children are pre-terminals.2 We create a training example for everyvisited node.We decided to use the following attributes fromthe parse trees and FrameNet examples:Target Position: The position of the target word as1The heuristic chooses the preposition for PP?s and the lastword of the phrase for all other phrases.2We later fixed this to traverse the tree to the pre-terminalsthemselves, as discussed further in Section 5.being BEFORE, AFTER, or CONTAINS (containedin) the constituent.Distance from target: The number of words be-tween the start of the constituent and target word.Depth: The depth of the constituent in the parsetree.Height: The number of levels in the parse tree be-low the constituent.Word Count: The number of words in the con-stituent.Path to Target: Gildea and Jurafsky (2002) showthat the path from a constituent node to the nodecorresponding to the target word is a good indicatorthat a constituent corresponds to a role.
We use the35 most frequently occurring paths in the trainingcorpus as attribute values, as these cover about 68%of the paths in the training corpus.
The remainingpaths are specified as ?OTHER?.Length of Path to Target: The number of nodesbetween the constituent and the target in the path.Constituent Phrase TypeTarget POS: The target word?s part-of-speech ?noun, verb, or adjective.Frame IDBy generating examples in the manner describedabove, we create a data set that is heavily biasedtowards negative examples ?
90.8% of the con-stituents are not role bearing.
Therefore, the classi-fier can obtain high accuracy by labeling everythingas negative.
This is undesirable since then no con-stituents would be passed to the Role Labeler.
How-ever, passing all constituents to the labeler wouldcause it to try to label all of them and thus achievelower accuracy.
This results in the classic precision-recall tradeoff.
We chose to try to bias the classifiertowards high recall by using a cost matrix that pe-nalizes missed positive examples more than missednegatives.
The resulting classifier?s cross-validationprecision was 0.19 and its recall was 0.91.
If wedo not use the cost matrix, the precision is 0.30 andthe recall is 0.82.
We are still short of our goal ofperfect recall and reasonable precision, but this pro-vides a good filtering mechanism for the next stepof role labeling.4 Combining Constituent Classificationwith Role LabelingThe constituent classifier correctly picks out most ofthe role bearing constituents.
However, as we haveseen, it still omits some constituents and, as it wasdesigned to, includes several irrelevant constituentsper sentence.
For this paper, because we plan toimprove the constituent classifier further, we did notuse it to bias the Role Labeler at training time, butonly used it to filter constituents at test time for thehard task.When using the classifier with the Role Labelerat testing time, there are two possibilities.
First, allconstituents deemed relevant by the classifier couldbe presented to the labeler.
However, because weaimed for high recall but possibly low precision,this would allow many irrelevant constituents as in-put.
This both lowers accuracy and increases thecomputational complexity of labeling.
The secondpossibility is thus to choose some reasonable subsetof the positively identified constituents to presentto the labeler.
The options we considered were atop-down search, a bottom-up search, and a greedysearch; we chose a top-down search for simplic-ity.
In this case, the algorithm searches from theroot down in the parse tree until it finds a posi-tively labeled constituent.
While this assumes thatno subtree of a role-bearing constituent is also role-bearing, we discovered that some role-bearing con-stituents do overlap with each other in the parsetrees.
However, in the Senseval training corpus,only 1.2% of the sentences contain a (single) over-lapping constituent.
In future work we plan to inves-tigate alternative approaches for constituent choice.After filtering via our top down technique, wepresent the resulting constituent sequence to therole labeler.
Since the role labeler is trained onsequences containing only true role-bearing con-stituents but tested on sequences with potentiallymissing and potentially irrelevant constituents, thisstage provides an opportunity for errors to creep intothe process.
However, because of the Markovian as-sumption, the presence of an irrelevant constituenthas only local effects on the overall choice of a rolesequence.5 EvaluationThe SENSEVAL-3 committee chose 40 of the mostfrequent 100 frames from FrameNet II for thecompetition.
In experiments with validation sets,our algorithm performed better using only theSENSEVAL-3 training data, as opposed to also us-ing sentences from the remaining frames, so all ourmodels were trained only on that data.
We cal-culated performance using SENSEVAL-3?s scoringsoftware.We submitted two set of answers for each task.We summarize each system?s performance in Ta-ble 1.
For the easy task, we used both the grouped(FEG Easy) and first order (FirstOrder Easy) mod-els.
The grouped model performed better on exper-iments with validation sets, perhaps due to the factthat many frames have a small number of possibleSystem Precision Recall OverlapFEG Easy 85.8% 84.9% 85.7%FirstOrder Easy 72.8% 72.1% 72.5%CostSens Hard 38.7% 33.5% 29.5%Hard 35.5% 45.3% 25.5%Table 1: System Scores.System Precision Recall OverlapCostSens Hard 47.2% 42.2% 41.5%Hard 60.2% 24.7% 57.1%Table 2: Newer System Scores.role permutations corresponding to a given numberof constituents.
In less artificial conditions this ver-sion would be less flexible in incorporating both rel-evant and irrelevant constituents.For the hard task, we used only the first ordermodel, due both to its greater flexibility and to thelow precision of our classifier: if all positively clas-sified constituents were passed to the group model,the sequence length would be greater than any seenat training time, when only correct constituents aregiven to the labeler.
We used both the cost sensi-tive classifier (CostSens Hard) and the regular con-stituent classifier to filter constituents (Hard).
Thereis a precision/recall tradeoff in using the differentclassifiers.
We were surprised how poorly our la-beler was performing on validation sets as we pre-pared our results.
We found out that our classi-fier was omitting about 70% of the role-bearingconstituents from consideration, because they onlymatched a parse constituent at a pre-terminal node.We fixed this bug after submission, learned a newconstituent classifier, and used the same role labeleras before.
The improved results are shown in Ta-ble 2.
Note that our recall has an upper limit of85.8% due to mismatches between roles and parsetree constituents.6 Future WorkWe have identified three problems for future re-search.
First, our constituent classifier should beimproved to produce fewer false positives and to in-clude a higher percentage of true positives.
To dothis, we first plan to enhance the feature set.
We willalso explore improved approaches to combining theresults of the classifier with the role labeler.
For ex-ample, in preliminary studies, a bottom-up searchfor positive constituents in the parse tree seems toyield better results than our current top-down ap-proach.Second, since false positives cannot be entirelyavoided, the labeler needs to better handle con-stituents that should not be labeled with a role.
Tosolve this problem, we will adapt the idea of nullgenerated words from machine translation (Brownet al, 1993).
Instead of having a word in the targetlanguage that corresponds to no word in the sourcelanguage, we have a constituent that corresponds tono state in the role sequence.Finally, we will address roles that do not label aconstituent, called null-instantiated roles.
An exam-ple is the sentence ?The man drove to the station,?in which the VEHICLE role does not have a con-stituent, but is implicitly there, since the man obvi-ously drove something to the station.
This problemis more difficult, since it involves obtaining infor-mation not actually in the sentence.
One possibilityis to consider inserting null-instantiated roles at ev-ery step.
We will consider only roles seen as null-instantiated at training time.
This method will re-strict the search space, which would otherwise beextremely large.7 ConclusionsIn conclusion, our generative model performs ro-bustly on the easy version of the SENSEVAL-3 rolelabeling task.
The combination of our constituentclassifier with the role labeling has more room forimprovement, but performed reasonably well con-sidering the difficulties of the task and the sparsefeature set that we incorporated into our generativemodel.
Alternative sentence chunking models forsemantic analysis, and the extension of our gener-ative models, should lead to future improvements.The key advantage of our approach is the treatmentof a sentence?s roles as a sequence.
This allows themodel to consider relationships between roles as itsemantically analyzes a sentence.8 AcknowledgementsThis work was supported in part by the AdvancedResearch and Development Activity?s AdvancedQuestion Answering for Intelligence Program.
Thebasic Role Labeler was developed in collaborationwith Chris Manning and Roger Levy.ReferencesP.
Brown, S. Della Pietra, V. Della Pietra, andR.
Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?311.M.
J. Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceed-ings of the 34th Annual Meeting of the Assoc.for Computational Linguistics, pages 184?191,Santa Cruz, CA.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic labeling of semantic roles.
ComputationalLinguistics, 28:245?288.M.
Jordan, editor.
1999.
Learning in GraphicalModels.
MIT Press, Cambridge, MA.C.
A. Thompson, R. Levy, and C. Manning.
2003.A generative model for semantic role labeling.In Proceedings of the Fourteenth European Con-ference on Machine Learning, pages 397?408,Croatia.I.
Witten and E. Frank.
2000.
Data Mining: Practi-cal Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann, SanFrancisco.
