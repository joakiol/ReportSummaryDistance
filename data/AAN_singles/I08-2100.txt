Non-Factoid Japanese Question Answering through Passage Retrievalthat Is Weighted Based on Types of AnswersMasaki Murata and Sachiyo TsukawakiNational Institute of Information andCommunications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japan{murata,tsuka}@nict.go.jpQing MaRyukoku UniversityOtsu, Shiga, 520-2194, Japanqma@math.ryukoku.ac.jpToshiyuki KanamaruKyoto UniversityYoshida-Nihonmatsu-Cho, SakyoKyoto, 606-8501 Japankanamaru@hi.h.kyoto-u.ac.jpHitoshi IsaharaNational Institute of Information andCommunications Technology3-5 Hikaridai, Seika-cho, Soraku-gun,Kyoto 619-0289, Japanisahara@nict.go.jpAbstractWe constructed a system for answering non-factoid Japanese questions.
We used var-ious methods of passage retrieval for thesystem.
We extracted paragraphs based onterms from an input question and outputthem as the preferred answers.
We classifiedthe non-factoid questions into six categories.We used a particular method for each cate-gory.
For example, we increased the scoresof paragraphs including the word ?reason?for questions including the word ?why.?
Weparticipated at NTCIR-6 QAC-4, where oursystem obtained the most correct answersout of all the eight participating teams.
Therate of accuracy was 0.77, which indicatesthat our methods were effective.1 IntroductionA question-answering system is an application de-signed to produce the correct answer to a questiongiven as input.
For example, when ?What is thecapital of Japan??
is given as input, a question-answering system may retrieve text containing sen-tences like ?Tokyo is Japan?s capital and the coun-try?s largest and most important city?, and ?Tokyois also one of Japan?s 47 prefectures?, from Web-sites, newspaper articles, or encyclopedias.
The sys-tem then outputs ?Tokyo?
as the correct answer.We believe question-answering systems will becomea more convenient alternative to other systems de-signed for information retrieval and a basic compo-nent of future artificial intelligence systems.
Numer-ous researchers have recently been attracted to thisimportant topic.
These researchers have producedmany interesting studies on question-answering sys-tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarkeet al, 2001; Dumis et al, 2002; Magnini et al, 2002;Moldovan et al, 2003).
Evaluation conferences andcontests on question-answering systems have alsobeen held.
In particular, the U.S.A. has held the TextREtrieval Conferences (TREC) (TREC-10 commit-tee, 2001), and Japan has hosted the Question-Answering Challenges (QAC) (National Institute ofInformatics, 2002) at NTCIR (NII Test Collectionfor IR Systems ) 3.
These conferences and contestshave aimed at improving question-answering sys-tems.
The researchers who participate in these createquestion-answering systems that they then use to an-swer the same questions, and each system?s perfor-mance is then evaluated to yield possible improve-ments.We addressed non-factoid question answering inNTCIR-6 QAC-4.
For example, when the questionwas ?Why are people opposed to the Private Infor-mation Protection Law??
the system retrieved sen-tences based on terms appearing in the question andoutput an answer using the retrieved sentences.
Nu-merous studies have addressed issues that are in-volved in the answering of non-factoid questions(Berger et al, 2000; Blair-Goldensohn et al, 2003;727Xu et al, 2003; Soricut and Brill, 2004; Han et al,2005; Morooka and Fukumoto, 2006; Maehara etal., 2006; Asada, 2006).We constructed a system for answering non-factoid Japanese questions for QAC-4.
We usedmethods of passage retrieval for the system.
Weextracted paragraphs based on terms from an inputquestion and output them as the preferred answers.We classified the non-factoid questions into six cat-egories.
We used a particular method for each cate-gory.
For example, we increased the scores of para-graphs including the word ?reason?
for questionsincluding the word ?why.?
We performed exper-iments using the NTCIR-6 QAC-4 data collectionand tested the effectiveness of our methods.2 Categories of Non-Factoid QuestionsWe used six categories of non-factoid questions inthis study.
We constructed the categories by con-sulting the dry run data in QAC-4.1.
Definition-oriented questions (Questions thatrequire a definition to be given in response.
)e.g., K-1 to wa nandesuka?
(What is K-1?)2.
Reason-oriented questions (Questions that re-quire a reason to be given in response.
)e.g., kojin jouhou hokogou ni hantai shiteiruhito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-mation Protection Law?)3.
Method-oriented questions (Questions that re-quire an explanation of a method to be given inresponse.
)e.g., sekai isan wa donoyouni shite kimeru nodesuka??
(How is a World Heritage Site deter-mined?)4.
Degree-oriented questions (Questions that re-quire an explanation of the degree of somethingto be given in response.)5.
Change-oriented questions (Questions that re-quire a description of things that change to begiven in response.
)e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)6.
Detail-oriented questions (Questions that re-quire a description of the particulars or detailssurrounding a sequence of events to be given inresponse.
)e.g., donoyouna keii de ryuukyuu oukoku wa ni-hon no ichibu ni natta no desuka?
(How didRyukyu come to belong to Japan?
)3 Question-answering Systems in thisStudyThe system has three basic components:1.
Prediction of type of answerThe system predicts the answer to be a partic-ular type of expression based on whether theinput question is indicated by an interrogativepronoun, an adjective, or an adverb.
For exam-ple, if the input question is ?Why are peopleopposed to the Private Information ProtectionLaw?
?, the word ?why?
suggests that the an-swer will be an expression that describes a rea-son.2.
Document retrievalThe system extracts terms from the input ques-tion and retrieves documents by using theseterms.
Documents that are likely to containthe correct answer are thus gathered during theretrieval process.
For example, for the inputquestion ?Why are people opposed to the Pri-vate Information Protection Law?
?, the systemextracts ?people,?
?opposed,?
?Private,?
?Infor-mation,?
?Protection,?
and ?Law?
as terms andretrieves the appropriate documents based onthese.3.
Answer detectionThe system separates the retrieved documentsinto paragraphs and retrieves those that containterms from the input question and a clue ex-pression (e.g., ?to wa?
(copula sentence) for thedefinition sentence).
The system outputs the re-trieved paragraphs as the preferred answer.3.1 Prediction of type of answerWe used the following rules for predicting the typeof answer.
We constructed the rules by consultingthe dry run data in QAC-4.7281.
Definition-oriented questions Questions in-cluding expressions such as ?to wa nani,??donna,?
?douiu,?
?douitta,?
?nanimono,?
?donoyouna mono,?
?donna mono,?
and ?douiukoto?
(which all mean ?what is?)
are rec-ognized by the system as being definition-oriented questions.2.
Reason-oriented questions Questions includingexpressions such as ?naze?
(why), ?naniyue?
(why), ?doushite?
(why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system asbeing reason-oriented questions.3.
Method-oriented questions Questions includ-ing expressions such as ?dou,?
?dousureba,??douyatte,?
?dono youni shite,?
?ikani shite,??ikani,?
and ?donnna houhou de?
(which allmean ?how?)
are recognized by the system asbeing method-oriented questions.4.
Degree-oriented questions Questions includingexpressions such as ?dorekurai?
(how much),?dorekurai no?
(to what extent), and ?donoteido?
(to what extent), are recognized by thesystem as being degree-oriented questions.5.
Change-oriented questions Questions includ-ing expressions such as ?naniga chigau?
(Whatis different), ?donoyuni kawaru?
(How is ...changed), and ?dokoga kotonaru?
(What is dif-ferent), are recognized by the system as beingchange-oriented questions.6.
Detail-oriented questions Questions includingexpressions such as ?dono you na keii,?
?donoyou na ikisatsu,?
and ?dono you na nariyuki?
(which all mean ?how was?)
are recognized bythe system as being detail-oriented questions.3.2 Document retrievalOur system extracts terms from a question by usingthe morphological analyzer, ChaSen (Matsumoto etal., 1999).
The analyzer first eliminates preposi-tions, articles, and similar parts of speech.
It thenretrieves documents by using the extracted terms.The documents are retrieved as follows:We first retrieve the top kdr1documents with thehighest scores calculated using the equationScore(d)=?term t??
?tf(d, t)tf(d, t) + ktlength(d) + k+?
+ k+?
logNdf(t)??
?,(1)where d is a document, t is a term extracted froma question, and tf(d, t) is the frequency of t oc-curring in d. Here, df(t) is the number of docu-ments in which t appears, N is the total numberof documents, length(d) is the length of d, and ?is the average length of all documents.
Constantsktand k+are defined based on experimental re-sults.
We based this equation on Robertson?s equa-tion (Robertson and Walker, 1994; Robertson et al,1994).
This approach is very effective, and we haveused it extensively for information retrieval (Murataet al, 2000; Murata et al, 2001; Murata et al, 2002).The question-answering system uses a large numberfor kt.We extracted the top 300 documents and usedthem in the next procedure.3.3 Answer detectionIn detecting answers, our system first generates can-didate expressions for them from the extracted docu-ments.
We use two methods for extracting candidateexpressions.
Method 1 uses a paragraph as a candi-date expression.
Method 2 uses a paragraph, twocontinuous paragraphs, or three continuous para-graphs as candidate expressions.We award each candidate expression the follow-ing score.Score(d)= ?mint1?T log?t2?T3(2dist(t1, t2)df(t2)N)+ 0.00000001 ?
length(d)= maxt1?T?t2?T3logN2dist(t1, t2) ?
df(t2)+ 0.00000001 ?
length(d)(2)729T3 = {t|t ?
T, 2dist(t1, t)df(t)N?
1}, (3)where d is a candidate expression, T is the set ofterms in the question, dist(t1, t2) is the distancebetween t1 and t2 (defined as the number of char-acters between them with dist(t1, t2) = 0.5 whent1 = t2), and length(d) is the number of charac-ters in a candidate expression.
The numerical term,0.00000001 ?
length(d), is used for increasing thescores of long paragraphs.For reason-oriented questions, our system usessome reason terms such as ?riyuu?
(reason),?gen?in?
(cause), and ?nazenara?
(because) as termsfor Eq.
2 in addition to terms from the input ques-tion.
This is because we would like to increase thescore of a document that includes reason terms forreason-oriented questions.For method-oriented questions, our system usessome method terms such as ?houhou?
(method),?tejun?
(procedure), and ?kotoniyori?
(by doing) asterms for second document retrieval (re-ranking) inaddition to terms from the input question.For detail-oriented questions, our system usessome method terms such as ?keii?
(a detail, or a se-quence of events), ?haikei?
(background), and ?rek-ishi?
(history) as terms for second document re-trieval (re-ranking) in addition to terms from the in-put question.For degree-oriented questions, when candidateparagraphs include numerical expressions, the score(Score(d)) is multiplied by 1.1.For definition-oriented questions, the system firstextracts focus expressions.
When the question in-cludes expressions such as ?X-wa?, ?X-towa?, ?X-toiunowa?, and ?X-tte?, X is extracted as a fo-cus expression.
The system multiplies the score,(Score(d)), of the candidate paragraph having ?X-wa?, ?X-towa or something by 1.1.
When the can-didate expression includes focus expressions havingmodifiers (including modifier clauses and modifierphrases), the modifiers are used as candidate expres-sions, and the scores of the candidate expressions aremultiplied by 1.1.Below is an example of a candidate expressionthat is a modifier clause in a sentence.Table 1: ResultsMethod Correct A B C DMethod 1 57 18 42 10 89Method 2 77 5 67 19 90(There were a total of 100 questions.
)Question sentence: sekai isan jouyaku towa dono youna jouyaku desu ka?
(What is the Convention concerning theProtection of the World Cultural and Nat-ural Heritage?
)Sentence including answers:1972 nen no dai 17 kai yunesuko soukai desaitaku sareta sekai isan jouyaku ....(Convention concerning the Pro-tection of the World Culturaland Natural Heritage, whichwas adopted in 1972 in the 17th gen-eral assembly meeting of the UN Educational,Scientific and Cultural Organization.
)Finally, our system extracts candidate expressionshaving high scores, (Score(d)s), as the preferredoutput.
Our system extracts candidate expressionshaving scores that are no less than the highest scoremultiplied by 0.9 as the preferred output.We constructed the methods for answer detectionby consulting the dry run data in QAC-4.4 ExperimentsThe experimental results are listed in Table 1.
Onehundred non-factoid questions were used in the ex-periment.
The questions, which were generated bythe QAC-4 organizers, were natural and not gener-ated by using target documents.
The QAC-4 orga-nizers checked four or fewer outputs for each ques-tion.
Methods 1 and 2 were used to determine whatwe used as answer candidate expressions (Method 1uses one paragraph as a candidate answer.
Method2 uses one paragraph, two paragraphs, or three para-graphs as candidate answers.).?A,?
?B,?
?C,?
and ?D?
are the evaluation criteria.?A?
indicates output that describes the same contentas that in the answer.
Even if there is a supplemen-tary expression in the output, which does not change730the content, the output is judged to be ?A.?
?B?
in-dicates output that contains some content similar tothat in the answer but contains different overall con-tent.
?C?
indicates output that contains part of thesame content as that in the answer.
?D?
indicatesoutput does not contain any of the same content asthat in the answer.
The numbers for ?A,?
?B,?
?C,?and ?D?
in Table 1 indicate the number of questionswhere an output belongs to ?A,?
?B,?
?C,?
and ?D?.?Correct?
indicates the number of questions wherean output belongs to ?A,?
?B,?
or ?C?.
The evalu-ation criteria ?Correct?
was also used officially atNTCIR-6 QAC-4.We found the following.?
Method 1 obtained higher scores in evaluationA than Method 2.
This indicates that Method 1can extract a completely relevant answer moreaccurately than Method 2.?
Method 2 obtained higher scores in evaluation?Correct?
than Method 1.
The rate of accuracyfor Method 2 was 0.77 according to evaluation?Correct?.
This indicates that Method 2 can ex-tract more partly relevant answers than Method1.
When we want to extract completely relevantanswers, we should use Method 1.
When wewant to extract more answers, including partlyrelevant answers, we should use Method 2.?
Method 2 was the most accurate (0.77) of thoseused by all eight participating teams.
We coulddetect paragraphs as answers including inputterms and the key terms related to answer typesbased the methods discussed in Section 3.3.Our system obtained the best results becauseour method of detecting answers was the mosteffective.Below is an example of the output of Method 1,which was judged to be ?A.
?Question sentence:jusei ran shindan wa douiu baai ni okon-awareru noka?
(When is amniocentesis performed on apregnant woman?
)System output:omoi idenbyou no kodono ga umareru nowo fusegu.
(To prevent the birth of children with seri-ous genetic disorders )Examples of answers given by organizers:omoi idenbyou(A serious genetic disorder)omoi idenbyou no kodomo ga umarerukanousei ga takai baai(To prevent the birth of children with seri-ous genetic disorders.
)5 ConclusionWe constructed a system for answering non-factoidJapanese questions.
An example of a non-factoidquestion is ?Why are people opposed to the Pri-vate Information Protection Law??
We used vari-ous methods of passage retrieval for the system.
Weextracted paragraphs based on terms from an inputquestion and output them as the preferred answers.We classified the non-factoid questions into six cat-egories.
We used a particular method for each cate-gory.
For example, we increased the scores of para-graphs including the word ?reason?
for questions in-cluding the word ?why.?
We participated at NTCIR-6 QAC-4, where our system obtained the most cor-rect answers out of all the eight participating teams.The rate of accuracy was 0.77, which indicates thatour methods were effective.We would like to apply our method and system toWeb data in the future.
We would like to construct asophisticated system that can answer many kinds ofcomplicated queries such as non-factoid questionsbased on a large amount of Web data.AcknowledgementsWe are grateful to all the organizers of NTCIR-6who gave us the chance to participate in their con-test to evaluate and improve our question-answeringsystem.
We greatly appreciate the kindness of allthose who helped us.731ReferencesYoshiaki Asada.
2006.
Processing of definition typequestions in a question answering system.
Master?sthesis, Yokohama National University.
(in Japanese).AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,and Vibhu Mittal.
2000.
Bridging the lexical chasm:Statistical approaches to answer-finding.
In Proceed-ings of the 23rd annual international ACM SIGIR con-ference on Research and development in informationretrieval (SIGIR-2000), pages 192?199.Sasha Blair-Goldensohn, Kathleen R. McKeown, andAndrew Hazen Schlaikjer.
2003.
A hybrid approachfor qa track definitional questions.
In Proceedingsof the 12th Text Retrieval Conference (TREC-2003),pages 185?192.Charles L. A. Clarke, Gordon V. Cormack, andThomas R. Lynam.
2001.
Exploiting redundancyin question answering.
In Proceedings of the 24thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, andAndrew Ng.
2002.
Web question answering: Is morealways better?
In Proceedings of the 25th Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval.Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, andHae-Chang Rim.
2005.
Phrase-based definitionalquestion answering using definition terminology.
InLecture Notes in Computer Science 3689, pages 246?259.Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, andAdwait Ratnaparkhi.
2001.
IBM?s Statistical Ques-tion Answering System.
In TREC-9 Proceedings.Julian Kupiec.
1993.
MURAX: A robust linguistic ap-proach for question answering using an on-line ency-clopedia.
In Proceedings of the Sixteenth Annual In-ternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval.Hideyuki Maehara, Jun?ichi Fukumoto, and NorikoKando.
2006.
A BE-based automated evaluationfor question-answering system.
IEICE-WGNLC2005-109, pages 19?24.
(in Japanese).Bernardo Magnini, Matto Negri, Roberto Prevete, andHristo Tanev.
2002.
Is it the right answer?
Exploitingweb redundancy for answer validation.
In Proceed-ings of the 41st Annual Meeting of the Association forComputational Linguistics.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Hiroshi Matsuda, and MasayukiAsahara.
1999.
Japanese morphological analysis sys-tem ChaSen version 2.0 manual 2nd edition.Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-hai Surdeanu.
2003.
Performance issues and er-ror analysis in an open-domain question answeringsystem.
ACM Transactions on Information Systems,21(2):133?154.Kokoro Morooka and Jun?ichi Fukumoto.
2006.
Answerextraction method for why-type question answeringsystem.
IEICE-WGNLC2005-107, pages 7?12.
(inJapanese).Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000.Japanese probabilistic information retrieval using lo-cation and category information.
The Fifth Interna-tional Workshop on Information Retrieval with AsianLanguages, pages 81?88.Masaki Murata, Masao Utiyama, Qing Ma, HiromiOzaku, and Hitoshi Isahara.
2001.
CRL at NTCIR2.Proceedings of the Second NTCIR Workshop Meetingon Evaluation of Chinese & Japanese Text Retrievaland Text Summarization, pages 5?21?5?31.Masaki Murata, Qing Ma, and Hitoshi Isahara.
2002.High performance information retrieval using manycharacteristics and many techniques.
Proceedings ofthe Third NTCIR Workshop (CLIR).National Institute of Informatics.
2002.
Proceedings ofthe Third NTCIR Workshop (QAC).S.
E. Robertson and S. Walker.
1994.
Some simpleeffective approximations to the 2-Poisson model forprobabilistic weighted retrieval.
In Proceedings of theSeventeenth Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval.S.
E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford.
1994.
Okapi at TREC-3.In TREC-3.Radu Soricut and Eric Brill.
2004.
Automatic questionanswering: Beyond the factoid.
In In Proceedingsof the Human Language Technology and Conferenceof the North American Chapter of the Association forComputational Linguistics (HLT-NAACL-2004), pages57?64.TREC-10 committee.
2001.
The tenth text retrieval con-ference.
http://trec.nist.gov/pubs/trec10/t10 proceed-ings.html.Jinxi Xu, Ana Licuanan, and Ralph Weischedel.
2003.TREC 2003 QA at BBN: answering definitional ques-tions.
In Proceedings of the 12th Text Retrieval Con-ference (TREC-2003), pages 98?106.732
