Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302?310,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Context Free TAG VariantBen SwansonBrown UniversityProvidence, RIchonger@cs.brown.eduEugene CharniakBrown UniversityProvidence, RIec@cs.brown.eduElif YamangilHarvard UniversityCambridge, MAelif@eecs.harvard.eduStuart ShieberHarvard UniversityCambridge, MAshieber@eecs.harvard.eduAbstractWe propose a new variant of Tree-Adjoining Grammar that allows adjunc-tion of full wrapping trees but still bearsonly context-free expressivity.
We providea transformation to context-free form, anda further reduction in probabilistic modelsize through factorization and pooling ofparameters.
This collapsed context-freeform is used to implement efficient gram-mar estimation and parsing algorithms.We perform parsing experiments the PennTreebank and draw comparisons to Tree-Substitution Grammars and between dif-ferent variations in probabilistic model de-sign.
Examination of the most probablederivations reveals examples of the lin-guistically relevant structure that our vari-ant makes possible.1 IntroductionWhile it is widely accepted that natural languageis not context-free, practical limitations of ex-isting algorithms motivate Context-Free Gram-mars (CFGs) as a good balance between model-ing power and asymptotic performance (Charniak,1996).
In constituent-based parsing work, the pre-vailing technique to combat this divide betweenefficient models and real world data has been toselectively strengthen the dependencies in a CFGby increasing the grammar size through methodssuch as symbol refinement (Petrov et al, 2006).Another approach is to employ a more power-ful grammatical formalism and devise constraintsand transformations that allow use of essential ef-ficient algorithms such as the Inside-Outside al-gorithm (Lari and Young, 1990) and CYK pars-ing.
Tree-Adjoining Grammar (TAG) is a naturalstarting point for such methods as it is the canoni-cal member of the mildly context-sensitive family,falling just above CFGs in the hierarchy of for-mal grammars.
TAG has a crucial advantage overCFGs in its ability to represent long distance in-teractions in the face of the interposing variationsthat commonly manifest in natural language (Joshiand Schabes, 1997).
Consider, for example, thesentencesThese pretzels are making me thirsty.These pretzels are not making me thirsty.These pretzels that I ate are making me thirsty.Using a context-free language model withproper phrase bracketing, the connection betweenthe words pretzels and thirsty must be recordedwith three separate patterns, which can lead topoor generalizability and unreliable sparse fre-quency estimates in probabilistic models.
Whilethese problems can be overcome to some extentwith large amounts of data, redundant representa-tion of patterns is particularly undesirable for sys-tems that seek to extract coherent and concise in-formation from text.TAG allows a linguistically motivated treatmentof the example sentences above by generating thelast two sentences through modification of thefirst, applying operations corresponding to nega-tion and the use of a subordinate clause.
Un-fortunately, the added expressive power of TAGcomes with O(n6) time complexity for essentialalgorithms on sentences of length n, as opposed toO(n3) for the CFG (Schabes, 1990).
This makesTAG infeasible to analyze real world data in a rea-sonable time frame.In this paper, we define OSTAG, a new way toconstrain TAG in a conceptually simple way so302SNP VP NPNPDTtheNNlackNPNNScomputersVPVBPdoRBnotVPNPNP PPNPPRPIPPINofPRPthemVPVBfearFigure 1: A simple Tree-Substitution Grammar using S as its start symbol.
This grammar derives thesentences from a quote of Isaac Asimov?s - ?I do not fear computers.
I fear the lack of them.
?that it can be reduced to a CFG, allowing the use oftraditional cubic-time algorithms.
The reduction isreversible, so that the original TAG derivation canbe recovered exactly from the CFG parse.
We pro-vide this reduction in detail below and highlightthe compression afforded by this TAG variant onsynthetic formal languages.We evaluate OSTAG on the familiar task ofparsing the Penn Treebank.
Using an automati-cally induced Tree-Substitution Grammar (TSG),we heuristically extract an OSTAG and estimateits parameters from data using models with var-ious reduced probabilistic models of adjunction.We contrast these models and investigate the useof adjunction in the most probable derivations ofthe test corpus, demonstating the superior model-ing performance of OSTAG over TSG.2 TAG and VariantsHere we provide a short history of the relevantwork in related grammar formalisms, leading upto a definition of OSTAG.
We start with context-free grammars, the components of which are?N,T,R, S?, where N and T are the sets of non-terminal and terminal symbols respectively, and Sis a distinguished nonterminal, the start symbol.The rules R can be thought of as elementary treesof depth 1, which are combined by substituting aderived tree rooted at a nonterminalX at some leafnode in an elementary tree with a frontier nodelabeled with that same nonterminal.
The derivedtrees rooted at the start symbol S are taken to bethe trees generated by the grammar.2.1 Tree-Substitution GrammarBy generalizing CFG to allow elementary trees inR to be of depth greater than or equal to 1, weget the Tree-Substitution Grammar.
TSG remainsin the family of context-free grammars, as can beeasily seen by the removal of the internal nodesin all elementary trees; what is left is a CFG thatgenerates the same language.
As a reversible al-ternative that preserves the internal structure, an-notation of each internal node with a unique indexcreates a large number of deterministic CFG rulesthat record the structure of the original elementarytrees.
A more compact CFG representation can beobtained by marking each node in each elemen-tary tree with a signature of its subtree.
This trans-form, presented by Goodman (2003), can rein inthe grammar constant G, as the crucial CFG algo-rithms for a sentence of length n have complexityO(Gn3).A simple probabilistic model for a TSG is a setof multinomials, one for each nonterminal in Ncorresponding to its possible substitutions in R. Amore flexible model allows a potentially infinitenumber of substitution rules using a Dirichlet Pro-cess (Cohn et al, 2009; Cohn and Blunsom, 2010).This model has proven effective for grammar in-duction via Markov Chain Monte Carlo (MCMC),in which TSG derivations of the training set are re-peatedly sampled to find frequently occurring el-ementary trees.
A straightforward technique forinduction of a finite TSG is to perform this non-parametric induction and select the set of rules thatappear in at least one sampled derivation at one orseveral of the final iterations.2.2 Tree-Adjoining GrammarTree-adjoining grammar (TAG) (Joshi, 1985;Joshi, 1987; Joshi and Schabes, 1997) is an exten-sion of TSG defined by a tuple ?N,T,R,A, S?,and differs from TSG only in the addition of a303VPalways VPVP* quickly+ SNP VPruns?
SNP VPalways VPVPrunsquicklyFigure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)to form a new derivation (right).
The adjunction site is circled, and the foot node of the auxiliary tree isdenoted with an asterisk.
The OSTAG constraint would disallow further adjunction at the bold VP nodeonly, as it is along the spine of the auxiliary tree.set of auxiliary trees A and the adjunction oper-ation that governs their use.
An auxiliary tree ?is an elementary tree containing a single distin-guished nonterminal leaf, the foot node, with thesame symbol as the root of ?.
An auxiliary treewith root and foot node X can be adjoined into aninternal node of an elementary tree labeled withX by splicing the auxiliary tree in at that internalnode, as pictured in Figure 2.
We refer to the pathbetween the root and foot nodes in an auxiliarytree as the spine of the tree.As mentioned above, the added power affordedby adjunction comes at a serious price in timecomplexity.
As such, probabilistic modeling forTAG in its original form is uncommon.
However,a large effort in non-probabilistic grammar induc-tion has been performed through manual annota-tion with the XTAG project(Doran et al, 1994).2.3 Tree Insertion GrammarTree Insertion Grammars (TIGs) are a longstand-ing compromise between the intuitive expressivityof TAG and the algorithmic simplicity of CFGs.Schabes and Waters (1995) showed that by re-stricting the form of the auxiliary trees in A andthe set of auxiliary trees that may adjoin at par-ticular nodes, a TAG generates only context-freelanguages.
The TIG restriction on auxiliary treesstates that the foot node must occur as either theleftmost or rightmost leaf node.
This introducesan important distinction between left, right, andwrapping auxiliary trees, of which only the firsttwo are allowed in TIG.
Furthermore, TIG disal-lows adjunction of left auxiliary trees on the spinesof right auxiliary trees, and vice versa.
This isto prevent the construction of wrapping auxiliarytrees, whose removal is essential for the simplifiedcomplexity of TIG.Several probabilistic models have been pro-posed for TIG.
While earlier approaches such asHwa (1998) and Chiang (2000) relied on hueristicinduction methods, they were nevertheless sucess-ful at parsing.
Later approaches (Shindo et al,2011; Yamangil and Shieber, 2012) were able toextend the non-parametric modeling of TSGs toTIG, providing methods for both modeling andgrammar induction.2.4 OSTAGOur new TAG variant is extremely simple.
We al-low arbitrary initial and auxiliary trees, and placeonly one restriction on adjunction: we disallowadjunction at any node on the spine of an aux-iliary tree below the root (though we discuss re-laxing that constraint in Section 4.2).
We refer tothis variant as Off Spine TAG (OSTAG) and notethat it allows the use of full wrapping rules, whichare forbidden in TIG.
This targeted blocking ofrecursion has similar motivations and benefits tothe approximation of CFGs with regular languages(Mohri and jan Nederhof, 2000).The following sections discuss in detail thecontext-free nature of OSTAG and alternativeprobabilistic models for its equivalent CFG form.We propose a simple but empirically effectiveheuristic for grammar induction for our experi-ments on Penn Treebank data.3 Transformation to CFGTo demonstrate that OSTAG has only context-free power, we provide a reduction to context-freegrammar.
Given an OSTAG ?N,T,R,A, S?, wedefine the set N of nodes of the correspondingCFG to be pairs of a tree inR orA together with an304?
: STxTy?
: Ta T* a?
: Tb T* bS ?
X Y S ?
X YX ?
x X ?
xY ?
y Y ?
yX ?
AX ?
BY ?
A?Y ?
B?A ?
a X ?
a X ?
a X aA?
?
a Y ?
a Y ?
a Y aX ?
?
XY ?
?
YB ?
b X ??
b X ?
b X bB?
?
b Y ??
b Y ?
b Y bX ??
?
XY ??
?
Y(a) (b) (c)Figure 3: (a) OSTAG for the language wxwRvyvR where w, v ?
{a|b}+ and R reverses a string.
(b) ACFG for the same language, which of necessity must distinguish between nonterminalsX and Y playingthe role of T in the OSTAG.
(c) Simplified CFG, conflating nonterminals, but which must still distinguishbetween X and Y .address (Gorn number (Gorn, 1965)) in that tree.We take the nonterminals of the target CFG gram-mar to be nodes or pairs of nodes, elements of thesetN +N ?N .
We notate the pairs of nodes witha kind of ?applicative?
notation.
Given two nodes?
and ?
?, we notate a target nonterminal as ?(??
).Now for each tree ?
and each interior node ?in ?
that is not on the spine of ?
, with children?1, .
.
.
, ?k, we add a context-free rule to the gram-mar?
?
?1 ?
?
?
?k (1)and if interior node ?
is on the spine of ?
with?s the child node also on the spine of ?
(that is,dominating the foot node of ? )
and ??
is a node (inany tree) where ?
is adjoinable, we add a rule?(??)?
?1 ?
?
?
?s(??)
?
?
?
?k .
(2)Rules of type (1) handle the expansion of a nodenot on the spine of an auxiliary tree and rules oftype (2) a spinal node.In addition, to initiate adjunction at any node ?
?where a tree ?
with root ?
is adjoinable, we use arule??
?
?(??)
(3)and for the foot node ?f of ?
, we use a rule?f (?)?
?
(4)The OSTAG constraint follows immediatelyfrom the structure of the rules of type (2).
Anychild spine node ?s manifests as a CFG nonter-minal ?s(??).
If child spine nodes themselves al-lowed adjunction, we would need a type (3) ruleof the form ?s(??)
?
?s(??)(???).
This rule itselfwould feed adjunction, requiring further stackingof nodes, and an infinite set of CFG nonterminalsand rules.
This echoes exactly the stacking foundin the LIG reduction of TAG .To handle substitution, any frontier node ?
thatallows substitution of a tree rooted with node ?
?engenders a rule?
?
??
(5)This transformation is reversible, which is tosay that each parse tree derived with this CFG im-plies exactly one OSTAG derivation, with substi-tutions and adjunctions coded by rules of type (5)and (3) respectively.
Depending on the definitionof a TAG derivation, however, the converse is notnecessarily true.
This arises from the spurious am-biguity between adjunction at a substitution site(before applying a type (5) rule) versus the sameadjunction at the root of the substituted initial tree(after applying a type (5) rule).
These choiceslead to different derivations in CFG form, but theirTAG derivations can be considered conceptually305identical.
To avoid double-counting derivations,which can adversely effect probabilistic modeling,type (3) and type (4) rules in which the side withthe unapplied symbol is a nonterminal leaf can beomitted.3.1 ExampleThe grammar of Figure 3(a) can be converted toa CFG by this method.
We indicate for each CFGrule its type as defined above the production arrow.All types are used save type (5), as substitutionis not employed in this example.
For the initialtree ?, we have the following generated rules (withnodes notated by the tree name and a Gorn numbersubscript):? 1??
?1 ?2 ?1 3??
?(?1)?1 1??
x ?1 3??
?(?1)?2 1??
y ?2 3??
?(?2)?2 3??
?(?2)For the auxiliary trees ?
and ?
we have:?(?1) 2??
a ?1(?1) a?(?2) 2??
a ?1(?2) a?1(?1) 4??
?1?1(?2) 4??
?2?(?1) 2??
b ?1(?1) b?(?2) 2??
b ?1(?2) b?1(?1) 4??
?1?1(?2) 4??
?2The grammar of Figure 3(b) is simply a renamingof this grammar.4 Applications4.1 Compact grammarsThe OSTAG framework provides some leverage inexpressing particular context-free languages morecompactly than a CFG or even a TSG can.
Asan example, consider the language of bracketedpalindromesPal = aiw aiwR ai1 ?
i ?
kw ?
{bj | 1 ?
j ?
m}?containing strings like a2 b1b3 a2 b3b1 a2.
AnyTSG for this language must include as substringssome subpalindrome constituents for long enoughstrings.
Whatever nonterminal covers such astring, it must be specific to the a index withinit, and must introduce at least one pair of bs aswell.
Thus, there are at least m such nontermi-nals, each introducing at least k rules, requiring atleast km rules overall.
The simplest such gram-mar, expressed as a CFG, is in Figure 4(a).
Theability to use adjunction allows expression of thesame language as an OSTAG with k +m elemen-tary trees (Figure 4(b)).
This example shows thatan OSTAG can be quadratically smaller than thecorresponding TSG or CFG.4.2 ExtensionsThe technique in OSTAG can be extended to ex-pand its expressiveness without increasing gener-ative capacity.First, OSTAG allows zero adjunctions on eachnode on the spine below the root of an auxiliarytree, but any non-zero finite bound on the num-ber of adjunctions allowed on-spine would simi-larly limit generative capacity.
The tradeoff is inthe grammar constant of the effective probabilis-tic CFG; an extension that allows k levels of onspine adjunction has a grammar constant that isO(|N |(k+2)).Second, the OSTAG form of adjunction is con-sistent with the TIG form.
That is, we can extendOSTAG by allowing on-spine adjunction of left- orright-auxiliary trees in keeping with the TIG con-straints without increasing generative capacity.4.3 Probabilistic OSTAGOne major motivation for adherence to a context-free grammar formalism is the ability to employalgorithms designed for probabilistic CFGs suchas the CYK algorithm for parsing or the Inside-Outside algorithm for grammar estimation.
In thissection we present a probabilistic model for an OS-TAG grammar in PCFG form that can be used insuch algorithms, and show that many parametersof this PCFG can be pooled or set equal to one andignored.
References to rules of types (1-5) belowrefer to the CFG transformation rules defined inSection 3.
While in the preceeding discussion weused Gorn numbers for clarity, our discussion ap-plies equally well for the Goodman transform dis-cussed above, in which each node is labeled with asignature of its subtree.
This simply redefines ?
inthe CFG reduction described in Section 3 to be asubtree indicator, and dramatically reduces redun-dancy in the generated grammar.306S ?
ai Ti aiTi ?
bj Ti bjTi ?
ai?i | 1 ?
i ?
k: Sai Taiai?j | 1 ?
j ?
m: Tbj T* bj(a) (b)Figure 4: A CFG (a) and more compact OSTAG (b) for the language PalThe first practical consideration is that CFGrules of type (2) are deterministic, and as suchwe need only record the rule itself and no asso-ciated parameter.
Furthermore, these rules employa template in which the stored symbol appears inthe left-hand side and in exactly one symbol onthe right-hand side where the spine of the auxil-iary tree proceeds.
One deterministic rule existsfor this template applied to each ?, and so we mayrecord only the template.
In order to perform CYKor IO, it is not even necessary to record the indexin the right-hand side where the spine continues;these algorithms fill a chart bottom up and we cansimply propagate the stored nonterminal up in thechart.CFG rules of type (4) are also deterministic anddo not require parameters.
In these cases it is notnecessary to record the rules, as they all have ex-actly the same form.
All that is required is a checkthat a given symbol is adjoinable, which is true forall symbols except nonterminal leaves and appliedsymbols.
Rules of type (5) are necessary to cap-ture the probability of substitution and so we willrequire a parameter for each.At first glance, it would seem that due to theidentical domain of the left-hand sides of rules oftypes (1) and (3) a parameter is required for eachsuch rule.
To avoid this we propose the follow-ing factorization for the probabilistic expansion ofan off spine node.
First, a decision is made as towhether a type (1) or (3) rule will be used; this cor-responds to deciding if adjunction will or will nottake place at the node.
If adjunction is rejected,then there is only one type (1) rule available, andso parameterization of type (1) rules is unneces-sary.
If we decide on adjunction, one of the avail-able type (3) rules is chosen from a multinomial.By conditioning the probability of adjunction onvarying amounts of information about the node,alternative models can easily be defined.5 ExperimentsAs a proof of concept, we investigate OSTAG inthe context of the classic Penn Treebank statisticalparsing setup; training on section 2-21 and testingon section 23.
For preprocessing, words that oc-cur only once in the training data are mapped tothe unknown categories employed in the parser ofPetrov et al (2006).
We also applied the annota-tion from Klein and Manning (2003) that appends?-U?
to each nonterminal node with a single child,drastically reducing the presence of looping unarychains.
This allows the use of a coarse to fineparsing strategy (Charniak et al, 2006) in whicha sentence is first parsed with the Maximum Like-lihood PCFG and only constituents whose prob-ability exceeds a cutoff of 10?4 are allowed inthe OSTAG chart.
Designed to facilitate sister ad-junction, we define our binarization scheme by ex-ample in which the added nodes, indicated by @,record both the parent and head child of the rule.NP@NN-NP@NN-NPDT @NN-NPJJ NNSBARA compact TSG can be obtained automaticallyusing the MCMC grammar induction technique ofCohn and Blunsom (2010), retaining all TSG rulesthat appear in at least one derivation in after 1000iterations of sampling.
We use EM to estimate theparameters of this grammar on sections 2-21, anduse this as our baseline.To generate a set of TAG rules, we considereach rule in our baseline TSG and find all possi-307All 40 #Adj #WrapTSG 85.00 86.08 ?
?TSG?
85.12 86.21 ?
?OSTAG1 85.42 86.43 1336 52OSTAG2 85.54 86.56 1952 44OSTAG3 85.86 86.84 3585 41Figure 5: Parsing F-Score for the models undercomparison for both the full test set and sentencesof length 40 or less.
For the OSTAG models, welist the number of adjunctions in the MPD of thefull test set, as well as the number of wrappingadjunctions.ble auxiliary root and foot node pairs it contains.For each such root/foot pair, we include the TAGrule implied by removal of the structure above theroot and below the foot.
We also include the TSGrule left behind when the adjunction of this auxil-iary tree is removed.
To be sure that experimentalgains are not due to this increased number of TSGinitial trees, we calculate parameters using EM forthis expanded TSG and use it as a second base-line (TSG?).
With our full set of initial and aux-iliary trees, we use EM and the PCFG reductiondescribed above to estimate the parameters of anOSTAG.We investigate three models for the probabil-ity of adjunction at a node.
The first uses a con-servative number of parameters, with a Bernoullivariable for each symbol (OSTAG1).
The secondemploys more parameters, conditioning on boththe node?s symbol and the symbol of its leftmostchild (OSTAG2).The third is highly parameterizedbut most prone to data sparsity, with a separateBernoulli distribution for each Goodman index ?(OSTAG3).
We report results for Most ProbableDerivation (MPD) parses of section 23 in Figure5.Our results show that OSTAG outperforms bothbaselines.
Furthermore, the various parameteri-zations of adjunction with OSTAG indicate that,at least in the case of the Penn Treebank, thefiner grained modeling of a full table of adjunctionprobabilities for each Goodman index OSTAG3overcomes the danger of sparse data estimates.Not only does such a model lead to better parsingperformance, but it uses adjunction more exten-sively than its more lightly parameterized alterna-tives.
While different representations make directcomparison inappropriate, the OSTAG results liein the same range as previous work with statisticalTIG on this task, such as Chiang (2000) (86.00)and Shindo et al (2011) (85.03).The OSTAG constraint can be relaxed as de-scribed in Section 4.2 to allow any finite number ofon-spine adjunctions without sacrificing context-free form.
However, the increase to the grammarconstant quickly makes parsing with such modelsan arduous task.
To determine the effect of such arelaxation, we allow a single level of on-spine ad-junction using the adjunction model of OSTAG1,and estimate this model with EM on the trainingdata.
We parse sentences of length 40 or less insection 23 and observe that on-spine adjunction isnever used in the MPD parses.
This suggests thatthe OSTAG constraint is reasonable, at least forthe domain of English news text.We performed further examination of the MPDusing OSTAG for each of the sentences in the testcorpus.
As an artifact of the English language, themajority have their foot node on the left spine andwould also be usable by TIG, and so we discussthe instances of wrapping auxiliary trees in thesederivations that are uniquely available to OSTAG.We remove binarization for clarity and denote thefoot node with an asterisk.A frequent use of wrapping adjunction is to co-ordinate symbols such as quotes, parentheses, anddashes on both sides of a noun phrase.
One com-mon wrapping auxiliary tree in our experiments isNP?
NP* ?
PPThis is used frequently in the news text ofthe Wall Street Journal for reported speech whenavoiding a full quotation.
This sentence is an ex-ample of the way the rule is employed, using whatJoshi and Schabes (1997) referred to as ?factoringrecursion from linguistic constraints?
with TAG.Note that replacing the quoted noun phrase andits following prepositional phrase with the nounphrase itself yields a valid sentence, in line withthe linguistic theory underlying TAG.Another frequent wrapping rule, shown below,allows direct coordination between the contents ofan appositive with the rest of the sentence.308NPNP , CCorNP* ,This is a valuable ability, as it is common touse an appositive to provide context or explanationfor a proper noun.
As our information on propernouns will most likely be very sparse, the apposi-tive may be more reliably connected to the rest ofthe sentence.
An example of this from one of thesentences in which this rule appears in the MPD isthe phrase ?since the market fell 156.83, or 8 %,a week after Black Monday?.
The wrapping ruleallows us to coordinate the verb ?fell?
with the pat-tern ?X %?
instead of 156.83, which is mapped toan unknown word category.These rules highlight the linguistic intuitionsthat back TAG; if their adjunction were undone,the remaining derivation would be a valid sen-tence that simply lacks the modifying structure ofthe auxiliary tree.
However, the MPD parses re-veal that not all useful adjunctions conform to thisparadigm, and that left-auxiliary trees that are notused for sister adjunction are susceptible to thisbehavior.
The most common such tree is used tocreate noun phrases such asP&G?s share of [the Japanese market]the House?s repeal of [a law]Apple?s family of [Macintosh Computers]Canada?s output of [crude oil]by adjoining the shared unbracketed syntax ontothe NP dominating the bracketed text.
If adjunc-tion is taken to model modification, this rule dras-tically changes the semantics of the unmodifiedsentence.
Furthermore, in some cases removingthe adjunction can leave a grammatically incorrectsentence, as in the third example where the nounphrase changes plurality.While our grammar induction method is a crude(but effective) heuristic, we can still highlight thequalities of the more important auxiliary treesby examining aggregate statistics over the MPDparses, shown in Figure 6.
The use of left-auxiliary trees for sister adjunction is a clear trend,as is the predominant use of right-auxiliary treesfor the complementary set of ?regular?
adjunc-tions, which is to be expected in a right branch-ing language such as English.
The statistics alsoAll Wrap Right LeftTotal 3585 (1374) 41 (26) 1698 (518) 1846 (830)Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)Lex 2244 (990) 28 (19) 894 (299) 1322 (672)FLex 1028 (558) 7 (2) 835 (472) 186 (84)Figure 6: Statistics for MPD auxiliary trees us-ing OSTAG3.
The columns indicate type of aux-iliary tree and the rows correspond respectively tothe full set found in the MPD, those that performsister adjunction, those that are lexicalized, andthose that are fully lexicalized.
Each cell showsthe number of tokens followed by the number oftypes of auxiliary tree that fit its conditions.reflect the importance of substitution in right-auxiliary trees, as they must capture the wide va-riety of right branching modifiers of the Englishlanguage.6 ConclusionThe OSTAG variant of Tree-Adjoining Grammaris a simple weakly context-free formalism thatstill provides for all types of adjunction and isa bit more concise than TSG (quadratically so).OSTAG can be reversibly transformed into CFGform, allowing the use of a wide range of wellstudied techniques in statistical parsing.OSTAG provides an alternative to TIG as acontext-free TAG variant that offers wrapping ad-junction in exchange for recursive left/right spineadjunction.
It would be interesting to apply bothOSTAG and TIG to different languages to deter-mine where the constraints of one or the other aremore or less appropriate.
Another possibility is thecombination of OSTAG with TIG, which wouldstrictly expand the abilities of both approaches.The most important direction of future work forOSTAG is the development of a principled gram-mar induction model, perhaps using the same tech-niques that have been successfully applied to TSGand TIG.
In order to motivate this and other re-lated research, we release our implementation ofEM and CYK parsing for OSTAG1.
Our systemperforms the CFG transform described above andoptionally employs coarse to fine pruning and re-laxed (finite) limits on the number of spine adjunc-tions.
As a TSG is simply a TAG without adjunc-tion rules, our parser can easily be used as a TSGestimator and parser as well.1bllip.cs.brown.edu/download/bucketparser.tar309ReferencesEugene Charniak, Mark Johnson, Micha Elsner,Joseph L. Austerweil, David Ellis, Isaac Hax-ton, Catherine Hill, R. Shrivaths, Jeremy Moore,Michael Pozar, and Theresa Vu.
2006.
Multilevelcoarse-to-fine PCFG parsing.
In North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies.Eugene Charniak.
1996.
Tree-bank grammars.
In As-sociation for the Advancement of Artificial Intelli-gence, pages 1031?1036.David Chiang.
2000.
Statistical parsing withan automatically-extracted tree adjoining grammar.Association for Computational Linguistics.Trevor Cohn and Phil Blunsom.
2010.
Blocked infer-ence in bayesian tree substitution grammars.
pages225?230.
Association for Computational Linguis-tics.Trevor Cohn, Sharon Goldwater, and Phil Blun-som.
2009.
Inducing compact but accurate tree-substitution grammars.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 548?556.Association for Computational Linguistics.Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-lore Srinivas, and Martin Zaidel.
1994.
XTAG sys-tem: a wide coverage grammar for English.
pages922?928.
Association for Computational Linguis-tics.J.
Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
Bod et al 2003.Saul Gorn.
1965.
Explicit definitions and linguisticdominoes.
In Systems and Computer Science, pages77?115.Rebecca Hwa.
1998.
An empirical evaluation of prob-abilistic lexicalized tree insertion grammars.
In Pro-ceedings of the 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics,pages 557?563.
Association for Computational Lin-guistics.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg and A. Salo-maa, editors, Handbook of Formal Languages, vol-ume 3, pages 69?124.
Springer.Aravind K Joshi.
1985.
Tree adjoining grammars:How much context-sensitivity is required to providereasonable structural descriptions?
University ofPennsylvania.Aravind K Joshi.
1987.
An introduction to tree ad-joining grammars.
Mathematics of Language, pages87?115.Dan Klein and Christopher D Manning.
2003.
Accu-rate unlexicalized parsing.
pages 423?430.
Associ-ation for Computational Linguistics.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,pages 35?56.Mehryar Mohri and Mark jan Nederhof.
2000.
Regu-lar approximation of context-free grammars throughtransformation.
In Robustness in language andspeech technology.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 433?440.
Association for Computational Linguistics.Yves Schabes and Richard C. Waters.
1995.
Treeinsertion grammar: a cubic-time, parsable formal-ism that lexicalizes context-free grammar withoutchanging the trees produced.
Computational Lin-guistics, (4):479?513.Yves Schabes.
1990.
Mathematical and computa-tional aspects of lexicalized grammars.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA, USA.Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.2011.
Insertion operator for bayesian tree substi-tution grammars.
pages 206?211.
Association forComputational Linguistics.Elif Yamangil and Stuart M. Shieber.
2012.
Estimat-ing compact yet rich tree insertion grammars.
pages110?114.
Association for Computational Linguis-tics.310
