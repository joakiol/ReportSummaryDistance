Proceedings of the ACL 2007 Student Research Workshop, pages 85?90,Prague, June 2007. c?2007 Association for Computational LinguisticsComputing Lexical Chains with Graph ClusteringOlena MedelyanComputer Science DepartmentThe University of WaikatoNew Zealandolena@cs.waikato.ac.nzAbstractThis paper describes a new method forcomputing lexical chains.
These aresequences of semantically related wordsthat reflect a text?s cohesive structure.
Incontrast to previous methods, we are ableto select chains based on their cohesivestrength.
This is achieved by analyzing theconnectivity in graphs representing thelexical chains.
We show that the generatedchains significantly improve performanceof automatic text summarization andkeyphrase indexing.1 IntroductionText understanding tasks such as topic detection,automatic summarization, discourse analysis andquestion answering require deep understanding ofthe text?s meaning.
The first step in determiningthis meaning is the analysis of the text?s conceptsand their inter-relations.
Lexical chains provide aframework for such an analysis.
They combinesemantically related words across sentences intomeaningful sequences that reflect the cohesivestructure of the text.Lexical chains, introduced by Morris and Hirst(1991), have been studied extensively in the lastdecade, since large lexical databases are availablein digital form.
Most approaches use WordNet orRoget?s thesaurus for computing the chains andapply the results for text summarization.We present a new approach for computinglexical chains by treating them as graphs, wherenodes are document terms and edges reflectsemantic relations between them.
In contrast toprevious methods, we analyze the cohesivestrength within a chain by computing the diameterof the chain graph.
Weakly cohesive chains with ahigh graph diameter are decomposed by a graphclustering algorithm into several highly cohesivechains.
We use WordNet and alternatively adomain-specific thesaurus for obtaining semanticrelations between the terms.We first give an overview of existing methodsfor computing lexical chains and related areas.Then we discuss the motivation behind the newapproach and describe the algorithm in detail.
Ourevaluation demonstrates the advantages of usingextracted lexical chains for the task of automatictext summarization and keyphrase indexing,compared to a simple baseline approach.
Theresults are compared to annotations produced by agroup of humans.2 Related WorkMorris and Hirst (1991) provide the theoreticalbackground behind lexical chains and demonstratehow they can be constructed manually fromRoget?s thesaurus.
The algorithm was re-implemented as soon as digital WordNet andRoget?s became available (Barzilay and Elhadad,1997) and its complexity was improved (Silber andMcCoy, 2002; Galley and McKeown, 2003).
Allthese algorithms perform explicit word sensedisambiguation while computing the chains.
Foreach word in a document the algorithm choosesonly one sense, the one that relates to members ofexisting lexical chains.
Reeve et al (2006)85compute lexical chains with a medical thesaurusand suggest an implicit disambiguation: once thechains are computed, weak ones containingirrelevant senses are eliminated.
We also followthis approach.One of the principles of building lexical chainsis that each term must belong to exactly one chain.If several chains are possible, Morris and Hirst(1991) choose the chain to whose overall score theterm contributes the most.
This score is a sum overweights of semantic relations between chainmembers.
This approach produces different lexicalchains depending on the order of words in thedocument.
This is not justified, as the same contentcan be expressed with different sequences ofstatements.
We propose an alternative orderindependent approach, where a graph clusteringalgorithm calculates the chain to which a termshould belong.3 Lexical ChainsThe following notation is used throughout thepaper.
A lexical chain is a graph G = (V,E) withnodes vi?V being terms and edges (vi, vj, wij)?Erepresenting semantic relations between them,where wij is a weight expressing the strength of therelation.
1 A set of terms and semantic relationsbuilding a graph is a valid lexical chain if the graphis connected, i.e.
there are no unconnected nodesand no isolated groups of nodes.The graph distance d(vi, vj) between two nodesvi and vj is the minimum length of the pathconnecting them.
And the graph diameter is the?longest shortest distance?
between any two nodesin a graph, defined as:(1) ),(max , jivv vvdm ji?
.1 The initial experiments presented in this paper use anunweighted graph with wi,j = 1 for any semantic relation.Because semantic relations are either bi-directional or inverse, we treat lexical chains asundirected graphs.3.1 The Cohesive StrengthLexical cohesion is the property of lexicalentities to ?stick together?
and function as a whole(Morris and Hirst, 1991).
How strongly theelements of a lexical chain ?stick together,?
that isthe cohesive strength of the chain, has beendefined as the sum of semantic relations betweenevery pair of chain members (e.g.
Morris and Hirst,1991; Silber and McCoy, 2002).
This numberincreases with the length of a chain, but longerlexical chains are not necessarily more cohesivethan shorter ones.Instead, we define the cohesive strength as thediameter of the chain graph.
Depending on theirdiameter we propose to group lexical chains asfollows:1.
Strongly cohesive lexical chains (Fig.
1a)build fully connected graphs where each term isrelated to all other chain members and m = 1.2.
Weakly cohesive lexical chains (Fig.
1b)connect terms without cycles and with a diameterm = |V| ?
1.3.
Moderately cohesive lexical chains (Fig.
1c)are in-between the above cases with m ?
[1, |V|?
1].To detect individual topics in texts it is moreuseful to extract strong lexical chains.
Forexample, Figure 1a describes ?physiographicfeatures?
and 1c refers to ?seafood,?
while it isdifficult to summarize the weak chain 1b with asingle term.
The goal is to compute lexical chainswith the highest possible cohesion.
Thus, thealgorithm must have a way to control the selection.physiographicfeaturesvalleys    lowland           plains    lagoons(a) strong m = 1symptoms eyesvisionsensespain(b) weak m = 4shelfishseafoodssquidsfoodsfish(c) average m = 2Semantic relation:            broader term               sister term                related termFigure 1.
Lexical chains of different cohesive strength.863.2 Computing Lexical ChainsThe algorithm consists of two stages.
First, wecompute lexical chains in a text with only onecondition: to be included into a chain a term needsto be related to at least one of its members.
Then,we apply graph clustering on the resulting weakchains to determine their strong subchains.I.
Determining all chains.
First, the documents?n-grams are mapped onto terms in the thesaurus.To improve conflation we ignore stopwords andsort the remaining stemmed words alphabetically.Second, for each thesaurus term t that was found inthe document we search for an appropriate lexicalchain.
We iterate over the list L containingpreviously created chains and check whether term tis related to any of the members of each chain.
Thefollowing cases are possible:1.
No lexical chains were found.A new lexical chain with the term t as asingle element is created and included in L.2.
One lexical chain was found.This chain is updated with the term t.3.
Two or more lexical chains were found.We merge these chains into a single newchain, and remove the old chains from L.II.
Clustering within the weak chains.Algorithms for graph clustering divide sparselyconnected graphs into dense subgraphs with asimilar diameter.
We consider each lexical chain inL with diameter 3?m as a weak chain and applygraph clustering to identify highly cohesivesubchains within this chain.
The list L is updatedwith the newly generated chains and the originalchain is removed.A popular graph clustering algorithm, MarkovClustering (MCL) is based on the idea that ?arandom walk that visits a dense cluster will likelynot leave the cluster until many of its vertices havebeen visited?
(van Dongen, 2000).
MCL isimplemented as a sequence of iterative operationson a matrix representing the graph.
We useChineseWhispers (Biemann, 2006), a special caseof MCL that performs the iteration in a moreaggressive way, with an optimized linearcomplexity with the number of graph edges.Figure 2 demonstrates how an original weaklycohesive lexical chain has been divided byChineseWhispers into five strong chains.4 Lexical Chains for Text SummarizationLexical chains are usually evaluated in terms of theirperformance on the automatic text summarizationtask, where the most significant sentences areextracted from a document into a summary of apredefined length.
The idea is to use the cohesiveinformation about sentence members stored inlexical chains.
We first describe the summarizationapproach and then compare results to manuallycreated summaries.4.1 Identifying the Main SentencesThe algorithm takes one document at a time andcomputes its lexical chains as described in Section3.2, using the lexical database WordNet.
First, weconsider all semantic senses of each documentterm.
However, after weighting the chains weeliminate senses appearing in low scored chains.Doran et al (2004) state that changes inweighting schemes have little effect on summaries.We have observed significant differences betweenreported functions on our data and achieved bestresults with the formula produced by Barzilay andElhadad (1997):(2) ??
????
?LCtLCttfreqtfreqLCLCScore )())(||1()(Here, |LC| is the length of the chain and freq(t) isthe frequency of the term t in the document.
Alllexical chains with score lower than a thresholdcontain irrelevant word senses and are eliminated.Next we identify the main sentences for the finalsummary of the document.
Different heuristicshave been proposed for sentence extraction basedon the information in lexical chains.
For each topscored chain, Barzilay and Elhadad (1997) extracteconometricsstatistsicalmethodseconomicanalysiscasestudiesmethodsmeasurementevaluationstatisticaldatadataanalysis cartographydatacollectionsurveyscensuresFigure 2.
Clustering of a weak chainwith ChineseWhispers.87Rater 2Positive NegativePositive a bRater 1 Negative c dTable 1.
Possible choices for any two ratersthat sentence which contains the first appearanceof a chain member.
Doran et al (2004) sum up theweights all words in the sentence, whichcorrespond to the chain weights in which thesewords occur.
We choose the latter heuristicbecause it significantly outperforms the formermethod in our experiments.The highest scoring sentences from thedocument, presented in their original order, formthe automatically generated summary.
How manysentences are extracted depends on the requestedsummary length, which is defined as thepercentage of the document length.4.2 Experimental SettingsFor evaluation we used a subset of a manuallyannotated corpus specifically created to evaluatetext summarization systems (Hasler et al 2003).We concentrate only on documents with at leasttwo manually produced summaries: 11 science and29 newswire articles with two summaries each, and7 articles additionally annotated by a third person.This data allows us to compare the consistency ofthe system with humans to their consistency witheach other.The results are evaluated with the Kappastatistic ?, defined for Table 1 as follows:(3) ))(()9)(()(2badbccabcab???????
?It takes into account the probability of chanceagreement and is widely used to measure inter-rater agreement (Hripcsak and Rothshild, 2005).The ideal automatic summarization algorithmshould have as high agreement with humansubjects as they have with each other.We also use a baseline approach (BL) toestimate the advantage of using the proposedlexical chaining algorithm (LCA).
It extracts textsummaries in exactly the manner described inSection 4.1, with the exception of the lexicalchaining stage.
Thus, when weighting sentences,the frequencies of all WordNet mappings are takeninto account without the implicit word sensedisambiguation provided by lexical chains.Humans BL LCAS1 0.19 0.2029 newswirearticles S2 0.32 0.20 0.24S1 0.08 0.1311 sciencearticles S2 0.34 0.13 0.22Table 2.
Kappa agreement on 40 summariesvs.
human2,3 and 1 vs. BL vs. LCAhuman 1 0,41 0,30 0,30human 2 0,38 0,22 0,24human 3 0,28 0,17 0,24average 0,36 0,23 0,26Table 3.
Kappa agreement on 7 newswire articles4.3 ResultsTable 2 compares the agreement among the humanannotators and their agreement with the baselineapproach BL and the lexical chain algorithm LCA.The agreement between humans is low, whichconfirms that sentence extraction is a highlysubjective task.
The lexical chain approach LCAsignificantly outperforms the baseline BL,particularly on the science articles.While the average agreement of the LCA withhumans is still low, the picture changes when welook at the agreement on individual documents.Human agreement varies a lot (stdev = 0.24), whileresults produced by LCA are more consistent(stdev = 0.18).
In fact, for over 50% of documentsLCA has greater or the same agreement with oneor both human annotators than they with eachother.
The overall superior performance of humansis due to exceptionally high agreement on a fewdocuments, whereas on another couple ofdocuments LCA failed to produce a consistentsummary with both subjects.
This finding is similarto the one mentioned by Silber and McCoy (2002).Table 3 shows the agreement values for 7newswire articles that were summarized by threehuman annotators.
Again, LCA clearlyoutperforms the baseline BL.
Interestingly, bothsystems have a greater agreement with the firstsubject than the first and the third human subjectswith each other.5 Lexical Chains for Keyphrase IndexingKeyphrase indexing is the task of identifying themain topics in a document.
The drawback ofconventional indexing systems is that they analyze88Professional Indexers1 2 3 4 5 6 Avg1 61 51 64 57 57 582 61 48 53 60 52 553 51 48 54 44 61 514 64 53 54 51 57 565 57 60 44 51 49 526 57 52 61 57 49 55BL 42 39 37 39 39 35 39LCA 43 42 40 40 39 40 41Table 4.
Topic consistency over 30 documentsdocument terms individually.
Lexical chains enabletopical indexing, where first highly cohesive termsare organized into larger topics and then the maintopics are selected.
Properties of chain membershelp to identify terms that represent eachkeyphrases.
To compute lexical chains and assignkeyphrases this time we use a domain-specificthesaurus instead of WordNet.5.1 Finding Keyphrases in Lexical ChainsThe ranking of lexical chains is essential fordetermining the main topics of a document.
Unlikein summarization, it should capture the specificityof the individual chains.
Also, for some topics, e.g.proper nouns, the number of terms to express it canbe limited; therefore we average frequencies overall chain members.
Our measure of chainspecificity combines TFIDFs and term length, 2which boosts chains containing specific terms thatare particularly frequent in a given document:(4)LCtlengthtTFIDFLCScore LCtLCt??????
)()()(We assume that the top ranked weighted lexicalchains represent the main topics in a document.
Todetermine the keyphrases, for each lexical chainwe need to choose a term that describes this chainin the best way, just as ?seafood?
is the bestdescriptor for the chain in Figure 1c.Each member of the chain t is scored as follows:(5) )()()()( tlengthtNDtTFIDFtScore ??
?where ND(t) is the node degree, or the number ofedges connecting term t to other chain members.The top scored term is chosen as a keyphrase.2 Term length, measured in words, gives an indirect butsimple measure of its specificity.
E.g., ?tropical rainforests?
is more specific than ?forests?.Professional indexers tend to choose more thanone term for a document?s most prominent topics.Thus, we extract the top two keyphrases from thetop two lexical chains with |LC| ?
3.
If the secondkeyphrase is a broader or a narrower term of thefirst one, this rule does not apply.5.2 Evaluation of the Extracted KeyphrasesThis approach is evaluated on 30 documentsindexed each by 6 professional indexers from theUN?s Food and Agriculture Organization.
Thekeyphrases are driven from the agriculturalthesaurus Agrovoc3 with around 40,000 terms and30,000 semantic relations between them.The effectiveness of the lexical chains is shownin comparison to a baseline approach, which givena document simply defines keyphrases as Agrovocterms with top TFIDF values.Indexing consistency is computed with the F-Measure F, which can be expressed in terms ofTable 1 (Section 4.1) as following:4(6) cbaaF ???
22The overlap between two keyphrase sets a isusually computed by exact matching of keyphrases.However, discrepancies between professionalhuman indexers show that there are no ?correct?keyphrases.
Capturing main topics rather thanexact term choices is more important.
Lexicalchains provide a way of measuring this so calledtopical consistency.
Given a set of lexical chainsextracted from a document, we first computechains that are covered in its keyphrase set andthen compute consistency in the usual manner.5.3 ResultsTable 4 shows topical consistency between eachpair of professional human indexers, as well asbetween the indexers and the two automaticapproaches, baseline BL and the lexical chainalgorithm LCA, averaged over 30 documents.The overall consistency between the humanindexers is 55%.
The baseline BL is 16 percentagepoints less consistent with the 6 indexers, while3 http://www.fao.org/agrovoc/4 When vocabulary is large, the consistency is the same,whether it is computed with the Kappa statistic or the F-Measure (Hripcsak and Rothshild, 2005).89LCA is 1 to 5 percentage points more consistentwith each indexer than the baseline.6 DiscussionProfessional human indexers first performconceptual analysis of a document and thentranslate the discovered topics into keyphrases.
Weshow how these two indexing steps are realizedwith lexical chain approach that first builds anintermediate semantic representation of adocument and then translates chains intokeyphrases.
Conceptual analysis with lexicalchains in text summarization helps to identifyirrelevant word senses.The initial results show that lexical chainsperform better than baseline approaches in bothexperiments.
In automatic summarization, lexicalchains produce summaries that in most cases havehigher consistency with human annotators thanthey with each other, even using a simplifiedweighting technique.
Integrating lexical chaininginto existing keyphrase indexing systems is apromising step towards their improvement.The lexical chaining does not require anyresources other than a controlled vocabulary.
Wehave shown that it performs well with a generallexical database and with a domain-specificthesaurus.
We use the Semantic KnowledgeOrganization Standard 5 which allows easy inter-changeability of thesauri.
Thus, this approach isdomain and language independent.7 ConclusionsWe have shown a new method for computinglexical chains based on graph clustering.
Whileprevious chaining algorithms did not analyze thelexical cohesion within each chain, we force ouralgorithm to produce highly cohesive lexicalchains based on the minimum diameter of the chaingraph.
The required cohesion can be controlled byincreasing the diameter value and adjustingparameters of the graph clustering algorithm.Experiments on text summarization and key-phrase indexing show that the lexical chainsapproach produces good results.
It combinessymbolic analysis with statistical features and5 http://www.w3.org/2004/02/skos/outperforms a purely statistical baseline.
Thefuture work will be to further improve the lexicalchaining technique and integrate it into a morecomplex topical indexing system.8 AcknowledgementsI would like to thank my PhD supervisorsIan H. Witten and Eibe Frank, as well as GordonPaynter and Michael Poprat and the anonymousreviewers of this paper for their valuable comments.This work is supported by a Google Scholarship.ReferencesChris Biemann 2006.
Chinese Whispers?an EfficientGraph Clustering Algorithm and its Application toNatural Language Processing Problems.
In Proc ofthe HLT-NAACL-06 Workshop on Textgraphs, pp.73-80.Regina Barzilay and Michael Elhadad.
1997.
UsingLexical Chains for Text Summarization, In Proc ofthe ACL Intelligent Scalable Text SummarizationWorkshop, pp.
10-17.Stijn M. van Dongen.
2000.
Graph Clustering by FlowSimulation.
PhD thesis, University of Utrecht.William P. Doran, Nicola Stokes, Joe Carthy and JohnDunnion.
2004.
Assessing the Impact of LexicalChain Scoring Methods on Summarization.
In Proc ofCICLING?04, pp.
627-635.Laura Hasler, Constantin Orasan and Ruslan Mitkov.2003.
Building Better Corpora for Summarization.
InProc of Corpus Linguistics CL?03, pp.
309-319.George Hripcsak and Adam S. Rothschild.
2005.Agreement, the F-Measure, and Reliability in IR.JAMIA, (12), pp.
296-298.Jane Morris and Graeme Hirst.
1991.
Lexical CohesionComputed by Thesaural Relations as an Indicator ofthe Structure of Text.
Computational Linguistics,17(1), pp.
21-48.Lawrence H. Reeve, Hyoil Han and Ari D. Brooks.2006.
BioChain: Using Lexical Chaining forBiomedical Text Summarization.
In Proc of the ACMSymposium on Applied Computing, pp.
180-184.Gregory Silber and Kathleen McCoy, 2002.
EfficientlyComputed Lexical Chains as an IntermediateRepresentation for Automatic Text Summarization.Computational Linguistics, vol.
28, pp.
487-496.90
