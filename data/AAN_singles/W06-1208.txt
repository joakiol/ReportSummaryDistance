Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 54?61,Sydney, July 2006. c?2006 Association for Computational LinguisticsInterpretation of Compound Nominalisations using Corpus and WebStatisticsJeremy Nicholson and Timothy BaldwinDepartment of Computer Science and Software EngineeringUniversity of Melbourne, VIC 3010, AustraliaandNICTA Victoria Research LaboratoriesUniversity of Melbourne, VIC 3010, Australia{jeremymn,tim}@csse.unimelb.edu.auAbstractWe present two novel paraphrase tests forautomatically predicting the inherent se-mantic relation of a given compound nom-inalisation as one of subject, direct object,or prepositional object.
We compare theseto the usual verb?argument paraphrase testusing corpus statistics, and frequencies ob-tained by scraping the Google search en-gine interface.
We also implemented amore robust statistical measure than max-imum likelihood estimation ?
the con-fidence interval.
A significant reductionin data sparseness was achieved, but thisalone is insufficient to provide a substan-tial performance improvement.1 IntroductionCompound nouns are a class of multiword expres-sion (MWE) that have been of interest in recentcomputational linguistic work, as any task with alexical semantic dimension (like machine transla-tion or information extraction) must take into ac-count their semantic markedness.
A compoundnoun is a sequence of two or more nouns compris-ing an N?
, for example, polystyrene garden-gnome.The productivity of compound nouns makes theirtreatment equally desirable and difficult.
They ap-pear frequently: more than 1% of the words in theBritish National Corpus (BNC: Burnard (2000))participate in noun compounds (Tanaka and Bald-win, 2003).
However, unestablished compoundsare common: almost 70% of compounds identi-fied in the BNC co-occur with a frequency of onlyone (Lapata and Lascarides, 2003).Analysis of the entire space of compound nounshas been hampered to some degree as the space de-fies some regular set of predicates to define the im-plicit semantics between a modifier and its head.This semantic underspecification led early analy-sis to be primarily of a semantic nature, but morerecent work has advanced into using syntax to pre-dict the semantics, in the spirit of the study byLevin (1993) on diathesis alternations.In this work, we examine compound nominal-isations, a subset of compound nouns where thehead has a morphologically?related verb.
Forexample, product replacement has an underlyingverbal head replace, whereas garden-gnome hasno such form.
While compound nouns in gen-eral have a set of semantic relationships betweenthe head and modifier that is potentially non-finite,compound nominalisations are better defined, inthat the modifier fills a syntactic argument rela-tion with respect to the head.
For example, prod-uct might fill the direct object slot of the verbto replace for the compound above.
Compoundnominalisations comprise a substantial minority ofcompound nouns, with figures of about 35% beingobserved (Grover et al, 2005; Nicholson, 2005).We propose two novel paraphrases for a corpusstatistical approach to predicting the relationshipfor a set of compound nominalisations, and inves-tigate how using the World Wide Web as a cor-pus alleviates the common phenomenon of datasparseness, and how the volume of data impactson the classification results.
We also examinea more robust statistical approach to interpreta-tion of the statistics than maximum likelihood es-timates, called the confidence interval.The rest of the paper is structured as follows: inSection 2, we present a brief background for ourwork, with a listing of our resources in Section 3.We detail our proposed method in Section 4, thecorresponding results in Section 5, with a discus-54sion in Section 6 and a brief conclusion in Sec-tion 7.2 Background2.1 Compound Noun InterpretationCompound nouns were seminally and thoroughlyanalysed by Levi (1978), who hand?constructs anine?way set of semantic relations that she identi-fies as broadly defining the observed relationshipsbetween the compound head and modifier.
War-ren (1978) also inspects the syntax of compoundnouns, to create a somewhat different set of twelveconceptual categories.Early attempts to automatically classify com-pound nouns have taken a semantic approach:Finin (1980) and Isabelle (1984) use ?role nomi-nals?
derived from the head of the compound tofill a slot with the modifier.
Vanderwende (1994)uses a rule?based technique that scores a com-pound on possible semantic interpretations, whileJones (1995) implements a graph?based unifica-tion procedure over semantic feature structures forthe head.
Finally, Rosario and Hearst (2001) makeuse of a domain?specific lexical resource to clas-sify according to neural networks and decisiontrees.Syntactic classification, using paraphrasing,was first used by Leonard (1984), who uses a pri-oritised rule?based approach across a number ofpossible readings.
Lauer (1995) employs a cor-pus statistical model over a similar paraphraseset based on prepositions.
Lapata (2002) andGrover et al (2005) again use a corpus statis-tical paraphrase?based approach, but with verb?argument relations for compound nominalisations?
attempting to define the relation as one of sub-ject, direct object, or a number of prepositional ob-jects in the latter.2.2 Web?as?Corpus ApproachesUsing the World Wide Web for corpus statisticsis a relatively recent phenomenon; we present afew notable examples.
Grefenstette (1998) anal-yses the plausibility of candidate translations ina machine translation task through Web statistics,and avoids some data sparseness within that con-text.
Zhu and Rosenfeld (2001) train a languagemodel from a large corpus, and use the Web toestimate low?density trigram frequencies.
Kellerand Lapata (2003) show that Web counts canobviate data sparseness for syntactic predicate?argument bigrams.
They also observe that thenoisiness of the Web, while unexplored in detail,does not greatly reduce the reliability of their re-sults.
Nakov and Hearst (2005) demonstrate thatWeb counts can aid in identifying the bracketing inhigher?arity noun compounds.
Finally, Lapata andKeller (2005) evaluate the performance of Webcounts on a wide range of natural language pro-cessing tasks, including compound noun bracket-ing and compound noun interpretation.2.3 Confidence IntervalsMaximum likelihood statistics are not robust whenmany sparse vectors are under consideration, i.e.naively ?choosing the largest number?
may not beaccurate in contexts when the relative value acrosssamplings may be relevant, for example, in ma-chine learning.
As such, we apply a statisticaltest with confidence intervals (Kenney and Keep-ing, 1962), where we compare sample z-scores ina pairwise manner, instead of frequencies globally.The confidence interval P , for z-score n, is:P = 2?pi?
n/?20e?t2dt (1)t is chosen to normalise the curve, and P is strictlyincreasing on n, so we are only required to find thelargest z-score.Calculating the z-score exactly can be quitecostly, so we instead use the binomial approxi-mation to the normal distribution with equal priorprobabilities and find that a given z-score Z is:Z = f ?
??
(2)where f is the frequency count, ?
is the mean ina pairwise test, and ?
is the standard deviation ofthe test.
A more complete derivation appears inNicholson (2005).3 ResourcesWe make use of a number of lexical resourcesin our implementation and evaluation.
For cor-pus statistics, we use the written component ofthe BNC, a balanced 90M token corpus.
To findverb?argument frequencies, we parse this usingRASP (Briscoe and Carroll, 2002), a tag sequencegrammar?based statistical parser.
We contrastthe corpus statistics with ones collected from the55Web, using an implementation of a freely avail-able Google ?scraper?
from CPAN.1For a given compound nominalisation, we wishto determine all possible verbal forms of the head.We do so using the combination of the morpho-logical component of CELEX (Burnage, 1990), alexical database, NOMLEX (Macleod et al, 1998),a nominalisation database, and CATVAR (Habashand Dorr, 2003), an automatically?constructeddatabase of clusters of inflected words based onthe Porter stemmer (Porter, 1997).Once the verbal forms have been identified, weconstruct canonical forms of the present partici-ple (+ing) and the past participle (+ed), using themorph lemmatiser (Minnen et al, 2001).
We con-struct canonical forms of the plural head and pluralmodifier (+s) in the same manner.For evaluation, we have the two?way classifieddata set used by Lapata (2002), and a three?wayclassified data set constructed from open text.Lapata automatically extracts candidates fromthe British National Corpus, and hand?curatesa set of 796 compound nominalisations whichwere interpreted as either a subjective relationSUBJ (e.g.
wood appearance ?wood appears?
),or a (direct) objective relation OBJ (e.g.
stressavoidance ?
[SO] avoids stress?.
We automaticallyvalidated this data set for consistency, removing:1. items that did not occur in the same chunk,according to a chunker based on fnTBL 1.0(Ngai and Florian, 2001),2. items whose head did not have a verbal formaccording to our lexical resources, and3.
items which consisted in part of propernouns,to end up with 695 consistent compounds.
Weused the method of Nicholson and Baldwin (2005)to derive a small data set of 129 compoundnominalisations, also from the BNC, which weinstructed three unskilled annotators to identifyeach as one of subjective (SUB), direct object(DOB), or prepositional object (POB, e.g.
sideshow ?
[SO] show [ST] on the side?).
The an-notators identified nine prepositional relations:{about,against,for,from,in,into,on,to,with}.1www.cpan.org: We limit our usage to examining the?Estimated Results Returned?, so that our usage is identi-cal to running the queries manually from the website.
TheGoogle API (www.google.com/apis) gives a methodfor examining the actual text of the returned documents.4 Proposed Method4.1 Paraphrase TestsTo derive preferences for the SUB, DOB, and var-ious POB interpretations for a given compoundnominalisation, the most obvious approach is toexamine a parsed corpus for instances of the verbalform of the head and the modifier occurring in thecorresponding verb?argument relation.
There areother constructions that can be informative, how-ever.We examine two novel paraphrase tests: oneprepositional and one participial.
The preposi-tional test is based in part on the work by Leonard(1984) and Lauer (1995): for a given compound,we search for instances of the head and modifiernouns separated by a preposition.
For example,for the compound nominalisation leg operation,we might search for operation on the leg, corre-sponding to the POB relation on.
Special cases areby, corresponding to a subjective reading akin to apassive construction (e.g.
investor hesitancy, hesi-tancy by the investor ?
?the investor hesitates?
),and of, corresponding to a direct object reading(e.g.
language speaker, speaker of the language?
?
[SO] speaks the language?
).The participial test is based on the paraphras-ing equivalence of using the present participle ofthe verbal head as an adjective before the modifier,for the SUB relation (e.g.
the hesitating investor ?
?the investor hesitates?
), compared to the past par-ticiple for the DOB relation (the spoken language?
?
[SO] speaks the language?).
The correspond-ing prepositional object construction is unusual inEnglish, but still possible: compare ?the operated-on leg and the lived-in village.4.2 The AlgorithmGiven a compound nominalisation, we perform anumber of steps to arrive at an interpretation.
First,we derive a set of verbal forms for the head fromthe combination of CELEX, NOMLEX, and CAT-VAR.
We find the participial forms of each of theverbal heads, and plurals for the nominal head andmodifier, using the morph lemmatiser.Next, we examine the BNC for instances of themodifier and one of the verbal head forms oc-curring in a verb?argument relation, with the aidof the RASP parse.
Using these frequencies, wecalculate the pairwise z-scores between SUB andDOB, and between SUB and POB: the score givento the SUB interpretation is the greater of the two.56We further examine the RASP parsed data for in-stances of the prepositional and participial tests forthe compound, and calculate the z-scores for theseas well.We then collect our Google counts.
Because theWeb data is unparsed, we cannot look for syntacticstructures explictly.
Instead, we query a number ofcollocations which we expect to be representativeof the desired structure.For the prepositional test, the head can be sin-gular or plural, the modifier can be singular or plu-ral, and there may or may not be an article be-tween the preposition and the modifier.
For exam-ple, for the compound nominalisation product re-placement and preposition of we search for all ofthe following: (and similarly for the other prepo-sitions)replacement of productreplacement of the productreplacement of productsreplacement of the productsreplacements of productreplacements of the productreplacements of productsreplacements of the productsFor the participial test, the modifier can be sin-gular or plural, and if we are examining a prepo-sitional relation, the head can be either a presentor past participle.
For product replacement, wesearch for, as well as other prepositions:the replacing productthe replacing productsthe replaced productthe replaced productsthe replacing?about productthe replacing?about productsthe replaced?about productthe replaced?about productsWe comment briefly on these tests in Section 6.We choose to use the as our canonical article be-cause it is a reliable marker of the left boundary ofan NP and number-neutral; using a/an representsa needless complication.We then calculate the z-scores using the methoddescribed in Section 2, where the individual fre-quency counts are the maximum of the results ob-tained across the query set.Once the z-scores have been obtained, wechoose a classification based on the greatest-valued observed test.
We contrast the confidenceinterval?based approach with the maximum like-lihood method of choosing the largest of the rawfrequencies.
We also experiment with a machinelearning package, to examine the mutual predic-tiveness of the separate tests.5 Observed ResultsFirst, we found majority-class baselines for eachof the data sets.
The two?way data set had258 SUBJ?classified items, and 437 OBJ?classifieditems, so choosing OBJ each time gives a baselineof 62.9%.
The three?way set had 22 SUB items,63 of DOB, and 44 of POB, giving a baseline of48.8%.Contrasting this with human performance onthe data set, Lapata recorded a raw inter-annotatoragreement of 89.7% on her test set, which cor-responds to a Kappa value ?
= 0.78.
On thethree?way data set, three annotators had a agree-ment of 98.4% for identification and classificationof observed compound nominalisations in opentext, and ?
= 0.83.
For the three-way data set,the annotators were asked to both identify andclassify compound nominalisations in free text,and agreement is thus calculated over all wordsin the test.
The high agreement figure is due tothe fact that most words could be trivially disre-garded (e.g.
were not nouns).
Kappa corrects thisfor chance agreement, so we conclude that thistask was still better-defined than the one posedby Lapata.
One possible reason for this was thenumber of poorly?behaved compounds that we re-moved due to chunk inconsistencies, lack of a ver-bal form, or proper nouns: it would be difficult forthe annotators to agree over compounds where anobvious well?defined interpretation was not avail-able.5.1 Comparison ClassificationResults for classification over the Lapata two?waydata set are given in Table 1, and results overthe open data three?way set are given in Table 2.For these, we selected the greatest raw frequencycount for a given test as the intended relation(Raw), or the greatest confidence interval accord-ing to the z-score (Z-Score).
If a relation could notbe selected due to ties (e.g., the scores were all 0),we selected the majority baseline.
To deal with thenature of the two?way data set with respect to ourthree?way selection, we mapped compounds thatwe would prefer to be POB to OBJ, as there are57Paraphrase Default Corpus Counts Web CountsRaw Z-Score Raw Z-ScoreVerb?Argument 62.9 67.9 68.3 ?
?Prepositional 62.9 62.1 62.4 62.6 63.0Participial 62.9 63.0 63.2 61.4 58.8Table 1: Classification Results over the two?way data set, in %.
Comparison of raw frequency countsvs.
confidence?based z-scores, for BNC data and Google scrapings shown.Paraphrase Default Corpus Counts Web CountsRaw Z-Score Raw Z-ScoreVerb?Argument 48.8 54.3 55.0 ?
?Prepositional 48.8 48.4 50.0 59.7 58.9Participial 48.8 43.2 45.4 43.4 38.0Table 2: Classification results over the three-way data set, in %.
Comparison of raw frequency countsvs.
confidence-based z-scores, for BNC data and Google scrapings shown.compounds in the set (e.g.
adult provision) thathave a prepositional object reading (?provide foradults?)
but have been classified as a direct objectOBJ.The verb?argument counts obtained from theparsed BNC are significantly better than the base-line for the Lapata data set (?2 = 4.12, p ?
0.05),but not significantly better for the open data set(?2 = 0.99, p ?
1).
Similar results were reportedby Lapata (2002) over her data set using backed?off smoothing, the most closely related method.Neither the prepositional nor participial para-phrases were significantly better than the baselinefor either the two?way (?2 = 0.00, p ?
1), orthe three?way data set (?2 = 3.52, p ?
0.10), al-though the prepositional test did slightly improveon the verb?argument results.5.2 Machine Learning ClassificationAlthough the results were not impressive, we stillbelieved that there was complementing informa-tion within the data, which could be extracted withthe aid of a machine learner.
For this, we madeuse of TiMBL (Daelemans et al, 2003), a nearest-neighbour classifier which stores the entire train-ing set and extrapolates further samples, as a prin-cipled method for combination of the data.
We useTiMBL?s in-built cross-validation method: 90% ofthe data set is used as training data to test the other10%, for each stratified tenth of the set.
The resultsit achieves are assumed to be able to generalise tonew samples if they are compared to the currenttraining data set.The results observed using TiMBL are shownCorpus Counts Web CountsTwo?way Set 72.4 74.2Three?way Set 51.1 50.4Table 3: TiMBL results for the combination ofparaphrase tests over the two?way and three?waydata sets for corpus and Web frequenciesin Table 3.
This was from the combinationof all of the available paraphrase tests: verb?argument, prepositional, and participial for thecorpus counts, and just prepositional and particip-ial for the Web counts.
The results for the two?way data set derived from Lapata?s data set were agood improvement over the simple classificationresults, significantly so for the Web frequencies(?2 = 20.3, p ?
0.01).
However, we also no-tice a corresponding decrease in the results for thethree?way open data set, which make these im-provements immaterial.Examining the other possible combinations forthe tests did indeed lead to varying results, but notin a consistent manner.
For example, the best com-bination for the open data set was using the par-ticipial raw scores and z-scores (58.1%), whichperformed particularly badly in simple compar-isons, and comparatively poorly (70.2%) for thetwo?way set.6 DiscussionAlthough the observed results failed to match, oreven approach, various benchmarks set by La-pata (2002) (87.3% accuracy) and Grover et al(2005) (77%) for the subject?object and subject?58direct object?prepositional objects classificationtasks respectively, the presented approach is notwithout merit.
Indeed, these results relied onmachine learning approaches incorporating manyfeatures independent of corpus counts: namely,context, suffix information, and semantic similar-ity resources.
Our results were an examinationof the possible contribution of lexical informationavailable from high?volume unparsed text.One important concept used in the above bench-marks was that of statistical smoothing, bothclass?based and distance?based.
The reason forthis is the inherent data sparseness within thecorpus statistics for these paraphrase tests.
La-pata (2002) observes that almost half (47%) ofthe verb?noun pairs constructed are not attestedwithin the BNC.
Grover et al (2005) also note thesparseness of observed relations.
Using the im-mense data source of the Web allows one to cir-cumvent this problem: only one compound (an-archist prohibition) has no instances of the para-phrases from the scraping,2 from more than 900compounds between the two data sets.
This ex-tra information, we surmise, would be beneficialfor the smoothing procedures, as the comparativeaccuracy between the two methods is similar.On the other hand, we also observe that sim-ply alleviating the data sparseness is insufficientto provide a reliable interpretation.
These resultsreinforce the contribution made by the statisticaland semantic resources used in arriving at thesebenchmarks.The approach suggested by Keller and Lapata(2003) for obtaining bigram information from theWeb could provide an approach for estimating thesyntactic verb?argument counts for a given com-pound (dashes in Tables 1 and 2).
In spite ofthe inherent unreliability of approximating long?range dependencies with n-gram information, re-sults look promising.
An examination of the effec-tiveness of this approach is left as further research.Similarly, various methods of combining corpuscounts with the Web counts, including smooth-ing, backing?off, and machine learning, could alsolead to interesting performance impacts.Another item of interest is the comparative dif-ficulty of the task presented by the three?way dataset extracted from open data, and the two?waydata set hand?curated by Lapata.
The baseline2Interestingly, Google only lists 3 occurrences of thiscompound anyway, so token relevance is low ?
further in-spection shows that those 3 are not well-formed in any case.of this set is much lower, even compared thatof the similar task (albeit domain?specific) fromGrover et al (2005) of 58.6%.
We posit that thehand?filtering of the data set in these works con-tributes to a biased sample.
For example, remov-ing prepositional objects for a two?way classifica-tion, which make up about a third of the open dataset, renders the task somewhat artificial.Comparison of the results between the maxi-mum likelihood estimates used in earlier work,and the more statistically robust confidence inter-vals were inconclusive as to performance improve-ment, and were most effective as a feature expan-sion algorithm.
The only obvious result is an aes-thetic one, in using ?robust statistics?.Finally, the paraphrase tests which we proposeare not without drawbacks.
In the prepositionaltest, a paraphrase with of does not strictly con-tribute to a direct object reading: consider schoolaim ?school aims?, for which instances of aim bythe school are overwhelmed by aim of the school.We experimented with permutations of the avail-able queries (e.g.
requiring the head and modifierto be of different number, to reflect the pluralis-ability of the head in such compounds, e.g.
aimsof the school), without observing substantially dif-ferent results.Another observation is the inherent bias of theprepositional test to the prepositional object re-lation.
Apparent prepositional relations can oc-cur in spite of the available verb frames: con-sider cash limitation, where the most populous in-stance is limitation on cash, despite the impossi-bility of *to limit on cash (for to place a limit oncash).
Another example, is bank agreement: find-ing instances of agreement with bank does not leadto the pragmatically absurd [SO] agrees with thebank.Correspondingly, the participial relation has theopposite bias: constructions of the form the lived-in at ?
[SO] lived in the flat?
are usually lexi-calised in English.
As such, only 17% of com-pounds in the two?way data set and 34% of thethree-way data set display non-zero values in theprepositional object relation for the participial test.We hoped that the inherent biases of the two testsmight balance each other, but there is little evi-dence of that from the results.597 ConclusionWe presented two novel paraphrase tests for au-tomatically predicting the inherent semantic rela-tion of a given compound nominalisation as one ofsubject, direct object, or prepositional object.
Wecompared these to the usual verb?argument para-phrase test, using corpus statistics, and frequen-cies obtained by scraping the Google search en-gine.
We also implemented a more robust statisti-cal measure than the insipid maximum likelihoodestimates ?
the confidence interval.
A significantreduction in data sparseness was achieved, but thisalone is insufficient to provide a substantial per-formance improvement.AcknowledgementsWe would like to thank the members of the Univer-sity of Melbourne LT group and the three anony-mous reviewers for their valuable input on this re-search, as well as Mirella Lapata for allowing useof the data.ReferencesTed Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof the 3rd International Conference on LanguageResources and Evaluation, pages 1499?1504, LasPalmas, Canary Islands.Gavin Burnage.
1990.
CELEX: A guide for users.Technical report, University of Nijmegen.Lou Burnard.
2000.
User Reference Guide for theBritish National Corpus.
Technical report, OxfordUniversity Computing Services.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2003.
TiMBL: Tilburg Mem-ory Based Learner, version 5.0, Reference Guide.ILK Technical Report 03-10.Tim Finin.
1980.
The semantic interpretation of nom-inal compounds.
In Proceedings of the First Na-tional Conference on Artificial Intelligence, pages310?315, Stanford, USA.
AAAI Press.Gregory Grefenstette.
1998.
The World Wide Webas a resource for example-based machine translationtasks.
In Proceedings of the ASLIB Conference onTranslating and the Computer, London, UK.Claire Grover, Mirella Lapata, and Alex Lascarides.2005.
A comparison of parsing technologies for thebiomedical domain.
Journal of Natural LanguageEngineering, 11(01):27?65.Nizar Habash and Bonnie Dorr.
2003.
A categorialvariation database for English.
In Proceedings ofthe 2003 Human Language Technology Conferenceof the North American Chapter of the ACL, pages17?23, Edmonton, Canada.Pierre Isabelle.
1984.
Another look at nominal com-pounds.
In Proceeedings of the 10th InternationalConference on Computational Linguistics and 22ndAnnual Meeting of the ACL, pages 509?516, Stan-ford, USA.Bernard Jones.
1995.
Predicating nominal com-pounds.
In Proceedings of the 17th InternationalConference of the Cognitive Science Society, pages130?5, Pittsburgh, USA.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.John F. Kenney and E. S. Keeping, 1962.
Mathematicsof Statistics, Pt.
1, chapter 11.4, pages 167?9.
VanNostrand, Princeton, USA, 3rd edition.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACMTransactions on Speech and Language Processing,2(1).Mirella Lapata and Alex Lascarides.
2003.
Detect-ing novel compounds: The role of distributionalevidence.
In Proceedings of the 10th Conferenceof the European Chapter of the Association forComputional Linguistics, pages 235?242, Budapest,Hungary.Maria Lapata.
2002.
The disambiguation of nomi-nalizations.
Computational Linguistics, 28(3):357?388.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Noun Compounds.
Ph.D.thesis, Macquarie University, Sydney, Australia.Rosemary Leonard.
1984.
The Interpretation of En-glish Noun Sequences on the Computer.
ElsevierScience, Amsterdam, the Netherlands.Judith Levi.
1978.
The Syntax and Semantics of Com-plex Nominals.
Academic Press, New York, USA.Beth Levin.
1993.
English Verb Classes and Alter-nations.
The University of Chicago Press, Chicago,USA.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
NOMLEX:A lexicon of nominalizations.
In Proceedings of the8th International Congress of the European Associ-ation for Lexicography, pages 187?193, Liege, Bel-gium.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?23.60Preslov Nakov and Marti Hearst.
2005.
Search en-gine statistics beyond the n-gram: Application tonoun compound bracketing.
In Proceedings of theNinth Conference on Computational Natural Lan-guage Learning, pages 17?24, Ann Arbor, USA.Grace Ngai and Radu Florian.
2001.
Transformation-based learning in the fast lane.
In Proceedings of the2nd Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics,pages 40?7, Pittsburgh, USA.Jeremy Nicholson and Timothy Baldwin.
2005.
Sta-tistical interpretation of compound nominalisations.In Proceeding of the Australasian Langugae Tech-nology Workshop 2005, Sydney, Australia.Jeremy Nicholson.
2005.
Statistical interpretation ofcompound nouns.
Honours Thesis, University ofMelbourne, Melbourne, Australia.Martin Porter.
1997.
An algorithm for suffix strip-ping.
In Karen Sparck Jones and Peter Willett,editors, Readings in information retrieval.
MorganKaufmann, San Francisco, USA.Barbara Rosario and Marti Hearst.
2001.
Classify-ing the semantic relations in noun compounds via adomain-specific lexical hierarchy.
In Proceedings ofthe 6th Conference on Empirical Methods in NaturalLanguage Processing, Pittsburgh, USA.Takaaki Tanaka and Timothy Baldwin.
2003.
Noun-noun compound machine translation: A feasibilitystudy on shallow processing.
In Proceedings ofthe ACL 2003 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment, pages 17?24,Sapporo, Japan.Lucy Vanderwende.
1994.
Algorithm for automaticinterpretation of noun sequences.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics, pages 782?788, Kyoto, Japan.Beatrice Warren.
1978.
Semantic Patterns of Noun-Noun Compounds.
Acta Universitatis Gothoburgen-sis, Go?teborg, Sweden.Xiaojin Zhu and Ronald Rosenfeld.
2001.
Improv-ing trigram language modeling with the World WideWeb.
In Proceedings of the International Confer-ence on Acoustics, Speech, and Signal Processing,pages 533?6, Salt Lake City, USA.61
