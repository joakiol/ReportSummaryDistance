Proceedings of NAACL HLT 2007, pages 65?72,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsStructured Local Training and Biased Potential Functions for ConditionalRandom Fields with Application to Coreference ResolutionYejin Choi and Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{ychoi,cardie}@cs.cornell.eduAbstractConditional Random Fields (CRFs) have showngreat success for problems involving structured out-put variables.
However, for many real-world NLPapplications, exact maximum-likelihood training isintractable because computing the global normal-ization factor even approximately can be extremelyhard.
In addition, optimizing likelihood often doesnot correlate with maximizing task-specific evalu-ation measures.
In this paper, we present a noveltraining procedure, structured local training, thatmaximizes likelihood while exploiting the benefitsof global inference during training: hidden vari-ables are used to capture interactions between lo-cal inference and global inference.
Furthermore,we introduce biased potential functions that empir-ically drive CRFs towards performance improve-ments w.r.t.
the preferred evaluation measure forthe learning task.
We report promising experimen-tal results on two coreference data sets using twotask-specific evaluation measures.1 IntroductionUndirected graphical models such as ConditionalRandom Fields (CRFs) (Lafferty et al, 2001) haveshown great success for problems involving struc-tured output variables (e.g.
Wellner et al (2004),Finkel et al (2005)).
For many real-world NLP ap-plications, however, the required graph structure canbe very complex, and computing the global normal-ization factor even approximately can be extremelyhard.
Previous approaches for training CRFs haveeither (1) opted for a training method that no longermaximizes the likelihood, (e.g.
McCallum and Well-ner (2004), Roth and Yih (2005)) 1, or (2) opted for a1Both McCallum and Wellner (2004) and Roth and Yih(2005) used the voted perceptron algorithm (Collins, 2002) totrain intractable CRFs.simplified graph structure to avoid intractable globalnormalization (e.g.
Roth and Yih (2005), Wellner etal.
(2004)).Solutions of the first type replace the computationof the global normalization factor?y p(y|x) withargmaxy p(y|x) during training, since finding anargmax of a probability distribution is often an eas-ier problem than finding the entire probability distri-bution.
Training via the voted perceptron algorithm(Collins, 2002) or using a max-margin criterion alsocorrespond to the first option (e.g.
McCallum andWellner (2004), Finley and Joachims (2005)).
Butwithout the global normalization, the maximum-likelihood criterion motivated by the maximum en-tropy principle (Berger et al, 1996) is no longer afeasible option as an optimization criterion.The second solution simplifies the graph struc-ture for training, and applies complex global infer-ence only for testing.
In spite of the discrepancybetween the training model and the testing model,it has been empirically shown that (1) performingglobal inference only during testing can improveperformance (e.g.
Finkel et al (2005), Roth and Yih(2005)), and (2) full-blown global training can of-ten perform worse due to insufficient training data(e.g.
Punyakanok et al (2005)).
Importantly, how-ever, attempts to reduce the discrepancy between thetraining and test models ?
by judiciously adding theeffect of global inference to the training ?
have pro-duced substantial performance improvements overlocally trained models (e.g.
Cohen and Carvalho(2005), Sutton and McCallum (2005a)).In this paper, we present structured local training,a novel training procedure for maximum-likelihood65training of undirected graphical models, such asCRFs.
The procedure maximizes likelihood whileexploiting the benefits of global inference duringtraining by capturing the interactions between localinference and global inference via hidden variables.Furthermore, we introduce biased potential func-tions that redefine the likelihood for CRFs so thatthe performance of CRFs trained under the max-imum likelihood criterion correlates better empiri-cally with the preferred evaluation measures such asF-score and MUC-score.We focus on the problem of coreference resolu-tion; however, our approaches are general and canbe extended to other NLP applications with struc-tured output.
Our approaches also extend to non-conditional graphical models such as Markov Ran-dom Fields.
In experiments on two coreference datasets, structured local training reduces the error ratesignificantly (3.5%) for one coreference data set andminimally (?
1%) for the other.
Experiments usingbiased potential functions increase recall uniformlyand significantly for both data sets and both task-specific evaluation measures.
Results for the com-bination of the two techniques are promising, butmixed: pairwise F1 increases by 0.8-5.5% for bothdata sets; MUC F1 increases by 3.5% for one dataset, but slightly hurts performance for the seconddata set.In ?2, we describe structured local training, andfollow with experimental results in ?3.
In ?4, wedescribe biased potential functions and follow withexperimental results in ?5.
We discuss related workin ?6.2 Structured Local Training2.1 DefinitionsFor clarity, we define the following terms that wewill use throughout the paper.?
local inference: 2 Inference factored into smallerindependent pieces, without considering thestructure of the output space.?
global inference: Inference applied on the entireset of output variables, considering the structureof the output space.2In this paper, inference refers to the operation of finding theargmax in particular.?
local training: Training that does not invokeglobal inference at each iteration.?
global training: Training that does invoke globalinference at each iteration.2.2 A Motivating Example for CoreferenceResolutionIn this section, we present an example of the coref-erence resolution problem to motivate our approach.It has been shown that global inference-based train-ing for coreference resolution outperforms trainingwith local inference only (e.g.
Finley and Joachims(2005), McCallum and Wellner (2004)).
In particu-lar, the output of coreference resolution must obeyequivalence relations, and exploiting such structuralconstraints on the output space during training canimprove performance.
Consider the coreference res-olution task for the following text.It was after the passage of this act, that Mary(1)?s attitudetowards Elizabeth(1) became overtly hostile.
The deliber-ations surrounding the act seem to have revived all Mary?smemories of the humiliations she had suffered at thehands of Anne Boleyn.
At the same time, Elizabeth(2)?scontinuing prevarications over religion confirmed that shewas indeed her mother?s daughter.In the above text, the ?she?
in the last sen-tence is coreferent with both mentions of?Elizabeth?.
However, when we consider?she?
and ?Elizabeth(1)?
in isolation from theremaining coreference chain, it can be difficult fora machine learning method to determine whetherthe pair is coreferent or not.
Indeed, such apair may not look very different from the pair?she?
and ?Mary(1)?
in terms of feature vectors.It is much easier, however, to determine that?she?
and ?Elizabeth(2)?
are coreferent, or that?Elizabeth(1)?
and ?Elizabeth(2)?
are coreferent.Only by taking the transitive closure of these pair-wise coreference relations does it become clear that?she?
and ?Elizabeth(1)?
are coreferent.
In otherwords, global training might handle potentiallyconfusing coreference cases better because it allowsparameter learning (for each pairwise coreferencedecision) to be informed by global inference.We argue that, with appropriate modification tothe learning instances, local training is adequate forthe coreference resolution task.
Specifically, we pro-pose that confusing pairs in the training data ?
such66as ?she?
and ?Elizabeth(1)?
?
be learned as not-coreferent, so long as the global inference step canfix this error by exploiting the structure of the out-put space, i.e.
by exploiting the equivalence rela-tions.
This is the key idea of structured local train-ing, which we elaborate formally in the followingsection.2.3 A Hidden-Variable ModelIn this section, we present a general description ofstructured local training.
Let y be a vector of out-put variables for structured output, and let x be avector of input variables.
In order to capture the in-teractions between global inference and local infer-ence, we introduce hidden variables h, |h| = |y|,so that the global inference for p(y, h|x) can be fac-tored into two components using the product rule, asfollows:p(y, h|x) = p(y|h, x) p(h|x)= p(y|h) p(h|x)The second component p(h|x) on the right hand sidecorresponds to the local model, for which the infer-ence factorizes into smaller independent pieces, e.g.argmaxhp(h|x) = {argmaxhi?
(hi, x)}.
And thefirst component p(y|h, x) on the right hand side cor-responds to the global model, whose inference maynot factorize nicely.
Further, we assume that y is in-dependent of x given h, so that p(y|h, x) = p(y|h).That is to say, h captures sufficient information fromx, so that given h, global inference of y only de-pends on h. The quantity of p(y|x) then is given bymarginalizing out h as follows:p(y|x) =?hp(y, h|x)Intuitively, the hidden variables h represent the lo-cal decisions that can lead to a good y after globalinference is applied.
In the case of coreference reso-lution, one natural factorization would be that globalinference is a clustering algorithm, and local infer-ence is a classification decision on each pair of nounphrases (or mentions).3 In this paper, we assume3Formally, we define each yi?
y to be the coreference de-cision for the ith pair of mentions, and xi?
x be the inputregarding the ith pair of mentions.
Then hicorresponds to thelocal coreference decision that can lead to a good coreferencedecision yiafter the clustering algorithm has been applied.that we only parameterize the local model p(h|x),although it would be possible to extend the parame-terization to the global model as well, depending onthe particular application under consideration.
Thesimilarity between a pair of mentions is parameter-ized via log-linear models.
However, once we havethe similarity scores extracted via local inference,the clustering algorithm does not require further pa-rameterization.For training, we apply the standard Expectation-Maximization (EM) algorithm (Dempster et al,1977) as follows:?
E Step: Compute a distribution?P (t) = P (h|y, x, ?(t?1))?
M Step: Set ?
(t) to ?
that maximizesE?P(t)[logP (y, h|x, ?
)]By repeatedly applying the above two steps fort = 1, 2, ..., the value of ?
converges to the localmaxima of the conditional log likelihood L(?)
=logP (y|x, ?
).2.4 Application to Coreference ResolutionFor yi?
y (and hi?
h) in the coreference resolutiontask, yi= 1 (and hi= 1) corresponds to ith pair ofmentions being coreferent, and yi= 0 (and hi= 0)corresponds to ith pair being not coreferent.
[Local Model P (h|x)] For the local model, we de-fine cliques as individual nodes,4 and parameterizeeach clique potential as?
(hi, x) = ?
(hi, xi) = exp?k?kfk(hi, xi)Let ?
(h|x) ??i?
(hi, xi).
Then,P (h|x) =?
(h, x)?h ?
(h, x)Notice that in this model, finding argmaxhP (h|x)corresponds to simply finding argmaxhi?
(hi, xi) in-dependently for each hi?
h.4Each node in the graphical representation of CRFs corre-sponds to the coreferent decision for each pair of mentions.
Thiscorresponds to the ?Model 3?
of McCallum and Wellner (2004).67ALGORITHM-1INPUT: x, true labeling y?, current local model P (h|x)GOAL: Find the highest confidence labeling y?such that y?
= single-link-clustering(y?)h?
?
argmaxhP (h|x)h?
?
single-link-clustering(h?
)construct a graph G = (V, E), whereE = {h?i: h?i?
h?
s.t.
y?i= 1}V = {v : v is a NP referred by a h?i?
E}with edge cost costh?i= ?
(h?i, xi) if h?i6= y?iwith edge cost costh?i= 0 if h?i= y?ifind a minimum spanning tree(or forest) M of Gfor each h?i?
h?if h?i= y?iy?i?
h?ielse if h?i?
My?i?
1elsey?i?
0end forreturn y?Figure 1: Algorithm to find the highest confidence labeling y?that can be clustered to the true labeling y?
[Global Model P (y|h)] For the global model, weassume a deterministic clustering algorithm is given.In particular, we focus on single-link clustering, as ithas been shown to be effective for coreference reso-lution (e.g.
Ng and Cardie (2002)).
With single-linkclustering, P (y|h) = 1 if h can be clustered to y,and P (y|h) = 0 if h cannot be clustered to y.5[Computation of the E-step] The E-step requirescomputation of the distribution of P (h|y, x, ?
(t?1)),which we will simply denote as P (h|y, x), since allour distributions are implicitly conditioned on themodel parameters ?.P (h|y, x) =P (h, y|x)P (y|x)?
P (y|h) P (h|x)Notice that when computing P (h|y, x), the denomi-nator P (y|x) stays as a constant for different valuesof h. The E-step requires enumeration of all possiblevalues of h, but it is intractable with our formulation,because inference for the global model P (y|h) doesnot factor out nicely.
Therefore, we must resort to an5Single-link clustering simply takes the transitive closure,and does not consider the distance metric.
In a pilot study, wealso tried a variant of a stochastic clustering algorithm that takesinto account the distance metric (set as the probabilities fromthe local model) for the global model, but the performance wasworse.ALGORITHM-2INPUT: x, true labeling y?, current local model P (h|x)GOAL: Find a high confidence labeling y?
that isclose to the true labeling y?h?
?
argmaxhP (h|x)h?
?
single-link-clustering(h?
)for each h?i?
h?if h?i= y?iy?i?
h?ielsey?i?
y?iend forreturn y?Figure 2: Algorithm to find a high confidence labeling y?
thatis close to the true labeling y?approximation method.
Neal and Hinton (1998) an-alyze and motivate various approximate EM trainingmethods.
One popular choice in practice is called?Viterbi training?, a variant of the EM algorithm,which has been shown effective in many NLP ap-plications.
Viterbi training approximates the distri-bution by assigning all probability mass to a singlebest assignment.
The algorithm for this is shown inFigure 1.We propose another approximation option for theE-step that is given by Figure 2.
Intuitively, whenthe current local model misses positive coreferencedecisions, the first algorithm constructs a y?
that isclosest to h?
for single-link clustering to recover thetrue labeling y?, while the second algorithm con-structs a y?
that is closer to y?
by preserving all ofthe missing positive coreference decisions.
6[Computation of M-step] Because P (y|h) is notparameterized, finding argmax?P (y, h|x) reducesto finding argmax?P (h|x), which is standard CRFtraining.
In order to speed up the training, we startconvex optimization for CRFs using the parame-ter values ?
(t?1) from the previous M-step.
Forthe very first iteration of EM, we start by settingP (y?|x) = 1 for E-step, so that the first M-step willfinds argmax?P (y?|x).6In a pilot study, we found that ALGORITHM-2 per-forms slightly better than ALGORITHM-1.
We also tried twoother approximation options, but none performed as well asALGORITHM-2.
One of them removes the confusing sub-instances and has the effect of setting a uniform distribution onthose sub-instances.
The other computes the actual distributionon a subset of sub-instances.
For brevity, we only present ex-perimental results using ALGORITHM-2 in this paper.68[Inference on the test data] It is intractable tomarginalize out h from P (y, h|x).
Therefore, sim-ilar to the Viterbi-training in the E-step, we approx-imate the distribution of h by argmaxhP (h|X).3 Experiments?IData set: We evaluate our approach with twocoreference data sets: MUC6 (MUC-6, 1995) andMPQA7(Wiebe et al, 2005).
For the MUC6 data set,we extract noun phrases (mentions) automatically,but for MPQA, we assume mentions for corefer-ence resolution are given as in Stoyanov and Cardie(2006).
For MUC6, we use the standard training/testdata split.
For MPQA, we use 150 documents fortraining, and 50 documents for testing.Configuration: We follow Ng and Cardie (2002)for feature vector construction for each pair of men-tions,8 and Finley and Joachims (2005) for con-structing a training/testing instance for each docu-ment: a training/testing instance consists of all pairsof mentions in a document.
Then, a single pair ofmentions is a sub-instance.
We use the Mallet9 im-plementation of CRFs, and set a Gaussian prior of1.0 for all experiments.
At each M-step, we trainCRFs starting from the parameters from the previousM-step.
We train CRFs up to 200 iterations, but be-cause we start training CRFs from the previous pa-rameters, the convergence from the second M-stepbecomes much faster.
We apply up to 5 EM itera-tions, and choose best performing ?
(t), 2 ?
t ?
5based on the performance on the training data.10Hypothesis: For the baseline (BASE) we employthe locally trained model for pairwise decisionswithout global inference.
Clustering is applied onlyat test time, in order to make the assignment on theoutput variables coherent.
We hypothesize that forthe baseline, maximizing the likelihood for trainingwill correlate more with the pairwise accuracy of the7Available at http://nrrc.mitre.org/NRRC/publications.htm.8In particular, our feature set corresponds to ?All Features?in Ng and Cardie (2002), and we discretized numeric values.9Available at http://mallet.cs.umass.edu.10Selecting ?
(t) on a separate tuning data would be better, butthe data for MUC6 in particular is very limited.
Notice that wedon?t pick ?1 when reporting the performance of SLT, becauseit is identical to the baseline.MUC6after clustering before clusteringe % R % P % F % e % R % P % F %BASE 1.50 59.2 56.2 57.7 1.18 38.0 85.6 52.6SLT 1.28 49.8 67.3 57.2 1.35 26.4 84.3 40.2MPQAafter clustering before clusteringe % R % P % F % e % R % P % F %BASE 9.83 75.8 57.0 65.1 7.05 52.1 83.4 64.1SLT 6.39 62.1 80.6 70.2 7.39 43.7 90.1 58.9Table 1: Performance of Structured Local Training: SLT re-duces error rate (e %) after applying single-link clustering.incoherent decisions before clustering than the pair-wise accuracy of the coherent decisions after cluster-ing.
We also hypothesize that by performing struc-tured local training (SLT), maximizing the likeli-hood will correlate more with the pairwise accuracyafter clustering.Results: Experimental results are shown in Ta-ble 1.
We report error rate (error rate = 100 ?accuracy) on the pairwise decisions (e %), and F1-score (F %) on the coreferent pairs.11 For compar-ison, we show numbers from both after and beforesingle-link clustering is applied.
As hypothesized,the error rate of BASE increases after clustering,while the error rate of SLT decreases after cluster-ing.
Moreover, the error rate of SLT is considerablylower than that of BASE after clustering.
However,the F1-score does not correlate with the error rate.That is, a lower error rate does not always lead to ahigher F1-score, which motivates the Biased Poten-tial Functions that we introduce in the next section.Notice that when we compare the precision/recallbreakdown after clustering, SLT has higher precisionand lower recall than BASE.4 Biased Potential FunctionsWe introduce biased potential functions for train-ing CRFs to empirically favor preferred evaluationmeasures for the learning task, such as F-score andMUC-score that have been considered hard for tradi-11Error rate and F1-score on the coreferent pairs are not idealmeasures for the quality of clustering, however, we show themhere in order to contrast the effect of SLT.
We present MUC-scores for the same experimental settings in Table 3.69tional likelihood-based methods to optimize for.
In-tuitively, biased potential functions emphasize thosesub-components of an instance that can be of greaterimportance than the rest of an instance.4.1 DefinitionsThe conditional probability of P (y|x)12 for CRFs isgiven by (Lafferty et al, 2001)P (y|x) =?i?
(Ci, x)?y?i?
(Ci, x)where ?
(Ci, x) is a potential function defined overeach clique Ci.
Potential functions are typically pa-rameterized in an exponential form as follows.?
(Ci, x) = exp?k?kfk(Ci, x)where ?kare the parameters and fk(?)
are fea-ture indicator functions.
Because the Hammersley-Clifford theorem (1971) for undirected graphicalmodels holds for any non-negative potential func-tions, we propose alternative potential functions asfollows.?
(Ci, x) ={??
(Ci, x) if ?
(Ci, x) = true?
(Ci, x) otherwisewhere ?
is a non-negative bias factor, and ?
(Ci, x)is a predicate (or an indicator function) to check cer-tain properties on (Ci, x).13 Examples of possible?(?)
would be whether the true assignment for Ciin the training data contains certain class values, orwhether the current observation indexed by Cihasparticular characteristics.
More specific details willbe given in ?4.2.Training and testing with biased potential func-tions is mostly identical to the traditional log-linearformulations by ?(?)
as defined above, except forsmall and straightforward modifications to the com-putation of the likelihood and the derivative of thelikelihood.12For the local model described in Section 2, y should bereplaced with h. We use y in this section however, as it is amore conventional notation in general.13In our problem formulation, cliques are individual nodes,and potential functions are defined over the observations in-dexed by the current i only: i.e.
?
(Ci, x) = ?
(yi, xi),?
(Ci, x) = ?
(yi, xi) and ?
(Ci, x) = ?
(yi, xi).The key idea for biased potential functions isnothing new, as it is conceptually similar to in-stance weighting for problems with non-structuredoutput (e.g.
Aha and Goldstone (1992), Cardie et al(1997)).
However, biased potential functions differtechnically in that they emphasize desired subcom-ponents without altering the i.i.d.
assumption, andstill weight each instance alike.
Despite the con-ceptual simplicity, we are not aware of any previ-ous work that explored biased potential functions forproblems with structured output.4.2 Applications to Coreference Resolution[Bias on Coreferent Pairs] For coreference res-olution, pairs that are coreferent are in a minorityclass14, and biased potential functions can mitigatethis skewed data problem, by amplifying the cliquepotentials that correspond to coreferent pairs.
Wedefine ?
(yi, xi) to be true if and only if the true as-signment for yiin the training data is ?coreferent?.Notice that ?(?)
does not depend on what particu-lar value yimight take, but only depends on the truevalue of yiin the training data.
For testing, ?
(yi, xi)will be always false.15[Bias on Closer Coreferent Pairs] For corefer-ence resolution, we hypothesize that coreferent pairsfor closer mentions have more significance, becausethey tend to have clearer linguistic clues to deter-mine coreference.
We further hypothesize that byemphasizing only close coreferent pairs, we canhave our model favor the MUC score.
For this, wedefine ?
(yi, xi) to be true if and only if xiis for apair of mentions that are the closest coreferent pair.5 Experiments?IIData sets and configurations for experiments areidentical to those used in ?3.Hypothesis: We hypothesize that using biased po-tential functions, maximizing the likelihood fortraining can correlate better with F1-score or MUC-score than the pairwise accuracy.
In particular,14Only 1.72% of the pairs are coreferent in the MUC6 data,and about 12% are coreferent in the MPQA data.15Notice that ?
(yi, xi) changes the surface of the likelihoodfor training, but does not affect the inference of finding theargmax in our local model.
That is, argmaxyi?
(yi, xi) =argmaxyi?
(yi, xi) (with yireplaced with hi).70MUC6pairwise MUCe % R % P % F % R % P % F %BASE 1.18 38.0 85.6 52.6 59.0 75.8 66.4BASIC-P11.5 1.20 38.9 82.1 52.8 64.2 71.8 67.8BASIC-P13.0 1.32 46.9 71.3 56.6 68.9 64.3 66.5BASIC-Pa1.5 1.15 44.2 79.9 56.9 62.1 68.7 65.2BASIC-Pa3.0 1.44 52.5 62.9 57.2 70.9 60.5 65.3MPQApairwise MUCe % R % P % F % R % P % F %BASE 7.05 52.1 83.4 64.1 75.6 81.5 78.4BASIC-P11.5 7.18 54.6 79.6 64.8 77.7 76.5 77.1BASIC-P13.0 7.22 59.9 75.4 66.8 83.3 71.7 77.1BASIC-Pa1.5 7.65 59.7 72.2 65.4 79.8 73.2 76.4BASIC-Pa3.0 8.22 69.2 65.1 67.1 85.8 67.8 75.7Table 2: Performance of Biased Potential Functions: pairwisescores are taken before single-link-clustering is applied.we hypothesize that biasing on every coreferentpair will correlate more with F1-score, and bias-ing on close coreferent pairs will correlate morewith MUC-score.
In general, we expect that bias-ing on coreferent pairs will boost recall, potentiallydecreasing precision.Results [BPF]: Experimental results for biasedpotential functions, without structured local train-ing, are shown in Table 2.
BASIC-P1?
denotes localtraining with biased potential on the closest corefer-ent pairs with bias factor ?, and BASIC-Pa?
denoteslocal training with biased potential on the all coref-erent pairs with bias factor ?, where ?
= 1.5 or 3.0.For brevity, we only show pairwise numbers beforeapplying single-link-clustering.16 As hypothesized,biased potential functions in general boost recall atthe cost of precision.
Also, for a fixed value of?, BASIC-P1?
gives better MUC-F1 than BASIC-Pa?
, and BASIC-Pa?
gives better pairwise-F1 thanBASIC-P1?
for both data sets.Results [SLT+BPF]: Experimental results thatcombine SLT and BPF are shown in Table 3.
Sim-ilarly as before, SLT-Px?
denotes SLT with biasedpotential scheme Px, with bias factor ?.
For brevity,16This is because we showed in ?3 that basic local trainingdoes not correlate well with pairwise scores after clustering, andin order to see the direct effect of biased potential functions, weexamine pairwise numbers before clustering.MUC6pairwise MUCe % R % P % F % R % P % F %BASE 1.50 59.2 56.2 57.7 59.0 75.8 66.4SLT 1.28 49.8 67.3 57.2 56.3 77.8 65.3SLT-P11.5 1.19 52.8 70.6 60.4 59.3 74.6 66.1SLT-P13.0 1.42 63.5 57.9 60.6 67.5 70.7 69.1SLT-Pa1.5 1.43 58.6 58.5 58.5* 64.0 73.6 68.5SLT-Pa3.0 1.71 65.2 50.3 56.8 70.5 69.3 69.9*MPQApairwise MUCe % R % P % F % R % P % F %BASE 9.83 75.8 57.0 65.1 75.6 81.5 78.4SLT 6.39 62.1 80.6 70.2 69.1 88.2 77.5SLT-P11.5 6.54 64.9 77.4 70.6* 72.2 84.5 77.9*SLT-P13.0 9.09 77.2 59.6 67.3 78.4 79.5 78.9SLT-Pa1.5 6.74 65.2 75.7 70.1 72.4 87.2 79.1SLT-Pa3.0 14.71 78.2 43.9 56.2 80.5 73.8 77.0Table 3: Performance of Biased Potential Functions withStructured Local Training: All numbers are taken after single-link clustering.we only show numbers after applying single-link-clustering.
Unlike the results shown in Table 2,for a fixed value of ?, SLT-P1?
correlates betterwith pairwise-F1, and SLT-Pa?
correlates better withMUC-F1.
This indicates that when biased poten-tial functions are used in conjunction with SLT, theeffect of biased potential functions can be differentfrom the case without SLT.
Comparing F1-scores inTable 2 and Table 3, we see that the combination ofbiased potential functions with SLT improves per-formance in general.
In particular, SLT-P13.0 andSLT-Pa1.5 consistently improve performance overBASE on both data sets, for both pairwise-F1 andMUC-F1.
We present performance scores for allvariations of configurations for reference, but wealso mark the particular configuration SLT-Px?
(by?*?
on F1-scores) that is chosen when selecting theconfiguration based on the performance on the train-ing data for each performance measure.
To con-clude, structured local training with biased poten-tial functions bring a substantial improvement forMUC-F1 score, from 66.4% to 69.9% for MUC6data set.
For pairwise-F1, the performance increasefrom 57.7% to 58.5% for MUC6, and from 65.1% to70.6% for MPQA.1717Performance on the MPQA data for MUC-F1 is slightlydecreased from 78.4% to 77.9%.
Note the MUC scores for the716 Related WorkStructured local training is motivated by recent re-search that has shown that reducing the discrep-ancy between the training model and testing modelcan improve the performance without incurring theheavy computational overhead of full-blown globalinference-based training.
18 (e.g.
Cohen and Car-valho (2005), Sutton and McCallum (2005a), Suttonand McCallum (2005b)).
Our work differs in that(1) we use hidden variables to capture the interac-tions between local inference and global inference,(2) we present an application to coreference resolu-tion, while previous work has shown applications forvariants of sequence tagging.
McCallum and Well-ner (2004) showed a global training approach withCRFs for coreference resolution, but they used thevoted perceptron algorithm for training, which nolonger maximizes the likelihood.
In addition, theyassume that all and only those noun phrases involvedin coreference resolution are given.The performance of our system on MUC6 dataset is comparable to previously reported systems.Using the same feature set, Ng and Cardie (2002)reports 64.5% of MUC-score, while our systemachieved 69.9%.
Ng and Cardie (2002) reports70.4% of MUC-score using hand-selected features.With an additional feature selection or feature induc-tion step, the performance of our system might fur-ther improve.
McCallum and Wellner (2004) reports73.42% of MUC-score on MUC6 data set, but theirexperiments assumed perfect identification of all andonly those noun phrases involved in a coreferencerelation, thus substantially simplifying the task.7 ConclusionWe present a novel training procedure, structuredlocal training, that maximizes likelihood whileexploiting the benefits of global inference duringtraining.
This is achieved by incorporating hiddenvariables to capture the interactions between localMPQA baseline are already quite high to begin with.18The computational cost for SLT in our experiments wereabout twice of the cost for the local training of the baseline.
Thisis the case because M-step converges very fast from the secondEM iteration, by initializing CRFs using parameters from theprevious M-step.
Biased potential functions hardly adds extracomputational cost.
In practice, BPFs reduce training time sub-stantially: we observed that the higher the bias is, the quickerCRFs converge.inference and global inference.
In addition, weintroduce biased potential functions that allowCRFs to empirically favor performance measuressuch as F1-score or MUC-score.
We focused on theapplication of coreference resolution in this paper,but the key ideas of our approaches can be extendedto other applications, and other machine learningtechniques motivated by Markov networks.Acknowledgments We thank the reviewers as wellas Eric Breck and Ves Stoyanov for their many helpful com-ments.
This work was supported by the Advanced Research andDevelopment Activity (ARDA), by NSF Grants BCS-0624277,IIS-0535099, and IIS-0208028, and by gifts from Google andthe Xerox Foundation.ReferencesD.W.
Aha and R.L.
Goldstone.
1992.
Concept learning and flexible weighting.
InProc.
of the Fourteenth Annual Conference of the Cognitive Science Society.A.
Berger, S.D.
Pietra, V.D.
Pietra 1996.
A Maximum Entropy Approach toNatural Language Processing.
In Computational Linguistics,22.C.
Cardie and N. Howe.
1997.
Improving Minority Class Prediction Using Case-Specific Feature Weights.
In ICML.W.W.
Cohen and V. Carvalho.
2005.
Stacked Sequential Learning.
In IJCAI.M.
Collins.
2002.
Discriminative Training Methods for Hidden Markov Models:Theory and Experiments with Perceptron Algorithms.
In EMNLP.A.P.
Dempster, N. M. Laird and D. B. Rubin.
1977.
Maximum Likelihood fromIncomplete Data via the EM Algorithm.
In Journal of the Loyal StatisticalSociety, B.39.J.
Finkel, T. Grenager and C. D. Manning.
2005.
Incorporating Non-local Infor-mation Into Information Extraction Systems By Gibbs Sampling.
In ACL.T.
Finley and T. Joachims.
2005.
Supervised Clustering with Support VectorMachines.
In ICML.J.
Hammersley and P. Clifford.
1971.
Markov fields on finite graphs and lattices.Unpublished manuscript.J.
Lafferty, A. McCallum and F. Pereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and Labeling Sequence Data.
In ICML.A.
McCallum and B. Wellner.
2004.
Conditional Models of Identity Uncertaintywith Application to Noun Coreference.
In NIPS.MUC-6 1995.
In Proc.
of the Sixth Message Understanding Conference (MUC-6)Morgan Kaufmann.R.
M. Neal and G. E. Hinton.
1998.
A view of the EM algorithm that justiesincremental, sparse, and other variants.
In Learning in Graphical Models,Kluwer.V.
Ng and C. Cardie.
2002.
Improving Machine Learning Approaches to Coref-erence Resolution.
In ACL.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak 2005.
Learning and Inferenceover Constrained Output.
In IJCAI.D.
Roth and W. Yih.
2005.
Integer Linear Programming Inference for ConditionalRandom Fields.
In ICML.V.
Stoyanov and C. Cardie.
2006.
Partially Supervised Coreference Resolutionfor Opinion Summarization through Structured Rule Learning.
In EMNLP.C.
Sutton and A. McCallum.
2005.
Fast, Piecewise Training for DiscriminativeFinite-state and Parsing Models.
In Technical Report IR-403, University ofMassachusetts.C.
Sutton and A. McCallum.
2005.
Piecewise Training for Undirected Models.In UAI.B.
Wellner, A. McCallum, F. Peng and M. Hay.
2004.
An Integrated, ConditionalModel of Information Extraction and Coreference with Application to CitationMatching.
In UAI.J.
Wiebe and T. Wilson and C. Cardie 2005.
Annotating Expressions of Opinionsand Emotions in Language.
In Language Resources and Evaluation, volume39, issue 2-3.72
