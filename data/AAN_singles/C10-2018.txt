Coling 2010: Poster Volume, pages 153?161,Beijing, August 2010Acquisition of Unknown Word Paradigms for Large-Scale GrammarsKostadin CholakovUniversity of GroningenThe Netherlandsk.cholakov@rug.nlGertjan van NoordUniversity of GroningenThe Netherlandsg.j.m.van.noord@rug.nlAbstractUnknown words are a major issue forlarge-scale grammars of natural language.We propose a machine learning based al-gorithm for acquiring lexical entries forall forms in the paradigm of a given un-known word.
The main advantages of ourmethod are the usage of word paradigmsto obtain valuable morphological knowl-edge, the consideration of different con-texts which the unknown word and allmembers of its paradigm occur in andthe employment of a full-blown syntacticparser and the grammar we want to im-prove to analyse these contexts and pro-vide elaborate syntactic constraints.
Wetest our algorithm on a large-scale gram-mar of Dutch and show that its applicationleads to an improved parsing accuracy.1 IntroductionIn this paper, we present an efficient machinelearning based method for automated lexical ac-quisition (LA) which improves the performanceof large-scale computational grammars on real-life tasks.Our approach has three main advantages whichdistinguish it from other methods applied to thesame task.
First, it enables the acquisition of thewhole paradigm of a given unknown word whileother approaches are only concerned with the par-ticular word form encountered in the data sub-ject to LA.
Second, we analyse different contextswhich the unknown word occurs in.
Third, theanalysis of these contexts is provided by a full-blown syntactic parser and the grammar we aimto improve which gives the grammar the opportu-nity to participate directly in the LA process.Our method achieves an F-measure of 84.6%on unknown words in experiments with the wide-coverage Alpino grammar (van Noord, 2006) ofDutch.
The integration of this method in theparser leads to a 4.2% error reduction in terms oflabelled dependencies.To predict a lexical entry for a given unknownword, we take into account two factors?
its mor-phology and the syntactic constraints imposed byits context.
As for the former, the acquisition ofthe whole paradigm provides us with a valuablesource of morphological information.
If we wereto deal with only one form of the unknown word,this information would not be accessible.Further, looking at different contexts of the un-known word gives us the possibility to work withlinguistically diverse data and to incorporate moresyntactic information into the LA process.
Caseswhere this is particularly important include mor-phologically ambiguous words and verbs whichsubcategorize for various types of syntactic argu-ments.
We also consider contexts of the othermembers of the paradigm of the unknown wordin order to increase the amount of linguistic dataour method has access to.Finally, the usage of a full-blown syntacticparser and the grammar we want to acquire lex-ical entries for has two advantages.
First, LAcan benefit from the high-quality analyses sucha parser produces and the elaborate syntactic in-formation they provide.
Second, this informationcomes directly from the grammar, thus allowingthe LA process to make predictions based on whatthe grammar considers to be best suited for it.153The remainder of the paper is organised as fol-lows.
Section 2 describes the basic steps in ourLA algorithm.
Section 3 presents initial exper-iments conducted with Alpino and shows thatthe main problems our LA method encountersare the acquisition of morphologically ambigu-ous words, the learning of the proper subcate-gorization frames for verbs and the acquisitionof particular types of adjectives.
In Section 4we make extensive use of the paradigms of theunknown words to develop specific solutions forthese problems.
Section 5 describes experimentswith our LA method applied to a set of real un-known words.
Section 6 provides a comparisonbetween our approach and work previously doneon LA.
This section also discusses the applicationof our method to other systems and languages.2 Basic AlgorithmThe Alpino wide-coverage dependency parser isbased on a large stochastic attribute value gram-mar.
The grammar takes a ?constructional?
ap-proach, with rich lexical representations stored inthe lexicon and a large number of detailed, con-struction specific rules (about 800).
Currently, thelexicon contains about 100K lexical entries anda list of about 200K named entities.
Each wordis assigned one or more lexical types.
For ex-ample, the verb amuseert (to amuse) is assignedtwo lexical types?
verb(hebben,sg3,intransitive)and verb(hebben,sg3,transitive)?
because it canbe used either transitively or intransitively.
Theother type features indicate that it is a present thirdperson singular verb and it forms perfect tensewith the auxiliary verb hebben.The goal of our LA method is to assign the cor-rect lexical type(s) to a given unknown word.
Themethod takes into account only open-class lexicaltypes: nouns, adjectives and verbs, under the as-sumption that the grammar is already able to han-dle all closed-class cases.
We call the types con-sidered by our method universal types.
The adjec-tives can be used as adverbs in Dutch and thus, wedo not consider the latter to be an open class.We employ a ME-based classifier which, forsome unknown word, takes various morphologicaland syntactic features as input and outputs lexicaltypes.
The probability of a lexical type t, given anunknown word and its context c is:(1) p(t|c) = exp(?i ?ifi(t,c))?t?
?T exp(?i ?ifi(t?,c))where fi(t, c) may encode arbitrary characteris-tics of the context and < ?1,?2, ... > can be eval-uated by maximising the pseudo-likelihood on atraining corpus (Malouf, 2002).Table 1 shows the features for the noun in-spraakprocedures (consultation procedures).
Row(i) contains 4 separate features derived from theprefix of the word and 4 other suffix features aregiven in row (ii).
The two features in rows (iii)and (iv) indicate whether the word starts with aparticle and if it contains a hyphen, respectively.Another source of morphological features is theparadigm of the unknown word which providesinformation that is otherwise inaccessible.
For ex-ample, in Dutch, neuter nouns always take the hetdefinite article while all other noun forms are usedwith the de article.
Since the article is distinguish-able only in the singular noun form, the correctarticle of a word, assigned a plural noun type, canbe determined if we know its singular form.We adopt the method presented in Cholakovand van Noord (2009) where a finite state mor-phology is applied to generate the paradigm(s) ofa given word.
The morphology does not have ac-cess to any additional linguistic information andthus, it generates all possible paradigms allowedby the word structure.
Then, the number ofsearch hits Yahoo returns for each form in a givenparadigm is combined with some simple heuris-tics to determine the correct paradigm(s).However, we make some modifications to thismethod because it deals only with regular mor-phological phenomena.
Though all typical irreg-ularities are included in the Alpino lexicon, thereare cases of irregular verbs composed with parti-cles which are not listed there.
One such exampleis the irregular verb meevliegen (to fly with some-one) for which no paradigm would be generated.To avoid this, we use a list of common parti-cles to strip off any particle from a given unknownword.
Once we have removed a particle, we checkif what is left from the word is listed in the lexiconas a verb (e.g.
vliegen in the case of meevliegen).If so, we extract all members of its paradigm from154Featuresi) i, in, ins, inspii) s, es, res, uresiii) particle yes #in this case iniv) hyphen nov) noun?de,pl?vi) noun(de,count,pl), tmp noun(de,count,sg)vii) noun(de), noun(count), noun(pl), tmp noun(de)tmp noun(count), tmp noun(sg)Table 1: Features for inspraakproceduresthe lexicon and use them to build the paradigm ofthe unknown word.
All forms are validated by us-ing the same web-based heuristics as in the origi-nal model of Cholakov and van Noord (2009).A single paradigm is generated for in-spraakprocedures indicating that this word is aplural de noun.
This information is explicitly usedas a feature in the classifier which is shown in row(v) of Table 1.Next, we obtain syntactic features for in-spraakprocedures by extracting a number of sen-tences which it occurs in from large corpora orInternet.
These sentences are parsed with a differ-ent ?mode?
of Alpino where this word is assignedall universal types, i.e.
it is treated as being maxi-mally ambiguous.
For each sentence only the bestparse is preserved.
Then, the lexical type that hasbeen assigned to inspraakprocedures in this parseis stored.
During parsing, Alpino?s POS tagger(Prins and van Noord, 2001) keeps filtering im-plausible type combinations.
For example, if a de-terminer occurs before the unknown word, all verbtypes are typically not taken into consideration.This heavily reduces the computational overloadand makes parsing with universal types computa-tionally feasible.
When all sentences have beenparsed, a list can be drawn up with the types thathave been used and their frequency:(2) noun(de,count,pl) 78tmp noun(de,count,sg) 7tmp noun(het,count,pl) 6proper name(pl,?PER?)
5proper name(pl,?ORG?)
3verb(hebben,pl,vp) 1The lexical types assigned to inspraakproceduresin at least 80% of the parses are used as featuresin the classifier.
These are the two features in row(vi) of Table 1.
Further, as illustrated in row (vii),each attribute of the considered types is also takenas a separate feature.
By doing this, we let thegrammar decide which lexical type is best suitedfor a given unknown word.
This is a new and ef-fective way to include the syntactic constraints ofthe context in the LA process.However, for the parsing method to work prop-erly, the disambiguation model of the parser needsto be adapted.
The model heavily relies on thelexicon and it has learnt preferences how to parsecertain phrases.
For example, it has learnt a pref-erence to parse prepositional phrases as verb com-plements, if the verb includes such a subcatego-rization frame.
This is problematic when parsingwith universal types.
If the unknown word is averb and it occurs together with a PP, it would al-ways get analysed as a verb which subcategorizesfor a PP.To avoid this, the disambiguation model is re-trained on a specific set of sentences meant tomake it more robust to input containing many un-known words.
We have selected words with lowfrequency in large corpora and removed them tem-porarily from the Alpino lexicon.
Less frequentwords are typically not listed in the lexicon andthe selected words are meant to simulate their be-haviour.
Then, all sentences from the Alpino tree-bank which contain these words are extracted andused to retrain the disambiguation model.3 Initial Experiments and EvaluationTo evaluate the performance of the classifier, weconduct an experiment with a target type inven-tory of 611 universal types.
A type is considereduniversal only if it is assigned to at least 15 dis-tinct words occurring in large Dutch newspapercorpora (?16M sentences) automatically parsedwith Alpino.In order to train the classifier, 2000 words aretemporarily removed from the Alpino lexicon.The same is done for another 500 words whichare used as a test set.
All words have between50 and 100 occurrences in the corpora.
This se-lection is again meant to simulate the behaviourof unknown words.
Experiments with a minimumlower than 50 occurrences have shown that this isa reasonable threshold to filter out typos, wordswritten together, etc.155The classifier yields a probability score for eachpredicted type.
Since a given unknown word canhave more than one correct type, we want to pre-dict multiple types.
However, the least frequenttypes, accounting together for less than 5% ofprobability mass, are discarded.We evaluate the results in terms of precisionand recall.
Precision indicates how many typesfound by the method are correct and recall indi-cates how many of the lexical types of a givenword are actually found.
The presented results arethe average precision and recall for the 500 testwords.Additionally, there are three baseline methods:?
Naive?
each unknown word is assignedthe most frequent type in the lexicon:noun(de,count,sg)?
POS tagger?
the unknown word is given thetype most frequently assigned by the AlpinoPOS tagger in the parsing stage?
Alpino?
the unknown word is assigned themost frequently used type in the parsingstageThe overall results are given in Table 2.
Table 3shows the results for each POS in our model.Model Precision(%) Recall(%) F-measure(%)Naive 19.60 18.77 19.17POS tagger 30 26.21 27.98Alpino 44.60 37.59 40.80Our model 86.59 78.62 82.41Table 2: Overall experiment resultsPOS Precision(%) Recall(%) F-measure(%)Nouns 93.83 88.61 91.15Adjectives 75.50 73.12 74.29Verbs 77.32 55.37 64.53Table 3: Detailed results for our modelOur LA method clearly improves upon thebaselines.
However, as we see in Table 3, adjec-tives and especially verbs remain difficult to pre-dict.The problems with the former are due to the factthat Alpino employs a rather complicated adjec-tive system.
The classifier has difficulties distin-guishing between 3 kinds of adjectives: i) adjec-tives which can attach to and modify verbs andverbal phrases (VPs) (3-a), ii) adjectives whichcan attach to verbs and VPs but modify one ofthe complements of the verb, typically the sub-ject (3-b) and iii) adjectives which cannot attachto verbs and VPs (3-c).
(3) a. DeDEThardloperrunnerlooptwalksmooi.nice?The runner runs nicely = The runner has agood running technique?b.
Hijhelooptwalksdronkendrunknaartohuis.home?He walks home drunk = He is walking homewhile being drunk?c.
*Hijhelooptwalksnederlandstalig.Dutch speaking?He walks Dutch speaking.
?Each of these is marked by a special attribute inthe lexical type definitions?
adv, padv and non-adv, respectively.
Since all three of them are seenin ?typical?
adjectival contexts where they modifynouns, it is hard for the classifier to make a distinc-tion.
The predictions appear to be arbitrary andthere are many cases where the unknown word isclassified both as a nonadv and an adv adjective.
Itis even more difficult to distinguish between padvand adv adjectives since this is a solely semanticdistinction.The main issue with verbs is the prediction ofthe correct subcategorization frame.
The classifiertends to predict mostly transitive and intransitiveverb types.
As a result, it either fails to capture in-frequent frames which decreases the recall or, incases where it is very uncertain what to predict, itassigns a lot of types that differ only in the subcatframe, thus damaging the precision.
For example,onderschrijf (?to agree with?)
has 2 correct sub-cat frames but receives 8 predictions which differonly in the subcat features.One last issue is the prediction, in some rarecases, of types of the wrong POS for morpholog-ically ambiguous words.
In most of these casesadjectives are wrongly assigned a past partici-ple type but also some nouns receive verb pre-dictions.
For instance, OESO-landen (?countriesof the OESO organisation?)
has one correct nountype but because landen is also the Dutch verb for?to land?
the classifier wrongly assigns a verb typeas well.1564 Improving LA4.1 POS CorrectionSince the vast majority of wrong POS predictionshas to do with the assignment of incorrect verbtypes, we decided to explicitly use the generatedverb paradigms as a filtering mechanism.
For eachword which is assigned a verb type, we check ifthere is a verb paradigm generated for it.
If not, allverb types predicted for the word are discarded.In very rare cases a word is assigned only verbtypes and therefore, it ends up with no predictions.For such words, we examine the ranked list of pre-dicted types yielded by the classifier and the wordreceives the non-verb lexical type with the high-est probability score.
If this type happens to bean adjective one, we first check whether there isan adjective paradigm generated for the word inquestion.
If not, the word gets the noun type withthe highest probability score.The same procedure is also applied to all wordswhich are assigned an adjective type.
However,it is not used for words predicted to be nouns be-cause the classifier is already very good at predict-ing nouns.
Further, the generated noun paradigmsare not reliable enough to be a filtering mechanismbecause there are mass nouns with no plural formsand thus with no paradigms generated.Another modification we make to the classifieroutput has to do with the fact that past participles(psp) in Dutch can also be used as adjectives.
Thissystematic ambiguity, however, is not treated assuch in Alpino.
Each psp should also have a sep-arate adjective lexical entry but this is not alwaysthe case.
That is why, in some cases, the classifierfails to capture the adjective type of a given psp.To account for it, all words predicted to be pastparticiples but not adjectives are assigned two ad-ditional adjective types?
one with the nonadv andone with the adv feature.
For reasons explainedlater on, a type with the padv feature is not added.After the application of these techniques, allcases of words wrongly predicted to be verbs oradjectives have been eliminated.4.2 Guessing Subcategorization FramesOur next step is to guess the correct subcatego-rization feature for verbs.
Learning the propersubcat frame is well studied (Brent, 1993; Man-ning, 1993; Briscoe and Caroll, 1997; Kinyon andProlo, 2002; O?Donovan et al, 2005).
Most ofthe work follows the ?classical?
Briscoe and Caroll(1997) approach where the verb and the subcate-gorized complements are extracted from the out-put analyses of a probabilistic parser and stored assyntactic patterns.
Further, some statistical tech-niques are applied to select the most probableframes out of the proposed syntactic patterns.Following the observations made in Korho-nen et al (2000), Lapata (1999) and Messiant(2008), we employ a maximum likelihood es-timate (MLE) from observed relative frequen-cies with an empirical threshold to filter out lowprobability frames.
For each word predicted tobe a verb, we look up the verb types assignedto it during the parsing with universal types.Then, the MLE for each subcat frame is deter-mined and only frames with MLE of 0.2 andabove are considered.
For example, jammert(to moan.3SG.PRES) is assigned a single type?verb(hebben,sg3,intransitive).
However, the cor-rect subcat features for it are intransitive and sbar.Here is the list of all verb types assigned to jam-mert during the parsing with universal types:(4) verb(hebben,sg3,intransitive) 48verb(hebben,sg3,transitive) 15verb(hebben,past(sg),np sbar) 3verb(hebben,past(sg),tr sbar) 3verb(zijn,sg3,intransitive) 2verb(hebben,past(sg),ld pp) 2verb(hebben,sg3,sbar) 1The MLE for the intransitive subcat feature is 0.68and for the transitive one?
0.2.
All previously pre-dicted verb types are discarded and each consid-ered subcat frame is used to create a new lexi-cal type.
That is how jammert gets two types atthe end?
the correct verb(hebben,sg3,intransitive)and the incorrect verb(hebben,sg3,transitive).
Thesbar frame is wrongly discarded.To avoid such cases, the generated wordparadigms are used to increase the number of con-texts observed for a given verb.
Up to 200 sen-tences are extracted for each form in the paradigmof a given word predicted to be a verb.
These sen-tences are again parsed with the universal typesand then, the MLE for each subcat frame is recal-157culated.We evaluated the performance of our MLE-based method on the 116 test words predicted tobe verbs.
We extracted the subcat features fromtheir type definitions in the Alpino lexicon to cre-ate a gold standard of subcat frames.
Addition-ally, we developed two baseline methods: i) allframes assigned during parsing are considered andii) each verb is taken to be both transitive and in-transitive.
Since most verbs have both or one ofthese frames, the purpose of the second baseline isto see if there is a simpler solution to the problemof finding the correct subcat frame.
The resultsare given in Table 4.Model Precision(%) Recall(%) F-measure(%)all frames 16.76 94.34 28.46tr./intr.
62.29 69.17 65.55our model 85.82 67.28 75.43Table 4: Subcat frames guessing resultsOur method significantly outperforms bothbaselines.
It is able to correctly identify the transi-tive and/or the intransitive frames.
Since they arethe most frequent ones in the test data, this boostsup the precision.
However, the method is also ableto capture other, less frequent subcat frames.
Forexample, after parsing the additional sentences forjammert, the sbar frame had enough occurrencesto get above the threshold.
The MLE for the tran-sitive one, on the other hand, fell below 0.2 and itwas correctly discarded.4.3 Guessing Adjective TypesWe follow a similar approach for finding the cor-rect adjective type.
It should be noted that thedistinction among nonadv, adv and padv doesnot exist for every adjective form.
Most ad-jectives in Dutch get an -e suffix when usedattributively?
de mooie/mooiere/mooiste jongen(the nice/nicer/nicest boy).
Since these inflectedforms can only occur before nouns, the distinctionwe are dealing with is not relevant for them.
Thuswe are only interested in the noninflected base,comparative and superlative adjective forms.One of the possible output formats of Alpinois dependency triples.
Here is the output for thesentence in (3-a):(5) verb:loop|hd/su|noun:hardlopernoun:hardloper|hd/det|det:deverb:loop|hd/mod|adj:mooiverb:loop|?/?|punct:.Each line is a single dependency triple.
The linecontains three fields separated by the ?|?
character.The first field contains the root of the head wordand its POS, the second field indicates the type ofthe dependency relation and the third one containsthe root of the dependent word and its POS.
Thethird line in (5) shows that the adjective mooi is amodifier of the head, in this case the verb loopt.Such a dependency relation indicates that this ad-jective can modify a verb and therefore, it belongsto the adv type.As already mentioned, padv adjectives cannotbe distinguished from the ones of the adv kind.That is why, if the classifier has decided to assigna padv type to a given unknown word, we discardall other adjective types assigned to it (if any) anddo not apply the technique described below to thisword.For each of the 59 words assigned an non-inflected adjective type after the POS correctionstage, we extract up to 200 sentences for all non-inflected forms in its paradigm.
These sentencesare parsed with Alpino and the universal types andthe output is dependency triples.
All triples wherethe unknown word occurs as a dependent word ina head modifier dependency (hd/mod, as shown in(5)) and its POS is adjective are extracted from theparse output.
We calculate the MLE of the caseswhere the head word is a verb, i.e.
where the un-known word modifies a verb.
If the MLE is 0.05or larger, the word is assigned an adv lexical type.For example, the classifier correctly identifiesthe word doortimmerd (solid) as being of the ad-jective(no e(nonadv)) type but it also predicts theadjective(no e(adv))1 type for it.
Since we havenot found enough sentences where this word mod-ifies a verb, the latter type is correctly discarded.Our technique produced correct results for 53 outof the 59 adjectives processed.1The no e type attribute denotes a noninflected base ad-jective form.1584.4 Improved Results and DiscussionTable 5 presents the results obtained after apply-ing the improvement techniques described in thissection to the output of the classifier (the ?Model2?
rows).
For comparison, we also give the re-sults from Table 3 again (the ?Model 1?
rows).The numbers for the nouns happen to remain un-changed and that is why they are not shown in Ta-ble 5.POS Models Prec.
(%) Rec.
(%) F-meas.
(%)Adj Model 1 75.50 73.12 74.29Model 2 85.16 80.16 82.58Verbs Model 1 77.32 55.37 64.53Model 2 80.56 56.24 66.24Overall Model 1 86.59 78.62 82.41Model 2 89.08 80.52 84.58Table 5: Improved resultsThe automatic addition of adjective types forpast participles improved significantly the recallfor adjectives and our method for choosing be-tween adv and nonadv types caused a 10% in-crease in precision.However, these procedures also revealed someincomplete lexical entries in Alpino.
For example,there are two past participles not listed as adjec-tives in the lexicon though they should be.
Thuswhen our method correctly assigned them adjec-tive types, it got punished since these types werenot in the gold standard.We see in Table 5 that the increase in precisionfor the verbs is small and recall remains practi-cally unchanged.
The unimproved recall showsthat we have not gained much from the subcatframe heuristics.
Even when the number of theobserved sentences was increased, less frequentframes often remained unrecognisable from thenoise in the parsed data.
This could be seen asa proof that in the vast majority of cases verbsare used transitively and/or intransitively.
Sincethe MLE method we employ proved to be good atrecognising these two frames and differentiatingbetween them, we have decided to continue usingit.The overall F-score improved by only 2% be-cause the modified verb and adjective predictionsare less than 30% of the total predictions made bythe classifier.5 Experiment with Real UnknownWordsTo investigate whether the proposed LA methodis also beneficial for the parser, we observe howparsing accuracy changes when the method is em-ployed.
Accuracy in Alpino is measured in termsof labelled dependencies.We have conducted an experiment with a testset of 300 sentences which contain 188 real un-known words.
The sentences have been randomlyselected from the manually annotated LASSYcorpus (van Noord, 2009) which contains textfrom various domains.
The average sentencelength is 26.54 tokens.The results are given in Table 6.
The standardAlpino model uses its guesser to assign types tothe unknown words.
Model 1 employs the trainedME-based classifier to predict lexical entries forthe unknown words offline and then uses themduring parsing.
Model 2 uses lexical entries modi-fied by applying the methods described in Section4 to the output of the classifier (Model 1).Model Accuracy (%) msec/sentenceAlpino 88.77 8658Model 1 89.06 8772Model 2 89.24 8906Table 6: Results with real unknown wordsOur LA system as a whole shows an error re-duction rate of more than 4% with parse times re-maining similar to those of the standard Alpinoversion.
It should also be noted that though muchof the unknown words are generally nouns, we seefrom the results that it makes sense to also employthe methods for improving the predictions for theother POS types.
A wrong verb or even adjec-tive prediction can cause much more damage tothe analysis than a wrong noun one.These results illustrate that the integration ofour method in the parser can improve its perfor-mance on real-life data.6 Discussion6.1 Comparison to Previous WorkThe performance of the LA method we presentedin this paper can be compared to the performance159of a number of other approaches previously ap-plied to the same task.Baldwin (2005) uses a set of binary classifiersto learn lexical entries for a large-scale gram-mar of English (ERG; (Copestake and Flickinger,2000)).
The main disadvantage of the method isthat it uses information obtained from secondarylanguage resources?
POS taggers, chunkers, etc.Therefore, the grammar takes no part in the LAprocess and the method acquires lexical entriesbased on incomplete linguistic information pro-vided by the various resources.
The highest F-measure (about 65%) is achieved by using fea-tures from a chunker but it is still 20% lower thanthe results we report here.
Further, no evalua-tion is done on how the method affects the per-formance of the ERG when the grammar is usedfor parsing.Zhang and Kordoni (2006) and Cholakov etal.
(2008), on the other hand, include featuresfrom the grammar in a maximum entropy (ME)classifier to predict new lexical entries for theERG and a large German grammar (GG; (Crys-mann, 2003)), respectively.
The development datafor this method consist of linguistically annotatedsentences from treebanks and the grammar fea-tures used in the classifier are derived from thisannotation.
However, when the method is appliedto open-text unannotated data, the grammar fea-tures are replaced with POS tags.
Therefore, thegrammar is no longer directly involved in the LAprocess which affects the quality of the predic-tions.
Evaluation on sentences containing real un-known words shows improvement of the coveragefor the GG when LA is employed but the accuracydecreases by 2%.
Such evaluation has not beendone for the ERG.
The results on the developmentdata are not comparable with ours because evalu-ation is done only in terms of precision while weare also able to measure recall.Statistical LA has previously been applied toAlpino as well (van de Cruys, 2006).
However,his method employs less morphosyntactic featuresin comparison to our approach and does not makeuse of word paradigms.
Further, though experi-ments on development data are performed on asmaller scale, the results in terms of F-measure are10% lower than those reported in our case study.Experiments with real unknown words have notbeen performed.Other, non-statistical LA methods also exist.Cussens and Pulman (2000) describe a symbolicapproach which employs inductive logic program-ming and Barg and Walther (1998) and Fouvry(2003) follow a unification-based approach.
How-ever, the generated lexical entries might be bothtoo general or too specific and it is doubtful ifthese methods can be used on a large scale.
Theyhave not been applied to broad-coverage gram-mars and no evaluation is provided.6.2 Application to Other Systems andLanguagesWe stress the fact that the experiments withAlpino represent only a case study.
The proposedLA method can be applied to other computationalgrammars and languages providing that the fol-lowing conditions are fulfilled.First, words have to be mapped onto some fi-nite set of labels of which a subset of open-class(universal) labels has to be selected.
This subsetrepresents the labels which the ME-based classi-fier can predict for unknown words.
Second, a(large) corpus has to be available, so that varioussentences in which a given unknown word occurscan be extracted.
This is crucial for obtaining dif-ferent contexts in which this word is found.Next, we need a parser to analyse the extractedsentences which allows for the syntactic con-straints imposed by these contexts to be includedin the prediction process.Finally, as for the paradigm generation, the ideaof combining a finite state morphology and webheuristics is general enough to be implementedfor different languages.
It is also important tonote that the classifier allows for arbitrary com-binations of features and therefore, a researcher isfree to include any (language-specific) features heor she considers useful for performing LA.We have already started investigating the appli-cability of our LA method to large-scale gram-mars of German and French and the initial experi-ments and results we have obtained are promising.160ReferencesBaldwin, Tim.
2005.
Bootstrapping deep lexical re-sources: Resources for courses.
In Proceedings ofthe ACL-SIGLEX 2005 Workshop on Deep LexicalAcquisition, Ann Arbor, USA.Barg, Petra and Markus Walther.
1998.
Processing un-known words in HPSG.
In Proceedings of the 36thConference of the ACL, Montreal, Quebec, Canada.Brent, Michael R. 1993.
From grammar to lexicon:unsupervised learning of lexical syntax.
Computa-tional Linguistics, 19(2):243?262.Briscoe, Ted and John Caroll.
1997.
Automatic ex-traction of subcategorization from corpora.
In Pro-ceedings of the 5th ACL Conference on Applied Nat-ural Language Processing, Washington, DC.Cholakov, Kostadin and Gertjan van Noord.
2009.Combining finite state and corpus-based techniquesfor unknown word prediction.
In Proceedings of the7th Recent Advances in Natural Language Process-ing (RANLP) conference, Borovets, Bulgaria.Cholakov, Kostadin, Valia Kordoni, and Yi Zhang.2008.
Towards domain-independent deep linguisticprocessing: Ensuring portability and re-usability oflexicalised grammars.
In Proceedings of COLING2008 Workshop on Grammar Engineering AcrossFrameworks (GEAF08), Manchester, UK.Copestake, Ann and Dan Flickinger.
2000.
Anopen-source grammar development environmentand broad-coverage English grammar using HPSG.In Proceedings of the 2nd International Confer-ence on Language Resource and Evaluation (LREC2000), Athens, Greece.Crysmann, Berthold.
2003.
On the efficient imple-mentation of German verb placement in HPSG.
InProceedings of RANLP 2003, Borovets, Bulgaria.Cussens, James and Stephen Pulman.
2000.
Incor-porating linguistic constraints into inductive logicprogramming.
In Proceedings of the Fourth Con-ference on Computational Natural Language Learn-ing.Fouvry, Frederik.
2003.
Lexicon acquisition with alarge-coverage unification-based grammar.
In Com-panion to the 10th Conference of EACL, pages 87?90, Budapest, Hungary.Kinyon, Alexandra and Carlos A Prolo.
2002.
Iden-tifying verb arguments and their syntactic functionin the Penn Treebank.
In Proceedings of the 3rd In-ternational Conference on Language Resource andEvaluation (LREC 2002), Las Palmas de Gran Ca-naria, Spain.Korhonen, Anna, Genevieve Gorell, and Diana Mc-Carthy.
2000.
Statistical filtering and subcatego-rization frame acquisition.
In Proceedings of theJoint SIGDAT Conference on Empirical Methods inNatural Language Processing and Very Large Cor-pora, Hong Kong, China.Lapata, Mirella.
1999.
Acquiring lexical generaliza-tions from corpora.
A case study for diathesis alter-nations.
In Proceedings of the 37th Annual Meetingof ACL, Maryland, USA.Malouf, Robert.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the 6th conference on Natural LanguageLearning (CoNLL-2002), pages 49?55, Taipei, Tai-wan.Manning, Christopher.
1993.
Automatic acquisitionof a large subcategorization dictionary from cor-pora.
In Proceedings of the 31st Annual Meetingof ACL, Columbus, OH.Messiant, Cedric.
2008.
A subcategorization acquisi-tion system for French verbs.
In Proceedings of theACL 2008 Student Research Workshop, Columbus,OH.O?Donovan, Ruth, Michael Burke, Aoife Cahill, Josefvan Genabith, and Andy Way.
2005.
Large-scaleinduction and evaluation of lexical resources fromthe Penn-II and Penn-III Treebanks.
ComputationalLinguistics, 31(3):329?365.Prins, Robbert and Gertjan van Noord.
2001.
Un-supervised POS-tagging improves parcing accuracyand parsing efficiency.
In Proceedings of IWPT,Beijing, China.van de Cruys, Tim.
2006.
Automatically extending thelexicon for parsing.
In Huitnik, Janneje and SophiaKatrenko, editors, Proceedings of the Eleventh ESS-LLI Student Session, pages 180?189.van Noord, Gertjan.
2006.
At last parsing is now oper-ational.
In Proceedings of TALN, Leuven, Belgium.van Noord, Gertjan.
2009.
Huge parsed corpora inLASSY.
In Proceedings of the Seventh Interna-tional Workshop on Treebanks and Linguistic The-ories (TLT 7), Groningen, The Netherlands.Zhang, Yi and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open text processing.In Proceedings of the Fifth International Confer-ence on Language Resourses and Evaluation (LREC2006), Genoa, Italy.161
