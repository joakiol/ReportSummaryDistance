Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 136?141,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsEffect of Using Regression on Class Confidence Scores in SentimentAnalysis of Twitter DataItir Onal*, Ali Mert Ertugrul?, Ruken Cakici**Department of Computer Engineering, Middle East Technical University, Ankara, Turkeyitir,ruken@ceng.metu.edu.tr?Department of Information Systems, Middle East Technical University, Ankara, Turkeyalimert@metu.edu.trAbstractIn this study, we aim to test our hypothesisthat confidence scores of sentiment valuesof tweets aid in classification of sentiment.We used several feature sets consisting oflexical features, emoticons, features basedon sentiment scores and combination oflexical and sentiment features.
Since ourdataset includes confidence scores of realnumbers in [0-1] range, we employ regres-sion analysis on each class of sentiments.We determine the class label of a tweet bylooking at the maximum of the confidencescores assigned to it by these regressors.We test the results against classificationresults obtained by converting the confi-dence scores into discrete labels.
Thus,the strength of sentiment is ignored.
Ourexpectation was that taking the strengthof sentiment into consideration would im-prove the classification results.
Contraryto our expectations, our results indicatethat using classification on discrete classlabels and ignoring sentiment strength per-form similar to using regression on contin-uous confidence scores.1 IntroductionIn the past few years, there has been a growinginterest in using microblogging sites such as Twit-ter.
Generally, people tend to share their opinions,ideas about entities, topics and issues via thesemicroblogs.
Therefore, companies show interestin these for the sentiment analysis to be used asmeans of customer satisfaction evaluation abouttheir products.Although some tweets express direct sentiment,the polarity and intent of some tweets cannot beunderstood even by humans because of lack ofcontext.
Moreover, a tweet may be perceived aspositive or negative by some people whereas oth-ers may think that the tweet is not polar.
There-fore, sometimes it is not easy to assign a senti-ment class to a tweet.
Instead of assigning a sin-gle sentiment to a tweet, confidence scores reflect-ing the likelihoods of sentiments of the tweet maybe provided.
Our dataset consists of tweets andtheir corresponding confidence scores of five sen-timents namely positive, negative, neutral, irrel-evant and unknown.
An analysis on the datasetreflects that, some tweets get similar confidencescores for many classes.
In other words, differ-ent people assign different class labels to the sametweet.
On the other hand, confidence scores ofsome tweets for a class are close to or equal to1, meaning that the sentiment of the tweets areclear.
If we have discrete class labels for all tweets,tweets assigned to classes with a low confidencescore will have equal effect as the ones whose con-fidence scores are high during the training phase ofsentiment analysis.In this study, we investigate whether thestrength of sentiment plays a role in classificationor not.
We build regression models to estimate theconfidence scores of tweets for each class sepa-rately.
Then, we assign the sentiment, whose con-fidence score is maximum among others to thetweet.
On the other hand, we also converted theconfidence scores to discrete class labels and per-formed classification directly.
The experimentsand results are explained in Section 5.2 Related WorkSentiment analysis on Twitter has some challengescompared to the classical sentiment analysis meth-ods on formal documents since the tweets mayhave irregular structure, short length and non-English words.
Moreover, they may include el-ements specific to microblogs such as hashtags,emoticons, etc.
Go et al.
(2009) used emoti-cons as features and Barbosa et al.
(2010) used136retweets, hashtags, emoticons, links, etc.
as fea-tures to classify the sentiments as positive or neg-ative.
Furthermore, Kouloumpis et al.
(2011)showed that the features including presence of in-tensifiers, positive/negative/neutral emoticons andabbreviations are more successful than part-of-speech tags for sentiment analysis on Twitter.
Saifet al.
(2012) extracted sentiment topics fromtweets and then used them to augment the fea-ture space.
Agarwal et al.
(2011) used tree kernelto determine features and they used SVM, Na?
?veBayes, Maximum entropy for classification.
In ourexperiments we used k-Nearest Neighbor (k-NN)and SVM as classifiers.Due to the rarity of class confidence scores ofdatasets in the literature, a few studies employ re-gression.
Jain et al.
(2012) use Support Vector Re-gression (SVR) for sentiment analysis in movie re-views but the labels they use are discrete.
So, theyuse SVR directly for classification purpose, not re-gression.
However, we employed SVR on con-fidence scores with the aim of regression.
More-over, Lu et al.
(2011) use SVR in multi-aspect sen-timent analysis to detect the ratings of each aspect.Since our approach does not include aspects, ourresults are not comparable with that of (Lu et al.,2011).
The study of Liu (2012) consists of studiesemploying regression in sentiment analysis.
Yet,in most of these studies the regressors are trainedusing discrete rating scores between 1 and 5.
Fur-thermore, Pang et al.
(2008) also mentions regres-sion to classify sentiments using discrete ratingscores.
Unlike these approaches, we employ re-gression on real-valued confidence scores between0 and 1.3 Data Description and Pre-processingThe data set we use (Kaggle, 2013) consists of77946 tweets which are obtained with the aim ofsentiment classification.
Each tweet is rated bymultiple raters and as a result, each tweet has con-fidence scores of five classes namely positive, neg-ative, neutral, irrelevant and unknown.
Among77946 tweets, only 800 of them has the maximumconfidence score of unknown class.
Therefore, inorder to have a balanced dataset in our experi-ments, we selected 800 tweets from each class.
Asa result, the dataset used in our experiments is bal-anced and includes a total of 4000 tweets.The data set includes tweets both relevant andirrelevant to weather.
Tweets are expected to gethigh confidence score of irrelevant class if thetweet is not related to weather.
Moreover, as theirname implies, positive and negative confidencevalues represent the polarity level of each tweettowards weather.
If a tweet is not polar, it is ex-pected to be given a high neutral confidence score.Unknown class is expected to have a high scorewhen the tweet is related with weather, but the po-larity of tweet cannot be decided.The tweets in the data set are labeled by mul-tiple raters.
Then, the confidence scores for la-bels are obtained by aggregating labels given totweets by raters and the individual reliability ofeach rater.
For a tweet, confidence scores of allcategories sum to 1 and confidence score valuesare in range [0,1].Before feature extraction, we pre-process thedata in a few steps.
Firstly, we remove links andmentions that are features specific to tweets.
Then,we remove emoticons from the text while record-ing their number for each tweet in order to usethem later.4 FeaturesOur features can be divided into four main cate-gories which are lexical features, emoticons, fea-tures based on sentiment scores and a combinationof the lexical and sentiment features.4.1 Lexical FeaturesWe extracted two different lexical features whichare word n-grams, part-of-speech (POS) n-grams.Using all tweets in our training data, we ex-tracted only unigrams of words to be used as base-line.
Moreover, after extracting POS tags of sen-tences in each tweet using the POS tagger given in(Toutanova et al., 2003), we computed unigramsand bigrams of POS tags.
We considered the pres-ence of word unigrams, POS unigrams and bi-grams.
Therefore, those features can get binaryvalues.4.2 EmoticonsIn the preprocessing step, we remove the emoti-cons from the text.
However, since emoti-cons carry sentiment information, we also recordwhether the tweet includes positive, negative orneutral emoticons (see Table 1) during the re-moval of emoticons.
Therefore, we extract 3 bi-nary features based on emoticon presence in thetweet.137Table 1: Emoticons and their sentimentsSentiment EmoticonPositive :) , :-), =), =D, :DNegative :( , :-(, =(, :/Neutral :|4.3 Features Based on Sentiment ScoresWe extract features based on sentiment scores us-ing two different approaches.
In the first one, weuse SentiWordNet 3.0 (Baccianella et al., 2010) toobtain the sentiment scores of each word.
We usedthe word and a tag representing the POS tag of theword to output the sentiment score of the word.Since the same word with different senses havedifferent scores, we obtained a single sentimentscore by computing the weighted average of Senti-WordNet scores for each sense.
Furthermore, POStagging is performed as explained in 4.1.
How-ever, since POS tags of Penn TreeBank and Senti-WordNet are different, we convert one to other asshown in Table 2.
Therefore, the sentiment scorefor a word is obtained after the Penn TreeBanktags are converted to SentiWordNet tags.
Usingall the words in a tweet and their correspondingSentiWordNet scores, we compute the followingfeatures:?
# of words having positive sentiment?
# of words having negative sentiment?
total sentiment scoreAs a result, using SentiWordNet, we extract 3more features.
We observe that the acronym lolrepresenting laughing out loud is used extensivelyin tweets.
In order to keep its meaning, when a lolis encountered, its sentiment score is assigned to 1.Moreover, sentiment scores of words having otherPOS tags than the ones in Table 2 are assignedto 0.
When not is encountered, we multiply thesentiment score of its successor word by ?1 andconvert the sentiment score of not to 0.Table 2: Conversion of POS tags to SentiWordNettagsSentiWordNet Tag Penn TreeBank taga (adjective) JJ, JJR, JJSn (noun) NN, NNS, NNP, NNPSv (verb) VB,VBD, VBG, VBN, VBP, VPZr (adverb) RB, RBR, RBSThe second approach is using LabMT word list(Dodds et al., 2011) which includes scores for sen-timent analysis.
It includes a list of words withtheir happiness rank, happiness average and hap-piness standard deviation.
In our study, we com-puted those values for all the words in a tweet andextracted the 6 features namely the minima and themaxima of happiness rank, happiness average andhappiness standard deviation.Note that, if a word is not encountered in eitherSentiWordNet or labMT dictionary, then the senti-ment score of that word is assigned to 0.4.4 Combination of Lexical and SentimentFeaturesWe extract features using POS tags and sentimentscores.
After the conversion of POS tags in Table2, we have four main tags namely, a (adjective),n (noun), v (verb), r (adverb).
For each tweet wecompute the number of adjectives, nouns, verbsand adverbs having positive, negative and neutralsentiments.
Therefore, we extract 12 features us-ing combination of lexical and sentiment features.Table 3 shows all the features used.5 ExperimentsIn our experiments we extract the features usingtraining data set.
Then, we formed training andtest feature matrices using these features.
By usingthese matrices, we both conduct classification andregression.We train separate regressors for each class usingthe training feature matrix and confidence scoresof the corresponding class.
We use Support VectorRegression (SVR) library of (Chang et al., 2011)in our computations.
Recall that, the confidencescores are between 0 and 1 and they carry informa-tion about how likely it is that a tweet belongs toa specified class.
For instance it is very likely thata positive with a 0.9 confidence score is actually apositive, whereas a positive with a 0.2 confidencescore is much less likely to be positive.
In order toassign a sentiment label to a test tweet, we sepa-rately test that tweet with the regressors trained foreach class.
Then, each regressor assigns a scorebetween 0 and 1 to that test tweet.
Finally, we as-sign the class label with maximum score to the testtweet.During classification, we convert confidencescores to discrete class labels by assigning themthe class which the majority of the raters agreedupon.
Using training feature matrix and their cor-responding discrete labels, we train a Support Vec-138Table 3: Features used in our experimentsLexicalword unigram f1POS unigram + bigram f2Emoticons # of pos, neg, neu emoticons f3Sentiment ScoresSentiWordNet (# of pos, neg words, total sentiment score) f4labMT ( min, max of happiness rank, avg and std) f5Sentiment+ Lexical# of pos a, pos n, pos v, pos r# of neg a, neg n, neg v, neg r# of neu a, neu n, neu v, neu rf6tor Machine (SVM) using the method of (Chang etal., 2011) and a k-Nearest Neighbor (k-NN) clas-sifier.
SVM and k-NN directly assigns class labelsto test tweets.We employed classification and regression onthree types of data having classes:?
positive - negative - neutral - irrelevant - un-known?
positive - negative - neutral?
positive - negative5.1 Positive vs.
Negative vs.
Neutral vs.Irrelevant vs. UnknownIn 5-class classification, our dataset consists of4000 tweets (800 for each class).
We used 3000 ofthem as training data (600 for each class) and 1000of them as test data (200 for each class).
Sinceour dataset is balanced, chance accuracy is 20% ifwe assing all the tweets to one class.
Using var-ious features to train k-NN, SVM and SVR, weobtained the results in Table 4.Table 4: k-NN, SVM and SVR Performances for5-class classificationFeatures k-NN SVM SVRUnigram (f1) 0,3140 0,4430 0,4290+f20,3130 0,4330 0,4300+f30,3350 0,4410 0,4350+f50,3280 0,4460 0,4340+f60,3490 0,4500 0,4260+f3, f40,3450 0,4570 0,4370+f3, f50,3300 0,4430 0,4340+f4, f50,3550 0,4490 0,4350+f4, f60,3490 0,4550 0,4260+f3, f4, f50,3530 0,4490 0,4430+f2, f3, f4, f50,3500 0,4350 0,4420+f2, f3, f4, f5, f60,3430 0,4250 0,4350Results in Table 4 show that, classification withSVM performs the best when emoticon features(f3) and SentiWordNet features (f4) are com-bined with unigram baseline.
Moreover, usingemoticon features (f3), and sentiment score fea-tures (both SentiWordNet (f4) and labMT (f5))together with the word unigram baseline performthe best among others when SVR is used.
No-tice that using regression performs slightly worsethan using SVM for most of the feature combina-tions.
However, the p-value of SVM vs. SVR is0.06, meaning that the performance improvementof SVM is insignificant.
On the other hand, usingSVR always performs much better than k-NN witha p-value of 2?
10?10.5.2 Positive vs.
Negative vs. NeutralIn 3-class classification, our dataset consists of2400 tweets (800 for each class).
We use 1800 ofthem as training data (600 for each class) and 600of them as test data (200 for each class).
Since ourdataset is balanced, chance accuracy is 33%.
Us-ing various features to train k-NN, SVM and SVR,we obtain the results in Table 5.Table 5: k-NN, SVM and SVR Performances for3-class classificationFeatures k-NN SVM SVRUnigram (f1) 0,5183 0,6650 0,6467+f20,5017 0,6267 0,6450+f30,5333 0,6767 0,6567+f50,5467 0,6617 0,6533+f60,5450 0,6767 0,6700+f3, f40,5550 0,6717 0,6583+f3, f50,5517 0,6700 0,6667+f4, f50,5533 0,6733 0,6567+f4, f60,5233 0,6750 0,6700+f3, f4, f50,5700 0,6700 0,6550+f2, f3, f4, f50,5367 0,6583 0,6567+f2, f3, f4, f5, f60,5450 0,6500 0,6550139Table 5 reflects that, using the combination ofsentiment and lexical features (f6) play an impor-tant role in positive - negative - neutral classifica-tion using SVR.
On the other hand, using emoti-con features (f3) with unigram baseline or labMTfeatures (f5) with unigram baseline performs thebest when SVM is used.
It can be seen that SVMperforms slightly better than SVR most of the timeyet the performance improvemen is again insignif-icant with a p-value of 0.58.
Furthermore, theyalways perform much better than k-NN with a p-value of 2?
10?8.5.3 Positive vs. NegativeIn 2-class classification, since we have 800 posi-tive and 800 negative tweets among 4000 tweets,we used 1600 tweets.
We used 1200 of them astraining data (600 for each class) and 400 of themas test data (200 for each class).
Since our datasetis balanced, chance accuracy is 50%.
Using thesame set of features to train k-NN, SVM and SVR,we obtained the results in Table 6.Table 6: k-NN, SVM and SVR Performances for2-class classificationFeatures k-NN SVM SVRUnigram (f1) 0,6275 0,7700 0,7775+f20,6575 0,7575 0,7375+f30,7225 0,7850 0,7775+f50,6900 0,7575 0,7700+f60,6950 0,7975 0,7975+f3, f40,6900 0,7825 0,7850+f3, f50,7125 0,7800 0,7700+f4, f50,6950 0,7800 0,7800+f4, f60,6725 0,7950 0,7975+f3, f4, f50,7000 0,7725 0,7800+f2, f3, f4, f50,6675 0,7700 0,7800+f2, f3, f4, f5, f60,6675 0,7825 0,7750In positive - negative classification, using com-bination of sentiment and lexical features (f6)with unigram baseline results in the highest per-formance among all when either SVM or SVRis used.
Similar to previous classification results,performance improvement of using SVM on dis-crete labels instead of using SVR is insignificantwith a p-value of 0.46 whereas SVR provides asignificant performance improvement over k-NNwith a p-value of 5?
10?4.6 ConclusionIn this study we conducted sentiment analysis ontweets about weather.
We performed two types ofexperiments, one using confidence scores directlyby regression and the other one by discreticis-ing this information and using discrete classifiers.We expected that employing regression on confi-dence scores would better discriminate the senti-ment classes of tweets than the classification ondiscrete labels since they consider the sentimentstrength.First, we extracted various types of featuresincluding lexical features, emoticons, sentimentscores and combination of lexical and sentimentfeatures.
Then, we created the feature vectorsfor these tweets.
We trained a regressor for eachclass separately using continuous valued confi-dence scores.
Then, a test tweet is assigned tothe label, whose estimated confidence score is thehighest among others.
In our second experiment,we assigned class labels having the maximum con-fidence score to the tweets in the training set di-rectly.
Using the training data and discrete valuedclass labels, we trained a classifier.
Then, a testtweet is assigned to a class label by the classifier.Our results indicate that using classification ondiscrete valued class labels performs slightly bet-ter than using regression, which considers confi-dence scores during training.
However, the per-formance improvement is shown to be insignifi-cant.
We would expect a significant performanceimprovement using SVR compared to SVM as inthe case of k-NN vs. SVR.
However, we exploredthat the effect of strength of sentiment is insignifi-cant.As future work, we will employ our methods ondatasets including continuous scores rather thandiscrete class labels such as movie reviews includ-ing ratings.
Moreover, we may enhance our ap-proach on multi-aspect sentiment analysis prob-lems where each aspect is given ratings.ReferencesAlec Go, Richa Bhayani and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Stanford Digital Library Technologies Project, NJ.Luciano Barbosa and Junlan Feng 2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
Proceedings of COLING, Beijing China, 36?44.140Efthymios Kouloumpis, Theresa Wilson and JohannaMoore 2011.
Twitter Sentiment Analysis: TheGood the Bad and the OMG!
Proceedings of theICWSM, Barcelona, Spain.Hassan Saif, Yulan He, and Harith Alani 2012.
Alle-viating data sparsity for twitter.
2nd Workshop onMaking Sense of Microposts, Lyon, France.Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow and Rebecca Passonneau 2011.
Sentimentanalysis of twitter data.
Proceedings of the Work-shop on Languages in Social Media, Portland, Ore-gon, USA, 30?38Siddharth Jain and Sushobhan Nayak 2012.
SentimentAnalysis of Movie Reviews: A Study of Featuresand Classifiers.
CS221 Course Project: ArtificialIntelligence , Stanford (Fall 2012) [Report].Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou2011.
Multi-aspect sentiment analysis with topicmodels.
The ICDM2011 Workshop on SentimentElicitation from Natural Text for Information Re-trieval and Extraction, Vancouver, Canada.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
Proceedings of HLT-NAACL 2003, Edmon-ton, Canada, 252?259.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani 2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, MaltaPeter S. Dodds, Kameron D.Harris, Isabel M.Kloumann, Catherine A. Bliss and Christopher M.Danforth 2011.
Temporal Patterns of Happinessand Information in a Global Social Network: He-donometrics and Twitter PLoS ONE 6(12): e26752Chih-Chung Chang and Chih-Jen Lin 2011.
LIB-SVM: A Library for Support Vector Machines ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27Kaggle ?Partly Sunny with a Chance ofHashtags?
competition dataset 2013http://www.kaggle.com/c/crowdflower-weather-twitterQuinn McNemar 1947 Note on the sampling errorof the difference between correlated proportions orpercentages Psychometrika 12(2):153-157Bo Pang and Lillian Lee 2008 Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval 2(1-2): p. 1?135.Bing Liu 2012 Sentiment Analysis and Opinion Min-ing Morgan & Claypool Publishers141
