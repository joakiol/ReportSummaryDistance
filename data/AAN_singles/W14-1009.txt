Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57,Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational LinguisticsAutomatic Building and Using Parallel Resources for SMT fromComparable CorporaSantanu Pal1, Partha Pakray2, Sudip Kumar Naskar31Universit?t Des Saarlandes, Saarbr?cken, Germany2Computer & Information Science,Norwegian University of Science and Technology, Trondheim, Norway3Department of Computer Science & Engineering,Jadavpur University, Kolkata, India1santanu.pal@uni-saarland.de,2partha.pakray@idi.ntnu.no,3sudip.naskar@cse.jdvu.ac.inAbstractBuilding parallel resources for corpusbased machine translation, especiallyStatistical Machine Translation (SMT),from comparable corpora has recentlyreceived wide attention in the fieldMachine Translation research.
In thispaper, we propose an automatic approachfor extraction of parallel fragments fromcomparable corpora.
The comparablecorpora are collected from Wikipediadocuments and this approach exploits themultilingualism of Wikipedia.
Theautomatic alignment process of paralleltext fragments uses a textual entailmenttechnique and Phrase Based SMT (PB-SMT) system.
The parallel textfragments extracted thus are used asadditional parallel translation examplesto complement the training data for a PB-SMT system.
The additional training dataextracted from comparable corporaprovided significant improvements interms of translation quality over thebaseline as measured by BLEU.1 IntroductionComparable corpora have recently attracted hugeinterest in natural language processing research.Comparable corpora are now considered as a richresource for acquiring parallel resources such asparallel corpus or parallel text fragments,.Parallel text extracted from comparable corporacan take an important role in improving thequality of machine translation (MT) (Smith et al.2010).
Parallel text extracted from comparablecorpora are typically added with the trainingcorpus as additional training material which isexpected to facilitate better performance of SMTsystems specifically for low density languagepairs.In the present work, we try to extractEnglish?Bengali parallel fragments of text fromcomparable corpora.
We have collecteddocument aligned corpus of English?Bengalidocument pairs from Wikipedia which provides ahuge collection of documents in many differentlanguages.
For automatic alignment of parallelfragments we have used two-way textualentailment (TE) system and a baseline SMTsystem.Textual entailment (TE), introduced by(Dagan and Glickman, 2004), is defined as adirectional relationship between pairs of textexpressions, denoted by the entailing text (T) andthe entailed hypothesis (H).
T entails H if themeaning of H can be inferred from the meaningof T. Textual Entailment has many applicationsin NLP tasks, such as summarization,information extraction, question answering,48information retrieval, machine translation, etc.
Inmachine translation, textual entailment can beapplied to MT evaluation (Pado et al., 2009).
Anumber of research works have been carried outon cross-lingual Textual entailment using MT(Mehdad et al.,2010; Negri et al., 2010; Neogi etal., 2012).
However, to the best of ourknowledge, the work presented here is the firstattempt towards employing textual entailment forthe purpose of extracting parallel text fragmentsfrom comparable corpora which in turn are usedto improve MT system.Munteanu and Marcu (2006) suggested thatcomparable corpora tend to have parallel data atsub-sentential level.
Hence, instead of findingsentence level parallel resource from comparablecorpora, in the present work we mainly focus onfinding parallel fragments of text.We carried out the task of automatic alignmentof parallel fragments using three steps: (i) miningcomparable corpora form Wikipedia, (ii)sentence level alignment using two-way TE anda baseline Bengali?English SMT system, andfinally (iii) clustering the parallel sentencealigned comparable corpora using textualentailment and then aligning parallel fragmentsof text by textual entailment and a baselineBengali?English SMT system.Although, we have collected documentaligned comparable corpora, the documents inthe corpus do not belong to any particulardomain.
Even with such a corpus we have beenable to improve the performance of an existingmachine translation system which was built ontourism domain data.
This also signifies thecontribution of this work towards domainadaptation of MT systems.The rest of the paper is organized as follows.Section 2 describes the related work.
Section 3describes the mining process of the comparablecorpora.
The two-way TE system architecture isdescribed in section 4.
Section 5 describes theautomatic alignment technique of parallelfragment of texts.
Section 6 describes the toolsand resources used for this work.
Theexperiments and evaluation results are presentedin section 7.
Section 8 concludes and presentsavenues for future work.2 Related WorkComparable corpora have been used in manyresearch areas in NLP, especially in machinetranslation.
Several earlier works have studiedthe use of comparable corpora in machinetranslation.
However, most of these approaches(Fung and McKeown, 1997; Fung and Yee, 1998;Rapp, 1999; Chiao and Zweigenbaum, 2002;Dejean et al., 2002; Kaji, 2005; Otero, 2007;Saralegui et al., 2008; Gupta et al., 2013) arespecifically focused on extracting wordtranslations from comparable corpora.
Most ofthe strategies follow a standard method based onthe context vector similarity measure such asfinding the target words that have the mostsimilar distributions with a given source word.
Inmost of the cases, a starting list contains the?seed expressions?
and this list is required tobuild the context vectors of the words in both thelanguages.
A bilingual dictionary can be used asa starting list.
The bilingual list can also beprepared form parallel corpus using bilingualcorrelation method (Otero, 2007).
Instead of abilingual list, multilingual thesaurus could alsobe used for this purpose (Dejean, 2002).Wikipedia is a multilingual encyclopediaavailable in different languages and it can beused as a source of comparable corpora.
Otero etal.
(2010) stored the entire Wikipedia for anytwo languages and transformed it into a newcollection: CorpusPedia.
Our work shows thatonly a small ad-hoc corpus containing Wikipediaarticles could prove to be beneficial for existingMT systems.In the NIST shared task on RecognizingTextual Entailment Challenge (RTE), severalmethods have been proposed to tackle the textualentailment problem.
Most of these systems usesome form of lexical matching, e.g., n-gram,word similarity, etc.
and even simple wordoverlap.
A number of systems represent the textsas parse trees (e.g., syntactic or dependency trees)49before the actual task.
Some of the systems usesemantic features (e.g., logical inference,Semantic Role Labelling) for solving the text andhypothesis entailment problem.
MacCartney et al.
(2006) proposed a new architecture for textualinference in which finding a good alignment isseparated from evaluating entailment.
Agichteinet al.
(2008) presented a supervised machinelearning approach to train a classifier over avariety of lexical, syntactic, and semantic metrics.Malakasiotis (2009) used string similaritymeasures applied to shallow abstractions of theinput sentences and a Maximum Entropyclassifier to learn how to combine the resultingfeatures.In the present work, we used the textualentailment system of Pakray et al.
(2011) whichperformed well on various RTE tasks anddatasets, as well as other NLP tasks like questionanswering, summarization, etc.
We integrated anew module to by using reVerb 1  tool andoptimized all the features produced by differentmodules.The main objective of the present work is toinvestigate whether textual entailment can beused to establish alignments between textfragments in comparable corpora and whetherthe parallel text fragments extracted thus canimprove MT system performance.3 Mining Comparable CorporaWe collected comparable corpora fromWikipedia - online collaborative encyclopediaavailable in a wide variety of languages.
EnglishWikipedia contains largest volume of data suchas millions of articles; there are many languageeditions with at least 100,000 articles.
Wikipedialinks articles on the same topic in differentlanguages using ?interwiki?
linking facility.Wikipedia is an enormously useful re-source forextracting parallel resources as the documents indifferent languages are already aligned.
We firstcollect an English document from Wikipedia andthen find the same document in Bengali if there1 http://reverb.cs.washington.edu/exists any inter-language link.
ExtractedEnglish?Bengali document pairs from Wikipediaare already comparable since they are writtenabout the same entity.
Although eachEnglish?Bengali document pairs are comparableand they discuss about the same topic, most ofthe times they are not exact translation of eachother; as a result parallel fragments of text arerarely found in these document pairs.
The biggerthe size of the fragment may result less probableparallel version will be found in the target side.Nevertheless, there is always chance of gettingparallel phrase, tokens or even sentences incomparable documents.We designed a crawler to collect comparablecorpora for English?Bengali document pairs.Based on an initial seed keyword list, the crawlerfirst visits each English page of Wikipedia, savesthe raw text (in HTML format), and then followsthe cross-lingual link for each English page andcollects the corresponding Bengali document.
Inthis way, we collect English?Bengali comparabledocuments in the tourism domain.
We retain onlythe textual information and all the other detailsare discarded.
We extract English and Bengalisentences from each document.
The extractedsentences from each English document are notparallel with the corresponding Bengalidocument.
Moreover, Bengali documents arecontained limited information compare to theEnglish document.
We align sentences ofEnglish?Bengali from these comparable corporathrough a baseline PB-SMT system.
A Bengali-English baseline PB-SMT system has beendeveloped which was trained onEnglish?Bengali tourism domain corpus.
Wetranslated Bengali sentences into English.
Thetranslated sentence is then examined forentailment in the English comparable documentby using two-way TE system proposed in section4.
If it is more than 50% entailed with the targetdocument then the target sentence is directlyfetched form the comparable English documentand the source-target sentence pair are saved in alist.
In this way, we extract parallel sentencesfrom comparable corpora.
These parallelsentences except those are 100% entailed may50not be completely parallel but they arecomparable.
So, we created a parallel fragmentlist which is proposed in section 5.4 Two-way Textual Entailment SystemA two-way automatic textual entailment (TE)recognition system that uses lexical, syntacticand semantic features has been described in thissection.
The system architecture has been shownin Figure 1.
The TE system has used the SupportVector Machine (SVM) technique that usesthirty-one features for training purpose.
In lexicalmodule there are eighteen features and elevenfeatures from syntactic module, one feature byusing reVerb and one feature from semanticmodule.Fig.1 Two way TE architecture4.1 Lexical ModuleIn this module six lexical comparisons andseventeen lexical distance comparisons betweentext and hypothesis has used.Six lexical comparisons are WordNet(Fellbaum, 1998) based unigram match, bigrammatch, longest common sub-sequence, skip-gram,stemming and named entity matching.
We havecalculated weight from each of these sixcomparisons in equation (1).weight =number - of - common - tokens - between - text - and - hypothesis?number - of - tokens - in - hypothesis?
(1)The API for WordNet Searching (JAWS) 2provides Java applications with the ability toretrieve data from the WordNet 2.1 database.For Named entity detection we have used TextTokenization Toolkit (LT-TTT2)3 (Grover et.
al.,1999).
The LT-TTT2 named entity componenthas been used.For lexical distance measure, we have usedfeatures of Vector Space Measures (Euclideandistance, Block distance, Minkowsky distance,Cosine similarity, Matching Coefficient), Set-based Similarities (Dice, Jaccard, Overlap,Harmonic), Edit Distance Measures (Levenshteindistance, Smith-Waterman distance, JaroDistance).
Lexical distance measurement hasused the libraries SimMetrics 4 , SimPack 5  andSecondString6.
SimMetrics is a Similarity MetricLibrary, e.g., from edit distance (Levenshtein,Gotoh, Jaro etc) to other metrics, (e.g Soundex,Chapman).4.2 Syntactic ModuleThe syntactic module compares the dependencyrelations in both hypothesis and text.
The systemextracts syntactic structures from the text-hypothesis pairs using Combinatory CategorialGrammar (C&C CCG) Parser 7  and StanfordParser 8  and compares the correspondingstructures to determine if the entailment relationis established.
Two different systems have beenimplemented one system used Stanford Parseroutput and another system used C&C CCGParser.
The system accepts pairs of text snippets(text and hypothesis) at the input and gives scorefor each comparison.
Some of the importantcomparisons on the dependency structures of thetext and the hypothesis are Subject-subjectcomparison, WordNet Based Subject-Verb2 http://lyle.smu.edu/~tspell/jaws/index.html3 http://www.ltg.ed.ac.uk/software/lt-ttt24 http://sourceforge.net/projects/simmetrics/5https://files.ifi.uzh.ch/ddis/oldweb/ddis/research/simpack/index.html6 http://sourceforge.net/projects/secondstring/7 http://svn.ask.it.usyd.edu.au/trac/candc/wiki8 http://nlp.stanford.edu/software/lex-parser.shtml51Comparison, Subject-Subject Comparison,Object-Verb Comparison, WordNet BasedObject-Verb Comparison, Cross Subject-ObjectComparison Number Comparison, NounComparison, Prepositional Phrase Comparison,Determiner Comparison and other relationComparison.4.3 reVerb ModuleReVerb 9  is a tool, which extracts binaryrelationships from English sentences.
Theextraction format is in Table 1.Extraction Format arg1 rel arg2Example A person is playing a guitarreVerb Extracts arg1= {A person}  rel = {isplaying} arg2 = {a guitar}Table 1: Example by reVerb ToolThe system parsed the text and the hypothesisby reverb tool.
Each of the relations comparesbetween text and hypothesis and calculates ascore for each pair.4.4 Semantic ModuleThe semantic module based on the UniversalNetworking Language (UNL) (Uchida and Zhu,2001).
The UNL can express information orknowledge in semantic network form with hyper-nodes.
The UNL is like a natural language forcomputers to represent and process humanknowledge.
There are two modules in UNLsystem - En-converter and De-converter module.The process of representing natural languagesentences in UNL graphs is called En-convertingand the process of generating natural languagesentences out of UNL graphs is called De-converting.
An En-Converter is a languageindependent parser, which provides a frameworkfor morphological, syntactic, and semanticanalysis synchronously.
The En-Converter isbased on a word dictionary and a set ofenconversion grammar rules.
It analysessentences according to the en-conversion rules.A De-Converter is a language independent9 http://reverb.cs.washington.edu/generator, which provides a framework forsyntactic and morphological generationsynchronously.An example UNL relation for a sentence?Pfizer is accused of murdering 11 children?
isshown in Table 2.
[S:00]{org:en} Pfizer is accused of murdering 11 children{/org}{unl}obj(accuse(icl>do,equ>charge,cob>abstract_thing,agt>person,obj>person).@entry.@present,pfizer.@topic)qua:01(child(icl>juvenile>thing).@pl,11)obj:01(murder(icl>kill>do,agt>thing,obj>living_thing).@entry,child(icl>juvenile>thing).@pl)cob(accuse(icl>do,equ>charge,cob>abstract_thing,agt>person,obj>person).@entry.
@present,:01){/unl}[/S]Table 2: Example of UNLThe system converts the text and thehypothesis into UNL relations by En-Converter.Then it compares the UNL relations in both thetext and the hypothesis and gives a score for eachcomparison.4.5 Feature Extraction ModuleThe features are listed in Table 3:Name of Features No of featuresLexical Module 18Syntactic Module 11reVerb Module 1Semantic Module 1Table 3: Features for SVM4.6 Support Vector Machines (SVM)Support Vector Machines (SVMs) 10  aresupervised learning models used forclassification and regression analysis.
The basicSVM takes a set of input data and predicts, for10 http://en.wikipedia.org/wiki/Support_vector_machine52each given input, which of two possible classesform the output, making it a non-probabilisticbinary linear classifier.The SVM based our Textual Entailmentsystem has used the following data sets: RTE-1development and RTE-1 annotated test set, RTE-2 development set and RTE-2 annotated test set,RTE-3 development set and RTE-3 annotatedtest set to deal with the two-way classificationtask.
The system has used the LIBSVM -- ALibrary for Support Vector Machines 11  for theclassifier to learn from this data set.5 Alignment of Parallel fragments usingproposed TE systemWe have extracted parallel fragment from theparallel sentence aligned comparable resourcelist as well as the training data.
Initially, wemake cluster on the English side of this list withthe help of two-way TE method.
More than 50%entailed sentences have been considered to take apart of the same cluster.
The TE system dividesthe complete set of comparable resources list intosome smaller sets of cluster.
Each clustercontains at least two English sentences.
EachEnglish cluster is corresponding to the setcomparable Bengali sentences.
So in this way wehave developed a number of English Bengaliparallel clusters.
We intersect between the bothEnglish and Bengali sentences which arebelonging to the same clusters.We try to align the English and Bengalifragments extracted from a parallel sentencealigned comparable resource list.
If both sidescontain only one fragment then the alignment istrivial, and we add such fragment pairs to seedanother parallel fragment corpus that containsexamples having only one token in both side.Otherwise, we establish alignments between theEnglish and Bengali fragments using translation.If both the English and Bengali side contains nnumber of fragments, and the alignments of n-1fragments can be established through translation11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/or by means of already existing alignments, thenthe nth alignment is trivial.These parallel fragments of text, extractedfrom the comparable corpora are added with thetourism domain training corpus to enhance theperformance of the baseline PB-SMT system.6 Tools and ResourcesA sentence-aligned English?Bengali parallelcorpus contains 23,492 parallel sentences fromthe travel and tourism domain has been used inthe present work.
The corpus has been collectedfrom the consortium-mode project ?Developmentof English to Indian Languages MachineTranslation (EILMT) System 12 ?.
The StanfordParser 13  and CRF chunker 14  (Xuan-Hieu Phan,2006) have been used for parsing and chunkingin the source side of the parallel corpus,respectively.The experiments were carried out using thestandard log-linear PB-SMT model as ourbaseline system: GIZA++ implementation ofIBM word alignment model 4, phrase-extractionheuristics described in (Koehn et al., 2003),minimum-error-rate training (Och, 2003) on aheld-out development set, target language modeltrained using SRILM toolkit (Stolcke, 2002) withKneser-Ney smoothing (Kneser and Ney, 1995)and the Moses decoder (Koehn et al., 2007) havebeen used in the present study.7 Experiments and ResultsWe randomly identified 500 sentences each forthe development set and the test set from theinitial parallel corpus.
The rest is considered asthe training corpus.
The training corpus wasfiltered with the maximum allowable sentencelength of 100 words and sentence length ratio of1:2 (either way).
Finally the training corpus12 The EILMT project is funded by the Department ofElectronics and Information Technology (DEITY), Ministryof Communications and Information Technology (MCIT),Government of India.13 http://nlp.stanford.edu/software/lex-parser.shtml14 http://crfchunker.sourceforge.net/53contained 22,492 sentences.
In addition to thetarget side of the parallel corpus, we used amonolingual Bengali corpus containing 488,026words from the tourism domain for building thetarget language model.
Experiments were carriedout with different n-gram settings for thelanguage model and the maximum phrase lengthand it was found that a 4-gram language modeland a maximum phrase length of 7 produce theoptimum baseline result on both the developmentand the test set.
We carried out the rest of theexperiments using these settings.The collected comparable corpus consisted of5582 English?Bengali document pairs.
It isevident from Table 4 that English documents aremore informative than the Bengali documents asthe number of sentences in English documents ismuch higher than those in the Bengali documents.When the Bengali fragments of texts were passedto the Bengali?English translation module someof them could not be translated into English andalso, some of them could be translated onlypartially.
Therefore, some of the tokens weretranslated while some were not.
Some of thosepartially translated text fragments were alignedthrough textual entailment; however, most ofthem were discarded.
As can be seen from Table4, 9,117 sentences were entailed in the Englishside, of which the system was able to establishcross-lingual entailment for 2,361English?Bengali sentence pairs.No.
ofEnglishsentenceNo.
ofBengalisentenceExtraction fromComparable corpora579037 169978more than 50% EntailedEnglish Sentences9117 -more than 50% Entailed(sentence alignedcomparable)2361 2361parallel fragment of textsfrom sentence alignedcomparable list3937 3937Table 4: Statistics of the sentence aligned comparablelist and the aligned parallel text fragments.Finally, the textual entailment based alignmentprocedure was able to align 3937 parallelfragments as reported in Table 4.
Manualinspection of the parallel list revealed that mostof the aligned texts were of good quality.We carried out evaluation of the MT qualityusing four automatic MT evaluation metrics:BLEU (Papineni et al., 2002), METEOR(Banerjee and Lavie, 2005), NIST (Doddington,2002) and TER (Snover et al., 2006).
Table 5shows the performance of the PB-SMT systemsbuilt on the initial training corpus and the largertraining corpus containing parallel text fragmentsextracted from the comparable corpora.
Treatingthe parallel text fragments extracted from thecomparable corpora as additional trainingmaterial results in significant improvement interms of BLEU (1.73 points, 15.84% relative)over the baseline system.
Similar improvementsare also obtained for the other metrics.
The lowevaluation scores could be attributed to the factthat Bengali is a morphologically rich languageand has a relatively free phrase order; besidesthere were only one set of reference translationsfor the testset.Experiments BLEU NIST METEOR TERBaseline 10.92 4.16 0.3073 75.34Baseline  +parallelfragments oftexts asadditionaltrainingmaterial12.65 4.32 0.3144 73.00Table 5: Evaluation results8 Conclusion and Future WorkIn this paper, we have successfully extractedEnglish?Bengali parallel fragments of text fromcomparable corpora using textual entailmenttechniques.
The parallel text fragments extractedthus were able to bring significant improvementsin the performance of an existing machinetranslation system.
For low density languagepairs, this approach can help to improve thestate-of-art machine translation quality.
Amanual inspection on a subset of the outputrevealed that the additional training material54extracted from comparable corpora effectivelyresulted in better lexical choice and less OOVwords than the baseline output.
As the collectedparallel text does not belong to any particulardomain, this work also signifies that out ofdomain data is also useful to enhance theperformance of a domain specific MT system.This aspect of the work would be useful fordomain adaptation in MT.
As future work, wewould like to carry out experiments on largerdatasets.AcknowledgmentsThe research leading to these results has receivedfunding from the EU project EXPERT ?thePeople Programme (Marie Curie Actions) of theEuropean Union's Seventh FrameworkProgramme FP7/2007-2013<tel:2007-2013>/under REA grant agreement no.
[317471].
Weacknowledge the support from Department ofComputer and Information Science, NorwegianUniversity of Science and Technology and alsosupport from ABCDE fellowship programme2012-1013.ReferencesBanerjee, Satanjeev and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation withImproved Correlation with Human Judgments.Proceedings of the ACL Workshop on Intrinsic andExtrinsic Evaluation Measures for MachineTranslation and/or Summarization, Ann Arbor,Michigan, pages 65?72.Chiao, Yun-Chuang and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedings ofthe 19th international conference on Computationallinguistics, Volume 2, Association forComputational Linguistics, pages 1-5.Dagan, Ido and Oren Glickman.
2004.
Probabilistictextual entailment: generic applied modeling oflanguage variability, In PASCAL Workshop onLearning Methods for Text Understanding andMining, Grenoble, France.De Marneffe, Marie-Catherine, Bill MacCartney,Trond Grenager, Daniel Cer, Anna Rafferty, andChristopher D. Manning.
2006.
Learning todistinguish valid textual entailments.
In B. Magniniand I. Dagan (eds.
), Proceedings of the SecondPASCAL Recognizing Textual EntailmentChallenge.
Venice: Springer, pages 74?79.D?jean, Herv?, ?ric Gaussier, and Fatia Sadat.
2002.Bilingual terminology extraction: an approachbased on a multilingual thesaurus applicable tocomparable corpora.
In Proceedings of the 19thInternational Conference on ComputationalLinguistics COLING, Pages 218-224.Doddington, George.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human LanguageTechnology Research .
Morgan KaufmannPublishers Inc, pages.
138-145.Fung, Pascale and Kathleen McKeown.
1997.
Findingterminology translations from non-parallel corpora.In Proceedings of the 5th Annual Workshop onVery Large Corpora, pages 192-202.Fung, Pascale and Lo Yuen Yee.
1998.
An IRapproach for translating new words fromnonparallel, comparable texts.
In Proceedings ofthe 17th international conference on Computationallinguistics-Volume 1, Association forComputational Linguistics, pages 414-420.Gupta, Rajdeep, Santanu Pal, and SivajiBandyopadhyay.
2013.
Improving MT SystemUsing Extracted Parallel Fragments of Text fromComparable Corpora.
In proceedings of 6thworkshop of Building and Using ComparableCorpora (BUCC), ACL, Sofia, Bulgaria, Pages 69-76.Kneser, Reinhard and Hermann Ney.
1995.
Improvedbacking-off for n-gram language modeling.
InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing,volume I. pages 181-184.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico,Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond rej Bojar,Alexandra Constantin, and Evan Herbst.
Moses:open source toolkit for statistical machinetranslation.
In Proceedings of the 45th AnnualMeeting of the ACL on Interactive Poster andDemonstration Sessions.
Association forComputational Linguistics, pages 177-180.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In55Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association forComputational Linguistics on Human LanguageTechnology-Volume 1, Association forComputational Linguistics, pages 48-54.Mehdad, Yashar, Matteo Negri, and MarcelloFederico.
2010.
Towards Cross-Lingual Textualentailment.
In Proceedings of the 11th AnnualConference of the North American Chapter of theAssociation for Computational Linguistics,NAACL-HLT 2010.
LA, USA.Munteanu,  Dragos Stefan and Daniel Marcu.
2006.Extracting parallel sub-sentential fragments fromnon-parallel corpora.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and the 44th annual meeting of theAssociation for Computational Linguistics,Association for Computational Linguistics, pages81-88.Negri, Matteo, and Yashar Mehdad.
2010.
Creating aBilingual Entailment Corpus through Translationswith Mechanical Turk: $100 for a 10-day Rush.
InProceedings of the NAACL-HLT 2010, CreatingSpeech and Text Language Data With Amazon'sMechanical Turk Workshop.
LA, USA.Neogi, Snehasis, Partha Pakray, SivajiBandyopadhyay, and Alexander Gelbukh.
2012.JU_CSE_NLP: Language Independent Cross-lingual Textual Entailment System.
(*SEM) FirstJoint Conference on Lexical and ComputationalSemantics, Collocated with NAACL-HLT 2012,Montreal, Canada.Och, F. Josef.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association forComputational Linguistics-Volume 1, Associationfor Computational Linguistics, pages 160-167.Och, F. Josef and Herman Ney.
2000.
Giza++:Training of statistical translation models.Otero, P. Gamallo.
2007.
Learning bilingual lexiconsfrom comparable english and spanish corpora.Proceedings of MT Summit xI, pages 191-198.Otero, P. Gamallo and Isaac Gonz?lez L?pez.
2010.Wikipedia as multilingual source of comparablecorpora.
In Proceedings of the 3rd Workshop onBuilding and Using Comparable Corpora, LREC,pages 21-25.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method forautomatic evaluation of machine translation.
InProceedings of the 40th annual meeting onassociation for computational linguistics,Association for Computational Linguistics, pages311-318.Prodromos Malakasiotis.
2009.
"AUEB at TAC 2009",In TAC 2009 Workshop, National Institute ofStandards and Technology Gaithersburg, MarylandUSA.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated English andGerman corpora.
In Proceedings of the 37th annualmeeting of the Association for ComputationalLinguistics on Computational Linguistics,Association for Computational Linguistics, pages519-526.Saralegui, X., San Vicente, I., and Gurrutxaga, A.2008.
Automatic generation of bilingual lexiconsfrom comparable corpora in a popular sciencedomain.
In LREC 2008 workshop on building andusing comparable corpora.Pado, Sebastian, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Textual entailmentfeatures for machine translation evaluation.
InProceedings of the EACL Workshop on StatisticalMachine Translation, Athens, Greece, pages 37?41.Smith, R. Jason, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences fromcomparable corpora using document levelalignment.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics, Association for ComputationalLinguistics, pages 403-411.Snover, Matthew, Bonnie Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
Astudy of translation edit rate with targeted humanannotation.
Proceedings of Association forMachine Translation in the Americas, Cambridge,Massachusetts, USA, pages 223?231.Pakray, Partha, Snehasis Neogi, Pinaki Bhaskar,Soujanya Poria, Sivaji Bandyopadhyay, andAlexander Gelbukh.
2011.
A Textual EntailmentSystem using Anaphora Resolution.
System Report,Text Analysis Conference Recognizing TextualEntailment Track (TAC RTE) Notebook,November 14-15, 2011, National Institute of56Standards and Technology, Gaithersburg,Maryland USAStolcke, Andreas.
2002.
SRILM-an extensiblelanguage modeling toolkit.
In Proceedings of theinternational conference on spoken languageprocessing, Volume 2, pages 901-904.Wang, Rui and G?nter Neumann.
2007.
RecognizingTextual Entailment Using Sentence Similaritybased on Dependency Tree Skeletons.
InProceedings of the third PASCAL RecognisingTextual Entailment Challenge.Xuan-Hieu Phan.
2006.
CRFChunker: CRF EnglishPhrase Chunker , http://crfchunker.sourceforge.net/.57
