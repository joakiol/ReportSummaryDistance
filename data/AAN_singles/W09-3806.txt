Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 37?48,Paris, October 2009. c?2009 Association for Computational LinguisticsPredictive Text Entry using Syntax and SemanticsSebastian Ganslandtsebastian@ganslandt.nuJakob J?rwallDepartment of Computer ScienceLund UniversityS-221 00 Lund, Swedend02jjr@student.lth.sePierre Nuguespierre.nugues@cs.lth.seAbstractMost cellular telephones use numeric key-pads, where texting is supported by dic-tionaries and frequency models.
Given akey sequence, the entry system recognizesthe matching words and proposes a rank-ordered list of candidates.
The rankingquality is instrumental to an effective en-try.This paper describes a new method to en-hance entry that combines syntax and lan-guage models.
We first investigate com-ponents to improve the ranking step: lan-guage models and semantic relatedness.We then introduce a novel syntactic modelto capture the word context, optimizeranking, and then reduce the number ofkeystrokes per character (KSPC) neededto write a text.
We finally combine thismodel with the other components and wediscuss the results.We show that our syntax-based modelreaches an error reduction in KSPC of12.4% on a Swedish corpus over a base-line using word frequencies.
We also showthat bigrams are superior to all the othermodels.
However, bigrams have a mem-ory footprint that is unfit for most devices.Nonetheless, bigrams can be further im-proved by the addition of syntactic mod-els with an error reduction that reaches29.4%.1 IntroductionThe 12-key input is the most common keypad lay-out on cellular telephones.
It divides the alpha-bet into eight lists of characters and each list ismapped onto one key as shown in Figure 1.
Sincethree or four characters are assigned to a key, asingle key press is ambiguous.Figure 1: Standard 12-button keypad layout (ISO9995-8).1.1 Multi-tapMulti-tap is an elementary method to disam-biguate input for a 12-button keypad.
Each charac-ter on a key is assigned an index that correspondsto its visual position, e.g.
?A?, 1, ?B?, 2, and ?C?,3 and each consecutive stroke ?
tap ?
on the samekey increments the index.
When the user wantsto type a letter, s/he presses the corresponding keyuntil the desired index is reached.
The user thenpresses another key or waits a predefined time toverify that the correct letter is selected.
The keysequence 8-4-4-3-3, for example, leads to the wordthe.Multi-tap is easy to implement and no dictio-nary is needed.
At the same time, it is slow andtedious for the user, notably when two consecutivecharacters are placed on the same key.1.2 Single Tap with Predictive TextSingle tap with predictive text requires only onekey press to enter a character.
Given a keystrokesequence, the system proposes words using a dic-tionary or language modeling techniques.Dictionary-based techniques search the wordsmatching the key sequence in a list that is storedby the system (Haestrup, 2001).
While some37keystroke sequences produce a unique word, oth-ers are ambiguous and the system returns a listwith all the candidates.
The key sequence 8-4-3,for example, corresponds to at least three possi-ble words: the, tie, and vie.
The list of candidatesis then sorted according to certain criteria, suchas the word or character frequencies.
If the worddoes not exist in the dictionary, the user has to fallback to multi-tap to enter it.
The T91 commercialproduct is an example of a dictionary-based sys-tem (Grover et al, 1998).LetterWise (MacKenzie et al, 2001) is a tech-nique that uses letter trigrams and their frequen-cies to predict the next character.
For example,pressing the key 3 after the letter bigram ?th?
willselect ?e?, because the trigram ?the?
is far more fre-quent than ?thd?
or ?thf?
in English.
When the sys-tem proposes a wrong letter, the user can accessthe next most likely one by pressing a next-key.LetterWise does not need a dictionary and has aKSPC of 1.1500 (MacKenzie, 2002).1.3 Modeling the ContextLanguage modeling can extend the context fromletter sequences to word n-grams.
In this case, thesystem is not restricted to the disambiguation orthe prediction of the typed characters.
It can com-plete words and even predict phrases.
HMS (Has-selgren et al, 2003) is an example of this that usesword bigrams in Swedish.
It reports a KSPCranging from 0.8807 to 1.0108, depending on thetype of text.
eZiText2 is a commercial example ofa word and phrase completion system.
However,having a large lexicon of bigrams still exceeds thememory capacity of many mobile devices.Some systems use a combination of syntac-tic and semantic information to model the con-text.
Gong et al (2008) is a recent example thatuses word frequencies, a part-of-speech languagemodel, and a semantic relatedness metric.
Thepart-of-speech language model acts as a lexicaln-gram language model, but occupies much lessmemory since the vocabulary is restricted to thepart-of-speech tagset.
The semantic relatedness,modified from Li and Hirst (2005), is defined asthe conditional probability of two stems appearingin the same context (the same sentence):1www.t9.com2www.zicorp.com/ezitext.htmSemR(w1|w2) = C(stem(w1), stem(w2))C(w2) .The three components are combined linearlyand their coefficients are adjusted using a devel-opment set.
Setting 1 as the limit of the KSPCfigure, Gong et al (2008) reported an error reduc-tion over the word frequency baseline of 4.6% forthe semantic model, 12.6% for the part-of-speechlanguage model, and 15.8% for the combinationof both.1.4 Syntax in Predictive TextBeyond part-of-speech language modeling, thereare few examples of systems using syntax in pre-dictive text entry.
Matiasek et al (2002) describesa predictive text environment aimed at disabledpersons, which originally relied on language mod-els.
Gustavii and Pettersson (2003) added a syn-tactic component to it based on grammar rules.The rules corresponded to common grammaticalerrors and were used to rerank the list of candidatewords.
The evaluation results were disappointingand the syntactic component was not added be-cause of the large overhead it introduced (Mati-asek, 2006).In the same vein, Sundarkantham and Shalinie(2007) used grammar rules to discard infeasiblegrammatical constructions.
The authors evaluatedtheir system by giving it an incomplete sentenceand seeing how often the system correctly guessedthe next word (Shannon, 1951).
They achievedbetter results than previously reported, althoughtheir system has not been used in the context ofpredictive text entry for mobile devices.2 Predictive Text Entry Using SyntaxWe propose a new technique that makes use ofa syntactic component to model the word contextand improve the KSPC figure.
It builds on Gonget al (2008)?s system and combines a dependencygrammar model with word frequencies, a part-of-speech language model, and the semantic related-ness defined in Sect.
1.3.
As far as we are aware,no predictive text entry system has yet used a data-driven syntactic model of the context.We used Swedish as our target language allover our experiments, but the results we obtainedshould be replicable in any other language.382.1 Reranking Candidate WordsThe system consists of two components.
The firstone disambiguates the typed characters using adictionary and produces a list of candidate words.The second component reranks the candidate list.Although the techniques we describe could be ap-plied to word completion, we set aside this aspectin this paper.More formally, we frame text input as a se-quence of keystrokes, ksi = ksi1 .
.
.
ksin, to en-ter a desired word, wi.
The words matchingthe key sequence in the system dictionary forman ordered set of alternatives, match(ksi) ={cw0, .
.
.
, cwm}, where it takes k extra keystrokesto reach candidate cwk.
Using our examplein Sect.
1.2, a lexical ordering would yieldmatch(8 ?
4 ?
3) = {the, tie, vie}, where twoextra keystrokes are needed to reach vie.We assign each candidate word w member ofmatch(ksi) a scoreScore(w|Context) =?s?S?s ?
s(w|Context),to rerank (sort) the prediction list, where s is ascoring function from a set S, ?s, the weight ofs, and Score(w|Context), the total score of w inthe current context.In this framework, optimizing predictive textentry is the task of finding the scoring functions,s, and the weights, ?s, so that they minimize k onaverage.As scoring functions, we considered lexical lan-guage models in the form of unigrams and bi-grams, sLM1 and sLM2, a part-of-speech modelusing sequences of part-of-speech tags of a lengthof up to five tags, sPOS , and a semantic affin-ity, sSemA, derived from the semantic relatedness.In addition, we introduce a syntactic componentin the form of a data-driven dependency syntax,sDepSyn so that the complete scoring set consistsofS = {sLM1, sLM2, sSemA, sPOS , sDepSyn}.2.2 Language and Part-of-Speech ModelsThe language model score is the probability of acandidate word w, knowing the sequence enteredso far, w1, .
.
.
, wi:P (w|w1, w2, .
.
.
, wi).We approximate it using unigrams, sLM1(w) =P (w), or bigrams, sLM2(w) = P (w|wi) that wederive from a corpus using the maximum like-lihood estimate.
To cope with sparse data, weused a deleted interpolation so that sLM2(w) =?1P (w|wi)+?2P (w), where we adjusted the val-ues of ?1 and ?2 on a development corpus.In practice, it is impossible to maintain a largelist of bigrams on cellular telephones as it wouldexceed the available memory of most devices.
Inour experiments, the sLM2 score serves as an indi-cator of an upper-limit performance, while sLM1serves as a baseline, as it is used in commercialdictionary-based products.Part-of-speech models offer an interesting alter-native to lexical models as the number of partsof speech does not exceed 100 tags in most lan-guages.
The possible number of bigrams is then atmost 10,000 and much less in practice.
We definedthe part-of-speech model score, sPOS asP (t|t1, t2, .
.
.
, ti),where ti is the part of speech of wi and t, the partof speech of the candidate word w. We used a5-gram approximation of this probability with asimple back-off model:sPOS =??????????
?P (t|ti?3, .
.
.
, ti) if C(ti?3, ..., ti) 6= 0P (t|ti?2, .
.
.
, ti) if C(ti?2, ..., ti) 6= 0...P (t), otherwiseWe used the Granska tagger (Carlberger andKann, 1999) to carry out the part-of-speech anno-tation of the word sequence.3 Semantic AffinityBecause of their arbitrary length, language mod-els miss possible relations between words that aresemantically connected in a sentence but withina distance greater than one, two, or three wordsapart, the practical length of most n-grams mod-els.
Li and Hirst (2005) introduced the semanticrelatedness between two words to measure suchrelations within a sentence.
They defined it asSemR(wi, wj) = C(wi, wj)C(wi)C(wj) ,where C(wi, wj) is the number of times the wordswi and wj co-occur in a sentence in the corpus,39and C(wi) is the count of word wi in the corpus.The relation is symmetrical, i.e.C(wi, wj) = C(wj , wi).The estimated semantic affinity of a word w isdefined as:SemA(w|H) = ?wj?HSemR(w,wj),where H is the context of the word w. In our case,H consists of words to the left of the current word.Gong et al (2008) used a similar model in a pre-dictive text application with a slight modificationto the SemR function:SemR(wi, wj) = C(stem(wi), stem(wj))C(stem(wj)) ,where the stem(w) function removes suffixesfrom words.
We refined this model further and wereplaced the stemming function with a real lemma-tization.4 Dependency ParsingDependency syntax (Tesni?re, 1966) has attracteda considerable interest in the recent years, spurredby the availability of data-driven parsers as wellas annotated data in multiple languages includ-ing Arabic, Chinese, Czech, English, German,Japanese, Portuguese, or Spanish (Buchholz andMarsi, 2006; Nivre et al, 2007).
We used thissyntactic formalism because of its availability inmany languages.4.1 Parser ImplementationThere are two main classes of data-driven de-pendency parsers: graph-based (McDonald andPereira, 2006) and transition-based (Nivre, 2003).We selected Nivre?s parser because of its imple-mentation simplicity, small memory footprint, andlinear time complexity.
Parsing is always achievedin at most 2n?
1 actions, where n is the length ofthe sentence.
Both types of parser can be com-bined, see Zhang and Clark (2008) for a discus-sion.Nivre?s parser is an extension to the shift?reduce algorithm that creates a projective andacyclic graph.
It uses a stack, a list of input words,and builds a set of arcs representing the graph ofdependencies.
The parser uses two operations inaddition to shift and reduce, left-arc and right-arc:?
Shift pushes the next input word onto thestack.?
Reduce pops the top of the stack with thecondition that the corresponding word has ahead.?
LeftArc adds an arc from the next inputword to the top of the stack and pops it.?
RightArc adds an arc from the top of thestack to the next input word and pushes theinput word on the stack.Table 1 shows the start and final parser states aswell as the four transitions and their conditionsand Algorithm 1 describes the parsing algorithm.4.2 FeaturesAt each step of the parsing procedure, the parserturns to a guide to decide on which transitionto apply among the set {LeftArc, RightArc,Shift, Reduce}.
We implemented this guideas a four-class classifier that uses features it ex-tracts from the parser state.
The features consistof words and their parts of speech in the stack, inthe queue, and in the partial graph resulting fromwhat has been parsed so far.
The classifier is basedon a linear logistic regression function that evalu-ates the transition probabilities from the featuresand predicts the next one.In the learning phase, we extracted a data setof feature vectors using the gold-standard parsingprocedure (Algorithm 2) that we applied to Tal-banken corpus of Swedish text (Einarsson, 1976;Nilsson et al, 2005).
Each vector being labeledwith one of the four possible transitions.
Wetrained the classifiers using the LIBLINEAR im-plementation (Fan et al, 2008) of logistic regres-sion.However, classes are not always separable us-ing linear classifiers.
We combined single featuresas pairs or triples.
This emulates to some extentquadratic kernels used in support vector machines,while preserving the speed of the linear models.Table 2 shows the complete feature set to predictthe transitions.
A feature is defined by?
A source: S for stack and Q for the queue;?
An offset: 0 for the top of the stack and firstin the queue; 1 and 2 for levels down in thestack or to the right in the queue;40Name Action ConditionInitialization ?nil,W, ?
?Termination ?S, nil, A?LeftArc ?n|S, n?|Q,A?
?
?S, n?|Q,A ?
{?n?, n?}?
??n?
?, ?n, n???
?
ARightArc ?n|S, n?|Q,A?
?
?n?|n|S,Q,A ?
?n, n???
??n?
?, ?n?, n???
?
AReduce ?n|S,Q,A?
?
?S,Q,A?
?n?, ?n, n??
?
AShift ?S, n|Q,A?
?
?n|S,Q,A?Table 1: Parser transitions.
W is the original input sentence, A is the dependency graph, S is the stack,andQ is the queue.
The triplet ?S,Q,A?
represents the parser state.
n, n?, and n??
are lexical tokens.
Thepair ?n?, n?
represents an arc from the head n?
to the dependent n.?
Possible applications of the function head,H ,leftmost child, LC, or righmost child, RC;?
The value: word, w, or POS tag, t, at thespecified position.Queue Q0wQ1wQ0tQ1tQ0tQ0wQ0tQ1tQ1wQ1tQ0tQ1tQ2tQ0wQ1tQ2tStack S0tS0wS0tS0wS0tS1tStack/Queue S0wQ0wQ0tS0tQ1tS0tQ0tS1tQ1tS1tS0tQ0tQ1tS0tQ0wQ0tPartial Graph S0HtS0tQ0tQ0LCtS0tQ0tQ0LCtS0tQ0wS0RCtS0tQ0tS0RCtS0tQ0wTable 2: Feature model for predicting parser ac-tions with combined features.4.3 Calculating Graph ProbabilitiesNivre (2006) showed that every terminating tran-sition sequence Am1 = (a1, ..., am) applied toa sentence Wn1 = (w1, ..., wn) defines exactlyone parse tree G. We approximated the prob-ability P (G|Wn1 ) of a dependency graph G asP (Am1 |Wn1 ) and we estimated the probability ofG as the product of the transition probabilities, sothatPParse(G|Wn1 ) = P (Am1 |Wn1 )= ?mk=1 P (ak|Ak?11 ,W ?
(k?1)1 ),where ak is member of the set {LeftArc,RightArc, Shift, Reduce} and ?
(k) corre-sponds to the index of the current word at tran-sition k.We finally approximated the termAk?11 ,W?
(k?1)1 to the feature set and com-puted probability estimates using the logisticregression output.4.4 Beam SearchWe extended Nivre?s parser with a beam search tomitigate error propagation that occurs with a de-terministic parser (Johansson and Nugues, 2006).We maintained N parser states in parallel and weapplied all the possible transitions to each state.We scored each transition action and we rankedthe states with the product of the action?s proba-bilities leading to this state.
Algorithm 3 outlinesbeam search with a diameter of N .An alternative to training parser transitions us-ing local features is to use an online learning al-gorithm (Johansson and Nugues, 2007; Zhang andClark, 2008).
The classifiers are then computedover the graph that has already been built insteadof considering the probability of a single transi-tion.414.5 EvaluationWe evaluated our dependency parser separatelyfrom the rest of the application and Table 3 showsthe results.
We optimized our parameter selectionfor the unlabeled attachment score (UAS).
Thisexplains the relatively high difference with the la-beled attachment score (LAS): about ?8.6.Table 3 also shows the highest scores ob-tained on the same Talbanken corpus of Swedishtext (Einarsson, 1976; Nilsson et al, 2005) inthe CoNLL-X evaluation (Buchholz and Marsi,2006): 89.58 for unlabeled attachments (Corston-Oliver and Aue, 2006) and 84.58 for labeled at-tachments (Nivre et al, 2006).
CoNLL-X systemswere optimized for the LAS category.The figures we reached were about 1.10% be-low those reported in CONLL-X for the UAS cat-egory.
However our results are not directly compa-rable as the parsers or the classifiers in CONLL-Xhave either a higher complexity or are more time-consuming.
We chose linear classifiers over kernelmachines as it was essential to our application torun on mobile devices with limited resources inboth CPU power and memory size.This paper CONLL-XBeam width LAS UAS LAS UAS1 79.45 88.05 84.58 89.542 79.76 88.414 79.75 88.408 79.77 88.4116 79.78 88.4232 79.77 88.4164 79.79 88.44Table 3: Parse results on the Swedish Talbankencorpus obtained for this paper as well as the bestreported results in CONLL-X on the same corpus(Buchholz and Marsi, 2006).5 Dependencies to Predict the Next WordWe built a syntactic score to measure the grammat-ical relevance of a candidate word w in the currentcontext, that is the word sequence so farw1, ..., wi.We defined it as the weighted sum of three terms:the score of the partial graph resulting from theanalysis of the words to the left of the candidateword and the scores of the link from w to its head,h(w), using their lexical forms and their parts ofspeech:sDepSyn(w) = ?1PParse(G(w)|w1, ..., wi, w)+?2PLink(w, h(w))+?3PLink(POS(w), POS(h(w))),where G(w) is the partial graph representing theword sequence w1, ..., wi, w. The PLink terms areintended to give an extra-weight to the probabil-ity of an association between the predicted wordand a possible head to the left of it.
They hint atthe strength of the ties between w and the wordsbefore it.We used the transition probabilities described inSect.
4.3 to compute the score of the partial graph,yieldingPParse(G(w)|w1, ..., wi, w) =j?k=1P (ak),where a1, ..., aj is the sequence of transition ac-tions producing G(w) and P (ak), the probabilityoutput of transition k given by the logistic regres-sion engine.The last two terms PLink(w, h(w)) andPLink(POS(w), POS(h(w))) are computedfrom counts in the training corpus using maxi-mum likelihood estimates:PLink(w, h(w)) =C(Link(w, h(w)) + 1?wl?PW C(Link(wl, h(wl)))+ |PW |andPLink(POS(w), POS(h(w))) =C(Link(POS(w), POS(h(w)))) + 1?wl?PW C(Link(POS(wl), h(POS(wl))))+|PW |,where PW = match(ksi), is the set of predictedwords for the current key sequence.If the current word w has not been assigned ahead yet, we default h(w) to the root of the graphand POS(h(w)) to the ROOT value.6 Experiments and Results6.1 Experimental SetupFigure 2 shows an overview of the three stagesto produce and evaluate our models: training,42tuning, and testing.
Ideally, we would havetrained the classifiers on a corpus matching atext entry application.
However, as there is nolarge available SMS corpus in Swedish, we usedthe Stockholm-Ume?
corpus (SUC) (Ejerhed andK?llgren, 1997).
SUC is balanced and the largestavailable POS-tagged corpus in Swedish withmore than 1 million words.We parsed the corpus and we divided it ran-domly into a training set (80%), a development set(10%), and a test set (10%).
The training set wasused to gather statistics on word n-grams, POSn-grams, collocations, lemma frequencies, depen-dent/head relations.
We discarded hapaxes: rela-tions and sequences occurring only once.
We usedlemmas instead of stems in the semantic related-ness score, SemR, because stemming is less ap-propriate in Swedish than in English.We used the development set to find optimalweights for the scoring functions, resulting in thelowest KSPC.
We ran an exhaustive search usingall possible linear combinations with incrementsof 0.1, except for two functions, where this wastoo coarse.
We used 0.01 then.We applied the resulting linear combinations ofscoring functions to the test set.
We first comparedthe frequency-based disambiguation acting as abaseline to linear combinations involving or notinvolving syntax, but always excluding bigrams.Table 4 shows the most significant combinations.We then compared a set of other combinationswith the bigram model.
They are shown in Ta-ble 6.6.2 MetricsWe redefined the KSPC metric of MacKenzie(2002), since the number of characters needed toinput a word is now dependent on the word?s leftcontext in the sentence.
Let S = (w1, .
.
.
, wn) ?L be a sentence in the test corpus.
The KSPC forthe test corpus then becomesKSPC =?S?L?w?SKS(w|LContext(w, S))?S?L?w?S Chars(w)where KS(w|LContext) is the number of keystrokes needed to enter a word in a given context,LContext(w, S) is the left context of w in S, andChars(w) is the number of characters in w.Another performance measure is the disam-biguation accuracy (DA), which is the percentageof words that are correctly disambiguated after allthe keys have been pressedDA =?S?L?w?SPredHit(w|LContext(w, S))#w ,where PredHit(w|Context) = 1 if w is thetop prediction and 0 otherwise, and #w, the to-tal number of words in L. A good DA means thatthe user can more often simply accept the defaultproposed word instead of navigating the predictionlist for the desired word.As scoring tokens, we chose to keep the onesthat actually have the ability to differentiate themodels, i.e.
we did not count the KSPC and DAfor words that were not in the dictionary.
Neitherdid we count white spaces, nor the punctuationmarks.All our measures are without word or phrasecompletion.
This means that the lower-limit fig-ure for KSPC is 1.6.3 ResultsAs all the KSPC figures are close to 1, we com-puted the error reduction rate (ERR), i.e.
the re-duction in the number of extra keystrokes neededbeyond one.
We carried out all the optimizationsconsidering KSPC, but we can observe that KSPCERR and DA ERR strongly correlate.Table 5 shows the results with scoring func-tions using the word frequencies.
The columnsinclude KSPC and DA together with KSPC ERRand DA ERR compared with the baseline.
Table 7shows the respective results when using a bigram-based disambiguation instead of just frequency.The ERR is still compared to the word frequencybaseline but attention should also be drawn on therelative increases: how much the new models canimprove bigram-based disambiguation.7 DiscussionWe can observe from the results that a model basedon dependency grammars improves the predictionconsiderably.
The DepSyn model is actually themost effective one when applied together with thefrequency counts.
Furthermore, the improvementsfrom the POS, SemA, and DepSyn model arealmost disjunct, as the combined model improve-ment matches the sum of their respective individ-ual contributions.The 4.2% ERR observed when adding theSemA model is consistent with the result from43Figure 2: System architecture, where the set of scoring functions is S = {sLM , sSemA, sPOS , sDepSyn}and the linear combination is =?s?S?s ?
s(w).Gong et al (2008), where a 4.6% ERR was found.On the other hand, the POS model only con-tributed 4.7% ERR in our case, whereas Gong etal.
(2008) observed 12.6%.
One possible expla-nation for this is that they clustered related POStags into 19 groups reducing the sparseness prob-lem.
By performing this grouping, we can effec-tively ignore morphological and lexical featuresthat have no relevance, when deciding which wordshould come next.
Other possible explanations in-clude that our backoff model is not well suited forthis problem or that the POS sequences are not anapplicable model for Swedish.The bigram language model has the largest im-pact on the performance.
The ERR for bigramsalone is higher than all the other models com-bined.
Still, the other models have the ability tocontribute on top of the bigram model.
For exam-ple, the POS model increases the ERR by about5% both when using bigram- and frequency-baseddisambiguation, suggesting that this information isnot captured by the bigrams.
On the other hand,DepSyn increases the ERR by a more modest 3%when using bigrams instead of 7% with word fre-quencies.
This is likely due to the fact that abouthalf of the dependency links only stretch to thenext preceding or succeeding word in the corpus.The most effective combination of models arethe bigrams together with the POS sequence andthe dependency structure, both embedding syntac-tic information.
With this combination, we wereable to reduce the number of erroneous disam-biguations as well as extra keystrokes by almostone third.8 Further WorkSMS texting, which is the target of our system,is more verbal than the genres gathered in theStockholm-Ume?
corpus.
The language modelsof a final application would then change consid-erably from the ones we extracted from the SUC.A further work would be to collect a SMS corpusand replicate the experiments: retrain the modelsand obtain the corresponding performance figures.Moreover, we carried out our implementationand simulations on desktop computers.
The POSmodel has an estimated size of 700KB (Gong etal., 2008).
The PParse term of theDepSynmodelcan be made as small as the feature model.
We ex-pect the optimized size of this model to be under100KB in an embedded environment.
The size ofthe lexical variant of PLink is comparable to the bi-gram model.
This could however be remedied byusing the probability of the action that constructedthis last link.
The computational power requiredby LIBLINEAR is certainly within the reach ofmodern hand-held devices.
However, a prototypesimulation with real hardware conditions would44be needed to prove an implementability on mobiledevices.Finally, a user might perceive subtle differencesin the presentation of the words compared withthat of popular commercial products.
Gutowitz(2003) noted the reluctance to single-tap inputmethods because of their ?unpredictable?
behav-ior.
Introducing syntax-based disambiguationcould increase this perception.
A next step wouldbe to carry out usability studies and assess this el-ement.ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the Tenth Conference on Com-putational Natural Language Learning (CoNLL-X),pages 149?164, New York City.Johan Carlberger and Viggo Kann.
1999.
Implement-ing an efficient part-of-speech tagger.
Software ?Practice and Experience, 29(2):815?832.Simon Corston-Oliver and Anthony Aue.
2006.
De-pendency parsing with reference to slovene, spanishand swedish.
In Proceedings of the Tenth Confer-ence on Computational Natural Language Learning(CoNLL-X), pages 196?200, New York City, June.Jan Einarsson.
1976.
Talbankens skriftspr?kskonkor-dans.
Technical report, Lund University, Institutio-nen f?r nordiska spr?k, Lund.Eva Ejerhed and Gunnel K?llgren.
1997.
StockholmUme?
Corpus version 1.0, SUC 1.0.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Jun Gong, Peter Tarasewich, and I. Scott MacKenzie.2008.
Improved word list ordering for text entry onambiguous keypads.
In NordiCHI ?08: Proceedingsof the 5th Nordic conference on Human-computerinteraction, pages 152?161, Lund, Sweden.Dale L. Grover, Martin T. King, and Clifford A. Kush-ler.
1998.
Reduced keyboard disambiguating com-puter.
U.S. Patent no.
5,818,437.Ebba Gustavii and Eva Pettersson.
2003.
A Swedishgrammar for word prediction.
Technical report, De-partment of Linguistics, Uppsala University.Howard Gutowitz.
2003.
Barriers to adoption ofdictionary-based text-entry methods; a field study.In Proceedings of the Workshop on Language Mod-eling for Text Entry Systems (EACL 2003), pages 33?41, Budapest.Jan Haestrup.
2001.
Communication terminal hav-ing a predictive editor application.
U.S. Patent no.6,223,059.Jon Hasselgren, Erik Montnemery, Pierre Nugues, andMarkus Svensson.
2003.
HMS: A predictive textentry method using bigrams.
In Proceedings ofthe Workshop on Language Modeling for Text EntryMethods (EACL 2003), pages 43?49, Budapest.Richard Johansson and Pierre Nugues.
2006.
In-vestigating multilingual dependency parsing.
InProceedings of the Tenth Conference on Compu-tational Natural Language Learning (CONLL-X),pages 206?210, New York.Richard Johansson and Pierre Nugues.
2007.
Incre-mental dependency parsing using online learning.In Proceedings of the CoNLL Shared Task Sessionof EMNLP-CoNLL, pages 1134?1138, Prague, June28-30.Jianhua Li and Graeme Hirst.
2005.
Semantic knowl-edge in word completion.
In Assets ?05: Proceed-ings of the 7th international ACM SIGACCESS con-ference on Computers and accessibility, pages 121?128, Baltimore.I.
Scott MacKenzie, Hedy Kober, Derek Smith, TerryJones, and Eugene Skepner.
2001.
LetterWise:Prefix-based disambiguation for mobile text input.In 14th Annual ACM Symposium on User InterfaceSoftware and Technology, Orlando, Florida.I.
Scott MacKenzie.
2002.
KSPC (keystrokes per char-acter) as a characteristic of text entry techniques.
InProceedings of the Fourth International Symposiumon Human Computer Interaction with Mobile De-vices, pages 195?210, Heidelberg, Germany.Johannes Matiasek, Marco Baroni, and Harald Trost.2002.
FASTY ?
A multi-lingual approach to textprediction.
In ICCHP ?02: Proceedings of the8th International Conference on Computers HelpingPeople with Special Needs, pages 243?250, London.Johannes Matiasek.
2006.
The language componentof the FASTY predictive typing system.
In KarinHarbusch, Kari-Jouko Raiha, and Kumiko Tanaka-Ishii, editors, Efficient Text Entry, number 05382 inDagstuhl Seminar Proceedings, Dagstuhl, Germany.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of the 11th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL), pages 81?88, Trento.Jens Nilsson, Johan Hall, and Joakim Nivre.
2005.MAMBA meets TIGER: Reconstructing a Swedishtreebank from antiquity.
In Proceedings of theNODALIDA Special Session on Treebanks, Joensuu,Finland.45Joakim Nivre, Johan Hall, Jens Nilsson, G?lsenEryigit, and Svetoslav Marinov.
2006.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of the Tenth Con-ference on Computational Natural Language Learn-ing (CoNLL-X), pages 221?225, June.Joakim Nivre, Johan Hall, Sandra K?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160, Nancy.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer, Dordrecht, The Netherlands.Claude Elwood Shannon.
1951.
Prediction and en-tropy of printed English.
The Bell System TechnicalJournal, pages 50?64, January.K.
Sundarkantham and S. Mercy Shalinie.
2007.
Wordpredictor using natural language grammar inductiontechnique.
Journal of Theoretical and Applied In-formation Technology, 3:1?8.Lucien Tesni?re.
1966.
?l?ments de syntaxe struc-turale.
Klincksieck, Paris, 2e edition.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 562?571, Hawaii, October 25?27.46Algorithm 1 Nivre?s algorithm.1: Queue?W2: Stack ?
nil3: while ?Queue.isEmpty() do4: features?
ExtractFeatures()5: action?
guide.Predict(features)6: if action = RightArc ?
canRightArc() then7: RightArc()8: else if action = LeftArc ?
canLeftArc() then9: LeftArc10: else if action = Reduce ?
canReduce() then11: Reduce()12: else13: Shift()14: end if15: end while16: return(A)Algorithm 2 Reference parsing.1: Queue?W2: Stack ?
nil3: while ?Queue.isEmpty() do4: x?
ExtractFeatures()5: if ?Stack.peek(), Queue.get(0)?
?
A ?
canRightArc() then6: t?
RightArc7: else if ?Queue.get(0), Stack.peek()?
?
A ?
canLeftArc() then8: t?
LeftArc9: else if ?w ?
Stack : ?w,Queue.get(0)?
?
A?
?Queue.get(0), w?
?
A) ?
canReduce() then10: t?
Reduce11: else12: t?
Shift13: end if14: store training example ?x, t?15: end whileAlgorithm 3 Beam parse.1: Agenda.add(InititalParserState)2: while ?done do3: for parserState ?
Agenda do4: Output.add(parserState.doLeftArc())5: Output.add(parserState.doRightArc())6: Output.add(parserState.doReduce())7: Output.add(parserState.doShift())8: end for9: Sort(Output)10: Clear(Agenda)11: Take N best parse trees from Output and put in Agenda.12: end while13: Return best item in Agenda.47Configuration Scoring model DepSyn weightsF1 baseline 1?
LM1 (Word frequencies) ?F2 0.9?
LM1 + 0.1?
POS ?F3 0.7?
LM1 + 0.3?
SemA ?F4 0.6?
LM1 + 0.4?DepSyn (0.3, 0.7, 0.0)F5 0.6?
LM1 + 0.1?
POS + 0.3?DepSyn (0.0 1.0 0.0)F6 0.5?
LM1 + 0.2?
SemA+ 0.3?DepSyn (0.2 0.7 0.1)F7 0.4?
LM1 + 0.1?
POS + 0.3?DepSyn+ 0.2?
SemA (0.2, 0.8, 0.0)Table 4: The different combinations of scoring models using frequency-based disambiguation as a base-line.
The DepSyn weight triples corresponds to (?1, ?2, ?3) in Sect.
5.Configuration KSPC DA KSPC ERR DA ERRF1 1.015559 94.15% 0.00% 0.00%F2 1.014829 94.31% 4.69% 2.72%F3 1.014902 94.36% 4.22% 3.62%F4 1.014462 94.56% 7.05% 7.04%F5 1.013625 94.75% 12.43% 10.28%F6 1.014159 94.62% 9.00% 8.10%F7 1.013438 94.86% 13.63% 12.16%Table 5: Results for the disambiguation based on word frequencies together with the semantic and syn-tactic models.Configuration Scoring model Bigram weights DepSyn weightsB1 1?
LM2 (Bigram frequencies) (0.9, 0.1) ?B2 0.9?
LM2 + 0.1?
POS (0.8, 0.2) ?B3 0.95?
LM2 + 0.05?
SemA (0.8, 0.2) ?B4 0.9?
LM2 + 0.1?DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)B5 0.8?
LM2 + 0.1?
POS + 0.1?
SemA (0.8, 0.2) ?B6 0.81?
LM2 + 0.08?
POS + 0.11?DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)Table 6: The different combinations of scoring models using bigram-based disambiguation as baseline.In addition to the DepSyn weights, this table also shows the language model interpolation weights, ?1and ?2 described in Sect.
2.2.Label KSPC DA KSPC ERR DA ERRB1 1.012159254 95.48% 21.85% 22.81%B2 1.011434213 95.75% 26.51% 27.41%B3 1.011860573 95.50% 23.77% 23.20%B4 1.011698693 95.62% 24.81% 25.19%B5 1.011146932 95.80% 28.36% 28.23%B6 1.010980592 95.91% 29.43% 30.09%Table 7: Results for the disambiguation based on bigrams plus the semantic and syntactical models.
Theerror reduction rate is relative to the word frequency baseline.48
