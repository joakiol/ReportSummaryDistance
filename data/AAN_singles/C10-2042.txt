Coling 2010: Poster Volume, pages 365?373,Beijing, August 2010Conundrums in Unsupervised Keyphrase Extraction:Making Sense of the State-of-the-ArtKazi Saidul Hasan and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at Dallas{saidul,vince}@hlt.utdallas.eduAbstractState-of-the-art approaches for unsuper-vised keyphrase extraction are typicallyevaluated on a single dataset with a singleparameter setting.
Consequently, it is un-clear how effective these approaches areon a new dataset from a different domain,and how sensitive they are to changes inparameter settings.
To gain a better under-standing of state-of-the-art unsupervisedkeyphrase extraction algorithms, we con-duct a systematic evaluation and analysisof these algorithms on a variety of stan-dard evaluation datasets.1 IntroductionThe keyphrases for a given document refer to agroup of phrases that represent the document.
Al-though we often come across texts from differentdomains such as scientific papers, news articlesand blogs, which are labeled with keyphrases bythe authors, a large portion of the Web content re-mains untagged.
While keyphrases are excellentmeans for providing a concise summary of a doc-ument, recent research results have suggested thatthe task of automatically identifying keyphrasesfrom a document is by no means trivial.
Re-searchers have explored both supervised and un-supervised techniques to address the problem ofautomatic keyphrase extraction.
Supervised meth-ods typically recast this problem as a binary clas-sification task, where a model is trained on anno-tated data to determine whether a given phrase isa keyphrase or not (e.g., Frank et al (1999), Tur-ney (2000; 2003), Hulth (2003), Medelyan et al(2009)).
A disadvantage of supervised approachesis that they require a lot of training data and yetshow bias towards the domain on which they aretrained, undermining their ability to generalizewell to new domains.
Unsupervised approachescould be a viable alternative in this regard.The unsupervised approaches for keyphrase ex-traction proposed so far have involved a numberof techniques, including language modeling (e.g.,Tomokiyo and Hurst (2003)), graph-based rank-ing (e.g., Zha (2002), Mihalcea and Tarau (2004),Wan et al (2007), Wan and Xiao (2008), Liuet al (2009a)), and clustering (e.g., Matsuo andIshizuka (2004), Liu et al (2009b)).
While thesemethods have been shown to work well on a par-ticular domain of text such as short paper abstractsand news articles, their effectiveness and portabil-ity across different domains have remained an un-explored issue.
Worse still, each of them is basedon a set of assumptions, which may only hold forthe dataset on which they are evaluated.Consequently, we have little understanding ofhow effective the state-of the-art systems would beon a completely new dataset from a different do-main.
A few questions arise naturally.
How wouldthese systems perform on a different dataset withtheir original configuration?
What could be theunderlying reasons in case they perform poorly?Is there any system that can generalize fairly wellacross various domains?We seek to gain a better understanding of thestate of the art in unsupervised keyphrase ex-traction by examining the aforementioned ques-tions.
More specifically, we compare five unsu-pervised keyphrase extraction algorithms on fourcorpora with varying domains and statistical char-acteristics.
These algorithms represent the ma-365jor directions in this research area, including Tf-Idf and four recently proposed systems, namely,TextRank (Mihalcea and Tarau, 2004), SingleR-ank (Wan and Xiao, 2008), ExpandRank (Wanand Xiao, 2008), and a clustering-based approach(Liu et al, 2009b).
Since none of these systems(except TextRank) are publicly available, we re-implement all of them and make them freely avail-able for research purposes.1 To our knowledge,this is the first attempt to compare the perfor-mance of state-of-the-art unsupervised keyphraseextraction systems on multiple datasets.2 CorporaOur four evaluation corpora belong to differentdomains with varying document properties.
Ta-ble 1 provides an overview of each corpus.The DUC-2001 dataset (Over, 2001), which isa collection of 308 news articles, is annotated byWan and Xiao (2008).
We report results on all 308articles in our evaluation.The Inspec dataset is a collection of 2,000 ab-stracts from journal papers including the paper ti-tle.
Each document has two sets of keyphrases as-signed by the indexers: the controlled keyphrases,which are keyphrases that appear in the In-spec thesaurus; and the uncontrolled keyphrases,which do not necessarily appear in the thesaurus.This is a relatively popular dataset for automatickeyphrase extraction, as it was first used by Hulth(2003) and later by Mihalcea and Tarau (2004)and Liu et al (2009b).
In our evaluation, we usethe set of 500 abstracts designated by these previ-ous approaches as the test set and its set of uncon-trolled keyphrases.
Note that the average docu-ment length for this dataset is the smallest amongall our datasets.The NUS Keyphrase Corpus (Nguyen andKan, 2007) includes 211 scientific conference pa-pers with lengths between 4 to 12 pages.
Eachpaper has one or more sets of keyphrases assignedby its authors and other annotators.
We use all the211 papers in our evaluation.
Since the numberof annotators can be different for different docu-ments and the annotators are not specified alongwith the annotations, we decide to take the union1See http://www.hlt.utdallas.edu/?saidul/code.html for details.of all the gold standard keyphrases from all thesets to construct one single set of annotation foreach paper.
As Table 1 shows, each NUS pa-per, both in terms of the average number of to-kens (8291) and candidate phrases (2027) per pa-per, is more than five times larger than any doc-ument from any other corpus.
Hence, the num-ber of candidate keyphrases that can be extractedis potentially large, making this corpus the mostchallenging of the four.Finally, the ICSI meeting corpus (Janin et al,2003), which is annotated by Liu et al (2009a),includes 161 meeting transcriptions.
FollowingLiu et al, we remove topic segments marked as?chitchat?
and ?digit?
from the dataset and use allthe remaining segments for evaluation.
Each tran-script contains three sets of keyphrases producedby the same three human annotators.
Since it ispossible to associate each set of keyphrases withits annotator, we evaluate each system on thisdataset three times, once for each annotator, andreport the average score.
Unlike the other threedatasets, the gold standard keys for the ICSI cor-pus are mostly unigrams.3 Unsupervised Keyphrase ExtractorsA generic unsupervised keyphrase extraction sys-tem typically operates in three steps (Section 3.1),which will help understand the unsupervised sys-tems explained in Section 3.2.3.1 Generic Keyphrase ExtractorStep 1: Candidate lexical unit selection Thefirst step is to filter out unnecessary word to-kens from the input document and generate a listof potential keywords using heuristics.
Com-monly used heuristics include (1) using a stopword list to remove non-keywords (e.g., Liu et al(2009b)) and (2) allowing words with certain part-of-speech tags (e.g., nouns, adjectives, verbs) tobe considered candidate keywords (Mihalcea andTarau (2004), Liu et al (2009a), Wan and Xiao(2008)).
In all of our experiments, we follow Wanand Xiao (2008) and select as candidates wordswith the following Penn Treebank tags: NN, NNS,NNP, NNPS, and JJ, which are obtained using theStanford POS tagger (Toutanova and Manning,2000).366CorporaDUC-2001 Inspec NUS ICSIType News articles Paper abstracts Full papers Meeting transcripts# Documents 308 500 211 161# Tokens/Document 876 134 8291 1611# Candidate words/Document 312 57 3271 453# Candidate phrases/Document 207 34 2027 296# Tokens/Candidate phrase 1.5 1.7 1.6 1.5# Gold keyphrases 2484 4913 2327 582# Gold keyphrases/Document 8.1 9.8 11.0 3.6U/B/T/O distribution (%) 17/61/18/4 13/53/25/9 27/50/16/7 68/29/2/1# Tokens/Gold keyphrase 2.1 2.3 2.1 1.3Table 1: Corpus statistics for the four datasets used in this paper.
A candidate word/phrase, typically a sequenceof one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase.
TheU/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and otherhigher order n-grams.Step 2: Lexical unit ranking Once the can-didate list is generated, the next task is to rankthese lexical units.
To accomplish this, it is nec-essary to build a representation of the input textfor the ranking algorithm.
Depending on the un-derlying approach, each candidate word is repre-sented by its syntactic and/or semantic relation-ship with other candidate words.
The relationshipcan be defined using co-occurrence statistics, ex-ternal resources (e.g., neighborhood documents,Wikipedia), or other syntactic clues.Step 3: Keyphrase formation In the final step,the ranked list of candidate words is used to formkeyphrases.
A candidate phrase, typically a se-quence of nouns and adjectives, is selected as akeyphrase if (1) it includes one or more of thetop-ranked candidate words (Mihalcea and Tarau(2004), Liu et al (2009b)), or (2) the sum of theranking scores of its constituent words makes it atop scoring phrase (Wan and Xiao, 2008).3.2 The Five Keyphrase ExtractorsAs mentioned above, we re-implement five unsu-pervised approaches for keyphrase extraction.
Be-low we provide a brief overview of each system.3.2.1 Tf-IdfTf-Idf assigns a score to each term t in a doc-ument d based on t?s frequency in d (term fre-quency) and how many other documents includet (inverse document frequency) and is defined as:tfidft = tft ?
log(D/Dt) (1)where D is the total number of documents and Dtis the number of documents containing t.Given a document, we first compute the Tf-Idf score of each candidate word (see Step 1 ofthe generic algorithm).
Then, we extract all thelongest n-grams consisting of candidate wordsand score each n-gram by summing the Tf-Idfscores of its constituent unigrams.
Finally, we out-put the top N n-grams as keyphrases.3.2.2 TextRankIn the TextRank algorithm (Mihalcea and Ta-rau, 2004), a text is represented by a graph.
Eachvertex corresponds to a word type.
A weight,wij , is assigned to the edge connecting the twovertices, vi and vj , and its value is the numberof times the corresponding word types co-occurwithin a window of W words in the associatedtext.
The goal is to (1) compute the score of eachvertex, which reflects its importance, and then (2)use the word types that correspond to the highest-scored vertices to form keyphrases for the text.The score for vi, S(vi), is initialized with a de-fault value and is computed in an iterative manneruntil convergence using this recursive formula:S(vi) = (1?
d) + d?
?vj?Adj(vi)wji?vk?Adj(vj)wjkS(vj)(2)where Adj(vi) denotes vi?s neighbors and d is thedamping factor set to 0.85 (Brin and Page, 1998).Intuitively, a vertex will receive a high score if ithas many high-scored neighbors.
As noted before,after convergence, the T% top-scored vertices are367selected as keywords.
Adjacent keywords are thencollapsed and output as a keyphrase.According to Mihalcea and Tarau (2004), Tex-tRank?s best score on the Inspec dataset isachieved when only nouns and adjectives are usedto create a uniformly weighted graph for the textunder consideration, where an edge connects twoword types only if they co-occur within a windowof two words.
Hence, our implementation of Tex-tRank follows this configuration.3.2.3 SingleRankSingleRank (Wan and Xiao, 2008) is essen-tially a TextRank approach with three major dif-ferences.
First, while each edge in a TextRankgraph (in Mihalcea and Tarau?s implementation)has the same weight, each edge in a SingleRankgraph has a weight equal to the number of timesthe two corresponding word types co-occur.
Sec-ond, while in TextRank only the word types thatcorrespond to the top-ranked vertices can be usedto form keyphrases, in SingleRank, we do not fil-ter out any low-scored vertices.
Rather, we (1)score each candidate keyphrase, which can be anylongest-matching sequence of nouns and adjec-tives in the text under consideration, by summingthe scores of its constituent word types obtainedfrom the SingleRank graph, and (2) output the Nhighest-scored candidates as the keyphrases forthe text.
Finally, SingleRank employs a windowsize of 10 rather than 2.3.2.4 ExpandRankExpandRank (Wan and Xiao, 2008) is aTextRank extension that exploits neighborhoodknowledge for keyphrase extraction.
For a givendocument d, the approach first finds its k near-est neighboring documents from the accompany-ing document collection using a similarity mea-sure (e.g., cosine similarity).
Then, the graph ford is built using the co-occurrence statistics of thecandidate words collected from the document it-self and its k nearest neighbors.Specifically, each document is represented bya term vector where each vector dimension cor-responds to a word type present in the documentand its value is the Tf-Idf score of that word typefor that document.
For a given document d0, its knearest neighbors are identified, and together theyform a larger document set of k+1 documents,D = {d0, d1, d2, ..., dk}.
Given this documentset, a graph is constructed, where each vertex cor-responds to a candidate word type in D, and eachedge connects two vertices vi and vj if the corre-sponding word types co-occur within a window ofW words in the document set.
The weight of anedge, w(vi, vj), is computed as follows:w(vi, vj) =?dk?Dsim(d0, dk)?
freqdk(vi, vj) (3)where sim(d0, dk) is the cosine similarity be-tween d0 and dk, and freqdk(vi, vj) is the co-occurrence frequency of vi and vj in document dk.Once the graph is constructed, the rest of the pro-cedure is identical to SingleRank.3.2.5 Clustering-based ApproachLiu et al (2009b) propose to cluster candidatewords based on their semantic relationship to en-sure that the extracted keyphrases cover the en-tire document.
The objective is to have each clus-ter represent a unique aspect of the document andtake a representative word from each cluster sothat the document is covered from all aspects.More specifically, their algorithm (henceforthreferred to as KeyCluster) first filters out the stopwords from a given document and treats the re-maining unigrams as candidate words.
Second,for each candidate, its relatedness with anothercandidate is computed by (1) counting how manytimes they co-occur within a window of size Win the document and (2) using Wikipedia-basedstatistics.
Third, candidate words are clusteredbased on their relatedness with other candidates.Three clustering algorithms are used of whichspectral clustering yields the best score.
Oncethe clusters are formed, one representative word,called an exemplar term, is picked from each clus-ter.
Finally, KeyCluster extracts from the docu-ment all the longest n-grams starting with zeroor more adjectives and ending with one or morenouns, and if such an n-gram includes one or moreexemplar words, it is selected as a keyphrase.
Asa post-processing step, a frequent word list gener-ated from Wikipedia is used to filter out the fre-quent unigrams that are selected as keyphrases.3684 Evaluation4.1 Experimental SetupTextRank and SingleRank setup FollowingMihalcea and Tarau (2004) and Wan and Xiao(2008), we set the co-occurrence window size forTextRank and SingleRank to 2 and 10, respec-tively, as these parameter values have yielded thebest results for their evaluation datasets.ExpandRank setup Following Wan and Xiao(2008), we find the 5 nearest neighbors for eachdocument from the remaining documents in thesame corpus.
The other parameters are set in thesame way as in SingleRank.KeyCluster setup As argued by Liu et al(2009b), Wikipedia-based relatedness is computa-tionally expensive to compute.
As a result, we fol-low them by computing the co-occurrence-basedrelatedness instead, using a window of size 10.Then, we cluster the candidate words using spec-tral clustering, and use the frequent word list thatthey generously provided us to post-process theresulting keyphrases by filtering out those that arefrequent unigrams.4.2 Results and DiscussionIn an attempt to gain a better insight into thefive unsupervised systems, we report their perfor-mance in terms of precision-recall curves for eachof the four datasets (see Figure 1).
This contrastswith essentially all previous work, where the per-formance of a keyphrase extraction system is re-ported in terms of an F-score obtained via a par-ticular parameter setting on a particular dataset.We generate the curves for each system as fol-lows.
For Tf-Idf, SingleRank, and ExpandRank,we vary the number of keyphrases, N , predictedby each system.
For TextRank, instead of vary-ing the number of predicted keyphrases, we varyT , the percentage of top-scored vertices (i.e., un-igrams) that are selected as keywords at the endof the ranking step.
The reason is that TextRankonly imposes a ranking on the unigrams but noton the keyphrases generated from the high-rankedunigrams.
For KeyCluster, we vary the numberof clusters produced by spectral clustering ratherthan the number of predicted keyphrases, againbecause KeyCluster does not impose a ranking onthe resulting keyphrases.
In addition, to give anestimate of how each system performs in terms ofF-score, we also plot curves corresponding to dif-ferent F-scores in these graphs.Tf-Idf Consistent with our intuition, the preci-sion of Tf-Idf drops as recall increases.
Althoughit is the simplest of the five approaches, Tf-Idf isthe best performing system on all but the Inspecdataset, where TextRank and KeyCluster beat Tf-Idf on just a few cases.
It clearly outperforms allother systems for NUS and ICSI.TextRank The TextRank curves show a differ-ent progression than Tf-Idf: precision does notdrop as much when recall increases.
For instance,in case of DUC and ICSI, precision is not sensi-tive to changes in recall.
Perhaps somewhat sur-prisingly, its precision increases with recall for In-spec, allowing it to even reach a point (towardsthe end of the curve) where it beats Tf-Idf.
Whileadditional experiments are needed to determinethe reason for this somewhat counter-intuitive re-sult, we speculate that this may be attributed tothe fact that the TextRank curves are generatedby progressively increasing T (i.e., the percent-age of top-ranked vertices/unigrams that are usedto generate keyphrases) rather than the number ofpredicted keyphrases, as mentioned before.
In-creasing T does not necessarily imply an increasein the number of predicted keyphrases, however.To see the reason, consider an example in whichwe want TextRank to extract the keyphrase ?ad-vanced machine learning?
for a given document.Assume that TextRank ranks the unigrams ?ad-vanced?, ?learning?, and ?machine?
first, second,and third, respectively in its ranking step.
WhenT = 2n , where n denotes the total number ofcandidate unigrams, only the two highest-rankedunigrams (i.e., ?advanced?
and ?learning?)
canbe used to form keyphrases.
This implies that?advanced?
and ?learning?
will each be predictedas a keyphrase, but ?advanced machine learning?will not.
However, when T = 3n , all three un-igrams can be used to form a keyphrase, andsince TextRank collapses unigrams adjacent toeach other in the text to form a keyphrase, it willcorrectly predict ?advanced machine learning?
asa keyphrase.
Note that as we increase T from 2nto 3n , recall increases, and so does precision, since369010203040500  20  40  60  80  100Precision(%)Recall (%)F=10F=20F=30F=40Tf-IdfTextRankSingleRankExpandRankKeyCluster(a) DUC010203040500  20  40  60  80  100Precision(%)Recall (%)F=10F=20F=30F=40Tf-IdfTextRankSingleRankExpandRankKeyCluster(b) Inspec02468100  20  40  60  80  100Precision(%)Recall (%)F=10Tf-IdfTextRankSingleRankExpandRankKeyCluster(c) NUS02468100  20  40  60  80  100Precision(%)Recall (%)F=10Tf-IdfTextRankSingleRankExpandRankKeyCluster(d) ICSIFigure 1: Precision-recall curves for all four datasets?advanced?
and ?learning?
are now combined toform one keyphrase (and hence the number of pre-dicted keyphrases decreases).
In other words, itis possible to see a simultaneous rise in precisionand recall in a TextRank curve.
A natural ques-tion is: why does is happen only for Inspec butnot the other datasets?
The reason could be at-tributed to the fact that Inspec is composed of ab-stracts: since the number of keyphrases that can begenerated from these short documents is relativelysmall, precision may not drop as severely as withthe other datasets even when all of the unigramsare used to form keyphrases.On average, TextRank performs much worsecompared to Tf-Idf.
The curves also prove Tex-tRank?s sensitivity to T on Inspec, but not on theother datasets.
This certainly gives more insightinto TextRank since it was evaluated on Inspeconly for T=33% by Mihalcea and Tarau (2004).SingleRank SingleRank, which is supposed tobe a simple variant of TextRank, surprisingly ex-hibits very different performance.
First, it showsa more intuitive nature: precision drops as recallincreases.
Second, SingleRank outperforms Tex-tRank by big margins on all the datasets.
Later,we will examine which of the differences betweenthem is responsible for the differing performance.370DUC Inspec NUS ICSIParameter F Parameter F Parameter F Parameter FTf-Idf N = 14 27.0 N = 14 36.3 N = 60 6.6 N = 9 12.1TextRank T = 100% 9.7 T = 100% 33.0 T = 5% 3.2 T = 25% 2.7SingleRank N = 16 25.6 N = 15 35.3 N = 190 3.8 N = 50 4.4ExpandRank N = 13 26.9 N = 15 35.3 N = 177 3.8 N = 51 4.3KeyCluster m = 0.9n 14.0 m = 0.9n 40.6 m = 0.25n 1.7 m = 0.9n 3.2Table 2: Best parameter settings.
N is the number of predicted keyphrases, T is the percentage of vertices selected askeywords in TextRank, m is the number of clusters in KeyCluster, expressed in terms of n, the fraction of candidate words.ExpandRank Consistent with Wan and Xiao(2008), ExpandRank beats SingleRank on DUCwhen a small number of phrases are predicted, buttheir difference diminishes as more phrases arepredicted.
On the other hand, their performanceis indistinguishable from each other on the otherthree datasets.
A natural question is: why doesExpandRank improve over SingleRank only forDUC but not for the other datasets?
To answerthis question, we look at the DUC articles andfind that in many cases, the 5-nearest neighborsof a document are on the same topic involving thesame entities as the document itself, presumablybecause many of these news articles are simplyupdated versions of an evolving event.
Conse-quently, the graph built from the neighboring doc-uments is helpful for predicting the keyphrases ofthe given document.
Such topic-wise similarityamong the nearest documents does not exist in theother datasets, however.KeyCluster As in TextRank, KeyCluster doesnot always yield a drop in precision as recall im-proves.
This, again, may be attributed to the factthat the KeyCluster curves are generated by vary-ing the number of clusters rather than the num-ber of predicted keyphrases, as well as the waykeyphrases are formed from the exemplars.
An-other reason is that the frequent Wikipedia uni-grams are excluded during post-processing, mak-ing KeyCluster more resistant to precision drops.Overall, KeyCluster performs slightly better thanTextRank on DUC and ICSI, yields the worst per-formance on NUS, and scores the best on Inspecwhen the number of clusters is high.
These resultsseem to suggest that KeyCluster works better ifmore clusters are used.Best parameter settings Table 2 shows for eachsystem the parameter values yielding the best F-score on each dataset.
Two points deserve men-tion.
First, in comparison to SingleRank andExpandRank, Tf-Idf outputs fewer keyphrases toachieve its best F-score on most datasets.
Second,the systems output more keyphrases on NUS thanon other datasets to achieve their best F-scores(e.g., 60 for Tf-Idf, 190 for SingleRank, and 177for ExpandRank).
This can be attributed in part tothe fact that the F-scores on NUS are low for allthe systems and exhibit only slight changes as weoutput more phrases.Our re-implementations Do our duplicatedsystems yield scores that match the originalscores?
Table 3 sheds light on this question.First, consider KeyCluster, where our scorelags behind the original score by approximately5%.
An examination of Liu et al?s (2009b) re-sults reveals a subtle caveat in keyphrase extrac-tion evaluations.
In Inspec, not all gold-standardkeyphrases appear in their associated document,and as a result, none of the five systems we con-sider in this paper can achieve a recall of 100.While Mihalcea and Tarau (2004) and our re-implementations use all of these gold-standardkeyphrases in our evaluation, Hulth (2003) andLiu et al address this issue by using as gold-standard keyphrases only those that appear in thecorresponding document when computing recall.2This explains why our KeyCluster score (38.9) islower than the original score (43.6).
If we fol-low Liu et al?s way of computing recall, our re-implementation score goes up to 42.4, which lagsbehind their score by only 1.2.Next, consider TextRank, where our score lagsbehind Mihalcea and Tarau?s original score bymore than 25 points.
We verified our implemen-tation against a publicly available implementation2As a result, Liu et al and Mihalcea and Tarau?s scoresare not directly comparable, but Liu et al did not point thisout while comparing scores in their paper.371Dataset F-scoreOriginal OursTf-Idf DUC 25.4 25.7TextRank Inspec 36.2 10.0SingleRank DUC 27.2 24.9ExpandRank DUC 31.7 26.4KeyCluster Inspec 43.6 38.9Table 3: Original vs. re-implementation scoresof TextRank3, and are confident that our imple-mentation is correct.
It is also worth mentioningthat using our re-implementation of SingleRank,we are able to match the best scores reported byMihalcea and Tarau (2004) on Inspec.We score 2 and 5 points less than Wan andXiao?s (2008) implementations of SingleRank andExpandRank, respectively.
We speculate that doc-ument pre-processing (e.g., stemming) has con-tributed to the discrepancy, but additional exper-iments are needed to determine the reason.SingleRank vs. TextRank Figure 1 shows thatSingleRank behaves very differently from Tex-tRank.
As mentioned in Section 3.2.3, the twoalgorithms differ in three major aspects.
To de-termine which aspect is chiefly responsible for thelarge difference in their performance, we conductthree ?ablation?
experiments.
Each experimentmodifies exactly one of these aspects in SingleR-ank so that it behaves like TextRank, effectivelyensuring that the two algorithms differ only in theremaining two aspects.
More specifically, in thethree experiments, we (1) change SingleRank?swindow size to 2, (2) build an unweighted graphfor SingleRank, and (3) incorporate TextRank?sway of forming keyphrases into SingleRank, re-spectively.
Figure 2 shows the resultant curvesalong with the SingleRank and TextRank curveson Inspec taken from Figure 1b.
As we can see,the way of forming phrases, rather than the win-dow size or the weight assignment method, hasthe largest impact on the scores.
In fact, after in-corporating TextRank?s way of forming phrases,SingleRank exhibits a remarkable drop in perfor-mance, yielding a curve that resembles the Tex-tRank curve.
Also note that SingleRank achievesbetter recall values than TextRank.
To see the rea-son, recall that TextRank requires that every wordof a gold keyphrase must appear among the top-3http://github.com/sharethis/textrank010203040500  20  40  60  80  100Precision(%)Recall (%)F=10F=20F=30F=40SingleRankSingleRank+Window size=2SingleRank+UnweightedSingleRank+TextRank phrasesTextRankFigure 2: Ablation results for SingleRank on In-specranked unigrams.
This is a fairly strict criterion,especially in comparison to SingleRank, whichdoes not require all unigrams of a gold keyphraseto be present in the top-ranked list.
We observesimilar trends for the other datasets.5 ConclusionsWe have conducted a systematic evaluation of fivestate-of-the-art unsupervised keyphrase extractionalgorithms on datasets from four different do-mains.
Several conclusions can be drawn fromour experimental results.
First, to fully under-stand the strengths and weaknesses of a keyphraseextractor, it is essential to evaluate it on multi-ple datasets.
In particular, evaluating it on a sin-gle dataset has proven inadequate, as good per-formance can sometimes be achieved due to cer-tain statistical characteristics of a dataset.
Sec-ond, as demonstrated by our experiments withTextRank and SingleRank, post-processing stepssuch as the way of forming keyphrases can havea large impact on the performance of a keyphraseextractor.
Hence, it may be worthwhile to investi-gate alternative methods for extracting candidatekeyphrases (e.g., Kumar and Srinathan (2008),You et al (2009)).
Finally, despite the largeamount of recent work on unsupervised keyphraseextractor, our results indicated that Tf-Idf remainsa strong baseline, offering very robust perfor-mance across different datasets.372AcknowledgmentsWe thank the three anonymous reviewers for theircomments.
Many thanks to Anette Hulth andYang Liu for providing us with the Inspec andICSI datasets; Rada Mihalcea, Paco Nathan, andXiaojun Wan for helping us understand their al-gorithms/implementations; and Peng Li for pro-viding us with the frequent word list that he andhis co-authors used in their paper.
This work wassupported in part by NSF Grant IIS-0812261.ReferencesBrin, Sergey and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual Web search engine.Computer Networks, 30(1?7):107?117.Frank, Eibe, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of the 16th International Joint Conference onArtificial Intelligence, pages 668?673.Hulth, Anette.
2003.
Improved automatic keywordextraction given more linguistic knowledge.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, pages216?223.Janin, Adam, Don Baron, Jane Edwards, Dan El-lis, David Gelbart, Nelson Morgan, Barbara Pe-skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stol-cke, and Chuck Wooters.
2003.
The ICSI meetingcorpus.
In Proceedings of 2003 IEEE Conferenceon Acoustics, Speech, and Signal Processing, pages364?367.Kumar, Niraj and Kannan Srinathan.
2008.
Automatickeyphrase extraction from scientific documents us-ing n-gram filtration technique.
In Proceedings ofthe Eighth ACM Symposium on Document Engi-neering, pages 199?208.Liu, Feifan, Deana Pennell, Fei Liu, and Yang Liu.2009a.
Unsupervised approaches for automatickeyword extraction using meeting transcripts.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 620?628.Liu, Zhiyuan, Peng Li, Yabin Zheng, and MaosongSun.
2009b.
Clustering to find exemplar terms forkeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 257?266.Matsuo, Yutaka and Mitsuru Ishizuka.
2004.
Key-word extraction from a single document using wordco-occurrence statistical information.
InternationalJournal on Artificial Intelligence Tools, 13(1):157?169.Medelyan, Olena, Eibe Frank, and Ian H. Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1318?1327.Mihalcea, Rada and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 404?411.Nguyen, Thuy Dung and Min-Yen Kan. 2007.Keyphrase extraction in scientific publications.
InProceedings of the International Conference onAsian Digital Libraries, pages 317?326.Over, Paul.
2001.
Introduction to DUC-2001: An in-trinsic evaluation of generic news text summariza-tion systems.
In Proceedings of the 2001 DocumentUnderstanding Conference.Tomokiyo, Takashi and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL Workshop on Multiword Ex-pressions.Toutanova, Kristina and Christopher D. Manning.2000.
Enriching the knowledge sources used in amaximum entropy part-of-speech tagger.
In Pro-ceedings of the 2000 Joint SIGDAT Conference onEmpirical Methods in Natural Language processingand Very Large Corpora, pages 63?70.Turney, Peter.
2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval,2:303?336.Turney, Peter.
2003.
Coherent keyphrase extractionvia web mining.
In Proceedings of the 18th Inter-national Joint Conference on Artificial Intelligence,pages 434?439.Wan, Xiaojun and Jianguo Xiao.
2008.
Singledocument keyphrase extraction using neighborhoodknowledge.
In Proceedings of the 23rd AAAI Con-ference on Artificial Intelligence, pages 855?860.Wan, Xiaojun, Jianwu Yang, and Jianguo Xiao.
2007.Towards an iterative reinforcement approach for si-multaneous document summarization and keywordextraction.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 552?559.You, Wei, Dominique Fontaine, and Jean-Paul Barthe`s.2009.
Automatic keyphrase extraction with a re-fined candidate set.
In Proceedings of the 2009IEEE/WIC/ACM International Joint Conference onWeb Intelligence and Intelligent Agent Technology,pages 576?579.Zha, Hongyuan.
2002.
Generic summarization andkeyphrase extraction using mutual reinforcementprinciple and sentence clustering.
In Proceedings ofthe 25th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 113?120.373
