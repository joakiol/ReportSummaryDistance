Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 117?122,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 14: Evaluation Setting for Word Sense Induction &Disambiguation SystemsSuresh ManandharDepartment of Computer ScienceUniversity of YorkYork, UK, YO10 5DDsuresh@cs.york.ac.ukIoannis P. KlapaftisDepartment of Computer ScienceUniversity of YorkYork, UK, YO10 5DDgiannis@cs.york.ac.ukAbstractThis paper presents the evaluation settingfor the SemEval-2010 Word Sense Induction(WSI) task.
The setting of the SemEval-2007WSI task consists of two evaluation schemes,i.e.
unsupervised evaluation and supervisedevaluation.
The first one evaluates WSI meth-ods in a similar fashion to Information Re-trieval exercises using F-Score.
However,F-Score suffers from the matching problemwhich does not allow: (1) the assessment ofthe entire membership of clusters, and (2) theevaluation of all clusters in a given solution.
Inthis paper, we present the use of V-measure asa measure of objectively assessing WSI meth-ods in an unsupervised setting, and we alsosuggest a small modification on the supervisedevaluation.1 IntroductionWSI is the task of identifying the different senses(uses) of a target word in a given text.
WSI is a fieldof significant value, because it aims to overcome thelimitations originated by representing word sensesas a fixed-list of dictionary definitions.
These lim-itations of hand-crafted lexicons include the use ofgeneral sense definitions, the lack of explicit seman-tic and topical relations between concepts (Agirre etal., 2001), and the inability to reflect the exact con-tent of the context in which a target word appears(Ve?ronis, 2004).Given the significance of WSI, the objective as-sessment and comparison of WSI methods is cru-cial.
The first effort to evaluate WSI methods un-der a common framework (evaluation schemes &dataset) was undertaken in the SemEval-2007 WSItask (SWSI) (Agirre and Soroa, 2007), where twoseparate evaluation schemes were employed.
Thefirst one, unsupervised evaluation, treats the WSI re-sults as clusters of target word contexts and GoldStandard (GS) senses as classes.
The traditionalclustering measure of F-Score (Zhao et al, 2005) isused to assess the performance of WSI systems.
Thesecond evaluation scheme, supervised evaluation,uses the training part of the dataset in order to mapthe automatically induced clusters to GS senses.
Inthe next step, the testing corpus is used to measurethe performance of systems in a Word Sense Disam-biguation (WSD) setting.A significant limitation of F-Score is that it doesnot evaluate the make up of clusters beyond themajority class (Rosenberg and Hirschberg, 2007).Moreover, F-Score might also fail to evaluate clus-ters which are not matched to any GS class dueto their small size.
These two limitations definethe matching problem of F-Score (Rosenberg andHirschberg, 2007) which can lead to: (1) identicalscores between different clustering solutions, and(2) inaccurate assessment of the clustering quality.The supervised evaluation scheme employs amethod in order to map the automatically inducedclusters to GS senses.
As a result, this process mightchange the distribution of clusters by mapping morethan one clusters to the same GS sense.
The out-come of this process might be more helpful for sys-tems that produce a large number of clusters.In this paper, we focus on analysing the SemEval-2007 WSI evaluation schemes showing their defi-ciencies.
Subsequently, we present the use of V-117measure (Rosenberg and Hirschberg, 2007) as anevaluation measure that can overcome the currentlimitations of F-Score.
Finally, we also suggesta small modification on the supervised evaluationscheme, which will possibly allow for a more reli-able estimation of WSD performance.
The proposedevaluation setting will be applied in the SemEval-2010 WSI task.2 SemEval-2007 WSI evaluation settingThe SemEval-2007 WSI task (Agirre and Soroa,2007) evaluates WSI systems on 35 nouns and 65verbs.
The corpus consists of texts of the Wall StreetJournal corpus, and is hand-tagged with OntoNotessenses (Hovy et al, 2006).
For each target word tw,the task consists of firstly identifying the senses oftw (e.g.
as clusters of target word instances, co-occurring words, etc.
), and secondly tagging the in-stances of the target word using the automaticallyinduced clusters.
In the next sections, we describeand review the two evaluation schemes.2.1 SWSI unsupervised evaluationLet us assume that given a target word tw, a WSImethod has produced 3 clusters which have tagged2100 instances of tw.
Table 1 shows the number oftagged instances for each cluster, as well as the com-mon instances between each cluster and each goldstandard sense.F-Score is used in a similar fashion to InformationRetrieval exercises.
Given a particular gold standardsense gsi of size ai and a cluster cj of size aj , sup-pose aij instances in the class gsi belong to cj .
Pre-cision of class gsi with respect to cluster cj is de-fined as the number of their common instances di-vided by the total cluster size, i.e.
P(gsi, cj) = aijaj .The recall of class gsi with respect to cluster cj isdefined as the number of their common instances di-vided by the total sense size, i.e.
R(gsi, cj) = aijai .The F-Score of gsi with respect to cj , F (gsi, cj), isthen defined as the harmonic mean of P (gsi, cj) andR(gsi, cj).The F-Score of class gsi, F (gsi), is the maximumF (gsi, cj) value attained at any cluster.
Finally, theF-Score of the entire clustering solution is definedas the weighted average of the F-Scores of each GSsense (Formula 1), where q is the number of GSsenses and N is the total number of target word in-gs1 gs2 gs3cl1 500 100 100cl2 100 500 100cl3 100 100 500Table 1: Clusters & GS senses matrix.stances.
If the clustering is identical to the originalclasses in the datasets, F-Score will be equal to one.In the example of Table 1, F-Score is equal to 0.714.F ?
Score =q?i=1|gsi|N F (gsi) (1)As it can be observed, F-Score assesses the qual-ity of a clustering solution by considering two dif-ferent angles, i.e.
homogeneity and completeness(Rosenberg and Hirschberg, 2007).
Homogeneityrefers to the degree that each cluster consists ofdata points, which primarily belong to a single GSclass.
On the other hand, completeness refers to thedegree that each GS class consists of data points,which have primarily been assigned to a single clus-ter.
A perfect homogeneity would result in a preci-sion equal to 1, while a perfect completeness wouldresult in a recall equal to 1.Purity and entropy (Zhao et al, 2005) are alsoused in SWSI as complementary measures.
How-ever, both of them evaluate only the homogeneity ofa clustering solution disregarding completeness.2.2 SWSI supervised evaluationIn supervised evaluation, the target word corpus issplit into a testing and a training part.
The trainingpart is used to map the automatically induced clus-ters to GS senses.
In the next step, the testing corpusis used to evaluate WSI methods in a WSD setting.Let us consider the example shown in Table 1 andassume that this matrix has been created by using thetraining part of our corpus.
Table 1 shows that cl1 ismore likely to be associated with gs1, cl2 is morelikely to be associated with gs2, and cl3 is morelikely to be associated with gs3.
This informationfrom the training part is utilised to map the clustersto GS senses.Particularly, the matrix shown in Table 1 is nor-malised to produce a matrix M , in which each en-try depicts the conditional probability P (gsi|clj).Given an instance I of tw from the testing cor-pus, a row cluster vector IC is created, in which118System F-Sc.
Pur.
Ent.
# Cl.
WSD1c1w-MFS 78.9 79.8 45.4 1 78.7UBC-AS 78.7 80.5 43.8 1.32 78.5upv si 66.3 83.8 33.2 5.57 79.1UMND2 66.1 81.7 40.5 1.36 80.6I2R 63.9 84.0 32.8 3.08 81.6UOY 56.1 86.1 27.1 9.28 77.71c1inst 9.5 100 0 139 N/ATable 2: SWSI Unsupervised & supervised evaluation.each entry k corresponds to the score assigned toclk to be the winning cluster for instance I .
Theproduct of IC and M provides a row sense vec-tor, IG, in which the highest scoring entry a de-notes that gsa is the winning sense for instance I .For example, if we produce the row cluster vector[cl1 = 0.8, cl2 = 0.1, cl3 = 0.1], and multiplyit with the normalised matrix of Table 1, then wewould get a row sense vector in which gs1 would bethe winning sense with a score equal to 0.6.2.3 SWSI results & discussionTable 2 shows the unsupervised and supervised per-formance of systems participating in SWSI.
As faras the baselines is concerned, the 1c1w baselinegroups all instances of a target word into a singlecluster, while the 1c1inst creates a new cluster foreach instance of a target word.
Note that the 1c1wbaseline is equivalent to the MFS in the supervisedevaluation.
As it can be observed, a system with lowentropy (high purity) does not necessarily achievehigh F-Score.
This is due to the fact that entropyand purity only measure the homogeneity of a clus-tering solution.
For that reason, the 1c1inst baselineachieves a perfect entropy and purity, although itsclustering solution is far from ideal.On the contrary, F-Score has a significant advan-tage over purity and entropy, since it measures bothhomogeneity (precision) and completeness (recall)of a clustering solution.
However, F-Score suffersfrom the matching problem, which manifests itselfeither by not evaluating the entire membership of acluster, or by not evaluating every cluster (Rosen-berg and Hirschberg, 2007).
The former situation ispresent, due to the fact that F-Score does not con-sider the make-up of the clusters beyond the major-ity class (Rosenberg and Hirschberg, 2007).
For ex-ample, in Table 3 the F-Score of the clustering so-gs1 gs2 gs3cl1 500 0 200cl2 200 500 0cl3 0 200 500Table 3: Clusters & GS senses matrix.lution is 0.714 and equal to the F-Score of the clus-tering solution shown in Table 1, although these aretwo significantly different clustering solutions.
Infact, the clustering shown in Table 3 should havea better homogeneity than the clustering shown inTable 1, since intuitively speaking each cluster con-tains fewer classes.
Moreover, the second clusteringshould also have a better completeness, since eachGS class contains fewer clusters.An additional instance of the matching problemmanifests itself, when F-Score fails to evaluate thequality of smaller clusters.
For example, if we addin Table 3 one more cluster (cl4), which only tags50 additional instances of gs1, then we will be ableto observe that this cluster will not be matched toany of the GS senses, since cl1 is matched to gs1.Although F-Score will decrease since the recall ofgs1 will decrease, the evaluation setting ignores theperfect homogeneity of this small cluster.In Table 2, we observe that no system managed tooutperform the 1c1w baseline in terms of F-Score.At the same time, some systems participating inSWSI were able to outperform the equivalent of the1c1w baseline (MFS) in the supervised evaluation.For example, UBC-AS achieved the best F-Scoreclose to the 1c1w baseline.
However, by looking atits supervised recall, we observe that it is below theMFS baseline.A clustering solution, which achieves high super-vised recall, does not necessarily achieve high F-Score.
One reason for that stems from the fact thatF-Score penalises systems for getting the number ofGS classes wrongly, as in 1c1inst baseline.
Accord-ing to Agirre & Soroa (2007), supervised evaluationseems to be more neutral regarding the number ofinduced clusters, because clusters are mapped into aweighted vector of senses, and therefore inducing anumber of clusters similar to the number of sensesis not a requirement for good results.However, a large number of clusters might alsolead to an unreliable mapping of clusters to GSsenses.
For example, high supervised recall also119means high purity and low entropy as in I2R, but notvice versa as in UOY.
UOY produces a large numberof clean clusters, in effect suffering from an unreli-able mapping of clusters to senses due to the lack ofadequate training data.Moreover, an additional supervised evaluation ofWSI methods using a different dataset split resultedin a different ranking, in which all of the systemsoutperformed the MFS baseline (Agirre and Soroa,2007).
This result indicates that the supervised eval-uation might not provide a reliable estimation ofWSD performance, particularly in the case wherethe mapping relies on a single dataset split.3 SemEval-2010 WSI evaluation setting3.1 Unsupervised evaluation using V-measureLet us assume that the dataset of a target word twcomprises of N instances (data points).
These datapoints are divided into two partitions, i.e.
a set of au-tomatically generated clusters C = {cj |j = 1 .
.
.
n}and a set of gold standard classes GS = {gsi|gs =1 .
.
.m}.
Moreover, let aij be the number of datapoints, which are members of class gsi and elementsof cluster cj .V-measure assesses the quality of a clustering so-lution by explicitly measuring its homogeneity andits completeness (Rosenberg and Hirschberg, 2007).Recall that homogeneity refers to the degree thateach cluster consists of data points which primar-ily belong to a single GS class.
V-measure assesseshomogeneity by examining the conditional entropyof the class distribution given the proposed cluster-ing, i.e.
H(GS|C).
H(GS|C) quantifies the re-maining entropy (uncertainty) of the class distribu-tion given that the proposed clustering is known.
Asa result, when H(GS|C) is 0, we have the perfectlyhomogeneous solution, since each cluster containsonly those data points that are members of a singleclass.
However in an imperfect situation, H(GS|C)depends on the size of the dataset and the distribu-tion of class sizes.
As a result, instead of taking theraw conditional entropy, V-measure normalises it bythe maximum reduction in entropy the clustering in-formation could provide, i.e.
H(GS).Formulas 2 and 3 define H(GS) and H(GS|C).When there is only a single class (H(GS) = 0), anyclustering would produce a perfectly homogeneoussolution.
In the worst case, the class distributionwithin each cluster is equal to the overall class dis-tribution (H(GS|C) = H(GS)), i.e.
clustering pro-vides no new information.
Overall, in accordancewith the convention of 1 being desirable and 0 unde-sirable, the homogeneity (h) of a clustering solutionis 1 if there is only a single class, and 1?
H(GS|C)H(GS) inany other case (Rosenberg and Hirschberg, 2007).H(GS) = ?|GS|?i=1?|C|j=1 aijN log?|C|j=1 aijN (2)H(GS|C) = ?|C|?j=1|GS|?i=1aijN logaij?|GS|k=1 akj(3)Symmetrically to homogeneity, completeness refersto the degree that each GS class consists of datapoints, which have primarily been assigned to a sin-gle cluster.
To evaluate completeness, V-measureexamines the distribution of cluster assignmentswithin each class.
The conditional entropy of thecluster given the class distribution, H(C|GS), quan-tifies the remaining entropy (uncertainty) of the clus-ter given that the class distribution is known.Consequently, when H(C|GS) is 0, we have theperfectly complete solution, since all the data pointsof a class belong to the same cluster.
Therefore,symmetrically to homogeneity, the completeness cof a clustering solution is 1 if there is only a sin-gle cluster (H(C) = 0), and 1 ?
H(C|GS)H(C) in anyother case.
In the worst case, completeness will beequal to 0, particularly when H(C|GS) is maxi-mal and equal to H(C).
This happens when eachGS class is included in all clusters with a distribu-tion equal to the distribution of sizes (Rosenberg andHirschberg, 2007).
Formulas 4 and 5 define H(C)and H(C|GS).
Finally h and c can be combined andproduce V-measure, which is the harmonic mean ofhomogeneity and completeness.H(C) = ?|C|?j=1?|GS|i=1 aijN log?|GS|i=1 aijN (4)H(C|GS) = ?|GS|?i=1|C|?j=1aijN logaij?|C|k=1 aik(5)Returning to our clustering example in Table 1, itsV-measure is equal to 0.275.
In section 2.3, wealso presented an additional clustering (Table 3),which had the same F-Score as the clustering in Ta-ble 1, despite the fact that it intuitively had a bet-ter completeness and homogeneity.
The V-measure120of the second clustering solution is equal to 0.45,and higher than the V-measure of the first cluster-ing.
This result shows that V-measure is able todiscriminate between these two clusterings by con-sidering the make-up of the clusters beyond the ma-jority class.
Furthermore, it is straightforward fromthe description in this section, that V-measure evalu-ates each cluster in terms of homogeneity and com-pleteness, unlike F-Score which relies on a post-hocmatching.3.2 V-measure results & discussionTable 4 shows the performance of SWSI partici-pating systems according to V-measure.
The lastfour columns of Table 4 show the weighted aver-age homogeneity and completeness for nouns andverbs.
Note that the homogeneity and complete-ness columns are weighted averages over all nounsor verbs, and are not used for the calculation ofthe weighted average V-measure (second column).The latter is calculated by measuring for each tar-get word?s clustering solution the harmonic mean ofhomogeneity and completeness separately, and thenproducing the weighted average.As it can be observed in Table 4, all WSI sys-tems have outperformed the random baseline whichmeans that they have learned useful information.Moreover, Table 4 shows that on average all sys-tems have outperformed the 1c1w baseline, whichgroups the instances of a target word to a single clus-ter.
The completeness of the 1c1w baseline is equalto 1 by definition, since all instances of GS classesare grouped to a single cluster.
However, this solu-tion is as inhomogeneous as possible and causes ahomogeneity equal to 0 in the case of nouns.
In theverb dataset however, some verbs appear with onlyone sense, in effect causing the 1c1w homogeneityto be equal to 1 in some cases, and the average V-measure greater than 0.In Table 4, we also observe that the 1c1inst base-line achieves a high performance.
In nouns only I2Ris able to outperform this baseline, while in verbs the1c1inst baseline achieves the highest result.
By thedefinition of homogeneity (section 3.1), this baselineis perfectly homogeneous, since each cluster con-tains one instance of a single sense.
However, itscompleteness is not 0, as one might intuitively ex-pect.
This is due to the fact that V-measure consid-ers as the worst solution in terms of completenessthe one, in which each class is represented by ev-ery cluster, and specifically with a distribution equalto the distribution of cluster sizes (Rosenberg andHirschberg, 2007).
This worst solution is not equiv-alent to the 1c1inst, hence completeness of 1c1instis greater than 0.
Additionally, completeness of thisbaseline benefits from the fact that around 18% ofGS senses have only one instance in the test set.Note however, that on average this baseline achievesa lower completeness than most of the systems.Another observation from Table 4 is that upv siand UOY have a better ranking than in Table 2.
Notethat these systems have generated a higher numberof clusters than the GS number of senses.
In verbsUOY has been extensively penalised by the F-Score.The inspection of their answers shows that both sys-tems generate highly skewed distributions, in whicha small number of clusters tag the majority of in-stances, while a larger number tag only a few.
Asmentioned in sections 2.1 and 2.3, these small clus-ters might not be matched to any GS sense, hencethey will decrease the unsupervised recall of a GSclass, and consequently the F-Score.
However, theirhigh homogeneity is not considered in the calcula-tion of F-Score.
On the contrary, V-measure is ableto evaluate the quality of these small clusters, andprovide a more objective assessment.Finally, in our evaluation we observe that I2Rhas on average the highest performance among theSWSI methods.
This is due to its high V-measure innouns, but not in verbs.
Particularly in nouns, I2Rachieves a consistent performance in terms of ho-mogeneity and completeness without being biasedtowards one of them, as is the case for the rest ofthe systems.
For example, UOY and upv si achieveon average the highest homogeneity (42.5 & 32.8resp.)
and the worst completeness (11.5 & 13.2resp.).
The opposite picture is present for UBC-ASand UMND2.
Despite that, UBC-AS and UMND2perform better than I2R in verbs, due to the smallnumber of generated clusters (high completeness),and a reasonable homogeneity mainly due to the ex-istence of verbs with one GS sense.3.3 Modified supervised WSI evaluationIn section 2.3, we mentioned that supervised eval-uation might favor methods which produce many121System V-measure Homogeneity CompletenessTotal Nouns Verbs Nouns Verbs Nouns Verbs1c1inst 21.6 19.2 24.3 100.0 100.0 11.3 15.8I2R 16.5 22.3 10.1 31.6 27.3 20.0 10.0UOY 15.6 17.2 13.9 38.9 46.6 12.0 11.1upv si 15.3 18.2 11.9 37.1 28.0 14.5 11.8UMND2 12.1 12.0 12.2 18.1 15.3 55.8 63.6UBC-AS 7.8 3.7 12.4 4.0 13.7 90.6 93.0Rand 7.2 4.9 9.7 12.0 30.0 14.1 14.31c1w 6.3 0.0 13.4 0.0 13.4 100.0 100.0Table 4: V-Measure, homogeneity and completeness of SemEval-2007 WSI systems.
The range of V-measure, homo-geneity & completeness is 0-100.clusters, since the mapping step can artificially in-crease completeness.
Furthermore, we have shownthat generating a large number of clusters might leadto an unreliable mapping of clusters to GS sensesdue to the lack of adequate training data.Despite that, the supervised evaluation can beconsidered as an application-oriented evaluation,since it allows the transformation of unsupervisedWSI systems to semi-supervised WSD ones.
Giventhe great difficulty of unsupervised WSD systems tooutperform the MFS baseline as well as the SWSIresults, which show that some systems outperformthe MFS by a significant amount in nouns, we be-lieve that this evaluation scheme should be used tocompare against supervised WSD methods.In section 2.3, we also mentioned that the super-vised evaluation on two different test/train splits pro-vided a different ranking of methods, and more im-portantly a different ranking with regard to the MFS.To deal with that problem, we believe that it wouldbe reasonable to perform k-fold cross validation inorder to collect statistically significant information.4 ConclusionWe presented and discussed the limitations of theSemEval-2007 evaluation setting for WSI methods.Based on our discussion, we described the use ofV-measure as the measure of assessing WSI perfor-mance on an unsupervised setting, and presented theresults of SWSI WSI methods.
We have also sug-gested a small modification on the supervised eval-uation scheme, which will allow for a more reliableestimation of WSD performance.
The new evalu-ation setting will be applied in the SemEval-2010WSI task.AcknowledgementsThis work is supported by the European Commis-sion via the EU FP7 INDECT project, Grant No.218086, Research area: SEC-2007-1.2-01 Intelli-gent Urban Environment Observation System.
Theauthors would like to thank the anonymous review-ers for their useful comments.ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007task 02: Evaluating word sense induction and discrim-ination systems.
In Proceedings of the 4rth Interna-tional Workshop on Semantic Evaluations, pages 7?12,Prague, Czech Republic, June.
ACL.Eneko Agirre, Olatz Ansa, David Martinez, and EduardHovy.
2001.
Enriching wordnet concepts with topicsignatures.
In Proceedings of the NAACL workshop onWordNet and Other Lexical Resources: Applications,Extensions and Customizations.
ACL.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the HumanLanguage Technology / North American Associationfor Computational Linguistics conference, New York,USA.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clusterevaluation measure.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 410?420.Jean Ve?ronis.
2004.
Hyperlex: lexical cartography forinformation retrieval.
Computer Speech & Language,18(3):223?252.Ying Zhao, George Karypis, and Usam Fayyad.2005.
Hierarchical clustering algorithms for docu-ment datasets.
Data Mining and Knowledge Discov-ery, 10(2):141?168.122
