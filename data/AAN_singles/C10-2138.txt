Coling 2010: Poster Volume, pages 1203?1210,Beijing, August 2010Utilizing Variability of Time and Term Content, within and acrossUsers in Session DetectionShuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao11Harbin Institute of Technology, 2Heilongjiang Institute of Technology{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cnhaoliang.qi@gmail.comAbstractIn this paper, we describe a SVM classi-fication framework of session detectiontask on both Chinese and English querylogs.
With eight features on the aspectsof temporal and content information ex-tracted from pairs of successive queries,the classification models achieve signifi-cantly superior performance than the stat-of-the-art method.
Additionally, we findthrough ROC analysis that there existsgreat discrimination power variabilityamong different features and within thesame feature across different users.
Tofully utilize this variability, we build lo-cal models for individual users and com-bine their predictions with those from theglobal model.
Experiments show that thelocal models do make significant im-provements to the global model, althoughthe amount is small.1 IntroductionTo provide users better experiences of searchengines, inspecting users?
activities and inferringusers?
interests are indispensible.
Query logs rec-orded by search engines serves well for thesepurposes.
Query log conveys the user interestinformation in the form of slices of the querystream.
Thus the task of session detection con-sists in distinguishing slice that corresponds to auser interest from other ones, and thus this paper,we adopt the definition of a session following(Jansen et al, 2007):(A session is) a series of interactions by the us-er toward addressing a single information need.This definition is equivalent to that of the?search goal?
proposed by Jones and Klinkner(2008), which corresponds to an atomic infor-mation need, resulting in one or more queries.This paper adopts a classification point ofview to the task of session detection (Jones andKlinkner, 2008).
Given a pair of successive que-ries in a query log, we examine it in variousviewpoints (i.e.
features) such as time proximityand similarity of the content of the two queries todetermine whether these two queries cross a bor-der of a search session.
In other words, we classi-fy the gap between the two queries into two clas-ses: session shift and session continuation.
Inpractice, search goals in a search mission anddifferent search missions could be intermingled,and increase the difficulty of correctly identify-ing them.
In this paper, we do not take this issueinto account and simply treat all boundaries be-tween intermingled search goals as session shifts.The chief advantage in this choice is that we willhave the opportunity to make classification mod-el working online without caching user?s queriesthat are pending to be assigned to a session.Various studies built accurate models in pre-dicting session boundaries and in distinguishingintermingled sessions, and they are summarizedin Section 2.
However, none of these works ana-lyzed the contribution of individual features froma user-oriented viewpoint, or evaluated a fea-ture?s discrimination power in a general scenarioindependent of its usage, as this paper does byconducting ROC analyses.
During these analyses,we found that the discrimination power of fea-tures varies dramatically, and for different users,the discrimination power of a particular featurealso does not remain constant.Thus, it is appealing to build local models forusers with have sufficient size of training exam-ples, and combine the local models?
predictionswith those made by the global model trained bythe whole training data.
However, few of previ-1203ous works build user-specific models for the sakeof characterizing the variability in user?s searchactivities, except that of Murray et al (2006).
Tofully make use of these two aspects of variability,inspired by Murray et al, we build users?
localmodels based on a much broader range of evi-dences, and show that different local models varyto a great extent, and experiments show that thelocal models do make significant improvementsto the global model, although the amount is small.The remainder of this paper is organized asfollows: Section 2 summarizes the related workof the session detection task.
In Section 3, wefirst describe our classification framework aswell as the features utilized.
Then we conductvarious evaluations on both English and Chinesequery logs.
Section 4 introduces the approachesto building local models based on an analysis ofthe variability of the discrimination power offeatures, and combine predictions of local mod-els with those of the global model.
Section 5 dis-cusses the experimental results and concludesthis paper.2 Related WorkThe simplest method in session detection isdefining a timeout threshold and marking anytime gaps of successive queries that exceed thethreshold as session shifts.
The thresholdsadopted in different studies were significantlydifferent, ranging from 5 minutes to 30 minutes(Silverstein et al, 1999; He and G?ker, 2000;Radlinski and Joachims, 2005; Downey et al,2007).
Other study suggested adopting a dynamictimeout threshold.
Murray et al (2006) proposeda user-centered hierarchical agglomerativeclustering algorithm to determine timeoutthreshold for each user dynamically, other thansetting a fixed threshold.
However, Jones andKlinkner (2008) pointed out that single timeoutcriterion is always of limited utility, whatever itslength is, and incorporating timeout features withother various features achieved satisfactoryclassification accuracy.An effective approach to combining the timeout features with various evidences for sessiondetection is machine learning.
He et al (2002)collected statistical information from human an-notated query logs to predict the probability a?New?
pattern indicates a session shift accordingto the time gap between successive queries.
?zmutlu and colleagues re-examined He et al?swork, and explored other machine learning tech-niques such as neural networks, multiple linearregression, Monte Carlo simulation, conditionalprobabilities (Gayo-Avello, 2009), and HMMs(?zmutlu, 2009).In recent studies, Jones and Klinkner (2008)built logistic regression models to identify searchgoals and missions, and tackled the intermingledsearch goal/mission issue by examining arbitrarypairs of queries in the query log.
Another contri-bution of Jones and Klinkner is that they made athorough analysis of contributions of individualfeatures.
However, they explored the features?contributions from a feature selection point ofview rather than from a user-oriented one, andthus failed to characterize the variability of thediscrimination power of the features when ap-plied to different users.3 Learning to Detect Session Shifts3.1 Feature ExtractionWe adopt eight features covering both the tem-poral and the content aspect of pairs of succes-sive queries.
Most these features are commonlyused by previous studies (He and G?ker, 2000;?zmutlu, 2006; Jones and Klinkner, 2008).However, in this paper, we will analyze theircontributions to the resulted model in a quite dif-ferent way from that in previous works.Let Q = (q1, q2, ?
, qn) denote a query log.The features are extracted from every successivepair of queries (qi, qi+1).
Table 1 summarizes thefeatures we adopt.
The normalization describedin Table1 is done according to the type of thefeature.
Features describing characters are nor-malized by the average length of the two queries,while those describing character-n-grams arenormalized by the average size of the n-gram setsof the two queries.
Character-n-grams (e.g.
bi-grams ?ca?
and ?at?
in ?cat?)
are robust to dif-ferent representations of the same topic (e.g.
?IR?as Information Retrieval) and typos (e.g.?speling?
as ?spelling?
), and serve as a simplestemming method.
In practice, character-n-gramsare accumulative, which means they consist ofall m-grams with m ?
n.The feature ?avg_ngram_distance?, a variantof the ?lexical distance?
in (Gayo-Avello, 2009),is more complicated than to be described briefly.1204Here we first define n-gram distance (ND) fromqi to qj, which is formalized as follows:jjiji nnNDqin   gram--char.
of #qin occur   qin   gram--char.
of #1)qq( ?=?Note that character-n-grams are accumulativeand there could be multiple occurrences of acharacter-n-gram in a query, so the number of acharacter-n-gram is the sum of that of all m-grams with m ?
n, and multiple occurrences areall considered.
At last, the average of character-n-gram distance (ACD) of the pair (qi, qi+1) is:2)qq()qq()q,q( 111iiiiiiNDNDACD?+?=+++There are seven features describing the contentaspect of a query pair, and they are more or lessoverlapped (e.g.
edit_distance vs. common_char).However, we show in the next subsection that allthese features are beneficial to the final perfor-mance.Feature Descriptiontime_interval time interval betweensuccessive queriesavg_ngram_distanceavg.
of character-n-gramdistancesedit_disance normalized Levenshteinedit distancecommon_prefix normalized length of pre-fix sharedcommon_suffix normalized length of suf-fix sharedcommon_char normalized number ofcharacters sharedcommon_ngram normalized number ofcharacter-n-grams sharedJaccard_ngram Jaccard distance betweencharacter-n-gram setsTable 1.
Features used in classification models3.2 Data PreparationThe query logs we explored include an Englishsearch log tracked by AOL from Mar 1, 2006 toMay, 31 2006 (Pass et al, 2006), and a Chinesesearch log tracked by Sogou.com, which is oneof the major Chinese Search Engines, from Mar1, 2007 to Mar 31, 20071.
We applied systematicsampling over the user space on the two logs,which yielded 223 users and 2809 users, corre-sponding to 6407 and 6917 query instances re-1 http://www.sogou.com/labs/resources.htmlspectively2.
Sampling over the user space insteadof over the query space avoids the bias to themost active users who submit much more queriesthan average users.For each sampled dataset, we invited annota-tors who are familiar with IR and search processto determine each pair of successive queries ofinterest is across the border of a session.
Wemade trivial pre-split process under two rules:Queries from different users are not in thesame session.Queries from different days are not in thesame session.Table 2 shows some basic statistics of the an-notated data set.
During the annotation process,the annotators were guided to identify the user?sinformation need at the finest granularity everpossible, because we focus on the atomic infor-mation needs as described in Section 1.
Conse-quently, the average numbers of queries in a ses-sion in both query logs are lower than previousstudies.AOL log Sogou logQueries 6407 6917Sessions 4571 5726Queries per session 1.40 1.21Longest session 21 12Table 2.
Summary of the annotation results inboth query logs3.3 Learning FrameworkIn this section we seek to build accurate globalclassification model based on the whole trainingdata obtained in the previous sub-subsection forboth the query logs.
We built the models withinSVM framework.
The implementation of SVMwe used is libSVM (Chang and Lin, 2001).
Forthe sake of evaluations and of model integrationin the next section, we set the prediction of SVMto be probability estimation of the test examplebeing positive.
All features were pre-scaled into[0, 1] interval.
We adopted the polynomial kernel,and for both datasets, we exhaustively tried eachof the subset of the eight features using 5-foldcross validation.
We found that using all theeight features yielded the best classification ac-curacy.
Thus in the experiments in rest of this2 The sampling schema and sample size was deter-mined following (Gayo-Avello, 2009).1205section and the next section, we adopt the entirefeature set to build global classification models.There is one parameter to be determined forfeature extraction: the length of character-n-grams.
The proper lengths on AOL log andSogou log are different.
We tried the length from1 to 9, and according to cross validation accuracy,we found the best lengths for the two logs as 6and 3 respectively.3.4 Experimental Results3.4.1 Baseline MethodsWe provide two base line methods for compari-sons.
The first method is the commonly usedtimeout methods.
We tried different timeoutthresholds from 5 minutes to 30 minutes with astep of 5 minutes, and found that for both querylogs the 5 minutes?
threshold yield the best over-all performance.The second method achieved the best perfor-mance on the AOL log (Gayo-Avello, 2009),which addresses the session detection problemusing a geometric interpolation method, in com-parison to previous studies on this query log.
Were-implemented this method and evaluated it onboth the datasets.
Similarly, the best parametersfor the two query logs are different, such as thelength of a character-n-gram.
We only report theperformance with the best parameter settings.3.4.2 Analyzing the PerformanceWe analyze the performance of the SVM modelsaccording to precision, recall, F1-mean and F1.5-mean of predictions on session shift and continu-ation against human annotation data.The F-mean is defined as:RPPR++=22)1(mean-F ??
?where P denotes precision and R denotes recall.He et al (2002) regards recall more importantthan precision, and set the value of   in F-meanto 1.5.
We also report performance under thismeasure.In addition to traditional precision / recallbased measures, we also perform ROC (ReceiverOperating Characteristic) analysis to determinethe discrimination power of different methods.The best merit of ROC analysis is that given areference set, which is usually the human annota-tion results, it evaluates a set of indicator?s dis-crimination power for arbitrary binary classifica-tion problem independent of the critical valuewith which the class predictions are made.Specifically, in the context session detection,regardless of the critical value that splits the clas-sifier outputs into positive ones and negativeones (e.g.
the 5-minutes?
timeout threshold and50% probability in SVM?s output), the ROCanalysis provides the overall discrimination pow-er evaluation of the output set of a certain meth-od (by trying to set each output value as the criti-cal value).
For the baseline method by Gayo-Avello, the core of the decision heuristics alsohad a critical value to be determined.
For details,readers could refer to (Gayo-Avello, 2009).3.4.3 Precision, Recall, and F-meansBefore we examine the discrimination power ofeach session detection method?s output independ-ent of the threshold value selected.
In this sub-subsection, we begin with a more traditional eval-uation schema: setting a proper threshold to pro-duce binary predictions.
It is straightforward to setthe threshold for SVM method to 50%, and asdescribed in sub-subsection 3.1.1, the thresholdfor timeout method is 5 minutes.
The threshold ofGayo-Avello?s method is implied in its heuristics.Table 3 and Table 4 show the experimental re-sults on AOL log and Sogou log respectively.For each dataset, we performed 1000-times boot-strap resampling, generating 1000 bootstrappeddatasets with the same size as the original dataset.To test the statistical significance of performancedifferences, we adopted Wilcoxon signed-ranktest on the performance measures computed fromthe 1000 bootstrapped dataset, and found com-parisons between each pair of methods were allsignificant at 95% level.The results show that SVM method clearlyoutperforms the baseline methods, and timeoutmethod performs poorly.
It may be argued thatthe poor performance of timeout method is dueto the improper threshold value chosen.
In thiscase, the ROC analysis, which assesses the dis-crimination power of a method?s output set inde-pendent of the threshold value chosen, is moresuitable for performance evaluation.Gayo-Avello method significantly outperformsthe timeout method.
But due to its heuristic na-ture, it is less likely to do better than the super-vised-learning methods, although it avoids theover fitting issue.
The Gayo-Avello method?sunstable performance in predicting session con-1206tinuations implies that its heuristics did not gen-eralize well to Chinese query logs.Timeout Gayo-Avello SVMPshift 75.92 89.35 90.96cont.
63.05 85.32 92.06Rshift 64.49 87.85 93.82cont.
74.77 87.08 88.50F1shift 69.74 88.60 92.37cont.
68.41 86.19 90.25F1.5shift 67.62 88.31 92.92cont.
70.72 86.53 89.57Table 3.
Precision (P), recall (R), F1-mean (F1),and F1.5-mean (F1.5) of SVM method and the twobaseline methods on AOL dataset.Timeout Gayo-Avello SVMPshift 67.75 75.10 87.53cont.
52.82 83.51 81.62Rshift 59.52 91.44 86.17cont.
61.53 58.84 83.33F1shift 63.37 82.47 86.85cont.
56.84 69.04 82.47F1.5shift 61.83 85.71 86.59cont.
58.56 64.72 82.80Table 4.
Precision (P), recall (R), F1-mean (F1),and F1.5-mean (F1.5) of SVM method and the twobaseline methods on Sogou dataset.3.4.4 ROC AnalysisBy setting certain threshold value, we analyzedthe three method?s performance using precision /recall based measures.
In this sub-subsection, wetry to set each value in an output set as thethreshold value, and evaluate the discriminationpower of methods by the area under the ROCcurve.Figure 1 shows the ROC curves of the SVMmethod and the two baseline methods: timeoutand Gayo-Avello, for predicting session shifts.ROC curves for predicting session continuationsare symmetric with respect to the reference line,so we omit them in the rest of this paper for thesake of space limit.The results show that SVM method clearlyoutperforms the baseline methods in the prospec-tive of discrimination power, with ROC area0.9562 on AOL dataset and 0.9154 on Sogoudataset.
The curves of the two baseline methodsare clearly under that of SVM method.
Thismeans baseline methods can never achieve accu-racy as high as SVM method w.r.t.
a fixed falsealarm (classification error) rate, nor false alarmrate as low as SVM method w.r.t.
a fixed accura-cy rate.
Again, Gayo-Avello method significantlyoutperforms timeout method, while underper-forms the SVM method.
For the question in theprevious sub-subsection, coinciding with previ-ous studies (Murray et al, 2006; Jones andKlinkner, 2008), applying single timeout thresh-old always yields limited discrimination power,wherever the operating point on ROC curve (i.e.threshold value) is set.4 Making Use of the Variability of Dis-crimination PowerIn this section, we first analyze the amount ofcontribution that each feature makes and showthat the contribution, i.e.
the discrimination pow-er of each feature varies dramatically across dif-ferent users.
Then, we propose an approach tomaking use of this variability.
Finally throughexperimental results, we show that the proposedapproach makes small, yet significant improve-ments to the SVM method in Section 3.4.1 Variability of Discrimination PowerThe ROC analysis of individual feature providesadequate characterizations of the discriminationpower of the feature.
Another advantage ofadopting ROC analysis is that the results are in-dependent not only of the critical value, but alsoof the scale of the feature values.Figure 2 shows the ROC curves of all the eightfeatures in both datasets.
Note that some featuresare with a higher value indicating session contin-uation rather than session shift, so their ROCcurves are below the reference line.
The feature?time_interval?
behaves exactly the same as thetimeout method in Figure 1.
For the rest of thefeatures, ?avg_ngram_distance?, ?common_ngram?and ?Jaccard_ngram?
achieve the best discrimi-nation powers, showing the character-n-gramrepresentation is effective.
The feature ?com-mon_char?
performs significantly better inSogou dataset than in AOL dataset, because Chi-nese characters convey much more informationthan English characters do.
?common_suffix?performing worse than ?common_prefix?
reflectsthe custom of users.
Users tend to add terms atthe end of the query in a searching iteration, thuspredicting session continuations by examiningthe common suffixes is problematic.12070.000.250.500.751.00Sensitivity0.00 0.25 0.50 0.75 1.001-SpecificityTimeout ROC area: 0.7707Gayo-Avello ROC area: 0.9130SVM ROC area: 0.9562ReferenceAOL0.000.250.501.000.75Sensitivity0.00 0.25 0.50 0.75 1.001-SpecificityTimeout ROC area: 0.6365Gayo-Avello ROC area: 0.8463SVM ROC area: 0.9154ReferenceSogouFigure 1.
ROC analysis of SVM method and two baseline methods for predicting session shifts onboth AOL and Sogou dataset.
All comparisons between ROC areas within the same dataset are atleast 95% statistically significant, because the corresponding confidence intervals do not overlap.0.000.250.500.751.00Sensitivity0.00 0.25 0.50 0.75 1.001-Specificitytime_interval ROC area: 0.7707avg_ngram_distance ROC area: 0.9560edit_disance ROC area: 0.8848common_prefix ROC area: 0.2177common_suffix ROC area: 0.2985common_char ROC area: 0.1360common_ngram ROC area: 0.0480Jaccard_ngram ROC area: 0.0464ReferenceAOL0.000.250.500.751.00Sensitivity0.00 0.25 0.50 0.75 1.001-Specificitytime_interval ROC area: 0.6365avg_ngram_distance ROC area: 0.9108edit_disance ROC area: 0.8333common_prefix ROC area: 0.2449common_suffix ROC area: 0.3745common_char ROC area: 0.0922common_ngram ROC area: 0.1018Jaccard_ngram ROC area: 0.0965ReferenceSogouFigure 2.
ROC analysis of individual features for predicting session shifts on both AOL and Sogoudataset.
Note that some curves with similar ROC area values overlap each other.In spite of the discrimination power a featurehas, its behavior on different users is worth-while to be examined.
For selecting users thathave sufficient data to draw stable conclusions,we consider only users who issued more than 50queries in the datasets.
Unfortunately, there aretoo few users (6 users) qualified in Sogou da-taset, so we show only the statistics of ROCarea values of each of the features in Table 5based on 37 users in AOL dataset.The statistics in Table 5 show that for differ-ent users.
Recall that in sub-subsection 3.3.2, a0.04 difference of ROC area make the perfor-mance of the SVM method significantly better1208than that of the Gayo-Avello?s method.
Thus,the discrimination power of a feature is likely tovary significantly, because all the standard de-viations are at 0.03 or even higher level.
Espe-cially, the minimum and maximum values showthat for these users, some of the findings abovefrom the whole dataset do not hold.
This impliesthat it is likely more feasible to build specificlocal models for these users to make full use ofthe variability within the same feature.Feature avg.
sdev.
min.
max.time_interval 0.780 0.088 0.476 0.912avg_ngram_distance0.954 0.034 0.861 1.000edit_disance 0.883 0.056 0.733 0.990common_prefix 0.224 0.069 0.099 0.327common_suffix 0.299 0.113 0.064 0.578common_char 0.143 0.082 0.037 0.493common_ngram 0.051 0.037 0.000 0.187Jaccard_ngram 0.049 0.036 0.000 0.173Table 5.
Average, standard deviation, minimum,and maximum ROC areas of individual features4.2 Building Local ModelsWe built individual local models for each userthat issued more than 50 queries in AOL dataset.We also performed 5-fold cross validations andset the prediction to be the probability estima-tion of a test example being positive.
The fea-ture selection process showed again that all theeight features are beneficial, and none of themshould be excluded.In each fold of cross validation, we per-formed 90%-bagging on the training set 10times to get the variance estimations of the localmodel.
For each example in the test set, we setthe final output on it to be the average of the 10outputs, and recorded the standard deviation ofthe outputs on this example which is used dur-ing the model combination.
We also conductedthe same process for the global model for thesake of combination process described below.4.3 Combing with the Global ModelSince the predictions of both the local and theglobal models are probability estimations, it isreasonable to combine them using linear combi-nation.
For each example, there are two outputsOl and Og coming from local and global modelsaccordingly.
For each example e of a user?s subdataset U, we have the outputs Ol(e) and Og(e)as well as the normalized deviations Dl(e) andDg(e) (by the largest deviation in U of the corre-sponding models).
The final output O(e) is de-fined as:)()()()()()()(eDeDeOeDeOeDeOglglgl+?+?=Global Local CombinePshift 90.48 88.53 90.43cont.
91.75 92.12 92.52Rshift 93.94 94.44 94.56cont.
87.20 84.16 87.04F1shift 92.18 91.39 92.45cont.
89.41 87.96 89.69F1.5shift 92.85 92.54 93.25cont.
88.55 86.46 88.65Table 6.
Precision (P), recall (R), F1-mean (F1),and F1.5-mean (F1.5) of global model (bagging),local model (bagging) and combined modelThis combination process is similar to (Osl etal., 2008).
Note that the more the deviation of amodel is, the less feasible the correspondingmodel is.
We compared the performance ofthree models: global model, local model, andcombined model.
The results are summarized inTable 6.
All comparisons between differentmodels are statistically significant at 95% level,based on the same bootstrapping settings in sub-subsection 3.4.3.
The combined model showsslight (may due to the inferior performance ofthe local model), yet significant improvement tothe global model.
In spite of the amount of theimprovement, the local model did correct someerrors of the global model.
It may be not ac-ceptable to build such an expensive combinedmodel for a limited improvement.
Nevertheless,the results do show that the variability acrossdifferent users is exploitable.5 Discussion and ConclusionIn this paper, we built a learning framework ofdetecting sessions which corresponds to user?sinterest in a query log.
We considered two as-pect of a pair of successive queries: temporalaspect and content aspect, and designed eightfeatures based on these two aspects, and theSVM models built with these features achievedsatisfactory performance (92.37% F1-mean onsession shift, 90.25% F1-mean on session con-tinuation), significantly better than the best-everapproach on AOL query log.1209The analysis of the features?
discriminationpower was conducted not only among differentfeatures, but also within the same feature whenapplied to different users in the query log.
Byanalyzing the statistics of ROC area values ofeach of the features based on 37 users in AOLdataset, experimental results showed that thereis considerable variability in both these aspects.To make full use of this variability, we builtlocal models for individual user and combinethe yielded predictions with those yielded by theglobal model.
Experiments showed that the lo-cal model did make significant improvements tothe global model, although the amount wassmall (92.45% vs. 92.18% F1-mean on sessionshift, 89.69% vs. 89.41% F1-mean on sessioncontinuation).In future studies, we will explore other learn-ing frameworks which better integrate the localmodel and the global model, and will try to ac-quire more data to build local models.
We willalso analyze more deeply the characteristics ofROC analysis in the feature selection process.AcknowledgementThis work is supported by the Key Project ofNatural Science Foundation of China (GrantNo.60736044), and National 863 Project (GrantNo.2006AA010108).
The authors are gratefulfor the anonymous reviewers for their valuablecomments.ReferencesChang Chih-Chung and Chih-Jen Lin.
2001.LIBSVM : a library for support vector machines.Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvmDowney Doug, Susan Dumais, and Eric Horvitz.2007.
Models of searching and brows-ing: languages, studies, and applications.
In Pro-ceedings of the 20th international joint conferenceon Artificial intelligence, pages 2740-2747, Hy-derabad, India.Gayo-Avello Daniel.
2009.
A survey on session de-tection methods in query logs and a proposal forfuture evaluation, Information Science179(12):1822-1843.He Daqing and Ayse G?ker.
2000.
Detecting SessionBoundaries from Web User Logs.
In BCS/IRSG22nd Annual Colloqui-um on Information Re-trieval Research, pages 57-66.He Daqing, Ayse G?ke, and David J. Harper.
2002.Combining evidence for automatic web sessionidentification.
Information Processing and Man-agement: an International Journal, 38(5):727-742.Jansen Bernard J., Amanda Spink, Chris Blakely,and Sherry Koshman.
2007.
Defining a session onWeb search engines: Research Articles.
Journal ofthe American Society for Information Science andTechnology, 58(6):862-871Jones Rosie and Kristina Lisa Klinkner.
2008.
Be-yond the session timeout: automatic hierarchicalsegmentation of search topics in query logs.
InProceedings of the 17th ACM conference on In-formation and knowledge management, pages699-708, Napa Valley, California, USA.Murray G. Craig, Jimmy Lin, and Abdur Chowdhury.2007.
Identification of user sessions with hierar-chical agglomerative clustering.
American Societyfor Information Science and Technology, 43(1):1-9.Osl Melanie, Christian Baumgartner, Bernhard Tilg,and Stephan Dreiseitl.
2008.
On the combinationof logistic regression and local probability esti-mates.
In Proceedings of Third International Con-ference on Broadband Communications, Infor-mation Technology & Biomedical Applications,pages 124-128.?zmutlu Seda.
2006.
Automatic new topic identifi-cation using multiple linear regression.
Infor-mation Processing and Management: an Interna-tional Journal, 42(4):934-950.?zmutlu Huseyin C. 2009.
Markovian analysis forautomatic new topic identification in search en-gine transaction logs.
Applied Stochastic Modelsin Business and Industry, 25(6):737-768.Pass Greg, Abdur Chowdhury, and Cayley Torgeson.2006.
A picture of search.
In Proceedings of the1st international conference on Scalable infor-mation systems, Hong Kong.Radlinski Filip and Thorsten Joachims.
2005.
Querychains: learning to rank from implicit feedback.
InProceedings of the eleventh ACM SIGKDD inter-national conference on Knowledge discovery indata mining, pages 239-248, Chicago, Illinois,USA.Silverstein Craig, Hannes Marais, Monika Henzinger,and Michael Moricz.
1999.
Analysis of a verylarge web search engine query log.
ACM SIGIRForum, 33(1):6-12.1210
