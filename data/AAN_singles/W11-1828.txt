Proceedings of BioNLP Shared Task 2011 Workshop, pages 183?191,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsGeneralizing Biomedical Event ExtractionJari Bjo?rne and Tapio SalakoskiDepartment of Information Technology, University of TurkuTurku Centre for Computer Science (TUCS)Joukahaisenkatu 3-5, 20520 Turku, Finlandfirstname.lastname@utu.fiAbstractWe present a system for extracting biomedicalevents (detailed descriptions of biomolecularinteractions) from research articles.
This sys-tem was developed for the BioNLP?11 SharedTask and extends our BioNLP?09 Shared Taskwinning Turku Event Extraction System.
Ituses support vector machines to first detectevent-defining words, followed by detectionof their relationships.
The theme of theBioNLP?11 Shared Task is generalization, ex-tending event extraction to varied biomedicaldomains.
Our current system successfully pre-dicts events for every domain case introducedin the BioNLP?11 Shared Task, being the onlysystem to participate in all eight tasks and allof their subtasks, with best performance infour tasks.1 IntroductionBiomedical event extraction is the process of auto-matically detecting statements of molecular interac-tions in research articles.
Using natural languageprocessing techniques, an event extraction systempredicts relations between proteins/genes and theprocesses they take part in.
Manually annotated cor-pora are used to evaluate event extraction techniquesand to train machine-learning based systems.Event extraction was popularised by theBioNLP?09 Shared Task on Event Extraction(Kim et al, 2009), providing a more detailedalternative for the older approach of binary inter-action detection, where each pair of protein namesco-occurring in the text is classified as interacting ornot.
Events extend this formalism by adding to therelations direction, type and nesting.
Events definethe type of interaction, such as phosphorylation,and commonly mark in the text a trigger word(e.g.
?phosphorylates?)
describing the interaction.Directed events can define the role of their proteinor gene arguments as e.g.
cause or theme, the agentor the target of the biological process.
Finally,events can act as arguments of other events, creatingcomplex nested structures that accurately describethe biological interactions stated in the text.
Forexample, in the case of a sentence stating ?Stat3phosphorylation is regulated by Vav?, a phospho-rylation-event would itself be the argument of aregulation-event.We developed for the BioNLP?09 Shared Task theTurku Event Extraction System, achieving the bestperformance at 51.95% F-score (Bjo?rne et al, 2009).This system separated event extraction into multipleclassification tasks, detecting individually the trig-ger words defining events, and the arguments thatdescribe which proteins or genes take part in theseevents.
Other approaches used in the Shared Task in-cluded e.g.
joint inference (Riedel et al, 2009).
Anoverall notable trend was the use of full dependencyparsing (Buyko et al, 2009; Van Landeghem et al,2009; Kilicoglu and Bergler, 2009).In the following years, event extraction has beenthe subject of continuous development.
In 2009, af-ter the BioNLP?09 Shared Task, we extended oursystem and improved its performance to 52.85%(Bjo?rne et al, 2011).
In 2010, the system introducedby Miwa et.
al.
reached a new record performance of56.00% (Miwa et al, 2010a).183RegulationNN NN VB NN CC .conj_and><nndobj><nsubjNNProteinSTAT3 Phosphorylationphosphorylation Regulationinvolve ProteinVav and ProteinRac-1 .Cause>Cause><ThemeunmergingDCEedge detectiontrigger detectionBAdobj>parsingProteinSTAT3 Phosphorylationphosphorylation Regulationinvolve ProteinVav and ProteinRac-1 .<Theme Cause> Cause>ProteinSTAT3 Phosphorylationphosphorylation Regulationinvolve ProteinVav and ProteinRac-1 .ProteinSTAT3 phosphorylation involve ProteinVav and ProteinRac-1 .STAT3 phosphorylation involve Vav and Rac-1 .Ser(727) mayNNappos> <auxVBSer(727) maySer(727) maySer(727) maySer(727) mayEntity<SiteEntityEntity<Theme<Theme<Site <ThemeRegulationProteinSTAT3 Phosphorylationphosphorylation involve ProteinVav and ProteinRac-1 .Cause>Cause><Themespeculation and negation detectionFSer(727) mayEntity<Theme<Site <ThemeRegulationSpecSpecFigure 1: Event extraction.
In most tasks named entitiesare given (A).
Sentences are parsed (B) to produce a de-pendency parse.
Entities not given are predicted throughtrigger detection (C).
Edge detection predicts event argu-ments between entities (D) and unmerging creates events(E).
Finally, event modality is predicted (F).
When thegraph is converted to the Shared Task format, site argu-ments are paired with core arguments that have the sametarget protein.In 2010, we applied the Turku Event Extrac-tion System to detecting events in all 18 millionPubMed abstracts, showing its scalability and gener-alizability into real-world data beyond domain cor-pora (Bjo?rne et al, 2010).
In the current BioNLP?11Shared Task1 (Kim et al, 2011), we demonstrate itsgeneralizability to different event extraction tasks byapplying what is, to a large extent, the same systemto every single task and subtask.2 System OverviewOur system divides event extraction into three mainsteps (Figure 1 C, D and E).
First, entities arepredicted for each word in a sentence.
Then, ar-guments are predicted between entities.
Finally,entity/argument sets are separated into individualevents.1http://sites.google.com/site/bionlpst/ProteinSTAT3 PhosphorylationphosphorylationA: Primary representationSer(727)Entity<Site <ThemeB: EPI representationProteinSTAT3 PhosphorylationphosphorylationSer(727)Entity<Site<ThemeProteinSTAT3 PhosphorylationphosphorylationSer(727)Entity<Site <ThemeC: HypotheticalFigure 2: Site argument representation.
Site argumentsadd detail to core arguments.
(A) In most tasks welink both core and site arguments to given protein nodes.This minimizes the number of outgoing edges per triggernode, simplifying unmerging, but loses the connectionbetween site and core arguments.
(B) In the EPI task, allevents with site-arguments have a single core argument,so linking sites to the trigger node preserves the site/coreconnection.
(C) To both limit number of arguments intrigger nodes and preserve site information, event argu-ments using sites could be linked to protein nodes throughthe site entity.
However, in this approach the core argu-ment would remain undetected if the site wasn?t detected.2.1 Graph RepresentationThe BioNLP?11 Shared Task consists of eight sep-arate tasks.
Most of these follow the BioNLP?09Shared Task annotation scheme, which definesevents as having a trigger entity and one or more ar-guments that link to other events or protein/gene en-tities.
This annotation can be represented as a graph,with trigger and protein/gene entities as nodes, andarguments (e.g.
theme) as edges.
In our graph repre-sentation, an event is defined implicitly as a triggernode and its outgoing edges (see Figure 1 F).Most of the BioNLP?11 Shared Task tasks definetask-specific annotation terminology, but largely fol-low the BioNLP?09 definition of events.
Some newannotation schemes, such as the bracket notation inthe CO-task can be viewed simply as alternative rep-resentations of arguments.
The major new featureis relations or triggerless events, used in the REL,REN, BB and BI tasks.
In our graph representation,this type of event is a single, directed edge.Some event arguments have a matching site ar-gument that determines the part of the protein theargument refers to (Figure 2).
To allow detection ofcore arguments independently of site arguments, in184most tasks we link site arguments directly to pro-teins (Figure 2 A).
This maximises extraction per-formance on core events, but losing the connectionbetween site and core arguments limits performanceon site arguments.To further simplify event extraction all sentencesare processed in isolation, so events crossing sen-tence boundaries (intersentence events, Table 2) can-not be detected.
This also limits the theoretical max-imum performance of the system (see Figure 3).In the provided data an event is annotated onlyonce for a set of equivalent proteins.
For example, inthe sentence ?Ubiquitination of caspase 8 (casp8)?a ubiquitination event would be annotated only for?caspase 8?, ?casp8?
being marked as equivalentto ?caspase 8?.
To improve training data consis-tency, our system fully resolves these equivalencesinto new events, also recursively when a duplicatedevent is nested in another event (Table 2).
Resolvedequivalences were used for event extraction in theBioNLP?11 GE, ID, EPI and BB tasks, althoughbased on tests with the GE dataset their impact onperformance was negligible.2.2 Machine LearningThe machine learning based event detection com-ponents classify examples into one of the positiveclasses or as negatives, based on a feature vectorrepresentation of the data.
To make these classifi-cations, we use the SVMmulticlass support vectormachine2 (Tsochantaridis et al, 2005) with a linearkernel.
An SVM must be optimized for each classi-fication task by experimentally determining the reg-ularization parameter C. This is done by training thesystem on a training dataset, and testing a number ofC values on a development dataset.
When producingpredictions for the test set, the classifier is retrainedwith combined training and development sets, andthe test data is classified with the previously deter-mined optimal value of C.Unlike in the BioNLP?09 Shared Task wherethe three main parameters (trigger-detector, recall-adjustment and edge-detector) were optimized in anexhaustive grid search against the final metric, inthe new system only the recall-adjustment param-2http://svmlight.joachims.org/svm_multiclass.htmleter (see Section 2.5) is optimized against the finalmetric, edge and trigger detector parameters beingoptimized in isolation to speed up experiments.2.3 Syntactic AnalysesThe machine learning features that are used inevent detection are mostly derived from the syntac-tic parses of the sentences.
Parsing links togetherrelated words that may be distant in their linear or-der, creating a parse tree (see Figure 1 B).We used the Charniak-Johnson parser (Char-niak and Johnson, 2005) with David McClosky?sbiomodel (McClosky, 2010) trained on the GENIAcorpus and unlabeled PubMed articles.
The parsetrees produced by the Charniak-Johnson parser werefurther processed with the Stanford conversion tool(de Marneffe et al, 2006), creating a dependencyparse (de Marneffe and Manning, 2008).In the supporting tasks (REL, REN and CO) thisparsing was done by us, but in the main tasks theorganizers provided official parses which were used.All parses for tasks where named entities were givenas gold data were further processed with a pro-tein name splitter that divides at punctuation tokenswhich contain named entities, such as ?p50/p65?
or?GATA3-binding?.2.4 Feature GroupsTo convert text into features understood by the clas-sifier, a number of analyses are performed on thesentences, mostly resulting in binary features statingthe presence or absence of some feature.
Applica-ble combinations of these features are then used bythe trigger detection, edge detection and unmergingsteps of the event extraction system.Token features can be generated for each wordtoken, and they define the text of the token, itsPorter-stem (Porter, 1980), its Penn treebank part-of-speech-tag, character bi- and trigrams, presenceof punctuation or numeric characters etc.Sentence features define the number of namedentities in the sentence as well as bag-of-wordscounts for all words.Dependency chains follow the syntactic depen-dencies up to a depth of three, starting from a tokenof interest.
They are used to define the immediatecontext of these words.185Dependency path N-grams, are built from theshortest undirected path of tokens and dependencieslinking together two entities, and are used in edgedetection.
N-grams join together a token with its twoflanking dependencies as well as each dependencywith its two flanking tokens.
While these N-gramsfollow the direction of the entire path, the governor-dependent directions of individual dependencies areused to define token bigrams.Trigger features can be built in cases where trig-gers are already present, such as edge detection andevent construction.
These features include the typesand supertypes of the trigger nodes, and combina-tions thereof.External features are additional features basedon data external to the corpus being processed.
Suchfeatures can include e.g.
the presence of a word ina list of key terms, Wordnet hypernyms, or otherresources that enhance performance on a particulartask.
These are described in detail in Section 3.2.5 Trigger DetectionTrigger words are detected by classifying each tokenas negative or as one of the positive trigger classes.Sometimes several triggers overlap, in which casea merged class (e.g.
phosphorylation?regulation) isused.
After trigger prediction, triggers of mergedclasses are split into their component classes.Most tasks evaluate trigger detection using ap-proximate span, so detecting a single token isenough.
However, this token must be chosen consis-tently for the classifier to be able to make accuratepredictions.
For multi-token triggers, we select asthe trigger word the syntactic head, the root token ofthe dependency parse subtree covering the entity.When optimizing the SVM C-parameter for trig-ger and edge detection, it is optimized in isolation,maximizing the F-score for that classification task.Edges can be predicted for an event only if its trig-ger has been detected, but often the C-parameter thatmaximizes trigger detection F-score has too low re-call for optimal edge detection.
A recall adjustmentstep is used to fit together the trigger and edge de-tectors.
For each example, the classifier gives a con-fidence score for each potential class, and picks asthe predicted class the one with the highest score.
Inrecall adjustment, the confidence score of each neg-ative example is multiplied with a multiplier, and ifthe result falls below the score of another class, thatclass becomes the new classification.
This multiplieris determined experimentally by optimizing againstoverall system performance, using the official taskmetric for cases where a downloadable evaluator isavailable (GE and BB).2.6 Edge DetectionEdge detection is used to predict event arguments ortriggerless events and relations, all of which are de-fined as edges in the graph representation.
The edgedetector defines one example per direction for eachpair of entities in the sentence, and uses the SVMclassifier to classify the examples as negatives or asbelonging to one of the positive classes.
As with thetrigger detector, overlapping positive classes are pre-dicted through merged classes (e.g.
cause?theme).Task-specific rules defining valid argument types foreach entity type are used to considerably reduce thenumber of examples that can only be negatives.2.7 UnmergingIn the graph representation, events are definedthrough their trigger word node, resulting in over-lapping nodes for overlapping events.
The triggerdetector can however predict a maximum of one trig-ger node per type for each token.
When edges arepredicted between these nodes, the result is a mergedgraph where overlapping events are merged into asingle node and its set of outgoing edges.
Takinginto account the limits of trigger prediction, the edgedetector is also trained on a merged graph version ofthe gold data.To produce the final events, these merged nodesneed to be ?pulled apart?
into valid trigger and argu-ment combinations.
In the BioNLP?09 Shared Task,this was done with a rule-based system.
Since then,further research has been done on machine learningapproaches for this question (Miwa et al, 2010b;Heimonen et al, 2010).
In our current system, un-merging is done as an SVM-classification step.
Anexample is constructed for each argument edge com-bination of each predicted node, and classified as atrue event or a false event to be removed.
Tested onthe BioNLP?09 Shared Task data, this system per-forms roughly on par with our earlier rule-based sys-tem, but has the advantage of being more generaland thus applicable to all BioNLP?11 Shared Task186GE1 GE2 GE3 EPI ID BB BI CO REL RENTask020406080100F-scoreFigure 3: Ranking of the systems participating in theBioNLP?11 Shared Task.
Our system is marked withblack dots and the dotted line shows its theoretical maxi-mum performance (see Section 2.1) with all correct clas-sifications.tasks.
The unmerging step is not required for trig-gerless events which are defined by a single edge.All of the tasks define varied, detailed limits onvalid event type and argument combinations.
A finalvalidation step based on task-specific rules is used toremove structurally incorrect events left over frompreceding machine learning steps.2.8 Modality DetectionSpeculation and negation are detected indepen-dently, with binary classification of trigger nodes.The features used are mostly the same as for triggerdetection, with the addition of a list of speculation-related words based on the BioNLP?09 ST corpus.3 Tasks and ResultsThe BioNLP?11 Shared Task consists of five maintasks and three supporting tasks.
Additionally, manyof these tasks specify separate subtasks.
Exceptfor the GE-task, which defines three main evalua-tion criteria, all tasks have a single primary evalua-tion criterion.
All evaluations are based on F-score,the harmonic mean of precision and recall.
Perfor-mance of all systems participating in the BioNLP?11Shared Task is shown in Figure 3.
Our system?s per-formance on both development and test sets of alltasks is shown in Table 1.Corpus Devel F Test FGE?09 task 1 56.27 53.15GE?09 task 2 54.25 50.68GE task 1 55.78 53.30GE task 2 53.39 51.97GE task 3 38.34 26.86EPI 56.41 53.33ID 44.92 42.57BB 27.01 26BI 77.24 77CO 36.22 23.77REL 65.99 57.7REN 84.62 87.0Table 1: Devel and test results for all tasks.
The perfor-mance of our new system on the BioNLP?09 ST GENIAdataset is shown for reference, with task 3 omitted due toa changed metric.
For GE-tasks, the Approximate Span& Recursive matching criterion is used.3.1 GENIA (GE)The GENIA task is the direct continuation of theBioNLP?09 Shared Task.
The BioNLP?09 ST cor-pus consisted only of abstracts.
The new version ex-tends this data by 30% with full text PubMed Centralarticles.Our system applied to the GE task is the mostsimilar to the one we developed for the BioNLP?09Shared Task.
The major difference is the replace-ment of the rule-based unmerging component withan SVM based one.The GE task has three subtasks, task 1 is detectionof events with their main arguments, task 2 extendsthis to detection of sites defining the exact molecu-lar location of interactions, and task 3 adds the de-tection of whether events are stated in a negated orspeculative context.For task 3, speculation and negation detection, weconsidered the GE, EPI and ID task corpora simi-lar enough to train a single model on.
Comparedto training on GE alone, example classification F-score decreased for negation by 8 pp and increasedfor speculation by 4 pp.
Overall task 3 processingwas considerably simplified.Our system placed third in task 1, second in task 2and first in task 3.
Task 1 had the most participants,making it the most useful for evaluating overall per-formance.
Our F-score of 53.30% was within threepercentage points of the best performing system (by187Corpus sentences events equiv events nesting events intersentence events neg/spec eventsGE?09 8906 11285 7.9% 38.8% 6.0% 12.1%GE 11581 14496 6.6% 37.2% 6.0% 13.3%EPI 7648 2684 9.1% 10.2% 9.3% 10.1%ID 3193 2931 5.3% 21.3% 3.9% 4.9%BB 1762 5843 79.4% N/A 86.0% 0%BI 120 458 0% N/A 0% 0%CO 8906 5284 0% N/A 8.5% N/AREL 8906 2440 4.2% N/A 0% 0%REN 13235 373 0% N/A 2.4% 0%Table 2: Corpus statistics.
Numbers are for all available annotated data, i.e.
the merged training and development sets.team FAUST), indicating that our chosen event de-tection approach still remains competitive.
For ref-erence, we ran our system also on the BioNLP?09data, reaching an F-score of 53.15%, a slight in-crease over the 52.85% we previously reported inBjo?rne et al (2011).3.2 Epigenetics and Post-translationalModifications (EPI)All events in the EPI task that have additional argu-ments (comparable to the site-arguments in the GE-task) have a single core argument.
We therefore usefor this task a slightly modified graph representation,where all additional arguments are treated as core ar-guments, linking directly to the event node (Figure 2B).
The number of argument combinations per pre-dicted event node remains manageable for the un-merging system and full recovery of additional ar-guments is possible.Eight of the EPI event types have correspond-ing reverse events, such as phosphorylation and de-phosphorylation.
Many of these reverse events arequite rare, resulting in too little training data for thetrigger detector to find them.
Therefore we mergeeach reverse event type into its corresponding for-ward event type.
After trigger detection, an addi-tional rule-based step separates them again.
Most ofthe reverse classes are characterized by a ?de?-prefixin their trigger word.
On the EPI training dataset,the rule-based step determined correctly whether anevent was reversed in 99.6% of cases (1698 out of1704 events).
Using this approach, primary criterionF-score on the development set increased 1.33 per-centage points from 55.08% to 56.41%.
Several pre-viously undetectable small reverse classes becamedetectable, with e.g.
deubiquitination (8 instances inthe development set) detected at 77.78% F-score.Our system ranked first on the EPI task, outper-forming the next-best system (team FAUST) by over18 percentage points.
On the alternative core metricour system was also the first, but the FAUST systemwas very close with only a 0.27 percentage point dif-ference.
Since the core metric disregards additionalarguments, it may be that our alternative approachfor representing these arguments (Figure 2 B) wasimportant for the primary criterion difference.3.3 Infectious Diseases (ID)The annotation scheme for the ID task closely fol-lows the GE task, except for an additional processevent type that may have no arguments, and for fivedifferent entity types in place of the protein type.Our approach for the ID task was identical to theGE task, but performance relative to the other teamswas considerably lower.
Primary evaluation metricF-score was 42.57% vs. 43.44% for the core metricwhich disregards additional arguments, indicatingthat these are not the reason for low performance.3.4 Bacteria Biotopes (BB)The BB task considers detection of events describ-ing bacteria and their habitats.
The task defines onlytwo event types but a large number of entity typeswhich fall into five supertypes.
All entities must bepredicted and all events are triggerless.Unlike in the other main tasks, in the BB task ex-act spans are required for Bacterium-type entities,which usually consist of more than one token (e.g.B.
subtilis).
After trigger detection, a rule-based stepattempts to extend predicted trigger spans forwardsand backwards to cover the correct span.
When ex-tending the spans of BB training set gold entity head188tokens, this step produced the correct span for 91%(399 out of 440) of Bacterium-type entities.To aid in detecting Bacterium-entities a list ofbacteria names from the List of Prokaryotic nameswith Standing in Nomenclature3 was used (Euze?by,1997) as external features.
To help in detecting theheterogeneous habitat-entities, synonyms and hy-pernyms from Wordnet were used (Fellbaum, 1998).The development set lacked some event classes, sowe moved some documents from the training set tothe development set to include these.Our F-score was the lowest of the three partici-pating systems, and detailed results show a consis-tently lower performance in detecting the entities.The large number of intersentence events (Table 2)also considerably limited performance (Figure 3).3.5 Bacteria Gene Interactions (BI)The BI-task considers events related to genetic pro-cesses of the bacterium Bacillus subtilis.
This taskdefines a large number of both entity and eventtypes, but all entities are given as gold-standard data,therefore we start from edge detection (Figure 1 D).All BI events are triggerless.In this task manually curated syntactic parses areprovided.
As also automated parses were available,we tested them as an alternative.
With the Charniak-Johnson/McClosky parses overall performance wasonly 0.65 percentage points lower (76.59% vs.77.24%).
As with the BB task, we moved some doc-uments from the training set to the development setto include missing classes.Despite this task being very straightforward com-pared to the other tasks we were the only participant.Therefore, too many conclusions shouldn?t be drawnfrom the performance, except to note that a ratherhigh F-score is to be expected with all the entitiesbeing given as gold data.3.6 Protein/Gene Coreference (CO)In the CO supporting task the goal is to extractanaphoric expressions.
Even though our event ex-traction system was not developed with corefer-ence resolution in mind, the graph representationcan be used for the coreference annotation, makingcoreference detection possible.
Anaphoras and An-3http://www.bacterio.cict.fr/tecedents are both represented as Exp-type entities,with Coref -type edges linking Anaphora-entities toAntecedent-entities and Target-type edges linkingProtein-type entities to Antecedent-entities.In the CO-task, character spans for detected enti-ties must be in the range of a full span and minimumspan.
Therefore in this task we used an alternativetrigger detector.
Instead of predicting one trigger pertoken, this component predicted one trigger per eachsyntactic phrase created by the Charniak-Johnsonparser.
Since these phrases don?t cover most of theCO-task triggers, they were further subdivided intoadditional phrases, e.g.
by cutting away determinersand creating an extra phrase for each noun-token,with the aim of maximizing the number of includedtriggers and minimizing the number of candidates.Our system placed fourth out of six, reaching anF-score of 23.77%.
Coreference resolution being anew subject for us and our system not being devel-oped for this domain, we consider this an encour-aging result, but conclude that in general dedicatedsystems should be used for coreference resolution.3.7 Entity Relations (REL)The REL supporting task concerns the detection ofstatic relationships, Subunit-Complex relations be-tween individual proteins and protein complexes andProtein-Component relations between a gene or pro-tein and its component, such as a protein domain orgene promoter.
In the graph representation these re-lations are defined as edges that link together givenprotein/gene names and Entity-type entities that aredetected by the trigger detector.To improve entity detection, additional featuresare used.
Derived from the REL annotation, thesefeatures highlight structures typical for biomolecularcomponents, such as aminoacids and their shorthandforms, domains, motifs, loci, termini and promot-ers.
Many of the REL entities span multiple tokens.Since the trigger detector predicts one entity per to-ken, additional features are defined to mark whethera token is part of a known multi-token name.Our system had the best performance out of fourparticipating systems with an F-score of 57.7%, over16 percentage points higher than the next.
Develop-ment set results show that performance for the twoevent classes was very close, 66.40% for Protein-Component and 65.23% for Subunit-Complex.1893.8 Bacteria Gene Renaming (REN)The REN supporting task is aimed at detecting state-ments of B. Subtilis gene renaming where a syn-onym is introduced for a gene.
The REL task definesa single relation type, Renaming, and a single entitytype, Gene.
All entities are given, so only edge de-tection is required.
Unlike the other tasks, the mainevaluation criterion ignores the direction of the rela-tions, so they are processed as undirected edges inthe graph representation.Edge detection performance was improved withexternal features based on two sources definingknown B. Subtilis synonym pairs: The Uniprot B.Subtilis gene list ?bacsu?4 and SubtiWiki5, the B.Subtilis research community annotation wiki.For the 300 renaming relations in the REN train-ing data, the synonym pair was found from theUniprot list in 66% (199 cases), from SubtiWiki in79% (237 cases) and from either resource in 81.3%(244 cases).
For the corresponding negative edgeexamples, Uniprot or SubtiWiki synonym pairs ap-peared in only 2.1% (351 out of 16640 examples).At 87.0% F-score our system had the highest per-formance out of the three participants, exceeding thenext highest system by 17.1 percentage points.
IfUniprot and SubtiWiki features are not used, perfor-mance on the development set is still 67.85%, closeto the second highest performing system on the task.4 ConclusionsWe have developed a system that addresses all tasksand subtasks in the BioNLP?11 Shared Task, withtop performance in several tasks.
With the modulardesign of the system, all tasks could be implementedwith relatively small modifications to the processingpipeline.
The graph representation which coverednaturally all different task annotations was a key fea-ture in enabling fast system development and test-ing.
As with the Turku Event Extraction System de-veloped for the BioNLP?09 Shared Task, we releasethis improved system for the BioNLP communityunder an open source license at bionlp.utu.fi.Of all the tasks, the GE-task, which extends theBioNLP?09 corpus, is best suited for evaluating ad-vances in event extraction in the past two years.4http://www.uniprot.org/docs/bacsu5http://subtiwiki.uni-goettingen.de/Comparing our system?s performance on the GE?09corpus with the current one, we can assume that thetwo corpora are of roughly equal difficulty.
There-fore we can reason that overall event extractionperformance has increased about three percentagepoints, the highest performance on the current GE-task being 56.04% by team FAUST.
It appears thatevent extraction is a hard problem, and that the im-mediate easy performance increases have alreadybeen found.
We hope the BioNLP?11 Shared Taskhas focused more interest in the field, hopefullyeventually leading to breakthroughs in event extrac-tion and bringing performance closer to establishedfields of BioNLP such as syntactic parsing or namedentity recognition.That our system could be generalized to work onall tasks and subtasks, indicates that the event extrac-tion approach can offer working solutions for severalbiomedical domains.
A potential limiting factor cur-rently is that most task-specific corpora annotate anon-overlapping set of sentences, necessitating thedevelopment of task-specific machine learning mod-els.
Training on multiple datasets could mean thatpositives of one task would be unannotated on textfrom the other task, confusing the classifier.
On theother hand, multiple overlapping task annotations onthe same text would permit the system to learn fromthe interactions and delineations of different annota-tions.
System generalization has been successfullyshown in the BioNLP?11 Shared Task, but has re-sulted in a number of separate extraction systems.
Itcould well be that the future of event extraction re-quires also the generalization of corpus annotations.As future directions, we intend to further improvethe scope and usability of our event extraction sys-tem.
We will also continue our work on PubMed-scale event extraction, possibly applying some of thenew extraction targets introduced by the BioNLP?11Shared Task.AcknowledgmentsWe thank the Academy of Finland for funding, CSC?
IT Center for Science Ltd for computational re-sources and Filip Ginter and Sofie Van Landeghemfor help with the manuscript.190ReferencesJari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings of the BioNLP 2009Work-shop Companion Volume for Shared Task, pages 10?18, Boulder, Colorado.
Association for ComputationalLinguistics.Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-jii, and Tapio Salakoski.
2010.
Scaling up biomed-ical event extraction to the entire PubMed.
In Pro-ceedings of the 2010 Workshop on Biomedical NaturalLanguage Processing, pages 28?36, Uppsala, Sweden,July.
Association for Computational Linguistics.Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2011.
Extract-ing contextualized complex biological events with richgraph-based feature sets.
Computational Intelligence,Special issue on Extracting Bio-molecular Events fromLiterature.
To appear, accepted in 2009.Ekaterina Buyko, Erik Faessler, Joachim Wermter, andUdo Hahn.
2009.
Event extraction from trimmed de-pendency graphs.
In Proceedings of the BioNLP 2009Workshop Companion Volume for Shared Task, pages19?27.
ACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180.
Association for Computational Lin-guistics.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher Manning.
2006.
Generating typed depen-dency parses from phrase structure parses.
In Proceed-ings of LREC-06, pages 449?454.J.
P. Euze?by.
1997.
List of bacterial names with standingin nomenclature: a folder available on the internet.
IntJ Syst Bacteriol, 47(2):590?592.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Juho Heimonen, Jari Bjo?rne, and Tapio Salakoski.
2010.Reconstruction of semantic relationships from theirprojections in biomolecular domain.
In Proceedings ofthe 2010 Workshop on Biomedical Natural LanguageProcessing, pages 108?116, Uppsala, Sweden, July.Association for Computational Linguistics.Halil Kilicoglu and Sabine Bergler.
2009.
Syntactic de-pendency based heuristics for biological event extrac-tion.
In Proceedings of the BioNLP 2009 WorkshopCompanion Volume for Shared Task, pages 119?127.ACL.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.
InProceedings of the BioNLP 2009 Workshop Compan-ion Volume for Shared Task, pages 1?9, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, and Jun?ichi Tsujii.
2011.
Overview ofBioNLP Shared Task 2011.
In Proceedings ofthe BioNLP 2011 Workshop Companion Volume forShared Task, Portland, Oregon, June.
Association forComputational Linguistics.Marie-Catherine de Marneffe and Christopher Manning.2008.
The Stanford typed dependencies representa-tion.
In COLING Workshop on Cross-framework andCross-domain Parser Evaluation.David McClosky.
2010.
Any Domain Parsing: Auto-matic Domain Adaptation for Natural Language Pars-ing.
Ph.D. thesis, Department of Computer Science,Brown University.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010a.
A comparative study of syn-tactic parsers for event extraction.
In Proceedings ofthe 2010 Workshop on Biomedical Natural LanguageProcessing, BioNLP ?10, pages 37?45, Stroudsburg,PA, USA.
Association for Computational Linguistics.Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichiTsujii.
2010b.
Event extraction with complex eventclassification using rich features.
J Bioinform ComputBiol, 8:131?146.Martin F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In Proceedings ofthe Workshop on Current Trends in Biomedical Natu-ral Language Processing: Shared Task, BioNLP ?09,pages 41?49, Stroudsburg, PA, USA.
Association forComputational Linguistics.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large marginmethods for structured and interdependent output vari-ables.
Journal of Machine Learning Research (JMLR),6(Sep):1453?1484.Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,and Yves Van de Peer.
2009.
Analyzing text in searchof bio-molecular events: a high-precision machinelearning framework.
In Proceedings of the BioNLP2009 Workshop Companion Volume for Shared Task,pages 128?136.
ACL.191
