Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 37?45,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Rule-Driven Dynamic Programming Decoder for Statistical MTChristoph TillmannIBM T.J. Watson Research CenterYorktown Heights, N.Y. 10598ctill@us.ibm.comAbstractThe paper presents an extension of a dynamicprogramming (DP) decoder for phrase-basedSMT (Koehn, 2004; Och and Ney, 2004) thattightly integrates POS-based re-order rules(Crego and Marino, 2006) into a left-to-rightbeam-search algorithm, rather than handlingthem in a pre-processing or re-order graphgeneration step.
The novel decoding algo-rithm can handle tens of thousands of rulesefficiently.
An improvement over a standardphrase-based decoder is shown on an Arabic-English translation task with respect to trans-lation accuracy and speed for large re-orderwindow sizes.1 IntroductionThe paper presents an extension of a dynamicprogramming (DP) decoder for phrase-based SMT(Koehn, 2004; Och and Ney, 2004) where POS-based re-order rules (Crego and Marino, 2006) aretightly integrated into a left-to-right run over theinput sentence.
In the literature, re-order rules areapplied to the source and/or target sentence as apre-processing step (Xia and McCord, 2004; Collinset al, 2005; Wang et al, 2007) where the rules canbe applied on both training and test data.
Anotherway of incorporating re-order rules is via extendedmonotone search graphs (Crego and Marino, 2006)or lattices (Zhang et al, 2007; Paulik et al, 2007).This paper presents a way of handling POS-basedre-order rules as an edge generation process: thePOS-based re-order rules are tightly integrated intoa left to right beam search decoder in a way that29 000 rules which may overlap in an arbitraryway (but not recursively) are handled efficiently.Example rules which are used to control the novelDP-based decoder are shown in Table 1, where eachPOS sequence is associated with possibly severalpermutations pi.
In order to apply the rules, the inputsentences are POS-tagged.
If a POS sequence of arule matches some identical POS sequence in the in-put sentence the corresponding words are re-orderedaccording to pi.
The contributions of this paper areas follows: 1) The novel DP decoder can handletens of thousands of POS-based rules efficientlyrather than a few dozen rules as is typically reportedin the SMT literature by tightly integrating theminto a beam search algorithm.
As a result phrasere-ordering with a large distortion window can becarried out efficiently and reliably.
2) The currentrule-driven decoder is a first step towards includingmore complex rules, i.e.
syntax-based rules as in(Wang et al, 2007) or chunk rules as in (Zhang etal., 2007) using a decoding algorithm that is con-ceptually similar to an Earley-style parser (Earley,1970).
More generally, ?rule-driven?
decoding istightly linked to standard phrase-based decoding.
Infuture, the edge generation technique presented inthis paper might be extended to handle hierarchicalrules (Chiang, 2007) in a simple left-to-right beamsearch decoder.In the next section, we briefly summarize thebaseline decoder.
Section 3 shows the novel rule-driven DP decoder.
Section 4 shows how the currentdecoder is related to both DP-based decoding algo-rithms in speech recognition and parsing.
Finally,37Table 1: A list of 28 878 reorder rules sorted according to the rule occurrence count N(r) is used in this paper.For each POS sequence the corresponding permutation pi is shown.
Rule ID is the ordinal number of a rule inthe sorted list.
The maximum rule length that can be handled efficiently is surprisingly long: about 20 words.Rule ID r POS sequence pi N(r)1 DET NOUN DET ADJ ?
2 3 0 1 4 4212 DET NOUN NSUFF-FEM-SG DET ADJ NSUFF-FEM-SG ?
3 4 5 0 1 2 2 257.........3 000 NOUN CASE-INDEF-ACC ADJ NSUFF-FEM-SG CONJ ADJ NSUFF-FEM-SG ?
2 3 4 5 6 0 1 6.........28 878 PREP DET NOUN DET ADJ PREP NOUN-PROP ADJ ?
0 1 2 7 8 3 4NSUFF-MASC-SG-ACC-INDEF CONJ IV3MS IV IVSUFF-DO:3FS 9 10 11 12 5 6 2Section 5 shows experimental results.2 Baseline DP DecoderThe translation model used in this paper is a phrase-based model (Koehn et al, 2003), where the trans-lation units are so-called blocks: a block b is a pairconsisting of a source phrase s and a target phraset which are translations of each other.
The ex-pression block is used here to emphasize that pairsof phrases (especially longer phrases) tend to formclosely linked units in such a way that the transla-tion process can be formalized as a block segmen-tation process (Nagata et al, 2006; Tillmann andZhang, 2007).
Here, the input sentence is segmentedfrom left to right while simultaneously generatingthe target sentence, one block at a time.
In prac-tice, phrase-based or block-based translation mod-els which largely monotone decoding algorithms ob-tain close to state-of-the-art performance by usingskip and window-based restrictions to reduce thesearch space (Berger et al, 1996).
During decod-ing, we maximize the score sw(bn1 ) of a phrase-pairsequence bn1 = (si, ti)n1 :sw(bn1 ) =n?i=1wT ?
f(bi, bi?1), (1)where bi is a block, bi?1 is its predecessor block,and f(bi, bi?1) is a 8-dimensional feature vectorwhere the features are derived from some probabilis-tic models: language model, translation model, anddistortion model probabilities.
n is the number ofblocks in the translation and the weight vector w istrained in a way as to maximize the decoder BLEUscore on some training data using an on-line algo-rithm (Tillmann and Zhang, 2008).
The decoder thatcarries out the optimization in Eq.
1 is similar to astandard phrase-based decoder (Koehn, 2004; Ochand Ney, 2004), where states are tuples of the fol-lowing type:[ C ; [i, j] ], (2)where C is the so-called coverage vector that keepstrack of the already processed source position, [i, j]is the source interval covered by the last sourcephrase match.
In comparison, (Koehn, 2004) usesonly the position of the final word of the last sourcephrase translated.
Since we are using the distortionmodel in (Al-Onaizan and Papineni, 2006) the entirelast source phrase interval needs to be stored.
Hy-pothesis score and language model history are omit-ted for brevity reasons.
The states are stored in listsor stacks and DP recombination is used to reduce thesize of the search space while extending states.The algorithm described in this paper uses an in-termediate data structure called an edge that repre-sents a source phrase together with a target phrasethat is one of its possible translation.
Formally, wedefine:[ [i, j] , tN1 ], (3)where tN1 is the target phrase linked to the sourcephrase si, ?
?
?
, sj .
The edges are stored in a so-calledchart.
For each input interval that is matched bysome source phrase in the block set, a list of pos-sible target phrase translations is stored in the chart.Here, simple edges as in Eq.
3 are used to gener-ate so-called rule edges that are defined later in thepaper.
A similar data structure corresponding to anedge is called translation option in (Koehn, 2004).While the edge generation potentially slows downthe overall decoding process, for the baseline de-38e = 1 e = 2e = 3a2a1a0e = 4e = 5a3a4?S imp le ??
E d g e s ?1.2 .1    01    2   0?
S imp le ?E d g e s ?A d d i t i o n a l?
R u l e ?
E d g eC o p i e se = 2 ,  r = 1p = B E GO R I G = [ 1 , 2 ]e = 1 ,  r = 1p = E N DO R I G = [ 0 , 1 ]e = 3 ,  r = 2p = B E GO R I G = [ 1 , 3 ]e = 1 ,  r = 2p = E N DO R I G = [ 0 , 1 ]e = 1 e = 2e = 3a2a1e = 4e = 5a3a4e = 1 ,  r = 3p = B E G I NO R I G = [ 0 , 1 ]e = 4 ,  r = 3p = I N T E RO R I G = [ 3 , 4 ]e = 3 ,  r = 3p = E N DO R I G = [ 1 , 2 ]0    3   4   1   2a0p0p0p0p1p1p1p2p2p3p43 .
?Or ig ina l ?C h a r t?Ex tended ?C h a r tFigure 1: Addition of rule edges to a chart containing 5 simple edges (some rule edges are not shown).
The simpleedges remain in the chart after the rule edges have been added: they are used to carry out monotone translations.coder generating all the simple edges takes less than0.3 % of the overall decoding time.3 DP-Search with RulesThis section explains the handling of the re-orderrules as an edge generation process.
Assuming amonotone translation, for the baseline DP decoder(Koehn, 2004) each edge ending at position j can becontinued by any edge starting at position j + 1, i.e.the simple edges are fully connected with respect totheir start and ending positions.
For the rule-drivendecoder, all the re-ordering is handled by generat-ing additional edges which are ?copies?
of the sim-ple edges in each rule context in which they occur.Here, a rule edge copy ending at position j is notfully connected with all other edges starting at po-sition j + 1.
Once a rule edge copy for a particularrule id r has been processed that edge can be con-tinued only by an edge copy for the same rule untilthe end of the rule has been reached.
To formalizethe approach, the search state definition in Eq.
2 ismodified as follows:[ s ; [i, j] , r , sr , e ?
{false, true} ] (4)Here, the coverage vector C is replaced by a singlenumber s: a monotone search is carried out and allthe source positions up to position s (including s)are covered.
[i, j] is the coverage interval for the lastsource phrase translated (the same as in Eq.
2).
r isthe rule identifier, i.e.
a rule position in the list inTable 1. sr is the starting position for the rule matchof rule r in the input sentence, and e is a flag thatindicates whether the hypothesis h has covered theentire span of rule r yet.
The search starts with thefollowing initial state:[ ?1 ; [?1,?1] ,?1 , ?1 , e = true ] , (5)where the starting positions s, sr, and the coverageinterval [i, j] are all initialized with ?1, a virtualsource position to the left of the uncovered sourceinput.
Throughout the search, a rule id of ?1 in-dicates that no rule is currently applied for that hy-pothesis, i.e.
a contiguous source interval to the leftof s is covered.39States are extended by finding matching edgesand the generation of these edges is illustrated inFig.
1 for the use of 3 overlapping rules on a sourcesegment of 5 words a0, ?
?
?
, a4 1.
Edges are shownas rectangles where the number on the left inside thebox corresponds to the enumeration of the simpleedges.
In the top half of the picture the simple edgeswhich correspond to 5 phrase-to-phrase translationsare shown.
In the bottom half all the edges after therule edge extension are shown (including simple andrule edges).
A rule edge contains additional compo-nents: the rule id r, a relative edge position p (ex-plained below), and the original source interval of arule edge before it has been re-ordered.
A rule edgeis generated from a simple edge via a re-order ruleapplication: the newly generated edges are addedinto the chart as shown in the lower half of Figure 1.Here, rule 1 and 2 generate two new edges and rule3 generates three new edges that are added into thechart at their new re-ordered positions, e.g.
copiesof edge 1 are added for the rule id r = 1 at startposition 2, for rule r = 2 at start position 3, and forrule r = 3 at start position 0.
Even if an edge copyis added at the same position as the original edge anew copy is needed.
The three rules correspond tomatching POS sequences, i.e.
the Arabic input sen-tence has been POS tagged and a POS pj has beenassigned to each Arabic word aj .
The same POSsequence might generate several different permuta-tions which is not shown here.More formally, the edge generation process is car-ried out as follows.
First, for each source interval[k, l] all the matching phrase pairs are found andadded into the chart as simple edges.
In a secondrun over the input sentence for each source inter-val [k, l] all matching POS sequences are computedand the corresponding source words ak, ?
?
?
, al arere-ordered according to the rule permutation.
Onthe re-ordered word sequence phrase matches arecomputed only for those source phrases that alreadyoccurred in the original (un-reordered) source sen-tence.
Both edge generation steps together still takeless than 1 % of the overall decoding time as shownin Section 5: most of the decoding time is needed toaccess the translation and the language model prob-1Rule edges and simple edges may overlap arbitrarily, butthe final translation constitutes a non-overlapping boundary se-quence.abilities when extending partial decoder hypotheses2.
Typically rule matches are much longer than edgematches where several simple edges are needed tocover the entire rule interval, i.e.
three edges for ruler = 3 in Fig.
1.
As the edge copies correspondingto the same rule must be processed in sequence theyare assigned one out of three possible positions p:?
BEG: Edge copy matches at the begin of rulematch.?
INTER: Edge copy lies within rule match inter-val.?
END: Edge copy matches at the end of rulematch.Formally, the rule edges in Fig.
1 are defined as fol-lows, where a rule edge includes all the componentsof a simple edge:[[i, j] , tN1 , r, p, [pi(i), pi(j)]], (6)where r is the rule id and p is the relative edge po-sition.
[pi(i), pi(j)] is the original coverage inter-val where the edge matched before being re-ordered.The original interval is not a necessary component ofthe rule-driven algorithm but it makes a direct com-parison with the window-based decoder straight-forward as explained below.
The rule edge defi-nition for a rule r that matches at position sr isslightly simplified: the processing interval is ac-tually [sr + i, sr + j] and the original interval is[sr+pi(i), sr+pi(j)].
For simplicity reasons, the off-set sr is omitted in Fig 1.
Using the original intervalhas the following advantage: as the edges are pro-cessed from left-to-right and the re-ordering is con-trolled by the rules the translation score computationis based on the original source interval [pi(i), pi(j)]and the monotone processing is based on the match-ing interval [i, j].
For the rule-driven decoder itlooks like the re-ordering is carried out like in a reg-ular decoder with a window-based re-ordering re-striction, but the rule-induced window can be large,i.e.
up to 15 source word positions.
In particulara distortion model can be applied when using the2Strictly speaking, the edge generation constitutes two addi-tional runs over the input sentence.
In future, the rule edges canbe computed ?on demand?
for each input position j resulting inan even stricter implementation of the beam search concept.40e = 1 e = 2 e = 3e = 4 e = 5 e = 6 e = 7 e = 82 , B E G I Ne = 96 ,  E N De = 1 1e = 1 05 , I N T E R6 , B E G I N 8 , I N T E R 7 , E N D1 , E N D7 , B E G I N2 0 11 0 3 22 .
R U L E :3 .
R U L E :a1a2a3a4a5a61 2 3 01 .
R U L E :a07 , B E G I N3 ,  I N T E R8 ,  I N T E Rh1h2 h 3 h 4 h 5 h 6h 7h8h 9 h 1 0h1 1h1 2h1 3h1 4Figure 2: Search lattice for the rule-driven decoder.
The gray circles indicated partial hypotheses.
An hypothesis isexpanded by applying an edge.
DP recombination is used to restrict the search space throughout the rule lattice.re-order rules.
Additionally, rule-based probabilitiescan be used as well.
This concept allows to directlycompare a window-based decoder and the currentrule based decoder in Section 5.The search space for the rule-driven decoder is il-lustrated in Fig.
2.
The gray shaded circles representtranslation hypotheses according to Eq.
4.
A trans-lation hypothesis h1 is extended by an edge whichcovers some uncovered portion of the input sen-tence to produce a new hypothesis h2.
The decodersearches monotonically through the entire chart ofedges, and word re-ordering is possible only throughthe use of rule edges.
The top half of the pictureshows the way simple edges contribute to the searchprocess: they are used to carry out a monotone trans-lation.
The dashed arrows indicate that hypothesescan be recombined: when extending hypothesis h3by edge e = 2 and hypothesis h4 by edge e = 8only a single hypothesis h5 is kept as the history ofedge extensions can be ignored for future decoderdecisions with respect to the uncovered source posi-tions.
Here, the distortion model and the languagemodel history are ignored for illustration purposes.As it can be seen in Fig.
2, the rule edge generationstep has created 3 copies of the simple edge e = 7,which are marked by a dashed borderline.
Hypothe-ses covering the same input may not be merged, i.e.hypotheses h9 and h13 for rules r = 1 and r = 2have to be kept separate from the hypothesis h4.
Butstate merging may occur for states generated by ruleedges for the same rule r, i.e.
rule r = 1 and stateh9.Since rule edges have to be processed in a sequen-tial order, looking up those that can extend a givenhypothesis h is more complicated than a phrasetranslation look-up in a regular decoder.
Given thesearch state definition in Eq.
4, for a given rule id rand coverage position s we have to be able to look-up all possible edge extensions efficiently.
This isimplemented by storing two lists:1.
For each source position j a list of possible?starting?
edges: these are all the simple edgesplus all rule edges with relative edge positionp = BEG.
This list is used to expand hypothesesaccording to the definition in Eq.
4 where therule flag e = true, i.e.
the search has finishedcovering an entire rule interval.2.
The second list is for continuing edges (p =INTER or p = END).
For each rule id r, rule41start position sr and source position j a list ofrule edges has to be stored that can continue analready started rule coverage.
This list is usedto expand hypotheses for which the rule flag eis e = false, i.e.
the hypothesis has not yetfinished covering the current rule interval, e.g.the hypotheses h9 and h11 in Fig.
2.The two lists are computed by a single run overthe chart after all chart edges have been generatedand before the search is carried out (the CPU timeto generate these lists is included in the edge gener-ation CPU time reported in Section 5).
The two listsare used to find the successor edges for each hypoth-esis h that corresponds to a rule r efficiently: onlya small fraction of the chart edges starting at posi-tion j needs to be retrieved for an extension.
Therule start position sr has to be included for the sec-ond list: it is possible that the same rule r matchesthe input sentences for two intervals [i, j] and [i?, j?
]which overlap.
This results in an invalid search stateconfiguration.
Based on the two lists a monotonesearch is carried out over the extended rule edge setwhich implicitly generates a reordering lattice as insimilar approaches (Crego and Marino, 2006; Zhanget al, 2007).
But because the handling of the edgesis tightly integrated into the beam search algorithmby applying the same beam thresholds it potentiallyhandles 10?s of thousands of rules efficiently.4 DP SearchThe DP decoder described in the previous sectionbears some resemblance with search algorithms forlarge vocabulary speech recognition.
For exam-ple, (Jelinek, 1998) presents a Viterbi decoder thatsearches a composite trellis consisting of smallerHMM acoustic trellises that are combined with lan-guage model states in the case a trigram languagemodel.
Multiple ?copies?
of the same acoustic submodels are incorporated into the overall trellis.
Thehighest probability word sequences is obtained us-ing a Viterbi shortest path finding algorithm in apossibly huge composite HMM (cf.
Fig.
5.3 of(Jelinek, 1998)).
In comparison, in this paper theedge ?copies?
are used to generate hypotheses thatare hypotheses ?copies?
of the same phrase match,e.g.
in Fig.
2 the states h4, h8, and h14 all resultfrom covering the same simple edge e7 as the mostrecent phrase match.
The states form a potentiallyhuge lattice as shown in Fig.
2.
Similarly, (Ort-manns and Ney, 2000) presents a DP search algo-rithm where the interdependent decisions betweennon-linear time alignment, word boundary detec-tion, and word identification (the pronunciation lex-icon is organized efficiently as a lexical tree) are allcarried out by searching a shortest path trough a pos-sibly huge composite trellis or HMM.
The similar-ity between those speech recognition algorithms andthe current rule decoder derives from the followingobservation: the use of a language model in speechrecognition introduces a coupling between adjacentacoustic word models.
Similarly, a rule match whichtypically spans several source phrase matches intro-duces a coupling between adjacent simple edges.Viewed in this way, the handling of copies is atechnique of incorporating higher-level knowledgesources into a simple one-step search process: ei-ther by processing acoustic models in the context ofa language model or by processing simple edges inthe context of bigger re-ordering units, which exploita richer linguistic context.The Earley parser in the presentation (Jurafskyand Martin, 2000) also uses the notion of edgeswhich represent partial constituents derived in theparsing process.
These constituents are interpretedas edges in a directed acyclic graph (DAG) whichrepresents the set of all sub parse trees considered.This paper uses the notion of edges as well fol-lowing (Tillmann, 2006) where phrase-based decod-ing is also linked to a DAG path finding problem.Since the re-order rules are not applied recursively,the rule-driven algorithm can be linked to an Earleyparser where parsing is done with a linear grammar(for a definition of linear grammar see (Harrison,1978)).
A formal analysis of the rule-driven decodermight be important because of the following consid-eration: in phrase-based machine translation the tar-get sentence is generated from left-to-right by con-catenating target phrases linked to source phrasesthat cover some source positions.
Here, a coveragevector is typically used to ensure that each sourceposition is covered a limited number of times (typi-cally once).
Including a coverage vector C into thesearch state definition results in an inherently expo-nential complexity: for an input sentence of lengthJ there are 2J coverage vectors (Koehn, 2004).
On42Table 2: Translation results on the MT06 data.
w is the distortion limit.words / sec generation [%] BLEU PREC TERBaseline decoder w = 0 171.6 1.90 34.6 35.2 65.3w = 2 25.4 0.29 36.6 37.7 63.5w = 5 8.2 0.10 35.0 36.1 65.1Rule decoder N(r) ?
2 9.1 0.75 37.1 38.2 63.5(w = 15) N(r) ?
5 10.5 0.43 37.2 38.2 63.5the contrary, the search state definition in Eq.
4 ex-plicitly avoids the use of a coverage vector result-ing in an essentially linear time decoding algorithm(Section 5 reports the size of the the extended searchgraph in terms of number of edges and shows thatthe number of permutations per POS sequence isless than 2 on average).
The rule-driven algorithmmight be formally correct in the following sense.
Aphrase-based decoder has to generate a phrase align-ment where each source position needs to be cov-ered by exactly one source phrase.
The rule-baseddecoder achieves this by local computation only: 1)no coverage vector is used, 2) the rule edge genera-tion is local to each individual rule, i.e.
looking onlyat the span of that rule, and 3) rules whose appli-cation spans overlap arbitrarily (but not recursively)are handled correctly.
In future, a formal correctnessproof might be given.5 Experimental ResultsWe test the novel edge generation algorithm ona standard Arabic-to-English translation tasks: theMT06 Arabic-English DARPA evaluation set con-sisting of 1 529 sentences with 58 331 Arabic wordsand 4 English reference translations .
The transla-tion model is defined in Eq.
1 where 8 probabilis-tic features (language, translation,distortion model)are used.
The distortion model is similar to (Al-Onaizan and Papineni, 2006).
An on-line algorithmsimilar to (Tillmann and Zhang, 2008) is used totrain the weight vector w. The decoder uses a 5-gram language model , and the phrase table consistsof about 3.2 million phrase pairs.
The phrase tableas well as the probabilistic features are trained on amuch larger training data consisting of 3.8 millionsentences.
Translation results are given in terms ofthe automatic BLEU evaluation metric (Papineni etal., 2002) as well as the TER metric (Snover et al,2006).Our baseline decoder is similar to (Koehn, 2004;Moore and Quirk, 2007).
The goal of the currentpaper is not to demonstrate an improvement in de-coding speed but show the validity of the rule edgegeneration algorithm.
While the baseline and therule-driven decoder are compared with respect tospeed, they are both run with conservatively largebeam thresholds, e.g.
a beam limit of 500 hypothe-ses and a beam threshold of 7.5 (logarithmic scale)per source position j.
The baseline decoder and therule decoder use only 2 stacks to carry out the search(rather than a stack for each source position) (Till-mann, 2006).
No rest-cost estimation is employed.For the results in line 2 the number of phrase ?holes?n in the coverage vector for a left to right traver-sal of the input sentence is restricted using a typi-cal skip-based decoder (Berger et al, 1996).
Up to2 phrases can be skipped.
Additionally, the phrasere-ordering is restricted to take place within a givenwindow size w. The 28, 878 rules used in this paperare obtained from 14 989 manually aligned Arabic-English sentences where the Arabic sentences havebeen segmented and POS tagged .
The rule selec-tion procedure is similar to the one used in (Cregoand Marino, 2006) and rules are extracted that oc-cur at least twice.
The rule-based re-ordering usesan additional probabilistic feature which is derivedfrom the rule unigram count N(r) shown in Table.
1:p(r) = N(r)?r?N(r?)
.
The average number of POS se-quence matches per input sentence is 34.9 where theaverage number of permutations that generate edgesis 57.7.
The average number of simple edges i.e.phrase pairs per input sentence is 751.1.
For therule-based decoder the average number of edges is3187.8 which includes the simple edges.Table 2 presents results that compare the base-line decoder with the rule-driven decoder in terms43of translation performance and decoding speed.
Thesecond column shows the distortion limit used bythe two decoders.
For the rule-based decoder a max-imum distortion limit w is implemented by filter-ing out all the rule matches where the size of therule in terms of number of POS symbols is greaterthan w, i.e.
the rule edges are processed mono-tonically but a monotone rule edge sequence forthe same rule id may not span more than w sourcepositions.
The third column shows the translationspeed in terms of words per second.
The fourthcolumn shows the percentage of CPU time neededfor the edge generation (including both simple andrule edges).
The final three columns report transla-tion results in terms of BLEU , BLEU precisionscore (PREC), and TER.
The rule-based reorder-ing restriction obtains the best translation scores onthe MT06 data: a BLEU score of 37.2 comparedto a BLEU score of 36.6 for the baseline decoder.The statistical significance interval is rather large:2.9 % on this test set as text from various gen-res is included.
Additional visual evaluation on thedev set data shows that some successful phrase re-ordering is carried out by the rule decoder which isnot handled correctly by the baseline decoder.
Ascan be seen from the results reducing the numberof rules by filtering all rules that occur at least 5times (about 10 000 rules) slightly improves trans-lation performance from 37.1 to 37.2.
The edgegeneration accounts for only a small fraction of theoverall decoding time.
Fig.
3 and Fig.
4 demonstrateadditional advantages when using the rule-based de-coder.
Fig.
3 shows the translation BLEU score asa function of the distortion limit window w. TheBLEU score actually decreases for the baseline de-coder as the size w is increased.
The optimal win-dow size is surprisingly small: w = 2.
A simi-lar behavior is also reported in (Moore and Quirk,2007) where w = 5 is used .
For the rule-driven de-coder however the BLEU score does not decreasefor large w: the rules restrict the local re-ordering inthe context of potentially very long POS sequenceswhich makes the re-ordering more reliable.
Fig.
4which shows the decoding speed as a function of thewindow size w demonstrates that the rule-based de-coder actually runs faster than the baseline decoderfor window sizes w ?
5.0.330.340.350.360.370.380.390  2  4  6  8  10  12  14  16BLEUmaximum window sizerule-driven decoder N(r)>=5BLEUrule-driven decoder N(r)>=2BLEUdistortion-limited phrase decoderFigure 3: BLEU score as a function of window size w.101000  2  4  6  8  10  12  14  16CPU[words /sec]maximum window sizerule-driven decoder N(r)>=5CPU[words /sec]rule-driven decoder N(r)>=2CPU[words /sec]distortion-limited phrase decoderFigure 4: Decoding speed as a function of window sizew.6 Discussion and Future WorkThe handling of the re-order rules is most similar towork in (Crego and Marino, 2006) where the rulesare used to create re-order lattices.
To make thisfeasible, the rules have been vigorously filtered in(Crego and Marino, 2006): only about 30 rules areused in their experiments.
On the contrary, the cur-rent approach tightly integrates the re-order rulesinto a phrase-based decoder such that 29 000 rulescan be handled efficiently.
In future work our novelapproach might allow to make use of lexicalized re-order rules as in (Xia and McCord, 2004) or syntac-tic rules as in (Wang et al, 2007).7 AcknowledgmentThis work was supported by the DARPA GALEproject under the contract number HR0011-06-2-00001.
The authors would like to thank the anony-mous reviewers for their detailed criticism.44ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion Models for Statistical Machine Translation.In Proceedings of ACL-COLING?06, pages 529?536,Sydney, Australia, July.Adam L. Berger, Peter F. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, Andrew S. Kehler, andRobert L. Mercer.
1996.
Language Translation Ap-paratus and Method of Using Context-Based Trans-lation Models.
United States Patent, Patent Number5510981, April.David Chiang.
2007.
Hierarchical Machine Translation.Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL?05, pages 531?540, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.J.M.
Crego and Jose?
B. Marino.
2006.
Integration ofPOStag-based Source Reordering into SMT Decodingby an Extended Search Graph.
In Proc.
of AMTA06,pages 29?36, Cambridge, MA, August.Jay Earley.
1970.
An Efficient Context-Free Parsing Al-gorithm.
Communications of the ACM, 13(2):94?102.Michael A. Harrison.
1978.
Introduction to Formal Lan-guage Theory.
Addison Wesley.Fred Jelinek.
1998.
Statistical Methods for SpeechRecognition.
The MIT Press, Cambridge, MA.Daniel Jurafsky and James H. Martin.
2000.
Speech andLanguage Processing.
Prentice Hall.Philipp Koehn, Franz J. Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL?03: Main Proceedings, pages 127?133, Ed-monton, Alberta, Canada, May 27 - June 1.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proceedings of AMTA?04, WashingtonDC, September-October.Robet C. Moore and Chris Quirk.
2007.
Faster Beam-search Decoding for Phrasal SMT.
Proc.
of the MTSummit XI, pages 321?327, September.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A Clustered GlobalPhrase Reordering Model for Statistical MachineTranslation.
In Proceedings of ACL-COLING?06,pages 713?720, Sydney, Australia, July.Franz-Josef Och and Hermann Ney.
2004.
The Align-ment Template Approach to Statistical Machine Trans-lation.
Computational Linguistics, 30(4):417?450.Stefan Ortmanns and Hermann Ney.
2000.
Progress inDynamic Programming Search for LVCSR.
Proc.
ofthe IEEE, 88(8):1224?1240.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for Auto-matic Evaluation of Machine Translation.
In Proc.
ofACL?02, pages 311?318, Philadelphia, PA, July.Matthias Paulik, Kay Rottmann, Jan Niehues, SiljaHildebrand, and Stephan Vogel.
2007.
The ISLPhrase-Based MT System for the 2007 ACL Workshopon SMT.
In Proc.
of the ACL 2007 Second Workshopon SMT, June.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proc.
of AMTA 2006, Boston,MA.Christoph Tillmann and Tong Zhang.
2007.
A Block Bi-gram Prediction Model for Statistical Machine Trans-lation.
ACM-TSLP, 4(6):1?31, July.Christoph Tillmann and Tong Zhang.
2008.
An OnlineRelevant Set Algorithm for Statistical Machine Trans-lation.
Accepted for publication in IEEE Transactionon Audio, Speech, and Language Processing.Christoph Tillmann.
2006.
Efficient Dynamic Program-ming Search Algorithms for Phrase-based SMT.
InProceedings of the Workshop CHPSLP at HLT?06,pages 9?16, New York City, NY, June.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese Syntactic reordering for statistical machinetranslation.
In Proc.
of EMNLP-CoNLL?07, pages737?745, Prague, Czech Republic, July.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proc.
of Coling 2004, pages 508?514,Geneva, Switzerland, Aug 23?Aug 27.
COLING.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Chunk-level Reordering of Source Language Sen-tences with Automatically Learned Rules for Statis-tical Machine Translation.
In Proc.
of SSST, NAACL-HLT?07 / AMTA Workshop, pages 1?8, Rochester, NY,April.45
