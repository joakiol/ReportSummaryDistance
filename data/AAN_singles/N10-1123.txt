Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813?821,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsJoint Inference for Knowledge Extraction from Biomedical LiteratureHoifung Poon?Dept.
of Computer Sci.
& Eng.University of WashingtonSeattle, WA 98195hoifung@cs.washington.eduLucy VanderwendeMicrosoft ResearchRedmond, WA 98052Lucy.Vanderwende@microsoft.comAbstractKnowledge extraction from online reposito-ries such as PubMed holds the promise ofdramatically speeding up biomedical researchand drug design.
After initially focusing onrecognizing proteins and binary interactions,the community has recently shifted their at-tention to the more ambitious task of recogniz-ing complex, nested event structures.
State-of-the-art systems use a pipeline architecture inwhich the candidate events are identified first,and subsequently the arguments.
This failsto leverage joint inference among events andarguments for mutual disambiguation.
Somejoint approaches have been proposed, but theystill lag much behind in accuracy.
In this pa-per, we present the first joint approach for bio-event extraction that obtains state-of-the-artresults.
Our system is based on Markov logicand adopts a novel formulation by jointly pre-dicting events and arguments, as well as indi-vidual dependency edges that compose the ar-gument paths.
On the BioNLP?09 Shared Taskdataset, it reduced F1 errors by more than 10%compared to the previous best joint approach.1 IntroductionExtracting knowledge from unstructured text hasbeen a long-standing goal of NLP and AI.
The ad-vent of the World Wide Web further increases itsimportance and urgency by making available an as-tronomical number of online documents containingvirtually unlimited amount of knowledge (Craven et?
This research was conducted during the author?s intern-ship at Microsoft Research.al., 1999).
A salient example domain is biomedicalliterature: the PubMed1 online repository containsover 18 million abstracts on biomedical research,with more than two thousand new abstracts addedeach day; the abstracts are written in grammaticalEnglish, which enables the use of advanced NLPtools such as syntactic and semantic parsers.Traditionally, research on knowledge extractionfrom text is primarily pursued in the field of in-formation extraction with a rather confined goal ofextracting instances for flat relational schemas withno nested structures (e.g, recognizing protein namesand protein-protein interaction (PPI)).
This restric-tion mainly stems from limitations in available re-sources and algorithms.
The BioNLP?09 SharedTask (Kim et al, 2009) is one of the first thatfaced squarely information needs that are complexand highly structured.
It aims to extract nestedbio-molecular events from research abstracts, wherean event may have variable number of argumentsand may contain other events as arguments.
Suchnested events are ubiquitous in biomedical literatureand can effectively represent complex biomedicalknowledge and subsequently support reasoning andautomated discovery.
The task has generated muchinterest, with twenty-four teams having submittedtheir results.
The top system by UTurku (Bjorne etal., 2009) attained the state-of-the-art F1 of 52.0%.The nested event structures make this task partic-ularly attractive for applying joint inference.
By al-lowing information to propagate among events andarguments, joint inference can facilitate mutual dis-ambiguation and potentially lead to substantial gain1http://www.ncbi.nlm.nih.gov/pubmed813in predictive accuracy.
However, joint inference isunderexplored for this task.
Most participants ei-ther reduced the task to classification (e.g., by usingSVM), or used heuristics to combine manual rulesand statistics.
The previous best joint approach wasRiedel et al (2009).
While competitive, it still lagsUTurku by more than 7 points in F1.In this paper, we present the first joint approachthat achieves state-of-the-art results for bio-event ex-traction.
Like Riedel et al (2009), our systemis based on Markov logic, but we adopted a novelformulation that models dependency edges in ar-gument paths and jointly predicts them along withevents and arguments.
By expanding the scope ofjoint inference to include individual argument edges,our system can leverage fine-grained correlations tomake learning more effective.
On the developmentset, by merely adding a few joint inference formu-las to a simple logistic regression model, our systemraised F1 from 28% to 54%, already tying UTurku.We also presented a heuristic method to fix errorsin syntactic parsing by leveraging available semanticinformation from task input, and showed that this inturn led to substantial performance gain in the task.Overall, our final system reduced F1 error by morethan 10% compared to Riedel et al (2009).We begin by describing the shared task and re-lated work.
We then introduce Markov logic and ourMarkov Logic Network (MLN) for joint bio-eventextraction.
Finally, we present our experimental re-sults and conclude.2 Bio-Event ExtractionWe follow the BioNLP?09 Shared Task (Kim etal., 2009) on problem setup for bio-event extrac-tion.
A bio-molecular event (bio-event) refers tothe change of state for bio-molecules such as DNAsand proteins.
The goal is to extract these eventsfrom unstructured text such as biomedical abstracts.For each event, one needs to identify the triggerwords that signifies the event and the theme argu-ments that undergo the change.
In addition, forregulation events, the cause argument also needs tobe identified if it is present.
The task considersnine event types: Expression, Transcription,Localization, Phosphorylation, Catabolism,Binding, Regulation, Positive regulation,and Negative regulation.
Only Binding cantake multiple themes.
Regulation events may takeevents as arguments.
To facilitate evaluation, thetask fixes the type of non-event arguments to pro-tein and provides ground truth of protein mentionsas input.
2Like any NLP task, ambiguity is a central prob-lem.
The same event can be expressed in manyvariations.
For example, a Negative regulationevent may be signified by ?inhibition?, ?down-regulation?, ?is abrogated by?, to name a few.
Onthe other hand, depending on the context, the sameexpression may represent different events.
For ex-ample, ?level?
may signify any one of five eventtypes in the training set, or signify none.In addition, the nested event structures presentnew challenges to knowledge extraction systems.
Torecognize a complex event, besides from identifyingthe event type and trigger words, one also needs toidentify its arguments and recursively identify theirevent structures.
A mistake in any part will render afailure in this extraction.The interdependencies among events and argu-ments naturally argue for joint predictions.
Forexample, given the snippet ?the level of VCAM-1 mRNA?, knowing that ?level?
might signify anevent helps to recognize the prepositional phrase(PP) as its theme.
Conversely, the presence of thePP suggests that ?level?
is likely an event.
More-over, the word ?mRNA?
in the PP indicates that theevent type is probably Transcription.Most existing systems adopt a pipeline architec-ture and reduce the task to independent classifica-tions of events and arguments.
For example, the bestsystem UTurku (Bjorne et al, 2009) first extracts alist of candidate triggers with types, and then deter-mines for each pair of candidate triggers or proteinswhether one is a theme or cause of the other.
Thetriggers missed in the first stage can never be recov-ered in the second one.
Moreover, since the secondstage is trained with gold triggers as input, any trig-ger identified in the first stage tends to get at least2The Shared Task also defines two other tasks (Tasks 2 and3), which aim either to extract additional arguments (e.g., sites),or to determine if an event is a negation or speculation.
In thispaper, we focus on the core task (Task 1) as it is what most sys-tems participate in, but our approach can be extended straight-forwardly to handle the other tasks.814one argument, even though it may not be an event atall.
As a result, the authors had to use an ad hoc pro-cedure to trade off precision and recall for the finalprediction task while training the first-stage extrac-tor.
In addition, each trigger or argument is classifiedindependently using a multi-class SVM.While joint inference can potentially improve ac-curacy, in practice, it is often very challenging tomake it work (Poon and Domingos, 2007).
The pre-vious best joint approach for this task was proposedby Riedel et al (2009) (labeled UT+DBLS in Kimet al (2009)).
Their system is also based on Markovlogic (Domingos and Lowd, 2009).
While compet-itive (ranked fourth in the evaluation), their systemstill lags UTurku by more than 7 points in F1.Most systems, Riedel et al?s included, classifyeach candidate argument path as a whole.
A notableexception is the UTokyo system (Saetre et al, 2009),which incorporated sequential modeling by adapt-ing a state-of-the-art PPI system based on MEMM.But they considered adjacent words in the sentence,which offered little help in this task, and their systemtrailed UTurku by 15 points in F1.All top systems for event extraction relied heav-ily on syntactic features.
We went one step furtherby formulating joint predictions directly on depen-dency edges.
While this leverages sequential corre-lation along argument paths, it also makes our sys-tem more prone to the adverse effect of syntacticerrors.
Joint syntactic and semantic processing hasreceived much attention lately (Hajic et al, 2009).In this paper, we explore using a heuristic methodto correct syntactic errors based on semantic infor-mation, and show that it leads to significant perfor-mance gain for event extraction.3 Markov LogicIn many NLP applications, there exist rich relationstructures among objects, and recent work in statisti-cal relational learning (Getoor and Taskar, 2007) andstructured prediction (Bakir et al, 2007) has shownthat leveraging these can greatly improve accuracy.One of the leading frameworks for joint inferenceis Markov logic, a probabilistic extension of first-order logic (Domingos and Lowd, 2009).
A Markovlogic network (MLN) is a set of weighted first-orderclauses.
Together with a set of constants, it defines aMarkov network with one node per ground atom andone feature per ground clause.
The weight of a fea-ture is the weight of the first-order clause that gener-ated it.
The probability of a state x in such a networkis given by P (x) = (1/Z) exp (?i wifi(x)), whereZ is a normalization constant, wi is the weight of theith clause, fi = 1 if the ith clause is true, and fi = 0otherwise.Markov logic makes it possible to compactlyspecify probability distributions over complex re-lational domains.
Efficient inference can be per-formed using MC-SAT (Poon and Domingos, 2006).MC-SAT is a ?slice sampling?
Markov chain MonteCarlo algorithm that uses an efficient satisfiabilitysolver to propose the next sample.
It is orders ofmagnitude faster than previous MCMC algorithmslike Gibbs sampling, making efficient sampling pos-sible on a scale that was previously out of reach.Supervised learning for Markov logic maximizesthe conditional log-likelihood of query predicatesgiven the evidence in the train data.
This learningobjective is convex and can be optimized using gra-dient descent, where the gradient is estimated usingMC-SAT.In practice, it is often difficult to tune the learn-ing rate, especially when the number of ground-ings varies widely among clauses (known as ill-conditioning in numerical optimization).
This prob-lem is particularly severe in relational domains.
Oneremedy is to apply preconditioning to the gradient.For example, Poon & Domingos (2007) divided theglobal learning rate by the number of true ground-ings of the corresponding clause in the training data,whereas Lowd & Domingos (2007) divided it by thevariance of the clause (also estimated using MC-SAT).
The latter can be viewed as approximatingthe Hessian with its diagonal, and is guaranteed op-timal when the weights are not correlated (e.g., inlogistic regression).
Lowd & Domingos (2007) alsoused a scaled conjugate gradient algorithm to incor-porate second-order information and further adaptthe search direction.The open-source Alchemy package (Kok et al,2009) provides implementations of existing algo-rithms for Markov logic.8154 An MLN for Joint Bio-Event ExtractionIn this section, we present our MLN for joint bio-event extraction.
As standard for this task, we as-sume that Stanford dependency parses are availablein the input.
Our MLN jointly makes the followingpredictions: for each token, whether it is a triggerword (and if so, what is the event type), and for eachdependency edge, whether it is in an argument pathleading to a theme or cause.To the best of our knowledge, the latter part makesthis formulation a novel one.
By breaking the pre-diction of an argument path into that on individualdependency edges, it can leverage the correlationamong adjacent edges and make learning more ef-fective.
Indeed, compared to other top systems, ourMLN uses a much simpler set of features, but is stillcapable of obtaining state-of-the-art results.3 Com-putationally, this formulation is also attractive.
Thenumber of predictions is bounded by the number oftokens and edges, and is linear in sentence length,rather than quadratic.Our MLN also handles the regulation eventsdifferently.
We notice that events of the threeregulation types often occur in similar contexts, andsometimes share trigger words (e.g., ?involve?
).Therefore, our MLN merges them into a singleevent type Regulation, and additionally predictsthe regulation direction (Positive or Negative).This allows it to pool information shared by thethree types.Base MLN: The following are the main query pred-icates we used, along with descriptions:Event(i): token i signifies an event;EvtType(i, e): i is of event type e;RegType(i, r): i is of regulation type r;InArgPath(i, j, a): the dependency edge from ito j is in an argument path of type a, with abeing either Theme or Cause.If event i has type Positive regulation,both EvtType(i, Regulation) andRegType(i, Positive) are true.
Similarlyfor Negative regulation.
If the type is3In future work, we plan to incorporate a much richer set offeatures; Markov logic makes such extensions straightforward.Table 1: Formulas in the base MLN.Token(i,+t) ?
EvtType(i,+e)Token(i,+t) ?
RegType(i,+r)Token(j,+t) ?
Dep(i, j, d) ?
EvtType(i,+e)Dep(i, j,+d) ?
InArgPath(i, j,+a)Dep(i, j,+d) ?
Prot(i) ?
InArgPath(i, j,+a)Dep(i, j,+d) ?
Prot(j) ?
InArgPath(i, j,+a)Token(i,+t) ?
Dep(i, j,+d) ?
InArgPath(i, j,+a)Token(j,+t) ?
Dep(i, j,+d) ?
InArgPath(i, j,+a)Regulation, only EvtType(i, Regulation) istrue.The main evidence predicates are:Token(i, w): token i has word w;Dep(i, j, d): there is a dependency edge from i toj with label d; 4Prot(i): i is a protein.Our base MLN is a logistic regression model, andcan be succintly captured by eight formulas in Ta-ble 1.
All free variables are implicitly universallyquantified.
The ?+?
notation signifies that the MLNcontains an instance of the formula, with a separateweight, for each value combination of the variableswith a plus sign.
The first three formulas predictthe event type and regulation direction based on thetoken word or its neighbor in the dependency tree.The next five formulas predict whether a depen-dency edge is in an argument path, based on somecombinations of token word, dependency label, andwhether the nodes are proteins.By default, we also added the unit formulas:Theme(x, y), Cause(x, y), EventType(x,+e),RegType(x,+r), which capture default regularities.Joint Inference: Like any classification system, theformulas in the base MLN make independent predic-tions at inference time.
This is suboptimal, becausequery atoms are interdependent due to either hardconstraints (e.g., an event must have a type) or softcorrelation (e.g., ?increase?
signifies an event andthe dobj edge from it leads to a theme).
We thus4For convenience, we include the reverse dependency edgesin the evidence.
For example, if Dep(i, j, nn) is true, then so isDep(j, i,?nn).816augment the base MLN with two groups of joint-inference formulas.
First we incorporate the follow-ing hard constraints.Event(i) ?
?t.
EvtType(i, t)EvtType(i, t) ?
Event(i)RegType(i, r) ?
EvtType(i, Regulation)InArgPath(i, j, Theme) ?
Event(i)?
?
k 6= j. InArgPath(k, i, Theme)InArgPath(i, j, Cause)?
EvtType(i, Regulation)?
?
k 6= j. InArgPath(k, i, Cause)InArgPath(i, j, Theme) ?
Prot(j)?
?
k 6= i. InArgPath(j, k, Theme)InArgPath(i, j, Cause) ?
Event(j) ?
Prot(j)?
?
k 6= i. InArgPath(j, k, Cause)The first three formulas enforce that events musthave a type, that a token assigned an event (regula-tion) type must be an (regulation) event.
The nextfour formulas enforce the consistency of argumentpath assignments: an argument path must start withan event, in particular, a cause path must start with aregulation event; a theme path must eventually traceto a protein, whereas a cause path may also stop atan event (which does not have a cause itself).
Toavoid looping, we forbid reverse edges in a path.5Notice that with these constraints, adjacent edgesin the dependency tree correlate with each otherin their InArgPath assignments, much like in anHMM for linear sequences.
Moreover, these assign-ments correlate with the event and event-type ones;knowing that i probably signifies an event makes iteasier to detect an argument path, and vice versa.In addition, events that share partial argument pathscan inform each other through the predictions onedges.
In the experiments section, we will see thatmerely adding these hard constraints leads to 26-point gain in F1.We also notice that different trigger words mayuse different dependencies to start an argument pathof a particular type.
For example, for many verbs,nsubj tends to start a cause path and dobj a theme5This is violated in some cases, and can be relaxed.
Weenforced it for simplicity in this paper.path.
However, for ?bind?
that signifies a Bindingevent, both lead to themes, as in ?A binds B?.Such soft regularities can be captured by a singlejoint formula: Token(i,+w) ?
Dep(i, j,+d) ?Event(i)?
InArgPath(i, j,+a), which correlatesevent and argument type with token and dependency.Linguistically-Motivated Formulas: Natural lan-guages often possess systematic syntactic alterna-tions.
For example, for the word ?increase?, if bothsubject and object are present, as in ?A increasesthe level of B?, the subject is the cause whereasthe object is the theme.
However, if only sub-ject is present, as in ?The level of B increases?,the subject is the theme.
We thus augment theMLN with a number of context-specific formulassuch as: Token(i, increase)?
Dep(i, j, nsubj)?Dep(i, k, dobj) ?
Event(i) ?
Cause(i, j).65 Learning And InferenceWhen training data comprises of many independentsubsets (e.g., individual abstracts), stochastic gradi-ent descent (SGD) is often a favorable method forparameter learning.
By adopting small and frequentupdates, it can dramatically speed up learning andsometimes even improve accuracy.
Moreover, it eas-ily scales to large datasets since each time it onlyneeds to bring a few subsets into the memory.In this paper, we used SGD to learn weights forour MLN.
During this process, we discovered somegeneral challenges for applying SGD to relationaldomains.
For example, the ill-conditioning problemis particularly severe, and using a single learningrate either makes learning extremely slow or leadsto divergence.
Like Lowd & Domingos (2007),we combat this by dividing the learning rate by thevariance.
However, this still leads to divergence aslearning progresses.
The reason is that some weightsare strongly correlated due to the joint formulas, es-pecially the hard constraints.
Therefore, the diag-onal approximates the Hessian poorly.
Inspired byPoon & Domingos (2007), for each formula, wecount the numbers of true and false groundings inthe train data, and add the smaller of the two plus oneto the variance, before dividing the global rate by it.6Available at http://research.microsoft.com/-en-us/people/lucyv/naacl10.817We found that this is effective for making learningstable in our experiments.To compute the most probable state, we used MC-SAT to estimate the marginal probability of eachquery atom, and returned the ones with probabilityabove a threshold.
This allows us to easily trade offprecision and recall by varying the threshold.
Tospeed up burn-in, we followed Poon et al (2009)and first ran MC-SAT with deterministic annealingfor initialization.6 Correcting Syntactic Errors WithSemantic InformationTwo typical types of syntactic errors are PP-attachment and coordination.
For semantic taskssuch as bio-event extraction, these errors also havethe most adverse impact to performance.
For ex-ample, for the snippet ?involvement of p70 acti-vation in IL-10 up-regulation by gp41?, the Stan-ford parser makes two errors by attaching ?up-regulation?
to ?activation?
instead of ?involvement?,and attaching ?gp41?
to ?involvement?
instead of?up-regulation?.
This makes it very difficult to pre-dict that ?gp41?
is the cause of ?up-regulation?,and that ?up-regulation?
is the theme of ?involve-ment?.
For conjucts such as ?IL-2 and IL-4 ex-pressions?, the parser will align ?IL-2?
with ?ex-pressions?, which makes it difficult to recognize theexpression event on ?IL-2?.
For nested events like?gp41 regulates IL-2 and IL-4 expressions?, this re-sults in three extraction errors: IL-2 expression andthe regulation event on it are missing, whereas anerroneous regulation event on IL-2 is predicted.Syntactic errors are often incurred due to lackof semantic information during parsing (e.g., theknowledge that IL-2 and IL-4 are both proteins).
Inthis paper, we used a heuristic method to fix sucherrors by incorporating two sources of semantic in-formation: argument paths in training data and in-put protein labels.
For conjuncts (signified by prefixconj in Stanford dependencies) between a proteinand a non-protein, we check whether the non-proteinhas a protein child, if so, we remove the conjunct andreattach the first protein to the non-protein.
For PP-attachments, we notice that often the errors can befixed by reattaching the child to the closest node thatfits a known attachment pattern (e.g., ?up-regulationby PROTEIN?).
We used the following heuristics togather attachment patterns.
For each argument pathin the training data, if it consists of a single PP edge,then we add the combination of governor, depen-dency label, and dependent to the pattern.
(Proteinnames are replaced with a special string.)
If a pathcontains multiple edges, but a PP edge attaches to aword to the left of the event trigger (e.g., ?gp41?
at-tached to ?involvement?
), our system concludes thatthe dependent should instead be attached to the trig-ger and adds the corresponding pattern.
In addition,we added a few default patterns like ?involvementin?
and ?effect on?.
For each PP edge, the candi-dates for reattachment include the current governor,and the governor?s parent and all rightmost descen-dants (i.e., its rightmost child, the rightmost child ofthat child, etc.)
that are to the left of the dependent.We reattach the dependent to the closest candidatethat fits an attachment pattern.
If there is none, theattachment remains unchanged.
In total, the fractionof reattachments is about 4%.7 ExperimentsWe evaluated our system on the dataset for Task 1in the BioNLP?09 Shared Task (Kim et al, 2009).It consists of 800 abstracts for training, 150 for de-velopment and 260 for test.
We conducted featuredevelopment and tuned hyperparameters using thedevelopment set, and evaluated our final system ontest using the online tool provided by the organizers.
(The test annotations are not released to the public.
)All results reported were obtained using the mainevaluation criteria for the shared task.77.1 SystemOur system first carries out lemmatization andbreaks up hyphenated words.8 It then uses the Stan-ford parser (de Marneffe et al, 2006) to generate de-pendencies.
For simplicity, if an event contains mul-tiple trigger words, only the head word is labeled.97Namely, ?Approximate Span/Approximate RecursiveMatching?.
See Kim et al (2009) for details.8E.g., ?gp41-induced?
becomes ?gp41?
and ?induced?, witha new dependency edge labeled hyphen from ?induced?
to?gp41?.
To avoid breaking up protein names with hyphens, weonly dehyphenate words with suffix in a small hand-picked list.9Most events have only one trigger, and the chosen wordsonly need to lie within an approximate span in evaluation.818Table 2: Comparison of our full system with its variantsand with UTurku on the development set.Rec.
Prc.
F1BASE 17.4 67.2 27.7BASE+HARD 49.4 58.5 53.6FULL 51.5 60.0 55.5?LING 50.5 59.6 54.7?SYN-FIX 48.2 54.6 51.2UTurku 51.5 55.6 53.5We implemented our system as an extension to theAlchemy system (Kok et al, 2009).
In particular, wedeveloped an efficient parallelized implementationof our stochastic gradient descent algorithm usingthe message-passing interface (MPI).
For learning,we used a mini-batch of 20 abstracts and iteratedthrough the training files twice.
For each mini-batch,we estimated the gradient by running MC-SAT for300 samples; the initialization was done by runningannealed MC-SAT for 200 samples, with tempera-ture dropping from 10 to 0.1 at 0.05 decrements.For inference, we initialized MC-SAT with 1000 an-nealed samples, with temperature dropping from 10to 0.1 at 0.01 decrements, we then ran MC-SAT for5000 samples to compute the marginal probabilities.This implementation is very efficient: learning tookabout 20 minutes in a 32-core cluster with 800 train-ing files; inference took a few minutes in average.To obtain the final assignment, we set the queryatoms with probability no less than 0.4 to true andthe rest to false.
The threshold is chosen to max-imize F1 in the development set.
To generate theevents, we first found arguments for each trigger iby gathering all proteins and event triggers that wereaccessible from i along an argument path withoutfirst encountering another trigger.
For triggers ofbase event types, we dropped other triggers fromits argument list.
For nested triggers, we generatedevents recursively by first processing argument trig-gers and generating their events, and then generatingevents for the parent trigger by including all combi-nations of argument events.
For Binding triggers,we group its arguments by the first dependency la-bels in the argument paths, and generate events by across-product of the group members.Table 3: Per-type recall/precision/F1 for our full systemon the development set.Rec.
Prc.
F1Expression 75.6 79.1 77.3Transcription 69.5 73.1 71.3Phosphorylation 87.2 87.2 87.2Catabolism 85.7 100 92.3Localization 66.0 85.4 74.5Binding 39.1 61.8 47.9Positive regulation 41.8 51.0 46.0Negative regulation 39.3 56.2 46.3Regulation 41.4 33.2 36.87.2 ResultsWe first conducted experiments on the develop-ment set to evaluate the contributions of individualcomponents.
Table 2 compares their performancesalong with that of UTurku.
The base MLN (BASE)alone performed rather poorly.
Surprisingly, by justadding the hard constraints to leverage joint infer-ence (BASE+HARD), our system almost doubledthe F1, and tied UTurku.
In addition, adding thesoft joint-inference formula results in further gain,and our full system (FULL) attained an F1 of 55.5.This is two points higher than UTurku and the bestreported result on this dataset.
The linguistically-motivated formulas are beneficial, as can seen bycomparing with the system without them (?LING),although the difference is small.
Fixing the syntacticerrors with semantic information, on the other hand,leads to substantial performance gain.
Without do-ing it (?SYN-FIX), our system suffers an F1 loss ofmore than four points.
This verifies that the qualityof syntactic analysis is important for event extrac-tion.
The differences between FULL and other vari-ants (except -LING) are all statistically significant at1% level using McNemar?s test.To understand the performance bottlenecks, weshow the per-type results in Table 3 and the re-sults at the predicate level in Table 4.10 Both trig-ger and argument-edge detections leave much roomfor improvement.
In particular, the system pro-posed many incorrect regulation triggers, partly be-cause regulation triggers have the most variations10Numbers in Table 3 refer to events, whereas in Table 4 totriggers.
A trigger may signify multiple events, so numbers inTable 4 can be smaller than that in Table 3.819Table 4: Predicate recall/precision/F1 for our full systemon the development set.Rec.
Prc.
F1Expression 80.1 82.0 81.0Transcription 68.8 71.0 69.8Phosphorylation 87.5 92.1 89.7Catabolism 84.2 100 91.4Localization 62.5 86.2 72.5Binding 62.4 82.4 71.1Positive regulation 65.8 70.7 68.2Negative regulation 58.3 71.7 64.3Regulation 61.7 43.4 50.9All triggers 68.1 71.7 69.9Argument edge 69.0 71.8 70.4Table 5: Comparison of our full system with top systemson the test set.Rec.
Prc.
F1UTurku 46.7 58.5 52.0JULIELab 45.8 47.5 46.7ConcordU 35.0 61.6 44.6Riedel et al 36.9 55.6 44.4FULL MLN 43.7 58.6 50.0among all types.
Our system did well in recognizingBinding triggers, but performed much poorer at theevent level.
This indicates that the bottleneck lies incorrectly identifying all arguments for multi-themeevents.
Indeed, if we evaluate on individual event-theme pairs for Binding, the F1 jumps 15 points to62.8%, with precision 82.7% and recall 50.6%.Finally, Table 5 compares our system with the topsystems on the test set.
Our system trailed UTurkudue to a somewhat lower recall, but substantiallyoutperformed all other systems.
In particular, it re-duced F1 error by more than 10% compared to theprevious best joint approach by Riedel et al (2009).7.3 Error AnalysisThrough manual inspection, we found that many re-maining errors were related to syntactic parses.
Theproblem is particularly severe when there are nestedor co-occuring PP-attachments and conjuncts (e.g.,?increased levels of IL-2 and IL-4 mRNA and pro-tein in the cell?).
Our rule-based procedure in Sec-tion 6 has high precision in fixing some of these er-rors, but the coverage is limited.
It also makes harddecisions in a preprocessing step, which cannot bereverted.
A principled solution is to resolve syntacticand semantic ambiguities in a joint model that inte-grates reattachment decisions and extractions.
Thiscan potentially resolve more syntactic errors, as ex-traction makes more semantic information available,and is more robust to reattachment uncertainty.In some challenging cases, we found further op-portunities for joint inference.
For example, in thesentence ?These cells are deficient in FasL expres-sion, although their cytokine IL-2 production is nor-mal?, ?normal?
signifies a Positive regulationevent over ?IL-2 production?
because of its contrastwith ?deficient?.
Such events can be detected by in-troducing additional joint inference rules that lever-age syntactic structures such as subclauses.We also found many cases where the annota-tions differ for the same expressions.
For ex-ample, ?cotransfection with PROTEIN?
is some-times labeled as both an Expression event and aPositive regulation event, and sometimes notlabeled at all.
This occurs more often for regulationevents, which partly explains the low precision forthem.8 ConclusionThis paper presents the first joint approach for bio-event extraction that achieves state-of-the-art results.This is made possible by adopting a novel formula-tion that jointly predicts events, arguments, as wellas individual dependency edges in argument paths.Our system is based on Markov logic and can beeasily extended to incorporate additional knowledgeand linguistic features to further improve accuracy.Directions for future work include: leveraging ad-ditional joint-inference opportunities, better integra-tion of syntactic parsing and event extraction, andapplying this approach to other extraction tasks anddomains.9 AcknowledgementsWe give warm thanks to Sebastian Riedel and threeanonymous reviewers for helpful comments andsuggestions.820ReferencesG.
Bakir, T. Hofmann, B.
B. Scho?lkopf, A. Smola,B.
Taskar, S. Vishwanathan, and (eds.).
2007.
Pre-dicting Structured Data.
MIT Press, Cambridge, MA.Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings of the BioNLP Workshop2009.Mark Craven, Dan DiPasquo, Dayne Freitag, AndrewMcCallum, Tom Mitchell, Kamal Nigam, and SeanSlattery.
1999.
Learning to construct knowledge basesfrom the world wide web.
Artificial Intelligence.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth International Conference onLanguage Resources and Evaluation, pages 449?454,Genoa, Italy.
ELRA.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan & Claypool, San Rafael, CA.Lise Getoor and Ben Taskar, editors.
2007.
Introductionto Statistical Relational Learning.
MIT Press, Cam-bridge, MA.Jan Hajic, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria Antonia Martii, Lluis Mar-quez, Adam Meyers, Joakim Nivre, Sebastian Pado,Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nian-wen Xue, and Yi Zhang.
2009.
The CoNLL-2009Shared Task: syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Junichi Tsujii.
2009.
Overview ofBioNLP-09 Shared Task on event extraction.
In Pro-ceedings of the BioNLP Workshop 2009.Stanley Kok, Parag Singla, Matt Richardson, PedroDomingos, Marc Sumner, Hoifung Poon, and DanielLowd.
2009.
The alchemy system for statistical re-lational ai.
Technical report, Dept.
of CSE, Univ.
ofWashington, http://alchemy.cs.washington.edu/.Daniel Lowd and Pedro Domingos.
2007.
Efficientweight learning for markov logic networks.
In Pro-ceedings of the Eleventh European Conference onPrinciples and Practice of Knowledge Discovery inDatabases, pages 200?211, Warsaw.
Springer.Hoifung Poon and Pedro Domingos.
2006.
Sound andefficient inference with probabilistic and determinis-tic dependencies.
In Proceedings of the Twenty FirstNational Conference on Artificial Intelligence, pages458?463, Boston, MA.
AAAI Press.Hoifung Poon and Pedro Domingos.
2007.
Joint infer-ence in information extraction.
In Proceedings of theTwenty Second National Conference on Artificial In-telligence, pages 913?918, Vancouver, Canada.
AAAIPress.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentation withlog-linear models.
In Proceedings of NAACL-HLT,Boulder, Colorado.
ACL.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Junichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In Proceedings ofthe BioNLP Workshop 2009.Rune Saetre, Makoto Miwa, Kazuhiro Yoshida, and Ju-nichi Tsujii.
2009.
From protein-protein interactionto molecular event extraction.
In Proceedings of theBioNLP Workshop 2009.821
